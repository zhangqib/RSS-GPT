------------------------------------------------------
Started: 2025-04-17 11:13:00.652375
Existing_entries: 0
Fetching from https://arxiv.org/rss/cs.CL
Feed error: 302
append_entries: 0
Finish: 2025-04-17 11:13:01.353527
------------------------------------------------------
Started: 2025-04-17 11:19:18.997494
Existing_entries: 0
Fetching from https://rss.arxiv.org/rss/cs.CL
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [SFT or RL? An Early Investigation into Training R1-Like Reasoning Large Vision-Language Models](https://arxiv.org/abs/2504.11468)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [ReTool: Reinforcement Learning for Strategic Tool Use in LLMs](https://arxiv.org/abs/2504.11536)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [AskQE: Question Answering as Automatic Evaluation for Machine Translation](https://arxiv.org/abs/2504.11582)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Improving Instruct Models for Free: A Study on Partial Adaptation](https://arxiv.org/abs/2504.11626)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Higher-Order Binding of Language Model Virtual Personas: a Study on Approximating Political Partisan Misperceptions](https://arxiv.org/abs/2504.11673)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Unsupervised Classification of English Words Based on Phonological Information: Discovery of Germanic and Latinate Clusters](https://arxiv.org/abs/2504.11770)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Enhancing Web Agents with Explicit Rollback Mechanisms](https://arxiv.org/abs/2504.11788)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Selective Attention Federated Learning: Improving Privacy and Efficiency for Clinical Text Classification](https://arxiv.org/abs/2504.11793)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Efficient and Adaptive Simultaneous Speech Translation with Fully Unidirectional Architecture](https://arxiv.org/abs/2504.11809)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [ARWI: Arabic Write and Improve](https://arxiv.org/abs/2504.11814)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [D\'ej\`a Vu: Multilingual LLM Evaluation through the Lens of Machine Translation Evaluation](https://arxiv.org/abs/2504.11829)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Could Thinking Multilingually Empower LLM Reasoning?](https://arxiv.org/abs/2504.11833)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [FiSMiness: A Finite State Machine Based Paradigm for Emotional Support Conversations](https://arxiv.org/abs/2504.11837)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Finding Flawed Fictions: Evaluating Complex Reasoning in Language Models via Plot Hole Detection](https://arxiv.org/abs/2504.11900)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [An LLM-as-a-judge Approach for Scalable Gender-Neutral Translation Evaluation](https://arxiv.org/abs/2504.11934)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Robust and Fine-Grained Detection of AI Generated Texts](https://arxiv.org/abs/2504.11952)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [LLM-as-a-Judge: Reassessing the Performance of LLMs in Extractive QA](https://arxiv.org/abs/2504.11972)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [SemEval-2025 Task 3: Mu-SHROOM, the Multilingual Shared Task on Hallucinations and Related Observable Overgeneration Mistakes](https://arxiv.org/abs/2504.11975)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Language Models as Quasi-Crystalline Thought: Structure, Constraint, and Emergence in Generative Systems](https://arxiv.org/abs/2504.11986)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Bayesian dynamic borrowing considering semantic similarity between outcomes for disproportionality analysis in FAERS](https://arxiv.org/abs/2504.12052)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Selective Demonstration Retrieval for Improved Implicit Hate Speech Detection](https://arxiv.org/abs/2504.12082)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Gauging Overprecision in LLMs: An Empirical Study](https://arxiv.org/abs/2504.12098)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Entropy-Guided Watermarking for LLMs: A Test-Time Framework for Robust and Traceable Text Generation](https://arxiv.org/abs/2504.12108)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Multilingual Contextualization of Large Language Models for Document-Level Machine Translation](https://arxiv.org/abs/2504.12140)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Poem Meter Classification of Recited Arabic Poetry: Integrating High-Resource Systems for a Low-Resource Task](https://arxiv.org/abs/2504.12172)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Mapping Controversies Using Artificial Intelligence: An Analysis of the Hamas-Israel Conflict on YouTube](https://arxiv.org/abs/2504.12177)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Trusting CHATGPT: how minor tweaks in the prompts lead to major differences in sentiment classification](https://arxiv.org/abs/2504.12180)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [SALAD: Improving Robustness and Generalization through Contrastive Learning with Structure-Aware and LLM-Driven Augmented Data](https://arxiv.org/abs/2504.12185)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [What Do Large Language Models Know? Tacit Knowledge as a Potential Causal-Explanatory Structure](https://arxiv.org/abs/2504.12187)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning](https://arxiv.org/abs/2504.12216)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [BitNet b1.58 2B4T Technical Report](https://arxiv.org/abs/2504.12285)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [From Conceptual Data Models to Multimodal Representation](https://arxiv.org/abs/2504.11459)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Semantic Matters: Multimodal Features for Affective Analysis](https://arxiv.org/abs/2504.11460)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Language and Knowledge Representation: A Stratified Approach](https://arxiv.org/abs/2504.11492)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Graph-Driven Multimodal Feature Learning Framework for Apparent Personality Assessment](https://arxiv.org/abs/2504.11515)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation](https://arxiv.org/abs/2504.11524)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [GraphicBench: A Planning Benchmark for Graphic Design with Language Agents](https://arxiv.org/abs/2504.11571)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [The Devil is in the Prompts: Retrieval-Augmented Prompt Optimization for Text-to-Video Generation](https://arxiv.org/abs/2504.11739)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Climbing the Ladder of Reasoning: What LLMs Can-and Still Can't-Solve after SFT?](https://arxiv.org/abs/2504.11741)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Evaluating the Goal-Directedness of Large Language Models](https://arxiv.org/abs/2504.11844)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Rethinking LLM-Based Recommendations: A Query Generation-Based, Training-Free Approach](https://arxiv.org/abs/2504.11889)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [ADAT: Time-Series-Aware Adaptive Transformer Architecture for Sign Language Translation](https://arxiv.org/abs/2504.11942)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Efficient Contrastive Decoding with Probabilistic Hallucination Detection - Mitigating Hallucinations in Large Vision Language Models -](https://arxiv.org/abs/2504.12137)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Watermarking Needs Input Repetition Masking](https://arxiv.org/abs/2504.12229)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Advancing Arabic Speech Recognition Through Large-Scale Weakly Supervised Learning](https://arxiv.org/abs/2504.12254)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Dysarthria Normalization via Local Lie Group Transformations for Robust ASR](https://arxiv.org/abs/2504.12279)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Knowledge Graph Reasoning with Self-supervised Reinforcement Learning](https://arxiv.org/abs/2405.13640)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Large Visual-Language Models Are Also Good Classifiers: A Study of In-Context Multimodal Fake News Detection](https://arxiv.org/abs/2407.12879)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Application of AI-based Models for Online Fraud Detection and Analysis](https://arxiv.org/abs/2409.19022)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [ReGenesis: LLMs can Grow into Reasoning Generalists via Self-Improvement](https://arxiv.org/abs/2410.02108)
Append: [Science Out of Its Ivory Tower: Improving Accessibility with Reinforcement Learning](https://arxiv.org/abs/2410.17088)
Append: [Fine-Grained Reward Optimization for Machine Translation using Error Severity Mappings](https://arxiv.org/abs/2411.05986)
Append: [Enhancing LLMs for Power System Simulations: A Feedback-driven Multi-agent Framework](https://arxiv.org/abs/2411.16707)
Append: [Sequence-Level Leakage Risk of Training Data in Large Language Models](https://arxiv.org/abs/2412.11302)
Append: [Automatic Item Generation for Personality Situational Judgment Tests with Large Language Models](https://arxiv.org/abs/2412.12144)
Append: [Enhancing Privacy in the Early Detection of Sexual Predators Through Federated Learning and Differential Privacy](https://arxiv.org/abs/2501.12537)
Append: [Visual Theory of Mind Enables the Invention of Proto-Writing](https://arxiv.org/abs/2502.01568)
Append: [How Inclusively do LMs Perceive Social and Moral Norms?](https://arxiv.org/abs/2502.02696)
Append: [Transforming Science with Large Language Models: A Survey on AI-assisted Scientific Discovery, Experimentation, Content Generation, and Evaluation](https://arxiv.org/abs/2502.05151)
Append: [Automatic Input Rewriting Improves Translation with Large Language Models](https://arxiv.org/abs/2502.16682)
Append: [Figurative Archive: an open dataset and web-based application for the study of metaphor](https://arxiv.org/abs/2503.00444)
Append: [FastCuRL: Curriculum Reinforcement Learning with Progressive Context Extension for Efficient Training R1-like Reasoning Models](https://arxiv.org/abs/2503.17287)
Append: [What, How, Where, and How Well? A Survey on Test-Time Scaling in Large Language Models](https://arxiv.org/abs/2503.24235)
Append: [Assessing how hyperparameters impact Large Language Models' sarcasm detection performance](https://arxiv.org/abs/2504.06166)
Append: [Evaluation Under Imperfect Benchmarks and Ratings: A Case Study in Text Simplification](https://arxiv.org/abs/2504.09394)
Append: [Syzygy of Thoughts: Improving LLM CoT with the Minimal Free Resolution](https://arxiv.org/abs/2504.09566)
Append: [LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in Current Benchmarks](https://arxiv.org/abs/2504.10185)
Append: [Exploring the Role of Knowledge Graph-Based RAG in Japanese Medical Question Answering with Small-Scale LLMs](https://arxiv.org/abs/2504.10982)
Append: [Automated Python Translation](https://arxiv.org/abs/2504.11290)
Append: [Local Grammar-Based Coding Revisited](https://arxiv.org/abs/2209.13636)
Append: [StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text](https://arxiv.org/abs/2403.14773)
Append: [Taming Data and Transformers for Audio Generation](https://arxiv.org/abs/2406.19388)
Append: [Natural Language Outlines for Code: Literate Programming in the LLM Era](https://arxiv.org/abs/2408.04820)
Append: [RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)](https://arxiv.org/abs/2409.02920)
Append: [Knowledge-Driven Feature Selection and Engineering for Genotype Data with Large Language Models](https://arxiv.org/abs/2410.01795)
Append: [No Need to Talk: Asynchronous Mixture of Language Models](https://arxiv.org/abs/2410.03529)
Append: [Leveraging Social Determinants of Health in Alzheimer's Research Using LLM-Augmented Literature Mining and Knowledge Graphs](https://arxiv.org/abs/2410.09080)
Append: [Document Parsing Unveiled: Techniques, Challenges, and Prospects for Structured Information Extraction](https://arxiv.org/abs/2410.21169)
Append: [Bridging the Visual Gap: Fine-Tuning Multimodal Models with Knowledge-Adapted Captions](https://arxiv.org/abs/2411.09018)
Append: [BPP-Search: Enhancing Tree of Thought Reasoning for Mathematical Modeling Problem Solving](https://arxiv.org/abs/2411.17404)
Append: [Orthus: Autoregressive Interleaved Image-Text Generation with Modality-Specific Heads](https://arxiv.org/abs/2412.00127)
Append: [ChaosEater: Fully Automating Chaos Engineering with Large Language Models](https://arxiv.org/abs/2501.11107)
Append: [FourierNAT: A Fourier-Mixing-Based Non-Autoregressive Transformer for Parallel Sequence Generation](https://arxiv.org/abs/2503.07630)
Append: [Think Before Recommend: Unleashing the Latent Reasoning Power for Sequential Recommendation](https://arxiv.org/abs/2503.22675)
Append: [Task Memory Engine (TME): A Structured Memory Framework with Graph-Aware Extensions for Multi-Step LLM Agent Tasks](https://arxiv.org/abs/2504.08525)
Append: [UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction Synthesis](https://arxiv.org/abs/2504.11257)
append_entries: 86
Finish: 2025-04-17 11:19:34.176790
------------------------------------------------------
Started: 2025-04-17 11:36:05.707066
Existing_entries: 86
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-17 11:36:05.882828
------------------------------------------------------
Started: 2025-04-17 12:31:42.607215
Existing_entries: 86
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-17 12:31:42.750078
------------------------------------------------------
Started: 2025-04-17 14:15:25.880323
Existing_entries: 86
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-17 14:15:26.002056
------------------------------------------------------
Started: 2025-04-17 16:20:08.581711
Existing_entries: 86
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-17 16:20:08.710971
------------------------------------------------------
Started: 2025-04-17 18:21:33.755192
Existing_entries: 86
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-17 18:21:33.931093
------------------------------------------------------
Started: 2025-04-17 20:17:29.620947
Existing_entries: 86
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-17 20:17:29.774904
------------------------------------------------------
Started: 2025-04-17 22:14:51.190252
Existing_entries: 86
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-17 22:14:51.351165
------------------------------------------------------
Started: 2025-04-18 01:14:25.579571
Existing_entries: 86
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-18 01:14:25.739458
------------------------------------------------------
Started: 2025-04-18 02:57:56.634221
Existing_entries: 86
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-18 02:57:56.791011
------------------------------------------------------
Started: 2025-04-18 04:22:12.545194
Existing_entries: 86
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1968
Summarized using GPT-3.5-turbo
Append: [Unmasking the Reality of PII Masking Models: Performance Gaps and the Call for Accountability](https://arxiv.org/abs/2504.12308)
Token length: 1411
Summarized using GPT-3.5-turbo
Append: [Learning Optimal Prompt Ensemble for Multi-source Visual Prompt Transfer](https://arxiv.org/abs/2504.12311)
Token length: 1587
Summarized using GPT-3.5-turbo
Append: [Socrates or Smartypants: Testing Logic Reasoning Capabilities of Large Language Models with Logic Programming-based Test Oracles](https://arxiv.org/abs/2504.12312)
Token length: 1299
Summarized using GPT-3.5-turbo
Append: [Exploring the Impact of Personality Traits on Conversational Recommender Systems: A Simulation with Large Language Models](https://arxiv.org/abs/2504.12313)
Token length: 1267
Summarized using GPT-3.5-turbo
Append: [How to Detect and Defeat Molecular Mirage: A Metric-Driven Benchmark for Hallucination in LLM-based Molecular Comprehension](https://arxiv.org/abs/2504.12314)
Token length: 1510
Summarized using GPT-3.5-turbo
Append: [Capybara-OMNI: An Efficient Paradigm for Building Omni-Modal Language Models](https://arxiv.org/abs/2504.12315)
Token length: 1184
Summarized using GPT-3.5-turbo
Append: [Data Metabolism: An Efficient Data Design Schema For Vision Language Model](https://arxiv.org/abs/2504.12316)
Token length: 1015
Summarized using GPT-3.5-turbo
Append: [ChatGPT as Linguistic Equalizer? Quantifying LLM-Driven Lexical Shifts in Academic Writing](https://arxiv.org/abs/2504.12317)
Token length: 1663
Summarized using GPT-3.5-turbo
Append: [Has the Creativity of Large-Language Models peaked? An analysis of inter- and intra-LLM variability](https://arxiv.org/abs/2504.12320)
Token length: 1875
Summarized using GPT-3.5-turbo
Append: [AttentionDefense: Leveraging System Prompt Attention for Explainable Defense Against Novel Jailbreaks](https://arxiv.org/abs/2504.12321)
Token length: 1687
Summarized using GPT-3.5-turbo
Append: [A Strategic Coordination Framework of Small LLMs Matches Large LLMs in Data Synthesis](https://arxiv.org/abs/2504.12322)
Token length: 1709
Summarized using GPT-3.5-turbo
Append: [The Other Side of the Coin: Exploring Fairness in Retrieval-Augmented Generation](https://arxiv.org/abs/2504.12323)
Token length: 1637
Summarized using GPT-3.5-turbo
Append: [Cross-Document Cross-Lingual Natural Language Inference via RST-enhanced Graph Fusion and Interpretability Prediction](https://arxiv.org/abs/2504.12324)
Token length: 888
Summarized using GPT-3.5-turbo
Append: [LLMTaxo: Leveraging Large Language Models for Constructing Taxonomy of Factual Claims from Social Media](https://arxiv.org/abs/2504.12325)
Token length: 1299
Summarized using GPT-3.5-turbo
Append: [Reconstructing Sepsis Trajectories from Clinical Case Reports using LLMs: the Textual Time Series Corpus for Sepsis](https://arxiv.org/abs/2504.12326)
Token length: 1214
Summarized using GPT-3.5-turbo
Append: [Word Embeddings Track Social Group Changes Across 70 Years in China](https://arxiv.org/abs/2504.12327)
Token length: 870
Summarized using GPT-3.5-turbo
Append: [A Comprehensive Survey of Reward Models: Taxonomy, Applications, Challenges, and Future](https://arxiv.org/abs/2504.12328)
Token length: 1454
Summarized using GPT-3.5-turbo
Append: [Speculative Thinking: Enhancing Small-Model Reasoning with Large Model Guidance at Inference Time](https://arxiv.org/abs/2504.12329)
Token length: 1809
Summarized using GPT-3.5-turbo
Append: [HM-RAG: Hierarchical Multi-Agent Multimodal Retrieval Augmented Generation](https://arxiv.org/abs/2504.12330)
Token length: 1779
Summarized using GPT-3.5-turbo
Append: [Span-level Emotion-Cause-Category Triplet Extraction with Instruction Tuning LLMs and Data Augmentation](https://arxiv.org/abs/2504.12331)
Token length: 1114
Summarized using GPT-3.5-turbo
Append: [Can the capability of Large Language Models be described by human ability? A Meta Study](https://arxiv.org/abs/2504.12332)
Token length: 1378
Summarized using GPT-3.5-turbo
Append: [Meta-Evaluating Local LLMs: Rethinking Performance Metrics for Serious Games](https://arxiv.org/abs/2504.12333)
Token length: 1438
Summarized using GPT-3.5-turbo
Append: [QM-ToT: A Medical Tree of Thoughts Reasoning Framework for Quantized Model](https://arxiv.org/abs/2504.12334)
Token length: 944
Summarized using GPT-3.5-turbo
Append: [You've Changed: Detecting Modification of Black-Box Large Language Models](https://arxiv.org/abs/2504.12335)
Token length: 1372
Summarized using GPT-3.5-turbo
Append: ["It Listens Better Than My Therapist": Exploring Social Media Discourse on LLMs as Mental Health Tool](https://arxiv.org/abs/2504.12337)
Token length: 1378
Summarized using GPT-3.5-turbo
Append: [Paging Dr. GPT: Extracting Information from Clinical Notes to Enhance Patient Predictions](https://arxiv.org/abs/2504.12338)
Token length: 1515
Summarized using GPT-3.5-turbo
Append: [GOAT-TTS: LLM-based Text-To-Speech Generation Optimized via A Dual-Branch Architecture](https://arxiv.org/abs/2504.12339)
Token length: 1191
Summarized using GPT-3.5-turbo
Append: [Streamlining Biomedical Research with Specialized LLMs](https://arxiv.org/abs/2504.12341)
Token length: 1134
Summarized using GPT-3.5-turbo
Append: [Benchmarking Biopharmaceuticals Retrieval-Augmented Generation Evaluation](https://arxiv.org/abs/2504.12342)
Token length: 1613
Summarized using GPT-3.5-turbo
Append: [Propaganda via AI? A Study on Semantic Backdoors in Large Language Models](https://arxiv.org/abs/2504.12344)
Token length: 1290
Summarized using GPT-3.5-turbo
Append: [Reimagining Urban Science: Scaling Causal Inference with Large Language Models](https://arxiv.org/abs/2504.12345)
Token length: 837
Summarized using GPT-3.5-turbo
Append: [Mathematical Capabilities of Large Language Models in Finnish Matriculation Examination](https://arxiv.org/abs/2504.12347)
Token length: 1064
Summarized using GPT-3.5-turbo
Append: [A Large-Language Model Framework for Relative Timeline Extraction from PubMed Case Reports](https://arxiv.org/abs/2504.12350)
Token length: 898
Summarized using GPT-3.5-turbo
Append: [Leveraging Large Language Models for Multi-Class and Multi-Label Detection of Drug Use and Overdose Symptoms on Social Media](https://arxiv.org/abs/2504.12355)
Token length: 639
Summarized using GPT-3.5-turbo
Append: [Replicating ReLM Results: Validating Large Language Models with ReLM](https://arxiv.org/abs/2504.12357)
Token length: 913
Summarized using GPT-3.5-turbo
Append: [A Method for Handling Negative Similarities in Explainable Graph Spectral Clustering of Text Documents -- Extended Version](https://arxiv.org/abs/2504.12360)
Token length: 1311
Summarized using GPT-3.5-turbo
Append: [Position: The Most Expensive Part of an LLM should be its Training Data](https://arxiv.org/abs/2504.12427)
Token length: 1934
Summarized using GPT-3.5-turbo
Append: [On Linear Representations and Pretraining Data Frequency in Language Models](https://arxiv.org/abs/2504.12459)
Token length: 1259
Summarized using GPT-3.5-turbo
Append: [SLURG: Investigating the Feasibility of Generating Synthetic Online Fallacious Discourse](https://arxiv.org/abs/2504.12466)
Token length: 1335
Summarized using GPT-3.5-turbo
Append: [Integrating Structural and Semantic Signals in Text-Attributed Graphs with BiGTex](https://arxiv.org/abs/2504.12474)
Token length: 1294
Summarized using GPT-3.5-turbo
Append: [Can Pre-training Indicators Reliably Predict Fine-tuning Outcomes of LLMs?](https://arxiv.org/abs/2504.12491)
Token length: 1197
Summarized using GPT-3.5-turbo
Append: [Accelerating Clinical NLP at Scale with a Hybrid Framework with Reduced GPU Demands: A Case Study in Dementia Identification](https://arxiv.org/abs/2504.12494)
Token length: 1233
Summarized using GPT-3.5-turbo
Append: [Beyond Text: Characterizing Domain Expert Needs in Document Research](https://arxiv.org/abs/2504.12495)
Token length: 904
Summarized using GPT-3.5-turbo
Append: [BrowseComp: A Simple Yet Challenging Benchmark for Browsing Agents](https://arxiv.org/abs/2504.12516)
Token length: 1570
Summarized using GPT-3.5-turbo
Append: [Evaluating the Diversity and Quality of LLM Generated Content](https://arxiv.org/abs/2504.12522)
Token length: 1283
Summarized using GPT-3.5-turbo
Append: [Memorization vs. Reasoning: Updating LLMs with New Knowledge](https://arxiv.org/abs/2504.12523)
Token length: 1108
Summarized using GPT-3.5-turbo
Append: [Memorization: A Close Look at Books](https://arxiv.org/abs/2504.12549)
Token length: 1453
Summarized using GPT-3.5-turbo
Append: [ELAB: Extensive LLM Alignment Benchmark in Persian Language](https://arxiv.org/abs/2504.12553)
Token length: 1303
Summarized using GPT-3.5-turbo
Append: [CDF-RAG: Causal Dynamic Feedback for Adaptive Retrieval-Augmented Generation](https://arxiv.org/abs/2504.12560)
Token length: 1595
Summarized using GPT-3.5-turbo
Append: [MetaSynth: Meta-Prompting-Driven Agentic Scaffolds for Diverse Synthetic Data Generation](https://arxiv.org/abs/2504.12563)
Append: [Identifying and Mitigating the Influence of the Prior Distribution in Large Language Models](https://arxiv.org/abs/2504.12585)
Append: [GeoSense: Evaluating Identification and Application of Geometric Principles in Multimodal Reasoning](https://arxiv.org/abs/2504.12597)
Append: [Towards Characterizing Subjectivity of Individuals through Modeling Value Conflicts and Trade-offs](https://arxiv.org/abs/2504.12633)
Append: [Scaling Instruction-Tuned LLMs to Million-Token Contexts via Hierarchical Synthetic Data Generation](https://arxiv.org/abs/2504.12637)
Append: [Persona-judge: Personalized Alignment of Large Language Models via Token-level Self-judgment](https://arxiv.org/abs/2504.12663)
Append: [ACoRN: Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models](https://arxiv.org/abs/2504.12673)
Append: [GRAIL: Gradient-Based Adaptive Unlearning for Privacy and Copyright in LLMs](https://arxiv.org/abs/2504.12681)
Append: [Data-efficient LLM Fine-tuning for Code Generation](https://arxiv.org/abs/2504.12687)
Append: [Why and How LLMs Hallucinate: Connecting the Dots with Subsequence Associations](https://arxiv.org/abs/2504.12691)
Append: [KODIS: A Multicultural Dispute Resolution Dialogue Corpus](https://arxiv.org/abs/2504.12723)
Append: [Pandora: A Code-Driven Large Language Model Agent for Unified Reasoning Across Diverse Structured Knowledge](https://arxiv.org/abs/2504.12734)
Append: [Chinese-Vicuna: A Chinese Instruction-following Llama-based Model](https://arxiv.org/abs/2504.12737)
Append: [Out of Sight Out of Mind, Out of Sight Out of Mind: Measuring Bias in Language Models Against Overlooked Marginalized Groups in Regional Contexts](https://arxiv.org/abs/2504.12767)
Append: [Enhancing the Geometric Problem-Solving Ability of Multimodal LLMs via Symbolic-Neural Integration](https://arxiv.org/abs/2504.12773)
Append: [Assesing LLMs in Art Contexts: Critique Generation and Theory of Mind Evaluation](https://arxiv.org/abs/2504.12805)
Append: [SMARTe: Slot-based Method for Accountable Relational Triple extraction](https://arxiv.org/abs/2504.12816)
Append: [Can LLMs reason over extended multilingual contexts? Towards long-context evaluation beyond retrieval and haystacks](https://arxiv.org/abs/2504.12845)
Append: [ViClaim: A Multilingual Multilabel Dataset for Automatic Claim Detection in Videos](https://arxiv.org/abs/2504.12882)
Append: [Are AI agents the new machine translation frontier? Challenges and opportunities of single- and multi-agent systems for multilingual digital communication](https://arxiv.org/abs/2504.12891)
Append: [Information Gain-Guided Causal Intervention for Autonomous Debiasing Large Language Models](https://arxiv.org/abs/2504.12898)
Append: [Benchmarking Multi-National Value Alignment for Large Language Models](https://arxiv.org/abs/2504.12911)
Append: [MAIN: Mutual Alignment Is Necessary for instruction tuning](https://arxiv.org/abs/2504.12913)
Append: [ConExion: Concept Extraction with Large Language Models](https://arxiv.org/abs/2504.12915)
Append: [Are Retrials All You Need? Enhancing Large Language Model Reasoning Without Verbalized Feedback](https://arxiv.org/abs/2504.12951)
Append: [Estimating Optimal Context Length for Hybrid Retrieval-augmented Multi-document Summarization](https://arxiv.org/abs/2504.12972)
Append: [Sparks of Science: Hypothesis Generation Using Structured Paper Data](https://arxiv.org/abs/2504.12976)
Append: [Accommodate Knowledge Conflicts in Retrieval-augmented LLMs: Towards Reliable Response Generation in the Wild](https://arxiv.org/abs/2504.12982)
Append: [SHA256 at SemEval-2025 Task 4: Selective Amnesia -- Constrained Unlearning for Large Language Models via Knowledge Isolation](https://arxiv.org/abs/2504.12996)
Append: [ChatEXAONEPath: An Expert-level Multimodal Large Language Model for Histopathology Using Whole Slide Images](https://arxiv.org/abs/2504.13023)
Append: [Aspect-Based Summarization with Self-Aspect Retrieval Enhanced Generation](https://arxiv.org/abs/2504.13054)
Append: [Accuracy is Not Agreement: Expert-Aligned Evaluation of Crash Narrative Classification Models](https://arxiv.org/abs/2504.13068)
Append: [Retrieval-Augmented Generation with Conflicting Evidence](https://arxiv.org/abs/2504.13079)
Append: [LLMs Meet Finance: Fine-Tuning Foundation Models for the Open FinLLM Leaderboard](https://arxiv.org/abs/2504.13125)
Append: [Energy-Based Reward Models for Robust Language Model Alignment](https://arxiv.org/abs/2504.13134)
Append: [Syntactic and Semantic Control of Large Language Models via Sequential Monte Carlo](https://arxiv.org/abs/2504.13139)
Append: [CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for Language Model Pre-training](https://arxiv.org/abs/2504.13161)
Append: [Large Language Model-Based Knowledge Graph System Construction for Sustainable Development Goals: An AI-Based Speculative Design Perspective](https://arxiv.org/abs/2504.12309)
Append: [Specialized text classification: an approach to classifying Open Banking transactions](https://arxiv.org/abs/2504.12319)
Append: [A Human-AI Comparative Analysis of Prompt Sensitivity in LLM-Based Relevance Judgment](https://arxiv.org/abs/2504.12408)
Append: [Towards Conversational AI for Human-Machine Collaborative MLOps](https://arxiv.org/abs/2504.12477)
Append: [Knowledge Acquisition on Mass-shooting Events via LLMs for AI-Driven Justice](https://arxiv.org/abs/2504.12545)
Append: [Benchmarking LLM-based Relevance Judgment Methods](https://arxiv.org/abs/2504.12558)
Append: [ZeroSumEval: Scaling LLM Evaluation with Inter-Model Competition](https://arxiv.org/abs/2504.12562)
Append: [Provable Secure Steganography Based on Adaptive Dynamic Sampling](https://arxiv.org/abs/2504.12579)
Append: [Simplifying Graph Transformers](https://arxiv.org/abs/2504.12588)
Append: [VLMGuard-R1: Proactive Safety Alignment for VLMs via Reasoning-Driven Prompt Optimization](https://arxiv.org/abs/2504.12661)
Append: [WebLists: Extracting Structured Information From Complex Interactive Websites Using Executable LLM Agents](https://arxiv.org/abs/2504.12682)
Append: [Towards Lossless Token Pruning in Late-Interaction Retrieval Models](https://arxiv.org/abs/2504.12778)
Append: [EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text Prompting](https://arxiv.org/abs/2504.12867)
Append: [Building Russian Benchmark for Evaluation of Information Retrieval Models](https://arxiv.org/abs/2504.12879)
Append: [A Phenomenological Approach to Analyzing User Queries in IT Systems Using Heidegger's Fundamental Ontology](https://arxiv.org/abs/2504.12977)
Append: [How Large Language Models Are Changing MOOC Essay Answers: A Comparison of Pre- and Post-LLM Responses](https://arxiv.org/abs/2504.13038)
Append: [RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins](https://arxiv.org/abs/2504.13059)
Append: [Tackling Social Bias against the Poor: A Dataset and Taxonomy on Aporophobia](https://arxiv.org/abs/2504.13085)
Append: [Probing and Inducing Combinational Creativity in Vision-Language Models](https://arxiv.org/abs/2504.13120)
Append: [FreshStack: Building Realistic Benchmarks for Evaluating Retrieval on Technical Documents](https://arxiv.org/abs/2504.13128)
Append: [Antidistillation Sampling](https://arxiv.org/abs/2504.13146)
Append: [MIB: A Mechanistic Interpretability Benchmark](https://arxiv.org/abs/2504.13151)
Append: [Sleep-time Compute: Beyond Inference Scaling at Test-time](https://arxiv.org/abs/2504.13171)
Append: [SemCORE: A Semantic-Enhanced Generative Cross-Modal Retrieval Framework with MLLMs](https://arxiv.org/abs/2504.13172)
Append: [Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation](https://arxiv.org/abs/2207.14000)
Append: [Baichuan 2: Open Large-scale Language Models](https://arxiv.org/abs/2309.10305)
Append: [Why Lift so Heavy? Slimming Large Language Models by Cutting Off the Layers](https://arxiv.org/abs/2402.11700)
Append: [Citation-Enhanced Generation for LLM-based Chatbots](https://arxiv.org/abs/2402.16063)
Append: [MemLLM: Finetuning LLMs to Use An Explicit Read-Write Memory](https://arxiv.org/abs/2404.11672)
Append: [Fleet of Agents: Coordinated Problem Solving with Large Language Models](https://arxiv.org/abs/2405.06691)
Append: [Unipa-GPT: Large Language Models for university-oriented QA in Italian](https://arxiv.org/abs/2407.14246)
Append: [ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities](https://arxiv.org/abs/2408.04682)
Append: [Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and Model Size in Large Language Models From Edge to Giant](https://arxiv.org/abs/2409.11055)
Append: [Evaluating Semantic Variation in Text-to-Image Synthesis: A Causal Perspective](https://arxiv.org/abs/2410.10291)
Append: [In-context KV-Cache Eviction for LLMs via Attention-Gate](https://arxiv.org/abs/2410.12876)
Append: [Do Vision-Language Models Represent Space and How? Evaluating Spatial Frame of Reference Under Ambiguities](https://arxiv.org/abs/2410.17385)
Append: [IdentifyMe: A Challenging Long-Context Mention Resolution Benchmark for LLMs](https://arxiv.org/abs/2411.07466)
Append: [AMPS: ASR with Multimodal Paraphrase Supervision](https://arxiv.org/abs/2411.18368)
Append: [Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Step Arithmetic Reasoning](https://arxiv.org/abs/2412.01113)
Append: [Arabizi vs LLMs: Can the Genie Understand the Language of Aladdin?](https://arxiv.org/abs/2502.20973)
Append: [ZeroSumEval: An Extensible Framework For Scaling LLM Evaluation with Inter-Model Competition](https://arxiv.org/abs/2503.10673)
Append: [Multi-Stakeholder Disaster Insights from Social Media Using Large Language Models](https://arxiv.org/abs/2504.00046)
Append: [OnRL-RAG: Real-Time Personalized Mental Health Dialogue System](https://arxiv.org/abs/2504.02894)
Append: [FuseRL: Dense Preference Optimization for Heterogeneous Model Fusion](https://arxiv.org/abs/2504.06562)
Append: [Evaluating the Bias in LLMs for Surveying Opinion and Decision Making in Healthcare](https://arxiv.org/abs/2504.08260)
Append: [Taxonomy and Analysis of Sensitive User Queries in Generative AI Search](https://arxiv.org/abs/2404.08672)
Append: [ALCM: Autonomous LLM-Augmented Causal Discovery Framework](https://arxiv.org/abs/2405.01744)
Append: [ValueCompass: A Framework for Measuring Contextual Value Alignment Between Human and LLMs](https://arxiv.org/abs/2409.09586)
Append: [Multi-Field Adaptive Retrieval](https://arxiv.org/abs/2410.20056)
Append: [Multimodal LLMs Can Reason about Aesthetics in Zero-Shot](https://arxiv.org/abs/2501.09012)
Append: [Contextual Agent Security: A Policy for Every Purpose](https://arxiv.org/abs/2501.17070)
Append: [DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments](https://arxiv.org/abs/2504.03160)
Append: [SIFT-50M: A Large-Scale Multilingual Dataset for Speech Instruction Fine-Tuning](https://arxiv.org/abs/2504.09081)
Append: [CameraBench: Benchmarking Visual Reasoning in MLLMs via Photography](https://arxiv.org/abs/2504.10090)
Append: [CSPLADE: Learned Sparse Retrieval with Causal Language Models](https://arxiv.org/abs/2504.10816)
append_entries: 141
Finish: 2025-04-18 04:23:56.116842
------------------------------------------------------
Started: 2025-04-18 06:22:49.448041
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-18 06:22:49.651237
------------------------------------------------------
Started: 2025-04-18 08:20:28.573579
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-18 08:20:28.802837
------------------------------------------------------
Started: 2025-04-18 10:16:58.114721
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-18 10:16:58.334868
------------------------------------------------------
Started: 2025-04-18 12:30:48.575312
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-18 12:30:48.782049
------------------------------------------------------
Started: 2025-04-18 14:14:26.630654
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-18 14:14:26.886108
------------------------------------------------------
Started: 2025-04-18 16:19:00.392459
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-18 16:19:00.604683
------------------------------------------------------
Started: 2025-04-18 18:21:10.218530
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-18 18:21:10.449185
------------------------------------------------------
Started: 2025-04-18 20:16:52.105165
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-18 20:16:52.304820
------------------------------------------------------
Started: 2025-04-18 22:15:47.891963
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-18 22:15:48.104332
------------------------------------------------------
Started: 2025-04-19 01:12:23.296762
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-19 01:12:23.505671
------------------------------------------------------
Started: 2025-04-19 02:54:27.494880
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-19 02:54:27.699064
------------------------------------------------------
Started: 2025-04-19 04:18:13.814489
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-19 04:18:13.860699
------------------------------------------------------
Started: 2025-04-19 06:20:49.522606
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-19 06:20:49.571120
------------------------------------------------------
Started: 2025-04-19 08:18:19.457168
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-19 08:18:19.506663
------------------------------------------------------
Started: 2025-04-19 10:14:57.449370
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-19 10:14:57.514971
------------------------------------------------------
Started: 2025-04-19 12:28:10.834622
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-19 12:28:10.896698
------------------------------------------------------
Started: 2025-04-19 14:13:29.774947
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-19 14:13:29.817857
------------------------------------------------------
Started: 2025-04-19 16:19:00.848640
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-19 16:19:00.895567
------------------------------------------------------
Started: 2025-04-19 18:19:24.418417
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-19 18:19:24.467285
------------------------------------------------------
Started: 2025-04-19 20:16:14.713065
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-19 20:16:14.756591
------------------------------------------------------
Started: 2025-04-19 22:14:08.761221
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-19 22:14:08.808443
------------------------------------------------------
Started: 2025-04-20 01:20:16.180253
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-20 01:20:16.224170
------------------------------------------------------
Started: 2025-04-20 03:05:48.313731
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-20 03:05:48.376659
------------------------------------------------------
Started: 2025-04-20 04:17:47.627738
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-20 04:17:47.679948
------------------------------------------------------
Started: 2025-04-20 06:20:54.713747
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-20 06:20:54.774109
------------------------------------------------------
Started: 2025-04-20 08:18:10.390530
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-20 08:18:10.441756
------------------------------------------------------
Started: 2025-04-20 10:15:19.146242
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-20 10:15:19.205367
------------------------------------------------------
Started: 2025-04-20 12:28:45.757837
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-20 12:28:45.799343
------------------------------------------------------
Started: 2025-04-20 14:13:54.969232
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-20 14:13:55.015833
------------------------------------------------------
Started: 2025-04-20 16:17:31.583863
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-20 16:17:31.649257
------------------------------------------------------
Started: 2025-04-20 18:19:58.800955
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-20 18:19:58.893111
------------------------------------------------------
Started: 2025-04-20 20:16:04.221792
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-20 20:16:04.296078
------------------------------------------------------
Started: 2025-04-20 22:14:46.880363
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-20 22:14:46.959585
------------------------------------------------------
Started: 2025-04-21 01:18:44.968624
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-21 01:18:45.016905
------------------------------------------------------
Started: 2025-04-21 03:07:34.438892
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-21 03:07:34.487419
------------------------------------------------------
Started: 2025-04-21 04:24:17.719976
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1549
Summarized using GPT-3.5-turbo
Append: [Benchmarking Large Language Models for Calculus Problem-Solving: A Comparative Analysis](https://arxiv.org/abs/2504.13187)
Token length: 1474
Summarized using GPT-3.5-turbo
Append: [BASIR: Budget-Assisted Sectoral Impact Ranking -- A Dataset for Sector Identification and Performance Prediction Using Language Models](https://arxiv.org/abs/2504.13189)
Token length: 1123
Summarized using GPT-3.5-turbo
Append: [KFinEval-Pilot: A Comprehensive Benchmark Suite for Korean Financial Language Understanding](https://arxiv.org/abs/2504.13216)
Token length: 1860
Summarized using GPT-3.5-turbo
Append: [Sustainability via LLM Right-sizing](https://arxiv.org/abs/2504.13217)
Token length: 1330
Summarized using GPT-3.5-turbo
Append: [DIDS: Domain Impact-aware Data Sampling for Large Language Model Training](https://arxiv.org/abs/2504.13227)
Token length: 1132
Summarized using GPT-3.5-turbo
Append: [ImPart: Importance-Aware Delta-Sparsification for Improved Model Compression and Merging in LLMs](https://arxiv.org/abs/2504.13237)
Token length: 1651
Summarized using GPT-3.5-turbo
Append: [CPG-EVAL: A Multi-Tiered Benchmark for Evaluating the Chinese Pedagogical Grammar Competence of Large Language Models](https://arxiv.org/abs/2504.13261)
Token length: 890
Summarized using GPT-3.5-turbo
Append: [Sentiment Analysis on the young people's perception about the mobile Internet costs in Senegal](https://arxiv.org/abs/2504.13284)
Token length: 1139
Summarized using GPT-3.5-turbo
Append: [THOUGHTTERMINATOR: Benchmarking, Calibrating, and Mitigating Overthinking in Reasoning Models](https://arxiv.org/abs/2504.13367)
Token length: 1332
Summarized using GPT-3.5-turbo
Append: [Secure Multifaceted-RAG for Enterprise: Hybrid Knowledge Retrieval with Security Filtering](https://arxiv.org/abs/2504.13425)
Token length: 1059
Summarized using GPT-3.5-turbo
Append: [D-GEN: Automatic Distractor Generation and Evaluation for Reliable Assessment of Generative Model](https://arxiv.org/abs/2504.13439)
Token length: 1548
Summarized using GPT-3.5-turbo
Append: [From Large to Super-Tiny: End-to-End Optimization for Cost-Efficient LLMs](https://arxiv.org/abs/2504.13471)
Token length: 1267
Summarized using GPT-3.5-turbo
Append: [LLM Sensitivity Evaluation Framework for Clinical Diagnosis](https://arxiv.org/abs/2504.13475)
Token length: 1345
Summarized using GPT-3.5-turbo
Append: [Prejudge-Before-Think: Enhancing Large Language Models at Test-Time by Process Prejudge Reasoning](https://arxiv.org/abs/2504.13500)
Token length: 1372
Summarized using GPT-3.5-turbo
Append: [CoT-RAG: Integrating Chain of Thought and Retrieval-Augmented Generation to Enhance Reasoning in Large Language Models](https://arxiv.org/abs/2504.13534)
Token length: 1410
Summarized using GPT-3.5-turbo
Append: [Enhancing Multilingual Sentiment Analysis with Explainability for Sinhala, English, and Code-Mixed Content](https://arxiv.org/abs/2504.13545)
Token length: 1372
Summarized using GPT-3.5-turbo
Append: [DETAM: Defending LLMs Against Jailbreak Attacks via Targeted Attention Modification](https://arxiv.org/abs/2504.13562)
Token length: 1308
Summarized using GPT-3.5-turbo
Append: [Improving Generalization in Intent Detection: GRPO with Reward-Based Curriculum Sampling](https://arxiv.org/abs/2504.13592)
Token length: 1078
Summarized using GPT-3.5-turbo
Append: [Continual Pre-Training is (not) What You Need in Domain Adaption](https://arxiv.org/abs/2504.13603)
Token length: 1840
Summarized using GPT-3.5-turbo
Append: [Long-context Non-factoid Question Answering in Indic Languages](https://arxiv.org/abs/2504.13615)
Token length: 1593
Summarized using GPT-3.5-turbo
Append: [Thought Manipulation: External Thought Can Be Efficient for Large Reasoning Models](https://arxiv.org/abs/2504.13626)
Token length: 1194
Summarized using GPT-3.5-turbo
Append: [Divergent LLM Adoption and Heterogeneous Convergence Paths in Research Writing](https://arxiv.org/abs/2504.13629)
Token length: 1086
Summarized using GPT-3.5-turbo
Append: [Remedy: Learning Machine Translation Evaluation from Human Preferences with Reward Modeling](https://arxiv.org/abs/2504.13630)
Token length: 1877
Summarized using GPT-3.5-turbo
Append: [Simulating Before Planning: Constructing Intrinsic User World Model for User-Tailored Dialogue Policy Planning](https://arxiv.org/abs/2504.13643)
Token length: 1592
Summarized using GPT-3.5-turbo
Append: [Word Embedding Techniques for Classification of Star Ratings](https://arxiv.org/abs/2504.13653)
Token length: 1458
Summarized using GPT-3.5-turbo
Append: [Multi-Type Context-Aware Conversational Recommender Systems via Mixture-of-Experts](https://arxiv.org/abs/2504.13655)
Token length: 934
Summarized using GPT-3.5-turbo
Append: [Revisiting Uncertainty Quantification Evaluation in Language Models: Spurious Interactions with Response Length Bias Results](https://arxiv.org/abs/2504.13677)
Token length: 1718
Summarized using GPT-3.5-turbo
Append: [Deep literature reviews: an application of fine-tuned language models to migration research](https://arxiv.org/abs/2504.13685)
Token length: 1077
Summarized using GPT-3.5-turbo
Append: [Controlled Territory and Conflict Tracking (CONTACT): (Geo-)Mapping Occupied Territory from Open Source Intelligence](https://arxiv.org/abs/2504.13730)
Token length: 1569
Summarized using GPT-3.5-turbo
Append: [BadApex: Backdoor Attack Based on Adaptive Optimization Mechanism of Black-box Large Language Models](https://arxiv.org/abs/2504.13775)
Token length: 1363
Summarized using GPT-3.5-turbo
Append: [Analyzing LLMs' Knowledge Boundary Cognition Across Languages Through the Lens of Internal Representations](https://arxiv.org/abs/2504.13816)
Token length: 1148
Summarized using GPT-3.5-turbo
Append: [Feature Alignment and Representation Transfer in Knowledge Distillation for Large Language Models](https://arxiv.org/abs/2504.13825)
Token length: 1309
Summarized using GPT-3.5-turbo
Append: [Generative AI Act II: Test Time Scaling Drives Cognition Engineering](https://arxiv.org/abs/2504.13828)
Token length: 1914
Summarized using GPT-3.5-turbo
Append: [Science Hierarchography: Hierarchical Organization of Science Literature](https://arxiv.org/abs/2504.13834)
Token length: 1534
Summarized using GPT-3.5-turbo
Append: [MIG: Automatic Data Selection for Instruction Tuning by Maximizing Information Gain in Semantic Space](https://arxiv.org/abs/2504.13835)
Token length: 886
Summarized using GPT-3.5-turbo
Append: [The Quantum LLM: Modeling Semantic Spaces with Quantum Principles](https://arxiv.org/abs/2504.13202)
Token length: 1400
Summarized using GPT-3.5-turbo
Append: [X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents](https://arxiv.org/abs/2504.13203)
Token length: 1635
Summarized using GPT-3.5-turbo
Append: [Interpersonal Theory of Suicide as a Lens to Examine Suicidal Ideation in Online Spaces](https://arxiv.org/abs/2504.13277)
Token length: 1562
Summarized using GPT-3.5-turbo
Append: [Acoustic to Articulatory Inversion of Speech; Data Driven Approaches, Challenges, Applications, and Future Scope](https://arxiv.org/abs/2504.13308)
Token length: 1891
Summarized using GPT-3.5-turbo
Append: [Cost-of-Pass: An Economic Framework for Evaluating Language Models](https://arxiv.org/abs/2504.13359)
Token length: 1012
Summarized using GPT-3.5-turbo
Append: [A mean teacher algorithm for unlearning of language models](https://arxiv.org/abs/2504.13388)
Token length: 1208
Summarized using GPT-3.5-turbo
Append: [LangCoop: Collaborative Driving with Language](https://arxiv.org/abs/2504.13406)
Token length: 1423
Summarized using GPT-3.5-turbo
Append: [STAMP Your Content: Proving Dataset Membership via Watermarked Rephrasings](https://arxiv.org/abs/2504.13416)
Token length: 1782
Summarized using GPT-3.5-turbo
Append: [CodeVisionary: An Agent-based Framework for Evaluating Large Language Models in Code Generation](https://arxiv.org/abs/2504.13472)
Token length: 1658
Summarized using GPT-3.5-turbo
Append: [Integrating Locality-Aware Attention with Transformers for General Geometry PDEs](https://arxiv.org/abs/2504.13480)
Token length: 1245
Summarized using GPT-3.5-turbo
Append: [Q-FAKER: Query-free Hard Black-box Attack via Controlled Generation](https://arxiv.org/abs/2504.13551)
Token length: 915
Summarized using GPT-3.5-turbo
Append: [Exploring the Potential for Large Language Models to Demonstrate Rational Probabilistic Beliefs](https://arxiv.org/abs/2504.13644)
Token length: 601
Summarized using GPT-3.5-turbo
Append: [Large Language Models Will Change The Way Children Think About Technology And Impact Every Interaction Paradigm](https://arxiv.org/abs/2504.13667)
Token length: 1375
Summarized using GPT-3.5-turbo
Append: [OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simulation](https://arxiv.org/abs/2504.13707)
Token length: 1269
Summarized using GPT-3.5-turbo
Append: [Learning to Attribute with Attention](https://arxiv.org/abs/2504.13752)
Append: [Scaling sparse feature circuit finding for in-context learning](https://arxiv.org/abs/2504.13756)
Append: [Not All Rollouts are Useful: Down-Sampling Rollouts in LLM Reinforcement Learning](https://arxiv.org/abs/2504.13818)
Append: [Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?](https://arxiv.org/abs/2504.13837)
Append: [Only Send What You Need: Learning to Communicate Efficiently in Federated Multilingual Machine Translation](https://arxiv.org/abs/2401.07456)
Append: [A Theory of LLM Sampling: Part Descriptive and Part Prescriptive](https://arxiv.org/abs/2402.11005)
Append: [Where is the answer? Investigating Positional Bias in Language Model Knowledge Extraction](https://arxiv.org/abs/2402.12170)
Append: [Argumentative Large Language Models for Explainable and Contestable Claim Verification](https://arxiv.org/abs/2405.02079)
Append: [Babysit A Language Model From Scratch: Interactive Language Learning by Trials and Demonstrations](https://arxiv.org/abs/2405.13828)
Append: [Is In-Context Learning Sufficient for Instruction Following in LLMs?](https://arxiv.org/abs/2405.19874)
Append: [Adversarial Style Augmentation via Large Language Model for Robust Fake News Detection](https://arxiv.org/abs/2406.11260)
Append: [Can Tool-augmented Large Language Models be Aware of Incomplete Conditions?](https://arxiv.org/abs/2406.12307)
Append: [The Comparative Trap: Pairwise Comparisons Amplifies Biased Preferences of LLM Evaluators](https://arxiv.org/abs/2406.12319)
Append: [Does Refusal Training in LLMs Generalize to the Past Tense?](https://arxiv.org/abs/2407.11969)
Append: [Understanding Epistemic Language with a Language-augmented Bayesian Theory of Mind](https://arxiv.org/abs/2408.12022)
Append: [Large Language Models are Good Multi-lingual Learners : When LLMs Meet Cross-lingual Prompts](https://arxiv.org/abs/2409.11056)
Append: [Reducing the Scope of Language Models](https://arxiv.org/abs/2410.21597)
Append: [Can LLMs assist with Ambiguity? A Quantitative Evaluation of various Large Language Models on Word Sense Disambiguation](https://arxiv.org/abs/2411.18337)
Append: [Are You Doubtful? Oh, It Might Be Difficult Then! Exploring the Use of Model Uncertainty for Question Difficulty Estimation](https://arxiv.org/abs/2412.11831)
Append: [StaICC: Standardized Evaluation for Classification Task in In-context Learning](https://arxiv.org/abs/2501.15708)
Append: [A-MEM: Agentic Memory for LLM Agents](https://arxiv.org/abs/2502.12110)
Append: [Token-Level Density-Based Uncertainty Quantification Methods for Eliciting Truthfulness of Large Language Models](https://arxiv.org/abs/2502.14427)
Append: [ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large Reasoning Models with Iterative Retrieval Augmented Generation](https://arxiv.org/abs/2503.21729)
Append: [Adaptive Layer-skipping in Pre-trained LLMs](https://arxiv.org/abs/2503.23798)
Append: [Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language Models](https://arxiv.org/abs/2504.05050)
Append: [From Token to Line: Enhancing Code Generation with a Long-Term Perspective](https://arxiv.org/abs/2504.07433)
Append: [Can postgraduate translation students identify machine-generated text?](https://arxiv.org/abs/2504.09164)
Append: [C-MTCSD: A Chinese Multi-Turn Conversational Stance Detection Dataset](https://arxiv.org/abs/2504.09958)
Append: [The Mirage of Performance Gains: Why Contrastive Decoding Fails to Address Multimodal Hallucination](https://arxiv.org/abs/2504.10020)
Append: [SysCaps: Language Interfaces for Simulation Surrogates of Complex Systems](https://arxiv.org/abs/2405.19653)
Append: [LLM-Select: Feature Selection with Large Language Models](https://arxiv.org/abs/2407.02694)
Append: [Towards Robust Alignment of Language Models: Distributionally Robustifying Direct Preference Optimization](https://arxiv.org/abs/2407.07880)
Append: [Spin glass model of in-context learning](https://arxiv.org/abs/2408.02288)
Append: [SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding](https://arxiv.org/abs/2408.15545)
Append: [United in Diversity? Contextual Biases in LLM-Based Predictions of the 2024 European Parliament Elections](https://arxiv.org/abs/2409.09045)
Append: [AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents](https://arxiv.org/abs/2410.09024)
Append: [ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference](https://arxiv.org/abs/2410.21465)
Append: [PriorDiffusion: Leverage Language Prior in Diffusion Models for Monocular Depth Estimation](https://arxiv.org/abs/2411.16750)
Append: [Prompt-Based Cost-Effective Evaluation and Operation of ChatGPT as a Computer Programming Teaching Assistant](https://arxiv.org/abs/2501.17176)
Append: [Project Alexandria: Towards Freeing Scientific Knowledge from Copyright Burdens via LLMs](https://arxiv.org/abs/2502.19413)
Append: [Psycholinguistic Analyses in Software Engineering Text: A Systematic Literature Review](https://arxiv.org/abs/2503.05992)
Append: [DocAgent: A Multi-Agent System for Automated Code Documentation Generation](https://arxiv.org/abs/2504.08725)
Append: [Assessing Judging Bias in Large Reasoning Models: An Empirical Study](https://arxiv.org/abs/2504.09946)
Append: [GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents](https://arxiv.org/abs/2504.10458)
append_entries: 93
Finish: 2025-04-21 04:26:21.470194
------------------------------------------------------
Started: 2025-04-21 06:23:11.446219
Existing_entries: 320
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-21 06:23:11.621175
------------------------------------------------------
Started: 2025-04-21 08:21:32.665323
Existing_entries: 320
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-21 08:21:32.824343
------------------------------------------------------
Started: 2025-04-21 10:17:11.123964
Existing_entries: 320
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-21 10:17:11.292386
------------------------------------------------------
Started: 2025-04-21 12:31:41.885468
Existing_entries: 320
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-21 12:31:42.045876
------------------------------------------------------
Started: 2025-04-21 14:14:45.792935
Existing_entries: 320
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-21 14:14:45.954796
------------------------------------------------------
Started: 2025-04-21 16:19:34.585177
Existing_entries: 320
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-21 16:19:34.784087
------------------------------------------------------
Started: 2025-04-21 18:21:23.595244
Existing_entries: 320
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-21 18:21:23.768949
------------------------------------------------------
Started: 2025-04-21 20:17:45.253500
Existing_entries: 320
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-21 20:17:45.413071
------------------------------------------------------
Started: 2025-04-21 22:16:01.866912
Existing_entries: 320
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-21 22:16:02.025241
------------------------------------------------------
Started: 2025-04-22 01:15:42.061651
Existing_entries: 320
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-22 01:15:42.240146
------------------------------------------------------
Started: 2025-04-22 03:01:08.089373
Existing_entries: 320
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-22 03:01:08.260772
------------------------------------------------------
Started: 2025-04-22 04:23:33.107276
Existing_entries: 320
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 940
Summarized using GPT-3.5-turbo
Append: [Seed-Thinking-v1.5: Advancing Superb Reasoning Models with Reinforcement Learning](https://arxiv.org/abs/2504.13914)
Token length: 1122
Summarized using GPT-3.5-turbo
Append: [Uncovering Conspiratorial Narratives within Arabic Online Content](https://arxiv.org/abs/2504.14037)
Token length: 855
Summarized using GPT-3.5-turbo
Append: [MEQA: A Meta-Evaluation Framework for Question & Answer LLM Benchmarks](https://arxiv.org/abs/2504.14039)
Token length: 996
Summarized using GPT-3.5-turbo
Append: [A Baseline for Self-state Identification and Classification in Mental Health Data: CLPsych 2025 Task](https://arxiv.org/abs/2504.14066)
Token length: 1571
Summarized using GPT-3.5-turbo
Append: [LogicTree: Structured Proof Exploration for Coherent and Rigorous Logical Reasoning with Large Language Models](https://arxiv.org/abs/2504.14089)
Token length: 1667
Summarized using GPT-3.5-turbo
Append: [PEFT A2Z: Parameter-Efficient Fine-Tuning Survey for Large Language and Vision Models](https://arxiv.org/abs/2504.14117)
Token length: 1477
Summarized using GPT-3.5-turbo
Append: [Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations](https://arxiv.org/abs/2504.14150)
Token length: 1281
Summarized using GPT-3.5-turbo
Append: [SConU: Selective Conformal Uncertainty in Large Language Models](https://arxiv.org/abs/2504.14154)
Token length: 1224
Summarized using GPT-3.5-turbo
Append: [Self-Correction Makes LLMs Better Parsers](https://arxiv.org/abs/2504.14165)
Token length: 1052
Summarized using GPT-3.5-turbo
Append: [Hypothetical Documents or Knowledge Leakage? Rethinking LLM-based Query Expansion](https://arxiv.org/abs/2504.14175)
Token length: 1545
Summarized using GPT-3.5-turbo
Append: [Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models](https://arxiv.org/abs/2504.14194)
Token length: 1166
Summarized using GPT-3.5-turbo
Append: [EIoU-EMC: A Novel Loss for Domain-specific Nested Entity Recognition](https://arxiv.org/abs/2504.14203)
Token length: 775
Summarized using GPT-3.5-turbo
Append: [Bias Analysis and Mitigation through Protected Attribute Detection and Regard Classification](https://arxiv.org/abs/2504.14212)
Token length: 1299
Summarized using GPT-3.5-turbo
Append: [Understanding the Repeat Curse in Large Language Models from a Feature Perspective](https://arxiv.org/abs/2504.14218)
Token length: 1164
Summarized using GPT-3.5-turbo
Append: [SimplifyMyText: An LLM-Based System for Inclusive Plain Language Text Simplification](https://arxiv.org/abs/2504.14223)
Token length: 1819
Summarized using GPT-3.5-turbo
Append: [Know Me, Respond to Me: Benchmarking LLMs for Dynamic User Profiling and Personalized Responses at Scale](https://arxiv.org/abs/2504.14225)
Token length: 1213
Summarized using GPT-3.5-turbo
Append: [Probing the Subtle Ideological Manipulation of Large Language Models](https://arxiv.org/abs/2504.14287)
Token length: 1354
Summarized using GPT-3.5-turbo
Append: [Multimodal Coreference Resolution for Chinese Social Media Dialogues: Dataset and Benchmark Approach](https://arxiv.org/abs/2504.14321)
Token length: 1311
Summarized using GPT-3.5-turbo
Append: [Empirical Evaluation of Knowledge Distillation from Transformers to Subquadratic Language Models](https://arxiv.org/abs/2504.14366)
Token length: 976
Summarized using GPT-3.5-turbo
Append: [Diverse Prompts: Illuminating the Prompt Space of Large Language Models with MAP-Elites](https://arxiv.org/abs/2504.14367)
Token length: 1426
Summarized using GPT-3.5-turbo
Append: [ParaPO: Aligning Language Models to Reduce Verbatim Reproduction of Pre-training Data](https://arxiv.org/abs/2504.14452)
Token length: 1590
Summarized using GPT-3.5-turbo
Append: [CoLoTa: A Dataset for Entity-based Commonsense Reasoning over Long-Tail Knowledge](https://arxiv.org/abs/2504.14462)
Token length: 996
Summarized using GPT-3.5-turbo
Append: [sEEG-based Encoding for Sentence Retrieval: A Contrastive Learning Approach to Brain-Language Alignment](https://arxiv.org/abs/2504.14468)
Token length: 1236
Summarized using GPT-3.5-turbo
Append: [DialogueAgents: A Hybrid Agent-Based Speech Synthesis Framework for Multi-Party Dialogue](https://arxiv.org/abs/2504.14482)
Token length: 1363
Summarized using GPT-3.5-turbo
Append: [FairSteer: Inference Time Debiasing for LLMs with Dynamic Activation Steering](https://arxiv.org/abs/2504.14492)
Token length: 1297
Summarized using GPT-3.5-turbo
Append: [Functional Abstraction of Knowledge Recall in Large Language Models](https://arxiv.org/abs/2504.14496)
Token length: 1026
Summarized using GPT-3.5-turbo
Append: [Causality for Natural Language Processing](https://arxiv.org/abs/2504.14530)
Token length: 1170
Summarized using GPT-3.5-turbo
Append: [BookWorld: From Novels to Interactive Agent Societies for Creative Story Generation](https://arxiv.org/abs/2504.14538)
Token length: 1612
Summarized using GPT-3.5-turbo
Append: [a1: Steep Test-time Scaling Law via Environment Augmented Generation](https://arxiv.org/abs/2504.14597)
Token length: 1552
Summarized using GPT-3.5-turbo
Append: [Translation Analytics for Freelancers: I. Introduction, Data Preparation, Baseline Evaluations](https://arxiv.org/abs/2504.14619)
Token length: 1276
Summarized using GPT-3.5-turbo
Append: [A Hierarchical Framework for Measuring Scientific Paper Innovation via Large Language Models](https://arxiv.org/abs/2504.14620)
Token length: 1198
Summarized using GPT-3.5-turbo
Append: [Automatic Text Summarization (ATS) for Research Documents in Sorani Kurdish](https://arxiv.org/abs/2504.14630)
Token length: 1753
Summarized using GPT-3.5-turbo
Append: [Harnessing Generative LLMs for Enhanced Financial Event Entity Extraction Performance](https://arxiv.org/abs/2504.14633)
Token length: 1321
Summarized using GPT-3.5-turbo
Append: [A Case Study Exploring the Current Landscape of Synthetic Medical Record Generation with Commercial LLMs](https://arxiv.org/abs/2504.14657)
Token length: 1037
Summarized using GPT-3.5-turbo
Append: [Trans-Zero: Self-Play Incentivizes Large Language Models for Multilingual Translation Without Parallel Data](https://arxiv.org/abs/2504.14669)
Token length: 1254
Summarized using GPT-3.5-turbo
Append: [FarsEval-PKBETS: A new diverse benchmark for evaluating Persian large language models](https://arxiv.org/abs/2504.14690)
Token length: 1475
Summarized using GPT-3.5-turbo
Append: [OmniV-Med: Scaling Medical Vision-Language Model for Universal Visual Understanding](https://arxiv.org/abs/2504.14692)
Token length: 863
Summarized using GPT-3.5-turbo
Append: [Evaluating BERTopic on Open-Ended Data: A Case Study with Belgian Dutch Daily Narratives](https://arxiv.org/abs/2504.14707)
Token length: 1239
Summarized using GPT-3.5-turbo
Append: [PROMPTEVALS: A Dataset of Assertions and Guardrails for Custom Production Large Language Model Pipelines](https://arxiv.org/abs/2504.14738)
Token length: 1305
Summarized using GPT-3.5-turbo
Append: [Disentangling Linguistic Features with Dimension-Wise Analysis of Vector Embeddings](https://arxiv.org/abs/2504.14766)
Token length: 1685
Summarized using GPT-3.5-turbo
Append: [Knowledge Distillation and Dataset Distillation of Large Language Models: Emerging Trends, Challenges, and Future Directions](https://arxiv.org/abs/2504.14772)
Token length: 1764
Summarized using GPT-3.5-turbo
Append: [Automatic Evaluation Metrics for Document-level Translation: Overview, Challenges and Trends](https://arxiv.org/abs/2504.14804)
Token length: 1221
Summarized using GPT-3.5-turbo
Append: [On Self-improving Token Embeddings](https://arxiv.org/abs/2504.14808)
Token length: 1085
Summarized using GPT-3.5-turbo
Append: [Transparentize the Internal and External Knowledge Utilization in LLMs with Trustworthy Citation](https://arxiv.org/abs/2504.14856)
Token length: 1129
Summarized using GPT-3.5-turbo
Append: [Natural Fingerprints of Large Language Models](https://arxiv.org/abs/2504.14871)
Token length: 1185
Summarized using GPT-3.5-turbo
Append: [Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey](https://arxiv.org/abs/2504.14891)
Token length: 1892
Summarized using GPT-3.5-turbo
Append: [CRAVE: A Conflicting Reasoning Approach for Explainable Claim Verification Using LLMs](https://arxiv.org/abs/2504.14905)
Token length: 1074
Summarized using GPT-3.5-turbo
Append: [Speaker Fuzzy Fingerprints: Benchmarking Text-Based Identification in Multiparty Dialogues](https://arxiv.org/abs/2504.14963)
Token length: 497
Summarized using GPT-3.5-turbo
Append: [Evaluating LLMs on Chinese Topic Constructions: A Research Proposal Inspired by Tian et al. (2024)](https://arxiv.org/abs/2504.14969)
Token length: 1190
Summarized using GPT-3.5-turbo
Append: [Efficient Pretraining Length Scaling](https://arxiv.org/abs/2504.14992)
Append: [Stay Hungry, Stay Foolish: On the Extended Reading Articles Generation with LLMs](https://arxiv.org/abs/2504.15013)
Append: [LLMs as Data Annotators: How Close Are We to Human Performance](https://arxiv.org/abs/2504.15022)
Append: [DistilQwen2.5: Industrial Practices of Training Distilled Open Lightweight Language Models](https://arxiv.org/abs/2504.15027)
Append: [RainbowPlus: Enhancing Adversarial Prompt Generation via Evolutionary Quality-Diversity Search](https://arxiv.org/abs/2504.15047)
Append: [Testing LLMs' Capabilities in Annotating Translations Based on an Error Typology Designed for LSP Translation: First Experiments with ChatGPT](https://arxiv.org/abs/2504.15052)
Append: [Rethinking the Potential of Multimodality in Collaborative Problem Solving Diagnosis with Large Language Models](https://arxiv.org/abs/2504.15093)
Append: [Kuwain 1.5B: An Arabic SLM via Language Injection](https://arxiv.org/abs/2504.15120)
Append: [EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language Models](https://arxiv.org/abs/2504.15133)
Append: [The Synthetic Imputation Approach: Generating Optimal Synthetic Texts For Underrepresented Categories In Supervised Classification Tasks](https://arxiv.org/abs/2504.15160)
Append: [On true empty category](https://arxiv.org/abs/2504.15168)
Append: [Support Evaluation for the TREC 2024 RAG Track: Comparing Human versus LLM Judges](https://arxiv.org/abs/2504.15205)
Append: [EvalAgent: Discovering Implicit Evaluation Criteria from the Web](https://arxiv.org/abs/2504.15219)
Append: [Fully Bayesian Approaches to Topics over Time](https://arxiv.org/abs/2504.15220)
Append: [Values in the Wild: Discovering and Analyzing Values in Real-World Language Model Interactions](https://arxiv.org/abs/2504.15236)
Append: [MR. Guard: Multilingual Reasoning Guardrail using Curriculum Learning](https://arxiv.org/abs/2504.15241)
Append: [Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as Test-Time Scaling Evaluators](https://arxiv.org/abs/2504.15253)
Append: [Interview AI-ssistant: Designing for Real-Time Human-AI Collaboration in Interview Preparation and Execution](https://arxiv.org/abs/2504.13847)
Append: [3MDBench: Medical Multimodal Multi-agent Dialogue Benchmark](https://arxiv.org/abs/2504.13861)
Append: [A Survey on (M)LLM-Based GUI Agents](https://arxiv.org/abs/2504.13865)
Append: [Toward Automated Qualitative Analysis: Leveraging Large Language Models for Tutoring Dialogue Evaluation](https://arxiv.org/abs/2504.13882)
Append: [AI as a deliberative partner fosters intercultural empathy for Americans but fails for Latin American participants](https://arxiv.org/abs/2504.13887)
Append: [Kanji Workbook: A Writing-Based Intelligent Tutoring System for Learning Proper Japanese Kanji Writing Technique with Instructor-Emulated Assessment](https://arxiv.org/abs/2504.13888)
Append: [Measuring Mental Health Variables in Computational Research: Toward Validated, Dimensional, and Transdiagnostic Approaches](https://arxiv.org/abs/2504.13890)
Append: [TALLMesh: a simple application for performing Thematic Analysis with Large Language Models](https://arxiv.org/abs/2504.13892)
Append: [Generative Framework for Personalized Persuasion: Inferring Causal, Counterfactual, and Latent Knowledge](https://arxiv.org/abs/2504.13904)
Append: [Evaluation and Incident Prevention in an Enterprise AI Assistant](https://arxiv.org/abs/2504.13924)
Append: [Enhancing Ultra-Low-Bit Quantization of Large Language Models Through Saliency-Aware Partial Retraining](https://arxiv.org/abs/2504.13932)
Append: [Thousand Voices of Trauma: A Large-Scale Synthetic Dataset for Modeling Prolonged Exposure Therapy Conversations](https://arxiv.org/abs/2504.13955)
Append: [ToolRL: Reward is All Tool Learning Needs](https://arxiv.org/abs/2504.13958)
Append: [AI Safety Should Prioritize the Future of Work](https://arxiv.org/abs/2504.13959)
Append: [One Jump Is All You Need: Short-Cutting Transformers for Early Exit Prediction with One Jump to Fit All Exit Levels](https://arxiv.org/abs/2504.13984)
Append: [Gradual Binary Search and Dimension Expansion : A general method for activation quantization in LLMs](https://arxiv.org/abs/2504.13989)
Append: [Sentiment Analysis of Airbnb Reviews: Exploring Their Impact on Acceptance Rates and Pricing Across Multiple U.S. Regions](https://arxiv.org/abs/2504.14053)
Append: [Linking forward-pass dynamics in Transformers and real-time human processing](https://arxiv.org/abs/2504.14107)
Append: [System of Agentic AI for the Discovery of Metal-Organic Frameworks](https://arxiv.org/abs/2504.14110)
Append: [Bayesian Principles Improve Prompt Learning In Vision-Language Models](https://arxiv.org/abs/2504.14123)
Append: [Large Language Model Enhanced Particle Swarm Optimization for Hyperparameter Tuning for Deep Learning Models](https://arxiv.org/abs/2504.14126)
Append: [TALES: Text Adventure Learning Environment Suite](https://arxiv.org/abs/2504.14128)
Append: [HF4Rec: Human-Like Feedback-Driven Optimization Framework for Explainable Recommendation](https://arxiv.org/abs/2504.14147)
Append: [Direct Advantage Regression: Aligning LLMs with Online AI Reward](https://arxiv.org/abs/2504.14177)
Append: [The First VoicePrivacy Attacker Challenge](https://arxiv.org/abs/2504.14183)
Append: [AI Idea Bench 2025: AI Research Idea Generation Benchmark](https://arxiv.org/abs/2504.14191)
Append: [Assessing AI-Generated Questions' Alignment with Cognitive Frameworks in Educational Assessment](https://arxiv.org/abs/2504.14232)
Append: [InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners](https://arxiv.org/abs/2504.14239)
Append: [Towards Explainable Fake Image Detection with Multi-Modal Large Language Models](https://arxiv.org/abs/2504.14245)
Append: [Cross-attention for State-based model RWKV-7](https://arxiv.org/abs/2504.14260)
Append: [A Multimodal Recaptioning Framework to Account for Perceptual Diversity in Multilingual Vision-Language Modeling](https://arxiv.org/abs/2504.14359)
Append: [Integrating Single-Cell Foundation Models with Graph Neural Networks for Drug Response Prediction](https://arxiv.org/abs/2504.14361)
Append: [Improving RL Exploration for LLM Reasoning through Retrospective Replay](https://arxiv.org/abs/2504.14363)
Append: [Density Measures for Language Generation](https://arxiv.org/abs/2504.14370)
Append: [LoRe: Personalizing LLMs via Low-Rank Reward Modeling](https://arxiv.org/abs/2504.14439)
Append: [Meta-Thinking in LLMs via Multi-Agent Reinforcement Learning: A Survey](https://arxiv.org/abs/2504.14520)
Append: [Are Vision LLMs Road-Ready? A Comprehensive Benchmark for Safety-Critical Driving Video Understanding](https://arxiv.org/abs/2504.14526)
Append: [HealthGenie: Empowering Users with Healthy Dietary Guidance through Knowledge Graph and Large Language Models](https://arxiv.org/abs/2504.14594)
Append: [Risk Assessment Framework for Code LLMs via Leveraging Internal States](https://arxiv.org/abs/2504.14640)
Append: [LeetCodeDataset: A Temporal Dataset for Robust Evaluation and Efficient Training of Code LLMs](https://arxiv.org/abs/2504.14655)
Append: [PLANET: A Collection of Benchmarks for Evaluating LLMs' Planning Capabilities](https://arxiv.org/abs/2504.14773)
Append: [Completing A Systematic Review in Hours instead of Months with Interactive AI Agents](https://arxiv.org/abs/2504.14822)
Append: [AlignRAG: An Adaptable Framework for Resolving Misalignments in Retrieval-Aware Reasoning of RAG](https://arxiv.org/abs/2504.14858)
Append: [OTC: Optimal Tool Calls via Reinforcement Learning](https://arxiv.org/abs/2504.14870)
Append: [VLM as Policy: Common-Law Content Moderation Framework for Short Video Platform](https://arxiv.org/abs/2504.14904)
Append: [EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent Dialogue Framework](https://arxiv.org/abs/2504.14928)
Append: [Learning to Reason under Off-Policy Guidance](https://arxiv.org/abs/2504.14945)
Append: [The Great Nugget Recall: Automating Fact Extraction and RAG Evaluation with Large Language Models](https://arxiv.org/abs/2504.15068)
Append: [Rhythm of Opinion: A Hawkes-Graph Framework for Dynamic Propagation Analysis](https://arxiv.org/abs/2504.15072)
Append: [KGMEL: Knowledge Graph-Enhanced Multimodal Entity Linking](https://arxiv.org/abs/2504.15135)
Append: [CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation](https://arxiv.org/abs/2504.15254)
Append: [Roll the dice & look before you leap: Going beyond the creative limits of next-token prediction](https://arxiv.org/abs/2504.15266)
Append: [An LMM for Efficient Video Understanding via Reinforced Compression of Video Cubes](https://arxiv.org/abs/2504.15270)
Append: [Seeing from Another Perspective: Evaluating Multi-View Understanding in MLLMs](https://arxiv.org/abs/2504.15280)
Append: [Persian Abstract Meaning Representation: Annotation Guidelines and Gold Standard Dataset](https://arxiv.org/abs/2205.07712)
Append: [GLoRE: Evaluating Logical Reasoning of Large Language Models](https://arxiv.org/abs/2310.09107)
Append: [LongStory: Coherent, Complete and Length Controlled Long story Generation](https://arxiv.org/abs/2311.15208)
Append: [Is Translation All You Need? A Study on Solving Multilingual Tasks with Large Language Models](https://arxiv.org/abs/2403.10258)
Append: [Aligning Language Models with Demonstrated Feedback](https://arxiv.org/abs/2406.00888)
Append: [Inverse Constitutional AI: Compressing Preferences into Principles](https://arxiv.org/abs/2406.06560)
Append: [Beyond Boundaries: Learning a Universal Entity Taxonomy across Datasets and Languages for Open Named Entity Recognition](https://arxiv.org/abs/2406.11192)
Append: [Temporal Knowledge Graph Question Answering: A Survey](https://arxiv.org/abs/2406.14191)
Append: [LiveBench: A Challenging, Contamination-Limited LLM Benchmark](https://arxiv.org/abs/2406.19314)
Append: [Training on the Test Task Confounds Evaluation and Emergence](https://arxiv.org/abs/2407.07890)
Append: [IFShip: Interpretable Fine-grained Ship Classification with Domain Knowledge-Enhanced Vision-Language Models](https://arxiv.org/abs/2408.06631)
Append: [DualKanbaFormer: An Efficient Selective Sparse Framework for Multimodal Aspect-based Sentiment Analysis](https://arxiv.org/abs/2408.15379)
Append: [Self-evolving Agents with reflective and memory-augmented abilities](https://arxiv.org/abs/2409.00872)
Append: [Task-Specific Directions: Definition, Exploration, and Utilization in Parameter Efficient Fine-Tuning](https://arxiv.org/abs/2409.01035)
Append: [Seek and Solve Reasoning for Table Question Answering](https://arxiv.org/abs/2409.05286)
Append: [Reasoning-Enhanced Healthcare Predictions with Knowledge Graph Community Retrieval](https://arxiv.org/abs/2410.04585)
Append: [Detecting Training Data of Large Language Models via Expectation Maximization](https://arxiv.org/abs/2410.07582)
Append: [Nudging: Inference-time Alignment of LLMs via Guided Decoding](https://arxiv.org/abs/2410.09300)
Append: [Adapting Multilingual LLMs to Low-Resource Languages using Continued Pre-training and Synthetic Corpus](https://arxiv.org/abs/2410.14815)
Append: [Magnetic Preference Optimization: Achieving Last-iterate Convergence for Language Model Alignment](https://arxiv.org/abs/2410.16714)
Append: [Fine-Grained and Multi-Dimensional Metrics for Document-Level Machine Translation](https://arxiv.org/abs/2410.20941)
Append: [ReSpAct: Harmonizing Reasoning, Speaking, and Acting Towards Building Large Language Model-Based Conversational AI Agents](https://arxiv.org/abs/2411.00927)
Append: [CHATTER: A Character Attribution Dataset for Narrative Understanding](https://arxiv.org/abs/2411.05227)
Append: [FactLens: Benchmarking Fine-Grained Fact Verification](https://arxiv.org/abs/2411.05980)
Append: [Star Attention: Efficient LLM Inference over Long Sequences](https://arxiv.org/abs/2411.17116)
Append: [WikiHint: A Human-Annotated Dataset for Hint Ranking and Generation](https://arxiv.org/abs/2412.01626)
Append: [Unanswerability Evaluation for Retrieval Augmented Generation](https://arxiv.org/abs/2412.12300)
Append: [State Space Models are Strong Text Rerankers](https://arxiv.org/abs/2412.14354)
Append: [LongProc: Benchmarking Long-Context Language Models on Long Procedural Generation](https://arxiv.org/abs/2501.05414)
Append: [How to Enable Effective Cooperation Between Humans and NLP Models: A Survey of Principles, Formalizations, and Beyond](https://arxiv.org/abs/2501.05714)
Append: [Idiom Detection in Sorani Kurdish Texts](https://arxiv.org/abs/2501.14528)
Append: [Can LLMs Rank the Harmfulness of Smaller LLMs? We are Not There Yet](https://arxiv.org/abs/2502.05291)
Append: [BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models](https://arxiv.org/abs/2502.07346)
Append: [SparQLe: Speech Queries to Text Translation Through LLMs](https://arxiv.org/abs/2502.09284)
Append: [HiddenDetect: Detecting Jailbreak Attacks against Large Vision-Language Models via Monitoring Hidden States](https://arxiv.org/abs/2502.14744)
Append: [LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention](https://arxiv.org/abs/2502.14866)
Append: [Wrong Answers Can Also Be Useful: PlausibleQA -- A Large-Scale QA Dataset with Answer Plausibility Scores](https://arxiv.org/abs/2502.16358)
Append: [Harnessing Multiple Large Language Models: A Survey on LLM Ensemble](https://arxiv.org/abs/2502.18036)
Append: [Semantic Wave Functions: Exploring Meaning in Large Language Models Through Quantum Formalism](https://arxiv.org/abs/2503.10664)
Append: [Halving transcription time: A fast, user-friendly and GDPR-compliant workflow to create AI-assisted transcripts for content analysis](https://arxiv.org/abs/2503.13031)
Append: [A Language Anchor-Guided Method for Robust Noisy Domain Generalization](https://arxiv.org/abs/2503.17211)
Append: [LLM Agents That Act Like Us: Accurate Human Behavior Simulation with Real-World Data](https://arxiv.org/abs/2503.20749)
Append: [ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging](https://arxiv.org/abs/2503.21088)
Append: [SCORE: Story Coherence and Retrieval Enhancement for AI Narratives](https://arxiv.org/abs/2503.23512)
Append: [ToReMi: Topic-Aware Data Reweighting for Dynamic Pre-Training Data Selection](https://arxiv.org/abs/2504.00695)
Append: [Scaling Video-Language Models to 10K Frames via Hierarchical Differential Distillation](https://arxiv.org/abs/2504.02438)
Append: [Extending the SAREF4ENER Ontology with Flexibility Based on FlexOffers](https://arxiv.org/abs/2504.03595)
Append: [NAACL2025 Tutorial: Adaptation of Large Language Models](https://arxiv.org/abs/2504.03931)
Append: [Following the Whispers of Values: Unraveling Neural Mechanisms Behind Value-Oriented Behaviors in LLMs](https://arxiv.org/abs/2504.04994)
Append: [Persona Dynamics: Unveiling the Impact of Personality Traits on Agents in Text-Based Games](https://arxiv.org/abs/2504.06868)
Append: [AI-Slop to AI-Polish? Aligning Language Models through Edit-Based Writing Rewards and Test-time Computation](https://arxiv.org/abs/2504.07532)
Append: [UXAgent: A System for Simulating Usability Testing of Web Design with LLM Agents](https://arxiv.org/abs/2504.09407)
Append: [Forecasting from Clinical Textual Time Series: Adaptations of the Encoder and Decoder Language Model Families](https://arxiv.org/abs/2504.10340)
Append: [Characterizing Knowledge Manipulation in a Russian Wikipedia Fork](https://arxiv.org/abs/2504.10663)
Append: [Agile-Quant: Activation-Guided Quantization for Faster Inference of LLMs on the Edge](https://arxiv.org/abs/2312.05693)
Append: [Embedding Ontologies via Incorporating Extensional and Intensional Knowledge](https://arxiv.org/abs/2402.01677)
Append: [SLMRec: Distilling Large Language Models into Small for Sequential Recommendation](https://arxiv.org/abs/2405.17890)
Append: [DataComp-LM: In search of the next generation of training sets for language models](https://arxiv.org/abs/2406.11794)
Append: [Jailbreaking as a Reward Misspecification Problem](https://arxiv.org/abs/2406.14393)
Append: [OpenHands: An Open Platform for AI Software Developers as Generalist Agents](https://arxiv.org/abs/2407.16741)
Append: [MoWE-Audio: Multitask AudioLLMs with Mixture of Weak Encoders](https://arxiv.org/abs/2409.06635)
Append: [DARE the Extreme: Revisiting Delta-Parameter Pruning For Fine-Tuned Models](https://arxiv.org/abs/2410.09344)
Append: [Context-Parametric Inversion: Why Instruction Finetuning Can Worsen Context Reliance](https://arxiv.org/abs/2410.10796)
Append: [Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in Vision-Language Alignment](https://arxiv.org/abs/2410.14148)
Append: [Aioli: A Unified Optimization Framework for Language Model Data Mixing](https://arxiv.org/abs/2411.05735)
Append: [Unraveling Arithmetic in Large Language Models: The Role of Algebraic Structures](https://arxiv.org/abs/2411.16260)
Append: [Rerouting Connection: Hybrid Computer Vision Analysis Reveals Visual Similarity Between Indus and Tibetan-Yi Corridor Writing Systems](https://arxiv.org/abs/2503.21074)
Append: [Advancing MoE Efficiency: A Collaboration-Constrained Routing (C2R) Strategy for Better Expert Parallelism Design](https://arxiv.org/abs/2504.01337)
Append: [Self-Resource Allocation in Multi-Agent LLM Systems](https://arxiv.org/abs/2504.02051)
Append: [Unleashing the Power of LLMs in Dense Retrieval with Query Likelihood Modeling](https://arxiv.org/abs/2504.05216)
Append: [Fine-tuning a Large Language Model for Automating Computational Fluid Dynamics Simulations](https://arxiv.org/abs/2504.09602)
Append: [EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety](https://arxiv.org/abs/2504.09689)
append_entries: 192
Finish: 2025-04-22 04:25:33.740083
------------------------------------------------------
Started: 2025-04-22 06:23:11.701646
Existing_entries: 512
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-22 06:23:12.073259
------------------------------------------------------
Started: 2025-04-22 08:21:26.118165
Existing_entries: 512
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-22 08:21:26.477918
------------------------------------------------------
Started: 2025-04-22 10:17:12.338506
Existing_entries: 512
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-22 10:17:12.670682
------------------------------------------------------
Started: 2025-04-22 12:31:55.759357
Existing_entries: 512
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-22 12:31:56.094493
------------------------------------------------------
Started: 2025-04-22 14:16:20.423052
Existing_entries: 512
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-22 14:16:20.782122
------------------------------------------------------
Started: 2025-04-22 16:19:48.170928
Existing_entries: 512
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-22 16:19:48.517521
------------------------------------------------------
Started: 2025-04-22 18:21:57.037809
Existing_entries: 512
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-22 18:21:57.372659
------------------------------------------------------
Started: 2025-04-22 20:17:54.394184
Existing_entries: 512
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-22 20:17:54.717448
------------------------------------------------------
Started: 2025-04-22 22:15:20.731987
Existing_entries: 512
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-22 22:15:21.064344
------------------------------------------------------
Started: 2025-04-23 01:16:01.524302
Existing_entries: 512
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-23 01:16:01.926845
------------------------------------------------------
Started: 2025-04-23 03:01:55.044807
Existing_entries: 512
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-23 03:01:55.364725
------------------------------------------------------
Started: 2025-04-23 04:23:47.884309
Existing_entries: 512
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1618
Summarized using GPT-3.5-turbo
Append: [Exploring Compositional Generalization (in ReCOGS_pos) by Transformers using Restricted Access Sequence Processing (RASP)](https://arxiv.org/abs/2504.15349)
Token length: 1245
Summarized using GPT-3.5-turbo
Append: [Tell Me What You Know About Sexism: Expert-LLM Interaction Strategies and Co-Created Definitions for Zero-Shot Sexism Detection](https://arxiv.org/abs/2504.15392)
Token length: 777
Summarized using GPT-3.5-turbo
Append: [Trillion 7B Technical Report](https://arxiv.org/abs/2504.15431)
Token length: 1169
Summarized using GPT-3.5-turbo
Append: [Feeding LLM Annotations to BERT Classifiers at Your Own Risk](https://arxiv.org/abs/2504.15432)
Token length: 1457
Summarized using GPT-3.5-turbo
Append: [Bigram Subnetworks: Mapping to Next Tokens in Transformer Language Models](https://arxiv.org/abs/2504.15471)
Token length: 770
Summarized using GPT-3.5-turbo
Append: [Speculative Sampling via Exponential Races](https://arxiv.org/abs/2504.15475)
Token length: 1254
Summarized using GPT-3.5-turbo
Append: [SimulS2S-LLM: Unlocking Simultaneous Inference of Speech LLMs for Speech-to-Speech Translation](https://arxiv.org/abs/2504.15509)
Token length: 1874
Summarized using GPT-3.5-turbo
Append: [The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks](https://arxiv.org/abs/2504.15521)
Token length: 1338
Summarized using GPT-3.5-turbo
Append: [IPBench: Benchmarking the Knowledge of Large Language Models in Intellectual Property](https://arxiv.org/abs/2504.15524)
Token length: 1432
Summarized using GPT-3.5-turbo
Append: [Compass-V2 Technical Report](https://arxiv.org/abs/2504.15527)
Token length: 1098
Summarized using GPT-3.5-turbo
Append: [llm-jp-modernbert: A ModernBERT Model Trained on a Large-Scale Japanese Corpus with Long Context Length](https://arxiv.org/abs/2504.15544)
Token length: 1452
Summarized using GPT-3.5-turbo
Append: [LLM-based Semantic Augmentation for Harmful Content Detection](https://arxiv.org/abs/2504.15548)
Token length: 1335
Summarized using GPT-3.5-turbo
Append: [Instruction-Tuning Data Synthesis from Scratch via Web Reconstruction](https://arxiv.org/abs/2504.15573)
Token length: 1896
Summarized using GPT-3.5-turbo
Append: [Exploring Next Token Prediction in Theory of Mind (ToM) Tasks: Comparative Experiments with GPT-2 and LLaMA-2 AI Models](https://arxiv.org/abs/2504.15604)
Token length: 1083
Summarized using GPT-3.5-turbo
Append: [Exploiting Contextual Knowledge in LLMs through V-usable Information based Layer Enhancement](https://arxiv.org/abs/2504.15630)
Token length: 1631
Summarized using GPT-3.5-turbo
Append: [Cost-Effective Text Clustering with Large Language Models](https://arxiv.org/abs/2504.15640)
Token length: 744
Summarized using GPT-3.5-turbo
Append: [Computational Typology](https://arxiv.org/abs/2504.15642)
Token length: 1744
Summarized using GPT-3.5-turbo
Append: [FinTextSim: Enhancing Financial Text Analysis with BERTopic](https://arxiv.org/abs/2504.15683)
Token length: 1875
Summarized using GPT-3.5-turbo
Append: [Subject islands do not reduce to construction-specific discourse function](https://arxiv.org/abs/2504.15688)
Token length: 1623
Summarized using GPT-3.5-turbo
Append: [Tina: Tiny Reasoning Models via LoRA](https://arxiv.org/abs/2504.15777)
Token length: 907
Summarized using GPT-3.5-turbo
Append: [Automated Creativity Evaluation for Large Language Models: A Reference-Based Approach](https://arxiv.org/abs/2504.15784)
Token length: 1800
Summarized using GPT-3.5-turbo
Append: [A closer look at how large language models trust humans: patterns and biases](https://arxiv.org/abs/2504.15801)
Token length: 1384
Summarized using GPT-3.5-turbo
Append: [What's the Difference? Supporting Users in Identifying the Effects of Prompt and Model Changes Through Token Patterns](https://arxiv.org/abs/2504.15815)
Token length: 1363
Summarized using GPT-3.5-turbo
Append: [Pre-DPO: Improving Data Utilization in Direct Preference Optimization Using a Guiding Reference Model](https://arxiv.org/abs/2504.15843)
Token length: 1828
Summarized using GPT-3.5-turbo
Append: [Exploring Cognitive and Aesthetic Causality for Multimodal Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2504.15848)
Token length: 1185
Summarized using GPT-3.5-turbo
Append: [Dynamic Early Exit in Reasoning Models](https://arxiv.org/abs/2504.15895)
Token length: 1504
Summarized using GPT-3.5-turbo
Append: [SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning](https://arxiv.org/abs/2504.15900)
Token length: 1513
Summarized using GPT-3.5-turbo
Append: [FairTranslate: An English-French Dataset for Gender Bias Evaluation in Machine Translation by Overcoming Gender Binarity](https://arxiv.org/abs/2504.15941)
Token length: 1635
Summarized using GPT-3.5-turbo
Append: [W-PCA Based Gradient-Free Proxy for Efficient Search of Lightweight Language Models](https://arxiv.org/abs/2504.15983)
Token length: 1173
Summarized using GPT-3.5-turbo
Append: [Few-shot Hate Speech Detection Based on the MindSpore Framework](https://arxiv.org/abs/2504.15987)
Token length: 1531
Summarized using GPT-3.5-turbo
Append: [CAPO: Cost-Aware Prompt Optimization](https://arxiv.org/abs/2504.16005)
Token length: 682
Summarized using GPT-3.5-turbo
Append: [Methods for Recognizing Nested Terms](https://arxiv.org/abs/2504.16007)
Token length: 1441
Summarized using GPT-3.5-turbo
Append: [Certified Mitigation of Worst-Case LLM Copyright Infringement](https://arxiv.org/abs/2504.16046)
Token length: 1838
Summarized using GPT-3.5-turbo
Append: [LongMamba: Enhancing Mamba's Long Context Capabilities via Training-Free Receptive Field Enlargement](https://arxiv.org/abs/2504.16053)
Token length: 1436
Summarized using GPT-3.5-turbo
Append: [Honey, I Shrunk the Language Model: Impact of Knowledge Distillation Methods on Performance and Explainability](https://arxiv.org/abs/2504.16056)
Token length: 1260
Summarized using GPT-3.5-turbo
Append: [Vision-Language Models Are Not Pragmatically Competent in Referring Expression Generation](https://arxiv.org/abs/2504.16060)
Token length: 1692
Summarized using GPT-3.5-turbo
Append: [A Python Tool for Reconstructing Full News Text from GDELT](https://arxiv.org/abs/2504.16063)
Token length: 1340
Summarized using GPT-3.5-turbo
Append: [Guiding VLM Agents with Process Rewards at Inference Time for GUI Navigation](https://arxiv.org/abs/2504.16073)
Token length: 1313
Summarized using GPT-3.5-turbo
Append: [PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models](https://arxiv.org/abs/2504.16074)
Token length: 1412
Summarized using GPT-3.5-turbo
Append: [TTRL: Test-Time Reinforcement Learning](https://arxiv.org/abs/2504.16084)
Token length: 1259
Summarized using GPT-3.5-turbo
Append: [Med-CoDE: Medical Critique based Disagreement Evaluation Framework](https://arxiv.org/abs/2504.15330)
Token length: 1585
Summarized using GPT-3.5-turbo
Append: [LongPerceptualThoughts: Distilling System-2 Reasoning for System-1 Perception](https://arxiv.org/abs/2504.15362)
Token length: 1526
Summarized using GPT-3.5-turbo
Append: [Towards Understanding Camera Motions in Any Video](https://arxiv.org/abs/2504.15376)
Token length: 1375
Summarized using GPT-3.5-turbo
Append: [IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning in Multimodal LLMs](https://arxiv.org/abs/2504.15415)
Token length: 1516
Summarized using GPT-3.5-turbo
Append: [Real-Time Sentiment Insights from X Using VADER, DistilBERT, and Web-Scraped Data](https://arxiv.org/abs/2504.15448)
Token length: 1527
Summarized using GPT-3.5-turbo
Append: [Learning Adaptive Parallel Reasoning with Language Models](https://arxiv.org/abs/2504.15466)
Token length: 1846
Summarized using GPT-3.5-turbo
Append: [CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting](https://arxiv.org/abs/2504.15485)
Token length: 1949
Summarized using GPT-3.5-turbo
Append: [A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment](https://arxiv.org/abs/2504.15585)
Token length: 1539
Summarized using GPT-3.5-turbo
Append: [CiteFix: Enhancing RAG Accuracy Through Post-Processing Citation Correction](https://arxiv.org/abs/2504.15629)
Token length: 1674
Summarized using GPT-3.5-turbo
Append: [VeriCoder: Enhancing LLM-Based RTL Code Generation through Functional Correctness Validation](https://arxiv.org/abs/2504.15659)
Append: [TrustGeoGen: Scalable and Formal-Verified Data Engine for Trustworthy Multi-modal Geometric Problem Solving](https://arxiv.org/abs/2504.15780)
Append: [How Private is Your Attention? Bridging Privacy with In-Context Learning](https://arxiv.org/abs/2504.16000)
Append: [Survey of Video Diffusion Models: Foundations, Implementations, and Applications](https://arxiv.org/abs/2504.16081)
Append: [Aggregating Soft Labels from Crowd Annotations Improves Uncertainty Estimation Under Distribution Shift](https://arxiv.org/abs/2212.09409)
Append: [On the Low-Rank Parametrization of Reward Models for Controlled Language Generation](https://arxiv.org/abs/2407.04615)
Append: [Review-driven Personalized Preference Reasoning with Large Language Models for Recommendation](https://arxiv.org/abs/2408.06276)
Append: [Open-World Evaluation for Retrieving Diverse Perspectives](https://arxiv.org/abs/2409.18110)
Append: [AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models](https://arxiv.org/abs/2410.02355)
Append: [SWITCH: Studying with Teacher for Knowledge Distillation of Large Language Models](https://arxiv.org/abs/2410.19503)
Append: [Diversity Helps Jailbreak Large Language Models](https://arxiv.org/abs/2411.04223)
Append: [Falcon: Faster and Parallel Inference of Large Language Models through Enhanced Semi-Autoregressive Drafting and Custom-Designed Decoding Tree](https://arxiv.org/abs/2412.12639)
Append: [Fine-tuning Whisper on Low-Resource Languages for Real-World Applications](https://arxiv.org/abs/2412.15726)
Append: [Fearful Falcons and Angry Llamas: Emotion Category Annotations of Arguments by Humans and LLMs](https://arxiv.org/abs/2412.15993)
Append: [Evaluating the Robustness of Multimodal Agents Against Active Environmental Injection Attacks](https://arxiv.org/abs/2502.13053)
Append: [Parallel Corpora for Machine Translation in Low-resource Indic Languages: A Comprehensive Review](https://arxiv.org/abs/2503.04797)
Append: [Plan-and-Act: Improving Planning of Agents for Long-Horizon Tasks](https://arxiv.org/abs/2503.09572)
Append: [Key, Value, Compress: A Systematic Exploration of KV Cache Compression Techniques](https://arxiv.org/abs/2503.11816)
Append: [FUSE : A Ridge and Random Forest-Based Metric for Evaluating MT in Indigenous Languages](https://arxiv.org/abs/2504.00021)
Append: [Investigating and Scaling up Code-Switching for Multilingual Language Model Pre-Training](https://arxiv.org/abs/2504.01801)
Append: [VocalNet: Speech LLM with Multi-Token Prediction for Faster and High-Quality Generation](https://arxiv.org/abs/2504.04060)
Append: [Regional Tiny Stories: Using Small Models to Compare Language Learning and Tokenizer Performance](https://arxiv.org/abs/2504.07989)
Append: [Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the Evaluation Protocol](https://arxiv.org/abs/2504.10284)
Append: [Certifying Knowledge Comprehension in LLMs](https://arxiv.org/abs/2402.15929)
Append: [Optimizing RLHF Training for Large Language Models with Stage Fusion](https://arxiv.org/abs/2409.13221)
Append: [Inspection and Control of Self-Generated-Text Recognition Ability in Llama3-8b-Instruct](https://arxiv.org/abs/2410.02064)
Append: [A Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement](https://arxiv.org/abs/2410.13828)
Append: [NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples](https://arxiv.org/abs/2410.14669)
Append: [Towards Unifying Evaluation of Counterfactual Explanations: Leveraging Large Language Models for Human-Centric Assessments](https://arxiv.org/abs/2410.21131)
Append: [Codenames as a Benchmark for Large Language Models](https://arxiv.org/abs/2412.11373)
Append: [FOReCAst: The Future Outcome Reasoning and Confidence Assessment Benchmark](https://arxiv.org/abs/2502.19676)
Append: [AgentA/B: Automated and Scalable Web A/BTesting with Interactive LLM Agents](https://arxiv.org/abs/2504.09723)
append_entries: 81
Finish: 2025-04-23 04:25:49.528895
------------------------------------------------------
Started: 2025-04-23 06:23:05.510321
Existing_entries: 593
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-23 06:23:05.708699
------------------------------------------------------
Started: 2025-04-23 08:21:12.383997
Existing_entries: 593
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-23 08:21:12.583563
------------------------------------------------------
Started: 2025-04-23 10:18:03.041403
Existing_entries: 593
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-23 10:18:03.218827
------------------------------------------------------
Started: 2025-04-23 12:32:26.453256
Existing_entries: 593
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-23 12:32:26.631945
------------------------------------------------------
Started: 2025-04-23 14:16:58.304240
Existing_entries: 593
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-23 14:16:58.503814
------------------------------------------------------
Started: 2025-04-23 16:20:48.513946
Existing_entries: 593
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-23 16:20:48.687699
------------------------------------------------------
Started: 2025-04-23 18:23:17.886460
Existing_entries: 593
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-23 18:23:18.072062
------------------------------------------------------
Started: 2025-04-23 20:18:04.155370
Existing_entries: 593
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-23 20:18:04.350490
------------------------------------------------------
Started: 2025-04-23 22:15:40.121407
Existing_entries: 593
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-23 22:15:40.325700
------------------------------------------------------
Started: 2025-04-24 01:16:03.671826
Existing_entries: 593
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-24 01:16:03.917041
------------------------------------------------------
Started: 2025-04-24 03:03:43.859659
Existing_entries: 593
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-24 03:03:44.034993
------------------------------------------------------
Started: 2025-04-24 04:23:41.148106
Existing_entries: 593
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 912
Summarized using GPT-3.5-turbo
Append: [FinNLI: Novel Dataset for Multi-Genre Financial Natural Language Inference Benchmarking](https://arxiv.org/abs/2504.16188)
Token length: 1173
Summarized using GPT-3.5-turbo
Append: [The Language of Attachment: Modeling Attachment Dynamics in Psychotherapy](https://arxiv.org/abs/2504.16271)
Token length: 1257
Summarized using GPT-3.5-turbo
Append: [The Paradox of Poetic Intent in Back-Translation: Evaluating the Quality of Large Language Models in Chinese Translation](https://arxiv.org/abs/2504.16286)
Token length: 806
Summarized using GPT-3.5-turbo
Append: [Capturing Symmetry and Antisymmetry in Language Models through Symmetry-Aware Training Objectives](https://arxiv.org/abs/2504.16312)
Token length: 1816
Summarized using GPT-3.5-turbo
Append: [Transformer-Based Extraction of Statutory Definitions from the U.S. Code](https://arxiv.org/abs/2504.16353)
Token length: 1318
Summarized using GPT-3.5-turbo
Append: [Text-to-TrajVis: Enabling Trajectory Data Visualizations from Natural Language Questions](https://arxiv.org/abs/2504.16358)
Token length: 1474
Summarized using GPT-3.5-turbo
Append: [SplitReason: Learning To Offload Reasoning](https://arxiv.org/abs/2504.16379)
Token length: 1363
Summarized using GPT-3.5-turbo
Append: [ConTextual: Improving Clinical Text Summarization in LLMs with Context-preserving Token Filtering and Knowledge Graphs](https://arxiv.org/abs/2504.16394)
Token length: 1134
Summarized using GPT-3.5-turbo
Append: [Less is More: Enhancing Structured Multi-Agent Reasoning via Quality-Guided Distillation](https://arxiv.org/abs/2504.16408)
Token length: 848
Summarized using GPT-3.5-turbo
Append: [Out-of-the-Box Conditional Text Embeddings from Large Language Models](https://arxiv.org/abs/2504.16411)
Token length: 1481
Summarized using GPT-3.5-turbo
Append: [Evaluating Multi-Hop Reasoning in Large Language Models: A Chemistry-Centric Case Study](https://arxiv.org/abs/2504.16414)
Token length: 1328
Summarized using GPT-3.5-turbo
Append: [Can Large Language Models Help Multimodal Language Analysis? MMLA: A Comprehensive Benchmark](https://arxiv.org/abs/2504.16427)
Token length: 1384
Summarized using GPT-3.5-turbo
Append: [EMRModel: A Large Language Model for Extracting Medical Consultation Dialogues into Structured Medical Records](https://arxiv.org/abs/2504.16448)
Token length: 1565
Summarized using GPT-3.5-turbo
Append: [T-VEC: A Telecom-Specific Vectorization Model with Enhanced Semantic Understanding via Deep Triplet Loss Fine-Tuning](https://arxiv.org/abs/2504.16460)
Token length: 1674
Summarized using GPT-3.5-turbo
Append: [QuaDMix: Quality-Diversity Balanced Data Selection for Efficient LLM Pretraining](https://arxiv.org/abs/2504.16511)
Token length: 1338
Summarized using GPT-3.5-turbo
Append: [Transformers for Complex Query Answering over Knowledge Hypergraphs](https://arxiv.org/abs/2504.16537)
Token length: 1543
Summarized using GPT-3.5-turbo
Append: [PIS: Linking Importance Sampling and Attention Mechanisms for Efficient Prompt Compression](https://arxiv.org/abs/2504.16574)
Token length: 901
Summarized using GPT-3.5-turbo
Append: [Comparing Large Language Models and Traditional Machine Translation Tools for Translating Medical Consultation Summaries: A Pilot Study](https://arxiv.org/abs/2504.16601)
Token length: 862
Summarized using GPT-3.5-turbo
Append: [Debunking with Dialogue? Exploring AI-Generated Counterspeech to Challenge Conspiracy Theories](https://arxiv.org/abs/2504.16604)
Token length: 734
Summarized using GPT-3.5-turbo
Append: [TIFIN India at SemEval-2025: Harnessing Translation to Overcome Multilingual IR Challenges in Fact-Checked Claim Retrieval](https://arxiv.org/abs/2504.16627)
Token length: 1013
Summarized using GPT-3.5-turbo
Append: [A Post-trainer's Guide to Multilingual Training Data: Uncovering Cross-lingual Transfer Dynamics](https://arxiv.org/abs/2504.16677)
Token length: 1515
Summarized using GPT-3.5-turbo
Append: [HEMA : A Hippocampus-Inspired Extended Memory Architecture for Long-Context AI Conversations](https://arxiv.org/abs/2504.16754)
Token length: 1521
Summarized using GPT-3.5-turbo
Append: [How Effective are Generative Large Language Models in Performing Requirements Classification?](https://arxiv.org/abs/2504.16768)
Token length: 1256
Summarized using GPT-3.5-turbo
Append: [Evaluation Framework for AI Systems in "the Wild"](https://arxiv.org/abs/2504.16778)
Token length: 1239
Summarized using GPT-3.5-turbo
Append: [MOOSComp: Improving Lightweight Long-Context Compressor via Mitigating Over-Smoothing and Incorporating Outlier Scores](https://arxiv.org/abs/2504.16786)
Token length: 1695
Summarized using GPT-3.5-turbo
Append: [Credible plan-driven RAG method for Multi-hop Question Answering](https://arxiv.org/abs/2504.16787)
Token length: 1421
Summarized using GPT-3.5-turbo
Append: [Random Long-Context Access for Mamba via Hardware-aligned Hierarchical Sparse Attention](https://arxiv.org/abs/2504.16795)
Token length: 748
Summarized using GPT-3.5-turbo
Append: [LLM-assisted Graph-RAG Information Extraction from IFC Data](https://arxiv.org/abs/2504.16813)
Token length: 1175
Summarized using GPT-3.5-turbo
Append: [GreenMind: A Next-Generation Vietnamese Large Language Model for Structured and Logical Reasoning](https://arxiv.org/abs/2504.16832)
Token length: 1354
Summarized using GPT-3.5-turbo
Append: [Monte Carlo Planning with Large Language Model for Text-Based Game Agents](https://arxiv.org/abs/2504.16855)
Token length: 1526
Summarized using GPT-3.5-turbo
Append: [Emo Pillars: Knowledge Distillation to Support Fine-Grained Context-Aware and Context-Less Emotion Classification](https://arxiv.org/abs/2504.16856)
Token length: 1336
Summarized using GPT-3.5-turbo
Append: [Planning with Diffusion Models for Target-Oriented Dialogue Systems](https://arxiv.org/abs/2504.16858)
Token length: 1135
Summarized using GPT-3.5-turbo
Append: [Do Large Language Models know who did what to whom?](https://arxiv.org/abs/2504.16884)
Token length: 1030
Summarized using GPT-3.5-turbo
Append: [Tracing Thought: Using Chain-of-Thought Reasoning to Identify the LLM Behind AI-Generated Text](https://arxiv.org/abs/2504.16913)
Token length: 1637
Summarized using GPT-3.5-turbo
Append: [OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents](https://arxiv.org/abs/2504.16918)
Token length: 1878
Summarized using GPT-3.5-turbo
Append: [IberBench: LLM Evaluation on Iberian Languages](https://arxiv.org/abs/2504.16921)
Token length: 1137
Summarized using GPT-3.5-turbo
Append: [Cooperative Speech, Semantic Competence, and AI](https://arxiv.org/abs/2504.16092)
Token length: 994
Summarized using GPT-3.5-turbo
Append: [HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM Inference via GPU Co-processing](https://arxiv.org/abs/2504.16112)
Token length: 1065
Summarized using GPT-3.5-turbo
Append: [LegalRAG: A Hybrid RAG System for Multilingual Legal Information Retrieval](https://arxiv.org/abs/2504.16121)
Token length: 1198
Summarized using GPT-3.5-turbo
Append: [Multimodal Large Language Models for Enhanced Traffic Safety: A Comprehensive Review and Future Trends](https://arxiv.org/abs/2504.16134)
Token length: 1921
Summarized using GPT-3.5-turbo
Append: [Reflexive Prompt Engineering: A Framework for Responsible Prompt Engineering and Interaction Design](https://arxiv.org/abs/2504.16204)
Token length: 757
Summarized using GPT-3.5-turbo
Append: [Using Phonemes in cascaded S2S translation pipeline](https://arxiv.org/abs/2504.16234)
Token length: 1574
Summarized using GPT-3.5-turbo
Append: [CLIRudit: Cross-Lingual Information Retrieval of Scientific Documents](https://arxiv.org/abs/2504.16264)
Token length: 1512
Summarized using GPT-3.5-turbo
Append: [SignX: The Foundation Model for Sign Recognition](https://arxiv.org/abs/2504.16315)
Token length: 687
Summarized using GPT-3.5-turbo
Append: [MAGIC: Near-Optimal Data Attribution for Deep Learning](https://arxiv.org/abs/2504.16430)
Token length: 1025
Summarized using GPT-3.5-turbo
Append: [ParetoHqD: Fast Offline Multiobjective Alignment of Large Language Models using Pareto High-quality Data](https://arxiv.org/abs/2504.16628)
Token length: 1260
Summarized using GPT-3.5-turbo
Append: [IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery](https://arxiv.org/abs/2504.16728)
Token length: 1514
Summarized using GPT-3.5-turbo
Append: [Process Reward Models That Think](https://arxiv.org/abs/2504.16828)
Token length: 1145
Summarized using GPT-3.5-turbo
Append: [AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset](https://arxiv.org/abs/2504.16891)
Token length: 947
Summarized using GPT-3.5-turbo
Append: [Beyond Self Attention: A Subquadratic Fourier Wavelet Transformer with Multi Modal Fusion](https://arxiv.org/abs/2111.15473)
Append: [Comparative Performance Evaluation of Large Language Models for Extracting Molecular Interactions and Pathway Knowledge](https://arxiv.org/abs/2307.08813)
Append: [A dataset and benchmark for hospital course summarization with adapted large language models](https://arxiv.org/abs/2403.05720)
Append: [NovelQA: Benchmarking Question Answering on Documents Exceeding 200K Tokens](https://arxiv.org/abs/2403.12766)
Append: [Large Language Model Sentinel: LLM Agent for Adversarial Purification](https://arxiv.org/abs/2405.20770)
Append: [Synthetic Lyrics Detection Across Languages and Genres](https://arxiv.org/abs/2406.15231)
Append: [SemioLLM: Evaluating Large Language Models for Diagnostic Reasoning from Unstructured Clinical Narratives in Epilepsy](https://arxiv.org/abs/2407.03004)
Append: [ITERTL: An Iterative Framework for Fine-tuning LLMs for RTL Code Generation](https://arxiv.org/abs/2407.12022)
Append: [Lawma: The Power of Specialization for Legal Annotation](https://arxiv.org/abs/2407.16615)
Append: [Modelling Multimodal Integration in Human Concept Processing with Vision-Language Models](https://arxiv.org/abs/2407.17914)
Append: [The advantages of context specific language models: the case of the Erasmian Language Model](https://arxiv.org/abs/2408.06931)
Append: [lamss: when large language models meet self-skepticism](https://arxiv.org/abs/2409.06601)
Append: [ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems](https://arxiv.org/abs/2410.19572)
Append: [MEG: Medical Knowledge-Augmented Large Language Models for Question Answering](https://arxiv.org/abs/2411.03883)
Append: [Needle Threading: Can LLMs Follow Threads through Near-Million-Scale Haystacks?](https://arxiv.org/abs/2411.05000)
Append: [Sufficient Context: A New Lens on Retrieval Augmented Generation Systems](https://arxiv.org/abs/2411.06037)
Append: [7B Fully Open Source Moxin-LLM -- From Pretraining to GRPO-based Reinforcement Learning Enhancement](https://arxiv.org/abs/2412.06845)
Append: [Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition](https://arxiv.org/abs/2412.13612)
Append: [Evaluating Text Style Transfer Evaluation: Are There Any Reliable Metrics?](https://arxiv.org/abs/2502.04718)
Append: [Clinical QA 2.0: Multi-Task Learning for Answer Extraction and Categorization](https://arxiv.org/abs/2502.13108)
Append: [EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test](https://arxiv.org/abs/2503.01840)
Append: [Calibrating Verbal Uncertainty as a Linear Feature to Reduce Hallucinations](https://arxiv.org/abs/2503.14477)
Append: [Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models](https://arxiv.org/abs/2503.16419)
Append: [Accelerate Parallelizable Reasoning via Parallel Decoding within One Sequence](https://arxiv.org/abs/2503.20533)
Append: [SocioVerse: A World Model for Social Simulation Powered by LLM Agents and A Pool of 10 Million Real-World Users](https://arxiv.org/abs/2504.10157)
Append: [TCAN: Text-oriented Cross Attention Network for Multimodal Sentiment Analysis](https://arxiv.org/abs/2404.04545)
Append: [Multimodal Situational Safety](https://arxiv.org/abs/2410.06172)
Append: [Automated Trustworthiness Oracle Generation for Machine Learning Text Classifiers](https://arxiv.org/abs/2410.22663)
Append: [Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning](https://arxiv.org/abs/2411.18203)
Append: [StarWhisper Telescope: Agent-Based Observation Assistant System to Approach AI Astrophysicist](https://arxiv.org/abs/2412.06412)
Append: [MedMax: Mixed-Modal Instruction Tuning for Training Biomedical Assistants](https://arxiv.org/abs/2412.12661)
Append: [Lorecast: Layout-Aware Performance and Power Forecasting from Natural Language](https://arxiv.org/abs/2503.11662)
Append: [Dynamic hashtag recommendation in social media with trend shift detection and adaptation](https://arxiv.org/abs/2504.00044)
Append: [Analyzing 16,193 LLM Papers for Fun and Profits](https://arxiv.org/abs/2504.08619)
append_entries: 83
Finish: 2025-04-24 04:25:54.807250
------------------------------------------------------
Started: 2025-04-24 06:23:36.982518
Existing_entries: 676
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-24 06:23:37.215010
------------------------------------------------------
Started: 2025-04-24 08:21:31.629261
Existing_entries: 676
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-24 08:21:31.818649
------------------------------------------------------
Started: 2025-04-24 10:17:44.427132
Existing_entries: 676
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-24 10:17:44.644702
------------------------------------------------------
Started: 2025-04-24 12:33:41.557085
Existing_entries: 676
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-24 12:33:41.775548
------------------------------------------------------
Started: 2025-04-24 14:15:17.148893
Existing_entries: 676
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-24 14:15:17.336330
------------------------------------------------------
Started: 2025-04-24 16:20:00.365624
Existing_entries: 676
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-24 16:20:00.579474
------------------------------------------------------
Started: 2025-04-24 18:21:39.126113
Existing_entries: 676
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-24 18:21:39.315127
------------------------------------------------------
Started: 2025-04-24 20:18:03.965115
Existing_entries: 676
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-24 20:18:04.153653
------------------------------------------------------
Started: 2025-04-24 22:15:20.157299
Existing_entries: 676
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-24 22:15:20.375474
------------------------------------------------------
Started: 2025-04-25 01:16:38.936972
Existing_entries: 676
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-25 01:16:39.121732
------------------------------------------------------
Started: 2025-04-25 03:03:50.691430
Existing_entries: 676
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-25 03:03:50.878160
------------------------------------------------------
Started: 2025-04-25 04:21:30.354646
Existing_entries: 676
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1362
Summarized using GPT-3.5-turbo
Append: [Bidirectional Mamba for Single-Cell Data: Efficient Context Learning with Biological Fidelity](https://arxiv.org/abs/2504.16956)
Token length: 1931
Summarized using GPT-3.5-turbo
Append: [Tokenization Matters: Improving Zero-Shot NER for Indic Languages](https://arxiv.org/abs/2504.16977)
Token length: 1304
Summarized using GPT-3.5-turbo
Append: [Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation](https://arxiv.org/abs/2504.17025)
Token length: 1375
Summarized using GPT-3.5-turbo
Append: [Do Words Reflect Beliefs? Evaluating Belief Depth in Large Language Models](https://arxiv.org/abs/2504.17052)
Token length: 1404
Summarized using GPT-3.5-turbo
Append: [Agree to Disagree? A Meta-Evaluation of LLM Misgendering](https://arxiv.org/abs/2504.17075)
Token length: 1705
Summarized using GPT-3.5-turbo
Append: [How Individual Traits and Language Styles Shape Preferences In Open-ended User-LLM Interaction: A Preliminary Study](https://arxiv.org/abs/2504.17083)
Token length: 1258
Summarized using GPT-3.5-turbo
Append: [Co-CoT: A Prompt-Based Framework for Collaborative Chain-of-Thought Reasoning](https://arxiv.org/abs/2504.17091)
Token length: 1522
Summarized using GPT-3.5-turbo
Append: [The Rise of Small Language Models in Healthcare: A Comprehensive Survey](https://arxiv.org/abs/2504.17119)
Token length: 907
Summarized using GPT-3.5-turbo
Append: [Steering the CensorShip: Uncovering Representation Vectors for LLM "Thought" Control](https://arxiv.org/abs/2504.17130)
Token length: 1408
Summarized using GPT-3.5-turbo
Append: [MIRAGE: A Metric-Intensive Benchmark for Retrieval-Augmented Generation Evaluation](https://arxiv.org/abs/2504.17137)
Token length: 1510
Summarized using GPT-3.5-turbo
Append: [Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning](https://arxiv.org/abs/2504.17192)
Token length: 1240
Summarized using GPT-3.5-turbo
Append: [A RAG-Based Multi-Agent LLM System for Natural Hazard Resilience and Adaptation](https://arxiv.org/abs/2504.17200)
Token length: 1636
Summarized using GPT-3.5-turbo
Append: [Does Knowledge Distillation Matter for Large Language Model based Bundle Generation?](https://arxiv.org/abs/2504.17220)
Token length: 1180
Summarized using GPT-3.5-turbo
Append: [Crisp: Cognitive Restructuring of Negative Thoughts through Multi-turn Supportive Dialogues](https://arxiv.org/abs/2504.17238)
Token length: 1187
Summarized using GPT-3.5-turbo
Append: [Low-Resource Neural Machine Translation Using Recurrent Neural Networks and Transfer Learning: A Case Study on English-to-Igbo](https://arxiv.org/abs/2504.17252)
Token length: 1072
Summarized using GPT-3.5-turbo
Append: [JurisCTC: Enhancing Legal Judgment Prediction via Cross-Domain Transfer and Contrastive Learning](https://arxiv.org/abs/2504.17264)
Token length: 1883
Summarized using GPT-3.5-turbo
Append: [Evaluating and Mitigating Bias in AI-Based Medical Text Generation](https://arxiv.org/abs/2504.17279)
Token length: 1106
Summarized using GPT-3.5-turbo
Append: [CoheMark: A Novel Sentence-Level Watermark for Enhanced Text Quality](https://arxiv.org/abs/2504.17309)
Token length: 1080
Summarized using GPT-3.5-turbo
Append: [FLUKE: A Linguistically-Driven and Task-Agnostic Framework for Robustness Evaluation](https://arxiv.org/abs/2504.17311)
Token length: 1101
Summarized using GPT-3.5-turbo
Append: [Bridging Cognition and Emotion: Empathy-Driven Multimodal Misinformation Detection](https://arxiv.org/abs/2504.17332)
Token length: 1196
Summarized using GPT-3.5-turbo
Append: [M-MRE: Extending the Mutual Reinforcement Effect to Multimodal Information Extraction](https://arxiv.org/abs/2504.17353)
Token length: 1423
Summarized using GPT-3.5-turbo
Append: [PatientDx: Merging Large Language Models for Protecting Data-Privacy in Healthcare](https://arxiv.org/abs/2504.17360)
Token length: 1562
Summarized using GPT-3.5-turbo
Append: [LiveLongBench: Tackling Long-Context Understanding for Spoken Texts from Live Streams](https://arxiv.org/abs/2504.17366)
Token length: 989
Summarized using GPT-3.5-turbo
Append: [PicPersona-TOD : A Dataset for Personalizing Utterance Style in Task-Oriented Dialogue with Image Persona](https://arxiv.org/abs/2504.17390)
Token length: 992
Summarized using GPT-3.5-turbo
Append: [Creating Targeted, Interpretable Topic Models with LLM-Generated Text Augmentation](https://arxiv.org/abs/2504.17445)
Token length: 1584
Summarized using GPT-3.5-turbo
Append: [Unified Attacks to Large Language Model Watermarks: Spoofing and Scrubbing in Unauthorized Knowledge Distillation](https://arxiv.org/abs/2504.17480)
Token length: 1510
Summarized using GPT-3.5-turbo
Append: [HalluLens: LLM Hallucination Benchmark](https://arxiv.org/abs/2504.17550)
Token length: 1500
Summarized using GPT-3.5-turbo
Append: [When Does Metadata Conditioning (NOT) Work for Language Model Pre-Training? A Study with Context-Free Grammars](https://arxiv.org/abs/2504.17562)
Token length: 1455
Summarized using GPT-3.5-turbo
Append: [DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale Difficulty-Graded Data Training](https://arxiv.org/abs/2504.17565)
Token length: 1097
Summarized using GPT-3.5-turbo
Append: [RAGAT-Mind: A Multi-Granular Modeling Approach for Rumor Detection Based on MindSpore](https://arxiv.org/abs/2504.17574)
Token length: 1198
Summarized using GPT-3.5-turbo
Append: [Towards a comprehensive taxonomy of online abusive language informed by machine leaning](https://arxiv.org/abs/2504.17653)
Token length: 1332
Summarized using GPT-3.5-turbo
Append: [Evaluating Grounded Reasoning by Code-Assisted Large Language Models for Mathematics](https://arxiv.org/abs/2504.17665)
Token length: 1656
Summarized using GPT-3.5-turbo
Append: [Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction](https://arxiv.org/abs/2504.17671)
Token length: 1539
Summarized using GPT-3.5-turbo
Append: [Energy Considerations of Large Language Model Inference and Efficiency Optimizations](https://arxiv.org/abs/2504.17674)
Token length: 1110
Summarized using GPT-3.5-turbo
Append: [Ensemble Bayesian Inference: Leveraging Small Language Models to Achieve LLM-level Accuracy in Profile Matching Tasks](https://arxiv.org/abs/2504.17685)
Token length: 836
Summarized using GPT-3.5-turbo
Append: [Safety in Large Reasoning Models: A Survey](https://arxiv.org/abs/2504.17704)
Token length: 1053
Summarized using GPT-3.5-turbo
Append: [Multilingual Performance Biases of Large Language Models in Education](https://arxiv.org/abs/2504.17720)
Token length: 993
Summarized using GPT-3.5-turbo
Append: [Conversational Assistants to support Heart Failure Patients: comparing a Neurosymbolic Architecture with ChatGPT](https://arxiv.org/abs/2504.17753)
Token length: 1702
Summarized using GPT-3.5-turbo
Append: [The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs](https://arxiv.org/abs/2504.17768)
Token length: 1722
Summarized using GPT-3.5-turbo
Append: [A Desideratum for Conversational Agents: Capabilities, Challenges, and Future Directions](https://arxiv.org/abs/2504.16939)
Token length: 1890
Summarized using GPT-3.5-turbo
Append: [(Im)possibility of Automated Hallucination Detection in Large Language Models](https://arxiv.org/abs/2504.17004)
Token length: 874
Summarized using GPT-3.5-turbo
Append: [SCALAR: A Part-of-speech Tagger for Identifiers](https://arxiv.org/abs/2504.17038)
Token length: 1601
Summarized using GPT-3.5-turbo
Append: [TimeSoccer: An End-to-End Multimodal Large Language Model for Soccer Commentary Generation](https://arxiv.org/abs/2504.17365)
Token length: 1619
Summarized using GPT-3.5-turbo
Append: [HMI: Hierarchical Knowledge Management for Efficient Multi-Tenant Inference in Pretrained Language Models](https://arxiv.org/abs/2504.17449)
Token length: 1960
Summarized using GPT-3.5-turbo
Append: [CADS: A Systematic Literature Review on the Challenges of Abstractive Dialogue Summarization](https://arxiv.org/abs/2406.07494)
Token length: 1228
Summarized using GPT-3.5-turbo
Append: [OPT-Tree: Speculative Decoding with Adaptive Draft Tree Structure](https://arxiv.org/abs/2406.17276)
Token length: 1271
Summarized using GPT-3.5-turbo
Append: [Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded Attributions and Learning to Refuse](https://arxiv.org/abs/2409.11242)
Token length: 1054
Summarized using GPT-3.5-turbo
Append: [Lab-AI: Using Retrieval Augmentation to Enhance Language Models for Personalized Lab Test Interpretation in Clinical Medicine](https://arxiv.org/abs/2409.18986)
Token length: 1539
Summarized using GPT-3.5-turbo
Append: [Can LLMs Really Learn to Translate a Low-Resource Language from One Grammar Book?](https://arxiv.org/abs/2409.19151)
Token length: 1504
Summarized using GPT-3.5-turbo
Append: [TypedThinker: Diversify Large Language Model Reasoning with Typed Thinking](https://arxiv.org/abs/2410.01952)
Append: [Selective Attention Improves Transformer](https://arxiv.org/abs/2410.02703)
Append: [Post-hoc Study of Climate Microtargeting on Social Media Ads with LLMs: Thematic Insights and Fairness Evaluation](https://arxiv.org/abs/2410.05401)
Append: [Parameter-Efficient Fine-Tuning in Large Models: A Survey of Methodologies](https://arxiv.org/abs/2410.19878)
Append: [Estimating the Influence of Sequentially Correlated Literary Properties in Textual Classification: A Data-Centric Hypothesis-Testing Approach](https://arxiv.org/abs/2411.04950)
Append: [jina-clip-v2: Multilingual Multimodal Embeddings for Text and Images](https://arxiv.org/abs/2412.08802)
Append: [Context-Aware Neural Gradient Mapping for Fine-Grained Instruction Processing](https://arxiv.org/abs/2501.14936)
Append: [Multilingual State Space Models for Structured Question Answering in Indic Languages](https://arxiv.org/abs/2502.01673)
Append: [Probabilistic Subspace Manifolds for Contextual Inference in Large Language Models](https://arxiv.org/abs/2502.05346)
Append: [Towards Reasoning Ability of Small Language Models](https://arxiv.org/abs/2502.11569)
Append: [PSCon: Product Search Through Conversations](https://arxiv.org/abs/2502.13881)
Append: [Automatically Evaluating the Paper Reviewing Capability of Large Language Models](https://arxiv.org/abs/2502.17086)
Append: [SemEval-2025 Task 11: Bridging the Gap in Text-Based Emotion Detection](https://arxiv.org/abs/2503.07269)
Append: [HyperDAS: Towards Automating Mechanistic Interpretability with Hypernetworks](https://arxiv.org/abs/2503.10894)
Append: [Shared Global and Local Geometry of Language Model Embeddings](https://arxiv.org/abs/2503.21073)
Append: [Cognitive Memory in Large Language Models](https://arxiv.org/abs/2504.02441)
Append: [Not All Data Are Unlearned Equally](https://arxiv.org/abs/2504.05058)
Append: [Multilingual MFA: Forced Alignment on Low-Resource Related Languages](https://arxiv.org/abs/2504.07315)
Append: [Transferable text data distillation by trajectory matching](https://arxiv.org/abs/2504.09818)
Append: [ReaL: Efficient RLHF Training of Large Language Models with Parameter Reallocation](https://arxiv.org/abs/2406.14088)
Append: [Regressing the Relative Future: Efficient Policy Optimization for Multi-turn RLHF](https://arxiv.org/abs/2410.04612)
Append: [CallNavi, A Challenge and Empirical Study on LLM Function Calling and Routing](https://arxiv.org/abs/2501.05255)
Append: [Are Transformers Able to Reason by Connecting Separated Knowledge in Training Data?](https://arxiv.org/abs/2501.15857)
Append: [Keyframe-oriented Vision Token Pruning: Enhancing Efficiency of Large Vision Language Models on Long-Form Video Processing](https://arxiv.org/abs/2503.10742)
Append: [Looking beyond the next token](https://arxiv.org/abs/2504.11336)
Append: [Teaching Large Language Models to Reason through Learning and Forgetting](https://arxiv.org/abs/2504.11364)
append_entries: 75
Finish: 2025-04-25 04:23:26.404476
------------------------------------------------------
Started: 2025-04-25 06:22:52.147618
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-25 06:22:52.370816
------------------------------------------------------
Started: 2025-04-25 08:21:37.045192
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-25 08:21:37.296065
------------------------------------------------------
Started: 2025-04-25 10:17:06.515090
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-25 10:17:06.699985
------------------------------------------------------
Started: 2025-04-25 12:32:33.596439
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-25 12:32:33.777022
------------------------------------------------------
Started: 2025-04-25 14:15:10.371478
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-25 14:15:10.584149
------------------------------------------------------
Started: 2025-04-25 16:19:37.968500
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-25 16:19:38.156762
------------------------------------------------------
Started: 2025-04-25 18:21:48.634558
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-25 18:21:48.842632
------------------------------------------------------
Started: 2025-04-25 20:17:46.391295
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-25 20:17:46.637887
------------------------------------------------------
Started: 2025-04-25 22:15:39.484283
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-25 22:15:39.724851
------------------------------------------------------
Started: 2025-04-26 01:13:49.773938
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-26 01:13:49.988037
------------------------------------------------------
Started: 2025-04-26 02:57:28.791726
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-26 02:57:29.025375
------------------------------------------------------
Started: 2025-04-26 04:18:23.245536
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-26 04:18:23.302458
------------------------------------------------------
Started: 2025-04-26 06:20:46.878442
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-26 06:20:46.961783
------------------------------------------------------
Started: 2025-04-26 08:18:43.599009
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-26 08:18:43.670760
------------------------------------------------------
Started: 2025-04-26 10:15:17.019775
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-26 10:15:17.091715
------------------------------------------------------
Started: 2025-04-26 12:28:37.289104
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-26 12:28:37.359790
------------------------------------------------------
Started: 2025-04-26 14:13:16.409050
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-26 14:13:16.482023
------------------------------------------------------
Started: 2025-04-26 16:18:26.092728
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-26 16:18:26.166364
------------------------------------------------------
Started: 2025-04-26 18:19:44.152571
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-26 18:19:44.240748
------------------------------------------------------
Started: 2025-04-26 20:15:43.516778
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-26 20:15:43.577322
------------------------------------------------------
Started: 2025-04-26 22:14:03.104258
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-26 22:14:03.192620
------------------------------------------------------
Started: 2025-04-27 01:20:37.743123
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-27 01:20:37.796413
------------------------------------------------------
Started: 2025-04-27 03:06:29.498566
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-27 03:06:29.552011
------------------------------------------------------
Started: 2025-04-27 04:18:17.123855
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-27 04:18:17.182030
------------------------------------------------------
Started: 2025-04-27 06:20:58.862543
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-27 06:20:58.920000
------------------------------------------------------
Started: 2025-04-27 08:18:50.720727
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-27 08:18:50.817021
------------------------------------------------------
Started: 2025-04-27 10:16:12.208862
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-27 10:16:12.277405
------------------------------------------------------
Started: 2025-04-27 12:28:55.734421
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-27 12:28:55.816515
------------------------------------------------------
Started: 2025-04-27 14:13:24.908627
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-27 14:13:24.965676
------------------------------------------------------
Started: 2025-04-27 16:18:11.271664
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-27 16:18:11.329220
------------------------------------------------------
Started: 2025-04-27 18:19:52.750001
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-27 18:19:52.802594
------------------------------------------------------
Started: 2025-04-27 20:16:35.406210
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-27 20:16:35.458989
------------------------------------------------------
Started: 2025-04-27 22:14:45.710258
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-27 22:14:45.793573
------------------------------------------------------
Started: 2025-04-28 01:18:28.296301
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-28 01:18:28.377376
------------------------------------------------------
Started: 2025-04-28 03:08:03.661980
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-28 03:08:03.718214
------------------------------------------------------
Started: 2025-04-28 04:24:01.274055
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1325
Summarized using GPT-3.5-turbo
Append: [Optimism, Expectation, or Sarcasm? Multi-Class Hope Speech Detection in Spanish and English](https://arxiv.org/abs/2504.17974)
Token length: 1227
Summarized using GPT-3.5-turbo
Append: [Improving LLM Personas via Rationalization with Psychological Scaffolds](https://arxiv.org/abs/2504.17993)
Token length: 1318
Summarized using GPT-3.5-turbo
Append: [Memory Reviving, Continuing Learning and Beyond: Evaluation of Pre-trained Encoders and Decoders for Multimodal Machine Translation](https://arxiv.org/abs/2504.18012)
Token length: 943
Summarized using GPT-3.5-turbo
Append: [RAG LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented Generation for Large Language Models](https://arxiv.org/abs/2504.18041)
Token length: 1224
Summarized using GPT-3.5-turbo
Append: [DREAM: Disentangling Risks to Enhance Safety Alignment in Multimodal Large Language Models](https://arxiv.org/abs/2504.18053)
Token length: 1228
Summarized using GPT-3.5-turbo
Append: [Exploring Personality-Aware Interactions in Salesperson Dialogue Agents](https://arxiv.org/abs/2504.18058)
Token length: 1533
Summarized using GPT-3.5-turbo
Append: [PropRAG: Guiding Retrieval with Beam Search over Proposition Paths](https://arxiv.org/abs/2504.18070)
Token length: 1620
Summarized using GPT-3.5-turbo
Append: [Stabilizing Reasoning in Medical LLMs with Continued Pretraining and Reasoning Preference Optimization](https://arxiv.org/abs/2504.18080)
Token length: 1326
Summarized using GPT-3.5-turbo
Append: [Random-Set Large Language Models](https://arxiv.org/abs/2504.18085)
Token length: 1240
Summarized using GPT-3.5-turbo
Append: [Application and Optimization of Large Models Based on Prompt Tuning for Fact-Check-Worthiness Estimation](https://arxiv.org/abs/2504.18104)
Token length: 928
Summarized using GPT-3.5-turbo
Append: [Comparative Study on the Discourse Meaning of Chinese and English Media in the Paris Olympics Based on LDA Topic Modeling Technology and LLM Prompt Engineering](https://arxiv.org/abs/2504.18106)
Token length: 1173
Summarized using GPT-3.5-turbo
Append: [Evaluating Evaluation Metrics -- The Mirage of Hallucination Detection](https://arxiv.org/abs/2504.18114)
Token length: 1085
Summarized using GPT-3.5-turbo
Append: [Temporal Entailment Pretraining for Clinical Language Models over EHR Data](https://arxiv.org/abs/2504.18128)
Token length: 1393
Summarized using GPT-3.5-turbo
Append: [EDU-NER-2025: Named Entity Recognition in Urdu Educational Texts using XLM-RoBERTa with X (formerly Twitter)](https://arxiv.org/abs/2504.18142)
Token length: 1122
Summarized using GPT-3.5-turbo
Append: [Aligning Language Models for Icelandic Legal Text Summarization](https://arxiv.org/abs/2504.18180)
Token length: 718
Summarized using GPT-3.5-turbo
Append: [Optimising ChatGPT for creativity in literary translation: A case study from English into Dutch, Chinese, Catalan and Spanish](https://arxiv.org/abs/2504.18221)
Token length: 1084
Summarized using GPT-3.5-turbo
Append: [Even Small Reasoners Should Quote Their Sources: Introducing the Pleias-RAG Model Family](https://arxiv.org/abs/2504.18225)
Token length: 907
Summarized using GPT-3.5-turbo
Append: [Efficient Single-Pass Training for Multi-Turn Reasoning](https://arxiv.org/abs/2504.18246)
Token length: 1194
Summarized using GPT-3.5-turbo
Append: [MAGI: Multi-Agent Guided Interview for Psychiatric Assessment](https://arxiv.org/abs/2504.18260)
Token length: 1351
Summarized using GPT-3.5-turbo
Append: [TextTIGER: Text-based Intelligent Generation with Entity Prompt Refinement for Text-to-Image Generation](https://arxiv.org/abs/2504.18269)
Token length: 1284
Summarized using GPT-3.5-turbo
Append: [Comparing Uncertainty Measurement and Mitigation Methods for Large Language Models: A Systematic Review](https://arxiv.org/abs/2504.18346)
Token length: 1104
Summarized using GPT-3.5-turbo
Append: [Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in Smart Personal Assistant](https://arxiv.org/abs/2504.18373)
Token length: 1282
Summarized using GPT-3.5-turbo
Append: [Pushing the boundary on Natural Language Inference](https://arxiv.org/abs/2504.18376)
Token length: 829
Summarized using GPT-3.5-turbo
Append: [A UD Treebank for Bohairic Coptic](https://arxiv.org/abs/2504.18386)
Token length: 1590
Summarized using GPT-3.5-turbo
Append: [HRScene: How Far Are VLMs from Effective High-Resolution Image Understanding?](https://arxiv.org/abs/2504.18406)
Token length: 1426
Summarized using GPT-3.5-turbo
Append: [Expressing stigma and inappropriate responses prevents LLMs from safely replacing mental health providers](https://arxiv.org/abs/2504.18412)
Token length: 900
Summarized using GPT-3.5-turbo
Append: [BitNet v2: Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs](https://arxiv.org/abs/2504.18415)
Token length: 1230
Summarized using GPT-3.5-turbo
Append: [PolyMath: Evaluating Mathematical Reasoning in Multilingual Contexts](https://arxiv.org/abs/2504.18428)
Token length: 1075
Summarized using GPT-3.5-turbo
Append: [Fast-Slow Thinking for Large Vision-Language Model Reasoning](https://arxiv.org/abs/2504.18458)
Token length: 1022
Summarized using GPT-3.5-turbo
Append: [Generative Induction of Dialogue Task Schemas with Streaming Refinement and Simulated Interactions](https://arxiv.org/abs/2504.18474)
Token length: 1182
Summarized using GPT-3.5-turbo
Append: [Investigating Co-Constructive Behavior of Large Language Models in Explanation Dialogues](https://arxiv.org/abs/2504.18483)
Token length: 1372
Summarized using GPT-3.5-turbo
Append: [TRACE Back from the Future: A Probabilistic Reasoning Approach to Controllable Language Generation](https://arxiv.org/abs/2504.18535)
Token length: 1516
Summarized using GPT-3.5-turbo
Append: [VideoVista-CulturalLingo: 360$^\circ$ Horizons-Bridging Cultures, Languages, and Domains in Video Comprehension](https://arxiv.org/abs/2504.17821)
Token length: 1913
Summarized using GPT-3.5-turbo
Append: [Unsupervised Corpus Poisoning Attacks in Continuous Space for Dense Retrieval](https://arxiv.org/abs/2504.17884)
Token length: 1123
Summarized using GPT-3.5-turbo
Append: [Token Sequence Compression for Efficient Multimodal Computing](https://arxiv.org/abs/2504.17892)
Token length: 1526
Summarized using GPT-3.5-turbo
Append: [CAMU: Context Augmentation for Meme Understanding](https://arxiv.org/abs/2504.17902)
Token length: 1006
Summarized using GPT-3.5-turbo
Append: [Toward a Human-Centered Evaluation Framework for Trustworthy LLM-Powered GUI Agents](https://arxiv.org/abs/2504.17934)
Token length: 1098
Summarized using GPT-3.5-turbo
Append: [Collaborating Action by Action: A Multi-agent LLM Framework for Embodied Reasoning](https://arxiv.org/abs/2504.17950)
Token length: 910
Summarized using GPT-3.5-turbo
Append: [SMARTFinRAG: Interactive Modularized Financial RAG Benchmark](https://arxiv.org/abs/2504.18024)
Token length: 1464
Summarized using GPT-3.5-turbo
Append: [Tracking Articulatory Dynamics in Speech with a Fixed-Weight BiLSTM-CNN Architecture](https://arxiv.org/abs/2504.18099)
Token length: 775
Summarized using GPT-3.5-turbo
Append: [Adversarial Attacks on LLM-as-a-Judge Systems: Insights from Prompt Injections](https://arxiv.org/abs/2504.18333)
Token length: 1332
Summarized using GPT-3.5-turbo
Append: [Kimi-Audio Technical Report](https://arxiv.org/abs/2504.18425)
Token length: 1498
Summarized using GPT-3.5-turbo
Append: [PRobELM: Plausibility Ranking Evaluation for Language Models](https://arxiv.org/abs/2404.03818)
Token length: 1459
Summarized using GPT-3.5-turbo
Append: [Nearest Neighbor Speculative Decoding for LLM Generation and Attribution](https://arxiv.org/abs/2405.19325)
Token length: 702
Summarized using GPT-3.5-turbo
Append: [AMR-RE: Abstract Meaning Representations for Retrieval-Based In-Context Learning in Relation Extraction](https://arxiv.org/abs/2406.10432)
Token length: 907
Summarized using GPT-3.5-turbo
Append: [Multilingual Large Language Models and Curse of Multilinguality](https://arxiv.org/abs/2406.10602)
Token length: 1589
Summarized using GPT-3.5-turbo
Append: [Using Large Language Models to Create AI Personas for Replication, Generalization and Prediction of Media Effects: An Empirical Test of 133 Published Experimental Research Findings](https://arxiv.org/abs/2408.16073)
Token length: 1164
Summarized using GPT-3.5-turbo
Append: [Your Weak LLM is Secretly a Strong Teacher for Alignment](https://arxiv.org/abs/2409.08813)
Token length: 1107
Summarized using GPT-3.5-turbo
Append: [MeTHanol: Modularized Thinking Language Models with Intermediate Layer Thinking, Decoding and Bootstrapping Reasoning](https://arxiv.org/abs/2409.12059)
Token length: 1361
Summarized using GPT-3.5-turbo
Append: [FaithEval: Can Your Language Model Stay Faithful to Context, Even If "The Moon is Made of Marshmallows"](https://arxiv.org/abs/2410.03727)
Append: [Knowledge-Augmented Multimodal Clinical Rationale Generation for Disease Diagnosis with Small Language Models](https://arxiv.org/abs/2411.07611)
Append: [ElChat: Adapting Chat Language Models Using Only Target Unlabeled Language Data](https://arxiv.org/abs/2412.11704)
Append: [Factual Knowledge in Language Models: Robustness and Anomalies under Simple Temporal Context Variations](https://arxiv.org/abs/2502.01220)
Append: [EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via Reinforcement Learning](https://arxiv.org/abs/2502.12486)
Append: [Machine-generated text detection prevents language model collapse](https://arxiv.org/abs/2502.15654)
Append: [LRAGE: Legal Retrieval Augmented Generation Evaluation Tool](https://arxiv.org/abs/2504.01840)
Append: [Generative Evaluation of Complex Reasoning in Large Language Models](https://arxiv.org/abs/2504.02810)
Append: [Can Reasoning LLMs Enhance Clinical Document Classification?](https://arxiv.org/abs/2504.08040)
Append: [Multiple-Instance, Cascaded Classification for Keyword Spotting in Narrow-Band Audio](https://arxiv.org/abs/1711.08058)
Append: [M4U: Evaluating Multilingual Understanding and Reasoning for Large Multimodal Models](https://arxiv.org/abs/2405.15638)
Append: [Combining X-Vectors and Bayesian Batch Active Learning: Two-Stage Active Learning Pipeline for Speech Recognition](https://arxiv.org/abs/2406.02566)
Append: [GOFA: A Generative One-For-All Model for Joint Graph Language Modeling](https://arxiv.org/abs/2407.09709)
Append: [Towards Robust and Parameter-Efficient Knowledge Unlearning for LLMs](https://arxiv.org/abs/2408.06621)
Append: [Adaptive Uncertainty Quantification for Generative AI](https://arxiv.org/abs/2408.08990)
Append: [MIND: Math Informed syNthetic Dialogues for Pretraining LLMs](https://arxiv.org/abs/2410.12881)
Append: [Leveraging Label Semantics and Meta-Label Refinement for Multi-Label Question Classification](https://arxiv.org/abs/2411.01841)
Append: [Repurposing the scientific literature with vision-language models](https://arxiv.org/abs/2502.19546)
Append: [Spatial Audio Processing with Large Language Model on Wearable Devices](https://arxiv.org/abs/2504.08907)
append_entries: 68
Finish: 2025-04-28 04:25:53.968059
------------------------------------------------------
Started: 2025-04-28 06:30:42.394218
Existing_entries: 819
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-28 06:30:42.588139
------------------------------------------------------
Started: 2025-04-28 08:57:44.055561
Existing_entries: 819
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-28 08:57:44.246486
------------------------------------------------------
Started: 2025-04-28 11:03:55.719388
Existing_entries: 819
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-28 11:03:55.907081
------------------------------------------------------
Started: 2025-04-28 12:33:56.928650
Existing_entries: 819
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-28 12:33:57.121104
------------------------------------------------------
Started: 2025-04-28 14:17:38.964210
Existing_entries: 819
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-28 14:17:39.207577
------------------------------------------------------
Started: 2025-04-28 16:19:30.632004
Existing_entries: 819
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-28 16:19:30.821051
------------------------------------------------------
Started: 2025-04-28 18:23:05.315985
Existing_entries: 819
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-28 18:23:05.536692
------------------------------------------------------
Started: 2025-04-28 20:17:28.413307
Existing_entries: 819
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-28 20:17:28.603960
------------------------------------------------------
Started: 2025-04-28 22:15:51.625514
Existing_entries: 819
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-28 22:15:51.814415
------------------------------------------------------
Started: 2025-04-29 01:16:35.727114
Existing_entries: 819
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-29 01:16:35.922910
------------------------------------------------------
Started: 2025-04-29 03:04:03.381857
Existing_entries: 819
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-29 03:04:03.574013
------------------------------------------------------
Started: 2025-04-29 04:22:36.669946
Existing_entries: 819
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 751
Summarized using GPT-3.5-turbo
Append: [Mind the Language Gap: Automated and Augmented Evaluation of Bias in LLMs for High- and Low-Resource Languages](https://arxiv.org/abs/2504.18560)
Token length: 1046
Summarized using GPT-3.5-turbo
Append: [Span-Level Hallucination Detection for LLM-Generated Answers](https://arxiv.org/abs/2504.18639)
Token length: 1348
Summarized using GPT-3.5-turbo
Append: [Can Third-parties Read Our Emotions?](https://arxiv.org/abs/2504.18673)
Token length: 1329
Summarized using GPT-3.5-turbo
Append: [Spatial Speech Translation: Translating Across Space With Binaural Hearables](https://arxiv.org/abs/2504.18715)
Token length: 1116
Summarized using GPT-3.5-turbo
Append: [Building UD Cairo for Old English in the Classroom](https://arxiv.org/abs/2504.18718)
Token length: 1098
Summarized using GPT-3.5-turbo
Append: [EvidenceBench: A Benchmark for Extracting Evidence from Biomedical Papers](https://arxiv.org/abs/2504.18736)
Token length: 922
Summarized using GPT-3.5-turbo
Append: [SynLexLM: Scaling Legal LLMs with Synthetic Data and Curriculum Learning](https://arxiv.org/abs/2504.18762)
Token length: 1292
Summarized using GPT-3.5-turbo
Append: [Stealing Creator's Workflow: A Creator-Inspired Agentic Framework with Iterative Feedback Loop for Improved Scientific Short-form Generation](https://arxiv.org/abs/2504.18805)
Token length: 1230
Summarized using GPT-3.5-turbo
Append: [Toward Generalizable Evaluation in the LLM Era: A Survey Beyond Benchmarks](https://arxiv.org/abs/2504.18838)
Token length: 1614
Summarized using GPT-3.5-turbo
Append: [Towards Robust Dialogue Breakdown Detection: Addressing Disruptors in Large Language Models with Self-Guided Reasoning](https://arxiv.org/abs/2504.18839)
Token length: 1104
Summarized using GPT-3.5-turbo
Append: [When2Call: When (not) to Call Tools](https://arxiv.org/abs/2504.18851)
Token length: 1615
Summarized using GPT-3.5-turbo
Append: [Effective Length Extrapolation via Dimension-Wise Positional Embeddings Manipulation](https://arxiv.org/abs/2504.18857)
Token length: 1706
Summarized using GPT-3.5-turbo
Append: [Latent Adversarial Training Improves the Representation of Refusal](https://arxiv.org/abs/2504.18872)
Token length: 701
Summarized using GPT-3.5-turbo
Append: [A Simple Ensemble Strategy for LLM Inference: Towards More Stable Text Classification](https://arxiv.org/abs/2504.18884)
Token length: 1348
Summarized using GPT-3.5-turbo
Append: [MTCSC: Retrieval-Augmented Iterative Refinement for Chinese Spelling Correction](https://arxiv.org/abs/2504.18938)
Token length: 1904
Summarized using GPT-3.5-turbo
Append: [LawFlow : Collecting and Simulating Lawyers' Thought Processes](https://arxiv.org/abs/2504.18942)
Token length: 1482
Summarized using GPT-3.5-turbo
Append: [Dynamic Fisher-weighted Model Merging via Bayesian Optimization](https://arxiv.org/abs/2504.18992)
Token length: 1546
Summarized using GPT-3.5-turbo
Append: [Graph of Attacks: Improved Black-Box and Interpretable Jailbreaks for LLMs](https://arxiv.org/abs/2504.19019)
Token length: 1167
Summarized using GPT-3.5-turbo
Append: [Advancing Scientific Text Classification: Fine-Tuned Models with Dataset Expansion and Hard-Voting](https://arxiv.org/abs/2504.19021)
Token length: 817
Summarized using GPT-3.5-turbo
Append: [KETCHUP: K-Step Return Estimation for Sequential Knowledge Distillation](https://arxiv.org/abs/2504.19024)
Token length: 1754
Summarized using GPT-3.5-turbo
Append: [Calibrating Translation Decoding with Quality Estimation on LLMs](https://arxiv.org/abs/2504.19044)
Token length: 1257
Summarized using GPT-3.5-turbo
Append: [Hallucinations and Key Information Extraction in Medical Texts: A Comprehensive Assessment of Open-Source Large Language Models](https://arxiv.org/abs/2504.19061)
Token length: 1732
Summarized using GPT-3.5-turbo
Append: [ClimaEmpact: Domain-Aligned Small Language Models and Datasets for Extreme Weather Analytics](https://arxiv.org/abs/2504.19066)
Token length: 961
Summarized using GPT-3.5-turbo
Append: [Sample-Efficient Language Model for Hinglish Conversational AI](https://arxiv.org/abs/2504.19070)
Token length: 1310
Summarized using GPT-3.5-turbo
Append: [Efficient Reasoning for LLMs through Speculative Chain-of-Thought](https://arxiv.org/abs/2504.19095)
Token length: 1726
Summarized using GPT-3.5-turbo
Append: [Privacy-Preserving Federated Embedding Learning for Localized Retrieval-Augmented Generation](https://arxiv.org/abs/2504.19101)
Token length: 1418
Summarized using GPT-3.5-turbo
Append: [APE-Bench I: Towards File-level Automated Proof Engineering of Formal Math Libraries](https://arxiv.org/abs/2504.19110)
Token length: 1620
Summarized using GPT-3.5-turbo
Append: [SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning](https://arxiv.org/abs/2504.19162)
Token length: 1266
Summarized using GPT-3.5-turbo
Append: [WuNeng: Hybrid State with Attention](https://arxiv.org/abs/2504.19191)
Token length: 777
Summarized using GPT-3.5-turbo
Append: [Dynamic Embedded Topic Models: properties and recommendations based on diverse corpora](https://arxiv.org/abs/2504.19209)
Token length: 1456
Summarized using GPT-3.5-turbo
Append: [Uncertainty Quantification for Language Models: A Suite of Black-Box, White-Box, LLM Judge, and Ensemble Scorers](https://arxiv.org/abs/2504.19254)
Token length: 991
Summarized using GPT-3.5-turbo
Append: [VIST-GPT: Ushering in the Era of Visual Storytelling with LLMs?](https://arxiv.org/abs/2504.19267)
Token length: 1095
Summarized using GPT-3.5-turbo
Append: [AndroidGen: Building an Android Language Agent under Data Scarcity](https://arxiv.org/abs/2504.19298)
Token length: 1677
Summarized using GPT-3.5-turbo
Append: [BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese](https://arxiv.org/abs/2504.19314)
Token length: 1426
Summarized using GPT-3.5-turbo
Append: [Unified Multi-Task Learning & Model Fusion for Efficient Language Model Guardrailing](https://arxiv.org/abs/2504.19333)
Token length: 947
Summarized using GPT-3.5-turbo
Append: [Explanatory Summarization with Discourse-Driven Planning](https://arxiv.org/abs/2504.19339)
Token length: 1391
Summarized using GPT-3.5-turbo
Append: [ICL CIPHERS: Quantifying "Learning'' in In-Context Learning via Substitution Ciphers](https://arxiv.org/abs/2504.19395)
Token length: 1606
Summarized using GPT-3.5-turbo
Append: [Context Selection and Rewriting for Video-based EducationalQuestion Generation](https://arxiv.org/abs/2504.19406)
Token length: 1969
Summarized using GPT-3.5-turbo
Append: [Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory](https://arxiv.org/abs/2504.19413)
Token length: 1321
Summarized using GPT-3.5-turbo
Append: [Context-Guided Dynamic Retrieval for Improving Generation Quality in RAG Models](https://arxiv.org/abs/2504.19436)
Token length: 1041
Summarized using GPT-3.5-turbo
Append: [Systematic Bias in Large Language Models: Discrepant Response Patterns in Binary vs. Continuous Judgment Tasks](https://arxiv.org/abs/2504.19445)
Token length: 1025
Summarized using GPT-3.5-turbo
Append: [Towards Long Context Hallucination Detection](https://arxiv.org/abs/2504.19457)
Token length: 1470
Summarized using GPT-3.5-turbo
Append: [BRIDGE: Benchmarking Large Language Models for Understanding Real-world Clinical Practice Text](https://arxiv.org/abs/2504.19467)
Token length: 1343
Summarized using GPT-3.5-turbo
Append: [Conflicts in Texts: Data, Implications and Challenges](https://arxiv.org/abs/2504.19472)
Token length: 935
Summarized using GPT-3.5-turbo
Append: [Detecting Effects of AI-Mediated Communication on Language Complexity and Sentiment](https://arxiv.org/abs/2504.19556)
Token length: 1562
Summarized using GPT-3.5-turbo
Append: [m-KAILIN: Knowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training](https://arxiv.org/abs/2504.19565)
Token length: 662
Summarized using GPT-3.5-turbo
Append: [Arabic Metaphor Sentiment Classification Using Semantic Information](https://arxiv.org/abs/2504.19590)
Token length: 935
Summarized using GPT-3.5-turbo
Append: [Coreference Resolution for Vietnamese Narrative Texts](https://arxiv.org/abs/2504.19606)
Token length: 1241
Summarized using GPT-3.5-turbo
Append: [VCM: Vision Concept Modeling Based on Implicit Contrastive Learning with Vision-Language Instruction Fine-Tuning](https://arxiv.org/abs/2504.19627)
Token length: 1523
Summarized using GPT-3.5-turbo
Append: [A Comprehensive Part-of-Speech Tagging to Standardize Central-Kurdish Language: A Research Guide for Kurdish Natural Language Processing Tasks](https://arxiv.org/abs/2504.19645)
Append: [Multimodal Conditioned Diffusive Time Series Forecasting](https://arxiv.org/abs/2504.19669)
Append: [Annif at SemEval-2025 Task 5: Traditional XMTC augmented by LLMs](https://arxiv.org/abs/2504.19675)
Append: [Taming the Titans: A Survey of Efficient LLM Inference Serving](https://arxiv.org/abs/2504.19720)
Append: [LLM-Assisted Automated Deductive Coding of Dialogue Data: Leveraging Dialogue-Specific Characteristics to Enhance Contextual Understanding](https://arxiv.org/abs/2504.19734)
Append: [Moral Reasoning Across Languages: The Critical Role of Low-Resource Languages in LLMs](https://arxiv.org/abs/2504.19759)
Append: [Can a Crow Hatch a Falcon? Lineage Matters in Predicting Large Language Model Performance](https://arxiv.org/abs/2504.19811)
Append: [To MT or not to MT: An eye-tracking study on the reception by Dutch readers of different translation and creativity levels](https://arxiv.org/abs/2504.19850)
Append: [Efficient Domain-adaptive Continual Pretraining for the Process Industry in the German Language](https://arxiv.org/abs/2504.19856)
Append: [semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated Computation and Unified Storage](https://arxiv.org/abs/2504.19867)
Append: [GenCLS++: Pushing the Boundaries of Generative Classification in LLMs Through Comprehensive SFT and RL Studies Across Diverse Datasets](https://arxiv.org/abs/2504.19898)
Append: [Assessing the Potential of Generative Agents in Crowdsourced Fact-Checking](https://arxiv.org/abs/2504.19940)
Append: [TD-EVAL: Revisiting Task-Oriented Dialogue Evaluation by Combining Turn-Level Precision with Dialogue-Level Comparisons](https://arxiv.org/abs/2504.19982)
Append: [Knowledge Distillation of Domain-adapted LLMs for Question-Answering in Telecom](https://arxiv.org/abs/2504.20000)
Append: [LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case Study on Neural News Recommendation](https://arxiv.org/abs/2504.20013)
Append: [Better To Ask in English? Evaluating Factual Accuracy of Multilingual LLMs in English and Low-Resource Languages](https://arxiv.org/abs/2504.20022)
Append: [AutoJudge: Judge Decoding Without Manual Annotation](https://arxiv.org/abs/2504.20039)
Append: [Optimizing the Privacy-Utility Balance using Synthetic Data and Configurable Perturbation Pipelines](https://arxiv.org/abs/2504.18596)
Append: [Generative Product Recommendations for Implicit Superlative Queries](https://arxiv.org/abs/2504.18748)
Append: [Clinical knowledge in LLMs does not translate to human interactions](https://arxiv.org/abs/2504.18919)
Append: [LINC: Supporting Language Independent Communication and Comprehension to Enhance Contribution in Multilingual Collaborative Meetings](https://arxiv.org/abs/2504.18988)
Append: [Generative AI for Character Animation: A Comprehensive Survey of Techniques, Applications, and Future Directions](https://arxiv.org/abs/2504.19056)
Append: [Versatile Framework for Song Generation with Prompt-based Control](https://arxiv.org/abs/2504.19062)
Append: [Hierarchical Attention Generates Better Proofs](https://arxiv.org/abs/2504.19188)
Append: [Anyprefer: An Agentic Framework for Preference Data Synthesis](https://arxiv.org/abs/2504.19276)
Append: [Large Language Models are Qualified Benchmark Builders: Rebuilding Pre-Training Datasets for Advancing Code Intelligence Tasks](https://arxiv.org/abs/2504.19444)
Append: [Mitigating Modality Bias in Multi-modal Entity Alignment from a Causal Perspective](https://arxiv.org/abs/2504.19458)
Append: [Improving Reasoning Performance in Large Language Models via Representation Engineering](https://arxiv.org/abs/2504.19483)
Append: [Masked Point-Entity Contrast for Open-Vocabulary 3D Scene Understanding](https://arxiv.org/abs/2504.19500)
Append: [FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation](https://arxiv.org/abs/2504.19519)
Append: [Graph-Based Spectral Decomposition for Parameter Coordination in Language Model Fine-Tuning](https://arxiv.org/abs/2504.19583)
Append: [Evaluate-and-Purify: Fortifying Code Language Models Against Adversarial Attacks Using LLM-as-a-Judge](https://arxiv.org/abs/2504.19730)
Append: [Reconstructing Context: Evaluating Advanced Chunking Strategies for Retrieval-Augmented Generation](https://arxiv.org/abs/2504.19754)
Append: [A Bayesian approach to modeling topic-metadata relationships](https://arxiv.org/abs/2104.02496)
Append: [Understanding Dataset Difficulty with $\mathcal{V}$-Usable Information](https://arxiv.org/abs/2110.08420)
Append: [Cognitive and Cultural Topology of Linguistic Categories:A Semantic-Pragmatic Metric Approach](https://arxiv.org/abs/2112.06876)
Append: [Generative Meta-Learning for Zero-Shot Relation Triplet Extraction](https://arxiv.org/abs/2305.01920)
Append: [Benchmarking large language models for biomedical natural language processing applications and recommendations](https://arxiv.org/abs/2305.16326)
Append: [Ragas: Automated Evaluation of Retrieval Augmented Generation](https://arxiv.org/abs/2309.15217)
Append: [Large language models for newspaper sentiment analysis during COVID-19: The Guardian](https://arxiv.org/abs/2405.13056)
Append: [Fake News Detection: It's All in the Data!](https://arxiv.org/abs/2407.02122)
Append: [Pula: Training Large Language Models for Setswana](https://arxiv.org/abs/2408.02239)
Append: [AdTEC: A Unified Benchmark for Evaluating Text Quality in Search Engine Advertising](https://arxiv.org/abs/2408.05906)
Append: [W-RAG: Weakly Supervised Dense Retrieval in RAG for Open-domain Question Answering](https://arxiv.org/abs/2408.08444)
Append: [Data Processing for the OpenGPT-X Model Family](https://arxiv.org/abs/2410.08800)
Append: [Speculative Knowledge Distillation: Bridging the Teacher-Student Gap Through Interleaved Sampling](https://arxiv.org/abs/2410.11325)
Append: [Open Domain Question Answering with Conflicting Contexts](https://arxiv.org/abs/2410.12311)
Append: [From Single to Multi: How LLMs Hallucinate in Multi-Document Summarization](https://arxiv.org/abs/2410.13961)
Append: [WikiNER-fr-gold: A Gold-Standard NER Corpus](https://arxiv.org/abs/2411.00030)
Append: [An Attempt to Develop a Neural Parser based on Simplified Head-Driven Phrase Structure Grammar on Vietnamese](https://arxiv.org/abs/2411.17270)
Append: [Investigating Length Issues in Document-level Machine Translation](https://arxiv.org/abs/2412.17592)
Append: [LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context](https://arxiv.org/abs/2412.17596)
Append: [Disambiguating Numeral Sequences to Decipher Ancient Accounting Corpora](https://arxiv.org/abs/2502.00090)
Append: [InfoQuest: Evaluating Multi-Turn Dialogue Agents for Open-Ended Conversations with Hidden Context](https://arxiv.org/abs/2502.12257)
Append: [Oreo: A Plug-in Context Reconstructor to Enhance Retrieval-Augmented Generation](https://arxiv.org/abs/2502.13019)
Append: [LIFT: Improving Long Context Understanding of Large Language Models through Long Input Fine-Tuning](https://arxiv.org/abs/2502.14644)
Append: [Latent Factor Models Meets Instructions: Goal-conditioned Latent Factor Discovery without Task Supervision](https://arxiv.org/abs/2502.15147)
Append: [Protecting multimodal large language models against misleading visualizations](https://arxiv.org/abs/2502.20503)
Append: [Compositional Subspace Representation Fine-tuning for Adaptive Large Language Models](https://arxiv.org/abs/2503.10617)
Append: [Evaluating the Quality of Benchmark Datasets for Low-Resource Languages: A Case Study on Turkish](https://arxiv.org/abs/2504.09714)
Append: [The Semantic Scholar Open Data Platform](https://arxiv.org/abs/2301.10140)
Append: [NoisyHate: Mining Online Human-Written Perturbations for Realistic Robustness Benchmarking of Content Moderation Models](https://arxiv.org/abs/2303.10430)
Append: [Early Prediction of Alzheimers Disease Leveraging Symptom Occurrences from Longitudinal Electronic Health Records of US Military Veterans](https://arxiv.org/abs/2307.12369)
Append: [Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation](https://arxiv.org/abs/2403.19103)
Append: [Unsolvable Problem Detection: Robust Understanding Evaluation for Large Multimodal Models](https://arxiv.org/abs/2403.20331)
Append: [GraphEx: A Graph-based Extraction Method for Advertiser Keyphrase Recommendation](https://arxiv.org/abs/2409.03140)
Append: [AI-LieDar: Examine the Trade-off Between Utility and Truthfulness in LLM Agents](https://arxiv.org/abs/2409.09013)
Append: [Unintentional Unalignment: Likelihood Displacement in Direct Preference Optimization](https://arxiv.org/abs/2410.08847)
Append: [CREAM: Consistency Regularized Self-Rewarding Language Models](https://arxiv.org/abs/2410.12735)
Append: [Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models](https://arxiv.org/abs/2410.18252)
Append: [A Guide to Misinformation Detection Data and Evaluation](https://arxiv.org/abs/2411.05060)
Append: [An Explainable Biomedical Foundation Model via Large-Scale Concept-Enhanced Vision-Language Pre-training](https://arxiv.org/abs/2501.15579)
Append: [Optimizing GPT for Video Understanding: Zero-Shot Performance and Prompt Engineering](https://arxiv.org/abs/2502.09573)
Append: [Exploring LLM-based Student Simulation for Metacognitive Cultivation](https://arxiv.org/abs/2502.11678)
Append: [NutriGen: Personalized Meal Plan Generator Leveraging Large Language Models to Enhance Dietary and Nutritional Adherence](https://arxiv.org/abs/2502.20601)
Append: [Siege: Autonomous Multi-Turn Jailbreaking of Large Language Models with Tree Search](https://arxiv.org/abs/2503.10619)
Append: [Distillation and Refinement of Reasoning in Small Language Models for Document Re-ranking](https://arxiv.org/abs/2504.03947)
Append: [Synthetic Data Generation & Multi-Step RL for Reasoning & Tool Use](https://arxiv.org/abs/2504.04736)
Append: [OmniCaptioner: One Captioner to Rule Them All](https://arxiv.org/abs/2504.07089)
Append: [Unveiling the Hidden: Movie Genre and User Bias in Spoiler Detection](https://arxiv.org/abs/2504.17834)
append_entries: 129
Finish: 2025-04-29 04:24:34.291533
------------------------------------------------------
Started: 2025-04-29 06:24:30.171459
Existing_entries: 948
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-29 06:24:30.486505
------------------------------------------------------
Started: 2025-04-29 08:22:47.034261
Existing_entries: 948
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-29 08:22:47.357448
------------------------------------------------------
Started: 2025-04-29 10:18:27.720219
Existing_entries: 948
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-29 10:18:28.043674
------------------------------------------------------
Started: 2025-04-29 12:34:40.885194
Existing_entries: 948
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-29 12:34:41.210261
------------------------------------------------------
Started: 2025-04-29 14:16:39.217356
Existing_entries: 948
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-29 14:16:39.544472
------------------------------------------------------
Started: 2025-04-29 16:20:02.492784
Existing_entries: 948
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-29 16:20:02.808175
------------------------------------------------------
Started: 2025-04-29 18:22:13.455317
Existing_entries: 948
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-29 18:22:13.839323
------------------------------------------------------
Started: 2025-04-29 20:18:07.124266
Existing_entries: 948
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-29 20:18:07.444309
------------------------------------------------------
Started: 2025-04-29 22:15:17.365462
Existing_entries: 948
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-29 22:15:17.718552
------------------------------------------------------
Started: 2025-04-30 01:16:44.203366
Existing_entries: 948
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-30 01:16:44.552175
------------------------------------------------------
Started: 2025-04-30 03:03:54.454656
Existing_entries: 948
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-30 03:03:54.786597
------------------------------------------------------
Started: 2025-04-30 04:23:40.425225
Existing_entries: 948
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1831
Summarized using GPT-3.5-turbo
Append: [It's the same but not the same: Do LLMs distinguish Spanish varieties?](https://arxiv.org/abs/2504.20049)
Token length: 1306
Summarized using GPT-3.5-turbo
Append: [Evaluating Large Language Models on Multiword Expressions in Multilingual and Code-Switched Contexts](https://arxiv.org/abs/2504.20051)
Token length: 1333
Summarized using GPT-3.5-turbo
Append: [Understanding and Mitigating Risks of Generative AI in Financial Services](https://arxiv.org/abs/2504.20086)
Token length: 1494
Summarized using GPT-3.5-turbo
Append: [Toward Evaluative Thinking: Meta Policy Optimization with Evolving Reward Models](https://arxiv.org/abs/2504.20157)
Token length: 1343
Summarized using GPT-3.5-turbo
Append: [MICE for CATs: Model-Internal Confidence Estimation for Calibrating Agents with Tools](https://arxiv.org/abs/2504.20168)
Token length: 1070
Summarized using GPT-3.5-turbo
Append: [A Multimodal Pipeline for Clinical Data Extraction: Applying Vision-Language Models to Scans of Transfusion Reaction Reports](https://arxiv.org/abs/2504.20220)
Token length: 1097
Summarized using GPT-3.5-turbo
Append: [A Platform for Generating Educational Activities to Teach English as a Second Language](https://arxiv.org/abs/2504.20251)
Token length: 419
Summarized using GPT-3.5-turbo
Append: [Enhancing Systematic Reviews with Large Language Models: Using GPT-4 and Kimi](https://arxiv.org/abs/2504.20276)
Token length: 653
Summarized using GPT-3.5-turbo
Append: [UD-English-CHILDES: A Collected Resource of Gold and Silver Universal Dependencies Trees for Child Language Interactions](https://arxiv.org/abs/2504.20304)
Token length: 1040
Summarized using GPT-3.5-turbo
Append: [Labeling Case Similarity based on Co-Citation of Legal Articles in Judgment Documents with Empirical Dispute-Based Evaluation](https://arxiv.org/abs/2504.20323)
Token length: 1121
Summarized using GPT-3.5-turbo
Append: [Local Prompt Optimization](https://arxiv.org/abs/2504.20355)
Token length: 1032
Summarized using GPT-3.5-turbo
Append: [What Causes Knowledge Loss in Multilingual Language Models?](https://arxiv.org/abs/2504.20356)
Token length: 1108
Summarized using GPT-3.5-turbo
Append: [DMDTEval: An Evaluation and Analysis of LLMs on Disambiguation in Multi-domain Translation](https://arxiv.org/abs/2504.20371)
Token length: 1206
Summarized using GPT-3.5-turbo
Append: [On Psychology of AI -- Does Primacy Effect Affect ChatGPT and Other LLMs?](https://arxiv.org/abs/2504.20444)
Token length: 801
Summarized using GPT-3.5-turbo
Append: [Team ACK at SemEval-2025 Task 2: Beyond Word-for-Word Machine Translation for English-Korean Pairs](https://arxiv.org/abs/2504.20451)
Token length: 967
Summarized using GPT-3.5-turbo
Append: [Fane at SemEval-2025 Task 10: Zero-Shot Entity Framing with Large Language Models](https://arxiv.org/abs/2504.20469)
Token length: 1352
Summarized using GPT-3.5-turbo
Append: [Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training](https://arxiv.org/abs/2504.20484)
Token length: 1411
Summarized using GPT-3.5-turbo
Append: [UniDetox: Universal Detoxification of Large Language Models via Dataset Distillation](https://arxiv.org/abs/2504.20500)
Token length: 972
Summarized using GPT-3.5-turbo
Append: [Revisiting the MIMIC-IV Benchmark: Experiments Using Language Models for Electronic Health Records](https://arxiv.org/abs/2504.20547)
Token length: 827
Summarized using GPT-3.5-turbo
Append: [BrAIcht, a theatrical agent that speaks like Bertolt Brecht's characters](https://arxiv.org/abs/2504.20552)
Token length: 493
Summarized using GPT-3.5-turbo
Append: [ClonEval: An Open Voice Cloning Benchmark](https://arxiv.org/abs/2504.20581)
Token length: 1440
Summarized using GPT-3.5-turbo
Append: [TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open Language Models](https://arxiv.org/abs/2504.20605)
Token length: 1047
Summarized using GPT-3.5-turbo
Append: [WenyanGPT: A Large Language Model for Classical Chinese Tasks](https://arxiv.org/abs/2504.20609)
Token length: 1023
Summarized using GPT-3.5-turbo
Append: [Cooking Up Creativity: A Cognitively-Inspired Approach for Enhancing LLM Creativity through Structured Representations](https://arxiv.org/abs/2504.20643)
Token length: 1105
Summarized using GPT-3.5-turbo
Append: [A Generative-AI-Driven Claim Retrieval System Capable of Detecting and Retrieving Claims from Social Media Platforms in Multiple Languages](https://arxiv.org/abs/2504.20668)
Token length: 1150
Summarized using GPT-3.5-turbo
Append: [Non-native Children's Automatic Speech Assessment Challenge (NOCASA)](https://arxiv.org/abs/2504.20678)
Token length: 1626
Summarized using GPT-3.5-turbo
Append: [Are Information Retrieval Approaches Good at Harmonising Longitudinal Survey Questions in Social Science?](https://arxiv.org/abs/2504.20679)
Token length: 855
Summarized using GPT-3.5-turbo
Append: [Can LLMs Detect Intrinsic Hallucinations in Paraphrasing and Machine Translation?](https://arxiv.org/abs/2504.20699)
Token length: 1314
Summarized using GPT-3.5-turbo
Append: [BrightCookies at SemEval-2025 Task 9: Exploring Data Augmentation for Food Hazard Classification](https://arxiv.org/abs/2504.20703)
Token length: 1691
Summarized using GPT-3.5-turbo
Append: [Beyond the Last Answer: Your Reasoning Trace Uncovers More than You Think](https://arxiv.org/abs/2504.20708)
Token length: 1479
Summarized using GPT-3.5-turbo
Append: [UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with Diverse Modalities and Granularities](https://arxiv.org/abs/2504.20734)
Token length: 1563
Summarized using GPT-3.5-turbo
Append: [Grokking in the Wild: Data Augmentation for Real-World Multi-Hop Reasoning with Transformers](https://arxiv.org/abs/2504.20752)
Token length: 1062
Summarized using GPT-3.5-turbo
Append: [Chain-of-Defensive-Thought: Structured Reasoning Elicits Robustness in Large Language Models against Reference Corruption](https://arxiv.org/abs/2504.20769)
Token length: 1614
Summarized using GPT-3.5-turbo
Append: [Turing Machine Evaluation for Large Language Model](https://arxiv.org/abs/2504.20771)
Token length: 1170
Summarized using GPT-3.5-turbo
Append: [Universal language model with the intervention of quantum theory](https://arxiv.org/abs/2504.20839)
Token length: 923
Summarized using GPT-3.5-turbo
Append: [JaccDiv: A Metric and Benchmark for Quantifying Diversity of Generated Marketing Text in the Music Industry](https://arxiv.org/abs/2504.20849)
Token length: 1465
Summarized using GPT-3.5-turbo
Append: [DYNAMAX: Dynamic computing for Transformers and Mamba based architectures](https://arxiv.org/abs/2504.20922)
Token length: 1536
Summarized using GPT-3.5-turbo
Append: [Trace-of-Thought: Enhanced Arithmetic Problem Solving via Reasoning Distillation From Large to Small Language Models](https://arxiv.org/abs/2504.20946)
Token length: 833
Summarized using GPT-3.5-turbo
Append: [Information Gravity: A Field-Theoretic Model for Token Selection in Large Language Models](https://arxiv.org/abs/2504.20951)
Token length: 1359
Summarized using GPT-3.5-turbo
Append: [OSVBench: Benchmarking LLMs on Specification Generation Tasks for Operating System Verification](https://arxiv.org/abs/2504.20964)
Token length: 1106
Summarized using GPT-3.5-turbo
Append: [SetKE: Knowledge Editing for Knowledge Elements Overlap](https://arxiv.org/abs/2504.20972)
Token length: 1115
Summarized using GPT-3.5-turbo
Append: [Recommending Clinical Trials for Online Patient Cases using Artificial Intelligence](https://arxiv.org/abs/2504.20059)
Token length: 1272
Summarized using GPT-3.5-turbo
Append: [RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2504.20073)
Token length: 1895
Summarized using GPT-3.5-turbo
Append: [AI Awareness](https://arxiv.org/abs/2504.20084)
Token length: 1091
Summarized using GPT-3.5-turbo
Append: [MATCHA: Can Multi-Agent Collaboration Build a Trustworthy Conversational Recommender?](https://arxiv.org/abs/2504.20094)
Token length: 1601
Summarized using GPT-3.5-turbo
Append: [ResearchCodeAgent: An LLM Multi-Agent System for Automated Codification of Research Methodologies](https://arxiv.org/abs/2504.20117)
Token length: 1228
Summarized using GPT-3.5-turbo
Append: [Weaving Context Across Images: Improving Vision-Language Models through Focus-Centric Visual Chains](https://arxiv.org/abs/2504.20199)
Token length: 1311
Summarized using GPT-3.5-turbo
Append: [mrCAD: Multimodal Refinement of Computer-aided Designs](https://arxiv.org/abs/2504.20294)
Token length: 1530
Summarized using GPT-3.5-turbo
Append: [Reviving Any-Subset Autoregressive Models with Principled Parallel Sampling and Speculative Decoding](https://arxiv.org/abs/2504.20456)
Token length: 1884
Summarized using GPT-3.5-turbo
Append: [Search-Based Interaction For Conversation Recommendation via Generative Reward Model Based Simulated User](https://arxiv.org/abs/2504.20458)
Append: [Reinforcement Learning for Reasoning in Large Language Models with One Training Example](https://arxiv.org/abs/2504.20571)
Append: [ReasonIR: Training Retrievers for Reasoning Tasks](https://arxiv.org/abs/2504.20595)
Append: [X-Cross: Dynamic Integration of Language Models for Cross-Domain Sequential Recommendation](https://arxiv.org/abs/2504.20859)
Append: [The Leaderboard Illusion](https://arxiv.org/abs/2504.20879)
Append: [ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning through Step-by-Step Verification](https://arxiv.org/abs/2504.20930)
Append: [Towards Understanding the Nature of Attention with Low-Rank Sparse Decomposition](https://arxiv.org/abs/2504.20938)
Append: [Semantic Consistency for Assuring Reliability of Large Language Models](https://arxiv.org/abs/2308.09138)
Append: [LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models](https://arxiv.org/abs/2310.03903)
Append: [Evaluating the Symbol Binding Ability of Large Language Models for Multiple-Choice Questions in Vietnamese General Education](https://arxiv.org/abs/2310.12059)
Append: [Agentic AI: The Era of Semantic Decoding](https://arxiv.org/abs/2403.14562)
Append: [A Practical Analysis of Human Alignment with *PO](https://arxiv.org/abs/2407.15229)
Append: [Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment](https://arxiv.org/abs/2408.00137)
Append: [AdaCAD: Adaptively Decoding to Balance Conflicts between Contextual and Parametric Knowledge](https://arxiv.org/abs/2409.07394)
Append: [Racing Thoughts: Explaining Contextualization Errors in Large Language Models](https://arxiv.org/abs/2410.02102)
Append: [Unleashing Multi-Hop Reasoning Potential in Large Language Models through Repetition of Misordered Context](https://arxiv.org/abs/2410.07103)
Append: [MDCure: A Scalable Pipeline for Multi-Document Instruction-Following](https://arxiv.org/abs/2410.23463)
Append: [Constraint Back-translation Improves Complex Instruction Following of Large Language Models](https://arxiv.org/abs/2410.24175)
Append: [Benchmarking LLMs' Judgments with No Gold Standard](https://arxiv.org/abs/2411.07127)
Append: [Beyond the Safety Bundle: Auditing the Helpful and Harmless Dataset](https://arxiv.org/abs/2411.08243)
Append: [A Bayesian Optimization Approach to Machine Translation Reranking](https://arxiv.org/abs/2411.09694)
Append: [Massive Values in Self-Attention Modules are the Key to Contextual Knowledge Understanding](https://arxiv.org/abs/2502.01563)
Append: [An LLM-Powered Agent for Physiological Data Analysis: A Case Study on PPG-based Heart Rate Estimation](https://arxiv.org/abs/2502.12836)
Append: [MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for Retrieval Augmented Generation](https://arxiv.org/abs/2502.17163)
Append: [SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity Representation](https://arxiv.org/abs/2503.15358)
Append: [CausalRAG: Integrating Causal Graphs into Retrieval-Augmented Generation](https://arxiv.org/abs/2503.19878)
Append: [Inaccuracy of an E-Dictionary and Its Influence on Chinese Language Users](https://arxiv.org/abs/2504.00799)
Append: [LLM-based Automated Grading with Human-in-the-Loop](https://arxiv.org/abs/2504.05239)
Append: [Kaleidoscope: In-language Exams for Massively Multilingual Vision Evaluation](https://arxiv.org/abs/2504.07072)
Append: [Prophet: Prompting Large Language Models with Complementary Answer Heuristics for Knowledge-based Visual Question Answering](https://arxiv.org/abs/2303.01903)
Append: [Pose-Based Sign Language Appearance Transfer](https://arxiv.org/abs/2410.13675)
Append: [DP-2Stage: Adapting Language Models as Differentially Private Tabular Data Generators](https://arxiv.org/abs/2412.02467)
Append: [SLA Management in Reconfigurable Multi-Agent RAG: A Systems Approach to Question Answering](https://arxiv.org/abs/2412.06832)
Append: [Owls are wise and foxes are unfaithful: Uncovering animal stereotypes in vision-language models](https://arxiv.org/abs/2501.12433)
Append: [From tools to thieves: Measuring and understanding public perceptions of AI through crowdsourced metaphors](https://arxiv.org/abs/2501.18045)
Append: [REALEDIT: Reddit Edits As a Large-scale Empirical Dataset for Image Transformations](https://arxiv.org/abs/2502.03629)
Append: [The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation](https://arxiv.org/abs/2503.04606)
Append: [Wanda++: Pruning Large Language Models via Regional Gradients](https://arxiv.org/abs/2503.04992)
Append: [LocAgent: Graph-Guided LLM Agents for Code Localization](https://arxiv.org/abs/2503.09089)
append_entries: 88
Finish: 2025-04-30 04:25:44.527424
------------------------------------------------------
Started: 2025-04-30 06:23:15.361193
Existing_entries: 1036
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-30 06:23:15.618642
------------------------------------------------------
Started: 2025-04-30 08:21:50.001071
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-30 08:21:50.237415
------------------------------------------------------
Started: 2025-04-30 10:17:34.865107
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-30 10:17:35.137225
------------------------------------------------------
Started: 2025-04-30 12:32:19.416250
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-30 12:32:19.691571
------------------------------------------------------
Started: 2025-04-30 14:15:51.805005
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-30 14:15:52.072578
------------------------------------------------------
Started: 2025-04-30 16:20:13.711852
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-30 16:20:13.952481
------------------------------------------------------
Started: 2025-04-30 18:22:26.118946
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-30 18:22:26.394410
------------------------------------------------------
Started: 2025-04-30 20:17:52.505808
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-30 20:17:52.744465
------------------------------------------------------
Started: 2025-04-30 22:15:17.634986
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-30 22:15:17.869587
------------------------------------------------------
Started: 2025-05-01 01:23:25.222117
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-01 01:23:25.488610
------------------------------------------------------
Started: 2025-05-01 03:14:18.921097
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-01 03:14:19.188706
------------------------------------------------------
Started: 2025-05-01 04:25:19.936055
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1579
Summarized using GPT-3.5-turbo
Append: [Waking Up an AI: A Quantitative Framework for Prompt-Induced Phase Transition in Large Language Models](https://arxiv.org/abs/2504.21012)
Token length: 1580
Summarized using GPT-3.5-turbo
Append: [Analyzing Feedback Mechanisms in AI-Generated MCQs: Insights into Readability, Lexical Properties, and Levels of Challenge](https://arxiv.org/abs/2504.21013)
Token length: 664
Summarized using GPT-3.5-turbo
Append: [Nested Named-Entity Recognition on Vietnamese COVID-19: Dataset and Experiments](https://arxiv.org/abs/2504.21016)
Token length: 1221
Summarized using GPT-3.5-turbo
Append: [ViQA-COVID: COVID-19 Machine Reading Comprehension Dataset for Vietnamese](https://arxiv.org/abs/2504.21017)
Token length: 1393
Summarized using GPT-3.5-turbo
Append: [HYPEROFA: Expanding LLM Vocabulary to New Languages via Hypernetwork-Based Embedding Initialization](https://arxiv.org/abs/2504.21018)
Token length: 1240
Summarized using GPT-3.5-turbo
Append: [Kill two birds with one stone: generalized and robust AI-generated text detection via dynamic perturbations](https://arxiv.org/abs/2504.21019)
Token length: 1346
Summarized using GPT-3.5-turbo
Append: [Context-Enhanced Contrastive Search for Improved LLM Text Generation](https://arxiv.org/abs/2504.21020)
Token length: 1174
Summarized using GPT-3.5-turbo
Append: [ConformalNL2LTL: Translating Natural Language Instructions into Temporal Logic Formulas with Conformal Correctness Guarantees](https://arxiv.org/abs/2504.21022)
Token length: 1718
Summarized using GPT-3.5-turbo
Append: [Param$\Delta$ for Direct Weight Mixing: Post-Train Large Language Model at Zero Cost](https://arxiv.org/abs/2504.21023)
Token length: 1606
Summarized using GPT-3.5-turbo
Append: [WebEvolver: Enhancing Web Agent Self-Improvement with Coevolving World Model](https://arxiv.org/abs/2504.21024)
Token length: 1677
Summarized using GPT-3.5-turbo
Append: [Durghotona GPT: A Web Scraping and Large Language Model Based Framework to Generate Road Accident Dataset Automatically in Bangladesh](https://arxiv.org/abs/2504.21025)
Token length: 1830
Summarized using GPT-3.5-turbo
Append: [Creating and Evaluating Code-Mixed Nepali-English and Telugu-English Datasets for Abusive Language Detection Using Traditional and Deep Learning Models](https://arxiv.org/abs/2504.21026)
Token length: 1896
Summarized using GPT-3.5-turbo
Append: [UrbanPlanBench: A Comprehensive Urban Planning Benchmark for Evaluating Large Language Models](https://arxiv.org/abs/2504.21027)
Token length: 967
Summarized using GPT-3.5-turbo
Append: [Beyond One-Size-Fits-All: Inversion Learning for Highly Effective NLG Evaluation Prompts](https://arxiv.org/abs/2504.21117)
Token length: 1054
Summarized using GPT-3.5-turbo
Append: [LLM Enhancer: Merged Approach using Vector Embedding for Reducing Large Language Model Hallucinations with External Knowledge](https://arxiv.org/abs/2504.21132)
Token length: 1289
Summarized using GPT-3.5-turbo
Append: [Detecting Manipulated Contents Using Knowledge-Grounded Inference](https://arxiv.org/abs/2504.21165)
Token length: 1767
Summarized using GPT-3.5-turbo
Append: [Small or Large? Zero-Shot or Finetuned? Guiding Language Model Choice for Specialized Applications in Healthcare](https://arxiv.org/abs/2504.21191)
Token length: 1468
Summarized using GPT-3.5-turbo
Append: [Automatic Legal Writing Evaluation of LLMs](https://arxiv.org/abs/2504.21202)
Token length: 1718
Summarized using GPT-3.5-turbo
Append: [Pretraining Large Brain Language Model for Active BCI: Silent Speech](https://arxiv.org/abs/2504.21214)
Token length: 1424
Summarized using GPT-3.5-turbo
Append: [Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language Models in Math](https://arxiv.org/abs/2504.21233)
Token length: 1187
Summarized using GPT-3.5-turbo
Append: [Memorization and Knowledge Injection in Gated LLMs](https://arxiv.org/abs/2504.21239)
Token length: 1345
Summarized using GPT-3.5-turbo
Append: [Talk Before You Retrieve: Agent-Led Discussions for Better RAG in Medical QA](https://arxiv.org/abs/2504.21252)
Token length: 1092
Summarized using GPT-3.5-turbo
Append: [BiasGuard: A Reasoning-enhanced Bias Detection Tool For Large Language Models](https://arxiv.org/abs/2504.21299)
Token length: 1162
Summarized using GPT-3.5-turbo
Append: [Confidence in Large Language Model Evaluation: A Bayesian Approach to Limited-Sample Challenges](https://arxiv.org/abs/2504.21303)
Token length: 1922
Summarized using GPT-3.5-turbo
Append: [Does the Prompt-based Large Language Model Recognize Students' Demographics and Introduce Bias in Essay Scoring?](https://arxiv.org/abs/2504.21330)
Token length: 1398
Summarized using GPT-3.5-turbo
Append: [Retrieval-Enhanced Few-Shot Prompting for Speech Event Extraction](https://arxiv.org/abs/2504.21372)
Token length: 1177
Summarized using GPT-3.5-turbo
Append: [The Distribution of Dependency Distance and Hierarchical Distance in Contemporary Written Japanese and Its Influencing Factors](https://arxiv.org/abs/2504.21421)
Token length: 1106
Summarized using GPT-3.5-turbo
Append: [RWKV-X: A Linear Complexity Hybrid Language Model](https://arxiv.org/abs/2504.21463)
Token length: 866
Summarized using GPT-3.5-turbo
Append: [Homa at SemEval-2025 Task 5: Aligning Librarian Records with OntoAligner for Subject Tagging](https://arxiv.org/abs/2504.21474)
Token length: 1402
Summarized using GPT-3.5-turbo
Append: [Advancing Arabic Reverse Dictionary Systems: A Transformer-Based Approach with Dataset Construction Guidelines](https://arxiv.org/abs/2504.21475)
Token length: 1191
Summarized using GPT-3.5-turbo
Append: [Improving Informally Romanized Language Identification](https://arxiv.org/abs/2504.21540)
Token length: 811
Summarized using GPT-3.5-turbo
Append: [TartuNLP at SemEval-2025 Task 5: Subject Tagging as Two-Stage Information Retrieval](https://arxiv.org/abs/2504.21547)
Token length: 1644
Summarized using GPT-3.5-turbo
Append: [Precision Where It Matters: A Novel Spike Aware Mixed-Precision Quantization Strategy for LLaMA-based Language Models](https://arxiv.org/abs/2504.21553)
Token length: 834
Summarized using GPT-3.5-turbo
Append: [DNB-AI-Project at SemEval-2025 Task 5: An LLM-Ensemble Approach for Automated Subject Indexing](https://arxiv.org/abs/2504.21589)
Token length: 1431
Summarized using GPT-3.5-turbo
Append: [Robust Misinformation Detection by Visiting Potential Commonsense Conflict](https://arxiv.org/abs/2504.21604)
Token length: 957
Summarized using GPT-3.5-turbo
Append: [RDF-Based Structured Quality Assessment Representation of Multilingual LLM Evaluations](https://arxiv.org/abs/2504.21605)
Token length: 924
Summarized using GPT-3.5-turbo
Append: [Meeseeks: An Iterative Benchmark Evaluating LLMs Multi-Turn Instruction-Following Ability](https://arxiv.org/abs/2504.21625)
Token length: 1203
Summarized using GPT-3.5-turbo
Append: [Sadeed: Advancing Arabic Diacritization Through Small Language Model](https://arxiv.org/abs/2504.21635)
Token length: 857
Summarized using GPT-3.5-turbo
Append: [20min-XD: A Comparable Corpus of Swiss News Articles](https://arxiv.org/abs/2504.21677)
Token length: 778
Summarized using GPT-3.5-turbo
Append: [Investigating the Effect of Parallel Data in the Cross-Lingual Transfer for Vision-Language Encoders](https://arxiv.org/abs/2504.21681)
Token length: 1500
Summarized using GPT-3.5-turbo
Append: [Enhancing Health Mention Classification Performance: A Study on Advancements in Parameter Efficient Tuning](https://arxiv.org/abs/2504.21685)
Token length: 807
Summarized using GPT-3.5-turbo
Append: [Investigating Literary Motifs in Ancient and Medieval Novels with Large Language Models](https://arxiv.org/abs/2504.21742)
Token length: 1047
Summarized using GPT-3.5-turbo
Append: [Improving Retrieval-Augmented Neural Machine Translation with Monolingual Data](https://arxiv.org/abs/2504.21747)
Token length: 867
Summarized using GPT-3.5-turbo
Append: [MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced Knowledge Boundary Awareness](https://arxiv.org/abs/2504.21773)
Token length: 1571
Summarized using GPT-3.5-turbo
Append: [WebThinker: Empowering Large Reasoning Models with Deep Research Capability](https://arxiv.org/abs/2504.21776)
Token length: 1499
Summarized using GPT-3.5-turbo
Append: [How Real Are Synthetic Therapy Conversations? Evaluating Fidelity in Prolonged Exposure Dialogues](https://arxiv.org/abs/2504.21800)
Token length: 1414
Summarized using GPT-3.5-turbo
Append: [DeepSeek-Prover-V2: Advancing Formal Mathematical Reasoning via Reinforcement Learning for Subgoal Decomposition](https://arxiv.org/abs/2504.21801)
Token length: 1439
Summarized using GPT-3.5-turbo
Append: [TRUST: An LLM-Based Dialogue System for Trauma Understanding and Structured Assessments](https://arxiv.org/abs/2504.21851)
Token length: 1508
Summarized using GPT-3.5-turbo
Append: [Don't Retrieve, Generate: Prompting LLMs for Synthetic Training Data in Dense Retrieval](https://arxiv.org/abs/2504.21015)
Token length: 1275
Summarized using GPT-3.5-turbo
Append: [A False Sense of Privacy: Evaluating Textual Data Sanitization Beyond Surface-level Privacy Leakage](https://arxiv.org/abs/2504.21035)
Append: [Multimodal Large Language Models for Medicine: A Comprehensive Survey](https://arxiv.org/abs/2504.21051)
Append: [Phi-4-reasoning Technical Report](https://arxiv.org/abs/2504.21318)
Append: [Who Gets the Callback? Generative AI and Gender Bias](https://arxiv.org/abs/2504.21400)
Append: [SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding](https://arxiv.org/abs/2504.21435)
Append: [Black-Box Visual Prompt Engineering for Mitigating Object Hallucination in Large Vision Language Models](https://arxiv.org/abs/2504.21559)
Append: [Glucagon and insulin production in pancreatic cells modeled using Petri nets and Boolean networks](https://arxiv.org/abs/2504.21578)
Append: [AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning Optimization](https://arxiv.org/abs/2504.21659)
Append: [LLM-Empowered Embodied Agent for Memory-Augmented Task Planning in Household Robotics](https://arxiv.org/abs/2504.21716)
Append: [CodeFlowBench: A Multi-turn, Iterative Benchmark for Complex Code Generation](https://arxiv.org/abs/2504.21751)
Append: [SWE-smith: Scaling Data for Software Engineering Agents](https://arxiv.org/abs/2504.21798)
Append: [LLMs and Finetuning: Benchmarking cross-domain performance for hate speech detection](https://arxiv.org/abs/2310.18964)
Append: [Round Trip Translation Defence against Large Language Model Jailbreaking Attacks](https://arxiv.org/abs/2402.13517)
Append: [Does Generative AI speak Nigerian-Pidgin?: Issues about Representativeness and Bias for Multilingualism in LLMs](https://arxiv.org/abs/2404.19442)
Append: [Emergence of a High-Dimensional Abstraction Phase in Language Transformers](https://arxiv.org/abs/2405.15471)
Append: [Extracting and Transferring Abilities For Building Multi-lingual Ability-enhanced Large Language Models](https://arxiv.org/abs/2410.07825)
Append: [Adsorb-Agent: Autonomous Identification of Stable Adsorption Configurations via Large Language Model Agent](https://arxiv.org/abs/2410.16658)
Append: [KnowRA: Knowledge Retrieval Augmented Method for Document-level Relation Extraction with Comprehensive Reasoning Abilities](https://arxiv.org/abs/2501.00571)
Append: [Leveraging Conditional Mutual Information to Improve Large Language Model Fine-Tuning For Classification](https://arxiv.org/abs/2502.11258)
Append: [Mapping Trustworthiness in Large Language Models: A Bibliometric Analysis Bridging Theory to Practice](https://arxiv.org/abs/2503.04785)
Append: [JuDGE: Benchmarking Judgment Document Generation for Chinese Legal System](https://arxiv.org/abs/2503.14258)
Append: [Proof or Bluff? Evaluating LLMs on 2025 USA Math Olympiad](https://arxiv.org/abs/2503.21934)
Append: [VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain Knowledge](https://arxiv.org/abs/2504.10342)
Append: [Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance Grounding](https://arxiv.org/abs/2301.11564)
Append: [SignLLM: Sign Language Production Large Language Models](https://arxiv.org/abs/2405.10718)
Append: [HateSieve: A Contrastive Learning Framework for Detecting and Segmenting Hateful Content in Multimodal Memes](https://arxiv.org/abs/2408.05794)
Append: [Addressing Emotion Bias in Music Emotion Recognition and Generation with Frechet Audio Distance](https://arxiv.org/abs/2409.15545)
Append: [Revise, Reason, and Recognize: LLM-Based Emotion Recognition via Emotion-Specific Prompts and ASR Error Correction](https://arxiv.org/abs/2409.15551)
Append: [Cross-Lingual Speech Emotion Recognition: Humans vs. Self-Supervised Models](https://arxiv.org/abs/2409.16920)
Append: [Semi-Supervised Cognitive State Classification from Speech with Multi-View Pseudo-Labeling](https://arxiv.org/abs/2409.16937)
Append: [Exploring Acoustic Similarity in Emotional Speech and Music via Self-Supervised Representations](https://arxiv.org/abs/2409.17899)
Append: [How to Construct Random Unitaries](https://arxiv.org/abs/2410.10116)
Append: [Retrieval, Reasoning, Re-ranking: A Context-Enriched Framework for Knowledge Graph Completion](https://arxiv.org/abs/2411.08165)
Append: [All Languages Matter: Evaluating LMMs on Culturally Diverse 100 Languages](https://arxiv.org/abs/2411.16508)
Append: [Mastering Board Games by External and Internal Planning with Language Models](https://arxiv.org/abs/2412.12119)
Append: [OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse Task Synthesis](https://arxiv.org/abs/2412.19723)
Append: [Agentic AI Systems Applied to tasks in Financial Services: Modeling and model risk management crews](https://arxiv.org/abs/2502.05439)
Append: [Iron Sharpens Iron: Defending Against Attacks in Machine-Generated Text Detection with Adversarial Training](https://arxiv.org/abs/2502.12734)
Append: [SparseTransX: Efficient Training of Translation-Based Knowledge Graph Embeddings Using Sparse Matrix Operations](https://arxiv.org/abs/2502.16949)
Append: [Learning Code-Edit Embedding to Model Student Debugging Behavior](https://arxiv.org/abs/2502.19407)
Append: [AC-Lite : A Lightweight Image Captioning Model for Low-Resource Assamese Language](https://arxiv.org/abs/2503.01453)
Append: [Urban Computing in the Era of Large Language Models](https://arxiv.org/abs/2504.02009)
append_entries: 91
Finish: 2025-05-01 04:27:19.501001
------------------------------------------------------
Started: 2025-05-01 06:24:07.644520
Existing_entries: 1091
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-01 06:24:07.915664
------------------------------------------------------
Started: 2025-05-01 08:21:25.162194
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-01 08:21:25.453475
------------------------------------------------------
Started: 2025-05-01 10:17:37.752923
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-01 10:17:37.987974
------------------------------------------------------
Started: 2025-05-01 12:31:33.382774
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-01 12:31:33.655939
------------------------------------------------------
Started: 2025-05-01 14:14:59.420908
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-01 14:14:59.676124
------------------------------------------------------
Started: 2025-05-01 16:20:18.825099
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-01 16:20:19.095232
------------------------------------------------------
Started: 2025-05-01 18:22:26.998973
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-01 18:22:27.231121
------------------------------------------------------
Started: 2025-05-01 20:16:40.740301
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-01 20:16:41.003808
------------------------------------------------------
Started: 2025-05-01 22:15:22.977906
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-01 22:15:23.214372
------------------------------------------------------
Started: 2025-05-02 01:17:00.594983
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-02 01:17:00.874513
------------------------------------------------------
Started: 2025-05-02 03:05:38.010694
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-02 03:05:38.274249
------------------------------------------------------
Started: 2025-05-02 04:22:47.078995
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1041
Summarized using GPT-3.5-turbo
Append: [Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning](https://arxiv.org/abs/2505.00001)
Token length: 857
Summarized using GPT-3.5-turbo
Append: [Symbol grounding in computational systems: A paradox of intentions](https://arxiv.org/abs/2505.00002)
Token length: 1025
Summarized using GPT-3.5-turbo
Append: [The Mind in the Machine: A Survey of Incorporating Psychological Theories in LLMs](https://arxiv.org/abs/2505.00003)
Token length: 1165
Summarized using GPT-3.5-turbo
Append: [LangVAE and LangSpace: Building and Probing for Language Model VAEs](https://arxiv.org/abs/2505.00004)
Token length: 979
Summarized using GPT-3.5-turbo
Append: [Toward a digital twin of U.S. Congress](https://arxiv.org/abs/2505.00006)
Token length: 1626
Summarized using GPT-3.5-turbo
Append: [A Scoping Review of Natural Language Processing in Addressing Medically Inaccurate Information: Errors, Misinformation, and Hallucination](https://arxiv.org/abs/2505.00008)
Token length: 1571
Summarized using GPT-3.5-turbo
Append: [Efficient Knowledge Transfer in Multi-Task Learning through Task-Adaptive Low-Rank Representation](https://arxiv.org/abs/2505.00009)
Token length: 1129
Summarized using GPT-3.5-turbo
Append: [Jailbreak Detection in Clinical Training LLMs Using Feature-Based Predictive Models](https://arxiv.org/abs/2505.00010)
Token length: 595
Summarized using GPT-3.5-turbo
Append: [The AI Co-Ethnographer: How Far Can Automation Take Qualitative Research?](https://arxiv.org/abs/2505.00012)
Token length: 1577
Summarized using GPT-3.5-turbo
Append: [Performance Evaluation of Emotion Classification in Japanese Using RoBERTa and DeBERTa](https://arxiv.org/abs/2505.00013)
Token length: 1404
Summarized using GPT-3.5-turbo
Append: [Manifold-Constrained Sentence Embeddings via Triplet Loss: Projecting Semantics onto Spheres, Tori, and M\"obius Strips](https://arxiv.org/abs/2505.00014)
Token length: 1884
Summarized using GPT-3.5-turbo
Append: [Design and Application of Multimodal Large Language Model Based System for End to End Automation of Accident Dataset Generation](https://arxiv.org/abs/2505.00015)
Token length: 1389
Summarized using GPT-3.5-turbo
Append: [Sparks of Tabular Reasoning via Text2SQL Reinforcement Learning](https://arxiv.org/abs/2505.00016)
Token length: 574
Summarized using GPT-3.5-turbo
Append: [ReCellTy: Domain-specific knowledge graph retrieval-augmented LLMs workflow for single-cell annotation](https://arxiv.org/abs/2505.00017)
Token length: 1008
Summarized using GPT-3.5-turbo
Append: [An Empirical Study on Prompt Compression for Large Language Models](https://arxiv.org/abs/2505.00019)
Token length: 1065
Summarized using GPT-3.5-turbo
Append: [Beyond Public Access in LLM Pre-Training Data](https://arxiv.org/abs/2505.00020)
Token length: 1073
Summarized using GPT-3.5-turbo
Append: [Ustnlp16 at SemEval-2025 Task 9: Improving Model Performance through Imbalance Handling and Focal Loss](https://arxiv.org/abs/2505.00021)
Token length: 1180
Summarized using GPT-3.5-turbo
Append: [Aleph-Alpha-GermanWeb: Improving German-language LLM pre-training with model-based data curation and synthetic data generation](https://arxiv.org/abs/2505.00022)
Token length: 1159
Summarized using GPT-3.5-turbo
Append: [CORG: Generating Answers from Complex, Interrelated Contexts](https://arxiv.org/abs/2505.00023)
Token length: 1370
Summarized using GPT-3.5-turbo
Append: [Nemotron-Research-Tool-N1: Tool-Using Language Models with Reinforced Reasoning](https://arxiv.org/abs/2505.00024)
Token length: 1728
Summarized using GPT-3.5-turbo
Append: [A Method for the Architecture of a Medical Vertical Large Language Model Based on Deepseek R1](https://arxiv.org/abs/2505.00025)
Token length: 842
Summarized using GPT-3.5-turbo
Append: [Theory of Mind in Large Language Models: Assessment and Enhancement](https://arxiv.org/abs/2505.00026)
Token length: 1353
Summarized using GPT-3.5-turbo
Append: [Extracting Abstraction Dimensions by Identifying Syntax Pattern from Texts](https://arxiv.org/abs/2505.00027)
Token length: 1372
Summarized using GPT-3.5-turbo
Append: [Enhancing Speech-to-Speech Dialogue Modeling with End-to-End Retrieval-Augmented Generation](https://arxiv.org/abs/2505.00028)
Token length: 1544
Summarized using GPT-3.5-turbo
Append: [Keep the General, Inject the Specific: Structured Dialogue Fine-Tuning for Knowledge Injection without Catastrophic Forgetting](https://arxiv.org/abs/2505.00029)
Token length: 636
Summarized using GPT-3.5-turbo
Append: [Can Language Models Represent the Past without Anachronism?](https://arxiv.org/abs/2505.00030)
Token length: 1610
Summarized using GPT-3.5-turbo
Append: [Learning to Plan Before Answering: Self-Teaching LLMs to Learn Abstract Plans for Problem Solving](https://arxiv.org/abs/2505.00031)
Token length: 1557
Summarized using GPT-3.5-turbo
Append: [MDD-LLM: Towards Accuracy Large Language Models for Major Depressive Disorder Diagnosis](https://arxiv.org/abs/2505.00032)
Token length: 1147
Summarized using GPT-3.5-turbo
Append: [From Attention to Atoms: Spectral Dictionary Learning for Fast, Interpretable Language Models](https://arxiv.org/abs/2505.00033)
Token length: 1064
Summarized using GPT-3.5-turbo
Append: [Improving Phishing Email Detection Performance of Small Large Language Models](https://arxiv.org/abs/2505.00034)
Token length: 1487
Summarized using GPT-3.5-turbo
Append: [Linguistic Complexity and Socio-cultural Patterns in Hip-Hop Lyrics](https://arxiv.org/abs/2505.00035)
Token length: 1810
Summarized using GPT-3.5-turbo
Append: [A Framework to Assess the Persuasion Risks Large Language Model Chatbots Pose to Democratic Societies](https://arxiv.org/abs/2505.00036)
Token length: 1963
Summarized using GPT-3.5-turbo
Append: [HyPerAlign: Hypotheses-driven Personalized Alignment](https://arxiv.org/abs/2505.00038)
Token length: 1077
Summarized using GPT-3.5-turbo
Append: [Graph RAG for Legal Norms: A Hierarchical and Temporal Approach](https://arxiv.org/abs/2505.00039)
Token length: 1146
Summarized using GPT-3.5-turbo
Append: [Base Models Beat Aligned Models at Randomness and Creativity](https://arxiv.org/abs/2505.00047)
Token length: 1478
Summarized using GPT-3.5-turbo
Append: [Emotional Analysis of Fashion Trends Using Social Media and AI: Sentiment Analysis on Twitter for Fashion Trend Forecasting](https://arxiv.org/abs/2505.00050)
Token length: 1077
Summarized using GPT-3.5-turbo
Append: [Clustering Internet Memes Through Template Matching and Multi-Dimensional Similarity](https://arxiv.org/abs/2505.00056)
Token length: 1220
Summarized using GPT-3.5-turbo
Append: [A Report on the llms evaluating the high school questions](https://arxiv.org/abs/2505.00057)
Token length: 1582
Summarized using GPT-3.5-turbo
Append: [BERSting at the Screams: A Benchmark for Distanced, Emotional and Shouted Speech Recognition](https://arxiv.org/abs/2505.00059)
Token length: 1843
Summarized using GPT-3.5-turbo
Append: [Fact-Consistency Evaluation of Text-to-SQL Generation for Business Intelligence Using Exaone 3.5](https://arxiv.org/abs/2505.00060)
Token length: 1167
Summarized using GPT-3.5-turbo
Append: [Enhancing Security and Strengthening Defenses in Automated Short-Answer Grading Systems](https://arxiv.org/abs/2505.00061)
Token length: 1460
Summarized using GPT-3.5-turbo
Append: [GDI-Bench: A Benchmark for General Document Intelligence with Vision and Reasoning Decoupling](https://arxiv.org/abs/2505.00063)
Token length: 1296
Summarized using GPT-3.5-turbo
Append: [ConSens: Assessing context grounding in open-book question answering](https://arxiv.org/abs/2505.00065)
Token length: 1073
Summarized using GPT-3.5-turbo
Append: [Fine-Tuning LLMs for Low-Resource Dialect Translation: The Case of Lebanese](https://arxiv.org/abs/2505.00114)
Token length: 1199
Summarized using GPT-3.5-turbo
Append: [Between Underthinking and Overthinking: An Empirical Study of Reasoning Length and correctness in LLMs](https://arxiv.org/abs/2505.00127)
Token length: 1582
Summarized using GPT-3.5-turbo
Append: [AdaptMI: Adaptive Skill-based In-context Math Instruction for Small Language Models](https://arxiv.org/abs/2505.00147)
Token length: 1070
Summarized using GPT-3.5-turbo
Append: [IP-CRR: Information Pursuit for Interpretable Classification of Chest Radiology Reports](https://arxiv.org/abs/2505.00191)
Token length: 828
Summarized using GPT-3.5-turbo
Append: [Enriching the Korean Learner Corpus with Multi-reference Annotations and Rubric-Based Scoring](https://arxiv.org/abs/2505.00261)
Token length: 998
Summarized using GPT-3.5-turbo
Append: [Consistency in Language Models: Current Landscape, Challenges, and Future Directions](https://arxiv.org/abs/2505.00268)
Token length: 1268
Summarized using GPT-3.5-turbo
Append: [Enhancing AI-Driven Education: Integrating Cognitive Frameworks, Linguistic Feedback Analysis, and Ethical Considerations for Improved Content Generation](https://arxiv.org/abs/2505.00339)
Append: [KoACD: The First Korean Adolescent Dataset for Cognitive Distortion Analysis](https://arxiv.org/abs/2505.00367)
Append: [CSE-SFP: Enabling Unsupervised Sentence Representation Learning via a Single Forward Pass](https://arxiv.org/abs/2505.00389)
Append: [Red Teaming Large Language Models for Healthcare](https://arxiv.org/abs/2505.00467)
Append: [Computational Identification of Regulatory Statements in EU Legislation](https://arxiv.org/abs/2505.00479)
Append: [HalluMix: A Task-Agnostic, Multi-Domain Benchmark for Real-World Hallucination Detection](https://arxiv.org/abs/2505.00506)
Append: [100 Days After DeepSeek-R1: A Survey on Replication Studies and More Directions for Reasoning Language Models](https://arxiv.org/abs/2505.00551)
Append: [Triggering Hallucinations in LLMs: A Quantitative Study of Prompt-Induced Hallucination in Large Language Models](https://arxiv.org/abs/2505.00557)
Append: [FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension](https://arxiv.org/abs/2505.00570)
Append: [Block Circulant Adapter for Large Language Models](https://arxiv.org/abs/2505.00582)
Append: [FineScope : Precision Pruning for Domain-Specialized Large Language Models Using SAE-Guided Self-Data Cultivation](https://arxiv.org/abs/2505.00624)
Append: [The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning (and How to Fix Them)](https://arxiv.org/abs/2505.00626)
Append: [Large Language Models Understanding: an Inherent Ambiguity Barrier](https://arxiv.org/abs/2505.00654)
Append: [On the generalization of language models from in-context learning and finetuning: a controlled study](https://arxiv.org/abs/2505.00661)
Append: [DeepCritic: Deliberate Critique with Large Language Models](https://arxiv.org/abs/2505.00662)
Append: [Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions](https://arxiv.org/abs/2505.00675)
Append: [Steering Large Language Models with Register Analysis for Arbitrary Style Transfer](https://arxiv.org/abs/2505.00679)
Append: [Humanizing LLMs: A Survey of Psychological Measurements with Tools, Datasets, and Human-Agent Applications](https://arxiv.org/abs/2505.00049)
Append: [Optimization of embeddings storage for RAG systems using quantization and dimensionality reduction techniques](https://arxiv.org/abs/2505.00105)
Append: [Detecting and Mitigating Hateful Content in Multimodal Memes with Vision-Language Models](https://arxiv.org/abs/2505.00150)
Append: [Which Agent Causes Task Failures and When? On Automated Failure Attribution of LLM Multi-Agent Systems](https://arxiv.org/abs/2505.00212)
Append: [Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks](https://arxiv.org/abs/2505.00234)
Append: [EnronQA: Towards Personalized RAG over Private Documents](https://arxiv.org/abs/2505.00263)
Append: [Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing](https://arxiv.org/abs/2505.00315)
Append: [T2VPhysBench: A First-Principles Benchmark for Physical Consistency in Text-to-Video Generation](https://arxiv.org/abs/2505.00337)
Append: [R&B: Domain Regrouping and Data Mixture Balancing for Efficient Foundation Model Training](https://arxiv.org/abs/2505.00358)
Append: [Toward Automated Regulatory Decision-Making: Trustworthy Medical Device Risk Classification with Multimodal Transformers and Self-Training](https://arxiv.org/abs/2505.00422)
Append: [Investigating Task Arithmetic for Zero-Shot Information Retrieval](https://arxiv.org/abs/2505.00649)
Append: [T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT](https://arxiv.org/abs/2505.00703)
Append: [EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers](https://arxiv.org/abs/2309.08532)
Append: [LegalDuet: Learning Fine-grained Representations for Legal Judgment Prediction via a Dual-View Contrastive Learning](https://arxiv.org/abs/2401.15371)
Append: ["Reasoning" with Rhetoric: On the Style-Evidence Tradeoff in LLM-Generated Counter-Arguments](https://arxiv.org/abs/2402.08498)
Append: [QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving](https://arxiv.org/abs/2405.04532)
Append: [(Perhaps) Beyond Human Translation: Harnessing Multi-Agent Collaboration for Translating Ultra-Long Literary Texts](https://arxiv.org/abs/2405.11804)
Append: [Automated Review Generation Method Based on Large Language Models](https://arxiv.org/abs/2407.20906)
Append: [Challenges and Future Directions of Data-Centric AI Alignment](https://arxiv.org/abs/2410.01957)
Append: [Are LLM-Judges Robust to Expressions of Uncertainty? Investigating the effect of Epistemic Markers on LLM-based Evaluation](https://arxiv.org/abs/2410.20774)
Append: [Domain-Specific Translation with Open-Source Large Language Models: Resource-Oriented Analysis](https://arxiv.org/abs/2412.05862)
Append: [A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods](https://arxiv.org/abs/2501.13947)
Append: [HSI: Head-Specific Intervention Can Induce Misaligned AI Coordination in Large Language Models](https://arxiv.org/abs/2502.05945)
Append: [Caught in the Web of Words: Do LLMs Fall for Spin in Medical Literature?](https://arxiv.org/abs/2502.07963)
Append: [UoR-NCL at SemEval-2025 Task 1: Using Generative LLMs and CLIP Models for Multilingual Multimodal Idiomaticity Representation](https://arxiv.org/abs/2502.20984)
Append: [Dynamic Parametric Retrieval Augmented Generation for Test-time Knowledge Enhancement](https://arxiv.org/abs/2503.23895)
Append: [Opioid Named Entity Recognition (ONER-2025) from Reddit](https://arxiv.org/abs/2504.00027)
Append: [LoRATK: LoRA Once, Backdoor Everywhere in the Share-and-Play Ecosystem](https://arxiv.org/abs/2403.00108)
Append: [Large Language Model Agent as a Mechanical Designer](https://arxiv.org/abs/2404.17525)
Append: [Folded Context Condensation in Path Integral Formalism for Infinite Context Transformers](https://arxiv.org/abs/2405.04620)
Append: [TaeBench: Improving Quality of Toxic Adversarial Examples](https://arxiv.org/abs/2410.05573)
Append: [Bridging Personalization and Control in Scientific Personalized Search](https://arxiv.org/abs/2411.02790)
Append: [Generating Traffic Scenarios via In-Context Learning to Learn Better Motion Planner](https://arxiv.org/abs/2412.18086)
Append: [Efficiency and Effectiveness of LLM-Based Summarization of Evidence in Crowdsourced Fact-Checking](https://arxiv.org/abs/2501.18265)
Append: [Efficient Reinforcement Finetuning via Adaptive Curriculum Learning](https://arxiv.org/abs/2504.05520)
append_entries: 101
Finish: 2025-05-02 04:24:50.973466
------------------------------------------------------
Started: 2025-05-02 06:22:58.565597
Existing_entries: 1101
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-02 06:22:58.831768
------------------------------------------------------
Started: 2025-05-02 08:21:03.015241
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-02 08:21:03.309858
------------------------------------------------------
Started: 2025-05-02 10:17:17.122492
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-02 10:17:17.382094
------------------------------------------------------
Started: 2025-05-02 12:31:57.572343
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-02 12:31:57.835419
------------------------------------------------------
Started: 2025-05-02 14:15:36.330689
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-02 14:15:36.613397
------------------------------------------------------
Started: 2025-05-02 16:20:00.940575
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-02 16:20:01.202289
------------------------------------------------------
Started: 2025-05-02 18:21:56.113792
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-02 18:21:56.364424
------------------------------------------------------
Started: 2025-05-02 20:17:57.421515
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-02 20:17:57.684845
------------------------------------------------------
Started: 2025-05-02 22:15:24.006114
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-02 22:15:24.290948
------------------------------------------------------
Started: 2025-05-03 01:15:13.934457
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-03 01:15:14.192303
------------------------------------------------------
Started: 2025-05-03 03:01:18.688971
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-03 03:01:18.959935
------------------------------------------------------
Started: 2025-05-03 04:18:37.330220
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-03 04:18:37.392968
------------------------------------------------------
Started: 2025-05-03 06:20:53.828651
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-03 06:20:53.905038
------------------------------------------------------
Started: 2025-05-03 08:18:55.653907
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-03 08:18:55.739166
------------------------------------------------------
Started: 2025-05-03 10:15:17.902925
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-03 10:15:17.958709
------------------------------------------------------
Started: 2025-05-03 12:29:35.037106
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-03 12:29:35.119349
------------------------------------------------------
Started: 2025-05-03 14:14:12.630838
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-03 14:14:12.715767
------------------------------------------------------
Started: 2025-05-03 16:18:19.315741
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-03 16:18:19.396727
------------------------------------------------------
Started: 2025-05-03 18:20:16.425313
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-03 18:20:16.490246
------------------------------------------------------
Started: 2025-05-03 20:16:21.203195
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-03 20:16:21.263042
------------------------------------------------------
Started: 2025-05-03 22:14:21.941006
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-03 22:14:22.013179
------------------------------------------------------
Started: 2025-05-04 01:24:07.894825
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-04 01:24:07.950782
------------------------------------------------------
Started: 2025-05-04 03:14:23.995746
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-04 03:14:24.054440
------------------------------------------------------
Started: 2025-05-04 04:19:51.392192
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-04 04:19:51.452550
------------------------------------------------------
Started: 2025-05-04 06:22:05.880652
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-04 06:22:05.939796
------------------------------------------------------
Started: 2025-05-04 08:19:11.811528
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-04 08:19:11.872311
------------------------------------------------------
Started: 2025-05-04 10:15:51.226512
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-04 10:15:51.288132
------------------------------------------------------
Started: 2025-05-04 12:29:52.291566
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-04 12:29:52.404019
------------------------------------------------------
Started: 2025-05-04 14:13:39.705622
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-04 14:13:39.767489
------------------------------------------------------
Started: 2025-05-04 16:18:20.734342
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-04 16:18:20.799536
------------------------------------------------------
Started: 2025-05-04 18:20:17.390516
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-04 18:20:17.446402
------------------------------------------------------
Started: 2025-05-04 20:16:38.507668
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-04 20:16:38.584513
------------------------------------------------------
Started: 2025-05-04 22:14:35.751245
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-04 22:14:35.829778
------------------------------------------------------
Started: 2025-05-05 01:20:10.431233
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-05 01:20:10.513727
------------------------------------------------------
Started: 2025-05-05 03:10:31.585894
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-05 03:10:31.645818
------------------------------------------------------
Started: 2025-05-05 04:24:16.722852
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1440
Summarized using GPT-3.5-turbo
Append: [FinBERT-QA: Financial Question Answering with pre-trained BERT Language Models](https://arxiv.org/abs/2505.00725)
Token length: 1320
Summarized using GPT-3.5-turbo
Append: [A Survey on Large Language Model based Human-Agent Systems](https://arxiv.org/abs/2505.00753)
Token length: 1220
Summarized using GPT-3.5-turbo
Append: [Reasoning Capabilities and Invariability of Large Language Models](https://arxiv.org/abs/2505.00776)
Token length: 1531
Summarized using GPT-3.5-turbo
Append: [Knowledge-augmented Pre-trained Language Models for Biomedical Relation Extraction](https://arxiv.org/abs/2505.00814)
Token length: 1160
Summarized using GPT-3.5-turbo
Append: [Large Language Model-Driven Dynamic Assessment of Grammatical Accuracy in English Language Learner Writing](https://arxiv.org/abs/2505.00931)
Token length: 1458
Summarized using GPT-3.5-turbo
Append: [Llama-Nemotron: Efficient Reasoning Models](https://arxiv.org/abs/2505.00949)
Token length: 1907
Summarized using GPT-3.5-turbo
Append: [A Character-based Diffusion Embedding Algorithm for Enhancing the Generation Quality of Generative Linguistic Steganographic Texts](https://arxiv.org/abs/2505.00977)
Token length: 1548
Summarized using GPT-3.5-turbo
Append: [Synthesize-on-Graph: Knowledgeable Synthetic Data Generation for Continue Pre-training of Large Language Models](https://arxiv.org/abs/2505.00979)
Token length: 846
Summarized using GPT-3.5-turbo
Append: [Position: Enough of Scaling LLMs! Lets Focus on Downscaling](https://arxiv.org/abs/2505.00985)
Token length: 1615
Summarized using GPT-3.5-turbo
Append: [VTS-LLM: Domain-Adaptive LLM Agent for Enhancing Awareness in Vessel Traffic Services through Natural Language](https://arxiv.org/abs/2505.00989)
Token length: 991
Summarized using GPT-3.5-turbo
Append: [Token-free Models for Sarcasm Detection](https://arxiv.org/abs/2505.01006)
Token length: 1462
Summarized using GPT-3.5-turbo
Append: [Value Portrait: Understanding Values of LLMs with Human-aligned Benchmark](https://arxiv.org/abs/2505.01015)
Token length: 1196
Summarized using GPT-3.5-turbo
Append: [Do We Need a Detailed Rubric for Automated Essay Scoring using Large Language Models?](https://arxiv.org/abs/2505.01035)
Token length: 1435
Summarized using GPT-3.5-turbo
Append: [Multimodal Transformers are Hierarchical Modal-wise Heterogeneous Graphs](https://arxiv.org/abs/2505.01068)
Token length: 1327
Summarized using GPT-3.5-turbo
Append: [MateICL: Mitigating Attention Dispersion in Large-Scale In-Context Learning](https://arxiv.org/abs/2505.01110)
Token length: 775
Summarized using GPT-3.5-turbo
Append: [On the Limitations of Steering in Language Model Alignment](https://arxiv.org/abs/2505.01162)
Token length: 1183
Summarized using GPT-3.5-turbo
Append: [Gender Bias in Explainability: Investigating Performance Disparity in Post-hoc Methods](https://arxiv.org/abs/2505.01198)
Token length: 1368
Summarized using GPT-3.5-turbo
Append: [EvalxNLP: A Framework for Benchmarking Post-Hoc Explainability Methods on NLP Models](https://arxiv.org/abs/2505.01238)
Token length: 912
Summarized using GPT-3.5-turbo
Append: [PREMISE: Matching-based Prediction for Accurate Review Recommendation](https://arxiv.org/abs/2505.01255)
Token length: 1296
Summarized using GPT-3.5-turbo
Append: [Anti-adversarial Learning: Desensitizing Prompts for Large Language Models](https://arxiv.org/abs/2505.01273)
Token length: 1006
Summarized using GPT-3.5-turbo
Append: [A Factorized Probabilistic Model of the Semantics of Vague Temporal Adverbials Relative to Different Event Types](https://arxiv.org/abs/2505.01311)
Token length: 823
Summarized using GPT-3.5-turbo
Append: [A Transformer-based Neural Architecture Search Method](https://arxiv.org/abs/2505.01314)
Token length: 1779
Summarized using GPT-3.5-turbo
Append: [Helping Big Language Models Protect Themselves: An Enhanced Filtering and Summarization System](https://arxiv.org/abs/2505.01315)
Token length: 1715
Summarized using GPT-3.5-turbo
Append: [TRAVELER: A Benchmark for Evaluating Temporal Reasoning across Vague, Implicit and Explicit References](https://arxiv.org/abs/2505.01325)
Token length: 1144
Summarized using GPT-3.5-turbo
Append: [Multi-Modal Language Models as Text-to-Image Model Evaluators](https://arxiv.org/abs/2505.00759)
Token length: 928
Summarized using GPT-3.5-turbo
Append: [A Mathematical Philosophy of Explanations in Mechanistic Interpretability -- The Strange Science Part I.i](https://arxiv.org/abs/2505.00808)
Token length: 1298
Summarized using GPT-3.5-turbo
Append: [SmallPlan: Leverage Small Language Models for Sequential Path Planning with Simulation-Powered, LLM-Guided Distillation](https://arxiv.org/abs/2505.00831)
Token length: 1185
Summarized using GPT-3.5-turbo
Append: [NeMo-Inspector: A Visualization Tool for LLM Generation Analysis](https://arxiv.org/abs/2505.00903)
Token length: 1619
Summarized using GPT-3.5-turbo
Append: [How Transformers Learn Regular Language Recognition: A Theoretical Study on Training Dynamics and Implicit Bias](https://arxiv.org/abs/2505.00926)
Token length: 1177
Summarized using GPT-3.5-turbo
Append: [Attack and defense techniques in large language models: A survey and new perspectives](https://arxiv.org/abs/2505.00976)
Token length: 875
Summarized using GPT-3.5-turbo
Append: [Towards the Resistance of Neural Network Watermarking to Fine-tuning](https://arxiv.org/abs/2505.01007)
Token length: 1965
Summarized using GPT-3.5-turbo
Append: [Evaluating Vision Language Model Adaptations for Radiology Report Generation in Low-Resource Languages](https://arxiv.org/abs/2505.01096)
Token length: 977
Summarized using GPT-3.5-turbo
Append: [Evaluating Explanations: An Explanatory Virtues Framework for Mechanistic Interpretability -- The Strange Science Part I.ii](https://arxiv.org/abs/2505.01372)
Token length: 1354
Summarized using GPT-3.5-turbo
Append: [Rethinking Scientific Summarization Evaluation: Grounding Explainable Metrics on Facet-aware Benchmark](https://arxiv.org/abs/2402.14359)
Token length: 1364
Summarized using GPT-3.5-turbo
Append: [Do Vision & Language Decoders use Images and Text equally? How Self-consistent are their Explanations?](https://arxiv.org/abs/2404.18624)
Token length: 1382
Summarized using GPT-3.5-turbo
Append: [REFFLY: Melody-Constrained Lyrics Editing Model](https://arxiv.org/abs/2409.00292)
Token length: 1205
Summarized using GPT-3.5-turbo
Append: [Does Self-Attention Need Separate Weights in Transformers?](https://arxiv.org/abs/2412.00359)
Token length: 1025
Summarized using GPT-3.5-turbo
Append: [When Every Token Counts: Optimal Segmentation for Low-Resource Language Models](https://arxiv.org/abs/2412.06926)
Token length: 1803
Summarized using GPT-3.5-turbo
Append: [AutoPrep: Natural Language Question-Aware Data Preparation with a Multi-Agent Framework](https://arxiv.org/abs/2412.10422)
Token length: 1861
Summarized using GPT-3.5-turbo
Append: [ICLR: In-Context Learning of Representations](https://arxiv.org/abs/2501.00070)
Token length: 1475
Summarized using GPT-3.5-turbo
Append: [Dynamics of Spontaneous Topic Changes in Next Token Prediction with Self-Attention](https://arxiv.org/abs/2501.06382)
Token length: 1258
Summarized using GPT-3.5-turbo
Append: [TableMaster: A Recipe to Advance Table Understanding with Language Models](https://arxiv.org/abs/2501.19378)
Token length: 1448
Summarized using GPT-3.5-turbo
Append: [CITER: Collaborative Inference for Efficient Large Language Model Decoding with Token-Level Routing](https://arxiv.org/abs/2502.01976)
Token length: 1152
Summarized using GPT-3.5-turbo
Append: [Accuracy is Not Agreement: Expert-Aligned Evaluation of Crash Narrative Classification Models](https://arxiv.org/abs/2504.13068)
Token length: 1024
Summarized using GPT-3.5-turbo
Append: [Automating the Generation of Prompts for LLM-based Action Choice in PDDL Planning](https://arxiv.org/abs/2311.09830)
Token length: 1249
Summarized using GPT-3.5-turbo
Append: [FlexLLM: A System for Co-Serving Large Language Model Inference and Parameter-Efficient Finetuning](https://arxiv.org/abs/2402.18789)
Token length: 1198
Summarized using GPT-3.5-turbo
Append: [Wiki-TabNER: Integrating Named Entity Recognition into Wikipedia Tables](https://arxiv.org/abs/2403.04577)
Token length: 1574
Summarized using GPT-3.5-turbo
Append: [MoDeGPT: Modular Decomposition for Large Language Model Compression](https://arxiv.org/abs/2408.09632)
Token length: 1710
Summarized using GPT-3.5-turbo
Append: [Competition Dynamics Shape Algorithmic Phases of In-Context Learning](https://arxiv.org/abs/2412.01003)
Token length: 1169
Summarized using GPT-3.5-turbo
Append: [A Causal World Model Underlying Next Token Prediction: Exploring GPT in a Controlled Environment](https://arxiv.org/abs/2412.07446)
Append: [A Rate-Distortion Framework for Summarization](https://arxiv.org/abs/2501.13100)
Append: [Activation Steering in Neural Theorem Provers](https://arxiv.org/abs/2502.15507)
append_entries: 52
Finish: 2025-05-05 04:26:15.688257
------------------------------------------------------
Started: 2025-05-05 06:24:33.721452
Existing_entries: 1052
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-05 06:24:33.936195
------------------------------------------------------
Started: 2025-05-05 08:23:01.354074
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-05 08:23:01.521425
------------------------------------------------------
Started: 2025-05-05 10:18:15.542958
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-05 10:18:15.735466
------------------------------------------------------
Started: 2025-05-05 12:33:08.883418
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-05 12:33:09.086435
------------------------------------------------------
Started: 2025-05-05 14:16:43.864983
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-05 14:16:44.062886
------------------------------------------------------
Started: 2025-05-05 16:21:10.141364
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-05 16:21:10.304225
------------------------------------------------------
Started: 2025-05-05 18:19:23.508124
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-05 18:19:23.705744
------------------------------------------------------
Started: 2025-05-05 20:18:03.120230
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-05 20:18:03.292202
------------------------------------------------------
Started: 2025-05-05 22:15:56.872759
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-05 22:15:57.034904
------------------------------------------------------
Started: 2025-05-06 01:17:38.706983
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-06 01:17:38.877576
------------------------------------------------------
Started: 2025-05-06 03:06:12.461245
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-06 03:06:12.638391
------------------------------------------------------
Started: 2025-05-06 04:24:00.678404
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1642
Summarized using GPT-3.5-turbo
Append: [Unlearning Sensitive Information in Multimodal LLMs: Benchmark and Attack-Defense Evaluation](https://arxiv.org/abs/2505.01456)
Token length: 1261
Summarized using GPT-3.5-turbo
Append: [MoxE: Mixture of xLSTM Experts with Entropy-Aware Routing for Efficient Language Modeling](https://arxiv.org/abs/2505.01459)
Token length: 1176
Summarized using GPT-3.5-turbo
Append: [SymPlanner: Deliberate Planning in Language Models with Symbolic Representation](https://arxiv.org/abs/2505.01479)
Token length: 1110
Summarized using GPT-3.5-turbo
Append: [On the effectiveness of Large Language Models in the mechanical design domain](https://arxiv.org/abs/2505.01559)
Token length: 1789
Summarized using GPT-3.5-turbo
Append: [AI agents may be worth the hype but not the resources (yet): An initial exploration of machine translation quality and costs in three language pairs in the legal and news domains](https://arxiv.org/abs/2505.01560)
Token length: 1481
Summarized using GPT-3.5-turbo
Append: [PIPA: A Unified Evaluation Protocol for Diagnosing Interactive Planning Agents](https://arxiv.org/abs/2505.01592)
Token length: 1138
Summarized using GPT-3.5-turbo
Append: [Always Tell Me The Odds: Fine-grained Conditional Probability Estimation](https://arxiv.org/abs/2505.01595)
Token length: 1665
Summarized using GPT-3.5-turbo
Append: [A Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency](https://arxiv.org/abs/2505.01658)
Token length: 1735
Summarized using GPT-3.5-turbo
Append: [High-Fidelity Pseudo-label Generation by Large Language Models for Training Robust Radiology Report Classifiers](https://arxiv.org/abs/2505.01693)
Token length: 1394
Summarized using GPT-3.5-turbo
Append: [Efficient Shapley Value-based Non-Uniform Pruning of Large Language Models](https://arxiv.org/abs/2505.01731)
Token length: 1090
Summarized using GPT-3.5-turbo
Append: [Same evaluation, more tokens: On the effect of input length for machine translation evaluation using Large Language Models](https://arxiv.org/abs/2505.01761)
Token length: 1302
Summarized using GPT-3.5-turbo
Append: [A Multimodal Framework for Explainable Evaluation of Soft Skills in Educational Environments](https://arxiv.org/abs/2505.01794)
Token length: 1141
Summarized using GPT-3.5-turbo
Append: [Distinguishing AI-Generated and Human-Written Text Through Psycholinguistic Analysis](https://arxiv.org/abs/2505.01800)
Token length: 1668
Summarized using GPT-3.5-turbo
Append: [$\textit{New News}$: System-2 Fine-tuning for Robust Integration of New Knowledge](https://arxiv.org/abs/2505.01812)
Token length: 787
Summarized using GPT-3.5-turbo
Append: [Intra-Layer Recurrence in Transformers for Language Modeling](https://arxiv.org/abs/2505.01855)
Token length: 1031
Summarized using GPT-3.5-turbo
Append: [Positional Attention for Efficient BERT-Based Named Entity Recognition](https://arxiv.org/abs/2505.01868)
Token length: 1942
Summarized using GPT-3.5-turbo
Append: [Humans can learn to detect AI-generated texts, or at least learn when they can't](https://arxiv.org/abs/2505.01877)
Token length: 847
Summarized using GPT-3.5-turbo
Append: [Automated Sentiment Classification and Topic Discovery in Large-Scale Social Media Streams](https://arxiv.org/abs/2505.01883)
Token length: 1934
Summarized using GPT-3.5-turbo
Append: [CAMOUFLAGE: Exploiting Misinformation Detection Systems Through LLM-driven Adversarial Claim Transformation](https://arxiv.org/abs/2505.01900)
Token length: 1316
Summarized using GPT-3.5-turbo
Append: [Analyzing Cognitive Differences Among Large Language Models through the Lens of Social Worldview](https://arxiv.org/abs/2505.01967)
Token length: 1967
Summarized using GPT-3.5-turbo
Append: [LLM-based Text Simplification and its Effect on User Comprehension and Cognitive Load](https://arxiv.org/abs/2505.01980)
Token length: 1386
Summarized using GPT-3.5-turbo
Append: [Towards Safer Pretraining: Analyzing and Filtering Harmful Content in Webscale datasets for Responsible LLMs](https://arxiv.org/abs/2505.02009)
Token length: 1484
Summarized using GPT-3.5-turbo
Append: [An overview of artificial intelligence in computer-assisted language learning](https://arxiv.org/abs/2505.02032)
Token length: 1084
Summarized using GPT-3.5-turbo
Append: [What do Language Model Probabilities Represent? From Distribution Estimation to Response Prediction](https://arxiv.org/abs/2505.02072)
Token length: 1029
Summarized using GPT-3.5-turbo
Append: [LecEval: An Automated Metric for Multimodal Knowledge Acquisition in Multimedia Learning](https://arxiv.org/abs/2505.02078)
Token length: 935
Summarized using GPT-3.5-turbo
Append: [LLM-OptiRA: LLM-Driven Optimization of Resource Allocation for Non-Convex Problems in Wireless Communications](https://arxiv.org/abs/2505.02091)
Token length: 1262
Summarized using GPT-3.5-turbo
Append: [Exploring the Potential of Offline RL for Reasoning in LLMs: A Preliminary Study](https://arxiv.org/abs/2505.02142)
Token length: 1860
Summarized using GPT-3.5-turbo
Append: [QiMeng-Xpiler: Transcompiling Tensor Programs for Deep Learning Systems with a Neural-Symbolic Approach](https://arxiv.org/abs/2505.02146)
Token length: 1412
Summarized using GPT-3.5-turbo
Append: [Think on your Feet: Adaptive Thinking via Reinforcement Learning for Social Agents](https://arxiv.org/abs/2505.02156)
Token length: 1003
Summarized using GPT-3.5-turbo
Append: [Incorporating Legal Structure in Retrieval-Augmented Generation: A Case Study on Copyright Fair Use](https://arxiv.org/abs/2505.02164)
Token length: 1450
Summarized using GPT-3.5-turbo
Append: [A New HOPE: Domain-agnostic Automatic Evaluation of Text Chunking](https://arxiv.org/abs/2505.02171)
Token length: 1386
Summarized using GPT-3.5-turbo
Append: [Identifying Legal Holdings with LLMs: A Systematic Study of Performance, Scale, and Memorization](https://arxiv.org/abs/2505.02172)
Token length: 1594
Summarized using GPT-3.5-turbo
Append: [Measuring Hong Kong Massive Multi-Task Language Understanding](https://arxiv.org/abs/2505.02177)
Token length: 1030
Summarized using GPT-3.5-turbo
Append: [SEval-Ex: A Statement-Level Framework for Explainable Summarization Evaluation](https://arxiv.org/abs/2505.02235)
Token length: 1242
Summarized using GPT-3.5-turbo
Append: [Personalisation or Prejudice? Addressing Geographic Bias in Hate Speech Detection using Debias Tuning in Large Language Models](https://arxiv.org/abs/2505.02252)
Token length: 1072
Summarized using GPT-3.5-turbo
Append: [Parameter-Efficient Transformer Embeddings](https://arxiv.org/abs/2505.02266)
Token length: 847
Summarized using GPT-3.5-turbo
Append: [Demystifying optimized prompts in language models](https://arxiv.org/abs/2505.02273)
Token length: 1366
Summarized using GPT-3.5-turbo
Append: [Generative Sign-description Prompts with Multi-positive Contrastive Learning for Sign Language Recognition](https://arxiv.org/abs/2505.02304)
Token length: 1398
Summarized using GPT-3.5-turbo
Append: [Invoke Interfaces Only When Needed: Adaptive Invocation for Large Language Models in Question Answering](https://arxiv.org/abs/2505.02311)
Token length: 1315
Summarized using GPT-3.5-turbo
Append: [SIMPLEMIX: Frustratingly Simple Mixing of Off- and On-policy Data in Language Model Preference Learning](https://arxiv.org/abs/2505.02363)
Token length: 1954
Summarized using GPT-3.5-turbo
Append: [JTCSE: Joint Tensor-Modulus Constraints and Cross-Attention for Unsupervised Contrastive Learning of Sentence Embeddings](https://arxiv.org/abs/2505.02366)
Token length: 1892
Summarized using GPT-3.5-turbo
Append: [RM-R1: Reward Modeling as Reasoning](https://arxiv.org/abs/2505.02387)
Token length: 1229
Summarized using GPT-3.5-turbo
Append: [Bielik 11B v2 Technical Report](https://arxiv.org/abs/2505.02410)
Token length: 1215
Summarized using GPT-3.5-turbo
Append: [Colombian Waitresses y Jueces canadienses: Gender and Country Biases in Occupation Recommendations from LLMs](https://arxiv.org/abs/2505.02456)
Token length: 1396
Summarized using GPT-3.5-turbo
Append: [Data Augmentation With Back translation for Low Resource languages: A case of English and Luganda](https://arxiv.org/abs/2505.02463)
Token length: 477
Summarized using GPT-3.5-turbo
Append: [Bemba Speech Translation: Exploring a Low-Resource African Language](https://arxiv.org/abs/2505.02518)
Token length: 1356
Summarized using GPT-3.5-turbo
Append: [EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and Flexible LLM Fine-Tuning](https://arxiv.org/abs/2505.02579)
Token length: 1155
Summarized using GPT-3.5-turbo
Append: [Ensemble Kalman filter for uncertainty in human language comprehension](https://arxiv.org/abs/2505.02590)
Token length: 1075
Summarized using GPT-3.5-turbo
Append: [Automatic Proficiency Assessment in L2 English Learners](https://arxiv.org/abs/2505.02615)
Token length: 942
Summarized using GPT-3.5-turbo
Append: [LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive Streaming Speech Synthesis](https://arxiv.org/abs/2505.02625)
Append: [Proper Name Diacritization for Arabic Wikipedia: A Benchmark Dataset](https://arxiv.org/abs/2505.02656)
Append: [A Survey on Progress in LLM Alignment from the Perspective of Reward Design](https://arxiv.org/abs/2505.02666)
Append: [Sailing AI by the Stars: A Survey of Learning from Rewards in Post-Training and Test-Time Scaling of Large Language Models](https://arxiv.org/abs/2505.02686)
Append: [fastabx: A library for efficient computation of ABX discriminability](https://arxiv.org/abs/2505.02692)
Append: [Bye-bye, Bluebook? Automating Legal Procedure with Large Language Models](https://arxiv.org/abs/2505.02763)
Append: [ReplaceMe: Network Simplification via Layer Pruning and Linear Transformations](https://arxiv.org/abs/2505.02819)
Append: [Enhancing TCR-Peptide Interaction Prediction with Pretrained Language Models and Molecular Representations](https://arxiv.org/abs/2505.01433)
Append: [AdaParse: An Adaptive Parallel PDF Parsing and Resource Scaling Engine](https://arxiv.org/abs/2505.01435)
Append: [CHORUS: Zero-shot Hierarchical Retrieval and Orchestration for Generating Linear Programming Code](https://arxiv.org/abs/2505.01485)
Append: [Structured Prompting and Feedback-Guided Reasoning with LLMs for Data Interpretation](https://arxiv.org/abs/2505.01636)
Append: [Inducing Robustness in a 2 Dimensional Direct Preference Optimization Paradigm](https://arxiv.org/abs/2505.01706)
Append: [Unraveling Media Perspectives: A Comprehensive Methodology Combining Large Language Models, Topic Modeling, Sentiment Analysis, and Ontology Learning to Analyse Media Bias](https://arxiv.org/abs/2505.01754)
Append: [Enhancing the Learning Experience: Using Vision-Language Models to Generate Questions for Educational Videos](https://arxiv.org/abs/2505.01790)
Append: [Explainability by design: an experimental analysis of the legal coding process](https://arxiv.org/abs/2505.01944)
Append: [A Comprehensive Analysis for Visual Object Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2505.01958)
Append: [Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data](https://arxiv.org/abs/2505.02130)
Append: [Exploring new Approaches for Information Retrieval through Natural Language Processing](https://arxiv.org/abs/2505.02199)
Append: [DNAZEN: Enhanced Gene Sequence Representations via Mixed Granularities of Coding Units](https://arxiv.org/abs/2505.02206)
Append: [Interpretable Emergent Language Using Inter-Agent Transformers](https://arxiv.org/abs/2505.02215)
Append: [Optimizing LLMs for Resource-Constrained Environments: A Survey of Model Compression Techniques](https://arxiv.org/abs/2505.02309)
Append: [Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL](https://arxiv.org/abs/2505.02391)
Append: [Incentivizing Inclusive Contributions in Model Sharing Markets](https://arxiv.org/abs/2505.02462)
Append: [Bielik v3 Small: Technical Report](https://arxiv.org/abs/2505.02550)
Append: [Enhancing Chemical Reaction and Retrosynthesis Prediction with Large Language Model and Dual-task Learning](https://arxiv.org/abs/2505.02639)
Append: [Predicting Movie Hits Before They Happen with LLMs](https://arxiv.org/abs/2505.02693)
Append: [Voila: Voice-Language Foundation Models for Real-Time Autonomous Interaction and Voice Role-Play](https://arxiv.org/abs/2505.02707)
Append: [Using Knowledge Graphs to harvest datasets for efficient CLIP model training](https://arxiv.org/abs/2505.02746)
Append: [Knowing You Don't Know: Learning When to Continue Search in Multi-round RAG through Self-Practicing](https://arxiv.org/abs/2505.02811)
Append: [AutoLibra: Agent Metric Induction from Open-Ended Feedback](https://arxiv.org/abs/2505.02820)
Append: [AOR: Anatomical Ontology-Guided Reasoning for Medical Large Multimodal Model in Chest X-Ray Interpretation](https://arxiv.org/abs/2505.02830)
Append: [R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning](https://arxiv.org/abs/2505.02835)
Append: [Transformadores: Fundamentos teoricos y Aplicaciones](https://arxiv.org/abs/2302.09327)
Append: [SMUTF: Schema Matching Using Generative Tags and Hybrid Features](https://arxiv.org/abs/2402.01685)
Append: [DECIDER: A Dual-System Rule-Controllable Decoding Framework for Language Generation](https://arxiv.org/abs/2403.01954)
Append: [ParaICL: Towards Parallel In-Context Learning](https://arxiv.org/abs/2404.00570)
Append: [Multilingual Brain Surgeon: Large Language Models Can be Compressed Leaving No Language Behind](https://arxiv.org/abs/2404.04748)
Append: [From Human Judgements to Predictive Models: Unravelling Acceptability in Code-Mixed Sentences](https://arxiv.org/abs/2405.05572)
Append: [Large Language Models as Carriers of Hidden Messages](https://arxiv.org/abs/2406.02481)
Append: [LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models](https://arxiv.org/abs/2407.12772)
Append: [A Logical Fallacy-Informed Framework for Argument Generation](https://arxiv.org/abs/2408.03618)
Append: [LipidBERT: A Lipid Language Model Pre-trained on METiS de novo Lipid Library](https://arxiv.org/abs/2408.06150)
Append: [Constructive Approach to Bidirectional Influence between Qualia Structure and Language Emergence](https://arxiv.org/abs/2409.09413)
Append: [ELOQ: Resources for Enhancing LLM Detection of Out-of-Scope Questions](https://arxiv.org/abs/2410.14567)
Append: [LLMs for Extremely Low-Resource Finno-Ugric Languages](https://arxiv.org/abs/2410.18902)
Append: [Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction](https://arxiv.org/abs/2412.04454)
Append: [AD-LLM: Benchmarking Large Language Models for Anomaly Detection](https://arxiv.org/abs/2412.11142)
Append: [ELECTRA and GPT-4o: Cost-Effective Partners for Sentiment Analysis](https://arxiv.org/abs/2501.00062)
Append: [LUSIFER: Language Universal Space Integration for Enhanced Multilingual Embeddings with Large Language Models](https://arxiv.org/abs/2501.00874)
Append: [Towards the Anonymization of the Language Modeling](https://arxiv.org/abs/2501.02407)
Append: [How do Humans and Language Models Reason About Creativity? A Comparative Analysis](https://arxiv.org/abs/2502.03253)
Append: [SpeechT: Findings of the First Mentorship in Speech Translation](https://arxiv.org/abs/2502.12050)
Append: [Almost AI, Almost Human: The Challenge of Detecting AI-Polished Writing](https://arxiv.org/abs/2502.15666)
Append: [Applying LLMs to Active Learning: Towards Cost-Efficient Cross-Task Text Classification without Manually Labeled Data](https://arxiv.org/abs/2502.16892)
Append: [Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs](https://arxiv.org/abs/2502.17424)
Append: [Semantic Volume: Quantifying and Detecting both External and Internal Uncertainty in LLMs](https://arxiv.org/abs/2502.21239)
Append: [A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications](https://arxiv.org/abs/2503.17003)
Append: [A Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well?](https://arxiv.org/abs/2503.24235)
Append: [Noise Augmented Fine Tuning for Mitigating Hallucinations in Large Language Models](https://arxiv.org/abs/2504.03302)
Append: [APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay](https://arxiv.org/abs/2504.03601)
Append: [Better Estimation of the KL Divergence Between Language Models](https://arxiv.org/abs/2504.10637)
Append: [MONOVAB : An Annotated Corpus for Bangla Multi-label Emotion Detection](https://arxiv.org/abs/2309.15670)
Append: [Impact of Noisy Supervision in Foundation Model Learning](https://arxiv.org/abs/2403.06869)
Append: [Tailored Design of Audio-Visual Speech Recognition Models using Branchformers](https://arxiv.org/abs/2407.06606)
Append: [Prompt-Based Cost-Effective Evaluation and Operation of ChatGPT as a Computer Programming Teaching Assistant](https://arxiv.org/abs/2501.17176)
Append: [Unveiling the Mechanisms of Explicit CoT Training: How CoT Enhances Reasoning Generalization](https://arxiv.org/abs/2502.04667)
Append: [EgoNormia: Benchmarking Physical Social Norm Understanding](https://arxiv.org/abs/2502.20490)
Append: [The Effectiveness of Large Language Models in Transforming Unstructured Text to Standardized Formats](https://arxiv.org/abs/2503.02650)
Append: [Dysarthria Normalization via Local Lie Group Transformations for Robust ASR](https://arxiv.org/abs/2504.12279)
append_entries: 118
Finish: 2025-05-06 04:26:11.843811
------------------------------------------------------
Started: 2025-05-06 06:23:59.634339
Existing_entries: 1118
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-06 06:23:59.940227
------------------------------------------------------
Started: 2025-05-06 08:22:21.601435
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-06 08:22:21.904152
------------------------------------------------------
Started: 2025-05-06 10:17:45.917359
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-06 10:17:46.216210
------------------------------------------------------
Started: 2025-05-06 12:35:05.056216
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-06 12:35:05.356908
------------------------------------------------------
Started: 2025-05-06 14:16:47.151709
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-06 14:16:47.449708
------------------------------------------------------
Started: 2025-05-06 16:19:03.156137
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-06 16:19:03.484089
------------------------------------------------------
Started: 2025-05-06 18:22:47.924209
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-06 18:22:48.258386
------------------------------------------------------
Started: 2025-05-06 20:18:16.547581
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-06 20:18:16.881116
------------------------------------------------------
Started: 2025-05-06 22:15:32.302078
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-06 22:15:32.596858
------------------------------------------------------
Started: 2025-05-07 01:18:02.364414
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-07 01:18:02.694941
------------------------------------------------------
Started: 2025-05-07 03:07:36.694136
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-07 03:07:36.996131
------------------------------------------------------
Started: 2025-05-07 04:21:39.613376
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1386
Summarized using GPT-3.5-turbo
Append: [Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in Large Language Models](https://arxiv.org/abs/2505.02847)
Token length: 1956
Summarized using GPT-3.5-turbo
Append: [Harnessing Structured Knowledge: A Concept Map-Based Approach for High-Quality Multiple Choice Question Generation with Effective Distractors](https://arxiv.org/abs/2505.02850)
Token length: 725
Summarized using GPT-3.5-turbo
Append: [30DayGen: Leveraging LLMs to Create a Content Corpus for Habit Formation](https://arxiv.org/abs/2505.02851)
Token length: 1639
Summarized using GPT-3.5-turbo
Append: [Ensuring Reproducibility in Generative AI Systems for General Use Cases: A Framework for Regression Testing and Open Datasets](https://arxiv.org/abs/2505.02854)
Token length: 1470
Summarized using GPT-3.5-turbo
Append: [Towards High-Fidelity Synthetic Multi-platform Social Media Datasets via Large Language Models](https://arxiv.org/abs/2505.02858)
Token length: 844
Summarized using GPT-3.5-turbo
Append: [Enhancing ML Model Interpretability: Leveraging Fine-Tuned Large Language Models for Better Understanding of AI](https://arxiv.org/abs/2505.02859)
Token length: 1236
Summarized using GPT-3.5-turbo
Append: [Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs](https://arxiv.org/abs/2505.02862)
Token length: 1245
Summarized using GPT-3.5-turbo
Append: [Accelerating Large Language Model Reasoning via Speculative Search](https://arxiv.org/abs/2505.02865)
Token length: 1155
Summarized using GPT-3.5-turbo
Append: [Decoding Open-Ended Information Seeking Goals from Eye Movements in Reading](https://arxiv.org/abs/2505.02872)
Token length: 656
Summarized using GPT-3.5-turbo
Append: [Logits-Constrained Framework with RoBERTa for Ancient Chinese NER](https://arxiv.org/abs/2505.02983)
Token length: 1119
Summarized using GPT-3.5-turbo
Append: [RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale](https://arxiv.org/abs/2505.03005)
Token length: 1314
Summarized using GPT-3.5-turbo
Append: [Memorization or Interpolation ? Detecting LLM Memorization through Input Perturbation Analysis](https://arxiv.org/abs/2505.03019)
Token length: 1106
Summarized using GPT-3.5-turbo
Append: [A Typology of Synthetic Datasets for Dialogue Processing in Clinical Contexts](https://arxiv.org/abs/2505.03025)
Token length: 944
Summarized using GPT-3.5-turbo
Append: [UCSC at SemEval-2025 Task 3: Context, Models and Prompt Optimization for Automated Hallucination Detection in LLM Output](https://arxiv.org/abs/2505.03030)
Token length: 1218
Summarized using GPT-3.5-turbo
Append: [Teaching Models to Understand (but not Generate) High-risk Data](https://arxiv.org/abs/2505.03052)
Token length: 921
Summarized using GPT-3.5-turbo
Append: [Developing A Framework to Support Human Evaluation of Bias in Generated Free Response Text](https://arxiv.org/abs/2505.03053)
Token length: 1383
Summarized using GPT-3.5-turbo
Append: [Improving Model Alignment Through Collective Intelligence of Open-Source LLMS](https://arxiv.org/abs/2505.03059)
Token length: 991
Summarized using GPT-3.5-turbo
Append: [Survey of Abstract Meaning Representation: Then, Now, Future](https://arxiv.org/abs/2505.03229)
Token length: 1368
Summarized using GPT-3.5-turbo
Append: [{\Psi}-Arena: Interactive Assessment and Optimization of LLM-based Psychological Counselors with Tripartite Feedback](https://arxiv.org/abs/2505.03293)
Token length: 763
Summarized using GPT-3.5-turbo
Append: [Recall with Reasoning: Chain-of-Thought Distillation for Mamba's Long-Context Memory and Extrapolation](https://arxiv.org/abs/2505.03320)
Token length: 1661
Summarized using GPT-3.5-turbo
Append: [Lightweight Clinical Decision Support System using QLoRA-Fine-Tuned LLMs and Retrieval-Augmented Generation](https://arxiv.org/abs/2505.03406)
Token length: 1283
Summarized using GPT-3.5-turbo
Append: [MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks](https://arxiv.org/abs/2505.03427)
Token length: 996
Summarized using GPT-3.5-turbo
Append: [An Analysis of Hyper-Parameter Optimization Methods for Retrieval Augmented Generation](https://arxiv.org/abs/2505.03452)
Token length: 1306
Summarized using GPT-3.5-turbo
Append: [Uncertainty-Aware Large Language Models for Explainable Disease Diagnosis](https://arxiv.org/abs/2505.03467)
Token length: 1321
Summarized using GPT-3.5-turbo
Append: [Long-Short Chain-of-Thought Mixture Supervised Fine-Tuning Eliciting Efficient Reasoning in Large Language Models](https://arxiv.org/abs/2505.03469)
Token length: 1374
Summarized using GPT-3.5-turbo
Append: [Evaluation of LLMs on Long-tail Entity Linking in Historical Documents](https://arxiv.org/abs/2505.03473)
Token length: 849
Summarized using GPT-3.5-turbo
Append: [Sentence Embeddings as an intermediate target in end-to-end summarisation](https://arxiv.org/abs/2505.03481)
Token length: 1361
Summarized using GPT-3.5-turbo
Append: [Faster MoE LLM Inference for Extremely Large Models](https://arxiv.org/abs/2505.03531)
Token length: 929
Summarized using GPT-3.5-turbo
Append: [Say It Another Way: A Framework for User-Grounded Paraphrasing](https://arxiv.org/abs/2505.03563)
Token length: 922
Summarized using GPT-3.5-turbo
Append: [Towards conversational assistants for health applications: using ChatGPT to generate conversations about heart failure](https://arxiv.org/abs/2505.03675)
Token length: 1227
Summarized using GPT-3.5-turbo
Append: [IndicSQuAD: A Comprehensive Multilingual Question Answering Dataset for Indic Languages](https://arxiv.org/abs/2505.03688)
Token length: 951
Summarized using GPT-3.5-turbo
Append: [NBF at SemEval-2025 Task 5: Light-Burst Attention Enhanced System for Multilingual Subject Recommendation](https://arxiv.org/abs/2505.03711)
Token length: 1784
Summarized using GPT-3.5-turbo
Append: [WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional Websites from Scratch](https://arxiv.org/abs/2505.03733)
Token length: 1579
Summarized using GPT-3.5-turbo
Append: [VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient Large Speech-Language Model](https://arxiv.org/abs/2505.03739)
Token length: 1137
Summarized using GPT-3.5-turbo
Append: [Aligning Large Language Models with Healthcare Stakeholders: A Pathway to Trustworthy AI Integration](https://arxiv.org/abs/2505.02848)
Token length: 779
Summarized using GPT-3.5-turbo
Append: [When Your Own Output Becomes Your Training Data: Noise-to-Meaning Loops and a Formal RSI Trigger](https://arxiv.org/abs/2505.02888)
Token length: 1945
Summarized using GPT-3.5-turbo
Append: [The Art of Repair: Optimizing Iterative Program Repair with Instruction-Tuned Models](https://arxiv.org/abs/2505.02931)
Token length: 947
Summarized using GPT-3.5-turbo
Append: [Iterative Resolution of Prompt Ambiguities Using a Progressive Cutting-Search Approach](https://arxiv.org/abs/2505.02952)
Token length: 692
Summarized using GPT-3.5-turbo
Append: [Radio: Rate-Distortion Optimization for Large Language Model Compression](https://arxiv.org/abs/2505.03031)
Token length: 1804
Summarized using GPT-3.5-turbo
Append: [BLAB: Brutally Long Audio Bench](https://arxiv.org/abs/2505.03054)
Token length: 1256
Summarized using GPT-3.5-turbo
Append: [SepALM: Audio Language Models Are Error Correctors for Robust Speech Separation](https://arxiv.org/abs/2505.03273)
Token length: 1768
Summarized using GPT-3.5-turbo
Append: [Absolute Zero: Reinforced Self-play Reasoning with Zero Data](https://arxiv.org/abs/2505.03335)
Token length: 1145
Summarized using GPT-3.5-turbo
Append: [Enhancing Target-unspecific Tasks through a Features Matrix](https://arxiv.org/abs/2505.03414)
Token length: 974
Summarized using GPT-3.5-turbo
Append: [Elevating Semantic Exploration: A Novel Approach Utilizing Distributed Repositories](https://arxiv.org/abs/2505.03443)
Token length: 1760
Summarized using GPT-3.5-turbo
Append: [BadLingual: A Novel Lingual-Backdoor Attack against Large Language Models](https://arxiv.org/abs/2505.03501)
Token length: 1003
Summarized using GPT-3.5-turbo
Append: [Rational Retrieval Acts: Leveraging Pragmatic Reasoning to Improve Sparse Retrieval](https://arxiv.org/abs/2505.03676)
Token length: 941
Summarized using GPT-3.5-turbo
Append: [Incoherent Probability Judgments in Large Language Models](https://arxiv.org/abs/2401.16646)
Token length: 1035
Summarized using GPT-3.5-turbo
Append: [LLaSA: A Multimodal LLM for Human Activity Analysis Through Wearable and Smartphone Sensors](https://arxiv.org/abs/2406.14498)
Token length: 1484
Summarized using GPT-3.5-turbo
Append: [CFBench: A Comprehensive Constraints-Following Benchmark for LLMs](https://arxiv.org/abs/2408.01122)
Token length: 1787
Summarized using GPT-3.5-turbo
Append: [LLM-3D Print: Large Language Models To Monitor and Control 3D Printing](https://arxiv.org/abs/2408.14307)
Append: [Beyond Single Concept Vector: Modeling Concept Subspace in LLMs with Gaussian Distribution](https://arxiv.org/abs/2410.00153)
Append: [SAPIENT: Mastering Multi-turn Conversational Recommendation with Strategic Planning and Monte Carlo Tree Search](https://arxiv.org/abs/2410.09580)
Append: [Personalization of Large Language Models: A Survey](https://arxiv.org/abs/2411.00027)
Append: [LSAQ: Layer-Specific Adaptive Quantization for Large Language Model Deployment](https://arxiv.org/abs/2412.18135)
Append: [Self-reflecting Large Language Models: A Hegelian Dialectical Approach](https://arxiv.org/abs/2501.14917)
Append: [Applications of Artificial Intelligence for Cross-language Intelligibility Assessment of Dysarthric Speech](https://arxiv.org/abs/2501.15858)
Append: [Predicting potentially abusive clauses in Chilean terms of services with natural language processing](https://arxiv.org/abs/2502.00865)
Append: [MoM: Linear Sequence Modeling with Mixture-of-Memories](https://arxiv.org/abs/2502.13685)
Append: [English Please: Evaluating Machine Translation with Large Language Models for Multilingual Bug Reports](https://arxiv.org/abs/2502.14338)
Append: [BIG-Bench Extra Hard](https://arxiv.org/abs/2502.19187)
Append: [CALLM: Understanding Cancer Survivors' Emotions and Intervention Opportunities via Mobile Diaries and Context-Aware Language Models](https://arxiv.org/abs/2503.10707)
Append: [Towards Hierarchical Multi-Step Reward Models for Enhanced Reasoning in Large Language Models](https://arxiv.org/abs/2503.13551)
Append: [CASE -- Condition-Aware Sentence Embeddings for Conditional Semantic Textual Similarity Measurement](https://arxiv.org/abs/2503.17279)
Append: [HAIR: Hardness-Aware Inverse Reinforcement Learning with Introspective Reasoning for LLM Alignment](https://arxiv.org/abs/2503.18991)
Append: [Clean & Clear: Feasibility of Safe LLM Clinical Guidance](https://arxiv.org/abs/2503.20953)
Append: [SEAL: Steerable Reasoning Calibration of Large Language Models for Free](https://arxiv.org/abs/2504.07986)
Append: [FGAIF: Aligning Large Vision-Language Models with Fine-grained AI Feedback](https://arxiv.org/abs/2404.05046)
Append: [OAC: Output-adaptive Calibration for Accurate Post-training Quantization](https://arxiv.org/abs/2405.15025)
Append: [Language Models Trained to do Arithmetic Predict Human Risky and Intertemporal Choice](https://arxiv.org/abs/2405.19313)
Append: [AudioBench: A Universal Benchmark for Audio Large Language Models](https://arxiv.org/abs/2406.16020)
Append: [Unlearning as multi-task optimization: A normalized gradient difference approach with an adaptive learning rate](https://arxiv.org/abs/2410.22086)
Append: [MATATA: Weakly Supervised End-to-End MAthematical Tool-Augmented Reasoning for Tabular Applications](https://arxiv.org/abs/2411.18915)
Append: [Fourier Position Embedding: Enhancing Attention's Periodic Extension for Length Generalization](https://arxiv.org/abs/2412.17739)
Append: [Music for All: Representational Bias and Cross-Cultural Adaptability of Music Generation Models](https://arxiv.org/abs/2502.07328)
Append: [BRIDGE: Bootstrapping Text to Control Time-Series Generation via Multi-Agent Iterative Optimization and Diffusion Modelling](https://arxiv.org/abs/2503.02445)
Append: [LiteWebAgent: The Open-Source Suite for VLM-Based Web-Agent Applications](https://arxiv.org/abs/2503.02950)
Append: [UI-Vision: A Desktop-centric GUI Benchmark for Visual Perception and Interaction](https://arxiv.org/abs/2503.15661)
Append: [The Devil is in the Prompts: Retrieval-Augmented Prompt Optimization for Text-to-Video Generation](https://arxiv.org/abs/2504.11739)
append_entries: 78
Finish: 2025-05-07 04:23:43.413339
------------------------------------------------------
Started: 2025-05-07 06:23:49.623941
Existing_entries: 1078
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-07 06:23:49.867404
------------------------------------------------------
Started: 2025-05-07 08:23:01.933170
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-07 08:23:02.182829
------------------------------------------------------
Started: 2025-05-07 10:18:03.800220
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-07 10:18:04.018132
------------------------------------------------------
Started: 2025-05-07 12:35:11.670684
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-07 12:35:11.904580
------------------------------------------------------
Started: 2025-05-07 14:17:11.945548
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-07 14:17:12.164682
------------------------------------------------------
Started: 2025-05-07 16:21:15.650555
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-07 16:21:15.918321
------------------------------------------------------
Started: 2025-05-07 18:23:24.636034
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-07 18:23:24.898600
------------------------------------------------------
Started: 2025-05-07 20:18:26.448800
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-07 20:18:26.669037
------------------------------------------------------
Started: 2025-05-07 22:16:09.694027
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-07 22:16:09.912166
------------------------------------------------------
Started: 2025-05-08 01:18:41.723176
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-08 01:18:41.945235
------------------------------------------------------
Started: 2025-05-08 03:11:21.572356
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-08 03:11:21.804024
------------------------------------------------------
Started: 2025-05-08 04:25:11.789348
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1373
Summarized using GPT-3.5-turbo
Append: [Calibrating Uncertainty Quantification of Multi-Modal LLMs using Grounding](https://arxiv.org/abs/2505.03788)
Token length: 1280
Summarized using GPT-3.5-turbo
Append: [Hesitation is defeat? Connecting Linguistic and Predictive Uncertainty](https://arxiv.org/abs/2505.03910)
Token length: 923
Summarized using GPT-3.5-turbo
Append: [A Reasoning-Focused Legal Retrieval Benchmark](https://arxiv.org/abs/2505.03970)
Token length: 1111
Summarized using GPT-3.5-turbo
Append: [Divide, Optimize, Merge: Fine-Grained LLM Agent Optimization at Scale](https://arxiv.org/abs/2505.03973)
Token length: 1600
Summarized using GPT-3.5-turbo
Append: [X-Reasoner: Towards Generalizable Reasoning Across Modalities and Domains](https://arxiv.org/abs/2505.03981)
Token length: 1428
Summarized using GPT-3.5-turbo
Append: [SLOT: Structuring the Output of Large Language Models](https://arxiv.org/abs/2505.04016)
Token length: 1288
Summarized using GPT-3.5-turbo
Append: [Advancing and Benchmarking Personalized Tool Invocation for LLMs](https://arxiv.org/abs/2505.04072)
Token length: 1386
Summarized using GPT-3.5-turbo
Append: [Natural Language Generation in Healthcare: A Review of Methods and Applications](https://arxiv.org/abs/2505.04073)
Token length: 1894
Summarized using GPT-3.5-turbo
Append: [Bringing legal knowledge to the public by constructing a legal question bank using large-scale pre-trained language model](https://arxiv.org/abs/2505.04132)
Token length: 652
Summarized using GPT-3.5-turbo
Append: [Enhancing Granular Sentiment Classification with Chain-of-Thought Prompting in Large Language Models](https://arxiv.org/abs/2505.04135)
Token length: 1345
Summarized using GPT-3.5-turbo
Append: [Unmasking the Canvas: A Dynamic Benchmark for Image Generation Jailbreaking and LLM Content Safety](https://arxiv.org/abs/2505.04146)
Token length: 1477
Summarized using GPT-3.5-turbo
Append: [Can Language Models Understand Social Behavior in Clinical Conversations?](https://arxiv.org/abs/2505.04152)
Token length: 895
Summarized using GPT-3.5-turbo
Append: [LLM-Independent Adaptive RAG: Let the Question Speak for Itself](https://arxiv.org/abs/2505.04253)
Token length: 1955
Summarized using GPT-3.5-turbo
Append: [GASCADE: Grouped Summarization of Adverse Drug Event for Enhanced Cancer Pharmacovigilance](https://arxiv.org/abs/2505.04284)
Token length: 1905
Summarized using GPT-3.5-turbo
Append: [The Aloe Family Recipe for Open and Specialized Healthcare LLMs](https://arxiv.org/abs/2505.04388)
Token length: 1682
Summarized using GPT-3.5-turbo
Append: [Large Means Left: Political Bias in Large Language Models Increases with Their Number of Parameters](https://arxiv.org/abs/2505.04393)
Token length: 1307
Summarized using GPT-3.5-turbo
Append: [YABLoCo: Yet Another Benchmark for Long Context Code Generation](https://arxiv.org/abs/2505.04406)
Token length: 991
Summarized using GPT-3.5-turbo
Append: [OBLIVIATE: Robust and Practical Machine Unlearning for Large Language Models](https://arxiv.org/abs/2505.04416)
Token length: 1192
Summarized using GPT-3.5-turbo
Append: [Detecting Spelling and Grammatical Anomalies in Russian Poetry Texts](https://arxiv.org/abs/2505.04507)
Token length: 1644
Summarized using GPT-3.5-turbo
Append: [Pangu Ultra MoE: How to Train Your Big MoE on Ascend NPUs](https://arxiv.org/abs/2505.04519)
Token length: 1542
Summarized using GPT-3.5-turbo
Append: [Overcoming Data Scarcity in Generative Language Modelling for Low-Resource Languages: A Systematic Review](https://arxiv.org/abs/2505.04531)
Token length: 1878
Summarized using GPT-3.5-turbo
Append: [ZeroSearch: Incentivize the Search Capability of LLMs without Searching](https://arxiv.org/abs/2505.04588)
Token length: 1696
Summarized using GPT-3.5-turbo
Append: [When Reasoning Beats Scale: A 1.5B Reasoning Model Outranks 13B LLMs as Discriminator](https://arxiv.org/abs/2505.03786)
Token length: 1684
Summarized using GPT-3.5-turbo
Append: [Scalability Matters: Overcoming Challenges in InstructGLM with Similarity-Degree-Based Sampling](https://arxiv.org/abs/2505.03799)
Token length: 1114
Summarized using GPT-3.5-turbo
Append: [Grouped Sequency-arranged Rotation: Optimizing Rotation Transformation for Quantization for Free](https://arxiv.org/abs/2505.03810)
Token length: 1265
Summarized using GPT-3.5-turbo
Append: [Cer-Eval: Certifiable and Cost-Efficient Evaluation Framework for LLMs](https://arxiv.org/abs/2505.03814)
Token length: 1361
Summarized using GPT-3.5-turbo
Append: [Sentiment-Aware Recommendation Systems in E-Commerce: A Review from a Natural Language Processing Perspective](https://arxiv.org/abs/2505.03828)
Token length: 1183
Summarized using GPT-3.5-turbo
Append: [The Power of Stories: Narrative Priming Shapes How LLM Agents Collaborate and Compete](https://arxiv.org/abs/2505.03961)
Token length: 959
Summarized using GPT-3.5-turbo
Append: [Quiet Feature Learning in Algorithmic Tasks](https://arxiv.org/abs/2505.03997)
Token length: 1198
Summarized using GPT-3.5-turbo
Append: [LLAMAPIE: Proactive In-Ear Conversation Assistants](https://arxiv.org/abs/2505.04066)
Token length: 1312
Summarized using GPT-3.5-turbo
Append: [Large Language Models are often politically extreme, usually ideologically inconsistent, and persuasive even in informational contexts](https://arxiv.org/abs/2505.04171)
Token length: 1380
Summarized using GPT-3.5-turbo
Append: [VideoPath-LLaVA: Pathology Diagnostic Reasoning Through Video Instruction Tuning](https://arxiv.org/abs/2505.04192)
Token length: 1945
Summarized using GPT-3.5-turbo
Append: [Benchmarking LLMs' Swarm intelligence](https://arxiv.org/abs/2505.04364)
Token length: 1442
Summarized using GPT-3.5-turbo
Append: [Miipher-2: A Universal Speech Restoration Model for Million-Hour Scale Data Restoration](https://arxiv.org/abs/2505.04457)
Token length: 1442
Summarized using GPT-3.5-turbo
Append: [Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal Problem-Solving](https://arxiv.org/abs/2505.04528)
Token length: 1082
Summarized using GPT-3.5-turbo
Append: [Playing repeated games with Large Language Models](https://arxiv.org/abs/2305.16867)
Token length: 1315
Summarized using GPT-3.5-turbo
Append: [Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models](https://arxiv.org/abs/2308.15022)
Token length: 1007
Summarized using GPT-3.5-turbo
Append: [Large Language Models Are Struggle to Cope with Unreasonability in Math Problems](https://arxiv.org/abs/2403.19346)
Token length: 1865
Summarized using GPT-3.5-turbo
Append: [Re-ReST: Reflection-Reinforced Self-Training for Language Agents](https://arxiv.org/abs/2406.01495)
Token length: 1369
Summarized using GPT-3.5-turbo
Append: [Towards Universal and Black-Box Query-Response Only Attack on LLMs with QROA](https://arxiv.org/abs/2406.02044)
Token length: 1417
Summarized using GPT-3.5-turbo
Append: [Is In-Context Learning a Type of Error-Driven Learning? Evidence from the Inverse Frequency Effect in Structural Priming](https://arxiv.org/abs/2406.18501)
Token length: 1611
Summarized using GPT-3.5-turbo
Append: [Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating the Hallucination for Path Planning](https://arxiv.org/abs/2408.13184)
Token length: 1719
Summarized using GPT-3.5-turbo
Append: [Advancements and limitations of LLMs in replicating human color-word associations](https://arxiv.org/abs/2411.02116)
Token length: 1910
Summarized using GPT-3.5-turbo
Append: [SWE-Fixer: Training Open-Source LLMs for Effective and Efficient GitHub Issue Resolution](https://arxiv.org/abs/2501.05040)
Token length: 1201
Summarized using GPT-3.5-turbo
Append: [Estimating LLM Uncertainty with Logits](https://arxiv.org/abs/2502.00290)
Token length: 1687
Summarized using GPT-3.5-turbo
Append: [Liger: Linearizing Large Language Models to Gated Recurrent Structures](https://arxiv.org/abs/2503.01496)
Token length: 1534
Summarized using GPT-3.5-turbo
Append: [Designing Speech Technologies for Australian Aboriginal English: Opportunities, Risks and Participation](https://arxiv.org/abs/2503.03186)
Token length: 1525
Summarized using GPT-3.5-turbo
Append: [High-Dimensional Interlingual Representations of Large Language Models](https://arxiv.org/abs/2503.11280)
Token length: 745
Summarized using GPT-3.5-turbo
Append: [OAEI-LLM-T: A TBox Benchmark Dataset for Understanding Large Language Model Hallucinations in Ontology Matching](https://arxiv.org/abs/2503.21813)
Token length: 1299
Summarized using GPT-3.5-turbo
Append: [RaDialog: A Large Vision-Language Model for Radiology Report Generation and Conversational Assistance](https://arxiv.org/abs/2311.18681)
Append: [Boosting Masked ECG-Text Auto-Encoders as Discriminative Learners](https://arxiv.org/abs/2410.02131)
Append: [Vision-Language Models Create Cross-Modal Task Representations](https://arxiv.org/abs/2410.22330)
Append: [LLM2CLIP: Powerful Language Model Unlocks Richer Visual Representation](https://arxiv.org/abs/2411.04997)
Append: [Cyclic Vision-Language Manipulator: Towards Reliable and Fine-Grained Image Interpretation for Automated Report Generation](https://arxiv.org/abs/2411.05261)
Append: [Automated Coding of Communications in Collaborative Problem-solving Tasks Using ChatGPT](https://arxiv.org/abs/2411.10246)
Append: [SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild](https://arxiv.org/abs/2503.18892)
append_entries: 56
Finish: 2025-05-08 04:27:25.060534
------------------------------------------------------
Started: 2025-05-08 06:24:50.458891
Existing_entries: 1056
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-08 06:24:50.651650
------------------------------------------------------
Started: 2025-05-08 08:21:53.195102
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-08 08:21:53.365599
------------------------------------------------------
Started: 2025-05-08 10:18:17.035680
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-08 10:18:17.232308
------------------------------------------------------
Started: 2025-05-08 12:33:06.354143
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-08 12:33:06.529342
------------------------------------------------------
Started: 2025-05-08 14:15:00.054341
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-08 14:15:00.260603
------------------------------------------------------
Started: 2025-05-08 16:21:01.117397
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-08 16:21:01.305577
------------------------------------------------------
Started: 2025-05-08 18:22:58.306924
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-08 18:22:58.483787
------------------------------------------------------
Started: 2025-05-08 20:18:32.451493
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-08 20:18:32.642931
------------------------------------------------------
Started: 2025-05-08 22:15:42.386952
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-08 22:15:42.592826
------------------------------------------------------
Started: 2025-05-09 01:17:54.167988
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-09 01:17:54.356910
------------------------------------------------------
Started: 2025-05-09 03:07:40.451032
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-09 03:07:40.674312
------------------------------------------------------
Started: 2025-05-09 04:30:56.020834
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1659
Summarized using GPT-3.5-turbo
Append: [How Social is It? A Benchmark for LLMs' Capabilities in Multi-user Multi-turn Social Agent Tasks](https://arxiv.org/abs/2505.04628)
Token length: 1524
Summarized using GPT-3.5-turbo
Append: [Adaptive Token Boundaries: Integrating Human Chunking Mechanisms into Multimodal LLMs](https://arxiv.org/abs/2505.04637)
Token length: 1216
Summarized using GPT-3.5-turbo
Append: [Language translation, and change of accent for speech-to-speech task using diffusion model](https://arxiv.org/abs/2505.04639)
Token length: 872
Summarized using GPT-3.5-turbo
Append: [A Comparative Benchmark of a Moroccan Darija Toxicity Detection Model (Typica.ai) and Major LLM-Based Moderation APIs (OpenAI, Mistral, Anthropic)](https://arxiv.org/abs/2505.04640)
Token length: 1192
Summarized using GPT-3.5-turbo
Append: [Rethinking Multimodal Sentiment Analysis: A High-Accuracy, Simplified Fusion Architecture](https://arxiv.org/abs/2505.04642)
Token length: 863
Summarized using GPT-3.5-turbo
Append: [Prediction-powered estimators for finite population statistics in highly imbalanced textual data: Public hate crime estimation](https://arxiv.org/abs/2505.04643)
Token length: 1625
Summarized using GPT-3.5-turbo
Append: [ChatGPT for automated grading of short answer questions in mechanical ventilation](https://arxiv.org/abs/2505.04645)
Token length: 1494
Summarized using GPT-3.5-turbo
Append: [FRAME: Feedback-Refined Agent Methodology for Enhancing Medical Research Insights](https://arxiv.org/abs/2505.04649)
Token length: 1381
Summarized using GPT-3.5-turbo
Append: [Scientific Hypothesis Generation and Validation: Methods, Datasets, and Future Directions](https://arxiv.org/abs/2505.04651)
Token length: 1969
Summarized using GPT-3.5-turbo
Append: [Advancing Conversational Diagnostic AI with Multimodal Reasoning](https://arxiv.org/abs/2505.04653)
Token length: 854
Summarized using GPT-3.5-turbo
Append: [A Comparative Analysis of Ethical and Safety Gaps in LLMs using Relative Danger Coefficient](https://arxiv.org/abs/2505.04654)
Token length: 1222
Summarized using GPT-3.5-turbo
Append: [Integration of Large Language Models and Traditional Deep Learning for Social Determinants of Health Prediction](https://arxiv.org/abs/2505.04655)
Token length: 1519
Summarized using GPT-3.5-turbo
Append: [AI-Generated Fall Data: Assessing LLMs and Diffusion Model for Wearable Fall Detection](https://arxiv.org/abs/2505.04660)
Token length: 1779
Summarized using GPT-3.5-turbo
Append: [Personalized Risks and Regulatory Strategies of Large Language Models in Digital Advertising](https://arxiv.org/abs/2505.04665)
Token length: 1945
Summarized using GPT-3.5-turbo
Append: [Fine-Tuning Large Language Models and Evaluating Retrieval Methods for Improved Question Answering on Building Codes](https://arxiv.org/abs/2505.04666)
Token length: 1652
Summarized using GPT-3.5-turbo
Append: [Reward-SQL: Boosting Text-to-SQL via Stepwise Reasoning and Process-Supervised Rewards](https://arxiv.org/abs/2505.04671)
Token length: 1697
Summarized using GPT-3.5-turbo
Append: [REVEAL: Multi-turn Evaluation of Image-Input Harms for Vision LLM](https://arxiv.org/abs/2505.04673)
Token length: 1239
Summarized using GPT-3.5-turbo
Append: [Advanced Deep Learning Approaches for Automated Recognition of Cuneiform Symbols](https://arxiv.org/abs/2505.04678)
Token length: 1781
Summarized using GPT-3.5-turbo
Append: [SOAEsV2-7B/72B: Full-Pipeline Optimization for State-Owned Enterprise LLMs via Continual Pre-Training, Domain-Progressive SFT and Distillation-Enhanced Speculative Decoding](https://arxiv.org/abs/2505.04723)
Token length: 1215
Summarized using GPT-3.5-turbo
Append: [Flower Across Time and Media: Sentiment Analysis of Tang Song Poetry and Visual Correspondence](https://arxiv.org/abs/2505.04785)
Token length: 1106
Summarized using GPT-3.5-turbo
Append: [Osiris: A Lightweight Open-Source Hallucination Detection System](https://arxiv.org/abs/2505.04844)
Token length: 1261
Summarized using GPT-3.5-turbo
Append: [Benchmarking LLM Faithfulness in RAG with Evolving Leaderboards](https://arxiv.org/abs/2505.04847)
Token length: 1661
Summarized using GPT-3.5-turbo
Append: [An Open-Source Dual-Loss Embedding Model for Semantic Retrieval in Higher Education](https://arxiv.org/abs/2505.04916)
Token length: 1161
Summarized using GPT-3.5-turbo
Append: [Chain-of-Thought Tokens are Computer Program Variables](https://arxiv.org/abs/2505.04955)
Token length: 1355
Summarized using GPT-3.5-turbo
Append: [Rethinking the Relationship between the Power Law and Hierarchical Structures](https://arxiv.org/abs/2505.04984)
Token length: 1533
Summarized using GPT-3.5-turbo
Append: [Latent Preference Coding: Aligning Large Language Models via Discrete Latent Codes](https://arxiv.org/abs/2505.04993)
Token length: 1146
Summarized using GPT-3.5-turbo
Append: [Rethinking Invariance in In-context Learning](https://arxiv.org/abs/2505.04994)
Token length: 1714
Summarized using GPT-3.5-turbo
Append: [The Pitfalls of Growing Group Complexity: LLMs and Social Choice-Based Aggregation for Group Recommendations](https://arxiv.org/abs/2505.05016)
Token length: 1311
Summarized using GPT-3.5-turbo
Append: [Scalable Multi-Stage Influence Function for Large Language Models via Eigenvalue-Corrected Kronecker-Factored Parameterization](https://arxiv.org/abs/2505.05017)
Token length: 1337
Summarized using GPT-3.5-turbo
Append: [G-FOCUS: Towards a Robust Method for Assessing UI Design Persuasiveness](https://arxiv.org/abs/2505.05026)
Token length: 731
Summarized using GPT-3.5-turbo
Append: [Image-Text Relation Prediction for Multilingual Tweets](https://arxiv.org/abs/2505.05040)
Token length: 788
Summarized using GPT-3.5-turbo
Append: [Teochew-Wild: The First In-the-wild Teochew Dataset with Orthographic Annotations](https://arxiv.org/abs/2505.05056)
Token length: 1084
Summarized using GPT-3.5-turbo
Append: [Performance Evaluation of Large Language Models in Bangla Consumer Health Query Summarization](https://arxiv.org/abs/2505.05070)
Token length: 1307
Summarized using GPT-3.5-turbo
Append: [Reliably Bounding False Positives: A Zero-Shot Machine-Generated Text Detection Framework via Multiscaled Conformal Prediction](https://arxiv.org/abs/2505.05084)
Token length: 1108
Summarized using GPT-3.5-turbo
Append: [Unveiling Language-Specific Features in Large Language Models via Sparse Autoencoders](https://arxiv.org/abs/2505.05111)
Token length: 1287
Summarized using GPT-3.5-turbo
Append: [A Benchmark Dataset and a Framework for Urdu Multimodal Named Entity Recognition](https://arxiv.org/abs/2505.05148)
Token length: 1295
Summarized using GPT-3.5-turbo
Append: [QualBench: Benchmarking Chinese LLMs with Localized Professional Qualifications for Vertical Domain Evaluation](https://arxiv.org/abs/2505.05225)
Token length: 1704
Summarized using GPT-3.5-turbo
Append: [T-T: Table Transformer for Tagging-based Aspect Sentiment Triplet Extraction](https://arxiv.org/abs/2505.05271)
Token length: 873
Summarized using GPT-3.5-turbo
Append: [Toward Reasonable Parrots: Why Large Language Models Should Argue with Us by Design](https://arxiv.org/abs/2505.05298)
Token length: 1467
Summarized using GPT-3.5-turbo
Append: [ICon: In-Context Contribution for Automatic Data Selection](https://arxiv.org/abs/2505.05327)
Token length: 997
Summarized using GPT-3.5-turbo
Append: [Frame In, Frame Out: Do LLMs Generate More Biased News Headlines than Humans?](https://arxiv.org/abs/2505.05406)
Token length: 1450
Summarized using GPT-3.5-turbo
Append: [Crosslingual Reasoning through Test-Time Scaling](https://arxiv.org/abs/2505.05408)
Token length: 1278
Summarized using GPT-3.5-turbo
Append: [Reasoning Models Don't Always Say What They Think](https://arxiv.org/abs/2505.05410)
Token length: 1759
Summarized using GPT-3.5-turbo
Append: [TransProQA: an LLM-based literary Translation evaluation metric with Professional Question Answering](https://arxiv.org/abs/2505.05423)
Token length: 1870
Summarized using GPT-3.5-turbo
Append: [Ultra-FineWeb: Efficient Data Filtering and Verification for High-Quality LLM Training Data](https://arxiv.org/abs/2505.05427)
Token length: 1380
Summarized using GPT-3.5-turbo
Append: [clem:todd: A Framework for the Systematic Benchmarking of LLM-Based Task-Oriented Dialogue System Realisations](https://arxiv.org/abs/2505.05445)
Token length: 897
Summarized using GPT-3.5-turbo
Append: [UKElectionNarratives: A Dataset of Misleading Narratives Surrounding Recent UK General Elections](https://arxiv.org/abs/2505.05459)
Token length: 1379
Summarized using GPT-3.5-turbo
Append: [Bring Reason to Vision: Understanding Perception and Reasoning through Model Merging](https://arxiv.org/abs/2505.05464)
Token length: 1312
Summarized using GPT-3.5-turbo
Append: [ComPO: Preference Alignment via Comparison Oracles](https://arxiv.org/abs/2505.05465)
Token length: 877
Summarized using GPT-3.5-turbo
Append: [From Dialect Gaps to Identity Maps: Tackling Variability in Speaker Verification](https://arxiv.org/abs/2505.04629)
Append: [Towards Artificial Intelligence Research Assistant for Expert-Involved Learning](https://arxiv.org/abs/2505.04638)
Append: [When Bad Data Leads to Good Models](https://arxiv.org/abs/2505.04741)
Append: [Red Teaming the Mind of the Machine: A Systematic Evaluation of Prompt Injection and Jailbreak Vulnerabilities in LLMs](https://arxiv.org/abs/2505.04806)
Append: [HiPerRAG: High-Performance Retrieval Augmented Generation for Scientific Insights](https://arxiv.org/abs/2505.04846)
Append: [CRAFT: Cultural Russian-Oriented Dataset Adaptation for Focused Text-to-Image Generation](https://arxiv.org/abs/2505.04851)
Append: [ConCISE: Confidence-guided Compression in Step-by-step Efficient Reasoning](https://arxiv.org/abs/2505.04881)
Append: [SpatialPrompting: Keyframe-driven Zero-Shot Spatial Reasoning with Off-the-Shelf Multimodal Large Language Models](https://arxiv.org/abs/2505.04911)
Append: [Enigme: Generative Text Puzzles for Evaluating Reasoning in Language Models](https://arxiv.org/abs/2505.04914)
Append: [Perception, Reason, Think, and Plan: A Survey on Large Multimodal Reasoning Models](https://arxiv.org/abs/2505.04921)
Append: [T2VTextBench: A Human Evaluation Benchmark for Textual Control in Video Generation Models](https://arxiv.org/abs/2505.04946)
Append: [Prompt-Based LLMs for Position Bias-Aware Reranking in Personalized Recommendations](https://arxiv.org/abs/2505.04948)
Append: [General Transform: A Unified Framework for Adaptive Transform to Enhance Representations](https://arxiv.org/abs/2505.04969)
Append: [CodeMixBench: Evaluating Large Language Models on Code Generation with Code-Mixed Prompts](https://arxiv.org/abs/2505.05063)
Append: [X-Driver: Explainable Autonomous Driving with Vision-Language Models](https://arxiv.org/abs/2505.05098)
Append: [Understanding In-context Learning of Addition via Activation Subspaces](https://arxiv.org/abs/2505.05145)
Append: [Revealing Weaknesses in Text Watermarking Through Self-Information Rewrite Attacks](https://arxiv.org/abs/2505.05190)
Append: [Scalable Chain of Thoughts via Elastic Reasoning](https://arxiv.org/abs/2505.05315)
Append: [TokLIP: Marry Visual Tokens to CLIP for Multimodal Comprehension and Generation](https://arxiv.org/abs/2505.05422)
Append: [Adaptive Markup Language Generation for Contextually-Grounded Visual Document Understanding](https://arxiv.org/abs/2505.05446)
Append: [StreamBridge: Turning Your Offline Video Large Language Model into a Proactive Streaming Assistant](https://arxiv.org/abs/2505.05467)
Append: [Position: AI Evaluation Should Learn from How We Test Humans](https://arxiv.org/abs/2306.10512)
Append: [DEGAP: Dual Event-Guided Adaptive Prefixes for Templated-Based Event Argument Extraction with Slot Querying](https://arxiv.org/abs/2405.13325)
Append: [Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted Phenomenon](https://arxiv.org/abs/2406.17746)
Append: [Scaling Synthetic Data Creation with 1,000,000,000 Personas](https://arxiv.org/abs/2406.20094)
Append: [Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and Model Size in Large Language Models From Edge to Giant](https://arxiv.org/abs/2409.11055)
Append: [To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning](https://arxiv.org/abs/2409.12183)
Append: [Self-Correction is More than Refinement: A Learning Framework for Visual and Language Reasoning Tasks](https://arxiv.org/abs/2410.04055)
Append: [WorldCuisines: A Massive-Scale Benchmark for Multilingual and Multicultural Visual Question Answering on Global Cuisines](https://arxiv.org/abs/2410.12705)
Append: [E2E-AFG: An End-to-End Model with Adaptive Filtering for Retrieval-Augmented Generation](https://arxiv.org/abs/2411.00437)
Append: [Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models](https://arxiv.org/abs/2411.04996)
Append: [ValuesRAG: Enhancing Cultural Alignment Through Retrieval-Augmented Contextual Learning](https://arxiv.org/abs/2501.01031)
Append: [Communicating Activations Between Language Model Agents](https://arxiv.org/abs/2501.14082)
Append: [Safety Evaluation of DeepSeek Models in Chinese Contexts](https://arxiv.org/abs/2502.11137)
Append: [Drift: Decoding-time Personalized Alignments with Implicit User Preferences](https://arxiv.org/abs/2502.14289)
Append: [Correctness Coverage Evaluation for Medical Multiple-Choice Question Answering Based on the Enhanced Conformal Prediction Framework](https://arxiv.org/abs/2503.05505)
Append: [Large Language Models for Outpatient Referral: Problem Definition, Benchmarking and Challenges](https://arxiv.org/abs/2503.08292)
Append: [Atyaephyra at SemEval-2025 Task 4: Low-Rank Negative Preference Optimization](https://arxiv.org/abs/2503.13690)
Append: [Benchmarking Open-Source Large Language Models on Healthcare Text Classification Tasks](https://arxiv.org/abs/2503.15169)
Append: [Combating Confirmation Bias: A Unified Pseudo-Labeling Framework for Entity Alignment](https://arxiv.org/abs/2307.02075)
Append: [HORAE: A Domain-Agnostic Language for Automated Service Regulation](https://arxiv.org/abs/2406.06600)
Append: [Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding](https://arxiv.org/abs/2409.03757)
Append: [Quantifying Risk Propensities of Large Language Models: Ethical Focus and Bias Detection through Role-Play](https://arxiv.org/abs/2411.08884)
Append: [Faster, Cheaper, Better: Multi-Objective Hyperparameter Optimization for LLM and RAG Systems](https://arxiv.org/abs/2502.18635)
Append: [Re-evaluating Open-ended Evaluation of Large Language Models](https://arxiv.org/abs/2502.20170)
Append: [TiC-LM: A Web-Scale Benchmark for Time-Continual LLM Pretraining](https://arxiv.org/abs/2504.02107)
Append: [Accurate and Diverse LLM Mathematical Reasoning via Automated PRM-Guided GFlowNets](https://arxiv.org/abs/2504.19981)
append_entries: 96
Finish: 2025-05-09 04:36:36.837588
------------------------------------------------------
Started: 2025-05-09 06:24:29.845477
Existing_entries: 1096
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1724
Summarized using GPT-3.5-turbo
Append: [TCAN: Text-oriented Cross Attention Network for Multimodal Sentiment Analysis](https://arxiv.org/abs/2404.04545)
append_entries: 1
Finish: 2025-05-09 06:24:39.843162
------------------------------------------------------
Started: 2025-05-09 08:21:41.261413
Existing_entries: 1001
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-09 08:21:41.534147
------------------------------------------------------
Started: 2025-05-09 10:17:26.988457
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-09 10:17:27.239872
------------------------------------------------------
Started: 2025-05-09 12:32:30.636984
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-09 12:32:30.888378
------------------------------------------------------
Started: 2025-05-09 14:16:10.248188
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-09 14:16:10.497113
------------------------------------------------------
Started: 2025-05-09 16:20:05.494920
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-09 16:20:05.834049
------------------------------------------------------
Started: 2025-05-09 18:22:28.070092
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-09 18:22:28.352192
------------------------------------------------------
Started: 2025-05-09 20:17:58.394100
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-09 20:17:58.730561
------------------------------------------------------
Started: 2025-05-09 22:15:19.357945
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-09 22:15:19.637458
------------------------------------------------------
Started: 2025-05-10 01:15:15.764777
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-10 01:15:16.048224
------------------------------------------------------
Started: 2025-05-10 03:01:31.343550
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-10 03:01:31.690190
------------------------------------------------------
Started: 2025-05-10 04:18:20.956078
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-10 04:18:21.014676
------------------------------------------------------
Started: 2025-05-10 06:20:29.195808
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-10 06:20:29.287884
------------------------------------------------------
Started: 2025-05-10 08:19:00.888975
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-10 08:19:00.955350
------------------------------------------------------
Started: 2025-05-10 10:15:51.112993
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-10 10:15:51.172193
------------------------------------------------------
Started: 2025-05-10 12:29:21.996055
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-10 12:29:22.090206
------------------------------------------------------
Started: 2025-05-10 14:13:36.155397
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-10 14:13:36.265321
------------------------------------------------------
Started: 2025-05-10 16:18:20.922105
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-10 16:18:20.986543
------------------------------------------------------
Started: 2025-05-10 18:19:50.477117
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-10 18:19:50.552951
------------------------------------------------------
Started: 2025-05-10 20:16:11.045855
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-10 20:16:11.106975
------------------------------------------------------
Started: 2025-05-10 22:14:06.690930
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-10 22:14:06.823538
------------------------------------------------------
Started: 2025-05-11 01:22:45.037232
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-11 01:22:45.097800
------------------------------------------------------
Started: 2025-05-11 03:12:35.847149
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-11 03:12:35.963953
------------------------------------------------------
Started: 2025-05-11 04:19:17.590073
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-11 04:19:17.741930
------------------------------------------------------
Started: 2025-05-11 06:21:36.442033
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-11 06:21:36.551911
------------------------------------------------------
Started: 2025-05-11 08:19:26.124793
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-11 08:19:26.194978
------------------------------------------------------
Started: 2025-05-11 10:15:21.828016
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-11 10:15:21.895291
------------------------------------------------------
Started: 2025-05-11 12:29:50.747601
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-11 12:29:50.874298
------------------------------------------------------
Started: 2025-05-11 14:14:05.915839
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-11 14:14:06.026804
------------------------------------------------------
Started: 2025-05-11 16:18:10.414755
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-11 16:18:10.479890
------------------------------------------------------
Started: 2025-05-11 18:20:18.644728
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-11 18:20:18.706304
------------------------------------------------------
Started: 2025-05-11 20:16:23.343317
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-11 20:16:23.401726
------------------------------------------------------
Started: 2025-05-11 22:14:52.226102
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-11 22:14:52.308179
------------------------------------------------------
Started: 2025-05-12 01:20:55.561761
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-12 01:20:55.681904
------------------------------------------------------
Started: 2025-05-12 03:11:58.255368
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-12 03:11:58.318219
------------------------------------------------------
Started: 2025-05-12 04:25:06.023151
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1462
Summarized using GPT-3.5-turbo
Append: [KG-HTC: Integrating Knowledge Graphs into LLMs for Effective Zero-shot Hierarchical Text Classification](https://arxiv.org/abs/2505.05583)
Token length: 692
Summarized using GPT-3.5-turbo
Append: [Privacy-Preserving Transformers: SwiftKey's Differential Privacy Implementation](https://arxiv.org/abs/2505.05648)
Token length: 1183
Summarized using GPT-3.5-turbo
Append: [Exploration of COVID-19 Discourse on Twitter: American Politician Edition](https://arxiv.org/abs/2505.05687)
Token length: 1416
Summarized using GPT-3.5-turbo
Append: [Assessing Robustness to Spurious Correlations in Post-Training Language Models](https://arxiv.org/abs/2505.05704)
Token length: 1492
Summarized using GPT-3.5-turbo
Append: [TopicVD: A Topic-Based Dataset of Video-Guided Multimodal Machine Translation for Documentaries](https://arxiv.org/abs/2505.05714)
Token length: 1592
Summarized using GPT-3.5-turbo
Append: [Insertion Language Models: Sequence Generation with Arbitrary-Position Insertions](https://arxiv.org/abs/2505.05755)
Token length: 1881
Summarized using GPT-3.5-turbo
Append: [Sparse Attention Remapping with Clustering for Efficient LLM Decoding on PIM](https://arxiv.org/abs/2505.05772)
Token length: 1187
Summarized using GPT-3.5-turbo
Append: [Tell Me Who Your Students Are: GPT Can Generate Valid Multiple-Choice Questions When Students' (Mis)Understanding Is Hinted](https://arxiv.org/abs/2505.05815)
Token length: 1354
Summarized using GPT-3.5-turbo
Append: [Symbol-based entity marker highlighting for enhanced text mining in materials science with generative AI](https://arxiv.org/abs/2505.05864)
Token length: 776
Summarized using GPT-3.5-turbo
Append: [Elastic Weight Consolidation for Full-Parameter Continual Pre-Training of Gemma2](https://arxiv.org/abs/2505.05946)
Token length: 727
Summarized using GPT-3.5-turbo
Append: [Summarisation of German Judgments in conjunction with a Class-based Evaluation](https://arxiv.org/abs/2505.05947)
Token length: 1419
Summarized using GPT-3.5-turbo
Append: [NeoQA: Evidence-based Question Answering with Generated News Events](https://arxiv.org/abs/2505.05949)
Token length: 1187
Summarized using GPT-3.5-turbo
Append: [Towards Developmentally Plausible Rewards: Communicative Success as a Learning Signal for Interactive Language Models](https://arxiv.org/abs/2505.05970)
Token length: 1689
Summarized using GPT-3.5-turbo
Append: [An Exploratory Analysis on the Explanatory Potential of Embedding-Based Measures of Semantic Transparency for Malay Word Recognition](https://arxiv.org/abs/2505.05973)
Token length: 882
Summarized using GPT-3.5-turbo
Append: [Exploring the Feasibility of Multilingual Grammatical Error Correction with a Single LLM up to 9B parameters: A Comparative Study of 17 Models](https://arxiv.org/abs/2505.06004)
Token length: 1036
Summarized using GPT-3.5-turbo
Append: [Do Not Change Me: On Transferring Entities Without Modification in Neural Machine Translation -- a Multilingual Perspective](https://arxiv.org/abs/2505.06010)
Token length: 1176
Summarized using GPT-3.5-turbo
Append: [Unilogit: Robust Machine Unlearning for LLMs Using Uniform-Target Self-Distillation](https://arxiv.org/abs/2505.06027)
Token length: 1421
Summarized using GPT-3.5-turbo
Append: [Healthy LLMs? Benchmarking LLM Knowledge of UK Government Public Health Information](https://arxiv.org/abs/2505.06046)
Token length: 1241
Summarized using GPT-3.5-turbo
Append: [Attention on Multiword Expressions: A Multilingual Study of BERT-based Models with Regard to Idiomaticity and Microsyntax](https://arxiv.org/abs/2505.06062)
Token length: 1037
Summarized using GPT-3.5-turbo
Append: [Multimodal Sentiment Analysis on CMU-MOSEI Dataset using Transformer-based Models](https://arxiv.org/abs/2505.06110)
Token length: 1319
Summarized using GPT-3.5-turbo
Append: [LLMs Get Lost In Multi-Turn Conversation](https://arxiv.org/abs/2505.06120)
Token length: 1317
Summarized using GPT-3.5-turbo
Append: [Towards Robust Few-Shot Text Classification Using Transformer Architectures and Dual Loss Strategies](https://arxiv.org/abs/2505.06145)
Token length: 984
Summarized using GPT-3.5-turbo
Append: [Can Prompting LLMs Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study](https://arxiv.org/abs/2505.06149)
Token length: 840
Summarized using GPT-3.5-turbo
Append: [A Scaling Law for Token Efficiency in LLM Fine-Tuning Under Fixed Compute Budgets](https://arxiv.org/abs/2505.06150)
Token length: 1915
Summarized using GPT-3.5-turbo
Append: [Estimating Quality in Therapeutic Conversations: A Multi-Dimensional Natural Language Processing Framework](https://arxiv.org/abs/2505.06151)
Token length: 1192
Summarized using GPT-3.5-turbo
Append: [Query-driven Document-level Scientific Evidence Extraction from Biomedical Studies](https://arxiv.org/abs/2505.06186)
Token length: 1443
Summarized using GPT-3.5-turbo
Append: [X-Transfer Attacks: Towards Super Transferable Adversarial Attacks on CLIP](https://arxiv.org/abs/2505.05528)
Token length: 1392
Summarized using GPT-3.5-turbo
Append: [Adaptive Stress Testing Black-Box LLM Planners](https://arxiv.org/abs/2505.05665)
Token length: 1162
Summarized using GPT-3.5-turbo
Append: [Prompted Meta-Learning for Few-shot Knowledge Graph Completion](https://arxiv.org/abs/2505.05684)
Token length: 1943
Summarized using GPT-3.5-turbo
Append: [Multimodal Integrated Knowledge Transfer to Large Language Models through Preference Optimization with Biomedical Applications](https://arxiv.org/abs/2505.05736)
Token length: 1576
Summarized using GPT-3.5-turbo
Append: [Harnessing LLMs Explanations to Boost Surrogate Models in Tabular Data Classification](https://arxiv.org/abs/2505.05744)
Token length: 1094
Summarized using GPT-3.5-turbo
Append: [BMMDetect: A Multimodal Deep Learning Framework for Comprehensive Biomedical Misconduct Detection](https://arxiv.org/abs/2505.05763)
Token length: 953
Summarized using GPT-3.5-turbo
Append: [An empathic GPT-based chatbot to talk about mental disorders with Spanish teenagers](https://arxiv.org/abs/2505.05828)
Token length: 1387
Summarized using GPT-3.5-turbo
Append: [Evolutionary ecology of words](https://arxiv.org/abs/2505.05863)
Token length: 986
Summarized using GPT-3.5-turbo
Append: [Short-circuiting Shortcuts: Mechanistic Investigation of Shortcuts in Text Classification](https://arxiv.org/abs/2505.06032)
Token length: 1926
Summarized using GPT-3.5-turbo
Append: [Differentiating Emigration from Return Migration of Scholars Using Name-Based Nationality Detection Models](https://arxiv.org/abs/2505.06107)
Token length: 1700
Summarized using GPT-3.5-turbo
Append: [From Millions of Tweets to Actionable Insights: Leveraging LLMs for User Profiling](https://arxiv.org/abs/2505.06184)
Token length: 1000
Summarized using GPT-3.5-turbo
Append: [Neuro-Symbolic Concepts](https://arxiv.org/abs/2505.06191)
Token length: 1189
Summarized using GPT-3.5-turbo
Append: [PART: Pre-trained Authorship Representation Transformer](https://arxiv.org/abs/2209.15373)
Token length: 1814
Summarized using GPT-3.5-turbo
Append: [Talking Heads: Understanding Inter-layer Communication in Transformer Language Models](https://arxiv.org/abs/2406.09519)
Token length: 1678
Summarized using GPT-3.5-turbo
Append: [NeedleBench: Can LLMs Do Retrieval and Reasoning in Information-Dense Context?](https://arxiv.org/abs/2407.11963)
Token length: 1473
Summarized using GPT-3.5-turbo
Append: [ReLiK: Retrieve and LinK, Fast and Accurate Entity Linking and Relation Extraction on an Academic Budget](https://arxiv.org/abs/2408.00103)
Token length: 1373
Summarized using GPT-3.5-turbo
Append: [Multi-Draft Speculative Sampling: Canonical Decomposition and Theoretical Limits](https://arxiv.org/abs/2410.18234)
Token length: 1477
Summarized using GPT-3.5-turbo
Append: [SRA-MCTS: Self-driven Reasoning Augmentation with Monte Carlo Tree Search for Code Generation](https://arxiv.org/abs/2411.11053)
Token length: 1917
Summarized using GPT-3.5-turbo
Append: [Can open source large language models be used for tumor documentation in Germany? -- An evaluation on urological doctors' notes](https://arxiv.org/abs/2501.12106)
Token length: 1397
Summarized using GPT-3.5-turbo
Append: [JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models](https://arxiv.org/abs/2501.14851)
Token length: 1376
Summarized using GPT-3.5-turbo
Append: [AdaCoT: Rethinking Cross-Lingual Factual Reasoning through Adaptive Chain-of-Thought](https://arxiv.org/abs/2501.16154)
Token length: 1116
Summarized using GPT-3.5-turbo
Append: [Phonetic accommodation and inhibition in a dynamic neural field model](https://arxiv.org/abs/2502.01210)
Token length: 1209
Summarized using GPT-3.5-turbo
Append: [The Order Effect: Investigating Prompt Sensitivity to Input Order in LLMs](https://arxiv.org/abs/2502.04134)
Token length: 1143
Summarized using GPT-3.5-turbo
Append: [k-LLMmeans: Scalable, Stable, and Interpretable Text Clustering via LLM-based Centroids](https://arxiv.org/abs/2502.09667)
Append: [Bridging Legal Knowledge and AI: Retrieval-Augmented Generation with Vector Stores, Knowledge Graphs, and Hierarchical Non-negative Matrix Factorization](https://arxiv.org/abs/2502.20364)
Append: [ConvoGen: Enhancing Conversational AI with Synthetic Data: A Multi-Agent Approach](https://arxiv.org/abs/2503.17460)
Append: [Reimagining Urban Science: Scaling Causal Inference with Large Language Models](https://arxiv.org/abs/2504.12345)
Append: [Recent Advances in Federated Learning Driven Large Language Models: A Survey on Architecture, Performance, and Security](https://arxiv.org/abs/2406.09831)
Append: [Rec-R1: Bridging Generative Large Language Models and User-Centric Recommendation Systems via Reinforcement Learning](https://arxiv.org/abs/2503.24289)
append_entries: 55
Finish: 2025-05-12 04:27:05.983439
------------------------------------------------------
Started: 2025-05-12 06:25:09.140121
Existing_entries: 1055
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-12 06:25:09.327037
------------------------------------------------------
Started: 2025-05-12 08:23:08.138885
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-12 08:23:08.403509
------------------------------------------------------
Started: 2025-05-12 10:18:49.206756
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-12 10:18:49.430031
------------------------------------------------------
Started: 2025-05-12 12:34:24.594379
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-12 12:34:24.815123
------------------------------------------------------
Started: 2025-05-12 14:17:23.442187
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-12 14:17:23.709236
------------------------------------------------------
Started: 2025-05-12 16:21:13.967442
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-12 16:21:14.155998
------------------------------------------------------
Started: 2025-05-12 18:22:49.634977
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-12 18:22:49.845269
------------------------------------------------------
Started: 2025-05-12 20:18:31.152441
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-12 20:18:31.345055
------------------------------------------------------
Started: 2025-05-12 22:15:59.147892
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-12 22:15:59.364834
------------------------------------------------------
Started: 2025-05-13 01:19:19.194204
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-13 01:19:19.410942
------------------------------------------------------
Started: 2025-05-13 03:09:51.376951
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-13 03:09:51.583310
------------------------------------------------------
Started: 2025-05-13 04:25:03.357222
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1611
Summarized using GPT-3.5-turbo
Append: [ScaleMCP: Dynamic and Auto-Synchronizing Model Context Protocol Tools for LLM Agents](https://arxiv.org/abs/2505.06416)
Token length: 1425
Summarized using GPT-3.5-turbo
Append: [Is your multimodal large language model a good science tutor?](https://arxiv.org/abs/2505.06418)
Token length: 593
Summarized using GPT-3.5-turbo
Append: [xGen-small Technical Report](https://arxiv.org/abs/2505.06496)
Token length: 1373
Summarized using GPT-3.5-turbo
Append: [Think in Safety: Unveiling and Mitigating Safety Alignment Collapse in Multimodal Large Reasoning Model](https://arxiv.org/abs/2505.06538)
Token length: 1289
Summarized using GPT-3.5-turbo
Append: [REFINE-AF: A Task-Agnostic Framework to Align Language Models via Self-Generated Instructions using Reinforcement Learning from Automated Feedback](https://arxiv.org/abs/2505.06548)
Token length: 1065
Summarized using GPT-3.5-turbo
Append: [References Indeed Matter? Reference-Free Preference Optimization for Conversational Query Reformulation](https://arxiv.org/abs/2505.06552)
Token length: 1312
Summarized using GPT-3.5-turbo
Append: [MacRAG: Compress, Slice, and Scale-up for Multi-Scale Adaptive Context RAG](https://arxiv.org/abs/2505.06569)
Token length: 800
Summarized using GPT-3.5-turbo
Append: [Evaluating LLM-Generated Q&A Test: a Student-Centered Study](https://arxiv.org/abs/2505.06591)
Token length: 1142
Summarized using GPT-3.5-turbo
Append: [Integrating Video and Text: A Balanced Approach to Multimodal Summary Generation and Evaluation](https://arxiv.org/abs/2505.06594)
Token length: 1627
Summarized using GPT-3.5-turbo
Append: [Bridging the Gap: An Intermediate Language for Enhanced and Cost-Effective Grapheme-to-Phoneme Conversion with Homographs with Multiple Pronunciations Disambiguation](https://arxiv.org/abs/2505.06599)
Token length: 1090
Summarized using GPT-3.5-turbo
Append: [Using External knowledge to Enhanced PLM for Semantic Matching](https://arxiv.org/abs/2505.06605)
Token length: 1753
Summarized using GPT-3.5-turbo
Append: [Boosting Neural Language Inference via Cascaded Interactive Reasoning](https://arxiv.org/abs/2505.06607)
Token length: 948
Summarized using GPT-3.5-turbo
Append: [The Efficiency of Pre-training with Objective Masking in Pseudo Labeling for Semi-Supervised Text Classification](https://arxiv.org/abs/2505.06624)
Token length: 1755
Summarized using GPT-3.5-turbo
Append: [Dynamic Domain Information Modulation Algorithm for Multi-domain Sentiment Analysis](https://arxiv.org/abs/2505.06630)
Token length: 867
Summarized using GPT-3.5-turbo
Append: [Attention Is Not All You Need: The Importance of Feedforward Networks in Transformer Models](https://arxiv.org/abs/2505.06633)
Token length: 1246
Summarized using GPT-3.5-turbo
Append: [TS-SUPERB: A Target Speech Processing Benchmark for Speech Self-Supervised Learning Models](https://arxiv.org/abs/2505.06660)
Token length: 1090
Summarized using GPT-3.5-turbo
Append: [Enhancing BERTopic with Intermediate Layer Representations](https://arxiv.org/abs/2505.06696)
Token length: 1861
Summarized using GPT-3.5-turbo
Append: [From Rankings to Insights: Evaluation Should Shift Focus from Leaderboard to Feedback](https://arxiv.org/abs/2505.06698)
Token length: 1469
Summarized using GPT-3.5-turbo
Append: [Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free](https://arxiv.org/abs/2505.06708)
Token length: 1669
Summarized using GPT-3.5-turbo
Append: [Utilizing LLMs to Investigate the Disputed Role of Evidence in Electronic Cigarette Health Policy Formation in Australia and the UK](https://arxiv.org/abs/2505.06782)
Token length: 970
Summarized using GPT-3.5-turbo
Append: [A Split-then-Join Approach to Abstractive Summarization for Very Long Documents in a Low Resource Setting](https://arxiv.org/abs/2505.06862)
Token length: 1366
Summarized using GPT-3.5-turbo
Append: [IM-BERT: Enhancing Robustness of BERT through the Implicit Euler Method](https://arxiv.org/abs/2505.06889)
Token length: 1047
Summarized using GPT-3.5-turbo
Append: [EcoLANG: Efficient and Effective Agent Communication Language Induction for Social Simulation](https://arxiv.org/abs/2505.06904)
Token length: 1197
Summarized using GPT-3.5-turbo
Append: [The Distracting Effect: Understanding Irrelevant Passages in RAG](https://arxiv.org/abs/2505.06914)
Token length: 962
Summarized using GPT-3.5-turbo
Append: [CNN-based Image Models Verify a Hypothesis that The Writers of Cuneiform Texts Improved Their Writing Skills When Studying at the Age of Hittite Empire](https://arxiv.org/abs/2505.06974)
Token length: 873
Summarized using GPT-3.5-turbo
Append: [Convert Language Model into a Value-based Strategic Planner](https://arxiv.org/abs/2505.06987)
Token length: 1605
Summarized using GPT-3.5-turbo
Append: [HAMLET: Healthcare-focused Adaptive Multilingual Learning Embedding-based Topic Modeling](https://arxiv.org/abs/2505.07157)
Token length: 1922
Summarized using GPT-3.5-turbo
Append: [Towards Actionable Pedagogical Feedback: A Multi-Perspective Analysis of Mathematics Teaching and Tutoring Dialogue](https://arxiv.org/abs/2505.07161)
Token length: 1862
Summarized using GPT-3.5-turbo
Append: [KDH-MLTC: Knowledge Distillation for Healthcare Multi-Label Text Classification](https://arxiv.org/abs/2505.07162)
Token length: 1364
Summarized using GPT-3.5-turbo
Append: [Structural Entropy Guided Agent for Detecting and Repairing Knowledge Deficiencies in LLMs](https://arxiv.org/abs/2505.07184)
Token length: 945
Summarized using GPT-3.5-turbo
Append: [On the Cost and Benefits of Training Context with Utterance or Full Conversation Training: A Comparative Stud](https://arxiv.org/abs/2505.07202)
Token length: 1350
Summarized using GPT-3.5-turbo
Append: [Benchmarking Ethical and Safety Risks of Healthcare LLMs in China-Toward Systemic Governance under Healthy China 2030](https://arxiv.org/abs/2505.07205)
Token length: 1315
Summarized using GPT-3.5-turbo
Append: [DynamicRAG: Leveraging Outputs of Large Language Model as Feedback for Dynamic Reranking in Retrieval-Augmented Generation](https://arxiv.org/abs/2505.07233)
Token length: 1424
Summarized using GPT-3.5-turbo
Append: [SAS-Bench: A Fine-Grained Benchmark for Evaluating Short Answer Scoring with Large Language Models](https://arxiv.org/abs/2505.07247)
Token length: 1634
Summarized using GPT-3.5-turbo
Append: [No Query, No Access](https://arxiv.org/abs/2505.07258)
Token length: 1583
Summarized using GPT-3.5-turbo
Append: [On the Robustness of Reward Models for Language Model Alignment](https://arxiv.org/abs/2505.07271)
Token length: 1077
Summarized using GPT-3.5-turbo
Append: [Semantic Retention and Extreme Compression in LLMs: Can We Have Both?](https://arxiv.org/abs/2505.07289)
Token length: 1478
Summarized using GPT-3.5-turbo
Append: [AttentionInfluence: Adopting Attention Head Influence for Weak-to-Strong Pretraining Data Selection](https://arxiv.org/abs/2505.07293)
Token length: 1169
Summarized using GPT-3.5-turbo
Append: [Towards Multi-Agent Reasoning Systems for Collaborative Expertise Delegation: An Exploratory Design Study](https://arxiv.org/abs/2505.07313)
Token length: 1124
Summarized using GPT-3.5-turbo
Append: [QUPID: Quantified Understanding for Enhanced Performance, Insights, and Decisions in Korean Search Engines](https://arxiv.org/abs/2505.07345)
Token length: 1072
Summarized using GPT-3.5-turbo
Append: [Computational Fact-Checking of Online Discourse: Scoring scientific accuracy in climate change related news articles](https://arxiv.org/abs/2505.07409)
Token length: 1437
Summarized using GPT-3.5-turbo
Append: [ViMRHP: A Vietnamese Benchmark Dataset for Multimodal Review Helpfulness Prediction via Human-AI Collaborative Annotation](https://arxiv.org/abs/2505.07416)
Token length: 1215
Summarized using GPT-3.5-turbo
Append: [Comparative sentiment analysis of public perception: Monkeypox vs. COVID-19 behavioral insights](https://arxiv.org/abs/2505.07430)
Token length: 1125
Summarized using GPT-3.5-turbo
Append: [Matching Tasks with Industry Groups for Augmenting Commonsense Knowledge](https://arxiv.org/abs/2505.07440)
Token length: 913
Summarized using GPT-3.5-turbo
Append: [Translating the Grievance Dictionary: a psychometric evaluation of Dutch, German, and Italian versions](https://arxiv.org/abs/2505.07495)
Token length: 993
Summarized using GPT-3.5-turbo
Append: [ToolACE-DEV: Self-Improving Tool Learning via Decomposition and EVolution](https://arxiv.org/abs/2505.07512)
Token length: 1640
Summarized using GPT-3.5-turbo
Append: [SEReDeEP: Hallucination Detection in Retrieval-Augmented Models via Semantic Entropy and Context-Parameter Fusion](https://arxiv.org/abs/2505.07528)
Token length: 1411
Summarized using GPT-3.5-turbo
Append: [A Multi-Dimensional Constraint Framework for Evaluating and Improving Instruction Following in Large Language Models](https://arxiv.org/abs/2505.07591)
Token length: 1526
Summarized using GPT-3.5-turbo
Append: [Reinforced Internal-External Knowledge Synergistic Reasoning for Efficient Adaptive Search Agent](https://arxiv.org/abs/2505.07596)
Token length: 1688
Summarized using GPT-3.5-turbo
Append: [Characterizing the Investigative Methods of Fictional Detectives with Large Language Models](https://arxiv.org/abs/2505.07601)
Append: [MiMo: Unlocking the Reasoning Potential of Language Model -- From Pretraining to Posttraining](https://arxiv.org/abs/2505.07608)
Append: [Concept-Level Explainability for Auditing & Steering LLM Responses](https://arxiv.org/abs/2505.07610)
Append: [Chronocept: Instilling a Sense of Time in Machines](https://arxiv.org/abs/2505.07637)
Append: [JobHop: A Large-Scale Dataset of Career Trajectories](https://arxiv.org/abs/2505.07653)
Append: [Using Information Theory to Characterize Prosodic Typology: The Case of Tone, Pitch-Accent and Stress-Accent](https://arxiv.org/abs/2505.07659)
Append: [Benchmarking Retrieval-Augmented Generation for Chemistry](https://arxiv.org/abs/2505.07671)
Append: [OnPrem.LLM: A Privacy-Conscious Document Intelligence Toolkit](https://arxiv.org/abs/2505.07672)
Append: [Codifying Character Logic in Role-Playing](https://arxiv.org/abs/2505.07705)
Append: [Spoken Language Understanding on Unseen Tasks With In-Context Learning](https://arxiv.org/abs/2505.07731)
Append: [Must Read: A Systematic Survey of Computational Persuasion](https://arxiv.org/abs/2505.07775)
Append: [Domain Regeneration: How well do LLMs match syntactic properties of text domains?](https://arxiv.org/abs/2505.07784)
Append: [Learning from Peers in Reasoning Models](https://arxiv.org/abs/2505.07787)
Append: [Learning Dynamics in Continual Pre-Training for Large Language Models](https://arxiv.org/abs/2505.07796)
Append: [A Comparative Analysis of Static Word Embeddings for Hungarian](https://arxiv.org/abs/2505.07809)
Append: [Lossless Compression of Large Language Model-Generated Text via Next-Token Prediction](https://arxiv.org/abs/2505.06297)
Append: [AI Approaches to Qualitative and Quantitative News Analytics on NATO Unity](https://arxiv.org/abs/2505.06313)
Append: [Divide (Text) and Conquer (Sentiment): Improved Sentiment Classification by Constituent Conflict Resolution](https://arxiv.org/abs/2505.06320)
Append: [Improving Block-Wise LLM Quantization by 4-bit Block-Wise Optimal Float (BOF4): Analysis and Variations](https://arxiv.org/abs/2505.06653)
Append: [Bridging Ears and Eyes: Analyzing Audio and Visual Large Language Models to Humans in Visible Sound Recognition and Reducing Their Sensory Gap via Cross-Modal Distillation](https://arxiv.org/abs/2505.06803)
Append: [Overview of the NLPCC 2025 Shared Task 4: Multi-modal, Multilingual, and Multi-hop Medical Instructional Video Question Answering Challenge](https://arxiv.org/abs/2505.06814)
Append: [Benign Samples Matter! Fine-tuning On Outlier Benign Samples Severely Breaks Safety](https://arxiv.org/abs/2505.06843)
Append: [Multi-Modal Explainable Medical AI Assistant for Trustworthy Human-AI Collaboration](https://arxiv.org/abs/2505.06898)
Append: [A digital perspective on the role of a stemma in material-philological transmission studies](https://arxiv.org/abs/2505.06938)
Append: [Web Page Classification using LLMs for Crawling Support](https://arxiv.org/abs/2505.06972)
Append: [Towards the Three-Phase Dynamics of Generalization Power of a DNN](https://arxiv.org/abs/2505.06993)
Append: [LLM-Augmented Chemical Synthesis and Design Decision Programs](https://arxiv.org/abs/2505.07027)
Append: [Reassessing Large Language Model Boolean Query Generation for Systematic Reviews](https://arxiv.org/abs/2505.07155)
Append: [Pre-training vs. Fine-tuning: A Reproducibility Study on Dense Retrieval Knowledge Acquisition](https://arxiv.org/abs/2505.07166)
Append: [One Trigger Token Is Enough: A Defense Strategy for Balancing Safety and Usability in Large Language Models](https://arxiv.org/abs/2505.07167)
Append: [Securing Genomic Data Against Inference Attacks in Federated Learning Environments](https://arxiv.org/abs/2505.07188)
Append: [Multi-Domain Audio Question Answering Toward Acoustic Content Reasoning in The DCASE 2025 Challenge](https://arxiv.org/abs/2505.07365)
Append: [A Survey on Collaborative Mechanisms Between Large and Small Language Models](https://arxiv.org/abs/2505.07460)
Append: [Direct Density Ratio Optimization: A Statistically Consistent Approach to Aligning Large Language Models](https://arxiv.org/abs/2505.07558)
Append: [Through the Looking Glass: Common Sense Consistency Evaluation of Weird Images](https://arxiv.org/abs/2505.07704)
Append: [Enhancing Code Generation via Bidirectional Comment-Level Mutual Grounding](https://arxiv.org/abs/2505.07768)
Append: [Clickbait Detection via Large Language Models](https://arxiv.org/abs/2306.09597)
Append: [Towards Understanding Sycophancy in Language Models](https://arxiv.org/abs/2310.13548)
Append: [Gemini: A Family of Highly Capable Multimodal Models](https://arxiv.org/abs/2312.11805)
Append: [Fleet of Agents: Coordinated Problem Solving with Large Language Models](https://arxiv.org/abs/2405.06691)
Append: [A Syntax-Injected Approach for Faster and More Accurate Sentiment Analysis](https://arxiv.org/abs/2406.15163)
Append: [From Distributional to Overton Pluralism: Investigating Large Language Model Alignment](https://arxiv.org/abs/2406.17692)
Append: [Order Matters in Hallucination: Reasoning Order as Benchmark and Reflexive Prompting for Large-Language-Models](https://arxiv.org/abs/2408.05093)
Append: [MABR: Multilayer Adversarial Bias Removal Without Prior Bias Knowledge](https://arxiv.org/abs/2408.05497)
Append: [Bridging the Language Gap: Enhancing Multilingual Prompt-Based Code Generation in LLMs via Zero-Shot Cross-Lingual Transfer](https://arxiv.org/abs/2408.09701)
Append: [Mapping Biomedical Ontology Terms to IDs: Effect of Domain Prevalence on Prediction Accuracy](https://arxiv.org/abs/2409.13746)
Append: [Endless Jailbreaks with Bijection Learning](https://arxiv.org/abs/2410.01294)
Append: [Isolated Causal Effects of Natural Language](https://arxiv.org/abs/2410.14812)
Append: [Does Data Contamination Detection Work (Well) for LLMs? A Survey and Evaluation on Detection Assumptions](https://arxiv.org/abs/2410.18966)
Append: [Evaluating Creative Short Story Generation in Humans and Large Language Models](https://arxiv.org/abs/2411.02316)
Append: [The Root Shapes the Fruit: On the Persistence of Gender-Exclusive Harms in Aligned Language Models](https://arxiv.org/abs/2411.03700)
Append: [Diversity Helps Jailbreak Large Language Models](https://arxiv.org/abs/2411.04223)
Append: [XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models](https://arxiv.org/abs/2411.15100)
Append: [Multi-Party Supervised Fine-tuning of Language Models for Multi-Party Dialogue Generation](https://arxiv.org/abs/2412.05342)
Append: [Advancing Single and Multi-task Text Classification through Large Language Model Fine-tuning](https://arxiv.org/abs/2412.08587)
Append: [Understanding the Impact of Confidence in Retrieval Augmented Generation: A Case Study in the Medical Domain](https://arxiv.org/abs/2412.20309)
Append: [MIRAGE: Exploring How Large Language Models Perform in Complex Social Interactive Environments](https://arxiv.org/abs/2501.01652)
Append: [Chain-of-Reasoning: Towards Unified Mathematical Reasoning in Large Language Models via a Multi-Paradigm Perspective](https://arxiv.org/abs/2501.11110)
Append: [Softplus Attention with Re-weighting Boosts Length Extrapolation in Large Language Models](https://arxiv.org/abs/2501.13428)
Append: [Visual Theory of Mind Enables the Invention of Proto-Writing](https://arxiv.org/abs/2502.01568)
Append: [VLM2-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit Matching Visual Cues](https://arxiv.org/abs/2502.12084)
Append: [None of the Others: a General Technique to Distinguish Reasoning from Memorization in Multiple-Choice LLM Evaluation Benchmarks](https://arxiv.org/abs/2502.12896)
Append: [SemViQA: A Semantic Question Answering System for Vietnamese Information Fact-Checking](https://arxiv.org/abs/2503.00955)
Append: [HREB-CRF: Hierarchical Reduced-bias EMA for Chinese Named Entity Recognition](https://arxiv.org/abs/2503.01217)
Append: [Can (A)I Change Your Mind?](https://arxiv.org/abs/2503.01844)
Append: [NCL-UoR at SemEval-2025 Task 3: Detecting Multilingual Hallucination and Related Observable Overgeneration Text Spans with Modified RefChecker and Modified SeflCheckGPT](https://arxiv.org/abs/2503.01921)
Append: [The Devil Is in the Details: Tackling Unimodal Spurious Correlations for Generalizable Multimodal Reward Models](https://arxiv.org/abs/2503.03122)
Append: [A Hybrid Architecture with Efficient Fine Tuning for Abstractive Patent Document Summarization](https://arxiv.org/abs/2503.10354)
Append: [From Large to Super-Tiny: End-to-End Optimization for Cost-Efficient LLMs](https://arxiv.org/abs/2504.13471)
Append: [Methods for Recognizing Nested Terms](https://arxiv.org/abs/2504.16007)
Append: [ConTextual: Improving Clinical Text Summarization in LLMs with Context-preserving Token Filtering and Knowledge Graphs](https://arxiv.org/abs/2504.16394)
Append: [Semantic and Expressive Variation in Image Captions Across Languages](https://arxiv.org/abs/2310.14356)
Append: [Beyond Boundaries: A Comprehensive Survey of Transferable Attacks on AI Systems](https://arxiv.org/abs/2311.11796)
Append: [AIOS: LLM Agent Operating System](https://arxiv.org/abs/2403.16971)
Append: [EffiLearner: Enhancing Efficiency of Generated Code via Self-Optimization](https://arxiv.org/abs/2405.15189)
Append: [MedualTime: A Dual-Adapter Language Model for Medical Time Series-Text Multimodal Learning](https://arxiv.org/abs/2406.06620)
Append: [CHARTOM: A Visual Theory-of-Mind Benchmark for Multimodal Large Language Models](https://arxiv.org/abs/2408.14419)
Append: [Self-Data Distillation for Recovering Quality in Pruned Large Language Models](https://arxiv.org/abs/2410.09982)
Append: [Similarity-Dissimilarity Loss for Multi-label Supervised Contrastive Learning](https://arxiv.org/abs/2410.13439)
Append: [Fun-tuning: Characterizing the Vulnerability of Proprietary LLMs to Optimization-based Prompt Injection Attacks via the Fine-Tuning Interface](https://arxiv.org/abs/2501.09798)
Append: [Training and Evaluating with Human Label Variation: An Empirical Study](https://arxiv.org/abs/2502.01891)
Append: [Integrating Expert Knowledge into Logical Programs via LLMs](https://arxiv.org/abs/2502.12275)
Append: [A Statistical Case Against Empirical Human-AI Alignment](https://arxiv.org/abs/2502.14581)
Append: [SegSub: Evaluating Robustness to Knowledge Conflicts and Hallucinations in Vision-Language Models](https://arxiv.org/abs/2502.14908)
Append: [I Predict Therefore I Am: Is Next Token Prediction Enough to Learn Human-Interpretable Concepts from Data?](https://arxiv.org/abs/2503.08980)
Append: [An Illusion of Progress? Assessing the Current State of Web Agents](https://arxiv.org/abs/2504.01382)
Append: [Understanding Learner-LLM Chatbot Interactions and the Impact of Prompting Guidelines](https://arxiv.org/abs/2504.07840)
append_entries: 136
Finish: 2025-05-13 04:27:34.565166
------------------------------------------------------
Started: 2025-05-13 06:24:19.088746
Existing_entries: 1136
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-13 06:24:19.440681
------------------------------------------------------
Started: 2025-05-13 08:22:56.297543
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-13 08:22:56.697694
------------------------------------------------------
Started: 2025-05-13 10:18:16.790316
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-13 10:18:17.111899
------------------------------------------------------
Started: 2025-05-13 12:34:53.868107
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-13 12:34:54.181889
------------------------------------------------------
Started: 2025-05-13 14:17:17.905696
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-13 14:17:18.309672
------------------------------------------------------
Started: 2025-05-13 16:21:12.955734
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-13 16:21:13.273154
------------------------------------------------------
Started: 2025-05-13 18:23:32.109942
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-13 18:23:32.452073
------------------------------------------------------
Started: 2025-05-13 20:18:24.456400
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-13 20:18:24.768735
------------------------------------------------------
Started: 2025-05-13 22:15:57.896985
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-13 22:15:58.261927
------------------------------------------------------
Started: 2025-05-14 01:18:02.163443
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-14 01:18:02.519141
------------------------------------------------------
Started: 2025-05-14 03:08:00.942377
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-14 03:08:01.312006
------------------------------------------------------
Started: 2025-05-14 04:26:11.661705
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 818
Summarized using GPT-3.5-turbo
Append: [Polysemy of Synthetic Neurons Towards a New Type of Explanatory Categorical Vector Spaces](https://arxiv.org/abs/2505.07831)
Token length: 1380
Summarized using GPT-3.5-turbo
Append: [A Tale of Two Identities: An Ethical Audit of Human and AI-Crafted Personas](https://arxiv.org/abs/2505.07850)
Token length: 1529
Summarized using GPT-3.5-turbo
Append: [Joint Detection of Fraud and Concept Drift inOnline Conversations with LLM-Assisted Judgment](https://arxiv.org/abs/2505.07852)
Token length: 1911
Summarized using GPT-3.5-turbo
Append: [CrashSage: A Large Language Model-Centered Framework for Contextual and Interpretable Traffic Crash Analysis](https://arxiv.org/abs/2505.07853)
Token length: 1384
Summarized using GPT-3.5-turbo
Append: [Unpacking Robustness in Inflectional Languages: Adversarial Evaluation and Mechanistic Insights](https://arxiv.org/abs/2505.07856)
Token length: 1758
Summarized using GPT-3.5-turbo
Append: [Enhanced Urdu Intent Detection with Large Language Models and Prototype-Informed Predictive Pipelines](https://arxiv.org/abs/2505.07857)
Token length: 1525
Summarized using GPT-3.5-turbo
Append: [Scaling Laws for Speculative Decoding](https://arxiv.org/abs/2505.07858)
Token length: 1060
Summarized using GPT-3.5-turbo
Append: [Boosting Performance on ARC is a Matter of Perspective](https://arxiv.org/abs/2505.07859)
Token length: 1064
Summarized using GPT-3.5-turbo
Append: [Scalable LLM Math Reasoning Acceleration with Low-rank Distillation](https://arxiv.org/abs/2505.07861)
Token length: 681
Summarized using GPT-3.5-turbo
Append: [Graph Laplacian Wavelet Transformer via Learnable Spectral Decomposition](https://arxiv.org/abs/2505.07862)
Token length: 1723
Summarized using GPT-3.5-turbo
Append: [QoSBERT: An Uncertainty-Aware Approach based on Pre-trained Language Models for Service Quality Prediction](https://arxiv.org/abs/2505.07863)
Token length: 1221
Summarized using GPT-3.5-turbo
Append: [Efficient Fairness Testing in Large Language Models: Prioritizing Metamorphic Relations for Bias Detection](https://arxiv.org/abs/2505.07870)
Token length: 1968
Summarized using GPT-3.5-turbo
Append: [Evaluating Financial Sentiment Analysis with Annotators Instruction Assisted Prompting: Enhancing Contextual Interpretation and Stock Prediction Accuracy](https://arxiv.org/abs/2505.07871)
Token length: 1266
Summarized using GPT-3.5-turbo
Append: [The Sound of Populism: Distinct Linguistic Features Across Populist Variants](https://arxiv.org/abs/2505.07874)
Token length: 1341
Summarized using GPT-3.5-turbo
Append: [Recovering Event Probabilities from Large Language Model Embeddings via Axiomatic Constraints](https://arxiv.org/abs/2505.07883)
Token length: 1836
Summarized using GPT-3.5-turbo
Append: [Development of a WAZOBIA-Named Entity Recognition System](https://arxiv.org/abs/2505.07884)
Token length: 1129
Summarized using GPT-3.5-turbo
Append: [PLHF: Prompt Optimization with Few-Shot Human Feedback](https://arxiv.org/abs/2505.07886)
Token length: 1746
Summarized using GPT-3.5-turbo
Append: [Implementing Long Text Style Transfer with LLMs through Dual-Layered Sentence and Paragraph Structure Extraction and Mapping](https://arxiv.org/abs/2505.07888)
Token length: 1844
Summarized using GPT-3.5-turbo
Append: [BioProBench: Comprehensive Dataset and Benchmark in Biological Protocol Understanding and Reasoning](https://arxiv.org/abs/2505.07889)
Token length: 1240
Summarized using GPT-3.5-turbo
Append: [TSLFormer: A Lightweight Transformer Model for Turkish Sign Language Recognition Using Skeletal Landmarks](https://arxiv.org/abs/2505.07890)
Token length: 1483
Summarized using GPT-3.5-turbo
Append: [TrumorGPT: Graph-Based Retrieval-Augmented Large Language Model for Fact-Checking](https://arxiv.org/abs/2505.07891)
Token length: 1201
Summarized using GPT-3.5-turbo
Append: [LongCodeBench: Evaluating Coding LLMs at 1M Context Windows](https://arxiv.org/abs/2505.07897)
Token length: 1168
Summarized using GPT-3.5-turbo
Append: [DeltaEdit: Enhancing Sequential Editing in Large Language Models by Controlling Superimposed Noise](https://arxiv.org/abs/2505.07899)
Token length: 1412
Summarized using GPT-3.5-turbo
Append: [SEM: Reinforcement Learning for Search-Efficient Large Language Models](https://arxiv.org/abs/2505.07903)
Token length: 1632
Summarized using GPT-3.5-turbo
Append: [Re$^2$: A Consistency-ensured Dataset for Full-stage Peer Review and Multi-turn Rebuttal Discussions](https://arxiv.org/abs/2505.07920)
Token length: 1113
Summarized using GPT-3.5-turbo
Append: [Assessing and Mitigating Medical Knowledge Drift and Conflicts in Large Language Models](https://arxiv.org/abs/2505.07968)
Token length: 1239
Summarized using GPT-3.5-turbo
Append: [Task-Adaptive Semantic Communications with Controllable Diffusion-based Data Regeneration](https://arxiv.org/abs/2505.07980)
Token length: 1500
Summarized using GPT-3.5-turbo
Append: [Large Language Models and Arabic Content: A Review](https://arxiv.org/abs/2505.08004)
Token length: 1054
Summarized using GPT-3.5-turbo
Append: [TiSpell: A Semi-Masked Methodology for Tibetan Spelling Correction covering Multi-Level Error with Data Augmentation](https://arxiv.org/abs/2505.08037)
Token length: 1113
Summarized using GPT-3.5-turbo
Append: [FalseReject: A Resource for Improving Contextual Safety and Mitigating Over-Refusals in LLMs via Structured Reasoning](https://arxiv.org/abs/2505.08054)
Token length: 758
Summarized using GPT-3.5-turbo
Append: [HYPERNYM MERCURY: Token Optimization through Semantic Field Constriction and Reconstruction from Hypernyms. A New Text Compression Method](https://arxiv.org/abs/2505.08058)
Token length: 1563
Summarized using GPT-3.5-turbo
Append: [Are LLMs complicated ethical dilemma analyzers?](https://arxiv.org/abs/2505.08106)
Token length: 1302
Summarized using GPT-3.5-turbo
Append: [Putting It All into Context: Simplifying Agents with LCLMs](https://arxiv.org/abs/2505.08120)
Token length: 1051
Summarized using GPT-3.5-turbo
Append: [ALOHA: Empowering Multilingual Agent for University Orientation with Hierarchical Retrieval](https://arxiv.org/abs/2505.08130)
Token length: 1953
Summarized using GPT-3.5-turbo
Append: [Fusing Bidirectional Chains of Thought and Reward Mechanisms A Method for Enhancing Question-Answering Capabilities of Large Language Models for Chinese Intangible Cultural Heritage](https://arxiv.org/abs/2505.08167)
Token length: 1224
Summarized using GPT-3.5-turbo
Append: [Exploiting Text Semantics for Few and Zero Shot Node Classification on Text-attributed Graph](https://arxiv.org/abs/2505.08168)
Token length: 1267
Summarized using GPT-3.5-turbo
Append: [A Head to Predict and a Head to Question: Pre-trained Uncertainty Quantification Heads for Hallucination Detection in LLM Outputs](https://arxiv.org/abs/2505.08200)
Token length: 1412
Summarized using GPT-3.5-turbo
Append: [Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement](https://arxiv.org/abs/2505.08245)
Token length: 1199
Summarized using GPT-3.5-turbo
Append: [Enhancing Cache-Augmented Generation (CAG) with Adaptive Contextual Compression for Scalable Knowledge Integration](https://arxiv.org/abs/2505.08261)
Token length: 1369
Summarized using GPT-3.5-turbo
Append: [Evaluating the Effectiveness of Black-Box Prompt Optimization as the Scale of LLMs Continues to Grow](https://arxiv.org/abs/2505.08303)
Token length: 1331
Summarized using GPT-3.5-turbo
Append: [AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale](https://arxiv.org/abs/2505.08311)
Token length: 1234
Summarized using GPT-3.5-turbo
Append: [On the Geometry of Semantics in Next-token Prediction](https://arxiv.org/abs/2505.08348)
Token length: 1178
Summarized using GPT-3.5-turbo
Append: [Alignment Drift in CEFR-prompted LLMs for Interactive Spanish Tutoring](https://arxiv.org/abs/2505.08351)
Token length: 1169
Summarized using GPT-3.5-turbo
Append: [Towards Contamination Resistant Benchmarks](https://arxiv.org/abs/2505.08389)
Token length: 1804
Summarized using GPT-3.5-turbo
Append: [Accelerating Chain-of-Thought Reasoning: When Goal-Gradient Importance Meets Dynamic Skipping](https://arxiv.org/abs/2505.08392)
Token length: 1627
Summarized using GPT-3.5-turbo
Append: [TUMS: Enhancing Tool-use Abilities of LLMs with Multi-structure Handlers](https://arxiv.org/abs/2505.08402)
Token length: 1240
Summarized using GPT-3.5-turbo
Append: [Hakim: Farsi Text Embedding Model](https://arxiv.org/abs/2505.08435)
Token length: 1060
Summarized using GPT-3.5-turbo
Append: [A document processing pipeline for the construction of a dataset for topic modeling based on the judgments of the Italian Supreme Court](https://arxiv.org/abs/2505.08439)
Token length: 1319
Summarized using GPT-3.5-turbo
Append: [IterKey: Iterative Keyword Generation with LLMs for Enhanced Retrieval Augmented Generation](https://arxiv.org/abs/2505.08450)
Token length: 1197
Summarized using GPT-3.5-turbo
Append: [RepCali: High Efficient Fine-tuning Via Representation Calibration in Latent Space for Pre-trained Language Models](https://arxiv.org/abs/2505.08463)
Append: [Large Language Models Meet Stance Detection: A Survey of Tasks, Methods, Applications, Challenges and Future Directions](https://arxiv.org/abs/2505.08464)
Append: [Judging the Judges: Can Large Vision-Language Models Fairly Evaluate Chart Comprehension and Reasoning?](https://arxiv.org/abs/2505.08468)
Append: [LCES: Zero-shot Automated Essay Scoring via Pairwise Comparisons Using Large Language Models](https://arxiv.org/abs/2505.08498)
Append: [Reassessing Graph Linearization for Sequence-to-sequence AMR Parsing: On the Advantages and Limitations of Triple-Based Encoding](https://arxiv.org/abs/2505.08504)
Append: [Are We Paying Attention to Her? Investigating Gender Disambiguation and Attention in Machine Translation](https://arxiv.org/abs/2505.08546)
Append: [Small but Significant: On the Promise of Small Language Models for Accessible AIED](https://arxiv.org/abs/2505.08588)
Append: [Enhancing Thyroid Cytology Diagnosis with RAG-Optimized LLMs and Pa-thology Foundation Models](https://arxiv.org/abs/2505.08590)
Append: [Automatic Task Detection and Heterogeneous LLM Speculative Decoding](https://arxiv.org/abs/2505.08600)
Append: [Scaling Context, Not Parameters: Training a Compact 7B Language Model for Efficient Long-Context Processing](https://arxiv.org/abs/2505.08651)
Append: [Revealing economic facts: LLMs know more than they say](https://arxiv.org/abs/2505.08662)
Append: [Adaptive Schema-aware Event Extraction with Retrieval-Augmented Generation](https://arxiv.org/abs/2505.08690)
Append: [NurValues: Real-World Nursing Values Evaluation for Large Language Models in Clinical Context](https://arxiv.org/abs/2505.08734)
Append: [Probability Consistency in Large Language Models: Theoretical Foundations Meet Empirical Discrepancies](https://arxiv.org/abs/2505.08739)
Append: [AC-Reason: Towards Theory-Guided Actual Causality Reasoning with Large Language Models](https://arxiv.org/abs/2505.08750)
Append: [Aya Vision: Advancing the Frontier of Multilingual Multimodality](https://arxiv.org/abs/2505.08751)
Append: [HealthBench: Evaluating Large Language Models Towards Improved Human Health](https://arxiv.org/abs/2505.08775)
Append: [Arrow-Guided VLM: Enhancing Flowchart Understanding via Arrow Direction Encoding](https://arxiv.org/abs/2505.07864)
Append: [CellVerse: Do Large Language Models Really Understand Cell Biology?](https://arxiv.org/abs/2505.07865)
Append: [Multimodal Assessment of Classroom Discourse Quality: A Text-Centered Attention-Based Multi-Task Learning Approach](https://arxiv.org/abs/2505.07902)
Append: [A Reproduction Study: The Kernel PCA Interpretation of Self-Attention Fails Under Scrutiny](https://arxiv.org/abs/2505.07908)
Append: [SciCom Wiki: Fact-Checking and FAIR Knowledge Distribution for Scientific Videos and Podcasts](https://arxiv.org/abs/2505.07912)
Append: [NAZM: Network Analysis of Zonal Metrics in Persian Poetic Tradition](https://arxiv.org/abs/2505.08052)
Append: [Beyond Input Activations: Identifying Influential Latents by Gradient Sparse Autoencoders](https://arxiv.org/abs/2505.08080)
Append: [Large Language Models for Computer-Aided Design: A Survey](https://arxiv.org/abs/2505.08137)
Append: [A Large-Scale Empirical Analysis of Custom GPTs' Vulnerabilities in the OpenAI Ecosystem](https://arxiv.org/abs/2505.08148)
Append: [Not that Groove: Zero-Shot Symbolic Music Editing](https://arxiv.org/abs/2505.08203)
Append: [Optimizing Retrieval-Augmented Generation: Analysis of Hyperparameter Impact on Performance and Efficiency](https://arxiv.org/abs/2505.08445)
Append: [Visually Guided Decoding: Gradient-Free Hard Prompt Inversion with Language Models](https://arxiv.org/abs/2505.08622)
Append: [TRAIL: Trace Reasoning and Agentic Issue Localization](https://arxiv.org/abs/2505.08638)
Append: [LLM-based Prompt Ensemble for Reliable Medical Entity Recognition from EHRs](https://arxiv.org/abs/2505.08704)
Append: [Memorization-Compression Cycles Improve Generalization](https://arxiv.org/abs/2505.08727)
Append: [CodePDE: An Inference Framework for LLM-driven PDE Solver Generation](https://arxiv.org/abs/2505.08783)
Append: [Bridging LLMs and KGs without Fine-Tuning: Intermediate Probing Meets Subgraph-Aware Entity Descriptions](https://arxiv.org/abs/2408.06787)
Append: [Studying the Effects of Collaboration in Interactive Theme Discovery Systems](https://arxiv.org/abs/2408.09030)
Append: [From Calculation to Adjudication: Examining LLM judges on Mathematical Reasoning Tasks](https://arxiv.org/abs/2409.04168)
Append: [Round and Round We Go! What makes Rotary Positional Encodings useful?](https://arxiv.org/abs/2410.06205)
Append: [CursorCore: Assist Programming through Aligning Anything](https://arxiv.org/abs/2410.07002)
Append: [No Preference Left Behind: Group Distributional Preference Optimization](https://arxiv.org/abs/2412.20299)
Append: [FutureVision: A methodology for the investigation of future cognition](https://arxiv.org/abs/2502.01597)
Append: [SMI: An Information-Theoretic Metric for Predicting Model Knowledge Solely from Pre-Training Signals](https://arxiv.org/abs/2502.04066)
Append: [Discriminative Finetuning of Generative Large Language Models without Reward Models and Human Preference Data](https://arxiv.org/abs/2502.18679)
Append: [Cite Before You Speak: Enhancing Context-Response Grounding in E-commerce Conversational LLM-Agents](https://arxiv.org/abs/2503.04830)
Append: [CURIE: Evaluating LLMs On Multitask Scientific Long Context Understanding and Reasoning](https://arxiv.org/abs/2503.13517)
Append: [Crossing Boundaries: Leveraging Semantic Divergences to Explore Cultural Novelty in Cooking Recipes](https://arxiv.org/abs/2503.24027)
Append: [Why do LLMs attend to the first token?](https://arxiv.org/abs/2504.02732)
Append: [AI Hiring with LLMs: A Context-Aware and Explainable Multi-Agent Framework for Resume Screening](https://arxiv.org/abs/2504.02870)
Append: [Beyond Single-Turn: A Survey on Multi-Turn Interactions with Large Language Models](https://arxiv.org/abs/2504.04717)
Append: [DeepSeek-R1 Thoughtology: Let's think about LLM Reasoning](https://arxiv.org/abs/2504.07128)
Append: [LLMSR@XLLM25: Less is More: Enhancing Structured Multi-Agent Reasoning via Quality-Guided Distillation](https://arxiv.org/abs/2504.16408)
Append: [DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale Difficulty-Graded Data Training](https://arxiv.org/abs/2504.17565)
Append: [MobA: Multifaceted Memory-Enhanced Adaptive Planning for Efficient Mobile Task Automation](https://arxiv.org/abs/2410.13757)
Append: [2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining](https://arxiv.org/abs/2501.00958)
Append: [Scaling Laws for Floating Point Quantization Training](https://arxiv.org/abs/2501.02423)
Append: [Vision-Language Models Do Not Understand Negation](https://arxiv.org/abs/2501.09425)
Append: [Are Transformers Able to Reason by Connecting Separated Knowledge in Training Data?](https://arxiv.org/abs/2501.15857)
Append: [Adaptive Integrated Layered Attention (AILA)](https://arxiv.org/abs/2503.22742)
Append: [Efficient Adaptation For Remote Sensing Visual Grounding](https://arxiv.org/abs/2503.23083)
Append: [Gradual Binary Search and Dimension Expansion : A general method for activation quantization in LLMs](https://arxiv.org/abs/2504.13989)
Append: [Integrating Single-Cell Foundation Models with Graph Neural Networks for Drug Response Prediction](https://arxiv.org/abs/2504.14361)
append_entries: 109
Finish: 2025-05-14 04:28:19.156852
------------------------------------------------------
Started: 2025-05-14 06:24:10.695034
Existing_entries: 1109
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-14 06:24:10.983477
------------------------------------------------------
Started: 2025-05-14 08:22:14.629183
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-14 08:22:14.905203
------------------------------------------------------
Started: 2025-05-14 10:17:47.481171
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-14 10:17:47.781346
------------------------------------------------------
Started: 2025-05-14 12:33:37.080492
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-14 12:33:37.355053
------------------------------------------------------
Started: 2025-05-14 14:16:51.916410
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-14 14:16:52.262249
------------------------------------------------------
Started: 2025-05-14 16:20:32.820366
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-14 16:20:33.180875
------------------------------------------------------
Started: 2025-05-14 18:21:00.601168
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-14 18:21:00.889546
------------------------------------------------------
Started: 2025-05-14 20:15:25.088274
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-14 20:15:25.384824
------------------------------------------------------
Started: 2025-05-14 22:13:34.172426
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-14 22:13:34.461730
------------------------------------------------------
Started: 2025-05-15 01:16:13.662011
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-15 01:16:14.030464
------------------------------------------------------
Started: 2025-05-15 03:07:33.869065
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-15 03:07:34.175994
------------------------------------------------------
Started: 2025-05-15 04:23:42.465084
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1691
Summarized using GPT-3.5-turbo
Append: [Human-AI Collaboration or Academic Misconduct? Measuring AI Use in Student Writing Through Stylometric Evidence](https://arxiv.org/abs/2505.08828)
Token length: 1173
Summarized using GPT-3.5-turbo
Append: [Clicking some of the silly options: Exploring Player Motivation in Static and Dynamic Educational Interactive Narratives](https://arxiv.org/abs/2505.08891)
Token length: 1518
Summarized using GPT-3.5-turbo
Append: [A suite of LMs comprehend puzzle statements as well as humans](https://arxiv.org/abs/2505.08996)
Token length: 1457
Summarized using GPT-3.5-turbo
Append: [For GPT-4 as with Humans: Information Structure Predicts Acceptability of Long-Distance Dependencies](https://arxiv.org/abs/2505.09005)
Token length: 1266
Summarized using GPT-3.5-turbo
Append: [Atomic Consistency Preference Optimization for Long-Form Question Answering](https://arxiv.org/abs/2505.09039)
Token length: 1520
Summarized using GPT-3.5-turbo
Append: [A Comprehensive Analysis of Large Language Model Outputs: Similarity, Diversity, and Bias](https://arxiv.org/abs/2505.09056)
Token length: 1271
Summarized using GPT-3.5-turbo
Append: [S-DAT: A Multilingual, GenAI-Driven Framework for Automated Divergent Thinking Assessment](https://arxiv.org/abs/2505.09068)
Token length: 1005
Summarized using GPT-3.5-turbo
Append: [CEC-Zero: Chinese Error Correction Solution Based on LLM](https://arxiv.org/abs/2505.09082)
Token length: 852
Summarized using GPT-3.5-turbo
Append: [How an unintended Side Effect of a Research Project led to Boosting the Power of UML](https://arxiv.org/abs/2505.09269)
Token length: 1804
Summarized using GPT-3.5-turbo
Append: [A Scalable Unsupervised Framework for multi-aspect labeling of Multilingual and Multi-Domain Review Data](https://arxiv.org/abs/2505.09286)
Token length: 1463
Summarized using GPT-3.5-turbo
Append: [Scent of Knowledge: Optimizing Search-Enhanced Reasoning with Information Foraging](https://arxiv.org/abs/2505.09316)
Token length: 1700
Summarized using GPT-3.5-turbo
Append: [Llama See, Llama Do: A Mechanistic Perspective on Contextual Entrainment and Distraction in LLMs](https://arxiv.org/abs/2505.09338)
Token length: 1830
Summarized using GPT-3.5-turbo
Append: [Qwen3 Technical Report](https://arxiv.org/abs/2505.09388)
Token length: 1320
Summarized using GPT-3.5-turbo
Append: [Multilingual Machine Translation with Quantum Encoder Decoder Attention-based Convolutional Variational Circuits](https://arxiv.org/abs/2505.09407)
Token length: 1534
Summarized using GPT-3.5-turbo
Append: [PT-MoE: An Efficient Finetuning Framework for Integrating Mixture-of-Experts into Prompt Tuning](https://arxiv.org/abs/2505.09519)
Token length: 1814
Summarized using GPT-3.5-turbo
Append: [WorldView-Bench: A Benchmark for Evaluating Global Cultural Perspectives in Large Language Models](https://arxiv.org/abs/2505.09595)
Token length: 1292
Summarized using GPT-3.5-turbo
Append: [The Geometry of Meaning: Perfect Spacetime Representations of Hierarchical Structures](https://arxiv.org/abs/2505.08795)
Token length: 1137
Summarized using GPT-3.5-turbo
Append: [An Extra RMSNorm is All You Need for Fine Tuning to 1.58 Bits](https://arxiv.org/abs/2505.08823)
Token length: 1483
Summarized using GPT-3.5-turbo
Append: [LibVulnWatch: A Deep Assessment Agent System and Leaderboard for Uncovering Hidden Vulnerabilities in Open-Source AI Libraries](https://arxiv.org/abs/2505.08842)
Token length: 1054
Summarized using GPT-3.5-turbo
Append: [Performance Gains of LLMs With Humans in a World of LLMs Versus Humans](https://arxiv.org/abs/2505.08902)
Token length: 1476
Summarized using GPT-3.5-turbo
Append: [Grounding Synthetic Data Evaluations of Language Models in Unsupervised Document Corpora](https://arxiv.org/abs/2505.08905)
Token length: 721
Summarized using GPT-3.5-turbo
Append: [Behind Maya: Building a Multilingual Vision Language Model](https://arxiv.org/abs/2505.08910)
Token length: 1095
Summarized using GPT-3.5-turbo
Append: [ForeCite: Adapting Pre-Trained Language Models to Predict Future Citation Rates of Academic Papers](https://arxiv.org/abs/2505.08941)
Token length: 1589
Summarized using GPT-3.5-turbo
Append: [Prioritizing Image-Related Tokens Enhances Vision-Language Pre-Training](https://arxiv.org/abs/2505.08971)
Token length: 1546
Summarized using GPT-3.5-turbo
Append: [Automated Meta Prompt Engineering for Alignment with the Theory of Mind](https://arxiv.org/abs/2505.09024)
Token length: 1142
Summarized using GPT-3.5-turbo
Append: [Improving the Reliability of LLMs: Combining CoT, RAG, Self-Consistency, and Self-Verification](https://arxiv.org/abs/2505.09031)
Token length: 793
Summarized using GPT-3.5-turbo
Append: [Ornithologist: Towards Trustworthy "Reasoning" about Central Bank Communications](https://arxiv.org/abs/2505.09083)
Token length: 1644
Summarized using GPT-3.5-turbo
Append: [Focus, Merge, Rank: Improved Question Answering Based on Semi-structured Knowledge Bases](https://arxiv.org/abs/2505.09246)
Token length: 1814
Summarized using GPT-3.5-turbo
Append: [CXMArena: Unified Dataset to benchmark performance in realistic CXM Scenarios](https://arxiv.org/abs/2505.09436)
Token length: 1877
Summarized using GPT-3.5-turbo
Append: [Customizing a Large Language Model for VHDL Design of High-Performance Microprocessors](https://arxiv.org/abs/2505.09610)
Token length: 1590
Summarized using GPT-3.5-turbo
Append: [Language Agents Mirror Human Causal Reasoning Biases. How Can We Help Them Think Like Scientists?](https://arxiv.org/abs/2505.09614)
Token length: 828
Summarized using GPT-3.5-turbo
Append: [LLM-based NLG Evaluation: Current Status and Challenges](https://arxiv.org/abs/2402.01383)
Token length: 1447
Summarized using GPT-3.5-turbo
Append: [Construction and Application of Materials Knowledge Graph in Multidisciplinary Materials Science via Large Language Model](https://arxiv.org/abs/2404.03080)
Token length: 1548
Summarized using GPT-3.5-turbo
Append: [FAMMA: A Benchmark for Financial Domain Multilingual Multimodal Question Answering](https://arxiv.org/abs/2410.04526)
Token length: 1179
Summarized using GPT-3.5-turbo
Append: [P-MMEval: A Parallel Multilingual Multitask Benchmark for Consistent Evaluation of LLMs](https://arxiv.org/abs/2411.09116)
Token length: 1419
Summarized using GPT-3.5-turbo
Append: [Principled Data Selection for Alignment: The Hidden Risks of Difficult Examples](https://arxiv.org/abs/2502.09650)
Token length: 1109
Summarized using GPT-3.5-turbo
Append: [PropNet: a White-Box and Human-Like Network for Sentence Representation](https://arxiv.org/abs/2502.10725)
Token length: 1911
Summarized using GPT-3.5-turbo
Append: [Simulating and Analysing Human Survey Responses with Large Language Models: A Case Study in Energy Stated Preference](https://arxiv.org/abs/2503.10652)
Token length: 1093
Summarized using GPT-3.5-turbo
Append: [Evaluating Clinical Competencies of Large Language Models with a General Practice Benchmark](https://arxiv.org/abs/2503.17599)
Token length: 1566
Summarized using GPT-3.5-turbo
Append: [Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks](https://arxiv.org/abs/2503.21696)
Token length: 991
Summarized using GPT-3.5-turbo
Append: [Is analogy enough to draw novel adjective-noun inferences?](https://arxiv.org/abs/2503.24293)
Token length: 1535
Summarized using GPT-3.5-turbo
Append: [What Features in Prompts Jailbreak LLMs? Investigating the Mechanisms Behind Attacks](https://arxiv.org/abs/2411.03343)
Token length: 1153
Summarized using GPT-3.5-turbo
Append: [FAS: Fast ANN-SNN Conversion for Spiking Large Language Models](https://arxiv.org/abs/2502.04405)
Token length: 1279
Summarized using GPT-3.5-turbo
Append: [InductionBench: LLMs Fail in the Simplest Complexity Class](https://arxiv.org/abs/2502.15823)
Token length: 1280
Summarized using GPT-3.5-turbo
Append: [An Analytical Emotion Framework of Rumour Threads on Social Media](https://arxiv.org/abs/2502.16560)
Token length: 1525
Summarized using GPT-3.5-turbo
Append: [Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study on Audio Question Answering](https://arxiv.org/abs/2503.11197)
append_entries: 46
Finish: 2025-05-15 04:25:30.181142
------------------------------------------------------
Started: 2025-05-15 06:24:21.829988
Existing_entries: 1046
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-15 06:24:21.987811
------------------------------------------------------
Started: 2025-05-15 08:22:29.976329
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-15 08:22:30.158590
------------------------------------------------------
Started: 2025-05-15 10:18:15.633083
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-15 10:18:15.809404
------------------------------------------------------
Started: 2025-05-15 12:33:45.423237
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-15 12:33:45.579638
------------------------------------------------------
Started: 2025-05-15 14:17:05.928336
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-15 14:17:06.101991
------------------------------------------------------
Started: 2025-05-15 16:20:49.317551
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-15 16:20:49.475292
------------------------------------------------------
Started: 2025-05-15 18:22:53.030341
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-15 18:22:53.214931
------------------------------------------------------
Started: 2025-05-15 20:18:17.266258
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-15 20:18:17.436020
------------------------------------------------------
Started: 2025-05-15 22:15:13.908506
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-15 22:15:14.073473
------------------------------------------------------
Started: 2025-05-16 01:19:05.234892
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-16 01:19:05.386928
------------------------------------------------------
Started: 2025-05-16 03:10:25.697402
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-16 03:10:25.902579
------------------------------------------------------
Started: 2025-05-16 04:25:39.740599
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 841
Summarized using GPT-3.5-turbo
Append: [Next Word Suggestion using Graph Neural Network](https://arxiv.org/abs/2505.09649)
Token length: 1394
Summarized using GPT-3.5-turbo
Append: [DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like Training of Large Language Models](https://arxiv.org/abs/2505.09655)
Token length: 1311
Summarized using GPT-3.5-turbo
Append: [Large Language Models Are More Persuasive Than Incentivized Human Persuaders](https://arxiv.org/abs/2505.09662)
Token length: 1406
Summarized using GPT-3.5-turbo
Append: [System Prompt Optimization with Meta-Learning](https://arxiv.org/abs/2505.09666)
Token length: 1369
Summarized using GPT-3.5-turbo
Append: [VeriFact: Enhancing Long-Form Factuality Evaluation with Refined Fact Extraction and Reference Facts](https://arxiv.org/abs/2505.09701)
Token length: 993
Summarized using GPT-3.5-turbo
Append: [An AI-Powered Research Assistant in the Lab: A Practical Guide for Text Analysis Through Iterative Collaboration with LLMs](https://arxiv.org/abs/2505.09724)
Token length: 1964
Summarized using GPT-3.5-turbo
Append: [Achieving Tokenizer Flexibility in Language Models through Heuristic Adaptation and Supertoken Learning](https://arxiv.org/abs/2505.09738)
Token length: 1967
Summarized using GPT-3.5-turbo
Append: [Automated Detection of Clinical Entities in Lung and Breast Cancer Reports Using NLP Techniques](https://arxiv.org/abs/2505.09794)
Token length: 922
Summarized using GPT-3.5-turbo
Append: [Exploring the generalization of LLM truth directions on conversational formats](https://arxiv.org/abs/2505.09807)
Token length: 1415
Summarized using GPT-3.5-turbo
Append: [KRISTEVA: Close Reading as a Novel Task for Benchmarking Interpretive Reasoning](https://arxiv.org/abs/2505.09825)
Token length: 1501
Summarized using GPT-3.5-turbo
Append: [Do Large Language Models Know Conflict? Investigating Parametric vs. Non-Parametric Knowledge of LLMs for Conflict Forecasting](https://arxiv.org/abs/2505.09852)
Token length: 1190
Summarized using GPT-3.5-turbo
Append: [Crossing Borders Without Crossing Boundaries: How Sociolinguistic Awareness Can Optimize User Engagement with Localized Spanish AI Models Across Hispanophone Countries](https://arxiv.org/abs/2505.09902)
Token length: 1235
Summarized using GPT-3.5-turbo
Append: [From Trade-off to Synergy: A Versatile Symbiotic Watermarking Framework for Large Language Models](https://arxiv.org/abs/2505.09924)
Token length: 1381
Summarized using GPT-3.5-turbo
Append: [Rethinking Prompt Optimizers: From Prompt Merits to Optimization](https://arxiv.org/abs/2505.09930)
Token length: 1286
Summarized using GPT-3.5-turbo
Append: [Personalizing Large Language Models using Retrieval Augmented Generation and Knowledge Graph](https://arxiv.org/abs/2505.09945)
Token length: 1095
Summarized using GPT-3.5-turbo
Append: [DIF: A Framework for Benchmarking and Verifying Implicit Bias in LLMs](https://arxiv.org/abs/2505.10013)
Token length: 1107
Summarized using GPT-3.5-turbo
Append: [CAFE: Retrieval Head-based Coarse-to-Fine Information Seeking to Enhance Multi-Document QA Capability](https://arxiv.org/abs/2505.10063)
Token length: 1558
Summarized using GPT-3.5-turbo
Append: [Dark LLMs: The Growing Threat of Unaligned AI Models](https://arxiv.org/abs/2505.10066)
Token length: 1228
Summarized using GPT-3.5-turbo
Append: [Designing and Contextualising Probes for African Languages](https://arxiv.org/abs/2505.10081)
Token length: 1214
Summarized using GPT-3.5-turbo
Append: [XRAG: Cross-lingual Retrieval-Augmented Generation](https://arxiv.org/abs/2505.10089)
Token length: 919
Summarized using GPT-3.5-turbo
Append: [What Does Neuro Mean to Cardio? Investigating the Role of Clinical Specialty Data in Medical LLMs](https://arxiv.org/abs/2505.10113)
Token length: 1330
Summarized using GPT-3.5-turbo
Append: [GE-Chat: A Graph Enhanced RAG Framework for Evidential Response Generation of LLMs](https://arxiv.org/abs/2505.10143)
Token length: 1566
Summarized using GPT-3.5-turbo
Append: [Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with Synthetic Data for LLM Reasoning](https://arxiv.org/abs/2505.10182)
Token length: 1337
Summarized using GPT-3.5-turbo
Append: [The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a Reasoning Model will Think](https://arxiv.org/abs/2505.10185)
Token length: 1537
Summarized using GPT-3.5-turbo
Append: [VQ-Logits: Compressing the Output Bottleneck of Large Language Models via Vector Quantized Logits](https://arxiv.org/abs/2505.10202)
Token length: 1156
Summarized using GPT-3.5-turbo
Append: [RAIDEN-R1: Improving Role-awareness of LLMs via GRPO with Verifiable Reward](https://arxiv.org/abs/2505.10218)
Token length: 1570
Summarized using GPT-3.5-turbo
Append: [Comparing LLM Text Annotation Skills: A Study on Human Rights Violations in Social Media Data](https://arxiv.org/abs/2505.10260)
Token length: 587
Summarized using GPT-3.5-turbo
Append: [The Evolving Landscape of Generative Large Language Models and Traditional Natural Language Processing in Medicine](https://arxiv.org/abs/2505.10261)
Token length: 1929
Summarized using GPT-3.5-turbo
Append: [From Questions to Clinical Recommendations: Large Language Models Driving Evidence-Based Clinical Decision Making](https://arxiv.org/abs/2505.10282)
Token length: 1224
Summarized using GPT-3.5-turbo
Append: [J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning](https://arxiv.org/abs/2505.10320)
Token length: 1347
Summarized using GPT-3.5-turbo
Append: [LDIR: Low-Dimensional Dense and Interpretable Text Embeddings with Relative Representations](https://arxiv.org/abs/2505.10354)
Token length: 957
Summarized using GPT-3.5-turbo
Append: [Coherent Language Reconstruction from Brain Recordings with Flexible Multi-Modal Input Stimuli](https://arxiv.org/abs/2505.10356)
Token length: 842
Summarized using GPT-3.5-turbo
Append: [Multi-domain Multilingual Sentiment Analysis in Industry: Predicting Aspect-based Opinion Quadruples](https://arxiv.org/abs/2505.10389)
Token length: 1501
Summarized using GPT-3.5-turbo
Append: [Rethinking Repetition Problems of LLMs in Code Generation](https://arxiv.org/abs/2505.10402)
Token length: 1744
Summarized using GPT-3.5-turbo
Append: [Are LLM-generated plain language summaries truly understandable? A large-scale crowdsourced evaluation](https://arxiv.org/abs/2505.10409)
Token length: 968
Summarized using GPT-3.5-turbo
Append: [Hierarchical Document Refinement for Long-context Retrieval-augmented Generation](https://arxiv.org/abs/2505.10413)
Token length: 1550
Summarized using GPT-3.5-turbo
Append: [Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models](https://arxiv.org/abs/2505.10446)
Token length: 1384
Summarized using GPT-3.5-turbo
Append: [CL-RAG: Bridging the Gap in Retrieval-Augmented Generation with Curriculum Learning](https://arxiv.org/abs/2505.10493)
Token length: 1267
Summarized using GPT-3.5-turbo
Append: [Can You Really Trust Code Copilots? Evaluating Large Language Models from a Code Security Perspective](https://arxiv.org/abs/2505.10494)
Token length: 1829
Summarized using GPT-3.5-turbo
Append: [The Devil Is in the Word Alignment Details: On Translation-Based Cross-Lingual Transfer for Token Classification Tasks](https://arxiv.org/abs/2505.10507)
Token length: 1179
Summarized using GPT-3.5-turbo
Append: [Multi-Token Prediction Needs Registers](https://arxiv.org/abs/2505.10518)
Token length: 1643
Summarized using GPT-3.5-turbo
Append: [WorldPM: Scaling Human Preference Modeling](https://arxiv.org/abs/2505.10527)
Token length: 1360
Summarized using GPT-3.5-turbo
Append: [Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large Reasoning Models](https://arxiv.org/abs/2505.10554)
Token length: 1961
Summarized using GPT-3.5-turbo
Append: [Tales of the 2025 Los Angeles Fire: Hotwash for Public Health Concerns in Reddit via LLM-Enhanced Topic Modeling](https://arxiv.org/abs/2505.09665)
Token length: 1130
Summarized using GPT-3.5-turbo
Append: [A Survey on Large Language Models in Multimodal Recommender Systems](https://arxiv.org/abs/2505.09777)
Token length: 1847
Summarized using GPT-3.5-turbo
Append: [Predictability Shapes Adaptation: An Evolutionary Perspective on Modes of Learning in Transformers](https://arxiv.org/abs/2505.09855)
Token length: 1518
Summarized using GPT-3.5-turbo
Append: [Comparing Exploration-Exploitation Strategies of LLMs and Humans: Insights from Standard Multi-armed Bandit Tasks](https://arxiv.org/abs/2505.09901)
Token length: 1439
Summarized using GPT-3.5-turbo
Append: [PIG: Privacy Jailbreak Attack on LLMs via Gradient-based Iterative In-Context Optimization](https://arxiv.org/abs/2505.09921)
Token length: 1886
Summarized using GPT-3.5-turbo
Append: [Advanced Crash Causation Analysis for Freeway Safety: A Large Language Model Approach to Identifying Key Contributing Factors](https://arxiv.org/abs/2505.09949)
Token length: 1601
Summarized using GPT-3.5-turbo
Append: [From Text to Network: Constructing a Knowledge Graph of Taiwan-Based China Studies Using Generative AI](https://arxiv.org/abs/2505.10093)
Append: [Learning Virtual Machine Scheduling in Cloud Computing through Language Agents](https://arxiv.org/abs/2505.10117)
Append: [Why 1 + 1 < 1 in Visual Token Pruning: Beyond Naive Integration via Multi-Objective Balanced Covering](https://arxiv.org/abs/2505.10118)
Append: [ComplexFormer: Disruptively Advancing Transformer Inference Ability via Head-Specific Complex Vector Attention](https://arxiv.org/abs/2505.10222)
Append: [On the Interplay of Human-AI Alignment,Fairness, and Performance Trade-offs in Medical Imaging](https://arxiv.org/abs/2505.10231)
Append: [StoryReasoning Dataset: Using Chain-of-Thought for Scene Understanding and Grounded Story Generation](https://arxiv.org/abs/2505.10292)
Append: [Superposition Yields Robust Neural Scaling](https://arxiv.org/abs/2505.10465)
Append: [Parallel Scaling Law for Language Models](https://arxiv.org/abs/2505.10475)
Append: [RouteNator: A Router-Based Multi-Modal Architecture for Generating Synthetic Training Data for Function Calling LLMs](https://arxiv.org/abs/2505.10495)
Append: [MASSV: Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of Vision-Language Models](https://arxiv.org/abs/2505.10526)
Append: [Towards a Deeper Understanding of Reasoning Capabilities in Large Language Models](https://arxiv.org/abs/2505.10543)
Append: [MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal Mathematical Reasoning](https://arxiv.org/abs/2505.10557)
Append: [Temporal Scaling Law for Large Language Models](https://arxiv.org/abs/2404.17785)
Append: [The Mosaic Memory of Large Language Models](https://arxiv.org/abs/2405.15523)
Append: [Tokenization Matters! Degrading Large Language Models through Challenging Their Tokenization](https://arxiv.org/abs/2405.17067)
Append: [RoBERTa-BiLSTM: A Context-Aware Hybrid Model for Sentiment Analysis](https://arxiv.org/abs/2406.00367)
Append: [PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information Funneling](https://arxiv.org/abs/2406.02069)
Append: [uDistil-Whisper: Label-Free Data Filtering for Knowledge Distillation in Low-Data Regimes](https://arxiv.org/abs/2407.01257)
Append: [Conversational Query Reformulation with the Guidance of Retrieved Documents](https://arxiv.org/abs/2407.12363)
Append: [PersLLM: A Personified Training Approach for Large Language Models](https://arxiv.org/abs/2407.12393)
Append: [Beyond Next Token Prediction: Patch-Level Training for Large Language Models](https://arxiv.org/abs/2407.12665)
Append: [Compensate Quantization Errors+: Quantized Models Are Inquisitive Learners](https://arxiv.org/abs/2407.15508)
Append: [Time Awareness in Large Language Models: Benchmarking Fact Recall Across Time](https://arxiv.org/abs/2409.13338)
Append: [MultiMed: Multilingual Medical Speech Recognition via Attention Encoder Decoder](https://arxiv.org/abs/2409.14074)
Append: [TopoLM: brain-like spatio-functional organization in a topographic language model](https://arxiv.org/abs/2410.11516)
Append: [How Does Knowledge Selection Help Retrieval Augmented Generation?](https://arxiv.org/abs/2410.13258)
Append: [ChronoFact: Timeline-based Temporal Fact Verification](https://arxiv.org/abs/2410.14964)
Append: [SceneGenAgent: Precise Industrial Scene Generation with Coding Agent](https://arxiv.org/abs/2410.21909)
Append: [Phase Diagram of Vision Large Language Models Inference: A Perspective from Interaction across Image and Instruction](https://arxiv.org/abs/2411.00646)
Append: [Disentangling Memory and Reasoning Ability in Large Language Models](https://arxiv.org/abs/2411.13504)
Append: [KBAlign: Efficient Self Adaptation on Specific Knowledge Bases](https://arxiv.org/abs/2411.14790)
Append: [Simple and Provable Scaling Laws for the Test-Time Compute of Large Language Models](https://arxiv.org/abs/2411.19477)
Append: [Not All Adapters Matter: Selective Adapter Freezing for Memory-Efficient Fine-Tuning of Language Models](https://arxiv.org/abs/2412.03587)
Append: [FitCF: A Framework for Automatic Feature Importance-guided Counterfactual Example Generation](https://arxiv.org/abs/2501.00777)
Append: [Benchmarking Generative AI for Scoring Medical Student Interviews in Objective Structured Clinical Examinations (OSCEs)](https://arxiv.org/abs/2501.13957)
Append: [TensorLLM: Tensorising Multi-Head Attention for Enhanced Reasoning and Compression in LLMs](https://arxiv.org/abs/2501.15674)
Append: [ARR: Question Answering with Large Language Models via Analyzing, Retrieving, and Reasoning](https://arxiv.org/abs/2502.04689)
Append: [Harnessing Multiple Large Language Models: A Survey on LLM Ensemble](https://arxiv.org/abs/2502.18036)
Append: [KwaiChat: A Large-Scale Video-Driven Multilingual Mixed-Type Dialogue Corpus](https://arxiv.org/abs/2503.06899)
Append: [Concise Reasoning via Reinforcement Learning](https://arxiv.org/abs/2504.05185)
Append: [Model Utility Law: Evaluating LLMs beyond Performance through Mechanism Interpretable Metric](https://arxiv.org/abs/2504.07440)
Append: [CLASH: Evaluating Language Models on Judging High-Stakes Dilemmas from Multiple Perspectives](https://arxiv.org/abs/2504.10823)
Append: [Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction](https://arxiv.org/abs/2504.17671)
Append: [Pose Priors from Language Models](https://arxiv.org/abs/2405.03689)
Append: [Latent Action Pretraining from Videos](https://arxiv.org/abs/2410.11758)
Append: [Natural Language Reinforcement Learning](https://arxiv.org/abs/2411.14251)
Append: [Mitigating Modality Bias in Multi-modal Entity Alignment from a Causal Perspective](https://arxiv.org/abs/2504.19458)
append_entries: 96
Finish: 2025-05-16 04:28:00.376203
------------------------------------------------------
Started: 2025-05-16 06:24:50.159901
Existing_entries: 1096
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-16 06:24:50.403957
------------------------------------------------------
Started: 2025-05-16 08:22:04.392653
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-16 08:22:04.648402
------------------------------------------------------
Started: 2025-05-16 10:17:38.361279
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-16 10:17:38.613920
------------------------------------------------------
Started: 2025-05-16 12:33:50.559468
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-16 12:33:50.840442
------------------------------------------------------
Started: 2025-05-16 14:16:01.577993
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-16 14:16:01.823176
------------------------------------------------------
Started: 2025-05-16 16:20:17.493465
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-16 16:20:17.738159
------------------------------------------------------
Started: 2025-05-16 18:22:27.241999
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-16 18:22:27.530125
------------------------------------------------------
Started: 2025-05-16 20:17:56.667442
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-16 20:17:56.984374
------------------------------------------------------
Started: 2025-05-16 22:15:23.444669
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-16 22:15:23.691005
------------------------------------------------------
Started: 2025-05-17 01:17:14.205421
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-17 01:17:14.518592
------------------------------------------------------
Started: 2025-05-17 03:05:25.955719
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-17 03:05:26.211248
------------------------------------------------------
Started: 2025-05-17 04:19:23.134244
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-17 04:19:23.214468
------------------------------------------------------
Started: 2025-05-17 06:21:23.492830
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-17 06:21:23.560662
------------------------------------------------------
Started: 2025-05-17 08:19:27.543713
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-17 08:19:27.627887
------------------------------------------------------
Started: 2025-05-17 10:16:34.255954
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-17 10:16:34.318065
------------------------------------------------------
Started: 2025-05-17 12:29:58.284037
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-17 12:29:58.347130
------------------------------------------------------
Started: 2025-05-17 14:13:39.650637
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-17 14:13:39.720559
------------------------------------------------------
Started: 2025-05-17 16:18:42.753874
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-17 16:18:42.813312
------------------------------------------------------
Started: 2025-05-17 18:20:45.399746
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-17 18:20:45.490750
------------------------------------------------------
Started: 2025-05-17 20:16:43.439064
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-17 20:16:43.504094
------------------------------------------------------
Started: 2025-05-17 22:14:06.814953
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-17 22:14:06.893712
------------------------------------------------------
Started: 2025-05-18 01:23:51.398096
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-18 01:23:51.496391
------------------------------------------------------
Started: 2025-05-18 03:15:15.477630
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-18 03:15:15.544305
------------------------------------------------------
Started: 2025-05-18 04:21:52.888062
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-18 04:21:52.953720
------------------------------------------------------
Started: 2025-05-18 06:21:58.213143
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-18 06:21:58.275032
------------------------------------------------------
Started: 2025-05-18 08:19:06.838699
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-18 08:19:06.930797
------------------------------------------------------
Started: 2025-05-18 10:16:00.678084
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-18 10:16:00.772593
------------------------------------------------------
Started: 2025-05-18 12:30:22.155034
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-18 12:30:22.219013
------------------------------------------------------
Started: 2025-05-18 14:13:42.887998
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-18 14:13:42.947718
------------------------------------------------------
Started: 2025-05-18 16:18:03.880707
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-18 16:18:03.959610
------------------------------------------------------
Started: 2025-05-18 18:20:41.159262
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-18 18:20:41.220329
------------------------------------------------------
Started: 2025-05-18 20:17:29.277522
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-18 20:17:29.338751
------------------------------------------------------
Started: 2025-05-18 22:14:39.650678
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-18 22:14:39.709852
------------------------------------------------------
Started: 2025-05-19 01:22:19.392978
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-19 01:22:19.454334
------------------------------------------------------
Started: 2025-05-19 03:15:52.418636
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-19 03:15:52.478690
------------------------------------------------------
Started: 2025-05-19 04:29:32.914640
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1307
Summarized using GPT-3.5-turbo
Append: [Artificial Intelligence Bias on English Language Learners in Automatic Scoring](https://arxiv.org/abs/2505.10643)
Token length: 1319
Summarized using GPT-3.5-turbo
Append: [GeoGrid-Bench: Can Foundation Models Understand Multimodal Gridded Geo-Spatial Data?](https://arxiv.org/abs/2505.10714)
Token length: 1532
Summarized using GPT-3.5-turbo
Append: [A Modular Approach for Clinical SLMs Driven by Synthetic Data with Pre-Instruction Tuning, Model Merging, and Clinical-Tasks Alignment](https://arxiv.org/abs/2505.10717)
Token length: 1003
Summarized using GPT-3.5-turbo
Append: [AI-enhanced semantic feature norms for 786 concepts](https://arxiv.org/abs/2505.10718)
Token length: 1286
Summarized using GPT-3.5-turbo
Append: [Tracr-Injection: Distilling Algorithms into Pre-trained Language Models](https://arxiv.org/abs/2505.10719)
Token length: 1438
Summarized using GPT-3.5-turbo
Append: [Model Performance-Guided Evaluation Data Selection for Effective Prompt Optimization](https://arxiv.org/abs/2505.10736)
Token length: 1205
Summarized using GPT-3.5-turbo
Append: [SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval](https://arxiv.org/abs/2505.10740)
Token length: 1342
Summarized using GPT-3.5-turbo
Append: [Ranked Voting based Self-Consistency of Large Language Models](https://arxiv.org/abs/2505.10772)
Token length: 1086
Summarized using GPT-3.5-turbo
Append: [A Systematic Analysis of Base Model Choice for Reward Modeling](https://arxiv.org/abs/2505.10775)
Token length: 891
Summarized using GPT-3.5-turbo
Append: [Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation](https://arxiv.org/abs/2505.10792)
Token length: 1192
Summarized using GPT-3.5-turbo
Append: [Relation Extraction Across Entire Books to Reconstruct Community Networks: The AffilKG Datasets](https://arxiv.org/abs/2505.10798)
Token length: 1149
Summarized using GPT-3.5-turbo
Append: [Enhancing Low-Resource Minority Language Translation with LLMs and Retrieval-Augmented Generation for Cultural Nuances](https://arxiv.org/abs/2505.10829)
Token length: 1575
Summarized using GPT-3.5-turbo
Append: [Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL](https://arxiv.org/abs/2505.10832)
Token length: 1098
Summarized using GPT-3.5-turbo
Append: [Multimodal Event Detection: Current Approaches and Defining the New Playground through LLMs and VLMs](https://arxiv.org/abs/2505.10836)
Token length: 755
Summarized using GPT-3.5-turbo
Append: [Have Multimodal Large Language Models (MLLMs) Really Learned to Tell the Time on Analog Clocks?](https://arxiv.org/abs/2505.10862)
Token length: 1353
Summarized using GPT-3.5-turbo
Append: [Improve Rule Retrieval and Reasoning with Self-Induction and Relevance ReEstimate](https://arxiv.org/abs/2505.10870)
Token length: 1513
Summarized using GPT-3.5-turbo
Append: [A Survey on the Safety and Security Threats of Computer-Using Agents: JARVIS or Ultron?](https://arxiv.org/abs/2505.10924)
Token length: 1486
Summarized using GPT-3.5-turbo
Append: [Connecting the Dots: A Chain-of-Collaboration Prompting Framework for LLM Agents](https://arxiv.org/abs/2505.10936)
Token length: 1623
Summarized using GPT-3.5-turbo
Append: [Reasoning with OmniThought: A Large CoT Dataset with Verbosity and Cognitive Difficulty Annotations](https://arxiv.org/abs/2505.10937)
Token length: 1275
Summarized using GPT-3.5-turbo
Append: [Accurate KV Cache Quantization with Outlier Tokens Tracing](https://arxiv.org/abs/2505.10938)
Token length: 1367
Summarized using GPT-3.5-turbo
Append: [GenKnowSub: Improving Modularity and Reusability of LLMs through General Knowledge Subtraction](https://arxiv.org/abs/2505.10939)
Token length: 1357
Summarized using GPT-3.5-turbo
Append: [Semantic Aware Linear Transfer by Recycling Pre-trained Language Models for Cross-lingual Transfer](https://arxiv.org/abs/2505.10945)
Token length: 926
Summarized using GPT-3.5-turbo
Append: [The Way We Prompt: Conceptual Blending, Neural Dynamics, and Prompt-Induced Transitions in LLMs](https://arxiv.org/abs/2505.10948)
Token length: 1290
Summarized using GPT-3.5-turbo
Append: [Survey of End-to-End Multi-Speaker Automatic Speech Recognition for Monaural Audio](https://arxiv.org/abs/2505.10975)
Token length: 1458
Summarized using GPT-3.5-turbo
Append: [Illusion or Algorithm? Investigating Memorization, Emergence, and Symbolic Processing in In-Context Learning](https://arxiv.org/abs/2505.11004)
Token length: 1151
Summarized using GPT-3.5-turbo
Append: [Reconstructing Syllable Sequences in Abugida Scripts with Incomplete Inputs](https://arxiv.org/abs/2505.11008)
Token length: 1331
Summarized using GPT-3.5-turbo
Append: [Review-Instruct: A Review-Driven Multi-Turn Conversations Generation Method for Large Language Models](https://arxiv.org/abs/2505.11010)
Token length: 896
Summarized using GPT-3.5-turbo
Append: [StRuCom: A Novel Dataset of Structured Code Comments in Russian](https://arxiv.org/abs/2505.11026)
Token length: 1263
Summarized using GPT-3.5-turbo
Append: [OntoURL: A Benchmark for Evaluating Large Language Models on Symbolic Ontological Understanding, Reasoning and Learning](https://arxiv.org/abs/2505.11031)
Token length: 727
Summarized using GPT-3.5-turbo
Append: [CAMEO: Collection of Multilingual Emotional Speech Corpora](https://arxiv.org/abs/2505.11051)
Token length: 1626
Summarized using GPT-3.5-turbo
Append: [BLEUBERI: BLEU is a surprisingly effective reward for instruction following](https://arxiv.org/abs/2505.11080)
Token length: 1320
Summarized using GPT-3.5-turbo
Append: [Towards Better Evaluation for Generated Patent Claims](https://arxiv.org/abs/2505.11095)
Token length: 1793
Summarized using GPT-3.5-turbo
Append: [Scaling Reasoning can Improve Factuality in Large Language Models](https://arxiv.org/abs/2505.11140)
Token length: 1623
Summarized using GPT-3.5-turbo
Append: [SoLoPO: Unlocking Long-Context Capabilities in LLMs via Short-to-Long Preference Optimization](https://arxiv.org/abs/2505.11166)
Token length: 864
Summarized using GPT-3.5-turbo
Append: [Low-Resource Language Processing: An OCR-Driven Summarization and Translation Pipeline](https://arxiv.org/abs/2505.11177)
Token length: 1698
Summarized using GPT-3.5-turbo
Append: [NoPE: The Counting Power of Transformers with No Positional Encodings](https://arxiv.org/abs/2505.11199)
Token length: 1609
Summarized using GPT-3.5-turbo
Append: [HAPO: Training Language Models to Reason Concisely via History-Aware Policy Optimization](https://arxiv.org/abs/2505.11225)
Token length: 851
Summarized using GPT-3.5-turbo
Append: [Semantic Caching of Contextual Summaries for Efficient Question-Answering with Language Models](https://arxiv.org/abs/2505.11271)
Token length: 1170
Summarized using GPT-3.5-turbo
Append: [Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning of LLMs](https://arxiv.org/abs/2505.11277)
Token length: 1473
Summarized using GPT-3.5-turbo
Append: [Temporal fine-tuning for early risk detection](https://arxiv.org/abs/2505.11280)
Token length: 1013
Summarized using GPT-3.5-turbo
Append: [Probing Subphonemes in Morphology Models](https://arxiv.org/abs/2505.11297)
Token length: 1371
Summarized using GPT-3.5-turbo
Append: [XtraGPT: LLMs for Human-AI Collaboration on Controllable Academic Paper Revision](https://arxiv.org/abs/2505.11336)
Token length: 1080
Summarized using GPT-3.5-turbo
Append: [Benchmarking Critical Questions Generation: A Challenging Reasoning Task for Large Language Models](https://arxiv.org/abs/2505.11341)
Token length: 1574
Summarized using GPT-3.5-turbo
Append: [LegoSLM: Connecting LLM with Speech Encoder using CTC Posteriors](https://arxiv.org/abs/2505.11352)
Token length: 1369
Summarized using GPT-3.5-turbo
Append: [GuideBench: Benchmarking Domain-Oriented Guideline Following for LLM Agents](https://arxiv.org/abs/2505.11368)
Token length: 1608
Summarized using GPT-3.5-turbo
Append: [A computational system to handle the orthographic layer of tajwid in contemporary Quranic Orthography](https://arxiv.org/abs/2505.11379)
Token length: 1382
Summarized using GPT-3.5-turbo
Append: [CARES: Comprehensive Evaluation of Safety and Adversarial Robustness in Medical LLMs](https://arxiv.org/abs/2505.11413)
Token length: 1725
Summarized using GPT-3.5-turbo
Append: [Towards Cultural Bridge by Bahnaric-Vietnamese Translation Using Transfer Learning of Sequence-To-Sequence Pre-training Language Model](https://arxiv.org/abs/2505.11421)
Token length: 1541
Summarized using GPT-3.5-turbo
Append: [When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs](https://arxiv.org/abs/2505.11423)
Token length: 1483
Summarized using GPT-3.5-turbo
Append: [GODBench: A Benchmark for Multimodal Large Language Models in Video Comment Art](https://arxiv.org/abs/2505.11436)
Append: [Is Compression Really Linear with Code Intelligence?](https://arxiv.org/abs/2505.11441)
Append: [Disentangling Reasoning and Knowledge in Medical Large Language Models](https://arxiv.org/abs/2505.11462)
Append: [No Gold Standard, No Problem: Reference-Free Evaluation of Taxonomies](https://arxiv.org/abs/2505.11470)
Append: [HelpSteer3-Preference: Open Human-Annotated Preference Data across Diverse Tasks and Languages](https://arxiv.org/abs/2505.11475)
Append: [Improving Assembly Code Performance with Large Language Models via Reinforcement Learning](https://arxiv.org/abs/2505.11480)
Append: [SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.11484)
Append: [Modeling cognitive processes of natural reading with transformer-based Language Models](https://arxiv.org/abs/2505.11485)
Append: [Relative Drawing Identification Complexity is Invariant to Modality in Vision-Language Models](https://arxiv.org/abs/2505.10583)
Append: [Towards Automated Situation Awareness: A RAG-Based Framework for Peacebuilding Reports](https://arxiv.org/abs/2505.10586)
Append: [Understanding Gen Alpha Digital Language: Evaluation of LLM Safety Systems for Content Moderation](https://arxiv.org/abs/2505.10588)
Append: [Two Minds Better Than One: Collaborative Reward Modeling for LLM Alignment](https://arxiv.org/abs/2505.10597)
Append: [UDDETTS: Unifying Discrete and Dimensional Emotions for Controllable Emotional Text-to-Speech](https://arxiv.org/abs/2505.10599)
Append: [MMLongBench: Benchmarking Long-Context Vision-Language Models Effectively and Thoroughly](https://arxiv.org/abs/2505.10610)
Append: [Creating General User Models from Computer Use](https://arxiv.org/abs/2505.10831)
Append: [LARGO: Latent Adversarial Reflection through Gradient Optimization for Jailbreaking LLMs](https://arxiv.org/abs/2505.10838)
Append: [Creativity or Brute Force? Using Brainteasers as a Window into the Problem-Solving Abilities of Large Language Models](https://arxiv.org/abs/2505.10844)
Append: [MatTools: Benchmarking Large Language Models for Materials Science Tools](https://arxiv.org/abs/2505.10852)
Append: [REI-Bench: Can Embodied Agents Understand Vague Human Instructions in Task Planning?](https://arxiv.org/abs/2505.10872)
Append: [Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling: A Perspective of Probability Theory](https://arxiv.org/abs/2505.10981)
Append: [$\mathcal{A}LLM4ADD$: Unlocking the Capabilities of Audio Large Language Models for Audio Deepfake Detection](https://arxiv.org/abs/2505.11079)
Append: [MPMA: Preference Manipulation Attack Against Model Context Protocol](https://arxiv.org/abs/2505.11154)
Append: [Maximizing Asynchronicity in Event-based Neural Networks](https://arxiv.org/abs/2505.11165)
Append: [CompAlign: Improving Compositional Text-to-Image Generation with a Complex Benchmark and Fine-Grained Feedback](https://arxiv.org/abs/2505.11178)
Append: [On Next-Token Prediction in LLMs: How End Goals Determine the Consistency of Decoding Algorithms](https://arxiv.org/abs/2505.11183)
Append: [Audio Turing Test: Benchmarking the Human-likeness of Large Language Model-based Text-to-Speech Systems in Chinese](https://arxiv.org/abs/2505.11200)
Append: [SelfBudgeter: Adaptive Token Allocation for Efficient LLM Reasoning](https://arxiv.org/abs/2505.11274)
Append: [CROC: Evaluating and Training T2I Metrics with Pseudo- and Human-Labeled Contrastive Robustness Checks](https://arxiv.org/abs/2505.11314)
Append: [Phare: A Safety Probe for Large Language Models](https://arxiv.org/abs/2505.11365)
Append: [EmotionHallucer: Evaluating Emotion Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2505.11405)
Append: [Large Language Model Use Impact Locus of Control](https://arxiv.org/abs/2505.11406)
Append: [Visual Planning: Let's Think Only with Images](https://arxiv.org/abs/2505.11409)
Append: [Fourier Transformer: Fast Long Range Modeling by Removing Sequence Redundancy with FFT Operator](https://arxiv.org/abs/2305.15099)
Append: [Can Authorship Attribution Models Distinguish Speakers in Speech Transcripts?](https://arxiv.org/abs/2311.07564)
Append: [COBIAS: Assessing the Contextual Reliability of Bias Benchmarks for Language Models](https://arxiv.org/abs/2402.14889)
Append: [ViTextVQA: A Large-Scale Visual Question Answering Dataset for Evaluating Vietnamese Text Comprehension in Images](https://arxiv.org/abs/2404.10652)
Append: [Towards Adapting Open-Source Large Language Models for Expert-Level Clinical Note Generation](https://arxiv.org/abs/2405.00715)
Append: [Self-Tuning: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching](https://arxiv.org/abs/2406.06326)
Append: [Does Liking Yellow Imply Driving a School Bus? Semantic Leakage in Language Models](https://arxiv.org/abs/2408.06518)
Append: [Towards understanding evolution of science through language model series](https://arxiv.org/abs/2409.09636)
Append: [Divided by discipline? A systematic literature review on the quantification of online sexism and misogyny using a semi-automated approach](https://arxiv.org/abs/2409.20204)
Append: [Training of Scaffolded Language Models with Language Supervision: A Survey](https://arxiv.org/abs/2410.16392)
Append: [ShifCon: Enhancing Non-Dominant Language Capabilities with a Shift-based Contrastive Framework](https://arxiv.org/abs/2410.19453)
Append: [How Good is Your Wikipedia? Auditing Data Quality for Low-resource and Multilingual NLP](https://arxiv.org/abs/2411.05527)
Append: [UniHR: Hierarchical Representation Learning for Unified Knowledge Graph Link Prediction](https://arxiv.org/abs/2411.07019)
Append: [On the Role of Speech Data in Reducing Toxicity Detection Bias](https://arxiv.org/abs/2411.08135)
Append: [When to Speak, When to Abstain: Contrastive Decoding with Abstention](https://arxiv.org/abs/2412.12527)
Append: [What External Knowledge is Preferred by LLMs? Characterizing and Exploring Chain of Evidence in Imperfect Context](https://arxiv.org/abs/2412.12632)
Append: [XRAG: eXamining the Core -- Benchmarking Foundational Components in Advanced Retrieval-Augmented Generation](https://arxiv.org/abs/2412.15529)
Append: [Dynamics of Adversarial Attacks on Large Language Model-Based Search Engines](https://arxiv.org/abs/2501.00745)
Append: [LLM Content Moderation and User Satisfaction: Evidence from Response Refusals in Chatbot Arena](https://arxiv.org/abs/2501.03266)
Append: [TreeKV: Smooth Key-Value Cache Compression with Tree Structures](https://arxiv.org/abs/2501.04987)
Append: [Know Your Mistakes: Towards Preventing Overreliance on Task-Oriented Conversational AI Through Accountability Modeling](https://arxiv.org/abs/2501.10316)
Append: [Med-R$^2$: Crafting Trustworthy LLM Physicians via Retrieval and Reasoning of Evidence-Based Medicine](https://arxiv.org/abs/2501.11885)
Append: [Re-ranking Using Large Language Models for Mitigating Exposure to Harmful Content on Social Media Platforms](https://arxiv.org/abs/2501.13977)
Append: [Do we really have to filter out random noise in pre-training data for language models?](https://arxiv.org/abs/2502.06604)
Append: [Mix Data or Merge Models? Balancing the Helpfulness, Honesty, and Harmlessness of Large Language Model via Model Merging](https://arxiv.org/abs/2502.06876)
Append: [Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More](https://arxiv.org/abs/2502.07490)
Append: [Hallucination, Monofacts, and Miscalibration: An Empirical Investigation](https://arxiv.org/abs/2502.08666)
Append: [Investigating Language Preference of Multilingual RAG Systems](https://arxiv.org/abs/2502.11175)
Append: [Can Your Uncertainty Scores Detect Hallucinated Entity?](https://arxiv.org/abs/2502.11948)
Append: [iAgent: LLM Agent as a Shield between User and Recommender Systems](https://arxiv.org/abs/2502.14662)
Append: [Unveiling Attractor Cycles in Large Language Models: A Dynamical Systems View of Successive Paraphrasing](https://arxiv.org/abs/2502.15208)
Append: [Call for Rigor in Reporting Quality of Instruction Tuning Data](https://arxiv.org/abs/2503.04807)
Append: [TigerLLM -- A Family of Bangla Large Language Models](https://arxiv.org/abs/2503.10995)
Append: [KVShare: An LLM Service System with Efficient and Effective Multi-Tenant KV Cache Reuse](https://arxiv.org/abs/2503.16525)
Append: [Safety Evaluation and Enhancement of DeepSeek Models in Chinese Contexts](https://arxiv.org/abs/2503.16529)
Append: [Mixture of Routers](https://arxiv.org/abs/2503.23362)
Append: [Do Theory of Mind Benchmarks Need Explicit Human-like Reasoning in Language Models?](https://arxiv.org/abs/2504.01698)
Append: [Parameterized Synthetic Text Generation with SimpleStories](https://arxiv.org/abs/2504.09184)
Append: [Safety in Large Reasoning Models: A Survey](https://arxiv.org/abs/2504.17704)
Append: [Customizing Visual-Language Foundation Models for Multi-modal Anomaly Detection and Reasoning](https://arxiv.org/abs/2403.11083)
Append: [Linear Attention Sequence Parallelism](https://arxiv.org/abs/2404.02882)
Append: [Intervention-Aware Forecasting: Breaking Historical Limits from a System Perspective](https://arxiv.org/abs/2405.13522)
Append: [Item-Language Model for Conversational Recommendation](https://arxiv.org/abs/2406.02844)
Append: [Words in Motion: Extracting Interpretable Control Vectors for Motion Transformers](https://arxiv.org/abs/2406.11624)
Append: [Strategic Collusion of LLM Agents: Market Division in Multi-Commodity Competitions](https://arxiv.org/abs/2410.00031)
Append: [TestAgent: A Framework for Domain-Adaptive Evaluation of LLMs via Dynamic Benchmark Construction and Exploratory Interaction](https://arxiv.org/abs/2410.11507)
Append: [DiSCo: LLM Knowledge Distillation for Efficient Sparse Retrieval in Conversational Search](https://arxiv.org/abs/2410.14609)
Append: [MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal Projection](https://arxiv.org/abs/2410.14731)
Append: [Sparsing Law: Towards Large Language Models with Greater Activation Sparsity](https://arxiv.org/abs/2411.02335)
Append: [Evaluating Vision-Language Models as Evaluators in Path Planning](https://arxiv.org/abs/2411.18711)
Append: [Zero-Shot Statistical Tests for LLM-Generated Text Detection using Finite Sample Concentration Inequalities](https://arxiv.org/abs/2501.02406)
Append: [Fine-Tuning Discrete Diffusion Models with Policy Gradient Methods](https://arxiv.org/abs/2502.01384)
Append: [Shuttle Between the Instructions and the Parameters of Large Language Models](https://arxiv.org/abs/2502.02315)
Append: [MIR-Bench: Can Your LLM Recognize Complicated Patterns via Many-Shot In-Context Reasoning?](https://arxiv.org/abs/2502.09933)
Append: [FOReCAst: The Future Outcome Reasoning and Confidence Assessment Benchmark](https://arxiv.org/abs/2502.19676)
Append: [Structured Preference Optimization for Vision-Language Long-Horizon Task Planning](https://arxiv.org/abs/2502.20742)
Append: [Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?](https://arxiv.org/abs/2504.13837)
Append: [Thousand Voices of Trauma: A Large-Scale Synthetic Dataset for Modeling Prolonged Exposure Therapy Conversations](https://arxiv.org/abs/2504.13955)
Append: [AI Idea Bench 2025: AI Research Idea Generation Benchmark](https://arxiv.org/abs/2504.14191)
append_entries: 140
Finish: 2025-05-19 04:32:19.711995
------------------------------------------------------
Started: 2025-05-19 06:25:43.413949
Existing_entries: 1140
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-19 06:25:43.746012
------------------------------------------------------
Started: 2025-05-19 08:24:40.082193
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-19 08:24:40.418120
------------------------------------------------------
Started: 2025-05-19 10:18:42.642578
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-19 10:18:42.972156
------------------------------------------------------
Started: 2025-05-19 12:35:18.764670
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-19 12:35:19.088869
------------------------------------------------------
Started: 2025-05-19 14:16:53.787825
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-19 14:16:54.128196
------------------------------------------------------
Started: 2025-05-19 16:20:44.459170
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-19 16:20:44.833076
------------------------------------------------------
Started: 2025-05-19 18:23:30.942168
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-19 18:23:31.278604
------------------------------------------------------
Started: 2025-05-19 20:18:25.401477
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-19 20:18:25.784530
------------------------------------------------------
Started: 2025-05-19 22:16:03.787886
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-19 22:16:04.129114
------------------------------------------------------
Started: 2025-05-20 01:20:08.980858
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-20 01:20:09.369961
------------------------------------------------------
Started: 2025-05-20 03:10:13.816305
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-20 03:10:14.185486
------------------------------------------------------
Started: 2025-05-20 04:24:54.654973
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1759
Summarized using GPT-3.5-turbo
Append: [A Data Synthesis Method Driven by Large Language Models for Proactive Mining of Implicit User Intentions in Tourism](https://arxiv.org/abs/2505.11533)
Token length: 1164
Summarized using GPT-3.5-turbo
Append: [AI-generated Text Detection: A Multifaceted Approach to Binary and Multiclass Classification](https://arxiv.org/abs/2505.11550)
Token length: 1722
Summarized using GPT-3.5-turbo
Append: [Assessing Collective Reasoning in Multi-Agent LLMs via Hidden Profile Tasks](https://arxiv.org/abs/2505.11556)
Token length: 1135
Summarized using GPT-3.5-turbo
Append: [Talk to Your Slides: Efficient Slide Editing Agent with Large Language Models](https://arxiv.org/abs/2505.11604)
Token length: 1355
Summarized using GPT-3.5-turbo
Append: [MedGUIDE: Benchmarking Clinical Decision-Making in Large Language Models](https://arxiv.org/abs/2505.11613)
Token length: 1018
Summarized using GPT-3.5-turbo
Append: [Steering Risk Preferences in Large Language Models by Aligning Behavioral and Neural Representations](https://arxiv.org/abs/2505.11615)
Token length: 733
Summarized using GPT-3.5-turbo
Append: [THELMA: Task Based Holistic Evaluation of Large Language Model Applications-RAG Question Answering](https://arxiv.org/abs/2505.11626)
Token length: 1169
Summarized using GPT-3.5-turbo
Append: [Critique-Guided Distillation: Improving Supervised Fine-tuning via Better Distillation](https://arxiv.org/abs/2505.11628)
Token length: 1123
Summarized using GPT-3.5-turbo
Append: [Can an Easy-to-Hard Curriculum Make Reasoning Emerge in Small Language Models? Evidence from a Four-Stage Curriculum on GPT-2](https://arxiv.org/abs/2505.11643)
Token length: 1783
Summarized using GPT-3.5-turbo
Append: [Multilingual Prompt Engineering in Large Language Models: A Survey Across NLP Tasks](https://arxiv.org/abs/2505.11665)
Token length: 1495
Summarized using GPT-3.5-turbo
Append: [Ambiguity Resolution in Text-to-Structured Data Mapping](https://arxiv.org/abs/2505.11679)
Token length: 1001
Summarized using GPT-3.5-turbo
Append: [Evaluating Design Decisions for Dual Encoder-based Entity Disambiguation](https://arxiv.org/abs/2505.11683)
Token length: 1602
Summarized using GPT-3.5-turbo
Append: [Automatic Speech Recognition for African Low-Resource Languages: Challenges and Future Directions](https://arxiv.org/abs/2505.11690)
Token length: 706
Summarized using GPT-3.5-turbo
Append: [Hierarchical Bracketing Encodings for Dependency Parsing as Tagging](https://arxiv.org/abs/2505.11693)
Token length: 1329
Summarized using GPT-3.5-turbo
Append: [Disambiguating Reference in Visually Grounded Dialogues through Joint Modeling of Textual and Multimodal Semantic Structures](https://arxiv.org/abs/2505.11726)
Token length: 1529
Summarized using GPT-3.5-turbo
Append: [MedCaseReasoning: Evaluating and learning diagnostic reasoning from clinical case reports](https://arxiv.org/abs/2505.11733)
Token length: 1969
Summarized using GPT-3.5-turbo
Append: [ZeroTuning: Unlocking the Initial Token's Power to Enhance Large Language Models Without Training](https://arxiv.org/abs/2505.11739)
Token length: 983
Summarized using GPT-3.5-turbo
Append: [Token Masking Improves Transformer-Based Text Classification](https://arxiv.org/abs/2505.11746)
Token length: 1648
Summarized using GPT-3.5-turbo
Append: [Masking in Multi-hop QA: An Analysis of How Language Models Perform with Context Permutation](https://arxiv.org/abs/2505.11754)
Token length: 1120
Summarized using GPT-3.5-turbo
Append: [Towards Universal Semantics With Large Language Models](https://arxiv.org/abs/2505.11764)
Token length: 1067
Summarized using GPT-3.5-turbo
Append: [Retrospex: Language Agent Meets Offline Reinforcement Learning Critic](https://arxiv.org/abs/2505.11807)
Token length: 1568
Summarized using GPT-3.5-turbo
Append: [Efficiently Building a Domain-Specific Large Language Model from Scratch: A Case Study of a Classical Chinese Large Language Model](https://arxiv.org/abs/2505.11810)
Token length: 1657
Summarized using GPT-3.5-turbo
Append: [BELLE: A Bi-Level Multi-Agent Reasoning Framework for Multi-Hop Question Answering](https://arxiv.org/abs/2505.11811)
Token length: 1793
Summarized using GPT-3.5-turbo
Append: [Chain-of-Model Learning for Language Model](https://arxiv.org/abs/2505.11820)
Token length: 1679
Summarized using GPT-3.5-turbo
Append: [Not All Thoughts are Generated Equal: Efficient LLM Reasoning via Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2505.11827)
Token length: 1275
Summarized using GPT-3.5-turbo
Append: [Class Distillation with Mahalanobis Contrast: An Efficient Training Paradigm for Pragmatic Language Understanding Tasks](https://arxiv.org/abs/2505.11829)
Token length: 1717
Summarized using GPT-3.5-turbo
Append: [Multilingual Collaborative Defense for Large Language Models](https://arxiv.org/abs/2505.11835)
Token length: 1325
Summarized using GPT-3.5-turbo
Append: [When AI Co-Scientists Fail: SPOT-a Benchmark for Automated Verification of Scientific Research](https://arxiv.org/abs/2505.11855)
Token length: 794
Summarized using GPT-3.5-turbo
Append: [NAMET: Robust Massive Model Editing via Noise-Aware Memory Optimization](https://arxiv.org/abs/2505.11876)
Token length: 1451
Summarized using GPT-3.5-turbo
Append: [AutoMedEval: Harnessing Language Models for Automatic Medical Capability Evaluation](https://arxiv.org/abs/2505.11887)
Token length: 1584
Summarized using GPT-3.5-turbo
Append: [Mobile-Bench-v2: A More Realistic and Comprehensive Benchmark for VLM-based Mobile Agents](https://arxiv.org/abs/2505.11891)
Token length: 1488
Summarized using GPT-3.5-turbo
Append: [RLAP: A Reinforcement Learning Enhanced Adaptive Planning Framework for Multi-step NLP Task Solving](https://arxiv.org/abs/2505.11893)
Token length: 1033
Summarized using GPT-3.5-turbo
Append: [Recursive Question Understanding for Complex Question Answering over Heterogeneous Personal Data](https://arxiv.org/abs/2505.11900)
Token length: 1294
Summarized using GPT-3.5-turbo
Append: [ELITE: Embedding-Less retrieval with Iterative Text Exploration](https://arxiv.org/abs/2505.11908)
Token length: 1450
Summarized using GPT-3.5-turbo
Append: [Enhancing Complex Instruction Following for Large Language Models with Mixture-of-Contexts Fine-tuning](https://arxiv.org/abs/2505.11922)
Token length: 1262
Summarized using GPT-3.5-turbo
Append: [An Explanation of Intrinsic Self-Correction via Linear Representations and Latent Concepts](https://arxiv.org/abs/2505.11924)
Token length: 993
Summarized using GPT-3.5-turbo
Append: [Neuro-Symbolic Query Compiler](https://arxiv.org/abs/2505.11932)
Token length: 1669
Summarized using GPT-3.5-turbo
Append: [ChartEdit: How Far Are MLLMs From Automating Chart Analysis? Evaluating MLLMs' Capability via Chart Editing](https://arxiv.org/abs/2505.11935)
Token length: 1507
Summarized using GPT-3.5-turbo
Append: [Counterspeech the ultimate shield! Multi-Conditioned Counterspeech Generation through Attributed Prefix Learning](https://arxiv.org/abs/2505.11958)
Token length: 938
Summarized using GPT-3.5-turbo
Append: [EmoHopeSpeech: An Annotated Dataset of Emotions and Hope Speech in English](https://arxiv.org/abs/2505.11959)
Token length: 930
Summarized using GPT-3.5-turbo
Append: [CCNU at SemEval-2025 Task 3: Leveraging Internal and External Knowledge of Large Language Models for Multilingual Hallucination Annotation](https://arxiv.org/abs/2505.11965)
Token length: 962
Summarized using GPT-3.5-turbo
Append: [An Annotated Corpus of Arabic Tweets for Hate Speech Analysis](https://arxiv.org/abs/2505.11969)
Token length: 1852
Summarized using GPT-3.5-turbo
Append: [Unveiling Knowledge Utilization Mechanisms in LLM-based Retrieval-Augmented Generation](https://arxiv.org/abs/2505.11995)
Token length: 1202
Summarized using GPT-3.5-turbo
Append: [Towards Comprehensive Argument Analysis in Education: Dataset, Tasks, and Method](https://arxiv.org/abs/2505.12028)
Token length: 1434
Summarized using GPT-3.5-turbo
Append: [MoL for LLMs: Dual-Loss Optimization to Enhance Domain Expertise While Preserving General Capabilities](https://arxiv.org/abs/2505.12043)
Token length: 1289
Summarized using GPT-3.5-turbo
Append: [ABoN: Adaptive Best-of-N Alignment](https://arxiv.org/abs/2505.12050)
Token length: 702
Summarized using GPT-3.5-turbo
Append: [GenderBench: Evaluation Suite for Gender Biases in LLMs](https://arxiv.org/abs/2505.12054)
Token length: 1513
Summarized using GPT-3.5-turbo
Append: [Why Not Act on What You Know? Unleashing Safety Potential of LLMs via Self-Aware Guard Enhancement](https://arxiv.org/abs/2505.12060)
Token length: 1875
Summarized using GPT-3.5-turbo
Append: [Historical and psycholinguistic perspectives on morphological productivity: A sketch of an integrative approach](https://arxiv.org/abs/2505.12071)
Token length: 1340
Summarized using GPT-3.5-turbo
Append: [Do different prompting methods yield a common task representation in language models?](https://arxiv.org/abs/2505.12075)
Append: [Model Merging in Pre-training of Large Language Models](https://arxiv.org/abs/2505.12082)
Append: [Personalized Author Obfuscation with Large Language Models](https://arxiv.org/abs/2505.12090)
Append: [Improving Fairness in LLMs Through Testing-Time Adversaries](https://arxiv.org/abs/2505.12100)
Append: [A Multi-Task Benchmark for Abusive Language Detection in Low-Resource Settings](https://arxiv.org/abs/2505.12116)
Append: [The AI Gap: How Socioeconomic Status Affects Language Technology Interactions](https://arxiv.org/abs/2505.12158)
Append: [Emotion Recognition for Low-Resource Turkish: Fine-Tuning BERTurk on TREMO and Testing on Xenophobic Political Discourse](https://arxiv.org/abs/2505.12160)
Append: [Truth Neurons](https://arxiv.org/abs/2505.12182)
Append: [Decoding the Mind of Large Language Models: A Quantitative Evaluation of Ideology and Biases](https://arxiv.org/abs/2505.12183)
Append: [Vectors from Larger Language Models Predict Human Reading Time and fMRI Data More Poorly when Dimensionality Expansion is Controlled](https://arxiv.org/abs/2505.12196)
Append: [How Reliable is Multilingual LLM-as-a-Judge?](https://arxiv.org/abs/2505.12201)
Append: [Data Whisperer: Efficient Data Selection for Task-Specific LLM Fine-Tuning via Few-Shot In-Context Learning](https://arxiv.org/abs/2505.12212)
Append: [GMSA: Enhancing Context Compression via Group Merging and Layer Semantic Alignment](https://arxiv.org/abs/2505.12215)
Append: [One-for-All Pruning: A Universal Model for Customized Compression of Large Language Models](https://arxiv.org/abs/2505.12216)
Append: [Examining Linguistic Shifts in Academic Writing Before and After the Launch of ChatGPT: A Study on Preprint Papers](https://arxiv.org/abs/2505.12218)
Append: [Bridging Generative and Discriminative Learning: Few-Shot Relation Extraction via Two-Stage Knowledge-Guided Pre-training](https://arxiv.org/abs/2505.12236)
Append: [PANORAMA: A synthetic PII-laced dataset for studying sensitive data memorization in LLMs](https://arxiv.org/abs/2505.12238)
Append: [Distribution Prompting: Understanding the Expressivity of Language Models Through the Next-Token Distributions They Can Produce](https://arxiv.org/abs/2505.12244)
Append: [Not All Documents Are What You Need for Extracting Instruction Tuning Data](https://arxiv.org/abs/2505.12250)
Append: [Teach2Eval: An Indirect Evaluation Method for LLM by Judging How It Teaches](https://arxiv.org/abs/2505.12259)
Append: [Learning Auxiliary Tasks Improves Reference-Free Hallucination Detection in Open-Domain Long-Form Generation](https://arxiv.org/abs/2505.12265)
Append: [$K$-MSHC: Unmasking Minimally Sufficient Head Circuits in Large Language Models with Experiments on Syntactic Classification Tasks](https://arxiv.org/abs/2505.12268)
Append: [LLM-Based Evaluation of Low-Resource Machine Translation: A Reference-less Dialect Guided Approach with a Refined Sylheti-English Benchmark](https://arxiv.org/abs/2505.12273)
Append: [The Tower of Babel Revisited: Multilingual Jailbreak Prompts on Closed-Source Large Language Models](https://arxiv.org/abs/2505.12287)
Append: [Enhance Mobile Agents Thinking Process Via Iterative Preference Learning](https://arxiv.org/abs/2505.12299)
Append: [HBO: Hierarchical Balancing Optimization for Fine-Tuning Large Language Models](https://arxiv.org/abs/2505.12300)
Append: [Bidirectional LMs are Better Knowledge Memorizers? A Benchmark for Real-world Knowledge Injection](https://arxiv.org/abs/2505.12306)
Append: [ExpertSteer: Intervening in LLMs through Expert Knowledge](https://arxiv.org/abs/2505.12313)
Append: [LLMSR@XLLM25: An Empirical Study of LLM for Structural Reasoning](https://arxiv.org/abs/2505.12328)
Append: [UniEdit: A Unified Knowledge Editing Benchmark for Large Language Models](https://arxiv.org/abs/2505.12345)
Append: [Wisdom from Diversity: Bias Mitigation Through Hybrid Human-LLM Crowds](https://arxiv.org/abs/2505.12349)
Append: [CAPTURE: Context-Aware Prompt Injection Testing and Robustness Enhancement](https://arxiv.org/abs/2505.12368)
Append: [From n-gram to Attention: How Model Architectures Learn and Propagate Bias in Language Modeling](https://arxiv.org/abs/2505.12381)
Append: [SLOT: Sample-specific Language Model Optimization at Test-time](https://arxiv.org/abs/2505.12392)
Append: [Traversal Verification for Speculative Tree Decoding](https://arxiv.org/abs/2505.12398)
Append: [The power of text similarity in identifying AI-LLM paraphrased documents: The case of BBC news articles and ChatGPT](https://arxiv.org/abs/2505.12405)
Append: [Table-R1: Region-based Reinforcement Learning for Table Understanding](https://arxiv.org/abs/2505.12415)
Append: [PSC: Extending Context Window of Large Language Models via Phase Shift Calibration](https://arxiv.org/abs/2505.12423)
Append: [Learning to Play Like Humans: A Framework for LLM Adaptation in Interactive Fiction Games](https://arxiv.org/abs/2505.12439)
Append: [Introspective Growth: Automatically Advancing LLM Expertise in Technology Judgment](https://arxiv.org/abs/2505.12452)
Append: [Towards DS-NER: Unveiling and Addressing Latent Noise in Distant Annotations](https://arxiv.org/abs/2505.12454)
Append: [What are they talking about? Benchmarking Large Language Models for Knowledge-Grounded Discussion Summarization](https://arxiv.org/abs/2505.12474)
Append: [Enhancing Large Language Models with Reward-guided Tree Search for Knowledge Graph Question and Answering](https://arxiv.org/abs/2505.12476)
Append: [KG-QAGen: A Knowledge-Graph-Based Framework for Systematic Question Generation and Long-Context LLM Evaluation](https://arxiv.org/abs/2505.12495)
Append: [LM$^2$otifs : An Explainable Framework for Machine-Generated Texts Detection](https://arxiv.org/abs/2505.12507)
Append: [DS-ProGen: A Dual-Structure Deep Language Model for Functional Protein Design](https://arxiv.org/abs/2505.12511)
Append: [ESC-Judge: A Framework for Comparing Emotional Support Conversational Agents](https://arxiv.org/abs/2505.12531)
Append: [Relation Extraction or Pattern Matching? Unravelling the Generalisation Limits of Language Models for Biographical RE](https://arxiv.org/abs/2505.12533)
Append: [Disambiguation in Conversational Question Answering in the Era of LLM: A Survey](https://arxiv.org/abs/2505.12543)
Append: [Towards Reliable and Interpretable Traffic Crash Pattern Prediction and Safety Interventions Using Customized Large Language Models](https://arxiv.org/abs/2505.12545)
Append: [Extracting memorized pieces of (copyrighted) books from open-weight language models](https://arxiv.org/abs/2505.12546)
Append: [The taggedPBC: Annotating a massive parallel corpus for crosslinguistic investigations](https://arxiv.org/abs/2505.12560)
Append: [Enriching Patent Claim Generation with European Patent Dataset](https://arxiv.org/abs/2505.12568)
Append: [Measuring Information Distortion in Hierarchical Ultra long Novel Generation:The Optimal Expansion Ratio](https://arxiv.org/abs/2505.12572)
Append: [Improving Multilingual Language Models by Aligning Representations through Steering](https://arxiv.org/abs/2505.12584)
Append: [CMLFormer: A Dual Decoder Transformer with Switching Point Learning for Code-Mixed Language Modeling](https://arxiv.org/abs/2505.12587)
Append: [PromptPrism: A Linguistically-Inspired Taxonomy for Prompts](https://arxiv.org/abs/2505.12592)
Append: [AD-AGENT: A Multi-agent Framework for End-to-end Anomaly Detection](https://arxiv.org/abs/2505.12594)
Append: [Duluth at SemEval-2025 Task 7: TF-IDF with Optimized Vector Dimensions for Multilingual Fact-Checked Claim Retrieval](https://arxiv.org/abs/2505.12616)
Append: [Think Before You Attribute: Improving the Performance of LLMs Attribution Systems](https://arxiv.org/abs/2505.12621)
Append: [R1dacted: Investigating Local Censorship in DeepSeek's R1 Language Model](https://arxiv.org/abs/2505.12625)
Append: [Revealing the Deceptiveness of Knowledge Editing: A Mechanistic Analysis of Superficial Editing](https://arxiv.org/abs/2505.12636)
Append: [Predicting Turn-Taking and Backchannel in Human-Machine Conversations Using Linguistic, Acoustic, and Visual Signals](https://arxiv.org/abs/2505.12654)
Append: [Know3-RAG: A Knowledge-aware RAG Framework with Adaptive Retrieval, Generation, and Filtering](https://arxiv.org/abs/2505.12662)
Append: [Shadow-FT: Tuning Instruct via Base](https://arxiv.org/abs/2505.12716)
Append: [ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving](https://arxiv.org/abs/2505.12717)
Append: [Automated Bias Assessment in AI-Generated Educational Content Using CEAT Framework](https://arxiv.org/abs/2505.12718)
Append: [On-Policy Optimization with Group Equivalent Preference for Multi-Programming Language Understanding](https://arxiv.org/abs/2505.12723)
Append: [What is Stigma Attributed to? A Theory-Grounded, Expert-Annotated Interview Corpus for Demystifying Mental-Health Stigma](https://arxiv.org/abs/2505.12727)
Append: [ReEx-SQL: Reasoning with Execution-Aware Reinforcement Learning for Text-to-SQL](https://arxiv.org/abs/2505.12768)
Append: [A Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation through Low-Rank Clone](https://arxiv.org/abs/2505.12781)
Append: [EAVIT: Efficient and Accurate Human Value Identification from Text data via LLMs](https://arxiv.org/abs/2505.12792)
Append: [Decentralized Arena: Towards Democratic and Scalable Automatic Evaluation of Language Models](https://arxiv.org/abs/2505.12808)
Append: [PsyMem: Fine-grained psychological alignment and Explicit Memory Control for Advanced Role-Playing LLMs](https://arxiv.org/abs/2505.12814)
Append: [SynDec: A Synthesize-then-Decode Approach for Arbitrary Textual Style Transfer via Large Language Models](https://arxiv.org/abs/2505.12821)
Append: [Contrastive Prompting Enhances Sentence Embeddings in LLMs through Inference-Time Steering](https://arxiv.org/abs/2505.12831)
Append: [FlightGPT: Towards Generalizable and Interpretable UAV Vision-and-Language Navigation with Vision-Language Models](https://arxiv.org/abs/2505.12835)
Append: [The Hidden Structure -- Improving Legal Document Understanding Through Explicit Text Formatting](https://arxiv.org/abs/2505.12837)
Append: [Re-identification of De-identified Documents with Autoregressive Infilling](https://arxiv.org/abs/2505.12859)
Append: [LEXam: Benchmarking Legal Reasoning on 340 Law Exams](https://arxiv.org/abs/2505.12864)
Append: [GAP: Graph-Assisted Prompts for Dialogue-based Medication Recommendation](https://arxiv.org/abs/2505.12888)
Append: [On the Thinking-Language Modeling Gap in Large Language Models](https://arxiv.org/abs/2505.12896)
Append: [PyFCG: Fluid Construction Grammar in Python](https://arxiv.org/abs/2505.12920)
Append: [Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs](https://arxiv.org/abs/2505.12929)
Append: [A3 : an Analytical Low-Rank Approximation Framework for Attention](https://arxiv.org/abs/2505.12942)
Append: [Neural Morphological Tagging for Nguni Languages](https://arxiv.org/abs/2505.12949)
Append: [GuRE:Generative Query REwriter for Legal Passage Retrieval](https://arxiv.org/abs/2505.12950)
Append: [MA-COIR: Leveraging Semantic Search Index and Generative Models for Ontology-Driven Biomedical Concept Recognition](https://arxiv.org/abs/2505.12964)
Append: [Calm-Whisper: Reduce Whisper Hallucination On Non-Speech By Calming Crazy Heads Down](https://arxiv.org/abs/2505.12969)
Append: [A Structured Literature Review on Traditional Approaches in Current Natural Language Processing](https://arxiv.org/abs/2505.12970)
Append: [Fast, Not Fancy: Rethinking G2P with Rich Data and Rule-Based Models](https://arxiv.org/abs/2505.12973)
Append: [An Empirical Study of Many-to-Many Summarization with Large Language Models](https://arxiv.org/abs/2505.12983)
Append: [ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced Reinforcement Learning](https://arxiv.org/abs/2505.12996)
Append: [EffiBench-X: A Multi-Language Benchmark for Measuring Efficiency of LLM-Generated Code](https://arxiv.org/abs/2505.13004)
Append: [Evaluating the Performance of RAG Methods for Conversational AI in the Airport Domain](https://arxiv.org/abs/2505.13006)
Append: [To Bias or Not to Bias: Detecting bias in News with bias-detector](https://arxiv.org/abs/2505.13010)
Append: [topicwizard -- a Modern, Model-agnostic Framework for Topic Model Visualization and Interpretation](https://arxiv.org/abs/2505.13034)
Append: [KIT's Offline Speech Translation and Instruction Following Submission for IWSLT 2025](https://arxiv.org/abs/2505.13036)
Append: [SNAPE-PM: Building and Utilizing Dynamic Partner Models for Adaptive Explanation Generation](https://arxiv.org/abs/2505.13053)
Append: [Suicide Risk Assessment Using Multimodal Speech Features: A Study on the SW1 Challenge Dataset](https://arxiv.org/abs/2505.13069)
Append: [Advancing Sequential Numerical Prediction in Autoregressive Models](https://arxiv.org/abs/2505.13077)
Append: [Systematic Generalization in Language Models Scales with Information Entropy](https://arxiv.org/abs/2505.13089)
Append: [The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation](https://arxiv.org/abs/2505.13090)
Append: [Benchmarking and Confidence Evaluation of LALMs For Temporal Reasoning](https://arxiv.org/abs/2505.13115)
Append: [ModernGBERT: German-only 1B Encoder Model Trained from Scratch](https://arxiv.org/abs/2505.13136)
Append: [Understanding Cross-Lingual Inconsistency in Large Language Models](https://arxiv.org/abs/2505.13141)
Append: [What if Deception Cannot be Detected? A Cross-Linguistic Study on the Limits of Deception Detection from Text](https://arxiv.org/abs/2505.13147)
Append: [Tianyi: A Traditional Chinese Medicine all-rounder language model and its Real-World Clinical Practice](https://arxiv.org/abs/2505.13156)
Append: [Role-Playing Evaluation for Large Language Models](https://arxiv.org/abs/2505.13157)
Append: [Positional Fragility in LLMs: How Offset Effects Reshape Our Understanding of Memorization Risks](https://arxiv.org/abs/2505.13171)
Append: [A Case Study of Cross-Lingual Zero-Shot Generalization for Classical Languages in LLMs](https://arxiv.org/abs/2505.13173)
Append: [ToolSpectrum : Towards Personalized Tool Utilization for Large Language Models](https://arxiv.org/abs/2505.13176)
Append: [Efficient Speech Language Modeling via Energy Distance in Continuous Latent Space](https://arxiv.org/abs/2505.13181)
Append: [Alignment-Augmented Speculative Decoding with Alignment Sampling and Conditional Verification](https://arxiv.org/abs/2505.13204)
Append: [Picturized and Recited with Dialects: A Multimodal Chinese Representation Framework for Sentiment Analysis of Classical Chinese Poetry](https://arxiv.org/abs/2505.13210)
Append: [SeedBench: A Multi-task Benchmark for Evaluating Large Language Models in Seed Science](https://arxiv.org/abs/2505.13220)
Append: [JNLP at SemEval-2025 Task 11: Cross-Lingual Multi-Label Emotion Detection Using Generative Models](https://arxiv.org/abs/2505.13244)
Append: [Stronger Together: Unleashing the Social Impact of Hate Speech Research](https://arxiv.org/abs/2505.13251)
Append: [Natural Language Planning via Coding and Inference Scaling](https://arxiv.org/abs/2505.13252)
Append: [HeteroSpec: Leveraging Contextual Heterogeneity for Efficient Speculative Decoding](https://arxiv.org/abs/2505.13254)
Append: [WikiPersonas: What Can We Learn From Personalized Alignment to Famous People?](https://arxiv.org/abs/2505.13257)
Append: [Effective and Transparent RAG: Adaptive-Reward Reinforcement Learning for Decision Traceability](https://arxiv.org/abs/2505.13258)
Append: [From Automation to Autonomy: A Survey on Large Language Models in Scientific Discovery](https://arxiv.org/abs/2505.13259)
Append: [Representation of perceived prosodic similarity of conversational feedback](https://arxiv.org/abs/2505.13268)
Append: [CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement Learning](https://arxiv.org/abs/2505.13271)
Append: [$\textit{Rank, Chunk and Expand}$: Lineage-Oriented Reasoning for Taxonomy Expansion](https://arxiv.org/abs/2505.13282)
Append: [I'll believe it when I see it: Images increase misinformation sharing in Vision-Language Models](https://arxiv.org/abs/2505.13302)
Append: [RBF++: Quantifying and Optimizing Reasoning Boundaries across Measurable and Unmeasurable Capabilities for Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.13307)
Append: [GUARD: Generation-time LLM Unlearning via Adaptive Restriction and Detection](https://arxiv.org/abs/2505.13312)
Append: [Rethinking Stateful Tool Use in Multi-Turn Dialogues: Benchmarks and Challenges](https://arxiv.org/abs/2505.13328)
Append: [Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data Condensation and Spoken QA Generation](https://arxiv.org/abs/2505.13338)
Append: [J4R: Learning to Judge with Equivalent Initial State Group Relative Preference Optimization](https://arxiv.org/abs/2505.13346)
Append: [Investigating the Vulnerability of LLM-as-a-Judge Architectures to Prompt-Injection Attacks](https://arxiv.org/abs/2505.13348)
Append: [Sense and Sensitivity: Examining the Influence of Semantic Recall on Long Context Code Reasoning](https://arxiv.org/abs/2505.13353)
Append: [What Prompts Don't Say: Understanding and Managing Underspecification in LLM Prompts](https://arxiv.org/abs/2505.13360)
Append: [Thinkless: LLM Learns When to Think](https://arxiv.org/abs/2505.13379)
Append: [R3: Robust Rubric-Agnostic Reward Models](https://arxiv.org/abs/2505.13388)
Append: [MR. Judge: Multimodal Reasoner as a Judge](https://arxiv.org/abs/2505.13403)
Append: [Granary: Speech Recognition and Translation Dataset in 25 European Languages](https://arxiv.org/abs/2505.13404)
Append: [AdaptThink: Reasoning Models Can Learn When to Think](https://arxiv.org/abs/2505.13417)
Append: [Dementia Through Different Eyes: Explainable Modeling of Human and LLM Perceptions for Early Awareness](https://arxiv.org/abs/2505.13418)
Append: [SMOTExT: SMOTE meets Large Language Models](https://arxiv.org/abs/2505.13434)
Append: [ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models](https://arxiv.org/abs/2505.13444)
Append: [CIE: Controlling Language Model Text Generations Using Continuous Signals](https://arxiv.org/abs/2505.13448)
Append: [TARGET: Benchmarking Table Retrieval for Generative Tasks](https://arxiv.org/abs/2505.11545)
Append: [ASR-FAIRBENCH: Measuring and Benchmarking Equity Across Speech Recognition Systems](https://arxiv.org/abs/2505.11572)
Append: [Spectral Policy Optimization: Coloring your Incorrect Reasoning in GRPO](https://arxiv.org/abs/2505.11595)
Append: [Probing the Vulnerability of Large Language Models to Polysemantic Interventions](https://arxiv.org/abs/2505.11611)
Append: [Using Reinforcement Learning to Train Large Language Models to Explain Human Decisions](https://arxiv.org/abs/2505.11614)
Append: [EnvInjection: Environmental Prompt Injection Attack to Multi-modal Web Agents](https://arxiv.org/abs/2505.11717)
Append: [Efficient Uncertainty Estimation via Distillation of Bayesian Large Language Models](https://arxiv.org/abs/2505.11731)
Append: [Token-Level Uncertainty Estimation for Large Language Model Reasoning](https://arxiv.org/abs/2505.11737)
Append: [Feature Hedging: Correlated Features Break Narrow Sparse Autoencoders](https://arxiv.org/abs/2505.11756)
Append: [Internal Causal Mechanisms Robustly Predict Language Model Out-of-Distribution Behaviors](https://arxiv.org/abs/2505.11770)
Append: [VenusX: Unlocking Fine-Grained Functional Understanding of Proteins](https://arxiv.org/abs/2505.11812)
Append: [Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs](https://arxiv.org/abs/2505.11842)
Append: [Fair-PP: A Synthetic Dataset for Aligning LLM with Personalized Preferences of Social Equity](https://arxiv.org/abs/2505.11861)
Append: [J1: Exploring Simple Test-Time Scaling for LLM-as-a-Judge](https://arxiv.org/abs/2505.11875)
Append: [Introduction to Analytical Software Engineering Design Paradigm](https://arxiv.org/abs/2505.11979)
Append: [AI-Driven Automation Can Become the Foundation of Next-Era Science of Science Research](https://arxiv.org/abs/2505.12039)
Append: [Tiny QA Benchmark++: Ultra-Lightweight, Synthetic Multilingual Dataset Generation & Smoke-Tests for Continuous LLM Evaluation](https://arxiv.org/abs/2505.12058)
Append: [Demystifying and Enhancing the Efficiency of Large Language Model Based Search Agents](https://arxiv.org/abs/2505.12065)
Append: [LLM-BABYBENCH: Understanding and Evaluating Grounded Planning and Reasoning in LLMs](https://arxiv.org/abs/2505.12135)
Append: [EVALOOP: Assessing LLM Robustness in Programming from a Self-consistency Perspective](https://arxiv.org/abs/2505.12185)
Append: [Mitigating Content Effects on Reasoning in Language Models through Fine-Grained Activation Steering](https://arxiv.org/abs/2505.12189)
Append: [Reward Inside the Model: A Lightweight Hidden-State Reward Model for LLM's Best-of-N sampling](https://arxiv.org/abs/2505.12225)
Append: [LightRetriever: A LLM-based Hybrid Retrieval Architecture with 1000x Faster Query Inference](https://arxiv.org/abs/2505.12260)
Append: [Vague Knowledge: Evidence from Analyst Reports](https://arxiv.org/abs/2505.12269)
Append: [Efficient RL Training for Reasoning Models via Length-Aware Optimization](https://arxiv.org/abs/2505.12284)
Append: [Beyond Single-Point Judgment: Distribution Alignment for LLM-as-a-Judge](https://arxiv.org/abs/2505.12301)
Append: [LogicOCR: Do Your Large Multimodal Models Excel at Logical Reasoning on Text-Rich Images?](https://arxiv.org/abs/2505.12307)
Append: [Visuospatial Cognitive Assistant](https://arxiv.org/abs/2505.12312)
Append: [Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts](https://arxiv.org/abs/2505.12363)
Append: [MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional Methods for Diverse Medical Tasks](https://arxiv.org/abs/2505.12371)
Append: [IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2505.12442)
Append: [UFO-RL: Uncertainty-Focused Optimization for Efficient Reinforcement Learning Data Selection](https://arxiv.org/abs/2505.12457)
Append: [mCLM: A Function-Infused and Synthesis-Friendly Modular Chemical Language Model](https://arxiv.org/abs/2505.12565)
Append: [Enhancing Latent Computation in Transformers with Latent Tokens](https://arxiv.org/abs/2505.12629)
Append: [Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents](https://arxiv.org/abs/2505.12632)
Append: [Ineq-Comp: Benchmarking Human-Intuitive Compositional Reasoning in Automated Theorem Proving on Inequalities](https://arxiv.org/abs/2505.12680)
Append: [Bullying the Machine: How Personas Increase LLM Vulnerability](https://arxiv.org/abs/2505.12692)
Append: [Rethinking Reward Model Evaluation Through the Lens of Reward Overoptimization](https://arxiv.org/abs/2505.12763)
Append: [GEM: Gaussian Embedding Modeling for Out-of-Distribution Detection in GUI Agents](https://arxiv.org/abs/2505.12842)
Append: [Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?](https://arxiv.org/abs/2505.12871)
Append: [Detection and Mitigation of Hallucination in Large Reasoning Models: A Mechanistic Perspective](https://arxiv.org/abs/2505.12886)
Append: [TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios](https://arxiv.org/abs/2505.12891)
Append: [AutoGEEval: A Multimodal and Automated Framework for Geospatial Code Generation on GEE with Large Language Models](https://arxiv.org/abs/2505.12900)
Append: [Leveraging LLM Inconsistency to Boost Pass@k Performance](https://arxiv.org/abs/2505.12938)
Append: [Fractured Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.12992)
Append: [Evaluatiing the efficacy of LLM Safety Solutions : The Palit Benchmark Dataset](https://arxiv.org/abs/2505.13028)
Append: [MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix](https://arxiv.org/abs/2505.13032)
Append: [LLM-KG-Bench 3.0: A Compass for SemanticTechnology Capabilities in the Ocean of LLMs](https://arxiv.org/abs/2505.13098)
Append: [FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference](https://arxiv.org/abs/2505.13109)
Append: [Zero-Shot Iterative Formalization and Planning in Partially Observable Environments](https://arxiv.org/abs/2505.13126)
Append: [Efficient Generation of Parameterised Quantum Circuits from Large Texts](https://arxiv.org/abs/2505.13208)
Append: [Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis](https://arxiv.org/abs/2505.13227)
Append: [SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based on Speech and Audio Information](https://arxiv.org/abs/2505.13237)
Append: [Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space](https://arxiv.org/abs/2505.13308)
Append: [CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via Competition](https://arxiv.org/abs/2505.13380)
Append: [IG Parser: A Software Package for the Encoding of Institutional Statements using the Institutional Grammar](https://arxiv.org/abs/2505.13393)
Append: [A Minimum Description Length Approach to Regularization in Neural Networks](https://arxiv.org/abs/2505.13398)
Append: [CoT-Kinetics: A Theoretical Modeling Assessing LRM Reasoning Process](https://arxiv.org/abs/2505.13408)
Append: [Fine-tuning Quantized Neural Networks with Zeroth-order Optimization](https://arxiv.org/abs/2505.13430)
Append: [Optimizing Anytime Reasoning via Budget Relative Policy Optimization](https://arxiv.org/abs/2505.13438)
Append: [Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2505.13445)
Append: [Large Linguistic Models: Investigating LLMs' metalinguistic abilities](https://arxiv.org/abs/2305.00948)
Append: [Physics of Language Models: Part 1, Learning Hierarchical Language Structures](https://arxiv.org/abs/2305.13673)
Append: [Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models](https://arxiv.org/abs/2310.10378)
Append: [Automatically generating Riddles aiding Concept Attainment](https://arxiv.org/abs/2310.18290)
Append: [Streaming Sequence Transduction through Dynamic Compression](https://arxiv.org/abs/2402.01172)
Append: [Can We Verify Step by Step for Incorrect Answer Detection?](https://arxiv.org/abs/2402.10528)
Append: [FormulaReasoning: A Dataset for Formula-Based Numerical Reasoning](https://arxiv.org/abs/2402.12692)
Append: [Comparing Specialised Small and General Large Language Models on Text Classification: 100 Labelled Samples to Achieve Break-Even Performance](https://arxiv.org/abs/2402.12819)
Append: [From Languages to Geographies: Towards Evaluating Cultural Bias in Hate Speech Datasets](https://arxiv.org/abs/2404.17874)
Append: [Sparse Matrix in Large Language Model Fine-tuning](https://arxiv.org/abs/2405.15525)
Append: [OR-Bench: An Over-Refusal Benchmark for Large Language Models](https://arxiv.org/abs/2405.20947)
Append: [ShareLoRA: Parameter Efficient and Robust Large Language Model Fine-tuning via Shared Low-Rank Adaptation](https://arxiv.org/abs/2406.10785)
Append: [Pruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging](https://arxiv.org/abs/2406.16330)
Append: [Brittle Minds, Fixable Activations: Understanding Belief Representations in Language Models](https://arxiv.org/abs/2406.17513)
Append: [DiffuseDef: Improved Robustness to Adversarial Attacks via Iterative Denoising](https://arxiv.org/abs/2407.00248)
Append: [A Bounding Box is Worth One Token: Interleaving Layout and Text in a Large Language Model for Document Understanding](https://arxiv.org/abs/2407.01976)
Append: [ClinicRealm: Re-evaluating Large Language Models with Conventional Machine Learning for Non-Generative Clinical Prediction Tasks](https://arxiv.org/abs/2407.18525)
Append: [SWIFT:A Scalable lightWeight Infrastructure for Fine-Tuning](https://arxiv.org/abs/2408.05517)
Append: [Turning Trash into Treasure: Accelerating Inference of Large Language Models with Token Recycling](https://arxiv.org/abs/2408.08696)
Append: [Large Language Models Might Not Care What You Are Saying: Prompt Format Beats Descriptions](https://arxiv.org/abs/2408.08780)
Append: [LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction](https://arxiv.org/abs/2408.12249)
Append: [What are the Essential Factors in Crafting Effective Long Context Multi-Hop Instruction Datasets? Insights and Best Practices](https://arxiv.org/abs/2409.01893)
Append: [Learning Efficient Recursive Numeral Systems via Reinforcement Learning](https://arxiv.org/abs/2409.07170)
Append: [PACE: Abstractions for Communicating Efficiently](https://arxiv.org/abs/2409.20120)
Append: [SSR: Alignment-Aware Modality Connector for Speech Language Models](https://arxiv.org/abs/2410.00168)
Append: [LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations](https://arxiv.org/abs/2410.02707)
Append: [Rodimus*: Breaking the Accuracy-Efficiency Trade-Off with Efficient Attentions](https://arxiv.org/abs/2410.06577)
Append: [MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses](https://arxiv.org/abs/2410.07076)
Append: [Enhancing LLM Evaluations: The Garbling Trick](https://arxiv.org/abs/2411.01533)
Append: [VersaTune: An Efficient Data Composition Framework for Training Multi-Capability LLMs](https://arxiv.org/abs/2411.11266)
Append: [Enhancing LLMs for Power System Simulations: A Feedback-driven Multi-agent Framework](https://arxiv.org/abs/2411.16707)
Append: [Can ChatGPT capture swearing nuances? Evidence from translating Arabic oaths](https://arxiv.org/abs/2412.02466)
Append: [Intention Knowledge Graph Construction for User Intention Relation Modeling](https://arxiv.org/abs/2412.11500)
Append: [DateLogicQA: Benchmarking Temporal Biases in Large Language Models](https://arxiv.org/abs/2412.13377)
Append: [Theoretical Proof that Auto-regressive Language Models Collapse when Real-world Data is a Finite Set](https://arxiv.org/abs/2412.14872)
Append: [Multi-Agent Sampling: Scaling Inference Compute for Data Synthesis with Tree Search-Based Agentic Collaboration](https://arxiv.org/abs/2412.17061)
Append: [ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models in Multi-Hop Tool Use](https://arxiv.org/abs/2501.02506)
Append: [Can MLLMs Generalize to Multi-Party dialog? Exploring Multilingual Response Generation in Complex Scenarios](https://arxiv.org/abs/2501.11269)
Append: [Advancing Multi-Party Dialogue Framework with Speaker-ware Contrastive Learning](https://arxiv.org/abs/2501.11292)
Append: [Generative AI and Large Language Models in Language Preservation: Opportunities and Challenges](https://arxiv.org/abs/2501.11496)
Append: [AdaServe: Accelerating Multi-SLO LLM Serving with SLO-Customized Speculative Decoding](https://arxiv.org/abs/2501.12162)
Append: [Option-ID Based Elimination For Multiple Choice Questions](https://arxiv.org/abs/2501.15175)
Append: [How Linguistics Learned to Stop Worrying and Love the Language Models](https://arxiv.org/abs/2501.17047)
Append: [Jailbreaking LLMs' Safeguard with Universal Magic Words for Text Embedding Models](https://arxiv.org/abs/2501.18280)
Append: [Vision-centric Token Compression in Large Language Model](https://arxiv.org/abs/2502.00791)
Append: [Joint Localization and Activation Editing for Low-Resource Fine-Tuning](https://arxiv.org/abs/2502.01179)
Append: [Scaling Embedding Layers in Language Models](https://arxiv.org/abs/2502.01637)
Append: [Generative Psycho-Lexical Approach for Constructing Value Systems in Large Language Models](https://arxiv.org/abs/2502.02444)
Append: [Reformulation for Pretraining Data Augmentation](https://arxiv.org/abs/2502.04235)
Append: [ATLAS: Autoformalizing Theorems through Lifting, Augmentation, and Synthesis of Data](https://arxiv.org/abs/2502.05567)
Append: [Evolving LLMs' Self-Refinement Capability via Iterative Preference Optimization](https://arxiv.org/abs/2502.05605)
Append: [Is LLM an Overconfident Judge? Unveiling the Capabilities of LLMs in Detecting Offensive Language with Annotation Disagreement](https://arxiv.org/abs/2502.06207)
Append: [Who Taught You That? Tracing Teachers in Model Distillation](https://arxiv.org/abs/2502.06659)
Append: [Can Vision-Language Models Infer Speaker's Ignorance? The Role of Visual and Linguistic Cues](https://arxiv.org/abs/2502.09120)
Append: [RAS: Retrieval-And-Structuring for Knowledge-Intensive LLM Generation](https://arxiv.org/abs/2502.10996)
Append: [Beyond Pairwise: Global Zero-shot Temporal Graph Generation](https://arxiv.org/abs/2502.11114)
Append: [Eye Tracking Based Cognitive Evaluation of Automatic Readability Assessment Measures](https://arxiv.org/abs/2502.11150)
Append: [The Mirage of Model Editing: Revisiting Evaluation in the Wild](https://arxiv.org/abs/2502.11177)
Append: [From the New World of Word Embeddings: A Comparative Study of Small-World Lexico-Semantic Networks in LLMs](https://arxiv.org/abs/2502.11380)
Append: [Beyond Single-Task: Robust Multi-Task Length Generalization for LLMs](https://arxiv.org/abs/2502.11525)
Append: [FaMTEB: Massive Text Embedding Benchmark in Persian Language](https://arxiv.org/abs/2502.11571)
Append: [To Think or Not to Think: Exploring the Unthinking Vulnerability in Large Reasoning Models](https://arxiv.org/abs/2502.12202)
Append: [SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models](https://arxiv.org/abs/2502.12464)
Append: [Is This Collection Worth My LLM's Time? Automatically Measuring Information Potential in Text Corpora](https://arxiv.org/abs/2502.13691)
Append: [Vulnerability of Text-to-Image Models to Prompt Template Stealing: A Differential Evolution Approach](https://arxiv.org/abs/2502.14285)
Append: [Machine-generated text detection prevents language model collapse](https://arxiv.org/abs/2502.15654)
Append: [FANformer: Improving Large Language Models Through Effective Periodicity Modeling](https://arxiv.org/abs/2502.21309)
Append: [MoSE: Hierarchical Self-Distillation Enhances Early Layer Embeddings](https://arxiv.org/abs/2503.03008)
Append: [Can Frontier LLMs Replace Annotators in Biomedical Text Mining? Analyzing Challenges and Exploring Solutions](https://arxiv.org/abs/2503.03261)
Append: [SCoRE: Benchmarking Long-Chain Reasoning in Commonsense Scenarios](https://arxiv.org/abs/2503.06218)
Append: [PolyPythias: Stability and Outliers across Fifty Language Model Pre-Training Runs](https://arxiv.org/abs/2503.09543)
Append: [Probabilistic Reasoning with LLMs for k-anonymity Estimation](https://arxiv.org/abs/2503.09674)
Append: [UC-MOA: Utility-Conditioned Multi-Objective Alignment for Distributional Pareto-Optimality](https://arxiv.org/abs/2503.10669)
Append: [HICD: Hallucination-Inducing via Attention Dispersion for Contrastive Decoding to Mitigate Hallucinations in Large Language Models](https://arxiv.org/abs/2503.12908)
Append: [Unifying Text Semantics and Graph Structures for Temporal Text-attributed Graphs with Large Language Models](https://arxiv.org/abs/2503.14411)
Append: [Challenging the Boundaries of Reasoning: An Olympiad-Level Math Benchmark for Large Language Models](https://arxiv.org/abs/2503.21380)
Append: [ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large Reasoning Models with Iterative Retrieval Augmented Generation](https://arxiv.org/abs/2503.21729)
Append: [ImF: Implicit Fingerprint for Large Language Models](https://arxiv.org/abs/2503.21805)
Append: [Why Stop at One Error? Benchmarking LLMs as Data Science Code Debuggers for Multi-Hop and Multi-Bug Errors](https://arxiv.org/abs/2503.22388)
Append: [RARE: Retrieval-Augmented Reasoning Modeling](https://arxiv.org/abs/2503.23513)
Append: [FISH-Tuning: Enhancing PEFT Methods with Fisher Information](https://arxiv.org/abs/2504.04050)
Append: [Leveraging Robust Optimization for LLM Alignment under Distribution Shifts](https://arxiv.org/abs/2504.05831)
Append: [LSR-MCTS: Alleviating Long Range Dependency in Code Generation](https://arxiv.org/abs/2504.07433)
Append: [Large Language Models Could Be Rote Learners](https://arxiv.org/abs/2504.08300)
Append: [DioR: Adaptive Cognitive Detection and Contextual Retrieval Optimization for Dynamic Retrieval-Augmented Generation](https://arxiv.org/abs/2504.10198)
Append: [Semantic Similarity-Informed Bayesian Borrowing for Quantitative Signal Detection of Adverse Events](https://arxiv.org/abs/2504.12052)
Append: [CoT-RAG: Integrating Chain of Thought and Retrieval-Augmented Generation to Enhance Reasoning in Large Language Models](https://arxiv.org/abs/2504.13534)
Append: [Multimodal Coreference Resolution for Chinese Social Media Dialogues: Dataset and Benchmark Approach](https://arxiv.org/abs/2504.14321)
Append: [Trans-Zero: Self-Play Incentivizes Large Language Models for Multilingual Translation Without Parallel Data](https://arxiv.org/abs/2504.14669)
Append: [Dynamic Early Exit in Reasoning Models](https://arxiv.org/abs/2504.15895)
Append: [PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models](https://arxiv.org/abs/2504.16074)
Append: [OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents](https://arxiv.org/abs/2504.16918)
Append: [Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning](https://arxiv.org/abs/2504.17192)
Append: [SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning](https://arxiv.org/abs/2504.19162)
Append: [VCM: Vision Concept Modeling Based on Implicit Contrastive Learning with Vision-Language Instruction Fine-Tuning](https://arxiv.org/abs/2504.19627)
Append: [Toward Evaluative Thinking: Meta Policy Optimization with Evolving Reward Models](https://arxiv.org/abs/2504.20157)
Append: [UniversalRAG: Retrieval-Augmented Generation over Corpora of Diverse Modalities and Granularities](https://arxiv.org/abs/2504.20734)
Append: [Computational Reasoning of Large Language Models](https://arxiv.org/abs/2504.20771)
Append: [FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension](https://arxiv.org/abs/2505.00570)
Append: [PlanFitting: Personalized Exercise Planning with Large Language Model-driven Conversational Agent](https://arxiv.org/abs/2309.12555)
Append: [BAT: Learning to Reason about Spatial Sounds with Large Language Models](https://arxiv.org/abs/2402.01591)
Append: [Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era](https://arxiv.org/abs/2403.08946)
Append: [Controlled Training Data Generation with Diffusion Models](https://arxiv.org/abs/2403.15309)
Append: [Efficient Indirect LLM Jailbreak via Multimodal-LLM Jailbreak](https://arxiv.org/abs/2405.20015)
Append: [$S^3$ -- Semantic Signal Separation](https://arxiv.org/abs/2406.09556)
Append: [Watermarking Language Models with Error Correcting Codes](https://arxiv.org/abs/2406.10281)
Append: [Task Facet Learning: A Structured Approach to Prompt Optimization](https://arxiv.org/abs/2406.10504)
Append: [Gradient descent with generalized Newton's method](https://arxiv.org/abs/2407.02772)
Append: [EfficientQAT: Efficient Quantization-Aware Training for Large Language Models](https://arxiv.org/abs/2407.11062)
Append: ["Yes, My LoRD." Guiding Language Model Extraction with Locality Reinforced Distillation](https://arxiv.org/abs/2409.02718)
Append: [Immunogenicity Prediction with Dual Attention Enables Vaccine Target Selection](https://arxiv.org/abs/2410.02647)
Append: [Inference and Verbalization Functions During In-Context Learning](https://arxiv.org/abs/2410.09349)
Append: [Bias Similarity Across Large Language Models](https://arxiv.org/abs/2410.12010)
Append: [BrainECHO: Semantic Brain Signal Decoding through Vector-Quantized Spectrogram Reconstruction for Whisper-Enhanced Text Generation](https://arxiv.org/abs/2410.14971)
Append: [LLMScan: Causal Scan for LLM Misbehavior Detection](https://arxiv.org/abs/2410.16638)
Append: [Decoding Time Series with LLMs: A Multi-Agent Framework for Cross-Domain Annotation](https://arxiv.org/abs/2410.17462)
Append: [VLSBench: Unveiling Visual Leakage in Multimodal Safety](https://arxiv.org/abs/2411.19939)
Append: [Training-Free Bayesianization for Low-Rank Adapters of Large Language Models](https://arxiv.org/abs/2412.05723)
Append: [MMedPO: Aligning Medical Vision-Language Models with Clinical-Aware Multimodal Preference Optimization](https://arxiv.org/abs/2412.06141)
Append: [Superhuman performance of a large language model on the reasoning tasks of a physician](https://arxiv.org/abs/2412.10849)
Append: [Feedback-Driven Vision-Language Alignment with Minimal Human Supervision](https://arxiv.org/abs/2501.04568)
Append: [Disentangling Length Bias In Preference Learning Via Response-Conditioned Modeling](https://arxiv.org/abs/2502.00814)
Append: [Explaining Context Length Scaling and Bounds for Language Models](https://arxiv.org/abs/2502.01481)
Append: [Leveraging the true depth of LLMs](https://arxiv.org/abs/2502.02790)
Append: [Exploring the Potential of Encoder-free Architectures in 3D LMMs](https://arxiv.org/abs/2502.09620)
Append: [Table-Critic: A Multi-Agent Framework for Collaborative Criticism and Refinement in Table Reasoning](https://arxiv.org/abs/2502.11799)
Append: [ARS: Automatic Routing Solver with Large Language Models](https://arxiv.org/abs/2502.15359)
Append: [The Hidden Strength of Disagreement: Unraveling the Consensus-Diversity Tradeoff in Adaptive Multi-Agent Systems](https://arxiv.org/abs/2502.16565)
Append: [Vision-Encoders (Already) Know What They See: Mitigating Object Hallucination via Simple Fine-Grained CLIPScore](https://arxiv.org/abs/2502.20034)
Append: [A Pilot Empirical Study on When and How to Use Knowledge Graphs as Retrieval Augmented Generation](https://arxiv.org/abs/2502.20854)
Append: [MAPS: Motivation-Aware Personalized Search via LLM-Driven Consultation Alignment](https://arxiv.org/abs/2503.01711)
Append: [Logic-in-Frames: Dynamic Keyframe Search via Visual Semantic-Logical Verification for Long Video Understanding](https://arxiv.org/abs/2503.13139)
Append: [DeLoRA: Decoupling Angles and Strength in Low-rank Adaptation](https://arxiv.org/abs/2503.18225)
Append: [MaintainCoder: Maintainable Code Generation Under Dynamic Requirements](https://arxiv.org/abs/2503.24260)
Append: [Effectively Controlling Reasoning Models through Thinking Intervention](https://arxiv.org/abs/2503.24370)
Append: [Prot42: a Novel Family of Protein Language Models for Target-aware Protein Binder Generation](https://arxiv.org/abs/2504.04453)
Append: [Signatures of human-like processing in Transformer forward passes](https://arxiv.org/abs/2504.14107)
Append: [AlignRAG: Leveraging Critique Learning for Evidence-Sensitive Retrieval-Augmented Reasoning](https://arxiv.org/abs/2504.14858)
Append: [A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment](https://arxiv.org/abs/2504.15585)
Append: [Process Reward Models That Think](https://arxiv.org/abs/2504.16828)
Append: [Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks](https://arxiv.org/abs/2505.00234)
append_entries: 395
Finish: 2025-05-20 04:27:00.270035
------------------------------------------------------
Started: 2025-05-20 06:26:04.397935
Existing_entries: 1395
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1300
Summarized using GPT-3.5-turbo
Append: [Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and Model Size in Large Language Models From Edge to Giant](https://arxiv.org/abs/2409.11055)
Token length: 1552
Summarized using GPT-3.5-turbo
Append: [Synthesize-on-Graph: Knowledgeable Synthetic Data Generation for Continue Pre-training of Large Language Models](https://arxiv.org/abs/2505.00979)
Token length: 1645
Summarized using GPT-3.5-turbo
Append: [RM-R1: Reward Modeling as Reasoning](https://arxiv.org/abs/2505.02387)
Token length: 1186
Summarized using GPT-3.5-turbo
Append: [RICo: Refined In-Context Contribution for Automatic Instruction-Tuning Data Selection](https://arxiv.org/abs/2505.05327)
Token length: 1942
Summarized using GPT-3.5-turbo
Append: [Benchmarking LLMs' Swarm intelligence](https://arxiv.org/abs/2505.04364)
append_entries: 5
Finish: 2025-05-20 06:26:19.666697
------------------------------------------------------
Started: 2025-05-20 08:23:02.950582
Existing_entries: 1005
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-20 08:23:03.790412
------------------------------------------------------
Started: 2025-05-20 10:18:44.050375
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-20 10:18:44.903192
------------------------------------------------------
Started: 2025-05-20 12:35:16.406592
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-20 12:35:17.219973
------------------------------------------------------
Started: 2025-05-20 14:16:46.580116
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-20 14:16:47.490675
------------------------------------------------------
Started: 2025-05-20 16:20:48.673500
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-20 16:20:49.482040
------------------------------------------------------
Started: 2025-05-20 18:23:29.680254
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-20 18:23:30.534037
------------------------------------------------------
Started: 2025-05-20 20:18:39.656533
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-20 20:18:40.506488
------------------------------------------------------
Started: 2025-05-20 22:15:27.733243
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-20 22:15:28.534784
------------------------------------------------------
Started: 2025-05-21 01:19:46.369245
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-21 01:19:47.210846
------------------------------------------------------
Started: 2025-05-21 03:09:49.567441
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-21 03:09:50.512521
------------------------------------------------------
Started: 2025-05-21 04:24:16.225321
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1262
Summarized using GPT-3.5-turbo
Append: [Evaluating Reasoning LLMs for Suicide Screening with the Columbia-Suicide Severity Rating Scale](https://arxiv.org/abs/2505.13480)
Token length: 1103
Summarized using GPT-3.5-turbo
Append: [EmoMeta: A Multimodal Dataset for Fine-grained Emotion Classification in Chinese Metaphors](https://arxiv.org/abs/2505.13483)
Token length: 1258
Summarized using GPT-3.5-turbo
Append: [Detecting Prefix Bias in LLM-based Reward Models](https://arxiv.org/abs/2505.13487)
Token length: 1322
Summarized using GPT-3.5-turbo
Append: [Source framing triggers systematic evaluation bias in Large Language Models](https://arxiv.org/abs/2505.13488)
Token length: 1393
Summarized using GPT-3.5-turbo
Append: [ProdRev: A DNN framework for empowering customers using generative pre-trained transformers](https://arxiv.org/abs/2505.13491)
Token length: 1724
Summarized using GPT-3.5-turbo
Append: [LLM4CD: Leveraging Large Language Models for Open-World Knowledge Augmented Cognitive Diagnosis](https://arxiv.org/abs/2505.13492)
Token length: 1482
Summarized using GPT-3.5-turbo
Append: [IRLBench: A Multi-modal, Culturally Grounded, Parallel Irish-English Benchmark for Open-Ended LLM Reasoning Evaluation](https://arxiv.org/abs/2505.13498)
Token length: 1068
Summarized using GPT-3.5-turbo
Append: [Noise Injection Systemically Degrades Large Language Model Safety Guardrails](https://arxiv.org/abs/2505.13500)
Token length: 1026
Summarized using GPT-3.5-turbo
Append: [EcoSafeRAG: Efficient Security through Context Analysis in Retrieval-Augmented Generation](https://arxiv.org/abs/2505.13506)
Token length: 1937
Summarized using GPT-3.5-turbo
Append: [Time-R1: Towards Comprehensive Temporal Reasoning in LLMs](https://arxiv.org/abs/2505.13508)
Token length: 1162
Summarized using GPT-3.5-turbo
Append: [Induction Head Toxicity Mechanistically Explains Repetition Curse in Large Language Models](https://arxiv.org/abs/2505.13514)
Token length: 956
Summarized using GPT-3.5-turbo
Append: [Logic Jailbreak: Efficiently Unlocking LLM Safety Restrictions Through Formal Logical Expression](https://arxiv.org/abs/2505.13527)
Token length: 1072
Summarized using GPT-3.5-turbo
Append: [Combining the Best of Both Worlds: A Method for Hybrid NMT and LLM Translation](https://arxiv.org/abs/2505.13554)
Token length: 1118
Summarized using GPT-3.5-turbo
Append: [CS-Sum: A Benchmark for Code-Switching Dialogue Summarization and the Limits of Large Language Models](https://arxiv.org/abs/2505.13559)
Token length: 730
Summarized using GPT-3.5-turbo
Append: [Cross-Lingual Representation Alignment Through Contrastive Image-Caption Tuning](https://arxiv.org/abs/2505.13628)
Token length: 900
Summarized using GPT-3.5-turbo
Append: [Clarifying orthography: Orthographic transparency as compressibility](https://arxiv.org/abs/2505.13657)
Token length: 1024
Summarized using GPT-3.5-turbo
Append: [Are Large Language Models Good at Detecting Propaganda?](https://arxiv.org/abs/2505.13706)
Token length: 1162
Summarized using GPT-3.5-turbo
Append: [SQLForge: Synthesizing Reliable and Diverse Data to Enhance Text-to-SQL Reasoning in LLMs](https://arxiv.org/abs/2505.13725)
Token length: 963
Summarized using GPT-3.5-turbo
Append: [Simulation Agent: A Framework for Integrating Simulation and Large Language Models for Enhanced Decision-Making](https://arxiv.org/abs/2505.13761)
Token length: 1023
Summarized using GPT-3.5-turbo
Append: [Krikri: Advancing Open Large Language Models for Greek](https://arxiv.org/abs/2505.13772)
Token length: 1916
Summarized using GPT-3.5-turbo
Append: [Interpretable Traces, Unexpected Outcomes: Investigating the Disconnect in Trace-Based Knowledge Distillation](https://arxiv.org/abs/2505.13792)
Token length: 1933
Summarized using GPT-3.5-turbo
Append: [EfficientLLM: Efficiency in Large Language Models](https://arxiv.org/abs/2505.13840)
Token length: 987
Summarized using GPT-3.5-turbo
Append: [Improve Language Model and Brain Alignment via Associative Memory](https://arxiv.org/abs/2505.13844)
Token length: 848
Summarized using GPT-3.5-turbo
Append: [Domain Gating Ensemble Networks for AI-Generated Text Detection](https://arxiv.org/abs/2505.13855)
Token length: 1219
Summarized using GPT-3.5-turbo
Append: [Reasoning Path Compression: Compressing Generation Trajectories for Efficient LLM Reasoning](https://arxiv.org/abs/2505.13866)
Token length: 1266
Summarized using GPT-3.5-turbo
Append: [Code2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General Reasoning](https://arxiv.org/abs/2505.13886)
Token length: 1423
Summarized using GPT-3.5-turbo
Append: [Mapping the Minds of LLMs: A Graph-Based Analysis of Reasoning LLM](https://arxiv.org/abs/2505.13890)
Token length: 1652
Summarized using GPT-3.5-turbo
Append: [InfiGFusion: Graph-on-Logits Distillation via Efficient Gromov-Wasserstein for Model Fusion](https://arxiv.org/abs/2505.13893)
Token length: 1843
Summarized using GPT-3.5-turbo
Append: [Let's Verify Math Questions Step by Step](https://arxiv.org/abs/2505.13903)
Token length: 977
Summarized using GPT-3.5-turbo
Append: [Cross-Linguistic Transfer in Multilingual NLP: The Role of Language Families and Morphology](https://arxiv.org/abs/2505.13908)
Token length: 1525
Summarized using GPT-3.5-turbo
Append: [Word length predicts word order: "Min-max"-ing drives language evolution](https://arxiv.org/abs/2505.13913)
Token length: 1529
Summarized using GPT-3.5-turbo
Append: [EEG-to-Text Translation: A Model for Deciphering Human Brain Activity](https://arxiv.org/abs/2505.13936)
Token length: 1814
Summarized using GPT-3.5-turbo
Append: [Towards Rehearsal-Free Continual Relation Extraction: Capturing Within-Task Variance with Adaptive Prompting](https://arxiv.org/abs/2505.13944)
Token length: 1631
Summarized using GPT-3.5-turbo
Append: [Memory-Centric Embodied Question Answer](https://arxiv.org/abs/2505.13948)
Token length: 1202
Summarized using GPT-3.5-turbo
Append: [FlashThink: An Early Exit Method For Efficient Reasoning](https://arxiv.org/abs/2505.13949)
Token length: 1580
Summarized using GPT-3.5-turbo
Append: [Through a Compressed Lens: Investigating the Impact of Quantization on LLM Explainability and Interpretability](https://arxiv.org/abs/2505.13963)
Token length: 1183
Summarized using GPT-3.5-turbo
Append: [CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring](https://arxiv.org/abs/2505.13965)
Token length: 1264
Summarized using GPT-3.5-turbo
Append: [Truth or Twist? Optimal Model Selection for Reliable Label Flipping Evaluation in LLM-based Counterfactuals](https://arxiv.org/abs/2505.13972)
Token length: 1068
Summarized using GPT-3.5-turbo
Append: [Toward Effective Reinforcement Learning Fine-Tuning for Medical VQA in Vision-Language Models](https://arxiv.org/abs/2505.13973)
Token length: 1255
Summarized using GPT-3.5-turbo
Append: [DRP: Distilled Reasoning Pruning with Skill-aware Step Decomposition for Efficient Large Reasoning Models](https://arxiv.org/abs/2505.13975)
Token length: 811
Summarized using GPT-3.5-turbo
Append: [Mixed Signals: Understanding Model Disagreement in Multimodal Empathy Detection](https://arxiv.org/abs/2505.13979)
Token length: 1323
Summarized using GPT-3.5-turbo
Append: [The Hallucination Tax of Reinforcement Finetuning](https://arxiv.org/abs/2505.13988)
Token length: 1347
Summarized using GPT-3.5-turbo
Append: [DecIF: Improving Instruction-Following through Meta-Decomposition](https://arxiv.org/abs/2505.13990)
Token length: 1559
Summarized using GPT-3.5-turbo
Append: [Social Sycophancy: A Broader Understanding of LLM Sycophancy](https://arxiv.org/abs/2505.13995)
Token length: 1453
Summarized using GPT-3.5-turbo
Append: [Activation-Guided Consensus Merging for Large Language Models](https://arxiv.org/abs/2505.14009)
Token length: 1485
Summarized using GPT-3.5-turbo
Append: [AUTOLAW: Enhancing Legal Compliance in Large Language Models via Case Law Generation and Jury-Inspired Deliberation](https://arxiv.org/abs/2505.14015)
Token length: 1158
Summarized using GPT-3.5-turbo
Append: [From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way Parallel Corpora](https://arxiv.org/abs/2505.14045)
Token length: 1212
Summarized using GPT-3.5-turbo
Append: [Improved Methods for Model Pruning and Knowledge Distillation](https://arxiv.org/abs/2505.14052)
Token length: 1217
Summarized using GPT-3.5-turbo
Append: [Enhancing LLMs via High-Knowledge Data Selection](https://arxiv.org/abs/2505.14070)
Token length: 1304
Summarized using GPT-3.5-turbo
Append: [BAR: A Backward Reasoning based Agent for Complex Minecraft Tasks](https://arxiv.org/abs/2505.14079)
Append: [Gender Trouble in Language Models: An Empirical Audit Guided by Gender Performativity Theory](https://arxiv.org/abs/2505.14080)
Append: [Beyond Chains: Bridging Large Language Models and Knowledge Bases in Complex Question Answering](https://arxiv.org/abs/2505.14099)
Append: [MultiHal: Multilingual Dataset for Knowledge-Graph Grounded Evaluation of LLM Hallucinations](https://arxiv.org/abs/2505.14101)
Append: [Legal Rule Induction: Towards Generalizable Principle Discovery from Analogous Judicial Precedents](https://arxiv.org/abs/2505.14104)
Append: [A Personalized Conversational Benchmark: Towards Simulating Personalized Conversations](https://arxiv.org/abs/2505.14106)
Append: [DiagnosisArena: Benchmarking Diagnostic Reasoning for Large Language Models](https://arxiv.org/abs/2505.14107)
Append: [Invisible Entropy: Towards Safe and Efficient Low-Entropy LLM Watermarking](https://arxiv.org/abs/2505.14112)
Append: [Self-Reasoning Language Models: Unfold Hidden Reasoning Chains with Few Reasoning Catalyst](https://arxiv.org/abs/2505.14116)
Append: [Probing BERT for German Compound Semantics](https://arxiv.org/abs/2505.14130)
Append: [Texts or Images? A Fine-grained Analysis on the Effectiveness of Input Representations and Models for Table Question Answering](https://arxiv.org/abs/2505.14131)
Append: [Enhancing Keyphrase Extraction from Academic Articles Using Section Structure Information](https://arxiv.org/abs/2505.14149)
Append: [Prior Prompt Engineering for Reinforcement Fine-Tuning](https://arxiv.org/abs/2505.14157)
Append: [Temporal Alignment of Time Sensitive Facts with Activation Engineering](https://arxiv.org/abs/2505.14158)
Append: [Breaking Language Barriers or Reinforcing Bias? A Study of Gender and Racial Disparities in Multilingual Contrastive Vision Language Models](https://arxiv.org/abs/2505.14160)
Append: [PL-FGSA: A Prompt Learning Framework for Fine-Grained Sentiment Analysis Based on MindSpore](https://arxiv.org/abs/2505.14165)
Append: [The Strawberry Problem: Emergence of Character-level Understanding in Tokenized Language Models](https://arxiv.org/abs/2505.14172)
Append: [THOR-MoE: Hierarchical Task-Guided and Context-Responsive Routing for Neural Machine Translation](https://arxiv.org/abs/2505.14173)
Append: [Cheaper, Better, Faster, Stronger: Robust Text-to-SQL without Chain-of-Thought or Fine-Tuning](https://arxiv.org/abs/2505.14174)
Append: [Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic Reasoning Limits](https://arxiv.org/abs/2505.14178)
Append: [Enhancing Abstractive Summarization of Scientific Papers Using Structure Information](https://arxiv.org/abs/2505.14179)
Append: [SlangDIT: Benchmarking LLMs in Interpretative Slang Translation](https://arxiv.org/abs/2505.14181)
Append: [ThinkSwitcher: When to Think Hard, When to Think Fast](https://arxiv.org/abs/2505.14183)
Append: [Unraveling Interwoven Roles of Large Language Models in Authorship Privacy: Obfuscation, Mimicking, and Verification](https://arxiv.org/abs/2505.14195)
Append: [Automatic Dataset Generation for Knowledge Intensive Question Answering Tasks](https://arxiv.org/abs/2505.14212)
Append: ["Haet Bhasha aur Diskrimineshun": Phonetic Perturbations in Code-Mixed Hinglish to Red-Team LLMs](https://arxiv.org/abs/2505.14226)
Append: [Mechanistic Fine-tuning for In-context Learning](https://arxiv.org/abs/2505.14233)
Append: [ABBA: Highly Expressive Hadamard Product Adaptation for Large Language Models](https://arxiv.org/abs/2505.14238)
Append: [Technical Report on classification of literature related to children speech disorder](https://arxiv.org/abs/2505.14242)
Append: [TransBench: Benchmarking Machine Translation for Industrial-Scale Applications](https://arxiv.org/abs/2505.14244)
Append: [FuxiMT: Sparsifying Large Language Models for Chinese-Centric Multilingual Machine Translation](https://arxiv.org/abs/2505.14256)
Append: [Think-J: Learning to Think for Generative LLM-as-a-Judge](https://arxiv.org/abs/2505.14268)
Append: [FAID: Fine-grained AI-generated Text Detection using Multi-task Auxiliary and Multi-level Contrastive Learning](https://arxiv.org/abs/2505.14271)
Append: [Data-Efficient Hate Speech Detection via Cross-Lingual Nearest Neighbor Retrieval with Limited Labeled Data](https://arxiv.org/abs/2505.14272)
Append: [YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering](https://arxiv.org/abs/2505.14279)
Append: [Universal Acoustic Adversarial Attacks for Flexible Control of Speech-LLMs](https://arxiv.org/abs/2505.14286)
Append: [Cross-Lingual Optimization for Language Transfer in Large Language Models](https://arxiv.org/abs/2505.14297)
Append: [JOLT-SQL: Joint Loss Tuning of Text-to-SQL with Confusion-aware Noisy Schema Sampling](https://arxiv.org/abs/2505.14305)
Append: [Studying the Role of Input-Neighbor Overlap in Retrieval-Augmented Language Models Training Efficiency](https://arxiv.org/abs/2505.14309)
Append: [HausaNLP: Current Status, Challenges and Future Directions for Hausa Natural Language Processing](https://arxiv.org/abs/2505.14311)
Append: [A MIND for Reasoning: Meta-learning for In-context Deduction](https://arxiv.org/abs/2505.14313)
Append: [QA-prompting: Improving Summarization with Large Language Models using Question-Answering](https://arxiv.org/abs/2505.14347)
Append: [OSoRA: Output-Dimension and Singular-Value Initialized Low-Rank Adaptation](https://arxiv.org/abs/2505.14350)
Append: [WirelessMathBench: A Mathematical Modeling Benchmark for LLMs in Wireless Communications](https://arxiv.org/abs/2505.14354)
Append: [Dual Decomposition of Weights and Singular Value Low Rank Adaptation](https://arxiv.org/abs/2505.14367)
Append: [AutoRev: Automatic Peer Review System for Academic Research Papers](https://arxiv.org/abs/2505.14376)
Append: [Editing Across Languages: A Survey of Multilingual Knowledge Editing](https://arxiv.org/abs/2505.14393)
Append: [MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation Capabilities in Any Language](https://arxiv.org/abs/2505.14395)
Append: [Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable Computation](https://arxiv.org/abs/2505.14398)
Append: [Pierce the Mists, Greet the Sky: Decipher Knowledge Overshadowing via Knowledge Circuit Analysis](https://arxiv.org/abs/2505.14406)
Append: [Hidden Ghost Hand: Unveiling Backdoor Vulnerabilities in MLLM-Powered Mobile GUI Agents](https://arxiv.org/abs/2505.14418)
Append: [SAE-FiRE: Enhancing Earnings Surprise Predictions Through Sparse Autoencoder Feature Selection](https://arxiv.org/abs/2505.14420)
Append: [Scaling Low-Resource MT via Synthetic Data Generation with LLMs](https://arxiv.org/abs/2505.14423)
Append: [From Templates to Natural Language: Generalization Challenges in Instruction-Tuned LLMs for Spatial Reasoning](https://arxiv.org/abs/2505.14425)
Append: [Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric Knowledge Transfer in Large Language Models](https://arxiv.org/abs/2505.14436)
Append: [Creative Preference Optimization](https://arxiv.org/abs/2505.14442)
Append: [CtrlDiff: Boosting Large Diffusion Language Models with Dynamic Block Prediction and Controllable Generation](https://arxiv.org/abs/2505.14455)
Append: [Not All Correct Answers Are Equal: Why Your Distillation Source Matters](https://arxiv.org/abs/2505.14464)
Append: [Void in Language Models](https://arxiv.org/abs/2505.14467)
Append: [Attributional Safety Failures in Large Language Models under Code-Mixed Perturbations](https://arxiv.org/abs/2505.14469)
Append: [Adapting Pretrained Language Models for Citation Classification via Self-Supervised Contrastive Learning](https://arxiv.org/abs/2505.14471)
Append: [PlanGPT-VL: Enhancing Urban Planning with Domain-Specific Vision-Language Models](https://arxiv.org/abs/2505.14481)
Append: [MoMoE: Mixture of Moderation Experts Framework for AI-Assisted Online Governance](https://arxiv.org/abs/2505.14483)
Append: [Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated Rationales](https://arxiv.org/abs/2505.14499)
Append: [ModRWKV: Transformer Multimodality in Linear Time](https://arxiv.org/abs/2505.14505)
Append: [Exploring Graph Representations of Logical Forms for Language Modeling](https://arxiv.org/abs/2505.14523)
Append: [Internal Chain-of-Thought: Empirical Evidence for Layer-wise Subtask Scheduling in LLMs](https://arxiv.org/abs/2505.14530)
Append: [Breaking Bad Tokens: Detoxification of LLMs Using Sparse Autoencoders](https://arxiv.org/abs/2505.14536)
Append: [KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation](https://arxiv.org/abs/2505.14552)
Append: [Pivot Language for Low-Resource Machine Translation](https://arxiv.org/abs/2505.14553)
Append: [TRATES: Trait-Specific Rubric-Assisted Cross-Prompt Essay Scoring](https://arxiv.org/abs/2505.14577)
Append: [Can Pruning Improve Reasoning? Revisiting Long-CoT Compression with Capability in Mind for Better Reasoning](https://arxiv.org/abs/2505.14582)
Append: [Context Reasoner: Incentivizing Reasoning Capability for Contextualized Privacy and Safety Compliance via Reinforcement Learning](https://arxiv.org/abs/2505.14585)
Append: [MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol](https://arxiv.org/abs/2505.14590)
Append: [Success is in the Details: Evaluate and Enhance Details Sensitivity of Code LLMs through Counterfactuals](https://arxiv.org/abs/2505.14597)
Append: [Toward Reliable Biomedical Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models](https://arxiv.org/abs/2505.14599)
Append: [sudoLLM : On Multi-role Alignment of Language Models](https://arxiv.org/abs/2505.14607)
Append: [Language Models Optimized to Fool Detectors Still Have a Distinct Style (And How to Change It)](https://arxiv.org/abs/2505.14608)
Append: [Linear Control of Test Awareness Reveals Differential Compliance in Reasoning Models](https://arxiv.org/abs/2505.14617)
Append: [Think Only When You Need with Large Hybrid-Reasoning Models](https://arxiv.org/abs/2505.14631)
Append: [Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas](https://arxiv.org/abs/2505.14633)
Append: [General-Reasoner: Advancing LLM Reasoning Across All Domains](https://arxiv.org/abs/2505.14652)
Append: [EmoGist: Efficient In-Context Learning for Visual Emotion Understanding](https://arxiv.org/abs/2505.14660)
Append: [Reward Reasoning Model](https://arxiv.org/abs/2505.14674)
Append: [UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in Large Language Models](https://arxiv.org/abs/2505.14679)
Append: [Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning](https://arxiv.org/abs/2505.14684)
Append: [Language Models use Lookbacks to Track Beliefs](https://arxiv.org/abs/2505.14685)
Append: [MedEIR: A Specialized Medical Embedding Model for Enhanced Information Retrieval](https://arxiv.org/abs/2505.13482)
Append: [Evaluating Large Language Models for Real-World Engineering Tasks](https://arxiv.org/abs/2505.13484)
Append: [Contrastive Cross-Course Knowledge Tracing via Concept Graph Guided Knowledge Transfer](https://arxiv.org/abs/2505.13489)
Append: [Can AI Freelancers Compete? Benchmarking Earnings, Reliability, and Task Success at Scale](https://arxiv.org/abs/2505.13511)
Append: [LoRASuite: Efficient LoRA Adaptation Across Large Language Model Upgrades](https://arxiv.org/abs/2505.13515)
Append: [BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs](https://arxiv.org/abs/2505.13529)
Append: [AdAEM: An Adaptively and Automated Extensible Measurement of LLMs' Value Difference](https://arxiv.org/abs/2505.13531)
Append: [InterFeat: An Automated Pipeline for Finding Interesting Hypotheses in Structured Biomedical Data](https://arxiv.org/abs/2505.13534)
Append: [RAR: Setting Knowledge Tripwires for Retrieval Augmented Rejection](https://arxiv.org/abs/2505.13581)
Append: [Guided Search Strategies in Non-Serializable Environments with Applications to Software Engineering Agents](https://arxiv.org/abs/2505.13652)
Append: [Warm Up Before You Train: Unlocking General Reasoning in Resource-Constrained Settings](https://arxiv.org/abs/2505.13718)
Append: [Power Lines: Scaling Laws for Weight Decay and Batch Size in LLM Pre-training](https://arxiv.org/abs/2505.13738)
Append: [LLM-Based Compact Reranking with Document Features for Scientific Retrieval](https://arxiv.org/abs/2505.13757)
Append: [Language Models Are Capable of Metacognitive Monitoring and Control of Their Internal Activations](https://arxiv.org/abs/2505.13763)
Append: [Advancing Software Quality: A Standards-Focused Review of LLM-Based Assurance Techniques](https://arxiv.org/abs/2505.13766)
Append: [Ice Cream Doesn't Cause Drowning: Benchmarking LLMs Against Statistical Pitfalls in Causal Inference](https://arxiv.org/abs/2505.13770)
Append: [Structured Agent Distillation for Large Language Model](https://arxiv.org/abs/2505.13820)
Append: [Forensic deepfake audio detection using segmental speech features](https://arxiv.org/abs/2505.13847)
Append: [PandaGuard: Systematic Evaluation of LLM Safety in the Era of Jailbreaking Attacks](https://arxiv.org/abs/2505.13862)
Append: [InfiFPO: Implicit Model Fusion via Preference Optimization in Large Language Models](https://arxiv.org/abs/2505.13878)
Append: [Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient Operational Knowledge Injection in Mobile Automation](https://arxiv.org/abs/2505.13887)
Append: [Efficient Agent Training for Computer Use](https://arxiv.org/abs/2505.13909)
Append: [MLZero: A Multi-Agent System for End-to-end Machine Learning Automation](https://arxiv.org/abs/2505.13941)
Append: [Beyond Text: Unveiling Privacy Vulnerabilities in Multi-modal Retrieval-Augmented Generation](https://arxiv.org/abs/2505.13957)
Append: [ProMind-LLM: Proactive Mental Health Care via Causal Reasoning with Sensor Data](https://arxiv.org/abs/2505.14038)
Append: [Textual Steering Vectors Can Improve Visual Understanding in Multimodal Large Language Models](https://arxiv.org/abs/2505.14071)
Append: [s3: You Don't Need That Much Data to Train a Search Agent via RL](https://arxiv.org/abs/2505.14146)
Append: [Safety Subspaces are Not Distinct: A Fine-Tuning Case Study](https://arxiv.org/abs/2505.14185)
Append: [Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning](https://arxiv.org/abs/2505.14216)
Append: [AAPO: Enhance the Reasoning Capabilities of LLMs with Advantage Momentum](https://arxiv.org/abs/2505.14264)
Append: [SafetyNet: Detecting Harmful Outputs in LLMs by Modeling and Monitoring Deceptive Behaviors](https://arxiv.org/abs/2505.14300)
Append: [Scaling Law for Quantization-Aware Training](https://arxiv.org/abs/2505.14302)
Append: [RADAR: Enhancing Radiology Report Generation with Supplementary Knowledge Injection](https://arxiv.org/abs/2505.14318)
Append: [FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis for \"U-Tsang, Amdo and Kham Speech Dataset Generation](https://arxiv.org/abs/2505.14351)
Append: [PersonaTAB: Predicting Personality Traits using Textual, Acoustic, and Behavioral Cues in Fully-Duplex Speech Dialogs](https://arxiv.org/abs/2505.14356)
Append: [Is Your Prompt Safe? Investigating Prompt Injection Attacks Against Open-Source LLMs](https://arxiv.org/abs/2505.14368)
Append: [Causal Cartographer: From Mapping to Reasoning Over Counterfactual Worlds](https://arxiv.org/abs/2505.14396)
Append: [OmniGenBench: A Modular Platform for Reproducible Genomic Foundation Models Benchmarking](https://arxiv.org/abs/2505.14402)
Append: [Pairwise Evaluation of Accent Similarity in Speech Synthesis](https://arxiv.org/abs/2505.14410)
Append: [PRL: Prompts from Reinforcement Learning](https://arxiv.org/abs/2505.14412)
Append: [Rank-K: Test-Time Reasoning for Listwise Reranking](https://arxiv.org/abs/2505.14432)
Append: [S2SBench: A Benchmark for Quantifying Intelligence Degradation in Speech-to-Speech Large Language Models](https://arxiv.org/abs/2505.14438)
Append: [Mitigating Subgroup Disparities in Multi-Label Speech Emotion Recognition: A Pseudo-Labeling and Unsupervised Learning Approach](https://arxiv.org/abs/2505.14449)
Append: [RAVENEA: A Benchmark for Multimodal Retrieval-Augmented Visual Culture Understanding](https://arxiv.org/abs/2505.14462)
Append: [PAST: Phonetic-Acoustic Speech Tokenizer](https://arxiv.org/abs/2505.14470)
Append: [Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach](https://arxiv.org/abs/2505.14479)
Append: [Reasoning Models Better Express Their Confidence](https://arxiv.org/abs/2505.14489)
Append: [Teaching Audio-Aware Large Language Models What Does Not Hear: Mitigating Hallucinations through Synthesized Negative Samples](https://arxiv.org/abs/2505.14518)
Append: [Agent Context Protocols Enhance Collective Inference](https://arxiv.org/abs/2505.14569)
Append: [SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle Generation from SAT Formulas](https://arxiv.org/abs/2505.14615)
Append: [Enhancing Learned Knowledge in LoRA Adapters Through Efficient Contrastive Decoding on Ascend NPUs](https://arxiv.org/abs/2505.14620)
Append: [TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning](https://arxiv.org/abs/2505.14625)
Append: [Debating for Better Reasoning: An Unsupervised Multimodal Approach](https://arxiv.org/abs/2505.14627)
Append: [KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large Language Models](https://arxiv.org/abs/2505.14629)
Append: [Dual Precision Quantization for Efficient and Accurate Deep Neural Networks Inference](https://arxiv.org/abs/2505.14638)
Append: [Beyond Words: Multimodal LLM Knows When to Speak](https://arxiv.org/abs/2505.14654)
Append: [SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment](https://arxiv.org/abs/2505.14667)
Append: [ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions](https://arxiv.org/abs/2505.14668)
Append: [NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search](https://arxiv.org/abs/2505.14680)
Append: [Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training](https://arxiv.org/abs/2505.14681)
Append: [Arithmetics-Based Decomposition of Numeral Words -- Arithmetic Conditions give the Unpacking Strategy](https://arxiv.org/abs/2312.10097)
Append: [Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning](https://arxiv.org/abs/2403.10056)
Append: [PLAYER*: Enhancing LLM-based Multi-Agent Communication and Interaction in Murder Mystery Games](https://arxiv.org/abs/2404.17662)
Append: [Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM Outputs](https://arxiv.org/abs/2407.01082)
Append: [PersonaGym: Evaluating Persona Agents and LLMs](https://arxiv.org/abs/2407.18416)
Append: [Automating Intervention Discovery from Scientific Literature: A Progressive Ontology Prompting and Dual-LLM Framework](https://arxiv.org/abs/2409.00054)
Append: [RoMath: A Mathematical Reasoning Benchmark in Romanian](https://arxiv.org/abs/2409.11074)
Append: [Revealing and Mitigating the Challenge of Detecting Character Knowledge Errors in LLM Role-Playing](https://arxiv.org/abs/2409.11726)
Append: [Learning from Committee: Reasoning Distillation from a Mixture of Teachers with Peer-Review](https://arxiv.org/abs/2410.03663)
Append: [SensorLLM: Human-Intuitive Alignment of Multivariate Sensor Data with LLMs for Activity Recognition](https://arxiv.org/abs/2410.10624)
Append: [RATE: Causal Explainability of Reward Models with Imperfect Counterfactuals](https://arxiv.org/abs/2410.11348)
Append: [Interpreting token compositionality in LLMs: A robustness analysis](https://arxiv.org/abs/2410.12924)
Append: [The Mystery of the Pathological Path-star Task for Language Models](https://arxiv.org/abs/2410.13779)
Append: [Unlearning Backdoor Attacks for LLMs with Weak-to-Strong Knowledge Distillation](https://arxiv.org/abs/2410.14425)
Append: [M-RewardBench: Evaluating Reward Models in Multilingual Settings](https://arxiv.org/abs/2410.15522)
Append: [Arithmetic Without Algorithms: Language Models Solve Math With a Bag of Heuristics](https://arxiv.org/abs/2410.21272)
Append: [Rate, Explain and Cite (REC): Enhanced Explanation and Attribution in Automatic Evaluation by Large Language Models](https://arxiv.org/abs/2411.02448)
Append: [Velocitune: A Velocity-based Dynamic Domain Reweighting Method for Continual Pre-training](https://arxiv.org/abs/2411.14318)
Append: [Can LLMs be Good Graph Judge for Knowledge Graph Construction?](https://arxiv.org/abs/2411.17388)
Append: [A Comparative Study of Learning Paradigms in Large Language Models via Intrinsic Dimension](https://arxiv.org/abs/2412.06245)
Append: [A Survey of Mathematical Reasoning in the Era of Multimodal Large Language Model: Benchmark, Method & Challenges](https://arxiv.org/abs/2412.11936)
Append: [TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks](https://arxiv.org/abs/2412.14161)
Append: [Agent-SafetyBench: Evaluating the Safety of LLM Agents](https://arxiv.org/abs/2412.14470)
Append: [SubData: Bridging Heterogeneous Datasets to Enable Theory-Driven Evaluation of Political and Demographic Perspectives in LLMs](https://arxiv.org/abs/2412.16783)
Append: [Cross-model Transferability among Large Language Models on the Platonic Representations of Concepts](https://arxiv.org/abs/2501.02009)
Append: [ACORD: An Expert-Annotated Retrieval Dataset for Legal Contract Drafting](https://arxiv.org/abs/2501.06582)
Append: [TiEBe: Tracking Language Model Recall of Notable Worldwide Events Through Time](https://arxiv.org/abs/2501.07482)
Append: [Mitigating Forgetting in LLM Fine-Tuning via Low-Perplexity Token Learning](https://arxiv.org/abs/2501.14315)
Append: [STATE ToxiCN: A Benchmark for Span-level Target-Aware Toxicity Extraction in Chinese Hate Speech Detection](https://arxiv.org/abs/2501.15451)
Append: [People who frequently use ChatGPT for writing tasks are accurate and robust detectors of AI-generated text](https://arxiv.org/abs/2501.15654)
Append: [Improving LLM Unlearning Robustness via Random Perturbations](https://arxiv.org/abs/2501.19202)
Append: [LongDPO: Unlock Better Long-form Generation Abilities for LLMs via Critique-augmented Stepwise Information](https://arxiv.org/abs/2502.02095)
Append: [Premise-Augmented Reasoning Chains Improve Error Identification in Math reasoning with LLMs](https://arxiv.org/abs/2502.02362)
Append: [A comparison of translation performance between DeepL and Supertext](https://arxiv.org/abs/2502.02577)
Append: [Speculative Prefill: Turbocharging TTFT with Lightweight and Training-Free Token Importance Estimation](https://arxiv.org/abs/2502.02789)
Append: [MMUnlearner: Reformulating Multimodal Machine Unlearning in the Era of Multimodal Large Language Models](https://arxiv.org/abs/2502.11051)
Append: [CARMA: Enhanced Compositionality in LLMs via Advanced Regularisation and Mutual Information Alignment](https://arxiv.org/abs/2502.11066)
Append: [Towards Achieving Concept Completeness for Textual Concept Bottleneck Models](https://arxiv.org/abs/2502.11100)
Append: [Plant in Cupboard, Orange on Rably, Inat Aphone. Benchmarking Incremental Learning of Situation and Language Model using a Text-Simulated Situated Environment](https://arxiv.org/abs/2502.11733)
Append: [FineFilter: A Fine-grained Noise Filtering Mechanism for Retrieval-Augmented Large Language Models](https://arxiv.org/abs/2502.11811)
Append: [EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models](https://arxiv.org/abs/2502.11916)
Append: [R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on Knowledge Graphs](https://arxiv.org/abs/2502.12767)
Append: [Robust Adaptation of Large Multimodal Models for Retrieval Augmented Hateful Meme Detection](https://arxiv.org/abs/2502.13061)
Append: [TreeCut: A Synthetic Unanswerable Math Word Problem Dataset for LLM Hallucination Evaluation](https://arxiv.org/abs/2502.13442)
Append: [DiffSampling: Enhancing Diversity and Accuracy in Neural Text Generation](https://arxiv.org/abs/2502.14037)
Append: [Enhancing Conversational Agents with Theory of Mind: Aligning Beliefs, Desires, and Intentions for Human-Like Interaction](https://arxiv.org/abs/2502.14171)
Append: [Moving Beyond Medical Exam Questions: A Clinician-Annotated Dataset of Real-World Tasks and Ambiguity in Mental Healthcare](https://arxiv.org/abs/2502.16051)
Append: [SQLong: Enhanced NL2SQL for Longer Contexts with LLMs](https://arxiv.org/abs/2502.16747)
Append: [Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and Mixture-of-Experts Optimization Alignment](https://arxiv.org/abs/2502.16894)
Append: [Char-mander Use mBackdoor! A Study of Cross-lingual Backdoor Attacks in Multilingual LLMs](https://arxiv.org/abs/2502.16901)
Append: [Erasing Without Remembering: Implicit Knowledge Forgetting in Large Language Models](https://arxiv.org/abs/2502.19982)
Append: [Multi2: Multi-Agent Test-Time Scalable Framework for Multi-Document Processing](https://arxiv.org/abs/2502.20592)
Append: [CODI: Compressing Chain-of-Thought into Continuous Space via Self-Distillation](https://arxiv.org/abs/2502.21074)
Append: [Evaluation and Facilitation of Online Discussions in the LLM Era: A Survey](https://arxiv.org/abs/2503.01513)
Append: [MCiteBench: A Multimodal Benchmark for Generating Text with Citations](https://arxiv.org/abs/2503.02589)
Append: [Assumed Identities: Quantifying Gender Bias in Machine Translation of Gender-Ambiguous Occupational Terms](https://arxiv.org/abs/2503.04372)
Append: [Cost-Optimal Grouped-Query Attention for Long-Context Modeling](https://arxiv.org/abs/2503.09579)
Append: [RouterEval: A Comprehensive Benchmark for Routing LLMs to Explore Model-level Scaling Up in LLMs](https://arxiv.org/abs/2503.10657)
Append: [MathAgent: Leveraging a Mixture-of-Math-Agent Framework for Real-World Multimodal Mathematical Error Detection](https://arxiv.org/abs/2503.18132)
Append: [Scaling Video-Language Models to 10K Frames via Hierarchical Differential Distillation](https://arxiv.org/abs/2504.02438)
Append: [Beyond Self-Reports: Multi-Observer Agents for Personality Assessment in Large Language Models](https://arxiv.org/abs/2504.08399)
Append: [S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability of Large Reasoning Models](https://arxiv.org/abs/2504.10368)
Append: [Cross-Document Cross-Lingual NLI via RST-Enhanced Graph Fusion and Interpretability Prediction](https://arxiv.org/abs/2504.12324)
Append: [Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations](https://arxiv.org/abs/2504.14150)
Append: [MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety](https://arxiv.org/abs/2504.15241)
Append: [DMDTEval: An Evaluation and Analysis of LLMs on Disambiguation in Multi-domain Translation](https://arxiv.org/abs/2504.20371)
Append: [HyPerAlign: Interpretable Personalized LLM Alignment via Hypothesis Generation](https://arxiv.org/abs/2505.00038)
Append: [A Survey on Large Language Model based Human-Agent Systems](https://arxiv.org/abs/2505.00753)
Append: [Adaptive Thinking via Mode Policy Optimization for Social Language Agents](https://arxiv.org/abs/2505.02156)
Append: [From Theft to Bomb-Making: The Ripple Effect of Unlearning in Defending Against Jailbreak Attacks](https://arxiv.org/abs/2407.02855)
Append: [Frozen Large Language Models Can Perceive Paralinguistic Aspects of Speech](https://arxiv.org/abs/2410.01162)
Append: [IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models](https://arxiv.org/abs/2410.02429)
Append: [Evaluating the Correctness of Inference Patterns Used by LLMs for Judgment](https://arxiv.org/abs/2410.09083)
Append: [Large Continual Instruction Assistant](https://arxiv.org/abs/2410.10868)
Append: [Scaling Stick-Breaking Attention: An Efficient Implementation and In-depth Study](https://arxiv.org/abs/2410.17980)
Append: [ChatNVD: Advancing Cybersecurity Vulnerability Assessment with Large Language Models](https://arxiv.org/abs/2412.04756)
Append: [ProcessBench: Identifying Process Errors in Mathematical Reasoning](https://arxiv.org/abs/2412.06559)
Append: [MMDocIR: Benchmarking Multi-Modal Retrieval for Long Documents](https://arxiv.org/abs/2501.08828)
Append: [InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model](https://arxiv.org/abs/2501.12368)
Append: [Fairshare Data Pricing via Data Valuation for Large Language Models](https://arxiv.org/abs/2502.00198)
Append: [From Words to Collisions: LLM-Guided Evaluation and Adversarial Generation of Safety-Critical Driving Scenarios](https://arxiv.org/abs/2502.02145)
Append: [Training Language Models to Reason Efficiently](https://arxiv.org/abs/2502.04463)
Append: [VLMs as GeoGuessr Masters: Exceptional Performance, Hidden Biases, and Privacy Risks](https://arxiv.org/abs/2502.11163)
Append: [EquiBench: Benchmarking Large Language Models' Understanding of Program Semantics via Equivalence Checking](https://arxiv.org/abs/2502.12466)
Append: [DeepResonance: Enhancing Multimodal Music Understanding via Music-centric Multi-way Instruction Tuning](https://arxiv.org/abs/2502.12623)
Append: [Does Acceleration Cause Hidden Instability in Vision Language Models? Uncovering Instance-Level Divergence Through a Large-Scale Empirical Study](https://arxiv.org/abs/2503.06794)
Append: [VisBias: Measuring Explicit and Implicit Social Biases in Vision Language Models](https://arxiv.org/abs/2503.07575)
Append: [Language Models, Graph Searching, and Supervision Adulteration: When More Supervision is Less and How to Make More More](https://arxiv.org/abs/2503.10542)
Append: [CRCE: Coreference-Retention Concept Erasure in Text-to-Image Diffusion Models](https://arxiv.org/abs/2503.14232)
Append: [DAPO: An Open-Source LLM Reinforcement Learning System at Scale](https://arxiv.org/abs/2503.14476)
Append: [Scalable Evaluation of Online Facilitation Strategies via Synthetic Simulation of Discussions](https://arxiv.org/abs/2503.16505)
Append: [Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding](https://arxiv.org/abs/2504.01281)
Append: [Learning to Reason under Off-Policy Guidance](https://arxiv.org/abs/2504.14945)
Append: [VideoVista-CulturalLingo: 360$^\circ$ Horizons-Bridging Cultures, Languages, and Domains in Video Comprehension](https://arxiv.org/abs/2504.17821)
Append: [CodeFlowBench: A Multi-turn, Iterative Benchmark for Complex Code Generation](https://arxiv.org/abs/2504.21751)
append_entries: 291
Finish: 2025-05-21 04:26:10.770767
------------------------------------------------------
Started: 2025-05-21 06:25:14.275548
Existing_entries: 1291
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 988
Summarized using GPT-3.5-turbo
Append: [Can Prompting LLMs Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study](https://arxiv.org/abs/2505.06149)
Token length: 1175
Summarized using GPT-3.5-turbo
Append: [Technical Report: Quantifying and Analyzing the Generalization Power of a DNN](https://arxiv.org/abs/2505.06993)
Token length: 1057
Summarized using GPT-3.5-turbo
Append: [Direct Density Ratio Optimization: A Statistically Consistent Approach to Aligning Large Language Models](https://arxiv.org/abs/2505.07558)
append_entries: 3
Finish: 2025-05-21 06:25:21.275814
------------------------------------------------------
Started: 2025-05-21 08:22:12.489475
Existing_entries: 1003
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-21 08:22:13.116071
------------------------------------------------------
Started: 2025-05-21 10:18:11.354685
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-21 10:18:11.974912
------------------------------------------------------
Started: 2025-05-21 12:34:07.703990
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-21 12:34:08.326567
------------------------------------------------------
Started: 2025-05-21 14:17:26.799262
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-21 14:17:27.506976
------------------------------------------------------
Started: 2025-05-21 16:21:12.123448
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-21 16:21:12.759748
------------------------------------------------------
Started: 2025-05-21 18:23:37.410084
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-21 18:23:38.036958
------------------------------------------------------
Started: 2025-05-21 20:18:20.404359
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-21 20:18:21.088953
------------------------------------------------------
Started: 2025-05-21 22:15:39.845177
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-21 22:15:40.493513
------------------------------------------------------
Started: 2025-05-22 01:18:51.220002
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-22 01:18:51.929571
------------------------------------------------------
Started: 2025-05-22 03:09:58.222323
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-22 03:09:58.868096
------------------------------------------------------
Started: 2025-05-22 04:25:00.150692
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 758
Summarized using GPT-3.5-turbo
Append: [Addressing the Challenges of Planning Language Generation](https://arxiv.org/abs/2505.14763)
Token length: 696
Summarized using GPT-3.5-turbo
Append: [Automated Journalistic Questions: A New Method for Extracting 5W1H in French](https://arxiv.org/abs/2505.14804)
Token length: 1216
Summarized using GPT-3.5-turbo
Append: [Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models](https://arxiv.org/abs/2505.14810)
Token length: 1237
Summarized using GPT-3.5-turbo
Append: [Language Mixing in Reasoning Language Models: Patterns, Impact, and Internal Causes](https://arxiv.org/abs/2505.14815)
Token length: 1229
Summarized using GPT-3.5-turbo
Append: [WebNovelBench: Placing LLM Novelists on the Web Novel Distribution](https://arxiv.org/abs/2505.14818)
Token length: 1429
Summarized using GPT-3.5-turbo
Append: [Tracing Multilingual Factual Knowledge Acquisition in Pretraining](https://arxiv.org/abs/2505.14824)
Token length: 1235
Summarized using GPT-3.5-turbo
Append: [Text Generation Beyond Discrete Token Sampling](https://arxiv.org/abs/2505.14827)
Token length: 1395
Summarized using GPT-3.5-turbo
Append: [SEPS: A Separability Measure for Robust Unlearning in LLMs](https://arxiv.org/abs/2505.14832)
Token length: 1410
Summarized using GPT-3.5-turbo
Append: [A Comparative Study of Large Language Models and Human Personality Traits](https://arxiv.org/abs/2505.14845)
Token length: 1259
Summarized using GPT-3.5-turbo
Append: [MAATS: A Multi-Agent Automated Translation System Based on MQM Evaluation](https://arxiv.org/abs/2505.14848)
Token length: 536
Summarized using GPT-3.5-turbo
Append: [EasyMath: A 0-shot Math Benchmark for SLMs](https://arxiv.org/abs/2505.14852)
Token length: 922
Summarized using GPT-3.5-turbo
Append: [Saten: Sparse Augmented Tensor Networks for Post-Training Compression of Large Language Models](https://arxiv.org/abs/2505.14871)
Token length: 1008
Summarized using GPT-3.5-turbo
Append: [Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric Speech Recognition in Low-Resource Languages](https://arxiv.org/abs/2505.14874)
Token length: 961
Summarized using GPT-3.5-turbo
Append: [Incorporating Token Usage into Prompting Strategy Evaluation](https://arxiv.org/abs/2505.14880)
Token length: 1335
Summarized using GPT-3.5-turbo
Append: [Strategic Planning and Rationalizing on Trees Make LLMs Better Debaters](https://arxiv.org/abs/2505.14886)
Token length: 1236
Summarized using GPT-3.5-turbo
Append: [In-Context Learning Boosts Speech Recognition via Human-like Adaptation to Speakers and Language Varieties](https://arxiv.org/abs/2505.14887)
Token length: 1295
Summarized using GPT-3.5-turbo
Append: [Scaling Laws for State Dynamics in Large Language Models](https://arxiv.org/abs/2505.14892)
Token length: 1492
Summarized using GPT-3.5-turbo
Append: [Concept Incongruence: An Exploration of Time and Death in Role Playing](https://arxiv.org/abs/2505.14905)
Token length: 1414
Summarized using GPT-3.5-turbo
Append: [Understanding 6G through Language Models: A Case Study on LLM-aided Structured Entity Extraction in Telecom Domain](https://arxiv.org/abs/2505.14906)
Token length: 1644
Summarized using GPT-3.5-turbo
Append: [ConspEmoLLM-v2: A robust and stable model to detect sentiment-transformed conspiracy theories](https://arxiv.org/abs/2505.14917)
Token length: 1330
Summarized using GPT-3.5-turbo
Append: [Reliable Decision Support with LLMs: A Framework for Evaluating Consistency in Binary Text Classification Applications](https://arxiv.org/abs/2505.14918)
Token length: 955
Summarized using GPT-3.5-turbo
Append: [Too Long, Didn't Model: Decomposing LLM Long-Context Understanding With Novels](https://arxiv.org/abs/2505.14925)
Token length: 1382
Summarized using GPT-3.5-turbo
Append: [MedBrowseComp: Benchmarking Medical Deep Research and Computer Use](https://arxiv.org/abs/2505.14963)
Token length: 1509
Summarized using GPT-3.5-turbo
Append: [DECASTE: Unveiling Caste Stereotypes in Large Language Models through Multi-Dimensional Bias Analysis](https://arxiv.org/abs/2505.14971)
Token length: 1862
Summarized using GPT-3.5-turbo
Append: [Multimodal Cultural Safety: Evaluation Frameworks and Alignment Strategies](https://arxiv.org/abs/2505.14972)
Token length: 1057
Summarized using GPT-3.5-turbo
Append: [CRAFT: Training-Free Cascaded Retrieval for Tabular QA](https://arxiv.org/abs/2505.14984)
Token length: 1780
Summarized using GPT-3.5-turbo
Append: [Language Specific Knowledge: Do Models Know Better in X than in English?](https://arxiv.org/abs/2505.14990)
Token length: 1273
Summarized using GPT-3.5-turbo
Append: [Effective and Efficient Schema-aware Information Extraction Using On-Device Large Language Models](https://arxiv.org/abs/2505.14992)
Token length: 1474
Summarized using GPT-3.5-turbo
Append: [Meta-Design Matters: A Self-Design Multi-Agent System](https://arxiv.org/abs/2505.14996)
Token length: 1474
Summarized using GPT-3.5-turbo
Append: [Towards Spoken Mathematical Reasoning: Benchmarking Speech-based Models over Multi-faceted Math Problems](https://arxiv.org/abs/2505.15000)
Token length: 1479
Summarized using GPT-3.5-turbo
Append: [Diagnosing our datasets: How does my language model learn clinical information?](https://arxiv.org/abs/2505.15024)
Token length: 909
Summarized using GPT-3.5-turbo
Append: [Are the confidence scores of reviewers consistent with the review content? Evidence from top conference proceedings in AI](https://arxiv.org/abs/2505.15031)
Token length: 726
Summarized using GPT-3.5-turbo
Append: [Denoising Concept Vectors with Sparse Autoencoders for Improved Language Model Steering](https://arxiv.org/abs/2505.15038)
Token length: 1125
Summarized using GPT-3.5-turbo
Append: [Diffusion vs. Autoregressive Language Models: A Text Embedding Perspective](https://arxiv.org/abs/2505.15045)
Token length: 1523
Summarized using GPT-3.5-turbo
Append: [ChartCards: A Chart-Metadata Generation Framework for Multi-Task Chart Understanding](https://arxiv.org/abs/2505.15046)
Token length: 1745
Summarized using GPT-3.5-turbo
Append: [Improving the fact-checking performance of language models by relying on their entailment ability](https://arxiv.org/abs/2505.15050)
Token length: 1358
Summarized using GPT-3.5-turbo
Append: [MolLangBench: A Comprehensive Benchmark for Language-Prompted Molecular Structure Recognition, Editing, and Generation](https://arxiv.org/abs/2505.15054)
Token length: 1080
Summarized using GPT-3.5-turbo
Append: [Lost in Benchmarks? Rethinking Large Language Model Benchmarking with Item Response Theory](https://arxiv.org/abs/2505.15055)
Token length: 1873
Summarized using GPT-3.5-turbo
Append: [Self-GIVE: Associative Thinking from Limited Structured Knowledge for Enhanced Large Language Model Reasoning](https://arxiv.org/abs/2505.15062)
Token length: 1312
Summarized using GPT-3.5-turbo
Append: [UrduFactCheck: An Agentic Fact-Checking Framework for Urdu with Evidence Boosting and Benchmarking](https://arxiv.org/abs/2505.15063)
Token length: 1381
Summarized using GPT-3.5-turbo
Append: [The Pursuit of Empathy: Evaluating Small Language Models for PTSD Dialogue Support](https://arxiv.org/abs/2505.15065)
Token length: 1017
Summarized using GPT-3.5-turbo
Append: [In-Domain African Languages Translation Using LLMs and Multi-armed Bandits](https://arxiv.org/abs/2505.15069)
Token length: 1237
Summarized using GPT-3.5-turbo
Append: [Can Large Language Models Understand Internet Buzzwords Through User-Generated Content](https://arxiv.org/abs/2505.15071)
Token length: 1498
Summarized using GPT-3.5-turbo
Append: [DISCO Balances the Scales: Adaptive Domain- and Difficulty-Aware Reinforcement Learning on Imbalanced Data](https://arxiv.org/abs/2505.15074)
Token length: 1027
Summarized using GPT-3.5-turbo
Append: [Traveling Across Languages: Benchmarking Cross-Lingual Consistency in Multimodal LLMs](https://arxiv.org/abs/2505.15075)
Token length: 1310
Summarized using GPT-3.5-turbo
Append: [HopWeaver: Synthesizing Authentic Multi-Hop Questions Across Text Corpora](https://arxiv.org/abs/2505.15087)
Token length: 1464
Summarized using GPT-3.5-turbo
Append: [DeFTX: Denoised Sparse Fine-Tuning for Zero-Shot Cross-Lingual Transfer](https://arxiv.org/abs/2505.15090)
Token length: 1250
Summarized using GPT-3.5-turbo
Append: [SciCUEval: A Comprehensive Dataset for Evaluating Scientific Context Understanding in Large Language Models](https://arxiv.org/abs/2505.15094)
Token length: 1297
Summarized using GPT-3.5-turbo
Append: [Nek Minit: Harnessing Pragmatic Metacognitive Prompting for Explainable Sarcasm Detection of Australian and Indian English](https://arxiv.org/abs/2505.15095)
Token length: 1458
Summarized using GPT-3.5-turbo
Append: [Mechanistic evaluation of Transformers and state space models](https://arxiv.org/abs/2505.15105)
Append: [StepSearch: Igniting LLMs Search Ability via Step-Wise Proximal Policy Optimization](https://arxiv.org/abs/2505.15107)
Append: [A Risk Taxonomy for Evaluating AI-Powered Psychotherapy Agents](https://arxiv.org/abs/2505.15108)
Append: [RoT: Enhancing Table Reasoning with Iterative Row-Wise Traversals](https://arxiv.org/abs/2505.15110)
Append: [An Empirical Study on Reinforcement Learning for Reasoning-Search Interleaved LLM Agents](https://arxiv.org/abs/2505.15117)
Append: [Prolonged Reasoning Is Not All You Need: Certainty-Based Adaptive Routing for Efficient LLM/MLLM Reasoning](https://arxiv.org/abs/2505.15154)
Append: [ReflAct: World-Grounded Decision Making in LLM Agents via Goal-State Reflection](https://arxiv.org/abs/2505.15182)
Append: [EcomScriptBench: A Multi-task Benchmark for E-commerce Script Planning via Step-wise Intention-Driven Product Association](https://arxiv.org/abs/2505.15196)
Append: [DUSK: Do Not Unlearn Shared Knowledge](https://arxiv.org/abs/2505.15209)
Append: [Deliberation on Priors: Trustworthy Reasoning of Large Language Models on Knowledge Graphs](https://arxiv.org/abs/2505.15210)
Append: [R-TOFU: Unlearning in Large Reasoning Models](https://arxiv.org/abs/2505.15214)
Append: [Multilingual Prompting for Improving LLM Generation Diversity](https://arxiv.org/abs/2505.15229)
Append: [Towards Explainable Temporal Reasoning in Large Language Models: A Structure-Aware Generative Framework](https://arxiv.org/abs/2505.15245)
Append: [Fooling the LVLM Judges: Visual Biases in LVLM-Based Evaluation](https://arxiv.org/abs/2505.15249)
Append: [MentalMAC: Enhancing Large Language Models for Detecting Mental Manipulation via Multi-Task Anti-Curriculum Distillation](https://arxiv.org/abs/2505.15255)
Append: [When Less Language is More: Language-Reasoning Disentanglement Makes LLMs Better Multilingual Reasoners](https://arxiv.org/abs/2505.15257)
Append: [AGENT-X: Adaptive Guideline-based Expert Network for Threshold-free AI-generated teXt detection](https://arxiv.org/abs/2505.15261)
Append: [Web-Shepherd: Advancing PRMs for Reinforcing Web Agents](https://arxiv.org/abs/2505.15277)
Append: [Exploring In-Image Machine Translation with Real-World Background](https://arxiv.org/abs/2505.15282)
Append: [Hallucinate at the Last in Long Response Generation: A Case Study on Long Document Summarization](https://arxiv.org/abs/2505.15291)
Append: [Chinese Toxic Language Mitigation via Sentiment Polarity Consistent Rewrites](https://arxiv.org/abs/2505.15297)
Append: [Multi-Hop Question Generation via Dual-Perspective Keyword Guidance](https://arxiv.org/abs/2505.15299)
Append: [Emotional Supporters often Use Multiple Strategies in a Single Turn](https://arxiv.org/abs/2505.15316)
Append: [Improving LLM First-Token Predictions in Multiple-Choice Question Answering via Prefilling Attack](https://arxiv.org/abs/2505.15323)
Append: [Leveraging Unit Language Guidance to Advance Speech Modeling in Textless Speech-to-Speech Translation](https://arxiv.org/abs/2505.15333)
Append: [Your Language Model Can Secretly Write Like Humans: Contrastive Paraphrase Attacks on LLM-Generated Text Detectors](https://arxiv.org/abs/2505.15337)
Append: [FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management](https://arxiv.org/abs/2505.15347)
Append: [The Super Emotion Dataset](https://arxiv.org/abs/2505.15348)
Append: [Revealing Language Model Trajectories via Kullback-Leibler Divergence](https://arxiv.org/abs/2505.15353)
Append: [Decoding Phone Pairs from MEG Signals Across Speech Modalities](https://arxiv.org/abs/2505.15355)
Append: [NL-Debugging: Exploiting Natural Language as an Intermediate Representation for Code Debugging](https://arxiv.org/abs/2505.15356)
Append: [X-WebAgentBench: A Multilingual Interactive Web Benchmark for Evaluating Global Agentic System](https://arxiv.org/abs/2505.15372)
Append: [RePPL: Recalibrating Perplexity by Uncertainty in Semantic Propagation and Language Generation for Explainable QA Hallucination Detection](https://arxiv.org/abs/2505.15386)
Append: [Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark Study](https://arxiv.org/abs/2505.15389)
Append: [An Empirical Study of the Anchoring Effect in LLMs: Existence, Mechanism, and Potential Mitigations](https://arxiv.org/abs/2505.15392)
Append: [How Should We Enhance the Safety of Large Reasoning Models: An Empirical Study](https://arxiv.org/abs/2505.15404)
Append: [Trends and Challenges in Authorship Analysis: A Review of ML, DL, and LLM Approaches](https://arxiv.org/abs/2505.15422)
Append: [Gated Integration of Low-Rank Adaptation for Continual Learning of Language Models](https://arxiv.org/abs/2505.15424)
Append: [NeoN: A Tool for Automated Detection, Linguistic and LLM-Driven Analysis of Neologisms in Polish](https://arxiv.org/abs/2505.15426)
Append: [Responsible Diffusion Models via Constraining Text Embeddings within Safe Regions](https://arxiv.org/abs/2505.15427)
Append: [Likelihood Variance as Text Importance for Resampling Texts to Map Language Models](https://arxiv.org/abs/2505.15428)
Append: [Hunyuan-TurboS: Advancing Large Language Models through Mamba-Transformer Synergy and Adaptive Chain-of-Thought](https://arxiv.org/abs/2505.15431)
Append: [On the Generalization vs Fidelity Paradox in Knowledge Distillation](https://arxiv.org/abs/2505.15442)
Append: [AdUE: Improving uncertainty estimation head for LoRA adapters in LLMs](https://arxiv.org/abs/2505.15443)
Append: [Single LLM, Multiple Roles: A Unified Retrieval-Augmented Generation Framework Using Role-Specific Token Optimization](https://arxiv.org/abs/2505.15444)
Append: [Teaching Language Models to Evolve with Users: Dynamic Profile Modeling for Personalized Alignment](https://arxiv.org/abs/2505.15456)
Append: [Joint Flashback Adaptation for Forgetting-Resistant Instruction Tuning](https://arxiv.org/abs/2505.15467)
Append: [CoLA: Collaborative Low-Rank Adaptation](https://arxiv.org/abs/2505.15471)
Append: [PhysicsArena: The First Multimodal Physics Reasoning Benchmark Exploring Variable, Process, and Solution Dimensions](https://arxiv.org/abs/2505.15472)
Append: [LFTF: Locating First and Then Fine-Tuning for Mitigating Gender Bias in Large Language Models](https://arxiv.org/abs/2505.15475)
Append: [KaFT: Knowledge-aware Fine-tuning for Boosting LLMs' Domain-specific Question-Answering Performance](https://arxiv.org/abs/2505.15480)
Append: [Collaborative Problem-Solving in an Optimization Game](https://arxiv.org/abs/2505.15490)
Append: [Protoknowledge Shapes Behaviour of LLMs in Downstream Tasks: Memorization and Generalization with Knowledge Graphs](https://arxiv.org/abs/2505.15501)
Append: [Multilingual Test-Time Scaling via Initial Thought Transfer](https://arxiv.org/abs/2505.15508)
Append: [Evaluate Bias without Manual Test Sets: A Concept Representation Perspective for LLMs](https://arxiv.org/abs/2505.15524)
Append: [Social Bias in Popular Question-Answering Benchmarks](https://arxiv.org/abs/2505.15553)
Append: [DayDreamer at CQs-Gen 2025: Generating Critical Questions through Argument Scheme Completion](https://arxiv.org/abs/2505.15554)
Append: [A Survey on Multilingual Mental Disorders Detection from Social Media Data](https://arxiv.org/abs/2505.15556)
Append: [Do RAG Systems Suffer From Positional Bias?](https://arxiv.org/abs/2505.15561)
Append: [Semantic-based Unsupervised Framing Analysis (SUFA): A Novel Approach for Computational Framing Analysis](https://arxiv.org/abs/2505.15563)
Append: [From Problem-Solving to Teaching Problem-Solving: Aligning LLMs with Pedagogy using Reinforcement Learning](https://arxiv.org/abs/2505.15607)
Append: [Learn to Reason Efficiently with Adaptive Length-based Reward Shaping](https://arxiv.org/abs/2505.15612)
Append: [Can LLMs $\textit{understand}$ Math? -- Exploring the Pitfalls in Mathematical Reasoning](https://arxiv.org/abs/2505.15623)
Append: [Listen to the Context: Towards Faithful Large Language Models for Retrieval Augmented Generation on Climate Questions](https://arxiv.org/abs/2505.15633)
Append: [Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning in Language Models](https://arxiv.org/abs/2505.15634)
Append: [Word Level Timestamp Generation for Automatic Speech Recognition and Translation](https://arxiv.org/abs/2505.15646)
Append: [Be Careful When Fine-tuning On Open-Source LLMs: Your Fine-tuning Data Could Be Secretly Stolen!](https://arxiv.org/abs/2505.15656)
Append: [Efficient and Direct Duplex Modeling for Speech-to-Speech Language Model](https://arxiv.org/abs/2505.15670)
Append: [UniErase: Unlearning Token as a Universal Erasure Primitive for Language Models](https://arxiv.org/abs/2505.15674)
Append: [The Representational Alignment between Humans and Language Models is implicitly driven by a Concreteness Effect](https://arxiv.org/abs/2505.15682)
Append: [A Federated Splitting Framework for LLMs: Security, Efficiency, and Adaptability](https://arxiv.org/abs/2505.15683)
Append: [ThinkLess: A Training-Free Inference-Efficient Method for Reducing Reasoning Redundancy](https://arxiv.org/abs/2505.15684)
Append: [Thought-Augmented Policy Optimization: Bridging External Guidance and Internal Capabilities](https://arxiv.org/abs/2505.15692)
Append: [Can Large Language Models be Effective Online Opinion Miners?](https://arxiv.org/abs/2505.15695)
Append: [MaxPoolBERT: Enhancing BERT Classification via Layer- and Token-Wise Aggregation](https://arxiv.org/abs/2505.15696)
Append: ["Alexa, can you forget me?" Machine Unlearning Benchmark in Spoken Language Understanding](https://arxiv.org/abs/2505.15700)
Append: [LyapLock: Bounded Knowledge Preservation in Sequential Large Language Model Editing](https://arxiv.org/abs/2505.15702)
Append: [Advancing LLM Safe Alignment with Safety Representation Ranking](https://arxiv.org/abs/2505.15710)
Append: [TurnaboutLLM: A Deductive Reasoning Benchmark from Detective Games](https://arxiv.org/abs/2505.15712)
Append: [Beyond Empathy: Integrating Diagnostic and Therapeutic Reasoning with Large Language Models for Mental Health Counseling](https://arxiv.org/abs/2505.15715)
Append: [Shared Path: Unraveling Memorization in Multilingual LLMs through Language Similarities](https://arxiv.org/abs/2505.15722)
Append: [VocalBench: Benchmarking the Vocal Conversational Abilities for Speech Interaction Models](https://arxiv.org/abs/2505.15727)
Append: [DEBATE, TRAIN, EVOLVE: Self Evolution of Language Model Reasoning](https://arxiv.org/abs/2505.15734)
Append: [Transfer of Structural Knowledge from Synthetic Languages](https://arxiv.org/abs/2505.15769)
Append: [Beyond Hard and Soft: Hybrid Context Compression for Balancing Local and Global Information Retention](https://arxiv.org/abs/2505.15774)
Append: [ConvSearch-R1: Enhancing Query Reformulation for Conversational Search with Reasoning via Reinforcement Learning](https://arxiv.org/abs/2505.15776)
Append: [Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous Concept Space](https://arxiv.org/abs/2505.15778)
Append: [dKV-Cache: The Cache for Diffusion Language Models](https://arxiv.org/abs/2505.15781)
Append: [Long-Form Information Alignment Evaluation Beyond Atomic Facts](https://arxiv.org/abs/2505.15792)
Append: [Reverse Engineering Human Preferences with Reinforcement Learning](https://arxiv.org/abs/2505.15795)
Append: [VerifyBench: Benchmarking Reference-based Reward Systems for Large Language Models](https://arxiv.org/abs/2505.15801)
Append: [Keep Security! Benchmarking Security Policy Preservation in Large Language Model Contexts Against Indirect Attacks in Question Answering](https://arxiv.org/abs/2505.15805)
Append: [The Atlas of In-Context Learning: How Attention Heads Shape In-Context Retrieval Augmentation](https://arxiv.org/abs/2505.15807)
Append: [GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI Agents](https://arxiv.org/abs/2505.15810)
Append: [Learning to Reason via Mixture-of-Thought for Logical Reasoning](https://arxiv.org/abs/2505.15817)
Append: [Sentiment Analysis in Software Engineering: Evaluating Generative Pre-trained Transformers](https://arxiv.org/abs/2505.14692)
Append: [Benchmarking Graph Neural Networks for Document Layout Analysis in Public Affairs](https://arxiv.org/abs/2505.14699)
Append: [QUADS: QUAntized Distillation Framework for Efficient Speech Language Understanding](https://arxiv.org/abs/2505.14723)
Append: [MORALISE: A Structured Benchmark for Moral Alignment in Visual Language Models](https://arxiv.org/abs/2505.14728)
Append: [FisherSFT: Data-Efficient Supervised Fine-Tuning of Language Models Using Information Gain](https://arxiv.org/abs/2505.14826)
Append: [Think, Reflect, Create: Metacognitive Learning for Zero-Shot Robotic Planning with LLMs](https://arxiv.org/abs/2505.14899)
Append: [TCSinger 2: Customizable Multilingual Zero-shot Singing Voice Synthesis](https://arxiv.org/abs/2505.14910)
Append: [Learning to Rank Chain-of-Thought: An Energy-Based Approach with Outcome Supervision](https://arxiv.org/abs/2505.14999)
Append: [RL Tango: Reinforcing Generator and Verifier Together for Language Reasoning](https://arxiv.org/abs/2505.15034)
Append: [ModelingAgent: Bridging LLMs and Mathematical Modeling for Real-World Challenges](https://arxiv.org/abs/2505.15068)
Append: [An Alternative to FLOPS Regularization to Effectively Productionize SPLADE-Doc](https://arxiv.org/abs/2505.15070)
Append: [MoTime: A Dataset Suite for Multimodal Time Series Forecasting](https://arxiv.org/abs/2505.15072)
Append: [SUS backprop: linear backpropagation algorithm for long inputs in transformers](https://arxiv.org/abs/2505.15080)
Append: [ALN-P3: Unified Language Alignment for Perception, Prediction, and Planning in Autonomous Driving](https://arxiv.org/abs/2505.15158)
Append: [Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems](https://arxiv.org/abs/2505.15201)
Append: [BountyBench: Dollar Impact of AI Agent Attackers and Defenders on Real-World Cybersecurity Systems](https://arxiv.org/abs/2505.15216)
Append: [ReGUIDE: Data Efficient GUI Grounding via Spatial Reasoning and Search](https://arxiv.org/abs/2505.15259)
Append: [When Can Large Reasoning Models Save Thinking? Mechanistic Analysis of Behavioral Divergence in Reasoning](https://arxiv.org/abs/2505.15276)
Append: [AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving](https://arxiv.org/abs/2505.15298)
Append: [Trajectory Bellman Residual Minimization: A Simple Value-Based Method for LLM Reasoning](https://arxiv.org/abs/2505.15311)
Append: [AI vs. Human Judgment of Content Moderation: LLM-as-a-Judge and Ethics-Based Response Refusals](https://arxiv.org/abs/2505.15365)
Append: [Better Safe Than Sorry? Overreaction Problem of Vision Language Models in Visual Emergency Recognition](https://arxiv.org/abs/2505.15367)
Append: [When to Continue Thinking: Adaptive Thinking Mode Switching for Efficient Reasoning](https://arxiv.org/abs/2505.15400)
Append: [ClickSight: Interpreting Student Clickstreams to Reveal Insights on Learning Strategies via LLMs](https://arxiv.org/abs/2505.15410)
Append: [Set-LLM: A Permutation-Invariant LLM](https://arxiv.org/abs/2505.15433)
Append: [A Participatory Strategy for AI Ethics in Education and Rehabilitation grounded in the Capability Approach](https://arxiv.org/abs/2505.15466)
Append: [Seeing Through Deception: Uncovering Misleading Creator Intent in Multimodal News with Vision-Language Models](https://arxiv.org/abs/2505.15489)
Append: [Visual Thoughts: A Unified Perspective of Understanding Multimodal Chain-of-Thought](https://arxiv.org/abs/2505.15510)
Append: [Explainable embeddings with Distance Explainer](https://arxiv.org/abs/2505.15516)
Append: [Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot Manipulation Datasets](https://arxiv.org/abs/2505.15517)
Append: [MIRB: Mathematical Information Retrieval Benchmark](https://arxiv.org/abs/2505.15585)
Append: [Mechanistic Insights into Grokking from the Embedding Layer](https://arxiv.org/abs/2505.15624)
Append: [Segmentation-Variant Codebooks for Preservation of Paralinguistic and Prosodic Information](https://arxiv.org/abs/2505.15667)
Append: [HDLxGraph: Bridging Large Language Models and HDL Repositories via HDL Graph Databases](https://arxiv.org/abs/2505.15701)
Append: [Alignment Under Pressure: The Case for Informed Adversaries When Evaluating LLM Defenses](https://arxiv.org/abs/2505.15738)
Append: [Evolutionary Computation and Large Language Models: A Survey of Methods, Synergies, and Applications](https://arxiv.org/abs/2505.15741)
Append: [Scalable Defense against In-the-wild Jailbreaking Attacks with Safety Context Retrieval](https://arxiv.org/abs/2505.15753)
Append: [MIKU-PAL: An Automated and Standardized Multi-Modal Method for Speech Paralinguistic and Affect Labeling](https://arxiv.org/abs/2505.15772)
Append: [ToxicTone: A Mandarin Audio Dataset Annotated for Toxicity and Toxic Utterance Tonality](https://arxiv.org/abs/2505.15773)
Append: [Large Language Models as Computable Approximations to Solomonoff Induction](https://arxiv.org/abs/2505.15784)
Append: [Predicting generalization performance with correctness discriminators](https://arxiv.org/abs/2311.09422)
Append: [A Framework for Real-time Safeguarding the Text Generation of Large Language Model](https://arxiv.org/abs/2404.19048)
Append: [MARS: Benchmarking the Metaphysical Reasoning Abilities of Language Models with a Multi-task Evaluation Dataset](https://arxiv.org/abs/2406.02106)
Append: [Exploring the Robustness of Language Models for Tabular Question Answering via Attention Analysis](https://arxiv.org/abs/2406.12719)
Append: [Helpful assistant or fruitful facilitator? Investigating how personas affect language model behavior](https://arxiv.org/abs/2407.02099)
Append: [A Comprehensive Evaluation of Large Language Models on Temporal Event Forecasting](https://arxiv.org/abs/2407.11638)
Append: [dMel: Speech Tokenization made Simple](https://arxiv.org/abs/2407.15835)
Append: [Fine-tuning Large Language Models for Entity Matching](https://arxiv.org/abs/2409.08185)
Append: [A Closer Look at Machine Unlearning for Large Language Models](https://arxiv.org/abs/2410.08109)
Append: [Meta-Chunking: Learning Text Segmentation and Semantic Completion via Logical Perception](https://arxiv.org/abs/2410.12788)
Append: [Retrospective Learning from Interactions](https://arxiv.org/abs/2410.13852)
Append: [GATEAU: Selecting Influential Samples for Long Context Alignment](https://arxiv.org/abs/2410.15633)
Append: [Exploring Pretraining via Active Forgetting for Improving Cross Lingual Transfer for Decoder Language Models](https://arxiv.org/abs/2410.16168)
Append: [Robust and Minimally Invasive Watermarking for EaaS](https://arxiv.org/abs/2410.17552)
Append: [Let's Be Self-generated via Step by Step: A Curriculum Learning Approach to Automated Reasoning with Large Language Models](https://arxiv.org/abs/2410.21728)
Append: [Untangling Hate Speech Definitions: A Semantic Componential Analysis Across Cultures and Domains](https://arxiv.org/abs/2411.07417)
Append: [FastDraft: How to Train Your Draft](https://arxiv.org/abs/2411.11055)
Append: [Dial-In LLM: Human-Aligned LLM-in-the-loop Intent Clustering for Customer Service Dialogues](https://arxiv.org/abs/2412.09049)
Append: [ROUTE: Robust Multitask Tuning and Collaboration for Text-to-SQL](https://arxiv.org/abs/2412.10138)
Append: [DARWIN 1.5: Large Language Models as Materials Science Adapted Learners](https://arxiv.org/abs/2412.11970)
Append: [Exploring Cross-lingual Latent Transplantation: Mutual Opportunities and Open Challenges](https://arxiv.org/abs/2412.12686)
Append: [MRAG: A Modular Retrieval Framework for Time-Sensitive Question Answering](https://arxiv.org/abs/2412.15540)
Append: [Shaping the Safety Boundaries: Understanding and Defending Against Jailbreaks in Large Language Models](https://arxiv.org/abs/2412.17034)
Append: [How to Enable Effective Cooperation Between Humans and NLP Models: A Survey of Principles, Formalizations, and Beyond](https://arxiv.org/abs/2501.05714)
Append: [Analyzing the Effect of Linguistic Similarity on Cross-Lingual Transfer: Tasks and Experimental Setups Matter](https://arxiv.org/abs/2501.14491)
Append: [ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference](https://arxiv.org/abs/2502.00299)
Append: [Massive Values in Self-Attention Modules are the Key to Contextual Knowledge Understanding](https://arxiv.org/abs/2502.01563)
Append: [Lifelong Knowledge Editing requires Better Regularization](https://arxiv.org/abs/2502.01636)
Append: [BARE: Leveraging Base Language Models for Few-Shot Synthetic Data Generation](https://arxiv.org/abs/2502.01697)
Append: [Can LLMs Maintain Fundamental Abilities under KV Cache Compression?](https://arxiv.org/abs/2502.01941)
Append: [Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization](https://arxiv.org/abs/2502.04295)
Append: [An Analysis for Reasoning Bias of Language Models with Small Initialization](https://arxiv.org/abs/2502.04375)
Append: [Uncertainty Quantification for LLMs through Minimum Bayes Risk: Bridging Confidence and Consistency](https://arxiv.org/abs/2502.04964)
Append: [CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction](https://arxiv.org/abs/2502.07316)
Append: [Which Retain Set Matters for LLM Unlearning? A Case Study on Entity Unlearning](https://arxiv.org/abs/2502.11441)
Append: [GLTW: Joint Improved Graph Transformer and LLM via Three-Word Language for Knowledge Graph Completion](https://arxiv.org/abs/2502.11471)
Append: [SEA: Low-Resource Safety Alignment for Multimodal Large Language Models via Synthetic Embeddings](https://arxiv.org/abs/2502.12562)
Append: [Linguistic Generalizations are not Rules: Impacts on Evaluation of LMs](https://arxiv.org/abs/2502.13195)
Append: [FineEdit: Unlock Instruction-Based Text Editing for LLMs](https://arxiv.org/abs/2502.13358)
Append: [Reducing Hallucinations in Language Model-based SPARQL Query Generation Using Post-Generation Memory Retrieval](https://arxiv.org/abs/2502.13369)
Append: [UniKnow: A Unified Framework for Reliable Language Model Behavior across Parametric and External Knowledge](https://arxiv.org/abs/2502.13648)
Append: [Rapid Word Learning Through Meta In-Context Learning](https://arxiv.org/abs/2502.14791)
Append: [Sparsity May Be All You Need: Sparse Random Parameter Adaptation](https://arxiv.org/abs/2502.15975)
Append: [Finding the Sweet Spot: Preference Data Construction for Scaling Preference Optimization](https://arxiv.org/abs/2502.16825)
Append: [Spontaneous Giving and Calculated Greed in Language Models](https://arxiv.org/abs/2502.17720)
Append: [Stay Focused: Problem Drift in Multi-Agent Debate](https://arxiv.org/abs/2502.19559)
Append: [Sensing and Steering Stereotypes: Extracting and Applying Gender Representation Vectors in LLMs](https://arxiv.org/abs/2502.19721)
Append: [Adaptively profiling models with task elicitation](https://arxiv.org/abs/2503.01986)
Append: [Scaling Laws for Many-Shot In-Context Learning with Self-Generated Annotations](https://arxiv.org/abs/2503.03062)
Append: [The Devil Is in the Details: Tackling Unimodal Spurious Correlations for Generalizable Multimodal Reward Models](https://arxiv.org/abs/2503.03122)
Append: [DB-Explore: Automated Database Exploration and Instruction Synthesis for Text-to-SQL](https://arxiv.org/abs/2503.04959)
Append: [Sketch-of-Thought: Efficient LLM Reasoning with Adaptive Cognitive-Inspired Sketching](https://arxiv.org/abs/2503.05179)
Append: [Large Language Models Post-training: Surveying Techniques from Alignment to Reasoning](https://arxiv.org/abs/2503.06072)
Append: [BriLLM: Brain-inspired Large Language Model](https://arxiv.org/abs/2503.11299)
Append: [Adaptive Group Policy Optimization: Towards Stable Training and Token-Efficient Reasoning](https://arxiv.org/abs/2503.15952)
Append: [AfroXLMR-Social: Adapting Pre-trained Language Models for African Languages Social Media Text](https://arxiv.org/abs/2503.18247)
Append: [A Multilingual, Culture-First Approach to Addressing Misgendering in LLM Applications](https://arxiv.org/abs/2503.20302)
Append: [Enhancing Large Language Models (LLMs) for Telecommunications using Knowledge Graphs and Retrieval-Augmented Generation](https://arxiv.org/abs/2503.24245)
Append: [GLiNER-BioMed: A Suite of Efficient Models for Open Biomedical Named Entity Recognition](https://arxiv.org/abs/2504.00676)
Append: [Think When You Need: Self-Adaptive Chain-of-Thought Learning](https://arxiv.org/abs/2504.03234)
Append: [Thinking Out Loud: Do Reasoning Models Know When They're Right?](https://arxiv.org/abs/2504.06564)
Append: [Understanding the Repeat Curse in Large Language Models from a Feature Perspective](https://arxiv.org/abs/2504.14218)
Append: [Instruction-Tuning Data Synthesis from Scratch via Web Reconstruction](https://arxiv.org/abs/2504.15573)
Append: [Improving Language Model Personas via Rationalization with Psychological Scaffolds](https://arxiv.org/abs/2504.17993)
Append: [Ask, Fail, Repeat: Meeseeks, an Iterative Feedback Benchmark for LLMs' Multi-turn Instruction-Following Ability](https://arxiv.org/abs/2504.21625)
Append: [Efficient Shapley Value-based Non-Uniform Pruning of Large Language Models](https://arxiv.org/abs/2505.01731)
Append: [Towards Safer Pretraining: Analyzing and Filtering Harmful Content in Webscale datasets for Responsible LLMs](https://arxiv.org/abs/2505.02009)
Append: [Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in Large Language Models](https://arxiv.org/abs/2505.02847)
Append: [Long-Short Chain-of-Thought Mixture Supervised Fine-Tuning Eliciting Efficient Reasoning in Large Language Models](https://arxiv.org/abs/2505.03469)
Append: [Think in Safety: Unveiling and Mitigating Safety Alignment Collapse in Multimodal Large Reasoning Model](https://arxiv.org/abs/2505.06538)
Append: [MacRAG: Compress, Slice, and Scale-up for Multi-Scale Adaptive Context RAG](https://arxiv.org/abs/2505.06569)
Append: [SpikeCLIP: A Contrastive Language-Image Pretrained Spiking Neural Network](https://arxiv.org/abs/2310.06488)
Append: [Uncertainty quantification in fine-tuned LLMs using LoRA ensembles](https://arxiv.org/abs/2402.12264)
Append: [DPO Meets PPO: Reinforced Token Optimization for RLHF](https://arxiv.org/abs/2404.18922)
Append: [Inverse Design of Metal-Organic Frameworks Using Quantum Natural Language Processing](https://arxiv.org/abs/2405.11783)
Append: [SPA-VL: A Comprehensive Safety Preference Alignment Dataset for Vision Language Model](https://arxiv.org/abs/2406.12030)
Append: [Parameter-Efficient Fine-Tuning via Circular Convolution](https://arxiv.org/abs/2407.19342)
Append: [An In-Depth Investigation of Data Collection in LLM App Ecosystems](https://arxiv.org/abs/2408.13247)
Append: [NESTFUL: A Benchmark for Evaluating LLMs on Nested Sequences of API Calls](https://arxiv.org/abs/2409.03797)
Append: [Quantifying Feature Space Universality Across Large Language Models via Sparse Autoencoders](https://arxiv.org/abs/2410.06981)
Append: [Parameter Efficient Fine-tuning via Explained Variance Adaptation](https://arxiv.org/abs/2410.07170)
Append: [How to Construct Random Unitaries](https://arxiv.org/abs/2410.10116)
Append: [WhiSPA: Semantically and Psychologically Aligned Whisper with Self-Supervised Contrastive and Student-Teacher Learning](https://arxiv.org/abs/2501.16344)
Append: [PixelWorld: Towards Perceiving Everything as Pixels](https://arxiv.org/abs/2501.19339)
Append: [How Contaminated Is Your Benchmark? Quantifying Dataset Leakage in Large Language Models with Kernel Divergence](https://arxiv.org/abs/2502.00678)
Append: [FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation](https://arxiv.org/abs/2502.01068)
Append: [The Jumping Reasoning Curve? Tracking the Evolution of Reasoning Performance in GPT-[n] and o-[n] Models on Multimodal Puzzles](https://arxiv.org/abs/2502.01081)
Append: [Neurons Speak in Ranges: Breaking Free from Discrete Neuronal Attribution](https://arxiv.org/abs/2502.06809)
Append: [MoHAVE: Mixture of Hierarchical Audio-Visual Experts for Robust Speech Recognition](https://arxiv.org/abs/2502.10447)
Append: [Probing Semantic Routing in Large Mixture-of-Expert Models](https://arxiv.org/abs/2502.10928)
Append: [Automated Visualization Code Synthesis via Multi-Path Reasoning and Feedback-Driven Optimization](https://arxiv.org/abs/2502.11140)
Append: [GiFT: Gibbs Fine-Tuning for Code Generation](https://arxiv.org/abs/2502.11466)
Append: [Scaling Text-Rich Image Understanding via Code-Guided Synthetic Multimodal Data Generation](https://arxiv.org/abs/2502.14846)
Append: [Intermediate Languages Matter: Formal Choice Drives Neurosymbolic LLM Reasoning](https://arxiv.org/abs/2502.17216)
Append: [Large Language Models are Powerful Electronic Health Record Encoders](https://arxiv.org/abs/2502.17403)
Append: [ZEBRA: Leveraging Model-Behavioral Knowledge for Zero-Annotation Preference Dataset Construction](https://arxiv.org/abs/2502.18744)
Append: [SQLCritic: Correcting Text-to-SQL Generation via Clause-wise Critic](https://arxiv.org/abs/2503.07996)
Append: [Tempest: Autonomous Multi-Turn Jailbreaking of Large Language Models with Tree Search](https://arxiv.org/abs/2503.10619)
Append: [Design and Implementation of an FPGA-Based Hardware Accelerator for Transformer](https://arxiv.org/abs/2503.16731)
Append: [Plain Transformers Can be Powerful Graph Learners](https://arxiv.org/abs/2504.12588)
Append: [ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning through Step-by-Step Verification](https://arxiv.org/abs/2504.20930)
Append: [Ada-R1: Hybrid-CoT via Bi-Level Adaptive Reasoning Optimization](https://arxiv.org/abs/2504.21659)
Append: [SWE-smith: Scaling Data for Software Engineering Agents](https://arxiv.org/abs/2504.21798)
Append: [Scalable Chain of Thoughts via Elastic Reasoning](https://arxiv.org/abs/2505.05315)
append_entries: 288
Finish: 2025-05-22 04:26:48.503834
------------------------------------------------------
Started: 2025-05-22 06:24:39.848926
Existing_entries: 1288
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1315
Summarized using GPT-3.5-turbo
Append: [Large Language Models Are More Persuasive Than Incentivized Human Persuaders](https://arxiv.org/abs/2505.09662)
Token length: 1547
Summarized using GPT-3.5-turbo
Append: [Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models](https://arxiv.org/abs/2505.10446)
Token length: 1487
Summarized using GPT-3.5-turbo
Append: [GODBench: A Benchmark for Multimodal Large Language Models in Video Comment Art](https://arxiv.org/abs/2505.11436)
append_entries: 3
Finish: 2025-05-22 06:24:47.496843
------------------------------------------------------
Started: 2025-05-22 08:25:06.511999
Existing_entries: 1003
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-22 08:25:07.187998
------------------------------------------------------
Started: 2025-05-22 10:18:16.315037
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-22 10:18:17.025357
------------------------------------------------------
Started: 2025-05-22 12:34:57.470534
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-22 12:34:58.070158
------------------------------------------------------
Started: 2025-05-22 14:16:38.128724
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-22 14:16:38.741083
------------------------------------------------------
Started: 2025-05-22 16:20:57.760111
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-22 16:20:58.471178
------------------------------------------------------
Started: 2025-05-22 18:23:07.590818
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-22 18:23:08.208239
------------------------------------------------------
Started: 2025-05-22 20:18:25.348077
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-22 20:18:25.981756
------------------------------------------------------
Started: 2025-05-22 22:16:05.593090
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-22 22:16:06.192508
------------------------------------------------------
Started: 2025-05-23 01:19:00.778553
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-23 01:19:01.476595
------------------------------------------------------
Started: 2025-05-23 03:09:21.731424
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-23 03:09:22.381982
------------------------------------------------------
Started: 2025-05-23 04:30:47.266131
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1317
Summarized using GPT-3.5-turbo
Append: [BR-TaxQA-R: A Dataset for Question Answering with References for Brazilian Personal Income Tax Law, including case law](https://arxiv.org/abs/2505.15916)
Token length: 1485
Summarized using GPT-3.5-turbo
Append: [Extracting Probabilistic Knowledge from Large Language Models for Bayesian Network Parameterization](https://arxiv.org/abs/2505.15918)
Token length: 1112
Summarized using GPT-3.5-turbo
Append: [Aligning Dialogue Agents with Global Feedback via Large Language Model Reward Decomposition](https://arxiv.org/abs/2505.15922)
Token length: 1512
Summarized using GPT-3.5-turbo
Append: [Citation Parsing and Analysis with Language Models](https://arxiv.org/abs/2505.15948)
Token length: 1721
Summarized using GPT-3.5-turbo
Append: [Training Step-Level Reasoning Verifiers with Formal Verification Tools](https://arxiv.org/abs/2505.15960)
Token length: 1021
Summarized using GPT-3.5-turbo
Append: [Pre-training Large Memory Language Models with Internal and External Knowledge](https://arxiv.org/abs/2505.15962)
Token length: 846
Summarized using GPT-3.5-turbo
Append: [Explaining Puzzle Solutions in Natural Language: An Exploratory Study on 6x6 Sudoku](https://arxiv.org/abs/2505.15993)
Token length: 1134
Summarized using GPT-3.5-turbo
Append: [Leveraging Online Data to Enhance Medical Knowledge in a Small Persian Language Model](https://arxiv.org/abs/2505.16000)
Token length: 1010
Summarized using GPT-3.5-turbo
Append: [Causal Interventions Reveal Shared Structure Across English Filler-Gap Constructions](https://arxiv.org/abs/2505.16002)
Token length: 1233
Summarized using GPT-3.5-turbo
Append: [SLMEval: Entropy-Based Calibration for Human-Aligned Evaluation of Large Language Models](https://arxiv.org/abs/2505.16003)
Token length: 1396
Summarized using GPT-3.5-turbo
Append: [LAGO: Few-shot Crosslingual Embedding Inversion Attacks via Language Similarity-Aware Graph Optimization](https://arxiv.org/abs/2505.16008)
Token length: 1743
Summarized using GPT-3.5-turbo
Append: [Ranking Free RAG: Replacing Re-ranking with Selection in RAG for Sensitive Domains](https://arxiv.org/abs/2505.16014)
Token length: 1164
Summarized using GPT-3.5-turbo
Append: [NOVER: Incentive Training for Language Models via Verifier-Free Reinforcement Learning](https://arxiv.org/abs/2505.16022)
Token length: 1309
Summarized using GPT-3.5-turbo
Append: [Prototypical Human-AI Collaboration Behaviors from LLM-Assisted Writing in the Wild](https://arxiv.org/abs/2505.16023)
Token length: 1274
Summarized using GPT-3.5-turbo
Append: [OpenEthics: A Comprehensive Ethical Evaluation of Open-Source Generative Large Language Models](https://arxiv.org/abs/2505.16036)
Token length: 936
Summarized using GPT-3.5-turbo
Append: [Internal and External Impacts of Natural Language Processing Papers](https://arxiv.org/abs/2505.16061)
Token length: 1386
Summarized using GPT-3.5-turbo
Append: [Small Language Models in the Real World: Insights from Industrial Text Classification](https://arxiv.org/abs/2505.16078)
Token length: 1492
Summarized using GPT-3.5-turbo
Append: [BiasLab: Toward Explainable Political Bias Detection with Dual-Axis Annotations and Rationale Indicators](https://arxiv.org/abs/2505.16081)
Token length: 1361
Summarized using GPT-3.5-turbo
Append: [Date Fragments: A Hidden Bottleneck of Tokenization for Temporal Reasoning](https://arxiv.org/abs/2505.16088)
Token length: 1569
Summarized using GPT-3.5-turbo
Append: [Continually Self-Improving Language Models for Bariatric Surgery Question--Answering](https://arxiv.org/abs/2505.16102)
Token length: 1063
Summarized using GPT-3.5-turbo
Append: [Hierarchical Safety Realignment: Lightweight Restoration of Safety in Pruned Large Vision-Language Models](https://arxiv.org/abs/2505.16104)
Token length: 1219
Summarized using GPT-3.5-turbo
Append: [MPL: Multiple Programming Languages with Large Language Models for Information Extraction](https://arxiv.org/abs/2505.16107)
Token length: 812
Summarized using GPT-3.5-turbo
Append: [Semiotic Reconstruction of Destination Expectation Constructs An LLM-Driven Computational Paradigm for Social Media Tourism Analytics](https://arxiv.org/abs/2505.16118)
Token length: 1591
Summarized using GPT-3.5-turbo
Append: [KoBALT: Korean Benchmark For Advanced Linguistic Tasks](https://arxiv.org/abs/2505.16125)
Token length: 1321
Summarized using GPT-3.5-turbo
Append: [Veracity Bias and Beyond: Uncovering LLMs' Hidden Beliefs in Problem-Solving Reasoning](https://arxiv.org/abs/2505.16128)
Token length: 932
Summarized using GPT-3.5-turbo
Append: [LLMs Are Not Scorers: Rethinking MT Evaluation with Generation-Based Methods](https://arxiv.org/abs/2505.16129)
Token length: 989
Summarized using GPT-3.5-turbo
Append: [Position of Uncertainty: A Cross-Linguistic Study of Positional Bias in Large Language Models](https://arxiv.org/abs/2505.16134)
Token length: 1579
Summarized using GPT-3.5-turbo
Append: [Distilling the Implicit Multi-Branch Structure in LLMs' Reasoning via Reinforcement Learning](https://arxiv.org/abs/2505.16142)
Token length: 1092
Summarized using GPT-3.5-turbo
Append: [EduBench: A Comprehensive Benchmarking Dataset for Evaluating Large Language Models in Diverse Educational Scenarios](https://arxiv.org/abs/2505.16160)
Token length: 1038
Summarized using GPT-3.5-turbo
Append: [KNN-SSD: Enabling Dynamic Self-Speculative Decoding via Nearest Neighbor Layer Set Optimization](https://arxiv.org/abs/2505.16162)
Token length: 1044
Summarized using GPT-3.5-turbo
Append: [Can LLMs Simulate Human Behavioral Variability? A Case Study in the Phonemic Fluency Task](https://arxiv.org/abs/2505.16164)
Token length: 1240
Summarized using GPT-3.5-turbo
Append: [When Do LLMs Admit Their Mistakes? Understanding the Role of Model Belief in Retraction](https://arxiv.org/abs/2505.16170)
Token length: 1553
Summarized using GPT-3.5-turbo
Append: [Automated Feedback Loops to Protect Text Simplification with Generative AI from Information Loss](https://arxiv.org/abs/2505.16172)
Token length: 1399
Summarized using GPT-3.5-turbo
Append: [Understanding Fact Recall in Language Models: Why Two-Stage Training Encourages Memorization but Mixed Training Teaches Knowledge](https://arxiv.org/abs/2505.16178)
Token length: 1180
Summarized using GPT-3.5-turbo
Append: [SAE-SSV: Supervised Steering in Sparse Representation Spaces for Reliable Control of Language Models](https://arxiv.org/abs/2505.16188)
Token length: 1193
Summarized using GPT-3.5-turbo
Append: [The Language of Interoception: Examining Embodiment and Emotion Through a Corpus of Body Part Mentions](https://arxiv.org/abs/2505.16189)
Token length: 1264
Summarized using GPT-3.5-turbo
Append: [An Empirical Study on Configuring In-Context Learning Demonstrations for Unleashing MLLMs' Sentimental Perception Capability](https://arxiv.org/abs/2505.16193)
Token length: 952
Summarized using GPT-3.5-turbo
Append: [Large Language Models based ASR Error Correction for Child Conversations](https://arxiv.org/abs/2505.16212)
Token length: 1095
Summarized using GPT-3.5-turbo
Append: [Memorization or Reasoning? Exploring the Idiom Understanding of LLMs](https://arxiv.org/abs/2505.16216)
Token length: 1267
Summarized using GPT-3.5-turbo
Append: [Don't Judge Code by Its Cover: Exploring Biases in LLM Judges for Code Evaluation](https://arxiv.org/abs/2505.16222)
Token length: 1412
Summarized using GPT-3.5-turbo
Append: [Explain Less, Understand More: Jargon Detection via Personalized Parameter-Efficient Fine-tuning](https://arxiv.org/abs/2505.16227)
Token length: 1265
Summarized using GPT-3.5-turbo
Append: [MuseRAG: Idea Originality Scoring At Scale](https://arxiv.org/abs/2505.16232)
Token length: 1631
Summarized using GPT-3.5-turbo
Append: [LIFEBench: Evaluating Length Instruction Following in Large Language Models](https://arxiv.org/abs/2505.16234)
Token length: 1789
Summarized using GPT-3.5-turbo
Append: [Align-GRAG: Reasoning-Guided Dual Alignment for Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2505.16237)
Token length: 1459
Summarized using GPT-3.5-turbo
Append: [Three Minds, One Legend: Jailbreak Large Reasoning Model with Adaptive Stacked Ciphers](https://arxiv.org/abs/2505.16241)
Token length: 1290
Summarized using GPT-3.5-turbo
Append: [Diverse, not Short: A Length-Controlled Self-Learning Framework for Improving Response Diversity of Language Models](https://arxiv.org/abs/2505.16245)
Token length: 970
Summarized using GPT-3.5-turbo
Append: [Does Localization Inform Unlearning? A Rigorous Examination of Local Parameter Attribution for Knowledge Unlearning in Language Models](https://arxiv.org/abs/2505.16252)
Token length: 873
Summarized using GPT-3.5-turbo
Append: [IRONIC: Coherence-Aware Reasoning Chains for Multi-Modal Sarcasm Detection](https://arxiv.org/abs/2505.16258)
Token length: 1479
Summarized using GPT-3.5-turbo
Append: [Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning](https://arxiv.org/abs/2505.16270)
Token length: 1195
Summarized using GPT-3.5-turbo
Append: [Spontaneous Speech Variables for Evaluating LLMs Cognitive Plausibility](https://arxiv.org/abs/2505.16277)
Append: [HiMATE: A Hierarchical Multi-Agent Framework for Machine Translation Evaluation](https://arxiv.org/abs/2505.16281)
Append: [Augmenting LLM Reasoning with Dynamic Notes Writing for Complex QA](https://arxiv.org/abs/2505.16293)
Append: [ToDi: Token-wise Distillation via Fine-Grained Divergence Control](https://arxiv.org/abs/2505.16297)
Append: [INFERENCEDYNAMICS: Efficient Routing Across LLMs through Structured Capability and Knowledge Profiling](https://arxiv.org/abs/2505.16303)
Append: [PMPO: Probabilistic Metric Prompt Optimization for Small and Large Language Models](https://arxiv.org/abs/2505.16307)
Append: [CLEAR: A Clinically-Grounded Tabular Framework for Radiology Report Evaluation](https://arxiv.org/abs/2505.16325)
Append: [SC4ANM: Identifying Optimal Section Combinations for Automated Novelty Prediction in Academic Papers](https://arxiv.org/abs/2505.16330)
Append: [Embodied Agents Meet Personalization: Exploring Memory Utilization for Personalized Assistance](https://arxiv.org/abs/2505.16348)
Append: [Ask, Retrieve, Summarize: A Modular Pipeline for Scientific Literature Summarization](https://arxiv.org/abs/2505.16349)
Append: [PaTH Attention: Position Encoding via Accumulating Householder Transformations](https://arxiv.org/abs/2505.16381)
Append: [Semantic Pivots Enable Cross-Lingual Transfer in Large Language Models](https://arxiv.org/abs/2505.16385)
Append: [Resource for Error Analysis in Text Simplification: New Taxonomy and Test Collection](https://arxiv.org/abs/2505.16392)
Append: [On the reliability of feature attribution methods for speech classification](https://arxiv.org/abs/2505.16406)
Append: [From Surveys to Narratives: Rethinking Cultural Value Adaptation in LLMs](https://arxiv.org/abs/2505.16408)
Append: [Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning](https://arxiv.org/abs/2505.16410)
Append: [Attributing Response to Context: A Jensen-Shannon Divergence Driven Mechanistic Study of Context Attribution in Retrieval-Augmented Generation](https://arxiv.org/abs/2505.16415)
Append: [Exploring the Relationship Between Diversity and Quality in Ad Text Generation](https://arxiv.org/abs/2505.16418)
Append: [WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2505.16421)
Append: [$I^2G$: Generating Instructional Illustrations via Text-Conditioned Diffusion](https://arxiv.org/abs/2505.16425)
Append: [Beyond Static Testbeds: An Interaction-Centric Agent Simulation Platform for Dynamic Recommender Systems](https://arxiv.org/abs/2505.16429)
Append: [University of Indonesia at SemEval-2025 Task 11: Evaluating State-of-the-Art Encoders for Multi-Label Emotion Detection](https://arxiv.org/abs/2505.16460)
Append: [Reading Between the Prompts: How Stereotypes Shape LLM's Implicit Personalization](https://arxiv.org/abs/2505.16467)
Append: [Teaching Large Language Models to Maintain Contextual Faithfulness via Synthetic Tasks and Reinforcement Learning](https://arxiv.org/abs/2505.16483)
Append: [LLaMAs Have Feelings Too: Unveiling Sentiment and Emotion Representations in LLaMA Models Through Probing](https://arxiv.org/abs/2505.16491)
Append: [Sparse Activation Editing for Reliable Instruction Following in Narratives](https://arxiv.org/abs/2505.16505)
Append: [AppealCase: A Dataset and Benchmark for Civil Case Appeal Scenarios](https://arxiv.org/abs/2505.16514)
Append: [CUB: Benchmarking Context Utilisation Techniques for Language Models](https://arxiv.org/abs/2505.16518)
Append: [Are the Hidden States Hiding Something? Testing the Limits of Factuality-Encoding Capabilities in LLMs](https://arxiv.org/abs/2505.16520)
Append: [Benchmarking and Pushing the Multi-Bias Elimination Boundary of LLMs via Causal Effect Estimation-guided Debiasing](https://arxiv.org/abs/2505.16522)
Append: [EnSToM: Enhancing Dialogue Systems with Entropy-Scaled Steering Vectors for Topic Maintenance](https://arxiv.org/abs/2505.16526)
Append: [Mechanistic Understanding and Mitigation of Language Confusion in English-Centric Large Language Models](https://arxiv.org/abs/2505.16538)
Append: [Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains](https://arxiv.org/abs/2505.16552)
Append: [ScholarBench: A Bilingual Benchmark for Abstraction, Comprehension, and Reasoning Evaluation in Academic Contexts](https://arxiv.org/abs/2505.16566)
Append: [URLs Help, Topics Guide: Understanding Metadata Utility in LLM Training](https://arxiv.org/abs/2505.16570)
Append: [EMULATE: A Multi-Agent Framework for Determining the Veracity of Atomic Claims by Emulating Human Actions](https://arxiv.org/abs/2505.16576)
Append: [O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended Question Answering](https://arxiv.org/abs/2505.16582)
Append: [Evaluating Large Language Model with Knowledge Oriented Language Specific Simple Question Answering](https://arxiv.org/abs/2505.16591)
Append: [What Media Frames Reveal About Stance: A Dataset and Study about Memes in Climate Change Discourse](https://arxiv.org/abs/2505.16592)
Append: [From Generic Empathy to Personalized Emotional Support: A Self-Evolution Framework for User Preference Alignment](https://arxiv.org/abs/2505.16610)
Append: [Steering Large Language Models for Machine Translation Personalization](https://arxiv.org/abs/2505.16612)
Append: [SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine Translation](https://arxiv.org/abs/2505.16637)
Append: [Collaboration among Multiple Large Language Models for Medical Question Answering](https://arxiv.org/abs/2505.16648)
Append: [Can reasoning models comprehend mathematical problems in Chinese ancient texts? An empirical study based on data from Suanjing Shishu](https://arxiv.org/abs/2505.16660)
Append: [A Japanese Language Model and Three New Evaluation Benchmarks for Pharmaceutical NLP](https://arxiv.org/abs/2505.16661)
Append: [Beyond Induction Heads: In-Context Meta Learning Induces Multi-Phase Circuit Emergence](https://arxiv.org/abs/2505.16694)
Append: [Locate-then-Merge: Neuron-Level Parameter Fusion for Mitigating Catastrophic Forgetting in Multimodal LLMs](https://arxiv.org/abs/2505.16703)
Append: [Breaking mBad! Supervised Fine-tuning for Cross-Lingual Detoxification](https://arxiv.org/abs/2505.16722)
Append: [TRIM: Achieving Extreme Sparsity with Targeted Row-wise Iterative Metric-driven Pruning](https://arxiv.org/abs/2505.16743)
Append: [IFEval-Audio: Benchmarking Instruction-Following Capability in Audio-based Large Language Models](https://arxiv.org/abs/2505.16774)
Append: [Reasoning Beyond Language: A Comprehensive Survey on Latent Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.16782)
Append: [Accidental Misalignment: Fine-Tuning Language Models Induces Unexpected Vulnerability](https://arxiv.org/abs/2505.16789)
Append: [Learning Beyond Limits: Multitask Learning and Synthetic Data for Low-Resource Canonical Morpheme Segmentation](https://arxiv.org/abs/2505.16800)
Append: [Two-way Evidence self-Alignment based Dual-Gated Reasoning Enhancement](https://arxiv.org/abs/2505.16806)
Append: [Does Synthetic Data Help Named Entity Recognition for Low-Resource Languages?](https://arxiv.org/abs/2505.16814)
Append: [Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs](https://arxiv.org/abs/2505.16831)
Append: [SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning Trajectory Synthesis](https://arxiv.org/abs/2505.16834)
Append: [R1-Compress: Long Chain-of-Thought Compression via Chunk Compression and Search](https://arxiv.org/abs/2505.16838)
Append: [Understanding and Analyzing Inappropriately Targeting Language in Online Discourse: A Comparative Annotation Study](https://arxiv.org/abs/2505.16847)
Append: [Nested Named Entity Recognition as Single-Pass Sequence Labeling](https://arxiv.org/abs/2505.16855)
Append: [Comparative analysis of subword tokenization approaches for Indian languages](https://arxiv.org/abs/2505.16868)
Append: [MPO: Multilingual Safety Alignment via Reward Gap Optimization](https://arxiv.org/abs/2505.16869)
Append: [CASTILLO: Characterizing Response Length Distributions of Large Language Models](https://arxiv.org/abs/2505.16881)
Append: [Shadows in the Attention: Contextual Perturbation and Representation Drift in the Dynamics of Hallucination in LLMs](https://arxiv.org/abs/2505.16894)
Append: [Power-Law Decay Loss for Large Language Model Finetuning: Focusing on Information Sparsity to Enhance Generation Quality](https://arxiv.org/abs/2505.16900)
Append: [UNCLE: Uncertainty Expressions in Long-Form Generation](https://arxiv.org/abs/2505.16922)
Append: [Latent Principle Discovery for Language Model Self-Improvement](https://arxiv.org/abs/2505.16927)
Append: [PIIvot: A Lightweight NLP Anonymization Framework for Question-Anchored Tutoring Dialogues](https://arxiv.org/abs/2505.16931)
Append: [In-Context Watermarks for Large Language Models](https://arxiv.org/abs/2505.16934)
Append: [On Multilingual Encoder Language Model Compression for Low-Resource Languages](https://arxiv.org/abs/2505.16956)
Append: [BP-Seg: A graphical model approach to unsupervised and non-contiguous text segmentation using belief propagation](https://arxiv.org/abs/2505.16965)
Append: [From Tens of Hours to Tens of Thousands: Scaling Back-Translation for Speech Recognition](https://arxiv.org/abs/2505.16972)
Append: [VeriFastScore: Speeding up long-form factuality evaluation](https://arxiv.org/abs/2505.16973)
Append: [LLM as Effective Streaming Processor: Bridging Streaming-Batch Mismatches with Group Position Encoding](https://arxiv.org/abs/2505.16983)
Append: [T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic Planning](https://arxiv.org/abs/2505.16986)
Append: [MASLab: A Unified and Comprehensive Codebase for LLM-based Multi-Agent Systems](https://arxiv.org/abs/2505.16988)
Append: [DecoupledESC: Enhancing Emotional Support Generation via Strategy-Response Decoupled Preference Optimization](https://arxiv.org/abs/2505.16995)
Append: [Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?](https://arxiv.org/abs/2505.16998)
Append: [R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs via Reinforcement Learning](https://arxiv.org/abs/2505.17005)
Append: [InfoDeepSeek: Benchmarking Agentic Information Seeking for Retrieval-Augmented Generation](https://arxiv.org/abs/2505.15872)
Append: [Highlighting What Matters: Promptable Embeddings for Attribute-Focused Image Retrieval](https://arxiv.org/abs/2505.15877)
Append: [GRIT: Teaching MLLMs to Think with Images](https://arxiv.org/abs/2505.15879)
Append: [ViQAgent: Zero-Shot Video Question Answering via Agent with Open-Vocabulary Grounding Validation](https://arxiv.org/abs/2505.15928)
Append: [MAPS: A Multilingual Benchmark for Global Agent Performance and Security](https://arxiv.org/abs/2505.15935)
Append: [Towards Holistic Evaluation of Large Audio-Language Models: A Comprehensive Survey](https://arxiv.org/abs/2505.15957)
Append: [OViP: Online Vision-Language Preference Learning](https://arxiv.org/abs/2505.15963)
Append: [Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning](https://arxiv.org/abs/2505.15966)
Append: [Interpretability Illusions with Sparse Autoencoders: Evaluating Robustness of Concept Representations](https://arxiv.org/abs/2505.16004)
Append: [Causal LLM Routing: End-to-End Regret Minimization from Observational Data](https://arxiv.org/abs/2505.16037)
Append: [Aug2Search: Enhancing Facebook Marketplace Search with LLM-Generated Synthetic Data Augmentation](https://arxiv.org/abs/2505.16065)
Append: [Merge to Mix: Mixing Datasets via Model Merging](https://arxiv.org/abs/2505.16066)
Append: [Optimizing LLM-Based Multi-Agent System with Textual Feedback: A Case Study on Software Development](https://arxiv.org/abs/2505.16086)
Append: [Can AI Read Between The Lines? Benchmarking LLMs On Financial Nuance](https://arxiv.org/abs/2505.16090)
Append: [A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization](https://arxiv.org/abs/2505.16094)
Append: [BioDSA-1K: Benchmarking Data Science Agents for Biomedical Research](https://arxiv.org/abs/2505.16100)
Append: [Steering LVLMs via Sparse Autoencoder for Hallucination Mitigation](https://arxiv.org/abs/2505.16146)
Append: [NAN: A Training-Free Solution to Coefficient Estimation in Model Merging](https://arxiv.org/abs/2505.16148)
Append: [When VLMs Meet Image Classification: Test Sets Renovation via Missing Label Identification](https://arxiv.org/abs/2505.16149)
Append: [Dynamic Sampling that Adapts: Iterative DPO for Self-Aware Mathematical Reasoning](https://arxiv.org/abs/2505.16176)
Append: [Redemption Score: An Evaluation Framework to Rank Image Captions While Redeeming Image Semantics and Language Pragmatics](https://arxiv.org/abs/2505.16180)
Append: [SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning](https://arxiv.org/abs/2505.16186)
Append: [NQKV: A KV Cache Quantization Scheme Based on Normal Distribution Characteristics](https://arxiv.org/abs/2505.16210)
Append: [AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large Language Models](https://arxiv.org/abs/2505.16211)
Append: [Meta-PerSER: Few-Shot Listener Personalized Speech Emotion Recognition via Meta-learning](https://arxiv.org/abs/2505.16220)
Append: [All You Need is "Leet": Evading Hate-speech Detection AI](https://arxiv.org/abs/2505.16263)
Append: [How do Scaling Laws Apply to Knowledge Graph Engineering Tasks? The Impact of Model Size on Large Language Model Performance](https://arxiv.org/abs/2505.16276)
Append: [Incentivizing Dual Process Thinking for Efficient Large Language Model Reasoning](https://arxiv.org/abs/2505.16315)
Append: [AdaSTaR: Adaptive Data Sampling for Training Self-Taught Reasoners](https://arxiv.org/abs/2505.16322)
Append: [AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning](https://arxiv.org/abs/2505.16400)
Append: [Benchmarking Retrieval-Augmented Multimomal Generation for Document Question Answering](https://arxiv.org/abs/2505.16470)
Append: [DuFFin: A Dual-Level Fingerprinting Framework for LLMs IP Protection](https://arxiv.org/abs/2505.16530)
Append: [CTRAP: Embedding Collapse Trap to Safeguard Large Language Models from Harmful Fine-Tuning](https://arxiv.org/abs/2505.16559)
Append: [Grounding Chest X-Ray Visual Question Answering with Generated Radiology Reports](https://arxiv.org/abs/2505.16624)
Append: [MiLQ: Benchmarking IR Models for Bilingual Web Search with Mixed Language Queries](https://arxiv.org/abs/2505.16631)
Append: [R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO](https://arxiv.org/abs/2505.16673)
Append: [SPaRC: A Spatial Pathfinding Reasoning Challenge](https://arxiv.org/abs/2505.16686)
Append: [Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing Optimization](https://arxiv.org/abs/2505.16737)
Append: [KTAE: A Model-Free Algorithm to Key-Tokens Advantage Estimation in Mathematical Reasoning](https://arxiv.org/abs/2505.16826)
Append: [From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework for Pedagogical Visualization](https://arxiv.org/abs/2505.16832)
Append: [ATR-Bench: A Federated Learning Benchmark for Adaptation, Trust, and Reasoning](https://arxiv.org/abs/2505.16850)
Append: [Don't "Overthink" Passage Reranking: Is Reasoning Truly Necessary?](https://arxiv.org/abs/2505.16886)
Append: [CAIN: Hijacking LLM-Humans Conversations via a Two-Stage Malicious System Prompt Generation and Refining Framework](https://arxiv.org/abs/2505.16888)
Append: [The Polar Express: Optimal Matrix Sign Methods and Their Application to the Muon Algorithm](https://arxiv.org/abs/2505.16932)
Append: [LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning](https://arxiv.org/abs/2505.16933)
Append: [NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop System from Hypothesis to Verification](https://arxiv.org/abs/2505.16938)
Append: [AGENTIF: Benchmarking Instruction Following of Large Language Models in Agentic Scenarios](https://arxiv.org/abs/2505.16944)
Append: [MedFrameQA: A Multi-Image Medical VQA Benchmark for Clinical Reasoning](https://arxiv.org/abs/2505.16964)
Append: [Fixing Data That Hurts Performance: Cascading LLMs to Relabel Hard Negatives for Robust Information Retrieval](https://arxiv.org/abs/2505.16967)
Append: [CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark](https://arxiv.org/abs/2505.16968)
Append: [SWE-Dev: Evaluating and Training Autonomous Feature-Driven Software Development](https://arxiv.org/abs/2505.16975)
Append: [UFT: Unifying Supervised and Reinforcement Fine-Tuning](https://arxiv.org/abs/2505.16984)
Append: [$\text{R}^2\text{ec}$: Towards Large Recommender Models with Reasoning](https://arxiv.org/abs/2505.16994)
Append: [X-MAS: Towards Building Multi-Agent Systems with Heterogeneous LLMs](https://arxiv.org/abs/2505.16997)
Append: [Multi-SpatialMLLM: Multi-Frame Spatial Understanding with Multi-Modal Large Language Models](https://arxiv.org/abs/2505.17015)
Append: [Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO](https://arxiv.org/abs/2505.17017)
Append: [GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation with Reinforcement Learning](https://arxiv.org/abs/2505.17022)
Append: [Language Models are Universal Embedders](https://arxiv.org/abs/2310.08232)
Append: [Large Language Models are Miscalibrated In-Context Learners](https://arxiv.org/abs/2312.13772)
Append: [EntGPT: Entity Linking with Generative Large Language Models](https://arxiv.org/abs/2402.06738)
Append: [Red-Teaming for Inducing Societal Bias in Large Language Models](https://arxiv.org/abs/2405.04756)
Append: [BlockPruner: Fine-grained Pruning for Large Language Models](https://arxiv.org/abs/2406.10594)
Append: [Determination of language families using deep learning](https://arxiv.org/abs/2409.02393)
Append: [MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark](https://arxiv.org/abs/2409.02813)
Append: [Normal forms in Virus Machines](https://arxiv.org/abs/2409.03327)
Append: [LangSAMP: Language-Script Aware Multilingual Pretraining](https://arxiv.org/abs/2409.18199)
Append: [GLEE: A Unified Framework and Benchmark for Language-based Economic Environments](https://arxiv.org/abs/2410.05254)
Append: [Can Knowledge Graphs Make Large Language Models More Trustworthy? An Empirical Study Over Open-ended Question Answering](https://arxiv.org/abs/2410.08085)
Append: [Keys to Robust Edits: from Theoretical Insights to Practical Advances](https://arxiv.org/abs/2410.09338)
Append: [A Unified Approach to Routing and Cascading for LLMs](https://arxiv.org/abs/2410.10347)
Append: [Do Robot Snakes Dream like Electric Sheep? Investigating the Effects of Architectural Inductive Biases on Hallucination](https://arxiv.org/abs/2410.17477)
Append: [Understanding Synthetic Context Extension via Retrieval Heads](https://arxiv.org/abs/2410.22316)
Append: [AAAR-1.0: Assessing AI's Potential to Assist Research](https://arxiv.org/abs/2410.22394)
Append: [Graph-based Confidence Calibration for Large Language Models](https://arxiv.org/abs/2411.02454)
Append: [Prompt-Guided Internal States for Hallucination Detection of Large Language Models](https://arxiv.org/abs/2411.04847)
Append: [Evaluating Automated Radiology Report Quality through Fine-Grained Phrasal Grounding of Clinical Findings](https://arxiv.org/abs/2412.01031)
Append: [Evaluating LLM-based Approaches to Legal Citation Prediction: Domain-specific Pre-training, Fine-tuning, or RAG? A Benchmark and an Australian Law Case Study](https://arxiv.org/abs/2412.06272)
Append: [My Words Imply Your Opinion: Reader Agent-based Propagation Enhancement for Personalized Implicit Emotion Analysis](https://arxiv.org/abs/2412.07367)
Append: [LITA: An Efficient LLM-assisted Iterative Topic Augmentation Framework](https://arxiv.org/abs/2412.12459)
Append: [DocFusion: A Unified Framework for Document Parsing Tasks](https://arxiv.org/abs/2412.12505)
Append: [Divide and Conquer: A Hybrid Strategy Defeats Multimodal Large Language Models](https://arxiv.org/abs/2412.16555)
Append: [BenCzechMark : A Czech-centric Multitask and Multimetric Benchmark for Large Language Models with Duel Scoring Mechanism](https://arxiv.org/abs/2412.17933)
Append: [Diverse Preference Optimization](https://arxiv.org/abs/2501.18101)
Append: [ReFoRCE: A Text-to-SQL Agent with Self-Refinement, Consensus Enforcement, and Column Exploration](https://arxiv.org/abs/2502.00675)
Append: [FIRE: Flexible Integration of Data Quality Ratings for Effective Pre-Training](https://arxiv.org/abs/2502.00761)
Append: [Diversity as a Reward: Fine-Tuning LLMs on a Mixture of Domain-Undetermined Data](https://arxiv.org/abs/2502.04380)
Append: [Is a Peeled Apple Still Red? Evaluating LLMs' Ability for Conceptual Combination with Property Type](https://arxiv.org/abs/2502.06086)
Append: [LCIRC: A Recurrent Compression Approach for Efficient Long-form Context and Query Dependent Modeling in LLMs](https://arxiv.org/abs/2502.06139)
Append: [C-3PO: Compact Plug-and-Play Proxy Optimization to Achieve Human-like Retrieval-Augmented Generation](https://arxiv.org/abs/2502.06205)
Append: [No Need for Explanations: LLMs can implicitly learn from mistakes in-context](https://arxiv.org/abs/2502.08550)
Append: [SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models](https://arxiv.org/abs/2502.09604)
Append: [The Hidden Dimensions of LLM Alignment: A Multi-Dimensional Analysis of Orthogonal Safety Directions](https://arxiv.org/abs/2502.09674)
Append: [GRIFFIN: Effective Token Alignment for Faster Speculative Decoding](https://arxiv.org/abs/2502.11018)
Append: [SAFE-SQL: Self-Augmented In-Context Learning with Fine-grained Example Selection for Text-to-SQL](https://arxiv.org/abs/2502.11438)
Append: [M-ABSA: A Multilingual Dataset for Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2502.11824)
Append: [Whose story is it? Personalizing story generation by inferring author styles](https://arxiv.org/abs/2502.13028)
Append: [Transferring Textual Preferences to Vision-Language Understanding through Model Merging](https://arxiv.org/abs/2502.13487)
Append: [CoT-ICL Lab: A Synthetic Framework for Studying Chain-of-Thought Learning from In-Context Demonstrations](https://arxiv.org/abs/2502.15132)
Append: [KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse](https://arxiv.org/abs/2502.16002)
Append: [FoREST: Frame of Reference Evaluation in Spatial Reasoning Tasks](https://arxiv.org/abs/2502.17775)
Append: [Towards Better Understanding of Program-of-Thought Reasoning in Cross-Lingual and Multilingual Environments](https://arxiv.org/abs/2502.17956)
Append: [Collab-Overcooked: Benchmarking and Evaluating Large Language Models as Collaborative Agents](https://arxiv.org/abs/2502.20073)
Append: [HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization](https://arxiv.org/abs/2503.04598)
Append: [ReviewAgents: Bridging the Gap Between Human and AI-Generated Paper Reviews](https://arxiv.org/abs/2503.08506)
Append: [Uncertainty Distillation: Teaching Language Models to Express Semantic Confidence](https://arxiv.org/abs/2503.14749)
Append: [From 1,000,000 Users to Every User: Scaling Up Personalized Preference for User-level Alignment](https://arxiv.org/abs/2503.15463)
Append: [Through the LLM Looking Glass: A Socratic Probing of Donkeys, Elephants, and Markets](https://arxiv.org/abs/2503.16674)
Append: [Praxis-VLM: Vision-Grounded Decision Making via Text-Driven Reinforcement Learning](https://arxiv.org/abs/2503.16965)
Append: [FastCuRL: Curriculum Reinforcement Learning with Stage-wise Context Scaling for Efficient Training R1-like Reasoning Models](https://arxiv.org/abs/2503.17287)
Append: [DomainCQA: Crafting Expert-Level QA from Domain-Specific Charts](https://arxiv.org/abs/2503.19498)
Append: [Universal Cross-Tokenizer Distillation via Approximate Likelihood Matching](https://arxiv.org/abs/2503.20083)
Append: [TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken Language Modeling](https://arxiv.org/abs/2504.07053)
Append: [Improving Multilingual Capabilities with Cultural and Local Knowledge in Large Language Models While Enhancing Native Performance](https://arxiv.org/abs/2504.09753)
Append: [Hallucination Detection in LLMs with Topological Divergence on Attention Graphs](https://arxiv.org/abs/2504.10063)
Append: [Robust and Fine-Grained Detection of AI Generated Texts](https://arxiv.org/abs/2504.11952)
Append: [SMARTe: Slot-based Method for Accountable Relational Triple extraction](https://arxiv.org/abs/2504.12816)
Append: [Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as Test-Time Scaling Evaluators](https://arxiv.org/abs/2504.15253)
Append: [TTRL: Test-Time Reinforcement Learning](https://arxiv.org/abs/2504.16084)
Append: [APE-Bench I: Towards File-level Automated Proof Engineering of Formal Math Libraries](https://arxiv.org/abs/2504.19110)
Append: [How Real Are Synthetic Therapy Conversations? Evaluating Fidelity in Prolonged Exposure Dialogues](https://arxiv.org/abs/2504.21800)
Append: [GDI-Bench: A Benchmark for General Document Intelligence with Vision and Reasoning Decoupling](https://arxiv.org/abs/2505.00063)
Append: [Identifying Legal Holdings with LLMs: A Systematic Study of Performance, Scale, and Memorization](https://arxiv.org/abs/2505.02172)
Append: [Say It Another Way: Auditing LLMs with a User-Grounded Automated Paraphrasing Framework](https://arxiv.org/abs/2505.03563)
Append: [LiTransProQA: an LLM-based Literary Translation evaluation metric with Professional Question Answering](https://arxiv.org/abs/2505.05423)
Append: [Semantic Aware Linear Transfer by Recycling Pre-trained Language Models for Cross-lingual Transfer](https://arxiv.org/abs/2505.10945)
Append: [Illusion or Algorithm? Investigating Memorization, Emergence, and Symbolic Processing in In-Context Learning](https://arxiv.org/abs/2505.11004)
Append: [LABO: Towards Learning Optimal Label Regularization via Bi-level Optimization](https://arxiv.org/abs/2305.04971)
Append: [CodeMind: Evaluating Large Language Models for Code Reasoning](https://arxiv.org/abs/2402.09664)
Append: [FiDeLiS: Faithful Reasoning in Large Language Model for Knowledge Graph Question Answering](https://arxiv.org/abs/2405.13873)
Append: [How Well Can a Long Sequence Model Model Long Sequences? Comparing Architechtural Inductive Biases on Long-Context Abilities](https://arxiv.org/abs/2407.08112)
Append: [More Text, Less Point: Towards 3D Data-Efficient Point-Language Understanding](https://arxiv.org/abs/2408.15966)
Append: [Breaking Information Cocoons: A Hyperbolic Graph-LLM Framework for Exploration and Exploitation in Recommender Systems](https://arxiv.org/abs/2411.13865)
Append: [Extractive Structures Learned in Pretraining Enable Generalization on Finetuned Facts](https://arxiv.org/abs/2412.04614)
Append: [Code Readability in the Age of Large Language Models: An Industrial Case Study from Atlassian](https://arxiv.org/abs/2501.11264)
Append: [To Code or not to Code? Adaptive Tool Integration for Math Language Models via Expectation-Maximization](https://arxiv.org/abs/2502.00691)
Append: [Slamming: Training a Speech Language Model on One GPU in a Day](https://arxiv.org/abs/2502.15814)
Append: [Similarity-Distance-Magnitude Universal Verification](https://arxiv.org/abs/2502.20167)
Append: [Retrieval-Augmented Perception: High-Resolution Image Perception Meets Visual RAG](https://arxiv.org/abs/2503.01222)
Append: [Steer LLM Latents for Hallucination Detection](https://arxiv.org/abs/2503.01917)
Append: [Transformers for molecular property prediction: Domain adaptation efficiently improves performance](https://arxiv.org/abs/2503.03360)
Append: [Capacity-Aware Inference: Mitigating the Straggler Effect in Mixture of Experts](https://arxiv.org/abs/2503.05066)
Append: [MindGYM: What Matters in Question Synthesis for Thinking-Centric Fine-Tuning?](https://arxiv.org/abs/2503.09499)
Append: [Optimizing Case-Based Reasoning System for Functional Test Script Generation with Large Language Models](https://arxiv.org/abs/2503.20576)
append_entries: 271
Finish: 2025-05-23 04:35:33.824028
------------------------------------------------------
Started: 2025-05-23 06:24:29.000999
Existing_entries: 1271
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1125
Summarized using GPT-3.5-turbo
Append: [SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models](https://arxiv.org/abs/2502.12464)
Token length: 1442
Summarized using GPT-3.5-turbo
Append: [Model Performance-Guided Evaluation Data Selection for Effective Prompt Optimization](https://arxiv.org/abs/2505.10736)
Token length: 1434
Summarized using GPT-3.5-turbo
Append: [ToolSpectrum : Towards Personalized Tool Utilization for Large Language Models](https://arxiv.org/abs/2505.13176)
Token length: 873
Summarized using GPT-3.5-turbo
Append: [Vague Knowledge: Evidence from Analyst Reports](https://arxiv.org/abs/2505.12269)
append_entries: 4
Finish: 2025-05-23 06:24:37.080464
------------------------------------------------------
Started: 2025-05-23 08:22:14.266211
Existing_entries: 1004
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-23 08:22:14.846135
------------------------------------------------------
Started: 2025-05-23 10:17:46.897739
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-23 10:17:47.478582
------------------------------------------------------
Started: 2025-05-23 12:33:10.922271
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-23 12:33:11.590726
------------------------------------------------------
Started: 2025-05-23 14:15:59.605118
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-23 14:16:00.225100
------------------------------------------------------
Started: 2025-05-23 16:20:00.026904
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-23 16:20:00.669140
------------------------------------------------------
Started: 2025-05-23 18:21:27.355076
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-23 18:21:27.938311
------------------------------------------------------
Started: 2025-05-23 20:18:20.074644
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-23 20:18:20.702629
------------------------------------------------------
Started: 2025-05-23 22:15:46.373433
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-23 22:15:46.946569
------------------------------------------------------
Started: 2025-05-24 01:16:09.705148
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-24 01:16:10.289553
------------------------------------------------------
Started: 2025-05-24 03:03:51.247139
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-24 03:03:51.916913
------------------------------------------------------
Started: 2025-05-24 04:18:59.578741
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-24 04:18:59.643109
------------------------------------------------------
Started: 2025-05-24 06:21:09.447247
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-24 06:21:09.524598
------------------------------------------------------
Started: 2025-05-24 08:19:02.227821
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-24 08:19:02.345400
------------------------------------------------------
Started: 2025-05-24 10:15:39.260693
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-24 10:15:39.320388
------------------------------------------------------
Started: 2025-05-24 12:30:09.592412
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-24 12:30:09.657835
------------------------------------------------------
Started: 2025-05-24 14:13:47.071589
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-24 14:13:47.133103
------------------------------------------------------
Started: 2025-05-24 16:18:12.897787
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-24 16:18:12.950882
------------------------------------------------------
Started: 2025-05-24 18:20:31.315128
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-24 18:20:31.369411
------------------------------------------------------
Started: 2025-05-24 20:16:41.143313
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-24 20:16:41.196972
------------------------------------------------------
Started: 2025-05-24 22:14:41.105882
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-24 22:14:41.164455
------------------------------------------------------
Started: 2025-05-25 01:25:47.440750
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-25 01:25:47.521681
------------------------------------------------------
Started: 2025-05-25 03:17:43.175539
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-25 03:17:43.244043
------------------------------------------------------
Started: 2025-05-25 04:23:13.399152
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-25 04:23:13.474157
------------------------------------------------------
Started: 2025-05-25 06:21:29.464935
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-25 06:21:29.535799
------------------------------------------------------
Started: 2025-05-25 08:19:15.733380
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-25 08:19:15.789991
------------------------------------------------------
Started: 2025-05-25 10:16:04.479884
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-25 10:16:04.539247
------------------------------------------------------
Started: 2025-05-25 12:30:25.520204
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-25 12:30:25.591784
------------------------------------------------------
Started: 2025-05-25 14:13:56.524611
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-25 14:13:56.599616
------------------------------------------------------
Started: 2025-05-25 16:18:30.668141
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-25 16:18:30.719751
------------------------------------------------------
Started: 2025-05-25 18:20:29.639196
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-25 18:20:29.694466
------------------------------------------------------
Started: 2025-05-25 20:16:44.848153
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-25 20:16:44.963651
------------------------------------------------------
Started: 2025-05-25 22:15:01.068796
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-25 22:15:01.127998
------------------------------------------------------
Started: 2025-05-26 01:21:06.135810
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-26 01:21:06.218666
------------------------------------------------------
Started: 2025-05-26 03:14:34.396552
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-26 03:14:34.457337
------------------------------------------------------
Started: 2025-05-26 04:26:27.806251
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1277
Summarized using GPT-3.5-turbo
Append: [Prompt Engineering: How Prompt Vocabulary affects Domain Knowledge](https://arxiv.org/abs/2505.17037)
Token length: 1277
Summarized using GPT-3.5-turbo
Append: [Signals from the Floods: AI-Driven Disaster Analysis through Multi-Source Data Fusion](https://arxiv.org/abs/2505.17038)
Token length: 1106
Summarized using GPT-3.5-turbo
Append: [A new classification system of beer categories and styles based on large-scale data mining and self-organizing maps of beer recipes](https://arxiv.org/abs/2505.17039)
Token length: 1047
Summarized using GPT-3.5-turbo
Append: [VLM-KG: Multimodal Radiology Knowledge Graph Generation](https://arxiv.org/abs/2505.17042)
Token length: 1214
Summarized using GPT-3.5-turbo
Append: [QRA++: Quantified Reproducibility Assessment for Common Types of Results in Natural Language Processing](https://arxiv.org/abs/2505.17043)
Token length: 795
Summarized using GPT-3.5-turbo
Append: [Assessing GPT's Bias Towards Stigmatized Social Groups: An Intersectional Case Study on Nationality Prejudice and Psychophobia](https://arxiv.org/abs/2505.17045)
Token length: 1595
Summarized using GPT-3.5-turbo
Append: [Assessing the Quality of AI-Generated Clinical Notes: A Validated Evaluation of a Large Language Model Scribe](https://arxiv.org/abs/2505.17047)
Token length: 1506
Summarized using GPT-3.5-turbo
Append: [Words That Unite The World: A Unified Framework for Deciphering Central Bank Communications Globally](https://arxiv.org/abs/2505.17048)
Token length: 1803
Summarized using GPT-3.5-turbo
Append: [Gender and Positional Biases in LLM-Based Hiring Decisions: Evidence from Comparative CV/R\'esum\'e Evaluations](https://arxiv.org/abs/2505.17049)
Token length: 1715
Summarized using GPT-3.5-turbo
Append: [Towards Robust Evaluation of STEM Education: Leveraging MLLMs in Project-Based Learning](https://arxiv.org/abs/2505.17050)
Token length: 1269
Summarized using GPT-3.5-turbo
Append: [Embedding-to-Prefix: Parameter-Efficient Personalization for Pre-Trained Large Language Models](https://arxiv.org/abs/2505.17051)
Token length: 904
Summarized using GPT-3.5-turbo
Append: [SpecEdge: Scalable Edge-Assisted Serving Framework for Interactive LLMs](https://arxiv.org/abs/2505.17052)
Token length: 1797
Summarized using GPT-3.5-turbo
Append: [Social preferences with unstable interactive reasoning: Large language models in economic trust games](https://arxiv.org/abs/2505.17053)
Token length: 1757
Summarized using GPT-3.5-turbo
Append: [METHOD: Modular Efficient Transformer for Health Outcome Discovery](https://arxiv.org/abs/2505.17054)
Token length: 1210
Summarized using GPT-3.5-turbo
Append: [Enhancing Mathematics Learning for Hard-of-Hearing Students Through Real-Time Palestinian Sign Language Recognition: A New Dataset](https://arxiv.org/abs/2505.17055)
Token length: 1405
Summarized using GPT-3.5-turbo
Append: [Are LLMs Ready for English Standardized Tests? A Benchmarking and Elicitation Perspective](https://arxiv.org/abs/2505.17056)
Token length: 1247
Summarized using GPT-3.5-turbo
Append: [DO-RAG: A Domain-Specific QA Framework Using Knowledge Graph-Enhanced Retrieval-Augmented Generation](https://arxiv.org/abs/2505.17058)
Token length: 948
Summarized using GPT-3.5-turbo
Append: [Medalyze: Lightweight Medical Report Summarization Application Using FLAN-T5-Large](https://arxiv.org/abs/2505.17059)
Token length: 1758
Summarized using GPT-3.5-turbo
Append: [SALMONN-omni: A Standalone Speech LLM without Codec Injection for Full-duplex Conversation](https://arxiv.org/abs/2505.17060)
Token length: 1184
Summarized using GPT-3.5-turbo
Append: [Mixture of Decoding: An Attention-Inspired Adaptive Decoding Strategy to Mitigate Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2505.17061)
Token length: 1365
Summarized using GPT-3.5-turbo
Append: [Synthetic Data RL: Task Definition Is All You Need](https://arxiv.org/abs/2505.17063)
Token length: 1590
Summarized using GPT-3.5-turbo
Append: [Decoding Rarity: Large Language Models in the Diagnosis of Rare Diseases](https://arxiv.org/abs/2505.17065)
Token length: 1278
Summarized using GPT-3.5-turbo
Append: [Unveil Multi-Picture Descriptions for Multilingual Mild Cognitive Impairment Detection via Contrastive Learning](https://arxiv.org/abs/2505.17067)
Token length: 878
Summarized using GPT-3.5-turbo
Append: [Predictively Combatting Toxicity in Health-related Online Discussions through Machine Learning](https://arxiv.org/abs/2505.17068)
Token length: 1047
Summarized using GPT-3.5-turbo
Append: [Improving endpoint detection in end-to-end streaming ASR for conversational speech](https://arxiv.org/abs/2505.17070)
Token length: 1126
Summarized using GPT-3.5-turbo
Append: [What's in a prompt? Language models encode literary style in prompt embeddings](https://arxiv.org/abs/2505.17071)
Token length: 1253
Summarized using GPT-3.5-turbo
Append: [Mechanistic Interpretability of GPT-like Models on Summarization Tasks](https://arxiv.org/abs/2505.17073)
Token length: 1487
Summarized using GPT-3.5-turbo
Append: [Semi-Clairvoyant Scheduling of Speculative Decoding Requests to Minimize LLM Inference Latency](https://arxiv.org/abs/2505.17074)
Token length: 1201
Summarized using GPT-3.5-turbo
Append: [Development and Validation of Engagement and Rapport Scales for Evaluating User Experience in Multimodal Dialogue Systems](https://arxiv.org/abs/2505.17075)
Token length: 1006
Summarized using GPT-3.5-turbo
Append: [Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English](https://arxiv.org/abs/2505.17076)
Token length: 966
Summarized using GPT-3.5-turbo
Append: [GloSS over Toxicity: Understanding and Mitigating Toxicity in LLMs via Global Toxic Subspace](https://arxiv.org/abs/2505.17078)
Token length: 1650
Summarized using GPT-3.5-turbo
Append: [Not Minds, but Signs: Reframing LLMs through Semiotics](https://arxiv.org/abs/2505.17080)
Token length: 1431
Summarized using GPT-3.5-turbo
Append: [GemMaroc: Unlocking Darija Proficiency in LLMs with Minimal Data](https://arxiv.org/abs/2505.17082)
Token length: 820
Summarized using GPT-3.5-turbo
Append: [Scale-invariant Attention](https://arxiv.org/abs/2505.17083)
Token length: 1337
Summarized using GPT-3.5-turbo
Append: [Reinforcing Question Answering Agents with Minimalist Policy Gradient Optimization](https://arxiv.org/abs/2505.17086)
Token length: 1284
Summarized using GPT-3.5-turbo
Append: [Informatics for Food Processing](https://arxiv.org/abs/2505.17087)
Token length: 1330
Summarized using GPT-3.5-turbo
Append: [Trust Me, I Can Handle It: Self-Generated Adversarial Scenario Extrapolation for Robust Language Models](https://arxiv.org/abs/2505.17089)
Token length: 1036
Summarized using GPT-3.5-turbo
Append: [Large Language Models Implicitly Learn to See and Hear Just By Reading](https://arxiv.org/abs/2505.17091)
Token length: 1449
Summarized using GPT-3.5-turbo
Append: [Are LLMs reliable? An exploration of the reliability of large language models in clinical note generation](https://arxiv.org/abs/2505.17095)
Token length: 1240
Summarized using GPT-3.5-turbo
Append: [TACO: Enhancing Multimodal In-context Learning via Task Mapping-Guided Sequence Configuration](https://arxiv.org/abs/2505.17098)
Token length: 1455
Summarized using GPT-3.5-turbo
Append: [Learning Interpretable Representations Leads to Semantically Faithful EEG-to-Text Generation](https://arxiv.org/abs/2505.17099)
Token length: 1751
Summarized using GPT-3.5-turbo
Append: [Any Large Language Model Can Be a Reliable Judge: Debiasing with a Reasoning-based Bias Detector](https://arxiv.org/abs/2505.17100)
Token length: 1347
Summarized using GPT-3.5-turbo
Append: [An approach to identify the most semantically informative deep representations of text and images](https://arxiv.org/abs/2505.17101)
Token length: 1054
Summarized using GPT-3.5-turbo
Append: [BanglaByT5: Byte-Level Modelling for Bangla](https://arxiv.org/abs/2505.17102)
Token length: 1063
Summarized using GPT-3.5-turbo
Append: [Forging Time Series with Language: A Large Language Model Approach to Synthetic Data Generation](https://arxiv.org/abs/2505.17103)
Token length: 1614
Summarized using GPT-3.5-turbo
Append: [P2P: Automated Paper-to-Poster Generation and Fine-Grained Benchmark](https://arxiv.org/abs/2505.17104)
Token length: 1398
Summarized using GPT-3.5-turbo
Append: [RRTL: Red Teaming Reasoning Large Language Models in Tool Learning](https://arxiv.org/abs/2505.17106)
Token length: 1296
Summarized using GPT-3.5-turbo
Append: [Multi-Modality Expansion and Retention for LLMs through Parameter Merging and Decoupling](https://arxiv.org/abs/2505.17110)
Token length: 1211
Summarized using GPT-3.5-turbo
Append: [Cultural Value Alignment in Large Language Models: A Prompt-based Analysis of Schwartz Values in Gemini, ChatGPT, and DeepSeek](https://arxiv.org/abs/2505.17112)
Token length: 1570
Summarized using GPT-3.5-turbo
Append: [RAVEN: Query-Guided Representation Alignment for Question Answering over Audio, Video, Embedded Sensors, and Natural Language](https://arxiv.org/abs/2505.17114)
Append: [Comparative Evaluation of Prompting and Fine-Tuning for Applying Large Language Models to Grid-Structured Geospatial Data](https://arxiv.org/abs/2505.17116)
Append: [From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning](https://arxiv.org/abs/2505.17117)
Append: [After Retrieval, Before Generation: Enhancing the Trustworthiness of Large Language Models in RAG](https://arxiv.org/abs/2505.17118)
Append: [Systematic Evaluation of Machine-Generated Reasoning and PHQ-9 Labeling for Depression Detection Using Large Language Models](https://arxiv.org/abs/2505.17119)
Append: [Self-Interpretability: LLMs Can Describe Complex Internal Processes that Drive Their Decisions, and Improve with Training](https://arxiv.org/abs/2505.17120)
Append: [NeSyGeo: A Neuro-Symbolic Framework for Multimodal Geometric Reasoning Data Generation](https://arxiv.org/abs/2505.17121)
Append: [Shallow Preference Signals: Large Language Model Aligns Even Better with Truncated Data?](https://arxiv.org/abs/2505.17122)
Append: [MTR-Bench: A Comprehensive Benchmark for Multi-Turn Reasoning Evaluation](https://arxiv.org/abs/2505.17123)
Append: [Conformal Language Model Reasoning with Coherent Factuality](https://arxiv.org/abs/2505.17126)
Append: [Relative Bias: A Comparative Framework for Quantifying Bias in LLMs](https://arxiv.org/abs/2505.17131)
Append: [LongMagpie: A Self-synthesis Method for Generating Large-scale Long-context Instructions](https://arxiv.org/abs/2505.17134)
Append: [When can isotropy help adapt LLMs' next word prediction to numerical domains?](https://arxiv.org/abs/2505.17135)
Append: [Foundation Models for Geospatial Reasoning: Assessing Capabilities of Large Language Models in Understanding Geometries and Topological Spatial Relations](https://arxiv.org/abs/2505.17136)
Append: [Cog-TiPRO: Iterative Prompt Refinement with LLMs to Detect Cognitive Decline via Longitudinal Voice Assistant Commands](https://arxiv.org/abs/2505.17137)
Append: [EarthSE: A Benchmark Evaluating Earth Scientific Exploration Capability for Large Language Models](https://arxiv.org/abs/2505.17139)
Append: [Data Doping or True Intelligence? Evaluating the Transferability of Injected Knowledge in LLMs](https://arxiv.org/abs/2505.17140)
Append: [MDIT-Bench: Evaluating the Dual-Implicit Toxicity in Large Multimodal Models](https://arxiv.org/abs/2505.17144)
Append: [Large Language Models for Predictive Analysis: How Far Are They?](https://arxiv.org/abs/2505.17149)
Append: [Bayesian Optimization for Enhanced Language Models: Optimizing Acquisition Functions](https://arxiv.org/abs/2505.17151)
Append: [Amplify Adjacent Token Differences: Enhancing Long Chain-of-Thought Reasoning with Shift-FFN](https://arxiv.org/abs/2505.17153)
Append: [PersonaBOT: Bringing Customer Personas to Life with LLMs and RAG](https://arxiv.org/abs/2505.17156)
Append: [Harry Potter is Still Here! Probing Knowledge Leakage in Targeted Unlearned Large Language Models via Automated Adversarial Prompting](https://arxiv.org/abs/2505.17160)
Append: [CRG Score: A Distribution-Aware Clinical Metric for Radiology Report Generation](https://arxiv.org/abs/2505.17167)
Append: [Next Token Perception Score: Analytical Assessment of your LLM Perception Skills](https://arxiv.org/abs/2505.17169)
Append: [FB-RAG: Improving RAG with Forward and Backward Lookup](https://arxiv.org/abs/2505.17206)
Append: [Mitigating Gender Bias via Fostering Exploratory Thinking in LLMs](https://arxiv.org/abs/2505.17217)
Append: [Humans Hallucinate Too: Language Models Identify and Correct Subjective Annotation Errors With Label-in-a-Haystack Prompts](https://arxiv.org/abs/2505.17222)
Append: [ExeSQL: Self-Taught Text-to-SQL Models with Execution-Driven Bootstrapping for SQL Dialects](https://arxiv.org/abs/2505.17231)
Append: [Personalizing Student-Agent Interactions Using Log-Contextualized Retrieval Augmented Generation (RAG)](https://arxiv.org/abs/2505.17238)
Append: [ReasoningShield: Content Safety Detection over Reasoning Traces of Large Reasoning Models](https://arxiv.org/abs/2505.17244)
Append: [ConciseRL: Conciseness-Guided Reinforcement Learning for Efficient Reasoning Models](https://arxiv.org/abs/2505.17250)
Append: [The Rise of Parameter Specialization for Knowledge Storage in Large Language Models](https://arxiv.org/abs/2505.17260)
Append: [CaseReportBench: An LLM Benchmark Dataset for Dense Information Extraction in Clinical Case Reports](https://arxiv.org/abs/2505.17265)
Append: [Select2Reason: Efficient Instruction-Tuning Data Selection for Long-CoT Reasoning](https://arxiv.org/abs/2505.17266)
Append: [GreekBarBench: A Challenging Benchmark for Free-Text Legal Reasoning and Citations](https://arxiv.org/abs/2505.17267)
Append: [Search Wisely: Mitigating Sub-optimal Agentic Searches By Reducing Uncertainty](https://arxiv.org/abs/2505.17281)
Append: [SELF: Self-Extend the Context Length With Logistic Growth Function](https://arxiv.org/abs/2505.17296)
Append: [Refusal Direction is Universal Across Safety-Aligned Languages](https://arxiv.org/abs/2505.17306)
Append: [Benchmarking Expressive Japanese Character Text-to-Speech with VITS and Style-BERT-VITS2](https://arxiv.org/abs/2505.17320)
Append: [From Compression to Expansion: A Layerwise Analysis of In-Context Learning](https://arxiv.org/abs/2505.17322)
Append: [GPT Editors, Not Authors: The Stylistic Footprint of LLMs in Academic Preprints](https://arxiv.org/abs/2505.17327)
Append: [SweEval: Do LLMs Really Swear? A Safety Benchmark for Testing Limits for Enterprise Use](https://arxiv.org/abs/2505.17332)
Append: [Language models should be subject to repeatable, open, domain-contextualized hallucination benchmarking](https://arxiv.org/abs/2505.17345)
Append: [A Fully Generative Motivational Interviewing Counsellor Chatbot for Moving Smokers Towards the Decision to Quit](https://arxiv.org/abs/2505.17362)
Append: [AI-Augmented LLMs Achieve Therapist-Level Responses in Motivational Interviewing](https://arxiv.org/abs/2505.17380)
Append: [WiNGPT-3.0 Technical Report](https://arxiv.org/abs/2505.17387)
Append: [Measuring diversity of synthetic prompts and data generated with fine-grained persona prompting](https://arxiv.org/abs/2505.17390)
Append: [Curriculum Guided Reinforcement Learning for Efficient Multi Hop Retrieval Augmented Generation](https://arxiv.org/abs/2505.17391)
Append: [FullFront: Benchmarking MLLMs Across the Full Front-End Engineering Workflow](https://arxiv.org/abs/2505.17399)
Append: [Language Matters: How Do Multilingual Input and Reasoning Paths Affect Large Reasoning Models?](https://arxiv.org/abs/2505.17407)
Append: [Conversations: Love Them, Hate Them, Steer Them](https://arxiv.org/abs/2505.17413)
Append: [DASH: Input-Aware Dynamic Layer Skipping for Efficient LLM Inference with Markov Decision Policies](https://arxiv.org/abs/2505.17420)
Append: [T$^2$: An Adaptive Test-Time Scaling Strategy for Contextual Question Answering](https://arxiv.org/abs/2505.17427)
Append: [Discovering Forbidden Topics in Language Models](https://arxiv.org/abs/2505.17441)
Append: [Exploring the Effect of Segmentation and Vocabulary Size on Speech Tokenization for Speech Language Models](https://arxiv.org/abs/2505.17446)
Append: [LeTS: Learning to Think-and-Search via Process-and-Outcome Reward Hybridization](https://arxiv.org/abs/2505.17447)
Append: [Towards Evaluating Proactive Risk Awareness of Multimodal Language Models](https://arxiv.org/abs/2505.17455)
Append: [Hydra: Structured Cross-Source Enhanced Large Language Model Reasoning](https://arxiv.org/abs/2505.17464)
Append: [A Position Paper on the Automatic Generation of Machine Learning Leaderboards](https://arxiv.org/abs/2505.17465)
Append: [SLearnLLM: A Self-Learning Framework for Efficient Domain-Specific Adaptation of Large Language Models](https://arxiv.org/abs/2505.17470)
Append: [FinRAGBench-V: A Benchmark for Multimodal RAG with Visual Citation in the Financial Domain](https://arxiv.org/abs/2505.17471)
Append: [MARCO: Meta-Reflection with Cross-Referencing for Code Reasoning](https://arxiv.org/abs/2505.17481)
Append: [keepitsimple at SemEval-2025 Task 3: LLM-Uncertainty based Approach for Multilingual Hallucination Span Detection](https://arxiv.org/abs/2505.17485)
Append: [Analyzing Mitigation Strategies for Catastrophic Forgetting in End-to-End Training of Spoken Language Models](https://arxiv.org/abs/2505.17496)
Append: [CReSt: A Comprehensive Benchmark for Retrieval-Augmented Generation with Complex Reasoning over Structured Documents](https://arxiv.org/abs/2505.17503)
Append: [L-MTP: Leap Multi-Token Prediction Beyond Adjacent Context for Large Language Models](https://arxiv.org/abs/2505.17505)
Append: [Large Language Models Do Multi-Label Classification Differently](https://arxiv.org/abs/2505.17510)
Append: [Multimodal Conversation Structure Understanding](https://arxiv.org/abs/2505.17536)
Append: [How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary Perception](https://arxiv.org/abs/2505.17537)
Append: [Swedish Whispers; Leveraging a Massive Speech Corpus for Swedish Speech Recognition](https://arxiv.org/abs/2505.17538)
Append: [Teaching with Lies: Curriculum DPO on Synthetic Negatives for Hallucination Detection](https://arxiv.org/abs/2505.17558)
Append: [PPT: A Process-based Preference Learning Framework for Self Improving Table Question Answering Models](https://arxiv.org/abs/2505.17565)
Append: [Reasoning Meets Personalization: Unleashing the Potential of Large Reasoning Model for Personalized Generation](https://arxiv.org/abs/2505.17571)
Append: [Wolf Hidden in Sheep's Conversations: Toward Harmless Data-Based Backdoor Attacks for Jailbreaking Large Language Models](https://arxiv.org/abs/2505.17601)
Append: [Distilling LLM Agent into Small Models with Retrieval and Code Tools](https://arxiv.org/abs/2505.17612)
Append: [Runaway is Ashamed, But Helpful: On the Early-Exit Behavior of Large Language Model-based Agents in Embodied Environments](https://arxiv.org/abs/2505.17616)
Append: [Enhancing Large Vision-Language Models with Layout Modality for Table Question Answering on Japanese Annual Securities Reports](https://arxiv.org/abs/2505.17625)
Append: [GIM: Improved Interpretability for Large Language Models](https://arxiv.org/abs/2505.17630)
Append: [Stereotype Detection in Natural Language Processing](https://arxiv.org/abs/2505.17642)
Append: [Bridging Electronic Health Records and Clinical Texts: Contrastive Learning for Enhanced Clinical Tasks](https://arxiv.org/abs/2505.17643)
Append: [EVADE: Multimodal Benchmark for Evasive Content Detection in E-Commerce Applications](https://arxiv.org/abs/2505.17654)
Append: [Too Consistent to Detect: A Study of Self-Consistent Errors in LLMs](https://arxiv.org/abs/2505.17656)
Append: [Towards Dynamic Theory of Mind: Evaluating LLM Adaptation to Temporal Evolution of Human States](https://arxiv.org/abs/2505.17663)
Append: [QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning](https://arxiv.org/abs/2505.17667)
Append: [MIDB: Multilingual Instruction Data Booster for Enhancing Multilingual Instruction Synthesis](https://arxiv.org/abs/2505.17671)
Append: [Tuning Language Models for Robust Prediction of Diverse User Behaviors](https://arxiv.org/abs/2505.17682)
Append: [ELSPR: Evaluator LLM Training Data Self-Purification on Non-Transitive Preferences via Tournament Graph Reconstruction](https://arxiv.org/abs/2505.17691)
Append: [Activation Control for Efficiently Eliciting Long Chain-of-thought Ability of Language Models](https://arxiv.org/abs/2505.17697)
Append: [SemSketches-2021: experimenting with the machine processing of the pilot semantic sketches corpus](https://arxiv.org/abs/2505.17704)
Append: [Understanding How Value Neurons Shape the Generation of Specified Values in LLMs](https://arxiv.org/abs/2505.17712)
Append: [The Pilot Corpus of the English Semantic Sketches](https://arxiv.org/abs/2505.17733)
Append: [Fast Quiet-STaR: Thinking Without Thought Tokens](https://arxiv.org/abs/2505.17746)
Append: [Discriminating Form and Meaning in Multilingual Models with Minimal-Pair ABX Tasks](https://arxiv.org/abs/2505.17747)
Append: [Resolving Conflicting Evidence in Automated Fact-Checking: A Study on Retrieval-Augmented LLMs](https://arxiv.org/abs/2505.17762)
Append: [The Real Barrier to LLM Agent Usability is Agentic ROI](https://arxiv.org/abs/2505.17767)
Append: [EXECUTE: A Multilingual Benchmark for LLM Token Understanding](https://arxiv.org/abs/2505.17784)
Append: [Compression Hacking: A Supplementary Perspective on Informatics Metric of Language Models from Geometric Distortion](https://arxiv.org/abs/2505.17793)
Append: [DialogXpert: Driving Intelligent and Emotion-Aware Conversations through Online Value-Based Reinforcement Learning with LLM Priors](https://arxiv.org/abs/2505.17795)
Append: [Don't Overthink it. Preferring Shorter Thinking Chains for Improved LLM Reasoning](https://arxiv.org/abs/2505.17813)
Append: [Low-Resource NMT: A Case Study on the Written and Spoken Languages in Hong Kong](https://arxiv.org/abs/2505.17816)
Append: [Not All Tokens Are What You Need In Thinking](https://arxiv.org/abs/2505.17827)
Append: [Stepwise Reasoning Checkpoint Analysis: A Test Time Scaling Method to Enhance LLMs' Reasoning](https://arxiv.org/abs/2505.17829)
Append: [Emerging categories in scientific explanations](https://arxiv.org/abs/2505.17832)
Append: [Investigating Affect Mining Techniques for Annotation Sample Selection in the Creation of Finnish Affective Speech Corpus](https://arxiv.org/abs/2505.17833)
Append: [Explaining Sources of Uncertainty in Automated Fact-Checking](https://arxiv.org/abs/2505.17855)
Append: [Just as Humans Need Vaccines, So Do Models: Model Immunization to Combat Falsehoods](https://arxiv.org/abs/2505.17870)
Append: [MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated Experimental Feedback](https://arxiv.org/abs/2505.17873)
Append: [Mutarjim: Advancing Bidirectional Arabic-English Translation with a Small Language Model](https://arxiv.org/abs/2505.17894)
Append: [Language models can learn implicit multi-hop reasoning, but only if they have lots of training data](https://arxiv.org/abs/2505.17923)
Append: [Handling Symbolic Language in Student Texts: A Comparative Study of NLP Embedding Models](https://arxiv.org/abs/2505.17950)
Append: [Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with Minimalist Rule-Based RL](https://arxiv.org/abs/2505.17952)
Append: [Counting Cycles with Deepseek](https://arxiv.org/abs/2505.17964)
Append: [AVerImaTeC: A Dataset for Automatic Verification of Image-Text Claims with Evidence from the Web](https://arxiv.org/abs/2505.17978)
Append: [TRACE for Tracking the Emergence of Semantic Representations in Transformers](https://arxiv.org/abs/2505.17998)
Append: [Training with Pseudo-Code for Instruction Following](https://arxiv.org/abs/2505.18011)
Append: [Contrastive Distillation of Emotion Knowledge from LLMs for Zero-Shot Emotion Recognition](https://arxiv.org/abs/2505.18040)
Append: [MathEDU: Towards Adaptive Feedback for Student Mathematical Problem-Solving](https://arxiv.org/abs/2505.18056)
Append: [Extended Inductive Reasoning for Personalized Preference Inference from Behavioral Signals](https://arxiv.org/abs/2505.18071)
Append: [QwenLong-CPRS: Towards $\infty$-LLMs with Dynamic Context Optimization](https://arxiv.org/abs/2505.18092)
Append: [Planning without Search: Refining Frontier LLMs with Offline Goal-Conditioned RL](https://arxiv.org/abs/2505.18098)
Append: [ManuSearch: Democratizing Deep Search in Large Language Models with a Transparent and Open Multi-Agent Framework](https://arxiv.org/abs/2505.18105)
Append: [Watch and Listen: Understanding Audio-Visual-Speech Moments with Multimodal LLM](https://arxiv.org/abs/2505.18110)
Append: [UNJOIN: Enhancing Multi-Table Text-to-SQL Generation via Schema Simplification](https://arxiv.org/abs/2505.18122)
Append: [Frankentext: Stitching random text fragments into long-form narratives](https://arxiv.org/abs/2505.18128)
Append: [Graph-Linguistic Fusion: Using Language Models for Wikidata Vandalism Detection](https://arxiv.org/abs/2505.18136)
Append: [Lost in the Haystack: Smaller Needles are More Difficult for LLMs to Find](https://arxiv.org/abs/2505.18148)
Append: [First Finish Search: Efficient Test-Time Scaling in Large Language Models](https://arxiv.org/abs/2505.18149)
Append: [Fann or Flop: A Multigenre, Multiera Benchmark for Arabic Poetry Understanding in LLMs](https://arxiv.org/abs/2505.18152)
Append: [The Staircase of Ethics: Probing LLM Value Priorities through Multi-Step Induction to Complex Moral Dilemmas](https://arxiv.org/abs/2505.18154)
Append: [Generalizing Large Language Model Usability Across Resource-Constrained](https://arxiv.org/abs/2505.17040)
Append: [Exploring EFL Secondary Students' AI-generated Text Editing While Composition Writing](https://arxiv.org/abs/2505.17041)
Append: [Safety Alignment Can Be Not Superficial With Explicit Safety Signals](https://arxiv.org/abs/2505.17072)
Append: [GSDFuse: Capturing Cognitive Inconsistencies from Multi-Dimensional Weak Signals in Social Media Steganalysis](https://arxiv.org/abs/2505.17085)
Append: [From Weak Labels to Strong Results: Utilizing 5,000 Hours of Noisy Classroom Transcripts with Minimal Accurate Data](https://arxiv.org/abs/2505.17088)
Append: [Voicing Personas: Rewriting Persona Descriptions into Style Prompts for Controllable Text-to-Speech](https://arxiv.org/abs/2505.17093)
Append: [CAMA: Enhancing Multimodal In-Context Learning with Context-Aware Modulated Attention](https://arxiv.org/abs/2505.17097)
Append: [Mitigating Cyber Risk in the Age of Open-Weight LLMs: Policy Gaps and Technical Realities](https://arxiv.org/abs/2505.17109)
Append: [Robustifying Vision-Language Models via Dynamic Token Reweighting](https://arxiv.org/abs/2505.17132)
Append: [TrimR: Verifier-based Training-Free Thinking Compression for Efficient Test-Time Scaling](https://arxiv.org/abs/2505.17155)
Append: [OCR-Reasoning Benchmark: Unveiling the True Capabilities of MLLMs in Complex Text-Rich Image Reasoning](https://arxiv.org/abs/2505.17163)
Append: [CHART-6: Human-Centered Evaluation of Data Visualization Understanding in Vision-Language Models](https://arxiv.org/abs/2505.17202)
Append: [CHAOS: Chart Analysis with Outlier Samples](https://arxiv.org/abs/2505.17235)
Append: [Zebra-Llama: Towards Extremely Efficient Hybrid Models](https://arxiv.org/abs/2505.17272)
Append: [Attention with Trained Embeddings Provably Selects Important Tokens](https://arxiv.org/abs/2505.17282)
Append: [Longer Context, Deeper Thinking: Uncovering the Role of Long-Context Ability in Reasoning](https://arxiv.org/abs/2505.17315)
Append: [Analyzing Fine-Grained Alignment and Enhancing Vision Understanding in Multimodal Language Models](https://arxiv.org/abs/2505.17316)
Append: [FS-DAG: Few Shot Domain Adapting Graph Networks for Visually Rich Document Understanding](https://arxiv.org/abs/2505.17330)
Append: [ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training](https://arxiv.org/abs/2505.17331)
Append: [DEL-ToM: Inference-Time Scaling for Theory-of-Mind Reasoning via Dynamic Epistemic Logic](https://arxiv.org/abs/2505.17348)
Append: [An End-to-End Approach for Child Reading Assessment in the Xhosa Language](https://arxiv.org/abs/2505.17371)
Append: [Value-Guided Search for Efficient Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.17373)
Append: [Chart-to-Experience: Benchmarking Multimodal LLMs for Predicting Experiential Impact of Charts](https://arxiv.org/abs/2505.17374)
Append: [LLM-based Generative Error Correction for Rare Words with Synthetic Data and Phonetic Context](https://arxiv.org/abs/2505.17410)
Append: [Speechless: Speech Instruction Training Without Speech for Low Resource Languages](https://arxiv.org/abs/2505.17417)
Append: [Debiasing CLIP: Interpreting and Correcting Bias in Attention Heads](https://arxiv.org/abs/2505.17425)
Append: [Self-Training Large Language Models with Confident Reasoning](https://arxiv.org/abs/2505.17454)
Append: [Diagnosing Vision Language Models' Perception by Leveraging Human Methods for Color Vision Deficiencies](https://arxiv.org/abs/2505.17461)
Append: [OrionBench: A Benchmark for Chart and Human-Recognizable Object Detection in Infographics](https://arxiv.org/abs/2505.17473)
Append: [From Reasoning to Generalization: Knowledge-Augmented LLMs for ARC Benchmark](https://arxiv.org/abs/2505.17482)
Append: [PD$^3$: A Project Duplication Detection Framework via Adapted Multi-Agent Debate](https://arxiv.org/abs/2505.17492)
Append: [ProxySPEX: Inference-Efficient Interpretability via Sparse Feature Interactions in LLMs](https://arxiv.org/abs/2505.17495)
Append: [On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning](https://arxiv.org/abs/2505.17508)
Append: [Probe by Gaming: A Game-based Benchmark for Assessing Conceptual Knowledge in LLMs](https://arxiv.org/abs/2505.17512)
Append: [What You Read Isn't What You Hear: Linguistic Sensitivity in Deepfake Speech Detection](https://arxiv.org/abs/2505.17513)
Append: [Chain-of-Lure: A Synthetic Narrative-Driven Approach to Compromise Large Language Models](https://arxiv.org/abs/2505.17519)
Append: [Co-Reinforcement Learning for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2505.17534)
Append: [CoMoE: Contrastive Representation for Mixture-of-Experts in Parameter-Efficient Fine-tuning](https://arxiv.org/abs/2505.17553)
Append: [NeUQI: Near-Optimal Uniform Quantization Parameter Initialization](https://arxiv.org/abs/2505.17595)
Append: [One Model Transfer to All: On Robust Jailbreak Prompts Generation against LLMs](https://arxiv.org/abs/2505.17598)
Append: [Controlled Agentic Planning & Reasoning for Mechanism Synthesis](https://arxiv.org/abs/2505.17607)
Append: [MMMG: a Comprehensive and Reliable Evaluation Suite for Multitask Multimodal Generation](https://arxiv.org/abs/2505.17613)
Append: [Large language model as user daily behavior data generator: balancing population diversity and individual personality](https://arxiv.org/abs/2505.17615)
Append: [Surfacing Semantic Orthogonality Across Model Safety Benchmarks: A Multi-Dimensional Analysis](https://arxiv.org/abs/2505.17636)
Append: [HoloLLM: Multisensory Foundation Model for Language-Grounded Human Sensing and Reasoning](https://arxiv.org/abs/2505.17645)
Append: [COUNTDOWN: Contextually Sparse Activation Filtering Out Unnecessary Weights in Down Projection](https://arxiv.org/abs/2505.17701)
Append: [PPO-BR: Dual-Signal Entropy-Reward Adaptation for Trust Region Policy Optimization](https://arxiv.org/abs/2505.17714)
Append: [PatientSim: A Persona-Driven Simulator for Realistic Doctor-Patient Interactions](https://arxiv.org/abs/2505.17818)
Append: [Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement Fine-Tuning of Large Language Models](https://arxiv.org/abs/2505.17826)
Append: [T2I-Eval-R1: Reinforcement Learning-Driven Reasoning for Interpretable Text-to-Image Evaluation](https://arxiv.org/abs/2505.17897)
Append: [Towards Practical Defect-Focused Automated Code Review](https://arxiv.org/abs/2505.17928)
Append: [Understanding Gated Neurons in Transformers from Their Input-Output Functionality](https://arxiv.org/abs/2505.17936)
Append: [Are Large Language Models Reliable AI Scientists? Assessing Reverse-Engineering of Black-Box Systems](https://arxiv.org/abs/2505.17968)
Append: [Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective](https://arxiv.org/abs/2505.17997)
Append: [Structured Thinking Matters: Improving LLMs Generalization in Causal Inference Tasks](https://arxiv.org/abs/2505.18034)
Append: [Deep Video Discovery: Agentic Search with Tool Use for Long-form Video Understanding](https://arxiv.org/abs/2505.18079)
Append: [Data Mixing Can Induce Phase Transitions in Knowledge Acquisition](https://arxiv.org/abs/2505.18091)
Append: [How Can I Publish My LLM Benchmark Without Giving the True Answers Away?](https://arxiv.org/abs/2505.18102)
Append: [Bridging Supervised Learning and Reinforcement Learning in Math Reasoning](https://arxiv.org/abs/2505.18116)
Append: [ProgRM: Build Better GUI Agents with Progress Rewards](https://arxiv.org/abs/2505.18121)
Append: [TabSTAR: A Foundation Tabular Model With Semantically Target-Aware Representations](https://arxiv.org/abs/2505.18125)
Append: [Reward Model Overoptimisation in Iterated RLHF](https://arxiv.org/abs/2505.18126)
Append: [One RL to See Them All: Visual Triple Unified Reinforcement Learning](https://arxiv.org/abs/2505.18129)
Append: [VideoGameBench: Can Vision-Language Models complete popular video games?](https://arxiv.org/abs/2505.18134)
Append: [Gaming Tool Preferences in Agentic LLMs](https://arxiv.org/abs/2505.18135)
Append: [QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources](https://arxiv.org/abs/2310.07147)
Append: [Offset Unlearning for Large Language Models](https://arxiv.org/abs/2404.11045)
Append: [Temporal Dynamics of Emotion and Cognition in Human Translation: Integrating the Task Segment Framework and the HOF Taxonomy](https://arxiv.org/abs/2405.03111)
Append: [ActiveLLM: Large Language Model-based Active Learning for Textual Few-Shot Scenarios](https://arxiv.org/abs/2405.10808)
Append: [Mitigate Position Bias in Large Language Models via Scaling a Single Dimension](https://arxiv.org/abs/2406.02536)
Append: [ReCaLL: Membership Inference via Relative Conditional Log-Likelihoods](https://arxiv.org/abs/2406.15968)
Append: [Large Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure for Jailbreak Attacks](https://arxiv.org/abs/2407.00869)
Append: [Ground Every Sentence: Improving Retrieval-Augmented LLMs with Interleaved Reference-Claim Generation](https://arxiv.org/abs/2407.01796)
Append: [Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled Refusal Training](https://arxiv.org/abs/2407.09121)
Append: [Genetic Instruct: Scaling up Synthetic Generation of Coding Instructions for Large Language Models](https://arxiv.org/abs/2407.21077)
Append: [Enhancing Robustness in Large Language Models: Prompting for Mitigating the Impact of Irrelevant Information](https://arxiv.org/abs/2408.10615)
Append: [Enhancing Low-Resource Language and Instruction Following Capabilities of Audio Language Models](https://arxiv.org/abs/2409.10999)
Append: [Task Arithmetic for Language Expansion in Speech Translation](https://arxiv.org/abs/2409.11274)
Append: [From Lists to Emojis: How Format Bias Affects Model Alignment](https://arxiv.org/abs/2409.11704)
Append: [Enhancing Unsupervised Sentence Embeddings via Knowledge-Driven Data Augmentation and Gaussian-Decayed Contrastive Learning](https://arxiv.org/abs/2409.12887)
Append: [Position IDs Matter: An Enhanced Position Layout for Efficient Context Compression in Large Language Models](https://arxiv.org/abs/2409.14364)
Append: [Cross-lingual Human-Preference Alignment for Neural Machine Translation with Direct Quality Optimization](https://arxiv.org/abs/2409.17673)
Append: [KCIF: Knowledge-Conditioned Instruction Following](https://arxiv.org/abs/2410.12972)
Append: [TrendFact: A Benchmark for Explainable Hotspot Perception in Fact-Checking with Natural Language Explanation](https://arxiv.org/abs/2410.15135)
Append: [Can Large Language Models Invent Algorithms to Improve Themselves?: Algorithm Discovery for Recursive Self-Improvement through Reinforcement Learning](https://arxiv.org/abs/2410.15639)
Append: [Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback](https://arxiv.org/abs/2410.19133)
Append: [Long Sequence Modeling with Attention Tensorization: From Sequence to Tensor Learning](https://arxiv.org/abs/2410.20926)
Append: [LL\"aMmlein: Compact and Competitive German-Only Language Models from Scratch](https://arxiv.org/abs/2411.11171)
Append: [Multi-modal Retrieval Augmented Multi-modal Generation: Datasets, Evaluation Metrics and Strong Baselines](https://arxiv.org/abs/2411.16365)
Append: [Initialization using Update Approximation is a Silver Bullet for Extremely Efficient Low-Rank Fine-Tuning](https://arxiv.org/abs/2411.19557)
Append: [MediaSpin: Exploring Media Bias Through Fine-Grained Analysis of News Headlines](https://arxiv.org/abs/2412.02271)
Append: [UAlign: Leveraging Uncertainty Estimations for Factuality Alignment on Large Language Models](https://arxiv.org/abs/2412.11803)
Append: [Boosting Long-Context Management via Query-Guided Activation Refilling](https://arxiv.org/abs/2412.12486)
Append: [TrustRAG: Enhancing Robustness and Trustworthiness in Retrieval-Augmented Generation](https://arxiv.org/abs/2501.00879)
Append: [URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics](https://arxiv.org/abs/2501.04686)
Append: [EpiCoder: Encompassing Diversity and Complexity in Code Generation](https://arxiv.org/abs/2501.04694)
Append: [Large Language Models Share Representations of Latent Grammatical Concepts Across Typologically Diverse Languages](https://arxiv.org/abs/2501.06346)
Append: [TAD-Bench: A Comprehensive Benchmark for Embedding-Based Text Anomaly Detection](https://arxiv.org/abs/2501.11960)
Append: [Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling](https://arxiv.org/abs/2501.16975)
Append: [CopySpec: Accelerating LLMs with Speculative Copy-and-Paste Without Compromising Quality](https://arxiv.org/abs/2502.08923)
Append: [Beyond One-Size-Fits-All Pruning via Evolutionary Metric Search for Large Language Models](https://arxiv.org/abs/2502.10735)
Append: [1bit-Merging: Dynamic Quantized Merging for Large Language Models](https://arxiv.org/abs/2502.10743)
Append: [LoRE-Merging: Exploring Low-Rank Estimation For Large Language Model Merging](https://arxiv.org/abs/2502.10749)
Append: [Investigating Language Preference of Multilingual RAG Systems](https://arxiv.org/abs/2502.11175)
Append: [Vendi-RAG: Adaptively Trading-Off Diversity And Quality Significantly Improves Retrieval Augmented Generation With LLMs](https://arxiv.org/abs/2502.11228)
Append: [System Message Generation for User Preferences using Open-Source Models](https://arxiv.org/abs/2502.11330)
Append: [Is Human-Like Text Liked by Humans? Multilingual Human Detection and Preference Against AI](https://arxiv.org/abs/2502.11614)
Append: [Personality Editing for Language Models through Relevant Knowledge Editing](https://arxiv.org/abs/2502.11789)
Append: [PASER: Post-Training Data Selection for Efficient Pruned Large Language Model Recovery](https://arxiv.org/abs/2502.12594)
Append: [None of the Others: a General Technique to Distinguish Reasoning from Memorization in Multiple-Choice LLM Evaluation Benchmarks](https://arxiv.org/abs/2502.12896)
Append: [Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests](https://arxiv.org/abs/2502.14359)
Append: [ICA-RAG: Information Completeness Guided Adaptive Retrieval-Augmented Generation for Disease Diagnosis](https://arxiv.org/abs/2502.14614)
Append: [Edit Once, Update Everywhere: A Simple Framework for Cross-Lingual Knowledge Synchronization in LLMs](https://arxiv.org/abs/2502.14645)
Append: [SafeInt: Shielding Large Language Models from Jailbreak Attacks via Safety-Aware Representation Intervention](https://arxiv.org/abs/2502.15594)
Append: [Retrieval-Augmented Fine-Tuning With Preference Optimization For Visual Program Generation](https://arxiv.org/abs/2502.16529)
Append: [PrivaCI-Bench: Evaluating Privacy with Contextual Integrity and Legal Compliance](https://arxiv.org/abs/2502.17041)
Append: [Mind the Blind Spots: A Focus-Level Evaluation Framework for LLM Reviews](https://arxiv.org/abs/2502.17086)
Append: [Unveiling Downstream Performance Scaling of LLMs: A Clustering-Based Perspective](https://arxiv.org/abs/2502.17262)
Append: [Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review](https://arxiv.org/abs/2502.19614)
Append: [Do Retrieval-Augmented Language Models Adapt to Varying User Needs?](https://arxiv.org/abs/2502.19779)
Append: [HoT: Highlighted Chain of Thought for Referencing Supporting Facts from Inputs](https://arxiv.org/abs/2503.02003)
Append: [Rewarding Doubt: A Reinforcement Learning Approach to Calibrated Confidence Expression of Large Language Models](https://arxiv.org/abs/2503.02623)
Append: [SEOE: A Scalable and Reliable Semantic Evaluation Framework for Open Domain Event Detection](https://arxiv.org/abs/2503.03303)
Append: [Compositional Causal Reasoning Evaluation in Language Models](https://arxiv.org/abs/2503.04556)
Append: [HICD: Hallucination-Inducing via Attention Dispersion for Contrastive Decoding to Mitigate Hallucinations in Large Language Models](https://arxiv.org/abs/2503.12908)
Append: [MedPlan:A Two-Stage RAG-Based System for Personalized Medical Plan Generation](https://arxiv.org/abs/2503.17900)
Append: [A Retrieval-Based Approach to Medical Procedure Matching in Romanian](https://arxiv.org/abs/2503.20556)
Append: [Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging](https://arxiv.org/abs/2503.20641)
Append: [Cognitive Debiasing Large Language Models for Decision-Making](https://arxiv.org/abs/2504.04141)
Append: [SemEval-2025 Task 5: LLMs4Subjects -- LLM-based Automated Subject Tagging for a National Technical Library's Open-Access Catalog](https://arxiv.org/abs/2504.07199)
Append: [ConceptCarve: Dynamic Realization of Evidence](https://arxiv.org/abs/2504.07228)
Append: [Playpen: An Environment for Exploring Learning Through Conversational Interaction](https://arxiv.org/abs/2504.08590)
Append: [DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and Verifiable Mathematical Dataset for Advancing Reasoning](https://arxiv.org/abs/2504.11456)
Append: [Information Gain-Guided Causal Intervention for Autonomous Debiasing Large Language Models](https://arxiv.org/abs/2504.12898)
Append: [Aleph-Alpha-GermanWeb: Improving German-language LLM pre-training with model-based data curation and synthetic data generation](https://arxiv.org/abs/2505.00022)
Append: [Rank, Chunk and Expand: Lineage-Oriented Reasoning for Taxonomy Expansion](https://arxiv.org/abs/2505.13282)
Append: [Explaining Black-box Model Predictions via Two-level Nested Feature Attributions with Consistency Property](https://arxiv.org/abs/2405.14522)
Append: [Tracing Representation Progression: Analyzing and Enhancing Layer-Wise Similarity](https://arxiv.org/abs/2406.14479)
Append: [Fundamental Limitations on Subquadratic Alternatives to Transformers](https://arxiv.org/abs/2410.04271)
Append: [MeNTi: Bridging Medical Calculator and LLM Agent with Nested Tool Calling](https://arxiv.org/abs/2410.13610)
Append: [Mastering Board Games by External and Internal Planning with Language Models](https://arxiv.org/abs/2412.12119)
Append: [FBQuant: FeedBack Quantization for Large Language Models](https://arxiv.org/abs/2501.16385)
Append: [Optimizing Large Language Model Training Using FP4 Quantization](https://arxiv.org/abs/2501.17116)
Append: [WILDCHAT-50M: A Deep Dive Into the Role of Synthetic Data in Post-Training](https://arxiv.org/abs/2501.18511)
Append: [Rethinking Bottlenecks in Safety Fine-Tuning of Vision Language Models](https://arxiv.org/abs/2501.18533)
Append: [SETS: Leveraging Self-Verification and Self-Correction for Improved Test-Time Scaling](https://arxiv.org/abs/2501.19306)
Append: [GRADIEND: Monosemantic Feature Learning within Neural Networks Applied to Gender Debiasing of Transformer Models](https://arxiv.org/abs/2502.01406)
Append: [Fairness through Difference Awareness: Measuring Desired Group Discrimination in LLMs](https://arxiv.org/abs/2502.01926)
Append: [Towards Copyright Protection for Knowledge Bases of Retrieval-augmented Language Models via Reasoning](https://arxiv.org/abs/2502.10440)
Append: [Table-Critic: A Multi-Agent Framework for Collaborative Criticism and Refinement in Table Reasoning](https://arxiv.org/abs/2502.11799)
Append: [Provably Correct Automata Embeddings for Optimal Automata-Conditioned Reinforcement Learning](https://arxiv.org/abs/2503.05042)
Append: [StealthRank: LLM Ranking Manipulation via Stealthy Prompt Optimization](https://arxiv.org/abs/2504.05804)
Append: [Hogwild! Inference: Parallel LLM Generation via Concurrent Attention](https://arxiv.org/abs/2504.06261)
Append: [The Quantum LLM: Modeling Semantic Spaces with Quantum Principles](https://arxiv.org/abs/2504.13202)
Append: [X-Transfer Attacks: Towards Super Transferable Adversarial Attacks on CLIP](https://arxiv.org/abs/2505.05528)
Append: [SelfBudgeter: Adaptive Token Allocation for Efficient LLM Reasoning](https://arxiv.org/abs/2505.11274)
Append: [Phare: A Safety Probe for Large Language Models](https://arxiv.org/abs/2505.11365)
Append: [GEM: Gaussian Embedding Modeling for Out-of-Distribution Detection in GUI Agents](https://arxiv.org/abs/2505.12842)
Append: [SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based on Speech and Audio Information](https://arxiv.org/abs/2505.13237)
append_entries: 338
Finish: 2025-05-26 04:28:03.495728
------------------------------------------------------
Started: 2025-05-26 06:26:06.199943
Existing_entries: 1338
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1797
Summarized using GPT-3.5-turbo
Append: [Chain-of-Model Learning for Language Model](https://arxiv.org/abs/2505.11820)
Token length: 966
Summarized using GPT-3.5-turbo
Append: [An Annotated Corpus of Arabic Tweets for Hate Speech Analysis](https://arxiv.org/abs/2505.11969)
Token length: 1386
Summarized using GPT-3.5-turbo
Append: [The AI Gap: How Socioeconomic Status Affects Language Technology Interactions](https://arxiv.org/abs/2505.12158)
Token length: 1587
Summarized using GPT-3.5-turbo
Append: [UniEdit: A Unified Knowledge Editing Benchmark for Large Language Models](https://arxiv.org/abs/2505.12345)
Token length: 1678
Summarized using GPT-3.5-turbo
Append: [Hidden Ghost Hand: Unveiling Backdoor Vulnerabilities in MLLM-Powered Mobile GUI Agents](https://arxiv.org/abs/2505.14418)
append_entries: 5
Finish: 2025-05-26 06:26:19.018598
------------------------------------------------------
Started: 2025-05-26 08:35:39.721530
Existing_entries: 1005
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1430
Summarized using GPT-3.5-turbo
Append: [HausaNLP: Current Status, Challenges and Future Directions for Hausa Natural Language Processing](https://arxiv.org/abs/2505.14311)
append_entries: 1
Finish: 2025-05-26 08:35:43.786190
------------------------------------------------------
Started: 2025-05-26 10:21:02.873200
Existing_entries: 1001
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-26 10:21:03.613807
------------------------------------------------------
Started: 2025-05-26 12:33:12.007667
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-26 12:33:12.736202
------------------------------------------------------
Started: 2025-05-26 14:16:38.766128
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-26 14:16:39.494597
------------------------------------------------------
Started: 2025-05-26 16:19:50.194213
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-26 16:19:50.893869
------------------------------------------------------
Started: 2025-05-26 18:21:52.577873
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-26 18:21:53.320226
------------------------------------------------------
Started: 2025-05-26 20:18:08.211632
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-26 20:18:08.899947
------------------------------------------------------
Started: 2025-05-26 22:15:15.323000
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-26 22:15:16.097041
------------------------------------------------------
Started: 2025-05-27 01:18:19.299410
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-27 01:18:19.995835
------------------------------------------------------
Started: 2025-05-27 03:09:03.308809
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-27 03:09:04.073008
------------------------------------------------------
Started: 2025-05-27 04:26:38.070620
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1122
Summarized using GPT-3.5-turbo
Append: [Advancing Uto-Aztecan Language Technologies: A Case Study on the Endangered Comanche Language](https://arxiv.org/abs/2505.18159)
Token length: 888
Summarized using GPT-3.5-turbo
Append: [Do BERT-Like Bidirectional Models Still Perform Better on Text Classification in the Era of LLMs?](https://arxiv.org/abs/2505.18215)
Token length: 1115
Summarized using GPT-3.5-turbo
Append: [CoMet: Metaphor-Driven Covert Communication for Multi-Agent Language Games](https://arxiv.org/abs/2505.18218)
Token length: 995
Summarized using GPT-3.5-turbo
Append: [IDA-Bench: Evaluating LLMs on Interactive Guided Data Analysis](https://arxiv.org/abs/2505.18223)
Token length: 1396
Summarized using GPT-3.5-turbo
Append: [Think or Not? Exploring Thinking Efficiency in Large Reasoning Models via an Information-Theoretic Lens](https://arxiv.org/abs/2505.18237)
Token length: 1131
Summarized using GPT-3.5-turbo
Append: [Taming LLMs with Negative Samples: A Reference-Free Framework to Evaluate Presentation Content with Actionable Feedback](https://arxiv.org/abs/2505.18240)
Token length: 1446
Summarized using GPT-3.5-turbo
Append: [Multi-Scale Probabilistic Generation Theory: A Hierarchical Framework for Interpreting Large Language Models](https://arxiv.org/abs/2505.18244)
Token length: 1752
Summarized using GPT-3.5-turbo
Append: [MetaGen Blended RAG: Higher Accuracy for Domain-Specific Q&A Without Fine-Tuning](https://arxiv.org/abs/2505.18247)
Token length: 1297
Summarized using GPT-3.5-turbo
Append: [TAGS: A Test-Time Generalist-Specialist Framework with Retrieval-Augmented Reasoning and Verification](https://arxiv.org/abs/2505.18283)
Token length: 1417
Summarized using GPT-3.5-turbo
Append: [Thinking Fast and Right: Balancing Accuracy and Reasoning Length with Adaptive Rewards](https://arxiv.org/abs/2505.18298)
Token length: 972
Summarized using GPT-3.5-turbo
Append: [Is It Bad to Work All the Time? Cross-Cultural Evaluation of Social Norm Biases in GPT-4](https://arxiv.org/abs/2505.18322)
Token length: 1154
Summarized using GPT-3.5-turbo
Append: [PerMedCQA: Benchmarking Large Language Models on Medical Consumer Question Answering in Persian Language](https://arxiv.org/abs/2505.18331)
Token length: 1551
Summarized using GPT-3.5-turbo
Append: [Model Editing with Graph-Based External Memory](https://arxiv.org/abs/2505.18343)
Token length: 1452
Summarized using GPT-3.5-turbo
Append: [The Unreasonable Effectiveness of Model Merging for Cross-Lingual Transfer in LLMs](https://arxiv.org/abs/2505.18356)
Token length: 1329
Summarized using GPT-3.5-turbo
Append: [SchemaGraphSQL: Efficient Schema Linking with Pathfinding Graph Algorithms for Text-to-SQL on Large-Scale Databases](https://arxiv.org/abs/2505.18363)
Token length: 1943
Summarized using GPT-3.5-turbo
Append: [ShIOEnv: A CLI Behavior-Capturing Environment Enabling Grammar-Guided Command Synthesis for Dataset Curation](https://arxiv.org/abs/2505.18374)
Token length: 1444
Summarized using GPT-3.5-turbo
Append: [NileChat: Towards Linguistically Diverse and Culturally Aware LLMs for Local Communities](https://arxiv.org/abs/2505.18383)
Token length: 1163
Summarized using GPT-3.5-turbo
Append: [RaDeR: Reasoning-aware Dense Retrieval Models](https://arxiv.org/abs/2505.18405)
Token length: 1433
Summarized using GPT-3.5-turbo
Append: [DanmakuTPPBench: A Multi-modal Benchmark for Temporal Point Process Modeling and Understanding](https://arxiv.org/abs/2505.18411)
Token length: 1187
Summarized using GPT-3.5-turbo
Append: [Retrieval Augmented Generation-based Large Language Models for Bridging Transportation Cybersecurity Legal Knowledge Gaps](https://arxiv.org/abs/2505.18426)
Token length: 1002
Summarized using GPT-3.5-turbo
Append: [Voice of a Continent: Mapping Africa's Speech Technology Frontier](https://arxiv.org/abs/2505.18436)
Token length: 1190
Summarized using GPT-3.5-turbo
Append: [Efficient Long CoT Reasoning in Small Language Models](https://arxiv.org/abs/2505.18440)
Token length: 1214
Summarized using GPT-3.5-turbo
Append: [BRIT: Bidirectional Retrieval over Unified Image-Text Graph](https://arxiv.org/abs/2505.18450)
Token length: 1359
Summarized using GPT-3.5-turbo
Append: [MedScore: Factuality Evaluation of Free-Form Medical Answers](https://arxiv.org/abs/2505.18452)
Token length: 1816
Summarized using GPT-3.5-turbo
Append: [Hybrid Latent Reasoning via Reinforcement Learning](https://arxiv.org/abs/2505.18454)
Token length: 1453
Summarized using GPT-3.5-turbo
Append: [Anchored Diffusion Language Model](https://arxiv.org/abs/2505.18456)
Token length: 1275
Summarized using GPT-3.5-turbo
Append: [Measuring South Asian Biases in Large Language Models](https://arxiv.org/abs/2505.18466)
Token length: 1109
Summarized using GPT-3.5-turbo
Append: [Investigating AI Rater Effects of Large Language Models: GPT, Claude, Gemini, and DeepSeek](https://arxiv.org/abs/2505.18486)
Token length: 1442
Summarized using GPT-3.5-turbo
Append: [The Pragmatic Mind of Machines: Tracing the Emergence of Pragmatic Competence in Large Language Models](https://arxiv.org/abs/2505.18497)
Token length: 1897
Summarized using GPT-3.5-turbo
Append: [How Does Sequence Modeling Architecture Influence Base Capabilities of Pre-trained Language Models? Exploring Key Architecture Design Principles to Avoid Base Capabilities Degradation](https://arxiv.org/abs/2505.18522)
Token length: 1204
Summarized using GPT-3.5-turbo
Append: [metaTextGrad: Automatically optimizing language model optimizers](https://arxiv.org/abs/2505.18524)
Token length: 1463
Summarized using GPT-3.5-turbo
Append: [Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal Large Language Models](https://arxiv.org/abs/2505.18536)
Token length: 1197
Summarized using GPT-3.5-turbo
Append: [Business as \textit{Rule}sual: A Benchmark and Framework for Business Rule Flow Modeling with LLMs](https://arxiv.org/abs/2505.18542)
Token length: 1315
Summarized using GPT-3.5-turbo
Append: [Composable Cross-prompt Essay Scoring by Merging Models](https://arxiv.org/abs/2505.18548)
Token length: 933
Summarized using GPT-3.5-turbo
Append: [MSA at BEA 2025 Shared Task: Disagreement-Aware Instruction Tuning for Multi-Dimensional Evaluation of LLMs as Math Tutors](https://arxiv.org/abs/2505.18549)
Token length: 1298
Summarized using GPT-3.5-turbo
Append: [Unraveling Misinformation Propagation in LLM Reasoning](https://arxiv.org/abs/2505.18555)
Token length: 1566
Summarized using GPT-3.5-turbo
Append: [Exploring the Vulnerability of the Content Moderation Guardrail in Large Language Models via Intent Manipulation](https://arxiv.org/abs/2505.18556)
Token length: 842
Summarized using GPT-3.5-turbo
Append: [TAG-INSTRUCT: Controlled Instruction Complexity Enhancement through Structure-based Augmentation](https://arxiv.org/abs/2505.18557)
Token length: 1131
Summarized using GPT-3.5-turbo
Append: [From Word to World: Evaluate and Mitigate Culture Bias via Word Association Test](https://arxiv.org/abs/2505.18562)
Token length: 1047
Summarized using GPT-3.5-turbo
Append: [Removal of Hallucination on Hallucination: Debate-Augmented RAG](https://arxiv.org/abs/2505.18581)
Token length: 1226
Summarized using GPT-3.5-turbo
Append: [Safety Alignment via Constrained Knowledge Unlearning](https://arxiv.org/abs/2505.18588)
Token length: 1491
Summarized using GPT-3.5-turbo
Append: [Debate-to-Detect: Reformulating Misinformation Detection as a Real-World Debate with Large Language Models](https://arxiv.org/abs/2505.18596)
Token length: 1534
Summarized using GPT-3.5-turbo
Append: [Flex-Judge: Think Once, Judge Anywhere](https://arxiv.org/abs/2505.18601)
Token length: 1009
Summarized using GPT-3.5-turbo
Append: [RASMALAI: Resources for Adaptive Speech Modeling in Indian Languages with Accents and Intonations](https://arxiv.org/abs/2505.18609)
Token length: 1931
Summarized using GPT-3.5-turbo
Append: [PM-KVQ: Progressive Mixed-precision KV Cache Quantization for Long-CoT LLMs](https://arxiv.org/abs/2505.18610)
Token length: 989
Summarized using GPT-3.5-turbo
Append: [MAVL: A Multilingual Audio-Video Lyrics Dataset for Animated Song Translation](https://arxiv.org/abs/2505.18614)
Token length: 1040
Summarized using GPT-3.5-turbo
Append: [DDO: Dual-Decision Optimization via Multi-Agent Collaboration for LLM-Based Medical Consultation](https://arxiv.org/abs/2505.18630)
Token length: 980
Summarized using GPT-3.5-turbo
Append: [Multilingual Question Answering in Low-Resource Settings: A Dzongkha-English Benchmark for Foundation Models](https://arxiv.org/abs/2505.18638)
Token length: 1486
Summarized using GPT-3.5-turbo
Append: [Skip-Thinking: Chunk-wise Chain-of-Thought Distillation Enable Smaller Language Models to Reason Better and Faster](https://arxiv.org/abs/2505.18642)
Token length: 1545
Summarized using GPT-3.5-turbo
Append: [On the Emergence of Linear Analogies in Word Embeddings](https://arxiv.org/abs/2505.18651)
Append: [Climate-Eval: A Comprehensive Benchmark for NLP Tasks Related to Climate Change](https://arxiv.org/abs/2505.18653)
Append: [Robustness in Large Language Models: A Survey of Mitigation Strategies and Evaluation Metrics](https://arxiv.org/abs/2505.18658)
Append: [Cross-Lingual Pitfalls: Automatic Probing Cross-Lingual Weakness of Multilingual Large Language Models](https://arxiv.org/abs/2505.18673)
Append: [Social Good or Scientific Curiosity? Uncovering the Research Framing Behind NLP Artefacts](https://arxiv.org/abs/2505.18677)
Append: [TULUN: Transparent and Adaptable Low-resource Machine Translation](https://arxiv.org/abs/2505.18683)
Append: [From Generation to Detection: A Multimodal Multi-Task Dataset for Benchmarking Health Misinformation](https://arxiv.org/abs/2505.18685)
Append: [Large Language Models in the Task of Automatic Validation of Text Classifier Predictions](https://arxiv.org/abs/2505.18688)
Append: [Benchmarking and Rethinking Knowledge Editing for Large Language Models](https://arxiv.org/abs/2505.18690)
Append: [Towards Semantic Integration of Opinions: Unified Opinion Concepts Ontology and Extraction Task](https://arxiv.org/abs/2505.18703)
Append: [A General Knowledge Injection Framework for ICD Coding](https://arxiv.org/abs/2505.18708)
Append: [Improving Bangla Linguistics: Advanced LSTM, Bi-LSTM, and Seq2Seq Models for Translating Sylheti to Modern Bangla](https://arxiv.org/abs/2505.18709)
Append: [Optimal Transport-Based Token Weighting scheme for Enhanced Preference Optimization](https://arxiv.org/abs/2505.18720)
Append: [LogicCat: A Chain-of-Thought Text-to-SQL Benchmark for Multi-Domain Reasoning Challenges](https://arxiv.org/abs/2505.18744)
Append: [Unifying Attention Heads and Task Vectors via Hidden State Geometry in In-Context Learning](https://arxiv.org/abs/2505.18752)
Append: [Few-Shot Optimization for Sensor Data Using Large Language Models: A Case Study on Fatigue Detection](https://arxiv.org/abs/2505.18754)
Append: [How Is LLM Reasoning Distracted by Irrelevant Context? An Analysis Using a Controlled Benchmark](https://arxiv.org/abs/2505.18761)
Append: [Towards an automatic method for generating topical vocabulary test forms for specific reading passages](https://arxiv.org/abs/2505.18762)
Append: [Disentangling Knowledge Representations for Large Language Model Editing](https://arxiv.org/abs/2505.18774)
Append: [A generalised editor calculus (Short Paper)](https://arxiv.org/abs/2505.18778)
Append: [ALPS: Attention Localization and Pruning Strategy for Efficient Alignment of Large Language Models](https://arxiv.org/abs/2505.18799)
Append: [Don't Look Only Once: Towards Multimodal Interactive Reasoning with Selective Visual Revisitation](https://arxiv.org/abs/2505.18842)
Append: [Multi-Party Conversational Agents: A Survey](https://arxiv.org/abs/2505.18845)
Append: [Smoothie: Smoothing Diffusion on Token Embeddings for Text Generation](https://arxiv.org/abs/2505.18853)
Append: [Writing Like the Best: Exemplar-Based Expository Text Generation](https://arxiv.org/abs/2505.18859)
Append: [Audio Jailbreak Attacks: Exposing Vulnerabilities in SpeechGPT in a White-Box Framework](https://arxiv.org/abs/2505.18864)
Append: [Sci-LoRA: Mixture of Scientific LoRAs for Cross-Domain Lay Paraphrasing](https://arxiv.org/abs/2505.18867)
Append: [CRMArena-Pro: Holistic Assessment of LLM Agents Across Diverse Business Scenarios and Interactions](https://arxiv.org/abs/2505.18878)
Append: [StandUp4AI: A New Multilingual Dataset for Humor Detection in Stand-up Comedy Videos](https://arxiv.org/abs/2505.18903)
Append: [Building a Functional Machine Translation Corpus for Kpelle](https://arxiv.org/abs/2505.18905)
Append: [Federated Retrieval-Augmented Generation: A Systematic Mapping Study](https://arxiv.org/abs/2505.18906)
Append: [SCRum-9: Multilingual Stance Classification over Rumours on Social Media](https://arxiv.org/abs/2505.18916)
Append: [Benchmarking Large Language Models for Cyberbullying Detection in Real-World YouTube Comments](https://arxiv.org/abs/2505.18927)
Append: [MetaMind: Modeling Human Social Thoughts with Metacognitive Multi-Agent Systems](https://arxiv.org/abs/2505.18943)
Append: [The Price of Format: Diversity Collapse in LLMs](https://arxiv.org/abs/2505.18949)
Append: [BnMMLU: Measuring Massive Multitask Language Understanding in Bengali](https://arxiv.org/abs/2505.18951)
Append: [Evaluating AI for Finance: Is AI Credible at Assessing Investment Risk?](https://arxiv.org/abs/2505.18953)
Append: [System-1.5 Reasoning: Traversal in Language and Latent Spaces with Dynamic Shortcuts](https://arxiv.org/abs/2505.18962)
Append: [Learning to Explain: Prototype-Based Surrogate Models for LLM Classification](https://arxiv.org/abs/2505.18970)
Append: [Is Architectural Complexity Overrated? Competitive and Interpretable Knowledge Graph Completion with RelatE](https://arxiv.org/abs/2505.18971)
Append: [Hierarchical Mamba Meets Hyperbolic Geometry: A New Paradigm for Structured Language Embeddings](https://arxiv.org/abs/2505.18973)
Append: [AI4Math: A Native Spanish Benchmark for University-Level Mathematical Reasoning in Large Language Models](https://arxiv.org/abs/2505.18978)
Append: [FiLLM -- A Filipino-optimized Large Language Model based on Southeast Asia Large Language Model (SEALLM)](https://arxiv.org/abs/2505.18995)
Append: [VerIPO: Cultivating Long Reasoning in Video-LLMs via Verifier-Gudied Iterative Policy Optimization](https://arxiv.org/abs/2505.19000)
Append: [CrosGrpsABS: Cross-Attention over Syntactic and Semantic Graphs for Aspect-Based Sentiment Analysis in a Low-Resource Language](https://arxiv.org/abs/2505.19018)
Append: [Efficient Data Selection at Scale via Influence Distillation](https://arxiv.org/abs/2505.19051)
Append: [An Embarrassingly Simple Defense Against LLM Abliteration Attacks](https://arxiv.org/abs/2505.19056)
Append: [UNCERTAINTY-LINE: Length-Invariant Estimation of Uncertainty for Large Language Models](https://arxiv.org/abs/2505.19060)
Append: [Towards Harmonized Uncertainty Estimation for Large Language Models](https://arxiv.org/abs/2505.19073)
Append: [ReadBench: Measuring the Dense Text Visual Reading Ability of Vision-Language Models](https://arxiv.org/abs/2505.19091)
Append: [ASPO: Adaptive Sentence-Level Preference Optimization for Fine-Grained Multimodal Reasoning](https://arxiv.org/abs/2505.19100)
Append: [WHISTRESS: Enriching Transcriptions with Sentence Stress Detection](https://arxiv.org/abs/2505.19103)
Append: [CCHall: A Novel Benchmark for Joint Cross-Lingual and Cross-Modal Hallucinations Detection in Large Language Models](https://arxiv.org/abs/2505.19108)
Append: [Self-Critique Guided Iterative Reasoning for Multi-hop Question Answering](https://arxiv.org/abs/2505.19112)
Append: [Controlling Language Confusion in Multilingual LLMs](https://arxiv.org/abs/2505.19116)
Append: [Delving into Multilingual Ethical Bias: The MSQAD with Statistical Hypothesis Tests for Large Language Models](https://arxiv.org/abs/2505.19121)
Append: [MMATH: A Multilingual Benchmark for Mathematical Reasoning](https://arxiv.org/abs/2505.19126)
Append: [RetrieveAll: A Multilingual Named Entity Recognition Framework with Large Language Models](https://arxiv.org/abs/2505.19128)
Append: [Shifting AI Efficiency From Model-Centric to Data-Centric Compression](https://arxiv.org/abs/2505.19147)
Append: [SpokenNativQA: Multilingual Everyday Spoken Queries for LLMs](https://arxiv.org/abs/2505.19163)
Append: [Assistant-Guided Mitigation of Teacher Preference Bias in LLM-as-a-Judge](https://arxiv.org/abs/2505.19176)
Append: [Two LLMs debate, both are certain they've won](https://arxiv.org/abs/2505.19184)
Append: [LIMOPro: Reasoning Refinement for Efficient and Effective Test-time Scaling](https://arxiv.org/abs/2505.19187)
Append: [Misleading through Inconsistency: A Benchmark for Political Inconsistencies Detection](https://arxiv.org/abs/2505.19191)
Append: [DREAM: Drafting with Refined Target Features and Entropy-Adaptive Cross-Attention Fusion for Multimodal Speculative Decoding](https://arxiv.org/abs/2505.19201)
Append: [SpeakStream: Streaming Text-to-Speech with Interleaved Data](https://arxiv.org/abs/2505.19206)
Append: [MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search](https://arxiv.org/abs/2505.19209)
Append: [When Ethics and Payoffs Diverge: LLM Agents in Morally Charged Social Dilemmas](https://arxiv.org/abs/2505.19212)
Append: [The Overthinker's DIET: Cutting Token Calories with DIfficulty-AwarE Training](https://arxiv.org/abs/2505.19217)
Append: [Evaluating Text Creativity across Diverse Domains: A Dataset and Large Language Model Evaluator](https://arxiv.org/abs/2505.19236)
Append: [LLLMs: A Data-Driven Survey of Evolving Research on Limitations of Large Language Models](https://arxiv.org/abs/2505.19240)
Append: [PATS: Process-Level Adaptive Thinking Mode Switching](https://arxiv.org/abs/2505.19250)
Append: [Unveiling Dual Quality in Product Reviews: An NLP-Based Approach](https://arxiv.org/abs/2505.19254)
Append: [A Graph Perspective to Probe Structural Patterns of Knowledge in Large Language Models](https://arxiv.org/abs/2505.19286)
Append: [100-LongBench: Are de facto Long-Context Benchmarks Literally Evaluating Long-Context Ability?](https://arxiv.org/abs/2505.19293)
Append: [A Necessary Step toward Faithfulness: Measuring and Improving Consistency in Free-Text Explanations](https://arxiv.org/abs/2505.19299)
Append: [SituatedThinker: Grounding LLM Reasoning with Real-World through Situated Thinking](https://arxiv.org/abs/2505.19300)
Append: [PatentScore: Multi-dimensional Evaluation of LLM-Generated Patent Claims](https://arxiv.org/abs/2505.19345)
Append: [GC-KBVQA: A New Four-Stage Framework for Enhancing Knowledge Based Visual Question Answering Performance](https://arxiv.org/abs/2505.19354)
Append: [Estimating Online Influence Needs Causal Modeling! Counterfactual Analysis of Social Media Engagement](https://arxiv.org/abs/2505.19355)
Append: [ChartLens: Fine-grained Visual Attribution in Charts](https://arxiv.org/abs/2505.19360)
Append: [Belief Attribution as Mental Explanation: The Role of Accuracy, Informativity, and Causality](https://arxiv.org/abs/2505.19376)
Append: [GSA-TTS : Toward Zero-Shot Speech Synthesis based on Gradual Style Adaptor](https://arxiv.org/abs/2505.19384)
Append: [gec-metrics: A Unified Library for Grammatical Error Correction Evaluation](https://arxiv.org/abs/2505.19388)
Append: [Simple and Effective Baselines for Code Summarisation Evaluation](https://arxiv.org/abs/2505.19392)
Append: [CoTGuard: Using Chain-of-Thought Triggering for Copyright Protection in Multi-Agent LLM Systems](https://arxiv.org/abs/2505.19405)
Append: [Self-Reflective Planning with Knowledge Graphs: Enhancing LLM Reasoning Reliability for Question Answering](https://arxiv.org/abs/2505.19410)
Append: [The Role of Diversity in In-Context Learning for Large Language Models](https://arxiv.org/abs/2505.19426)
Append: [Frictional Agent Alignment Framework: Slow Down and Don't Break Things](https://arxiv.org/abs/2505.19428)
Append: [Rhapsody: A Dataset for Highlight Detection in Podcasts](https://arxiv.org/abs/2505.19429)
Append: [Deriving Strategic Market Insights with Large Language Models: A Benchmark for Forward Counterfactual Generation](https://arxiv.org/abs/2505.19430)
Append: [Route to Reason: Adaptive Routing for LLM and Reasoning Strategy Selection](https://arxiv.org/abs/2505.19435)
Append: [Surrogate Signals from Format and Length: Reinforcement Learning for Solving Mathematical Problems without Ground Truth Answers](https://arxiv.org/abs/2505.19439)
Append: [The Birth of Knowledge: Emergent Features across Time, Space, and Scale in Large Language Models](https://arxiv.org/abs/2505.19440)
Append: [Balancing Computation Load and Representation Expressivity in Parallel Hybrid Neural Networks](https://arxiv.org/abs/2505.19472)
Append: [Continuous Self-Improvement of Large Language Models by Test-time Training with Verifier-Driven Sample Selection](https://arxiv.org/abs/2505.19475)
Append: [CulFiT: A Fine-grained Cultural-aware LLM Training Paradigm via Multilingual Critique Data Synthesis](https://arxiv.org/abs/2505.19484)
Append: [Anveshana: A New Benchmark Dataset for Cross-Lingual Information Retrieval On English Queries and Sanskrit Documents](https://arxiv.org/abs/2505.19494)
Append: [LLM Meets Scene Graph: Can Large Language Models Understand and Generate Scene Graphs? A Benchmark and Empirical Study](https://arxiv.org/abs/2505.19510)
Append: [Causal Distillation: Transferring Structured Explanations from Large to Compact Language Models](https://arxiv.org/abs/2505.19511)
Append: [SIPDO: Closed-Loop Prompt Optimization via Synthetic Data Feedback](https://arxiv.org/abs/2505.19514)
Append: [Bias in Political Dialogue: Tagging U.S. Presidential Debates with an Extended DAMSL Framework](https://arxiv.org/abs/2505.19515)
Append: [AmpleHate: Amplifying the Attention for Versatile Implicit Hate Detection](https://arxiv.org/abs/2505.19528)
Append: [Small Language Models: Architectures, Techniques, Evaluation, Problems and Future Adaptation](https://arxiv.org/abs/2505.19529)
Append: [DoctorRAG: Medical RAG Fusing Knowledge with Patient Analogy through Textual Gradients](https://arxiv.org/abs/2505.19538)
Append: [How Syntax Specialization Emerges in Language Models](https://arxiv.org/abs/2505.19548)
Append: [Towards Multi-Granularity Memory Association and Selection for Long-Term Conversational Agents](https://arxiv.org/abs/2505.19549)
Append: [DocMEdit: Towards Document-Level Model Editing](https://arxiv.org/abs/2505.19572)
Append: [TailorKV: A Hybrid Framework for Long-Context Inference via Tailored KV Cache Optimization](https://arxiv.org/abs/2505.19586)
Append: [Multi-Agent Collaboration via Evolving Orchestration](https://arxiv.org/abs/2505.19591)
Append: [Evaluating Robustness of Large Audio Language Models to Audio Injection: An Empirical Study](https://arxiv.org/abs/2505.19598)
Append: [Inconsistent Tokenizations Cause Language Models to be Perplexed by Japanese Grammar](https://arxiv.org/abs/2505.19599)
Append: [Evaluating Machine Translation Models for English-Hindi Language Pairs: A Comparative Analysis](https://arxiv.org/abs/2505.19604)
Append: [Languages in Multilingual Speech Foundation Models Align Both Phonetically and Semantically](https://arxiv.org/abs/2505.19606)
Append: [HomeBench: Evaluating LLMs in Smart Homes with Valid and Invalid Instructions Across Single and Multiple Devices](https://arxiv.org/abs/2505.19628)
Append: [DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning System for Multi-Turn Clinical Dialogue](https://arxiv.org/abs/2505.19630)
Append: [Segment First or Comprehend First? Explore the Limit of Unsupervised Word Segmentation with Large Language Models](https://arxiv.org/abs/2505.19631)
Append: [Faster and Better LLMs via Latency-Aware Test-Time Scaling](https://arxiv.org/abs/2505.19634)
Append: [Interleaved Reasoning for Large Language Models via Reinforcement Learning](https://arxiv.org/abs/2505.19640)
Append: [Select, Read, and Write: A Multi-Agent Framework of Full-Text-based Related Work Generation](https://arxiv.org/abs/2505.19647)
Append: [GenKI: Enhancing Open-Domain Question Answering with Knowledge Integration and Controllable Generation in Large Language Models](https://arxiv.org/abs/2505.19660)
Append: [LeCoDe: A Benchmark Dataset for Interactive Legal Consultation Dialogue Evaluation](https://arxiv.org/abs/2505.19667)
Append: [Reshaping Representation Space to Balance the Safety and Over-rejection in Large Audio Language Models](https://arxiv.org/abs/2505.19670)
Append: [Comparing Moral Values in Western English-speaking societies and LLMs with Word Associations](https://arxiv.org/abs/2505.19674)
Append: [Calibrating Pre-trained Language Classifiers on LLM-generated Noisy Labels via Iterative Refinement](https://arxiv.org/abs/2505.19675)
Append: [Grounding Language with Vision: A Conditional Mutual Information Calibrated Decoding Strategy for Reducing Hallucinations in LVLMs](https://arxiv.org/abs/2505.19678)
Append: [KIT's Low-resource Speech Translation Systems for IWSLT2025: System Enhancement with Synthetic Data and Model Regularization](https://arxiv.org/abs/2505.19679)
Append: [Leveraging Importance Sampling to Detach Alignment Modules from Large Language Models](https://arxiv.org/abs/2505.19700)
Append: [Error Typing for Smarter Rewards: Improving Process Reward Models with Error-Aware Hierarchical Supervision](https://arxiv.org/abs/2505.19706)
Append: [MT$^{3}$: Scaling MLLM-based Text Image Machine Translation via Multi-Task Reinforcement Learning](https://arxiv.org/abs/2505.19714)
Append: [Graceful Forgetting in Generative Language Models](https://arxiv.org/abs/2505.19715)
Append: [Distilling Closed-Source LLM's Knowledge for Locally Stable and Economic Biomedical Entity Linking](https://arxiv.org/abs/2505.19722)
Append: [Token-level Accept or Reject: A Micro Alignment Approach for Large Language Models](https://arxiv.org/abs/2505.19743)
Append: [NeuSym-RAG: Hybrid Neural Symbolic Retrieval with Multiview Structuring for PDF Question Answering](https://arxiv.org/abs/2505.19754)
Append: [Efficient Reasoning via Chain of Unconscious Thought](https://arxiv.org/abs/2505.19756)
Append: [SGM: A Framework for Building Specification-Guided Moderation Filters](https://arxiv.org/abs/2505.19766)
Append: [T^2Agent A Tool-augmented Multimodal Misinformation Detection Agent with Monte Carlo Tree Search](https://arxiv.org/abs/2505.19768)
Append: [What Really Matters in Many-Shot Attacks? An Empirical Study of Long-Context Vulnerabilities in LLMs](https://arxiv.org/abs/2505.19773)
Append: [Analyzing Political Bias in LLMs via Target-Oriented Sentiment Classification](https://arxiv.org/abs/2505.19776)
Append: [The Avengers: A Simple Recipe for Uniting Smaller Language Models to Challenge Proprietary Giants](https://arxiv.org/abs/2505.19797)
Append: [MOLE: Metadata Extraction and Validation in Scientific Papers Using LLMs](https://arxiv.org/abs/2505.19800)
Append: [Compliance-to-Code: Enhancing Financial Compliance Checking via Code Generation](https://arxiv.org/abs/2505.19804)
Append: [Exploring Consciousness in LLMs: A Systematic Survey of Theories, Implementations, and Frontier Risks](https://arxiv.org/abs/2505.19806)
Append: [Deciphering Trajectory-Aided LLM Reasoning: An Optimization Perspective](https://arxiv.org/abs/2505.19815)
Append: [FoodTaxo: Generating Food Taxonomies with Large Language Models](https://arxiv.org/abs/2505.19838)
Append: [Improving Multilingual Math Reasoning for African Languages](https://arxiv.org/abs/2505.19848)
Append: [Beyond Specialization: Benchmarking LLMs for Transliteration of Indian Languages](https://arxiv.org/abs/2505.19851)
Append: [REA-RL: Reflection-Aware Online Reinforcement Learning for Efficient Large Reasoning Models](https://arxiv.org/abs/2505.19862)
Append: [APE: A Data-Centric Benchmark for Efficient LLM Adaptation in Text Summarization](https://arxiv.org/abs/2505.19912)
Append: [Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles](https://arxiv.org/abs/2505.19914)
Append: [ALAS: Measuring Latent Speech-Text Alignment For Spoken Language Understanding In Multimodal LLMs](https://arxiv.org/abs/2505.19937)
Append: [MiniLongBench: The Low-cost Long Context Understanding Benchmark for Large Language Models](https://arxiv.org/abs/2505.19959)
Append: [CP-Router: An Uncertainty-Aware Router Between LLM and LRM](https://arxiv.org/abs/2505.19970)
Append: [Conversational Lexicography: Querying Lexicographic Data on Knowledge Graphs with SPARQL through Natural Language](https://arxiv.org/abs/2505.19971)
Append: [DeepDialogue: A Multi-Turn Emotionally-Rich Spoken Dialogue Dataset](https://arxiv.org/abs/2505.19978)
Append: [How Well Do Large Reasoning Models Translate? A Comprehensive Evaluation for Multi-Domain Machine Translation](https://arxiv.org/abs/2505.19987)
Append: [Mixture of LoRA Experts for Low-Resourced Multi-Accent Automatic Speech Recognition](https://arxiv.org/abs/2505.20006)
Append: [WebCoT: Enhancing Web Agent Reasoning by Reconstructing Chain-of-Thought in Reflection, Branching, and Rollback](https://arxiv.org/abs/2505.20013)
Append: [Does Rationale Quality Matter? Enhancing Mental Disorder Detection via Selective Reasoning Distillation](https://arxiv.org/abs/2505.20014)
Append: [On the class of coding optimality of human languages and the origins of Zipf's law](https://arxiv.org/abs/2505.20015)
Append: [TTPA: Token-level Tool-use Preference Alignment Training Framework with Fine-grained Evaluation](https://arxiv.org/abs/2505.20016)
Append: [Training LLM-Based Agents with Synthetic Self-Reflected Trajectories and Partial Masking](https://arxiv.org/abs/2505.20023)
Append: [Uncertainty-Aware Attention Heads: Efficient Unsupervised Uncertainty Quantification for LLMs](https://arxiv.org/abs/2505.20045)
Append: [Grammars of Formal Uncertainty: When to Trust LLMs in Automated Reasoning Tasks](https://arxiv.org/abs/2505.20047)
Append: [Incentivizing Reasoning from Weak Supervision](https://arxiv.org/abs/2505.20072)
Append: [Inference-time Alignment in Continuous Space](https://arxiv.org/abs/2505.20081)
Append: [Multi-Domain Explainability of Preferences](https://arxiv.org/abs/2505.20088)
Append: [MA-RAG: Multi-Agent Retrieval-Augmented Generation via Collaborative Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.20096)
Append: [S2LPP: Small-to-Large Prompt Prediction across LLMs](https://arxiv.org/abs/2505.20097)
Append: [Large Language Models Meet Knowledge Graphs for Question Answering: Synthesis and Opportunities](https://arxiv.org/abs/2505.20099)
Append: [Adaptive Deep Reasoning: Triggering Deep Thinking When Needed](https://arxiv.org/abs/2505.20101)
Append: [Language-Agnostic Suicidal Risk Detection Using Large Language Models](https://arxiv.org/abs/2505.20109)
Append: [ResSVD: Residual Compensated SVD for Large Language Model Compression](https://arxiv.org/abs/2505.20112)
Append: [Named Entity Recognition in Historical Italian: The Case of Giacomo Leopardi's Zibaldone](https://arxiv.org/abs/2505.20113)
Append: [TrojanStego: Your Language Model Can Secretly Be A Steganographic Privacy Leaking Agent](https://arxiv.org/abs/2505.20118)
Append: [Iterative Self-Incentivization Empowers Large Language Models as Agentic Searchers](https://arxiv.org/abs/2505.20128)
Append: [AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings](https://arxiv.org/abs/2505.20133)
Append: [SeMe: Training-Free Language Model Merging via Semantic Alignment](https://arxiv.org/abs/2505.20144)
Append: [UORA: Uniform Orthogonal Reinitialization Adaptation in Parameter-Efficient Fine-Tuning of Large Models](https://arxiv.org/abs/2505.20154)
Append: [Pangu Light: Weight Re-Initialization for Pruning and Accelerating LLMs](https://arxiv.org/abs/2505.20155)
Append: [Exploring Generative Error Correction for Dysarthric Speech Recognition](https://arxiv.org/abs/2505.20163)
Append: [Visual Abstract Thinking Empowers Multimodal Reasoning](https://arxiv.org/abs/2505.20164)
Append: ["KAN you hear me?" Exploring Kolmogorov-Arnold Networks for Spoken Language Understanding](https://arxiv.org/abs/2505.20176)
Append: [THiNK: Can Large Language Models Think-aloud?](https://arxiv.org/abs/2505.20184)
Append: [Monocle: Hybrid Local-Global In-Context Evaluation for Long-Text Generation with Uncertainty-Based Active Learning](https://arxiv.org/abs/2505.20195)
Append: [Adaptive Classifier-Free Guidance via Dynamic Low-Confidence Masking](https://arxiv.org/abs/2505.20199)
Append: [Reasoning Is Not All You Need: Examining LLMs for Multi-Turn Mental Health Conversations](https://arxiv.org/abs/2505.20201)
Append: [How to Improve the Robustness of Closed-Source Models on NLI](https://arxiv.org/abs/2505.20209)
Append: [Dependency Parsing is More Parameter-Efficient with Normalization](https://arxiv.org/abs/2505.20215)
Append: [FLAME-MoE: A Transparent End-to-End Research Platform for Mixture-of-Experts Language Models](https://arxiv.org/abs/2505.20225)
Append: [Bridging the Long-Term Gap: A Memory-Active Policy for Multi-Session Task-Oriented Dialogue](https://arxiv.org/abs/2505.20231)
Append: [Efficient Speech Translation through Model Compression and Knowledge Distillation](https://arxiv.org/abs/2505.20237)
Append: [It's High Time: A Survey of Temporal Information Retrieval and Question Answering](https://arxiv.org/abs/2505.20243)
Append: [KnowTrace: Bootstrapping Iterative Retrieval-Augmented Generation with Structured Knowledge Tracing](https://arxiv.org/abs/2505.20245)
Append: [WXImpactBench: A Disruptive Weather Impact Understanding Benchmark for Evaluating Large Language Models](https://arxiv.org/abs/2505.20249)
Append: [ARM: Adaptive Reasoning Model](https://arxiv.org/abs/2505.20258)
Append: [We Need to Measure Data Diversity in NLP -- Better and Broader](https://arxiv.org/abs/2505.20264)
Append: [Does quantization affect models' performance on long-context tasks?](https://arxiv.org/abs/2505.20276)
Append: [OmniCharacter: Towards Immersive Role-Playing Agents with Seamless Speech-Language Personality Interaction](https://arxiv.org/abs/2505.20277)
Append: [One-shot Entropy Minimization](https://arxiv.org/abs/2505.20282)
Append: [MASKSEARCH: A Universal Pre-Training Framework to Enhance Agentic Search Capability](https://arxiv.org/abs/2505.20285)
Append: [Enhancing the Comprehensibility of Text Explanations via Unsupervised Concept Discovery](https://arxiv.org/abs/2505.20293)
Append: [Self-reflective Uncertainties: Do LLMs Know Their Internal Answer Distribution?](https://arxiv.org/abs/2505.20295)
Append: [Reasoning LLMs are Wandering Solution Explorers](https://arxiv.org/abs/2505.20296)
Append: [MangaVQA and MangaLMM: A Benchmark and Specialized Model for Multimodal Manga Understanding](https://arxiv.org/abs/2505.20298)
Append: [Training Acceleration of Low-Rank Decomposed Networks using Sequential Freezing and Rank Quantization](https://arxiv.org/abs/2309.03824)
Append: [Improving Resnet-9 Generalization Trained on Small Datasets](https://arxiv.org/abs/2309.03965)
Append: [GQKVA: Efficient Pre-training of Transformers by Grouping Queries, Keys, and Values](https://arxiv.org/abs/2311.03426)
Append: [News Without Borders: Domain Adaptation of Multilingual Sentence Embeddings for Cross-lingual News Recommendation](https://arxiv.org/abs/2406.12634)
Append: [Accelerating the Low-Rank Decomposed Models](https://arxiv.org/abs/2407.20266)
Append: [Walk&Retrieve: Simple Yet Effective Zero-shot Retrieval-Augmented Generation via Knowledge Graph Walks](https://arxiv.org/abs/2505.16849)
Append: [Towards medical AI misalignment: a preliminary study](https://arxiv.org/abs/2505.18212)
Append: [Evidence-Grounded Multimodal Misinformation Detection with Attention-Based GNNs](https://arxiv.org/abs/2505.18221)
Append: [ELDeR: Getting Efficient LLMs through Data-Driven Regularized Layer-wise Pruning](https://arxiv.org/abs/2505.18232)
Append: [Will Large Language Models Transform Clinical Prediction?](https://arxiv.org/abs/2505.18246)
Append: [Collaborative Memory: Multi-User Memory Sharing in LLM Agents with Dynamic Access Control](https://arxiv.org/abs/2505.18279)
Append: [Task Specific Pruning with LLM-Sieve: How Many Parameters Does Your Task Really Need?](https://arxiv.org/abs/2505.18350)
Append: [Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems](https://arxiv.org/abs/2505.18366)
Append: [LatentLLM: Attention-Aware Joint Tensor Compression](https://arxiv.org/abs/2505.18413)
Append: [$\mu$-MoE: Test-Time Pruning as Micro-Grained Mixture-of-Experts](https://arxiv.org/abs/2505.18451)
Append: [A Survey of LLM $\times$ DATA](https://arxiv.org/abs/2505.18458)
Append: [From Reddit to Generative AI: Evaluating Large Language Models for Anxiety Support Fine-tuned on Social Media Data](https://arxiv.org/abs/2505.18464)
Append: [Pedagogy-R1: Pedagogically-Aligned Reasoning Model with Balanced Educational Benchmark](https://arxiv.org/abs/2505.18467)
Append: [Synthesizing and Adapting Error Correction Data for Mobile Large Language Model Applications](https://arxiv.org/abs/2505.18488)
Append: [Knowledge Grafting of Large Language Models](https://arxiv.org/abs/2505.18502)
Append: [AcuRank: Uncertainty-Aware Adaptive Computation for Listwise Reranking](https://arxiv.org/abs/2505.18512)
Append: [B-score: Detecting biases in large language models using response history](https://arxiv.org/abs/2505.18545)
Append: [Enhancing Efficiency and Exploration in Reinforcement Learning for LLMs](https://arxiv.org/abs/2505.18573)
Append: [RvLLM: LLM Runtime Verification with Domain Knowledge](https://arxiv.org/abs/2505.18585)
Append: [Enhancing Generalization of Speech Large Language Models with Multi-Task Behavior Imitation and Speech-Text Interleaving](https://arxiv.org/abs/2505.18644)
Append: [SEW: Self-Evolving Agentic Workflows for Automated Code Generation](https://arxiv.org/abs/2505.18646)
Append: [ChartGalaxy: A Dataset for Infographic Chart Understanding and Generation](https://arxiv.org/abs/2505.18668)
Append: [Can MLLMs Guide Me Home? A Benchmark Study on Fine-Grained Visual Reasoning from Transit Maps](https://arxiv.org/abs/2505.18675)
Append: [$PD^3F$: A Pluggable and Dynamic DoS-Defense Framework Against Resource Consumption Attacks Targeting Large Language Models](https://arxiv.org/abs/2505.18680)
Append: [Neural Parameter Search for Slimmer Fine-Tuned Models and Better Transfer](https://arxiv.org/abs/2505.18713)
Append: [Evaluating the Usefulness of Non-Diagnostic Speech Data for Developing Parkinson's Disease Classifiers](https://arxiv.org/abs/2505.18722)
Append: [From Output to Evaluation: Does Raw Instruction-Tuned Code LLMs Output Suffice for Fill-in-the-Middle Code Generation?](https://arxiv.org/abs/2505.18789)
Append: [AdaCtrl: Towards Adaptive and Controllable Reasoning via Difficulty-Aware Budgeting](https://arxiv.org/abs/2505.18822)
Append: [On the Effect of Negative Gradient in Group Relative Deep Reinforcement Optimization](https://arxiv.org/abs/2505.18830)
Append: [Signal, Image, or Symbolic: Exploring the Best Input Representation for Electrocardiogram-Language Models Through a Unified Framework](https://arxiv.org/abs/2505.18847)
Append: [Inference Compute-Optimal Video Vision Language Models](https://arxiv.org/abs/2505.18855)
Append: [Meta-aware Learning in text-to-SQL Large Language Model](https://arxiv.org/abs/2505.18929)
Append: [Can Large Language Models Infer Causal Relationships from Real-World Text?](https://arxiv.org/abs/2505.18931)
Append: [REACT: Representation Extraction And Controllable Tuning to Overcome Overfitting in LLM Knowledge Editing](https://arxiv.org/abs/2505.18933)
Append: [Language Models Surface the Unwritten Code of Science and Society](https://arxiv.org/abs/2505.18942)
Append: [STRICT: Stress Test of Rendering Images Containing Text](https://arxiv.org/abs/2505.18985)
Append: [Co-AttenDWG: Co-Attentive Dimension-Wise Gating and Expert Fusion for Multi-Modal Offensive Content Detection](https://arxiv.org/abs/2505.19010)
Append: [SQUiD: Synthesizing Relational Databases from Unstructured Text](https://arxiv.org/abs/2505.19025)
Append: [Speech-IFEval: Evaluating Instruction-Following and Quantifying Catastrophic Forgetting in Speech-Aware Language Models](https://arxiv.org/abs/2505.19037)
Append: [Universal Reasoner: A Single, Composable Plug-and-Play Reasoner for Frozen LLMs](https://arxiv.org/abs/2505.19075)
Append: [Sparse-to-Dense: A Free Lunch for Lossless Acceleration of Video Understanding in LLMs](https://arxiv.org/abs/2505.19155)
Append: [GUARDIAN: Safeguarding LLM Multi-Agent Collaborations with Temporal Graph Modeling](https://arxiv.org/abs/2505.19234)
Append: [Next Token Prediction Is a Dead End for Creativity](https://arxiv.org/abs/2505.19277)
Append: [Towards Reliable Large Audio Language Model](https://arxiv.org/abs/2505.19294)
Append: [ODIN: A NL2SQL Recommender to Handle Schema Ambiguity](https://arxiv.org/abs/2505.19302)
Append: [Architectures of Error: A Philosophical Inquiry into AI and Human Code Generation](https://arxiv.org/abs/2505.19353)
Append: [Optimized Text Embedding Models and Benchmarks for Amharic Passage Retrieval](https://arxiv.org/abs/2505.19356)
Append: [Task Memory Engine: Spatial Memory for Robust Multi-Step LLM Agents](https://arxiv.org/abs/2505.19436)
Append: [Vibe Coding vs. Agentic Coding: Fundamentals and Practical Implications of Agentic AI](https://arxiv.org/abs/2505.19443)
Append: [BizFinBench: A Business-Driven Real-World Financial Benchmark for Evaluating LLMs](https://arxiv.org/abs/2505.19457)
Append: [DOGe: Defensive Output Generation for LLM Protection Against Knowledge Distillation](https://arxiv.org/abs/2505.19504)
Append: [FlowCut: Rethinking Redundancy via Information Flow for Efficient Vision-Language Models](https://arxiv.org/abs/2505.19536)
Append: [Automated Text-to-Table for Reasoning-Intensive Table QA: Pipeline Design and Benchmarking Insights](https://arxiv.org/abs/2505.19563)
Append: [Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing](https://arxiv.org/abs/2505.19578)
Append: [Learning to Reason without External Rewards](https://arxiv.org/abs/2505.19590)
Append: [Preference Optimization by Estimating the Ratio of the Data Distribution](https://arxiv.org/abs/2505.19601)
Append: [Think Again! The Effect of Test-Time Compute on Preferences, Opinions, and Beliefs of Large Language Models](https://arxiv.org/abs/2505.19621)
Append: [SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond](https://arxiv.org/abs/2505.19641)
Append: [Large Language Models for Planning: A Comprehensive and Systematic Survey](https://arxiv.org/abs/2505.19683)
Append: [Discrete Markov Bridge](https://arxiv.org/abs/2505.19752)
Append: [CIDRe: A Reference-Free Multi-Aspect Criterion for Code Comment Quality Measurement](https://arxiv.org/abs/2505.19757)
Append: [Understanding the Performance Gap in Preference Learning: A Dichotomy of RLHF and DPO](https://arxiv.org/abs/2505.19770)
Append: [HS-STAR: Hierarchical Sampling for Self-Taught Reasoners via Difficulty Estimation and Budget Reallocation](https://arxiv.org/abs/2505.19866)
Append: [ESLM: Risk-Averse Selective Language Modeling for Efficient Pretraining](https://arxiv.org/abs/2505.19893)
Append: [Large Language Models as Autonomous Spacecraft Operators in Kerbal Space Program](https://arxiv.org/abs/2505.19896)
Append: [ScienceBoard: Evaluating Multimodal Autonomous Agents in Realistic Scientific Workflows](https://arxiv.org/abs/2505.19897)
Append: [Can Visual Encoder Learn to See Arrows?](https://arxiv.org/abs/2505.19944)
Append: [An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning](https://arxiv.org/abs/2505.19954)
Append: [MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research](https://arxiv.org/abs/2505.19955)
Append: [DCG-SQL: Enhancing In-Context Learning for Text-to-SQL with Deep Contextual Schema Link Graph](https://arxiv.org/abs/2505.19956)
Append: [The Limits of Preference Data for Post-Training](https://arxiv.org/abs/2505.19964)
Append: [Embracing Imperfection: Simulating Students with Diverse Cognitive Levels Using LLM-based Agents](https://arxiv.org/abs/2505.19997)
Append: [Multi-modal brain encoding models for multi-modal stimuli](https://arxiv.org/abs/2505.20027)
Append: [REARANK: Reasoning Re-ranking Agent via Reinforcement Learning](https://arxiv.org/abs/2505.20046)
Append: [MVP: Multi-source Voice Pathology detection](https://arxiv.org/abs/2505.20050)
Append: [Multimodal LLM-Guided Semantic Correction in Text-to-Image Diffusion](https://arxiv.org/abs/2505.20053)
Append: [SAEs Are Good for Steering -- If You Select the Right Features](https://arxiv.org/abs/2505.20063)
Append: [Safety Through Reasoning: An Empirical Study of Reasoning Guardrail Models](https://arxiv.org/abs/2505.20087)
Append: [SCIRGC: Multi-Granularity Citation Recommendation and Citation Sentence Preference Alignment](https://arxiv.org/abs/2505.20103)
Append: [StructEval: Benchmarking LLMs' Capabilities to Generate Structural Outputs](https://arxiv.org/abs/2505.20139)
Append: [Hard Negative Contrastive Learning for Fine-Grained Geometric Understanding in Large Multimodal Models](https://arxiv.org/abs/2505.20152)
Append: [Prismatic Synthesis: Gradient-based Data Diversification Boosts Generalization in LLM Reasoning](https://arxiv.org/abs/2505.20161)
Append: [From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data](https://arxiv.org/abs/2505.20166)
Append: [On Path to Multimodal Historical Reasoning: HistBench and HistAgent](https://arxiv.org/abs/2505.20246)
Append: [Learning Extrapolative Sequence Transformations from Markov Chains](https://arxiv.org/abs/2505.20251)
Append: [Position: Mechanistic Interpretability Should Prioritize Feature Consistency in SAEs](https://arxiv.org/abs/2505.20254)
Append: [Lifelong Safety Alignment for Language Models](https://arxiv.org/abs/2505.20259)
Append: [The Coverage Principle: A Framework for Understanding Compositional Generalization](https://arxiv.org/abs/2505.20278)
Append: [VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D Reconstruction](https://arxiv.org/abs/2505.20279)
Append: [Visualized Text-to-Image Retrieval](https://arxiv.org/abs/2505.20291)
Append: [DiSA: Diffusion Step Annealing in Autoregressive Image Generation](https://arxiv.org/abs/2505.20297)
Append: [AtteSTNet -- An attention and subword tokenization based approach for code-switched text hate speech detection](https://arxiv.org/abs/2112.11479)
Append: [ADEPT: A DEbiasing PrompT Framework](https://arxiv.org/abs/2211.05414)
Append: [The More Similar, the Better? Associations between Latent Semantic Similarity and Emotional Experiences Differ across Conversation Contexts](https://arxiv.org/abs/2309.12646)
Append: [Exploring the Impact of Corpus Diversity on Financial Pretrained Language Models](https://arxiv.org/abs/2310.13312)
Append: [Unearthing Large Scale Domain-Specific Knowledge from Public Corpora](https://arxiv.org/abs/2401.14624)
Append: [Can Large Language Models be Good Emotional Supporter? Mitigating Preference Bias on Emotional Support Conversation](https://arxiv.org/abs/2402.13211)
Append: [A Unified Taxonomy-Guided Instruction Tuning Framework for Entity Set Expansion and Taxonomy Expansion](https://arxiv.org/abs/2402.13405)
Append: [MlingConf: A Comprehensive Study of Multilingual Confidence Estimation on Large Language Models](https://arxiv.org/abs/2402.13606)
Append: [Bias and Volatility: A Statistical Framework for Evaluating Large Language Model's Stereotypes and the Associated Generation Inconsistency](https://arxiv.org/abs/2402.15481)
Append: [MELoRA: Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2402.17263)
Append: [Less for More: Enhanced Feedback-aligned Mixed LLMs for Molecule Caption Generation and Fine-Grained NLI Evaluation](https://arxiv.org/abs/2405.13984)
Append: [UniICL: An Efficient Unified Framework Unifying Compression, Selection, and Generation](https://arxiv.org/abs/2405.17062)
Append: [Toward Reliable Ad-hoc Scientific Information Extraction: A Case Study on Two Materials Datasets](https://arxiv.org/abs/2406.05348)
Append: [Unveiling the Power of Source: Source-based Minimum Bayes Risk Decoding for Neural Machine Translation](https://arxiv.org/abs/2406.11632)
Append: [USDC: A Dataset of $\underline{U}$ser $\underline{S}$tance and $\underline{D}$ogmatism in Long $\underline{C}$onversations](https://arxiv.org/abs/2406.16833)
Append: [Can Large Language Models Generate High-quality Patent Claims?](https://arxiv.org/abs/2406.19465)
Append: [The Impact of LoRA Adapters for LLMs on Clinical NLP Classification Under Data Limitations](https://arxiv.org/abs/2407.19299)
Append: [Turning Trash into Treasure: Accelerating Inference of Large Language Models with Token Recycling](https://arxiv.org/abs/2408.08696)
Append: [CodeTaxo: Enhancing Taxonomy Expansion with Limited Examples via Code Language Prompts](https://arxiv.org/abs/2408.09070)
Append: [Language Models Benefit from Preparation with Elicited Knowledge](https://arxiv.org/abs/2409.01345)
Append: [The Faetar Benchmark: Speech Recognition in a Very Under-Resourced Language](https://arxiv.org/abs/2409.08103)
Append: [Identifying Knowledge Editing Types in Large Language Models](https://arxiv.org/abs/2409.19663)
Append: [QAEncoder: Towards Aligned Representation Learning in Question Answering System](https://arxiv.org/abs/2409.20434)
Append: [Do Vision-Language Models Really Understand Visual Language?](https://arxiv.org/abs/2410.00193)
Append: [In-context Demonstration Matters: On Prompt Optimization for Pseudo-Supervision Refinement](https://arxiv.org/abs/2410.03124)
Append: [Lens: Rethinking Multilingual Enhancement for Large Language Models](https://arxiv.org/abs/2410.04407)
Append: [PII-Scope: A Comprehensive Study on Training Data PII Extraction Attacks in LLMs](https://arxiv.org/abs/2410.06704)
Append: [Stuffed Mamba: Oversized States Lead to the Inability to Forget](https://arxiv.org/abs/2410.07145)
Append: [LLMs know their vulnerabilities: Uncover Safety Gaps through Natural Distribution Shifts](https://arxiv.org/abs/2410.10700)
Append: [Reversal of Thought: Enhancing Large Language Models with Preference-Guided Reverse Reasoning Warm-up](https://arxiv.org/abs/2410.12323)
Append: [Conformity in Large Language Models](https://arxiv.org/abs/2410.12428)
Append: [SynapticRAG: Enhancing Temporal Memory Retrieval in Large Language Models through Synaptic Mechanisms](https://arxiv.org/abs/2410.13553)
Append: [RESTOR: Knowledge Recovery in Machine Unlearning](https://arxiv.org/abs/2411.00204)
Append: [Regress, Don't Guess -- A Regression-like Loss on Number Tokens for Language Models](https://arxiv.org/abs/2411.02083)
Append: [Attacking Vision-Language Computer Agents via Pop-ups](https://arxiv.org/abs/2411.02391)
Append: [Benchmarking Multimodal Retrieval Augmented Generation with Dynamic VQA Dataset and Self-adaptive Planning Agent](https://arxiv.org/abs/2411.02937)
Append: [Contextualized Evaluations: Judging Language Model Responses to Underspecified Queries](https://arxiv.org/abs/2411.07237)
Append: [SHARP: Unlocking Interactive Hallucination via Stance Transfer in Role-Playing LLMs](https://arxiv.org/abs/2411.07965)
Append: [On the Compatibility of Generative AI and Generative Linguistics](https://arxiv.org/abs/2411.10533)
Append: [Is Training Data Quality or Quantity More Impactful to Small Language Model Performance?](https://arxiv.org/abs/2411.15821)
Append: [Enhancing Character-Level Understanding in LLMs through Token Internal Structure Learning](https://arxiv.org/abs/2411.17679)
Append: [Can LLMs assist with Ambiguity? A Quantitative Evaluation of various Large Language Models on Word Sense Disambiguation](https://arxiv.org/abs/2411.18337)
Append: [Patent-CR: A Dataset for Patent Claim Revision](https://arxiv.org/abs/2412.02549)
Append: [Interpretable Company Similarity with Sparse Autoencoders](https://arxiv.org/abs/2412.02605)
Append: [HARP: Hesitation-Aware Reframing in Transformer Inference Pass](https://arxiv.org/abs/2412.07282)
Append: [On the Limit of Language Models as Planning Formalizers](https://arxiv.org/abs/2412.09879)
Append: [MALAMUTE: A Multilingual, Highly-granular, Template-free, Education-based Probing Dataset](https://arxiv.org/abs/2412.10105)
Append: [Rethinking Chain-of-Thought from the Perspective of Self-Training](https://arxiv.org/abs/2412.10827)
Append: [Separate the Wheat from the Chaff: A Post-Hoc Approach to Safety Re-Alignment for Fine-Tuned Language Models](https://arxiv.org/abs/2412.11041)
Append: [Segment-Level Diffusion: A Framework for Controllable Long-Form Generation with Diffusion Language Models](https://arxiv.org/abs/2412.11333)
Append: [What External Knowledge is Preferred by LLMs? Characterizing and Exploring Chain of Evidence in Imperfect Context for Multi-Hop QA](https://arxiv.org/abs/2412.12632)
Append: [Expansion Span: Combining Fading Memory and Retrieval in Hybrid State Space Models](https://arxiv.org/abs/2412.13328)
Append: [EscapeBench: Towards Advancing Creative Intelligence of Language Model Agents](https://arxiv.org/abs/2412.13549)
Append: [Crabs: Consuming Resource via Auto-generation for LLM-DoS Attack under Black-box Settings](https://arxiv.org/abs/2412.13879)
Append: [How to Synthesize Text Data without Model Collapse?](https://arxiv.org/abs/2412.14689)
Append: [DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context LLMs](https://arxiv.org/abs/2412.14838)
Append: [ComparisonQA: Evaluating Factuality Robustness of LLMs Through Knowledge Frequency Control and Uncertainty](https://arxiv.org/abs/2412.20251)
Append: [Registering Source Tokens to Target Language Spaces in Multilingual Neural Machine Translation](https://arxiv.org/abs/2501.02979)
Append: [OpenOmni: Advancing Open-Source Omnimodal Large Language Models with Progressive Multimodal Alignment and Real-Time Self-Aware Emotional Speech Synthesis](https://arxiv.org/abs/2501.04561)
Append: [A partition cover approach to tokenization](https://arxiv.org/abs/2501.06246)
Append: [Language Fusion for Parameter-Efficient Cross-lingual Transfer](https://arxiv.org/abs/2501.06892)
Append: [Domain Adaptation of Foundation LLMs for e-Commerce](https://arxiv.org/abs/2501.09706)
Append: [iTool: Reinforced Fine-Tuning with Dynamic Deficiency Calibration for Advanced Tool Use](https://arxiv.org/abs/2501.09766)
Append: [Each Graph is a New Language: Graph Learning with LLMs](https://arxiv.org/abs/2501.11478)
Append: [NExtLong: Toward Effective Long-Context Training without Long Documents](https://arxiv.org/abs/2501.12766)
Append: [Think Outside the Data: Colonial Biases and Systemic Issues in Automated Moderation Pipelines for Low-Resource Languages](https://arxiv.org/abs/2501.13836)
Append: [Token Sampling Uncertainty Does Not Explain Homogeneity Bias in Large Language Models](https://arxiv.org/abs/2501.19337)
Append: [A Checks-and-Balances Framework for Context-Aware Ethical AI Alignment](https://arxiv.org/abs/2502.00136)
Append: [UGPhysics: A Comprehensive Benchmark for Undergraduate Physics Reasoning with Large Language Models](https://arxiv.org/abs/2502.00334)
Append: [A statistically consistent measure of semantic uncertainty using Language Models](https://arxiv.org/abs/2502.00507)
Append: [SMI: An Information-Theoretic Metric for Predicting Model Knowledge Solely from Pre-Training Signals](https://arxiv.org/abs/2502.04066)
Append: [JingFang: An Expert-Level Large Language Model for Traditional Chinese Medicine Clinical Consultation and Syndrome Differentiation-Based Treatment](https://arxiv.org/abs/2502.04345)
Append: [DECT: Harnessing LLM-assisted Fine-Grained Linguistic Knowledge and Label-Switched and Label-Preserved Data Generation for Diagnosis of Alzheimer's Disease](https://arxiv.org/abs/2502.04394)
Append: [Group-Adaptive Threshold Optimization for Robust AI-Generated Text Detection](https://arxiv.org/abs/2502.04528)
Append: [Aligning Large Language Models to Follow Instructions and Hallucinate Less via Effective Data Filtering](https://arxiv.org/abs/2502.07340)
Append: [What Is That Talk About? A Video-to-Text Summarization Dataset for Scientific Presentations](https://arxiv.org/abs/2502.08279)
Append: [SelfElicit: Your Language Model Secretly Knows Where is the Relevant Evidence](https://arxiv.org/abs/2502.08767)
Append: [A Survey of LLM-based Agents in Medicine: How far are we from Baymax?](https://arxiv.org/abs/2502.11211)
Append: [HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning](https://arxiv.org/abs/2502.11393)
Append: [Investigating Inference-time Scaling for Chain of Multi-modal Thought: A Preliminary Study](https://arxiv.org/abs/2502.11514)
Append: [Can LLM Watermarks Robustly Prevent Unauthorized Knowledge Distillation?](https://arxiv.org/abs/2502.11598)
Append: [ReviewEval: An Evaluation Framework for AI-Generated Reviews](https://arxiv.org/abs/2502.11736)
Append: [Balancing Truthfulness and Informativeness with Uncertainty-Aware Instruction Fine-Tuning](https://arxiv.org/abs/2502.11962)
Append: [How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines](https://arxiv.org/abs/2502.12051)
Append: [TokenSkip: Controllable Chain-of-Thought Compression in LLMs](https://arxiv.org/abs/2502.12067)
Append: [Evaluating Step-by-step Reasoning Traces: A Survey](https://arxiv.org/abs/2502.12289)
Append: [A Cognitive Writing Perspective for Constrained Long-Form Text Generation](https://arxiv.org/abs/2502.12568)
Append: [Speech-FT: Merging Pre-trained And Fine-Tuned Speech Representation Models For Cross-Task Generalization](https://arxiv.org/abs/2502.12672)
Append: [Fraud-R1 : A Multi-Round Benchmark for Assessing the Robustness of LLM Against Augmented Fraud and Phishing Inducements](https://arxiv.org/abs/2502.12904)
Append: [Conditioning LLMs to Generate Code-Switched Text](https://arxiv.org/abs/2502.12924)
Append: [Natural Language Generation from Visual Events: Challenges and Future Directions](https://arxiv.org/abs/2502.13034)
Append: [Training Turn-by-Turn Verifiers for Dialogue Tutoring Agents: The Curious Case of LLMs as Your Coding Tutors](https://arxiv.org/abs/2502.13311)
Append: [iAgent: LLM Agent as a Shield between User and Recommender Systems](https://arxiv.org/abs/2502.14662)
Append: [A Tale of Two Structures: Do LLMs Capture the Fractal Complexity of Language?](https://arxiv.org/abs/2502.14924)
Append: [Judging It, Washing It: Scoring and Greenwashing Corporate Climate Disclosures using Large Language Models](https://arxiv.org/abs/2502.15094)
Append: [Does Reasoning Introduce Bias? A Study of Social Bias Evaluation and Mitigation in LLM Reasoning](https://arxiv.org/abs/2502.15361)
Append: [GraphCheck: Breaking Long-Term Text Barriers with Extracted Knowledge Graph-Powered Fact-Checking](https://arxiv.org/abs/2502.16514)
Append: [CORAL: Learning Consistent Representations across Multi-step Training with Lighter Speculative Drafter](https://arxiv.org/abs/2502.16880)
Append: [Cheems: A Practical Guidance for Building and Evaluating Chinese Reward Models from Scratch](https://arxiv.org/abs/2502.17173)
Append: [Unveiling the Key Factors for Distilling Chain-of-Thought Reasoning](https://arxiv.org/abs/2502.18001)
Append: [Can LLMs Help Uncover Insights about LLMs? A Large-Scale, Evolving Literature Analysis of Frontier LLMs](https://arxiv.org/abs/2502.18791)
Append: [Exploring the Generalizability of Factual Hallucination Mitigation via Enhancing Precise Knowledge Utilization](https://arxiv.org/abs/2502.19127)
Append: [Amulet: ReAlignment During Test Time for Personalized Preference Adaptation of LLMs](https://arxiv.org/abs/2502.19148)
Append: [R1-T1: Fully Incentivizing Translation Capability in LLMs via Reasoning Learning](https://arxiv.org/abs/2502.19735)
Append: [GeoEdit: Geometric Knowledge Editing for Large Language Models](https://arxiv.org/abs/2502.19953)
Append: [PersuasiveToM: A Benchmark for Evaluating Machine Theory of Mind in Persuasive Dialogues](https://arxiv.org/abs/2502.21017)
Append: [Detecting LLM-Generated Korean Text through Linguistic Feature Analysis](https://arxiv.org/abs/2503.00032)
Append: [Retrieval Models Aren't Tool-Savvy: Benchmarking Tool Retrieval for Large Language Models](https://arxiv.org/abs/2503.01763)
Append: [SteerConf: Steering LLMs for Confidence Elicitation](https://arxiv.org/abs/2503.02863)
Append: [LINGOLY-TOO: Disentangling Memorisation from Knowledge with Linguistic Templatisation and Orthographic Obfuscation](https://arxiv.org/abs/2503.02972)
Append: [Not-Just-Scaling Laws: Towards a Better Understanding of the Downstream Impact of Language Model Design Decisions](https://arxiv.org/abs/2503.03862)
Append: [DiffPO: Diffusion-styled Preference Optimization for Efficient Inference-Time Alignment of Large Language Models](https://arxiv.org/abs/2503.04240)
Append: [One-Shot is Enough: Consolidating Multi-Turn Attacks into Efficient Single-Turn Prompts for LLMs](https://arxiv.org/abs/2503.04856)
Append: [InftyThink: Breaking the Length Limits of Long-Context Reasoning in Large Language Models](https://arxiv.org/abs/2503.06692)
Append: [MoC: Mixtures of Text Chunking Learners for Retrieval-Augmented Generation System](https://arxiv.org/abs/2503.09600)
Append: [MMLU-ProX: A Multilingual Benchmark for Advanced Large Language Model Evaluation](https://arxiv.org/abs/2503.10497)
Append: [CULEMO: Cultural Lenses on Emotion -- Benchmarking LLMs for Cross-Cultural Emotion Understanding](https://arxiv.org/abs/2503.10688)
Append: [General Table Question Answering via Answer-Formula Joint Generation](https://arxiv.org/abs/2503.12345)
Append: [RAG-RL: Advancing Retrieval-Augmented Generation via RL and Curriculum Learning](https://arxiv.org/abs/2503.12759)
Append: [Optimizing Decomposition for Optimal Claim Verification](https://arxiv.org/abs/2503.15354)
Append: [Prompting is Not All You Need! Evaluating LLM Agent Simulation Methodologies with Real-World Online Customer Behavior Data](https://arxiv.org/abs/2503.20749)
Append: [GTR: Graph-Table-RAG for Cross-Table Question Answering](https://arxiv.org/abs/2504.01346)
Append: [FISH-Tuning: Enhancing PEFT Methods with Fisher Information](https://arxiv.org/abs/2504.04050)
Append: [Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language Models](https://arxiv.org/abs/2504.05050)
Append: [NoveltyBench: Evaluating Language Models for Humanlike Diversity](https://arxiv.org/abs/2504.05228)
Append: [Model Utility Law: Evaluating LLMs beyond Performance through Mechanism Interpretable Metric](https://arxiv.org/abs/2504.07440)
Append: [MOSAIC: Modeling Social AI for Content Dissemination and Regulation in Multi-Agent Simulations](https://arxiv.org/abs/2504.07830)
Append: [PASS-FC: Progressive and Adaptive Search Scheme for Fact Checking of Comprehensive Claims](https://arxiv.org/abs/2504.09866)
Append: [TextArena](https://arxiv.org/abs/2504.11442)
Append: [Empirical Evaluation of Knowledge Distillation from Transformers to Subquadratic Language Models](https://arxiv.org/abs/2504.14366)
Append: [Safety in Large Reasoning Models: A Survey](https://arxiv.org/abs/2504.17704)
Append: [Efficient Reasoning for LLMs through Speculative Chain-of-Thought](https://arxiv.org/abs/2504.19095)
Append: [Explanatory Summarization with Discourse-Driven Planning](https://arxiv.org/abs/2504.19339)
Append: [Position: Enough of Scaling LLMs! Lets Focus on Downscaling](https://arxiv.org/abs/2505.00985)
Append: [Intra-Layer Recurrence in Transformers for Language Modeling](https://arxiv.org/abs/2505.01855)
Append: [Bemba Speech Translation: Exploring a Low-Resource African Language](https://arxiv.org/abs/2505.02518)
Append: [Accelerating Large Language Model Reasoning via Speculative Search](https://arxiv.org/abs/2505.02865)
Append: [AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale](https://arxiv.org/abs/2505.08311)
Append: [What Does Neuro Mean to Cardio? Investigating the Role of Clinical Specialty Data in Medical LLMs](https://arxiv.org/abs/2505.10113)
Append: [GeoGrid-Bench: Can Foundation Models Understand Multimodal Gridded Geo-Spatial Data?](https://arxiv.org/abs/2505.10714)
Append: [A Survey on the Safety and Security Threats of Computer-Using Agents: JARVIS or Ultron?](https://arxiv.org/abs/2505.10924)
Append: [Is Compression Really Linear with Code Intelligence?](https://arxiv.org/abs/2505.11441)
Append: [Talk to Your Slides: Language-Driven Agents for Efficient Slide Editing](https://arxiv.org/abs/2505.11604)
Append: [Not All Thoughts are Generated Equal: Efficient LLM Reasoning via Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2505.11827)
Append: [Mobile-Bench-v2: A More Realistic and Comprehensive Benchmark for VLM-based Mobile Agents](https://arxiv.org/abs/2505.11891)
Append: [One-for-All Pruning: A Universal Model for Customized Compression of Large Language Models](https://arxiv.org/abs/2505.12216)
Append: [SLOT: Sample-specific Language Model Optimization at Test-time](https://arxiv.org/abs/2505.12392)
Append: [R3: Robust Rubric-Agnostic Reward Models](https://arxiv.org/abs/2505.13388)
Append: [Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated Rationales](https://arxiv.org/abs/2505.14499)
Append: [Linear Control of Test Awareness Reveals Differential Compliance in Reasoning Models](https://arxiv.org/abs/2505.14617)
Append: [Towards End-to-End Training of Automatic Speech Recognition for Nigerian Pidgin](https://arxiv.org/abs/2010.11123)
Append: [GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models](https://arxiv.org/abs/2402.03299)
Append: [JailbreakRadar: Comprehensive Assessment of Jailbreak Attacks Against LLMs](https://arxiv.org/abs/2402.05668)
Append: [Query Performance Prediction using Relevance Judgments Generated by Large Language Models](https://arxiv.org/abs/2404.01012)
Append: [Model Extrapolation Expedites Alignment](https://arxiv.org/abs/2404.16792)
Append: [Constructing a BPE Tokenization DFA](https://arxiv.org/abs/2405.07671)
Append: [AgentClinic: a multimodal agent benchmark to evaluate AI in simulated clinical environments](https://arxiv.org/abs/2405.07960)
Append: [SliM-LLM: Salience-Driven Mixed-Precision Quantization for Large Language Models](https://arxiv.org/abs/2405.14917)
Append: [Explaining the role of Intrinsic Dimensionality in Adversarial Training](https://arxiv.org/abs/2405.17130)
Append: [Little Data, Big Impact: Privacy-Aware Visual Language Models via Minimal Tuning](https://arxiv.org/abs/2405.17423)
Append: [Parrot: Multilingual Visual Instruction Tuning](https://arxiv.org/abs/2406.02539)
Append: [Algorithmic Language Models with Neurally Compiled Libraries](https://arxiv.org/abs/2407.04899)
Append: [MAPLE: Enhancing Review Generation with Multi-Aspect Prompt LEarning in Explainable Recommendation](https://arxiv.org/abs/2408.09865)
Append: [ReflectDiffu:Reflect between Emotion-intent Contagion and Mimicry for Empathetic Response Generation via a RL-Diffusion Framework](https://arxiv.org/abs/2409.10289)
Append: [Training Nonlinear Transformers for Chain-of-Thought Inference: A Theoretical Generalization Analysis](https://arxiv.org/abs/2410.02167)
Append: [Many Heads Are Better Than One: Improved Scientific Idea Generation by A LLM-Based Multi-Agent System](https://arxiv.org/abs/2410.09403)
Append: [AgentOccam: A Simple Yet Strong Baseline for LLM-Based Web Agents](https://arxiv.org/abs/2410.13825)
Append: [LLMScan: Causal Scan for LLM Misbehavior Detection](https://arxiv.org/abs/2410.16638)
Append: [Automated Trustworthiness Oracle Generation for Machine Learning Text Classifiers](https://arxiv.org/abs/2410.22663)
Append: [P$^2$ Law: Scaling Law for Post-Training After Model Pruning](https://arxiv.org/abs/2411.10272)
Append: [FuseGPT: Learnable Layers Fusion of Generative Pre-trained Transformers](https://arxiv.org/abs/2411.14507)
Append: [BPP-Search: Enhancing Tree of Thought Reasoning for Mathematical Modeling Problem Solving](https://arxiv.org/abs/2411.17404)
Append: [Demonstration Selection for In-Context Learning via Reinforcement Learning](https://arxiv.org/abs/2412.03966)
Append: [ProcessBench: Identifying Process Errors in Mathematical Reasoning](https://arxiv.org/abs/2412.06559)
Append: [Visual Program Distillation with Template-Based Augmentation](https://arxiv.org/abs/2412.08564)
Append: [GMoE: Empowering LLMs Fine-Tuning via MoE Graph Collaboration](https://arxiv.org/abs/2412.16216)
Append: [Latent-space adversarial training with post-aware calibration for defending large language models against jailbreak attacks](https://arxiv.org/abs/2501.10639)
Append: [Divide-Then-Aggregate: An Efficient Tool Learning Method via Parallel Tool Invocation](https://arxiv.org/abs/2501.12432)
Append: [Understanding Multimodal LLMs Under Distribution Shifts: An Information-Theoretic Approach](https://arxiv.org/abs/2502.00577)
Append: [Polynomial, trigonometric, and tropical activations](https://arxiv.org/abs/2502.01247)
Append: [Preference Leakage: A Contamination Problem in LLM-as-a-judge](https://arxiv.org/abs/2502.01534)
Append: [ACECODER: Acing Coder RL via Automated Test-Case Synthesis](https://arxiv.org/abs/2502.01718)
Append: [DiTAR: Diffusion Transformer Autoregressive Modeling for Speech Generation](https://arxiv.org/abs/2502.03930)
Append: [When More is Less: Understanding Chain-of-Thought Length in LLMs](https://arxiv.org/abs/2502.07266)
Append: [QueryAttack: Jailbreaking Aligned Large Language Models Using Structured Non-natural Query Language](https://arxiv.org/abs/2502.09723)
Append: [SMART: Self-Aware Agent for Tool Overuse Mitigation](https://arxiv.org/abs/2502.11435)
Append: [Why Vision Language Models Struggle with Visual Arithmetic? Towards Enhanced Chart and Geometry Understanding](https://arxiv.org/abs/2502.11492)
Append: [APB: Accelerating Distributed Long-Context Inference by Passing Compressed Context Blocks across GPUs](https://arxiv.org/abs/2502.12085)
Append: [HopRAG: Multi-Hop Reasoning for Logic-Aware Retrieval-Augmented Generation](https://arxiv.org/abs/2502.12442)
Append: [Multi-Step Alignment as Markov Games: An Optimistic Online Gradient Descent Approach with Convergence Guarantees](https://arxiv.org/abs/2502.12678)
Append: [K-Paths: Reasoning over Graph Paths for Drug Repurposing and Drug Interaction Prediction](https://arxiv.org/abs/2502.13344)
Append: [Automated Knowledge Component Generation and Knowledge Tracing for Coding Problems](https://arxiv.org/abs/2502.18632)
Append: [TheoremExplainAgent: Towards Video-based Multimodal Explanations for LLM Theorem Understanding](https://arxiv.org/abs/2502.19400)
Append: [Beyond the Tip of Efficiency: Uncovering the Submerged Threats of Jailbreak Attacks in Small Language Models](https://arxiv.org/abs/2502.19883)
Append: [Generalizable Prompt Learning of CLIP: A Brief Overview](https://arxiv.org/abs/2503.01263)
Append: [Language Models, Graph Searching, and Supervision Adulteration: When More Supervision is Less and How to Make More More](https://arxiv.org/abs/2503.10542)
Append: [Time-R1: Post-Training Large Vision Language Model for Temporal Video Grounding](https://arxiv.org/abs/2503.13377)
Append: [Mixture of Lookup Experts](https://arxiv.org/abs/2503.15798)
Append: [MoLAE: Mixture of Latent Experts for Parameter-Efficient Language Models](https://arxiv.org/abs/2503.23100)
Append: [An Illusion of Progress? Assessing the Current State of Web Agents](https://arxiv.org/abs/2504.01382)
Append: [Sugar-Coated Poison: Benign Generation Unlocks LLM Jailbreaking](https://arxiv.org/abs/2504.05652)
Append: [FamilyTool: A Multi-hop Personalized Tool Use Benchmark](https://arxiv.org/abs/2504.06766)
Append: [DocAgent: A Multi-Agent System for Automated Code Documentation Generation](https://arxiv.org/abs/2504.08725)
Append: [ARise: Towards Knowledge-Augmented Reasoning via Risk-Adaptive Search](https://arxiv.org/abs/2504.10893)
Append: [AI Idea Bench 2025: AI Research Idea Generation Benchmark](https://arxiv.org/abs/2504.14191)
Append: [Roll the dice & look before you leap: Going beyond the creative limits of next-token prediction](https://arxiv.org/abs/2504.15266)
Append: [IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery](https://arxiv.org/abs/2504.16728)
Append: [RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2504.20073)
Append: [Reinforcement Learning for Reasoning in Large Language Models with One Training Example](https://arxiv.org/abs/2504.20571)
Append: [Which Agent Causes Task Failures and When? On Automated Failure Attribution of LLM Multi-Agent Systems](https://arxiv.org/abs/2505.00212)
Append: [SepALM: Audio Language Models Are Error Correctors for Robust Speech Separation](https://arxiv.org/abs/2505.03273)
Append: [Benign Samples Matter! Fine-tuning On Outlier Benign Samples Severely Breaks Safety](https://arxiv.org/abs/2505.06843)
Append: [LLM-based Prompt Ensemble for Reliable Medical Entity Recognition from EHRs](https://arxiv.org/abs/2505.08704)
Append: [Efficient Uncertainty Estimation via Distillation of Bayesian Large Language Models](https://arxiv.org/abs/2505.11731)
Append: [Fractured Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.12992)
Append: [Warm Up Before You Train: Unlocking General Reasoning in Resource-Constrained Settings](https://arxiv.org/abs/2505.13718)
Append: [PandaGuard: Systematic Evaluation of LLM Safety against Jailbreaking Attacks](https://arxiv.org/abs/2505.13862)
Append: [GraphemeAug: A Systematic Approach to Synthesized Hard Negative Keyword Spotting Examples](https://arxiv.org/abs/2505.14814)
append_entries: 568
Finish: 2025-05-27 04:28:29.763551
------------------------------------------------------
Started: 2025-05-27 06:26:35.991993
Existing_entries: 1568
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1424
Summarized using GPT-3.5-turbo
Append: [AAAR-1.0: Assessing AI's Potential to Assist Research](https://arxiv.org/abs/2410.22394)
Token length: 1597
Summarized using GPT-3.5-turbo
Append: [ROUTE: Robust Multitask Tuning and Collaboration for Text-to-SQL](https://arxiv.org/abs/2412.10138)
Token length: 1477
Summarized using GPT-3.5-turbo
Append: [BriLLM: Brain-inspired Large Language Model](https://arxiv.org/abs/2503.11299)
Token length: 1456
Summarized using GPT-3.5-turbo
Append: [FastCuRL: Curriculum Reinforcement Learning with Stage-wise Context Scaling for Efficient Training R1-like Reasoning Models](https://arxiv.org/abs/2503.17287)
Token length: 1386
Summarized using GPT-3.5-turbo
Append: [Identifying Legal Holdings with LLMs: A Systematic Study of Performance, Scale, and Memorization](https://arxiv.org/abs/2505.02172)
Token length: 988
Summarized using GPT-3.5-turbo
Append: [Can Prompting LLMs Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study](https://arxiv.org/abs/2505.06149)
Token length: 1131
Summarized using GPT-3.5-turbo
Append: [A Personalized Conversational Benchmark: Towards Simulating Personalized Conversations](https://arxiv.org/abs/2505.14106)
Token length: 1747
Summarized using GPT-3.5-turbo
Append: [DiagnosisArena: Benchmarking Diagnostic Reasoning for Large Language Models](https://arxiv.org/abs/2505.14107)
Token length: 1473
Summarized using GPT-3.5-turbo
Append: [Data-Efficient Hate Speech Detection via Cross-Lingual Nearest Neighbor Retrieval with Limited Labeled Data](https://arxiv.org/abs/2505.14272)
Token length: 1220
Summarized using GPT-3.5-turbo
Append: [Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models](https://arxiv.org/abs/2505.14810)
Token length: 1466
Summarized using GPT-3.5-turbo
Append: [MAS-ZERO: Designing Multi-Agent Systems with Zero Supervision](https://arxiv.org/abs/2505.14996)
Token length: 1877
Summarized using GPT-3.5-turbo
Append: [Self-GIVE: Associative Thinking from Limited Structured Knowledge for Enhanced Large Language Model Reasoning](https://arxiv.org/abs/2505.15062)
Token length: 1374
Summarized using GPT-3.5-turbo
Append: [StepSearch: Igniting LLMs Search Ability via Step-Wise Proximal Policy Optimization](https://arxiv.org/abs/2505.15107)
Token length: 1507
Summarized using GPT-3.5-turbo
Append: [Your Language Model Can Secretly Write Like Humans: Contrastive Paraphrase Attacks on LLM-Generated Text Detectors](https://arxiv.org/abs/2505.15337)
Token length: 1149
Summarized using GPT-3.5-turbo
Append: [Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning in Language Models](https://arxiv.org/abs/2505.15634)
Token length: 1301
Summarized using GPT-3.5-turbo
Append: [Thought-Augmented Policy Optimization: Bridging External Guidance and Internal Capabilities](https://arxiv.org/abs/2505.15692)
Token length: 943
Summarized using GPT-3.5-turbo
Append: ["Alexa, can you forget me?" Machine Unlearning Benchmark in Spoken Language Understanding](https://arxiv.org/abs/2505.15700)
Token length: 1227
Summarized using GPT-3.5-turbo
Append: [VerifyBench: Benchmarking Reference-based Reward Systems for Large Language Models](https://arxiv.org/abs/2505.15801)
Token length: 1603
Summarized using GPT-3.5-turbo
Append: [O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended Question Answering](https://arxiv.org/abs/2505.16582)
Token length: 1292
Summarized using GPT-3.5-turbo
Append: [SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning Trajectory Synthesis](https://arxiv.org/abs/2505.16834)
Token length: 1398
Summarized using GPT-3.5-turbo
Append: [ReGUIDE: Data Efficient GUI Grounding via Spatial Reasoning and Search](https://arxiv.org/abs/2505.15259)
Token length: 1464
Summarized using GPT-3.5-turbo
Append: [Seeing Through Deception: Uncovering Misleading Creator Intent in Multimodal News with Vision-Language Models](https://arxiv.org/abs/2505.15489)
Token length: 1855
Summarized using GPT-3.5-turbo
Append: [Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning](https://arxiv.org/abs/2505.15966)
Token length: 1360
Summarized using GPT-3.5-turbo
Append: [NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop System from Hypothesis to Verification](https://arxiv.org/abs/2505.16938)
Token length: 1296
Summarized using GPT-3.5-turbo
Append: [CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark](https://arxiv.org/abs/2505.16968)
append_entries: 25
Finish: 2025-05-27 06:27:36.820994
------------------------------------------------------
Started: 2025-05-27 08:22:28.722995
Existing_entries: 1025
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1290
Summarized using GPT-3.5-turbo
Append: [AppealCase: A Dataset and Benchmark for Civil Case Appeal Scenarios](https://arxiv.org/abs/2505.16514)
Token length: 1281
Summarized using GPT-3.5-turbo
Append: [Are the Hidden States Hiding Something? Testing the Limits of Factuality-Encoding Capabilities in LLMs](https://arxiv.org/abs/2505.16520)
append_entries: 2
Finish: 2025-05-27 08:22:33.710128
------------------------------------------------------
Started: 2025-05-27 10:18:35.287357
Existing_entries: 1002
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-27 10:18:36.515366
------------------------------------------------------
Started: 2025-05-27 12:35:18.008371
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-27 12:35:19.218601
------------------------------------------------------
Started: 2025-05-27 14:16:23.343462
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-27 14:16:24.474794
------------------------------------------------------
Started: 2025-05-27 16:21:03.442434
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-27 16:21:04.618719
------------------------------------------------------
Started: 2025-05-27 18:22:09.997551
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-27 18:22:11.236560
------------------------------------------------------
Started: 2025-05-27 20:18:37.073028
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-27 20:18:38.191566
------------------------------------------------------
Started: 2025-05-27 22:15:37.517071
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-27 22:15:38.707862
------------------------------------------------------
Started: 2025-05-28 01:19:45.327893
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-28 01:19:46.475260
------------------------------------------------------
Started: 2025-05-28 03:11:16.977459
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-28 03:11:18.268401
------------------------------------------------------
Started: 2025-05-28 04:25:12.991782
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1552
Summarized using GPT-3.5-turbo
Append: [Guiding Giants: Lightweight Controllers for Weighted Activation Steering in LLMs](https://arxiv.org/abs/2505.20309)
Token length: 1378
Summarized using GPT-3.5-turbo
Append: [Arctic-Text2SQL-R1: Simple Rewards, Strong Reasoning in Text-to-SQL](https://arxiv.org/abs/2505.20315)
Token length: 1325
Summarized using GPT-3.5-turbo
Append: [Beyond Demonstrations: Dynamic Vector Construction from Latent Representations](https://arxiv.org/abs/2505.20318)
Token length: 1127
Summarized using GPT-3.5-turbo
Append: [Less Context, Same Performance: A RAG Framework for Resource-Efficient LLM-Based Clinical NLP](https://arxiv.org/abs/2505.20320)
Token length: 1563
Summarized using GPT-3.5-turbo
Append: [BiomedSQL: Text-to-SQL for Scientific Reasoning on Biomedical Knowledge Bases](https://arxiv.org/abs/2505.20321)
Token length: 1173
Summarized using GPT-3.5-turbo
Append: [Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering Target Atoms](https://arxiv.org/abs/2505.20322)
Token length: 1431
Summarized using GPT-3.5-turbo
Append: [PMOA-TTS: Introducing the PubMed Open Access Textual Times Series Corpus](https://arxiv.org/abs/2505.20323)
Token length: 1299
Summarized using GPT-3.5-turbo
Append: [Guided by Gut: Efficient Test-Time Scaling with Reinforced Intrinsic Confidence](https://arxiv.org/abs/2505.20325)
Token length: 1049
Summarized using GPT-3.5-turbo
Append: [Multi-Scale Manifold Alignment: A Unified Framework for Enhanced Explainability of Large Language Models](https://arxiv.org/abs/2505.20333)
Token length: 1185
Summarized using GPT-3.5-turbo
Append: [Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via Pseudo Query](https://arxiv.org/abs/2505.20334)
Token length: 1251
Summarized using GPT-3.5-turbo
Append: [Language Model Distillation: A Temporal Difference Imitation Learning Perspective](https://arxiv.org/abs/2505.20335)
Token length: 1449
Summarized using GPT-3.5-turbo
Append: [MOSLIM:Align with diverse preferences in prompts through reward classification](https://arxiv.org/abs/2505.20336)
Token length: 1885
Summarized using GPT-3.5-turbo
Append: [Assessing the Capability of LLMs in Solving POSCOMP Questions](https://arxiv.org/abs/2505.20338)
Token length: 910
Summarized using GPT-3.5-turbo
Append: [Dynamic Manifold Evolution Theory: Modeling and Stability Analysis of Latent Representations in Large Language Models](https://arxiv.org/abs/2505.20340)
Token length: 1748
Summarized using GPT-3.5-turbo
Append: [Do LLMs have a Gender (Entropy) Bias?](https://arxiv.org/abs/2505.20343)
Token length: 1429
Summarized using GPT-3.5-turbo
Append: [SeRL: Self-Play Reinforcement Learning for Large Language Models with Limited Data](https://arxiv.org/abs/2505.20347)
Token length: 1147
Summarized using GPT-3.5-turbo
Append: [Rethinking Text-based Protein Understanding: Retrieval or LLM?](https://arxiv.org/abs/2505.20354)
Token length: 1508
Summarized using GPT-3.5-turbo
Append: [Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision](https://arxiv.org/abs/2505.20415)
Token length: 1419
Summarized using GPT-3.5-turbo
Append: [GraphGen: Enhancing Supervised Fine-Tuning for LLMs with Knowledge-Driven Synthetic Data Generation](https://arxiv.org/abs/2505.20416)
Token length: 1191
Summarized using GPT-3.5-turbo
Append: [SEMMA: A Semantic Aware Knowledge Graph Foundation Model](https://arxiv.org/abs/2505.20422)
Token length: 877
Summarized using GPT-3.5-turbo
Append: [The UD-NewsCrawl Treebank: Reflections and Challenges from a Large-scale Tagalog Syntactic Annotation Project](https://arxiv.org/abs/2505.20428)
Token length: 1035
Summarized using GPT-3.5-turbo
Append: [PreP-OCR: A Complete Pipeline for Document Image Restoration and Enhanced OCR Accuracy](https://arxiv.org/abs/2505.20429)
Token length: 1588
Summarized using GPT-3.5-turbo
Append: [HAMburger: Accelerating LLM Inference via Token Smashing](https://arxiv.org/abs/2505.20438)
Token length: 1011
Summarized using GPT-3.5-turbo
Append: [In-context Language Learning for Endangered Languages in Speech Recognition](https://arxiv.org/abs/2505.20445)
Token length: 1469
Summarized using GPT-3.5-turbo
Append: [Amulet: Putting Complex Multi-Turn Conversations on the Stand with LLM Juries](https://arxiv.org/abs/2505.20451)
Token length: 1526
Summarized using GPT-3.5-turbo
Append: [Conversation Kernels: A Flexible Mechanism to Learn Relevant Context for Online Conversation Understanding](https://arxiv.org/abs/2505.20482)
Token length: 1066
Summarized using GPT-3.5-turbo
Append: [InFact: Informativeness Alignment for Improved LLM Factuality](https://arxiv.org/abs/2505.20487)
Token length: 1138
Summarized using GPT-3.5-turbo
Append: [Inceptive Transformers: Enhancing Contextual Representations through Multi-Scale Feature Learning Across Domains and Languages](https://arxiv.org/abs/2505.20496)
Token length: 1255
Summarized using GPT-3.5-turbo
Append: [Beyond Keywords: Evaluating Large Language Model Classification of Nuanced Ableism](https://arxiv.org/abs/2505.20500)
Token length: 966
Summarized using GPT-3.5-turbo
Append: [Gatsby Without the 'E': Crafting Lipograms with LLMs](https://arxiv.org/abs/2505.20501)
Token length: 1479
Summarized using GPT-3.5-turbo
Append: [Large Language Models for IT Automation Tasks: Are We There Yet?](https://arxiv.org/abs/2505.20505)
Token length: 842
Summarized using GPT-3.5-turbo
Append: [ArVoice: A Multi-Speaker Dataset for Arabic Speech Synthesis](https://arxiv.org/abs/2505.20506)
Token length: 915
Summarized using GPT-3.5-turbo
Append: [Multimodal Emotion Recognition in Conversations: A Survey of Methods, Trends, Challenges and Prospects](https://arxiv.org/abs/2505.20511)
Token length: 1548
Summarized using GPT-3.5-turbo
Append: [AstroVisBench: A Code Benchmark for Scientific Computing and Visualization in Astronomy](https://arxiv.org/abs/2505.20538)
Token length: 1211
Summarized using GPT-3.5-turbo
Append: [Paths Not Taken: Understanding and Mending the Multilingual Factual Recall Pipeline](https://arxiv.org/abs/2505.20546)
Token length: 1048
Summarized using GPT-3.5-turbo
Append: [The NaijaVoices Dataset: Cultivating Large-Scale, High-Quality, Culturally-Rich Speech Data for African Languages](https://arxiv.org/abs/2505.20564)
Token length: 1663
Summarized using GPT-3.5-turbo
Append: [Emotion Classification In-Context in Spanish](https://arxiv.org/abs/2505.20571)
Token length: 1666
Summarized using GPT-3.5-turbo
Append: [Effectiveness of Prompt Optimization in NL2SQL Systems](https://arxiv.org/abs/2505.20591)
Token length: 955
Summarized using GPT-3.5-turbo
Append: [Towards Pretraining Robust ASR Foundation Model with Acoustic-Aware Data Augmentation](https://arxiv.org/abs/2505.20606)
Token length: 1163
Summarized using GPT-3.5-turbo
Append: [REAL-Prover: Retrieval Augmented Lean Prover for Mathematical Reasoning](https://arxiv.org/abs/2505.20613)
Token length: 1315
Summarized using GPT-3.5-turbo
Append: [SeqPO-SiMT: Sequential Policy Optimization for Simultaneous Machine Translation](https://arxiv.org/abs/2505.20622)
Token length: 1264
Summarized using GPT-3.5-turbo
Append: [POLAR: A Benchmark for Multilingual, Multicultural, and Multi-Event Online Polarization](https://arxiv.org/abs/2505.20624)
Token length: 1618
Summarized using GPT-3.5-turbo
Append: [Long Context Scaling: Divide and Conquer via Multi-Agent Question-driven Collaboration](https://arxiv.org/abs/2505.20625)
Token length: 1555
Summarized using GPT-3.5-turbo
Append: [Test-Time Learning for Large Language Models](https://arxiv.org/abs/2505.20633)
Token length: 1279
Summarized using GPT-3.5-turbo
Append: [STEER-BENCH: A Benchmark for Evaluating the Steerability of Large Language Models](https://arxiv.org/abs/2505.20645)
Token length: 1446
Summarized using GPT-3.5-turbo
Append: [FinTagging: An LLM-ready Benchmark for Extracting and Structuring Financial Information](https://arxiv.org/abs/2505.20650)
Token length: 1156
Summarized using GPT-3.5-turbo
Append: [Chinese Cyberbullying Detection: Dataset, Method, and Validation](https://arxiv.org/abs/2505.20654)
Token length: 1520
Summarized using GPT-3.5-turbo
Append: [Enhancing Transformation from Natural Language to Signal Temporal Logic Using LLMs with Diverse External Knowledge](https://arxiv.org/abs/2505.20658)
Token length: 1126
Summarized using GPT-3.5-turbo
Append: [BacktrackAgent: Enhancing GUI Agent with Error Detection and Backtracking Mechanism](https://arxiv.org/abs/2505.20660)
Token length: 1359
Summarized using GPT-3.5-turbo
Append: [Self-Route: Automatic Mode Switching via Capability Estimation for Efficient Reasoning](https://arxiv.org/abs/2505.20664)
Append: [Pretraining Language Models to Ponder in Continuous Space](https://arxiv.org/abs/2505.20674)
Append: [SELF-PERCEPT: Introspection Improves Large Language Models' Detection of Multi-Person Mental Manipulation in Conversations](https://arxiv.org/abs/2505.20679)
Append: [Phir Hera Fairy: An English Fairytaler is a Strong Faker of Fluent Speech in Low-Resource Indian Languages](https://arxiv.org/abs/2505.20693)
Append: [Beyond Templates: Dynamic Adaptation of Reasoning Demonstrations via Feasibility-Aware Exploration](https://arxiv.org/abs/2505.20700)
Append: [Dissecting Physics Reasoning in Small Language Models: A Multi-Dimensional Analysis from an Educational Perspective](https://arxiv.org/abs/2505.20707)
Append: [SPA-RL: Reinforcing LLM Agents via Stepwise Progress Attribution](https://arxiv.org/abs/2505.20732)
Append: [Silencer: From Discovery to Mitigation of Self-Bias in LLM-as-Benchmark-Generator](https://arxiv.org/abs/2505.20738)
Append: [CogniBench: A Legal-inspired Framework and Dataset for Assessing Cognitive Faithfulness of Large Language Models](https://arxiv.org/abs/2505.20767)
Append: [SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long Sequences](https://arxiv.org/abs/2505.20776)
Append: [CHIMERA: A Knowledge Base of Idea Recombination in Scientific Literature](https://arxiv.org/abs/2505.20779)
Append: [Improved Representation Steering for Language Models](https://arxiv.org/abs/2505.20809)
Append: [RSCF: Relation-Semantics Consistent Filter for Entity Embedding of Knowledge Graph](https://arxiv.org/abs/2505.20813)
Append: [Rethinking Information Synthesis in Multimodal Question Answering A Multi-Agent Perspective](https://arxiv.org/abs/2505.20816)
Append: [Tracing and Reversing Rank-One Model Edits](https://arxiv.org/abs/2505.20819)
Append: [Reinforced Informativeness Optimization for Long-Form Retrieval-Augmented Generation](https://arxiv.org/abs/2505.20825)
Append: [AdParaphrase v2.0: Generating Attractive Ad Texts Using a Preference-Annotated Paraphrase Dataset](https://arxiv.org/abs/2505.20826)
Append: [Concealment of Intent: A Game-Theoretic Analysis](https://arxiv.org/abs/2505.20841)
Append: [Divide-Then-Align: Honest Alignment based on the Knowledge Boundary of RAG](https://arxiv.org/abs/2505.20871)
Append: [Can LLMs Learn to Map the World from Local Descriptions?](https://arxiv.org/abs/2505.20874)
Append: [Trans-EnV: A Framework for Evaluating the Linguistic Robustness of LLMs Against English Varieties](https://arxiv.org/abs/2505.20875)
Append: [MSA at SemEval-2025 Task 3: High Quality Weak Labeling and LLM Ensemble Verification for Multilingual Hallucination Detection](https://arxiv.org/abs/2505.20880)
Append: [EasyDistill: A Comprehensive Toolkit for Effective Knowledge Distillation of Large Language Models](https://arxiv.org/abs/2505.20888)
Append: [Dub-S2ST: Textless Speech-to-Speech Translation for Seamless Dubbing](https://arxiv.org/abs/2505.20899)
Append: [A Stereotype Content Analysis on Color-related Social Bias in Large Vision Language Models](https://arxiv.org/abs/2505.20901)
Append: [Towards Objective Fine-tuning: How LLMs' Prior Knowledge Causes Potential Poor Calibration?](https://arxiv.org/abs/2505.20903)
Append: [Automated Privacy Information Annotation in Large Language Model Interactions](https://arxiv.org/abs/2505.20910)
Append: [Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models](https://arxiv.org/abs/2505.20921)
Append: [Multi-objective Large Language Model Alignment with Hierarchical Experts](https://arxiv.org/abs/2505.20925)
Append: [Information-Theoretic Complementary Prompts for Improved Continual Text Classification](https://arxiv.org/abs/2505.20933)
Append: [On VLMs for Diverse Tasks in Multimodal Meme Classification](https://arxiv.org/abs/2505.20937)
Append: [Research Community Perspectives on "Intelligence" and Large Language Models](https://arxiv.org/abs/2505.20959)
Append: [Context-Aware Content Moderation for German Newspaper Comments](https://arxiv.org/abs/2505.20963)
Append: [Personalized Query Auto-Completion for Long and Short-Term Interests with Adaptive Detoxification Generation](https://arxiv.org/abs/2505.20966)
Append: [Reason-Align-Respond: Aligning LLM Reasoning with Knowledge Graphs for KGQA](https://arxiv.org/abs/2505.20971)
Append: [Contrastive Learning on LLM Back Generation Treebank for Cross-domain Constituency Parsing](https://arxiv.org/abs/2505.20976)
Append: [Evaluating and Steering Modality Preferences in Multimodal Large Language Model](https://arxiv.org/abs/2505.20977)
Append: [Who Reasons in the Large Language Models?](https://arxiv.org/abs/2505.20993)
Append: [Articulatory strategy in vowel production as a basis for speaker discrimination](https://arxiv.org/abs/2505.20995)
Append: [Uncertainty Unveiled: Can Exposure to More In-context Examples Mitigate Uncertainty for Large Language Models?](https://arxiv.org/abs/2505.21003)
Append: [LLMs are Frequency Pattern Learners in Natural Language Inference](https://arxiv.org/abs/2505.21011)
Append: [Def-DTS: Deductive Reasoning for Open-domain Dialogue Topic Segmentation](https://arxiv.org/abs/2505.21033)
Append: [FCKT: Fine-Grained Cross-Task Knowledge Transfer with Semantic Contrastive Learning for Targeted Sentiment Analysis](https://arxiv.org/abs/2505.21040)
Append: [Visual Cues Enhance Predictive Turn-Taking for Two-Party Human Interaction](https://arxiv.org/abs/2505.21043)
Append: [Predicting Implicit Arguments in Procedural Video Instructions](https://arxiv.org/abs/2505.21068)
Append: [Faithfulness-Aware Uncertainty Quantification for Fact-Checking the Output of Retrieval Augmented Generation](https://arxiv.org/abs/2505.21072)
Append: [LLMs Think, But Not In Your Flow: Reasoning-Level Personalization for Black-Box Large Language Models](https://arxiv.org/abs/2505.21082)
Append: [BLUCK: A Benchmark Dataset for Bengali Linguistic Understanding and Cultural Knowledge](https://arxiv.org/abs/2505.21092)
Append: [Thinker: Learning to Think Fast and Slow](https://arxiv.org/abs/2505.21097)
Append: [A Lightweight Multi-Expert Generative Language Model System for Engineering Information and Knowledge Extraction](https://arxiv.org/abs/2505.21109)
Append: [Will It Still Be True Tomorrow? Multilingual Evergreen Question Classification to Improve Trustworthy QA](https://arxiv.org/abs/2505.21115)
Append: [Scaling and Prompting for Improved End-to-End Spoken Grammatical Error Correction](https://arxiv.org/abs/2505.21137)
Append: [Leveraging LLM and Self-Supervised Training Models for Speech Recognition in Chinese Dialects: A Comparative Analysis](https://arxiv.org/abs/2505.21138)
Append: [Assessment of L2 Oral Proficiency using Speech Large Language Models](https://arxiv.org/abs/2505.21148)
Append: [M-Wanda: Improving One-Shot Pruning for Multilingual LLMs](https://arxiv.org/abs/2505.21171)
Append: [TAT-R1: Terminology-Aware Translation with Reinforcement Learning and Word Alignment](https://arxiv.org/abs/2505.21172)
Append: [Walk Before You Run! Concise LLM Reasoning via Reinforcement Learning](https://arxiv.org/abs/2505.21178)
Append: [Exploring the Latent Capacity of LLMs for One-Step Text Generation](https://arxiv.org/abs/2505.21189)
Append: [Lunguage: A Benchmark for Structured and Sequential Chest X-ray Interpretation](https://arxiv.org/abs/2505.21190)
Append: [Unveiling Instruction-Specific Neurons & Experts: An Analytical Framework for LLM's Instruction-Following Capabilities](https://arxiv.org/abs/2505.21191)
Append: [Pretrained LLMs Learn Multiple Types of Uncertainty](https://arxiv.org/abs/2505.21218)
Append: [A Representation Level Analysis of NMT Model Robustness to Grammatical Errors](https://arxiv.org/abs/2505.21224)
Append: [LMCD: Language Models are Zeroshot Cognitive Diagnosis Learners](https://arxiv.org/abs/2505.21239)
Append: [Evaluation of LLMs in Medical Text Summarization: The Role of Vocabulary Adaptation in High OOV Settings](https://arxiv.org/abs/2505.21242)
Append: [ReSCORE: Label-free Iterative Retriever Training for Multi-hop Question Answering with Relevance-Consistency Supervision](https://arxiv.org/abs/2505.21250)
Append: [Multilingual Pretraining for Pixel Language Models](https://arxiv.org/abs/2505.21265)
Append: [rStar-Coder: Scaling Competitive Code Reasoning with a Large-Scale Verified Dataset](https://arxiv.org/abs/2505.21297)
Append: [How Humans and LLMs Organize Conceptual Knowledge: Exploring Subordinate Categories in Italian](https://arxiv.org/abs/2505.21301)
Append: [Charting the Landscape of African NLP: Mapping Progress and Shaping the Road Ahead](https://arxiv.org/abs/2505.21315)
Append: [Leveraging large language models and traditional machine learning ensembles for ADHD detection from narrative transcripts](https://arxiv.org/abs/2505.21324)
Append: [PEDANTIC: A Dataset for the Automatic Examination of Definiteness in Patent Claims](https://arxiv.org/abs/2505.21342)
Append: [Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning](https://arxiv.org/abs/2505.21354)
Append: [Evaluating LLM Adaptation to Sociodemographic Factors: User Profile vs. Dialogue History](https://arxiv.org/abs/2505.21362)
Append: [Analyzing values about gendered language reform in LLMs' revisions](https://arxiv.org/abs/2505.21378)
Append: [PHISH in MESH: Korean Adversarial Phonetic Substitution and Phonetic-Semantic Feature Integration Defense](https://arxiv.org/abs/2505.21380)
Append: [AutoJudger: An Agent-Driven Framework for Efficient Benchmarking of MLLMs](https://arxiv.org/abs/2505.21389)
Append: [Improving Research Idea Generation Through Data: An Empirical Investigation in Social Science](https://arxiv.org/abs/2505.21396)
Append: [DecisionFlow: Advancing Large Language Model as Principled Decision Maker](https://arxiv.org/abs/2505.21397)
Append: [Factual Self-Awareness in Language Models: Representation, Robustness, and Scaling](https://arxiv.org/abs/2505.21399)
Append: [RelationalFactQA: A Benchmark for Evaluating Tabular Fact Retrieval from Large Language Models](https://arxiv.org/abs/2505.21409)
Append: [Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity](https://arxiv.org/abs/2505.21411)
Append: [RefTool: Enhancing Model Reasoning with Reference-Guided Tool Creation](https://arxiv.org/abs/2505.21413)
Append: [Towards Better Instruction Following Retrieval Models](https://arxiv.org/abs/2505.21439)
Append: [Words Like Knives: Backstory-Personalized Modeling and Detection of Violent Communication](https://arxiv.org/abs/2505.21451)
Append: [Do LLMs Need to Think in One Language? Correlation between Latent Language and Task Performance](https://arxiv.org/abs/2505.21458)
Append: [Accelerating Diffusion Language Model Inference via Efficient KV Caching and Guided Diffusion](https://arxiv.org/abs/2505.21467)
Append: [Scaling External Knowledge Input Beyond Context Windows of LLMs via Multi-Agent Collaboration](https://arxiv.org/abs/2505.21471)
Append: [Are Language Models Consequentialist or Deontological Moral Reasoners?](https://arxiv.org/abs/2505.21479)
Append: [UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based Mobile GUI Agents](https://arxiv.org/abs/2505.21496)
Append: [Silence is Not Consensus: Disrupting Agreement Bias in Multi-Agent LLMs via Catfish Agent for Clinical Decision Making](https://arxiv.org/abs/2505.21503)
Append: [How does Alignment Enhance LLMs' Multilingual Capabilities? A Language Neurons Perspective](https://arxiv.org/abs/2505.21505)
Append: [InstructPart: Task-Oriented Part Segmentation with Instruction Reasoning](https://arxiv.org/abs/2505.18291)
Append: [Cultural Awareness in Vision-Language Models: A Cross-Country Exploration](https://arxiv.org/abs/2505.20326)
Append: [Towards Emotionally Consistent Text-Based Speech Editing: Introducing EmoCorrector and The ECD-TSE Dataset](https://arxiv.org/abs/2505.20341)
Append: [Hierarchical Retrieval with Evidence Curation for Open-Domain Financial Question Answering on Standardized Documents](https://arxiv.org/abs/2505.20368)
Append: [What Changed? Detecting and Evaluating Instruction-Guided Image Edits with Multimodal Large Language Models](https://arxiv.org/abs/2505.20405)
Append: [SWE-rebench: An Automated Pipeline for Task Collection and Decontaminated Evaluation of Software Engineering Agents](https://arxiv.org/abs/2505.20411)
Append: [The Impact of a Chatbot's Ephemerality-Framing on Self-Disclosure Perceptions](https://arxiv.org/abs/2505.20464)
Append: [BrainStratify: Coarse-to-Fine Disentanglement of Intracranial Neural Dynamics](https://arxiv.org/abs/2505.20480)
Append: [Embodied AI with Foundation Models for Mobile Service Robots: A Systematic Review](https://arxiv.org/abs/2505.20503)
Append: [Project Riley: Multimodal Multi-Agent LLM Collaboration with Emotional Reasoning and Voting](https://arxiv.org/abs/2505.20521)
Append: [Scaling over Scaling: Exploring Test-Time Scaling Pareto in Large Reasoning Models](https://arxiv.org/abs/2505.20522)
Append: [Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning](https://arxiv.org/abs/2505.20561)
Append: [Comparisons between a Large Language Model-based Real-Time Compound Diagnostic Medical AI Interface and Physicians for Common Internal Medicine Cases using Simulated Patients](https://arxiv.org/abs/2505.20609)
Append: [Roboflow100-VL: A Multi-Domain Object Detection Benchmark for Vision-Language Models](https://arxiv.org/abs/2505.20612)
Append: [SV-TrustEval-C: Evaluating Structure and Semantic Reasoning in Large Language Models for Source Code Vulnerability Analysis](https://arxiv.org/abs/2505.20630)
Append: [TeroSeek: An AI-Powered Knowledge Base and Retrieval Generation Platform for Terpenoid Research](https://arxiv.org/abs/2505.20663)
Append: [Can we Debias Social Stereotypes in AI-Generated Images? Examining Text-to-Image Outputs and User Perceptions](https://arxiv.org/abs/2505.20692)
Append: [MUSEG: Reinforcing Video Temporal Understanding via Timestamp-Aware Multi-Segment Grounding](https://arxiv.org/abs/2505.20715)
Append: [What LLMs Miss in Recommendations: Bridging the Gap with Retrieval-Augmented Collaborative Signals](https://arxiv.org/abs/2505.20730)
Append: [An LLM-as-Judge Metric for Bridging the Gap with Human Evaluation in SE Tasks](https://arxiv.org/abs/2505.20854)
Append: [How Do Transformers Learn Variable Binding in Symbolic Programs?](https://arxiv.org/abs/2505.20896)
Append: [Cross from Left to Right Brain: Adaptive Text Dreamer for Vision-and-Language Navigation](https://arxiv.org/abs/2505.20897)
Append: [RefAV: Towards Planning-Centric Scenario Mining](https://arxiv.org/abs/2505.20981)
Append: [Pause Tokens Strictly Increase the Expressivity of Constant-Depth Transformers](https://arxiv.org/abs/2505.21024)
Append: [Position is Power: System Prompts as a Mechanism of Bias in Large Language Models (LLMs)](https://arxiv.org/abs/2505.21091)
Append: [Creativity in LLM-based Multi-Agent Systems: A Survey](https://arxiv.org/abs/2505.21116)
Append: [Leveraging GANs for citation intent classification and its impact on citation network analysis](https://arxiv.org/abs/2505.21162)
Append: [PoisonSwarm: Universal Harmful Information Synthesis via Model Crowdsourcing](https://arxiv.org/abs/2505.21184)
Append: [PSRB: A Comprehensive Benchmark for Evaluating Persian ASR Systems](https://arxiv.org/abs/2505.21230)
Append: [Breaking the Ceiling: Exploring the Potential of Jailbreak Attacks through Expanding Strategy Space](https://arxiv.org/abs/2505.21277)
Append: [Optimizing fMRI Data Acquisition for Decoding Natural Speech with Limited Participants](https://arxiv.org/abs/2505.21304)
Append: [Something's Fishy In The Data Lake: A Critical Re-evaluation of Table Union Search Benchmarks](https://arxiv.org/abs/2505.21329)
Append: [The Multilingual Divide and Its Impact on Global AI Safety](https://arxiv.org/abs/2505.21344)
Append: [ID-Align: RoPE-Conscious Position Remapping for Dynamic High-Resolution Adaptation in Vision-Language Models](https://arxiv.org/abs/2505.21465)
Append: [Mitigating Hallucination in Large Vision-Language Models via Adaptive Attention Calibration](https://arxiv.org/abs/2505.21472)
Append: [Hardware-Efficient Attention for Fast Decoding](https://arxiv.org/abs/2505.21487)
Append: [Reinforcing General Reasoning without Verifiers](https://arxiv.org/abs/2505.21493)
Append: [Paper2Poster: Towards Multimodal Poster Automation from Scientific Papers](https://arxiv.org/abs/2505.21497)
Append: [ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models](https://arxiv.org/abs/2505.21500)
Append: [WizardLM: Empowering large pre-trained language models to follow complex instructions](https://arxiv.org/abs/2304.12244)
Append: [WizardCoder: Empowering Code Large Language Models with Evol-Instruct](https://arxiv.org/abs/2306.08568)
Append: [Tradeoffs Between Alignment and Helpfulness in Language Models with Steering Methods](https://arxiv.org/abs/2401.16332)
Append: [LLMs with Industrial Lens: Deciphering the Challenges and Prospects -- A Survey](https://arxiv.org/abs/2402.14558)
Append: [An In-depth Evaluation of Large Language Models in Sentence Simplification with Error-based Human Assessment](https://arxiv.org/abs/2403.04963)
Append: [Bias-Augmented Consistency Training Reduces Biased Reasoning in Chain-of-Thought](https://arxiv.org/abs/2403.05518)
Append: [Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning](https://arxiv.org/abs/2403.10056)
Append: [OR-Bench: An Over-Refusal Benchmark for Large Language Models](https://arxiv.org/abs/2405.20947)
Append: [Predicting drug-gene relations via analogy tasks with word embeddings](https://arxiv.org/abs/2406.00984)
Append: [NAP^2: A Benchmark for Naturalness and Privacy-Preserving Text Rewriting by Learning from Human](https://arxiv.org/abs/2406.03749)
Append: [Raising the Bar: Investigating the Values of Large Language Models via Generative Evolving Testing](https://arxiv.org/abs/2406.14230)
Append: [Can Small Language Models Learn, Unlearn, and Retain Noise Patterns?](https://arxiv.org/abs/2407.00996)
Append: [Fine-Tuning on Diverse Reasoning Chains Drives Within-Inference CoT Refinement in LLMs](https://arxiv.org/abs/2407.03181)
Append: [Autoregressive Speech Synthesis without Vector Quantization](https://arxiv.org/abs/2407.08551)
Append: [Sentiment Reasoning for Healthcare](https://arxiv.org/abs/2407.21054)
Append: [Pandora's Box or Aladdin's Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models](https://arxiv.org/abs/2408.13533)
Append: [GALLa: Graph Aligned Large Language Models for Improved Source Code Understanding](https://arxiv.org/abs/2409.04183)
Append: [Rethinking Semantic Parsing for Large Language Models: Enhancing LLM Performance with Semantic Hints](https://arxiv.org/abs/2409.14469)
Append: [Efficient Length-Generalizable Attention via Causal Retrieval for Long-Context Language Modeling](https://arxiv.org/abs/2410.01651)
Append: [Subtle Errors in Reasoning: Preference Learning via Error-injected Self-editing](https://arxiv.org/abs/2410.06638)
Append: [Conversational Code Generation: a Case Study of Designing a Dialogue System for Generating Driving Scenarios for Testing Autonomous Vehicles](https://arxiv.org/abs/2410.09829)
Append: [The Best of Both Worlds: Bridging Quality and Diversity in Data Selection with Bipartite Graph](https://arxiv.org/abs/2410.12458)
Append: [BQA: Body Language Question Answering Dataset for Video Large Language Models](https://arxiv.org/abs/2410.13206)
Append: [Aggregation Artifacts in Subjective Tasks Collapse Large Language Models' Posteriors](https://arxiv.org/abs/2410.13776)
Append: [Distance between Relevant Information Pieces Causes Bias in Long-Context LLMs](https://arxiv.org/abs/2410.14641)
Append: [Unleashing LLM Reasoning Capability via Scalable Question Synthesis from Scratch](https://arxiv.org/abs/2410.18693)
Append: [Frequency matters: Modeling irregular morphological patterns in Spanish with Transformers](https://arxiv.org/abs/2410.21013)
Append: [STEM-POM: Evaluating Language Models Math-Symbol Reasoning in Document Parsing](https://arxiv.org/abs/2411.00387)
Append: [Align-SLM: Textless Spoken Language Models with Reinforcement Learning from AI Feedback](https://arxiv.org/abs/2411.01834)
Append: [Efficient and Accurate Prompt Optimization: the Benefit of Memory in Exemplar-Guided Reflection](https://arxiv.org/abs/2411.07446)
Append: [Rethinking MUSHRA: Addressing Modern Challenges in Text-to-Speech Evaluation](https://arxiv.org/abs/2411.12719)
Append: [DRPruning: Efficient Large Language Model Pruning through Distributionally Robust Optimization](https://arxiv.org/abs/2411.14055)
Append: [How Private are Language Models in Abstractive Summarization?](https://arxiv.org/abs/2412.12040)
Append: [Knowledge Boundary of Large Language Models: A Survey](https://arxiv.org/abs/2412.12472)
Append: [ProgCo: Program Helps Self-Correction of Large Language Models](https://arxiv.org/abs/2501.01264)
Append: [VoxEval: Benchmarking the Knowledge Understanding Capabilities of End-to-End Spoken Language Models](https://arxiv.org/abs/2501.04962)
Append: [Mitigating Forgetting in LLM Fine-Tuning via Low-Perplexity Token Learning](https://arxiv.org/abs/2501.14315)
Append: [Tuning LLM Judge Design Decisions for 1/1000 of the Cost](https://arxiv.org/abs/2501.17178)
Append: [KBQA-o1: Agentic Knowledge Base Question Answering with Monte Carlo Tree Search](https://arxiv.org/abs/2501.18922)
Append: [Thinking beyond the anthropomorphic paradigm benefits LLM research](https://arxiv.org/abs/2502.09192)
Append: [The Hidden Dimensions of LLM Alignment: A Multi-Dimensional Analysis of Orthogonal Safety Directions](https://arxiv.org/abs/2502.09674)
Append: [MMUnlearner: Reformulating Multimodal Machine Unlearning in the Era of Multimodal Large Language Models](https://arxiv.org/abs/2502.11051)
Append: [ANCHOLIK-NER: A Benchmark Dataset for Bangla Regional Named Entity Recognition](https://arxiv.org/abs/2502.11198)
Append: [SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs](https://arxiv.org/abs/2502.12134)
Append: [Hallucinations are inevitable but can be made statistically negligible. The "innate" inevitability of hallucinations cannot explain practical LLM issues](https://arxiv.org/abs/2502.12187)
Append: [Task-Informed Anti-Curriculum by Masking Improves Downstream Performance on Text](https://arxiv.org/abs/2502.12953)
Append: [Agentic Medical Knowledge Graphs Enhance Medical Question Answering: Bridging the Gap Between LLMs and Evolving Medical Knowledge](https://arxiv.org/abs/2502.13010)
Append: [Can Community Notes Replace Professional Fact-Checkers?](https://arxiv.org/abs/2502.14132)
Append: [Behavioral Analysis of Information Salience in Large Language Models](https://arxiv.org/abs/2502.14613)
Append: [Predicting Through Generation: Why Generation Is Better for Prediction](https://arxiv.org/abs/2502.17817)
Append: [Learning to Align Multi-Faceted Evaluation: A Unified and Robust Framework](https://arxiv.org/abs/2502.18874)
Append: [Between Circuits and Chomsky: Pre-pretraining on Formal Languages Imparts Linguistic Biases](https://arxiv.org/abs/2502.19249)
Append: [Plan2Align: Predictive Planning Based Test-Time Preference Alignment for Large Language Models](https://arxiv.org/abs/2502.20795)
Append: [The Power of Personality: A Human Simulation Perspective to Investigate Large Language Model Agents](https://arxiv.org/abs/2502.20859)
Append: [Beware of Your Po! Measuring and Mitigating AI Safety Risks in Role-Play Fine-Tuning of LLMs](https://arxiv.org/abs/2502.20968)
Append: [MA-LoT: Model-Collaboration Lean-based Long Chain-of-Thought Reasoning enhances Formal Theorem Proving](https://arxiv.org/abs/2503.03205)
Append: [HalluCounter: Reference-free LLM Hallucination Detection in the Wild!](https://arxiv.org/abs/2503.04615)
Append: [How to Protect Yourself from 5G Radiation? Investigating LLM Responses to Implicit Misinformation](https://arxiv.org/abs/2503.09598)
Append: [No LLM is Free From Bias: A Comprehensive Study of Bias Evaluation in Large Language Models](https://arxiv.org/abs/2503.11985)
Append: [Enabling Inclusive Systematic Reviews: Incorporating Preprint Articles with Large Language Model-Driven Evaluations](https://arxiv.org/abs/2503.13857)
Append: [S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability of Large Reasoning Models](https://arxiv.org/abs/2504.10368)
Append: [Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions](https://arxiv.org/abs/2505.00675)
Append: [Unveiling Language-Specific Features in Large Language Models via Sparse Autoencoders](https://arxiv.org/abs/2505.05111)
Append: [Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large Reasoning Models](https://arxiv.org/abs/2505.10554)
Append: [SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.11484)
Append: [Retrospex: Language Agent Meets Offline Reinforcement Learning Critic](https://arxiv.org/abs/2505.11807)
Append: [Enhance Mobile Agents Thinking Process Via Iterative Preference Learning](https://arxiv.org/abs/2505.12299)
Append: [Shadow-FT: Tuning Instruct via Base](https://arxiv.org/abs/2505.12716)
Append: [Systematic Generalization in Language Models Scales with Information Entropy](https://arxiv.org/abs/2505.13089)
Append: [DRP: Distilled Reasoning Pruning with Skill-aware Step Decomposition for Efficient Large Reasoning Models](https://arxiv.org/abs/2505.13975)
Append: [SEPS: A Separability Measure for Robust Unlearning in LLMs](https://arxiv.org/abs/2505.14832)
Append: [Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric Speech Recognition in Low-Resource Languages](https://arxiv.org/abs/2505.14874)
Append: [DUSK: Do Not Unlearn Shared Knowledge](https://arxiv.org/abs/2505.15209)
Append: [R-TOFU: Unlearning in Large Reasoning Models](https://arxiv.org/abs/2505.15214)
Append: [Benchmarking and Pushing the Multi-Bias Elimination Boundary of LLMs via Causal Effect Estimation-guided Debiasing](https://arxiv.org/abs/2505.16522)
Append: [Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains](https://arxiv.org/abs/2505.16552)
Append: [Does Synthetic Data Help Named Entity Recognition for Low-Resource Languages?](https://arxiv.org/abs/2505.16814)
Append: [Power-Law Decay Loss for Large Language Model Finetuning: Focusing on Information Sparsity to Enhance Generation Quality](https://arxiv.org/abs/2505.16900)
Append: [CLEVRER-Humans: Describing Physical and Causal Events the Human Way](https://arxiv.org/abs/2310.03635)
Append: [Retrieve to Explain: Evidence-driven Predictions for Explainable Drug Target Identification](https://arxiv.org/abs/2402.04068)
Append: [GigaSpeech 2: An Evolving, Large-Scale and Multi-domain ASR Corpus for Low-Resource Languages with Automated Crawling, Transcription and Refinement](https://arxiv.org/abs/2406.11546)
Append: [Can Large Language Models Understand Symbolic Graphics Programs?](https://arxiv.org/abs/2408.08313)
Append: ["Oh LLM, I'm Asking Thee, Please Give Me a Decision Tree": Zero-Shot Decision Tree Induction and Embedding with Large Language Models](https://arxiv.org/abs/2409.18594)
Append: [EPIC: Efficient Position-Independent Caching for Serving Large Language Models](https://arxiv.org/abs/2410.15332)
Append: [Yesterday's News: Benchmarking Multi-Dimensional Out-of-Distribution Generalization of Misinformation Detection Models](https://arxiv.org/abs/2410.18122)
Append: [Interlocking-free Selective Rationalization Through Genetic-based Learning](https://arxiv.org/abs/2412.10312)
Append: [Leveraging Large Language Models for Active Merchant Non-player Characters](https://arxiv.org/abs/2412.11189)
Append: [Transparent and Coherent Procedural Mistake Detection](https://arxiv.org/abs/2412.11927)
Append: [Enhancing Code LLMs with Reinforcement Learning in Code Generation: A Survey](https://arxiv.org/abs/2412.20367)
Append: [Efficiently Scaling LLM Reasoning with Certaindex](https://arxiv.org/abs/2412.20993)
Append: [More is not always better? Enhancing Many-Shot In-Context Learning with Differentiated and Reweighting Objectives](https://arxiv.org/abs/2501.04070)
Append: [vCache: Verified Semantic Prompt Caching](https://arxiv.org/abs/2502.03771)
Append: [A Lightweight Method to Disrupt Memorized Sequences in LLM](https://arxiv.org/abs/2502.05159)
Append: [Towards LLM Unlearning Resilient to Relearning Attacks: A Sharpness-Aware Minimization Perspective and Beyond](https://arxiv.org/abs/2502.05374)
Append: [Scaling Laws for Forgetting during Finetuning with Pretraining Data Injection](https://arxiv.org/abs/2502.06042)
Append: [GeLLMO: Generalizing Large Language Models for Multi-property Molecule Optimization](https://arxiv.org/abs/2502.13398)
Append: [Training a Generally Curious Agent](https://arxiv.org/abs/2502.17543)
Append: [Voting or Consensus? Decision-Making in Multi-Agent Debate](https://arxiv.org/abs/2502.19130)
Append: [Path Pooling: Training-Free Structure Enhancement for Efficient Knowledge Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2503.05203)
Append: [ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2503.09501)
Append: [Exploring the Necessity of Reasoning in LLM-based Agent Scenarios](https://arxiv.org/abs/2503.11074)
Append: [SciHorizon: Benchmarking AI-for-Science Readiness from Scientific Data to Large Language Models](https://arxiv.org/abs/2503.13503)
Append: [LLM-FE: Automated Feature Engineering for Tabular Data with LLMs as Evolutionary Optimizers](https://arxiv.org/abs/2503.14434)
Append: [Optimizing Case-Based Reasoning System for Functional Test Script Generation with Large Language Models](https://arxiv.org/abs/2503.20576)
Append: [Prompt-Based LLMs for Position Bias-Aware Reranking in Personalized Recommendations](https://arxiv.org/abs/2505.04948)
Append: [ComplexFormer: Disruptively Advancing Transformer Inference Ability via Head-Specific Complex Vector Attention](https://arxiv.org/abs/2505.10222)
Append: [MMLongBench: Benchmarking Long-Context Vision-Language Models Effectively and Thoroughly](https://arxiv.org/abs/2505.10610)
Append: [Visuospatial Cognitive Assistant](https://arxiv.org/abs/2505.12312)
Append: [Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts](https://arxiv.org/abs/2505.12363)
Append: [SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment](https://arxiv.org/abs/2505.14667)
Append: [Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training](https://arxiv.org/abs/2505.14681)
Append: [TCSinger 2: Customizable Multilingual Zero-shot Singing Voice Synthesis](https://arxiv.org/abs/2505.14910)
append_entries: 291
Finish: 2025-05-28 04:27:04.742668
------------------------------------------------------
Started: 2025-05-28 06:25:15.080654
Existing_entries: 1291
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1412
Summarized using GPT-3.5-turbo
Append: [Information Gain-Guided Causal Intervention for Autonomous Debiasing Large Language Models](https://arxiv.org/abs/2504.12898)
Token length: 1463
Summarized using GPT-3.5-turbo
Append: [QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning](https://arxiv.org/abs/2505.17667)
Token length: 1567
Summarized using GPT-3.5-turbo
Append: [QwenLong-CPRS: Towards $\infty$-LLMs with Dynamic Context Optimization](https://arxiv.org/abs/2505.18092)
Token length: 1220
Summarized using GPT-3.5-turbo
Append: [OrionBench: A Benchmark for Chart and Human-Recognizable Object Detection in Infographics](https://arxiv.org/abs/2505.17473)
Token length: 1465
Summarized using GPT-3.5-turbo
Append: [NeUQI: Near-Optimal Uniform Quantization Parameter Initialization](https://arxiv.org/abs/2505.17595)
Token length: 1114
Summarized using GPT-3.5-turbo
Append: [Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective](https://arxiv.org/abs/2505.17997)
Token length: 1355
Summarized using GPT-3.5-turbo
Append: [Structured Thinking Matters: Improving LLMs Generalization in Causal Inference Tasks](https://arxiv.org/abs/2505.18034)
append_entries: 7
Finish: 2025-05-28 06:25:32.168821
------------------------------------------------------
Started: 2025-05-28 08:22:24.863839
Existing_entries: 1007
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-28 08:22:25.514336
------------------------------------------------------
Started: 2025-05-28 10:18:49.376337
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-28 10:18:50.115461
------------------------------------------------------
Started: 2025-05-28 12:35:04.812962
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-28 12:35:05.455127
------------------------------------------------------
Started: 2025-05-28 14:17:45.260354
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-28 14:17:45.923769
------------------------------------------------------
Started: 2025-05-28 16:19:39.824501
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-28 16:19:40.545935
------------------------------------------------------
Started: 2025-05-28 18:22:08.933407
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-28 18:22:09.589879
------------------------------------------------------
Started: 2025-05-28 20:18:47.335795
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-28 20:18:47.984769
------------------------------------------------------
Started: 2025-05-28 22:15:33.698776
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-28 22:15:34.344525
------------------------------------------------------
Started: 2025-05-29 01:19:39.834538
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-29 01:19:40.479882
------------------------------------------------------
Started: 2025-05-29 03:11:33.079014
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-29 03:11:33.738525
------------------------------------------------------
Started: 2025-05-29 04:26:33.202518
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1331
Summarized using GPT-3.5-turbo
Append: [More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models](https://arxiv.org/abs/2505.21523)
Token length: 1037
Summarized using GPT-3.5-turbo
Append: [Loquacious Set: 25,000 Hours of Transcribed and Diverse English Speech Recognition Data for Research and Commercial Use](https://arxiv.org/abs/2505.21578)
Token length: 1306
Summarized using GPT-3.5-turbo
Append: [Rethinking Data Mixture for Large Language Models: A Comprehensive Survey and New Perspectives](https://arxiv.org/abs/2505.21598)
Token length: 1485
Summarized using GPT-3.5-turbo
Append: [R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing](https://arxiv.org/abs/2505.21600)
Token length: 1388
Summarized using GPT-3.5-turbo
Append: [How does Misinformation Affect Large Language Model Behaviors and Preferences?](https://arxiv.org/abs/2505.21608)
Token length: 1393
Summarized using GPT-3.5-turbo
Append: [Iterative Corpus Refinement for Materials Property Prediction Based on Scientific Texts](https://arxiv.org/abs/2505.21646)
Token length: 994
Summarized using GPT-3.5-turbo
Append: [Explainability of Large Language Models using SMILE: Statistical Model-agnostic Interpretability with Local Explanations](https://arxiv.org/abs/2505.21657)
Token length: 1124
Summarized using GPT-3.5-turbo
Append: [Rethinking the Outlier Distribution in Large Language Models: An In-depth Study](https://arxiv.org/abs/2505.21670)
Token length: 1510
Summarized using GPT-3.5-turbo
Append: [LLMPR: A Novel LLM-Driven Transfer Learning based Petition Ranking Model](https://arxiv.org/abs/2505.21689)
Token length: 1565
Summarized using GPT-3.5-turbo
Append: [MAKIEval: A Multilingual Automatic WiKidata-based Framework for Cultural Awareness Evaluation for LLMs](https://arxiv.org/abs/2505.21693)
Token length: 1202
Summarized using GPT-3.5-turbo
Append: [Do We Know What LLMs Don't Know? A Study of Consistency in Knowledge Probing](https://arxiv.org/abs/2505.21701)
Token length: 1240
Summarized using GPT-3.5-turbo
Append: [Assessing and Refining ChatGPT's Performance in Identifying Targeting and Inappropriate Language: A Comparative Study](https://arxiv.org/abs/2505.21710)
Token length: 1059
Summarized using GPT-3.5-turbo
Append: [Counterfactual Simulatability of LLM Explanations for Generation Tasks](https://arxiv.org/abs/2505.21740)
Token length: 1309
Summarized using GPT-3.5-turbo
Append: [BehaviorSFT: Behavioral Token Conditioning for Clinical Agents Across the Proactivity Spectrum](https://arxiv.org/abs/2505.21757)
Token length: 1330
Summarized using GPT-3.5-turbo
Append: [Calibrating LLM Confidence by Probing Perturbed Representation Stability](https://arxiv.org/abs/2505.21772)
Token length: 886
Summarized using GPT-3.5-turbo
Append: [GMU Systems for the IWSLT 2025 Low-Resource Speech Translation Shared Task](https://arxiv.org/abs/2505.21781)
Token length: 1079
Summarized using GPT-3.5-turbo
Append: [VeriTrail: Closed-Domain Hallucination Detection with Traceability](https://arxiv.org/abs/2505.21786)
Token length: 886
Summarized using GPT-3.5-turbo
Append: [Revisiting Common Assumptions about Arabic Dialects in NLP](https://arxiv.org/abs/2505.21816)
Token length: 1068
Summarized using GPT-3.5-turbo
Append: [Representative Language Generation](https://arxiv.org/abs/2505.21819)
Token length: 1313
Summarized using GPT-3.5-turbo
Append: [Principled Content Selection to Generate Diverse and Personalized Multi-Document Summaries](https://arxiv.org/abs/2505.21859)
Token length: 1105
Summarized using GPT-3.5-turbo
Append: [Evaluating the Retrieval Robustness of Large Language Models](https://arxiv.org/abs/2505.21870)
Token length: 1545
Summarized using GPT-3.5-turbo
Append: [EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV Cache Reuse](https://arxiv.org/abs/2505.21889)
Token length: 1454
Summarized using GPT-3.5-turbo
Append: [Co-Saving: Resource Aware Multi-Agent Collaboration for Software Development](https://arxiv.org/abs/2505.21898)
Token length: 1536
Summarized using GPT-3.5-turbo
Append: [Beyond Completion: A Foundation Model for General Knowledge Graph Reasoning](https://arxiv.org/abs/2505.21926)
Token length: 1933
Summarized using GPT-3.5-turbo
Append: [RedTeamCUA: Realistic Adversarial Testing of Computer-Use Agents in Hybrid Web-OS Environments](https://arxiv.org/abs/2505.21936)
Token length: 1160
Summarized using GPT-3.5-turbo
Append: [Graph-Assisted Culturally Adaptable Idiomatic Translation for Indic Languages](https://arxiv.org/abs/2505.21937)
Token length: 1268
Summarized using GPT-3.5-turbo
Append: [RISE: Reasoning Enhancement via Iterative Self-Exploration in Multi-hop Question Answering](https://arxiv.org/abs/2505.21940)
Token length: 774
Summarized using GPT-3.5-turbo
Append: [Test-Time Scaling with Repeated Sampling Improves Multilingual Text Generation](https://arxiv.org/abs/2505.21941)
Token length: 1695
Summarized using GPT-3.5-turbo
Append: [Resolving Knowledge Conflicts in Domain-specific Data Selection: A Case Study on Medical Instruction-tuning](https://arxiv.org/abs/2505.21958)
Token length: 1573
Summarized using GPT-3.5-turbo
Append: [LaMDAgent: An Autonomous Framework for Post-Training Pipeline Optimization via LLM Agents](https://arxiv.org/abs/2505.21963)
Token length: 1124
Summarized using GPT-3.5-turbo
Append: [Seeing the Threat: Vulnerabilities in Vision-Language Models to Adversarial Attack](https://arxiv.org/abs/2505.21967)
Token length: 1134
Summarized using GPT-3.5-turbo
Append: [Pearl: A Multimodal Culturally-Aware Arabic Instruction Dataset](https://arxiv.org/abs/2505.21979)
Token length: 1442
Summarized using GPT-3.5-turbo
Append: [Leveraging Interview-Informed LLMs to Model Survey Responses: Comparative Insights from AI-Generated and Human Data](https://arxiv.org/abs/2505.21997)
Token length: 1222
Summarized using GPT-3.5-turbo
Append: [Found in Translation: Measuring Multilingual LLM Consistency as Simple as Translate then Evaluate](https://arxiv.org/abs/2505.21999)
Token length: 1606
Summarized using GPT-3.5-turbo
Append: [Legal Assist AI: Leveraging Transformer-Based Model for Effective Legal Assistance](https://arxiv.org/abs/2505.22003)
Token length: 1426
Summarized using GPT-3.5-turbo
Append: [CoThink: Token-Efficient Reasoning via Instruct Models Guiding Reasoning Models](https://arxiv.org/abs/2505.22017)
Token length: 1432
Summarized using GPT-3.5-turbo
Append: [Improving Continual Pre-training Through Seamless Data Packing](https://arxiv.org/abs/2505.22018)
Token length: 1958
Summarized using GPT-3.5-turbo
Append: [VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning](https://arxiv.org/abs/2505.22019)
Token length: 1345
Summarized using GPT-3.5-turbo
Append: [Jailbreak Distillation: Renewable Safety Benchmarking](https://arxiv.org/abs/2505.22037)
Token length: 751
Summarized using GPT-3.5-turbo
Append: [Voice Adaptation for Swiss German](https://arxiv.org/abs/2505.22054)
Token length: 949
Summarized using GPT-3.5-turbo
Append: [Safeguarding Privacy of Retrieval Data against Membership Inference Attacks: Is This Query Too Close to Home?](https://arxiv.org/abs/2505.22061)
Token length: 1095
Summarized using GPT-3.5-turbo
Append: [Beyond path selection: Better LLMs for Scientific Information Extraction with MimicSFT and Relevance and Rule-induced(R$^2$)GRPO](https://arxiv.org/abs/2505.22068)
Token length: 1189
Summarized using GPT-3.5-turbo
Append: [ArgInstruct: Specialized Instruction Fine-Tuning for Computational Argumentation](https://arxiv.org/abs/2505.22076)
Token length: 1541
Summarized using GPT-3.5-turbo
Append: [Learning to Route Queries Across Knowledge Bases for Step-wise Retrieval-Augmented Reasoning](https://arxiv.org/abs/2505.22095)
Token length: 1415
Summarized using GPT-3.5-turbo
Append: [Knowledge Base Construction for Knowledge-Augmented Text-to-SQL](https://arxiv.org/abs/2505.22096)
Token length: 1537
Summarized using GPT-3.5-turbo
Append: [MemOS: An Operating System for Memory-Augmented Generation (MAG) in Large Language Models](https://arxiv.org/abs/2505.22101)
Token length: 1453
Summarized using GPT-3.5-turbo
Append: [Curse of High Dimensionality Issue in Transformer for Long-context Modeling](https://arxiv.org/abs/2505.22107)
Token length: 1158
Summarized using GPT-3.5-turbo
Append: [THINK-Bench: Evaluating Thinking Efficiency and Chain-of-Thought Quality of Large Reasoning Models](https://arxiv.org/abs/2505.22113)
Token length: 1648
Summarized using GPT-3.5-turbo
Append: [Multimodal Forecasting of Sparse Intraoperative Hypotension Events Powered by Language Model](https://arxiv.org/abs/2505.22116)
Token length: 1201
Summarized using GPT-3.5-turbo
Append: [Multilingual vs Crosslingual Retrieval of Fact-Checked Claims: A Tale of Two Approaches](https://arxiv.org/abs/2505.22118)
Append: [LoKI: Low-damage Knowledge Implanting of Large Language Models](https://arxiv.org/abs/2505.22120)
Append: [EULER: Enhancing the Reasoning Ability of Large Language Models through Error-Induced Learning](https://arxiv.org/abs/2505.22131)
Append: [RAD: Redundancy-Aware Distillation for Hybrid Models via Self-Speculative Decoding](https://arxiv.org/abs/2505.22135)
Append: [Limited Generalizability in Argument Mining: State-Of-The-Art Models Learn Datasets, Not Arguments](https://arxiv.org/abs/2505.22137)
Append: [InComeS: Integrating Compression and Selection Mechanisms into LLMs for Efficient Model Editing](https://arxiv.org/abs/2505.22156)
Append: [Stratified Selective Sampling for Instruction Tuning with Dedicated Scoring Strategy](https://arxiv.org/abs/2505.22157)
Append: [Unifying Continuous and Discrete Text Diffusion with Non-simultaneous Diffusion Processes](https://arxiv.org/abs/2505.22165)
Append: [ReliableEval: A Recipe for Stochastic LLM Evaluation via Method of Moments](https://arxiv.org/abs/2505.22169)
Append: [Reverse Preference Optimization for Complex Instruction Following](https://arxiv.org/abs/2505.22172)
Append: [TabXEval: Why this is a Bad Table? An eXhaustive Rubric for Table Evaluation](https://arxiv.org/abs/2505.22176)
Append: [Speculative Decoding Meets Quantization: Compatibility Evaluation and Hierarchical Framework Design](https://arxiv.org/abs/2505.22179)
Append: [Breaking the Cloak! Unveiling Chinese Cloaked Toxicity with Homophone Graph and Toxic Lexicon](https://arxiv.org/abs/2505.22184)
Append: [Let's Predict Sentence by Sentence](https://arxiv.org/abs/2505.22202)
Append: [Judging Quality Across Languages: A Multilingual Approach to Pretraining Data Filtering with Language Models](https://arxiv.org/abs/2505.22232)
Append: [A Linguistically Motivated Analysis of Intonational Phrasing in Text-to-Speech Systems: Revealing Gaps in Syntactic Sensitivity](https://arxiv.org/abs/2505.22236)
Append: [BioHopR: A Benchmark for Multi-Hop, Multi-Answer Reasoning in Biomedical Domain](https://arxiv.org/abs/2505.22240)
Append: [MRT at SemEval-2025 Task 8: Maximizing Recovery from Tables with Multiple Steps](https://arxiv.org/abs/2505.22264)
Append: [Comprehensive Evaluation on Lexical Normalization: Boundary-Aware Approaches for Unsegmented Languages](https://arxiv.org/abs/2505.22273)
Append: [Natural Language Processing in Support of Evidence-based Medicine: A Scoping Review](https://arxiv.org/abs/2505.22280)
Append: [Compensating for Data with Reasoning: Low-Resource Machine Translation with LLMs](https://arxiv.org/abs/2505.22293)
Append: [360-LLaMA-Factory: Plug & Play Sequence Parallelism for Long Post-Training](https://arxiv.org/abs/2505.22296)
Append: [Adaptive Detoxification: Safeguarding General Capabilities of LLMs through Toxicity-Aware Knowledge Editing](https://arxiv.org/abs/2505.22298)
Append: [If Pigs Could Fly... Can LLMs Logically Reason Through Counterfactuals?](https://arxiv.org/abs/2505.22318)
Append: [Advancing Expert Specialization for Better MoE](https://arxiv.org/abs/2505.22323)
Append: [NLP for Social Good: A Survey of Challenges, Opportunities, and Responsible Deployment](https://arxiv.org/abs/2505.22327)
Append: [Advancing Multimodal Reasoning via Reinforcement Learning with Cold Start](https://arxiv.org/abs/2505.22334)
Append: [Text2Grad: Reinforcement Learning from Natural Language Feedback](https://arxiv.org/abs/2505.22338)
Append: [LLMs Struggle to Reject False Presuppositions when Misinformation Stakes are High](https://arxiv.org/abs/2505.22354)
Append: [Pangu Embedded: An Efficient Dual-system LLM Reasoner with Metacognition](https://arxiv.org/abs/2505.22375)
Append: [RAG-Zeval: Towards Robust and Interpretable Evaluation on RAG Responses through End-to-End Rule-Guided Reasoning](https://arxiv.org/abs/2505.22430)
Append: [Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO](https://arxiv.org/abs/2505.22453)
Append: [EvolveSearch: An Iterative Self-Evolving Search Agent](https://arxiv.org/abs/2505.22501)
Append: [Multi-MLLM Knowledge Distillation for Out-of-Context News Detection](https://arxiv.org/abs/2505.22517)
Append: [Emotion-o1: Adaptive Long Reasoning for Emotion Understanding in LLMs](https://arxiv.org/abs/2505.22548)
Append: [ClaimPKG: Enhancing Claim Verification via Pseudo-Subgraph Generation with Lightweight Specialized LLM](https://arxiv.org/abs/2505.22552)
Append: [Do Large Language Models Think Like the Brain? Sentence-Level Evidence from fMRI and Hierarchical Embeddings](https://arxiv.org/abs/2505.22563)
Append: [Agent-UniRAG: A Trainable Open-Source LLM Agent Framework for Unified Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2505.22571)
Append: [Fusion Steering: Prompt-Specific Activation Control](https://arxiv.org/abs/2505.22572)
Append: [Less, but Better: Efficient Multilingual Expansion for LLMs via Layer-wise Mixture-of-Experts](https://arxiv.org/abs/2505.22582)
Append: [Precise In-Parameter Concept Erasure in Large Language Models](https://arxiv.org/abs/2505.22586)
Append: [Self-Error-Instruct: Generalizing from Errors for LLMs Mathematical Reasoning](https://arxiv.org/abs/2505.22591)
Append: [Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding](https://arxiv.org/abs/2505.22618)
Append: [Chain-of-Talkers (CoTalk): Fast Human Annotation of Dense Image Captions](https://arxiv.org/abs/2505.22627)
Append: [Stochastic Chameleons: Irrelevant Context Hallucinations Reveal Class-Based (Mis)Generalization in LLMs](https://arxiv.org/abs/2505.22630)
Append: [Spatial Knowledge Graph-Guided Multimodal Synthesis](https://arxiv.org/abs/2505.22633)
Append: [Learning Composable Chains-of-Thought](https://arxiv.org/abs/2505.22635)
Append: [Characterizing Bias: Benchmarking Large Language Models in Simplified versus Traditional Chinese](https://arxiv.org/abs/2505.22645)
Append: [WebDancer: Towards Autonomous Information Seeking Agency](https://arxiv.org/abs/2505.22648)
Append: [The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in Learning to Reason](https://arxiv.org/abs/2505.22653)
Append: [GuessArena: Guess Who I Am? A Self-Adaptive Framework for Evaluating LLMs in Domain-Specific Knowledge and Reasoning](https://arxiv.org/abs/2505.22661)
Append: [AutoL2S: Auto Long-Short Reasoning for Efficient Large Language Models](https://arxiv.org/abs/2505.22662)
Append: [Capability-Based Scaling Laws for LLM Red-Teaming](https://arxiv.org/abs/2505.20162)
Append: [Complexity counts: global and local perspectives on Indo-Aryan numeral systems](https://arxiv.org/abs/2505.21510)
Append: [VietASR: Achieving Industry-level Vietnamese ASR with 50-hour labeled data and Large-Scale Speech Pretraining](https://arxiv.org/abs/2505.21527)
Append: [How Much Do Large Language Models Know about Human Motion? A Case Study in 3D Avatar Control](https://arxiv.org/abs/2505.21531)
Append: [Vision Meets Language: A RAG-Augmented YOLOv8 Framework for Coffee Disease Diagnosis and Farmer Assistance](https://arxiv.org/abs/2505.21544)
Append: [Fluent but Culturally Distant: Can Regional Training Teach Cultural Understanding?](https://arxiv.org/abs/2505.21548)
Append: [Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal Transformer Distillation](https://arxiv.org/abs/2505.21549)
Append: [ChemHAS: Hierarchical Agent Stacking for Enhancing Chemistry Tools](https://arxiv.org/abs/2505.21569)
Append: [R1-Code-Interpreter: Training LLMs to Reason with Code via Supervised and Reinforcement Learning](https://arxiv.org/abs/2505.21668)
Append: [Revisiting Bi-Linear State Transitions in Recurrent Neural Networks](https://arxiv.org/abs/2505.21749)
Append: [From prosthetic memory to prosthetic denial: Auditing whether large language models are prone to mass atrocity denialism](https://arxiv.org/abs/2505.21753)
Append: [FRAMES-VQA: Benchmarking Fine-Tuning Robustness across Multi-Modal Shifts in Visual Question Answering](https://arxiv.org/abs/2505.21755)
Append: [Towards Safety Reasoning in LLMs: AI-agentic Deliberation for Policy-embedded CoT Data Creation](https://arxiv.org/abs/2505.21784)
Append: [Born a Transformer -- Always a Transformer?](https://arxiv.org/abs/2505.21785)
Append: [From Directions to Cones: Exploring Multidimensional Representations of Propositional Facts in LLMs](https://arxiv.org/abs/2505.21800)
Append: [Scientific Paper Retrieval with LLM-Guided Semantic-Based Ranking](https://arxiv.org/abs/2505.21815)
Append: [Let Me Think! A Long Chain-of-Thought Can Be Worth Exponentially Many Short Ones](https://arxiv.org/abs/2505.21825)
Append: [GETReason: Enhancing Image Context Extraction through Hierarchical Multi-Agent Reasoning](https://arxiv.org/abs/2505.21863)
Append: [Incorporating LLMs for Large-Scale Urban Complex Mobility Simulation](https://arxiv.org/abs/2505.21880)
Append: [Modeling and Optimizing User Preferences in AI Copilots: A Comprehensive Survey and Taxonomy](https://arxiv.org/abs/2505.21907)
Append: [Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets](https://arxiv.org/abs/2505.21930)
Append: [Cross-modal RAG: Sub-dimensional Retrieval-Augmented Text-to-Image Generation](https://arxiv.org/abs/2505.21956)
Append: [EnsemW2S: Enhancing Weak-to-Strong Generalization with Large Language Model Ensembles](https://arxiv.org/abs/2505.21959)
Append: [UI-Evol: Automatic Knowledge Evolving for Computer Use Agents](https://arxiv.org/abs/2505.21964)
Append: [MapStory: LLM-Powered Text-Driven Map Animation Prototyping with Human-in-the-Loop Editing](https://arxiv.org/abs/2505.21966)
Append: [Learning Compositional Behaviors from Demonstration and Language](https://arxiv.org/abs/2505.21981)
Append: [Visual Cues Support Robust Turn-taking Prediction in Noise](https://arxiv.org/abs/2505.22088)
Append: [Flexible Tool Selection through Low-dimensional Attribute Alignment of Vision and Language](https://arxiv.org/abs/2505.22146)
Append: [Improving Brain-to-Image Reconstruction via Fine-Grained Text Bridging](https://arxiv.org/abs/2505.22150)
Append: [Pitfalls of Rule- and Model-based Verifiers -- A Case Study on Mathematical Reasoning](https://arxiv.org/abs/2505.22203)
Append: [Look & Mark: Leveraging Radiologist Eye Fixations and Bounding boxes in Multimodal Large Language Models for Chest X-ray Report Generation](https://arxiv.org/abs/2505.22222)
Append: [Advancing Hearing Assessment: An ASR-Based Frequency-Specific Speech Test for Diagnosing Presbycusis](https://arxiv.org/abs/2505.22231)
Append: [Evaluation of LLMs in Speech is Often Flawed: Test Set Contamination in Large Language Models for Speech Recognition](https://arxiv.org/abs/2505.22251)
Append: [Train Sparse Autoencoders Efficiently by Utilizing Features Correlation](https://arxiv.org/abs/2505.22255)
Append: [Test-Time Immunization: A Universal Defense Framework Against Jailbreaks for (Multimodal) Large Language Models](https://arxiv.org/abs/2505.22271)
Append: [Rethinking the Unsolvable: When In-Context Search Meets Test-Time Scaling](https://arxiv.org/abs/2505.22290)
Append: [Skywork Open Reasoner 1 Technical Report](https://arxiv.org/abs/2505.22312)
Append: [Mitigating Overthinking in Large Reasoning Models via Manifold Steering](https://arxiv.org/abs/2505.22411)
Append: [Scaling Reasoning without Attention](https://arxiv.org/abs/2505.22425)
Append: [Fostering Video Reasoning via Next-Event Prediction](https://arxiv.org/abs/2505.22457)
Append: [Effective Context in Neural Speech Models](https://arxiv.org/abs/2505.22487)
Append: [Thinking with Generated Images](https://arxiv.org/abs/2505.22525)
Append: [RICO: Improving Accuracy and Completeness in Image Recaptioning via Visual Reconstruction](https://arxiv.org/abs/2505.22613)
Append: [The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models](https://arxiv.org/abs/2505.22617)
Append: [Sherlock: Self-Correcting Reasoning in Vision-Language Models](https://arxiv.org/abs/2505.22651)
Append: [Position: Uncertainty Quantification Needs Reassessment for Large-language Model Agents](https://arxiv.org/abs/2505.22655)
Append: [3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model](https://arxiv.org/abs/2505.22657)
Append: [Machine Translation Models are Zero-Shot Detectors of Translation Direction](https://arxiv.org/abs/2401.06769)
Append: [Tracking Semantic Change in Slovene: A Novel Dataset and Optimal Transport-Based Distance](https://arxiv.org/abs/2402.16596)
Append: [Personality-aware Student Simulation for Conversational Intelligent Tutoring Systems](https://arxiv.org/abs/2404.06762)
Append: [Mitigating Text Toxicity with Counterfactual Generation](https://arxiv.org/abs/2405.09948)
Append: [REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space](https://arxiv.org/abs/2406.09325)
Append: [Evaluating Implicit Bias in Large Language Models by Attacking From a Psychometric Perspective](https://arxiv.org/abs/2406.14023)
Append: [Dissecting the Ullman Variations with a SCALPEL: Why do LLMs fail at Trivial Alterations to the False Belief Task?](https://arxiv.org/abs/2406.14737)
Append: [Large Vocabulary Size Improves Large Language Models](https://arxiv.org/abs/2406.16508)
Append: [Empirical analysis of binding precedent efficiency in Brazilian Supreme Court via case classification](https://arxiv.org/abs/2407.07004)
Append: [Prompt-based Personality Profiling: Reinforcement Learning for Relevance Filtering](https://arxiv.org/abs/2409.04122)
Append: [Nonlinear second-order dynamics describe labial constriction trajectories across languages and contexts](https://arxiv.org/abs/2410.08351)
Append: [Which Demographics do LLMs Default to During Annotation?](https://arxiv.org/abs/2410.08820)
Append: [Graph-constrained Reasoning: Faithful Reasoning on Knowledge Graphs with Large Language Models](https://arxiv.org/abs/2410.13080)
Append: [SafetyAnalyst: Interpretable, Transparent, and Steerable Safety Moderation for AI Behavior](https://arxiv.org/abs/2410.16665)
Append: [Can Code-Switched Texts Activate a Knowledge Switch in LLMs? A Case Study on English-Korean Code-Switching](https://arxiv.org/abs/2410.18436)
Append: [Understanding Synthetic Context Extension via Retrieval Heads](https://arxiv.org/abs/2410.22316)
Append: [Controllable Context Sensitivity and the Knob Behind It](https://arxiv.org/abs/2411.07404)
Append: [LL\"aMmlein: Compact and Competitive German-Only Language Models from Scratch](https://arxiv.org/abs/2411.11171)
Append: [Overcoming Non-monotonicity in Transducer-based Streaming Generation](https://arxiv.org/abs/2411.17170)
Append: [ConKE: Conceptualization-Augmented Knowledge Editing in Large Language Models for Commonsense Reasoning](https://arxiv.org/abs/2412.11418)
Append: [Core Context Aware Transformers for Long Context Language Modeling](https://arxiv.org/abs/2412.12465)
Append: [Revisiting In-Context Learning with Long Context Language Models](https://arxiv.org/abs/2412.16926)
Append: [FitCF: A Framework for Automatic Feature Importance-guided Counterfactual Example Generation](https://arxiv.org/abs/2501.00777)
Append: [PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models](https://arxiv.org/abs/2501.03124)
Append: [LLMs Reproduce Stereotypes of Sexual and Gender Minorities](https://arxiv.org/abs/2501.05926)
Append: [Gender-Neutral Large Language Models for Medical Applications: Reducing Bias in PubMed Abstracts](https://arxiv.org/abs/2501.06365)
Append: [K-COMP: Retrieval-Augmented Medical Domain Question Answering With Knowledge-Injected Compressor](https://arxiv.org/abs/2501.13567)
Append: [Redundancy Principles for MLLMs Benchmarks](https://arxiv.org/abs/2501.13953)
Append: [Domaino1s: Guiding LLM Reasoning for Explainable Answers in High-Stakes Domains](https://arxiv.org/abs/2501.14431)
Append: [Mitigating Heterogeneous Token Overfitting in LLM Knowledge Editing](https://arxiv.org/abs/2502.00602)
Append: [Advancing Reasoning in Large Language Models: Promising Methods and Approaches](https://arxiv.org/abs/2502.03671)
Append: [Beyond External Monitors: Enhancing Transparency of Large Language Models for Easier Monitoring](https://arxiv.org/abs/2502.05242)
Append: [LongReD: Mitigating Short-Text Degradation of Long-Context Large Language Models via Restoration Distillation](https://arxiv.org/abs/2502.07365)
Append: [CoSER: Coordinating LLM-Based Persona Simulation of Established Roles](https://arxiv.org/abs/2502.09082)
Append: [Towards Achieving Concept Completeness for Textual Concept Bottleneck Models](https://arxiv.org/abs/2502.11100)
Append: [ReLearn: Unlearning via Learning for Large Language Models](https://arxiv.org/abs/2502.11190)
Append: [Which Retain Set Matters for LLM Unlearning? A Case Study on Entity Unlearning](https://arxiv.org/abs/2502.11441)
Append: [BRIGHTER: BRIdging the Gap in Human-Annotated Textual Emotion Recognition Datasets for 28 Languages](https://arxiv.org/abs/2502.11926)
Append: [ThinkGuard: Deliberative Slow Thinking Leads to Cautious Guardrails](https://arxiv.org/abs/2502.13458)
Append: [How Do LLMs Perform Two-Hop Reasoning in Context?](https://arxiv.org/abs/2502.13913)
Append: [Mitigating Lost-in-Retrieval Problems in Retrieval Augmented Multi-Hop Question Answering](https://arxiv.org/abs/2502.14245)
Append: [Self-Taught Agentic Long Context Understanding](https://arxiv.org/abs/2502.15920)
Append: [Personalized Causal Graph Reasoning for LLMs: A Case Study on Dietary Recommendations](https://arxiv.org/abs/2503.00134)
Append: [Shaping Shared Languages: Human and Large Language Models' Inductive Biases in Emergent Communication](https://arxiv.org/abs/2503.04395)
Append: [Odysseus Navigates the Sirens' Song: Dynamic Focus Decoding for Factual and Diverse Open-Ended Text Generation](https://arxiv.org/abs/2503.08057)
Append: [Explicit Learning and the LLM in Machine Translation](https://arxiv.org/abs/2503.09454)
Append: [Probabilistic Reasoning with LLMs for k-anonymity Estimation](https://arxiv.org/abs/2503.09674)
Append: [Constrained Discrete Diffusion](https://arxiv.org/abs/2503.09790)
Append: [Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and Beyond](https://arxiv.org/abs/2503.10460)
Append: [TLUE: A Tibetan Language Understanding Evaluation Benchmark](https://arxiv.org/abs/2503.12051)
Append: [Experience Retrieval-Augmentation with Electronic Health Records Enables Accurate Discharge QA](https://arxiv.org/abs/2503.17933)
Append: [Sun-Shine: A Foundation Large Language Model for Tibetan Culture and Heritage](https://arxiv.org/abs/2503.18288)
Append: [Token embeddings violate the manifold hypothesis](https://arxiv.org/abs/2504.01002)
Append: [Evaluating Compact LLMs for Zero-Shot Iberian Language Tasks on End-User Devices](https://arxiv.org/abs/2504.03312)
Append: [SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge Refinement](https://arxiv.org/abs/2504.03561)
Append: [AI for Climate Finance: Agentic Retrieval and Multi-Step Reasoning for Early Warning System Investments](https://arxiv.org/abs/2504.05104)
Append: [Domain-Specific Pruning of Large Mixture-of-Experts Models with Few-shot Demonstrations](https://arxiv.org/abs/2504.06792)
Append: [Layers at Similar Depths Generate Similar Activations Across LLM Architectures](https://arxiv.org/abs/2504.08775)
Append: [GOAT-TTS: Expressive and Realistic Speech Generation via A Dual-Branch LLM](https://arxiv.org/abs/2504.12339)
Append: [ClonEval: An Open Voice Cloning Benchmark](https://arxiv.org/abs/2504.20581)
Append: [Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL](https://arxiv.org/abs/2505.10832)
Append: [Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning of LLMs](https://arxiv.org/abs/2505.11277)
Append: [Advancing Sequential Numerical Prediction in Autoregressive Models](https://arxiv.org/abs/2505.13077)
Append: [Language-Specific Latent Process Hinders Cross-Lingual Performance](https://arxiv.org/abs/2505.13141)
Append: [Positional Fragility in LLMs: How Offset Effects Reshape Our Understanding of Memorization Risks](https://arxiv.org/abs/2505.13171)
Append: [Adapting Pretrained Language Models for Citation Classification via Self-Supervised Contrastive Learning](https://arxiv.org/abs/2505.14471)
Append: [General-Reasoner: Advancing LLM Reasoning Across All Domains](https://arxiv.org/abs/2505.14652)
Append: [Text Generation Beyond Discrete Token Sampling](https://arxiv.org/abs/2505.14827)
Append: [KaFT: Knowledge-aware Fine-tuning for Boosting LLMs' Domain-specific Question-Answering Performance](https://arxiv.org/abs/2505.15480)
Append: [EduBench: A Comprehensive Benchmarking Dataset for Evaluating Large Language Models in Diverse Educational Scenarios](https://arxiv.org/abs/2505.16160)
Append: [When Do LLMs Admit Their Mistakes? Understanding the Role of Model Belief in Retraction](https://arxiv.org/abs/2505.16170)
Append: [Wolf Hidden in Sheep's Conversations: Toward Harmless Data-Based Backdoor Attacks for Jailbreaking Large Language Models](https://arxiv.org/abs/2505.17601)
Append: [Watermarks in the Sand: Impossibility of Strong Watermarking for Generative Models](https://arxiv.org/abs/2311.04378)
Append: [Mini-batch Coresets for Memory-efficient Language Model Training on Data Mixtures](https://arxiv.org/abs/2407.19580)
Append: [VQ-CTAP: Cross-Modal Fine-Grained Sequence Representation Learning for Speech Processing](https://arxiv.org/abs/2408.05758)
Append: [Fine-Grained and Thematic Evaluation of LLMs in Social Deduction Game](https://arxiv.org/abs/2408.09946)
Append: [On the Within-class Variation Issue in Alzheimer's Disease Detection](https://arxiv.org/abs/2409.16322)
Append: [Exploring the Limitations of Mamba in COPY and CoT Reasoning](https://arxiv.org/abs/2410.03810)
Append: [AdvAgent: Controllable Blackbox Red-teaming on Web Agents](https://arxiv.org/abs/2410.17401)
Append: [Natural Language Reinforcement Learning](https://arxiv.org/abs/2411.14251)
Append: [AutoElicit: Using Large Language Models for Expert Prior Elicitation in Predictive Modelling](https://arxiv.org/abs/2411.17284)
Append: [Preference Adaptive and Sequential Text-to-Image Generation](https://arxiv.org/abs/2412.10419)
Append: [You Do Not Fully Utilize Transformer's Representation Capacity](https://arxiv.org/abs/2502.09245)
Append: [Non-Markovian Discrete Diffusion with Causal Language Models](https://arxiv.org/abs/2502.09767)
Append: [Closed-Form Training Dynamics Reveal Learned Features and Linear Structure in Word2Vec-like Models](https://arxiv.org/abs/2502.09863)
Append: [Leveraging Dual Process Theory in Language Agent Framework for Real-time Simultaneous Human-AI Collaboration](https://arxiv.org/abs/2502.11882)
Append: [MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway Dynamic Dense Connections](https://arxiv.org/abs/2502.12170)
Append: [WiseMind: Recontextualizing AI with a Knowledge-Guided, Theory-Informed Multi-Agent Framework for Instrumental and Humanistic Benefits](https://arxiv.org/abs/2502.20689)
Append: [Wanda++: Pruning Large Language Models via Regional Gradients](https://arxiv.org/abs/2503.04992)
Append: [WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image Generation](https://arxiv.org/abs/2503.07265)
Append: [Tempest: Autonomous Multi-Turn Jailbreaking of Large Language Models with Tree Search](https://arxiv.org/abs/2503.10619)
Append: [Generative Framework for Personalized Persuasion: Inferring Causal, Counterfactual, and Latent Knowledge](https://arxiv.org/abs/2504.13904)
Append: [A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment](https://arxiv.org/abs/2504.15585)
Append: [Enhancing Target-unspecific Tasks through a Features Matrix](https://arxiv.org/abs/2505.03414)
Append: [Benchmarking LLMs' Swarm intelligence](https://arxiv.org/abs/2505.04364)
Append: [From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework for Reasoning-Driven Pedagogical Visualization](https://arxiv.org/abs/2505.16832)
Append: [Towards Practical Defect-Focused Automated Code Review](https://arxiv.org/abs/2505.17928)
Append: [Deep Video Discovery: Agentic Search with Tool Use for Long-form Video Understanding](https://arxiv.org/abs/2505.18079)
Append: [Bridging Supervised Learning and Reinforcement Learning in Math Reasoning](https://arxiv.org/abs/2505.18116)
append_entries: 247
Finish: 2025-05-29 04:28:44.012511
------------------------------------------------------
Started: 2025-05-29 06:25:38.653807
Existing_entries: 1247
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1426
Summarized using GPT-3.5-turbo
Append: [GraphCheck: Breaking Long-Term Text Barriers with Extracted Knowledge Graph-Powered Fact-Checking](https://arxiv.org/abs/2502.16514)
Token length: 1457
Summarized using GPT-3.5-turbo
Append: [LINGOLY-TOO: Disentangling Reasoning from Knowledge with Templatised Orthographic Obfuscation](https://arxiv.org/abs/2503.02972)
Token length: 1337
Summarized using GPT-3.5-turbo
Append: [CULEMO: Cultural Lenses on Emotion -- Benchmarking LLMs for Cross-Cultural Emotion Understanding](https://arxiv.org/abs/2503.10688)
append_entries: 3
Finish: 2025-05-29 06:25:48.119924
------------------------------------------------------
Started: 2025-05-29 08:22:27.216220
Existing_entries: 1003
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-29 08:22:27.795545
------------------------------------------------------
Started: 2025-05-29 10:18:44.271650
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-29 10:18:44.822579
------------------------------------------------------
Started: 2025-05-29 12:34:00.300977
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-29 12:34:00.858918
------------------------------------------------------
Started: 2025-05-29 14:16:13.619860
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-29 14:16:14.203877
------------------------------------------------------
Started: 2025-05-29 16:20:19.301069
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-29 16:20:19.908487
------------------------------------------------------
Started: 2025-05-29 18:22:50.090030
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-29 18:22:50.694769
------------------------------------------------------
Started: 2025-05-29 20:18:51.623273
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-29 20:18:52.274538
------------------------------------------------------
Started: 2025-05-29 22:15:53.687654
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-29 22:15:54.281277
------------------------------------------------------
Started: 2025-05-30 01:17:33.438446
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-30 01:17:34.049622
------------------------------------------------------
Started: 2025-05-30 03:09:40.627553
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-30 03:09:41.185059
------------------------------------------------------
Started: 2025-05-30 04:24:45.166820
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1257
Summarized using GPT-3.5-turbo
Append: [Training Language Models to Generate Quality Code with Program Analysis Feedback](https://arxiv.org/abs/2505.22704)
Token length: 801
Summarized using GPT-3.5-turbo
Append: [Climate Finance Bench](https://arxiv.org/abs/2505.22752)
Token length: 1163
Summarized using GPT-3.5-turbo
Append: [Pre-Training Curriculum for Multi-Token Prediction in Language Models](https://arxiv.org/abs/2505.22757)
Token length: 1030
Summarized using GPT-3.5-turbo
Append: [FAMA: The First Large-Scale Open-Science Speech Foundation Model for English and Italian](https://arxiv.org/abs/2505.22759)
Token length: 1621
Summarized using GPT-3.5-turbo
Append: [StressTest: Can YOUR Speech LM Handle the Stress?](https://arxiv.org/abs/2505.22765)
Token length: 947
Summarized using GPT-3.5-turbo
Append: [Automated Essay Scoring Incorporating Annotations from Automated Feedback Systems](https://arxiv.org/abs/2505.22771)
Token length: 1609
Summarized using GPT-3.5-turbo
Append: [Counting trees: A treebank-driven exploration of syntactic variation in speech and writing across languages](https://arxiv.org/abs/2505.22774)
Token length: 1340
Summarized using GPT-3.5-turbo
Append: [MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain Chatbots and Dialogue Evaluators](https://arxiv.org/abs/2505.22777)
Token length: 1816
Summarized using GPT-3.5-turbo
Append: [Can Large Language Models Match the Conclusions of Systematic Reviews?](https://arxiv.org/abs/2505.22787)
Token length: 1016
Summarized using GPT-3.5-turbo
Append: [Towards a More Generalized Approach in Open Relation Extraction](https://arxiv.org/abs/2505.22801)
Token length: 1065
Summarized using GPT-3.5-turbo
Append: [First Steps Towards Overhearing LLM Agents: A Case Study With Dungeons & Dragons Gameplay](https://arxiv.org/abs/2505.22809)
Token length: 1567
Summarized using GPT-3.5-turbo
Append: [Self-Critique and Refinement for Faithful Natural Language Explanations](https://arxiv.org/abs/2505.22823)
Token length: 1226
Summarized using GPT-3.5-turbo
Append: [What Has Been Lost with Synthetic Evaluation?](https://arxiv.org/abs/2505.22830)
Token length: 927
Summarized using GPT-3.5-turbo
Append: [Bayesian Attention Mechanism: A Probabilistic Framework for Positional Encoding and Context Length Extrapolation](https://arxiv.org/abs/2505.22842)
Token length: 1472
Summarized using GPT-3.5-turbo
Append: [LiTEx: A Linguistic Taxonomy of Explanations for Understanding Within-Label Variation in Natural Language Inference](https://arxiv.org/abs/2505.22848)
Token length: 1134
Summarized using GPT-3.5-turbo
Append: [GateNLP at SemEval-2025 Task 10: Hierarchical Three-Step Prompting for Multilingual Narrative Classification](https://arxiv.org/abs/2505.22867)
Token length: 1183
Summarized using GPT-3.5-turbo
Append: [When Models Reason in Your Language: Controlling Thinking Trace Language Comes at the Cost of Accuracy](https://arxiv.org/abs/2505.22888)
Token length: 1135
Summarized using GPT-3.5-turbo
Append: [VIGNETTE: Socially Grounded Bias Evaluation for Vision-Language Models](https://arxiv.org/abs/2505.22897)
Token length: 862
Summarized using GPT-3.5-turbo
Append: [Talent or Luck? Evaluating Attribution Bias in Large Language Models](https://arxiv.org/abs/2505.22910)
Token length: 1660
Summarized using GPT-3.5-turbo
Append: [ER-REASON: A Benchmark Dataset for LLM-Based Clinical Reasoning in the Emergency Room](https://arxiv.org/abs/2505.22919)
Token length: 1693
Summarized using GPT-3.5-turbo
Append: [Structured Memory Mechanisms for Stable Context Representation in Large Language Models](https://arxiv.org/abs/2505.22921)
Token length: 1360
Summarized using GPT-3.5-turbo
Append: [Unraveling LoRA Interference: Orthogonal Subspaces for Robust Model Merging](https://arxiv.org/abs/2505.22934)
Token length: 912
Summarized using GPT-3.5-turbo
Append: [Improving QA Efficiency with DistilBERT: Fine-Tuning and Inference on mobile Intel CPUs](https://arxiv.org/abs/2505.22937)
Token length: 1187
Summarized using GPT-3.5-turbo
Append: [WorkForceAgent-R1: Incentivizing Reasoning Capability in LLM-based Web Agents via Reinforcement Learning](https://arxiv.org/abs/2505.22942)
Token length: 982
Summarized using GPT-3.5-turbo
Append: [Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of Pre-trained Multimodal Representation via Text Updates](https://arxiv.org/abs/2505.22943)
Token length: 1567
Summarized using GPT-3.5-turbo
Append: [OWL: Probing Cross-Lingual Recall of Memorized Texts via World Literature](https://arxiv.org/abs/2505.22945)
Token length: 1191
Summarized using GPT-3.5-turbo
Append: [NegVQA: Can Vision Language Models Understand Negation?](https://arxiv.org/abs/2505.22946)
Token length: 1142
Summarized using GPT-3.5-turbo
Append: [StrucSum: Graph-Structured Reasoning for Long Document Extractive Summarization with LLMs](https://arxiv.org/abs/2505.22950)
Token length: 1061
Summarized using GPT-3.5-turbo
Append: [LLMs for Argument Mining: Detection, Extraction, and Relationship Classification of pre-defined Arguments in Online Comments](https://arxiv.org/abs/2505.22956)
Token length: 1710
Summarized using GPT-3.5-turbo
Append: [LLM-based HSE Compliance Assessment: Benchmark, Performance, and Advancements](https://arxiv.org/abs/2505.22959)
Token length: 1810
Summarized using GPT-3.5-turbo
Append: [ToMAP: Training Opponent-Aware LLM Persuaders with Theory of Mind](https://arxiv.org/abs/2505.22961)
Token length: 1206
Summarized using GPT-3.5-turbo
Append: [Exploring Scaling Laws for EHR Foundation Models](https://arxiv.org/abs/2505.22964)
Token length: 1586
Summarized using GPT-3.5-turbo
Append: [Verify-in-the-Graph: Entity Disambiguation Enhancement for Complex Claim Verification with Interactive Graph Representation](https://arxiv.org/abs/2505.22993)
Token length: 1401
Summarized using GPT-3.5-turbo
Append: [DyePack: Provably Flagging Test Set Contamination in LLMs Using Backdoors](https://arxiv.org/abs/2505.23001)
Token length: 1057
Summarized using GPT-3.5-turbo
Append: [A Practical Approach for Building Production-Grade Conversational Agents with Workflow Graphs](https://arxiv.org/abs/2505.23006)
Token length: 1652
Summarized using GPT-3.5-turbo
Append: [Detecting Stealthy Backdoor Samples based on Intra-class Distance for Large Language Models](https://arxiv.org/abs/2505.23015)
Token length: 1215
Summarized using GPT-3.5-turbo
Append: [Context Robust Knowledge Editing for Language Models](https://arxiv.org/abs/2505.23026)
Token length: 1165
Summarized using GPT-3.5-turbo
Append: [Uncovering Visual-Semantic Psycholinguistic Properties from the Distributional Structure of Text Embedding Spac](https://arxiv.org/abs/2505.23029)
Token length: 1648
Summarized using GPT-3.5-turbo
Append: [Can Modern NLP Systems Reliably Annotate Chest Radiography Exams? A Pre-Purchase Evaluation and Comparative Study of Solutions from AWS, Google, Azure, John Snow Labs, and Open-Source Models on an Independent Pediatric Dataset](https://arxiv.org/abs/2505.23030)
Token length: 1369
Summarized using GPT-3.5-turbo
Append: [Machine-Facing English: Defining a Hybrid Register Shaped by Human-AI Discourse](https://arxiv.org/abs/2505.23035)
Token length: 1087
Summarized using GPT-3.5-turbo
Append: [Improving Multilingual Social Media Insights: Aspect-based Comment Analysis](https://arxiv.org/abs/2505.23037)
Token length: 1768
Summarized using GPT-3.5-turbo
Append: [EL4NER: Ensemble Learning for Named Entity Recognition via Multiple Small-Parameter Large Language Models](https://arxiv.org/abs/2505.23038)
Token length: 1365
Summarized using GPT-3.5-turbo
Append: [Query Routing for Retrieval-Augmented Language Models](https://arxiv.org/abs/2505.23052)
Token length: 1445
Summarized using GPT-3.5-turbo
Append: [Self-Correcting Code Generation Using Small Language Models](https://arxiv.org/abs/2505.23060)
Token length: 1296
Summarized using GPT-3.5-turbo
Append: [SNS-Bench-VL: Benchmarking Multimodal Large Language Models in Social Networking Services](https://arxiv.org/abs/2505.23065)
Token length: 1188
Summarized using GPT-3.5-turbo
Append: [Document-Level Text Generation with Minimum Bayes Risk Decoding using Optimal Transport](https://arxiv.org/abs/2505.23078)
Token length: 1232
Summarized using GPT-3.5-turbo
Append: [Generating Diverse Training Samples for Relation Extraction with Large Language Models](https://arxiv.org/abs/2505.23108)
Token length: 1225
Summarized using GPT-3.5-turbo
Append: [Dataset Cartography for Large Language Model Alignment: Mapping and Diagnosing Preference Data](https://arxiv.org/abs/2505.23114)
Token length: 1372
Summarized using GPT-3.5-turbo
Append: [Elicit and Enhance: Advancing Multimodal Reasoning in Medical Scenarios](https://arxiv.org/abs/2505.23118)
Token length: 1057
Summarized using GPT-3.5-turbo
Append: [ContextQFormer: A New Context Modeling Method for Multi-Turn Multi-Modal Conversations](https://arxiv.org/abs/2505.23121)
Append: [PBEBench: A Multi-Step Programming by Examples Reasoning Benchmark inspired by Historical Linguistics](https://arxiv.org/abs/2505.23126)
Append: [Enhancing Large Language Models'Machine Translation via Dynamic Focus Anchoring](https://arxiv.org/abs/2505.23140)
Append: [Cross-Domain Bilingual Lexicon Induction via Pretrained Language Models](https://arxiv.org/abs/2505.23146)
Append: [Tell, Don't Show: Leveraging Language Models' Abstractive Retellings to Model Literary Themes](https://arxiv.org/abs/2505.23166)
Append: [ZIPA: A family of efficient models for multilingual phone recognition](https://arxiv.org/abs/2505.23170)
Append: [Map&Make: Schema Guided Text to Table Generation](https://arxiv.org/abs/2505.23174)
Append: [Infinite-Instruct: Synthesizing Scaling Code instruction Data with Bidirectional Synthesis and Static Verification](https://arxiv.org/abs/2505.23177)
Append: [Unsupervised Word-level Quality Estimation for Machine Translation Through the Lens of Annotators (Dis)agreement](https://arxiv.org/abs/2505.23183)
Append: [Cross-Task Experiential Learning on LLM-based Multi-Agent Collaboration](https://arxiv.org/abs/2505.23187)
Append: [ExpeTrans: LLMs Are Experiential Transfer Learners](https://arxiv.org/abs/2505.23191)
Append: [MMBoundary: Advancing MLLM Knowledge Boundary Awareness through Reasoning Step Confidence Calibration](https://arxiv.org/abs/2505.23224)
Append: [MCTSr-Zero: Self-Reflective Psychological Counseling Dialogues Generation via Principles and Adaptive Exploration](https://arxiv.org/abs/2505.23229)
Append: [ChartMind: A Comprehensive Benchmark for Complex Real-world Multimodal Chart Question Answering](https://arxiv.org/abs/2505.23242)
Append: [Automatic Construction of Multiple Classification Dimensions for Managing Approaches in Scientific Papers](https://arxiv.org/abs/2505.23252)
Append: [The Arabic AI Fingerprint: Stylometric Analysis and Detection of Large Language Models Text](https://arxiv.org/abs/2505.23276)
Append: [Sentinel: Attention Probing of Proxy Models for LLM Context Compression with an Understanding Perspective](https://arxiv.org/abs/2505.23277)
Append: [ScEdit: Script-based Assessment of Knowledge Editing](https://arxiv.org/abs/2505.23291)
Append: [How Does Response Length Affect Long-Form Factuality](https://arxiv.org/abs/2505.23295)
Append: [EmoBench-UA: A Benchmark Dataset for Emotion Detection in Ukrainian](https://arxiv.org/abs/2505.23297)
Append: [Data-efficient Meta-models for Evaluation of Context-based Questions and Answers in LLMs](https://arxiv.org/abs/2505.23299)
Append: [Generalized Category Discovery in Event-Centric Contexts: Latent Pattern Mining with LLMs](https://arxiv.org/abs/2505.23304)
Append: [Enhancing Marker Scoring Accuracy through Ordinal Confidence Modelling in Educational Assessments](https://arxiv.org/abs/2505.23315)
Append: [Proximalized Preference Optimization for Diverse Feedback Types: A Decomposed Perspective on DPO](https://arxiv.org/abs/2505.23316)
Append: [Neither Stochastic Parroting nor AGI: LLMs Solve Tasks through Context-Directed Extrapolation from Training Data Priors](https://arxiv.org/abs/2505.23323)
Append: [Discriminative Policy Optimization for Token-Level Reward Models](https://arxiv.org/abs/2505.23363)
Append: [Threading the Needle: Reweaving Chain-of-Thought Reasoning to Explain Human Label Variation](https://arxiv.org/abs/2505.23368)
Append: [Adaptive Jailbreaking Strategies Based on the Semantic Understanding Capabilities of Large Language Models](https://arxiv.org/abs/2505.23404)
Append: [From Parameters to Prompts: Understanding and Mitigating the Factuality Gap between Fine-Tuned LLMs](https://arxiv.org/abs/2505.23410)
Append: [The Warmup Dilemma: How Learning Rate Strategies Impact Speech-to-Text Model Convergence](https://arxiv.org/abs/2505.23420)
Append: [UAQFact: Evaluating Factual Knowledge Utilization of LLMs on Unanswerable Questions](https://arxiv.org/abs/2505.23461)
Append: [Evaluating the performance and fragility of large language models on the self-assessment for neurological surgeons](https://arxiv.org/abs/2505.23477)
Append: [Revisiting Overthinking in Long Chain-of-Thought from the Perspective of Self-Doubt](https://arxiv.org/abs/2505.23480)
Append: [Spoken Language Modeling with Duration-Penalized Self-Supervised Units](https://arxiv.org/abs/2505.23494)
Append: [Diagnosing and Addressing Pitfalls in KG-RAG Datasets: Toward More Reliable Benchmarking](https://arxiv.org/abs/2505.23495)
Append: [CLaC at SemEval-2025 Task 6: A Multi-Architecture Approach for Corporate Environmental Promise Verification](https://arxiv.org/abs/2505.23538)
Append: [Probability-Consistent Preference Optimization for Enhanced LLM Reasoning](https://arxiv.org/abs/2505.23540)
Append: [Translation in the Wild](https://arxiv.org/abs/2505.23548)
Append: [Understanding Refusal in Language Models with Sparse Autoencoders](https://arxiv.org/abs/2505.23556)
Append: [Evaluating AI capabilities in detecting conspiracy theories on YouTube](https://arxiv.org/abs/2505.23570)
Append: [Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software Engineering](https://arxiv.org/abs/2505.23604)
Append: [Table-R1: Inference-Time Scaling for Table Reasoning](https://arxiv.org/abs/2505.23621)
Append: [Characterizing the Expressivity of Transformer Language Models](https://arxiv.org/abs/2505.23623)
Append: [AutoSchemaKG: Autonomous Knowledge Graph Construction through Dynamic Schema Induction from Web-Scale Corpora](https://arxiv.org/abs/2505.23628)
Append: [GeNRe: A French Gender-Neutral Rewriting System Using Collective Nouns](https://arxiv.org/abs/2505.23630)
Append: [Are Reasoning Models More Prone to Hallucination?](https://arxiv.org/abs/2505.23646)
Append: [ARC: Argument Representation and Coverage Analysis for Zero-Shot Long Document Summarization with Instruction Following LLMs](https://arxiv.org/abs/2505.23654)
Append: [Active Layer-Contrastive Decoding Reduces Hallucination in Large Language Model Generation](https://arxiv.org/abs/2505.23657)
Append: [ToolHaystack: Stress-Testing Tool-Augmented Language Models in Realistic Long-Term Interactions](https://arxiv.org/abs/2505.23662)
Append: [LoLA: Low-Rank Linear Attention With Sparse Caching](https://arxiv.org/abs/2505.23666)
Append: [Automatic classification of stop realisation with wav2vec2.0](https://arxiv.org/abs/2505.23688)
Append: [Child-Directed Language Does Not Consistently Boost Syntax Learning in Language Models](https://arxiv.org/abs/2505.23689)
Append: [Can LLMs Reason Abstractly Over Math Word Problems Without CoT? Disentangling Abstract Formulation From Arithmetic Computation](https://arxiv.org/abs/2505.23701)
Append: [SocialMaze: A Benchmark for Evaluating Social Reasoning in Large Language Models](https://arxiv.org/abs/2505.23713)
Append: [SenWiCh: Sense-Annotation of Low-Resource Languages for WiC using Hybrid Methods](https://arxiv.org/abs/2505.23714)
Append: [Don't Take the Premise for Granted: Evaluating the Premise Critique Ability of Large Language Models](https://arxiv.org/abs/2505.23715)
Append: [Label-Guided In-Context Learning for Named Entity Recognition](https://arxiv.org/abs/2505.23722)
Append: [ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning Engineering](https://arxiv.org/abs/2505.23723)
Append: [Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time](https://arxiv.org/abs/2505.23729)
Append: [ATLAS: Learning to Optimally Memorize the Context at Test Time](https://arxiv.org/abs/2505.23735)
Append: [DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural Language and Reinforcement Learning](https://arxiv.org/abs/2505.23754)
Append: [Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint](https://arxiv.org/abs/2505.23759)
Append: [From Chat Logs to Collective Insights: Aggregative Question Answering](https://arxiv.org/abs/2505.23765)
Append: [VScan: Rethinking Visual Token Reduction for Efficient Large Vision-Language Models](https://arxiv.org/abs/2505.22654)
Append: [Decomposing Elements of Problem Solving: What "Math" Does RL Teach?](https://arxiv.org/abs/2505.22756)
Append: [FlashFormer: Whole-Model Kernels for Efficient Low-Batch Inference](https://arxiv.org/abs/2505.22758)
Append: [Cultural Evaluations of Vision-Language Models Have a Lot to Learn from Cultural Theory](https://arxiv.org/abs/2505.22793)
Append: [NGPU-LM: GPU-Accelerated N-Gram Language Model for Context-Biasing in Greedy ASR Decoding](https://arxiv.org/abs/2505.22857)
Append: [Large Language Models for Depression Recognition in Spoken Language Integrating Psychological Knowledge](https://arxiv.org/abs/2505.22863)
Append: [Conversational Alignment with Artificial Intelligence in Context](https://arxiv.org/abs/2505.22907)
Append: [Enhancing Study-Level Inference from Clinical Trial Papers via RL-based Numeric Reasoning](https://arxiv.org/abs/2505.22928)
Append: [Synthetic Document Question Answering in Hungarian](https://arxiv.org/abs/2505.23008)
Append: [AgentAlign: Navigating Safety Alignment in the Shift from Informative to Agentic Large Language Models](https://arxiv.org/abs/2505.23020)
Append: [TailorSQL: An NL2SQL System Tailored to Your Query Workload](https://arxiv.org/abs/2505.23039)
Append: [DenoiseRotator: Enhance Pruning Robustness for LLMs via Importance Concentration](https://arxiv.org/abs/2505.23049)
Append: [Be.FM: Open Foundation Models for Human Behavior](https://arxiv.org/abs/2505.23058)
Append: [Infi-MMR: Curriculum-based Unlocking Multimodal Reasoning via Phased Reinforcement Learning in Multimodal Small Language Models](https://arxiv.org/abs/2505.23091)
Append: [MAP: Revisiting Weight Decomposition for Low-Rank Adaptation](https://arxiv.org/abs/2505.23094)
Append: [Does Machine Unlearning Truly Remove Model Knowledge? A Framework for Auditing Unlearning in LLMs](https://arxiv.org/abs/2505.23270)
Append: [Nosey: Open-source hardware for acoustic nasalance](https://arxiv.org/abs/2505.23339)
Append: [SWE-bench Goes Live!](https://arxiv.org/abs/2505.23419)
Append: [Rethinking Regularization Methods for Knowledge Graph Completion](https://arxiv.org/abs/2505.23442)
Append: [Socratic-PRMBench: Benchmarking Process Reward Models with Systematic Reasoning Patterns](https://arxiv.org/abs/2505.23474)
Append: [R2I-Bench: Benchmarking Reasoning-Driven Text-to-Image Generation](https://arxiv.org/abs/2505.23493)
Append: [Identity resolution of software metadata using Large Language Models](https://arxiv.org/abs/2505.23500)
Append: [Domain-Aware Tensor Network Structure Search](https://arxiv.org/abs/2505.23537)
Append: [Segment Policy Optimization: Effective Segment-Level Credit Assignment in RL for Large Language Models](https://arxiv.org/abs/2505.23564)
Append: [On-Policy RL with Optimal Reward Baseline](https://arxiv.org/abs/2505.23585)
Append: [Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with Jigsaw Puzzles](https://arxiv.org/abs/2505.23590)
Append: [Human Empathy as Encoder: AI-Assisted Depression Assessment in Special Education](https://arxiv.org/abs/2505.23631)
Append: [GSO: Challenging Software Optimization Tasks for Evaluating SWE-Agents](https://arxiv.org/abs/2505.23671)
Append: [VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos](https://arxiv.org/abs/2505.23693)
Append: [Differential Information: An Information-Theoretic Perspective on Preference Optimization](https://arxiv.org/abs/2505.23761)
Append: [ZeroGUI: Automating Online GUI Learning at Zero Human Cost](https://arxiv.org/abs/2505.23762)
Append: [MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence](https://arxiv.org/abs/2505.23764)
Append: [ReflectionCoder: Learning from Reflection Sequence for Enhanced One-off Code Generation](https://arxiv.org/abs/2405.17057)
Append: [mOSCAR: A Large-scale Multilingual and Multimodal Document-level Corpus](https://arxiv.org/abs/2406.08707)
Append: [Neuro-symbolic Training for Reasoning over Spatial Language](https://arxiv.org/abs/2406.13828)
Append: [CLEME2.0: Towards Interpretable Evaluation by Disentangling Edits for Grammatical Error Correction](https://arxiv.org/abs/2407.00934)
Append: [ASTPrompter: Preference-Aligned Automated Language Model Red-Teaming to Generate Low-Perplexity Unsafe Prompts](https://arxiv.org/abs/2407.09447)
Append: [$T^5Score$: A Methodology for Automatically Assessing the Quality of LLM Generated Multi-Document Topic Sets](https://arxiv.org/abs/2407.17390)
Append: [BA-LoRA: Bias-Alleviating Low-Rank Adaptation to Mitigate Catastrophic Inheritance in Large Language Models](https://arxiv.org/abs/2408.04556)
Append: [X-TURING: Towards an Enhanced and Efficient Turing Test for Long-Term Dialogue Agents](https://arxiv.org/abs/2408.09853)
Append: [Resolving Lexical Bias in Model Editing](https://arxiv.org/abs/2408.10411)
Append: [Structure-Enhanced Protein Instruction Tuning: Towards General-Purpose Protein Understanding with LLMs](https://arxiv.org/abs/2410.03553)
Append: [Graph of Records: Boosting Retrieval Augmented Generation for Long-context Summarization with Graphs](https://arxiv.org/abs/2410.11001)
Append: [On the Risk of Evidence Pollution for Malicious Social Text Detection in the Era of LLMs](https://arxiv.org/abs/2410.12600)
Append: [BenchmarkCards: Large Language Model and Risk Reporting](https://arxiv.org/abs/2410.12974)
Append: [RULEBREAKERS: Challenging LLMs at the Crossroads between Formal Logic and Human-like Reasoning](https://arxiv.org/abs/2410.16502)
Append: [Reducing Tool Hallucination via Reliability Alignment](https://arxiv.org/abs/2412.04141)
Append: [C$^2$LEVA: Toward Comprehensive and Contamination-Free Language Model Evaluation](https://arxiv.org/abs/2412.04947)
Append: [EXIT: Context-Aware Extractive Compression for Enhancing Retrieval-Augmented Generation](https://arxiv.org/abs/2412.12559)
Append: [FCMR: Robust Evaluation of Financial Cross-Modal Multi-Hop Reasoning](https://arxiv.org/abs/2412.12567)
Append: [AntiLeakBench: Preventing Data Contamination by Automatically Constructing Benchmarks with Updated Real-World Knowledge](https://arxiv.org/abs/2412.13670)
Append: [SimGRAG: Leveraging Similar Subgraphs for Knowledge Graphs Driven Retrieval-Augmented Generation](https://arxiv.org/abs/2412.15272)
Append: [Human-Readable Adversarial Prompts: An Investigation into LLM Vulnerabilities Using Situational Context](https://arxiv.org/abs/2412.16359)
Append: [Divide and Conquer: A Hybrid Strategy Defeats Multimodal Large Language Models](https://arxiv.org/abs/2412.16555)
Append: [A Reality Check on Context Utilisation for Retrieval-Augmented Generation](https://arxiv.org/abs/2412.17031)
Append: [Tensor Product Attention Is All You Need](https://arxiv.org/abs/2501.06425)
Append: [Enhancing Automated Interpretability with Output-Centric Feature Descriptions](https://arxiv.org/abs/2501.08319)
Append: [Re-ranking Using Large Language Models for Mitigating Exposure to Harmful Content on Social Media Platforms](https://arxiv.org/abs/2501.13977)
Append: [Chain of Grounded Objectives: Bridging Process and Goal-oriented Prompting for Code Generation](https://arxiv.org/abs/2501.13978)
Append: [Decomposed Opinion Summarization with Verified Aspect-Aware Modules](https://arxiv.org/abs/2501.17191)
Append: [Joint Localization and Activation Editing for Low-Resource Fine-Tuning](https://arxiv.org/abs/2502.01179)
Append: [Fast Large Language Model Collaborative Decoding via Speculation](https://arxiv.org/abs/2502.01662)
Append: [SPRI: Aligning Large Language Models with Context-Situated Principles](https://arxiv.org/abs/2502.03397)
Append: [Toward universal steering and monitoring of AI models](https://arxiv.org/abs/2502.03708)
Append: [CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance](https://arxiv.org/abs/2502.04350)
Append: [Position: Scaling LLM Agents Requires Asymptotic Analysis with LLM Primitives](https://arxiv.org/abs/2502.04358)
Append: [Uncertainty Quantification for LLMs through Minimum Bayes Risk: Bridging Confidence and Consistency](https://arxiv.org/abs/2502.04964)
Append: [Jailbreaking to Jailbreak](https://arxiv.org/abs/2502.09638)
Append: [Multilingual Encoder Knows more than You Realize: Shared Weights Pretraining for Extremely Low-Resource Languages](https://arxiv.org/abs/2502.10852)
Append: [Are Generative Models Underconfident? Better Quality Estimation with Boosted Model Probability](https://arxiv.org/abs/2502.11115)
Append: [Token Pruning in Multimodal Large Language Models: Are We Solving the Right Problem?](https://arxiv.org/abs/2502.11501)
Append: [Understanding In-Context Machine Translation for Low-Resource Languages: A Case Study on Manchu](https://arxiv.org/abs/2502.11862)
Append: [LongFaith: Enhancing Long-Context Reasoning in LLMs with Faithful Synthetic Data](https://arxiv.org/abs/2502.12583)
Append: [Reasoning-to-Defend: Safety-Aware Reasoning Can Defend Large Language Models from Jailbreaking](https://arxiv.org/abs/2502.12970)
Append: [Do we still need Human Annotators? Prompting Large Language Models for Aspect Sentiment Quad Prediction](https://arxiv.org/abs/2502.13044)
Append: [FlexDuo: A Pluggable System for Enabling Full-Duplex Capabilities in Speech Dialogue Systems](https://arxiv.org/abs/2502.13472)
Append: [LoRA-MGPO: Mitigating Double Descent in Low-Rank Adaptation via Momentum-Guided Perturbation Optimization](https://arxiv.org/abs/2502.14538)
Append: [Length-Controlled Margin-Based Preference Optimization without Reference Model](https://arxiv.org/abs/2502.14643)
Append: [SOTOPIA-$\Omega$: Dynamic Strategy Injection Learning and Social Instruction Following Evaluation for Social Agents](https://arxiv.org/abs/2502.15538)
Append: [ParamMute: Suppressing Knowledge-Critical FFNs for Faithful Retrieval-Augmented Generation](https://arxiv.org/abs/2502.15543)
Append: [DReSD: Dense Retrieval for Speculative Decoding](https://arxiv.org/abs/2502.15572)
Append: [Instruction-Tuning LLMs for Event Extraction with Annotation Guidelines](https://arxiv.org/abs/2502.16377)
Append: [A Survey of Uncertainty Estimation Methods on Large Language Models](https://arxiv.org/abs/2503.00172)
Append: [What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models](https://arxiv.org/abs/2503.09894)
Append: [DynaCode: A Dynamic Complexity-Aware Code Benchmark for Evaluating Large Language Models in Code Generation](https://arxiv.org/abs/2503.10452)
Append: [Enhancing Retrieval for ESGLLM via ESG-CID -- A Disclosure Content Index Finetuning Dataset for Mapping GRI and ESRS](https://arxiv.org/abs/2503.10674)
Append: [HiDe-LLaVA: Hierarchical Decoupling for Continual Instruction Tuning of Multimodal Large Language Model](https://arxiv.org/abs/2503.12941)
Append: [FutureGen: LLM-RAG Approach to Generate the Future Work of Scientific Article](https://arxiv.org/abs/2503.16561)
Append: [Temporal Relation Extraction in Clinical Texts: A Span-based Graph Transformer Approach](https://arxiv.org/abs/2503.18085)
Append: [Multi-Modal Framing Analysis of News](https://arxiv.org/abs/2503.20960)
Append: [Firm or Fickle? Evaluating Large Language Models Consistency in Sequential Interactions](https://arxiv.org/abs/2503.22353)
Append: [Agentic Knowledgeable Self-awareness](https://arxiv.org/abs/2504.03553)
Append: [NeedleInATable: Exploring Long-Context Capability of Large Language Models towards Long-Structured Tables](https://arxiv.org/abs/2504.06560)
Append: [DeepSeek vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and Summarization?](https://arxiv.org/abs/2504.08120)
Append: [LLMs Can Achieve High-quality Simultaneous Machine Translation as Efficiently as Offline](https://arxiv.org/abs/2504.09570)
Append: [PolyMath: Evaluating Mathematical Reasoning in Multilingual Contexts](https://arxiv.org/abs/2504.18428)
Append: [The Aloe Family Recipe for Open and Specialized Healthcare LLMs](https://arxiv.org/abs/2505.04388)
Append: [BioProBench: Comprehensive Dataset and Benchmark in Biological Protocol Understanding and Reasoning](https://arxiv.org/abs/2505.07889)
Append: [Fusing Bidirectional Chains of Thought and Reward Mechanisms A Method for Enhancing Question-Answering Capabilities of Large Language Models for Chinese Intangible Cultural Heritage](https://arxiv.org/abs/2505.08167)
Append: [RepCali: High Efficient Fine-tuning Via Representation Calibration in Latent Space for Pre-trained Language Models](https://arxiv.org/abs/2505.08463)
Append: [LEXam: Benchmarking Legal Reasoning on 340 Law Exams](https://arxiv.org/abs/2505.12864)
Append: [YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering](https://arxiv.org/abs/2505.14279)
Append: [LLM as Effective Streaming Processor: Bridging Streaming-Batch Mismatches with Group Position Encoding](https://arxiv.org/abs/2505.16983)
Append: [EarthSE: A Benchmark Evaluating Earth Scientific Exploration Capability for Large Language Models](https://arxiv.org/abs/2505.17139)
Append: [Too Consistent to Detect: A Study of Self-Consistent Errors in LLMs](https://arxiv.org/abs/2505.17656)
Append: [Frankentext: Stitching random text fragments into long-form narratives](https://arxiv.org/abs/2505.18128)
Append: [Hijacking Large Language Models via Adversarial In-Context Learning](https://arxiv.org/abs/2311.09948)
Append: [Theoretical guarantees on the best-of-n alignment policy](https://arxiv.org/abs/2401.01879)
Append: [Learning to Poison Large Language Models for Downstream Manipulation](https://arxiv.org/abs/2402.13459)
Append: [BioVL-QR: Egocentric Biochemical Vision-and-Language Dataset Using Micro QR Codes](https://arxiv.org/abs/2404.03161)
Append: [Shortcut-connected Expert Parallelism for Accelerating Mixture-of-Experts](https://arxiv.org/abs/2404.05019)
Append: [Sample-Efficient Human Evaluation of Large Language Models via Maximum Discrepancy Competition](https://arxiv.org/abs/2404.08008)
Append: [Instruct-SkillMix: A Powerful Pipeline for LLM Instruction Tuning](https://arxiv.org/abs/2408.14774)
Append: [Towards Logically Sound Natural Language Reasoning with Logic-Enhanced Language Model Agents](https://arxiv.org/abs/2408.16081)
Append: [On-Device Collaborative Language Modeling via a Mixture of Generalists and Specialists](https://arxiv.org/abs/2409.13931)
Append: [CodePMP: Scalable Preference Model Pretraining for Large Language Model Reasoning](https://arxiv.org/abs/2410.02229)
Append: [GIVE: Structured Reasoning of Large Language Models with Knowledge Graph Inspired Veracity Extrapolation](https://arxiv.org/abs/2410.08475)
Append: [Can We Predict Performance of Large Models across Vision-Language Tasks?](https://arxiv.org/abs/2410.10112)
Append: [GraphNarrator: Generating Textual Explanations for Graph Neural Networks](https://arxiv.org/abs/2410.15268)
Append: [Improving Parallel Program Performance with LLM Optimizers via Agent-System Interfaces](https://arxiv.org/abs/2410.15625)
Append: [GWQ: Gradient-Aware Weight Quantization for Large Language Models](https://arxiv.org/abs/2411.00850)
Append: [SequentialBreak: Large Language Models Can be Fooled by Embedding Jailbreak Prompts into Sequential Prompt Chains](https://arxiv.org/abs/2411.06426)
Append: [VideoRAG: Retrieval-Augmented Generation over Video Corpus](https://arxiv.org/abs/2501.05874)
Append: [Multimodal Inverse Attention Network with Intrinsic Discriminant Feature Exploitation for Fake News Detection](https://arxiv.org/abs/2502.01699)
Append: [Unveiling Environmental Impacts of Large Language Model Serving: A Functional Unit View](https://arxiv.org/abs/2502.11256)
Append: [GSQ-Tuning: Group-Shared Exponents Integer in Fully Quantized Training for LLMs On-Device Fine-tuning](https://arxiv.org/abs/2502.12913)
Append: [K-Paths: Reasoning over Graph Paths for Drug Repurposing and Drug Interaction Prediction](https://arxiv.org/abs/2502.13344)
Append: [STeCa: Step-level Trajectory Calibration for LLM Agent Learning](https://arxiv.org/abs/2502.14276)
Append: [Learning to Reason from Feedback at Test-Time](https://arxiv.org/abs/2502.15771)
Append: [Dataset Featurization: Uncovering Natural Language Features through Unsupervised Data Reconstruction](https://arxiv.org/abs/2502.17541)
Append: [Understanding Bias Reinforcement in LLM Agents Debate](https://arxiv.org/abs/2503.16814)
Append: [Learning to Reason under Off-Policy Guidance](https://arxiv.org/abs/2504.14945)
Append: [How Transformers Learn Regular Language Recognition: A Theoretical Study on Training Dynamics and Implicit Bias](https://arxiv.org/abs/2505.00926)
append_entries: 245
Finish: 2025-05-30 04:26:30.689264
------------------------------------------------------
Started: 2025-05-30 06:24:32.402844
Existing_entries: 1245
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1400
Summarized using GPT-3.5-turbo
Append: [Multi-Domain Explainability of Preferences](https://arxiv.org/abs/2505.20088)
append_entries: 1
Finish: 2025-05-30 06:24:35.163004
------------------------------------------------------
Started: 2025-05-30 08:21:46.605609
Existing_entries: 1001
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-30 08:21:47.187615
------------------------------------------------------
Started: 2025-05-30 10:17:39.524530
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-30 10:17:40.120347
------------------------------------------------------
Started: 2025-05-30 12:33:50.138874
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-30 12:33:50.749088
------------------------------------------------------
Started: 2025-05-30 14:17:03.695096
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-30 14:17:04.344905
------------------------------------------------------
Started: 2025-05-30 16:20:42.470038
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-30 16:20:43.105595
------------------------------------------------------
Started: 2025-05-30 18:22:43.112111
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-30 18:22:43.744087
------------------------------------------------------
Started: 2025-05-30 20:18:13.976261
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-30 20:18:14.510861
------------------------------------------------------
Started: 2025-05-30 22:15:35.697715
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-30 22:15:36.259501
------------------------------------------------------
Started: 2025-05-31 01:17:44.922081
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-31 01:17:45.492159
------------------------------------------------------
Started: 2025-05-31 03:08:38.585071
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-31 03:08:39.127535
------------------------------------------------------
Started: 2025-05-31 04:20:30.513549
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-31 04:20:30.590506
------------------------------------------------------
Started: 2025-05-31 06:22:02.359108
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-31 06:22:02.415079
------------------------------------------------------
Started: 2025-05-31 08:20:00.796072
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-31 08:20:00.854968
------------------------------------------------------
Started: 2025-05-31 10:16:15.967913
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-31 10:16:16.026289
------------------------------------------------------
Started: 2025-05-31 12:30:39.237802
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-31 12:30:39.312090
------------------------------------------------------
Started: 2025-05-31 14:14:14.855388
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-31 14:14:14.962627
------------------------------------------------------
Started: 2025-05-31 16:18:36.607553
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-31 16:18:36.715546
------------------------------------------------------
Started: 2025-05-31 18:21:13.424259
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-31 18:21:13.479007
------------------------------------------------------
Started: 2025-05-31 20:16:59.111552
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-31 20:16:59.175262
------------------------------------------------------
Started: 2025-05-31 22:14:57.376650
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-31 22:14:57.484143
------------------------------------------------------
Started: 2025-06-01 01:42:10.073194
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-01 01:42:10.137267
------------------------------------------------------
Started: 2025-06-01 03:38:35.378486
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-01 03:38:35.438782
------------------------------------------------------
Started: 2025-06-01 04:32:47.254844
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-01 04:32:47.321983
------------------------------------------------------
Started: 2025-06-01 06:23:28.372292
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-01 06:23:28.479077
------------------------------------------------------
Started: 2025-06-01 08:20:29.011882
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-01 08:20:29.071643
------------------------------------------------------
Started: 2025-06-01 10:16:57.112343
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-01 10:16:57.166072
------------------------------------------------------
Started: 2025-06-01 12:31:28.090267
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-01 12:31:28.149102
------------------------------------------------------
Started: 2025-06-01 14:14:01.708030
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-01 14:14:01.764645
------------------------------------------------------
Started: 2025-06-01 16:19:04.204415
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-01 16:19:04.279091
------------------------------------------------------
Started: 2025-06-01 18:20:58.866517
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-01 18:20:58.952196
------------------------------------------------------
Started: 2025-06-01 20:17:33.337730
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-01 20:17:33.410518
------------------------------------------------------
Started: 2025-06-01 22:15:24.282501
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-01 22:15:24.340714
------------------------------------------------------
Started: 2025-06-02 01:22:49.566735
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-02 01:22:49.640402
------------------------------------------------------
Started: 2025-06-02 03:18:35.955103
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-02 03:18:36.016221
------------------------------------------------------
Started: 2025-06-02 04:33:13.543829
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1797
Summarized using GPT-3.5-turbo
Append: [Meaning Is Not A Metric: Using LLMs to make cultural context legible at scale](https://arxiv.org/abs/2505.23785)
Token length: 1457
Summarized using GPT-3.5-turbo
Append: [Nine Ways to Break Copyright Law and Why Our LLM Won't: A Fair Use Aligned Generation Framework](https://arxiv.org/abs/2505.23788)
Token length: 1259
Summarized using GPT-3.5-turbo
Append: [Conversational Exploration of Literature Landscape with LitChat](https://arxiv.org/abs/2505.23789)
Token length: 1634
Summarized using GPT-3.5-turbo
Append: [Rethinking the Understanding Ability across LLMs through Mutual Information](https://arxiv.org/abs/2505.23790)
Token length: 1645
Summarized using GPT-3.5-turbo
Append: [R3-RAG: Learning Step-by-Step Reasoning and Retrieval for LLMs via Reinforcement Learning](https://arxiv.org/abs/2505.23794)
Token length: 850
Summarized using GPT-3.5-turbo
Append: [Emergent LLM behaviors are observationally equivalent to data leakage](https://arxiv.org/abs/2505.23796)
Token length: 1227
Summarized using GPT-3.5-turbo
Append: [Detection of Suicidal Risk on Social Media: A Hybrid Model](https://arxiv.org/abs/2505.23797)
Token length: 1531
Summarized using GPT-3.5-turbo
Append: [My Answer Is NOT 'Fair': Mitigating Social Bias in Vision-Language Models via Fair and Biased Residuals](https://arxiv.org/abs/2505.23798)
Token length: 1290
Summarized using GPT-3.5-turbo
Append: [Estimating LLM Consistency: A User Baseline vs Surrogate Metrics](https://arxiv.org/abs/2505.23799)
Token length: 1433
Summarized using GPT-3.5-turbo
Append: [SEMFED: Semantic-Aware Resource-Efficient Federated Learning for Heterogeneous NLP Tasks](https://arxiv.org/abs/2505.23801)
Token length: 1911
Summarized using GPT-3.5-turbo
Append: [MedHELM: Holistic Evaluation of Large Language Models for Medical Tasks](https://arxiv.org/abs/2505.23802)
Token length: 1362
Summarized using GPT-3.5-turbo
Append: [Calibrating LLMs for Text-to-SQL Parsing by Leveraging Sub-clause Frequencies](https://arxiv.org/abs/2505.23804)
Token length: 1256
Summarized using GPT-3.5-turbo
Append: [MedOrchestra: A Hybrid Cloud-Local LLM Approach for Clinical Data Interpretation](https://arxiv.org/abs/2505.23806)
Token length: 1390
Summarized using GPT-3.5-turbo
Append: [DLP: Dynamic Layerwise Pruning in Large Language Models](https://arxiv.org/abs/2505.23807)
Token length: 1379
Summarized using GPT-3.5-turbo
Append: [DenseLoRA: Dense Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2505.23808)
Token length: 802
Summarized using GPT-3.5-turbo
Append: [LLM-Driven E-Commerce Marketing Content Optimization: Balancing Creativity and Conversion](https://arxiv.org/abs/2505.23809)
Token length: 1423
Summarized using GPT-3.5-turbo
Append: [MARS-Bench: A Multi-turn Athletic Real-world Scenario Benchmark for Dialogue Evaluation](https://arxiv.org/abs/2505.23810)
Token length: 1560
Summarized using GPT-3.5-turbo
Append: [LayerIF: Estimating Layer Quality for Large Language Models using Influence Functions](https://arxiv.org/abs/2505.23811)
Token length: 1870
Summarized using GPT-3.5-turbo
Append: [Emotion-aware Dual Cross-Attentive Neural Network with Label Fusion for Stance Detection in Misinformative Social Media Content](https://arxiv.org/abs/2505.23812)
Token length: 1313
Summarized using GPT-3.5-turbo
Append: [Aligning LLMs by Predicting Preferences from User Writing Samples](https://arxiv.org/abs/2505.23815)
Token length: 1453
Summarized using GPT-3.5-turbo
Append: [A Course Correction in Steerability Evaluation: Revealing Miscalibration and Side Effects in LLMs](https://arxiv.org/abs/2505.23816)
Token length: 1434
Summarized using GPT-3.5-turbo
Append: [Ratas framework: A comprehensive genai-based approach to rubric-based marking of real-world textual exams](https://arxiv.org/abs/2505.23818)
Token length: 1172
Summarized using GPT-3.5-turbo
Append: [Arbiters of Ambivalence: Challenges of Using LLMs in No-Consensus Tasks](https://arxiv.org/abs/2505.23820)
Token length: 1340
Summarized using GPT-3.5-turbo
Append: [Speech as a Multimodal Digital Phenotype for Multi-Task LLM-based Mental Health Prediction](https://arxiv.org/abs/2505.23822)
Token length: 1198
Summarized using GPT-3.5-turbo
Append: [RAGPPI: RAG Benchmark for Protein-Protein Interactions in Drug Discovery](https://arxiv.org/abs/2505.23823)
Token length: 999
Summarized using GPT-3.5-turbo
Append: [Reviewing Scientific Papers for Critical Problems With Reasoning LLMs: Baseline Approaches and Automatic Evaluation](https://arxiv.org/abs/2505.23824)
Token length: 1310
Summarized using GPT-3.5-turbo
Append: [ValueSim: Generating Backstories to Model Individual Value Systems](https://arxiv.org/abs/2505.23827)
Token length: 1238
Summarized using GPT-3.5-turbo
Append: [BiasFilter: An Inference-Time Debiasing Framework for Large Language Models](https://arxiv.org/abs/2505.23829)
Token length: 1932
Summarized using GPT-3.5-turbo
Append: [EvoMoE: Expert Evolution in Mixture of Experts for Multimodal Large Language Models](https://arxiv.org/abs/2505.23830)
Token length: 1667
Summarized using GPT-3.5-turbo
Append: [ICH-Qwen: A Large Language Model Towards Chinese Intangible Cultural Heritage](https://arxiv.org/abs/2505.23831)
Token length: 1264
Summarized using GPT-3.5-turbo
Append: [LegalSearchLM: Rethinking Legal Case Retrieval as Legal Elements Generation](https://arxiv.org/abs/2505.23832)
Token length: 1550
Summarized using GPT-3.5-turbo
Append: [Benchmarking Abstract and Reasoning Abilities Through A Theoretical Perspective](https://arxiv.org/abs/2505.23833)
Token length: 1744
Summarized using GPT-3.5-turbo
Append: [Say What You Mean: Natural Language Access Control with Large Language Models for Internet of Things](https://arxiv.org/abs/2505.23835)
Token length: 1511
Summarized using GPT-3.5-turbo
Append: [Large Language Models Often Know When They Are Being Evaluated](https://arxiv.org/abs/2505.23836)
Token length: 1678
Summarized using GPT-3.5-turbo
Append: [CoMaPOI: A Collaborative Multi-Agent Framework for Next POI Prediction Bridging the Gap Between Trajectory and Language](https://arxiv.org/abs/2505.23837)
Token length: 1018
Summarized using GPT-3.5-turbo
Append: [Exploring the Landscape of Text-to-SQL with Large Language Models: Progresses, Challenges and Opportunities](https://arxiv.org/abs/2505.23838)
Token length: 1439
Summarized using GPT-3.5-turbo
Append: [Measuring Sycophancy of Language Models in Multi-turn Dialogues](https://arxiv.org/abs/2505.23840)
Token length: 1497
Summarized using GPT-3.5-turbo
Append: [Document Valuation in LLM Summaries: A Cluster Shapley Approach](https://arxiv.org/abs/2505.23842)
Token length: 851
Summarized using GPT-3.5-turbo
Append: [Evaluation Hallucination in Multi-Round Incomplete Information Lateral-Driven Reasoning Tasks](https://arxiv.org/abs/2505.23843)
Token length: 1520
Summarized using GPT-3.5-turbo
Append: [Enabling Flexible Multi-LLM Integration for Scalable Knowledge Aggregation](https://arxiv.org/abs/2505.23844)
Token length: 1146
Summarized using GPT-3.5-turbo
Append: [Read Your Own Mind: Reasoning Helps Surface Self-Confidence Signals in LLMs](https://arxiv.org/abs/2505.23845)
Token length: 1963
Summarized using GPT-3.5-turbo
Append: [Scalable, Symbiotic, AI and Non-AI Agent Based Parallel Discrete Event Simulations](https://arxiv.org/abs/2505.23846)
Token length: 1076
Summarized using GPT-3.5-turbo
Append: [Derailing Non-Answers via Logit Suppression at Output Subspace Boundaries in RLHF-Aligned Language Models](https://arxiv.org/abs/2505.23848)
Token length: 1960
Summarized using GPT-3.5-turbo
Append: [ASyMOB: Algebraic Symbolic Mathematical Operations Benchmark](https://arxiv.org/abs/2505.23851)
Token length: 1764
Summarized using GPT-3.5-turbo
Append: [Large Language Model-Based Agents for Automated Research Reproducibility: An Exploratory Study in Alzheimer's Disease](https://arxiv.org/abs/2505.23852)
Token length: 1581
Summarized using GPT-3.5-turbo
Append: [Revisiting Uncertainty Estimation and Calibration of Large Language Models](https://arxiv.org/abs/2505.23854)
Token length: 1311
Summarized using GPT-3.5-turbo
Append: [OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Modalities](https://arxiv.org/abs/2505.23856)
Token length: 1372
Summarized using GPT-3.5-turbo
Append: [Infi-Med: Low-Resource Medical MLLMs with Robust Reasoning Evaluation](https://arxiv.org/abs/2505.23867)
Token length: 803
Summarized using GPT-3.5-turbo
Append: [One Task Vector is not Enough: A Large-Scale Study for In-Context Learning](https://arxiv.org/abs/2505.23911)
Token length: 1476
Summarized using GPT-3.5-turbo
Append: [Reinforcement Learning for Better Verbalized Confidence in Long-Form Generation](https://arxiv.org/abs/2505.23912)
Append: [Probing Association Biases in LLM Moderation Over-Sensitivity](https://arxiv.org/abs/2505.23914)
Append: [ChARM: Character-based Act-adaptive Reward Modeling for Advanced Role-Playing Language Agents](https://arxiv.org/abs/2505.23923)
Append: [Scaling up the think-aloud method](https://arxiv.org/abs/2505.23931)
Append: [SwingArena: Competitive Programming Arena for Long-context GitHub Issue Solving](https://arxiv.org/abs/2505.23932)
Append: [Retrieval Augmented Generation based Large Language Models for Causality Mining](https://arxiv.org/abs/2505.23944)
Append: [A Closer Look at Bias and Chain-of-Thought Faithfulness of Large (Vision) Language Models](https://arxiv.org/abs/2505.23945)
Append: [FLAT-LLM: Fine-grained Low-rank Activation Space Transformation for Large Language Model Compression](https://arxiv.org/abs/2505.23966)
Append: [Is Your Model Fairly Certain? Uncertainty-Aware Fairness Evaluation for LLMs](https://arxiv.org/abs/2505.23996)
Append: [Diversity of Transformer Layers: One Aspect of Parameter Scaling Laws](https://arxiv.org/abs/2505.24009)
Append: [Large Language Model Meets Constraint Propagation](https://arxiv.org/abs/2505.24012)
Append: [BeaverTalk: Oregon State University's IWSLT 2025 Simultaneous Speech Translation System](https://arxiv.org/abs/2505.24016)
Append: [Hidden Persuasion: Detecting Manipulative Narratives on Social Media During the 2022 Russian Invasion of Ukraine](https://arxiv.org/abs/2505.24028)
Append: [The Surprising Soupability of Documents in State Space Models](https://arxiv.org/abs/2505.24033)
Append: [MedPAIR: Measuring Physicians and AI Relevance Alignment in Medical Question Answering](https://arxiv.org/abs/2505.24040)
Append: [TCM-Ladder: A Benchmark for Multimodal Question Answering on Traditional Chinese Medicine](https://arxiv.org/abs/2505.24063)
Append: [HardTests: Synthesizing High-Quality Test Cases for LLM Coding](https://arxiv.org/abs/2505.24098)
Append: [Training LLMs for EHR-Based Reasoning Tasks via Reinforcement Learning](https://arxiv.org/abs/2505.24105)
Append: [The State of Multilingual LLM Safety Research: From Measuring the Language Gap to Mitigating It](https://arxiv.org/abs/2505.24119)
Append: [R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration](https://arxiv.org/abs/2505.24133)
Append: [CrossICL: Cross-Task In-Context Learning via Unsupervised Demonstration Transfer](https://arxiv.org/abs/2505.24143)
Append: [Rationales Are Not Silver Bullets: Measuring the Impact of Rationales on Model Performance and Reliability](https://arxiv.org/abs/2505.24147)
Append: [LKD-KGC: Domain-Specific KG Construction via LLM-driven Knowledge Dependency Parsing](https://arxiv.org/abs/2505.24163)
Append: [Mixed-R1: Unified Reward Perspective For Reasoning Capability in Multimodal Large Language Models](https://arxiv.org/abs/2505.24164)
Append: [Tag-Evol: Achieving Efficient Instruction Evolving via Tag Injection](https://arxiv.org/abs/2505.24165)
Append: [Adaptive LoRA Merge with Parameter Pruning for Low-Resource Generation](https://arxiv.org/abs/2505.24174)
Append: [Beyond Exponential Decay: Rethinking Error Accumulation in Large Language Models](https://arxiv.org/abs/2505.24187)
Append: [CLaSp: In-Context Layer Skip for Self-Speculative Decoding](https://arxiv.org/abs/2505.24196)
Append: [Intuitionistic Fuzzy Sets for Large Language Model Data Annotation: A Novel Approach to Side-by-Side Preference Labeling](https://arxiv.org/abs/2505.24199)
Append: [Are Any-to-Any Models More Consistent Across Modality Transfers Than Specialists?](https://arxiv.org/abs/2505.24211)
Append: [Semi-structured LLM Reasoners Can Be Rigorously Audited](https://arxiv.org/abs/2505.24217)
Append: [ERU-KG: Efficient Reference-aligned Unsupervised Keyphrase Generation](https://arxiv.org/abs/2505.24219)
Append: [Automated Structured Radiology Report Generation](https://arxiv.org/abs/2505.24223)
Append: [Dynamic Context-Aware Streaming Pretrained Language Model For Inverse Text Normalization](https://arxiv.org/abs/2505.24229)
Append: [Advantageous Parameter Expansion Training Makes Better Large Language Models](https://arxiv.org/abs/2505.24241)
Append: [Mamba Knockout for Unraveling Factual Information Flow](https://arxiv.org/abs/2505.24244)
Append: [Proactive Guidance of Multi-Turn Conversation in Industrial Search](https://arxiv.org/abs/2505.24251)
Append: [Effects of Theory of Mind and Prosocial Beliefs on Steering Human-Aligned Behaviors of LLMs in Ultimatum Games](https://arxiv.org/abs/2505.24255)
Append: [Simulating Training Data Leakage in Multiple-Choice Benchmarks for LLM Evaluation](https://arxiv.org/abs/2505.24263)
Append: [Faithful and Robust LLM-Driven Theorem Proving for NLI Explanations](https://arxiv.org/abs/2505.24264)
Append: [ScienceMeter: Tracking Scientific Knowledge Updates in Language Models](https://arxiv.org/abs/2505.24302)
Append: [HiCaM: A Hierarchical-Causal Modification Framework for Long-Form Text Modification](https://arxiv.org/abs/2505.24319)
Append: [Context-Aware Sentiment Forecasting via LLM-based Multi-Perspective Role-Playing Agents](https://arxiv.org/abs/2505.24331)
Append: [Pangu DeepDiver: Adaptive Search Intensity Scaling via Open-Web Reinforcement Learning](https://arxiv.org/abs/2505.24332)
Append: [Exploring Multimodal Challenges in Toxic Chinese Detection: Taxonomy, Benchmark, and Findings](https://arxiv.org/abs/2505.24341)
Append: [Fewer Hallucinations, More Verification: A Three-Stage LLM-Based Framework for ASR Error Correction](https://arxiv.org/abs/2505.24347)
Append: [Unifying Language Agent Algorithms with Graph-based Orchestration Engine for Reproducible Agent Research](https://arxiv.org/abs/2505.24354)
Append: [Multilingual Gloss-free Sign Language Translation: Towards Building a Sign Language Foundation Model](https://arxiv.org/abs/2505.24355)
Append: [Knowing Before Saying: LLM Representations Encode Information About Chain-of-Thought Success Before Completion](https://arxiv.org/abs/2505.24362)
Append: [LLM Inference Enhanced by External Knowledge: A Survey](https://arxiv.org/abs/2505.24377)
Append: [ClueAnchor: Clue-Anchored Knowledge Reasoning Exploration and Optimization for Retrieval-Augmented Generation](https://arxiv.org/abs/2505.24388)
Append: [LLMs Are Globally Multilingual Yet Locally Monolingual: Exploring Knowledge Transfer via Language and Thought Theory](https://arxiv.org/abs/2505.24409)
Append: [MMAFFBen: A Multilingual and Multimodal Affective Analysis Benchmark for Evaluating LLMs and VLMs](https://arxiv.org/abs/2505.24423)
Append: [Donate or Create? Comparing Data Collection Strategies for Emotion-labeled Multimodal Social Media Posts](https://arxiv.org/abs/2505.24427)
Append: [Model Unlearning via Sparse Autoencoder Subspace Guided Projections](https://arxiv.org/abs/2505.24428)
Append: [Exploring the Impact of Occupational Personas on Domain-Specific QA](https://arxiv.org/abs/2505.24448)
Append: [When Large Multimodal Models Confront Evolving Knowledge:Challenges and Pathways](https://arxiv.org/abs/2505.24449)
Append: [Domain Pre-training Impact on Representations](https://arxiv.org/abs/2505.24455)
Append: [CaMMT: Benchmarking Culturally Aware Multimodal Machine Translation](https://arxiv.org/abs/2505.24456)
Append: [VietMix: A Naturally Occurring Vietnamese-English Code-Mixed Corpus with Iterative Augmentation for Machine Translation](https://arxiv.org/abs/2505.24472)
Append: [Towards Effective Code-Integrated Reasoning](https://arxiv.org/abs/2505.24480)
Append: [TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning for Enhancing LLMs' Social Intelligence](https://arxiv.org/abs/2505.24500)
Append: [Stress-testing Machine Generated Text Detection: Shifting Language Models Writing Style to Fool Detectors](https://arxiv.org/abs/2505.24523)
Append: [Limited-Resource Adapters Are Regularizers, Not Linguists](https://arxiv.org/abs/2505.24525)
Append: [DEEPQUESTION: Systematic Generation of Real-World Challenges for Evaluating LLMs Performance](https://arxiv.org/abs/2505.24532)
Append: [Don't Erase, Inform! Detecting and Contextualizing Harmful Language in Cultural Heritage Collections](https://arxiv.org/abs/2505.24538)
Append: [Localizing Persona Representations in LLMs](https://arxiv.org/abs/2505.24539)
Append: [Cross-Attention Speculative Decoding](https://arxiv.org/abs/2505.24544)
Append: [A*-Thought: Efficient Reasoning via Bidirectional Compression for Low-Resource Settings](https://arxiv.org/abs/2505.24550)
Append: [CREFT: Sequential Multi-Agent LLM for Character Relation Extraction](https://arxiv.org/abs/2505.24553)
Append: [Bench4KE: Benchmarking Automated Competency Question Generation](https://arxiv.org/abs/2505.24554)
Append: [Improving Language and Modality Transfer in Translation by Character-level Modeling](https://arxiv.org/abs/2505.24561)
Append: [NexusSum: Hierarchical LLM Agents for Long-Form Narrative Summarization](https://arxiv.org/abs/2505.24575)
Append: [GATE: General Arabic Text Embedding for Enhanced Semantic Textual Similarity with Matryoshka Representation Learning and Hybrid Loss Training](https://arxiv.org/abs/2505.24581)
Append: [Decoding Knowledge Attribution in Mixture-of-Experts: A Framework of Basic-Refinement Collaboration and Efficiency Analysis](https://arxiv.org/abs/2505.24593)
Append: [Explainable Depression Detection using Masked Hard Instance Mining](https://arxiv.org/abs/2505.24609)
Append: [When Harry Meets Superman: The Role of The Interlocutor in Persona-Based Dialogue Generation](https://arxiv.org/abs/2505.24613)
Append: [Harnessing Large Language Models for Scientific Novelty Detection](https://arxiv.org/abs/2505.24615)
Append: [Eye of Judgement: Dissecting the Evaluation of Russian-speaking LLMs with POLLUX](https://arxiv.org/abs/2505.24616)
Append: [Interpretable phenotyping of Heart Failure patients with Dutch discharge letters](https://arxiv.org/abs/2505.24619)
Append: [Benchmarking Large Language Models for Cryptanalysis and Mismatched-Generalization](https://arxiv.org/abs/2505.24621)
Append: [The Hallucination Dilemma: Factuality-Aware Reinforcement Learning for Large Reasoning Models](https://arxiv.org/abs/2505.24630)
Append: [Disentangling Language and Culture for Evaluating Multilingual Large Language Models](https://arxiv.org/abs/2505.24635)
Append: [Efficient Text Encoders for Labor Market Analysis](https://arxiv.org/abs/2505.24640)
Append: [Are Optimal Algorithms Still Optimal? Rethinking Sorting in LLM-Based Pairwise Ranking with Batching and Caching](https://arxiv.org/abs/2505.24643)
Append: [PRISM: A Framework for Producing Interpretable Political Bias Embeddings with Political-Aware Cross-Encoder](https://arxiv.org/abs/2505.24646)
Append: [MSDA: Combining Pseudo-labeling and Self-Supervision for Unsupervised Domain Adaptation in ASR](https://arxiv.org/abs/2505.24656)
Append: [Multiple LLM Agents Debate for Equitable Cultural Alignment](https://arxiv.org/abs/2505.24671)
Append: [TRIDENT: Enhancing Large Language Model Safety with Tri-Dimensional Diversified Red-Teaming Data Synthesis](https://arxiv.org/abs/2505.24672)
Append: [A Simple Linear Patch Revives Layer-Pruned Large Language Models](https://arxiv.org/abs/2505.24680)
Append: [Should I Share this Translation? Evaluating Quality Feedback for User Reliance on Machine Translation](https://arxiv.org/abs/2505.24683)
Append: [Soft Reasoning: Navigating Solution Spaces in Large Language Models through Controlled Embedding Exploration](https://arxiv.org/abs/2505.24688)
Append: [BPE Stays on SCRIPT: Structured Encoding for Robust Multilingual Pretokenization](https://arxiv.org/abs/2505.24689)
Append: [Speech-to-Text Translation with Phoneme-Augmented CoT: Enhancing Cross-Lingual Transfer in Low-Resource Scenarios](https://arxiv.org/abs/2505.24691)
Append: [Multi-Domain ABSA Conversation Dataset Generation via LLMs for Real-World Evaluation and Model Comparison](https://arxiv.org/abs/2505.24701)
Append: [HESEIA: A community-based dataset for evaluating social biases in large language models, co-designed in real school settings in Latin America](https://arxiv.org/abs/2505.24712)
Append: [Voice Conversion Improves Cross-Domain Robustness for Spoken Arabic Dialect Identification](https://arxiv.org/abs/2505.24713)
Append: [FinMME: Benchmark Dataset for Financial Multi-Modal Reasoning Evaluation](https://arxiv.org/abs/2505.24714)
Append: [Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning](https://arxiv.org/abs/2505.24726)
Append: [Circuit Stability Characterizes Language Model Generalization](https://arxiv.org/abs/2505.24731)
Append: [Don't Reinvent the Wheel: Efficient Instruction-Following Text Embedding based on Guided Space Transformation](https://arxiv.org/abs/2505.24754)
Append: [LGAR: Zero-Shot LLM-Guided Neural Ranking for Abstract Screening in Systematic Literature Reviews](https://arxiv.org/abs/2505.24757)
Append: [From Macro to Micro: Probing Dataset Diversity in Language Model Fine-Tuning](https://arxiv.org/abs/2505.24768)
Append: [Revisiting Epistemic Markers in Confidence Estimation: Can Markers Accurately Reflect Large Language Models' Uncertainty?](https://arxiv.org/abs/2505.24778)
Append: [Drop Dropout on Single-Epoch Language Model Pretraining](https://arxiv.org/abs/2505.24788)
Append: [Guiding Generative Storytelling with Knowledge Graphs](https://arxiv.org/abs/2505.24803)
Append: [LegalEval-Q: A New Benchmark for The Quality Evaluation of LLM-Generated Legal Text](https://arxiv.org/abs/2505.24826)
Append: [Improving Reliability and Explainability of Medical Question Answering through Atomic Fact Checking in Retrieval-Augmented LLMs](https://arxiv.org/abs/2505.24830)
Append: [How much do language models memorize?](https://arxiv.org/abs/2505.24832)
Append: [Multilinguality Does not Make Sense: Investigating Factors Behind Zero-Shot Transfer in Sense-Aware Tasks](https://arxiv.org/abs/2505.24834)
Append: [MetaFaith: Faithful Natural Language Uncertainty Expression in LLMs](https://arxiv.org/abs/2505.24858)
Append: [AlphaOne: Reasoning Models Thinking Slow and Fast at Test Time](https://arxiv.org/abs/2505.24863)
Append: [ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models](https://arxiv.org/abs/2505.24864)
Append: [Boosting In-Context Learning in LLMs Through the Lens of Classical Supervised Learning](https://arxiv.org/abs/2505.23783)
Append: [SkewRoute: Training-Free LLM Routing for Knowledge Graph Retrieval-Augmented Generation via Score Skewness of Retrieved Context](https://arxiv.org/abs/2505.23841)
Append: [Using Reasoning Models to Generate Search Heuristics that Solve Open Instances of Combinatorial Design Problems](https://arxiv.org/abs/2505.23881)
Append: [BioCLIP 2: Emergent Properties from Scaling Hierarchical Contrastive Learning](https://arxiv.org/abs/2505.23883)
Append: [Test-Time Training Done Right](https://arxiv.org/abs/2505.23884)
Append: [OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation](https://arxiv.org/abs/2505.23885)
Append: [ScaleLong: A Multi-Timescale Benchmark for Long Video Understanding](https://arxiv.org/abs/2505.23922)
Append: [Information Structure in Mappings: An Approach to Learning, Representation, and Generalisation](https://arxiv.org/abs/2505.23960)
Append: [Large Language Models for Controllable Multi-property Multi-objective Molecule Optimization](https://arxiv.org/abs/2505.23987)
Append: [Redefining Research Crowdsourcing: Incorporating Human Feedback with LLM-Powered Digital Twins](https://arxiv.org/abs/2505.24004)
Append: [Fine-Tune an SLM or Prompt an LLM? The Case of Generating Low-Code Workflows](https://arxiv.org/abs/2505.24189)
Append: [WikiGap: Promoting Epistemic Equity by Surfacing Knowledge Gaps Between English Wikipedia and other Language Editions](https://arxiv.org/abs/2505.24195)
Append: [Improving Multilingual Speech Models on ML-SUPERB 2.0: Fine-tuning with Data Augmentation and LID-Aware CTC](https://arxiv.org/abs/2505.24200)
Append: [Reasoning Can Hurt the Inductive Abilities of Large Language Models](https://arxiv.org/abs/2505.24225)
Append: [From Hallucinations to Jailbreaks: Rethinking the Vulnerability of Large Foundation Models](https://arxiv.org/abs/2505.24232)
Append: [An Adversary-Resistant Multi-Agent LLM System via Credibility Scoring](https://arxiv.org/abs/2505.24239)
Append: [Mind the Quote: Enabling Quotation-Aware Dialogue in LLMs via Plug-and-Play Modules](https://arxiv.org/abs/2505.24292)
Append: [Large Language Models are Locally Linear Mappings](https://arxiv.org/abs/2505.24293)
Append: [SwiftEval: Developing a Language-Specific Benchmark for LLM-generated Code Evaluation](https://arxiv.org/abs/2505.24324)
Append: [GeoVision Labeler: Zero-Shot Geospatial Classification with Vision and Language Models](https://arxiv.org/abs/2505.24340)
Append: [KEVER^2: Knowledge-Enhanced Visual Emotion Reasoning and Retrieval](https://arxiv.org/abs/2505.24342)
Append: [Breaking the Gold Standard: Extracting Forgotten Data under Exact Unlearning in Large Language Models](https://arxiv.org/abs/2505.24379)
Append: [Optimizing the Interface Between Knowledge Graphs and LLMs for Complex Reasoning](https://arxiv.org/abs/2505.24478)
Append: [Leveraging Knowledge Graphs and LLMs for Structured Generation of Misinformation](https://arxiv.org/abs/2505.24479)
Append: [AMIA: Automatic Masking and Joint Intention Analysis Makes LVLMs Robust Jailbreak Defenders](https://arxiv.org/abs/2505.24519)
Append: [Beyond Linear Steering: Unified Multi-Attribute Control for Language Models](https://arxiv.org/abs/2505.24535)
Append: [Identifying Primary Stress Across Related Languages and Dialects with Transformer-based Speech Encoder Models](https://arxiv.org/abs/2505.24571)
Append: [Causal-aware Large Language Models: Enhancing Decision-Making Through Learning, Adapting and Acting](https://arxiv.org/abs/2505.24710)
Append: [CoRet: Improved Retriever for Code Editing](https://arxiv.org/abs/2505.24715)
Append: ["Dyadosyncrasy", Idiosyncrasy and Demographic Factors in Turn-Taking](https://arxiv.org/abs/2505.24736)
Append: [SUMO: Subspace-Aware Moment-Orthogonalization for Accelerating Memory-Efficient LLM Training](https://arxiv.org/abs/2505.24749)
Append: [REASONING GYM: Reasoning Environments for Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2505.24760)
Append: [Draw ALL Your Imagine: A Holistic Benchmark and Agent Framework for Complex Instruction-based Image Generation](https://arxiv.org/abs/2505.24787)
Append: [PhySense: Principle-Based Physics Reasoning Benchmarking for Large Language Models](https://arxiv.org/abs/2505.24823)
Append: [Vision LLMs Are Bad at Hierarchical Visual Understanding, and LLMs Are the Bottleneck](https://arxiv.org/abs/2505.24840)
Append: [Chameleon: A Flexible Data-mixing Framework for Language Model Pretraining and Finetuning](https://arxiv.org/abs/2505.24844)
Append: [MiCRo: Mixture Modeling and Context-aware Routing for Personalized Preference Learning](https://arxiv.org/abs/2505.24846)
Append: [Harnessing Negative Signals: Reinforcement Distillation from Teacher Data for LLM Reasoning](https://arxiv.org/abs/2505.24850)
Append: [Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization](https://arxiv.org/abs/2505.24859)
Append: [MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement Learning](https://arxiv.org/abs/2505.24871)
Append: [ProxyThinker: Test-Time Guidance through Small Visual Reasoners](https://arxiv.org/abs/2505.24872)
Append: [ReasonGen-R1: CoT for Autoregressive Image generation models through SFT and RL](https://arxiv.org/abs/2505.24875)
Append: [Agent-X: Evaluating Deep Multimodal Reasoning in Vision-Centric Agentic Tasks](https://arxiv.org/abs/2505.24876)
Append: [Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents](https://arxiv.org/abs/2505.24878)
Append: [Contrastive Perplexity for Controlled Generation: An Application in Detoxifying Large Language Models](https://arxiv.org/abs/2401.08491)
Append: [An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Model is not a General Substitute for GPT-4](https://arxiv.org/abs/2403.02839)
Append: [Efficient Universal Goal Hijacking with Semantics-guided Prompt Organization](https://arxiv.org/abs/2405.14189)
Append: [Know the Unknown: An Uncertainty-Sensitive Method for LLM Instruction Tuning](https://arxiv.org/abs/2406.10099)
Append: [Evaluating Visual and Cultural Interpretation: The K-Viscuit Benchmark with Human-VLM Collaboration](https://arxiv.org/abs/2406.16469)
Append: [NativQA: Multilingual Culturally-Aligned Natural Query for LLMs](https://arxiv.org/abs/2407.09823)
Append: [Modular Sentence Encoders: Separating Language Specialization from Cross-Lingual Alignment](https://arxiv.org/abs/2407.14878)
Append: [MIDAS: Multi-level Intent, Domain, And Slot Knowledge Distillation for Multi-turn NLU](https://arxiv.org/abs/2408.08144)
Append: [EvalYaks: Instruction Tuning Datasets and LoRA Fine-tuned Models for Automated Scoring of CEFR B2 Speaking Assessment Transcripts](https://arxiv.org/abs/2408.12226)
Append: [BanStereoSet: A Dataset to Measure Stereotypical Social Biases in LLMs for Bangla](https://arxiv.org/abs/2409.11638)
Append: [DemoShapley: Valuation of Demonstrations for In-Context Learning](https://arxiv.org/abs/2410.07523)
Append: [ChuLo: Chunk-Level Key Information Representation for Long Document Processing](https://arxiv.org/abs/2410.11119)
Append: [From Citations to Criticality: Predicting Legal Decision Influence in the Multilingual Swiss Jurisprudence](https://arxiv.org/abs/2410.13460)
Append: [Addressing Blind Guessing: Calibration of Selection Bias in Multiple-Choice Question Answering by Video Language Models](https://arxiv.org/abs/2410.14248)
Append: [Let's Be Self-generated via Step by Step: A Curriculum Learning Approach to Automated Reasoning with Large Language Models](https://arxiv.org/abs/2410.21728)
Append: [Dialectal Coverage And Generalization in Arabic Speech Recognition](https://arxiv.org/abs/2411.05872)
Append: [Expansion Quantization Network: An Efficient Micro-emotion Annotation and Detection Framework](https://arxiv.org/abs/2411.06160)
Append: [Star Attention: Efficient LLM Inference over Long Sequences](https://arxiv.org/abs/2411.17116)
Append: [DEMO: Reframing Dialogue Interaction with Fine-grained Element Modeling](https://arxiv.org/abs/2412.04905)
Append: [Multi-perspective Alignment for Increasing Naturalness in Neural Machine Translation](https://arxiv.org/abs/2412.08473)
Append: [RuleArena: A Benchmark for Rule-Guided Reasoning with LLMs in Real-World Scenarios](https://arxiv.org/abs/2412.08972)
Append: [A Rose by Any Other Name: LLM-Generated Explanations Are Good Proxies for Human Explanations to Collect Label Distributions on NLI](https://arxiv.org/abs/2412.13942)
Append: [Enhancing LLM-based Hatred and Toxicity Detection with Meta-Toxic Knowledge Graph](https://arxiv.org/abs/2412.15268)
Append: [Contrastive Learning for Task-Independent SpeechLLM-Pretraining](https://arxiv.org/abs/2412.15712)
Append: [CLIX: Cross-Lingual Explanations of Idiomatic Expressions](https://arxiv.org/abs/2501.03191)
Append: [AlphaPO: Reward Shape Matters for LLM Alignment](https://arxiv.org/abs/2501.03884)
Append: [Autonomy-of-Experts Models](https://arxiv.org/abs/2501.13074)
Append: [M+: Extending MemoryLLM with Scalable Long-Term Memory](https://arxiv.org/abs/2502.00592)
Append: [Bias Beware: The Impact of Cognitive Biases on LLM-Driven Product Recommendations](https://arxiv.org/abs/2502.01349)
Append: [Boosting Multimodal Reasoning with Automated Structured Thinking](https://arxiv.org/abs/2502.02339)
Append: [A Training-Free Length Extrapolation Approach for LLMs: Greedy Attention Logit Interpolation (GALI)](https://arxiv.org/abs/2502.02659)
Append: [Exploring Imbalanced Annotations for Effective In-Context Learning](https://arxiv.org/abs/2502.04037)
Append: [SparQLe: Speech Queries to Text Translation Through LLMs](https://arxiv.org/abs/2502.09284)
Append: [Rewrite to Jailbreak: Discover Learnable and Transferable Implicit Harmfulness Instruction](https://arxiv.org/abs/2502.11084)
Append: [VLDBench Evaluating Multimodal Disinformation with Regulatory Alignment](https://arxiv.org/abs/2502.11361)
Append: [ToolCoder: A Systematic Code-Empowered Tool Learning Framework for Large Language Models](https://arxiv.org/abs/2502.11404)
Append: [GLTW: Joint Improved Graph Transformer and LLM via Three-Word Language for Knowledge Graph Completion](https://arxiv.org/abs/2502.11471)
Append: [MuSC: Improving Complex Instruction Following with Multi-granularity Self-Contrastive Training](https://arxiv.org/abs/2502.11541)
Append: [LLM Agents Making Agent Tools](https://arxiv.org/abs/2502.11705)
Append: ["See the World, Discover Knowledge": A Chinese Factuality Evaluation for Large Vision Language Models](https://arxiv.org/abs/2502.11718)
Append: [CoCo-CoLa: Evaluating and Improving Language Adherence in Multilingual LLMs](https://arxiv.org/abs/2502.12476)
Append: [StructFlowBench: A Structured Flow Benchmark for Multi-turn Instruction Following](https://arxiv.org/abs/2502.14494)
Append: [Can LLMs Predict Citation Intent? An Experimental Analysis of In-context Learning and Fine-tuning on Open LLMs](https://arxiv.org/abs/2502.14561)
Append: [iAgent: LLM Agent as a Shield between User and Recommender Systems](https://arxiv.org/abs/2502.14662)
Append: [Middle-Layer Representation Alignment for Cross-Lingual Transfer in Fine-Tuned LLMs](https://arxiv.org/abs/2502.14830)
Append: [TETRIS: Optimal Draft Token Selection for Batch Speculative Decoding](https://arxiv.org/abs/2502.15197)
Append: [Problem-Solving Logic Guided Curriculum In-Context Learning for LLMs Complex Reasoning](https://arxiv.org/abs/2502.15401)
Append: [Mixup Model Merge: Enhancing Model Merging Performance through Randomized Linear Interpolation](https://arxiv.org/abs/2502.15434)
Append: [All That Glitters is Not Novel: Plagiarism in AI Generated Research](https://arxiv.org/abs/2502.16487)
Append: [All-in-one: Understanding and Generation in Multimodal Reasoning with the MAIA Benchmark](https://arxiv.org/abs/2502.16989)
Append: [Do Language Models Understand Honorific Systems in Javanese?](https://arxiv.org/abs/2502.20864)
Append: [SwiLTra-Bench: The Swiss Legal Translation Benchmark](https://arxiv.org/abs/2503.01372)
Append: [Beyond Prompting: An Efficient Embedding Framework for Open-Domain Question Answering](https://arxiv.org/abs/2503.01606)
Append: [Vision-Language Models Struggle to Align Entities across Modalities](https://arxiv.org/abs/2503.03854)
Append: [HelpSteer3: Human-Annotated Feedback and Edit Data to Empower Inference-Time Scaling in Open-Ended General-Domain Tasks](https://arxiv.org/abs/2503.04378)
Append: [ZOGRASCOPE: A New Benchmark for Semantic Parsing over Property Graphs](https://arxiv.org/abs/2503.05268)
Append: [DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs](https://arxiv.org/abs/2503.07067)
Append: [TROVE: A Challenge for Fine-Grained Text Provenance via Source Sentence Tracing and Relationship Classification](https://arxiv.org/abs/2503.15289)
Append: [MAGIC-VQA: Multimodal And Grounded Inference with Commonsense Knowledge for Visual Question Answering](https://arxiv.org/abs/2503.18491)
Append: [Inverse Reinforcement Learning with Dynamic Reward Scaling for LLM Alignment](https://arxiv.org/abs/2503.18991)
Append: [An Explicit Syllogistic Legal Reasoning Framework for Large Language Models](https://arxiv.org/abs/2504.04042)
Append: [RAISE: Reinforced Adaptive Instruction Selection For Large Language Models](https://arxiv.org/abs/2504.07282)
Append: [GUM-SAGE: A Novel Dataset and Approach for Graded Entity Salience Prediction](https://arxiv.org/abs/2504.10792)
Append: [From Misleading Queries to Accurate Answers: A Three-Stage Fine-Tuning Method for LLMs](https://arxiv.org/abs/2504.11277)
Append: [What's the Difference? Supporting Users in Identifying the Effects of Prompt and Model Changes Through Token Patterns](https://arxiv.org/abs/2504.15815)
Append: [Query-driven Document-level Scientific Evidence Extraction from Biomedical Studies](https://arxiv.org/abs/2505.06186)
Append: [Krikri: Advancing Open Large Language Models for Greek](https://arxiv.org/abs/2505.13772)
Append: [BAR: A Backward Reasoning based Agent for Complex Minecraft Tasks](https://arxiv.org/abs/2505.14079)
Append: [MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol](https://arxiv.org/abs/2505.14590)
Append: [LLaMAs Have Feelings Too: Unveiling Sentiment and Emotion Representations in LLaMA Models Through Probing](https://arxiv.org/abs/2505.16491)
Append: [Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English](https://arxiv.org/abs/2505.17076)
Append: [MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated Experimental Feedback](https://arxiv.org/abs/2505.17873)
Append: [ResSVD: Residual Compensated SVD for Large Language Model Compression](https://arxiv.org/abs/2505.20112)
Append: [Deep Augmentation: Dropout as Augmentation for Self-Supervised Learning](https://arxiv.org/abs/2303.14537)
Append: [Behind the Magic, MERLIM: Multi-modal Evaluation Benchmark for Large Image-Language Models](https://arxiv.org/abs/2312.02219)
Append: [StyleSinger: Style Transfer for Out-of-Domain Singing Voice Synthesis](https://arxiv.org/abs/2312.10741)
Append: [Model Extrapolation Expedites Alignment](https://arxiv.org/abs/2404.16792)
Append: [Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis](https://arxiv.org/abs/2405.21075)
Append: [GUICourse: From General Vision Language Models to Versatile GUI Agents](https://arxiv.org/abs/2406.11317)
Append: [Compress then Serve: Serving Thousands of LoRA Adapters with Little Overhead](https://arxiv.org/abs/2407.00066)
Append: [VITA: Towards Open-Source Interactive Omni Multimodal LLM](https://arxiv.org/abs/2408.05211)
Append: [Reefknot: A Comprehensive Benchmark for Relation Hallucination Evaluation, Analysis and Mitigation in Multimodal Large Language Models](https://arxiv.org/abs/2408.09429)
Append: [On the Vulnerability of Applying Retrieval-Augmented Generation within Knowledge-Intensive Application Domains](https://arxiv.org/abs/2409.17275)
Append: [SVIP: Towards Verifiable Inference of Open-source Large Language Models](https://arxiv.org/abs/2410.22307)
Append: [ChinaTravel: An Open-Ended Benchmark for Language Agents in Chinese Travel Planning](https://arxiv.org/abs/2412.13682)
Append: [GeAR: Generation Augmented Retrieval](https://arxiv.org/abs/2501.02772)
Append: [Latent-space adversarial training with post-aware calibration for defending large language models against jailbreak attacks](https://arxiv.org/abs/2501.10639)
Append: [Safety Reasoning with Guidelines](https://arxiv.org/abs/2502.04040)
Append: [Scalable Oversight for Superhuman AI via Recursive Self-Critiquing](https://arxiv.org/abs/2502.04675)
Append: [Flow-of-Options: Diversified and Improved LLM Reasoning by Thinking Through Options](https://arxiv.org/abs/2502.12929)
Append: [You need to MIMIC to get FAME: Solving Meeting Transcript Scarcity with a Multi-Agent Conversations](https://arxiv.org/abs/2502.13001)
Append: [Repo2Run: Automated Building Executable Environment for Code Repository at Scale](https://arxiv.org/abs/2502.13681)
Append: [PairBench: Are Vision-Language Models Reliable at Comparing What They See?](https://arxiv.org/abs/2502.15210)
Append: [Recurrent Knowledge Identification and Fusion for Language Model Continual Learning](https://arxiv.org/abs/2502.17510)
Append: [SuPreME: A Supervised Pre-training Framework for Multimodal ECG Representation Learning](https://arxiv.org/abs/2502.19668)
Append: [Image Captioning Evaluation in the Age of Multimodal LLMs: Challenges and Future Perspectives](https://arxiv.org/abs/2503.14604)
Append: [FactSelfCheck: Fact-Level Black-Box Hallucination Detection for LLMs](https://arxiv.org/abs/2503.17229)
Append: [Expanding the Boundaries of Vision Prior Knowledge in Multi-modal Large Language Models](https://arxiv.org/abs/2503.18034)
Append: [Efficient Adaptation For Remote Sensing Visual Grounding](https://arxiv.org/abs/2503.23083)
Append: [AgentNet: Decentralized Evolutionary Coordination for LLM-based Multi-Agent Systems](https://arxiv.org/abs/2504.00587)
Append: [Overcoming Sparsity Artifacts in Crosscoders to Interpret Chat-Tuning](https://arxiv.org/abs/2504.02922)
Append: [Learning to Reason Over Time: Timeline Self-Reflection for Improved Temporal Reasoning in Language Models](https://arxiv.org/abs/2504.05258)
Append: [Versatile Framework for Song Generation with Prompt-based Control](https://arxiv.org/abs/2504.19062)
Append: [Glucagon and insulin production in pancreatic cells modeled using Petri nets and Boolean networks](https://arxiv.org/abs/2504.21578)
Append: [Using Knowledge Graphs to harvest datasets for efficient CLIP model training](https://arxiv.org/abs/2505.02746)
Append: [X-Transfer Attacks: Towards Super Transferable Adversarial Attacks on CLIP](https://arxiv.org/abs/2505.05528)
Append: [NAZM: Network Analysis of Zonal Metrics in Persian Poetic Tradition](https://arxiv.org/abs/2505.08052)
Append: [Creativity or Brute Force? Using Brainteasers as a Window into the Problem-Solving Abilities of Large Language Models](https://arxiv.org/abs/2505.10844)
Append: [Mitigating Subgroup Disparities in Multi-Label Speech Emotion Recognition: A Pseudo-Labeling and Unsupervised Learning Approach](https://arxiv.org/abs/2505.14449)
Append: [MoTime: A Dataset Suite for Multimodal Time Series Forecasting](https://arxiv.org/abs/2505.15072)
Append: [AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning](https://arxiv.org/abs/2505.16400)
Append: [Safety Alignment Can Be Not Superficial With Explicit Safety Signals](https://arxiv.org/abs/2505.17072)
Append: [VideoGameBench: Can Vision-Language Models complete popular video games?](https://arxiv.org/abs/2505.18134)
append_entries: 319
Finish: 2025-06-02 04:35:15.492298
------------------------------------------------------
Started: 2025-06-02 06:26:32.396058
Existing_entries: 1319
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1153
Summarized using GPT-3.5-turbo
Append: [KBQA-o1: Agentic Knowledge Base Question Answering with Monte Carlo Tree Search](https://arxiv.org/abs/2501.18922)
Token length: 1012
Summarized using GPT-3.5-turbo
Append: [Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric Speech Recognition in Low-Resource Languages](https://arxiv.org/abs/2505.14874)
Token length: 1281
Summarized using GPT-3.5-turbo
Append: [Are the Hidden States Hiding Something? Testing the Limits of Factuality-Encoding Capabilities in LLMs](https://arxiv.org/abs/2505.16520)
Token length: 1287
Summarized using GPT-3.5-turbo
Append: [ALPS: Attention Localization and Pruning Strategy for Efficient Alignment of Large Language Models](https://arxiv.org/abs/2505.18799)
Token length: 1754
Summarized using GPT-3.5-turbo
Append: [LLLMs: A Data-Driven Survey of Evolving Research on Limitations of Large Language Models](https://arxiv.org/abs/2505.19240)
Token length: 1705
Summarized using GPT-3.5-turbo
Append: [Surrogate Signals from Format and Length: Reinforcement Learning for Solving Mathematical Problems without Ground Truth Answers](https://arxiv.org/abs/2505.19439)
Token length: 1364
Summarized using GPT-3.5-turbo
Append: [TCSinger 2: Customizable Multilingual Zero-shot Singing Voice Synthesis](https://arxiv.org/abs/2505.14910)
append_entries: 7
Finish: 2025-06-02 06:26:50.136532
------------------------------------------------------
Started: 2025-06-02 08:23:47.813908
Existing_entries: 1007
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-02 08:23:48.462235
------------------------------------------------------
Started: 2025-06-02 10:19:09.196669
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-02 10:19:09.945006
------------------------------------------------------
Started: 2025-06-02 12:35:06.710087
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-02 12:35:07.357906
------------------------------------------------------
Started: 2025-06-02 14:17:38.361956
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-02 14:17:39.110603
------------------------------------------------------
Started: 2025-06-02 16:21:45.299168
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-02 16:21:45.997137
------------------------------------------------------
Started: 2025-06-02 18:23:42.330764
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-02 18:23:43.010052
------------------------------------------------------
Started: 2025-06-02 20:18:53.728029
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-02 20:18:54.384695
------------------------------------------------------
Started: 2025-06-02 22:16:29.051270
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-02 22:16:29.742481
------------------------------------------------------
Started: 2025-06-03 01:20:58.432863
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-03 01:20:59.184316
------------------------------------------------------
Started: 2025-06-03 03:13:55.835708
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-03 03:13:56.539851
------------------------------------------------------
Started: 2025-06-03 04:27:34.349244
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 687
Summarized using GPT-3.5-turbo
Append: [Amadeus-Verbo Technical Report: The powerful Qwen2.5 family models trained in Portuguese](https://arxiv.org/abs/2506.00019)
Token length: 1665
Summarized using GPT-3.5-turbo
Append: [Scaling Physical Reasoning with the PHYSICS Dataset](https://arxiv.org/abs/2506.00022)
Token length: 1527
Summarized using GPT-3.5-turbo
Append: [From Mathematical Reasoning to Code: Generalization of Process Reward Models in Test-Time Scaling](https://arxiv.org/abs/2506.00027)
Token length: 1289
Summarized using GPT-3.5-turbo
Append: [Enhancing Tool Learning in Large Language Models with Hierarchical Error Checklists](https://arxiv.org/abs/2506.00042)
Token length: 1297
Summarized using GPT-3.5-turbo
Append: [Unraveling SITT: Social Influence Technique Taxonomy and Detection with LLMs](https://arxiv.org/abs/2506.00061)
Token length: 973
Summarized using GPT-3.5-turbo
Append: [Mis-prompt: Benchmarking Large Language Models for Proactive Error Handling](https://arxiv.org/abs/2506.00064)
Token length: 1146
Summarized using GPT-3.5-turbo
Append: [You Prefer This One, I Prefer Yours: Using Reference Words is Harder Than Vocabulary Words for Humans and Multimodal Language Models](https://arxiv.org/abs/2506.00065)
Token length: 1347
Summarized using GPT-3.5-turbo
Append: [Probing Politico-Economic Bias in Multilingual Large Language Models: A Cultural Analysis of Low-Resource Pakistani Languages](https://arxiv.org/abs/2506.00068)
Token length: 1331
Summarized using GPT-3.5-turbo
Append: [Evaluating the Sensitivity of LLMs to Prior Context](https://arxiv.org/abs/2506.00069)
Token length: 1087
Summarized using GPT-3.5-turbo
Append: [Gaussian mixture models as a proxy for interacting language models](https://arxiv.org/abs/2506.00077)
Token length: 1022
Summarized using GPT-3.5-turbo
Append: [COSMIC: Generalized Refusal Direction Identification in LLM Activations](https://arxiv.org/abs/2506.00085)
Token length: 1833
Summarized using GPT-3.5-turbo
Append: [SwitchLingua: The First Large-Scale Multilingual and Multi-Ethnic Code-Switching Dataset](https://arxiv.org/abs/2506.00087)
Token length: 1196
Summarized using GPT-3.5-turbo
Append: [HD-NDEs: Neural Differential Equations for Hallucination Detection in LLMs](https://arxiv.org/abs/2506.00088)
Token length: 1877
Summarized using GPT-3.5-turbo
Append: [Writing-Zero: Bridge the Gap Between Non-verifiable Problems and Verifiable Rewards](https://arxiv.org/abs/2506.00103)
Token length: 837
Summarized using GPT-3.5-turbo
Append: [Spurious Correlations and Beyond: Understanding and Mitigating Shortcut Learning in SDOH Extraction with Large Language Models](https://arxiv.org/abs/2506.00134)
Token length: 1305
Summarized using GPT-3.5-turbo
Append: [LaMP-QA: A Benchmark for Personalized Long-form Question Answering](https://arxiv.org/abs/2506.00137)
Token length: 1204
Summarized using GPT-3.5-turbo
Append: [Vedavani: A Benchmark Corpus for ASR on Vedic Sanskrit Poetry](https://arxiv.org/abs/2506.00145)
Token length: 941
Summarized using GPT-3.5-turbo
Append: [Werewolf: A Straightforward Game Framework with TTS for Improved User Engagement](https://arxiv.org/abs/2506.00160)
Token length: 1196
Summarized using GPT-3.5-turbo
Append: [Let Them Down Easy! Contextual Effects of LLM Guardrails on User Perceptions and Preferences](https://arxiv.org/abs/2506.00195)
Token length: 1557
Summarized using GPT-3.5-turbo
Append: [Structuring Radiology Reports: Challenging LLMs with Lightweight Models](https://arxiv.org/abs/2506.00200)
Token length: 1033
Summarized using GPT-3.5-turbo
Append: [Structure-Aware Fill-in-the-Middle Pretraining for Code](https://arxiv.org/abs/2506.00204)
Token length: 1163
Summarized using GPT-3.5-turbo
Append: [REIC: RAG-Enhanced Intent Classification at Scale](https://arxiv.org/abs/2506.00210)
Token length: 1802
Summarized using GPT-3.5-turbo
Append: [ComposeRAG: A Modular and Composable RAG for Corpus-Grounded Multi-Hop Question Answering](https://arxiv.org/abs/2506.00232)
Token length: 1965
Summarized using GPT-3.5-turbo
Append: [MedOrch: Medical Diagnosis with Tool-Augmented Reasoning Agents for Flexible Extensibility](https://arxiv.org/abs/2506.00235)
Token length: 1617
Summarized using GPT-3.5-turbo
Append: [PersianMedQA: Language-Centric Evaluation of LLMs in the Persian Medical Domain](https://arxiv.org/abs/2506.00250)
Token length: 1106
Summarized using GPT-3.5-turbo
Append: [Aligned but Blind: Alignment Increases Implicit Bias by Reducing Awareness of Race](https://arxiv.org/abs/2506.00253)
Token length: 1269
Summarized using GPT-3.5-turbo
Append: [The Impact of Disability Disclosure on Fairness and Bias in LLM-Driven Candidate Selection](https://arxiv.org/abs/2506.00256)
Token length: 1186
Summarized using GPT-3.5-turbo
Append: [MultiHoax: A Dataset of Multi-hop False-Premise Questions](https://arxiv.org/abs/2506.00264)
Token length: 940
Summarized using GPT-3.5-turbo
Append: [CASPER: A Large Scale Spontaneous Speech Dataset](https://arxiv.org/abs/2506.00267)
Token length: 1088
Summarized using GPT-3.5-turbo
Append: [Hierarchical Level-Wise News Article Clustering via Multilingual Matryoshka Embeddings](https://arxiv.org/abs/2506.00277)
Token length: 1265
Summarized using GPT-3.5-turbo
Append: [Emergent Abilities of Large Language Models under Continued Pretraining for Language Adaptation](https://arxiv.org/abs/2506.00288)
Token length: 1151
Summarized using GPT-3.5-turbo
Append: [DLM-One: Diffusion Language Models for One-Step Sequence Generation](https://arxiv.org/abs/2506.00290)
Token length: 1145
Summarized using GPT-3.5-turbo
Append: [Can LLMs Understand Unvoiced Speech? Exploring EMG-to-Text Conversion with LLMs](https://arxiv.org/abs/2506.00304)
Token length: 1211
Summarized using GPT-3.5-turbo
Append: [Lossless Token Sequence Compression via Meta-Tokens](https://arxiv.org/abs/2506.00307)
Token length: 1587
Summarized using GPT-3.5-turbo
Append: [An evaluation of LLMs for generating movie reviews: GPT-4o, Gemini-2.0 and DeepSeek-V3](https://arxiv.org/abs/2506.00312)
Token length: 1021
Summarized using GPT-3.5-turbo
Append: [SkillVerse : Assessing and Enhancing LLMs with Tree Evaluation](https://arxiv.org/abs/2506.00319)
Token length: 1420
Summarized using GPT-3.5-turbo
Append: [TreeRare: Syntax Tree-Guided Retrieval and Reasoning for Knowledge-Intensive Question Answering](https://arxiv.org/abs/2506.00331)
Token length: 1137
Summarized using GPT-3.5-turbo
Append: [Disentangling Codemixing in Chats: The NUS ABC Codemixed Corpus](https://arxiv.org/abs/2506.00332)
Token length: 1117
Summarized using GPT-3.5-turbo
Append: [Beyond Context to Cognitive Appraisal: Emotion Reasoning as a Theory of Mind Benchmark for Large Language Models](https://arxiv.org/abs/2506.00334)
Token length: 1057
Summarized using GPT-3.5-turbo
Append: [OWSM v4: Improving Open Whisper-Style Speech Models via Data Scaling and Cleaning](https://arxiv.org/abs/2506.00338)
Token length: 1191
Summarized using GPT-3.5-turbo
Append: [Efficient Latent Semantic Clustering for Scaling Test-Time Computation of LLMs](https://arxiv.org/abs/2506.00344)
Token length: 972
Summarized using GPT-3.5-turbo
Append: [Neuro2Semantic: A Transfer Learning Framework for Semantic Reconstruction of Continuous Language from Human Intracranial EEG](https://arxiv.org/abs/2506.00381)
Token length: 1437
Summarized using GPT-3.5-turbo
Append: [Adaptive-VP: A Framework for LLM-Based Virtual Patients that Adapts to Trainees' Dialogue to Facilitate Nurse Communication Training](https://arxiv.org/abs/2506.00386)
Token length: 1296
Summarized using GPT-3.5-turbo
Append: [SHARE: An SLM-based Hierarchical Action CorREction Assistant for Text-to-SQL](https://arxiv.org/abs/2506.00391)
Token length: 1237
Summarized using GPT-3.5-turbo
Append: [Speculative Reward Model Boosts Decision Making Ability of LLMs Cost-Effectively](https://arxiv.org/abs/2506.00396)
Token length: 1268
Summarized using GPT-3.5-turbo
Append: [Scaling Textual Gradients via Sampling-Based Momentum](https://arxiv.org/abs/2506.00400)
Token length: 1048
Summarized using GPT-3.5-turbo
Append: [Causal Structure Discovery for Error Diagnostics of Children's ASR](https://arxiv.org/abs/2506.00402)
Token length: 1176
Summarized using GPT-3.5-turbo
Append: [Accelerating Diffusion LLMs via Adaptive Parallel Decoding](https://arxiv.org/abs/2506.00413)
Token length: 1202
Summarized using GPT-3.5-turbo
Append: [Dual Debiasing for Noisy In-Context Learning for Text Generation](https://arxiv.org/abs/2506.00418)
Token length: 1685
Summarized using GPT-3.5-turbo
Append: [Enabling Chatbots with Eyes and Ears: An Immersive Multimodal Conversation System for Dynamic Interactions](https://arxiv.org/abs/2506.00421)
Append: [DYNAC: Dynamic Vocabulary based Non-Autoregressive Contextualization for Speech Recognition](https://arxiv.org/abs/2506.00422)
Append: [Inter-Passage Verification for Multi-evidence Multi-answer QA](https://arxiv.org/abs/2506.00425)
Append: [G2S: A General-to-Specific Learning Framework for Temporal Knowledge Graph Forecasting with Large Language Models](https://arxiv.org/abs/2506.00445)
Append: [Fact-Controlled Diagnosis of Hallucinations in Medical Text Summarization](https://arxiv.org/abs/2506.00448)
Append: [Massively Multilingual Adaptation of Large Language Models Using Bilingual Translation Data](https://arxiv.org/abs/2506.00469)
Append: [EffiVLM-BENCH: A Comprehensive Benchmark for Evaluating Training-Free Acceleration in Large Vision-Language Models](https://arxiv.org/abs/2506.00479)
Append: [PVP: An Image Dataset for Personalized Visual Persuasion with Persuasion Strategies, Viewer Characteristics, and Persuasiveness Ratings](https://arxiv.org/abs/2506.00481)
Append: [Auto-Patching: Enhancing Multi-Hop Reasoning in Language Models](https://arxiv.org/abs/2506.00483)
Append: [Synergizing LLMs with Global Label Propagation for Multimodal Fake News Detection](https://arxiv.org/abs/2506.00488)
Append: [Exploring In-context Example Generation for Machine Translation](https://arxiv.org/abs/2506.00507)
Append: [Goal-Aware Identification and Rectification of Misinformation in Multi-Agent Systems](https://arxiv.org/abs/2506.00509)
Append: [Evaluating the Evaluation of Diversity in Commonsense Generation](https://arxiv.org/abs/2506.00514)
Append: [CausalAbstain: Enhancing Multilingual LLMs with Causal Reasoning for Trustworthy Abstention](https://arxiv.org/abs/2506.00519)
Append: [Retrieval-Augmented Generation Systems for Intellectual Property via Synthetic Multi-Angle Fine-tuning](https://arxiv.org/abs/2506.00527)
Append: [Decoupling Reasoning and Knowledge Injection for In-Context Knowledge Editing](https://arxiv.org/abs/2506.00536)
Append: [ARIA: Training Language Agents with Intention-Driven Reward Aggregation](https://arxiv.org/abs/2506.00539)
Append: [Towards Multi-dimensional Evaluation of LLM Summarization across Domains and Languages](https://arxiv.org/abs/2506.00549)
Append: [AnnaAgent: Dynamic Evolution Agent System with Multi-Session Memory for Realistic Seeker Simulation](https://arxiv.org/abs/2506.00551)
Append: [The Hidden Language of Harm: Examining the Role of Emojis in Harmful Online Communication and Content Moderation](https://arxiv.org/abs/2506.00583)
Append: [Entriever: Energy-based Retriever for Knowledge-Grounded Dialog Systems](https://arxiv.org/abs/2506.00585)
Append: [PAKTON: A Multi-Agent Framework for Question Answering in Long Legal Agreements](https://arxiv.org/abs/2506.00608)
Append: [Enhancing Clinical Multiple-Choice Questions Benchmarks with Knowledge Graph Guided Distractor Generation](https://arxiv.org/abs/2506.00612)
Append: [Improving Dialogue State Tracking through Combinatorial Search for In-Context Examples](https://arxiv.org/abs/2506.00622)
Append: [LID Models are Actually Accent Classifiers: Implications and Solutions for LID on Accented Speech](https://arxiv.org/abs/2506.00628)
Append: [Social Construction of Urban Space: Understanding Neighborhood Boundaries Using Rental Listings](https://arxiv.org/abs/2506.00634)
Append: [ViToSA: Audio-Based Toxic Spans Detection on Vietnamese Speech Utterances](https://arxiv.org/abs/2506.00636)
Append: [Improving the Calibration of Confidence Scores in Text Generation Using the Output Distribution's Characteristics](https://arxiv.org/abs/2506.00637)
Append: [SATA-BENCH: Select All That Apply Benchmark for Multiple Choice Questions](https://arxiv.org/abs/2506.00643)
Append: [Clinical Annotations for Automatic Stuttering Severity Assessment](https://arxiv.org/abs/2506.00644)
Append: [GuideX: Guided Synthetic Data Generation for Zero-Shot Information Extraction](https://arxiv.org/abs/2506.00649)
Append: [Sarc7: Evaluating Sarcasm Detection and Generation with Seven Types and Emotion-Informed Techniques](https://arxiv.org/abs/2506.00658)
Append: [SafeTy Reasoning Elicitation Alignment for Multi-Turn Dialogues](https://arxiv.org/abs/2506.00668)
Append: [DeepRAG: Integrating Hierarchical Reasoning and Process Supervision for Biomedical Multi-Hop QA](https://arxiv.org/abs/2506.00671)
Append: [Measuring Faithfulness and Abstention: An Automated Pipeline for Evaluating LLM-Generated 3-ply Case-Based Legal Arguments](https://arxiv.org/abs/2506.00694)
Append: [From Argumentative Text to Argument Knowledge Graph: A New Framework for Structured Argumentation](https://arxiv.org/abs/2506.00713)
Append: [Chain-of-Thought Training for Open E2E Spoken Dialogue Systems](https://arxiv.org/abs/2506.00722)
Append: [Structured Gradient Guidance for Few-Shot Adaptation in Large Language Models](https://arxiv.org/abs/2506.00726)
Append: [Narrative Media Framing in Political Discourse](https://arxiv.org/abs/2506.00737)
Append: [DefenderBench: A Toolkit for Evaluating Language Agents in Cybersecurity Environments](https://arxiv.org/abs/2506.00739)
Append: [Length Aware Speech Translation for Video Dubbing](https://arxiv.org/abs/2506.00740)
Append: [Data Swarms: Optimizable Generation of Synthetic Evaluation Data](https://arxiv.org/abs/2506.00741)
Append: [Assortment of Attention Heads: Accelerating Federated PEFT with Head Pruning and Strategic Client Selection](https://arxiv.org/abs/2506.00743)
Append: [Translate With Care: Addressing Gender Bias, Neutrality, and Reasoning in Large Language Model Translations](https://arxiv.org/abs/2506.00748)
Append: [Understanding and Mitigating Cross-lingual Privacy Leakage via Language-specific and Universal Privacy Neurons](https://arxiv.org/abs/2506.00759)
Append: [Dynamic Chunking and Selection for Reading Comprehension of Ultra-Long Context in Large Language Models](https://arxiv.org/abs/2506.00773)
Append: [Improving Automatic Evaluation of Large Language Models (LLMs) in Biomedical Relation Extraction via LLMs-as-the-Judge](https://arxiv.org/abs/2506.00777)
Append: [KG-TRACES: Enhancing Large Language Models with Knowledge Graph-constrained Trajectory Reasoning and Attribution Supervision](https://arxiv.org/abs/2506.00783)
Append: [Research Borderlands: Analysing Writing Across Research Cultures](https://arxiv.org/abs/2506.00784)
Append: [RARE: Retrieval-Aware Robustness Evaluation for Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2506.00789)
Append: [Fast or Slow? Integrating Fast Intuition and Deliberate Thinking for Enhancing Visual Question Answering](https://arxiv.org/abs/2506.00806)
Append: [GuessBench: Sensemaking Multimodal Creativity in the Wild](https://arxiv.org/abs/2506.00814)
Append: [From Plain Text to Poetic Form: Generating Metrically-Constrained Sanskrit Verses](https://arxiv.org/abs/2506.00815)
Append: [One for All: Update Parameterized Knowledge Across Multiple Models](https://arxiv.org/abs/2506.00817)
Append: [Probing the Geometry of Truth: Consistency and Generalization of Truth Directions in LLMs Across Logical Transformations and Question Answering Tasks](https://arxiv.org/abs/2506.00823)
Append: [HERGC: Heterogeneous Experts Representation and Generative Completion for Multimodal Knowledge Graphs](https://arxiv.org/abs/2506.00826)
Append: [COMPKE: Complex Question Answering under Knowledge Editing](https://arxiv.org/abs/2506.00829)
Append: [Toward Structured Knowledge Reasoning: Contrastive Retrieval-Augmented Generation on Experience](https://arxiv.org/abs/2506.00842)
Append: [EEG2TEXT-CN: An Exploratory Study of Open-Vocabulary Chinese Text-EEG Alignment via Large Language Model and Contrastive Learning on ChineseEEG](https://arxiv.org/abs/2506.00854)
Append: [How Bidirectionality Helps Language Models Learn Better via Dynamic Bottleneck Estimation](https://arxiv.org/abs/2506.00859)
Append: [L3Cube-MahaEmotions: A Marathi Emotion Recognition Dataset with Synthetic Annotations using CoTR prompting and Large Language Models](https://arxiv.org/abs/2506.00863)
Append: [What's Missing in Vision-Language Models? Probing Their Struggles with Causal Order Reasoning](https://arxiv.org/abs/2506.00869)
Append: [CC-Tuning: A Cross-Lingual Connection Mechanism for Improving Joint Multilingual Supervised Fine-Tuning](https://arxiv.org/abs/2506.00875)
Append: [Not Every Token Needs Forgetting: Selective Unlearning to Limit Change in Utility in Large Language Model Unlearning](https://arxiv.org/abs/2506.00876)
Append: [Improve MLLM Benchmark Efficiency through Interview](https://arxiv.org/abs/2506.00883)
Append: [Affordance Benchmark for MLLMs](https://arxiv.org/abs/2506.00893)
Append: [SocialEval: Evaluating Social Intelligence of Large Language Models](https://arxiv.org/abs/2506.00900)
Append: [Pi-SQL: Enhancing Text-to-SQL with Fine-Grained Guidance from Pivot Programming Languages](https://arxiv.org/abs/2506.00912)
Append: [How do Transformer Embeddings Represent Compositions? A Functional Analysis](https://arxiv.org/abs/2506.00914)
Append: [anyECG-chat: A Generalist ECG-MLLM for Flexible ECG Input and Multi-Task Understanding](https://arxiv.org/abs/2506.00942)
Append: [Leveraging Large Language Models for Sarcastic Speech Annotation in Sarcasm Detection](https://arxiv.org/abs/2506.00955)
Append: [From Objectives to Questions: A Planning-based Framework for Educational Mathematical Question Generation](https://arxiv.org/abs/2506.00963)
Append: [ACCESS DENIED INC: The First Benchmark Environment for Sensitivity Awareness](https://arxiv.org/abs/2506.00964)
Append: [XGUARD: A Graded Benchmark for Evaluating Safety Failures of Large Language Models on Extremist Content](https://arxiv.org/abs/2506.00973)
Append: [NTPP: Generative Speech Language Modeling for Dual-Channel Spoken Dialogue via Next-Token-Pair Prediction](https://arxiv.org/abs/2506.00975)
Append: [LEMONADE: A Large Multilingual Expert-Annotated Abstractive Event Dataset for the Real World](https://arxiv.org/abs/2506.00980)
Append: [What do self-supervised speech models know about Dutch? Analyzing advantages of language-specific pre-training](https://arxiv.org/abs/2506.00981)
Append: [Do LLMs Understand Why We Write Diaries? A Method for Purpose Extraction and Clustering](https://arxiv.org/abs/2506.00985)
Append: [Talking to Data: Designing Smart Assistants for Humanities Databases](https://arxiv.org/abs/2506.00986)
Append: [Less is More: Local Intrinsic Dimensions of Contextual Language Models](https://arxiv.org/abs/2506.01034)
Append: [Probing Neural Topology of Large Language Models](https://arxiv.org/abs/2506.01042)
Append: [CHEER-Ekman: Fine-grained Embodied Emotion Classification](https://arxiv.org/abs/2506.01047)
Append: [SealQA: Raising the Bar for Reasoning in Search-Augmented Language Models](https://arxiv.org/abs/2506.01062)
Append: [How Programming Concepts and Neurons Are Shared in Code Language Models](https://arxiv.org/abs/2506.01074)
Append: [zip2zip: Inference-Time Adaptive Vocabularies for Language Models via Token Compression](https://arxiv.org/abs/2506.01084)
Append: [Un-considering Contextual Information: Assessing LLMs' Understanding of Indexical Elements](https://arxiv.org/abs/2506.01089)
Append: [Contextual Candor: Enhancing LLM Trustworthiness Through Hierarchical Unanswerability Detection](https://arxiv.org/abs/2506.01104)
Append: [From Words to Waves: Analyzing Concept Formation in Speech and Text-Based Foundation Models](https://arxiv.org/abs/2506.01133)
Append: [A Word is Worth 4-bit: Efficient Log Parsing with Binary Coded Decimal Recognition](https://arxiv.org/abs/2506.01147)
Append: [Mispronunciation Detection Without L2 Pronunciation Dataset in Low-Resource Setting: A Case Study in Finland Swedish](https://arxiv.org/abs/2506.01156)
Append: [The Inverse Scaling Effect of Pre-Trained Language Model Surprisal Is Not Due to Data Leakage](https://arxiv.org/abs/2506.01172)
Append: [LAQuer: Localized Attribution Queries in Content-grounded Generation](https://arxiv.org/abs/2506.01187)
Append: [Culturally-Grounded Chain-of-Thought (CG-CoT):Enhancing LLM Performance on Culturally-Specific Tasks in Low-Resource Languages](https://arxiv.org/abs/2506.01190)
Append: [CoBRA: Quantifying Strategic Language Use and LLM Pragmatics](https://arxiv.org/abs/2506.01195)
Append: [Incorporating Hierarchical Semantics in Sparse Autoencoder Architectures](https://arxiv.org/abs/2506.01197)
Append: [Trick or Neat: Adversarial Ambiguity and Language Model Evaluation](https://arxiv.org/abs/2506.01205)
Append: [Mamba Drafters for Speculative Decoding](https://arxiv.org/abs/2506.01206)
Append: [Compress, Gather, and Recompute: REFORMing Long-Context Processing in Transformers](https://arxiv.org/abs/2506.01215)
Append: [Polishing Every Facet of the GEM: Testing Linguistic Competence of LLMs and Humans in Korean](https://arxiv.org/abs/2506.01237)
Append: [ExpertLongBench: Benchmarking Language Models on Expert-Level Long-Form Generation Tasks with Structured Checklists](https://arxiv.org/abs/2506.01241)
Append: [MTCMB: A Multi-Task Benchmark Framework for Evaluating LLMs on Knowledge, Reasoning, and Safety in Traditional Chinese Medicine](https://arxiv.org/abs/2506.01252)
Append: [CoRE: Condition-based Reasoning for Identifying Outcome Variance in Complex Events](https://arxiv.org/abs/2506.01253)
Append: [Memory-Efficient FastText: A Comprehensive Approach Using Double-Array Trie Structures and Mark-Compact Memory Management](https://arxiv.org/abs/2506.01254)
Append: [DeepSeek in Healthcare: A Survey of Capabilities, Risks, and Clinical Applications of Open-Source Large Language Models](https://arxiv.org/abs/2506.01257)
Append: [Exploring the Potential of LLMs as Personalized Assistants: Dataset, Evaluation, and Analysis](https://arxiv.org/abs/2506.01262)
Append: [WCTC-Biasing: Retraining-free Contextual Biasing ASR with Wildcard CTC-based Keyword Spotting and Inter-layer Biasing](https://arxiv.org/abs/2506.01263)
Append: [Beyond In-Context Learning: Aligning Long-form Generation of Large Language Models via Task-Inherent Attribute Guidelines](https://arxiv.org/abs/2506.01265)
Append: [Detoxification of Large Language Models through Output-layer Fusion with a Calibration Model](https://arxiv.org/abs/2506.01266)
Append: [Schema as Parameterized Tools for Universal Information Extraction](https://arxiv.org/abs/2506.01276)
Append: [VM14K: First Vietnamese Medical Benchmark](https://arxiv.org/abs/2506.01305)
Append: [A Platform for Investigating Public Health Content with Efficient Concern Classification](https://arxiv.org/abs/2506.01308)
Append: [Growing Through Experience: Scaling Episodic Grounding in Language Models](https://arxiv.org/abs/2506.01312)
Append: [Zero-Shot Text-to-Speech for Vietnamese](https://arxiv.org/abs/2506.01322)
Append: [Evaluating Large Language Models in Crisis Detection: A Real-World Benchmark from Psychological Support Hotlines](https://arxiv.org/abs/2506.01329)
Append: [Enhancing Interpretable Image Classification Through LLM Agents and Conditional Concept Bottleneck Models](https://arxiv.org/abs/2506.01334)
Append: [The Landscape of Arabic Large Language Models (ALLMs): A New Era for Arabic Language Technology](https://arxiv.org/abs/2506.01340)
Append: [TurnBench-MS: A Benchmark for Evaluating Multi-Turn, Multi-Step Reasoning in Large Language Models](https://arxiv.org/abs/2506.01341)
Append: [Follow the Flow: Fine-grained Flowchart Attribution with Neurosymbolic Agents](https://arxiv.org/abs/2506.01344)
Append: [The Surprising Effectiveness of Negative Reinforcement in LLM Reasoning](https://arxiv.org/abs/2506.01347)
Append: [KokoroChat: A Japanese Psychological Counseling Dialogue Dataset Collected via Role-Playing by Trained Counselors](https://arxiv.org/abs/2506.01357)
Append: [MMD-Flagger: Leveraging Maximum Mean Discrepancy to Detect Hallucinations](https://arxiv.org/abs/2506.01367)
Append: [AdaRewriter: Unleashing the Power of Prompting-based Conversational Query Reformulation via Test-Time Adaptation](https://arxiv.org/abs/2506.01381)
Append: [Speech-to-Speech Translation Pipelines for Conversations in Low-Resource Languages](https://arxiv.org/abs/2506.01406)
Append: [Comparing LLM-generated and human-authored news text using formal syntactic theory](https://arxiv.org/abs/2506.01407)
Append: [UniversalCEFR: Enabling Open Multilingual Research on Language Proficiency Assessment](https://arxiv.org/abs/2506.01419)
Append: [Self-Refining Language Model Anonymizers via Adversarial Distillation](https://arxiv.org/abs/2506.01420)
Append: [Redundancy, Isotropy, and Intrinsic Dimensionality of Prompt-based Text Embeddings](https://arxiv.org/abs/2506.01435)
Append: [Whale: Large-Scale multilingual ASR model with w2v-BERT and E-Branchformer with large speech data](https://arxiv.org/abs/2506.01439)
Append: [Building Entity Association Mining Framework for Knowledge Discovery](https://arxiv.org/abs/2506.01451)
Append: [TalTech Systems for the Interspeech 2025 ML-SUPERB 2.0 Challenge](https://arxiv.org/abs/2506.01458)
Append: [Integrating Neural and Symbolic Components in a Model of Pragmatic Question-Answering](https://arxiv.org/abs/2506.01474)
Append: [LLM in the Loop: Creating the PARADEHATE Dataset for Hate Speech Detoxification](https://arxiv.org/abs/2506.01484)
Append: [Argument-Centric Causal Intervention Method for Mitigating Bias in Cross-Document Event Coreference Resolution](https://arxiv.org/abs/2506.01488)
Append: [Multilingual Definition Modeling](https://arxiv.org/abs/2506.01489)
Append: [CVC: A Large-Scale Chinese Value Rule Corpus for Value Alignment of Large Language Models](https://arxiv.org/abs/2506.01495)
Append: [Continual Speech Learning with Fused Speech Features](https://arxiv.org/abs/2506.01496)
Append: [Representations of Fact, Fiction and Forecast in Large Language Models: Epistemics and Attitudes](https://arxiv.org/abs/2506.01512)
Append: [FormFactory: An Interactive Benchmarking Suite for Multimodal Form-Filling Agents](https://arxiv.org/abs/2506.01520)
Append: [V-VAE: A Variational Auto Encoding Framework Towards Fine-Grained Control over Human-Like Chat](https://arxiv.org/abs/2506.01524)
Append: [STORM-BORN: A Challenging Mathematical Derivations Dataset Curated via a Human-in-the-Loop Multi-Agent Framework](https://arxiv.org/abs/2506.01531)
Append: [Dictionaries to the Rescue: Cross-Lingual Vocabulary Transfer for Low-Resource Languages Using Bilingual Dictionaries](https://arxiv.org/abs/2506.01535)
Append: [Hanfu-Bench: A Multimodal Benchmark on Cross-Temporal Cultural Understanding and Transcreation](https://arxiv.org/abs/2506.01565)
Append: [Prompt Engineering Large Language Models' Forecasting Capabilities](https://arxiv.org/abs/2506.01578)
Append: [Unified Large Language Models for Misinformation Detection in Low-Resource Linguistic Settings](https://arxiv.org/abs/2506.01587)
Append: [Statement-Tuning Enables Efficient Cross-lingual Generalization in Encoder-only Models](https://arxiv.org/abs/2506.01592)
Append: [MMD-Sense-Analysis: Word Sense Detection Leveraging Maximum Mean Discrepancy](https://arxiv.org/abs/2506.01602)
Append: [IndicRAGSuite: Large-Scale Datasets and a Benchmark for Indian Language RAG Systems](https://arxiv.org/abs/2506.01615)
Append: [Domain Lexical Knowledge-based Word Embedding Learning for Text Classification under Small Data](https://arxiv.org/abs/2506.01621)
Append: [MVAN: Multi-View Attention Networks for Fake News Detection on Social Media](https://arxiv.org/abs/2506.01627)
Append: [Cross-Lingual Generalization and Compression: From Language-Specific to Shared Neurons](https://arxiv.org/abs/2506.01629)
Append: [ESGenius: Benchmarking LLMs on Environmental, Social, and Governance (ESG) and Sustainability Knowledge](https://arxiv.org/abs/2506.01646)
Append: [Cross-Lingual Transfer of Cultural Knowledge: An Asymmetric Phenomenon](https://arxiv.org/abs/2506.01675)
Append: [StochasTok: Improving Fine-Grained Subword Understanding in LLMs](https://arxiv.org/abs/2506.01687)
Append: [When LLMs Team Up: The Emergence of Collaborative Affective Computing](https://arxiv.org/abs/2506.01698)
Append: [mdok of KInIT: Robustly Fine-tuned LLM for Binary and Multiclass AI-Generated Text Detection](https://arxiv.org/abs/2506.01702)
Append: [Fairness Dynamics During Training](https://arxiv.org/abs/2506.01709)
Append: [Reasoning-Table: Exploring Reinforcement Learning for Table Reasoning](https://arxiv.org/abs/2506.01710)
Append: [SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware Reinforcement Learning](https://arxiv.org/abs/2506.01713)
Append: [Tug-of-war between idiom's figurative and literal meanings in LLMs](https://arxiv.org/abs/2506.01723)
Append: [Common Corpus: The Largest Collection of Ethical Data for LLM Pre-Training](https://arxiv.org/abs/2506.01732)
Append: [Benford's Curse: Tracing Digit Bias to Numerical Hallucination in LLMs](https://arxiv.org/abs/2506.01734)
Append: [Thinking in Character: Advancing Role-Playing Agents with Role-Aware Reasoning](https://arxiv.org/abs/2506.01748)
Append: [Developing a Mixed-Methods Pipeline for Community-Oriented Digitization of Kwak'wala Legacy Texts](https://arxiv.org/abs/2506.01775)
Append: [MaXIFE: Multilingual and Cross-lingual Instruction Following Evaluation](https://arxiv.org/abs/2506.01776)
Append: [iQUEST: An Iterative Question-Guided Framework for Knowledge Base Question Answering](https://arxiv.org/abs/2506.01784)
Append: [Human-Centric Evaluation for Foundation Models](https://arxiv.org/abs/2506.01793)
Append: [Read it in Two Steps: Translating Extremely Low-Resource Languages with Code-Augmented Grammar Books](https://arxiv.org/abs/2506.01796)
Append: [Propaganda and Information Dissemination in the Russo-Ukrainian War: Natural Language Processing of Russian and Western Twitter Narratives](https://arxiv.org/abs/2506.01807)
Append: [NAVER LABS Europe Submission to the Instruction-following Track](https://arxiv.org/abs/2506.01808)
Append: [Analysis of LLM Bias (Chinese Propaganda & Anti-US Sentiment) in DeepSeek-R1 vs. ChatGPT o3-mini-high](https://arxiv.org/abs/2506.01814)
Append: [BD at BEA 2025 Shared Task: MPNet Ensembles for Pedagogical Mistake Identification and Localization in AI Tutor Responses](https://arxiv.org/abs/2506.01817)
Append: [Not All Jokes Land: Evaluating Large Language Models Understanding of Workplace Humor](https://arxiv.org/abs/2506.01819)
Append: [CiteEval: Principle-Driven Citation Evaluation for Source Attribution](https://arxiv.org/abs/2506.01829)
Append: [Minimal Pair-Based Evaluation of Code-Switching](https://arxiv.org/abs/2506.01840)
Append: [Code-Switching and Syntax: A Large-Scale Experiment](https://arxiv.org/abs/2506.01846)
Append: [CONFETTI: Conversational Function-Calling Evaluation Through Turn-Level Interactions](https://arxiv.org/abs/2506.01859)
Append: [Is Extending Modality The Right Path Towards Omni-Modality?](https://arxiv.org/abs/2506.01872)
Append: [Spatial Coordinates as a Cell Language: A Multi-Sentence Framework for Imaging Mass Cytometry Analysis](https://arxiv.org/abs/2506.01918)
Append: [From Guidelines to Practice: A New Paradigm for Arabic Language Model Evaluation](https://arxiv.org/abs/2506.01920)
Append: [Esoteric Language Models](https://arxiv.org/abs/2506.01928)
Append: [RewardBench 2: Advancing Reward Model Evaluation](https://arxiv.org/abs/2506.01937)
Append: [Novel Benchmark for NER in the Wastewater and Stormwater Domain](https://arxiv.org/abs/2506.01938)
Append: [Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2506.01939)
Append: [Self-ensemble: Mitigating Confidence Distortion for Large Language Models](https://arxiv.org/abs/2506.01951)
Append: [WebChoreArena: Evaluating Web Browsing Agents on Realistic Tedious Web Tasks](https://arxiv.org/abs/2506.01952)
Append: [DRAG: Distilling RAG for SLMs from LLMs to Transfer Knowledge and Mitigate Hallucination via Evidence and Graph-based Distillation](https://arxiv.org/abs/2506.01954)
Append: [GLEN: Generative Retrieval via Lexical Index Learning](https://arxiv.org/abs/2311.03057)
Append: [Enhancing Finite State Machine Design Automation with Large Language Models and Prompt Engineering Techniques](https://arxiv.org/abs/2506.00001)
Append: [Probing Audio-Generation Capabilities of Text-Based Language Models](https://arxiv.org/abs/2506.00003)
Append: [Retrieval-Augmented Generation: A Comprehensive Survey of Architectures, Enhancements, and Robustness Frontiers](https://arxiv.org/abs/2506.00054)
Append: [Comparative analysis of privacy-preserving open-source LLMs regarding extraction of diagnostic information from clinical CMR imaging reports](https://arxiv.org/abs/2506.00060)
Append: [SafeCOMM: What about Safety Alignment in Fine-Tuned Telecom Large Language Models?](https://arxiv.org/abs/2506.00062)
Append: [Evaluating Prompt Engineering Techniques for Accuracy and Confidence Elicitation in Medical LLMs](https://arxiv.org/abs/2506.00072)
Append: [The Automated but Risky Game: Modeling Agent-to-Agent Negotiations and Transactions in Consumer Markets](https://arxiv.org/abs/2506.00073)
Append: [Optimizing Storytelling, Improving Audience Retention, and Reducing Waste in the Entertainment Industry](https://arxiv.org/abs/2506.00076)
Append: [Bottom-Up Perspectives on AI Governance: Insights from User Reviews of AI Products](https://arxiv.org/abs/2506.00080)
Append: [ClinBench-HPB: A Clinical Benchmark for Evaluating LLMs in Hepato-Pancreato-Biliary Diseases](https://arxiv.org/abs/2506.00095)
Append: [Children's Voice Privacy: First Steps And Emerging Challenges](https://arxiv.org/abs/2506.00100)
Append: [Disentangled Safety Adapters Enable Efficient Guardrails and Flexible Inference-Time Alignment](https://arxiv.org/abs/2506.00166)
Append: [Pushing the Limits of Beam Search Decoding for Transducer-based ASR models](https://arxiv.org/abs/2506.00185)
Append: [Control-R: Towards controllable test-time scaling](https://arxiv.org/abs/2506.00189)
Append: [Localized LoRA: A Structured Low-Rank Approximation for Efficient Fine-Tuning](https://arxiv.org/abs/2506.00236)
Append: [ZeShot-VQA: Zero-Shot Visual Question Answering Framework with Answer Mapping for Natural Disaster Damage Assessment](https://arxiv.org/abs/2506.00238)
Append: [Whispers of Many Shores: Cultural Alignment through Collaborative Cultural Expertise](https://arxiv.org/abs/2506.00242)
Append: [Beyond Semantic Entropy: Boosting LLM Uncertainty Quantification with Pairwise Semantic Similarity](https://arxiv.org/abs/2506.00245)
Append: [MIR: Methodology Inspiration Retrieval for Scientific Research Problems](https://arxiv.org/abs/2506.00249)
Append: [GPR: Empowering Generation with Graph-Pretrained Retriever](https://arxiv.org/abs/2506.00261)
Append: [RoboMoRe: LLM-based Robot Co-design via Joint Optimization of Morphology and Reward](https://arxiv.org/abs/2506.00276)
Append: [MythTriage: Scalable Detection of Opioid Use Disorder Myths on a Video-Sharing Platform](https://arxiv.org/abs/2506.00308)
Append: [Dyna-Think: Synergizing Reasoning, Acting, and World Model Simulation in AI Agents](https://arxiv.org/abs/2506.00320)
Append: [Adapting General-Purpose Embedding Models to Private Datasets Using Keyword-based Retrieval](https://arxiv.org/abs/2506.00363)
Append: [Spectral Insights into Data-Oblivious Critical Layers in Large Language Models](https://arxiv.org/abs/2506.00382)
Append: [XMAD-Bench: Cross-Domain Multilingual Audio Deepfake Benchmark](https://arxiv.org/abs/2506.00462)
Append: [BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM Evaluation](https://arxiv.org/abs/2506.00482)
Append: [FLoE: Fisher-Based Layer Selection for Efficient Sparse Adaptation of Low-Rank Experts](https://arxiv.org/abs/2506.00495)
Append: [CityLens: Benchmarking Large Language-Vision Models for Urban Socioeconomic Sensing](https://arxiv.org/abs/2506.00530)
Append: [Con Instruction: Universal Jailbreaking of Multimodal Large Language Models via Non-Textual Modalities](https://arxiv.org/abs/2506.00548)
Append: [MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal Medical Reasoning](https://arxiv.org/abs/2506.00555)
Append: [Reasoning Like an Economist: Post-Training on Economic Problems Induces Strategic Generalization in LLMs](https://arxiv.org/abs/2506.00577)
Append: [Linear Representation Transferability Hypothesis: Leveraging Small Models to Steer Large Models](https://arxiv.org/abs/2506.00653)
Append: [Existing Large Language Model Unlearning Evaluations Are Inconclusive](https://arxiv.org/abs/2506.00688)
Append: [DrKGC: Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph Completion across General and Biomedical Domains](https://arxiv.org/abs/2506.00708)
Append: [Bregman Conditional Random Fields: Sequence Labeling with Parallelizable Inference Algorithms](https://arxiv.org/abs/2506.00732)
Append: [LIFT the Veil for the Truth: Principal Weights Emerge after Rank Reduction for Reasoning-Focused Supervised Fine-Tuning](https://arxiv.org/abs/2506.00772)
Append: [HSCR: Hierarchical Self-Contrastive Rewarding for Aligning Medical Vision Language Models](https://arxiv.org/abs/2506.00805)
Append: [Generalizable LLM Learning of Graph Synthetic Data with Reinforcement Learning](https://arxiv.org/abs/2506.00845)
Append: [Towards Predicting Any Human Trajectory In Context](https://arxiv.org/abs/2506.00871)
Append: [CODEMENV: Benchmarking Large Language Models on Code Migration](https://arxiv.org/abs/2506.00894)
Append: [Position as Probability: Self-Supervised Transformers that Think Past Their Training for Length Extrapolation](https://arxiv.org/abs/2506.00920)
Append: [Deep Temporal Reasoning in Video Language Models: A Cross-Linguistic Evaluation of Action Duration and Completion through Perfect Times](https://arxiv.org/abs/2506.00928)
Append: [Aligning VLM Assistants with Personalized Situated Cognition](https://arxiv.org/abs/2506.00930)
Append: [Speaking Beyond Language: A Large-Scale Multimodal Dataset for Learning Nonverbal Cues from Video-Grounded Dialogues](https://arxiv.org/abs/2506.00958)
Append: [Bridging the Gap: From Ad-hoc to Proactive Search in Conversations](https://arxiv.org/abs/2506.00983)
Append: [Simple Prompt Injection Attacks Can Leak Personal Data Observed by LLM Agents During Task Execution](https://arxiv.org/abs/2506.01055)
Append: [Attention Retrieves, MLP Memorizes: Disentangling Trainable Components in the Transformer](https://arxiv.org/abs/2506.01115)
Append: [Earley-Driven Dynamic Pruning for Efficient Structured Decoding](https://arxiv.org/abs/2506.01151)
Append: [Confidence intervals for forced alignment boundaries using model ensembles](https://arxiv.org/abs/2506.01256)
Append: [Abstractive Visual Understanding of Multi-modal Structured Knowledge: A New Perspective for MLLM Evaluation](https://arxiv.org/abs/2506.01293)
Append: [Overcoming Multi-step Complexity in Multimodal Theory-of-Mind Reasoning: A Scalable Bayesian Planner](https://arxiv.org/abs/2506.01301)
Append: [An Empirical Study of Group Conformity in Multi-Agent Systems](https://arxiv.org/abs/2506.01332)
Append: [Attention Is Not Always the Answer: Optimizing Voice Activity Detection with Simple Feature Fusion](https://arxiv.org/abs/2506.01365)
Append: [AI Scientists Fail Without Strong Implementation Capability](https://arxiv.org/abs/2506.01372)
Append: [AgentCPM-GUI: Building Mobile-Use Agents with Reinforcement Fine-Tuning](https://arxiv.org/abs/2506.01391)
Append: [Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models](https://arxiv.org/abs/2506.01413)
Append: [PGPO: Enhancing Agent Reasoning via Pseudocode-style Planning Guided Preference Optimization](https://arxiv.org/abs/2506.01475)
Append: [MUDI: A Multimodal Biomedical Dataset for Understanding Pharmacodynamic Drug-Drug Interactions](https://arxiv.org/abs/2506.01478)
Append: [LinearVC: Linear transformations of self-supervised features through the lens of voice conversion](https://arxiv.org/abs/2506.01510)
Append: [EvolveNav: Self-Improving Embodied Reasoning for LLM-Based Vision-Language Navigation](https://arxiv.org/abs/2506.01551)
Append: [AIMSCheck: Leveraging LLMs for AI-Assisted Review of Modern Slavery Statements Across Jurisdictions](https://arxiv.org/abs/2506.01671)
Append: [GRAM: Generative Recommendation via Semantic-aware Multi-granular Late Fusion](https://arxiv.org/abs/2506.01673)
Append: [Respond Beyond Language: A Benchmark for Video Generation in Response to Realistic User Intents](https://arxiv.org/abs/2506.01689)
Append: [Self-Challenging Language Model Agents](https://arxiv.org/abs/2506.01716)
Append: [Datasheets Aren't Enough: DataRubrics for Automated Quality Metrics and Accountability](https://arxiv.org/abs/2506.01789)
Append: [Unified Scaling Laws for Compressed Representations](https://arxiv.org/abs/2506.01863)
Append: [When Should Dense Retrievers Be Updated in Evolving Corpora? Detecting Out-of-Distribution Corpora Using GradNormIR](https://arxiv.org/abs/2506.01877)
Append: [WHEN TO ACT, WHEN TO WAIT: Modeling Structural Trajectories for Intent Triggerability in Task-Oriented Dialogue](https://arxiv.org/abs/2506.01881)
Append: [Enhancing Biomedical Multi-modal Representation Learning with Multi-scale Pre-training and Perturbed Report Discrimination](https://arxiv.org/abs/2506.01902)
Append: [Large language models can learn and generalize steganographic chain-of-thought under process supervision](https://arxiv.org/abs/2506.01926)
Append: [Dual-Process Image Generation](https://arxiv.org/abs/2506.01955)
Append: [On Meta-Prompting](https://arxiv.org/abs/2312.06562)
Append: [Beyond Output Matching: Bidirectional Alignment for Enhanced In-Context Learning](https://arxiv.org/abs/2312.17055)
Append: [Multilingual Text-to-Image Generation Magnifies Gender Stereotypes and Prompt Engineering May Not Help You](https://arxiv.org/abs/2401.16092)
Append: [Do Large Language Models Latently Perform Multi-Hop Reasoning?](https://arxiv.org/abs/2402.16837)
Append: [White Men Lead, Black Women Help? Benchmarking and Mitigating Language Agency Social Biases in LLMs](https://arxiv.org/abs/2404.10508)
Append: [Relevant or Random: Can LLMs Truly Perform Analogical Reasoning?](https://arxiv.org/abs/2404.12728)
Append: [Mixture of insighTful Experts (MoTE): The Synergy of Thought Chains and Expert Mixtures in Self-Alignment](https://arxiv.org/abs/2405.00557)
Append: [Anchored Answers: Unravelling Positional Bias in GPT-2's Multiple-Choice Questions](https://arxiv.org/abs/2405.03205)
Append: [LexGen: Domain-aware Multilingual Lexicon Generation](https://arxiv.org/abs/2405.11200)
Append: [Towards Better Chain-of-Thought: A Reflection on Effectiveness and Faithfulness](https://arxiv.org/abs/2405.18915)
Append: [Pattern Recognition or Medical Knowledge? The Problem with Multiple-Choice Questions in Medicine](https://arxiv.org/abs/2406.02394)
Append: [Multi-Prompting Decoder Helps Better Language Understanding](https://arxiv.org/abs/2406.06279)
Append: [RAEmoLLM: Retrieval Augmented LLMs for Cross-Domain Misinformation Detection Using In-Context Learning Based on Emotional Information](https://arxiv.org/abs/2406.11093)
Append: [A Semantic-Aware Layer-Freezing Approach to Computation-Efficient Fine-Tuning of Language Models](https://arxiv.org/abs/2406.11753)
Append: [BMIKE-53: Investigating Cross-Lingual Knowledge Editing with In-Context Learning](https://arxiv.org/abs/2406.17764)
Append: [LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks](https://arxiv.org/abs/2406.18403)
Append: [Optimus: Accelerating Large-Scale Multi-Modal LLM Training by Bubble Exploitation](https://arxiv.org/abs/2408.03505)
Append: [Pandora's Box or Aladdin's Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models](https://arxiv.org/abs/2408.13533)
Append: [Awes, Laws, and Flaws From Today's LLM Research](https://arxiv.org/abs/2408.15409)
Append: [Learning from Negative Samples in Generative Biomedical Entity Linking](https://arxiv.org/abs/2408.16493)
Append: [Wait, that's not an option: LLMs Robustness with Incorrect Multiple-Choice Options](https://arxiv.org/abs/2409.00113)
Append: [STRICTA: Structured Reasoning in Critical Text Assessment for Peer Review and Beyond](https://arxiv.org/abs/2409.05367)
Append: [CKnowEdit: A New Chinese Knowledge Editing Dataset for Linguistics, Facts, and Logic Error Correction in LLMs](https://arxiv.org/abs/2409.05806)
Append: [Towards Diverse and Efficient Audio Captioning via Diffusion Models](https://arxiv.org/abs/2409.09401)
Append: [A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders](https://arxiv.org/abs/2409.14507)
Append: [Scalable Fine-tuning from Multiple Data Sources: A First-Order Approximation Approach](https://arxiv.org/abs/2409.19458)
Append: [The Nature of NLP: Analyzing Contributions in NLP Papers](https://arxiv.org/abs/2409.19505)
Append: [AfriHuBERT: A self-supervised speech representation model for African languages](https://arxiv.org/abs/2409.20201)
Append: [Estimating Privacy Leakage of Augmented Contextual Knowledge in Language Models](https://arxiv.org/abs/2410.03026)
Append: [Can Language Models Reason about Individualistic Human Values and Preferences?](https://arxiv.org/abs/2410.03868)
Append: [Stereotype or Personalization? User Identity Biases Chatbot Recommendations](https://arxiv.org/abs/2410.05613)
Append: [MEXA: Multilingual Evaluation of English-Centric LLMs via Cross-Lingual Alignment](https://arxiv.org/abs/2410.05873)
Append: [Optimizing the Training Schedule of Multilingual NMT using Reinforcement Learning](https://arxiv.org/abs/2410.06118)
Append: [Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models](https://arxiv.org/abs/2410.07176)
Append: [Insight Over Sight: Exploring the Vision-Knowledge Conflicts in Multimodal LLMs](https://arxiv.org/abs/2410.08145)
Append: [RoCoFT: Efficient Finetuning of Large Language Models with Row-Column Updates](https://arxiv.org/abs/2410.10075)
Append: [Model Swarms: Collaborative Search to Adapt LLM Experts via Swarm Intelligence](https://arxiv.org/abs/2410.11163)
Append: [BridG MT: Enhancing LLMs' Machine Translation Capabilities with Sentence Bridging and Gradual MT](https://arxiv.org/abs/2410.11693)
Append: [Exploring Model Kinship for Merging Large Language Models](https://arxiv.org/abs/2410.12613)
Append: [A Little Human Data Goes A Long Way](https://arxiv.org/abs/2410.13098)
Append: [Router-Tuning: A Simple and Effective Approach for Enabling Dynamic-Depth in Transformers](https://arxiv.org/abs/2410.13184)
Append: [BanTH: A Multi-label Hate Speech Detection Dataset for Transliterated Bangla](https://arxiv.org/abs/2410.13281)
Append: [LoGU: Long-form Generation with Uncertainty Expressions](https://arxiv.org/abs/2410.14309)
Append: [Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities Using Only Forward Passes](https://arxiv.org/abs/2410.16930)
Append: [CogSteer: Cognition-Inspired Selective Layer Intervention for Efficiently Steering Large Language Models](https://arxiv.org/abs/2410.17714)
Append: [Scaling Diffusion Language Models via Adaptation from Autoregressive Models](https://arxiv.org/abs/2410.17891)
Append: [Improving Model Factuality with Fine-grained Critique-based Evaluator](https://arxiv.org/abs/2410.18359)
Append: [GrammaMT: Improving Machine Translation with Grammar-Informed In-Context Learning](https://arxiv.org/abs/2410.18702)
Append: [Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback](https://arxiv.org/abs/2410.19133)
Append: [TrajAgent: An LLM-based Agent Framework for Automated Trajectory Modeling via Collaboration of Large and Small Models](https://arxiv.org/abs/2410.20445)
Append: [Anticipating Future with Large Language Model for Simultaneous Machine Translation](https://arxiv.org/abs/2410.22499)
Append: [STEM-POM: Evaluating Language Models Math-Symbol Reasoning in Document Parsing](https://arxiv.org/abs/2411.00387)
Append: [Towards Building Large Scale Datasets and State-of-the-Art Automatic Speech Translation Systems for 14 Indian Languages](https://arxiv.org/abs/2411.04699)
Append: [KnowCoder-X: Boosting Multilingual Information Extraction via Code](https://arxiv.org/abs/2411.04794)
Append: [FactLens: Benchmarking Fine-Grained Fact Verification](https://arxiv.org/abs/2411.05980)
Append: [Neural Topic Modeling with Large Language Models in the Loop](https://arxiv.org/abs/2411.08534)
Append: [HateDay: Insights from a Global Hate Speech Dataset Representative of a Day on Twitter](https://arxiv.org/abs/2411.15462)
Append: [Do Large Language Models Perform Latent Multi-Hop Reasoning without Exploiting Shortcuts?](https://arxiv.org/abs/2411.16679)
Append: [Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS](https://arxiv.org/abs/2411.18478)
Append: [If Eleanor Rigby Had Met ChatGPT: A Study on Loneliness in a Post-LLM World](https://arxiv.org/abs/2412.01617)
Append: [Nemotron-CC: Transforming Common Crawl into a Refined Long-Horizon Pretraining Dataset](https://arxiv.org/abs/2412.02595)
Append: [CNNSum: Exploring Long-Context Summarization with Large Language Models in Chinese Novels](https://arxiv.org/abs/2412.02819)
Append: [RARE: Retrieval-Augmented Reasoning Enhancement for Large Language Models](https://arxiv.org/abs/2412.02830)
Append: [PromptRefine: Enhancing Few-Shot Performance on Low-Resource Indic Languages with Example Selection from Related Example Banks](https://arxiv.org/abs/2412.05710)
Append: [Domain-Specific Translation with Open-Source Large Language Models: Resource-Oriented Analysis](https://arxiv.org/abs/2412.05862)
Append: [KnowShiftQA: How Robust are RAG Systems when Textbook Knowledge Shifts in K-12 Education?](https://arxiv.org/abs/2412.08985)
Append: [On the Limit of Language Models as Planning Formalizers](https://arxiv.org/abs/2412.09879)
Append: [LLM-as-an-Interviewer: Beyond Static Testing Through Dynamic LLM Evaluation](https://arxiv.org/abs/2412.10424)
Append: [INTERACT: Enabling Interactive, Question-Driven Learning in Large Language Models](https://arxiv.org/abs/2412.11388)
Append: [The Impact of Token Granularity on the Predictive Power of Language Model Surprisal](https://arxiv.org/abs/2412.11940)
Append: [Inferring Functionality of Attention Heads from their Parameters](https://arxiv.org/abs/2412.11965)
Append: [Emergence and Effectiveness of Task Vectors in In-Context Learning: An Encoder Decoder Perspective](https://arxiv.org/abs/2412.12276)
Append: [Quantifying Lexical Semantic Shift via Unbalanced Optimal Transport](https://arxiv.org/abs/2412.12569)
Append: [Algorithmic Fidelity of Large Language Models in Generating Synthetic German Public Opinions: A Case Study](https://arxiv.org/abs/2412.13169)
Append: [SummExecEdit: A Factual Consistency Benchmark in Summarization with Executable Edits](https://arxiv.org/abs/2412.13378)
Append: [GAMEBoT: Transparent Assessment of LLM Reasoning in Games](https://arxiv.org/abs/2412.13602)
Append: [SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation](https://arxiv.org/abs/2412.13649)
Append: [Cross-Lingual Transfer of Debiasing and Detoxification in Multilingual LLMs: An Extensive Investigation](https://arxiv.org/abs/2412.14050)
Append: [Neuron Empirical Gradient: Discovering and Quantifying Neurons Global Linear Controllability](https://arxiv.org/abs/2412.18053)
Append: [Improving Factuality with Explicit Working Memory](https://arxiv.org/abs/2412.18069)
Append: [Do Language Models Understand the Cognitive Tasks Given to Them? Investigations with the N-Back Paradigm](https://arxiv.org/abs/2412.18120)
Append: [CoAM: Corpus of All-Type Multiword Expressions](https://arxiv.org/abs/2412.18151)
Append: [Token-Budget-Aware LLM Reasoning](https://arxiv.org/abs/2412.18547)
Append: ["My life is miserable, have to sign 500 autographs everyday": Exposing Humblebragging, the Brags in Disguise](https://arxiv.org/abs/2412.20057)
Append: [Towards Neural No-Resource Language Translation: A Comparative Evaluation of Approaches](https://arxiv.org/abs/2412.20584)
Append: [Enhancing Transformers for Generalizable First-Order Logical Entailment](https://arxiv.org/abs/2501.00759)
Append: [Are LLMs effective psychological assessors? Leveraging adaptive RAG for interpretable mental health screening through psychometric practice](https://arxiv.org/abs/2501.00982)
Append: [Improving Medical Large Vision-Language Models with Abnormal-Aware Feedback](https://arxiv.org/abs/2501.01377)
Append: [Automating Legal Interpretation with LLMs: Retrieval, Generation, and Evaluation](https://arxiv.org/abs/2501.01743)
Append: [Personalized Graph-Based Retrieval for Large Language Models](https://arxiv.org/abs/2501.02157)
Append: [Explicit vs. Implicit: Investigating Social Bias in Large Language Models through Self-Reflection](https://arxiv.org/abs/2501.02295)
Append: [Towards Omni-RAG: Comprehensive Retrieval-Augmented Generation for Large Language Models in Medical Applications](https://arxiv.org/abs/2501.02460)
Append: [Beyond Factual Accuracy: Evaluating Coverage of Diverse Factual Information in Long-form Text Generation](https://arxiv.org/abs/2501.03545)
Append: [TACLR: A Scalable and Efficient Retrieval-based Method for Industrial Product Attribute Value Identification](https://arxiv.org/abs/2501.03835)
Append: [Step-by-Step Mastery: Enhancing Soft Constraint Following Ability of Large Language Models](https://arxiv.org/abs/2501.04945)
Append: [Effective faking of verbal deception detection with target-aligned adversarial attacks](https://arxiv.org/abs/2501.05962)
Append: [Curiosity-Driven Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2501.11463)
Append: [Whose Boat Does it Float? Improving Personalization in Preference Tuning via Inferred User Personas](https://arxiv.org/abs/2501.11549)
Append: [Generating Plausible Distractors for Multiple-Choice Questions via Student Choice Prediction](https://arxiv.org/abs/2501.13125)
Append: [ExPerT: Effective and Explainable Evaluation of Personalized Long-Form Text Generation](https://arxiv.org/abs/2501.14956)
Append: [Dialogue Systems for Emotional Support via Value Reinforcement](https://arxiv.org/abs/2501.17182)
Append: [How to Select Datapoints for Efficient Human Evaluation of NLG Models?](https://arxiv.org/abs/2501.18251)
Append: [Calling a Spade a Heart: Gaslighting Multimodal Large Language Models via Negation](https://arxiv.org/abs/2501.19017)
Append: [Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search](https://arxiv.org/abs/2502.02508)
Append: [Reflection-Window Decoding: Text Generation with Selective Refinement](https://arxiv.org/abs/2502.03678)
Append: [Developmentally-plausible Working Memory Shapes a Critical Period for Language Acquisition](https://arxiv.org/abs/2502.04795)
Append: [Iterative Deepening Sampling as Efficient Test-Time Scaling](https://arxiv.org/abs/2502.05449)
Append: [FRAME: Boosting LLMs with A Four-Quadrant Multi-Stage Pretraining Strategy](https://arxiv.org/abs/2502.05551)
Append: [Non-literal Understanding of Number Words by Language Models](https://arxiv.org/abs/2502.06204)
Append: [Position: It's Time to Act on the Risk of Efficient Personalized Text Generation](https://arxiv.org/abs/2502.06560)
Append: [Survey on Vision-Language-Action Models](https://arxiv.org/abs/2502.06851)
Append: [GCoT: Chain-of-Thought Prompt Learning for Graphs](https://arxiv.org/abs/2502.08092)
Append: [Quality-Aware Decoding: Unifying Quality Estimation and Decoding](https://arxiv.org/abs/2502.08561)
Append: [RoToR: Towards More Reliable Responses for Order-Invariant Inputs](https://arxiv.org/abs/2502.08662)
Append: [Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation](https://arxiv.org/abs/2502.08826)
Append: [Can Uniform Meaning Representation Help GPT-4 Translate from Indigenous Languages?](https://arxiv.org/abs/2502.08900)
Append: [Prediction hubs are context-informed frequent tokens in LLMs](https://arxiv.org/abs/2502.10201)
Append: [A Survey of Large Language Models in Psychotherapy: Current Landscape and Future Directions](https://arxiv.org/abs/2502.11095)
Append: [Investigating Language Preference of Multilingual RAG Systems](https://arxiv.org/abs/2502.11175)
Append: [The Mirage of Model Editing: Revisiting Evaluation in the Wild](https://arxiv.org/abs/2502.11177)
Append: [LLMs can Perform Multi-Dimensional Analytic Writing Assessments: A Case Study of L2 Graduate-Level Academic English Writing](https://arxiv.org/abs/2502.11368)
Append: [Exploring Persona Sentiment Sensitivity in Personalized Dialogue Generation](https://arxiv.org/abs/2502.11423)
Append: [If Attention Serves as a Cognitive Model of Human Memory Retrieval, What is the Plausible Memory Representation?](https://arxiv.org/abs/2502.11469)
Append: [Diversity-oriented Data Augmentation with Large Language Models](https://arxiv.org/abs/2502.11671)
Append: [MT-RAIG: Novel Benchmark and Evaluation Framework for Retrieval-Augmented Insight Generation over Multiple Tables](https://arxiv.org/abs/2502.11735)
Append: [SpeechT: Findings of the First Mentorship in Speech Translation](https://arxiv.org/abs/2502.12050)
Append: [Pragmatics in the Era of Large Language Models: A Survey on Datasets, Evaluation, Opportunities and Challenges](https://arxiv.org/abs/2502.12378)
Append: [Theoretical Guarantees for Minimum Bayes Risk Decoding](https://arxiv.org/abs/2502.12685)
Append: [Pitfalls of Scale: Investigating the Inverse Task of Redefinition in Large Language Models](https://arxiv.org/abs/2502.12821)
Append: [Subword models struggle with word learning, but surprisal hides it](https://arxiv.org/abs/2502.12835)
Append: [HPSS: Heuristic Prompting Strategy Search for LLM Evaluators](https://arxiv.org/abs/2502.13031)
Append: [HumT DumT: Measuring and controlling human-like language in LLMs](https://arxiv.org/abs/2502.13259)
Append: [VITAL: A New Dataset for Benchmarking Pluralistic Alignment in Healthcare](https://arxiv.org/abs/2502.13775)
Append: [TESS 2: A Large-Scale Generalist Diffusion Language Model](https://arxiv.org/abs/2502.13917)
Append: [RAG-Gym: Systematic Optimization of Language Agents for Retrieval-Augmented Generation](https://arxiv.org/abs/2502.13957)
Append: [Which of These Best Describes Multiple Choice Evaluation with LLMs? A) Forced B) Flawed C) Fixable D) All of the Above](https://arxiv.org/abs/2502.14127)
Append: [Does Time Have Its Place? Temporal Heads: Where Language Models Recall Time-specific Information](https://arxiv.org/abs/2502.14258)
Append: [SEA-HELM: Southeast Asian Holistic Evaluation of Language Models](https://arxiv.org/abs/2502.14301)
Append: [Data-Constrained Synthesis of Training Data for De-Identification](https://arxiv.org/abs/2502.14677)
Append: [Harnessing PDF Data for Improving Japanese Large Multimodal Models](https://arxiv.org/abs/2502.14778)
Append: [Mapping 1,000+ Language Models via the Log-Likelihood Vector](https://arxiv.org/abs/2502.16173)
Append: [NUTSHELL: A Dataset for Abstract Generation from Scientific Talks](https://arxiv.org/abs/2502.16942)
Append: [Measuring Data Diversity for Instruction Tuning: A Systematic Analysis and A Reliable Metric](https://arxiv.org/abs/2502.17184)
Append: [Anything Goes? A Crosslinguistic Study of (Im)possible Language Learning in LMs](https://arxiv.org/abs/2502.18795)
Append: [Know You First and Be You Better: Modeling Human-Like User Simulators via Implicit Profiles](https://arxiv.org/abs/2502.18968)
Append: [TestNUC: Enhancing Test-Time Computing Approaches and Scaling through Neighboring Unlabeled Data Consistency](https://arxiv.org/abs/2502.19163)
Append: [EdiText: Controllable Coarse-to-Fine Text Editing with Diffusion Language Models](https://arxiv.org/abs/2502.19765)
Append: [FINEREASON: Evaluating and Improving LLMs' Deliberate Reasoning through Reflective Puzzle Solving](https://arxiv.org/abs/2502.20238)
Append: [How Much is Enough? The Diminishing Returns of Tokenization Training Data](https://arxiv.org/abs/2502.20273)
Append: [Protecting multimodal large language models against misleading visualizations](https://arxiv.org/abs/2502.20503)
Append: [Enhancing Text Editing for Grammatical Error Correction: Arabic as a Case Study](https://arxiv.org/abs/2503.00985)
Append: [MiLiC-Eval: Benchmarking Multilingual LLMs for China's Minority Languages](https://arxiv.org/abs/2503.01150)
Append: [A Comprehensive Survey of Machine Unlearning Techniques for Large Language Models](https://arxiv.org/abs/2503.01854)
Append: [Measuring What Makes You Unique: Difference-Aware User Modeling for Enhancing LLM Personalization](https://arxiv.org/abs/2503.02450)
Append: [EnigmaToM: Improve LLMs' Theory-of-Mind Reasoning Capabilities with Neural Knowledge Base of Entity States](https://arxiv.org/abs/2503.03340)
Append: [Large Language Models in Bioinformatics: A Survey](https://arxiv.org/abs/2503.04490)
Append: [Optimizing Multi-Hop Document Retrieval Through Intermediate Representations](https://arxiv.org/abs/2503.04796)
Append: [HoH: A Dynamic Benchmark for Evaluating the Impact of Outdated Information on Retrieval-Augmented Generation](https://arxiv.org/abs/2503.04800)
Append: [CSTRL: Context-Driven Sequential Transfer Learning for Abstractive Radiology Report Summarization](https://arxiv.org/abs/2503.05750)
Append: [GMLM: Bridging Graph Neural Networks and Language Models for Heterophilic Node Classification](https://arxiv.org/abs/2503.05763)
Append: [Beyond Decoder-only: Large Language Models Can be Good Encoders for Machine Translation](https://arxiv.org/abs/2503.06594)
Append: [Implicit Reasoning in Transformers is Reasoning through Shortcuts](https://arxiv.org/abs/2503.07604)
Append: [Context-aware Biases for Length Extrapolation](https://arxiv.org/abs/2503.08067)
Append: [Why Prompt Design Matters and Works: A Complexity Analysis of Prompt Search Space in LLMs](https://arxiv.org/abs/2503.10084)
Append: [ClusComp: A Simple Paradigm for Model Compression and Efficient Finetuning](https://arxiv.org/abs/2503.13089)
Append: [Navigating Rifts in Human-LLM Grounding: Study and Benchmark](https://arxiv.org/abs/2503.13975)
Append: [SPILL: Domain-Adaptive Intent Clustering based on Selection and Pooling with Large Language Models](https://arxiv.org/abs/2503.15351)
Append: [A Dual-Directional Context-Aware Test-Time Learning for Text Classification](https://arxiv.org/abs/2503.15469)
Append: [Deceptive Humor: A Synthetic Multilingual Benchmark Dataset for Bridging Fabricated Claims with Humorous Content](https://arxiv.org/abs/2503.16031)
Append: [CASE -- Condition-Aware Sentence Embeddings for Conditional Semantic Textual Similarity Measurement](https://arxiv.org/abs/2503.17279)
Append: [ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving Systems](https://arxiv.org/abs/2503.20756)
Append: [Both Direct and Indirect Evidence Contribute to Dative Alternation Preferences in Language Models](https://arxiv.org/abs/2503.20850)
Append: [Understanding Inequality of LLM Fact-Checking over Geographic Regions with Agent and Retrieval models](https://arxiv.org/abs/2503.22877)
Append: [A Conformal Risk Control Framework for Granular Word Assessment and Uncertainty Calibration of CLIPScore Quality Estimates](https://arxiv.org/abs/2504.01225)
Append: [Bridging the Linguistic Divide: A Survey on Leveraging Large Language Models for Machine Translation](https://arxiv.org/abs/2504.01919)
Append: [Post-Training Language Models for Continual Relation Extraction](https://arxiv.org/abs/2504.05214)
Append: [Persona Dynamics: Unveiling the Impact of Personality Traits on Agents in Text-Based Games](https://arxiv.org/abs/2504.06868)
Append: [Supervised Optimism Correction: Be Confident When LLMs Are Sure](https://arxiv.org/abs/2504.07527)
Append: [SD$^2$: Self-Distilled Sparse Drafters](https://arxiv.org/abs/2504.08838)
Append: [Parameterized Synthetic Text Generation with SimpleStories](https://arxiv.org/abs/2504.09184)
Append: [Guiding Reasoning in Small Language Models with LLM Assistance](https://arxiv.org/abs/2504.09923)
Append: [LazyReview A Dataset for Uncovering Lazy Thinking in NLP Peer Reviews](https://arxiv.org/abs/2504.11042)
Append: [Knowledge-augmented Pre-trained Language Models for Biomedical Relation Extraction](https://arxiv.org/abs/2505.00814)
Append: [Bemba Speech Translation: Exploring a Low-Resource African Language](https://arxiv.org/abs/2505.02518)
Append: [Using Information Theory to Characterize Prosodic Typology: The Case of Tone, Pitch-Accent and Stress-Accent](https://arxiv.org/abs/2505.07659)
Append: [Domain Regeneration: How well do LLMs match syntactic properties of text domains?](https://arxiv.org/abs/2505.07784)
Append: [Tracr-Injection: Distilling Algorithms into Pre-trained Language Models](https://arxiv.org/abs/2505.10719)
Append: [Probing Subphonemes in Morphology Models](https://arxiv.org/abs/2505.11297)
Append: [Disambiguating Reference in Visually Grounded Dialogues through Joint Modeling of Textual and Multimodal Semantic Structures](https://arxiv.org/abs/2505.11726)
Append: [Counterspeech the ultimate shield! Multi-Conditioned Counterspeech Generation through Attributed Prefix Learning](https://arxiv.org/abs/2505.11958)
Append: [Data Whisperer: Efficient Data Selection for Task-Specific LLM Fine-Tuning via Few-Shot In-Context Learning](https://arxiv.org/abs/2505.12212)
Append: [What is Stigma Attributed to? A Theory-Grounded, Expert-Annotated Interview Corpus for Demystifying Mental-Health Stigma](https://arxiv.org/abs/2505.12727)
Append: [A3 : an Analytical Low-Rank Approximation Framework for Attention](https://arxiv.org/abs/2505.12942)
Append: [A Case Study of Cross-Lingual Zero-Shot Generalization for Classical Languages in LLMs](https://arxiv.org/abs/2505.13173)
Append: [Rank, Chunk and Expand: Lineage-Oriented Reasoning for Taxonomy Expansion](https://arxiv.org/abs/2505.13282)
Append: [Enhancing LLMs via High-Knowledge Data Selection](https://arxiv.org/abs/2505.14070)
Append: [TRATES: Trait-Specific Rubric-Assisted Cross-Prompt Essay Scoring](https://arxiv.org/abs/2505.14577)
Append: [DUSK: Do Not Unlearn Shared Knowledge](https://arxiv.org/abs/2505.15209)
Append: [Attributing Response to Context: A Jensen-Shannon Divergence Driven Mechanistic Study of Context Attribution in Retrieval-Augmented Generation](https://arxiv.org/abs/2505.16415)
Append: [Power-Law Decay Loss for Large Language Model Finetuning: A Theory Perspective](https://arxiv.org/abs/2505.16900)
Append: [A Fully Generative Motivational Interviewing Counsellor Chatbot for Moving Smokers Towards the Decision to Quit](https://arxiv.org/abs/2505.17362)
Append: [Exploring the Effect of Segmentation and Vocabulary Size on Speech Tokenization for Speech Language Models](https://arxiv.org/abs/2505.17446)
Append: [Multimodal Conversation Structure Understanding](https://arxiv.org/abs/2505.17536)
Append: [Discriminating Form and Meaning in Multilingual Models with Minimal-Pair ABX Tasks](https://arxiv.org/abs/2505.17747)
Append: [TAG-INSTRUCT: Controlled Instruction Complexity Enhancement through Structure-based Augmentation](https://arxiv.org/abs/2505.18557)
Append: [Moderating Harm: Benchmarking Large Language Models for Cyberbullying Detection in YouTube Comments](https://arxiv.org/abs/2505.18927)
Append: [System-1.5 Reasoning: Traversal in Language and Latent Spaces with Dynamic Shortcuts](https://arxiv.org/abs/2505.18962)
Append: [Learning to Explain: Prototype-Based Surrogate Models for LLM Classification](https://arxiv.org/abs/2505.18970)
Append: [NeuSym-RAG: Hybrid Neural Symbolic Retrieval with Multiview Structuring for PDF Question Answering](https://arxiv.org/abs/2505.19754)
Append: [Efficient Speech Translation through Model Compression and Knowledge Distillation](https://arxiv.org/abs/2505.20237)
Append: [It's High Time: A Survey of Temporal Information Retrieval and Question Answering](https://arxiv.org/abs/2505.20243)
Append: [Learning distributed representations with efficient SoftMax normalization](https://arxiv.org/abs/2303.17475)
Append: [Towards a Neural Lambda Calculus: Neurosymbolic AI Applied to the Foundations of Functional Programming](https://arxiv.org/abs/2304.09276)
Append: [StarVector: Generating Scalable Vector Graphics Code from Images and Text](https://arxiv.org/abs/2312.11556)
Append: [SongComposer: A Large Language Model for Lyric and Melody Generation in Song Composition](https://arxiv.org/abs/2402.17645)
Append: [Calibration of Large Language Models on Code Summarization](https://arxiv.org/abs/2404.19318)
Append: [CityBench: Evaluating the Capabilities of Large Language Models for Urban Tasks](https://arxiv.org/abs/2406.13945)
Append: [CityGPT: Empowering Urban Spatial Cognition of Large Language Models](https://arxiv.org/abs/2406.13948)
Append: [Curriculum Learning with Quality-Driven Data Selection](https://arxiv.org/abs/2407.00102)
Append: [LETS-C: Leveraging Text Embedding for Time Series Classification](https://arxiv.org/abs/2407.06533)
Append: [Leveraging Variation Theory in Counterfactual Data Augmentation for Optimized Active Learning](https://arxiv.org/abs/2408.03819)
Append: [WET: Overcoming Paraphrasing Vulnerabilities in Embeddings-as-a-Service with Linear Transformation Watermarks](https://arxiv.org/abs/2409.04459)
Append: [ReflectDiffu:Reflect between Emotion-intent Contagion and Mimicry for Empathetic Response Generation via a RL-Diffusion Framework](https://arxiv.org/abs/2409.10289)
Append: [SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving Model Transformation](https://arxiv.org/abs/2410.03960)
Append: [SciEvo: A 2 Million, 30-Year Cross-disciplinary Dataset for Temporal Scientometric Analysis](https://arxiv.org/abs/2410.09510)
Append: [LLM-Mixer: Multiscale Mixing in LLMs for Time Series Forecasting](https://arxiv.org/abs/2410.11674)
Append: [Communication-Efficient and Tensorized Federated Fine-Tuning of Large Language Models](https://arxiv.org/abs/2410.13097)
Append: [Disentangling Likes and Dislikes in Personalized Generative Explainable Recommendation](https://arxiv.org/abs/2410.13248)
Append: [ChitroJera: A Regionally Relevant Visual Question Answering Dataset for Bangla](https://arxiv.org/abs/2410.14991)
Append: [CLEAR: Character Unlearning in Textual and Visual Modalities](https://arxiv.org/abs/2410.18057)
Append: [Autoregressive Models in Vision: A Survey](https://arxiv.org/abs/2411.05902)
Append: [Probing LLM Hallucination from Within: Perturbation-Driven Approach via Internal Knowledge](https://arxiv.org/abs/2411.09689)
Append: [VL-RewardBench: A Challenging Benchmark for Vision-Language Generative Reward Models](https://arxiv.org/abs/2411.17451)
Append: [Are Your LLMs Capable of Stable Reasoning?](https://arxiv.org/abs/2412.13147)
Append: [Mind Your Theory: Theory of Mind Goes Deeper Than Reasoning](https://arxiv.org/abs/2412.13631)
Append: [Are We in the AI-Generated Text World Already? Quantifying and Monitoring AIGT on Social Media](https://arxiv.org/abs/2412.18148)
Append: [Exploring Compositional Generalization of Multimodal LLMs for Medical Imaging](https://arxiv.org/abs/2412.20070)
Append: [Why Are Positional Encodings Nonessential for Deep Autoregressive Transformers? Revisiting a Petroglyph](https://arxiv.org/abs/2501.00659)
Append: [Generalizing from SIMPLE to HARD Visual Reasoning: Can We Mitigate Modality Imbalance in VLMs?](https://arxiv.org/abs/2501.02669)
Append: [Towards Early Prediction of Self-Supervised Speech Model Performance](https://arxiv.org/abs/2501.05966)
Append: [Feedback-Aware Monte Carlo Tree Search for Efficient Information Seeking in Goal-Oriented Conversations](https://arxiv.org/abs/2501.15056)
Append: [PIP: Perturbation-based Iterative Pruning for Large Language Models](https://arxiv.org/abs/2501.15278)
Append: [WhiSPA: Semantically and Psychologically Aligned Whisper with Self-Supervised Contrastive and Student-Teacher Learning](https://arxiv.org/abs/2501.16344)
Append: [The TIP of the Iceberg: Revealing a Hidden Class of Task-in-Prompt Adversarial Attacks on LLMs](https://arxiv.org/abs/2501.18626)
Append: [KVTuner: Sensitivity-Aware Layer-Wise Mixed-Precision KV Cache Quantization for Efficient and Nearly Lossless LLM Inference](https://arxiv.org/abs/2502.04420)
Append: [Safety at Scale: A Comprehensive Survey of Large Model Safety](https://arxiv.org/abs/2502.05206)
Append: [Bone Soups: A Seek-and-Soup Model Merging Approach for Controllable Multi-Objective Generation](https://arxiv.org/abs/2502.10762)
Append: [Primus: A Pioneering Collection of Open-Source Datasets for Cybersecurity LLM Training](https://arxiv.org/abs/2502.11191)
Append: [How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training](https://arxiv.org/abs/2502.11196)
Append: [From Selection to Generation: A Survey of LLM-based Active Learning](https://arxiv.org/abs/2502.11767)
Append: [AdaptiveStep: Automatically Dividing Reasoning Step through Model Confidence](https://arxiv.org/abs/2502.13943)
Append: [Standard Benchmarks Fail - Auditing LLM Agents in Finance Must Prioritize Risk](https://arxiv.org/abs/2502.15865)
Append: [From Perceptions to Decisions: Wildfire Evacuation Decision Prediction with Behavioral Theory-informed LLMs](https://arxiv.org/abs/2502.17701)
Append: [ZEBRA: Leveraging Model-Behavioral Knowledge for Zero-Annotation Preference Dataset Construction](https://arxiv.org/abs/2502.18744)
Append: [Tokens for Learning, Tokens for Unlearning: Mitigating Membership Inference Attacks in Large Language Models via Dual-Purpose Training](https://arxiv.org/abs/2502.19726)
Append: [OmniRouter: Budget and Performance Controllable Multi-LLM Routing](https://arxiv.org/abs/2502.20576)
Append: [I see what you mean: Co-Speech Gestures for Reference Resolution in Multimodal Dialogue](https://arxiv.org/abs/2503.00071)
Append: [Semantic Integrity Constraints: Declarative Guardrails for AI-Augmented Data Processing Systems](https://arxiv.org/abs/2503.00600)
Append: [Marco-o1 v2: Towards Widening The Distillation Bottleneck for Reasoning Models](https://arxiv.org/abs/2503.01461)
Append: [MMSciBench: Benchmarking Language Models on Chinese Multimodal Scientific Problems](https://arxiv.org/abs/2503.01891)
Append: [ProjectEval: A Benchmark for Programming Agents Automated Evaluation on Project-Level Code Generation](https://arxiv.org/abs/2503.07010)
Append: [SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language Model Interpretability](https://arxiv.org/abs/2503.09532)
Append: [MentalChat16K: A Benchmark Dataset for Conversational Mental Health Assistance](https://arxiv.org/abs/2503.13509)
Append: [Redefining Toxicity: An Objective and Context-Aware Approach for Stress-Level-Based Detection](https://arxiv.org/abs/2503.16072)
Append: [CodeReviewQA: The Code Review Comprehension Assessment for Large Language Models](https://arxiv.org/abs/2503.16167)
Append: [REALM: A Dataset of Real-World LLM Use Cases](https://arxiv.org/abs/2503.18792)
Append: [Large Language and Reasoning Models are Shallow Disjunctive Reasoners](https://arxiv.org/abs/2503.23487)
Append: [OmniCaptioner: One Captioner to Rule Them All](https://arxiv.org/abs/2504.07089)
Append: [Graph-Driven Multimodal Feature Learning Framework for Apparent Personality Assessment](https://arxiv.org/abs/2504.11515)
Append: [3MDBench: Medical Multimodal Multi-agent Dialogue Benchmark](https://arxiv.org/abs/2504.13861)
Append: [Completing A Systematic Review in Hours instead of Months with Interactive AI Agents](https://arxiv.org/abs/2504.14822)
Append: [Acting Less is Reasoning More! Teaching Model to Act Efficiently](https://arxiv.org/abs/2504.14870)
Append: [Visualizing Public Opinion on X: A Real-Time Sentiment Dashboard Using VADER and DistilBERT](https://arxiv.org/abs/2504.15448)
Append: [(Im)possibility of Automated Hallucination Detection in Large Language Models](https://arxiv.org/abs/2504.17004)
Append: [Graph-Based Spectral Decomposition for Parameter Coordination in Language Model Fine-Tuning](https://arxiv.org/abs/2504.19583)
Append: [Which Agent Causes Task Failures and When? On Automated Failure Attribution of LLM Multi-Agent Systems](https://arxiv.org/abs/2505.00212)
Append: [Reassessing Large Language Model Boolean Query Generation for Systematic Reviews](https://arxiv.org/abs/2505.07155)
Append: [EVALOOP: Assessing LLM Robustness in Programming from a Self-consistency Perspective](https://arxiv.org/abs/2505.12185)
Append: [Forensic deepfake audio detection using segmental speech features](https://arxiv.org/abs/2505.13847)
Append: [RADAR: Enhancing Radiology Report Generation with Supplementary Knowledge Injection](https://arxiv.org/abs/2505.14318)
Append: [SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment](https://arxiv.org/abs/2505.14667)
Append: [TrimR: Verifier-based Training-Free Thinking Compression for Efficient Test-Time Scaling](https://arxiv.org/abs/2505.17155)
Append: [An End-to-End Approach for Child Reading Assessment in the Xhosa Language](https://arxiv.org/abs/2505.17371)
Append: [Co-Reinforcement Learning for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2505.17534)
Append: [One RL to See Them All: Visual Triple Unified Reinforcement Learning](https://arxiv.org/abs/2505.18129)
Append: [A Survey of LLM $\times$ DATA](https://arxiv.org/abs/2505.18458)
Append: [ChartGalaxy: A Dataset for Infographic Chart Understanding and Generation](https://arxiv.org/abs/2505.18668)
Append: [VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D Reconstruction](https://arxiv.org/abs/2505.20279)
append_entries: 596
Finish: 2025-06-03 04:29:26.961562
------------------------------------------------------
Started: 2025-06-03 06:26:30.174997
Existing_entries: 1596
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 899
Summarized using GPT-3.5-turbo
Append: [SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge Refinement](https://arxiv.org/abs/2504.03561)
Token length: 1052
Summarized using GPT-3.5-turbo
Append: [The NaijaVoices Dataset: Cultivating Large-Scale, High-Quality, Culturally-Rich Speech Data for African Languages](https://arxiv.org/abs/2505.20564)
Token length: 1335
Summarized using GPT-3.5-turbo
Append: [More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models](https://arxiv.org/abs/2505.21523)
Token length: 1937
Summarized using GPT-3.5-turbo
Append: [RedTeamCUA: Realistic Adversarial Testing of Computer-Use Agents in Hybrid Web-OS Environments](https://arxiv.org/abs/2505.21936)
Token length: 1057
Summarized using GPT-3.5-turbo
Append: [TabXEval: Why this is a Bad Table? An eXhaustive Rubric for Table Evaluation](https://arxiv.org/abs/2505.22176)
Token length: 1215
Summarized using GPT-3.5-turbo
Append: [Judging Quality Across Languages: A Multilingual Approach to Pretraining Data Filtering with Language Models](https://arxiv.org/abs/2505.22232)
Token length: 1499
Summarized using GPT-3.5-turbo
Append: [Chain-of-Talkers (CoTalk): Fast Human Annotation of Dense Image Captions](https://arxiv.org/abs/2505.22627)
Token length: 950
Summarized using GPT-3.5-turbo
Append: [Active Layer-Contrastive Decoding Reduces Hallucination in Large Language Model Generation](https://arxiv.org/abs/2505.23657)
Token length: 1344
Summarized using GPT-3.5-turbo
Append: [Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time](https://arxiv.org/abs/2505.23729)
Token length: 1293
Summarized using GPT-3.5-turbo
Append: [AdvAgent: Controllable Blackbox Red-teaming on Web Agents](https://arxiv.org/abs/2410.17401)
Token length: 1112
Summarized using GPT-3.5-turbo
Append: [Wanda++: Pruning Large Language Models via Regional Gradients](https://arxiv.org/abs/2503.04992)
Token length: 1246
Summarized using GPT-3.5-turbo
Append: [Hierarchical Retrieval with Evidence Curation for Open-Domain Financial Question Answering on Standardized Documents](https://arxiv.org/abs/2505.20368)
Token length: 1692
Summarized using GPT-3.5-turbo
Append: [How Do Transformers Learn Variable Binding in Symbolic Programs?](https://arxiv.org/abs/2505.20896)
Token length: 1546
Summarized using GPT-3.5-turbo
Append: [Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal Transformer Distillation](https://arxiv.org/abs/2505.21549)
Token length: 1518
Summarized using GPT-3.5-turbo
Append: [Modeling and Optimizing User Preferences in AI Copilots: A Comprehensive Survey and Taxonomy](https://arxiv.org/abs/2505.21907)
Token length: 1742
Summarized using GPT-3.5-turbo
Append: [SWE-bench Goes Live!](https://arxiv.org/abs/2505.23419)
Token length: 1943
Summarized using GPT-3.5-turbo
Append: [Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with Jigsaw Puzzles](https://arxiv.org/abs/2505.23590)
Token length: 1184
Summarized using GPT-3.5-turbo
Append: [GSO: Challenging Software Optimization Tasks for Evaluating SWE-Agents](https://arxiv.org/abs/2505.23671)
append_entries: 18
Finish: 2025-06-03 06:27:12.362394
------------------------------------------------------
Started: 2025-06-03 08:24:19.684449
Existing_entries: 1018
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1037
Summarized using GPT-3.5-turbo
Append: [ScEdit: Script-based Assessment of Knowledge Editing](https://arxiv.org/abs/2505.23291)
append_entries: 1
Finish: 2025-06-03 08:24:22.928284
------------------------------------------------------
Started: 2025-06-03 10:18:56.570917
Existing_entries: 1001
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-03 10:18:57.742727
------------------------------------------------------
Started: 2025-06-03 12:35:14.423829
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-03 12:35:15.610200
------------------------------------------------------
Started: 2025-06-03 14:17:35.886558
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-03 14:17:37.097031
------------------------------------------------------
Started: 2025-06-03 16:23:08.937482
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-03 16:23:10.090441
------------------------------------------------------
Started: 2025-06-03 18:24:16.542015
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-03 18:24:17.697837
------------------------------------------------------
Started: 2025-06-03 20:19:30.821899
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-03 20:19:32.051524
------------------------------------------------------
Started: 2025-06-03 22:16:30.484959
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-03 22:16:31.646851
------------------------------------------------------
Started: 2025-06-04 01:20:41.708384
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-04 01:20:42.967672
------------------------------------------------------
Started: 2025-06-04 03:14:18.165248
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-04 03:14:19.397618
------------------------------------------------------
Started: 2025-06-04 04:26:03.977868
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1193
Summarized using GPT-3.5-turbo
Append: [Research on Medical Named Entity Identification Based On Prompt-Biomrc Model and Its Application in Intelligent Consultation System](https://arxiv.org/abs/2506.01961)
Token length: 1329
Summarized using GPT-3.5-turbo
Append: [No Free Lunch in Active Learning: LLM Embedding Quality Dictates Query Strategy Success](https://arxiv.org/abs/2506.01992)
Token length: 1171
Summarized using GPT-3.5-turbo
Append: [NovelHopQA: Diagnosing Multi-Hop Reasoning Failures in Long Narrative Contexts](https://arxiv.org/abs/2506.02000)
Token length: 931
Summarized using GPT-3.5-turbo
Append: [Pruning for Performance: Efficient Idiom and Metaphor Classification in Low-Resource Konkani Using mBERT](https://arxiv.org/abs/2506.02005)
Token length: 1660
Summarized using GPT-3.5-turbo
Append: [Enhancing Paraphrase Type Generation: The Impact of DPO and RLHF Evaluated with Human-Ranked Data](https://arxiv.org/abs/2506.02018)
Token length: 895
Summarized using GPT-3.5-turbo
Append: [ChatCFD: an End-to-End CFD Agent with Domain-specific Structured Thinking](https://arxiv.org/abs/2506.02019)
Token length: 1450
Summarized using GPT-3.5-turbo
Append: [FinS-Pilot: A Benchmark for Online Financial System](https://arxiv.org/abs/2506.02037)
Token length: 1388
Summarized using GPT-3.5-turbo
Append: [Enhancing Multimodal Continual Instruction Tuning with BranchLoRA](https://arxiv.org/abs/2506.02041)
Token length: 1257
Summarized using GPT-3.5-turbo
Append: [Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?](https://arxiv.org/abs/2506.02058)
Token length: 1522
Summarized using GPT-3.5-turbo
Append: [Knowledge or Reasoning? A Close Look at How LLMs Think Across Domains](https://arxiv.org/abs/2506.02126)
Token length: 1547
Summarized using GPT-3.5-turbo
Append: [Model Internal Sleuthing: Finding Lexical Identity and Inflectional Morphology in Modern Language Models](https://arxiv.org/abs/2506.02132)
Token length: 1063
Summarized using GPT-3.5-turbo
Append: [BabyLM's First Constructions: Causal interventions provide a signal of learning](https://arxiv.org/abs/2506.02147)
Token length: 1425
Summarized using GPT-3.5-turbo
Append: [HENT-SRT: Hierarchical Efficient Neural Transducer with Self-Distillation for Joint Speech Recognition and Translation](https://arxiv.org/abs/2506.02157)
Token length: 927
Summarized using GPT-3.5-turbo
Append: [Different Speech Translation Models Encode and Translate Speaker Gender Differently](https://arxiv.org/abs/2506.02172)
Token length: 1935
Summarized using GPT-3.5-turbo
Append: [AI Debate Aids Assessment of Controversial Claims](https://arxiv.org/abs/2506.02175)
Token length: 1034
Summarized using GPT-3.5-turbo
Append: [Echoes of Phonetics: Unveiling Relevant Acoustic Cues for ASR via Feature Attribution](https://arxiv.org/abs/2506.02181)
Token length: 1277
Summarized using GPT-3.5-turbo
Append: [BehaviorBox: Automated Discovery of Fine-Grained Performance Differences Between Language Models](https://arxiv.org/abs/2506.02204)
Token length: 1188
Summarized using GPT-3.5-turbo
Append: [Leveraging Natural Language Processing to Unravel the Mystery of Life: A Review of NLP Approaches in Genomics, Transcriptomics, and Proteomics](https://arxiv.org/abs/2506.02212)
Token length: 1014
Summarized using GPT-3.5-turbo
Append: [Investigating the Impact of Word Informativeness on Speech Emotion Recognition](https://arxiv.org/abs/2506.02239)
Token length: 1272
Summarized using GPT-3.5-turbo
Append: [CoDial: Interpretable Task-Oriented Dialogue Systems Through Dialogue Flow Alignment](https://arxiv.org/abs/2506.02264)
Token length: 1365
Summarized using GPT-3.5-turbo
Append: [ImpRAG: Retrieval-Augmented Generation with Implicit Queries](https://arxiv.org/abs/2506.02279)
Token length: 1004
Summarized using GPT-3.5-turbo
Append: [Sounding Like a Winner? Prosodic Differences in Post-Match Interviews](https://arxiv.org/abs/2506.02283)
Token length: 1345
Summarized using GPT-3.5-turbo
Append: [LAM SIMULATOR: Advancing Data Generation for Large Action Model Training via Online Exploration and Trajectory Feedback](https://arxiv.org/abs/2506.02298)
Token length: 1133
Summarized using GPT-3.5-turbo
Append: [Explain-then-Process: Using Grammar Prompting to Enhance Grammatical Acceptability Judgments](https://arxiv.org/abs/2506.02302)
Token length: 1309
Summarized using GPT-3.5-turbo
Append: [Quantifying Misattribution Unfairness in Authorship Attribution](https://arxiv.org/abs/2506.02321)
Token length: 1002
Summarized using GPT-3.5-turbo
Append: [Something Just Like TRuST : Toxicity Recognition of Span and Target](https://arxiv.org/abs/2506.02326)
Token length: 1316
Summarized using GPT-3.5-turbo
Append: [One Missing Piece for Open-Source Reasoning Models: A Dataset to Mitigate Cold-Starting Short CoT LLMs in RL](https://arxiv.org/abs/2506.02338)
Token length: 1490
Summarized using GPT-3.5-turbo
Append: [STORYTELLER: An Enhanced Plot-Planning Framework for Coherent and Cohesive Story Generation](https://arxiv.org/abs/2506.02347)
Token length: 1450
Summarized using GPT-3.5-turbo
Append: [Truth over Tricks: Measuring and Mitigating Shortcut Learning in Misinformation Detection](https://arxiv.org/abs/2506.02350)
Token length: 1217
Summarized using GPT-3.5-turbo
Append: [DIAMOND: An LLM-Driven Agent for Context-Aware Baseball Highlight Summarization](https://arxiv.org/abs/2506.02351)
Token length: 1004
Summarized using GPT-3.5-turbo
Append: [AnswerCarefully: A Dataset for Improving the Safety of Japanese LLM Output](https://arxiv.org/abs/2506.02372)
Token length: 907
Summarized using GPT-3.5-turbo
Append: [Exploring Explanations Improves the Robustness of In-Context Learning](https://arxiv.org/abs/2506.02378)
Token length: 1244
Summarized using GPT-3.5-turbo
Append: [Consultant Decoding: Yet Another Synergistic Mechanism](https://arxiv.org/abs/2506.02391)
Token length: 1870
Summarized using GPT-3.5-turbo
Append: [GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2506.02404)
Token length: 1220
Summarized using GPT-3.5-turbo
Append: [SingaKids: A Multilingual Multimodal Dialogic Tutor for Language Learning](https://arxiv.org/abs/2506.02412)
Token length: 1008
Summarized using GPT-3.5-turbo
Append: [Gender Inequality in English Textbooks Around the World: an NLP Approach](https://arxiv.org/abs/2506.02425)
Token length: 1273
Summarized using GPT-3.5-turbo
Append: [Comparative Analysis of AI Agent Architectures for Entity Relationship Classification](https://arxiv.org/abs/2506.02426)
Token length: 905
Summarized using GPT-3.5-turbo
Append: [From Anger to Joy: How Nationality Personas Shape Emotion Attribution in Large Language Models](https://arxiv.org/abs/2506.02431)
Token length: 977
Summarized using GPT-3.5-turbo
Append: [Should LLM Safety Be More Than Refusing Harmful Instructions?](https://arxiv.org/abs/2506.02442)
Token length: 1031
Summarized using GPT-3.5-turbo
Append: [IP-Dialog: Evaluating Implicit Personalization in Dialogue Systems with Synthetic Data](https://arxiv.org/abs/2506.02449)
Token length: 1509
Summarized using GPT-3.5-turbo
Append: [Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports From Scratch with Agentic Framework](https://arxiv.org/abs/2506.02454)
Token length: 1553
Summarized using GPT-3.5-turbo
Append: [MidPO: Dual Preference Optimization for Safety and Helpfulness in Large Language Models via a Mixture of Experts Framework](https://arxiv.org/abs/2506.02460)
Token length: 970
Summarized using GPT-3.5-turbo
Append: [XToM: Exploring the Multilingual Theory of Mind for Large Language Models](https://arxiv.org/abs/2506.02461)
Token length: 1069
Summarized using GPT-3.5-turbo
Append: [FroM: Frobenius Norm-Based Data-Free Adaptive Model Merging](https://arxiv.org/abs/2506.02478)
Token length: 1449
Summarized using GPT-3.5-turbo
Append: [ORPP: Self-Optimizing Role-playing Prompts to Enhance Language Model Capabilities](https://arxiv.org/abs/2506.02480)
Token length: 1421
Summarized using GPT-3.5-turbo
Append: [Do Language Models Think Consistently? A Study of Value Preferences Across Varying Response Lengths](https://arxiv.org/abs/2506.02481)
Token length: 953
Summarized using GPT-3.5-turbo
Append: [Enhancing Large Language Models with Neurosymbolic Reasoning for Multilingual Tasks](https://arxiv.org/abs/2506.02483)
Token length: 1263
Summarized using GPT-3.5-turbo
Append: [Minos: A Multimodal Evaluation Model for Bidirectional Generation Between Image and Text](https://arxiv.org/abs/2506.02494)
Token length: 1477
Summarized using GPT-3.5-turbo
Append: [KARE-RAG: Knowledge-Aware Refinement and Enhancement for RAG](https://arxiv.org/abs/2506.02503)
Token length: 1409
Summarized using GPT-3.5-turbo
Append: [M$^3$FinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial Meeting Understanding Evaluation Dataset](https://arxiv.org/abs/2506.02510)
Append: [FinChain: A Symbolic Benchmark for Verifiable Chain-of-Thought Financial Reasoning](https://arxiv.org/abs/2506.02515)
Append: [Learning Together to Perform Better: Teaching Small-Scale LLMs to Collaborate via Preferential Rationale Tuning](https://arxiv.org/abs/2506.02519)
Append: [Multilingual Information Retrieval with a Monolingual Knowledge Base](https://arxiv.org/abs/2506.02527)
Append: [ReasoningFlow: Semantic Structure of Complex Reasoning Traces](https://arxiv.org/abs/2506.02532)
Append: [Natural Language Processing to Enhance Deliberation in Political Online Discussions: A Survey](https://arxiv.org/abs/2506.02533)
Append: [Answer Convergence as a Signal for Early Stopping in Reasoning](https://arxiv.org/abs/2506.02536)
Append: [CoRe-MMRAG: Cross-Source Knowledge Reconciliation for Multimodal RAG](https://arxiv.org/abs/2506.02544)
Append: [Pruning General Large Language Models into Customized Expert Models](https://arxiv.org/abs/2506.02561)
Append: [IndoSafety: Culturally Grounded Safety for LLMs in Indonesian Languages](https://arxiv.org/abs/2506.02573)
Append: [Prosodic Structure Beyond Lexical Content: A Study of Self-Supervised Learning](https://arxiv.org/abs/2506.02584)
Append: [Evaluating Named Entity Recognition Models for Russian Cultural News Texts: From BERT to LLM](https://arxiv.org/abs/2506.02589)
Append: [On Generalization across Measurement Systems: LLMs Entail More Test-Time Compute for Underrepresented Cultures](https://arxiv.org/abs/2506.02591)
Append: [Beyond the Surface: Measuring Self-Preference in LLM Judgments](https://arxiv.org/abs/2506.02592)
Append: [EssayBench: Evaluating Large Language Models in Multi-Genre Chinese Essay Writing](https://arxiv.org/abs/2506.02596)
Append: [Overcoming Data Scarcity in Multi-Dialectal Arabic ASR via Whisper Fine-Tuning](https://arxiv.org/abs/2506.02627)
Append: [Are Economists Always More Introverted? Analyzing Consistency in Persona-Assigned LLMs](https://arxiv.org/abs/2506.02659)
Append: [EvaLearn: Quantifying the Learning Capability and Efficiency of LLMs via Sequential Problem Solving](https://arxiv.org/abs/2506.02672)
Append: [TL;DR: Too Long, Do Re-weighting for Effcient LLM Reasoning Compression](https://arxiv.org/abs/2506.02678)
Append: [Decompose, Plan in Parallel, and Merge: A Novel Paradigm for Large Language Models based Planning with Multiple Constraints](https://arxiv.org/abs/2506.02683)
Append: [MASTER: Enhancing Large Language Model via Multi-Agent Simulated Teaching](https://arxiv.org/abs/2506.02689)
Append: [On Entity Identification in Language Models](https://arxiv.org/abs/2506.02701)
Append: [RACE-Align: Retrieval-Augmented and Chain-of-Thought Enhanced Preference Alignment for Large Language Models](https://arxiv.org/abs/2506.02726)
Append: [Stereotypical gender actions can be extracted from Web text](https://arxiv.org/abs/2506.02740)
Append: [Multi-task Learning with Active Learning for Arabic Offensive Speech Detection](https://arxiv.org/abs/2506.02753)
Append: [Exploiting the English Vocabulary Profile for L2 word-level vocabulary assessment with LLMs](https://arxiv.org/abs/2506.02758)
Append: [SemVink: Advancing VLMs' Semantic Understanding of Optical Illusions via Visual Global Thinking](https://arxiv.org/abs/2506.02803)
Append: [ProcrustesGPT: Compressing LLMs with Structured Matrices and Orthogonal Transformations](https://arxiv.org/abs/2506.02818)
Append: [TO-GATE: Clarifying Questions and Summarizing Responses with Trajectory Optimization for Eliciting Human Preference](https://arxiv.org/abs/2506.02827)
Append: [Token and Span Classification for Entity Recognition in French Historical Encyclopedias](https://arxiv.org/abs/2506.02872)
Append: [CoT is Not True Reasoning, It Is Just a Tight Constraint to Imitate: A Theory Perspective](https://arxiv.org/abs/2506.02878)
Append: [A Multi-Dialectal Dataset for German Dialect ASR and Dialect-to-Standard Speech Translation](https://arxiv.org/abs/2506.02894)
Append: [IMPARA-GED: Grammatical Error Detection is Boosting Reference-free Grammatical Error Quality Estimator](https://arxiv.org/abs/2506.02899)
Append: [Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with Reinforcement Learning](https://arxiv.org/abs/2506.02911)
Append: [A Controllable Examination for Long-Context Language Models](https://arxiv.org/abs/2506.02921)
Append: [INESC-ID @ eRisk 2025: Exploring Fine-Tuned, Similarity-Based, and Prompt-Based Approaches to Depression Symptom Identification](https://arxiv.org/abs/2506.02924)
Append: [Quantitative LLM Judges](https://arxiv.org/abs/2506.02945)
Append: [Adaptive Graph Pruning for Multi-Agent Communication](https://arxiv.org/abs/2506.02951)
Append: [HACo-Det: A Study Towards Fine-Grained Machine-Generated Text Detection under Human-AI Coauthoring](https://arxiv.org/abs/2506.02959)
Append: [FlowerTune: A Cross-Domain Benchmark for Federated Fine-Tuning of Large Language Models](https://arxiv.org/abs/2506.02961)
Append: [Expanding before Inferring: Enhancing Factuality in Large Language Models through Premature Layers Interpolation](https://arxiv.org/abs/2506.02973)
Append: [Towards a Japanese Full-duplex Spoken Dialogue System](https://arxiv.org/abs/2506.02979)
Append: [Performance of leading large language models in May 2025 in Membership of the Royal College of General Practitioners-style examination questions: a cross-sectional analysis](https://arxiv.org/abs/2506.02987)
Append: [It's Not a Walk in the Park! Challenges of Idiom Translation in Speech-to-text Systems](https://arxiv.org/abs/2506.02995)
Append: [A Multi-Agent Framework for Mitigating Dialect Biases in Privacy Policy Question-Answering Systems](https://arxiv.org/abs/2506.02998)
Append: [Conditioning Large Language Models on Legal Systems? Detecting Punishable Hate Speech](https://arxiv.org/abs/2506.03009)
Append: [Coding Agents with Multimodal Browsing are Generalist Problem Solvers](https://arxiv.org/abs/2506.03011)
Append: [Leveraging Information Retrieval to Enhance Spoken Language Understanding Prompts in Few-Shot Learning](https://arxiv.org/abs/2506.03035)
Append: [Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective](https://arxiv.org/abs/2506.03038)
Append: [Facts Do Care About Your Language: Assessing Answer Quality of Multilingual LLMs](https://arxiv.org/abs/2506.03051)
Append: [Literary Evidence Retrieval via Long-Context Language Models](https://arxiv.org/abs/2506.03090)
Append: [Beyond Text Compression: Evaluating Tokenizers Across Scales](https://arxiv.org/abs/2506.03101)
Append: [Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback](https://arxiv.org/abs/2506.03106)
Append: [AUTOCIRCUIT-RL: Reinforcement Learning-Driven LLM for Automated Circuit Topology Generation](https://arxiv.org/abs/2506.03122)
Append: [Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning](https://arxiv.org/abs/2506.03136)
Append: [GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents](https://arxiv.org/abs/2506.03143)
Append: [Entity-Augmented Neuroscience Knowledge Retrieval Using Ontology and Semantic Understanding Capability of LLM](https://arxiv.org/abs/2506.03145)
Append: [Causal Estimation of Tokenisation Bias](https://arxiv.org/abs/2506.03149)
Append: [Generate, Not Recommend: Personalized Multimodal Content Generation](https://arxiv.org/abs/2506.01704)
Append: [Breaking Quadratic Barriers: A Non-Attention LLM for Ultra-Long Context Horizons](https://arxiv.org/abs/2506.01963)
Append: [Turning LLM Activations Quantization-Friendly](https://arxiv.org/abs/2506.01967)
Append: [Inter(sectional) Alia(s): Ambiguity in Voice Agent Identity via Intersectional Japanese Self-Referents](https://arxiv.org/abs/2506.01998)
Append: [Enhancing Speech Instruction Understanding and Disambiguation in Robotics via Speech Prosody](https://arxiv.org/abs/2506.02057)
Append: [Learning More with Less: Self-Supervised Approaches for Low-Resource Speech Emotion Recognition](https://arxiv.org/abs/2506.02059)
Append: [Assigning Distinct Roles to Quantized and Low-Rank Matrices Toward Optimal Weight Decomposition](https://arxiv.org/abs/2506.02077)
Append: [Unveiling Audio Deepfake Origins: A Deep Metric learning And Conformer Network Approach With Ensemble Fusion](https://arxiv.org/abs/2506.02085)
Append: [Enhancing Speech Emotion Recognition with Graph-Based Multimodal Fusion and Prosodic Features for the Speech Emotion Recognition in Naturalistic Conditions Challenge at Interspeech 2025](https://arxiv.org/abs/2506.02088)
Append: [SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis](https://arxiv.org/abs/2506.02096)
Append: [A Dynamic Framework for Semantic Grouping of Common Data Elements (CDE) Using Embeddings and Clustering](https://arxiv.org/abs/2506.02160)
Append: [Cocktail-Party Audio-Visual Speech Recognition](https://arxiv.org/abs/2506.02178)
Append: [KDRL: Post-Training Reasoning LLMs via Unified Knowledge Distillation and Reinforcement Learning](https://arxiv.org/abs/2506.02208)
Append: [VLCD: Vision-Language Contrastive Distillation for Accurate and Efficient Automatic Placenta Analysis](https://arxiv.org/abs/2506.02229)
Append: [ResearchCodeBench: Benchmarking LLMs on Implementing Novel Machine Learning Research Code](https://arxiv.org/abs/2506.02314)
Append: [StarVC: A Unified Auto-Regressive Framework for Joint Text and Speech Generation in Voice Conversion](https://arxiv.org/abs/2506.02414)
Append: [Comba: Improving Nonlinear RNNs with Closed-loop Control](https://arxiv.org/abs/2506.02475)
Append: [BitBypass: A New Direction in Jailbreaking Aligned Large Language Models with Bitstream Camouflage](https://arxiv.org/abs/2506.02479)
Append: [Automated Web Application Testing: End-to-End Test Case Generation with Large Language Models and Screen Transition Graphs](https://arxiv.org/abs/2506.02529)
Append: [Response-Level Rewards Are All You Need for Online Reinforcement Learning in LLMs: A Mathematical Perspective](https://arxiv.org/abs/2506.02553)
Append: [Synthetic Speech Source Tracing using Metric Learning](https://arxiv.org/abs/2506.02590)
Append: [Iterative Self-Improvement of Vision Language Models for Image Scoring and Self-Explanation](https://arxiv.org/abs/2506.02708)
Append: [Benchmarking and Advancing Large Language Models for Local Life Services](https://arxiv.org/abs/2506.02720)
Append: [An Exploratory Framework for Future SETI Applications: Detecting Generative Reactivity via Language Models](https://arxiv.org/abs/2506.02730)
Append: [Rethinking Machine Unlearning in Image Generation Models](https://arxiv.org/abs/2506.02761)
Append: [Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens are Information Peaks in LLM Reasoning](https://arxiv.org/abs/2506.02867)
Append: [Scaling Fine-Grained MoE Beyond 50B Parameters: Empirical Evaluation and Practical Insights](https://arxiv.org/abs/2506.02890)
Append: [Mitigating Manipulation and Enhancing Persuasion: A Reflective Multi-Agent Approach for Legal Argument Generation](https://arxiv.org/abs/2506.02992)
Append: [MAEBE: Multi-Agent Emergent Behavior Framework](https://arxiv.org/abs/2506.03053)
Append: [Retrieval-Augmented Generation as Noisy In-Context Learning: A Unified Theory and Risk Bounds](https://arxiv.org/abs/2506.03100)
Append: [OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models](https://arxiv.org/abs/2506.03135)
Append: [MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition Query](https://arxiv.org/abs/2506.03144)
Append: [UniWorld: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation](https://arxiv.org/abs/2506.03147)
Append: [Can Character-based Language Models Improve Downstream Task Performance in Low-Resource and Noisy Language Scenarios?](https://arxiv.org/abs/2110.13658)
Append: [TransAug: Translate as Augmentation for Sentence Embeddings](https://arxiv.org/abs/2111.00157)
Append: [Improving Transformer Performance for French Clinical Notes Classification Using Mixture of Experts on a Limited Dataset](https://arxiv.org/abs/2303.12892)
Append: [UltraWiki: Ultra-fine-grained Entity Set Expansion with Negative Seed Entities](https://arxiv.org/abs/2403.04247)
Append: [Revealing the Parallel Multilingual Learning within Large Language Models](https://arxiv.org/abs/2403.09073)
Append: [Checkpoint Merging via Bayesian Optimization in LLM Pretraining](https://arxiv.org/abs/2403.19390)
Append: [LLMs can Find Mathematical Reasoning Mistakes by Pedagogical Chain-of-Thought](https://arxiv.org/abs/2405.06705)
Append: [SignMusketeers: An Efficient Multi-Stream Approach for Sign Language Translation at Scale](https://arxiv.org/abs/2406.06907)
Append: [Unmasking Database Vulnerabilities: Zero-Knowledge Schema Inference Attacks in Text-to-SQL Systems](https://arxiv.org/abs/2406.14545)
Append: [Free-text Rationale Generation under Readability Level Control](https://arxiv.org/abs/2407.01384)
Append: [UnSeenTimeQA: Time-Sensitive Question-Answering Beyond LLMs' Memorization](https://arxiv.org/abs/2407.03525)
Append: [Localizing and Mitigating Errors in Long-form Question Answering](https://arxiv.org/abs/2407.11930)
Append: [A Survey on Employing Large Language Models for Text-to-SQL Tasks](https://arxiv.org/abs/2407.15186)
Append: [Cross-Institutional Dental EHR Entity Extraction via Generative AI and Synthetic Notes](https://arxiv.org/abs/2407.21050)
Append: [Lower Layers Matter: Alleviating Hallucination via Multi-Layer Fusion Contrastive Decoding with Truthfulness Refocused](https://arxiv.org/abs/2408.08769)
Append: [XTRUST: On the Multilingual Trustworthiness of Large Language Models](https://arxiv.org/abs/2409.15762)
Append: [How to Connect Speech Foundation Models and Large Language Models? What Matters and What Does Not](https://arxiv.org/abs/2409.17044)
Append: [Answer When Needed, Forget When Not: Language Models Pretend to Forget via In-Context Knowledge Unlearning](https://arxiv.org/abs/2410.00382)
Append: [CulturalBench: A Robust, Diverse, and Challenging Cultural Benchmark by Human-AI CulturalTeaming](https://arxiv.org/abs/2410.02677)
Append: [Chain-of-Jailbreak Attack for Image Generation Models via Editing Step by Step](https://arxiv.org/abs/2410.03869)
Append: [Large Language Model Evaluation via Matrix Nuclear-Norm](https://arxiv.org/abs/2410.10672)
Append: [Watching the Watchers: Exposing Gender Disparities in Machine Translation Quality Estimation](https://arxiv.org/abs/2410.10995)
Append: [Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning](https://arxiv.org/abs/2410.11020)
Append: [BenchmarkCards: Standardized Documentation for Large Language Model Benchmarks](https://arxiv.org/abs/2410.12974)
Append: [A Complexity-Based Theory of Compositionality](https://arxiv.org/abs/2410.14817)
Append: [EoRA: Fine-tuning-free Compensation for Compressed LLM with Eigenspace Low-Rank Approximation](https://arxiv.org/abs/2410.21271)
Append: [Self-Evolved Reward Learning for LLMs](https://arxiv.org/abs/2411.00418)
Append: [Generative Emotion Cause Explanation in Multimodal Conversations](https://arxiv.org/abs/2411.02430)
Append: [What Goes Into a LM Acceptability Judgment? Rethinking the Impact of Frequency and Length](https://arxiv.org/abs/2411.02528)
Append: [SuffixDecoding: Extreme Speculative Decoding for Emerging AI Applications](https://arxiv.org/abs/2411.04975)
Append: [SHARP: Unlocking Interactive Hallucination via Stance Transfer in Role-Playing LLMs](https://arxiv.org/abs/2411.07965)
Append: [Value-Spectrum: Quantifying Preferences of Vision-Language Models via Value Decomposition in Social Media Contexts](https://arxiv.org/abs/2411.11479)
Append: [SHuBERT: Self-Supervised Sign Language Representation Learning via Multi-Stream Cluster Prediction](https://arxiv.org/abs/2411.16765)
Append: [Is it the end of (generative) linguistics as we know it?](https://arxiv.org/abs/2412.12797)
Append: [Can Input Attributions Explain Inductive Reasoning in In-Context Learning?](https://arxiv.org/abs/2412.15628)
Append: [Computational Analysis of Character Development in Holocaust Testimonies](https://arxiv.org/abs/2412.17063)
Append: [Diving into Self-Evolving Training for Multimodal Reasoning](https://arxiv.org/abs/2412.17451)
Append: [Behind Closed Words: Creating and Investigating the forePLay Annotated Dataset for Polish Erotic Discourse](https://arxiv.org/abs/2412.17533)
Append: [Instruction-Following Pruning for Large Language Models](https://arxiv.org/abs/2501.02086)
Append: [FocalPO: Enhancing Preference Optimizing by Focusing on Correct Preference Rankings](https://arxiv.org/abs/2501.06645)
Append: [Large Language Models to Diffusion Finetuning](https://arxiv.org/abs/2501.15781)
Append: [UGPhysics: A Comprehensive Benchmark for Undergraduate Physics Reasoning with Large Language Models](https://arxiv.org/abs/2502.00334)
Append: [Inference-time sparse attention with asymmetric indexing](https://arxiv.org/abs/2502.08246)
Append: [Rethinking Evaluation Metrics for Grammatical Error Correction: Why Use a Different Evaluation Process than Human?](https://arxiv.org/abs/2502.09416)
Append: [Akan Cinematic Emotions (ACE): A Multimodal Multi-party Dataset for Emotion Recognition in Movie Dialogues](https://arxiv.org/abs/2502.10973)
Append: [Exposing Numeracy Gaps: A Benchmark to Evaluate Fundamental Numerical Abilities in Large Language Models](https://arxiv.org/abs/2502.11075)
Append: [Can't See the Forest for the Trees: Benchmarking Multimodal Safety Awareness for Multimodal LLMs](https://arxiv.org/abs/2502.11184)
Append: [SEA: Low-Resource Safety Alignment for Multimodal Large Language Models via Synthetic Embeddings](https://arxiv.org/abs/2502.12562)
Append: [A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary Position Embedding and Query-Aware Vector Quantization](https://arxiv.org/abs/2502.12665)
Append: [Q-STRUM Debate: Query-Driven Contrastive Summarization for Recommendation Comparison](https://arxiv.org/abs/2502.12921)
Append: [A Similarity Paradigm Through Textual Regularization Without Forgetting](https://arxiv.org/abs/2502.14376)
Append: [Social Genome: Grounded Social Reasoning Abilities of Multimodal Models](https://arxiv.org/abs/2502.15109)
Append: [Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient Operational Knowledge Injection in Mobile Automation](https://arxiv.org/abs/2502.17110)
Append: [CoT-UQ: Improving Response-wise Uncertainty Quantification in LLMs with Chain-of-Thought](https://arxiv.org/abs/2502.17214)
Append: [Towards Enhanced Immersion and Agency for LLM-based Interactive Drama](https://arxiv.org/abs/2502.17878)
Append: [DRAMA: Diverse Augmentation from Large Language Models to Smaller Dense Retrievers](https://arxiv.org/abs/2502.18460)
Append: [PolyPrompt: Automating Knowledge Extraction from Multilingual Language Models with Dynamic Prompt Generation](https://arxiv.org/abs/2502.19756)
Append: [Finite State Automata Inside Transformers with Chain-of-Thought: A Mechanistic Study on State Tracking](https://arxiv.org/abs/2502.20129)
Append: [Unnatural Languages Are Not Bugs but Features for LLMs](https://arxiv.org/abs/2503.01926)
Append: [Generator-Assistant Stepwise Rollback Framework for Large Language Model Agent](https://arxiv.org/abs/2503.02519)
Append: [Collapse of Dense Retrievers: Short, Early, and Literal Biases Outranking Factual Evidence](https://arxiv.org/abs/2503.05037)
Append: [OASST-ETC Dataset: Alignment Signals from Eye-tracking Analysis of LLM Responses](https://arxiv.org/abs/2503.10927)
Append: [The time scale of redundancy between prosody and linguistic context](https://arxiv.org/abs/2503.11630)
Append: [Splintering Nonconcatenative Languages for Better Tokenization](https://arxiv.org/abs/2503.14433)
Append: [Meta-Learning Neural Mechanisms rather than Bayesian Priors](https://arxiv.org/abs/2503.16048)
Append: [Leveraging Human Production-Interpretation Asymmetries to Test LLM Cognitive Plausibility](https://arxiv.org/abs/2503.17579)
Append: [Negation: A Pink Elephant in the Large Language Models' Room?](https://arxiv.org/abs/2503.22395)
Append: [Efficient Annotator Reliability Assessment with EffiARA](https://arxiv.org/abs/2504.00589)
Append: [Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language Models](https://arxiv.org/abs/2504.05050)
Append: [A Fully Automated Pipeline for Conversational Discourse Annotation: Tree Scheme Generation and Labeling with Large Language Models](https://arxiv.org/abs/2504.08961)
Append: [d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning](https://arxiv.org/abs/2504.12216)
Append: [Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs](https://arxiv.org/abs/2505.02862)
Append: [Recall with Reasoning: Chain-of-Thought Distillation for Mamba's Long-Context Memory and Extrapolation](https://arxiv.org/abs/2505.03320)
Append: [A Scaling Law for Token Efficiency in LLM Fine-Tuning Under Fixed Compute Budgets](https://arxiv.org/abs/2505.06150)
Append: [KRISTEVA: Close Reading as a Novel Task for Benchmarking Interpretive Reasoning](https://arxiv.org/abs/2505.09825)
Append: [Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data Condensation and Spoken QA Generation](https://arxiv.org/abs/2505.13338)
Append: [Time-R1: Towards Comprehensive Temporal Reasoning in LLMs](https://arxiv.org/abs/2505.13508)
Append: [Multi-Hop Question Generation via Dual-Perspective Keyword Guidance](https://arxiv.org/abs/2505.15299)
Append: [Ranking Free RAG: Replacing Re-ranking with Selection in RAG for Sensitive Domains](https://arxiv.org/abs/2505.16014)
Append: [Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains](https://arxiv.org/abs/2505.16552)
Append: [Can reasoning models comprehend mathematical problems in Chinese ancient texts? An empirical study based on data from Suanjing Shishu](https://arxiv.org/abs/2505.16660)
Append: [LongMagpie: A Self-synthesis Method for Generating Large-scale Long-context Instructions](https://arxiv.org/abs/2505.17134)
Append: [On the class of coding optimality of human languages and the origins of Zipf's law](https://arxiv.org/abs/2505.20015)
Append: [One-shot Entropy Minimization](https://arxiv.org/abs/2505.20282)
Append: [Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering Target Atoms](https://arxiv.org/abs/2505.20322)
Append: [VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning](https://arxiv.org/abs/2505.22019)
Append: [Multimodal Forecasting of Sparse Intraoperative Hypotension Events Powered by Language Model](https://arxiv.org/abs/2505.22116)
Append: [Threading the Needle: Reweaving Chain-of-Thought Reasoning to Explain Human Label Variation](https://arxiv.org/abs/2505.23368)
Append: [DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural Language and Reinforcement Learning](https://arxiv.org/abs/2505.23754)
Append: [Conti Inc.: Understanding the Internal Discussions of a large Ransomware-as-a-Service Operator with Machine Learning](https://arxiv.org/abs/2308.16061)
Append: [MCU: An Evaluation Framework for Open-Ended Game Agents](https://arxiv.org/abs/2310.08367)
Append: [GPTVQ: The Blessing of Dimensionality for LLM Quantization](https://arxiv.org/abs/2402.15319)
Append: [AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs](https://arxiv.org/abs/2404.16873)
Append: [A Hitchhiker's Guide to Scaling Law Estimation](https://arxiv.org/abs/2410.11840)
Append: [Evaluating and Advancing Multimodal Large Language Models in Perception Ability Lens](https://arxiv.org/abs/2411.14725)
Append: [Superhuman performance of a large language model on the reasoning tasks of a physician](https://arxiv.org/abs/2412.10849)
Append: [SATA: A Paradigm for LLM Jailbreak via Simple Assistive Task Linkage](https://arxiv.org/abs/2412.15289)
Append: [Tracking the Feature Dynamics in LLM Training: A Mechanistic Study](https://arxiv.org/abs/2412.17626)
Append: [Ola: Pushing the Frontiers of Omni-Modal Language Model](https://arxiv.org/abs/2502.04328)
Append: [Logits are All We Need to Adapt Closed Models](https://arxiv.org/abs/2502.06806)
Append: [Symmetrical Visual Contrastive Optimization: Aligning Vision-Language Models with Minimal Contrastive Images](https://arxiv.org/abs/2502.13928)
Append: [Self-Improvement Towards Pareto Optimality: Mitigating Preference Conflicts in Multi-Objective Alignment](https://arxiv.org/abs/2502.14354)
Append: [A Mousetrap: Fooling Large Reasoning Models for Jailbreak with Chain of Iterative Chaos](https://arxiv.org/abs/2502.15806)
Append: [Grounded Persuasive Language Generation for Automated Marketing](https://arxiv.org/abs/2502.16810)
Append: [ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic Iterative Reasoning Agents](https://arxiv.org/abs/2502.18017)
Append: [We Should Chart an Atlas of All the World's Models](https://arxiv.org/abs/2503.10633)
Append: [Unique Hard Attention: A Tale of Two Sides](https://arxiv.org/abs/2503.14615)
Append: [Enhancing Ultra-Low-Bit Quantization of Large Language Models Through Saliency-Aware Partial Retraining](https://arxiv.org/abs/2504.13932)
Append: [Enhancing Target-unspecific Tasks through a Features Matrix](https://arxiv.org/abs/2505.03414)
Append: [X-Driver: Explainable Autonomous Driving with Vision-Language Models](https://arxiv.org/abs/2505.05098)
Append: [Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient Operational Knowledge Injection in Mobile Automation](https://arxiv.org/abs/2505.13887)
Append: [The Polar Express: Optimal Matrix Sign Methods and Their Application to the Muon Algorithm](https://arxiv.org/abs/2505.16932)
Append: [GETReason: Enhancing Image Context Extraction through Hierarchical Multi-Agent Reasoning](https://arxiv.org/abs/2505.21863)
append_entries: 253
Finish: 2025-06-04 04:27:52.014105
------------------------------------------------------
Started: 2025-06-04 06:25:03.273619
Existing_entries: 1253
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1230
Summarized using GPT-3.5-turbo
Append: [What Has Been Lost with Synthetic Evaluation?](https://arxiv.org/abs/2505.22830)
Token length: 1476
Summarized using GPT-3.5-turbo
Append: [LiTEx: A Linguistic Taxonomy of Explanations for Understanding Within-Label Variation in Natural Language Inference](https://arxiv.org/abs/2505.22848)
Token length: 1405
Summarized using GPT-3.5-turbo
Append: [DyePack: Provably Flagging Test Set Contamination in LLMs Using Backdoors](https://arxiv.org/abs/2505.23001)
Token length: 1229
Summarized using GPT-3.5-turbo
Append: [Dataset Cartography for Large Language Model Alignment: Mapping and Diagnosing Preference Data](https://arxiv.org/abs/2505.23114)
Token length: 943
Summarized using GPT-3.5-turbo
Append: [Improving Multilingual Speech Models on ML-SUPERB 2.0: Fine-tuning with Data Augmentation and LID-Aware CTC](https://arxiv.org/abs/2505.24200)
append_entries: 5
Finish: 2025-06-04 06:25:16.311166
------------------------------------------------------
Started: 2025-06-04 08:23:18.428279
Existing_entries: 1005
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-04 08:23:19.043790
------------------------------------------------------
Started: 2025-06-04 10:18:38.145383
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-04 10:18:38.801781
------------------------------------------------------
Started: 2025-06-04 12:35:05.278879
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-04 12:35:05.888903
------------------------------------------------------
Started: 2025-06-04 14:14:10.602354
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-04 14:14:11.186674
------------------------------------------------------
Started: 2025-06-04 16:18:54.555966
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-04 16:18:55.218839
------------------------------------------------------
Started: 2025-06-04 18:23:33.708333
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-04 18:23:34.303250
------------------------------------------------------
Started: 2025-06-04 20:15:07.611205
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-04 20:15:08.252487
------------------------------------------------------
Started: 2025-06-04 22:14:55.438714
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-04 22:14:56.065797
------------------------------------------------------
Started: 2025-06-05 01:19:43.504710
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-05 01:19:44.099638
------------------------------------------------------
Started: 2025-06-05 03:16:07.467069
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-05 03:16:08.067841
------------------------------------------------------
Started: 2025-06-05 04:29:04.223573
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1565
Summarized using GPT-3.5-turbo
Append: [Evaluating Large Language Models for Zero-Shot Disease Labeling in CT Radiology Reports Across Organ Systems](https://arxiv.org/abs/2506.03259)
Token length: 154
Summarized using GPT-3.5-turbo
Append: [A conclusive remark on linguistic theorizing and language modeling](https://arxiv.org/abs/2506.03268)
Token length: 1857
Summarized using GPT-3.5-turbo
Append: [FailureSensorIQ: A Multi-Choice QA Dataset for Understanding Sensor Relationships and Failure Modes](https://arxiv.org/abs/2506.03278)
Token length: 1102
Summarized using GPT-3.5-turbo
Append: [HyperSteer: Activation Steering at Scale with Hypernetworks](https://arxiv.org/abs/2506.03292)
Token length: 1581
Summarized using GPT-3.5-turbo
Append: [Unleashing the Reasoning Potential of Pre-trained LLMs by Critique Fine-Tuning on One Problem](https://arxiv.org/abs/2506.03295)
Token length: 954
Summarized using GPT-3.5-turbo
Append: [From Instructions to ODRL Usage Policies: An Ontology Guided Approach](https://arxiv.org/abs/2506.03301)
Token length: 951
Summarized using GPT-3.5-turbo
Append: [Hopscotch: Discovering and Skipping Redundancies in Language Models](https://arxiv.org/abs/2506.03303)
Token length: 1365
Summarized using GPT-3.5-turbo
Append: [The Reader is the Metric: How Textual Features and Reader Profiles Explain Conflicting Evaluations of AI Creative Writing](https://arxiv.org/abs/2506.03310)
Token length: 922
Summarized using GPT-3.5-turbo
Append: [Cross-Platform Violence Detection on Social Media: A Dataset and Analysis](https://arxiv.org/abs/2506.03312)
Token length: 1443
Summarized using GPT-3.5-turbo
Append: [Ask a Local: Detecting Hallucinations With Specialized Model Divergence](https://arxiv.org/abs/2506.03357)
Token length: 1169
Summarized using GPT-3.5-turbo
Append: [A Multimodal, Multilingual, and Multidimensional Pipeline for Fine-grained Crowdsourcing Earthquake Damage Evaluation](https://arxiv.org/abs/2506.03360)
Token length: 1049
Summarized using GPT-3.5-turbo
Append: [Trajectory Prediction Meets Large Language Models: A Survey](https://arxiv.org/abs/2506.03408)
Token length: 945
Summarized using GPT-3.5-turbo
Append: [DistRAG: Towards Distance-Based Spatial Reasoning in LLMs](https://arxiv.org/abs/2506.03424)
Token length: 1209
Summarized using GPT-3.5-turbo
Append: [Time Course MechInterp: Analyzing the Evolution of Components and Knowledge in Large Language Models](https://arxiv.org/abs/2506.03434)
Token length: 884
Summarized using GPT-3.5-turbo
Append: [Culture Matters in Toxic Language Detection in Persian](https://arxiv.org/abs/2506.03458)
Token length: 1221
Summarized using GPT-3.5-turbo
Append: [Delta-KNN: Improving Demonstration Selection in In-Context Learning for Alzheimer's Disease Detection](https://arxiv.org/abs/2506.03476)
Token length: 1207
Summarized using GPT-3.5-turbo
Append: [APT: Improving Specialist LLM Performance with Weakness Case Acquisition and Iterative Preference Training](https://arxiv.org/abs/2506.03483)
Token length: 1700
Summarized using GPT-3.5-turbo
Append: [Explainable AI: XAI-Guided Context-Aware Data Augmentation](https://arxiv.org/abs/2506.03484)
Token length: 1049
Summarized using GPT-3.5-turbo
Append: [EpiCoDe: Boosting Model Performance Beyond Training with Extrapolation and Contrastive Decoding](https://arxiv.org/abs/2506.03489)
Token length: 1622
Summarized using GPT-3.5-turbo
Append: [Beyond Memorization: A Rigorous Evaluation Framework for Medical Knowledge Editing](https://arxiv.org/abs/2506.03490)
Token length: 1672
Summarized using GPT-3.5-turbo
Append: [Measuring Human Involvement in AI-Generated Text: A Case Study on Academic Writing](https://arxiv.org/abs/2506.03501)
Token length: 1171
Summarized using GPT-3.5-turbo
Append: [Accurate Sublayer Pruning for Large Language Models by Exploiting Latency and Tunability Information](https://arxiv.org/abs/2506.03510)
Token length: 1330
Summarized using GPT-3.5-turbo
Append: [An Efficient Task-Oriented Dialogue Policy: Evolutionary Reinforcement Learning Injected by Elite Individuals](https://arxiv.org/abs/2506.03519)
Token length: 1384
Summarized using GPT-3.5-turbo
Append: [TokAlign: Efficient Vocabulary Adaptation via Token Alignment](https://arxiv.org/abs/2506.03523)
Token length: 1443
Summarized using GPT-3.5-turbo
Append: [Seed-Coder: Let the Code Model Curate Data for Itself](https://arxiv.org/abs/2506.03524)
Token length: 995
Summarized using GPT-3.5-turbo
Append: [Go-Browse: Training Web Agents with Structured Exploration](https://arxiv.org/abs/2506.03533)
Token length: 1184
Summarized using GPT-3.5-turbo
Append: [Debate, Reflect, and Distill: Multi-Agent Feedback with Tree-Structured Preference Optimization for Efficient Language Model Enhancement](https://arxiv.org/abs/2506.03541)
Token length: 1464
Summarized using GPT-3.5-turbo
Append: [BPO: Revisiting Preference Modeling in Direct Preference Optimization](https://arxiv.org/abs/2506.03557)
Token length: 1397
Summarized using GPT-3.5-turbo
Append: [ConsistentChat: Building Skeleton-Guided Consistent Dialogues for Large Language Models from Scratch](https://arxiv.org/abs/2506.03558)
Token length: 1119
Summarized using GPT-3.5-turbo
Append: [POSS: Position Specialist Generates Better Draft for Speculative Decoding](https://arxiv.org/abs/2506.03566)
Token length: 1118
Summarized using GPT-3.5-turbo
Append: [MiMo-VL Technical Report](https://arxiv.org/abs/2506.03569)
Token length: 1196
Summarized using GPT-3.5-turbo
Append: [FreePRM: Training Process Reward Models Without Ground Truth Process Labels](https://arxiv.org/abs/2506.03570)
Token length: 974
Summarized using GPT-3.5-turbo
Append: [Exchange of Perspective Prompting Enhances Reasoning in Large Language Models](https://arxiv.org/abs/2506.03573)
Token length: 1380
Summarized using GPT-3.5-turbo
Append: [KG-BiLM: Knowledge Graph Embedding via Bidirectional Language Models](https://arxiv.org/abs/2506.03576)
Token length: 1240
Summarized using GPT-3.5-turbo
Append: [Automatically Suggesting Diverse Example Sentences for L2 Japanese Learners Using Pre-Trained Language Models](https://arxiv.org/abs/2506.03580)
Token length: 1085
Summarized using GPT-3.5-turbo
Append: [From Understanding to Generation: An Efficient Shortcut for Evaluating Language Models](https://arxiv.org/abs/2506.03592)
Token length: 1618
Summarized using GPT-3.5-turbo
Append: [Is linguistically-motivated data augmentation worth it?](https://arxiv.org/abs/2506.03593)
Token length: 1010
Summarized using GPT-3.5-turbo
Append: [Auto prompt sql: a resource-efficient architecture for text-to-sql translation in constrained environments](https://arxiv.org/abs/2506.03598)
Token length: 1228
Summarized using GPT-3.5-turbo
Append: [Learning to Insert [PAUSE] Tokens for Better Reasoning](https://arxiv.org/abs/2506.03616)
Token length: 1507
Summarized using GPT-3.5-turbo
Append: [Do Large Language Models Know Folktales? A Case Study of Yokai in Japanese Folktales](https://arxiv.org/abs/2506.03619)
Token length: 1499
Summarized using GPT-3.5-turbo
Append: [Robustness of Prompting: Enhancing Robustness of Large Language Models Against Prompting Attacks](https://arxiv.org/abs/2506.03627)
Token length: 1515
Summarized using GPT-3.5-turbo
Append: [RewardAnything: Generalizable Principle-Following Reward Models](https://arxiv.org/abs/2506.03637)
Token length: 1385
Summarized using GPT-3.5-turbo
Append: [Trustworthy Medical Question Answering: An Evaluation-Centric Survey](https://arxiv.org/abs/2506.03659)
Token length: 795
Summarized using GPT-3.5-turbo
Append: [ROSA: Addressing text understanding challenges in photographs via ROtated SAmpling](https://arxiv.org/abs/2506.03665)
Token length: 1004
Summarized using GPT-3.5-turbo
Append: [Efficient Data Selection for Domain Adaptation of ASR Using Pseudo-Labels and Multi-Stage Filtering](https://arxiv.org/abs/2506.03681)
Token length: 1407
Summarized using GPT-3.5-turbo
Append: [Robust Preference Optimization via Dynamic Target Margins](https://arxiv.org/abs/2506.03690)
Token length: 1952
Summarized using GPT-3.5-turbo
Append: [AdaDecode: Accelerating LLM Decoding with Adaptive Layer Parallelism](https://arxiv.org/abs/2506.03700)
Token length: 1366
Summarized using GPT-3.5-turbo
Append: [ScoreRAG: A Retrieval-Augmented Generation Framework with Consistency-Relevance Scoring and Structured Summarization for News Generation](https://arxiv.org/abs/2506.03704)
Token length: 1023
Summarized using GPT-3.5-turbo
Append: [MFLA: Monotonic Finite Look-ahead Attention for Streaming Speech Recognition](https://arxiv.org/abs/2506.03722)
Token length: 1251
Summarized using GPT-3.5-turbo
Append: [Verbalized Confidence Triggers Self-Verification: Emergent Behavior Without Explicit Reasoning Supervision](https://arxiv.org/abs/2506.03723)
Append: [Generating Pedagogically Meaningful Visuals for Math Word Problems: A New Benchmark and Analysis of Text-to-Image Models](https://arxiv.org/abs/2506.03735)
Append: [Act-as-Pet: Benchmarking the Abilities of Large Language Models as E-Pets in Social Network Services](https://arxiv.org/abs/2506.03761)
Append: [AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models](https://arxiv.org/abs/2506.03762)
Append: [ClozeMath: Improving Mathematical Reasoning in Language Models by Learning to Fill Equations](https://arxiv.org/abs/2506.03763)
Append: [Unifying Uniform and Binary-coding Quantization for Accurate Compression of Large Language Models](https://arxiv.org/abs/2506.03781)
Append: [Knockout LLM Assessment: Using Large Language Models for Evaluations through Iterative Pairwise Comparisons](https://arxiv.org/abs/2506.03785)
Append: [Mark My Words: A Robust Multilingual Model for Punctuation in Text and Speech Transcripts](https://arxiv.org/abs/2506.03793)
Append: [Automatic Correction of Writing Anomalies in Hausa Texts](https://arxiv.org/abs/2506.03820)
Append: [CRAWLDoc: A Dataset for Robust Ranking of Bibliographic Documents](https://arxiv.org/abs/2506.03822)
Append: [Multi-objective Aligned Bidword Generation Model for E-commerce Search Advertising](https://arxiv.org/abs/2506.03827)
Append: [Brain-tuned Speech Models Better Reflect Speech Processing Stages in the Brain](https://arxiv.org/abs/2506.03832)
Append: [PulseReddit: A Novel Reddit Dataset for Benchmarking MAS in High-Frequency Cryptocurrency Trading](https://arxiv.org/abs/2506.03861)
Append: [EuroGEST: Investigating gender stereotypes in multilingual language models](https://arxiv.org/abs/2506.03867)
Append: [RadialRouter: Structured Representation for Efficient and Robust Large Language Models Routing](https://arxiv.org/abs/2506.03880)
Append: [Kinship in Speech: Leveraging Linguistic Relatedness for Zero-Shot TTS in Indian Languages](https://arxiv.org/abs/2506.03884)
Append: [Pre$^3$: Enabling Deterministic Pushdown Automata for Faster Structured LLM Generation](https://arxiv.org/abs/2506.03887)
Append: [Magic Mushroom: A Customizable Benchmark for Fine-grained Analysis of Retrieval Noise Erosion in RAG Systems](https://arxiv.org/abs/2506.03901)
Append: [The Harmonic Structure of Information Contours](https://arxiv.org/abs/2506.03902)
Append: [When Fairness Isn't Statistical: The Limits of Machine Learning in Evaluating Legal Reasoning](https://arxiv.org/abs/2506.03913)
Append: [Compositional Generalisation for Explainable Hate Speech Detection](https://arxiv.org/abs/2506.03916)
Append: [HSSBench: Benchmarking Humanities and Social Sciences Ability for Multimodal Large Language Models](https://arxiv.org/abs/2506.03922)
Append: [More or Less Wrong: A Benchmark for Directional Bias in LLM Comparative Reasoning](https://arxiv.org/abs/2506.03923)
Append: [Hanging in the Balance: Pivotal Moments in Crisis Counseling Conversations](https://arxiv.org/abs/2506.03941)
Append: [TableEval: A Real-World Benchmark for Complex, Multilingual, and Multi-Structured Table Question Answering](https://arxiv.org/abs/2506.03949)
Append: [From Real to Synthetic: Synthesizing Millions of Diversified and Complicated User Instructions with Attributed Grounding](https://arxiv.org/abs/2506.03968)
Append: [Structured Pruning for Diverse Best-of-N Reasoning Optimization](https://arxiv.org/abs/2506.03978)
Append: [Voice Activity Projection Model with Multimodal Encoders](https://arxiv.org/abs/2506.03980)
Append: [Around the World in 24 Hours: Probing LLM Knowledge of Time and Place](https://arxiv.org/abs/2506.03984)
Append: [Stronger Baselines for Retrieval-Augmented Generation with Long-Context Language Models](https://arxiv.org/abs/2506.03989)
Append: [DynTok: Dynamic Compression of Visual Tokens for Efficient and Effective Video Understanding](https://arxiv.org/abs/2506.03990)
Append: [Words of Warmth: Trust and Sociability Norms for over 26k English Words](https://arxiv.org/abs/2506.03993)
Append: [Seeing What Tastes Good: Revisiting Multimodal Distributional Semantics in the Billion Parameter Era](https://arxiv.org/abs/2506.03994)
Append: [QQSUM: A Novel Task and Model of Quantitative Query-Focused Summarization for Review-based Product Question Answering](https://arxiv.org/abs/2506.04020)
Append: [AI Agents for Conversational Patient Triage: Preliminary Simulation-Based Evaluation with Real-World EHR Data](https://arxiv.org/abs/2506.04032)
Append: [The mutual exclusivity bias of bilingual visually grounded speech models](https://arxiv.org/abs/2506.04037)
Append: [LexTime: A Benchmark for Temporal Ordering of Legal Events](https://arxiv.org/abs/2506.04041)
Append: [Unveiling and Eliminating the Shortcut Learning for Locate-Then-Edit Knowledge Editing via Both Subject and Relation Awareness](https://arxiv.org/abs/2506.04042)
Append: [Think Like a Person Before Responding: A Multi-Faceted Evaluation of Persona-Guided LLMs for Countering Hate](https://arxiv.org/abs/2506.04043)
Append: [Lacuna Inc. at SemEval-2025 Task 4: LoRA-Enhanced Influence-Based Unlearning for LLMs](https://arxiv.org/abs/2506.04044)
Append: [On Support Samples of Next Word Prediction](https://arxiv.org/abs/2506.04047)
Append: [Explainability-Based Token Replacement on LLM-Generated Text](https://arxiv.org/abs/2506.04050)
Append: [High Accuracy, Less Talk (HALT): Reliable LLMs through Capability-Aligned Finetuning](https://arxiv.org/abs/2506.04051)
Append: [Progressive Mastery: Customized Curriculum Learning with Guided Prompting for Mathematical Reasoning](https://arxiv.org/abs/2506.04065)
Append: [LaF-GRPO: In-Situ Navigation Instruction Generation for the Visually Impaired via GRPO with LLM-as-Follower Reward](https://arxiv.org/abs/2506.04070)
Append: [Controlling Difficulty of Generated Text for AI-Assisted Language Learning](https://arxiv.org/abs/2506.04072)
Append: [Acoustically Precise Hesitation Tagging Is Essential for End-to-End Verbatim Transcription Systems](https://arxiv.org/abs/2506.04076)
Append: [A Novel Data Augmentation Approach for Automatic Speaking Assessment on Opinion Expressions](https://arxiv.org/abs/2506.04077)
Append: [LLMEval-Med: A Real-world Clinical Benchmark for Medical LLMs with Physician Validation](https://arxiv.org/abs/2506.04078)
Append: [EuroLLM-9B: Technical Report](https://arxiv.org/abs/2506.04079)
Append: [TextAtari: 100K Frames Game Playing with Language Agents](https://arxiv.org/abs/2506.04098)
Append: [Rectified Sparse Attention](https://arxiv.org/abs/2506.04108)
Append: [CLAIM: An Intent-Driven Multi-Agent Framework for Analyzing Manipulation in Courtroom Dialogues](https://arxiv.org/abs/2506.04131)
Append: [Are Lexicon-Based Tools Still the Gold Standard for Valence Analysis in Low-Resource Flemish?](https://arxiv.org/abs/2506.04139)
Append: [Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis](https://arxiv.org/abs/2506.04142)
Append: [A Dataset for Addressing Patient's Information Needs related to Clinical Course of Hospitalization](https://arxiv.org/abs/2506.04156)
Append: [SkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and Module Decoupling](https://arxiv.org/abs/2506.04179)
Append: [SuperWriter: Reflection-Driven Long-Form Generation with Large Language Models](https://arxiv.org/abs/2506.04180)
Append: [Long or short CoT? Investigating Instance-level Switch of Large Reasoning Models](https://arxiv.org/abs/2506.04182)
Append: [R-Search: Empowering LLM Reasoning with Search via Multi-Reward Reinforcement Learning](https://arxiv.org/abs/2506.04185)
Append: [Efficient Knowledge Editing via Minimal Precomputation](https://arxiv.org/abs/2506.04226)
Append: [mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented Generation](https://arxiv.org/abs/2505.24073)
Append: [Infinity Parser: Layout Aware Reinforcement Learning for Scanned Document Parsing](https://arxiv.org/abs/2506.03197)
Append: [DiaBlo: Diagonal Blocks Are Sufficient For Finetuning](https://arxiv.org/abs/2506.03230)
Append: [Comparison of different Unique hard attention transformer models by the formal languages they can recognize](https://arxiv.org/abs/2506.03370)
Append: [Adaptive Task Vectors for Large Language Models](https://arxiv.org/abs/2506.03426)
Append: [Exploiting LLMs for Automatic Hypothesis Assessment via a Logit-Based Calibrated Prior](https://arxiv.org/abs/2506.03444)
Append: [ProRank: Prompt Warmup via Reinforcement Learning for Small Language Models Reranking](https://arxiv.org/abs/2506.03487)
Append: [Video-Skill-CoT: Skill-based Chain-of-Thoughts for Domain-Adaptive Video Reasoning](https://arxiv.org/abs/2506.03525)
Append: [Preface to the Special Issue of the TAL Journal on Scholarly Document Processing](https://arxiv.org/abs/2506.03587)
Append: [BiMa: Towards Biases Mitigation for Text-Video Retrieval via Scene Element Guidance](https://arxiv.org/abs/2506.03589)
Append: [Tone recognition in low-resource languages of North-East India: peeling the layers of SSL-based speech models](https://arxiv.org/abs/2506.03606)
Append: [VLMs Can Aggregate Scattered Training Patches](https://arxiv.org/abs/2506.03614)
Append: [PromptCanvas: Composable Prompting Workspaces Using Dynamic Widgets for Exploration and Iteration in Creative Writing](https://arxiv.org/abs/2506.03741)
Append: [Prompt Candidates, then Distill: A Teacher-Student Framework for LLM-driven Data Annotation](https://arxiv.org/abs/2506.03857)
Append: [VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code Generation](https://arxiv.org/abs/2506.03930)
Append: [Graph Counselor: Adaptive Graph Exploration via Multi-Agent Synergy to Enhance LLM Reasoning](https://arxiv.org/abs/2506.03939)
Append: [AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in LLM-Based Agents](https://arxiv.org/abs/2506.04018)
Append: [CETBench: A Novel Dataset constructed via Transformations over Programs for Benchmarking LLMs for Code-Equivalence Checking](https://arxiv.org/abs/2506.04019)
Append: [Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization](https://arxiv.org/abs/2506.04039)
Append: [Multimodal Tabular Reasoning with Privileged Structured Information](https://arxiv.org/abs/2506.04088)
Append: [AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment](https://arxiv.org/abs/2506.04089)
Append: [MMR-V: What's Left Unsaid? A Benchmark for Multimodal Deep Reasoning in Videos](https://arxiv.org/abs/2506.04141)
Append: [Advancing Multimodal Reasoning: From Optimized Cold Start to Staged Reinforcement Learning](https://arxiv.org/abs/2506.04207)
Append: [Does Thinking More always Help? Understanding Test-Time Scaling in Reasoning Models](https://arxiv.org/abs/2506.04210)
Append: [Transformers in Speech Processing: A Survey](https://arxiv.org/abs/2303.11607)
Append: [WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct](https://arxiv.org/abs/2308.09583)
Append: [Concept-Guided Chain-of-Thought Prompting for Pairwise Comparison Scoring of Texts with Large Language Models](https://arxiv.org/abs/2310.12049)
Append: [CheckEmbed: Effective Verification of LLM Solutions to Open-Ended Tasks](https://arxiv.org/abs/2406.02524)
Append: [AlignMMBench: Evaluating Chinese Multimodal Alignment in Large Vision-Language Models](https://arxiv.org/abs/2406.09295)
Append: [UBench: Benchmarking Uncertainty in Large Language Models with Multiple Choice Questions](https://arxiv.org/abs/2406.12784)
Append: [UIO-LLMs: Unbiased Incremental Optimization for Long-Context LLMs](https://arxiv.org/abs/2406.18173)
Append: [REAL: Response Embedding-based Alignment for LLMs](https://arxiv.org/abs/2409.17169)
Append: [Geometric Signatures of Compositionality Across a Language Model's Lifetime](https://arxiv.org/abs/2410.01444)
Append: [Nudging: Inference-time Alignment of LLMs via Guided Decoding](https://arxiv.org/abs/2410.09300)
Append: [RULEBREAKERS: Challenging LLMs at the Crossroads between Formal Logic and Human-like Reasoning](https://arxiv.org/abs/2410.16502)
Append: [DynaSaur: Large Language Agents Beyond Predefined Actions](https://arxiv.org/abs/2411.01747)
Append: [Enabling LLM Knowledge Analysis via Extensive Materialization](https://arxiv.org/abs/2411.04920)
Append: [Improving Radiology Report Conciseness and Structure via Local Large Language Models](https://arxiv.org/abs/2411.05042)
Append: [Beyond the Safety Bundle: Auditing the Helpful and Harmless Dataset](https://arxiv.org/abs/2411.08243)
Append: [A Survey of Event Causality Identification: Taxonomy, Challenges, Assessment, and Prospects](https://arxiv.org/abs/2411.10371)
Append: [MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at Scale](https://arxiv.org/abs/2412.05237)
Append: [Data Laundering: Artificially Boosting Benchmark Results through Knowledge Distillation](https://arxiv.org/abs/2412.15255)
Append: [Verbosity-Aware Rationale Reduction: Effective Reduction of Redundant Rationale via Principled Criteria](https://arxiv.org/abs/2412.21006)
Append: [ConSim: Measuring Concept-Based Explanations' Effectiveness with Automated Simulatability](https://arxiv.org/abs/2501.05855)
Append: [Through the Prism of Culture: Evaluating LLMs' Understanding of Indian Subcultures and Traditions](https://arxiv.org/abs/2501.16748)
Append: [ReFoRCE: A Text-to-SQL Agent with Self-Refinement, Consensus Enforcement, and Column Exploration](https://arxiv.org/abs/2502.00675)
Append: [Refining Sentence Embedding Model through Ranking Sentences Generation with Large Language Models](https://arxiv.org/abs/2502.13656)
Append: [Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety Mechanisms Tend to Be Anchored in The Template Region](https://arxiv.org/abs/2502.13946)
Append: [Dehumanizing Machines: Mitigating Anthropomorphic Behaviors in Text Generation Systems](https://arxiv.org/abs/2502.14019)
Append: [Large Language Models Struggle to Describe the Haystack without Human Help: Human-in-the-loop Evaluation of Topic Models](https://arxiv.org/abs/2502.14748)
Append: [D2S-FLOW: Automated Parameter Extraction from Datasheets for SPICE Model Generation Using Large Language Models](https://arxiv.org/abs/2502.16540)
Append: [Sliding Window Attention Training for Efficient Large Language Models](https://arxiv.org/abs/2502.18845)
Append: [Where Are We? Evaluating LLM Performance on African Languages](https://arxiv.org/abs/2502.19582)
Append: [DOVE: A Large-Scale Multi-Dimensional Predictions Dataset Towards Meaningful LLM Evaluation](https://arxiv.org/abs/2503.01622)
Append: [On the Acquisition of Shared Grammatical Representations in Bilingual Language Models](https://arxiv.org/abs/2503.03962)
Append: [Full-Duplex-Bench: A Benchmark to Evaluate Full-duplex Spoken Dialogue Models on Turn-taking Capabilities](https://arxiv.org/abs/2503.04721)
Append: [PFDial: A Structured Dialogue Instruction Fine-tuning Method Based on UML Flowcharts](https://arxiv.org/abs/2503.06706)
Append: [LSC-Eval: A General Framework to Evaluate Methods for Assessing Dimensions of Lexical Semantic Change Using LLM-Generated Synthetic Data](https://arxiv.org/abs/2503.08042)
Append: [An Expanded Massive Multilingual Dataset for High-Performance Language Technologies (HPLT)](https://arxiv.org/abs/2503.10267)
Append: [Probing LLMs for Multilingual Discourse Generalization Through a Unified Label Set](https://arxiv.org/abs/2503.10515)
Append: [SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity Representation](https://arxiv.org/abs/2503.15358)
Append: [Uncertainty Quantification and Confidence Calibration in Large Language Models: A Survey](https://arxiv.org/abs/2503.15850)
Append: [SCORE: Story Coherence and Retrieval Enhancement for AI Narratives](https://arxiv.org/abs/2503.23512)
Append: [Rubrik's Cube: Testing a New Rubric for Evaluating Explanations on the CUBE dataset](https://arxiv.org/abs/2503.23899)
Append: [Token-Driven GammaTune: Adaptive Calibration for Enhanced Speculative Decoding](https://arxiv.org/abs/2504.00030)
Append: [CARE: Assessing the Impact of Multilingual Human Preference Learning on Cultural Awareness](https://arxiv.org/abs/2504.05154)
Append: [Enhancing LLM-Based Short Answer Grading with Retrieval-Augmented Generation](https://arxiv.org/abs/2504.05276)
Append: [Identifying Aspects in Peer Reviews](https://arxiv.org/abs/2504.06910)
Append: [Revisiting Uncertainty Quantification Evaluation in Language Models: Spurious Interactions with Response Length Bias Results](https://arxiv.org/abs/2504.13677)
Append: [Hypothetical Documents or Knowledge Leakage? Rethinking LLM-based Query Expansion](https://arxiv.org/abs/2504.14175)
Append: [Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models](https://arxiv.org/abs/2504.14194)
Append: [Is Compression Really Linear with Code Intelligence?](https://arxiv.org/abs/2505.11441)
Append: [THELMA: Task Based Holistic Evaluation of Large Language Model Applications-RAG Question Answering](https://arxiv.org/abs/2505.11626)
Append: [MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol](https://arxiv.org/abs/2505.14590)
Append: [T$^2$: An Adaptive Test-Time Scaling Strategy for Contextual Question Answering](https://arxiv.org/abs/2505.17427)
Append: [Assistant-Guided Mitigation of Teacher Preference Bias in LLM-as-a-Judge](https://arxiv.org/abs/2505.19176)
Append: [AstroVisBench: A Code Benchmark for Scientific Computing and Visualization in Astronomy](https://arxiv.org/abs/2505.20538)
Append: [STEER-BENCH: A Benchmark for Evaluating the Steerability of Large Language Models](https://arxiv.org/abs/2505.20645)
Append: [Trans-EnV: A Framework for Evaluating the Linguistic Robustness of LLMs Against English Varieties](https://arxiv.org/abs/2505.20875)
Append: [LLMs Think, But Not In Your Flow: Reasoning-Level Personalization for Black-Box Large Language Models](https://arxiv.org/abs/2505.21082)
Append: [The Arabic AI Fingerprint: Stylometric Analysis and Detection of Large Language Models Text](https://arxiv.org/abs/2505.23276)
Append: [Soft Reasoning: Navigating Solution Spaces in Large Language Models through Controlled Embedding Exploration](https://arxiv.org/abs/2505.24688)
Append: [Trust-Oriented Adaptive Guardrails for Large Language Models](https://arxiv.org/abs/2408.08959)
Append: [VinePPO: Refining Credit Assignment in RL Training of LLMs](https://arxiv.org/abs/2410.01679)
Append: [A LLM-Powered Automatic Grading Framework with Human-Level Guidelines Optimization](https://arxiv.org/abs/2410.02165)
Append: [MMedPO: Aligning Medical Vision-Language Models with Clinical-Aware Multimodal Preference Optimization](https://arxiv.org/abs/2412.06141)
Append: [From Intention To Implementation: Automating Biomedical Research via LLMs](https://arxiv.org/abs/2412.09429)
Append: [Scaling Laws for Floating Point Quantization Training](https://arxiv.org/abs/2501.02423)
Append: [Sleepless Nights, Sugary Days: Creating Synthetic Users with Health Conditions for Realistic Coaching Agent Interactions](https://arxiv.org/abs/2502.13135)
Append: [InSerter: Speech Instruction Following with Unsupervised Interleaved Pre-training](https://arxiv.org/abs/2503.02769)
Append: [Dynamic Benchmarking of Reasoning Capabilities in Code Large Language Models Under Data Contamination](https://arxiv.org/abs/2503.04149)
Append: [ROGRAG: A Robustly Optimized GraphRAG Framework](https://arxiv.org/abs/2503.06474)
Append: [GRU: Mitigating the Trade-off between Unlearning and Retention for Large Language Models](https://arxiv.org/abs/2503.09117)
Append: [A Survey on (M)LLM-Based GUI Agents](https://arxiv.org/abs/2504.13865)
Append: [Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling: A Perspective of Probability Theory](https://arxiv.org/abs/2505.10981)
Append: [PAST: Phonetic-Acoustic Speech Tokenizer](https://arxiv.org/abs/2505.14470)
Append: [LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning](https://arxiv.org/abs/2505.16933)
Append: [SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond](https://arxiv.org/abs/2505.19641)
Append: [What LLMs Miss in Recommendations: Bridging the Gap with Retrieval-Augmented Collaborative Signals](https://arxiv.org/abs/2505.20730)
Append: [On-Policy RL with Optimal Reward Baseline](https://arxiv.org/abs/2505.23585)
Append: [Let's Reason Formally: Natural-Formal Hybrid Reasoning Enhances LLM's Math Capability](https://arxiv.org/abs/2505.23703)
Append: [Large Language Models are Locally Linear Mappings](https://arxiv.org/abs/2505.24293)
append_entries: 212
Finish: 2025-06-05 04:31:02.700935
------------------------------------------------------
Started: 2025-06-05 06:25:51.527102
Existing_entries: 1212
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1565
Summarized using GPT-3.5-turbo
Append: [LayerIF: Estimating Layer Quality for Large Language Models using Influence Functions](https://arxiv.org/abs/2505.23811)
Token length: 1340
Summarized using GPT-3.5-turbo
Append: [Bench4KE: Benchmarking Automated Competency Question Generation](https://arxiv.org/abs/2505.24554)
Token length: 1413
Summarized using GPT-3.5-turbo
Append: [SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language Model Interpretability](https://arxiv.org/abs/2503.09532)
append_entries: 3
Finish: 2025-06-05 06:26:00.068334
------------------------------------------------------
Started: 2025-06-05 08:22:37.157954
Existing_entries: 1003
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-05 08:22:37.649875
------------------------------------------------------
Started: 2025-06-05 10:18:56.635777
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-05 10:18:57.102918
------------------------------------------------------
Started: 2025-06-05 12:35:01.137934
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-05 12:35:01.631488
------------------------------------------------------
Started: 2025-06-05 14:17:04.033968
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-05 14:17:04.532926
------------------------------------------------------
Started: 2025-06-05 16:21:07.165321
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-05 16:21:07.632087
------------------------------------------------------
Started: 2025-06-05 18:25:29.907511
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-05 18:25:30.433833
------------------------------------------------------
Started: 2025-06-05 20:15:44.468539
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-05 20:15:44.963811
------------------------------------------------------
Started: 2025-06-05 22:14:57.758999
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-05 22:14:58.265566
------------------------------------------------------
Started: 2025-06-06 01:19:43.480677
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-06 01:19:44.043306
------------------------------------------------------
Started: 2025-06-06 03:13:27.971425
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-06 03:13:28.519673
------------------------------------------------------
Started: 2025-06-06 04:28:52.277781
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1532
Summarized using GPT-3.5-turbo
Append: [GEM: Empowering LLM for both Embedding Generation and Language Understanding](https://arxiv.org/abs/2506.04344)
Token length: 1051
Summarized using GPT-3.5-turbo
Append: [Effects of Speaker Count, Duration, and Accent Diversity on Zero-Shot Accent Robustness in Low-Resource ASR](https://arxiv.org/abs/2506.04364)
Token length: 1159
Summarized using GPT-3.5-turbo
Append: [Mechanistic Decomposition of Sentence Representations](https://arxiv.org/abs/2506.04373)
Token length: 1328
Summarized using GPT-3.5-turbo
Append: [Hierarchical Text Classification Using Contrastive Learning Informed Path Guided Hierarchy](https://arxiv.org/abs/2506.04381)
Token length: 1323
Summarized using GPT-3.5-turbo
Append: [MELABenchv1: Benchmarking Large Language Models against Smaller Fine-Tuned Models for Low-Resource Maltese NLP](https://arxiv.org/abs/2506.04385)
Token length: 1551
Summarized using GPT-3.5-turbo
Append: [Building a Few-Shot Cross-Domain Multilingual NLU Model for Customer Care](https://arxiv.org/abs/2506.04389)
Token length: 1224
Summarized using GPT-3.5-turbo
Append: [MedAgentGym: Training LLM Agents for Code-Based Medical Reasoning at Scale](https://arxiv.org/abs/2506.04405)
Token length: 1219
Summarized using GPT-3.5-turbo
Append: [Unpacking Let Alone: Human-Scale Models Generalize to a Rare Construction in Form but not Meaning](https://arxiv.org/abs/2506.04408)
Token length: 615
Summarized using GPT-3.5-turbo
Append: [Empaths at SemEval-2025 Task 11: Retrieval-Augmented Approach to Perceived Emotions Prediction](https://arxiv.org/abs/2506.04409)
Token length: 1234
Summarized using GPT-3.5-turbo
Append: [Zero-Shot Open-Schema Entity Structure Discovery](https://arxiv.org/abs/2506.04458)
Token length: 1746
Summarized using GPT-3.5-turbo
Append: [Watermarking Degrades Alignment in Language Models: Analysis and Mitigation](https://arxiv.org/abs/2506.04462)
Token length: 1518
Summarized using GPT-3.5-turbo
Append: [Aligning Large Language Models with Implicit Preferences from User-Generated Content](https://arxiv.org/abs/2506.04463)
Token length: 959
Summarized using GPT-3.5-turbo
Append: [SQLens: An End-to-End Framework for Error Detection and Correction in Text-to-SQL](https://arxiv.org/abs/2506.04494)
Token length: 1184
Summarized using GPT-3.5-turbo
Append: [DRE: An Effective Dual-Refined Method for Integrating Small and Large Language Models in Open-Domain Dialogue Evaluation](https://arxiv.org/abs/2506.04516)
Token length: 1083
Summarized using GPT-3.5-turbo
Append: [Please Translate Again: Two Simple Experiments on Whether Human-Like Reasoning Helps Translation](https://arxiv.org/abs/2506.04521)
Token length: 766
Summarized using GPT-3.5-turbo
Append: [Is It JUST Semantics? A Case Study of Discourse Particle Understanding in LLMs](https://arxiv.org/abs/2506.04534)
Token length: 450
Summarized using GPT-3.5-turbo
Append: [BSBench: will your LLM find the largest prime number?](https://arxiv.org/abs/2506.04535)
Token length: 1307
Summarized using GPT-3.5-turbo
Append: [SSA-COMET: Do LLMs Outperform Learned Metrics in Evaluating MT for Under-Resourced African Languages?](https://arxiv.org/abs/2506.04557)
Token length: 1612
Summarized using GPT-3.5-turbo
Append: [Demonstrations of Integrity Attacks in Multi-Agent Systems](https://arxiv.org/abs/2506.04572)
Token length: 1573
Summarized using GPT-3.5-turbo
Append: [Reasoning or Overthinking: Evaluating Large Language Models on Financial Sentiment Analysis](https://arxiv.org/abs/2506.04574)
Token length: 1703
Summarized using GPT-3.5-turbo
Append: [Are LLMs Reliable Translators of Logical Reasoning Across Lexically Diversified Contexts?](https://arxiv.org/abs/2506.04575)
Token length: 1345
Summarized using GPT-3.5-turbo
Append: [Selecting Demonstrations for Many-Shot In-Context Learning via Gradient Matching](https://arxiv.org/abs/2506.04579)
Token length: 1183
Summarized using GPT-3.5-turbo
Append: [SUCEA: Reasoning-Intensive Retrieval for Adversarial Fact-checking through Claim Decomposition and Editing](https://arxiv.org/abs/2506.04583)
Token length: 1276
Summarized using GPT-3.5-turbo
Append: [MuSciClaims: Multimodal Scientific Claim Verification](https://arxiv.org/abs/2506.04585)
Token length: 1023
Summarized using GPT-3.5-turbo
Append: [LESS: Large Language Model Enhanced Semi-Supervised Learning for Speech Foundational Models](https://arxiv.org/abs/2506.04586)
Token length: 1541
Summarized using GPT-3.5-turbo
Append: [Safe: Enhancing Mathematical Reasoning in Large Language Models via Retrospective Step-aware Formal Verification](https://arxiv.org/abs/2506.04592)
Token length: 1113
Summarized using GPT-3.5-turbo
Append: [A MISMATCHED Benchmark for Scientific Natural Language Inference](https://arxiv.org/abs/2506.04603)
Token length: 838
Summarized using GPT-3.5-turbo
Append: [Revisiting Test-Time Scaling: A Survey and a Diversity-Aware Method for Efficient Reasoning](https://arxiv.org/abs/2506.04611)
Token length: 1751
Summarized using GPT-3.5-turbo
Append: [Subjective Perspectives within Learned Representations Predict High-Impact Innovation](https://arxiv.org/abs/2506.04616)
Token length: 945
Summarized using GPT-3.5-turbo
Append: [Static Word Embeddings for Sentence Semantic Representation](https://arxiv.org/abs/2506.04624)
Token length: 1878
Summarized using GPT-3.5-turbo
Append: [Advancing Tool-Augmented Large Language Models via Meta-Verification and Reflection Learning](https://arxiv.org/abs/2506.04625)
Token length: 1043
Summarized using GPT-3.5-turbo
Append: [ViCocktail: Automated Multi-Modal Data Collection for Vietnamese Audio-Visual Speech Recognition](https://arxiv.org/abs/2506.04635)
Token length: 1359
Summarized using GPT-3.5-turbo
Append: [TaDA: Training-free recipe for Decoding with Adaptive KV Cache Compression and Mean-centering](https://arxiv.org/abs/2506.04642)
Token length: 1387
Summarized using GPT-3.5-turbo
Append: [Flex-TravelPlanner: A Benchmark for Flexible Planning with Language Agents](https://arxiv.org/abs/2506.04649)
Token length: 1402
Summarized using GPT-3.5-turbo
Append: [Normative Conflicts and Shallow AI Alignment](https://arxiv.org/abs/2506.04679)
Token length: 844
Summarized using GPT-3.5-turbo
Append: [MMRefine: Unveiling the Obstacles to Robust Refinement in Multimodal Large Language Models](https://arxiv.org/abs/2506.04688)
Token length: 1797
Summarized using GPT-3.5-turbo
Append: [Recycling the Web: A Method to Enhance Pre-training Data Quality and Quantity for Language Models](https://arxiv.org/abs/2506.04689)
Token length: 888
Summarized using GPT-3.5-turbo
Append: [Cracking the Code: Enhancing Implicit Hate Speech Detection through Coding Classification](https://arxiv.org/abs/2506.04693)
Token length: 1660
Summarized using GPT-3.5-turbo
Append: [Accelerated Test-Time Scaling with Model-Free Speculative Sampling](https://arxiv.org/abs/2506.04708)
Token length: 1142
Summarized using GPT-3.5-turbo
Append: [IIITH-BUT system for IWSLT 2025 low-resource Bhojpuri to Hindi speech translation](https://arxiv.org/abs/2506.04714)
Token length: 1390
Summarized using GPT-3.5-turbo
Append: [SPARTA ALIGNMENT: Collectively Aligning Multiple Language Models through Combat](https://arxiv.org/abs/2506.04721)
Token length: 1372
Summarized using GPT-3.5-turbo
Append: [Lifelong Evolution: Collaborative Learning between Large and Small Language Models for Continuous Emergent Fake News Detection](https://arxiv.org/abs/2506.04739)
Token length: 1010
Summarized using GPT-3.5-turbo
Append: [Identifying Reliable Evaluation Metrics for Scientific Text Revision](https://arxiv.org/abs/2506.04772)
Token length: 1323
Summarized using GPT-3.5-turbo
Append: [Fine-Grained Interpretation of Political Opinions in Large Language Models](https://arxiv.org/abs/2506.04774)
Token length: 1660
Summarized using GPT-3.5-turbo
Append: [MMSU: A Massive Multi-task Spoken Language Understanding and Reasoning Benchmark](https://arxiv.org/abs/2506.04779)
Token length: 1321
Summarized using GPT-3.5-turbo
Append: [Towards LLM-Centric Multimodal Fusion: A Survey on Integration Strategies and Techniques](https://arxiv.org/abs/2506.04788)
Token length: 1330
Summarized using GPT-3.5-turbo
Append: [Dissecting Logical Reasoning in LLMs: A Fine-Grained Evaluation and Supervision Study](https://arxiv.org/abs/2506.04810)
Token length: 1655
Summarized using GPT-3.5-turbo
Append: [Design of intelligent proofreading system for English translation based on CNN and BERT](https://arxiv.org/abs/2506.04811)
Token length: 1052
Summarized using GPT-3.5-turbo
Append: [Evaluating Vision-Language and Large Language Models for Automated Student Assessment in Indonesian Classrooms](https://arxiv.org/abs/2506.04822)
Token length: 1046
Summarized using GPT-3.5-turbo
Append: [A Reasoning-Based Approach to Cryptic Crossword Clue Solving](https://arxiv.org/abs/2506.04824)
Append: [Joint Evaluation of Answer and Reasoning Consistency for Hallucination Detection in Large Reasoning Models](https://arxiv.org/abs/2506.04832)
Append: [MockConf: A Student Interpretation Dataset: Analysis, Word- and Span-level Alignment and Baselines](https://arxiv.org/abs/2506.04848)
Append: [Multiple-Choice Question Generation Using Large Language Models: Methodology and Educator Insights](https://arxiv.org/abs/2506.04851)
Append: [Prompting LLMs: Length Control for Isometric Machine Translation](https://arxiv.org/abs/2506.04855)
Append: [Evaluating the Effectiveness of Linguistic Knowledge in Pretrained Language Models: A Case Study of Universal Dependencies](https://arxiv.org/abs/2506.04887)
Append: [ICPC-Eval: Probing the Frontiers of LLM Reasoning with Competitive Programming Contests](https://arxiv.org/abs/2506.04894)
Append: [Verbose ListOps (VLO): Beyond Long Context -- Unmasking LLM's Reasoning Blind Spots](https://arxiv.org/abs/2506.04907)
Append: [A Practitioner's Guide to Building ASR Models for Low-Resource Languages: A Case Study on Scottish Gaelic](https://arxiv.org/abs/2506.04915)
Append: [Simulating LLM-to-LLM Tutoring for Multilingual Math Feedback](https://arxiv.org/abs/2506.04920)
Append: [ConECT Dataset: Overcoming Data Scarcity in Context-Aware E-Commerce MT](https://arxiv.org/abs/2506.04929)
Append: [From Struggle (06-2024) to Mastery (02-2025) LLMs Conquer Advanced Algorithm Exams and Pave the Way for Editorial Generation](https://arxiv.org/abs/2506.04965)
Append: [Better Semi-supervised Learning for Multi-domain ASR Through Incremental Retraining and Data Filtering](https://arxiv.org/abs/2506.04981)
Append: [SCOP: Evaluating the Comprehension Process of Large Language Models from a Cognitive View](https://arxiv.org/abs/2506.05000)
Append: [ComfyUI-Copilot: An Intelligent Assistant for Automated Workflow Development](https://arxiv.org/abs/2506.05010)
Append: [Controlling Summarization Length Through EOS Token Weighting](https://arxiv.org/abs/2506.05017)
Append: [Automatic Robustness Stress Testing of LLMs as Mathematical Problem Solvers](https://arxiv.org/abs/2506.05038)
Append: [TALL -- A Trainable Architecture for Enhancing LLM Performance in Low-Resource Languages](https://arxiv.org/abs/2506.05057)
Append: [Debatable Intelligence: Benchmarking LLM Judges via Debate Speech Evaluation](https://arxiv.org/abs/2506.05062)
Append: [Does It Make Sense to Speak of Introspection in Large Language Models?](https://arxiv.org/abs/2506.05068)
Append: [RIVAL: Reinforcement Learning with Iterative and Adversarial Optimization for Machine Translation](https://arxiv.org/abs/2506.05070)
Append: [Just a Scratch: Enhancing LLM Capabilities for Self-harm Detection through Intent Differentiation and Emoji Interpretation](https://arxiv.org/abs/2506.05073)
Append: [Parking, Perception, and Retail: Street-Level Determinants of Community Vitality in Harbin](https://arxiv.org/abs/2506.05080)
Append: [CL-ISR: A Contrastive Learning and Implicit Stance Reasoning Framework for Misleading Text Detection on Social Media](https://arxiv.org/abs/2506.05107)
Append: [The NTNU System at the S&I Challenge 2025 SLA Open Track](https://arxiv.org/abs/2506.05121)
Append: [DiCoRe: Enhancing Zero-shot Event Detection via Divergent-Convergent LLM Reasoning](https://arxiv.org/abs/2506.05128)
Append: [Information Locality as an Inductive Bias for Neural Language Models](https://arxiv.org/abs/2506.05136)
Append: [AudioLens: A Closer Look at Auditory Attribute Perception of Large Audio-Language Models](https://arxiv.org/abs/2506.05140)
Append: [Do Large Language Models Judge Error Severity Like Humans?](https://arxiv.org/abs/2506.05142)
Append: [Knowledgeable-r1: Policy Optimization for Knowledge Exploration in Retrieval-Augmented Generation](https://arxiv.org/abs/2506.05154)
Append: [Dissecting Bias in LLMs: A Mechanistic Interpretability Perspective](https://arxiv.org/abs/2506.05166)
Append: [ECoRAG: Evidentiality-guided Compression for Long Context RAG](https://arxiv.org/abs/2506.05167)
Append: [Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models](https://arxiv.org/abs/2506.05176)
Append: [Counterfactual reasoning: an analysis of in-context emergence](https://arxiv.org/abs/2506.05188)
Append: [RELIC: Evaluating Compositional Instruction Following via Language Recognition](https://arxiv.org/abs/2506.05205)
Append: [The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text](https://arxiv.org/abs/2506.05209)
Append: [Improving Low-Resource Morphological Inflection via Self-Supervised Objectives](https://arxiv.org/abs/2506.05227)
Append: [Towards a Unified System of Representation for Continuity and Discontinuity in Natural Language](https://arxiv.org/abs/2506.05235)
Append: [CLATTER: Comprehensive Entailment Reasoning for Hallucination Detection](https://arxiv.org/abs/2506.05243)
Append: [Micro-Act: Mitigate Knowledge Conflict in Question Answering via Actionable Self-Reasoning](https://arxiv.org/abs/2506.05278)
Append: [ProRefine: Inference-time Prompt Refinement with Textual Feedback](https://arxiv.org/abs/2506.05305)
Append: [Constrained Entropic Unlearning: A Primal-Dual Framework for Large Language Models](https://arxiv.org/abs/2506.05314)
Append: [Search Arena: Analyzing Search-Augmented LLMs](https://arxiv.org/abs/2506.05334)
Append: [Flattery, Fluff, and Fog: Diagnosing and Mitigating Idiosyncratic Biases in Preference Models](https://arxiv.org/abs/2506.05339)
Append: [Contextual Integrity in LLMs via Reasoning and Reinforcement Learning](https://arxiv.org/abs/2506.04245)
Append: [A Graph-Retrieval-Augmented Generation Framework Enhances Decision-Making in the Circular Economy](https://arxiv.org/abs/2506.04252)
Append: [ReXVQA: A Large-scale Visual Question Answering Benchmark for Generalist Chest X-ray Understanding](https://arxiv.org/abs/2506.04353)
Append: [Can we reconstruct a dysarthric voice with the large speech model Parler TTS?](https://arxiv.org/abs/2506.04397)
Append: [Matter-of-Fact: A Benchmark for Verifying the Feasibility of Literature-Supported Claims in Materials Science](https://arxiv.org/abs/2506.04410)
Append: [Behavioural vs. Representational Systematicity in End-to-End Models: An Opinionated Survey](https://arxiv.org/abs/2506.04461)
Append: [Understanding and Meeting Practitioner Needs When Measuring Representational Harms Caused by LLM-Based Systems](https://arxiv.org/abs/2506.04482)
Append: [Towards Efficient Speech-Text Jointly Decoding within One Speech Language Model](https://arxiv.org/abs/2506.04518)
Append: [Grapheme-Coherent Phonemic and Prosodic Annotation of Speech by Implicit and Explicit Grapheme Conditioning](https://arxiv.org/abs/2506.04527)
Append: [Clustering and Median Aggregation Improve Differentially Private Inference](https://arxiv.org/abs/2506.04566)
Append: [EMO-Debias: Benchmarking Gender Debiasing Techniques in Multi-Label Speech Emotion Recognition](https://arxiv.org/abs/2506.04652)
Append: [Urania: Differentially Private Insights into AI Use](https://arxiv.org/abs/2506.04681)
Append: [LLM-based phoneme-to-grapheme for phoneme-based speech recognition](https://arxiv.org/abs/2506.04711)
Append: [Evaluation is All You Need: Strategic Overclaiming of LLM Reasoning Capabilities Through Evaluation Design](https://arxiv.org/abs/2506.04734)
Append: [Exp4Fuse: A Rank Fusion Framework for Enhanced Sparse Retrieval using Large Language Model-based Query Expansion](https://arxiv.org/abs/2506.04760)
Append: [GOLFer: Smaller LM-Generated Documents Hallucination Filter & Combiner for Query Expansion in Information Retrieval](https://arxiv.org/abs/2506.04762)
Append: [From EHRs to Patient Pathways: Scalable Modeling of Longitudinal Health Trajectories with LLMs](https://arxiv.org/abs/2506.04831)
Append: [When Thinking LLMs Lie: Unveiling the Strategic Deception in Representations of Reasoning Models](https://arxiv.org/abs/2506.04909)
Append: [Dissecting Long Reasoning Models: An Empirical Study](https://arxiv.org/abs/2506.04913)
Append: [Towards Storage-Efficient Visual Document Retrieval: An Empirical Study on Reducing Patch-Level Embeddings](https://arxiv.org/abs/2506.04997)
Append: [Interpretable Multimodal Framework for Human-Centered Street Assessment: Integrating Visual-Language Models for Perceptual Urban Diagnostics](https://arxiv.org/abs/2506.05087)
Append: [CIVET: Systematic Evaluation of Understanding in VLMs](https://arxiv.org/abs/2506.05146)
Append: [LLM-First Search: Self-Guided Exploration of the Solution Space](https://arxiv.org/abs/2506.05213)
Append: [Mitigating Degree Bias Adaptively with Hard-to-Learn Nodes in Graph Contrastive Learning](https://arxiv.org/abs/2506.05214)
Append: [Diagonal Batching Unlocks Parallelism in Recurrent Memory Transformers for Long Contexts](https://arxiv.org/abs/2506.05229)
Append: [MesaNet: Sequence Modeling by Locally Optimal Test-Time Training](https://arxiv.org/abs/2506.05233)
Append: [Time to Talk: LLM Agents for Asynchronous Group Communication in Mafia Games](https://arxiv.org/abs/2506.05309)
Append: [Improving Data Efficiency for LLM Reinforcement Fine-tuning Through Difficulty-targeted Online Data Selection and Rollout Replay](https://arxiv.org/abs/2506.05316)
Append: [Unleashing Hour-Scale Video Training for Long Video-Language Understanding](https://arxiv.org/abs/2506.05332)
Append: [Kinetics: Rethinking Test-Time Scaling Laws](https://arxiv.org/abs/2506.05333)
Append: [Inference-Time Hyper-Scaling with KV Cache Compression](https://arxiv.org/abs/2506.05345)
Append: [Why LLM Safety Guardrails Collapse After Fine-tuning: A Similarity Analysis Between Alignment and Fine-tuning Datasets](https://arxiv.org/abs/2506.05346)
Append: [The Vector Grounding Problem](https://arxiv.org/abs/2304.01481)
Append: [Bias in Language Models: Beyond Trick Tests and Toward RUTEd Evaluation](https://arxiv.org/abs/2402.12649)
Append: [Mosaic-IT: Cost-Free Compositional Data Synthesis for Instruction Tuning](https://arxiv.org/abs/2405.13326)
Append: [The Impossibility of Fair LLMs](https://arxiv.org/abs/2406.03198)
Append: [Multi-Head RAG: Solving Multi-Aspect Problems with LLMs](https://arxiv.org/abs/2406.05085)
Append: [Investigating Distributions of Telecom Adapted Sentence Embeddings for Document Retrieval](https://arxiv.org/abs/2406.12336)
Append: [Leveraging LLMs for Bangla Grammar Error Correction:Error Categorization, Synthetic Data, and Model Evaluation](https://arxiv.org/abs/2406.14284)
Append: [DataGen: Unified Synthetic Dataset Generation via Large Language Models](https://arxiv.org/abs/2406.18966)
Append: [Inducing lexicons of in-group language with socio-temporal context](https://arxiv.org/abs/2409.19257)
Append: [An Exploration of Self-Supervised Mutual Information Alignment for Multi-Task Settings](https://arxiv.org/abs/2410.01704)
Append: [Self-Correction is More than Refinement: A Learning Framework for Visual and Language Reasoning Tasks](https://arxiv.org/abs/2410.04055)
Append: [Evaluating Morphological Compositional Generalization in Large Language Models](https://arxiv.org/abs/2410.12656)
Append: [Not All Options Are Created Equal: Textual Option Weighting for Token-Efficient LLM-Based Knowledge Tracing](https://arxiv.org/abs/2410.12872)
Append: [Isolated Causal Effects of Natural Language](https://arxiv.org/abs/2410.14812)
Append: [What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A Gradient Perspective](https://arxiv.org/abs/2410.23743)
Append: [FastDraft: How to Train Your Draft](https://arxiv.org/abs/2411.11055)
Append: [MuLan: Adapting Multilingual Diffusion Models for Hundreds of Languages with Negligible Cost](https://arxiv.org/abs/2412.01271)
Append: [The broader spectrum of in-context learning](https://arxiv.org/abs/2412.03782)
Append: [GRAF: Graph Retrieval Augmented by Facts for Romanian Legal Multi-Choice Question Answering](https://arxiv.org/abs/2412.04119)
Append: [The Lessons of Developing Process Reward Models in Mathematical Reasoning](https://arxiv.org/abs/2501.07301)
Append: [Explainability in Practice: A Survey of Explainable NLP Across Various Domains](https://arxiv.org/abs/2502.00837)
Append: [Mind the Confidence Gap: Overconfidence, Calibration, and Distractor Effects in Large Language Models](https://arxiv.org/abs/2502.11028)
Append: [On the Robust Approximation of ASR Metrics](https://arxiv.org/abs/2502.12408)
Append: [Lost in Transcription, Found in Distribution Shift: Demystifying Hallucination in Speech Foundation Models](https://arxiv.org/abs/2502.12414)
Append: [Should I Trust You? Detecting Deception in Negotiations using Counterfactual RL](https://arxiv.org/abs/2502.12436)
Append: [Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the Limits of Embedding Space Capacity](https://arxiv.org/abs/2502.13063)
Append: [A Survey on Data Contamination for Large Language Models](https://arxiv.org/abs/2502.14425)
Append: [Towards Robust ESG Analysis Against Greenwashing Risks: Aspect-Action Analysis with Cross-Category Generalization](https://arxiv.org/abs/2502.15821)
Append: [SNaRe: Domain-aware Data Generation for Low-Resource Event Detection](https://arxiv.org/abs/2502.17394)
Append: [From Benign import Toxic: Jailbreaking the Language Model via Adversarial Metaphors](https://arxiv.org/abs/2503.00038)
Append: [Argument Summarization and its Evaluation in the Era of Large Language Models](https://arxiv.org/abs/2503.00847)
Append: [ATLaS: Agent Tuning via Learning Critical Steps](https://arxiv.org/abs/2503.02197)
Append: [When Claims Evolve: Evaluating and Enhancing the Robustness of Embedding Models Against Misinformation Edits](https://arxiv.org/abs/2503.03417)
Append: [Implicit Cross-Lingual Rewarding for Efficient Multilingual Preference Alignment](https://arxiv.org/abs/2503.04647)
Append: [COMI-LINGUA: Expert Annotated Large-Scale Dataset for Multitask NLP in Hindi-English Code-Mixing](https://arxiv.org/abs/2503.21670)
Append: [Firm or Fickle? Evaluating Large Language Models Consistency in Sequential Interactions](https://arxiv.org/abs/2503.22353)
Append: [Is LLM the Silver Bullet to Low-Resource Languages Machine Translation?](https://arxiv.org/abs/2503.24102)
Append: [NorEval: A Norwegian Language Understanding and Generation Evaluation Benchmark](https://arxiv.org/abs/2504.07749)
Append: [DREAM: Disentangling Risks to Enhance Safety Alignment in Multimodal Large Language Models](https://arxiv.org/abs/2504.18053)
Append: [Calibrating Translation Decoding with Quality Estimation on LLMs](https://arxiv.org/abs/2504.19044)
Append: [Full-Parameter Continual Pretraining of Gemma2: Insights into Fluency and Domain Knowledge](https://arxiv.org/abs/2505.05946)
Append: [MiMo: Unlocking the Reasoning Potential of Language Model -- From Pretraining to Posttraining](https://arxiv.org/abs/2505.07608)
Append: [$K$-MSHC: Unmasking Minimally Sufficient Head Circuits in Large Language Models with Experiments on Syntactic Classification Tasks](https://arxiv.org/abs/2505.12268)
Append: [DECASTE: Unveiling Caste Stereotypes in Large Language Models through Multi-Dimensional Bias Analysis](https://arxiv.org/abs/2505.14971)
Append: [Distilling the Implicit Multi-Branch Structure in LLMs' Reasoning via Reinforcement Learning](https://arxiv.org/abs/2505.16142)
Append: [When can isotropy help adapt LLMs' next word prediction to numerical domains?](https://arxiv.org/abs/2505.17135)
Append: [WiNGPT-3.0 Technical Report](https://arxiv.org/abs/2505.17387)
Append: [MetaGen Blended RAG: Unlocking Zero-Shot Precision for Specialized Domain Question-Answering](https://arxiv.org/abs/2505.18247)
Append: [MAVL: A Multilingual Audio-Video Lyrics Dataset for Animated Song Translation](https://arxiv.org/abs/2505.18614)
Append: [The Role of Diversity in In-Context Learning for Large Language Models](https://arxiv.org/abs/2505.19426)
Append: [Deriving Strategic Market Insights with Large Language Models: A Benchmark for Forward Counterfactual Generation](https://arxiv.org/abs/2505.19430)
Append: [OmniCharacter: Towards Immersive Role-Playing Agents with Seamless Speech-Language Personality Interaction](https://arxiv.org/abs/2505.20277)
Append: [Rethinking Text-based Protein Understanding: Retrieval or LLM?](https://arxiv.org/abs/2505.20354)
Append: [In-context Language Learning for Endangered Languages in Speech Recognition](https://arxiv.org/abs/2505.20445)
Append: [CHIMERA: A Knowledge Base of Idea Recombination in Scientific Literature](https://arxiv.org/abs/2505.20779)
Append: [Curse of High Dimensionality Issue in Transformer for Long-context Modeling](https://arxiv.org/abs/2505.22107)
Append: [Breaking the Cloak! Unveiling Chinese Cloaked Toxicity with Homophone Graph and Toxic Lexicon](https://arxiv.org/abs/2505.22184)
Append: [Uncovering Visual-Semantic Psycholinguistic Properties from the Distributional Structure of Text Embedding Space](https://arxiv.org/abs/2505.23029)
Append: [MMBoundary: Advancing MLLM Knowledge Boundary Awareness through Reasoning Step Confidence Calibration](https://arxiv.org/abs/2505.23224)
Append: [Adaptive Jailbreaking Strategies Based on the Semantic Understanding Capabilities of Large Language Models](https://arxiv.org/abs/2505.23404)
Append: [ValueSim: Generating Backstories to Model Individual Value Systems](https://arxiv.org/abs/2505.23827)
Append: [Fewer Hallucinations, More Verification: A Three-Stage LLM-Based Framework for ASR Error Correction](https://arxiv.org/abs/2505.24347)
Append: [Scaling Trends in Language Model Robustness](https://arxiv.org/abs/2407.18213)
Append: [Focus On This, Not That! Steering LLMs with Adaptive Feature Specification](https://arxiv.org/abs/2410.22944)
Append: [Failure Modes of LLMs for Causal Reasoning on Narratives](https://arxiv.org/abs/2410.23884)
Append: [DEFAME: Dynamic Evidence-based FAct-checking with Multimodal Experts](https://arxiv.org/abs/2412.10510)
Append: [HashEvict: A Pre-Attention KV Cache Eviction Strategy using Locality-Sensitive Hashing](https://arxiv.org/abs/2412.16187)
Append: [Can Large Language Models Understand Intermediate Representations in Compilers?](https://arxiv.org/abs/2502.06854)
Append: [EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents](https://arxiv.org/abs/2502.09560)
Append: [GoRA: Gradient-driven Adaptive Low Rank Adaptation](https://arxiv.org/abs/2502.12171)
Append: [Empowering LLMs with Logical Reasoning: A Comprehensive Survey](https://arxiv.org/abs/2502.15652)
Append: [Contrastive Visual Data Augmentation](https://arxiv.org/abs/2502.17709)
Append: [TurboFuzzLLM: Turbocharging Mutation-based Fuzzing for Effectively Jailbreaking Large Language Models in Practice](https://arxiv.org/abs/2502.18504)
Append: [ChatWise: A Strategy-Guided Chatbot for Enhancing Cognitive Support in Older Adults](https://arxiv.org/abs/2503.05740)
Append: [LLM Social Simulations Are a Promising Research Method](https://arxiv.org/abs/2504.02234)
Append: [Toward a Human-Centered Evaluation Framework for Trustworthy LLM-Powered GUI Agents](https://arxiv.org/abs/2504.17934)
Append: [Optimizing Anytime Reasoning via Budget Relative Policy Optimization](https://arxiv.org/abs/2505.13438)
Append: [SUS backprop: linear backpropagation algorithm for long inputs in transformers](https://arxiv.org/abs/2505.15080)
Append: [AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning](https://arxiv.org/abs/2505.16400)
Append: [FlowCut: Rethinking Redundancy via Information Flow for Efficient Vision-Language Models](https://arxiv.org/abs/2505.19536)
Append: [Position is Power: System Prompts as a Mechanism of Bias in Large Language Models (LLMs)](https://arxiv.org/abs/2505.21091)
Append: [EnsemW2S: Enhancing Weak-to-Strong Generalization with Large Language Model Ensembles](https://arxiv.org/abs/2505.21959)
Append: [Evaluation of LLMs in Speech is Often Flawed: Test Set Contamination in Large Language Models for Speech Recognition](https://arxiv.org/abs/2505.22251)
Append: [WikiGap: Promoting Epistemic Equity by Surfacing Knowledge Gaps Between English Wikipedia and other Language Editions](https://arxiv.org/abs/2505.24195)
Append: [MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement Learning](https://arxiv.org/abs/2505.24871)
Append: [ReasonGen-R1: CoT for Autoregressive Image generation models through SFT and RL](https://arxiv.org/abs/2505.24875)
append_entries: 211
Finish: 2025-06-06 04:31:46.266073
------------------------------------------------------
Started: 2025-06-06 06:25:45.101117
Existing_entries: 1211
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-06 06:25:45.596120
------------------------------------------------------
Started: 2025-06-06 08:22:11.482493
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-06 08:22:11.940539
------------------------------------------------------
Started: 2025-06-06 10:18:44.038705
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-06 10:18:44.501287
------------------------------------------------------
Started: 2025-06-06 12:33:47.158301
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-06 12:33:47.694350
------------------------------------------------------
Started: 2025-06-06 14:16:27.459708
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-06 14:16:27.934053
------------------------------------------------------
Started: 2025-06-06 16:21:41.270655
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-06 16:21:41.828328
------------------------------------------------------
Started: 2025-06-06 18:23:35.195437
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-06 18:23:35.732169
------------------------------------------------------
Started: 2025-06-06 20:19:06.450826
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-06 20:19:06.908542
------------------------------------------------------
Started: 2025-06-06 22:15:59.923676
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-06 22:16:00.386451
------------------------------------------------------
Started: 2025-06-07 01:19:10.586998
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-07 01:19:11.055590
------------------------------------------------------
Started: 2025-06-07 03:11:19.553744
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-07 03:11:20.057006
------------------------------------------------------
Started: 2025-06-07 04:19:59.517040
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-07 04:19:59.576112
------------------------------------------------------
Started: 2025-06-07 06:22:12.142867
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-07 06:22:12.215288
------------------------------------------------------
Started: 2025-06-07 08:19:49.865984
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-07 08:19:49.949167
------------------------------------------------------
Started: 2025-06-07 10:17:02.596306
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-07 10:17:02.652156
------------------------------------------------------
Started: 2025-06-07 12:31:04.203250
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-07 12:31:04.308931
------------------------------------------------------
Started: 2025-06-07 14:13:58.716874
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-07 14:13:58.775227
------------------------------------------------------
Started: 2025-06-07 16:18:52.181858
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-07 16:18:52.236105
------------------------------------------------------
Started: 2025-06-07 18:21:01.102549
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-07 18:21:01.156106
------------------------------------------------------
Started: 2025-06-07 20:16:45.597812
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-07 20:16:45.672207
------------------------------------------------------
Started: 2025-06-07 22:15:12.966292
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-07 22:15:13.044882
------------------------------------------------------
Started: 2025-06-08 01:27:14.292596
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-08 01:27:14.351494
------------------------------------------------------
Started: 2025-06-08 03:21:48.697274
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-08 03:21:48.768108
------------------------------------------------------
Started: 2025-06-08 04:27:35.690859
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-08 04:27:35.823752
------------------------------------------------------
Started: 2025-06-08 06:23:19.838208
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-08 06:23:19.895383
------------------------------------------------------
Started: 2025-06-08 08:19:38.365624
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-08 08:19:38.450881
------------------------------------------------------
Started: 2025-06-08 10:16:14.838488
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-08 10:16:14.911399
------------------------------------------------------
Started: 2025-06-08 12:30:55.622514
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-08 12:30:55.677771
------------------------------------------------------
Started: 2025-06-08 14:14:20.747874
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-08 14:14:20.832008
------------------------------------------------------
Started: 2025-06-08 16:18:51.893768
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-08 16:18:51.976594
------------------------------------------------------
Started: 2025-06-08 18:20:42.233150
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-08 18:20:42.301914
------------------------------------------------------
Started: 2025-06-08 20:17:04.510527
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-08 20:17:04.576963
------------------------------------------------------
Started: 2025-06-08 22:14:53.927759
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-08 22:14:54.008999
------------------------------------------------------
Started: 2025-06-09 01:24:22.883059
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-09 01:24:22.941949
------------------------------------------------------
Started: 2025-06-09 03:19:54.379887
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-09 03:19:54.440874
------------------------------------------------------
Started: 2025-06-09 04:32:39.886499
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1411
Summarized using GPT-3.5-turbo
Append: [EvidenceOutcomes: a Dataset of Clinical Trial Publications with Clinically Meaningful Outcomes](https://arxiv.org/abs/2506.05380)
Token length: 1002
Summarized using GPT-3.5-turbo
Append: [LLMs Can Also Do Well! Breaking Barriers in Semantic Role Labeling via Large Language Models](https://arxiv.org/abs/2506.05385)
Token length: 1331
Summarized using GPT-3.5-turbo
Append: [Beyond RAG: Reinforced Reasoning Augmented Generation for Clinical Notes](https://arxiv.org/abs/2506.05386)
Token length: 1024
Summarized using GPT-3.5-turbo
Append: [Advancing Decoding Strategies: Enhancements in Locally Typical Sampling for LLMs](https://arxiv.org/abs/2506.05387)
Token length: 1120
Summarized using GPT-3.5-turbo
Append: [taz2024full: Analysing German Newspapers for Gender Bias and Discrimination across Decades](https://arxiv.org/abs/2506.05388)
Token length: 1305
Summarized using GPT-3.5-turbo
Append: [Understanding Gender Bias in AI-Generated Product Descriptions](https://arxiv.org/abs/2506.05390)
Token length: 1496
Summarized using GPT-3.5-turbo
Append: [Are Large Language Models Good Temporal Graph Learners?](https://arxiv.org/abs/2506.05393)
Token length: 1344
Summarized using GPT-3.5-turbo
Append: [Auto Review: Second Stage Error Detection for Highly Accurate Information Extraction from Phone Conversations](https://arxiv.org/abs/2506.05400)
Token length: 1214
Summarized using GPT-3.5-turbo
Append: [Homogeneous Keys, Heterogeneous Values: Exploiting Local KV Cache Asymmetry for Long-Context LLMs](https://arxiv.org/abs/2506.05410)
Token length: 851
Summarized using GPT-3.5-turbo
Append: [SmoothRot: Combining Channel-Wise Scaling and Rotation for Quantization-Friendly LLMs](https://arxiv.org/abs/2506.05413)
Token length: 891
Summarized using GPT-3.5-turbo
Append: [Automatically Detecting Amusing Games in Wordle](https://arxiv.org/abs/2506.05415)
Token length: 1025
Summarized using GPT-3.5-turbo
Append: [MLLM-CL: Continual Learning for Multimodal Large Language Models](https://arxiv.org/abs/2506.05453)
Token length: 1489
Summarized using GPT-3.5-turbo
Append: [Multidimensional Analysis of Specific Language Impairment Using Unsupervised Learning Through PCA and Clustering](https://arxiv.org/abs/2506.05498)
Token length: 1842
Summarized using GPT-3.5-turbo
Append: [Improving LLMs with a knowledge from databases](https://arxiv.org/abs/2506.05560)
Token length: 662
Summarized using GPT-3.5-turbo
Append: [Combating Misinformation in the Arab World: Challenges & Opportunities](https://arxiv.org/abs/2506.05582)
Token length: 857
Summarized using GPT-3.5-turbo
Append: [UTSA-NLP at ArchEHR-QA 2025: Improving EHR Question Answering via Self-Consistency Prompting](https://arxiv.org/abs/2506.05589)
Token length: 1099
Summarized using GPT-3.5-turbo
Append: [SynthesizeMe! Inducing Persona-Guided Prompts for Personalized Reward Models in LLMs](https://arxiv.org/abs/2506.05598)
Token length: 1232
Summarized using GPT-3.5-turbo
Append: [OPeRA: A Dataset of Observation, Persona, Rationale, and Action for Evaluating LLMs on Human Online Shopping Behavior Simulation](https://arxiv.org/abs/2506.05606)
Token length: 975
Summarized using GPT-3.5-turbo
Append: [Mitigating Confounding in Speech-Based Dementia Detection through Weight Masking](https://arxiv.org/abs/2506.05610)
Token length: 848
Summarized using GPT-3.5-turbo
Append: [Leveraging Self-Attention for Input-Dependent Soft Prompting in LLMs](https://arxiv.org/abs/2506.05629)
Token length: 836
Summarized using GPT-3.5-turbo
Append: [IYKYK: Using language models to decode extremist cryptolects](https://arxiv.org/abs/2506.05635)
Token length: 1089
Summarized using GPT-3.5-turbo
Append: [A Fictional Q&A Dataset for Studying Memorization and Knowledge Acquisition](https://arxiv.org/abs/2506.05639)
Token length: 1429
Summarized using GPT-3.5-turbo
Append: [Can LLMs Express Personality Across Cultures? Introducing CulturalPersonas for Evaluating Trait Alignment](https://arxiv.org/abs/2506.05670)
Token length: 1333
Summarized using GPT-3.5-turbo
Append: [Zero-Shot Event Causality Identification via Multi-source Evidence Fuzzy Aggregation with Large Language Models](https://arxiv.org/abs/2506.05675)
Token length: 1110
Summarized using GPT-3.5-turbo
Append: [A Unified Representation for Continuity and Discontinuity: Syntactic and Computational Motivations](https://arxiv.org/abs/2506.05686)
Token length: 1462
Summarized using GPT-3.5-turbo
Append: [When to use Graphs in RAG: A Comprehensive Analysis for Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2506.05690)
Token length: 1682
Summarized using GPT-3.5-turbo
Append: [Being Strong Progressively! Enhancing Knowledge Distillation of Large Language Models through a Curriculum Learning Framework](https://arxiv.org/abs/2506.05695)
Token length: 762
Summarized using GPT-3.5-turbo
Append: [RKEFino1: A Regulation Knowledge-Enhanced Large Language Model](https://arxiv.org/abs/2506.05700)
Token length: 1545
Summarized using GPT-3.5-turbo
Append: [Large Language Models are Good Relational Learners](https://arxiv.org/abs/2506.05725)
Token length: 1397
Summarized using GPT-3.5-turbo
Append: [Do LLMs Really Forget? Evaluating Unlearning with Knowledge Correlation and Confidence Awareness](https://arxiv.org/abs/2506.05735)
Token length: 1037
Summarized using GPT-3.5-turbo
Append: [LLM-Symbolic Integration for Robust Temporal Tabular Reasoning](https://arxiv.org/abs/2506.05746)
Token length: 1227
Summarized using GPT-3.5-turbo
Append: [Writing-RL: Advancing Long-form Writing via Adaptive Curriculum Reinforcement Learning](https://arxiv.org/abs/2506.05760)
Token length: 1189
Summarized using GPT-3.5-turbo
Append: [BioMol-MQA: A Multi-Modal Question Answering Dataset For LLM Reasoning Over Bio-Molecular Interactions](https://arxiv.org/abs/2506.05766)
Token length: 927
Summarized using GPT-3.5-turbo
Append: [dots.llm1 Technical Report](https://arxiv.org/abs/2506.05767)
Token length: 1247
Summarized using GPT-3.5-turbo
Append: [Discrete Minds in a Continuous World: Do Language Models Know Time Passes?](https://arxiv.org/abs/2506.05790)
Token length: 1045
Summarized using GPT-3.5-turbo
Append: [MAPLE: Multi-Agent Adaptive Planning with Long-Term Memory for Table Reasoning](https://arxiv.org/abs/2506.05813)
Token length: 1407
Summarized using GPT-3.5-turbo
Append: [FinanceReasoning: Benchmarking Financial Numerical Reasoning More Credible, Comprehensive and Challenging](https://arxiv.org/abs/2506.05828)
Token length: 1727
Summarized using GPT-3.5-turbo
Append: [Cross-lingual Collapse: How Language-Centric Foundation Models Shape Reasoning in Large Language Models](https://arxiv.org/abs/2506.05850)
Token length: 1927
Summarized using GPT-3.5-turbo
Append: [Route-and-Reason: Scaling Large Language Model Reasoning with Reinforced Model Router](https://arxiv.org/abs/2506.05901)
Token length: 1438
Summarized using GPT-3.5-turbo
Append: [Generating Grounded Responses to Counter Misinformation via Learning Efficient Fine-Grained Critiques](https://arxiv.org/abs/2506.05924)
Token length: 632
Summarized using GPT-3.5-turbo
Append: [LengClaro2023: A Dataset of Administrative Texts in Spanish with Plain Language adaptations](https://arxiv.org/abs/2506.05927)
Token length: 1425
Summarized using GPT-3.5-turbo
Append: [MoA: Heterogeneous Mixture of Adapters for Parameter-Efficient Fine-Tuning of Large Language Models](https://arxiv.org/abs/2506.05928)
Token length: 1234
Summarized using GPT-3.5-turbo
Append: [DynamicMind: A Tri-Mode Thinking System for Large Language Models](https://arxiv.org/abs/2506.05936)
Token length: 1576
Summarized using GPT-3.5-turbo
Append: [IntentionESC: An Intention-Centered Framework for Enhancing Emotional Support in Dialogue Systems](https://arxiv.org/abs/2506.05947)
Token length: 1195
Summarized using GPT-3.5-turbo
Append: [NameTag 3: A Tool and a Service for Multilingual/Multitagset NER](https://arxiv.org/abs/2506.05949)
Token length: 1410
Summarized using GPT-3.5-turbo
Append: [Elementary Math Word Problem Generation using Large Language Models](https://arxiv.org/abs/2506.05950)
Token length: 1123
Summarized using GPT-3.5-turbo
Append: [Let's Put Ourselves in Sally's Shoes: Shoes-of-Others Prefixing Improves Theory of Mind in Large Language Models](https://arxiv.org/abs/2506.05970)
Token length: 575
Summarized using GPT-3.5-turbo
Append: [LTG at SemEval-2025 Task 10: Optimizing Context for Classification of Narrative Roles](https://arxiv.org/abs/2506.05976)
Token length: 883
Summarized using GPT-3.5-turbo
Append: [Tau-Eval: A Unified Evaluation Framework for Useful and Private Text Anonymization](https://arxiv.org/abs/2506.05979)
Token length: 1386
Summarized using GPT-3.5-turbo
Append: [A Culturally-Rich Romanian NLP Dataset from "Who Wants to Be a Millionaire?" Videos](https://arxiv.org/abs/2506.05991)
Append: [Token Signature: Predicting Chain-of-Thought Gains with Token Decoding Feature in Large Language Models](https://arxiv.org/abs/2506.06008)
Append: [Unlocking Recursive Thinking of LLMs: Alignment via Refinement](https://arxiv.org/abs/2506.06009)
Append: [AgentSwift: Efficient LLM Agent Design via Value-guided Hierarchical Search](https://arxiv.org/abs/2506.06017)
Append: [When to Trust Context: Self-Reflective Debates for Context Reliability](https://arxiv.org/abs/2506.06020)
Append: [Large Language Models are Demonstration Pre-Selectors for Themselves](https://arxiv.org/abs/2506.06033)
Append: [MATP-BENCH: Can MLLM Be a Good Automated Theorem Prover for Multimodal Problems?](https://arxiv.org/abs/2506.06034)
Append: [Hey, That's My Data! Label-Only Dataset Inference in Large Language Models](https://arxiv.org/abs/2506.06057)
Append: [Simple Yet Effective: Extracting Private Data Across Clients in Federated Fine-Tuning of Large Language Models](https://arxiv.org/abs/2506.06060)
Append: [Zero-Shot Detection of LLM-Generated Code via Approximated Task Conditioning](https://arxiv.org/abs/2506.06069)
Append: [MIRIAD: Augmenting LLMs with millions of medical query-response pairs](https://arxiv.org/abs/2506.06091)
Append: [Reinforcing Code Generation: Improving Text-to-SQL with Execution-Based Learning](https://arxiv.org/abs/2506.06093)
Append: [Bridging the Gap: In-Context Learning for Modeling Human Disagreement](https://arxiv.org/abs/2506.06113)
Append: [Phonetically-Augmented Discriminative Rescoring for Voice Search Error Correction](https://arxiv.org/abs/2506.06117)
Append: [Let's CONFER: A Dataset for Evaluating Natural Language Inference Models on CONditional InFERence and Presupposition](https://arxiv.org/abs/2506.06133)
Append: [semantic-features: A User-Friendly Tool for Studying Contextual Word Embeddings in Interpretable Semantic Spaces](https://arxiv.org/abs/2506.06169)
Append: [Does It Run and Is That Enough? Revisiting Text-to-Chart Generation with a Multi-Agent Approach](https://arxiv.org/abs/2506.06175)
Append: [Detecting Voice Phishing with Precision: Fine-Tuning Small Language Models](https://arxiv.org/abs/2506.06180)
Append: [Building Models of Neurological Language](https://arxiv.org/abs/2506.06208)
Append: [PuzzleWorld: A Benchmark for Multimodal, Open-Ended Reasoning in Puzzlehunts](https://arxiv.org/abs/2506.06211)
Append: [Can Theoretical Physics Research Benefit from Language Agents?](https://arxiv.org/abs/2506.06214)
Append: [Explaining Matters: Leveraging Definitions and Semantic Expansion for Sexism Detection](https://arxiv.org/abs/2506.06238)
Append: [Bridging External and Parametric Knowledge: Mitigating Hallucination of LLMs with Shared-Private Semantic Synergy in Dual-Stream Knowledge](https://arxiv.org/abs/2506.06240)
Append: [Cartridges: Lightweight and general-purpose long context representations via self-study](https://arxiv.org/abs/2506.06266)
Append: [AdvSumm: Adversarial Training for Bias Mitigation in Text Summarization](https://arxiv.org/abs/2506.06273)
Append: [Attention-based transformer models for image captioning across languages: An in-depth survey and evaluation](https://arxiv.org/abs/2506.05399)
Append: [Can Vision Language Models Infer Human Gaze Direction? A Controlled Study](https://arxiv.org/abs/2506.05412)
Append: [Coordinated Robustness Evaluation Framework for Vision-Language Models](https://arxiv.org/abs/2506.05429)
Append: [LLMs Can Compensate for Deficiencies in Visual Representations](https://arxiv.org/abs/2506.05439)
Append: [BYO-Eval: Build Your Own Dataset for Fine-Grained Visual Assessment of Multimodal Language Models](https://arxiv.org/abs/2506.05440)
Append: [Interpretation Meets Safety: A Survey on Interpretation Methods and Tools for Improving LLM Safety](https://arxiv.org/abs/2506.05451)
Append: [MORSE-500: A Programmatically Controllable Video Benchmark to Stress-Test Multimodal Reasoning](https://arxiv.org/abs/2506.05523)
Append: [When Models Know More Than They Can Explain: Quantifying Knowledge Transfer in Human-AI Collaboration](https://arxiv.org/abs/2506.05579)
Append: [MMTU: A Massive Multi-Task Table Understanding and Reasoning Benchmark](https://arxiv.org/abs/2506.05587)
Append: [SoK: Are Watermarks in LLMs Ready for Deployment?](https://arxiv.org/abs/2506.05594)
Append: [Deployability-Centric Infrastructure-as-Code Generation: An LLM-based Iterative Framework](https://arxiv.org/abs/2506.05623)
Append: [Projectable Models: One-Shot Generation of Small Specialized Transformers from Large Ones](https://arxiv.org/abs/2506.05641)
Append: [BAQ: Efficient Bit Allocation Quantization for Large Language Models](https://arxiv.org/abs/2506.05664)
Append: [Low-Resource Domain Adaptation for Speech LLMs via Text-Only Fine-Tuning](https://arxiv.org/abs/2506.05671)
Append: [Contextually Guided Transformers via Low-Rank Adaptation](https://arxiv.org/abs/2506.05672)
Append: [Voice Impression Control in Zero-Shot TTS](https://arxiv.org/abs/2506.05688)
Append: [Constrained Sampling for Language Models Should Be Easy: An MCMC Perspective](https://arxiv.org/abs/2506.05754)
Append: [Do Large Vision-Language Models Distinguish between the Actual and Apparent Features of Illusions?](https://arxiv.org/abs/2506.05765)
Append: [CodeContests+: High-Quality Test Case Generation for Competitive Programming](https://arxiv.org/abs/2506.05817)
Append: [Proactive Assistant Dialogue Generation from Streaming Egocentric Videos](https://arxiv.org/abs/2506.05904)
Append: [Audio-Aware Large Language Models as Judges for Speaking Styles](https://arxiv.org/abs/2506.05984)
Append: [Bootstrapping World Models from Dynamics Models in Multimodal Foundation Models](https://arxiv.org/abs/2506.06006)
Append: [CO-VADA: A Confidence-Oriented Voice Augmentation Debiasing Approach for Fair Speech Emotion Recognition](https://arxiv.org/abs/2506.06071)
Append: [Label-Context-Dependent Internal Language Model Estimation for CTC](https://arxiv.org/abs/2506.06096)
Append: [Table-r1: Self-supervised and Reinforcement Learning for Program-based Table Reasoning in Small Language Models](https://arxiv.org/abs/2506.06137)
Append: [CLaMR: Contextualized Late-Interaction for Multimodal Content Retrieval](https://arxiv.org/abs/2506.06144)
Append: [Masked Language Models are Good Heterogeneous Graph Generalizers](https://arxiv.org/abs/2506.06157)
Append: [The Lock-in Hypothesis: Stagnation by Algorithm](https://arxiv.org/abs/2506.06166)
Append: [Corrector Sampling in Language Models](https://arxiv.org/abs/2506.06215)
Append: [PersonaAgent: When Large Language Model Agents Meet Personalization at Test Time](https://arxiv.org/abs/2506.06254)
Append: [Movie Facts and Fibs (MF$^2$): A Benchmark for Long Movie Understanding](https://arxiv.org/abs/2506.06275)
Append: [CAT-LLM: Style-enhanced Large Language Models with Text Style Definition for Chinese Article-style Transfer](https://arxiv.org/abs/2401.05707)
Append: [TMT: Tri-Modal Translation between Speech, Image, and Text by Processing Different Modalities as Different Languages](https://arxiv.org/abs/2402.16021)
Append: [Multi-Agent Collaboration via Cross-Team Orchestration](https://arxiv.org/abs/2406.08979)
Append: [Opt-Out: Investigating Entity-Level Unlearning for Large Language Models via Optimal Transport](https://arxiv.org/abs/2406.12329)
Append: [HIGHT: Hierarchical Graph Tokenization for Molecule-Language Alignment](https://arxiv.org/abs/2406.14021)
Append: [Banyan: Improved Representation Learning with Explicit Structure](https://arxiv.org/abs/2407.17771)
Append: [A semantic embedding space based on large language models for modelling human beliefs](https://arxiv.org/abs/2408.07237)
Append: [Where is the signal in tokenization space?](https://arxiv.org/abs/2408.08541)
Append: [WER We Stand: Benchmarking Urdu ASR Models](https://arxiv.org/abs/2409.11252)
Append: [Judgment of Learning: A Human Ability Beyond Generative Artificial Intelligence](https://arxiv.org/abs/2410.13392)
Append: [The Impact of Inference Acceleration on Bias of LLMs](https://arxiv.org/abs/2410.22118)
Append: [Unveiling Topological Structures from Language: A Comprehensive Survey of Topological Data Analysis Applications in NLP](https://arxiv.org/abs/2411.10298)
Append: [The Synergy of LLMs & RL Unlocks Offline Learning of Generalizable Language-Conditioned Policies with Low-fidelity Data](https://arxiv.org/abs/2412.06877)
Append: [Benchmarking and Improving Large Vision-Language Models for Fundamental Visual Graph Understanding and Reasoning](https://arxiv.org/abs/2412.13540)
Append: [Reasoning Through Execution: Unifying Process and Outcome Rewards for Code Generation](https://arxiv.org/abs/2412.15118)
Append: [ResearchTown: Simulator of Human Research Community](https://arxiv.org/abs/2412.17767)
Append: [MapEval: A Map-Based Evaluation of Geo-Spatial Reasoning in Foundation Models](https://arxiv.org/abs/2501.00316)
Append: [GRASP: Replace Redundant Layers with Adaptive Singular Parameters for Efficient Model Compression](https://arxiv.org/abs/2501.00339)
Append: [Emergent Response Planning in LLMs](https://arxiv.org/abs/2502.06258)
Append: [DPO-Shift: Shifting the Distribution of Direct Preference Optimization](https://arxiv.org/abs/2502.07599)
Append: [Lost in the Passage: Passage-level In-context Learning Does Not Necessarily Need a "Passage"](https://arxiv.org/abs/2502.10634)
Append: [Towards Effective Extraction and Evaluation of Factual Claims](https://arxiv.org/abs/2502.10855)
Append: [TituLLMs: A Family of Bangla LLMs with Comprehensive Benchmarking](https://arxiv.org/abs/2502.11187)
Append: [On the Query Complexity of Verifier-Assisted Language Generation](https://arxiv.org/abs/2502.12123)
Append: [The Canary's Echo: Auditing Privacy Risks of LLM-Generated Synthetic Text](https://arxiv.org/abs/2502.14921)
Append: [MimeQA: Towards Socially-Intelligent Nonverbal Foundation Models](https://arxiv.org/abs/2502.16671)
Append: [Improving Customer Service with Automatic Topic Detection in User Emails](https://arxiv.org/abs/2502.19115)
Append: [Emergent Symbolic Mechanisms Support Abstract Reasoning in Large Language Models](https://arxiv.org/abs/2502.20332)
Append: [GraphCheck: Multi-Path Fact-Checking with Entity-Relationship Graphs](https://arxiv.org/abs/2502.20785)
Append: [Adversarial Tokenization](https://arxiv.org/abs/2503.02174)
Append: [TRACT: Regression-Aware Fine-tuning Meets Chain-of-Thought Reasoning for LLM-as-a-Judge](https://arxiv.org/abs/2503.04381)
Append: [DiMA: An LLM-Powered Ride-Hailing Assistant at DiDi](https://arxiv.org/abs/2503.04768)
Append: [Taming Knowledge Conflicts in Language Models](https://arxiv.org/abs/2503.10996)
Append: [Reasoning Towards Fairness: Mitigating Bias in Language Models through Reasoning-Guided Fine-Tuning](https://arxiv.org/abs/2504.05632)
Append: [Analyzing LLMs' Knowledge Boundary Cognition Across Languages Through the Lens of Internal Representations](https://arxiv.org/abs/2504.13816)
Append: [Detect, Explain, Escalate: Low-Carbon Dialogue Breakdown Management for LLM-Powered Agents](https://arxiv.org/abs/2504.18839)
Append: [m-KAILIN: Knowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training](https://arxiv.org/abs/2504.19565)
Append: [Automated Journalistic Questions: A New Method for Extracting 5W1H in French](https://arxiv.org/abs/2505.14804)
Append: [Efficient and Direct Duplex Modeling for Speech-to-Speech Language Model](https://arxiv.org/abs/2505.15670)
Append: [IDA-Bench: Evaluating LLMs on Interactive Guided Data Analysis](https://arxiv.org/abs/2505.18223)
Append: [MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain Chatbots and Dialogue Evaluators](https://arxiv.org/abs/2505.22777)
Append: [LGAR: Zero-Shot LLM-Guided Neural Ranking for Abstract Screening in Systematic Literature Reviews](https://arxiv.org/abs/2505.24757)
Append: [Structure Guided Large Language Model for SQL Generation](https://arxiv.org/abs/2402.13284)
Append: [Images Speak Louder than Words: Understanding and Mitigating Bias in Vision-Language Model from a Causal Mediation Perspective](https://arxiv.org/abs/2407.02814)
Append: [CoIR: A Comprehensive Benchmark for Code Information Retrieval Models](https://arxiv.org/abs/2407.02883)
Append: [Leopard: A Vision Language Model For Text-Rich Multi-Image Tasks](https://arxiv.org/abs/2410.01744)
Append: [AutoML-Agent: A Multi-Agent LLM Framework for Full-Pipeline AutoML](https://arxiv.org/abs/2410.02958)
Append: [PoisonBench: Assessing Large Language Model Vulnerability to Data Poisoning](https://arxiv.org/abs/2410.08811)
Append: [ProSec: Fortifying Code LLMs with Proactive Security Alignment](https://arxiv.org/abs/2411.12882)
Append: [Fundamental Limits of Prompt Tuning Transformers: Universality, Capacity and Efficiency](https://arxiv.org/abs/2411.16525)
Append: [Training Software Engineering Agents and Verifiers with SWE-Gym](https://arxiv.org/abs/2412.21139)
Append: [MedXpertQA: Benchmarking Expert-Level Medical Reasoning and Understanding](https://arxiv.org/abs/2501.18362)
Append: [Peri-LN: Revisiting Normalization Layer in the Transformer Architecture](https://arxiv.org/abs/2502.02732)
Append: [LLMs on the Line: Data Determines Loss-to-Loss Scaling Laws](https://arxiv.org/abs/2502.12120)
Append: [Investigating Non-Transitivity in LLM-as-a-Judge](https://arxiv.org/abs/2502.14074)
Append: [CAPability: A Comprehensive Visual Caption Benchmark for Evaluating Both Correctness and Thoroughness](https://arxiv.org/abs/2502.14914)
Append: [Jacobian Sparse Autoencoders: Sparsify Computations, Not Just Activations](https://arxiv.org/abs/2502.18147)
Append: [Instructor-Worker Large Language Model System for Policy Recommendation: a Case Study on Air Quality Analysis of the January 2025 Los Angeles Wildfires](https://arxiv.org/abs/2503.00566)
Append: [A Survey on Sparse Autoencoders: Interpreting the Internal Mechanisms of Large Language Models](https://arxiv.org/abs/2503.05613)
Append: [TiC-LM: A Web-Scale Benchmark for Time-Continual LLM Pretraining](https://arxiv.org/abs/2504.02107)
Append: [Not All Rollouts are Useful: Down-Sampling Rollouts in LLM Reinforcement Learning](https://arxiv.org/abs/2504.13818)
Append: [Infi-MMR: Curriculum-based Unlocking Multimodal Reasoning via Phased Reinforcement Learning in Multimodal Small Language Models](https://arxiv.org/abs/2505.23091)
append_entries: 167
Finish: 2025-06-09 04:34:27.742381
------------------------------------------------------
Started: 2025-06-09 06:26:40.329003
Existing_entries: 1167
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1142
Summarized using GPT-3.5-turbo
Append: [LLM in the Loop: Creating the ParaDeHate Dataset for Hate Speech Detoxification](https://arxiv.org/abs/2506.01484)
Token length: 1132
Summarized using GPT-3.5-turbo
Append: [Tug-of-war between idiom's figurative and literal meanings in LLMs](https://arxiv.org/abs/2506.01723)
Token length: 654
Summarized using GPT-3.5-turbo
Append: [Not All Jokes Land: Evaluating Large Language Models Understanding of Workplace Humor](https://arxiv.org/abs/2506.01819)
Token length: 1546
Summarized using GPT-3.5-turbo
Append: [Generalizable LLM Learning of Graph Synthetic Data with Reinforcement Learning](https://arxiv.org/abs/2506.00845)
append_entries: 4
Finish: 2025-06-09 06:26:50.586352
------------------------------------------------------
Started: 2025-06-09 08:24:27.043037
Existing_entries: 1004
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-09 08:24:27.477205
------------------------------------------------------
Started: 2025-06-09 10:18:55.652327
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-09 10:18:56.047213
------------------------------------------------------
Started: 2025-06-09 12:35:09.756193
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-09 12:35:10.199835
------------------------------------------------------
Started: 2025-06-09 14:17:44.962293
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-09 14:17:45.335246
------------------------------------------------------
Started: 2025-06-09 16:21:34.841037
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-09 16:21:35.252025
------------------------------------------------------
Started: 2025-06-09 18:24:41.511163
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-09 18:24:41.895846
------------------------------------------------------
Started: 2025-06-09 20:18:53.510189
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-09 20:18:53.952318
------------------------------------------------------
Started: 2025-06-09 22:16:28.538421
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-09 22:16:28.910521
------------------------------------------------------
Started: 2025-06-10 01:21:30.772727
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-10 01:21:31.179626
------------------------------------------------------
Started: 2025-06-10 03:16:20.160407
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-10 03:16:20.567011
------------------------------------------------------
Started: 2025-06-10 04:28:53.156548
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1133
Summarized using GPT-3.5-turbo
Append: [How Significant Are the Real Performance Gains? An Unbiased Evaluation Framework for GraphRAG](https://arxiv.org/abs/2506.06331)
Token length: 1195
Summarized using GPT-3.5-turbo
Append: [TESU-LLM: Training Speech-LLMs Without Speech via Unified Encoder Alignment](https://arxiv.org/abs/2506.06343)
Token length: 1348
Summarized using GPT-3.5-turbo
Append: [Unified Game Moderation: Soft-Prompting and LLM-Assisted Label Transfer for Resource-Efficient Toxicity Detection](https://arxiv.org/abs/2506.06347)
Token length: 1144
Summarized using GPT-3.5-turbo
Append: [Relationship Detection on Tabular Data Using Statistical Analysis and Large Language Models](https://arxiv.org/abs/2506.06371)
Token length: 1560
Summarized using GPT-3.5-turbo
Append: [Enhancing Decision-Making of Large Language Models via Actor-Critic](https://arxiv.org/abs/2506.06376)
Token length: 1472
Summarized using GPT-3.5-turbo
Append: [Detection Method for Prompt Injection by Integrating Pre-trained Model and Heuristic Feature Engineering](https://arxiv.org/abs/2506.06384)
Token length: 754
Summarized using GPT-3.5-turbo
Append: [Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models](https://arxiv.org/abs/2506.06395)
Token length: 1768
Summarized using GPT-3.5-turbo
Append: [Natural Language Interaction with Databases on Edge Devices in the Internet of Battlefield Things](https://arxiv.org/abs/2506.06396)
Token length: 1534
Summarized using GPT-3.5-turbo
Append: [Direct Behavior Optimization: Unlocking the Potential of Lightweight LLMs](https://arxiv.org/abs/2506.06401)
Token length: 1202
Summarized using GPT-3.5-turbo
Append: [Unintended Harms of Value-Aligned LLMs: Psychological and Empirical Insights](https://arxiv.org/abs/2506.06404)
Token length: 1019
Summarized using GPT-3.5-turbo
Append: [SMAR: Soft Modality-Aware Routing Strategy for MoE-based Multimodal Large Language Models Preserving Language Capabilities](https://arxiv.org/abs/2506.06406)
Token length: 1333
Summarized using GPT-3.5-turbo
Append: [Canonical Autoregressive Generation](https://arxiv.org/abs/2506.06446)
Token length: 1127
Summarized using GPT-3.5-turbo
Append: [What Is Seen Cannot Be Unseen: The Disruptive Effect of Knowledge Conflict on Large Language Models](https://arxiv.org/abs/2506.06485)
Token length: 1242
Summarized using GPT-3.5-turbo
Append: [Improving LLM-Powered EDA Assistants with RAFT](https://arxiv.org/abs/2506.06500)
Token length: 1336
Summarized using GPT-3.5-turbo
Append: [Biases Propagate in Encoder-based Vision-Language Models: A Systematic Analysis From Intrinsic Measures to Zero-shot Retrieval Outcomes](https://arxiv.org/abs/2506.06506)
Token length: 1897
Summarized using GPT-3.5-turbo
Append: [Fixing It in Post: A Comparative Study of LLM Post-Training Data Quality and Model Performance](https://arxiv.org/abs/2506.06522)
Token length: 1401
Summarized using GPT-3.5-turbo
Append: [Beyond Facts: Evaluating Intent Hallucination in Large Language Models](https://arxiv.org/abs/2506.06539)
Token length: 1320
Summarized using GPT-3.5-turbo
Append: [LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure Profiles](https://arxiv.org/abs/2506.06561)
Token length: 1601
Summarized using GPT-3.5-turbo
Append: [Precise Information Control in Long-Form Text Generation](https://arxiv.org/abs/2506.06589)
Token length: 901
Summarized using GPT-3.5-turbo
Append: [MedCite: Can Language Models Generate Verifiable Text for Medicine?](https://arxiv.org/abs/2506.06605)
Token length: 1549
Summarized using GPT-3.5-turbo
Append: [Training-Free Tokenizer Transplantation via Orthogonal Matching Pursuit](https://arxiv.org/abs/2506.06607)
Token length: 1244
Summarized using GPT-3.5-turbo
Append: [Transferring Features Across Language Models With Model Stitching](https://arxiv.org/abs/2506.06609)
Token length: 1386
Summarized using GPT-3.5-turbo
Append: [Interpretable Depression Detection from Social Media Text Using LLM-Derived Embeddings](https://arxiv.org/abs/2506.06616)
Token length: 1226
Summarized using GPT-3.5-turbo
Append: [BriefMe: A Legal NLP Benchmark for Assisting with Legal Briefs](https://arxiv.org/abs/2506.06619)
Token length: 1070
Summarized using GPT-3.5-turbo
Append: [Psychological Counseling Cannot Be Achieved Overnight: Automated Psychological Counseling Through Multi-Session Conversations](https://arxiv.org/abs/2506.06626)
Token length: 1291
Summarized using GPT-3.5-turbo
Append: [SafeLawBench: Towards Safe Alignment of Large Language Models](https://arxiv.org/abs/2506.06636)
Token length: 1500
Summarized using GPT-3.5-turbo
Append: [Quantile Regression with Large Language Models for Price Prediction](https://arxiv.org/abs/2506.06657)
Token length: 1178
Summarized using GPT-3.5-turbo
Append: [Learning Distribution-Wise Control in Representation Space for Language Models](https://arxiv.org/abs/2506.06686)
Token length: 1403
Summarized using GPT-3.5-turbo
Append: [Dynamic and Parametric Retrieval-Augmented Generation](https://arxiv.org/abs/2506.06704)
Token length: 1199
Summarized using GPT-3.5-turbo
Append: [DivScore: Zero-Shot Detection of LLM-Generated Text in Specialized Domains](https://arxiv.org/abs/2506.06705)
Token length: 1409
Summarized using GPT-3.5-turbo
Append: [A Survey of Retentive Network](https://arxiv.org/abs/2506.06708)
Token length: 1427
Summarized using GPT-3.5-turbo
Append: [C-PATH: Conversational Patient Assistance and Triage in Healthcare System](https://arxiv.org/abs/2506.06737)
Token length: 935
Summarized using GPT-3.5-turbo
Append: [Geopolitical biases in LLMs: what are the "good" and the "bad" countries according to contemporary language models](https://arxiv.org/abs/2506.06751)
Token length: 1300
Summarized using GPT-3.5-turbo
Append: [They want to pretend not to understand: The Limits of Current LLMs in Interpreting Implicit Content of Political Discourse](https://arxiv.org/abs/2506.06775)
Token length: 1292
Summarized using GPT-3.5-turbo
Append: [Extending dependencies to the taggedPBC: Word order in transitive clauses](https://arxiv.org/abs/2506.06785)
Token length: 1602
Summarized using GPT-3.5-turbo
Append: [On the Adaptive Psychological Persuasion of Large Language Models](https://arxiv.org/abs/2506.06800)
Token length: 1242
Summarized using GPT-3.5-turbo
Append: [Label-semantics Aware Generative Approach for Domain-Agnostic Multilabel Classification](https://arxiv.org/abs/2506.06806)
Token length: 675
Summarized using GPT-3.5-turbo
Append: [Not quite Sherlock Holmes: Language model predictions do not reliably differentiate impossible from improbable events](https://arxiv.org/abs/2506.06808)
Token length: 1062
Summarized using GPT-3.5-turbo
Append: [Advancing Question Generation with Joint Narrative and Difficulty Control](https://arxiv.org/abs/2506.06812)
Token length: 805
Summarized using GPT-3.5-turbo
Append: [BTPD: A Multilingual Hand-curated Dataset of Bengali Transnational Political Discourse Across Online Communities](https://arxiv.org/abs/2506.06813)
Token length: 1250
Summarized using GPT-3.5-turbo
Append: [How do datasets, developers, and models affect biases in a low-resourced language?](https://arxiv.org/abs/2506.06816)
Token length: 1085
Summarized using GPT-3.5-turbo
Append: [Beyond Classification: Towards Speech Emotion Reasoning with Multitask AudioLLMs](https://arxiv.org/abs/2506.06820)
Token length: 1367
Summarized using GPT-3.5-turbo
Append: [Can LLMs Generate Reliable Test Case Generators? A Study on Competition-Level Programming Problems](https://arxiv.org/abs/2506.06821)
Token length: 1107
Summarized using GPT-3.5-turbo
Append: [PCoT: Persuasion-Augmented Chain of Thought for Detecting Fake News and Social Media Disinformation](https://arxiv.org/abs/2506.06842)
Token length: 1339
Summarized using GPT-3.5-turbo
Append: [Adapt Once, Thrive with Updates: Transferable Parameter-Efficient Fine-Tuning on Evolving Base Models](https://arxiv.org/abs/2506.06844)
Token length: 1109
Summarized using GPT-3.5-turbo
Append: [Right Is Not Enough: The Pitfalls of Outcome Supervision in Training LLMs for Math Reasoning](https://arxiv.org/abs/2506.06877)
Token length: 1015
Summarized using GPT-3.5-turbo
Append: [Mixture of Small and Large Models for Chinese Spelling Check](https://arxiv.org/abs/2506.06887)
Token length: 1023
Summarized using GPT-3.5-turbo
Append: [Automatic Speech Recognition of African American English: Lexical and Contextual Effects](https://arxiv.org/abs/2506.06888)
Token length: 797
Summarized using GPT-3.5-turbo
Append: [Hybrid Extractive Abstractive Summarization for Multilingual Sentiment Analysis](https://arxiv.org/abs/2506.06929)
Token length: 1117
Summarized using GPT-3.5-turbo
Append: [DiscoSum: Discourse-aware News Summarization](https://arxiv.org/abs/2506.06930)
Append: [What Makes a Good Natural Language Prompt?](https://arxiv.org/abs/2506.06950)
Append: [BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning](https://arxiv.org/abs/2506.06955)
Append: [Learning to Clarify by Reinforcement Learning Through Reward-Weighted Fine-Tuning](https://arxiv.org/abs/2506.06964)
Append: [A dependently-typed calculus of event telicity and culminativity](https://arxiv.org/abs/2506.06968)
Append: [Break-The-Chain: Reasoning Failures in LLMs via Adversarial Prompting in Code Generation](https://arxiv.org/abs/2506.06971)
Append: [Atomic Reasoning for Scientific Table Claim Verification](https://arxiv.org/abs/2506.06972)
Append: [Chain of Methodologies: Scaling Test Time Computation without Training](https://arxiv.org/abs/2506.06982)
Append: [Cultural Bias Matters: A Cross-Cultural Benchmark Dataset and Sentiment-Enriched Model for Understanding Multimodal Metaphors](https://arxiv.org/abs/2506.06987)
Append: [What makes Reasoning Models Different? Follow the Reasoning Leader for Efficient Decoding](https://arxiv.org/abs/2506.06998)
Append: [Adversarial Paraphrasing: A Universal Attack for Humanizing AI-Generated Text](https://arxiv.org/abs/2506.07001)
Append: [A Culturally-diverse Multilingual Multimodal Video Benchmark & Model](https://arxiv.org/abs/2506.07032)
Append: [KG2QA: Knowledge Graph-enhanced Retrieval-Augmented Generation for Communication Standards Question Answering](https://arxiv.org/abs/2506.07037)
Append: [Reasoning with RAGged events: RAG-Enhanced Event Knowledge Base Construction and reasoning with proof-assistants](https://arxiv.org/abs/2506.07042)
Append: [Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning](https://arxiv.org/abs/2506.07044)
Append: [Com$^2$: A Causal-Guided Benchmark for Exploring Complex Commonsense Reasoning in Large Language Models](https://arxiv.org/abs/2506.07064)
Append: [Representation Decomposition for Learning Similarity and Contrastness Across Modalities for Affective Computing](https://arxiv.org/abs/2506.07086)
Append: [How Far Are We from Optimal Reasoning Efficiency?](https://arxiv.org/abs/2506.07104)
Append: [Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive, and Inductive Reasoning in Language Models](https://arxiv.org/abs/2506.07106)
Append: [Prompting Science Report 2: The Decreasing Value of Chain of Thought in Prompting](https://arxiv.org/abs/2506.07142)
Append: [Semantic-preserved Augmentation with Confidence-weighted Fine-tuning for Aspect Category Sentiment Analysis](https://arxiv.org/abs/2506.07148)
Append: [Syntactic Control of Language Models by Posterior Inference](https://arxiv.org/abs/2506.07154)
Append: [GeometryZero: Improving Geometry Solving for LLM with Group Contrastive Policy Optimization](https://arxiv.org/abs/2506.07160)
Append: [CTDGSI: A comprehensive exploitation of instance selection methods for automatic text classification. VII Concurso de Teses, Disserta\c{c}\~oes e Trabalhos de Gradua\c{c}\~ao em SI -- XXI Simp\'osio Brasileiro de Sistemas de Informa\c{c}\~ao](https://arxiv.org/abs/2506.07169)
Append: [RULE: Reinforcement UnLEarning Achieves Forget-Retain Pareto Optimality](https://arxiv.org/abs/2506.07171)
Append: [Flattery in Motion: Benchmarking and Analyzing Sycophancy in Video-LLMs](https://arxiv.org/abs/2506.07180)
Append: [SDE-SQL: Enhancing Text-to-SQL Generation in Large Language Models via Self-Driven Exploration with SQL Probes](https://arxiv.org/abs/2506.07245)
Append: [Improving the Efficiency of Long Document Classification using Sentence Ranking Approach](https://arxiv.org/abs/2506.07248)
Append: [Bias Attribution in Filipino Language Models: Extending a Bias Interpretability Metric for Application on Agglutinative Languages](https://arxiv.org/abs/2506.07249)
Append: [Question Answering under Temporal Conflict: Evaluating and Organizing Evolving Knowledge with LLMs](https://arxiv.org/abs/2506.07270)
Append: [Parsing the Switch: LLM-Based UD Annotation for Complex Code-Switched and Low-Resource Languages](https://arxiv.org/abs/2506.07274)
Append: [Exploring the Impact of Temperature on Large Language Models:Hot or Cold?](https://arxiv.org/abs/2506.07295)
Append: [Subjectivity in the Annotation of Bridging Anaphora](https://arxiv.org/abs/2506.07297)
Append: [ConfQA: Answer Only If You Are Confident](https://arxiv.org/abs/2506.07309)
Append: [Reward Model Interpretability via Optimal and Pessimal Tokens](https://arxiv.org/abs/2506.07326)
Append: [Improving LLM Reasoning through Interpretable Role-Playing Steering](https://arxiv.org/abs/2506.07335)
Append: [Refusal-Feature-guided Teacher for Safe Finetuning via Data Filtering and Alignment Distillation](https://arxiv.org/abs/2506.07356)
Append: [SEED: Enhancing Text-to-SQL Performance and Practical Usability Through Automatic Evidence Generation](https://arxiv.org/abs/2506.07423)
Append: [Plug-in and Fine-tuning: Bridging the Gap between Small Language Models and Large Language Models](https://arxiv.org/abs/2506.07424)
Append: [Conjoined Predication and Scalar Implicature](https://arxiv.org/abs/2506.07429)
Append: [Well Begun is Half Done: Low-resource Preference Alignment by Weak-to-Strong Decoding](https://arxiv.org/abs/2506.07434)
Append: [LG-ANNA-Embedding technical report](https://arxiv.org/abs/2506.07438)
Append: [Understanding Cross-Domain Adaptation in Low-Resource Topic Modeling](https://arxiv.org/abs/2506.07453)
Append: [KScope: A Framework for Characterizing the Knowledge Status of Language Models](https://arxiv.org/abs/2506.07458)
Append: [From Calibration to Collaboration: LLM Uncertainty Quantification Should Be More Human-Centered](https://arxiv.org/abs/2506.07461)
Append: [CCI4.0: A Bilingual Pretraining Dataset for Enhancing Reasoning in Large Language Models](https://arxiv.org/abs/2506.07463)
Append: [Improving Fairness of Large Language Models in Multi-document Summarization](https://arxiv.org/abs/2506.07479)
Append: [A Hybrid GA LLM Framework for Structured Task Optimization](https://arxiv.org/abs/2506.07483)
Append: [DEBATE: A Dataset for Disentangling Textual Ambiguity in Mandarin Through Speech](https://arxiv.org/abs/2506.07502)
Append: [What Do Indonesians Really Need from Language Technology? A Nationwide Survey](https://arxiv.org/abs/2506.07506)
Append: [DeRAGEC: Denoising Named Entity Candidates with Synthetic Rationale for ASR Error Correction](https://arxiv.org/abs/2506.07510)
Append: [Towards Large Language Models with Self-Consistent Natural Language Explanations](https://arxiv.org/abs/2506.07523)
Append: [Bit-level BPE: Below the byte boundary](https://arxiv.org/abs/2506.07541)
Append: [SELT: Self-Evaluation Tree Search for LLMs with Task Decomposition](https://arxiv.org/abs/2506.07557)
Append: [Beyond the Sentence: A Survey on Context-Aware Machine Translation with Large Language Models](https://arxiv.org/abs/2506.07583)
Append: [Instructing Large Language Models for Low-Resource Languages: A Systematic Study for Basque](https://arxiv.org/abs/2506.07597)
Append: [PolitiSky24: U.S. Political Bluesky Dataset with User Stance Labels](https://arxiv.org/abs/2506.07606)
Append: [Vuyko Mistral: Adapting LLMs for Low-Resource Dialectal Translation](https://arxiv.org/abs/2506.07617)
Append: [LoRMA: Low-Rank Multiplicative Adaptation for LLMs](https://arxiv.org/abs/2506.07621)
Append: [Intent Matters: Enhancing AI Tutoring with Fine-Grained Pedagogical Intent Annotation](https://arxiv.org/abs/2506.07626)
Append: [Unblocking Fine-Grained Evaluation of Detailed Captions: An Explaining AutoRater and Critic-and-Revise Pipeline](https://arxiv.org/abs/2506.07631)
Append: [TreeReview: A Dynamic Tree of Questions Framework for Deep and Efficient LLM-based Scientific Peer Review](https://arxiv.org/abs/2506.07642)
Append: [Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models](https://arxiv.org/abs/2506.07645)
Append: [Transcript-Prompted Whisper with Dictionary-Enhanced Decoding for Japanese Speech Annotation](https://arxiv.org/abs/2506.07646)
Append: [Beyond Benchmarks: A Novel Framework for Domain-Specific LLM Evaluation and Knowledge Mapping](https://arxiv.org/abs/2506.07658)
Append: [Synthesis by Design: Controlled Data Generation via Structural Guidance](https://arxiv.org/abs/2506.07664)
Append: [Silencing Empowerment, Allowing Bigotry: Auditing the Moderation of Hate Speech on Twitch](https://arxiv.org/abs/2506.07667)
Append: [GaRAGe: A Benchmark with Grounding Annotations for RAG Evaluation](https://arxiv.org/abs/2506.07671)
Append: [Training Superior Sparse Autoencoders for Instruct Models](https://arxiv.org/abs/2506.07691)
Append: [Through the Valley: Path to Effective Long CoT Training for Small Language Models](https://arxiv.org/abs/2506.07712)
Append: [Multilingual Grammatical Error Annotation: Combining Language-Agnostic Framework with Language-Specific Flexibility](https://arxiv.org/abs/2506.07719)
Append: [Swiss Parliaments Corpus Re-Imagined (SPC_R): Enhanced Transcription with RAG-based Correction and Predicted BLEU](https://arxiv.org/abs/2506.07726)
Append: [Augmenting LLMs' Reasoning by Reinforcing Abstract Thinking](https://arxiv.org/abs/2506.07751)
Append: [LLM Unlearning Should Be Form-Independent](https://arxiv.org/abs/2506.07795)
Append: [MultiMatch: Multihead Consistency Regularization Matching for Semi-Supervised Text Classification](https://arxiv.org/abs/2506.07801)
Append: [WebUIBench: A Comprehensive Benchmark for Evaluating Multimodal Large Language Models in WebUI-to-Code](https://arxiv.org/abs/2506.07818)
Append: [Learning to Focus: Causal Attention Distillation via Gradient-Guided Token Pruning](https://arxiv.org/abs/2506.07851)
Append: [MEMOIR: Lifelong Model Editing with Minimal Overwrite and Informed Retention for LLMs](https://arxiv.org/abs/2506.07899)
Append: [MiniCPM4: Ultra-Efficient LLMs on End Devices](https://arxiv.org/abs/2506.07900)
Append: [Quantum Graph Transformer for NLP Sentiment Classification](https://arxiv.org/abs/2506.07937)
Append: [Statistical Hypothesis Testing for Auditing Robustness in Language Models](https://arxiv.org/abs/2506.07947)
Append: [Language Models over Canonical Byte-Pair Encodings](https://arxiv.org/abs/2506.07956)
Append: [Correlated Errors in Large Language Models](https://arxiv.org/abs/2506.07962)
Append: [Reinforcement Pre-Training](https://arxiv.org/abs/2506.08007)
Append: [GLProtein: Global-and-Local Structure Aware Protein Representation Learning](https://arxiv.org/abs/2506.06294)
Append: [dLLM-Cache: Accelerating Diffusion Large Language Models with Adaptive Caching](https://arxiv.org/abs/2506.06295)
Append: [How Malicious AI Swarms Can Threaten Democracy](https://arxiv.org/abs/2506.06299)
Append: [Reward Is Enough: LLMs Are In-Context Reinforcement Learners](https://arxiv.org/abs/2506.06303)
Append: [DISRetrieval: Harnessing Discourse Structure for Long Document Retrieval](https://arxiv.org/abs/2506.06313)
Append: [Is BERTopic Better than PLSA for Extracting Key Topics in Aviation Safety Reports?](https://arxiv.org/abs/2506.06328)
Append: [The Hype Index: an NLP-driven Measure of Market News Attention](https://arxiv.org/abs/2506.06329)
Append: [FinBERT2: A Specialized Bidirectional Encoder for Bridging the Gap in Finance-Specific Deployment of Large Language Models](https://arxiv.org/abs/2506.06335)
Append: [Optimizing RAG Pipelines for Arabic: A Systematic Analysis of Core Components](https://arxiv.org/abs/2506.06339)
Append: [LLMs as World Models: Data-Driven and Human-Centered Pre-Event Simulation for Disaster Impact Assessment](https://arxiv.org/abs/2506.06355)
Append: [On the Fundamental Impossibility of Hallucination Control in Large Language Models](https://arxiv.org/abs/2506.06382)
Append: [From Rogue to Safe AI: The Role of Explicit Refusals in Aligning LLMs with International Humanitarian Law](https://arxiv.org/abs/2506.06391)
Append: [HeavyWater and SimplexWater: Watermarking Low-Entropy Text Distributions](https://arxiv.org/abs/2506.06409)
Append: [Large Language Models Can Be a Viable Substitute for Expert Political Surveys When a Shock Disrupts Traditional Measurement Approaches](https://arxiv.org/abs/2506.06540)
Append: [Future of Work with AI Agents: Auditing Automation and Augmentation Potential across the U.S. Workforce](https://arxiv.org/abs/2506.06576)
Append: [Towards Efficient Multi-LLM Inference: Characterization and Analysis of LLM Routing and Hierarchical Techniques](https://arxiv.org/abs/2506.06579)
Append: [Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning](https://arxiv.org/abs/2506.06632)
Append: [Contextual Experience Replay for Self-Improvement of Language Agents](https://arxiv.org/abs/2506.06698)
Append: [MarginSel : Max-Margin Demonstration Selection for LLMs](https://arxiv.org/abs/2506.06699)
Append: [Mitigating Object Hallucination via Robust Local Perception Search](https://arxiv.org/abs/2506.06729)
Append: [Cross-Entropy Games for Language Models: From Implicit Knowledge to General Capability Measures](https://arxiv.org/abs/2506.06832)
Append: [Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering](https://arxiv.org/abs/2506.06905)
Append: [The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity](https://arxiv.org/abs/2506.06941)
Append: [Auditing Black-Box LLM APIs with a Rank-Based Uniformity Test](https://arxiv.org/abs/2506.06975)
Append: [HauntAttack: When Attack Follows Reasoning as a Shadow](https://arxiv.org/abs/2506.07031)
Append: [Interpretable and Reliable Detection of AI-Generated Images via Grounded Reasoning in MLLMs](https://arxiv.org/abs/2506.07045)
Append: [Learning Compact Vision Tokens for Efficient Large Multimodal Models](https://arxiv.org/abs/2506.07138)
Append: [Efficient Text-Attributed Graph Learning through Selective Annotation and Graph Alignment](https://arxiv.org/abs/2506.07168)
Append: [Mitigating Behavioral Hallucination in Multimodal Large Language Models for Sequential Images](https://arxiv.org/abs/2506.07184)
Append: [SAP-Bench: Benchmarking Multimodal Large Language Models in Surgical Action Planning](https://arxiv.org/abs/2506.07196)
Append: [Hallucination at a Glance: Controlled Visual Edits and Fine-Grained Multimodal Learning](https://arxiv.org/abs/2506.07227)
Append: [Reducing Object Hallucination in Large Audio-Language Models via Audio-Aware Decoding](https://arxiv.org/abs/2506.07233)
Append: [Multi-Step Visual Reasoning with Visual Tokens Scaling and Verification](https://arxiv.org/abs/2506.07235)
Append: [G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems](https://arxiv.org/abs/2506.07398)
Append: [Beyond Jailbreaks: Revealing Stealthier and Broader LLM Security Risks Stemming from Alignment Failures](https://arxiv.org/abs/2506.07402)
Append: [LlamaRec-LKG-RAG: A Single-Pass, Learnable Knowledge Graph-RAG Framework for LLM-Based Ranking](https://arxiv.org/abs/2506.07449)
Append: [When Style Breaks Safety: Defending Language Models Against Superficial Style Alignment](https://arxiv.org/abs/2506.07452)
Append: [GLOS: Sign Language Generation with Temporally Aligned Gloss-Level Conditioning](https://arxiv.org/abs/2506.07460)
Append: [Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models](https://arxiv.org/abs/2506.07468)
Append: [Graph-of-Causal Evolution: Challenging Chain-of-Model for Reasoning](https://arxiv.org/abs/2506.07501)
Append: [Speaker-Distinguishable CTC: Learning Speaker Distinction Using CTC for Multi-Talker Speech Recognition](https://arxiv.org/abs/2506.07515)
Append: [ChemAgent: Enhancing LLMs for Chemistry and Materials Science through Tree-Search Based Tool Learning](https://arxiv.org/abs/2506.07551)
Append: [SAFEFLOW: A Principled Protocol for Trustworthy and Transactional Autonomous Agent Systems](https://arxiv.org/abs/2506.07564)
Append: [Learning Speaker-Invariant Visual Features for Lipreading](https://arxiv.org/abs/2506.07572)
Append: [E-LDA: Toward Interpretable LDA Topic Models with Strong Guarantees in Logarithmic Parallel Time](https://arxiv.org/abs/2506.07747)
Append: [Improving large language models with concept-aware fine-tuning](https://arxiv.org/abs/2506.07833)
Append: [Evaluating Large Language Models on the Frame and Symbol Grounding Problems: A Zero-shot Benchmark](https://arxiv.org/abs/2506.07896)
Append: [LUCIFER: Language Understanding and Context-Infused Framework for Exploration and Behavior Refinement](https://arxiv.org/abs/2506.07915)
Append: [Uncovering the Functional Roles of Nonlinearity in Memory](https://arxiv.org/abs/2506.07919)
Append: [Solving Inequality Proofs with Large Language Models](https://arxiv.org/abs/2506.07927)
Append: [Mimicking or Reasoning: Rethinking Multi-Modal In-Context Learning in Vision-Language Models](https://arxiv.org/abs/2506.07936)
Append: [ProtocolLLM: RTL Benchmark for SystemVerilog Generation of Communication Protocols](https://arxiv.org/abs/2506.07945)
Append: [Reinforcing Multimodal Understanding and Generation with Dual Self-rewards](https://arxiv.org/abs/2506.07963)
Append: [HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization](https://arxiv.org/abs/2506.07972)
Append: [$\tau^2$-Bench: Evaluating Conversational Agents in a Dual-Control Environment](https://arxiv.org/abs/2506.07982)
Append: [Reparameterized LLM Training via Orthogonal Equivalence Transformation](https://arxiv.org/abs/2506.08001)
Append: [Play to Generalize: Learning to Reason Through Game Play](https://arxiv.org/abs/2506.08011)
Append: [Highly Fast Text Segmentation With Pairwise Markov Chains](https://arxiv.org/abs/2102.11037)
Append: [ViMMRC 2.0 -- Enhancing Machine Reading Comprehension on Vietnamese Literature Text](https://arxiv.org/abs/2303.18162)
Append: [Rational Decision-Making Agent with Internalized Utility Judgment](https://arxiv.org/abs/2308.12519)
Append: [AfroBench: How Good are Large Language Models on African Languages?](https://arxiv.org/abs/2311.07978)
Append: [When Attention Collapses: How Degenerate Layers in LLMs Enable Smaller, Stronger Models](https://arxiv.org/abs/2404.08634)
Append: [Can Perplexity Predict Fine-tuning Performance? An Investigation of Tokenization Effects on Sequential Language Models for Nepali](https://arxiv.org/abs/2404.18071)
Append: [Knowledge-to-Jailbreak: Investigating Knowledge-driven Jailbreaking Attacks for Large Language Models](https://arxiv.org/abs/2406.11682)
Append: [ZeroDL: Zero-shot Distribution Learning for Text Clustering via Large Language Models](https://arxiv.org/abs/2406.13342)
Append: [AutoPal: Autonomous Adaptation to Users for Personal AI Companionship](https://arxiv.org/abs/2406.13960)
Append: [Modality-Specialized Synergizers for Interleaved Vision-Language Generalists](https://arxiv.org/abs/2407.03604)
Append: [Diversifying the Expert Knowledge for Task-Agnostic Pruning in Sparse Mixture-of-Experts](https://arxiv.org/abs/2407.09590)
Append: [Cool-Fusion: Fuse Large Language Models without Training](https://arxiv.org/abs/2407.19807)
Append: [Synergizing Unsupervised Episode Detection with LLMs for Large-Scale News Events](https://arxiv.org/abs/2408.04873)
Append: [WeQA: A Benchmark for Retrieval Augmented Generation in Wind Energy Domain](https://arxiv.org/abs/2408.11800)
Append: [Open-FinLLMs: Open Multimodal Large Language Models for Financial Applications](https://arxiv.org/abs/2408.11878)
Append: [PECAN: LLM-Guided Dynamic Progress Control with Attention-Guided Hierarchical Weighted Graph for Long-Document QA](https://arxiv.org/abs/2410.04790)
Append: [TorchTitan: One-stop PyTorch native solution for production ready LLM pre-training](https://arxiv.org/abs/2410.06511)
Append: [Efficient Pretraining Data Selection for Language Models via Multi-Actor Collaboration](https://arxiv.org/abs/2410.08102)
Append: [Assessing Dialect Fairness and Robustness of Large Language Models in Reasoning Tasks](https://arxiv.org/abs/2410.11005)
Append: [LabSafety Bench: Benchmarking LLMs on Safety Issues in Scientific Labs](https://arxiv.org/abs/2410.14182)
Append: [SudoLM: Learning Access Control of Parametric Knowledge with Authorization Alignment](https://arxiv.org/abs/2410.14676)
Append: [Diversity Explains Inference Scaling Laws: Through a Case Study of Minimum Bayes Risk Decoding](https://arxiv.org/abs/2410.15021)
Append: [Value Residual Learning](https://arxiv.org/abs/2410.17897)
Append: [Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for Measuring the Capabilities of Spoken Language Models with 180 Tasks](https://arxiv.org/abs/2411.05361)
Append: [Epistemic Integrity in Large Language Models](https://arxiv.org/abs/2411.06528)
Append: [Enhancing Character-Level Understanding in LLMs through Token Internal Structure Learning](https://arxiv.org/abs/2411.17679)
Append: [MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache](https://arxiv.org/abs/2411.18077)
Append: [LLMs Can Simulate Standardized Patients via Agent Coevolution](https://arxiv.org/abs/2412.11716)
Append: [Evaluating Zero-Shot Multilingual Aspect-Based Sentiment Analysis with Large Language Models](https://arxiv.org/abs/2412.12564)
Append: [DISC: Plug-and-Play Decoding Intervention with Similarity of Characters for Chinese Spelling Check](https://arxiv.org/abs/2412.12863)
Append: [Performance Gap in Entity Knowledge Extraction Across Modalities in Vision Language Models](https://arxiv.org/abs/2412.14133)
Append: [On Adversarial Robustness of Language Models in Transfer Learning](https://arxiv.org/abs/2501.00066)
Append: [Eliciting In-context Retrieval and Reasoning for Long-context Large Language Models](https://arxiv.org/abs/2501.08248)
Append: [ExLM: Rethinking the Impact of [MASK] Tokens in Masked Language Models](https://arxiv.org/abs/2501.13397)
Append: [Unraveling Token Prediction Refinement and Identifying Essential Layers in Language Models](https://arxiv.org/abs/2501.15054)
Append: [Latent Thought Models with Variational Bayes Inference-Time Computation](https://arxiv.org/abs/2502.01567)
Append: [Token Cleaning: Fine-Grained Data Selection for LLM Supervised Fine-Tuning](https://arxiv.org/abs/2502.01968)
Append: [Minerva: A Programmable Memory Test Benchmark for Language Models](https://arxiv.org/abs/2502.03358)
Append: [Sparse Autoencoders for Hypothesis Generation](https://arxiv.org/abs/2502.04382)
Append: [Retrieval-augmented Large Language Models for Financial Time Series Forecasting](https://arxiv.org/abs/2502.05878)
Append: [RomanLens: The Role Of Latent Romanization In Multilinguality In LLMs](https://arxiv.org/abs/2502.07424)
Append: [Measuring Diversity in Synthetic Datasets](https://arxiv.org/abs/2502.08512)
Append: [Gumbel Reranking: Differentiable End-to-End Reranker Optimization](https://arxiv.org/abs/2502.11116)
Append: [CORDIAL: Can Multimodal Large Language Models Effectively Understand Coherence Relationships?](https://arxiv.org/abs/2502.11300)
Append: [Stop Looking for Important Tokens in Multimodal Language Models: Duplication Matters More](https://arxiv.org/abs/2502.11494)
Append: [AdaSplash: Adaptive Sparse Flash Attention](https://arxiv.org/abs/2502.12082)
Append: [From Sub-Ability Diagnosis to Human-Aligned Generation: Bridging the Gap for Text Length Control via MARKERGEN](https://arxiv.org/abs/2502.13544)
Append: [MMTEB: Massive Multilingual Text Embedding Benchmark](https://arxiv.org/abs/2502.13595)
Append: [Enhancing Input-Label Mapping in In-Context Learning with Contrastive Decoding](https://arxiv.org/abs/2502.13738)
Append: [From Tools to Teammates: Evaluating LLMs in Multi-Session Coding Interactions](https://arxiv.org/abs/2502.13791)
Append: [ParallelComp: Parallel Long-Context Compressor for Length Extrapolation](https://arxiv.org/abs/2502.14317)
Append: [Tree-of-Debate: Multi-Persona Debate Trees Elicit Critical Thinking for Scientific Comparative Analysis](https://arxiv.org/abs/2502.14767)
Append: [DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance](https://arxiv.org/abs/2502.16886)
Append: [NeoBERT: A Next-Generation BERT](https://arxiv.org/abs/2502.19587)
Append: [Dialogue Without Limits: Constant-Sized KV Caches for Extended Responses in LLMs](https://arxiv.org/abs/2503.00979)
Append: [Examining the Mental Health Impact of Misinformation on Social Media Using a Hybrid Transformer-Based Approach](https://arxiv.org/abs/2503.02333)
Append: [Generalized Interpolating Discrete Diffusion](https://arxiv.org/abs/2503.04482)
Append: [Sentence-level Reward Model can Generalize Better for Aligning LLM from Human Preference](https://arxiv.org/abs/2503.04793)
Append: [RONA: Pragmatically Diverse Image Captioning with Coherence Relations](https://arxiv.org/abs/2503.10997)
Append: [nvBench 2.0: Resolving Ambiguity in Text-to-Visualization through Stepwise Reasoning](https://arxiv.org/abs/2503.12880)
Append: [Right Answer, Wrong Score: Uncovering the Inconsistencies of LLM Evaluation in Multiple-Choice Question Answering](https://arxiv.org/abs/2503.14996)
Append: [Imagine to Hear: Auditory Knowledge Generation can be an Effective Assistant for Language Models](https://arxiv.org/abs/2503.16853)
Append: [sudo rm -rf agentic_security](https://arxiv.org/abs/2503.20279)
Append: [ThinkEdit: Interpretable Weight Editing to Mitigate Overly Short Thinking in Reasoning Models](https://arxiv.org/abs/2503.22048)
Append: [Taxonomizing Representational Harms using Speech Act Theory](https://arxiv.org/abs/2504.00928)
Append: [Legal Mathematical Reasoning with LLMs: Procedural Alignment through Two-Stage Reinforcement Learning](https://arxiv.org/abs/2504.02590)
Append: [Can LLMs Interpret and Leverage Structured Linguistic Representations? A Case Study with AMRs](https://arxiv.org/abs/2504.04745)
Append: [LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models](https://arxiv.org/abs/2504.10415)
Append: [A UD Treebank for Bohairic Coptic](https://arxiv.org/abs/2504.18386)
Append: [Alignment Drift in CEFR-prompted LLMs for Interactive Spanish Tutoring](https://arxiv.org/abs/2505.08351)
Append: [BLEUBERI: BLEU is a surprisingly effective reward for instruction following](https://arxiv.org/abs/2505.11080)
Append: [Introspective Growth: Automatically Advancing LLM Expertise in Technology Judgment](https://arxiv.org/abs/2505.12452)
Append: [Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models](https://arxiv.org/abs/2505.14599)
Append: [General-Reasoner: Advancing LLM Reasoning Across All Domains](https://arxiv.org/abs/2505.14652)
Append: [Mechanistic evaluation of Transformers and state space models](https://arxiv.org/abs/2505.15105)
Append: [Power-Law Decay Loss for Large Language Model Finetuning: A Theory Perspective](https://arxiv.org/abs/2505.16900)
Append: [Mixture of Decoding: An Attention-Inspired Adaptive Decoding Strategy to Mitigate Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2505.17061)
Append: [EVADE: Multimodal Benchmark for Evasive Content Detection in E-Commerce Applications](https://arxiv.org/abs/2505.17654)
Append: [Towards Dynamic Theory of Mind: Evaluating LLM Adaptation to Temporal Evolution of Human States](https://arxiv.org/abs/2505.17663)
Append: [When Two LLMs Debate, Both Think They'll Win](https://arxiv.org/abs/2505.19184)
Append: [Compliance-to-Code: Enhancing Financial Compliance Checking via Code Generation](https://arxiv.org/abs/2505.19804)
Append: [APE: Selective Fine-tuning with Acceptance Criteria for Language Model Adaptation](https://arxiv.org/abs/2505.19912)
Append: [Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles](https://arxiv.org/abs/2505.19914)
Append: [BioHopR: A Benchmark for Multi-Hop, Multi-Answer Reasoning in Biomedical Domain](https://arxiv.org/abs/2505.22240)
Append: [WorkForceAgent-R1: Incentivizing Reasoning Capability in LLM-based Web Agents via Reinforcement Learning](https://arxiv.org/abs/2505.22942)
Append: [Large Language Models Often Know When They Are Being Evaluated](https://arxiv.org/abs/2505.23836)
Append: [Diversity of Transformer Layers: One Aspect of Parameter Scaling Laws](https://arxiv.org/abs/2505.24009)
Append: [CVC: A Large-Scale Chinese Value Rule Corpus for Value Alignment of Large Language Models](https://arxiv.org/abs/2506.01495)
Append: [SecFormer: Fast and Accurate Privacy-Preserving Inference for Transformer Models via SMPC](https://arxiv.org/abs/2401.00793)
Append: [Active Preference Optimization for Sample Efficient RLHF](https://arxiv.org/abs/2402.10500)
Append: [Unsolvable Problem Detection: Robust Understanding Evaluation for Large Multimodal Models](https://arxiv.org/abs/2403.20331)
Append: [Binary Classifier Optimization for Large Language Model Alignment](https://arxiv.org/abs/2404.04656)
Append: [JailbreakLens: Visual Analysis of Jailbreak Attacks Against Large Language Models](https://arxiv.org/abs/2404.08793)
Append: [Outlier-weighed Layerwise Sampling for LLM Fine-tuning](https://arxiv.org/abs/2405.18380)
Append: [Watermarking Language Models with Error Correcting Codes](https://arxiv.org/abs/2406.10281)
Append: [Data Shapley in One Training Run](https://arxiv.org/abs/2406.11011)
Append: [Is poisoning a real threat to LLM alignment? Maybe more so than you think](https://arxiv.org/abs/2406.12091)
Append: [Beyond Numeric Rewards: In-Context Dueling Bandits with LLM Agents](https://arxiv.org/abs/2407.01887)
Append: [Selective Prompt Anchoring for Code Generation](https://arxiv.org/abs/2408.09121)
Append: [HSF: Defending against Jailbreak Attacks with Hidden State Filtering](https://arxiv.org/abs/2409.03788)
Append: [RED QUEEN: Safeguarding Large Language Models against Concealed Multi-Turn Jailbreaking](https://arxiv.org/abs/2409.17458)
Append: [Easy2Hard-Bench: Standardized Difficulty Labels for Profiling LLM Performance and Generalization](https://arxiv.org/abs/2409.18433)
Append: [Parameter-Efficient Fine-Tuning of State Space Models](https://arxiv.org/abs/2410.09016)
Append: [How Does DPO Reduce Toxicity? A Mechanistic Neuron-Level Analysis](https://arxiv.org/abs/2411.06424)
Append: [Unveiling and Addressing Pseudo Forgetting in Large Language Models](https://arxiv.org/abs/2411.11932)
Append: [Watermark under Fire: A Robustness Evaluation of LLM Watermarking](https://arxiv.org/abs/2411.13425)
Append: [Enhancing Few-Shot Vision-Language Classification with Large Multimodal Model Features](https://arxiv.org/abs/2412.00142)
Append: [Semantic Exploration with Adaptive Gating for Efficient Problem Solving with Language Models](https://arxiv.org/abs/2501.05752)
Append: [Scalable Vision Language Model Training via High Quality Data Curation](https://arxiv.org/abs/2501.05952)
Append: [From Informal to Formal -- Incorporating and Evaluating LLMs on Natural Language Requirements to Verifiable Formal Proofs](https://arxiv.org/abs/2501.16207)
Append: [Scaling Inference-Efficient Language Models](https://arxiv.org/abs/2501.18107)
Append: [BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning](https://arxiv.org/abs/2501.18858)
Append: [DeepRAG: Thinking to Retrieve Step by Step for Large Language Models](https://arxiv.org/abs/2502.01142)
Append: [Multi-agent Architecture Search via Agentic Supernet](https://arxiv.org/abs/2502.04180)
Append: [SyncMind: Measuring Agent Out-of-Sync Recovery in Collaborative Software Engineering](https://arxiv.org/abs/2502.06994)
Append: [Automated Capability Discovery via Foundation Model Self-Exploration](https://arxiv.org/abs/2502.07577)
Append: [When Incentives Backfire, Data Stops Being Human](https://arxiv.org/abs/2502.07732)
Append: [Theoretical Benefit and Limitation of Diffusion Language Model](https://arxiv.org/abs/2502.09622)
Append: [DeltaProduct: Improving State-Tracking in Linear RNNs via Householder Products](https://arxiv.org/abs/2502.10297)
Append: [PlanGenLLMs: A Modern Survey of LLM Planning Capabilities](https://arxiv.org/abs/2502.11221)
Append: [DISC: DISC: Dynamic Decomposition Improves LLM Inference Scaling](https://arxiv.org/abs/2502.16706)
Append: [Synthetic Text Generation for Training Large Language Models via Gradient Matching](https://arxiv.org/abs/2502.17607)
Append: [AMPO: Active Multi-Preference Optimization for Self-play Preference Selection](https://arxiv.org/abs/2502.18293)
Append: [PhantomWiki: On-Demand Datasets for Reasoning and Retrieval Evaluation](https://arxiv.org/abs/2502.20377)
Append: [EgoNormia: Benchmarking Physical Social Norm Understanding](https://arxiv.org/abs/2502.20490)
Append: [HAIC: Improving Human Action Understanding and Generation with Better Captions for Multi-modal Large Language Models](https://arxiv.org/abs/2502.20811)
Append: [LLaSE-G1: Incentivizing Generalization Capability for LLaMA-based Speech Enhancement](https://arxiv.org/abs/2503.00493)
Append: [BRIDGE: Bootstrapping Text to Control Time-Series Generation via Multi-Agent Iterative Optimization and Diffusion Modeling](https://arxiv.org/abs/2503.02445)
Append: [Broaden your SCOPE! Efficient Multi-turn Conversation Planning for LLMs with Semantic Space](https://arxiv.org/abs/2503.11586)
Append: [FedALT: Federated Fine-Tuning through Adaptive Local Training with Rest-of-World LoRA](https://arxiv.org/abs/2503.11880)
Append: [AdaReTaKe: Adaptive Redundancy Reduction to Perceive Longer for Video-language Understanding](https://arxiv.org/abs/2503.12559)
Append: [EconEvals: Benchmarks and Litmus Tests for LLM Agents in Unknown Environments](https://arxiv.org/abs/2503.18825)
Append: [Exploring Training and Inference Scaling Laws in Generative Retrieval](https://arxiv.org/abs/2503.18941)
Append: [Representation Bending for Large Language Model Safety](https://arxiv.org/abs/2504.01550)
Append: [Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought](https://arxiv.org/abs/2504.05599)
Append: [MIB: A Mechanistic Interpretability Benchmark](https://arxiv.org/abs/2504.13151)
Append: [STAMP Your Content: Proving Dataset Membership via Watermarked Rephrasings](https://arxiv.org/abs/2504.13416)
Append: [A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment](https://arxiv.org/abs/2504.15585)
Append: [Robustifying Vision-Language Models via Dynamic Token Reweighting](https://arxiv.org/abs/2505.17132)
Append: [Attention with Trained Embeddings Provably Selects Important Tokens](https://arxiv.org/abs/2505.17282)
Append: [ChartGalaxy: A Dataset for Infographic Chart Understanding and Generation](https://arxiv.org/abs/2505.18668)
Append: [Can MLLMs Guide Me Home? A Benchmark Study on Fine-Grained Visual Reasoning from Transit Maps](https://arxiv.org/abs/2505.18675)
Append: [On Path to Multimodal Historical Reasoning: HistBench and HistAgent](https://arxiv.org/abs/2505.20246)
Append: [Scaling over Scaling: Exploring Test-Time Scaling Plateau in Large Reasoning Models](https://arxiv.org/abs/2505.20522)
Append: [AI Scientists Fail Without Strong Implementation Capability](https://arxiv.org/abs/2506.01372)
append_entries: 335
Finish: 2025-06-10 04:30:37.340953
------------------------------------------------------
Started: 2025-06-10 06:25:35.476474
Existing_entries: 1335
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1110
Summarized using GPT-3.5-turbo
Append: [Aligned but Blind: Alignment Increases Implicit Bias by Reducing Awareness of Race](https://arxiv.org/abs/2506.00253)
Token length: 1467
Summarized using GPT-3.5-turbo
Append: [SATA-BENCH: Select All That Apply Benchmark for Multiple Choice Questions](https://arxiv.org/abs/2506.00643)
Token length: 1402
Summarized using GPT-3.5-turbo
Append: [Understanding and Mitigating Cross-lingual Privacy Leakage via Language-specific and Universal Privacy Neurons](https://arxiv.org/abs/2506.00759)
Token length: 1233
Summarized using GPT-3.5-turbo
Append: [NTPP: Generative Speech Language Modeling for Dual-Channel Spoken Dialogue via Next-Token-Pair Prediction](https://arxiv.org/abs/2506.00975)
Token length: 1568
Summarized using GPT-3.5-turbo
Append: [CoT is Not True Reasoning, It Is Just a Tight Constraint to Imitate: A Theory Perspective](https://arxiv.org/abs/2506.02878)
Token length: 932
Summarized using GPT-3.5-turbo
Append: [Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective](https://arxiv.org/abs/2506.03038)
Token length: 1460
Summarized using GPT-3.5-turbo
Append: [SATA: A Paradigm for LLM Jailbreak via Simple Assistive Task Linkage](https://arxiv.org/abs/2412.15289)
Token length: 1169
Summarized using GPT-3.5-turbo
Append: [Grounded Persuasive Language Generation for Automated Marketing](https://arxiv.org/abs/2502.16810)
Token length: 1071
Summarized using GPT-3.5-turbo
Append: [Comba: Improving Bilinear RNNs with Closed-loop Control](https://arxiv.org/abs/2506.02475)
Token length: 1013
Summarized using GPT-3.5-turbo
Append: [Retrieval-Augmented Generation as Noisy In-Context Learning: A Unified Theory and Risk Bounds](https://arxiv.org/abs/2506.03100)
append_entries: 10
Finish: 2025-06-10 06:25:56.949819
------------------------------------------------------
Started: 2025-06-10 08:23:36.330405
Existing_entries: 1010
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-10 08:23:37.049459
------------------------------------------------------
Started: 2025-06-10 10:18:34.824332
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-10 10:18:35.504727
------------------------------------------------------
Started: 2025-06-10 12:35:26.193721
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-10 12:35:26.959875
------------------------------------------------------
Started: 2025-06-10 14:17:23.456915
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-10 14:17:24.164479
------------------------------------------------------
Started: 2025-06-10 16:21:39.126706
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-10 16:21:39.845226
------------------------------------------------------
Started: 2025-06-10 18:23:31.568342
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-10 18:23:32.310931
------------------------------------------------------
Started: 2025-06-10 20:19:01.714533
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-10 20:19:02.483642
------------------------------------------------------
Started: 2025-06-10 22:16:15.583122
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-10 22:16:16.287235
------------------------------------------------------
Started: 2025-06-11 01:21:18.317909
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-11 01:21:19.050825
------------------------------------------------------
Started: 2025-06-11 03:15:20.291069
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-11 03:15:20.966332
------------------------------------------------------
Started: 2025-06-11 04:32:29.737823
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 960
Summarized using GPT-3.5-turbo
Append: [Conservative Bias in Large Language Models: Measuring Relation Predictions](https://arxiv.org/abs/2506.08120)
Token length: 1293
Summarized using GPT-3.5-turbo
Append: [QA-LIGN: Aligning LLMs through Constitutionally Decomposed QA](https://arxiv.org/abs/2506.08123)
Token length: 1315
Summarized using GPT-3.5-turbo
Append: [EconWebArena: Benchmarking Autonomous Agents on Economic Tasks in Realistic Web Environments](https://arxiv.org/abs/2506.08136)
Token length: 1928
Summarized using GPT-3.5-turbo
Append: [Multilingual Hate Speech Detection in Social Media Using Translation-Based Approaches with Large Language Models](https://arxiv.org/abs/2506.08147)
Token length: 1608
Summarized using GPT-3.5-turbo
Append: [ETT-CKGE: Efficient Task-driven Tokens for Continual Knowledge Graph Embedding](https://arxiv.org/abs/2506.08158)
Token length: 1150
Summarized using GPT-3.5-turbo
Append: [Can Artificial Intelligence Write Like Borges? An Evaluation Protocol for Spanish Microfiction](https://arxiv.org/abs/2506.08172)
Token length: 1716
Summarized using GPT-3.5-turbo
Append: [LLM-BT: Back-Translation as a Framework for Terminology Standardization and Dynamic Semantic Embedding](https://arxiv.org/abs/2506.08174)
Token length: 1358
Summarized using GPT-3.5-turbo
Append: [Unable to forget: Proactive lnterference Reveals Working Memory Limits in LLMs Beyond Context Length](https://arxiv.org/abs/2506.08184)
Token length: 1315
Summarized using GPT-3.5-turbo
Append: ["I Wrote, I Paused, I Rewrote" Teaching LLMs to Read Between the Lines of Student Writing](https://arxiv.org/abs/2506.08221)
Token length: 1177
Summarized using GPT-3.5-turbo
Append: [Compound AI Systems Optimization: A Survey of Methods, Challenges, and Future Directions](https://arxiv.org/abs/2506.08234)
Token length: 1659
Summarized using GPT-3.5-turbo
Append: [Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\rightarrow$ Evidence Reasoning](https://arxiv.org/abs/2506.08235)
Token length: 1306
Summarized using GPT-3.5-turbo
Append: [Automatic Generation of Inference Making Questions for Reading Comprehension Assessments](https://arxiv.org/abs/2506.08260)
Token length: 1711
Summarized using GPT-3.5-turbo
Append: [Institutional Books 1.0: A 242B token dataset from Harvard Library's collections, refined for accuracy and usability](https://arxiv.org/abs/2506.08300)
Token length: 863
Summarized using GPT-3.5-turbo
Append: [Wait, We Don't Need to "Wait"! Removing Thinking Tokens Improves Reasoning Efficiency](https://arxiv.org/abs/2506.08343)
Token length: 1123
Summarized using GPT-3.5-turbo
Append: [Evaluating LLMs Across Multi-Cognitive Levels: From Medical Knowledge Mastery to Scenario-Based Problem Solving](https://arxiv.org/abs/2506.08349)
Token length: 1330
Summarized using GPT-3.5-turbo
Append: [Text Embeddings Should Capture Implicit Semantics, Not Just Surface Meaning](https://arxiv.org/abs/2506.08354)
Token length: 1588
Summarized using GPT-3.5-turbo
Append: [DEAL: Disentangling Transformer Head Activations for LLM Steering](https://arxiv.org/abs/2506.08359)
Token length: 1287
Summarized using GPT-3.5-turbo
Append: [CC-RAG: Structured Multi-Hop Reasoning via Theme-Based Causal Graphs](https://arxiv.org/abs/2506.08364)
Token length: 1092
Summarized using GPT-3.5-turbo
Append: [Mitigating Posterior Salience Attenuation in Long-Context LLMs with Positional Contrastive Decoding](https://arxiv.org/abs/2506.08371)
Token length: 1519
Summarized using GPT-3.5-turbo
Append: [Draft-based Approximate Inference for LLMs](https://arxiv.org/abs/2506.08373)
Token length: 1339
Summarized using GPT-3.5-turbo
Append: [EIFBENCH: Extremely Complex Instruction Following Benchmark for Large Language Models](https://arxiv.org/abs/2506.08375)
Token length: 1050
Summarized using GPT-3.5-turbo
Append: [mSTEB: Massively Multilingual Evaluation of LLMs on Speech and Text Tasks](https://arxiv.org/abs/2506.08400)
Token length: 1916
Summarized using GPT-3.5-turbo
Append: [TACTIC: Translation Agents with Cognitive-Theoretic Interactive Collaboration](https://arxiv.org/abs/2506.08403)
Token length: 1209
Summarized using GPT-3.5-turbo
Append: [Large Language Models Have Intrinsic Meta-Cognition, but Need a Good Lens](https://arxiv.org/abs/2506.08410)
Token length: 1277
Summarized using GPT-3.5-turbo
Append: [Know-MRI: A Knowledge Mechanisms Revealer&Interpreter for Large Language Models](https://arxiv.org/abs/2506.08427)
Token length: 1170
Summarized using GPT-3.5-turbo
Append: [CAF-I: A Collaborative Multi-Agent Framework for Enhanced Irony Detection with Large Language Models](https://arxiv.org/abs/2506.08430)
Token length: 943
Summarized using GPT-3.5-turbo
Append: [Low-resource domain adaptation while minimizing energy and hardware resource consumption](https://arxiv.org/abs/2506.08433)
Token length: 1509
Summarized using GPT-3.5-turbo
Append: [Olica: Efficient Structured Pruning of Large Language Models without Retraining](https://arxiv.org/abs/2506.08436)
Token length: 1477
Summarized using GPT-3.5-turbo
Append: [Detecting Harmful Memes with Decoupled Understanding and Guided CoT Reasoning](https://arxiv.org/abs/2506.08477)
Token length: 1272
Summarized using GPT-3.5-turbo
Append: [Efficient Context Selection for Long-Context QA: No Tuning, No Iteration, Just Adaptive-$k$](https://arxiv.org/abs/2506.08479)
Token length: 744
Summarized using GPT-3.5-turbo
Append: [Re-Thinking the Automatic Evaluation of Image-Text Alignment in Text-to-Image Models](https://arxiv.org/abs/2506.08480)
Token length: 1639
Summarized using GPT-3.5-turbo
Append: [Fairness is Not Silence: Unmasking Vacuous Neutrality in Small Language Models](https://arxiv.org/abs/2506.08487)
Token length: 800
Summarized using GPT-3.5-turbo
Append: [EtiCor++: Towards Understanding Etiquettical Bias in LLMs](https://arxiv.org/abs/2506.08488)
Token length: 1168
Summarized using GPT-3.5-turbo
Append: [Integration of Old and New Knowledge for Generalized Intent Discovery: A Consistency-driven Prototype-Prompting Framework](https://arxiv.org/abs/2506.08490)
Token length: 1095
Summarized using GPT-3.5-turbo
Append: [DRAGged into Conflicts: Detecting and Addressing Conflicting Sources in Search-Augmented LLMs](https://arxiv.org/abs/2506.08500)
Token length: 805
Summarized using GPT-3.5-turbo
Append: [CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in conversations](https://arxiv.org/abs/2506.08504)
Token length: 1346
Summarized using GPT-3.5-turbo
Append: [Efficient Post-Training Refinement of Latent Reasoning in Large Language Models](https://arxiv.org/abs/2506.08552)
Token length: 1917
Summarized using GPT-3.5-turbo
Append: [Neighbors and relatives: How do speech embeddings reflect linguistic connections across the world?](https://arxiv.org/abs/2506.08564)
Token length: 1411
Summarized using GPT-3.5-turbo
Append: [CounselBench: A Large-Scale Expert Evaluation and Adversarial Benchmark of Large Language Models in Mental Health Counseling](https://arxiv.org/abs/2506.08584)
Token length: 1036
Summarized using GPT-3.5-turbo
Append: [Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity Dilemma of Embeddings](https://arxiv.org/abs/2506.08592)
Token length: 1066
Summarized using GPT-3.5-turbo
Append: [Hateful Person or Hateful Model? Investigating the Role of Personas in Hate Speech Detection by Large Language Models](https://arxiv.org/abs/2506.08593)
Token length: 774
Summarized using GPT-3.5-turbo
Append: [RAISE: Enhancing Scientific Reasoning in LLMs via Step-by-Step Retrieval](https://arxiv.org/abs/2506.08625)
Token length: 1179
Summarized using GPT-3.5-turbo
Append: [MEMETRON: Metaheuristic Mechanisms for Test-time Response Optimization of Large Language Models](https://arxiv.org/abs/2506.08643)
Token length: 1348
Summarized using GPT-3.5-turbo
Append: [TableDreamer: Progressive and Weakness-guided Data Synthesis from Scratch for Table Instruction Tuning](https://arxiv.org/abs/2506.08646)
Token length: 718
Summarized using GPT-3.5-turbo
Append: [Summarization for Generative Relation Extraction in the Microbiome Domain](https://arxiv.org/abs/2506.08647)
Token length: 1560
Summarized using GPT-3.5-turbo
Append: [RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling](https://arxiv.org/abs/2506.08672)
Token length: 1190
Summarized using GPT-3.5-turbo
Append: [Brevity is the soul of sustainability: Characterizing LLM response lengths](https://arxiv.org/abs/2506.08686)
Token length: 1129
Summarized using GPT-3.5-turbo
Append: [ClimateViz: A Benchmark for Statistical Reasoning and Fact Verification on Scientific Charts](https://arxiv.org/abs/2506.08700)
Token length: 1145
Summarized using GPT-3.5-turbo
Append: [ConfPO: Exploiting Policy Model Confidence for Critical Token Selection in Large Language Model Preference Optimization](https://arxiv.org/abs/2506.08712)
Token length: 1118
Summarized using GPT-3.5-turbo
Append: [Explainable Compliance Detection with Multi-Hop Natural Language Inference on Assurance Case Structure](https://arxiv.org/abs/2506.08713)
Append: [Multi-Teacher Language-Aware Knowledge Distillation for Multilingual Speech Emotion Recognition](https://arxiv.org/abs/2506.08717)
Append: [Improved LLM Agents for Financial Document Question Answering](https://arxiv.org/abs/2506.08726)
Append: [Societal AI Research Has Become Less Interdisciplinary](https://arxiv.org/abs/2506.08738)
Append: [Towards Secure and Private Language Models for Nuclear Power Plants](https://arxiv.org/abs/2506.08746)
Append: [Unlocking the Potential of Large Language Models in the Nuclear Industry with Synthetic Data](https://arxiv.org/abs/2506.08750)
Append: [Factors affecting the in-context learning abilities of LLMs for dialogue state tracking](https://arxiv.org/abs/2506.08753)
Append: [Enhancing Accuracy and Maintainability in Nuclear Plant Data Retrieval: A Function-Calling LLM Approach Over NL-to-SQL](https://arxiv.org/abs/2506.08757)
Append: [AraReasoner: Evaluating Reasoning-Based LLMs for Arabic NLP](https://arxiv.org/abs/2506.08768)
Append: [The impact of fine tuning in LLaMA on hallucinations for named entity extraction in legal documentation](https://arxiv.org/abs/2506.08827)
Append: [Advancing STT for Low-Resource Real-World Speech](https://arxiv.org/abs/2506.08836)
Append: [AdversariaL attacK sAfety aLIgnment(ALKALI): Safeguarding LLMs through GRACE: Geometric Representation-Aware Contrastive Enhancement- Introducing Adversarial Vulnerability Quality Index (AVQI)](https://arxiv.org/abs/2506.08885)
Append: [PlantBert: An Open Source Language Model for Plant Science](https://arxiv.org/abs/2506.08897)
Append: [From Legal Texts to Defeasible Deontic Logic via LLMs: A Study in Automated Semantic Analysis](https://arxiv.org/abs/2506.08899)
Append: [Dialect Normalization using Large Language Models and Morphological Rules](https://arxiv.org/abs/2506.08907)
Append: [PropMEND: Hypernetworks for Knowledge Propagation in LLMs](https://arxiv.org/abs/2506.08920)
Append: [Can A Gamer Train A Mathematical Reasoning Model?](https://arxiv.org/abs/2506.08935)
Append: [FaithfulRAG: Fact-Level Conflict Modeling for Context-Faithful Retrieval-Augmented Generation](https://arxiv.org/abs/2506.08938)
Append: [Can LLMs Ground when they (Don't) Know: A Study on Direct and Loaded Political Questions](https://arxiv.org/abs/2506.08952)
Append: [Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers](https://arxiv.org/abs/2506.08966)
Append: [Atomic-to-Compositional Generalization for Mobile Agents with A New Benchmark and Scheduling System](https://arxiv.org/abs/2506.08972)
Append: [FROST-EMA: Finnish and Russian Oral Speech Dataset of Electromagnetic Articulography Measurements with L1, L2 and Imitated L2 Accents](https://arxiv.org/abs/2506.08981)
Append: [Naturalistic Language-related Movie-Watching fMRI Task for Detecting Neurocognitive Decline and Disorder](https://arxiv.org/abs/2506.08986)
Append: [Employing self-supervised learning models for cross-linguistic child speech maturity classification](https://arxiv.org/abs/2506.08999)
Append: [SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner](https://arxiv.org/abs/2506.09003)
Append: [UD-KSL Treebank v1.3: A semi-automated framework for aligning XPOS-extracted units with UPOS tags](https://arxiv.org/abs/2506.09009)
Append: [Learning to Reason Across Parallel Samples for LLM Reasoning](https://arxiv.org/abs/2506.09014)
Append: [Comparing human and LLM proofreading in L2 writing: Impact on lexical and syntactic features](https://arxiv.org/abs/2506.09021)
Append: [Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning](https://arxiv.org/abs/2506.09033)
Append: [Same Task, Different Circuits: Disentangling Modality-Specific Mechanisms in VLMs](https://arxiv.org/abs/2506.09047)
Append: [Modality-Balancing Preference Optimization of Large Multimodal Models by Adversarial Negative Mining](https://arxiv.org/abs/2506.08022)
Append: [Hierarchical Lexical Graph for Enhanced Multi-Hop Retrieval](https://arxiv.org/abs/2506.08074)
Append: [Bingo: Boosting Efficient Reasoning of LLMs via Dynamic and Significance-based Reinforcement Learning](https://arxiv.org/abs/2506.08125)
Append: [AutoSDT: Scaling Data-Driven Discovery Tasks Toward Open Co-Scientists](https://arxiv.org/abs/2506.08140)
Append: [GradEscape: A Gradient-Based Evader Against AI-Generated Text Detectors](https://arxiv.org/abs/2506.08188)
Append: [A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation](https://arxiv.org/abs/2506.08210)
Append: [RADAR: Benchmarking Language Models on Imperfect Tabular Data](https://arxiv.org/abs/2506.08249)
Append: [Reinforcement Learning from Human Feedback with High-Confidence Safety Constraints](https://arxiv.org/abs/2506.08266)
Append: [Instruction-Tuned Video-Audio Models Elucidate Functional Specialization in the Brain](https://arxiv.org/abs/2506.08277)
Append: [From Debate to Equilibrium: Belief-Driven Multi-Agent LLM Reasoning via Bayesian Nash Equilibrium](https://arxiv.org/abs/2506.08292)
Append: [From Passive to Active Reasoning: Can Large Language Models Ask the Right Questions under Incomplete Information?](https://arxiv.org/abs/2506.08295)
Append: [How Much To Guide: Revisiting Adaptive Guidance in Classifier-Free Guidance Text-to-Vision Diffusion Models](https://arxiv.org/abs/2506.08351)
Append: [Reinforce LLM Reasoning through Multi-Agent Reflection](https://arxiv.org/abs/2506.08379)
Append: [Reinforcement Learning Teachers of Test Time Scaling](https://arxiv.org/abs/2506.08388)
Append: [A Survey on Large Language Models for Mathematical Reasoning](https://arxiv.org/abs/2506.08446)
Append: [The Geometries of Truth Are Orthogonal Across Tasks](https://arxiv.org/abs/2506.08572)
Append: [Approaching Dialogue State Tracking via Aligning Speech Encoders and LLMs](https://arxiv.org/abs/2506.08633)
Append: [Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2506.08745)
Append: [EDINET-Bench: Evaluating LLMs on Complex Financial Tasks using Japanese Financial Statements](https://arxiv.org/abs/2506.08762)
Append: [Paths to Causality: Finding Informative Subgraphs Within Knowledge Graphs for Knowledge-Based Causal Discovery](https://arxiv.org/abs/2506.08771)
Append: [Measuring Data Science Automation: A Survey of Evaluation Tools for AI Assistants and Agents](https://arxiv.org/abs/2506.08800)
Append: [CulturalFrames: Assessing Cultural Expectation Alignment in Text-to-Image Models and Evaluation Metrics](https://arxiv.org/abs/2506.08835)
Append: [Socratic-MCTS: Test-Time Visual Reasoning by Asking the Right Questions](https://arxiv.org/abs/2506.08927)
Append: [Step-Audio-AQAA: a Fully End-to-End Expressive Large Audio Language Model](https://arxiv.org/abs/2506.08967)
Append: [SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2506.08989)
Append: [e3: Learning to Explore Enables Extrapolation of Test-Time Compute for LLMs](https://arxiv.org/abs/2506.09026)
Append: [Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better](https://arxiv.org/abs/2506.09040)
Append: [A Decomposition-Based Approach for Evaluating and Analyzing Inter-Annotator Disagreement](https://arxiv.org/abs/2206.05446)
Append: [A Survey on Long Text Modeling with Transformers](https://arxiv.org/abs/2302.14502)
Append: [Cross-lingual Transfer in Programming Languages: An Extensive Empirical Study](https://arxiv.org/abs/2310.16937)
Append: [Poro 34B and the Blessing of Multilinguality](https://arxiv.org/abs/2404.01856)
Append: [P-React: Synthesizing Topic-Adaptive Reactions of Personality Traits via Mixture of Specialized LoRA Experts](https://arxiv.org/abs/2406.12548)
Append: [SK-VQA: Synthetic Knowledge Generation at Scale for Training Context-Augmented Multimodal LLMs](https://arxiv.org/abs/2406.19593)
Append: [High-Throughput Phenotyping of Clinical Text Using Large Language Models](https://arxiv.org/abs/2408.01214)
Append: [Exploring SSL Discrete Speech Features for Zipformer-based Contextual ASR](https://arxiv.org/abs/2409.08797)
Append: [Guidelines for Fine-grained Sentence-level Arabic Readability Annotation](https://arxiv.org/abs/2410.08674)
Append: [Can Large Language Models Invent Algorithms to Improve Themselves?: Algorithm Discovery for Recursive Self-Improvement through Reinforcement Learning](https://arxiv.org/abs/2410.15639)
Append: [FairMT-Bench: Benchmarking Fairness for Multi-turn Dialogue in Conversational LLMs](https://arxiv.org/abs/2410.19317)
Append: [SHARE: Shared Memory-Aware Open-Domain Long-Term Dialogue Dataset Constructed from Movie Script](https://arxiv.org/abs/2410.20682)
Append: [Length-Induced Embedding Collapse in PLM-based Models](https://arxiv.org/abs/2410.24200)
Append: [The BS-meter: A ChatGPT-Trained Instrument to Detect Sloppy Language-Games](https://arxiv.org/abs/2411.15129)
Append: [From Language Models over Tokens to Language Models over Characters](https://arxiv.org/abs/2412.03719)
Append: [JuStRank: Benchmarking LLM Judges for System Ranking](https://arxiv.org/abs/2412.09569)
Append: [Cracking the Code of Hallucination in LVLMs with Vision-aware Head Divergence](https://arxiv.org/abs/2412.13949)
Append: [Generative Psycho-Lexical Approach for Constructing Value Systems in Large Language Models](https://arxiv.org/abs/2502.02444)
Append: [Position: Editing Large Language Models Poses Serious Safety Risks](https://arxiv.org/abs/2502.02958)
Append: [LLM Alignment as Retriever Optimization: An Information Retrieval Perspective](https://arxiv.org/abs/2502.03699)
Append: [In Praise of Stubbornness: An Empirical Case for Cognitive-Dissonance Aware Continual Update of Knowledge in LLMs](https://arxiv.org/abs/2502.04390)
Append: [Accelerating LLM Inference with Lossless Speculative Decoding Algorithms for Heterogeneous Vocabularies](https://arxiv.org/abs/2502.05202)
Append: [R.R.: Unveiling LLM Training Privacy through Recollection and Ranking](https://arxiv.org/abs/2502.12658)
Append: [Retrieval-augmented systems can be dangerous medical communicators](https://arxiv.org/abs/2502.14898)
Append: [Understand User Opinions of Large Language Models via LLM-Powered In-the-Moment User Experience Interviews](https://arxiv.org/abs/2502.15226)
Append: [Self-Training Elicits Concise Reasoning in Large Language Models](https://arxiv.org/abs/2502.20122)
Append: [Compositional Causal Reasoning Evaluation in Language Models](https://arxiv.org/abs/2503.04556)
Append: [A Hybrid Architecture with Efficient Fine Tuning for Abstractive Patent Document Summarization](https://arxiv.org/abs/2503.10354)
Append: [Enhancing Arabic Automated Essay Scoring with Synthetic Data and Error Injection](https://arxiv.org/abs/2503.17739)
Append: [Summarizing Speech: A Comprehensive Survey](https://arxiv.org/abs/2504.08024)
Append: [VIST-GPT: Ushering in the Era of Visual Storytelling with LLMs?](https://arxiv.org/abs/2504.19267)
Append: [BiasGuard: A Reasoning-enhanced Bias Detection Tool For Large Language Models](https://arxiv.org/abs/2504.21299)
Append: [Value Portrait: Assessing Language Models' Values through Psychometrically and Ecologically Valid Items](https://arxiv.org/abs/2505.01015)
Append: [An Analysis of Hyper-Parameter Optimization Methods for Retrieval Augmented Generation](https://arxiv.org/abs/2505.03452)
Append: [Fusing Bidirectional Chains of Thought and Reward Mechanisms A Method for Enhancing Question-Answering Capabilities of Large Language Models for Chinese Intangible Cultural Heritage](https://arxiv.org/abs/2505.08167)
Append: [DISCO Balances the Scales: Adaptive Domain- and Difficulty-Aware Reinforcement Learning on Imbalanced Data](https://arxiv.org/abs/2505.15074)
Append: [Learning to Reason via Mixture-of-Thought for Logical Reasoning](https://arxiv.org/abs/2505.15817)
Append: [Beyond Induction Heads: In-Context Meta Learning Induces Multi-Phase Circuit Emergence](https://arxiv.org/abs/2505.16694)
Append: [RAVEN: Query-Guided Representation Alignment for Question Answering over Audio, Video, Embedded Sensors, and Natural Language](https://arxiv.org/abs/2505.17114)
Append: [Iterative Corpus Refinement for Materials Property Prediction Based on Scientific Texts](https://arxiv.org/abs/2505.21646)
Append: [CASPER: A Large Scale Spontaneous Speech Dataset](https://arxiv.org/abs/2506.00267)
Append: [AnnaAgent: Dynamic Evolution Agent System with Multi-Session Memory for Realistic Seeker Simulation](https://arxiv.org/abs/2506.00551)
Append: [DefenderBench: A Toolkit for Evaluating Language Agents in Cybersecurity Environments](https://arxiv.org/abs/2506.00739)
Append: [TL;DR: Too Long, Do Re-weighting for Efficient LLM Reasoning Compression](https://arxiv.org/abs/2506.02678)
Append: [Speech Synthesis By Unrolling Diffusion Process using Neural Network Layers](https://arxiv.org/abs/2309.09652)
Append: [Exploring the Escalation of Source Bias in User, Data, and Recommender System Feedback Loop](https://arxiv.org/abs/2405.17998)
Append: [Textual Unlearning Gives a False Sense of Unlearning](https://arxiv.org/abs/2406.13348)
Append: [Human-like object concept representations emerge naturally in multimodal large language models](https://arxiv.org/abs/2407.01067)
Append: [How transformers learn structured data: insights from hierarchical filtering](https://arxiv.org/abs/2408.15138)
Append: [TPP-LLM: Modeling Temporal Point Processes by Efficiently Fine-Tuning Large Language Models](https://arxiv.org/abs/2410.02062)
Append: [NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples](https://arxiv.org/abs/2410.14669)
Append: [xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video Even in VLMs](https://arxiv.org/abs/2410.16267)
Append: [Phonology-Guided Speech-to-Speech Translation for African Languages](https://arxiv.org/abs/2410.23323)
Append: [DiffLM: Controllable Synthetic Data Generation via Diffusion Language Models](https://arxiv.org/abs/2411.03250)
Append: [PrisonBreak: Jailbreaking Large Language Models with Fewer Than Twenty-Five Targeted Bit-flips](https://arxiv.org/abs/2412.07192)
Append: [RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation](https://arxiv.org/abs/2501.08617)
Append: [TRAVEL: Training-Free Retrieval and Alignment for Vision-and-Language Navigation](https://arxiv.org/abs/2502.07306)
Append: [Self-Supervised Transformers as Iterative Solution Improvers for Constraint Satisfaction](https://arxiv.org/abs/2502.15794)
Append: ["Would You Want an AI Tutor?" Understanding Stakeholder Perceptions of LLM-based Systems in the Classroom](https://arxiv.org/abs/2503.02885)
Append: [Big Help or Big Brother? Auditing Tracking, Profiling, and Personalization in Generative AI Assistants](https://arxiv.org/abs/2503.16586)
Append: [Understanding Bias Reinforcement in LLM Agents Debate](https://arxiv.org/abs/2503.16814)
Append: [Quamba2: A Robust and Scalable Post-training Quantization Framework for Selective State Space Models](https://arxiv.org/abs/2503.22879)
Append: [Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach](https://arxiv.org/abs/2505.14479)
Append: [Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO](https://arxiv.org/abs/2505.17017)
Append: [From Output to Evaluation: Does Raw Instruction-Tuned Code LLMs Output Suffice for Fill-in-the-Middle Code Generation?](https://arxiv.org/abs/2505.18789)
Append: [Optimized Text Embedding Models and Benchmarks for Amharic Passage Retrieval](https://arxiv.org/abs/2505.19356)
Append: [Flexible Tool Selection through Low-dimensional Attribute Alignment of Vision and Language](https://arxiv.org/abs/2505.22146)
append_entries: 173
Finish: 2025-06-11 04:38:59.537315
------------------------------------------------------
Started: 2025-06-11 06:25:19.660076
Existing_entries: 1173
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1281
Summarized using GPT-3.5-turbo
Append: [BehaviorBox: Automated Discovery of Fine-Grained Performance Differences Between Language Models](https://arxiv.org/abs/2506.02204)
Token length: 1586
Summarized using GPT-3.5-turbo
Append: [TextAtari: 100K Frames Game Playing with Language Agents](https://arxiv.org/abs/2506.04098)
append_entries: 2
Finish: 2025-06-11 06:25:25.586577
------------------------------------------------------
Started: 2025-06-11 08:22:47.650704
Existing_entries: 1002
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-11 08:22:48.046564
------------------------------------------------------
Started: 2025-06-11 10:18:28.392346
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-11 10:18:28.823092
------------------------------------------------------
Started: 2025-06-11 12:35:18.226380
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-11 12:35:18.629053
------------------------------------------------------
Started: 2025-06-11 14:16:02.974704
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-11 14:16:03.423820
------------------------------------------------------
Started: 2025-06-11 16:22:22.608987
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-11 16:22:23.052565
------------------------------------------------------
Started: 2025-06-11 18:24:00.194775
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-11 18:24:00.622549
------------------------------------------------------
Started: 2025-06-11 20:17:28.100501
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-11 20:17:28.517716
------------------------------------------------------
Started: 2025-06-11 22:16:10.217761
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-11 22:16:10.694314
------------------------------------------------------
Started: 2025-06-12 01:19:59.842415
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-12 01:20:00.242887
------------------------------------------------------
Started: 2025-06-12 03:13:59.449133
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-12 03:13:59.858371
------------------------------------------------------
Started: 2025-06-12 04:27:32.553389
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1233
Summarized using GPT-3.5-turbo
Append: [LLM-as-a-qualitative-judge: automating error analysis in natural language generation](https://arxiv.org/abs/2506.09147)
Token length: 907
Summarized using GPT-3.5-turbo
Append: [PHRASED: Phrase Dictionary Biasing for Speech Translation](https://arxiv.org/abs/2506.09175)
Token length: 1028
Summarized using GPT-3.5-turbo
Append: [A Technique for Isolating Lexically-Independent Phonetic Dependencies in Generative CNNs](https://arxiv.org/abs/2506.09218)
Token length: 1473
Summarized using GPT-3.5-turbo
Append: [Extrapolation by Association: Length Generalization Transfer in Transformers](https://arxiv.org/abs/2506.09251)
Token length: 1733
Summarized using GPT-3.5-turbo
Append: [Self-Anchored Attention Model for Sample-Efficient Classification of Prosocial Text Chat](https://arxiv.org/abs/2506.09259)
Token length: 1068
Summarized using GPT-3.5-turbo
Append: [Did I Faithfully Say What I Thought? Bridging the Gap Between Neural Activity and Self-Explanations in Large Language Models](https://arxiv.org/abs/2506.09277)
Token length: 1082
Summarized using GPT-3.5-turbo
Append: [$(RSA)^2$: A Rhetorical-Strategy-Aware Rational Speech Act Framework for Figurative Language Understanding](https://arxiv.org/abs/2506.09301)
Token length: 1004
Summarized using GPT-3.5-turbo
Append: [Alzheimer's Dementia Detection Using Perplexity from Paired Large Language Models](https://arxiv.org/abs/2506.09315)
Token length: 1962
Summarized using GPT-3.5-turbo
Append: [Towards Efficient and Effective Alignment of Large Language Models](https://arxiv.org/abs/2506.09329)
Token length: 1422
Summarized using GPT-3.5-turbo
Append: [Multi-Agent Language Models: Advancing Cooperation, Coordination, and Adaptation](https://arxiv.org/abs/2506.09331)
Token length: 1098
Summarized using GPT-3.5-turbo
Append: [RePO: Replay-Enhanced Policy Optimization](https://arxiv.org/abs/2506.09340)
Token length: 1199
Summarized using GPT-3.5-turbo
Append: [Latent Multi-Head Attention for Small Language Models](https://arxiv.org/abs/2506.09342)
Token length: 1394
Summarized using GPT-3.5-turbo
Append: [OmniDRCA: Parallel Speech-Text Foundation Model via Dual-Resolution Speech Representations and Contrastive Alignment](https://arxiv.org/abs/2506.09349)
Token length: 1365
Summarized using GPT-3.5-turbo
Append: [DIVE into MoE: Diversity-Enhanced Reconstruction of Large Language Models from Dense into Mixture-of-Experts](https://arxiv.org/abs/2506.09351)
Token length: 518
Summarized using GPT-3.5-turbo
Append: [Taming SQL Complexity: LLM-Based Equivalence Evaluation for Text-to-SQL](https://arxiv.org/abs/2506.09359)
Token length: 1223
Summarized using GPT-3.5-turbo
Append: [COGENT: A Curriculum-oriented Framework for Generating Grade-appropriate Educational Content](https://arxiv.org/abs/2506.09367)
Token length: 1018
Summarized using GPT-3.5-turbo
Append: [CoLMbo: Speaker Language Model for Descriptive Profiling](https://arxiv.org/abs/2506.09375)
Token length: 1163
Summarized using GPT-3.5-turbo
Append: [Binary classification for perceived quality of headlines and links on worldwide news websites, 2018-2024](https://arxiv.org/abs/2506.09381)
Token length: 1136
Summarized using GPT-3.5-turbo
Append: [Comparing human and LLM politeness strategies in free production](https://arxiv.org/abs/2506.09391)
Token length: 1135
Summarized using GPT-3.5-turbo
Append: [A Hierarchical Probabilistic Framework for Incremental Knowledge Tracing in Classroom Settings](https://arxiv.org/abs/2506.09393)
Token length: 1141
Summarized using GPT-3.5-turbo
Append: [Token Constraint Decoding Improves Robustness on Question Answering for Large Language Models](https://arxiv.org/abs/2506.09408)
Token length: 1943
Summarized using GPT-3.5-turbo
Append: [PGDA-KGQA: A Prompt-Guided Generative Framework with Multiple Data Augmentation Strategies for Knowledge Graph Question Answering](https://arxiv.org/abs/2506.09414)
Token length: 1366
Summarized using GPT-3.5-turbo
Append: [Hidden in Plain Sight: Evaluation of the Deception Detection Capabilities of LLMs in Multimodal Settings](https://arxiv.org/abs/2506.09424)
Token length: 966
Summarized using GPT-3.5-turbo
Append: [Improved Supervised Fine-Tuning for Large Language Models to Mitigate Catastrophic Forgetting](https://arxiv.org/abs/2506.09428)
Token length: 1098
Summarized using GPT-3.5-turbo
Append: [GigaChat Family: Efficient Russian Language Modeling Through Mixture of Experts Architecture](https://arxiv.org/abs/2506.09440)
Token length: 1259
Summarized using GPT-3.5-turbo
Append: [UniToMBench: Integrating Perspective-Taking to Improve Theory of Mind in LLMs](https://arxiv.org/abs/2506.09450)
Token length: 1677
Summarized using GPT-3.5-turbo
Append: [Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms](https://arxiv.org/abs/2506.09457)
Token length: 1940
Summarized using GPT-3.5-turbo
Append: [Bridging Online Behavior and Clinical Insight: A Longitudinal LLM-based Study of Suicidality on YouTube Reveals Novel Digital Markers](https://arxiv.org/abs/2506.09495)
Token length: 1768
Summarized using GPT-3.5-turbo
Append: [Give Me FP32 or Give Me Death? Challenges and Solutions for Reproducible Reasoning](https://arxiv.org/abs/2506.09501)
Token length: 1695
Summarized using GPT-3.5-turbo
Append: [TransXSSM: A Hybrid Transformer State Space Model with Unified Rotary Position Embedding](https://arxiv.org/abs/2506.09507)
Token length: 1107
Summarized using GPT-3.5-turbo
Append: [ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning](https://arxiv.org/abs/2506.09513)
Token length: 1203
Summarized using GPT-3.5-turbo
Append: [KG-Infused RAG: Augmenting Corpus-Based RAG with External Knowledge Graphs](https://arxiv.org/abs/2506.09542)
Token length: 985
Summarized using GPT-3.5-turbo
Append: [MEDUSA: A Multimodal Deep Fusion Multi-Stage Training Framework for Speech Emotion Recognition in Naturalistic Conditions](https://arxiv.org/abs/2506.09556)
Token length: 1335
Summarized using GPT-3.5-turbo
Append: [Gender Bias in English-to-Greek Machine Translation](https://arxiv.org/abs/2506.09558)
Token length: 1652
Summarized using GPT-3.5-turbo
Append: [Towards Open Foundation Language Model and Corpus for Macedonian: A Low-Resource Language](https://arxiv.org/abs/2506.09560)
Token length: 1016
Summarized using GPT-3.5-turbo
Append: [From Symbolic to Neural and Back: Exploring Knowledge Graph-Large Language Model Synergies](https://arxiv.org/abs/2506.09566)
Token length: 999
Summarized using GPT-3.5-turbo
Append: [Memorization in Language Models through the Lens of Intrinsic Dimension](https://arxiv.org/abs/2506.09591)
Token length: 1405
Summarized using GPT-3.5-turbo
Append: [Benchmarking Debiasing Methods for LLM-based Parameter Estimates](https://arxiv.org/abs/2506.09627)
Token length: 1031
Summarized using GPT-3.5-turbo
Append: [Modeling Probabilistic Reduction using Information Theory and Naive Discriminative Learning](https://arxiv.org/abs/2506.09641)
Token length: 1468
Summarized using GPT-3.5-turbo
Append: [Using Sign Language Production as Data Augmentation to enhance Sign Language Translation](https://arxiv.org/abs/2506.09643)
Token length: 1851
Summarized using GPT-3.5-turbo
Append: [Learning Efficient and Generalizable Graph Retriever for Knowledge-Graph Question Answering](https://arxiv.org/abs/2506.09645)
Token length: 950
Summarized using GPT-3.5-turbo
Append: [Bridging the Gap Between Open-Source and Proprietary LLMs in Table QA](https://arxiv.org/abs/2506.09657)
Token length: 1094
Summarized using GPT-3.5-turbo
Append: [Query-Level Uncertainty in Large Language Models](https://arxiv.org/abs/2506.09669)
Token length: 1403
Summarized using GPT-3.5-turbo
Append: [Is Fine-Tuning an Effective Solution? Reassessing Knowledge Editing for Unstructured Data](https://arxiv.org/abs/2506.09672)
Token length: 1535
Summarized using GPT-3.5-turbo
Append: [Inv-Entropy: A Fully Probabilistic Framework for Uncertainty Quantification in Language Models](https://arxiv.org/abs/2506.09684)
Token length: 1574
Summarized using GPT-3.5-turbo
Append: [ComfyUI-R1: Exploring Reasoning Models for Workflow Generation](https://arxiv.org/abs/2506.09790)
Token length: 1342
Summarized using GPT-3.5-turbo
Append: [Do LLMs Give Psychometrically Plausible Responses in Educational Assessments?](https://arxiv.org/abs/2506.09796)
Token length: 1617
Summarized using GPT-3.5-turbo
Append: [CoRT: Code-integrated Reasoning within Thinking](https://arxiv.org/abs/2506.09820)
Token length: 1552
Summarized using GPT-3.5-turbo
Append: [EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech Emotion Detection](https://arxiv.org/abs/2506.09827)
Token length: 1110
Summarized using GPT-3.5-turbo
Append: [Error-Guided Pose Augmentation: Enhancing Rehabilitation Exercise Assessment through Targeted Data Generation](https://arxiv.org/abs/2506.09833)
Append: [Dataset of News Articles with Provenance Metadata for Media Relevance Assessment](https://arxiv.org/abs/2506.09847)
Append: [Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning](https://arxiv.org/abs/2506.09853)
Append: [Attention Head Embeddings with Trainable Deep Kernels for Hallucination Detection in LLMs](https://arxiv.org/abs/2506.09886)
Append: [The Emergence of Abstract Thought in Large Language Models Beyond Any Language](https://arxiv.org/abs/2506.09890)
Append: [PersonaLens: A Benchmark for Personalization Evaluation in Conversational AI Assistants](https://arxiv.org/abs/2506.09902)
Append: [Aspect-Based Opinion Summarization with Argumentation Schemes](https://arxiv.org/abs/2506.09917)
Append: [VerIF: Verification Engineering for Reinforcement Learning in Instruction Following](https://arxiv.org/abs/2506.09942)
Append: [Query-Focused Retrieval Heads Improve Long-Context Reasoning and Re-ranking](https://arxiv.org/abs/2506.09944)
Append: [Resa: Transparent Reasoning Models via SAEs](https://arxiv.org/abs/2506.09967)
Append: [When Detection Fails: The Power of Fine-Tuned Models to Generate Human-Like Social Media Text](https://arxiv.org/abs/2506.09975)
Append: [Step-by-step Instructions and a Simple Tabular Output Format Improve the Dependency Parsing Accuracy of LLMs](https://arxiv.org/abs/2506.09983)
Append: [Large Language Models for Toxic Language Detection in Low-Resource Balkan Languages](https://arxiv.org/abs/2506.09992)
Append: [From Judgment to Interference: Early Stopping LLM Harmful Outputs via Streaming Content Monitoring](https://arxiv.org/abs/2506.09996)
Append: [An Interpretable N-gram Perplexity Threat Model for Large Language Model Jailbreaks](https://arxiv.org/abs/2410.16222)
Append: [FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation](https://arxiv.org/abs/2506.09081)
Append: [Too Big to Think: Capacity, Memorization, and Generalization in Pre-Trained Transformers](https://arxiv.org/abs/2506.09099)
Append: [SensorLM: Learning the Language of Wearable Sensors](https://arxiv.org/abs/2506.09108)
Append: [CAIRe: Cultural Attribution of Images by Retrieval-Augmented Evaluation](https://arxiv.org/abs/2506.09109)
Append: [Adversarial Text Generation with Dynamic Contextual Perturbation](https://arxiv.org/abs/2506.09148)
Append: [Improving LLM Agent Planning with In-Context Learning via Atomic Fact Augmentation and Lookahead Search](https://arxiv.org/abs/2506.09171)
Append: [SimClass: A Classroom Speech Dataset Generated via Game Engine Simulation For Automatic Speech Recognition Research](https://arxiv.org/abs/2506.09206)
Append: [ThinkQE: Query Expansion via an Evolving Thinking Process](https://arxiv.org/abs/2506.09260)
Append: [UTBoost: Rigorous Evaluation of Coding Agents on SWE-Bench](https://arxiv.org/abs/2506.09289)
Append: [Natural Language Guided Ligand-Binding Protein Design](https://arxiv.org/abs/2506.09332)
Append: [Ming-Omni: A Unified Multimodal Model for Perception and Generation](https://arxiv.org/abs/2506.09344)
Append: [A Call for Collaborative Intelligence: Why Human-Agent Systems Should Precede AI Autonomy](https://arxiv.org/abs/2506.09420)
Append: [OWSM-Biasing: Contextualizing Open Whisper-Style Speech Models for Automatic Speech Recognition with Dynamic Vocabulary](https://arxiv.org/abs/2506.09448)
Append: [Learning Obfuscations Of LLM Embedding Sequences: Stained Glass Transform](https://arxiv.org/abs/2506.09452)
Append: [You Are What You Say: Exploiting Linguistic Content for VoicePrivacy Attacks](https://arxiv.org/abs/2506.09521)
Append: [Revisit What You See: Disclose Language Prior in Vision Tokens for Efficient Guided Decoding of LVLMs](https://arxiv.org/abs/2506.09522)
Append: [Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models](https://arxiv.org/abs/2506.09532)
Append: [Effective Red-Teaming of Policy-Adherent Agents](https://arxiv.org/abs/2506.09600)
Append: [Intent Factored Generation: Unleashing the Diversity in Your Language Model](https://arxiv.org/abs/2506.09659)
Append: [Adding simple structure at inference improves Vision-Language Compositionality](https://arxiv.org/abs/2506.09691)
Append: [Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal Localization of Prolonged Exposure Therapy Elements](https://arxiv.org/abs/2506.09707)
Append: [Regularizing Learnable Feature Extraction for Automatic Speech Recognition](https://arxiv.org/abs/2506.09804)
Append: [Advancing Exchange Rate Forecasting: Leveraging Machine Learning and AI for Enhanced Accuracy in Global Financial Markets](https://arxiv.org/abs/2506.09851)
Append: [Outside Knowledge Conversational Video (OKCV) Dataset -- Dialoguing over Videos](https://arxiv.org/abs/2506.09953)
Append: [Flipping Against All Odds: Reducing LLM Coin Flip Bias via Verbalized Rejection Sampling](https://arxiv.org/abs/2506.09998)
Append: [AMELI: Enhancing Multimodal Entity Linking with Fine-Grained Attributes](https://arxiv.org/abs/2305.14725)
Append: [DREsS: Dataset for Rubric-based Essay Scoring on EFL Writing](https://arxiv.org/abs/2402.16733)
Append: [Emphasising Structured Information: Integrating Abstract Meaning Representation into LLMs for Enhanced Open-Domain Dialogue Evaluation](https://arxiv.org/abs/2404.01129)
Append: [Let's Fuse Step by Step: A Generative Fusion Decoding Algorithm with LLMs for Robust and Instruction-Aware ASR and OCR](https://arxiv.org/abs/2405.14259)
Append: [Language Models Resist Alignment: Evidence From Data Compression](https://arxiv.org/abs/2406.06144)
Append: [Standard Language Ideology in AI-Generated Language](https://arxiv.org/abs/2406.08726)
Append: [Raising the Bar: Investigating the Values of Large Language Models via Generative Evolving Testing](https://arxiv.org/abs/2406.14230)
Append: [CaLMQA: Exploring culturally specific long-form question answering across 23 languages](https://arxiv.org/abs/2406.17761)
Append: [CiteFusion: An Ensemble Framework for Citation Intent Classification Harnessing Dual-Model Binary Couples and SHAP Analyses](https://arxiv.org/abs/2407.13329)
Append: [MMREC: LLM Based Multi-Modal Recommender System](https://arxiv.org/abs/2408.04211)
Append: [LogProber: Disentangling confidence from contamination in LLM responses](https://arxiv.org/abs/2408.14352)
Append: [Critic-CoT: Boosting the reasoning abilities of large language model via Chain-of-thoughts Critic](https://arxiv.org/abs/2408.16326)
Append: [Automatic Pseudo-Harmful Prompt Generation for Evaluating False Refusals in Large Language Models](https://arxiv.org/abs/2409.00598)
Append: [MOSAIC: Multiple Observers Spotting AI Content](https://arxiv.org/abs/2409.07615)
Append: [Explaining word embeddings with perfect fidelity: Case study in research impact prediction](https://arxiv.org/abs/2409.15912)
Append: [GenARM: Reward Guided Generation with Autoregressive Reward Model for Test-time Alignment](https://arxiv.org/abs/2410.08193)
Append: [How Do Multilingual Language Models Remember Facts?](https://arxiv.org/abs/2410.14387)
Append: [Self-Steering Optimization: Autonomous Preference Optimization for Large Language Models](https://arxiv.org/abs/2410.17131)
Append: [Code-Switching Curriculum Learning for Multilingual Transfer in LLMs](https://arxiv.org/abs/2411.02460)
Append: [CROW: Eliminating Backdoors from Large Language Models via Internal Consistency Regularization](https://arxiv.org/abs/2411.12768)
Append: [Meaningless is better: hashing bias-inducing words in LLM prompts improves performance in logical reasoning and statistical learning](https://arxiv.org/abs/2411.17304)
Append: [Retrofitting Large Language Models with Dynamic Tokenization](https://arxiv.org/abs/2411.18553)
Append: [Steps are all you need: Rethinking STEM Education with Prompt Engineering](https://arxiv.org/abs/2412.05023)
Append: [Multi-Party Supervised Fine-tuning of Language Models for Multi-Party Dialogue Generation](https://arxiv.org/abs/2412.05342)
Append: [Knowledge Graphs are all you need: Leveraging KGs in Physics Question Answering](https://arxiv.org/abs/2412.05453)
Append: [7B Fully Open Source Moxin-LLM/VLM -- From Pretraining to GRPO-based Reinforcement Learning Enhancement](https://arxiv.org/abs/2412.06845)
Append: [Irony Detection, Reasoning and Understanding in Zero-shot Learning](https://arxiv.org/abs/2501.16884)
Append: [Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for Multimodal Reasoning Models](https://arxiv.org/abs/2502.16033)
Append: [Revisiting Self-Consistency from Dynamic Distributional Alignment Perspective on Answer Aggregation](https://arxiv.org/abs/2502.19830)
Append: [AskToAct: Enhancing LLMs Tool Use via Self-Correcting Clarification](https://arxiv.org/abs/2503.01940)
Append: [Measuring What Makes You Unique: Difference-Aware User Modeling for Enhancing LLM Personalization](https://arxiv.org/abs/2503.02450)
Append: [Unlocking General Long Chain-of-Thought Reasoning Capabilities of Large Language Models via Representation Engineering](https://arxiv.org/abs/2503.11314)
Append: [MAGIC-VQA: Multimodal And Grounded Inference with Commonsense Knowledge for Visual Question Answering](https://arxiv.org/abs/2503.18491)
Append: [Style over Substance: Distilled Language Models Reason Via Stylistic Replication](https://arxiv.org/abs/2504.01738)
Append: [One Pic is All it Takes: Poisoning Visual Document Retrieval Augmented Generation with a Single Image](https://arxiv.org/abs/2504.02132)
Append: [Assessment of Evolving Large Language Models in Upper Secondary Mathematics](https://arxiv.org/abs/2504.12347)
Append: [Persona-judge: Personalized Alignment of Large Language Models via Token-level Self-judgment](https://arxiv.org/abs/2504.12663)
Append: [Convert Language Model into a Value-based Strategic Planner](https://arxiv.org/abs/2505.06987)
Append: [Product of Experts with LLMs: Boosting Performance on ARC Is a Matter of Perspective](https://arxiv.org/abs/2505.07859)
Append: [DecIF: Improving Instruction-Following through Meta-Decomposition](https://arxiv.org/abs/2505.13990)
Append: [LIFEBench: Evaluating Length Instruction Following in Large Language Models](https://arxiv.org/abs/2505.16234)
Append: [Discovering Forbidden Topics in Language Models](https://arxiv.org/abs/2505.17441)
Append: [Decoding Knowledge Attribution in Mixture-of-Experts: A Framework of Basic-Refinement Collaboration and Efficiency Analysis](https://arxiv.org/abs/2505.24593)
Append: [Writing-Zero: Bridge the Gap Between Non-verifiable Tasks and Verifiable Rewards](https://arxiv.org/abs/2506.00103)
Append: [LID Models are Actually Accent Classifiers: Implications and Solutions for LID on Accented Speech](https://arxiv.org/abs/2506.00628)
Append: [StochasTok: Improving Fine-Grained Subword Understanding in LLMs](https://arxiv.org/abs/2506.01687)
Append: [GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2506.02404)
Append: [Listen, Chat, and Remix: Text-Guided Soundscape Remixing for Enhanced Auditory Experience](https://arxiv.org/abs/2402.03710)
Append: [Using Shapley interactions to understand how models use structure](https://arxiv.org/abs/2403.13106)
Append: [LLM2TEA: Agentic AI Designer Finds Innovative Objects with Generative Evolutionary Multitasking](https://arxiv.org/abs/2406.14917)
Append: [Code-Switching Red-Teaming: LLM Evaluation for Safety and Multilingual Understanding](https://arxiv.org/abs/2406.15481)
Append: [The Remarkable Robustness of LLMs: Stages of Inference?](https://arxiv.org/abs/2406.19384)
Append: [AcTracer: Active Testing of Large Language Model via Multi-Stage Sampling](https://arxiv.org/abs/2408.03573)
Append: [EMMA: Efficient Visual Alignment in Multi-Modal LLMs](https://arxiv.org/abs/2410.02080)
Append: [Beyond Bradley-Terry Models: A General Preference Model for Language Model Alignment](https://arxiv.org/abs/2410.02197)
Append: [MindForge: Empowering Embodied Agents with Theory of Mind for Lifelong Cultural Learning](https://arxiv.org/abs/2411.12977)
Append: [Enhancing Code LLMs with Reinforcement Learning in Code Generation: A Survey](https://arxiv.org/abs/2412.20367)
Append: [ICONS: Influence Consensus for Vision-Language Data Selection](https://arxiv.org/abs/2501.00654)
Append: [Reasoning Language Models: A Blueprint](https://arxiv.org/abs/2501.11223)
Append: [Trustworthy AI: Safety, Bias, and Privacy -- A Survey](https://arxiv.org/abs/2502.10450)
Append: [Rethinking Diverse Human Preference Learning through Principal Component Analysis](https://arxiv.org/abs/2502.13131)
Append: [AAD-LLM: Neural Attention-Driven Auditory Scene Understanding](https://arxiv.org/abs/2502.16794)
Append: [ImageChain: Advancing Sequential Image-to-Text Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2502.19409)
Append: [Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security Analysis](https://arxiv.org/abs/2502.20383)
Append: [Chem42: a Family of chemical Language Models for Target-aware Ligand Generation](https://arxiv.org/abs/2503.16563)
Append: [CiteFix: Enhancing RAG Accuracy Through Post-Processing Citation Correction](https://arxiv.org/abs/2504.15629)
Append: [Unveiling the Hidden: Movie Genre and User Bias in Spoiler Detection](https://arxiv.org/abs/2504.17834)
Append: [OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation](https://arxiv.org/abs/2505.23885)
append_entries: 157
Finish: 2025-06-12 04:29:23.772729
------------------------------------------------------
Started: 2025-06-12 06:25:52.034641
Existing_entries: 1157
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1151
Summarized using GPT-3.5-turbo
Append: [Rethinking Text-based Protein Understanding: Retrieval or LLM?](https://arxiv.org/abs/2505.20354)
Token length: 1691
Summarized using GPT-3.5-turbo
Append: [TableEval: A Real-World Benchmark for Complex, Multilingual, and Multi-Structured Table Question Answering](https://arxiv.org/abs/2506.03949)
append_entries: 2
Finish: 2025-06-12 06:25:56.133874
------------------------------------------------------
Started: 2025-06-12 08:22:53.691038
Existing_entries: 1002
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-12 08:22:54.072592
------------------------------------------------------
Started: 2025-06-12 10:18:51.706568
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-12 10:18:52.109448
------------------------------------------------------
Started: 2025-06-12 12:34:37.667645
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-12 12:34:38.098023
------------------------------------------------------
Started: 2025-06-12 14:17:08.463940
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-12 14:17:08.868055
------------------------------------------------------
Started: 2025-06-12 16:20:57.527784
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-12 16:20:57.993118
------------------------------------------------------
Started: 2025-06-12 18:23:15.910733
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-12 18:23:16.309032
------------------------------------------------------
Started: 2025-06-12 20:18:56.719447
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-12 20:18:57.129318
------------------------------------------------------
Started: 2025-06-12 22:16:03.857411
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-12 22:16:04.278322
------------------------------------------------------
Started: 2025-06-13 01:21:19.173822
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-13 01:21:19.593838
------------------------------------------------------
Started: 2025-06-13 03:15:36.098798
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-13 03:15:36.542134
------------------------------------------------------
Started: 2025-06-13 04:28:38.654655
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1040
Summarized using GPT-3.5-turbo
Append: [A Survey of Automatic Evaluation Methods on Text, Visual and Speech Generations](https://arxiv.org/abs/2506.10019)
Token length: 987
Summarized using GPT-3.5-turbo
Append: [TaskCraft: Automated Generation of Agentic Tasks](https://arxiv.org/abs/2506.10055)
Token length: 1850
Summarized using GPT-3.5-turbo
Append: [A quantum semantic framework for natural language processing](https://arxiv.org/abs/2506.10077)
Token length: 898
Summarized using GPT-3.5-turbo
Append: [Chat-of-Thought: Collaborative Multi-Agent System for Generating Domain Specific Information](https://arxiv.org/abs/2506.10086)
Token length: 779
Summarized using GPT-3.5-turbo
Append: [When Meaning Stays the Same, but Models Drift: Evaluating Quality of Service under Token-Level Behavioral Instability in LLMs](https://arxiv.org/abs/2506.10095)
Token length: 1699
Summarized using GPT-3.5-turbo
Append: [ChartReasoner: Code-Driven Modality Bridging for Long-Chain Reasoning in Chart Question Answering](https://arxiv.org/abs/2506.10116)
Token length: 1164
Summarized using GPT-3.5-turbo
Append: [Unsupervised Elicitation of Language Models](https://arxiv.org/abs/2506.10139)
Token length: 1343
Summarized using GPT-3.5-turbo
Append: [When Large Language Models are Reliable for Judging Empathic Communication](https://arxiv.org/abs/2506.10150)
Token length: 968
Summarized using GPT-3.5-turbo
Append: [Analyzing Emotions in Bangla Social Media Comments Using Machine Learning and LIME](https://arxiv.org/abs/2506.10154)
Token length: 1057
Summarized using GPT-3.5-turbo
Append: [Measuring Corporate Human Capital Disclosures: Lexicon, Data, Code, and Research Opportunities](https://arxiv.org/abs/2506.10155)
Token length: 1358
Summarized using GPT-3.5-turbo
Append: [Can LLMs Generate Good Stories? Insights and Challenges from a Narrative Planning Perspective](https://arxiv.org/abs/2506.10161)
Token length: 1262
Summarized using GPT-3.5-turbo
Append: [Q2E: Query-to-Event Decomposition for Zero-Shot Multilingual Text-to-Video Retrieval](https://arxiv.org/abs/2506.10202)
Token length: 1524
Summarized using GPT-3.5-turbo
Append: [TTT-Bench: A Benchmark for Evaluating Reasoning Ability with Simple and Novel Tic-Tac-Toe-style Games](https://arxiv.org/abs/2506.10209)
Token length: 1235
Summarized using GPT-3.5-turbo
Append: [Classifying Unreliable Narrators with Large Language Models](https://arxiv.org/abs/2506.10231)
Token length: 1141
Summarized using GPT-3.5-turbo
Append: [ToxSyn-PT: A Large-Scale Synthetic Dataset for Hate Speech Detection in Portuguese](https://arxiv.org/abs/2506.10245)
Token length: 1589
Summarized using GPT-3.5-turbo
Append: [Do Language Models Have Bayesian Brains? Distinguishing Stochastic and Deterministic Decision Patterns within Large Language Models](https://arxiv.org/abs/2506.10268)
Token length: 1289
Summarized using GPT-3.5-turbo
Append: [ClusterUCB: Efficient Gradient-Based Data Selection for Targeted Fine-Tuning of LLMs](https://arxiv.org/abs/2506.10288)
Token length: 1929
Summarized using GPT-3.5-turbo
Append: [Flick: Few Labels Text Classification using K-Aware Intermediate Learning in Multi-Task Low-Resource Languages](https://arxiv.org/abs/2506.10292)
Token length: 1347
Summarized using GPT-3.5-turbo
Append: ["Check My Work?": Measuring Sycophancy in a Simulated Educational Context](https://arxiv.org/abs/2506.10297)
Token length: 1002
Summarized using GPT-3.5-turbo
Append: [Scheduled Interleaved Speech-Text Training for Speech-to-Speech Translation with LLMs](https://arxiv.org/abs/2506.10299)
Token length: 1053
Summarized using GPT-3.5-turbo
Append: [Code Execution as Grounded Supervision for LLM Reasoning](https://arxiv.org/abs/2506.10343)
Token length: 1262
Summarized using GPT-3.5-turbo
Append: [TableRAG: A Retrieval Augmented Generation Framework for Heterogeneous Document Reasoning](https://arxiv.org/abs/2506.10380)
Token length: 1261
Summarized using GPT-3.5-turbo
Append: [PAG: Multi-Turn Reinforced LLM Self-Correction with Policy as Generative Verifier](https://arxiv.org/abs/2506.10406)
Token length: 827
Summarized using GPT-3.5-turbo
Append: [Burn After Reading: Do Multimodal Large Language Models Truly Capture Order of Events in Image Sequences?](https://arxiv.org/abs/2506.10415)
Token length: 1033
Summarized using GPT-3.5-turbo
Append: [Beyond the Battlefield: Framing Analysis of Media Coverage in Conflict Reporting](https://arxiv.org/abs/2506.10421)
Token length: 1317
Summarized using GPT-3.5-turbo
Append: [Fast on the Easy, Deep on the Hard: Efficient Reasoning via Powered Length Penalty](https://arxiv.org/abs/2506.10446)
Token length: 1099
Summarized using GPT-3.5-turbo
Append: [Table-Text Alignment: Explaining Claim Verification Against Tables in Scientific Papers](https://arxiv.org/abs/2506.10486)
Token length: 1119
Summarized using GPT-3.5-turbo
Append: [Surface Fairness, Deep Bias: A Comparative Study of Bias in Language Models](https://arxiv.org/abs/2506.10491)
Token length: 1162
Summarized using GPT-3.5-turbo
Append: [Beyond Single-User Dialogue: Assessing Multi-User Dialogue State Tracking Capabilities of Large Language Models](https://arxiv.org/abs/2506.10504)
Token length: 1578
Summarized using GPT-3.5-turbo
Append: [Reliable Reasoning Path: Distilling Effective Guidance for LLM Reasoning with Knowledge Graphs](https://arxiv.org/abs/2506.10508)
Token length: 942
Summarized using GPT-3.5-turbo
Append: [Unsupervised Protoform Reconstruction through Parsimonious Rule-guided Heuristics and Evolutionary Search](https://arxiv.org/abs/2506.10614)
Token length: 910
Summarized using GPT-3.5-turbo
Append: [SDialog: A Python Toolkit for Synthetic Dialogue Generation and Analysis](https://arxiv.org/abs/2506.10622)
Token length: 1149
Summarized using GPT-3.5-turbo
Append: [NeuralNexus at BEA 2025 Shared Task: Retrieval-Augmented Prompting for Mistake Identification in AI Tutors](https://arxiv.org/abs/2506.10627)
Token length: 1009
Summarized using GPT-3.5-turbo
Append: [Spelling-out is not Straightforward: LLMs' Capability of Tokenization from Token to Characters](https://arxiv.org/abs/2506.10641)
Token length: 1254
Summarized using GPT-3.5-turbo
Append: [Large Language Models for Detection of Life-Threatening Texts](https://arxiv.org/abs/2506.10687)
Token length: 618
Summarized using GPT-3.5-turbo
Append: [Inferring Adjective Hypernyms with Language Models to Increase the Connectivity of Open English Wordnet](https://arxiv.org/abs/2506.10715)
Token length: 1363
Summarized using GPT-3.5-turbo
Append: [PREMISE: Scalable and Strategic Prompt Optimization for Efficient Mathematical Reasoning in Large Models](https://arxiv.org/abs/2506.10716)
Token length: 1648
Summarized using GPT-3.5-turbo
Append: [Beyond True or False: Retrieval-Augmented Hierarchical Analysis of Nuanced Claims](https://arxiv.org/abs/2506.10728)
Token length: 1518
Summarized using GPT-3.5-turbo
Append: [TaxoAdapt: Aligning LLM-Based Multidimensional Taxonomy Construction to Evolving Research Corpora](https://arxiv.org/abs/2506.10737)
Token length: 1388
Summarized using GPT-3.5-turbo
Append: [One Tokenizer To Rule Them All: Emergent Language Plasticity via Multilingual Tokenizers](https://arxiv.org/abs/2506.10766)
Token length: 977
Summarized using GPT-3.5-turbo
Append: [Different Questions, Different Models: Fine-Grained Evaluation of Uncertainty and Calibration in Clinical QA with LLMs](https://arxiv.org/abs/2506.10769)
Token length: 1023
Summarized using GPT-3.5-turbo
Append: [Improving Named Entity Transcription with Contextual LLM-based Revision](https://arxiv.org/abs/2506.10779)
Token length: 1568
Summarized using GPT-3.5-turbo
Append: [Mitigating Negative Interference in Multilingual Sequential Knowledge Editing through Null-Space Constraints](https://arxiv.org/abs/2506.10800)
Token length: 1468
Summarized using GPT-3.5-turbo
Append: [ReCUT: Balancing Reasoning Length and Accuracy in LLMs via Stepwise Trails and Preference Optimization](https://arxiv.org/abs/2506.10822)
Token length: 678
Summarized using GPT-3.5-turbo
Append: [CIIR@LiveRAG 2025: Optimizing Multi-Agent Retrieval Augmented Generation through Self-Training](https://arxiv.org/abs/2506.10844)
Token length: 1307
Summarized using GPT-3.5-turbo
Append: [Accelerating Diffusion Large Language Models with SlowFast: The Three Golden Principles](https://arxiv.org/abs/2506.10848)
Token length: 936
Summarized using GPT-3.5-turbo
Append: [Analyzing the relationships between pretraining language, phonetic, tonal, and speaker information in self-supervised speech models](https://arxiv.org/abs/2506.10855)
Token length: 1185
Summarized using GPT-3.5-turbo
Append: [Enhancing Medical Dialogue Generation through Knowledge Refinement and Dynamic Prompt Adjustment](https://arxiv.org/abs/2506.10877)
Token length: 807
Summarized using GPT-3.5-turbo
Append: [Slimming Down LLMs Without Losing Their Minds](https://arxiv.org/abs/2506.10885)
Token length: 1690
Summarized using GPT-3.5-turbo
Append: [Generalization or Hallucination? Understanding Out-of-Context Reasoning in Transformers](https://arxiv.org/abs/2506.10887)
Append: [BioClinical ModernBERT: A State-of-the-Art Long-Context Encoder for Biomedical and Clinical NLP](https://arxiv.org/abs/2506.10896)
Append: [Beyond Gold Standards: Epistemic Ensemble of LLM Judges for Formal Mathematical Reasoning](https://arxiv.org/abs/2506.10903)
Append: [Magistral](https://arxiv.org/abs/2506.10910)
Append: [Decomposing MLP Activations into Interpretable Features via Semi-Nonnegative Matrix Factorization](https://arxiv.org/abs/2506.10920)
Append: [Dynamic Epistemic Friction in Dialogue](https://arxiv.org/abs/2506.10934)
Append: [Domain2Vec: Vectorizing Datasets to Find the Optimal Data Mixture without Training](https://arxiv.org/abs/2506.10952)
Append: [ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark](https://arxiv.org/abs/2506.10960)
Append: [AutoMind: Adaptive Knowledgeable Agent for Automated Data Science](https://arxiv.org/abs/2506.10974)
Append: [How Well Can Reasoning Models Identify and Recover from Unhelpful Thoughts?](https://arxiv.org/abs/2506.10979)
Append: [Multimodal Cinematic Video Synthesis Using Text-to-Image and Audio Generation Models](https://arxiv.org/abs/2506.10005)
Append: [Multimodal Large Language Models: A Survey](https://arxiv.org/abs/2506.10016)
Append: [From Threat to Tool: Leveraging Refusal-Aware Injection Attacks for Safety Alignment](https://arxiv.org/abs/2506.10020)
Append: [Private Memorization Editing: Turning Memorization into a Defense to Strengthen Data Privacy in Large Language Models](https://arxiv.org/abs/2506.10024)
Append: [Evaluation empirique de la s\'ecurisation et de l'alignement de ChatGPT et Gemini: analyse comparative des vuln\'erabilit\'es par exp\'erimentations de jailbreaks](https://arxiv.org/abs/2506.10029)
Append: [GenBreak: Red Teaming Text-to-Image Generators Using Large Language Models](https://arxiv.org/abs/2506.10047)
Append: [Omni-DPO: A Dual-Perspective Paradigm for Dynamic Preference Learning of LLMs](https://arxiv.org/abs/2506.10054)
Append: [One Patient, Many Contexts: Scaling Medical AI Through Contextual Intelligence](https://arxiv.org/abs/2506.10157)
Append: [Disclosure Audits for LLM Agents](https://arxiv.org/abs/2506.10171)
Append: [Prompt Attacks Reveal Superficial Knowledge Removal in Unlearning Methods](https://arxiv.org/abs/2506.10236)
Append: [Discrete Audio Tokens: More Than a Survey!](https://arxiv.org/abs/2506.10274)
Append: [AC/DC: LLM-based Audio Comprehension via Dialogue Continuation](https://arxiv.org/abs/2506.10312)
Append: [Detecting Sockpuppetry on Wikipedia Using Meta-Learning](https://arxiv.org/abs/2506.10314)
Append: [Provably Learning from Language Feedback](https://arxiv.org/abs/2506.10341)
Append: [An Analysis of Datasets, Metrics and Models in Keyphrase Generation](https://arxiv.org/abs/2506.10346)
Append: [Can We Infer Confidential Properties of Training Data from LLMs?](https://arxiv.org/abs/2506.10364)
Append: [Discovering Hierarchical Latent Capabilities of Language Models via Causal Representation Learning](https://arxiv.org/abs/2506.10378)
Append: [Time-IMM: A Dataset and Benchmark for Irregular Multimodal Multivariate Time Series](https://arxiv.org/abs/2506.10412)
Append: [PAL: Probing Audio Encoders via LLMs -- A Study of Information Transfer from Audio Encoders to LLMs](https://arxiv.org/abs/2506.10423)
Append: [Towards Robust Multimodal Emotion Recognition under Missing Modalities and Distribution Shifts](https://arxiv.org/abs/2506.10452)
Append: [Scientists' First Exam: Probing Cognitive Abilities of MLLM via Perception, Understanding, and Reasoning](https://arxiv.org/abs/2506.10521)
Append: [Encoding call-by-push-value in the pi-calculus](https://arxiv.org/abs/2506.10584)
Append: [Deep Learning-Based Digitization of Overlapping ECG Images with Open-Source Python Code](https://arxiv.org/abs/2506.10617)
Append: [Conversational Search: From Fundamentals to Frontiers in the LLM Era](https://arxiv.org/abs/2506.10635)
Append: [Robust Unsupervised Adaptation of a Speech Recogniser Using Entropy Minimisation and Speaker Codes](https://arxiv.org/abs/2506.10653)
Append: [TeleMath: A Benchmark for Large Language Models in Telecom Mathematical Problem Solving](https://arxiv.org/abs/2506.10674)
Append: [Neural at ArchEHR-QA 2025: Agentic Prompt Optimization for Evidence-Grounded Clinical Question Answering](https://arxiv.org/abs/2506.10751)
Append: [FASCIST-O-METER: Classifier for Neo-fascist Discourse Online](https://arxiv.org/abs/2506.10789)
Append: [VideoDeepResearch: Long Video Understanding With Agentic Tool Using](https://arxiv.org/abs/2506.10821)
Append: [The Diffusion Duality](https://arxiv.org/abs/2506.10892)
Append: [Breaking Bad Molecules: Are MLLMs Ready for Structure-Level Molecular Detoxification?](https://arxiv.org/abs/2506.10912)
Append: [Robustly Improving LLM Fairness in Realistic Settings via Interpretability](https://arxiv.org/abs/2506.10922)
Append: [VINCIE: Unlocking In-context Image Editing from Video](https://arxiv.org/abs/2506.10941)
Append: [GUARD: Guided Unlearning and Retention via Data Attribution for Large Language Models](https://arxiv.org/abs/2506.10946)
Append: [Build the web for agents, not agents for the web](https://arxiv.org/abs/2506.10953)
Append: [MMMG: A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for Text-to-Image Reasoning](https://arxiv.org/abs/2506.10963)
Append: [ConvD: Attention Enhanced Dynamic Convolutional Embeddings for Knowledge Graph Completion](https://arxiv.org/abs/2312.07589)
Append: [Weak-to-Strong Jailbreaking on Large Language Models](https://arxiv.org/abs/2401.17256)
Append: [Visually Descriptive Language Model for Vector Graphics Reasoning](https://arxiv.org/abs/2404.06479)
Append: [IndoToxic2024: A Demographically-Enriched Dataset of Hate Speech and Toxicity Types for Indonesian Language](https://arxiv.org/abs/2406.19349)
Append: [Benchmarking LLMs for Environmental Review and Permitting](https://arxiv.org/abs/2407.07321)
Append: [Multi-group Uncertainty Quantification for Long-form Text Generation](https://arxiv.org/abs/2407.21057)
Append: [SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language Models](https://arxiv.org/abs/2408.08545)
Append: [The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization Dataset Generation](https://arxiv.org/abs/2408.08688)
Append: [Paired Completion: Flexible Quantification of Issue-framing at Scale with LLMs](https://arxiv.org/abs/2408.09742)
Append: [Efficient Length-Generalizable Attention via Causal Retrieval for Long-Context Language Modeling](https://arxiv.org/abs/2410.01651)
Append: [Efficiently Identifying Watermarked Segments in Mixed-Source Texts](https://arxiv.org/abs/2410.03600)
Append: [Persistent Topological Features in Large Language Models](https://arxiv.org/abs/2410.11042)
Append: [On Many-Shot In-Context Learning for Long-Context Evaluation](https://arxiv.org/abs/2411.07130)
Append: [Squeezed Attention: Accelerating Long Context Length LLM Inference](https://arxiv.org/abs/2411.09688)
Append: [Prompt-based Depth Pruning of Large Language Models](https://arxiv.org/abs/2502.04348)
Append: [Pragmatics in the Era of Large Language Models: A Survey on Datasets, Evaluation, Opportunities and Challenges](https://arxiv.org/abs/2502.12378)
Append: [BeamLoRA: Beam-Constraint Low-Rank Adaptation](https://arxiv.org/abs/2502.13604)
Append: [Measuring Chain of Thought Faithfulness by Unlearning Reasoning Steps](https://arxiv.org/abs/2502.14829)
Append: [Obliviate: Efficient Unmemorization for Protecting Intellectual Property in Large Language Models](https://arxiv.org/abs/2502.15010)
Append: [Mind the Style Gap: Meta-Evaluation of Style and Attribute Transfer Metrics](https://arxiv.org/abs/2502.15022)
Append: [The Esethu Framework: Reimagining Sustainable Dataset Governance and Curation for Low-Resource Languages](https://arxiv.org/abs/2502.15916)
Append: [Amulet: ReAlignment During Test Time for Personalized Preference Adaptation of LLMs](https://arxiv.org/abs/2502.19148)
Append: [Large Language Models for Multilingual Previously Fact-Checked Claim Detection](https://arxiv.org/abs/2503.02737)
Append: [Improving LLM Safety Alignment with Dual-Objective Optimization](https://arxiv.org/abs/2503.03710)
Append: [Social Bias Benchmark for Generation: A Comparison of Generation and QA-Based Evaluations](https://arxiv.org/abs/2503.06987)
Append: [Large Language Models for Outpatient Referral: Problem Definition, Benchmarking and Challenges](https://arxiv.org/abs/2503.08292)
Append: [Computation Mechanism Behind LLM Position Generalization](https://arxiv.org/abs/2503.13305)
Append: [PLAY2PROMPT: Zero-shot Tool Instruction Optimization for LLM Agents via Tool Play](https://arxiv.org/abs/2503.14432)
Append: [SCORE: Story Coherence and Retrieval Enhancement for AI Narratives](https://arxiv.org/abs/2503.23512)
Append: [IPA-CHILDES & G2P+: Feature-Rich Resources for Cross-Lingual Phonology and Phonemic Language Modeling](https://arxiv.org/abs/2504.03036)
Append: [BabyLM's First Words: Word Segmentation as a Phonological Probing Task](https://arxiv.org/abs/2504.03338)
Append: [M-MRE: Extending the Mutual Reinforcement Effect to Multimodal Information Extraction](https://arxiv.org/abs/2504.17353)
Append: [Building UD Cairo for Old English in the Classroom](https://arxiv.org/abs/2504.18718)
Append: [Sailing by the Stars: A Survey on Reward Models and Learning Strategies for Learning from Rewards](https://arxiv.org/abs/2505.02686)
Append: [Research Borderlands: Analysing Writing Across Research Cultures](https://arxiv.org/abs/2506.00784)
Append: [SealQA: Raising the Bar for Reasoning in Search-Augmented Language Models](https://arxiv.org/abs/2506.01062)
Append: [iQUEST: An Iterative Question-Guided Framework for Knowledge Base Question Answering](https://arxiv.org/abs/2506.01784)
Append: [Mitigating Object Hallucination in Large Vision-Language Models via Image-Grounded Guidance](https://arxiv.org/abs/2402.08680)
Append: [PRSA: Prompt Stealing Attacks against Real-World Prompt Services](https://arxiv.org/abs/2402.19200)
Append: [Failure Modes of LLMs for Causal Reasoning on Narratives](https://arxiv.org/abs/2410.23884)
Append: [Debiasing Watermarks for Large Language Models via Maximal Coupling](https://arxiv.org/abs/2411.11203)
Append: [Great Models Think Alike and this Undermines AI Oversight](https://arxiv.org/abs/2502.04313)
Append: [A Minimalist Approach to LLM Reasoning: from Rejection Sampling to Reinforce](https://arxiv.org/abs/2504.11343)
Append: [AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving](https://arxiv.org/abs/2505.15298)
Append: [VScan: Rethinking Visual Token Reduction for Efficient Large Vision-Language Models](https://arxiv.org/abs/2505.22654)
Append: [The Automated but Risky Game: Modeling Agent-to-Agent Negotiations and Transactions in Consumer Markets](https://arxiv.org/abs/2506.00073)
Append: [Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models](https://arxiv.org/abs/2506.01413)
Append: [CHANCERY: Evaluating Corporate Governance Reasoning Capabilities in Language Models](https://arxiv.org/abs/2506.04636)
Append: [FedRAG: A Framework for Fine-Tuning Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2506.09200)
append_entries: 144
Finish: 2025-06-13 04:30:25.623282
------------------------------------------------------
Started: 2025-06-13 06:25:30.616398
Existing_entries: 1144
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1014
Summarized using GPT-3.5-turbo
Append: [Identifying Reliable Evaluation Metrics for Scientific Text Revision](https://arxiv.org/abs/2506.04772)
Token length: 1159
Summarized using GPT-3.5-turbo
Append: [Context Is Not Comprehension](https://arxiv.org/abs/2506.04907)
append_entries: 2
Finish: 2025-06-13 06:25:34.925491
------------------------------------------------------
Started: 2025-06-13 08:22:45.594802
Existing_entries: 1002
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-13 08:22:45.952744
------------------------------------------------------
Started: 2025-06-13 10:18:31.793981
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-13 10:18:32.267309
------------------------------------------------------
Started: 2025-06-13 12:34:29.461747
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-13 12:34:29.825573
------------------------------------------------------
Started: 2025-06-13 14:16:45.536597
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-13 14:16:45.872708
------------------------------------------------------
Started: 2025-06-13 16:21:07.790554
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-13 16:21:08.144450
------------------------------------------------------
Started: 2025-06-13 18:23:35.792284
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-13 18:23:36.158667
------------------------------------------------------
Started: 2025-06-13 20:18:25.304133
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-13 20:18:25.731964
------------------------------------------------------
Started: 2025-06-13 22:16:00.474079
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-13 22:16:00.820197
------------------------------------------------------
Started: 2025-06-14 01:18:05.558142
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-14 01:18:05.904606
------------------------------------------------------
Started: 2025-06-14 03:09:46.767517
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-14 03:09:47.119832
------------------------------------------------------
Started: 2025-06-14 04:20:28.613627
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-14 04:20:28.676882
------------------------------------------------------
Started: 2025-06-14 06:22:13.792303
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-14 06:22:13.855355
------------------------------------------------------
Started: 2025-06-14 08:19:47.543123
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-14 08:19:47.617712
------------------------------------------------------
Started: 2025-06-14 10:16:20.968631
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-14 10:16:21.026051
------------------------------------------------------
Started: 2025-06-14 12:31:03.004511
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-14 12:31:03.109193
------------------------------------------------------
Started: 2025-06-14 14:14:01.968645
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-14 14:14:02.030481
------------------------------------------------------
Started: 2025-06-14 16:19:04.997302
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-14 16:19:05.074649
------------------------------------------------------
Started: 2025-06-14 18:20:36.348469
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-14 18:20:36.410831
------------------------------------------------------
Started: 2025-06-14 20:16:53.502558
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-14 20:16:53.613041
------------------------------------------------------
Started: 2025-06-14 22:15:26.618558
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-14 22:15:26.678122
------------------------------------------------------
Started: 2025-06-15 01:37:02.032821
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-15 01:37:02.093431
------------------------------------------------------
Started: 2025-06-15 03:22:05.100443
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-15 03:22:05.184858
------------------------------------------------------
Started: 2025-06-15 04:26:28.488459
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-15 04:26:28.545407
------------------------------------------------------
Started: 2025-06-15 06:22:20.947827
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-15 06:22:21.021354
------------------------------------------------------
Started: 2025-06-15 08:19:52.354240
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-15 08:19:52.413587
------------------------------------------------------
Started: 2025-06-15 10:17:30.061432
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-15 10:17:30.131830
------------------------------------------------------
Started: 2025-06-15 12:31:24.917332
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-15 12:31:24.977664
------------------------------------------------------
Started: 2025-06-15 14:14:06.731638
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-15 14:14:06.786544
------------------------------------------------------
Started: 2025-06-15 16:19:08.420063
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-15 16:19:08.508015
------------------------------------------------------
Started: 2025-06-15 18:21:16.252466
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-15 18:21:16.344721
------------------------------------------------------
Started: 2025-06-15 20:17:23.065907
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-15 20:17:23.123213
------------------------------------------------------
Started: 2025-06-15 22:15:17.539911
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-15 22:15:17.602232
------------------------------------------------------
Started: 2025-06-16 01:23:30.845755
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-16 01:23:30.917893
------------------------------------------------------
Started: 2025-06-16 03:19:45.643135
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-16 03:19:45.743046
------------------------------------------------------
Started: 2025-06-16 04:32:45.645366
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1826
Summarized using GPT-3.5-turbo
Append: [TeleEval-OS: Performance evaluations of large language models for operations scheduling](https://arxiv.org/abs/2506.11017)
Token length: 1448
Summarized using GPT-3.5-turbo
Append: [Who is in the Spotlight: The Hidden Bias Undermining Multimodal Retrieval-Augmented Generation](https://arxiv.org/abs/2506.11063)
Token length: 858
Summarized using GPT-3.5-turbo
Append: [Smotrom tvoja pa ander drogoj verden! Resurrecting Dead Pidgin with Generative Models: Russenorsk Case Study](https://arxiv.org/abs/2506.11065)
Token length: 1195
Summarized using GPT-3.5-turbo
Append: [A Large Language Model Based Pipeline for Review of Systems Entity Recognition from Clinical Notes](https://arxiv.org/abs/2506.11067)
Token length: 1079
Summarized using GPT-3.5-turbo
Append: [Deontological Keyword Bias: The Impact of Modal Expressions on Normative Judgments of Language Models](https://arxiv.org/abs/2506.11068)
Token length: 1291
Summarized using GPT-3.5-turbo
Append: [Targeted control of fast prototyping through domain-specific interface](https://arxiv.org/abs/2506.11070)
Token length: 1390
Summarized using GPT-3.5-turbo
Append: [CLAIM: Mitigating Multilingual Object Hallucination in Large Vision-Language Models with Cross-Lingual Attention Intervention](https://arxiv.org/abs/2506.11073)
Token length: 1597
Summarized using GPT-3.5-turbo
Append: [CyclicReflex: Improving Large Reasoning Models via Cyclical Reflection Token Scheduling](https://arxiv.org/abs/2506.11077)
Token length: 1536
Summarized using GPT-3.5-turbo
Append: [RoE-FND: A Case-Based Reasoning Approach with Dual Verification for Fake News Detection via LLMs](https://arxiv.org/abs/2506.11078)
Token length: 1405
Summarized using GPT-3.5-turbo
Append: [MANBench: Is Your Multimodal Model Smarter than Human?](https://arxiv.org/abs/2506.11080)
Token length: 1459
Summarized using GPT-3.5-turbo
Append: [SAGE:Specification-Aware Grammar Extraction for Automated Test Case Generation with LLMs](https://arxiv.org/abs/2506.11081)
Token length: 1351
Summarized using GPT-3.5-turbo
Append: [PRISM: A Transformer-based Language Model of Structured Clinical Event Data](https://arxiv.org/abs/2506.11082)
Token length: 1297
Summarized using GPT-3.5-turbo
Append: [RedDebate: Safer Responses through Multi-Agent Red Teaming Debates](https://arxiv.org/abs/2506.11083)
Token length: 1159
Summarized using GPT-3.5-turbo
Append: [Two Birds with One Stone: Improving Factuality and Faithfulness of LLMs via Dynamic Interactive Subspace Editing](https://arxiv.org/abs/2506.11088)
Token length: 978
Summarized using GPT-3.5-turbo
Append: [Customizing Speech Recognition Model with Large Language Model Feedback](https://arxiv.org/abs/2506.11091)
Token length: 1227
Summarized using GPT-3.5-turbo
Append: [Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing Multi-Turn Planning and Tool Adaptation](https://arxiv.org/abs/2506.11092)
Token length: 1925
Summarized using GPT-3.5-turbo
Append: [The Scales of Justitia: A Comprehensive Survey on Safety Evaluation of LLMs](https://arxiv.org/abs/2506.11094)
Token length: 1167
Summarized using GPT-3.5-turbo
Append: [Persistent Homology of Topic Networks for the Prediction of Reader Curiosity](https://arxiv.org/abs/2506.11095)
Token length: 1744
Summarized using GPT-3.5-turbo
Append: [C-SEO Bench: Does Conversational SEO Work?](https://arxiv.org/abs/2506.11097)
Token length: 1583
Summarized using GPT-3.5-turbo
Append: [Evolutionary Perspectives on the Evaluation of LLM-Based AI Agents: A Comprehensive Survey](https://arxiv.org/abs/2506.11102)
Token length: 1653
Summarized using GPT-3.5-turbo
Append: [You Only Fine-tune Once: Many-Shot In-Context Fine-Tuning for Large Language Model](https://arxiv.org/abs/2506.11103)
Token length: 1224
Summarized using GPT-3.5-turbo
Append: [DAM: Dynamic Attention Mask for Long-Context Large Language Model Inference Acceleration](https://arxiv.org/abs/2506.11104)
Token length: 1022
Summarized using GPT-3.5-turbo
Append: [Enabling On-Device Medical AI Assistants via Input-Driven Saliency Adaptation](https://arxiv.org/abs/2506.11105)
Token length: 1381
Summarized using GPT-3.5-turbo
Append: [Graph-based RAG Enhancement via Global Query Disambiguation and Dependency-Aware Reranking](https://arxiv.org/abs/2506.11106)
Token length: 848
Summarized using GPT-3.5-turbo
Append: [History-Aware Cross-Attention Reinforcement: Self-Supervised Multi Turn and Chain-of-Thought Fine-Tuning with vLLM](https://arxiv.org/abs/2506.11108)
Token length: 1518
Summarized using GPT-3.5-turbo
Append: [Enhancing Large Language Models for Mobility Analytics with Semantic Location Tokenization](https://arxiv.org/abs/2506.11109)
Token length: 1261
Summarized using GPT-3.5-turbo
Append: [AssertBench: A Benchmark for Evaluating Self-Assertion in Large Language Models](https://arxiv.org/abs/2506.11110)
Token length: 1858
Summarized using GPT-3.5-turbo
Append: [Evaluating and Improving Robustness in Large Language Models: A Survey and Future Directions](https://arxiv.org/abs/2506.11111)
Token length: 619
Summarized using GPT-3.5-turbo
Append: [Manifesto from Dagstuhl Perspectives Workshop 24352 -- Conversational Agents: A Framework for Evaluation (CAFE)](https://arxiv.org/abs/2506.11112)
Token length: 1131
Summarized using GPT-3.5-turbo
Append: [Breaking the Reviewer: Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks](https://arxiv.org/abs/2506.11113)
Token length: 1369
Summarized using GPT-3.5-turbo
Append: [KokushiMD-10: Benchmark for Evaluating Large Language Models on Ten Japanese National Healthcare Licensing Examinations](https://arxiv.org/abs/2506.11114)
Token length: 1214
Summarized using GPT-3.5-turbo
Append: [Incorporating Domain Knowledge into Materials Tokenization](https://arxiv.org/abs/2506.11115)
Token length: 1582
Summarized using GPT-3.5-turbo
Append: [Infinity Instruct: Scaling Instruction Selection and Synthesis to Enhance Language Models](https://arxiv.org/abs/2506.11116)
Token length: 1733
Summarized using GPT-3.5-turbo
Append: [ScIRGen: Synthesize Realistic and Large-Scale RAG Dataset for Scientific Research](https://arxiv.org/abs/2506.11117)
Token length: 1747
Summarized using GPT-3.5-turbo
Append: [Benchmarking Foundation Speech and Language Models for Alzheimer's Disease and Related Dementia Detection from Spontaneous Speech](https://arxiv.org/abs/2506.11119)
Token length: 1321
Summarized using GPT-3.5-turbo
Append: [SDMPrune: Self-Distillation MLP Pruning for Efficient Large Language Models](https://arxiv.org/abs/2506.11120)
Token length: 1041
Summarized using GPT-3.5-turbo
Append: [SUTA-LM: Bridging Test-Time Adaptation and Language Model Rescoring for Robust ASR](https://arxiv.org/abs/2506.11121)
Token length: 1191
Summarized using GPT-3.5-turbo
Append: [ASRJam: Human-Friendly AI Speech Jamming to Prevent Automated Phone Scams](https://arxiv.org/abs/2506.11125)
Token length: 1322
Summarized using GPT-3.5-turbo
Append: [GUIRoboTron-Speech: Towards Automated GUI Agents Based on Speech Instructions](https://arxiv.org/abs/2506.11127)
Token length: 1474
Summarized using GPT-3.5-turbo
Append: [Stronger Language Models Produce More Human-Like Errors](https://arxiv.org/abs/2506.11128)
Token length: 1109
Summarized using GPT-3.5-turbo
Append: [Trustworthy AI for Medicine: Continuous Hallucination Detection and Elimination with CHECK](https://arxiv.org/abs/2506.11129)
Token length: 1051
Summarized using GPT-3.5-turbo
Append: [A Self-Refining Framework for Enhancing ASR Using TTS-Synthesized Data](https://arxiv.org/abs/2506.11130)
Token length: 765
Summarized using GPT-3.5-turbo
Append: [Large Language Models and Emergence: A Complex Systems Perspective](https://arxiv.org/abs/2506.11135)
Token length: 1862
Summarized using GPT-3.5-turbo
Append: [Scalable Medication Extraction and Discontinuation Identification from Electronic Health Records Using Large Language Models](https://arxiv.org/abs/2506.11137)
Token length: 1184
Summarized using GPT-3.5-turbo
Append: [RETUYT-INCO at BEA 2025 Shared Task: How Far Can Lightweight Models Go in AI-powered Tutor Evaluation?](https://arxiv.org/abs/2506.11243)
Token length: 999
Summarized using GPT-3.5-turbo
Append: [Iterative Multilingual Spectral Attribute Erasure](https://arxiv.org/abs/2506.11244)
Token length: 1275
Summarized using GPT-3.5-turbo
Append: [No Universal Prompt: Unifying Reasoning through Adaptive Prompting for Temporal Table Reasoning](https://arxiv.org/abs/2506.11246)
Token length: 1263
Summarized using GPT-3.5-turbo
Append: [Learning a Continue-Thinking Token for Enhanced Test-Time Scaling](https://arxiv.org/abs/2506.11274)
Token length: 1165
Summarized using GPT-3.5-turbo
Append: [Beyond Random Sampling: Efficient Language Model Pretraining via Curriculum Learning](https://arxiv.org/abs/2506.11300)
Token length: 1333
Summarized using GPT-3.5-turbo
Append: [Don't Pay Attention](https://arxiv.org/abs/2506.11305)
Append: [Surprisal from Larger Transformer-based Language Models Predicts fMRI Data More Poorly](https://arxiv.org/abs/2506.11338)
Append: [From Replication to Redesign: Exploring Pairwise Comparisons for LLM-Based Peer Review](https://arxiv.org/abs/2506.11343)
Append: [Do We Still Need Audio? Rethinking Speaker Diarization with a Text-Based Approach Using Multiple Prediction Models](https://arxiv.org/abs/2506.11344)
Append: [The Biased Samaritan: LLM biases in Perceived Kindness](https://arxiv.org/abs/2506.11361)
Append: [A Variational Approach for Mitigating Entity Bias in Relation Extraction](https://arxiv.org/abs/2506.11381)
Append: [Curriculum-Guided Layer Scaling for Language Model Pretraining](https://arxiv.org/abs/2506.11389)
Append: [Predicting Early-Onset Colorectal Cancer with Large Language Models](https://arxiv.org/abs/2506.11410)
Append: [Efficient Long-Context LLM Inference via KV Cache Clustering](https://arxiv.org/abs/2506.11418)
Append: [Agent-RLVR: Training Software Engineering Agents via Guidance and Environment Rewards](https://arxiv.org/abs/2506.11425)
Append: [KoGEC : Korean Grammatical Error Correction with Pre-trained Translation Models](https://arxiv.org/abs/2506.11432)
Append: [AbsenceBench: Language Models Can't Tell What's Missing](https://arxiv.org/abs/2506.11440)
Append: [A Gamified Evaluation and Recruitment Platform for Low Resource Language Machine Translation Systems](https://arxiv.org/abs/2506.11467)
Append: [Med-PRM: Medical Reasoning Models with Stepwise, Guideline-verified Process Rewards](https://arxiv.org/abs/2506.11474)
Append: [ImmunoFOMO: Are Language Models missing what oncologists see?](https://arxiv.org/abs/2506.11478)
Append: [Relational Schemata in BERT Are Inducible, Not Emergent: A Study of Performance vs. Competence in Language Models](https://arxiv.org/abs/2506.11485)
Append: [Lag-Relative Sparse Attention In Long Context Training](https://arxiv.org/abs/2506.11498)
Append: [On the Effectiveness of Integration Methods for Multimodal Dialogue Response Retrieval](https://arxiv.org/abs/2506.11499)
Append: [From Persona to Person: Enhancing the Naturalness with Multiple Discourse Relations Graph Learning in Personalized Dialogue Generation](https://arxiv.org/abs/2506.11557)
Append: [Are LLMs Good Text Diacritizers? An Arabic and Yor\`ub\'a Case Study](https://arxiv.org/abs/2506.11602)
Append: [SceneGram: Conceptualizing and Describing Tangrams in Scene Context](https://arxiv.org/abs/2506.11631)
Append: [LoRA-Gen: Specializing Large Language Model via Online LoRA Generation](https://arxiv.org/abs/2506.11638)
Append: [Converting Annotated Clinical Cases into Structured Case Report Forms](https://arxiv.org/abs/2506.11666)
Append: [Improving Causal Interventions in Amnesic Probing with Mean Projection or LEACE](https://arxiv.org/abs/2506.11673)
Append: [LLMs for Sentence Simplification: A Hybrid Multi-Agent prompting Approach](https://arxiv.org/abs/2506.11681)
Append: [Configurable Preference Tuning with Rubric-Guided Synthetic Data](https://arxiv.org/abs/2506.11702)
Append: [The Cambrian Explosion of Mixed-Precision Matrix Multiplication for Quantized Deep Learning Inference](https://arxiv.org/abs/2506.11728)
Append: [DART: Distilling Autoregressive Reasoning to Silent Thought](https://arxiv.org/abs/2506.11752)
Append: [DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents](https://arxiv.org/abs/2506.11763)
Append: [Long-Short Alignment for Effective Long-Context Modeling in LLMs](https://arxiv.org/abs/2506.11769)
Append: [Persona-driven Simulation of Voting Behavior in the European Parliament with Large Language Models](https://arxiv.org/abs/2506.11798)
Append: [Are Multimodal Large Language Models Pragmatically Competent Listeners in Simple Reference Resolution Tasks?](https://arxiv.org/abs/2506.11807)
Append: [Post Persona Alignment for Multi-Session Dialogue Generation](https://arxiv.org/abs/2506.11857)
Append: [Beyond Homogeneous Attention: Memory-Efficient LLMs via Fourier-Approximated KV Cache](https://arxiv.org/abs/2506.11886)
Append: [GeistBERT: Breathing Life into German NLP](https://arxiv.org/abs/2506.11903)
Append: [Effectiveness of Counter-Speech against Abusive Content: A Multidimensional Annotation and Classification Study](https://arxiv.org/abs/2506.11919)
Append: [Feedback Friction: LLMs Struggle to Fully Incorporate External Feedback](https://arxiv.org/abs/2506.11930)
Append: [Improving Large Language Model Safety with Contrastive Representation Learning](https://arxiv.org/abs/2506.11938)
Append: [code_transformed: The Influence of Large Language Models on Code](https://arxiv.org/abs/2506.12014)
Append: [Developing a Dyslexia Indicator Using Eye Tracking](https://arxiv.org/abs/2506.11004)
Append: [A Survey of Task-Oriented Knowledge Graph Reasoning: Status, Applications, and Prospects](https://arxiv.org/abs/2506.11012)
Append: [Security Degradation in Iterative AI Code Generation -- A Systematic Analysis of the Paradox](https://arxiv.org/abs/2506.11022)
Append: [Task-aligned prompting improves zero-shot detection of AI-generated images by Vision-Language Models](https://arxiv.org/abs/2506.11031)
Append: [CausalVLBench: Benchmarking Visual Causal Reasoning in Large Vision-Language Models](https://arxiv.org/abs/2506.11034)
Append: [Tversky Neural Networks: Psychologically Plausible Deep Learning with Differentiable Tversky Similarity](https://arxiv.org/abs/2506.11035)
Append: [Large Language models for Time Series Analysis: Techniques, Applications, and Challenges](https://arxiv.org/abs/2506.11040)
Append: [CodeMirage: A Multi-Lingual Benchmark for Detecting AI-Generated and Paraphrased Source Code from Production-Level LLMs](https://arxiv.org/abs/2506.11059)
Append: [PMF-CEC: Phoneme-augmented Multimodal Fusion for Context-aware ASR Error Correction with Error-specific Selective Decoding](https://arxiv.org/abs/2506.11064)
Append: [Regularized Federated Learning for Privacy-Preserving Dysarthric and Elderly Speech Recognition](https://arxiv.org/abs/2506.11069)
Append: [Can We Trust Machine Learning? The Reliability of Features from Open-Source Speech Analysis Tools for Speech Modeling](https://arxiv.org/abs/2506.11072)
Append: [Improving Child Speech Recognition and Reading Mistake Detection by Using Prompts](https://arxiv.org/abs/2506.11079)
Append: [LeanExplore: A search engine for Lean 4 declarations](https://arxiv.org/abs/2506.11085)
Append: [ADAMIX: Adaptive Mixed-Precision Delta-Compression with Quantization Error Optimization for Large Language Models](https://arxiv.org/abs/2506.11087)
Append: [Better Pseudo-labeling with Multi-ASR Fusion and Error Correction by SpeechLLM](https://arxiv.org/abs/2506.11089)
Append: [Assessing the Impact of Anisotropy in Neural Representations of Speech: A Case Study on Keyword Spotting](https://arxiv.org/abs/2506.11096)
Append: [Knowledge Graph Embeddings with Representing Relations as Annular Sectors](https://arxiv.org/abs/2506.11099)
Append: [LLM-as-a-Fuzzy-Judge: Fine-Tuning Large Language Models as a Clinical Evaluation Judge with Fuzzy Logic](https://arxiv.org/abs/2506.11221)
Append: [LLM-as-a-Judge for Reference-less Automatic Code Validation and Refinement for Natural Language to Bash in IT Automation](https://arxiv.org/abs/2506.11237)
Append: [GLAP: General contrastive audio-text pretraining across domains and languages](https://arxiv.org/abs/2506.11350)
Append: [Benchmarking Multimodal LLMs on Recognition and Understanding over Chemical Tables](https://arxiv.org/abs/2506.11375)
Append: [Large Language Model-Powered Conversational Agent Delivering Problem-Solving Therapy (PST) for Family Caregivers: Enhancing Empathy and Therapeutic Alliance Using In-Context Learning](https://arxiv.org/abs/2506.11376)
Append: [LoRA Users Beware: A Few Spurious Tokens Can Manipulate Your Finetuned Model](https://arxiv.org/abs/2506.11402)
Append: [Bias Amplification in RAG: Poisoning Knowledge Retrieval to Steer LLMs](https://arxiv.org/abs/2506.11415)
Append: [AutoGen Driven Multi Agent Framework for Iterative Crime Data Analysis and Prediction](https://arxiv.org/abs/2506.11475)
Append: [Manager: Aggregating Insights from Unimodal Experts in Two-Tower VLMs and MLLMs](https://arxiv.org/abs/2506.11515)
Append: [Brewing Knowledge in Context: Distillation Perspectives on In-Context Learning](https://arxiv.org/abs/2506.11516)
Append: [RAG+: Enhancing Retrieval-Augmented Generation with Application-Aware Reasoning](https://arxiv.org/abs/2506.11555)
Append: [DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning with Video LLMs](https://arxiv.org/abs/2506.11558)
Append: [VLM@school -- Evaluation of AI image understanding on German middle school knowledge](https://arxiv.org/abs/2506.11604)
Append: [(SimPhon Speech Test): A Data-Driven Method for In Silico Design and Validation of a Phonetically Balanced Speech Test](https://arxiv.org/abs/2506.11620)
Append: [Quizzard@INOVA Challenge 2025 -- Track A: Plug-and-Play Technique in Interleaved Multi-Image Model](https://arxiv.org/abs/2506.11737)
Append: [On the Performance of LLMs for Real Estate Appraisal](https://arxiv.org/abs/2506.11812)
Append: [Rethinking Multilingual Vision-Language Translation: Dataset, Evaluation, and Adaptation](https://arxiv.org/abs/2506.11820)
Append: [Addressing Bias in LLMs: Strategies and Application to Fair AI-based Recruitment](https://arxiv.org/abs/2506.11880)
Append: [Towards a Cascaded LLM Framework for Cost-effective Human-AI Decision-Making](https://arxiv.org/abs/2506.11887)
Append: [TreeRL: LLM Reinforcement Learning with On-Policy Tree Search](https://arxiv.org/abs/2506.11902)
Append: [LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive Programming?](https://arxiv.org/abs/2506.11928)
Append: [Schema-R1: A reasoning training approach for schema linking in Text-to-SQL Task](https://arxiv.org/abs/2506.11986)
Append: [VGR: Visual Grounded Reasoning](https://arxiv.org/abs/2506.11991)
Append: [Generative Representational Learning of Foundation Models for Recommendation](https://arxiv.org/abs/2506.11999)
Append: [FlashBack:Efficient Retrieval-Augmented Language Modeling for Long Context Inference](https://arxiv.org/abs/2405.04065)
Append: [JBBQ: Japanese Bias Benchmark for Analyzing Social Biases in Large Language Models](https://arxiv.org/abs/2406.02050)
Append: [Evaluating Implicit Bias in Large Language Models by Attacking From a Psychometric Perspective](https://arxiv.org/abs/2406.14023)
Append: [TrajAgent: An LLM-based Agent Framework for Automated Trajectory Modeling via Collaboration of Large and Small Models](https://arxiv.org/abs/2410.20445)
Append: [Deep Sparse Latent Feature Models for Knowledge Graph Completion](https://arxiv.org/abs/2411.15694)
Append: [MapQaTor: An Extensible Framework for Efficient Annotation of Map-Based QA Datasets](https://arxiv.org/abs/2412.21015)
Append: [Women, Infamous, and Exotic Beings: What Honorific Usages in Wikipedia Reflect on the Cross-Cultural Sociolinguistic Norms?](https://arxiv.org/abs/2501.03479)
Append: [Factual Knowledge in Language Models: Robustness and Anomalies under Simple Temporal Context Variations](https://arxiv.org/abs/2502.01220)
Append: [PANDAS: Improving Many-shot Jailbreaking via Positive Affirmation, Negative Demonstration, and Adaptive Sampling](https://arxiv.org/abs/2502.01925)
Append: [TUMLU: A Unified and Native Language Understanding Benchmark for Turkic Languages](https://arxiv.org/abs/2502.11020)
Append: [Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit Analysis](https://arxiv.org/abs/2502.11812)
Append: [Conformal Linguistic Calibration: Trading-off between Factuality and Specificity](https://arxiv.org/abs/2502.19110)
Append: [MEDDxAgent: A Unified Modular Agent Framework for Explainable Automatic Differential Diagnosis](https://arxiv.org/abs/2502.19175)
Append: [How Much is Enough? The Diminishing Returns of Tokenization Training Data](https://arxiv.org/abs/2502.20273)
Append: [PFDial: A Structured Dialogue Instruction Fine-tuning Method Based on UML Flowcharts](https://arxiv.org/abs/2503.06706)
Append: [Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts](https://arxiv.org/abs/2503.09347)
Append: [LLaVA-CMoE: Towards Continual Mixture of Experts for Large Vision-Language Models](https://arxiv.org/abs/2503.21227)
Append: [Deep Binding of Language Model Virtual Personas: a Study on Approximating Political Partisan Misperceptions](https://arxiv.org/abs/2504.11673)
Append: [D-GEN: Automatic Distractor Generation and Evaluation for Reliable Assessment of Generative Model](https://arxiv.org/abs/2504.13439)
Append: [Long-context Non-factoid Question Answering in Indic Languages](https://arxiv.org/abs/2504.13615)
Append: [Understanding the Repeat Curse in Large Language Models from a Feature Perspective](https://arxiv.org/abs/2504.14218)
Append: [BitNet v2: Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs](https://arxiv.org/abs/2504.18415)
Append: [Table-R1: Region-based Reinforcement Learning for Table Understanding](https://arxiv.org/abs/2505.12415)
Append: [Can reasoning models comprehend mathematical problems in Chinese ancient texts? An empirical study based on data from Suanjing Shishu](https://arxiv.org/abs/2505.16660)
Append: [Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English](https://arxiv.org/abs/2505.17076)
Append: [RSCF: Relation-Semantics Consistent Filter for Entity Embedding of Knowledge Graph](https://arxiv.org/abs/2505.20813)
Append: [Explainability of Large Language Models using SMILE: Statistical Model-agnostic Interpretability with Local Explanations](https://arxiv.org/abs/2505.21657)
Append: [Automatic Construction of Multiple Classification Dimensions for Managing Approaches in Scientific Papers](https://arxiv.org/abs/2505.23252)
Append: [Improving the Calibration of Confidence Scores in Text Generation Using the Output Distribution's Characteristics](https://arxiv.org/abs/2506.00637)
Append: [VM14K: First Vietnamese Medical Benchmark](https://arxiv.org/abs/2506.01305)
Append: [Word Sense Detection Leveraging Maximum Mean Discrepancy](https://arxiv.org/abs/2506.01602)
Append: [LLMEval-Med: A Real-world Clinical Benchmark for Medical LLMs with Physician Validation](https://arxiv.org/abs/2506.04078)
Append: [MAGPIE: Multi-Task Media-Bias Analysis Generalization for Pre-Trained Identification of Expressions](https://arxiv.org/abs/2403.07910)
Append: [Ad Auctions for LLMs via Retrieval Augmented Generation](https://arxiv.org/abs/2406.09459)
Append: [An overview of domain-specific foundation model: key technologies, applications and challenges](https://arxiv.org/abs/2409.04267)
Append: [Jointly modelling the evolution of social structure and language in online communities](https://arxiv.org/abs/2409.19243)
Append: [Glider: Global and Local Instruction-Driven Expert Router](https://arxiv.org/abs/2410.07172)
Append: [Attuned to Change: Causal Fine-Tuning under Latent-Confounded Shifts](https://arxiv.org/abs/2410.14375)
Append: [Transferable Post-training via Inverse Value Learning](https://arxiv.org/abs/2410.21027)
Append: [Entropy Controllable Direct Preference Optimization](https://arxiv.org/abs/2411.07595)
Append: [T1: Advancing Language Model Reasoning through Reinforcement Learning and Inference Scaling](https://arxiv.org/abs/2501.11651)
Append: [Graph of Attacks with Pruning: Optimizing Stealthy Jailbreak Prompt Generation for Enhanced LLM Content Moderation](https://arxiv.org/abs/2501.18638)
Append: [Vision-Language Models for Edge Networks: A Comprehensive Survey](https://arxiv.org/abs/2502.07855)
Append: [ColorBench: Can VLMs See and Understand the Colorful World? A Comprehensive Benchmark for Color Perception, Reasoning, and Robustness](https://arxiv.org/abs/2504.10514)
Append: [Enhancing multimodal analogical reasoning with Logic Augmented Generation](https://arxiv.org/abs/2504.11190)
Append: [FreshStack: Building Realistic Benchmarks for Evaluating Retrieval on Technical Documents](https://arxiv.org/abs/2504.13128)
Append: [Attention Retrieves, MLP Memorizes: Disentangling Trainable Components in the Transformer](https://arxiv.org/abs/2506.01115)
Append: [Does Thinking More always Help? Understanding Test-Time Scaling in Reasoning Models](https://arxiv.org/abs/2506.04210)
Append: [Towards Efficient Speech-Text Jointly Decoding within One Speech Language Model](https://arxiv.org/abs/2506.04518)
append_entries: 178
Finish: 2025-06-16 04:34:20.513055
------------------------------------------------------
Started: 2025-06-16 06:27:03.985428
Existing_entries: 1178
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1543
Summarized using GPT-3.5-turbo
Append: [Cartridges: Lightweight and general-purpose long context representations via self-study](https://arxiv.org/abs/2506.06266)
append_entries: 1
Finish: 2025-06-16 06:27:06.303503
------------------------------------------------------
Started: 2025-06-16 08:24:18.714703
Existing_entries: 1001
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-16 08:24:19.131393
------------------------------------------------------
Started: 2025-06-16 10:19:09.325759
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-16 10:19:09.725996
------------------------------------------------------
Started: 2025-06-16 12:35:49.948389
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-16 12:35:50.384859
------------------------------------------------------
Started: 2025-06-16 14:17:27.475282
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-16 14:17:27.907819
------------------------------------------------------
Started: 2025-06-16 16:21:24.333953
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-16 16:21:24.726644
------------------------------------------------------
Started: 2025-06-16 18:24:12.772896
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-16 18:24:13.171804
------------------------------------------------------
Started: 2025-06-16 20:18:39.371803
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-16 20:18:39.817317
------------------------------------------------------
Started: 2025-06-16 22:16:22.466165
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-16 22:16:22.857164
------------------------------------------------------
Started: 2025-06-17 01:21:19.824989
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-17 01:21:20.225266
------------------------------------------------------
Started: 2025-06-17 03:15:42.039790
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-17 03:15:42.447316
------------------------------------------------------
Started: 2025-06-17 04:28:58.198309
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1261
Summarized using GPT-3.5-turbo
Append: [Focusing on Students, not Machines: Grounded Question Generation and Automated Answer Grading](https://arxiv.org/abs/2506.12066)
Token length: 1472
Summarized using GPT-3.5-turbo
Append: [ChatbotManip: A Dataset to Facilitate Evaluation and Oversight of Manipulative Chatbot Behaviour](https://arxiv.org/abs/2506.12090)
Token length: 1181
Summarized using GPT-3.5-turbo
Append: [Continuously Updating Digital Twins using Large Language Models](https://arxiv.org/abs/2506.12091)
Token length: 1368
Summarized using GPT-3.5-turbo
Append: [Enhancing Traffic Accident Classifications: Application of NLP Methods for City Safety](https://arxiv.org/abs/2506.12092)
Token length: 902
Summarized using GPT-3.5-turbo
Append: [UCD: Unlearning in LLMs via Contrastive Decoding](https://arxiv.org/abs/2506.12097)
Token length: 1054
Summarized using GPT-3.5-turbo
Append: [Personalized LLM Decoding via Contrasting Personal Preference](https://arxiv.org/abs/2506.12109)
Token length: 1879
Summarized using GPT-3.5-turbo
Append: [Eliciting Reasoning in Language Models with Cognitive Tools](https://arxiv.org/abs/2506.12115)
Token length: 1259
Summarized using GPT-3.5-turbo
Append: [Unsupervised Document and Template Clustering using Multimodal Embeddings](https://arxiv.org/abs/2506.12116)
Token length: 1361
Summarized using GPT-3.5-turbo
Append: [Can Mixture-of-Experts Surpass Dense LLMs Under Strictly Equal Resources?](https://arxiv.org/abs/2506.12119)
Token length: 793
Summarized using GPT-3.5-turbo
Append: [Hatevolution: What Static Benchmarks Don't Tell Us](https://arxiv.org/abs/2506.12148)
Token length: 1682
Summarized using GPT-3.5-turbo
Append: [Maximally-Informative Retrieval for State Space Model Generation](https://arxiv.org/abs/2506.12149)
Token length: 1258
Summarized using GPT-3.5-turbo
Append: [A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages](https://arxiv.org/abs/2506.12158)
Token length: 1177
Summarized using GPT-3.5-turbo
Append: [Instruction Tuning and CoT Prompting for Contextual Medical QA with LLMs](https://arxiv.org/abs/2506.12182)
Token length: 1418
Summarized using GPT-3.5-turbo
Append: [Supernova Event Dataset: Interpreting Large Language Model's Personality through Critical Event Analysis](https://arxiv.org/abs/2506.12189)
Token length: 1601
Summarized using GPT-3.5-turbo
Append: [Infini-gram mini: Exact n-gram Search at the Internet Scale with FM-Index](https://arxiv.org/abs/2506.12229)
Token length: 1823
Summarized using GPT-3.5-turbo
Append: [Large Language Models for History, Philosophy, and Sociology of Science: Interpretive Uses, Methodological Challenges, and Critical Perspectives](https://arxiv.org/abs/2506.12242)
Token length: 1395
Summarized using GPT-3.5-turbo
Append: [The Behavior Gap: Evaluating Zero-shot LLM Agents in Complex Task-Oriented Dialogs](https://arxiv.org/abs/2506.12266)
Token length: 1406
Summarized using GPT-3.5-turbo
Append: [Med-U1: Incentivizing Unified Medical Reasoning in LLMs via Large-scale Reinforcement Learning](https://arxiv.org/abs/2506.12307)
Token length: 1056
Summarized using GPT-3.5-turbo
Append: [Phonikud: Hebrew Grapheme-to-Phoneme Conversion for Real-Time Text-to-Speech](https://arxiv.org/abs/2506.12311)
Token length: 752
Summarized using GPT-3.5-turbo
Append: [Intersectional Bias in Japanese Large Language Models from a Contextualized Perspective](https://arxiv.org/abs/2506.12327)
Token length: 1072
Summarized using GPT-3.5-turbo
Append: [Investigating the Effects of Cognitive Biases in Prompts on Large Language Model Outputs](https://arxiv.org/abs/2506.12338)
Token length: 1221
Summarized using GPT-3.5-turbo
Append: [Refract ICL: Rethinking Example Selection in the Era of Million-Token Models](https://arxiv.org/abs/2506.12346)
Token length: 1731
Summarized using GPT-3.5-turbo
Append: [Efficient Reasoning Through Suppression of Self-Affirmation Reflections in Large Reasoning Models](https://arxiv.org/abs/2506.12353)
Token length: 1473
Summarized using GPT-3.5-turbo
Append: [Advances in LLMs with Focus on Reasoning, Adaptability, Efficiency and Ethics](https://arxiv.org/abs/2506.12365)
Token length: 1779
Summarized using GPT-3.5-turbo
Append: [Understanding the Effect of Knowledge Graph Extraction Error on Downstream Graph Analyses: A Case Study on Affiliation Graphs](https://arxiv.org/abs/2506.12367)
Token length: 1194
Summarized using GPT-3.5-turbo
Append: [Training-free LLM Merging for Multi-task Learning](https://arxiv.org/abs/2506.12379)
Token length: 1077
Summarized using GPT-3.5-turbo
Append: [Recent Advances and Future Directions in Literature-Based Discovery](https://arxiv.org/abs/2506.12385)
Token length: 1176
Summarized using GPT-3.5-turbo
Append: [Group then Scale: Dynamic Mixture-of-Experts Multilingual Language Model](https://arxiv.org/abs/2506.12388)
Token length: 1376
Summarized using GPT-3.5-turbo
Append: [Exploring Cultural Variations in Moral Judgments with Large Language Models](https://arxiv.org/abs/2506.12433)
Token length: 1315
Summarized using GPT-3.5-turbo
Append: [From Outcomes to Processes: Guiding PRM Learning from ORM for Inference-Time Alignment](https://arxiv.org/abs/2506.12446)
Token length: 1347
Summarized using GPT-3.5-turbo
Append: [Language Surgery in Multilingual Large Language Models](https://arxiv.org/abs/2506.12450)
Token length: 1520
Summarized using GPT-3.5-turbo
Append: [A Pluggable Multi-Task Learning Framework for Sentiment-Aware Financial Relation Extraction](https://arxiv.org/abs/2506.12452)
Token length: 874
Summarized using GPT-3.5-turbo
Append: [TagRouter: Learning Route to LLMs through Tags for Open-Domain Text Generation Tasks](https://arxiv.org/abs/2506.12473)
Token length: 1040
Summarized using GPT-3.5-turbo
Append: [FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented Generation](https://arxiv.org/abs/2506.12494)
Token length: 1253
Summarized using GPT-3.5-turbo
Append: [Improving Factuality for Dialogue Response Generation via Graph-Based Knowledge Augmentation](https://arxiv.org/abs/2506.12496)
Token length: 1505
Summarized using GPT-3.5-turbo
Append: [Towards Fairness Assessment of Dutch Hate Speech Detection](https://arxiv.org/abs/2506.12502)
Token length: 1340
Summarized using GPT-3.5-turbo
Append: [Detection, Classification, and Mitigation of Gender Bias in Large Language Models](https://arxiv.org/abs/2506.12527)
Token length: 1207
Summarized using GPT-3.5-turbo
Append: [Speech-Language Models with Decoupled Tokenizers and Multi-Token Prediction](https://arxiv.org/abs/2506.12537)
Token length: 1223
Summarized using GPT-3.5-turbo
Append: [RealFactBench: A Benchmark for Evaluating Large Language Models in Real-World Fact-Checking](https://arxiv.org/abs/2506.12538)
Token length: 1541
Summarized using GPT-3.5-turbo
Append: [Profiling News Media for Factuality and Bias Using LLMs and the Fact-Checking Methodology of Human Experts](https://arxiv.org/abs/2506.12552)
Token length: 1155
Summarized using GPT-3.5-turbo
Append: [DoTA-RAG: Dynamic of Thought Aggregation RAG](https://arxiv.org/abs/2506.12571)
Token length: 1079
Summarized using GPT-3.5-turbo
Append: [Overview of the NLPCC 2025 Shared Task: Gender Bias Mitigation Challenge](https://arxiv.org/abs/2506.12574)
Token length: 1228
Summarized using GPT-3.5-turbo
Append: [Enabling Precise Topic Alignment in Large Language Models Via Sparse Autoencoders](https://arxiv.org/abs/2506.12576)
Token length: 1879
Summarized using GPT-3.5-turbo
Append: [OneEval: Benchmarking LLM Knowledge-intensive Reasoning over Diverse Knowledge Bases](https://arxiv.org/abs/2506.12577)
Token length: 1015
Summarized using GPT-3.5-turbo
Append: [An Exploration of Mamba for Speech Self-Supervised Models](https://arxiv.org/abs/2506.12606)
Token length: 1818
Summarized using GPT-3.5-turbo
Append: [Towards Building General Purpose Embedding Models for Industry 4.0 Agents](https://arxiv.org/abs/2506.12607)
Token length: 1171
Summarized using GPT-3.5-turbo
Append: [Konooz: Multi-domain Multi-dialect Corpus for Named Entity Recognition](https://arxiv.org/abs/2506.12615)
Token length: 1297
Summarized using GPT-3.5-turbo
Append: [OpenUnlearning: Accelerating LLM Unlearning via Unified Benchmarking of Methods and Metrics](https://arxiv.org/abs/2506.12618)
Token length: 1030
Summarized using GPT-3.5-turbo
Append: [Between Predictability and Randomness: Seeking Artistic Inspiration from AI Generative Models](https://arxiv.org/abs/2506.12634)
Token length: 1049
Summarized using GPT-3.5-turbo
Append: [How Grounded is Wikipedia? A Study on Structured Evidential Support](https://arxiv.org/abs/2506.12637)
Append: [Synthetic Socratic Debates: Examining Persona Effects on Moral Decision and Persuasion Dynamics](https://arxiv.org/abs/2506.12657)
Append: [Enhancing Clinical Models with Pseudo Data for De-identification](https://arxiv.org/abs/2506.12674)
Append: [Flexible Realignment of Language Models](https://arxiv.org/abs/2506.12704)
Append: [Rethinking Hate Speech Detection on Social Media: Can LLMs Replace Traditional Models?](https://arxiv.org/abs/2506.12744)
Append: [Democratic or Authoritarian? Probing a New Dimension of Political Biases in Large Language Models](https://arxiv.org/abs/2506.12758)
Append: [Surprise Calibration for Better In-Context Learning](https://arxiv.org/abs/2506.12796)
Append: [Medical Argument Mining: Exploitation of Scarce Data Using NLI Systems](https://arxiv.org/abs/2506.12823)
Append: [Transforming Chatbot Text: A Sequence-to-Sequence Approach](https://arxiv.org/abs/2506.12843)
Append: [QFFT, Question-Free Fine-Tuning for Adaptive Reasoning](https://arxiv.org/abs/2506.12860)
Append: [ArgHiTZ at ArchEHR-QA 2025: A Two-Step Divide and Conquer Approach to Patient Question Answering for Top Factuality](https://arxiv.org/abs/2506.12886)
Append: [Assessing the Performance Gap Between Lexical and Semantic Models for Information Retrieval With Formulaic Legal Language](https://arxiv.org/abs/2506.12895)
Append: [JEBS: A Fine-grained Biomedical Lexical Simplification Task](https://arxiv.org/abs/2506.12898)
Append: [SciDA: Scientific Dynamic Assessor of LLMs](https://arxiv.org/abs/2506.12909)
Append: [PersonaFeedback: A Large-scale Human-annotated Benchmark For Personalization](https://arxiv.org/abs/2506.12915)
Append: [SoundMind: RL-Incentivized Logic Reasoning for Audio-Language Models](https://arxiv.org/abs/2506.12935)
Append: [CliniDial: A Naturally Occurring Multimodal Dialogue Dataset for Team Reflection in Action During Clinical Operation](https://arxiv.org/abs/2506.12936)
Append: [Assessing the Role of Data Quality in Training Bilingual Language Models](https://arxiv.org/abs/2506.12966)
Append: [Multi-document Summarization through Multi-document Event Relation Graph Reasoning in LLMs: a case study in Framing Bias Mitigation](https://arxiv.org/abs/2506.12978)
Append: [Large Language Models Enhanced by Plug and Play Syntactic Knowledge for Aspect-based Sentiment Analysis](https://arxiv.org/abs/2506.12991)
Append: [Missing the human touch? A computational stylometry analysis of GPT-4 translations of online Chinese literature](https://arxiv.org/abs/2506.13013)
Append: [Edeflip: Supervised Word Translation between English and Yoruba](https://arxiv.org/abs/2506.13020)
Append: [Just Go Parallel: Improving the Multilingual Capabilities of Large Language Models](https://arxiv.org/abs/2506.13044)
Append: [CFBenchmark-MM: Chinese Financial Assistant Benchmark for Multimodal Large Language Model](https://arxiv.org/abs/2506.13055)
Append: [Multipole Attention for Efficient Long Context Reasoning](https://arxiv.org/abs/2506.13059)
Append: [MotiveBench: How Far Are We From Human-Like Motivational Reasoning in Large Language Models?](https://arxiv.org/abs/2506.13065)
Append: [FinLMM-R1: Enhancing Financial Reasoning in LMM through Scalable Data and Reward Design](https://arxiv.org/abs/2506.13066)
Append: [CHILL at SemEval-2025 Task 2: You Can't Just Throw Entities and Hope -- Make Your LLM to Get Them Right](https://arxiv.org/abs/2506.13070)
Append: [Rethinking Test-Time Scaling for Medical AI: Model and Task-Aware Strategies for LLMs and VLMs](https://arxiv.org/abs/2506.13102)
Append: [Leveraging In-Context Learning for Language Model Agents](https://arxiv.org/abs/2506.13109)
Append: [CMU's IWSLT 2025 Simultaneous Speech Translation System](https://arxiv.org/abs/2506.13143)
Append: [Adapting LLMs for Minimal-edit Grammatical Error Correction](https://arxiv.org/abs/2506.13148)
Append: [Ai-Facilitated Analysis of Abstracts and Conclusions: Flagging Unsubstantiated Claims and Ambiguous Pronouns](https://arxiv.org/abs/2506.13172)
Append: [Development of the user-friendly decision aid Rule-based Evaluation and Support Tool (REST) for optimizing the resources of an information extraction task](https://arxiv.org/abs/2506.13177)
Append: [Enhancing Large Language Models with Reliable Knowledge Graphs](https://arxiv.org/abs/2506.13178)
Append: [Dynamic Acoustic Model Architecture Optimization in Training for ASR](https://arxiv.org/abs/2506.13180)
Append: [Align-then-Unlearn: Embedding Alignment for LLM Unlearning](https://arxiv.org/abs/2506.13181)
Append: [Breaking Thought Patterns: A Multi-Dimensional Reasoning Framework for LLMs](https://arxiv.org/abs/2506.13192)
Append: [Do Music Preferences Reflect Cultural Values? A Cross-National Analysis Using Music Embedding and World Values Survey](https://arxiv.org/abs/2506.13199)
Append: [Capability Salience Vector: Fine-grained Alignment of Loss and Capabilities for Downstream Task Scaling Law](https://arxiv.org/abs/2506.13216)
Append: [IGD: Token Decisiveness Modeling via Information Gain in LLMs for Personalized Recommendation](https://arxiv.org/abs/2506.13229)
Append: [AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy](https://arxiv.org/abs/2506.13284)
Append: [Mitigating Safety Fallback in Editing-based Backdoor Injection on LLMs](https://arxiv.org/abs/2506.13285)
Append: [Seewo's Submission to MLC-SLM: Lessons learned from Speech Reasoning Language Models](https://arxiv.org/abs/2506.13300)
Append: [Large Language Models as 'Hidden Persuaders': Fake Product Reviews are Indistinguishable to Humans and Machines](https://arxiv.org/abs/2506.13313)
Append: [Document-Level Tabular Numerical Cross-Checking: A Coarse-to-Fine Approach](https://arxiv.org/abs/2506.13328)
Append: [EAQuant: Enhancing Post-Training Quantization for MoE Models via Expert-Aware Optimization](https://arxiv.org/abs/2506.13329)
Append: [NTU Speechlab LLM-Based Multilingual ASR System for Interspeech MLC-SLM Challenge 2025](https://arxiv.org/abs/2506.13339)
Append: [Direct Reasoning Optimization: LLMs Can Reward And Refine Their Own Reasoning for Open-Ended Tasks](https://arxiv.org/abs/2506.13351)
Append: [StoryBench: A Dynamic Benchmark for Evaluating Long-Term Memory with Multi Turns](https://arxiv.org/abs/2506.13356)
Append: [Efficient Medical VIE via Reinforcement Learning](https://arxiv.org/abs/2506.13363)
Append: [Enhancing Goal-oriented Proactive Dialogue Systems via Consistency Reflection and Correction](https://arxiv.org/abs/2506.13366)
Append: [Decompositional Reasoning for Graph Retrieval with Large Language Models](https://arxiv.org/abs/2506.13380)
Append: [Bi-directional Context-Enhanced Speech Large Language Models for Multilingual Conversational ASR](https://arxiv.org/abs/2506.13396)
Append: [RealHiTBench: A Comprehensive Realistic Hierarchical Table Benchmark for Evaluating LLM-Based Table Analysis](https://arxiv.org/abs/2506.13405)
Append: [A Neural Model for Word Repetition](https://arxiv.org/abs/2506.13450)
Append: [Unveiling the Learning Mind of Language Models: A Cognitive Framework and Empirical Study](https://arxiv.org/abs/2506.13464)
Append: [Enhancing Omics Cohort Discovery for Research on Neurodegeneration through Ontology-Augmented Embedding Models](https://arxiv.org/abs/2506.13467)
Append: [An Interdisciplinary Approach to Human-Centered Machine Translation](https://arxiv.org/abs/2506.13468)
Append: [Abstract, Align, Predict: Zero-Shot Stance Detection via Cognitive Inductive Reasoning](https://arxiv.org/abs/2506.13470)
Append: [ROSAQ: Rotation-based Saliency-Aware Weight Quantization for Efficiently Compressing Large Language Models](https://arxiv.org/abs/2506.13472)
Append: [Language Agents for Hypothesis-driven Clinical Decision Making with Reinforcement Learning](https://arxiv.org/abs/2506.13474)
Append: [Position: Pause Recycling LoRAs and Prioritize Mechanisms to Uncover Limits and Effectiveness](https://arxiv.org/abs/2506.13479)
Append: [TurBLiMP: A Turkish Benchmark of Linguistic Minimal Pairs](https://arxiv.org/abs/2506.13487)
Append: [BOW: Bottlenecked Next Word Exploration](https://arxiv.org/abs/2506.13502)
Append: [K/DA: Automated Data Generation Pipeline for Detoxifying Implicitly Offensive Language in Korean](https://arxiv.org/abs/2506.13513)
Append: [TensorSLM: Energy-efficient Embedding Compression of Sub-billion Parameter Language Models on Low-end Devices](https://arxiv.org/abs/2506.13514)
Append: [Mixture of Weight-shared Heterogeneous Group Attention Experts for Dynamic Token-wise KV Optimization](https://arxiv.org/abs/2506.13541)
Append: [Understand the Implication: Learning to Think for Pragmatic Understanding](https://arxiv.org/abs/2506.13559)
Append: [Characterizing Linguistic Shifts in Croatian News via Diachronic Word Embeddings](https://arxiv.org/abs/2506.13569)
Append: [MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention](https://arxiv.org/abs/2506.13585)
Append: [Qwen vs. Gemma Integration with Whisper: A Comparative Study in Multilingual SpeechLLM Systems](https://arxiv.org/abs/2506.13596)
Append: [CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility Simulation](https://arxiv.org/abs/2506.13599)
Append: [A Structured Bangla Dataset of Disease-Symptom Associations to Improve Diagnostic Accuracy](https://arxiv.org/abs/2506.13610)
Append: [An Empirical Study of LLM-as-a-Judge: How Design Choices Impact Evaluation Reliability](https://arxiv.org/abs/2506.13639)
Append: [EvolvTrip: Enhancing Literary Character Understanding with Temporal Theory-of-Mind Graphs](https://arxiv.org/abs/2506.13641)
Append: [Prefix-Tuning+: Modernizing Prefix-Tuning through Attention Independent Prefix Data](https://arxiv.org/abs/2506.13674)
Append: [Turning Down the Heat: A Critical Analysis of Min-p Sampling in Language Models](https://arxiv.org/abs/2506.13681)
Append: [Balancing Knowledge Delivery and Emotional Comfort in Healthcare Conversational Systems](https://arxiv.org/abs/2506.13692)
Append: [Instruction Following by Boosting Attention of Large Language Models](https://arxiv.org/abs/2506.13734)
Append: [LTRR: Learning To Rank Retrievers for LLMs](https://arxiv.org/abs/2506.13743)
Append: [Steering LLM Thinking with Budget Guidance](https://arxiv.org/abs/2506.13752)
Append: [CMT-LLM: Contextual Multi-Talker ASR Utilizing Large Language Models](https://arxiv.org/abs/2506.12059)
Append: [Seamless Dysfluent Speech Text Alignment for Disordered Speech Analysis](https://arxiv.org/abs/2506.12073)
Append: [Artificial Intelligence and Civil Discourse: How LLMs Moderate Climate Change Conversations](https://arxiv.org/abs/2506.12077)
Append: [Modeling Earth-Scale Human-Like Societies with One Billion Agents](https://arxiv.org/abs/2506.12078)
Append: [The CAISAR Platform: Extending the Reach of Machine Learning Specification and Verification](https://arxiv.org/abs/2506.12084)
Append: [Quantum-Inspired Differentiable Integral Neural Networks (QIDINNs): A Feynman-Based Architecture for Continuous Learning Over Streaming Data](https://arxiv.org/abs/2506.12111)
Append: [Generative or Discriminative? Revisiting Text Classification in the Era of Transformers](https://arxiv.org/abs/2506.12181)
Append: [From Emergence to Control: Probing and Modulating Self-Reflection in Language Models](https://arxiv.org/abs/2506.12217)
Append: [Zero-Shot Scene Understanding with Multimodal Large Language Models for Automated Vehicles](https://arxiv.org/abs/2506.12232)
Append: [Datrics Text2SQL: A Framework for Natural Language to SQL Query Generation](https://arxiv.org/abs/2506.12234)
Append: [ProVox: Personalization and Proactive Planning for Situated Human-Robot Collaboration](https://arxiv.org/abs/2506.12248)
Append: [InfoFlood: Jailbreaking Large Language Models with Information Overload](https://arxiv.org/abs/2506.12274)
Append: [Can LLMs Generate High-Quality Test Cases for Algorithm Problems? TestCase-Eval: A Systematic Evaluation of Fault Coverage and Exposure](https://arxiv.org/abs/2506.12278)
Append: [Perspective on Utilizing Foundation Models for Laboratory Automation in Materials Research](https://arxiv.org/abs/2506.12312)
Append: [GSDNet: Revisiting Incomplete Multimodal-Diffusion from Graph Spectrum Perspective for Conversation Emotion Recognition](https://arxiv.org/abs/2506.12325)
Append: [Information Suppression in Large Language Models: Auditing, Quantifying, and Characterizing Censorship in DeepSeek](https://arxiv.org/abs/2506.12349)
Append: [QiMeng-Attention: SOTA Attention Operator is generated by SOTA Attention Algorithm](https://arxiv.org/abs/2506.12355)
Append: [MM-R5: MultiModal Reasoning-Enhanced ReRanker via Reinforcement Learning for Document Retrieval](https://arxiv.org/abs/2506.12364)
Append: [ConsistencyChecker: Tree-based Evaluation of LLM Generalization Capabilities](https://arxiv.org/abs/2506.12376)
Append: [Model Merging for Knowledge Editing](https://arxiv.org/abs/2506.12384)
Append: [Plan Your Travel and Travel with Your Plan: Wide-Horizon Planning and Evaluation via LLM](https://arxiv.org/abs/2506.12421)
Append: [AI Flow: Perspectives, Scenarios, and Approaches](https://arxiv.org/abs/2506.12479)
Append: [MALM: A Multi-Information Adapter for Large Language Models to Mitigate Hallucination](https://arxiv.org/abs/2506.12483)
Append: [Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption Masking And Normalization](https://arxiv.org/abs/2506.12484)
Append: [StreamMel: Real-Time Zero-shot Text-to-Speech via Interleaved Continuous Autoregressive Modeling](https://arxiv.org/abs/2506.12570)
Append: [MS4UI: A Dataset for Multi-modal Summarization of User Interface Instructional Videos](https://arxiv.org/abs/2506.12623)
Append: [SC-SOT: Conditioning the Decoder on Diarized Speaker Information for End-to-End Overlapped Speech Recognition](https://arxiv.org/abs/2506.12672)
Append: [SecurityLingua: Efficient Defense of LLM Jailbreak Attacks via Security-Aware Prompt Compression](https://arxiv.org/abs/2506.12707)
Append: [Humanity's Last Code Exam: Can Advanced LLMs Conquer Human's Hardest Code Competition?](https://arxiv.org/abs/2506.12713)
Append: [Strategic Scaling of Test-Time Compute: A Bandit Learning Approach](https://arxiv.org/abs/2506.12721)
Append: [Rethinking DPO: The Role of Rejected Responses in Preference Misalignment](https://arxiv.org/abs/2506.12725)
Append: [WereWolf-Plus: An Update of Werewolf Game setting Based on DSGBench](https://arxiv.org/abs/2506.12841)
Append: [Identifying and Investigating Global News Coverage of Critical Events Such as Disasters and Terrorist Attacks](https://arxiv.org/abs/2506.12925)
Append: [Sectoral Coupling in Linguistic State Space](https://arxiv.org/abs/2506.12927)
Append: [HypER: Literature-grounded Hypothesis Generation and Distillation with Provenance](https://arxiv.org/abs/2506.12937)
Append: [Forecasting Time Series with LLMs via Patch-Based Prompting and Decomposition](https://arxiv.org/abs/2506.12953)
Append: [Efficient Neuro-Symbolic Retrieval-Augmented Generation through Adaptive Query Routing](https://arxiv.org/abs/2506.12981)
Append: [Knowledge Graph Fusion with Large Language Models for Accurate, Explainable Manufacturing Process Planning](https://arxiv.org/abs/2506.13026)
Append: [Stress-Testing Multimodal Foundation Models for Crystallographic Reasoning](https://arxiv.org/abs/2506.13051)
Append: [PRISM2: Unlocking Multi-Modal General Pathology AI with Clinical Dialogue](https://arxiv.org/abs/2506.13063)
Append: [Equitable Electronic Health Record Prediction with FAME: Fairness-Aware Multimodal Embedding](https://arxiv.org/abs/2506.13104)
Append: [Crime Hotspot Prediction Using Deep Graph Convolutional Networks](https://arxiv.org/abs/2506.13116)
Append: [ZINA: Multimodal Fine-grained Hallucination Detection and Editing](https://arxiv.org/abs/2506.13130)
Append: [Dynamic Context-oriented Decomposition for Task-aware Low-rank Adaptation with Less Forgetting and Faster Convergence](https://arxiv.org/abs/2506.13187)
Append: [SPOT: Bridging Natural Language and Geospatial Search for Investigative Journalists](https://arxiv.org/abs/2506.13188)
Append: [Thought Crime: Backdoors and Emergent Misalignment in Reasoning Models](https://arxiv.org/abs/2506.13206)
Append: [Distinct Computations Emerge From Compositional Curricula in In-Context Learning](https://arxiv.org/abs/2506.13253)
Append: [AdaLRS: Loss-Guided Adaptive Learning Rate Search for Efficient Foundation Model Pretraining](https://arxiv.org/abs/2506.13274)
Append: [SeqPE: Transformer with Sequential Position Encoding](https://arxiv.org/abs/2506.13277)
Append: [Verifying the Verifiers: Unveiling Pitfalls and Potentials in Fact Verifiers](https://arxiv.org/abs/2506.13342)
Append: [Leveraging Vision-Language Pre-training for Human Activity Recognition in Still Images](https://arxiv.org/abs/2506.13458)
Append: [Flexible-length Text Infilling for Discrete Diffusion Models](https://arxiv.org/abs/2506.13579)
Append: [Stream-Omni: Simultaneous Multimodal Interactions with Large Language-Vision-Speech Model](https://arxiv.org/abs/2506.13642)
Append: [Attribution-guided Pruning for Compression, Circuit Discovery, and Targeted Correction in LLMs](https://arxiv.org/abs/2506.13727)
Append: [Is Smaller Always Faster? Tradeoffs in Compressing Self-Supervised Speech Transformers](https://arxiv.org/abs/2211.09949)
Append: [Smurfs: Multi-Agent System using Context-Efficient DFSDT for Tool Planning](https://arxiv.org/abs/2405.05955)
Append: [OR-Bench: An Over-Refusal Benchmark for Large Language Models](https://arxiv.org/abs/2405.20947)
Append: [WorldAPIs: The World Is Worth How Many APIs? A Thought Experiment](https://arxiv.org/abs/2407.07778)
Append: [Fast-and-Frugal Text-Graph Transformers are Effective Link Predictors](https://arxiv.org/abs/2408.06778)
Append: [AgentCourt: Simulating Court with Adversarial Evolvable Lawyer Agents](https://arxiv.org/abs/2408.08089)
Append: [EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling MiXed Emotions and Discourse Dynamics](https://arxiv.org/abs/2408.08782)
Append: [Improving Clinical Note Generation from Complex Doctor-Patient Conversation](https://arxiv.org/abs/2408.14568)
Append: [Co-occurrence is not Factual Association in Language Models](https://arxiv.org/abs/2409.14057)
Append: [Making LLMs Better Many-to-Many Speech-to-Text Translators with Curriculum Learning](https://arxiv.org/abs/2409.19510)
Append: [Scaling Laws For Mixed Qquantization](https://arxiv.org/abs/2410.06722)
Append: [Upcycling Large Language Models into Mixture of Experts](https://arxiv.org/abs/2410.07524)
Append: [FlatQuant: Flatness Matters for LLM Quantization](https://arxiv.org/abs/2410.09426)
Append: [EffiCoder: Enhancing Code Generation in Large Language Models through Efficiency-Aware Fine-tuning](https://arxiv.org/abs/2410.10209)
Append: [Accurate and Regret-aware Numerical Problem Solver for Tabular Question Answering](https://arxiv.org/abs/2410.12846)
Append: [POROver: Improving Safety and Reducing Overrefusal in Large Language Models with Overgeneration and Preference Optimization](https://arxiv.org/abs/2410.12999)
Append: [ReflecTool: Towards Reflection-Aware Tool-Augmented Clinical Agents](https://arxiv.org/abs/2410.17657)
Append: [Knowledge-Augmented Multimodal Clinical Rationale Generation for Disease Diagnosis with Small Language Models](https://arxiv.org/abs/2411.07611)
Append: [A dataset of questions on decision-theoretic reasoning in Newcomb-like problems](https://arxiv.org/abs/2411.10588)
Append: [Bridging Relevance and Reasoning: Rationale Distillation in Retrieval-Augmented Generation](https://arxiv.org/abs/2412.08519)
Append: [CoT-based Synthesizer: Enhancing LLM Performance through Answer Synthesis](https://arxiv.org/abs/2501.01668)
Append: [An Investigation into Value Misalignment in LLM-Generated Texts for Cultural Heritage](https://arxiv.org/abs/2501.02039)
Append: [Benchmarking Rotary Position Embeddings for Automatic Speech Recognition](https://arxiv.org/abs/2501.06051)
Append: [Foundations of Large Language Models](https://arxiv.org/abs/2501.09223)
Append: [Rethinking Table Instruction Tuning](https://arxiv.org/abs/2501.14693)
Append: [Activation-Informed Merging of Large Language Models](https://arxiv.org/abs/2502.02421)
Append: [Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search](https://arxiv.org/abs/2502.02508)
Append: [BOUQuET: dataset, Benchmark and Open initiative for Universal Quality Evaluation in Translation](https://arxiv.org/abs/2502.04314)
Append: [Fino1: On the Transferability of Reasoning-Enhanced LLMs and Reinforcement Learning to Finance](https://arxiv.org/abs/2502.08127)
Append: [Truth Knows No Language: Evaluating Truthfulness Beyond English](https://arxiv.org/abs/2502.09387)
Append: [SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models](https://arxiv.org/abs/2502.09604)
Append: [MTLM: Incorporating Bidirectional Text Information to Enhance Language Model Training in Speech Recognition Systems](https://arxiv.org/abs/2502.10058)
Append: [CMCTS: A Constrained Monte Carlo Tree Search Framework for Mathematical Reasoning in Large Language Model](https://arxiv.org/abs/2502.11169)
Append: [Idiosyncrasies in Large Language Models](https://arxiv.org/abs/2502.12150)
Append: [NaturalReasoning: Reasoning in the Wild with 2.8M Challenging Questions](https://arxiv.org/abs/2502.13124)
Append: [ProMedTS: A Self-Supervised, Prompt-Guided Multimodal Approach for Integrating Medical Text and Time Series](https://arxiv.org/abs/2502.13509)
Append: [A Large and Balanced Corpus for Fine-grained Arabic Readability Assessment](https://arxiv.org/abs/2502.13520)
Append: [Self-Regularization with Sparse Autoencoders for Controllable LLM-based Classification](https://arxiv.org/abs/2502.14133)
Append: [Entity Framing and Role Portrayal in the News](https://arxiv.org/abs/2502.14718)
Append: [A Training-free LLM-based Approach to General Chinese Character Error Correction](https://arxiv.org/abs/2502.15266)
Append: [InfiniSST: Simultaneous Translation of Unbounded Speech with Large Language Model](https://arxiv.org/abs/2503.02969)
Append: [Efficient Safety Alignment of Large Language Models via Preference Re-ranking and Representation-based Reward Modeling](https://arxiv.org/abs/2503.10093)
Append: [REPA: Russian Error Types Annotation for Evaluating Text Generation and Judgment Capabilities](https://arxiv.org/abs/2503.13102)
Append: [MathFusion: Enhancing Mathematical Problem-solving of LLM through Instruction Fusion](https://arxiv.org/abs/2503.16212)
Append: [ShED-HD: A Shannon Entropy Distribution Framework for Lightweight Hallucination Detection on Edge Devices](https://arxiv.org/abs/2503.18242)
Append: [LinkAlign: Scalable Schema Linking for Real-World Large-Scale Multi-Database Text-to-SQL](https://arxiv.org/abs/2503.18596)
Append: [Efficient Inference for Large Reasoning Models: A Survey](https://arxiv.org/abs/2503.23077)
Append: [Evaluating how LLM annotations represent diverse views on contentious topics](https://arxiv.org/abs/2503.23243)
Append: [Experiential Semantic Information and Brain Alignment: Are Multimodal Models Better than Language Models?](https://arxiv.org/abs/2504.00942)
Append: [NeedleInATable: Exploring Long-Context Capability of Large Language Models towards Long-Structured Tables](https://arxiv.org/abs/2504.06560)
Append: [Scholar Inbox: Personalized Paper Recommendations for Scientists](https://arxiv.org/abs/2504.08385)
Append: [Unsupervised Classification of English Words Based on Phonological Information: Discovery of Germanic and Latinate Clusters](https://arxiv.org/abs/2504.11770)
Append: [Nested Named-Entity Recognition on Vietnamese COVID-19: Dataset and Experiments](https://arxiv.org/abs/2504.21016)
Append: [ViQA-COVID: COVID-19 Machine Reading Comprehension Dataset for Vietnamese](https://arxiv.org/abs/2504.21017)
Append: [Toward Reasonable Parrots: Why Large Language Models Should Argue with Us by Design](https://arxiv.org/abs/2505.05298)
Append: [Visual Abstract Thinking Empowers Multimodal Reasoning](https://arxiv.org/abs/2505.20164)
Append: [Leveraging LLM and Self-Supervised Training Models for Speech Recognition in Chinese Dialects: A Comparative Analysis](https://arxiv.org/abs/2505.21138)
Append: [R-KV: Redundancy-aware KV Cache Compression for Reasoning Models](https://arxiv.org/abs/2505.24133)
Append: [Disentangling Codemixing in Chats: The NUS ABC Codemixed Corpus](https://arxiv.org/abs/2506.00332)
Append: [From Argumentative Text to Argument Knowledge Graph: A New Framework for Structured Argumentation](https://arxiv.org/abs/2506.00713)
Append: [Unifying Uniform and Binary-coding Quantization for Accurate Compression of Large Language Models](https://arxiv.org/abs/2506.03781)
Append: [MELABenchv1: Benchmarking Large Language Models against Smaller Fine-Tuned Models for Low-Resource Maltese NLP](https://arxiv.org/abs/2506.04385)
Append: [OPeRA: A Dataset of Observation, Persona, Rationale, and Action for Evaluating LLMs on Human Online Shopping Behavior Simulation](https://arxiv.org/abs/2506.05606)
Append: [Video Understanding with Large Language Models: A Survey](https://arxiv.org/abs/2312.17432)
Append: [Ask Optimal Questions: Aligning Large Language Models with Retriever's Preference in Conversation](https://arxiv.org/abs/2402.11827)
Append: [Personalized Wireless Federated Learning for Large Language Models](https://arxiv.org/abs/2404.13238)
Append: [Efficient Sequential Decision Making with Large Language Models](https://arxiv.org/abs/2406.12125)
Append: [PKU-SafeRLHF: Towards Multi-Level Safety Alignment for LLMs with Human Preference](https://arxiv.org/abs/2406.15513)
Append: [Navigating LLM Ethics: Advancements, Challenges, and Future Directions](https://arxiv.org/abs/2406.18841)
Append: [SMILE: Speech Meta In-Context Learning for Low-Resource Language Automatic Speech Recognition](https://arxiv.org/abs/2409.10429)
Append: [RATIONALYST: Mining Implicit Rationales for Process Supervision of Reasoning](https://arxiv.org/abs/2410.01044)
Append: [How Much Can We Forget about Data Contamination?](https://arxiv.org/abs/2410.03249)
Append: [Building, Reusing, and Generalizing Abstract Representations from Concrete Sequences](https://arxiv.org/abs/2410.21332)
Append: [Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on Tasks where Thinking Makes Humans Worse](https://arxiv.org/abs/2410.21333)
Append: [Regular-pattern-sensitive CRFs for Distant Label Interactions](https://arxiv.org/abs/2411.12484)
Append: [MORTAR: Multi-turn Metamorphic Testing for LLM-based Dialogue Systems](https://arxiv.org/abs/2412.15557)
Append: [Unifying Specialized Visual Encoders for Video Language Models](https://arxiv.org/abs/2501.01426)
Append: [Layer by Layer: Uncovering Hidden Representations in Language Models](https://arxiv.org/abs/2502.02013)
Append: [Scaling Laws for Upcycling Mixture-of-Experts Language Models](https://arxiv.org/abs/2502.03009)
Append: [Adapt-Pruner: Adaptive Structural Pruning for Efficient Small Language Model Training](https://arxiv.org/abs/2502.03460)
Append: [Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions](https://arxiv.org/abs/2502.04322)
Append: [Optimizing Temperature for Language Models with Multi-Sample Inference](https://arxiv.org/abs/2502.05234)
Append: [Life-Code: Central Dogma Modeling with Multi-Omics Sequence Unification](https://arxiv.org/abs/2502.07299)
Append: [HARBOR: Exploring Persona Dynamics in Multi-Agent Competition](https://arxiv.org/abs/2502.12149)
Append: [Less is More: Improving LLM Alignment via Preference Data Selection](https://arxiv.org/abs/2502.14560)
Append: [From Euler to AI: Unifying Formulas for Mathematical Constants](https://arxiv.org/abs/2502.17533)
Append: [Watch Out Your Album! On the Inadvertent Privacy Memorization in Multi-Modal Large Language Models](https://arxiv.org/abs/2503.01208)
Append: [What do Large Language Models Say About Animals? Investigating Risks of Animal Harm in Generated Text](https://arxiv.org/abs/2503.04804)
Append: [Compute Optimal Scaling of Skills: Knowledge vs Reasoning](https://arxiv.org/abs/2503.10061)
Append: [Transformers without Normalization](https://arxiv.org/abs/2503.10622)
Append: [QualiSpeech: A Speech Quality Assessment Dataset with Natural Language Reasoning and Descriptions](https://arxiv.org/abs/2503.20290)
Append: [Affordable AI Assistants with Knowledge Graph of Thoughts](https://arxiv.org/abs/2504.02670)
Append: [On Synthesizing Data for Context Attribution in Question Answering](https://arxiv.org/abs/2504.05317)
Append: [JEPA4Rec: Learning Effective Language Representations for Sequential Recommendation via Joint Embedding Predictive Architecture](https://arxiv.org/abs/2504.10512)
Append: [Learning to Rank Chain-of-Thought: An Energy-Based Approach with Outcome Supervision](https://arxiv.org/abs/2505.14999)
Append: [Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal Transformer Distillation](https://arxiv.org/abs/2505.21549)
append_entries: 281
Finish: 2025-06-17 04:30:29.357021
------------------------------------------------------
Started: 2025-06-17 06:25:56.863881
Existing_entries: 1281
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1798
Summarized using GPT-3.5-turbo
Append: [Reasoning with RAGged events: RAG-Enhanced Event Knowledge Base Construction and reasoning with proof-assistants](https://arxiv.org/abs/2506.07042)
Token length: 962
Summarized using GPT-3.5-turbo
Append: [A Hybrid GA LLM Framework for Structured Task Optimization](https://arxiv.org/abs/2506.07483)
Token length: 1719
Summarized using GPT-3.5-turbo
Append: [G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems](https://arxiv.org/abs/2506.07398)
Token length: 912
Summarized using GPT-3.5-turbo
Append: [Reparameterized LLM Training via Orthogonal Equivalence Transformation](https://arxiv.org/abs/2506.08001)
append_entries: 4
Finish: 2025-06-17 06:26:04.493447
------------------------------------------------------
Started: 2025-06-17 08:23:34.090262
Existing_entries: 1004
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-17 08:23:34.739431
------------------------------------------------------
Started: 2025-06-17 10:18:36.555311
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-17 10:18:37.158883
------------------------------------------------------
Started: 2025-06-17 12:35:52.739674
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-17 12:35:53.383146
------------------------------------------------------
Started: 2025-06-17 14:17:20.585328
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-17 14:17:21.204818
------------------------------------------------------
Started: 2025-06-17 16:21:45.352590
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-17 16:21:46.008851
------------------------------------------------------
Started: 2025-06-17 18:23:57.017037
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-17 18:23:57.615192
------------------------------------------------------
Started: 2025-06-17 20:18:36.665723
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-17 20:18:37.344744
------------------------------------------------------
Started: 2025-06-17 22:15:55.366501
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-17 22:15:56.058754
------------------------------------------------------
Started: 2025-06-18 01:20:37.824605
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-18 01:20:38.456571
------------------------------------------------------
Started: 2025-06-18 03:14:31.543791
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-18 03:14:32.140730
------------------------------------------------------
Started: 2025-06-18 04:29:13.017823
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1779
Summarized using GPT-3.5-turbo
Append: [ClimateChat: Designing Data and Methods for Instruction Tuning LLMs to Answer Climate Change Queries](https://arxiv.org/abs/2506.13796)
Token length: 1225
Summarized using GPT-3.5-turbo
Append: [Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles](https://arxiv.org/abs/2506.13886)
Token length: 1187
Summarized using GPT-3.5-turbo
Append: [VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and Iterative Training](https://arxiv.org/abs/2506.13888)
Token length: 1205
Summarized using GPT-3.5-turbo
Append: [EmoNews: A Spoken Dialogue System for Expressive News Conversations](https://arxiv.org/abs/2506.13894)
Token length: 1631
Summarized using GPT-3.5-turbo
Append: [Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise Pooled Representations](https://arxiv.org/abs/2506.13901)
Token length: 1278
Summarized using GPT-3.5-turbo
Append: [ASMR: Augmenting Life Scenario using Large Generative Models for Robotic Action Reflection](https://arxiv.org/abs/2506.13956)
Token length: 1250
Summarized using GPT-3.5-turbo
Append: [Are manual annotations necessary for statutory interpretations retrieval?](https://arxiv.org/abs/2506.13965)
Token length: 1619
Summarized using GPT-3.5-turbo
Append: [AI shares emotion with humans across languages and cultures](https://arxiv.org/abs/2506.13978)
Token length: 1054
Summarized using GPT-3.5-turbo
Append: [Lost in the Mix: Evaluating LLM Understanding of Code-Switched Text](https://arxiv.org/abs/2506.14012)
Token length: 1517
Summarized using GPT-3.5-turbo
Append: [MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark for Financial LLM Evaluation](https://arxiv.org/abs/2506.14028)
Token length: 747
Summarized using GPT-3.5-turbo
Append: [An Interdisciplinary Review of Commonsense Reasoning and Intent Detection](https://arxiv.org/abs/2506.14040)
Token length: 698
Summarized using GPT-3.5-turbo
Append: [Ace-CEFR -- A Dataset for Automated Evaluation of the Linguistic Difficulty of Conversational Texts for LLM Applications](https://arxiv.org/abs/2506.14046)
Token length: 978
Summarized using GPT-3.5-turbo
Append: [Automatic Extraction of Clausal Embedding Based on Large-Scale English Text Data](https://arxiv.org/abs/2506.14064)
Token length: 937
Summarized using GPT-3.5-turbo
Append: [Abstract Meaning Representation for Hospital Discharge Summarization](https://arxiv.org/abs/2506.14101)
Token length: 870
Summarized using GPT-3.5-turbo
Append: [Essential-Web v1.0: 24T tokens of organized web data](https://arxiv.org/abs/2506.14111)
Token length: 1457
Summarized using GPT-3.5-turbo
Append: [Sampling from Your Language Model One Byte at a Time](https://arxiv.org/abs/2506.14123)
Token length: 1239
Summarized using GPT-3.5-turbo
Append: [DCRM: A Heuristic to Measure Response Pair Quality in Preference Optimization](https://arxiv.org/abs/2506.14157)
Token length: 1190
Summarized using GPT-3.5-turbo
Append: [S$^4$C: Speculative Sampling with Syntactic and Semantic Coherence for Efficient Inference of Large Language Models](https://arxiv.org/abs/2506.14158)
Token length: 1230
Summarized using GPT-3.5-turbo
Append: [MIST: Towards Multi-dimensional Implicit Bias and Stereotype Evaluation of LLMs via Theory of Mind](https://arxiv.org/abs/2506.14161)
Token length: 1225
Summarized using GPT-3.5-turbo
Append: [GRAM: A Generative Foundation Reward Model for Reward Generalization](https://arxiv.org/abs/2506.14175)
Token length: 1039
Summarized using GPT-3.5-turbo
Append: [Can we train ASR systems on Code-switch without real code-switch data? Case study for Singapore's languages](https://arxiv.org/abs/2506.14177)
Token length: 981
Summarized using GPT-3.5-turbo
Append: [AsyncSwitch: Asynchronous Text-Speech Adaptation for Code-Switched ASR](https://arxiv.org/abs/2506.14190)
Token length: 950
Summarized using GPT-3.5-turbo
Append: [MAS-LitEval : Multi-Agent System for Literary Translation Quality Assessment](https://arxiv.org/abs/2506.14199)
Token length: 1455
Summarized using GPT-3.5-turbo
Append: [ELI-Why: Evaluating the Pedagogical Utility of Language Model Explanations](https://arxiv.org/abs/2506.14200)
Token length: 1212
Summarized using GPT-3.5-turbo
Append: [Intended Target Identification for Anomia Patients with Gradient-based Selective Augmentation](https://arxiv.org/abs/2506.14203)
Token length: 1332
Summarized using GPT-3.5-turbo
Append: [AgentSynth: Scalable Task Generation for Generalist Computer-Use Agents](https://arxiv.org/abs/2506.14205)
Token length: 1432
Summarized using GPT-3.5-turbo
Append: [CausalDiffTab: Mixed-Type Causal-Aware Diffusion for Tabular Data Generation](https://arxiv.org/abs/2506.14206)
Token length: 1357
Summarized using GPT-3.5-turbo
Append: [Explainable Detection of Implicit Influential Patterns in Conversations via Data Augmentation](https://arxiv.org/abs/2506.14211)
Token length: 1433
Summarized using GPT-3.5-turbo
Append: [Chaining Event Spans for Temporal Relation Grounding](https://arxiv.org/abs/2506.14213)
Token length: 1910
Summarized using GPT-3.5-turbo
Append: [Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just Like an Olympiad Team](https://arxiv.org/abs/2506.14234)
Token length: 877
Summarized using GPT-3.5-turbo
Append: [A Multi-Expert Structural-Semantic Hybrid Framework for Unveiling Historical Patterns in Temporal Knowledge Graphs](https://arxiv.org/abs/2506.14235)
Token length: 1518
Summarized using GPT-3.5-turbo
Append: [Re-Initialization Token Learning for Tool-Augmented Large Language Models](https://arxiv.org/abs/2506.14248)
Token length: 1112
Summarized using GPT-3.5-turbo
Append: [From What to Respond to When to Respond: Timely Response Generation for Open-domain Dialogue Agents](https://arxiv.org/abs/2506.14285)
Token length: 1422
Summarized using GPT-3.5-turbo
Append: [Expectation Confirmation Preference Optimization for Multi-Turn Conversational Recommendation Agent](https://arxiv.org/abs/2506.14302)
Token length: 1333
Summarized using GPT-3.5-turbo
Append: [Evaluation Should Not Ignore Variation: On the Impact of Reference Set Choice on Summarization Metrics](https://arxiv.org/abs/2506.14335)
Token length: 1147
Summarized using GPT-3.5-turbo
Append: [A Vision for Geo-Temporal Deep Research Systems: Towards Comprehensive, Transparent, and Reproducible Geo-Temporal Information Synthesis](https://arxiv.org/abs/2506.14345)
Token length: 856
Summarized using GPT-3.5-turbo
Append: [Digital Gatekeepers: Google's Role in Curating Hashtags and Subreddits](https://arxiv.org/abs/2506.14370)
Token length: 1001
Summarized using GPT-3.5-turbo
Append: [ELLIS Alicante at CQs-Gen 2025: Winning the critical thinking questions shared task: LLM-based question generation and selection](https://arxiv.org/abs/2506.14371)
Token length: 912
Summarized using GPT-3.5-turbo
Append: [Thunder-NUBench: A Benchmark for LLMs' Sentence-Level Negation Understanding](https://arxiv.org/abs/2506.14397)
Token length: 1192
Summarized using GPT-3.5-turbo
Append: [ImpliRet: Benchmarking the Implicit Fact Retrieval Challenge](https://arxiv.org/abs/2506.14407)
Token length: 1754
Summarized using GPT-3.5-turbo
Append: [LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs](https://arxiv.org/abs/2506.14429)
Token length: 1414
Summarized using GPT-3.5-turbo
Append: [How Far Can LLMs Improve from Experience? Measuring Test-Time Learning Ability in LLMs with Human Comparison](https://arxiv.org/abs/2506.14448)
Token length: 1617
Summarized using GPT-3.5-turbo
Append: [LexiMark: Robust Watermarking via Lexical Substitutions to Enhance Membership Verification of an LLM's Textual Training Data](https://arxiv.org/abs/2506.14474)
Token length: 1666
Summarized using GPT-3.5-turbo
Append: [LingoLoop Attack: Trapping MLLMs via Linguistic Context and State Entrapment into Endless Loops](https://arxiv.org/abs/2506.14493)
Token length: 972
Summarized using GPT-3.5-turbo
Append: [M2BeamLLM: Multimodal Sensing-empowered mmWave Beam Prediction with Large Language Models](https://arxiv.org/abs/2506.14532)
Token length: 1208
Summarized using GPT-3.5-turbo
Append: [AlphaDecay:Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs](https://arxiv.org/abs/2506.14562)
Token length: 1685
Summarized using GPT-3.5-turbo
Append: [GenerationPrograms: Fine-grained Attribution with Executable Programs](https://arxiv.org/abs/2506.14580)
Token length: 1648
Summarized using GPT-3.5-turbo
Append: [Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC Transpilation with Testing Guarantees](https://arxiv.org/abs/2506.14606)
Token length: 823
Summarized using GPT-3.5-turbo
Append: [When Does Meaning Backfire? Investigating the Role of AMRs in NLI](https://arxiv.org/abs/2506.14613)
Token length: 1088
Summarized using GPT-3.5-turbo
Append: [Probabilistic Aggregation and Targeted Embedding Optimization for Collective Moral Reasoning in Large Language Models](https://arxiv.org/abs/2506.14625)
Append: [AIn't Nothing But a Survey? Using Large Language Models for Coding German Open-Ended Survey Responses on Survey Motivation](https://arxiv.org/abs/2506.14634)
Append: [Revisiting Chain-of-Thought Prompting: Zero-shot Can Be Stronger than Few-shot](https://arxiv.org/abs/2506.14641)
Append: [Passing the Turing Test in Political Discourse: Fine-Tuning LLMs to Mimic Polarized Social Media Comments](https://arxiv.org/abs/2506.14645)
Append: [GuiLoMo: Allocating Expert Number and Rank for LoRA-MoE via Bilevel Optimization with GuidedSelection Vectors](https://arxiv.org/abs/2506.14646)
Append: [Massive Supervised Fine-tuning Experiments Reveal How Data, Layer, and Training Factors Shape LLM Alignment Quality](https://arxiv.org/abs/2506.14681)
Append: [Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time Markers](https://arxiv.org/abs/2506.14702)
Append: [Capacity Matters: a Proof-of-Concept for Transformer Memorization on Real-World Data](https://arxiv.org/abs/2506.14704)
Append: [Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning for LLMs](https://arxiv.org/abs/2506.14731)
Append: [Reasoning with Exploration: An Entropy Perspective](https://arxiv.org/abs/2506.14758)
Append: [From Bytes to Ideas: Language Modeling with Autoregressive U-Nets](https://arxiv.org/abs/2506.14761)
Append: [A Variational Framework for Improving Naturalness in Generative Spoken Language Models](https://arxiv.org/abs/2506.14767)
Append: [LittleBit: Ultra Low-Bit Quantization via Latent Factorization](https://arxiv.org/abs/2506.13771)
Append: [Knowledge Compression via Question Generation: Enhancing Multihop Document Retrieval without Fine-tuning](https://arxiv.org/abs/2506.13778)
Append: [AcademicBrowse: Benchmarking Academic Browse Ability of LLMs](https://arxiv.org/abs/2506.13784)
Append: [ICE-ID: A Novel Historical Census Data Benchmark Comparing NARS against LLMs, \& a ML Ensemble on Longitudinal Identity Resolution](https://arxiv.org/abs/2506.13792)
Append: [Investigating the Potential of Large Language Model-Based Router Multi-Agent Architectures for Foundation Design Automation: A Task Classification and Expert Selection Study](https://arxiv.org/abs/2506.13811)
Append: [Adaptive Guidance Accelerates Reinforcement Learning of Reasoning Models](https://arxiv.org/abs/2506.13923)
Append: [Multimodal Fusion with Semi-Supervised Learning Minimizes Annotation Quantity for Modeling Videoconference Conversation Experience](https://arxiv.org/abs/2506.13971)
Append: [CRITICTOOL: Evaluating Self-Critique Capabilities of Large Language Models in Tool-Calling Error Scenarios](https://arxiv.org/abs/2506.13977)
Append: [AssistedDS: Benchmarking How External Domain Knowledge Assists LLMs in Automated Data Science](https://arxiv.org/abs/2506.13992)
Append: [InsertRank: LLMs can reason over BM25 scores to Improve Listwise Reranking](https://arxiv.org/abs/2506.14086)
Append: [Innovating China's Intangible Cultural Heritage with DeepSeek + MidJourney: The Case of Yangliuqing theme Woodblock Prints](https://arxiv.org/abs/2506.14104)
Append: [RadFabric: Agentic AI System with Reasoning Capability for Radiology](https://arxiv.org/abs/2506.14142)
Append: [Acoustic scattering AI for non-invasive object classifications: A case study on hair assessment](https://arxiv.org/abs/2506.14148)
Append: [Pushing the Performance of Synthetic Speech Detection with Kolmogorov-Arnold Networks and Self-Supervised Learning Models](https://arxiv.org/abs/2506.14153)
Append: [Improving Practical Aspects of End-to-End Multi-Talker Speech Recognition for Online and Offline Scenarios](https://arxiv.org/abs/2506.14204)
Append: [Fretting-Transformer: Encoder-Decoder Model for MIDI to Tablature Transcription](https://arxiv.org/abs/2506.14223)
Append: [Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes Correct Reasoning in Base LLMs](https://arxiv.org/abs/2506.14245)
Append: [Improving LoRA with Variational Learning](https://arxiv.org/abs/2506.14280)
Append: [TGDPO: Harnessing Token-Level Reward Guidance for Enhancing Direct Preference Optimization](https://arxiv.org/abs/2506.14574)
Append: [Computational Studies in Influencer Marketing: A Systematic Literature Review](https://arxiv.org/abs/2506.14602)
Append: [VisText-Mosquito: A Multimodal Dataset and Benchmark for AI-Based Mosquito Breeding Site Detection and Reasoning](https://arxiv.org/abs/2506.14629)
Append: [Optimizing Length Compression in Large Reasoning Models](https://arxiv.org/abs/2506.14755)
Append: [ASCD: Attention-Steerable Contrastive Decoding for Reducing Hallucination in MLLM](https://arxiv.org/abs/2506.14766)
Append: [Compression of enumerations and gain](https://arxiv.org/abs/2304.03030)
Append: [FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with Human Feedback](https://arxiv.org/abs/2307.10867)
Append: [Exploring news intent and its application: A theory-driven approach](https://arxiv.org/abs/2312.16490)
Append: [Assessing the Reasoning Capabilities of LLMs in the context of Evidence-based Claim Verification](https://arxiv.org/abs/2402.10735)
Append: [Leveraging Large Language Models to Measure Gender Representation Bias in Gendered Language Corpora](https://arxiv.org/abs/2406.13677)
Append: [ETM: Modern Insights into Perspective on Text-to-SQL Evaluation in the Age of Large Language Models](https://arxiv.org/abs/2407.07313)
Append: [Geometric Signatures of Compositionality Across a Language Model's Lifetime](https://arxiv.org/abs/2410.01444)
Append: [Uncovering Overfitting in Large Language Model Editing](https://arxiv.org/abs/2410.07819)
Append: [Beyond Browsing: API-Based Web Agents](https://arxiv.org/abs/2410.16464)
Append: [Towards Better Open-Ended Text Generation: A Multicriteria Evaluation Framework](https://arxiv.org/abs/2410.18653)
Append: [Ensemble Watermarks for Large Language Models](https://arxiv.org/abs/2411.19563)
Append: [BESSTIE: A Benchmark for Sentiment and Sarcasm Classification for Varieties of English](https://arxiv.org/abs/2412.04726)
Append: [Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache Compression](https://arxiv.org/abs/2412.05693)
Append: [ClusterChat: Multi-Feature Search for Corpus Exploration](https://arxiv.org/abs/2412.14533)
Append: [Language and Planning in Robotic Navigation: A Multilingual Evaluation of State-of-the-Art Models](https://arxiv.org/abs/2501.05478)
Append: [The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs](https://arxiv.org/abs/2501.10970)
Append: [Counterfactual-Consistency Prompting for Relative Temporal Understanding in Large Language Models](https://arxiv.org/abs/2502.11425)
Append: [Towards Geo-Culturally Grounded LLM Generations](https://arxiv.org/abs/2502.13497)
Append: [PredictaBoard: Benchmarking LLM Score Predictability](https://arxiv.org/abs/2502.14445)
Append: [Modality-Aware Neuron Pruning for Unlearning in Multimodal Large Language Models](https://arxiv.org/abs/2502.15910)
Append: [LongSpec: Long-Context Lossless Speculative Decoding with Efficient Drafting and Verification](https://arxiv.org/abs/2502.17421)
Append: [Rectifying Belief Space via Unlearning to Harness LLMs' Reasoning](https://arxiv.org/abs/2502.20620)
Append: [SynGraph: A Dynamic Graph-LLM Synthesis Framework for Sparse Streaming User Sentiment Modeling](https://arxiv.org/abs/2503.04619)
Append: [Effect of Selection Format on LLM Performance](https://arxiv.org/abs/2503.06926)
Append: [SOPBench: Evaluating Language Agents at Following Standard Operating Procedures and Constraints](https://arxiv.org/abs/2503.08669)
Append: [Do Construction Distributions Shape Formal Language Learning In German BabyLMs?](https://arxiv.org/abs/2503.11593)
Append: [Automated Construction of a Knowledge Graph of Nuclear Fusion Energy for Effective Elicitation and Retrieval of Information](https://arxiv.org/abs/2504.07738)
Append: [CAPO: Cost-Aware Prompt Optimization](https://arxiv.org/abs/2504.16005)
Append: [Graph RAG for Legal Norms: A Hierarchical, Temporal and Deterministic Approach](https://arxiv.org/abs/2505.00039)
Append: [LongCodeBench: Evaluating Coding LLMs at 1M Context Windows](https://arxiv.org/abs/2505.07897)
Append: [GuideBench: Benchmarking Domain-Oriented Guideline Following for LLM Agents](https://arxiv.org/abs/2505.11368)
Append: [CAPTURE: Context-Aware Prompt Injection Testing and Robustness Enhancement](https://arxiv.org/abs/2505.12368)
Append: [Personalizing Student-Agent Interactions Using Log-Contextualized Retrieval Augmented Generation (RAG)](https://arxiv.org/abs/2505.17238)
Append: [REAL-Prover: Retrieval Augmented Lean Prover for Mathematical Reasoning](https://arxiv.org/abs/2505.20613)
Append: [EEG2TEXT-CN: An Exploratory Study of Open-Vocabulary Chinese Text-EEG Alignment via Large Language Model and Contrastive Learning on ChineseEEG](https://arxiv.org/abs/2506.00854)
Append: [Hanfu-Bench: A Multimodal Benchmark on Cross-Temporal Cultural Understanding and Transcreation](https://arxiv.org/abs/2506.01565)
Append: [EuroLLM-9B: Technical Report](https://arxiv.org/abs/2506.04079)
Append: [MultiMatch: Multihead Consistency Regularization Matching for Semi-Supervised Text Classification](https://arxiv.org/abs/2506.07801)
Append: [Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature](https://arxiv.org/abs/2308.12420)
Append: [Bridging Social Media and Search Engines: Dredge Words and the Detection of Unreliable Domains](https://arxiv.org/abs/2406.11423)
Append: [Do Large Language Models Exhibit Cognitive Dissonance? Studying the Difference Between Revealed Beliefs and Stated Answers](https://arxiv.org/abs/2406.14986)
Append: [Controllable and Reliable Knowledge-Intensive Task-Oriented Conversational Agents with Declarative Genie Worksheets](https://arxiv.org/abs/2407.05674)
Append: [Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents](https://arxiv.org/abs/2410.05243)
Append: [ONEBench to Test Them All: Sample-Level Benchmarking Over Open-Ended Capabilities](https://arxiv.org/abs/2412.06745)
Append: [Agent Laboratory: Using LLM Agents as Research Assistants](https://arxiv.org/abs/2501.04227)
Append: [From tools to thieves: Measuring and understanding public perceptions of AI through crowdsourced metaphors](https://arxiv.org/abs/2501.18045)
Append: [SAE-V: Interpreting Multimodal Models for Enhanced Alignment](https://arxiv.org/abs/2502.17514)
Append: [Reward Shaping to Mitigate Reward Hacking in RLHF](https://arxiv.org/abs/2502.18770)
Append: [OWLViz: An Open-World Benchmark for Visual Question Answering](https://arxiv.org/abs/2503.07631)
Append: [Chain-of-Thought Reasoning In The Wild Is Not Always Faithful](https://arxiv.org/abs/2503.08679)
Append: [Assessing Consistency and Reproducibility in the Outputs of Large Language Models: Evidence Across Diverse Finance and Accounting Tasks](https://arxiv.org/abs/2503.16974)
Append: [Inherent and emergent liability issues in LLM-based agentic systems: a principal-agent perspective](https://arxiv.org/abs/2504.03255)
Append: [IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2505.12442)
Append: [Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis](https://arxiv.org/abs/2505.13227)
Append: [Roboflow100-VL: A Multi-Domain Object Detection Benchmark for Vision-Language Models](https://arxiv.org/abs/2505.20612)
Append: [MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal Medical Reasoning](https://arxiv.org/abs/2506.00555)
Append: [AgentCPM-GUI: Building Mobile-Use Agents with Reinforcement Fine-Tuning](https://arxiv.org/abs/2506.01391)
append_entries: 141
Finish: 2025-06-18 04:30:45.857841
------------------------------------------------------
Started: 2025-06-18 06:25:14.575700
Existing_entries: 1141
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1169
Summarized using GPT-3.5-turbo
Append: [Position: Editing Large Language Models Poses Serious Safety Risks](https://arxiv.org/abs/2502.02958)
append_entries: 1
Finish: 2025-06-18 06:25:16.948504
------------------------------------------------------
Started: 2025-06-18 08:23:04.665089
Existing_entries: 1001
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-18 08:23:05.007042
------------------------------------------------------
Started: 2025-06-18 10:18:24.081473
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-18 10:18:24.510375
------------------------------------------------------
Started: 2025-06-18 12:35:37.856203
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-18 12:35:38.220329
------------------------------------------------------
Started: 2025-06-18 14:16:57.575974
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-18 14:16:57.921369
------------------------------------------------------
Started: 2025-06-18 16:21:39.183904
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-18 16:21:39.596410
------------------------------------------------------
Started: 2025-06-18 18:23:39.804750
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-18 18:23:40.154300
------------------------------------------------------
Started: 2025-06-18 20:19:51.333192
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-18 20:19:51.765525
------------------------------------------------------
Started: 2025-06-18 22:15:58.301289
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-18 22:15:58.640863
------------------------------------------------------
Started: 2025-06-19 01:21:18.406145
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-19 01:21:18.747846
------------------------------------------------------
Started: 2025-06-19 03:15:22.599127
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-19 03:15:22.975099
------------------------------------------------------
Started: 2025-06-19 04:28:46.839137
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1387
Summarized using GPT-3.5-turbo
Append: [Adverse Event Extraction from Discharge Summaries: A New Dataset, Annotation Scheme, and Initial Findings](https://arxiv.org/abs/2506.14900)
Token length: 1410
Summarized using GPT-3.5-turbo
Append: [Combining Constrained and Unconstrained Decoding via Boosting: BoostCD and Its Application to Information Extraction](https://arxiv.org/abs/2506.14901)
Token length: 1329
Summarized using GPT-3.5-turbo
Append: [CrEst: Credibility Estimation for Contexts in LLMs via Weak Supervision](https://arxiv.org/abs/2506.14912)
Token length: 1691
Summarized using GPT-3.5-turbo
Append: [MDBench: A Synthetic Multi-Document Reasoning Benchmark Generated with Knowledge Guidance](https://arxiv.org/abs/2506.14927)
Token length: 1433
Summarized using GPT-3.5-turbo
Append: [From Chat to Checkup: Can Large Language Models Assist in Diabetes Prediction?](https://arxiv.org/abs/2506.14949)
Token length: 863
Summarized using GPT-3.5-turbo
Append: [Memory Tokens: Large Language Models Can Generate Reversible Sentence Embeddings](https://arxiv.org/abs/2506.15001)
Token length: 1216
Summarized using GPT-3.5-turbo
Append: [Identifying social isolation themes in NVDRS text narratives using topic modeling and text-classification methods](https://arxiv.org/abs/2506.15030)
Token length: 1315
Summarized using GPT-3.5-turbo
Append: [Semantically-Aware Rewards for Open-Ended R1 Training in Free-Form Generation](https://arxiv.org/abs/2506.15068)
Token length: 943
Summarized using GPT-3.5-turbo
Append: [Learning-Time Encoding Shapes Unlearning in LLMs](https://arxiv.org/abs/2506.15076)
Token length: 1266
Summarized using GPT-3.5-turbo
Append: [Improving Dialogue Discourse Parsing through Discourse-aware Utterance Clarification](https://arxiv.org/abs/2506.15081)
Token length: 1489
Summarized using GPT-3.5-turbo
Append: [CKD-EHR:Clinical Knowledge Distillation for Electronic Health Records](https://arxiv.org/abs/2506.15118)
Token length: 1350
Summarized using GPT-3.5-turbo
Append: [Modeling the One-to-Many Property in Open-Domain Dialogue with LLMs](https://arxiv.org/abs/2506.15131)
Token length: 961
Summarized using GPT-3.5-turbo
Append: [Thunder-Tok: Minimizing Tokens per Word in Tokenizing Korean Texts for Generative Language Models](https://arxiv.org/abs/2506.15138)
Token length: 1293
Summarized using GPT-3.5-turbo
Append: [Emergence of Primacy and Recency Effect in Mamba: A Mechanistic Point of View](https://arxiv.org/abs/2506.15156)
Token length: 1204
Summarized using GPT-3.5-turbo
Append: [A Comparative Study of Task Adaptation Techniques of Large Language Models for Identifying Sustainable Development Goals](https://arxiv.org/abs/2506.15208)
Token length: 1811
Summarized using GPT-3.5-turbo
Append: [ProtoReasoning: Prototypes as the Foundation for Generalizable Reasoning in LLMs](https://arxiv.org/abs/2506.15211)
Token length: 1489
Summarized using GPT-3.5-turbo
Append: [MinosEval: Distinguishing Factoid and Non-Factoid for Tailored Open-Ended QA Evaluation with LLMs](https://arxiv.org/abs/2506.15215)
Token length: 963
Summarized using GPT-3.5-turbo
Append: [Lost in Variation? Evaluating NLI Performance in Basque and Spanish Geographical Variants](https://arxiv.org/abs/2506.15239)
Token length: 1355
Summarized using GPT-3.5-turbo
Append: [Research on Graph-Retrieval Augmented Generation Based on Historical Text Knowledge Graphs](https://arxiv.org/abs/2506.15241)
Token length: 994
Summarized using GPT-3.5-turbo
Append: [TopClustRAG at SIGIR 2025 LiveRAG Challenge](https://arxiv.org/abs/2506.15246)
Token length: 1133
Summarized using GPT-3.5-turbo
Append: [Thunder-DeID: Accurate and Efficient De-identification Framework for Korean Court Judgments](https://arxiv.org/abs/2506.15266)
Token length: 1042
Summarized using GPT-3.5-turbo
Append: [Cohort Discovery: A Survey on LLM-Assisted Clinical Trial Recruitment](https://arxiv.org/abs/2506.15301)
Token length: 765
Summarized using GPT-3.5-turbo
Append: [ConLID: Supervised Contrastive Learning for Low-Resource Language Identification](https://arxiv.org/abs/2506.15304)
Token length: 1466
Summarized using GPT-3.5-turbo
Append: [DeVisE: Behavioral Testing of Medical Large Language Models](https://arxiv.org/abs/2506.15339)
Token length: 1250
Summarized using GPT-3.5-turbo
Append: [SANSKRITI: A Comprehensive Benchmark for Evaluating Language Models' Knowledge of Indian Culture](https://arxiv.org/abs/2506.15355)
Token length: 1545
Summarized using GPT-3.5-turbo
Append: [COSMMIC: Comment-Sensitive Multimodal Multilingual Indian Corpus for Summarization and Headline Generation](https://arxiv.org/abs/2506.15372)
Token length: 1746
Summarized using GPT-3.5-turbo
Append: [Targeted Lexical Injection: Unlocking Latent Cross-Lingual Alignment in Lugha-Llama via Early-Layer LoRA Fine-Tuning](https://arxiv.org/abs/2506.15415)
Token length: 1041
Summarized using GPT-3.5-turbo
Append: [Understanding GUI Agent Localization Biases through Logit Sharpness](https://arxiv.org/abs/2506.15425)
Token length: 1750
Summarized using GPT-3.5-turbo
Append: [AgentGroupChat-V2: Divide-and-Conquer Is What LLM-Based Multi-Agent System Need](https://arxiv.org/abs/2506.15451)
Token length: 1231
Summarized using GPT-3.5-turbo
Append: [RE-IMAGINE: Symbolic Benchmark Synthesis for Reasoning Evaluation](https://arxiv.org/abs/2506.15455)
Token length: 1562
Summarized using GPT-3.5-turbo
Append: [Context-Informed Grounding Supervision](https://arxiv.org/abs/2506.15480)
Token length: 1436
Summarized using GPT-3.5-turbo
Append: [SPARE: Single-Pass Annotation with Reference-Guided Evaluation for Automatic Process Supervision and Reward Modelling](https://arxiv.org/abs/2506.15498)
Token length: 1519
Summarized using GPT-3.5-turbo
Append: [Enhancing Hyperbole and Metaphor Detection with Their Bidirectional Dynamic Interaction and Emotion Knowledge](https://arxiv.org/abs/2506.15504)
Token length: 1490
Summarized using GPT-3.5-turbo
Append: [Lessons from Training Grounded LLMs with Verifiable Rewards](https://arxiv.org/abs/2506.15522)
Token length: 1795
Summarized using GPT-3.5-turbo
Append: [RATTENTION: Towards the Minimal Sliding Window Size in Local-Global Attention Models](https://arxiv.org/abs/2506.15545)
Token length: 1037
Summarized using GPT-3.5-turbo
Append: [Approximating Language Model Training Data from Weights](https://arxiv.org/abs/2506.15553)
Token length: 1365
Summarized using GPT-3.5-turbo
Append: [PredGen: Accelerated Inference of Large Language Models through Input-Time Speculation for Real-Time Speech Interaction](https://arxiv.org/abs/2506.15556)
Token length: 1088
Summarized using GPT-3.5-turbo
Append: [Gender Inclusivity Fairness Index (GIFI): A Multilevel Framework for Evaluating Gender Diversity in Large Language Models](https://arxiv.org/abs/2506.15568)
Token length: 1037
Summarized using GPT-3.5-turbo
Append: [SciVer: Evaluating Foundation Models for Multimodal Scientific Claim Verification](https://arxiv.org/abs/2506.15569)
Token length: 1625
Summarized using GPT-3.5-turbo
Append: [DiscoSG: Towards Discourse-Level Text Scene Graph Parsing through Iterative Graph Refinement](https://arxiv.org/abs/2506.15583)
Token length: 1380
Summarized using GPT-3.5-turbo
Append: [WikiMixQA: A Multimodal Benchmark for Question Answering over Tables and Charts](https://arxiv.org/abs/2506.15594)
Token length: 1482
Summarized using GPT-3.5-turbo
Append: [From Model to Classroom: Evaluating Generated MCQs for Portuguese with Narrative and Difficulty Concerns](https://arxiv.org/abs/2506.15598)
Token length: 1776
Summarized using GPT-3.5-turbo
Append: [The Compositional Architecture of Regret in Large Language Models](https://arxiv.org/abs/2506.15617)
Token length: 1086
Summarized using GPT-3.5-turbo
Append: [Minding the Politeness Gap in Cross-cultural Communication](https://arxiv.org/abs/2506.15623)
Token length: 1163
Summarized using GPT-3.5-turbo
Append: [Revisiting Compositional Generalization Capability of Large Language Models Considering Instruction Following Ability](https://arxiv.org/abs/2506.15629)
Token length: 935
Summarized using GPT-3.5-turbo
Append: [Oldies but Goldies: The Potential of Character N-grams for Romanian Texts](https://arxiv.org/abs/2506.15650)
Token length: 1054
Summarized using GPT-3.5-turbo
Append: [CC-LEARN: Cohort-based Consistency Learning](https://arxiv.org/abs/2506.15662)
Token length: 957
Summarized using GPT-3.5-turbo
Append: [Leaky Thoughts: Large Reasoning Models Are Not Private Thinkers](https://arxiv.org/abs/2506.15674)
Token length: 1001
Summarized using GPT-3.5-turbo
Append: [Gender-Neutral Machine Translation Strategies in Practice](https://arxiv.org/abs/2506.15676)
Token length: 1265
Summarized using GPT-3.5-turbo
Append: [GenRecal: Generation after Recalibration from Large to Small Vision-Language Models](https://arxiv.org/abs/2506.15681)
Append: [PhantomHunter: Detecting Unseen Privately-Tuned LLM-Generated Text via Family-Aware Learning](https://arxiv.org/abs/2506.15683)
Append: [ETS: Open Vocabulary Electroencephalography-To-Text Decoding and Sentiment Classification](https://arxiv.org/abs/2506.14783)
Append: [SemIRNet: A Semantic Irony Recognition Network for Multimodal Sarcasm Detection](https://arxiv.org/abs/2506.14791)
Append: [Assembly of Experts: Linear-time construction of the Chimera LLM variants with emergent and adaptable behaviors](https://arxiv.org/abs/2506.14794)
Append: [Argus Inspection: Do Multimodal Large Language Models Possess the Eye of Panoptes?](https://arxiv.org/abs/2506.14805)
Append: [Detecting Narrative Shifts through Persistent Structures: A Topological Analysis of Media Discourse](https://arxiv.org/abs/2506.14836)
Append: [Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching](https://arxiv.org/abs/2506.14852)
Append: [Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain Perspective](https://arxiv.org/abs/2506.14965)
Append: [Hypothesis Testing for Quantifying LLM-Human Misalignment in Multiple Choice Settings](https://arxiv.org/abs/2506.14997)
Append: [Optimal Embedding Learning Rate in LLMs: The Effect of Vocabulary Size](https://arxiv.org/abs/2506.15025)
Append: [An accurate and revised version of optical character recognition-based speech synthesis using LabVIEW](https://arxiv.org/abs/2506.15029)
Append: [Identifying economic narratives in large text corpora -- An integrated approach using Large Language Models](https://arxiv.org/abs/2506.15041)
Append: [SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning](https://arxiv.org/abs/2506.15154)
Append: [video-SALMONN 2: Captioning-Enhanced Audio-Visual Large Language Models](https://arxiv.org/abs/2506.15220)
Append: [When and How Unlabeled Data Provably Improve In-Context Learning](https://arxiv.org/abs/2506.15329)
Append: [Factorized RVQ-GAN For Disentangled Speech Tokenization](https://arxiv.org/abs/2506.15456)
Append: [Capturing Polysemanticity with PRISM: A Multi-Concept Feature Description Framework](https://arxiv.org/abs/2506.15538)
Append: [LoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning](https://arxiv.org/abs/2506.15606)
Append: [AutoRule: Reasoning Chain-of-thought Extracted Rule-based Rewards Improve Preference Learning](https://arxiv.org/abs/2506.15651)
Append: [Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence](https://arxiv.org/abs/2506.15677)
Append: [Dense SAE Latents Are Features, Not Bugs](https://arxiv.org/abs/2506.15679)
Append: [HiURE: Hierarchical Exemplar Contrastive Learning for Unsupervised Relation Extraction](https://arxiv.org/abs/2205.02225)
Append: [An Effective Incorporating Heterogeneous Knowledge Curriculum Learning for Sequence Labeling](https://arxiv.org/abs/2402.13534)
Append: [Lean Workbook: A large-scale Lean problem set formalized from natural language math problems](https://arxiv.org/abs/2406.03847)
Append: [A Systematic Survey of Natural Language Processing for the Greek Language](https://arxiv.org/abs/2407.09861)
Append: [Robust Utility-Preserving Text Anonymization Based on Large Language Models](https://arxiv.org/abs/2407.11770)
Append: [RadioRAG: Online Retrieval-augmented Generation for Radiology Question Answering](https://arxiv.org/abs/2407.15621)
Append: [Root Defence Strategies: Ensuring Safety of LLM at the Decoding Level](https://arxiv.org/abs/2410.06809)
Append: [Pap2Pat: Benchmarking Outline-Guided Long-Text Patent Generation with Patent-Paper Pairs](https://arxiv.org/abs/2410.07009)
Append: [Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities Using Only Forward Passes](https://arxiv.org/abs/2410.16930)
Append: [Interchangeable Token Embeddings for Extendable Vocabulary and Alpha-Equivalence](https://arxiv.org/abs/2410.17161)
Append: [LL\"aMmlein: Transparent, Compact and Competitive German-Only Language Models from Scratch](https://arxiv.org/abs/2411.11171)
Append: [REVOLVE: Optimizing AI Systems by Tracking Response Evolution in Textual Optimization](https://arxiv.org/abs/2412.03092)
Append: [Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition](https://arxiv.org/abs/2412.13612)
Append: [Aligning AI Research with the Needs of Clinical Coding Workflows: Eight Recommendations Based on US Data Analysis and Critical Review](https://arxiv.org/abs/2412.18043)
Append: [Can LLMs Ask Good Questions?](https://arxiv.org/abs/2501.03491)
Append: [Perspective Transition of Large Language Models for Solving Subjective Tasks](https://arxiv.org/abs/2501.09265)
Append: [I-MCTS: Enhancing Agentic AutoML via Introspective Monte Carlo Tree Search](https://arxiv.org/abs/2502.14693)
Append: [CODESYNC: Synchronizing Large Language Models with Dynamic Code Evolution at Scale](https://arxiv.org/abs/2502.16645)
Append: [Alleviating Distribution Shift in Synthetic Data for Machine Translation Quality Estimation](https://arxiv.org/abs/2502.19941)
Append: [PsychBench: A comprehensive and professional benchmark for evaluating the performance of LLM-assisted psychiatric clinical practice](https://arxiv.org/abs/2503.01903)
Append: [Adding Chocolate to Mint: Mitigating Metric Interference in Machine Translation](https://arxiv.org/abs/2503.08327)
Append: [Resolving UnderEdit & OverEdit with Iterative & Neighbor-Assisted Model Editing](https://arxiv.org/abs/2503.11895)
Append: [UD-English-CHILDES: A Collected Resource of Gold and Silver Universal Dependencies Trees for Child Language Interactions](https://arxiv.org/abs/2504.20304)
Append: [TSLFormer: A Lightweight Transformer Model for Turkish Sign Language Recognition Using Skeletal Landmarks](https://arxiv.org/abs/2505.07890)
Append: [Efficiently Building a Domain-Specific Large Language Model from Scratch: A Case Study of a Classical Chinese Large Language Model](https://arxiv.org/abs/2505.11810)
Append: [J4R: Learning to Judge with Equivalent Initial State Group Relative Policy Optimization](https://arxiv.org/abs/2505.13346)
Append: [GreekBarBench: A Challenging Benchmark for Free-Text Legal Reasoning and Citations](https://arxiv.org/abs/2505.17267)
Append: [Efficient Long CoT Reasoning in Small Language Models](https://arxiv.org/abs/2505.18440)
Append: [ALPS: Attention Localization and Pruning Strategy for Efficient Alignment of Large Language Models](https://arxiv.org/abs/2505.18799)
Append: [The Avengers: A Simple Recipe for Uniting Smaller Language Models to Challenge Proprietary Giants](https://arxiv.org/abs/2505.19797)
Append: [PEDANTIC: A Dataset for the Automatic Examination of Definiteness in Patent Claims](https://arxiv.org/abs/2505.21342)
Append: [How much do language models memorize?](https://arxiv.org/abs/2505.24832)
Append: [SemVink: Advancing VLMs' Semantic Understanding of Optical Illusions via Visual Global Thinking](https://arxiv.org/abs/2506.02803)
Append: [LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure Profiles](https://arxiv.org/abs/2506.06561)
Append: [BriefMe: A Legal NLP Benchmark for Assisting with Legal Briefs](https://arxiv.org/abs/2506.06619)
Append: [BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning](https://arxiv.org/abs/2506.06955)
Append: [Bi-VLDoc: Bidirectional Vision-Language Modeling for Visually-Rich Document Understanding](https://arxiv.org/abs/2206.13155)
Append: [OM4OV: Leveraging Ontology Matching for Ontology Versioning](https://arxiv.org/abs/2409.20302)
Append: [A Guide to Misinformation Detection Data and Evaluation](https://arxiv.org/abs/2411.05060)
Append: [Entropy-based Exploration Conduction for Multi-step Reasoning](https://arxiv.org/abs/2503.15848)
Append: [Fractured Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.12992)
Append: [Aug2Search: Enhancing Facebook Marketplace Search with LLM-Generated Synthetic Data Augmentation](https://arxiv.org/abs/2505.16065)
Append: [ChemHAS: Hierarchical Agent Stacking for Enhancing Chemistry Tools](https://arxiv.org/abs/2505.21569)
append_entries: 114
Finish: 2025-06-19 04:30:23.432981
------------------------------------------------------
Started: 2025-06-19 06:25:12.483518
Existing_entries: 1114
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 867
Summarized using GPT-3.5-turbo
Append: [Wait, We Don't Need to "Wait"! Removing Thinking Tokens Improves Reasoning Efficiency](https://arxiv.org/abs/2506.08343)
Token length: 1543
Summarized using GPT-3.5-turbo
Append: [Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning](https://arxiv.org/abs/2506.09033)
append_entries: 2
Finish: 2025-06-19 06:25:15.646540
------------------------------------------------------
Started: 2025-06-19 08:23:00.473611
Existing_entries: 1002
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-19 08:23:00.759202
------------------------------------------------------
Started: 2025-06-19 10:18:21.199639
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-19 10:18:21.476186
------------------------------------------------------
Started: 2025-06-19 12:35:04.321412
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-19 12:35:04.641154
------------------------------------------------------
Started: 2025-06-19 14:15:59.158276
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-19 14:15:59.441146
------------------------------------------------------
Started: 2025-06-19 16:20:31.477448
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-19 16:20:31.768552
------------------------------------------------------
Started: 2025-06-19 18:22:22.826300
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-19 18:22:23.136957
------------------------------------------------------
Started: 2025-06-19 20:19:40.338760
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-19 20:19:40.666703
------------------------------------------------------
Started: 2025-06-19 22:15:56.631994
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-19 22:15:56.915795
------------------------------------------------------
Started: 2025-06-20 01:20:32.802720
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-20 01:20:33.090081
------------------------------------------------------
Started: 2025-06-20 03:14:37.109688
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-20 03:14:37.473648
------------------------------------------------------
Started: 2025-06-20 04:24:07.088822
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-20 04:24:07.149362
------------------------------------------------------
Started: 2025-06-20 06:25:14.333314
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-20 06:25:14.414442
------------------------------------------------------
Started: 2025-06-20 08:22:20.292950
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-20 08:22:20.356995
------------------------------------------------------
Started: 2025-06-20 10:18:15.119469
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-20 10:18:15.195183
------------------------------------------------------
Started: 2025-06-20 12:35:01.974185
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-20 12:35:02.049921
------------------------------------------------------
Started: 2025-06-20 14:16:16.518692
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-20 14:16:16.579628
------------------------------------------------------
Started: 2025-06-20 16:20:52.062176
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-20 16:20:52.139675
------------------------------------------------------
Started: 2025-06-20 18:22:54.087380
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-20 18:22:54.205989
------------------------------------------------------
Started: 2025-06-20 20:18:25.833157
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-20 20:18:25.894284
------------------------------------------------------
Started: 2025-06-20 22:15:56.021602
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-20 22:15:56.079643
------------------------------------------------------
Started: 2025-06-21 01:19:37.418597
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-21 01:19:37.477271
------------------------------------------------------
Started: 2025-06-21 03:10:42.750228
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-21 03:10:42.840465
------------------------------------------------------
Started: 2025-06-21 04:20:31.525742
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-21 04:20:31.588142
------------------------------------------------------
Started: 2025-06-21 06:22:24.325548
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-21 06:22:24.418093
------------------------------------------------------
Started: 2025-06-21 08:20:22.672897
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-21 08:20:22.767589
------------------------------------------------------
Started: 2025-06-21 10:16:07.977687
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-21 10:16:08.044368
------------------------------------------------------
Started: 2025-06-21 12:31:03.698777
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-21 12:31:03.807688
------------------------------------------------------
Started: 2025-06-21 14:15:32.980635
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-21 14:15:33.051506
------------------------------------------------------
Started: 2025-06-21 16:19:22.967227
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-21 16:19:23.025289
------------------------------------------------------
Started: 2025-06-21 18:20:50.235298
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-21 18:20:50.312556
------------------------------------------------------
Started: 2025-06-21 20:16:57.679019
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-21 20:16:57.740157
------------------------------------------------------
Started: 2025-06-21 22:14:58.307841
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-21 22:14:58.384846
------------------------------------------------------
Started: 2025-06-22 01:28:15.958453
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-22 01:28:16.017776
------------------------------------------------------
Started: 2025-06-22 03:23:13.160309
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-22 03:23:13.219590
------------------------------------------------------
Started: 2025-06-22 04:28:00.002643
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-22 04:28:00.128051
------------------------------------------------------
Started: 2025-06-22 06:23:04.042004
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-22 06:23:04.128212
------------------------------------------------------
Started: 2025-06-22 08:20:20.025841
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-22 08:20:20.088587
------------------------------------------------------
Started: 2025-06-22 10:16:47.223640
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-22 10:16:47.279912
------------------------------------------------------
Started: 2025-06-22 12:31:10.869150
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-22 12:31:10.968034
------------------------------------------------------
Started: 2025-06-22 14:14:29.745239
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-22 14:14:29.816109
------------------------------------------------------
Started: 2025-06-22 16:19:13.016825
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-22 16:19:13.130723
------------------------------------------------------
Started: 2025-06-22 18:21:35.481086
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-22 18:21:35.575113
------------------------------------------------------
Started: 2025-06-22 20:17:26.769902
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-22 20:17:26.867949
------------------------------------------------------
Started: 2025-06-22 22:15:10.386097
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-22 22:15:10.485625
------------------------------------------------------
Started: 2025-06-23 01:25:44.551640
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-23 01:25:44.614689
------------------------------------------------------
Started: 2025-06-23 03:23:59.220343
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-23 03:23:59.282722
------------------------------------------------------
Started: 2025-06-23 04:34:43.213992
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 867
Summarized using GPT-3.5-turbo
Append: [Veracity: An Open-Source AI Fact-Checking System](https://arxiv.org/abs/2506.15794)
Token length: 699
Summarized using GPT-3.5-turbo
Append: [Rethinking LLM Training through Information Geometry and Quantum Metrics](https://arxiv.org/abs/2506.15830)
Token length: 1664
Summarized using GPT-3.5-turbo
Append: [MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents](https://arxiv.org/abs/2506.15841)
Token length: 873
Summarized using GPT-3.5-turbo
Append: [Finance Language Model Evaluation (FLaME)](https://arxiv.org/abs/2506.15846)
Token length: 1199
Summarized using GPT-3.5-turbo
Append: [Entropy-Driven Pre-Tokenization for Byte-Pair Encoding](https://arxiv.org/abs/2506.15889)
Token length: 1172
Summarized using GPT-3.5-turbo
Append: [Language Models can perform Single-Utterance Self-Correction of Perturbed Reasoning](https://arxiv.org/abs/2506.15894)
Token length: 1212
Summarized using GPT-3.5-turbo
Append: [From RAG to Agentic: Validating Islamic-Medicine Responses with LLM Agents](https://arxiv.org/abs/2506.15911)
Token length: 1134
Summarized using GPT-3.5-turbo
Append: [Reranking-based Generation for Unbiased Perspective Summarization](https://arxiv.org/abs/2506.15925)
Token length: 998
Summarized using GPT-3.5-turbo
Append: [A Vietnamese Dataset for Text Segmentation and Multiple Choices Reading Comprehension](https://arxiv.org/abs/2506.15978)
Token length: 1345
Summarized using GPT-3.5-turbo
Append: [Double Entendre: Robust Audio-Based AI-Generated Lyrics Detection via Multi-View Fusion](https://arxiv.org/abs/2506.15981)
Token length: 1378
Summarized using GPT-3.5-turbo
Append: [From General to Targeted Rewards: Surpassing GPT-4 in Open-Ended Long-Context Generation](https://arxiv.org/abs/2506.16024)
Token length: 1264
Summarized using GPT-3.5-turbo
Append: [EvoLM: In Search of Lost Language Model Training Dynamics](https://arxiv.org/abs/2506.16029)
Token length: 803
Summarized using GPT-3.5-turbo
Append: [Enhancing Document-Level Question Answering via Multi-Hop Retrieval-Augmented Generation with LLaMA 3](https://arxiv.org/abs/2506.16037)
Token length: 1380
Summarized using GPT-3.5-turbo
Append: [DynScaling: Efficient Verifier-free Inference Scaling via Dynamic and Integrated Sampling](https://arxiv.org/abs/2506.16043)
Token length: 1614
Summarized using GPT-3.5-turbo
Append: [A Hybrid DeBERTa and Gated Broad Learning System for Cyberbullying Detection in English Text](https://arxiv.org/abs/2506.16052)
Token length: 1103
Summarized using GPT-3.5-turbo
Append: [Knee-Deep in C-RASP: A Transformer Depth Hierarchy](https://arxiv.org/abs/2506.16055)
Token length: 1553
Summarized using GPT-3.5-turbo
Append: [Self-Critique-Guided Curiosity Refinement: Enhancing Honesty and Helpfulness in Large Language Models via In-Context Learning](https://arxiv.org/abs/2506.16064)
Token length: 1642
Summarized using GPT-3.5-turbo
Append: [Cyberbullying Detection in Hinglish Text Using MURIL and Explainable AI](https://arxiv.org/abs/2506.16066)
Token length: 1480
Summarized using GPT-3.5-turbo
Append: [FinCoT: Grounding Chain-of-Thought in Expert Financial Reasoning](https://arxiv.org/abs/2506.16123)
Token length: 1466
Summarized using GPT-3.5-turbo
Append: [Under the Shadow of Babel: How Language Shapes Reasoning in LLMs](https://arxiv.org/abs/2506.16151)
Token length: 1260
Summarized using GPT-3.5-turbo
Append: [SGIC: A Self-Guided Iterative Calibration Framework for RAG](https://arxiv.org/abs/2506.16172)
Token length: 704
Summarized using GPT-3.5-turbo
Append: [JETHICS: Japanese Ethics Understanding Evaluation Dataset](https://arxiv.org/abs/2506.16187)
Token length: 576
Summarized using GPT-3.5-turbo
Append: [Web(er) of Hate: A Survey on How Hate Speech Is Typed](https://arxiv.org/abs/2506.16190)
Token length: 1106
Summarized using GPT-3.5-turbo
Append: [Comparative Analysis of Abstractive Summarization Models for Clinical Radiology Reports](https://arxiv.org/abs/2506.16247)
Token length: 1046
Summarized using GPT-3.5-turbo
Append: [End-to-End Speech Translation for Low-Resource Languages Using Weakly Labeled Data](https://arxiv.org/abs/2506.16251)
Token length: 1042
Summarized using GPT-3.5-turbo
Append: [Advancing Automated Speaking Assessment Leveraging Multifaceted Relevance and Grammar Information](https://arxiv.org/abs/2506.16285)
Token length: 1110
Summarized using GPT-3.5-turbo
Append: [PL-Guard: Benchmarking Language Model Safety for Polish](https://arxiv.org/abs/2506.16322)
Token length: 1205
Summarized using GPT-3.5-turbo
Append: [Generalizability of Media Frames: Corpus creation and analysis across countries](https://arxiv.org/abs/2506.16337)
Token length: 970
Summarized using GPT-3.5-turbo
Append: [Analyzing the Influence of Knowledge Graph Information on Relation Extraction](https://arxiv.org/abs/2506.16343)
Token length: 926
Summarized using GPT-3.5-turbo
Append: [DISCIE -- Discriminative Closed Information Extraction](https://arxiv.org/abs/2506.16348)
Token length: 1086
Summarized using GPT-3.5-turbo
Append: [Can structural correspondences ground real world representational content in Large Language Models?](https://arxiv.org/abs/2506.16370)
Token length: 1670
Summarized using GPT-3.5-turbo
Append: [InstructTTSEval: Benchmarking Complex Natural-Language Instruction Following in Text-to-Speech Systems](https://arxiv.org/abs/2506.16381)
Token length: 1220
Summarized using GPT-3.5-turbo
Append: [Large Language Models in Argument Mining: A Survey](https://arxiv.org/abs/2506.16383)
Token length: 669
Summarized using GPT-3.5-turbo
Append: [HausaNLP at SemEval-2025 Task 11: Advancing Hausa Text-based Emotion Detection](https://arxiv.org/abs/2506.16388)
Token length: 1262
Summarized using GPT-3.5-turbo
Append: [RiOT: Efficient Prompt Refinement with Residual Optimization Tree](https://arxiv.org/abs/2506.16389)
Token length: 1711
Summarized using GPT-3.5-turbo
Append: [From LLM-anation to LLM-orchestrator: Coordinating Small Models for Data Labeling](https://arxiv.org/abs/2506.16393)
Token length: 1067
Summarized using GPT-3.5-turbo
Append: [OJBench: A Competition Level Code Benchmark For Large Language Models](https://arxiv.org/abs/2506.16395)
Token length: 997
Summarized using GPT-3.5-turbo
Append: [NepaliGPT: A Generative Language Model for the Nepali Language](https://arxiv.org/abs/2506.16399)
Token length: 1186
Summarized using GPT-3.5-turbo
Append: [When Does Divide and Conquer Work for Long Context LLM? A Noise Decomposition Framework](https://arxiv.org/abs/2506.16411)
Token length: 1947
Summarized using GPT-3.5-turbo
Append: [REIS: A High-Performance and Energy-Efficient Retrieval System with In-Storage Processing](https://arxiv.org/abs/2506.16444)
Token length: 1473
Summarized using GPT-3.5-turbo
Append: [StoryWriter: A Multi-Agent Framework for Long Story Generation](https://arxiv.org/abs/2506.16445)
Token length: 1204
Summarized using GPT-3.5-turbo
Append: [Towards Generalizable Generic Harmful Speech Datasets for Implicit Hate Speech Detection](https://arxiv.org/abs/2506.16476)
Token length: 1390
Summarized using GPT-3.5-turbo
Append: [Relic: Enhancing Reward Model Generalization for Low-Resource Indic Languages with Few-Shot Examples](https://arxiv.org/abs/2506.16502)
Token length: 1028
Summarized using GPT-3.5-turbo
Append: [Automatic Speech Recognition Biases in Newcastle English: an Error Analysis](https://arxiv.org/abs/2506.16558)
Token length: 1019
Summarized using GPT-3.5-turbo
Append: [Weight Factorization and Centralization for Continual Learning in Speech Recognition](https://arxiv.org/abs/2506.16574)
Token length: 665
Summarized using GPT-3.5-turbo
Append: [Streaming Non-Autoregressive Model for Accent Conversion and Pronunciation Improvement](https://arxiv.org/abs/2506.16580)
Token length: 1682
Summarized using GPT-3.5-turbo
Append: [Measuring (a Sufficient) World Model in LLMs: A Variance Decomposition Framework](https://arxiv.org/abs/2506.16584)
Token length: 1241
Summarized using GPT-3.5-turbo
Append: [A Scoping Review of Synthetic Data Generation for Biomedical Research and Applications](https://arxiv.org/abs/2506.16594)
Token length: 1934
Summarized using GPT-3.5-turbo
Append: [Modeling Public Perceptions of Science in Media](https://arxiv.org/abs/2506.16622)
Token length: 1230
Summarized using GPT-3.5-turbo
Append: [Initial Investigation of LLM-Assisted Development of Rule-Based Clinical NLP System](https://arxiv.org/abs/2506.16628)
Append: [GeoGuess: Multimodal Reasoning based on Hierarchy of Visual Information in Street View](https://arxiv.org/abs/2506.16633)
Append: [Long-Context Generalization with Sparse Attention](https://arxiv.org/abs/2506.16640)
Append: [Arch-Router: Aligning LLM Routing with Human Preferences](https://arxiv.org/abs/2506.16655)
Append: [Mechanisms vs. Outcomes: Probing for Syntax Fails to Explain Performance on Targeted Syntactic Evaluations](https://arxiv.org/abs/2506.16678)
Append: [LegiGPT: Party Politics and Transport Policy with Large Language Model](https://arxiv.org/abs/2506.16692)
Append: [ReasonGRM: Enhancing Generative Reward Models through Large Reasoning Models](https://arxiv.org/abs/2506.16712)
Append: [The Role of Model Confidence on Bias Effects in Measured Uncertainties](https://arxiv.org/abs/2506.16724)
Append: [LM-SPT: LM-Aligned Semantic Distillation for Speech Tokenization](https://arxiv.org/abs/2506.16738)
Append: [Language-Informed Synthesis of Rational Agent Models for Grounded Theory-of-Mind Reasoning On-The-Fly](https://arxiv.org/abs/2506.16755)
Append: [SocialSim: Towards Socialized Simulation of Emotional Support Conversation](https://arxiv.org/abs/2506.16756)
Append: [Cross-Modal Obfuscation for Jailbreak Attacks on Large Vision-Language Models](https://arxiv.org/abs/2506.16760)
Append: [DistillNote: LLM-based clinical note summaries improve heart failure diagnosis](https://arxiv.org/abs/2506.16777)
Append: [MIST: Jailbreaking Black-box Large Language Models via Iterative Semantic Tuning](https://arxiv.org/abs/2506.16792)
Append: [From Data to Knowledge: Evaluating How Efficiently Language Models Learn Facts](https://arxiv.org/abs/2506.16912)
Append: [Language Bottleneck Models: A Framework for Interpretable Knowledge Tracing and Beyond](https://arxiv.org/abs/2506.16982)
Append: [TeXpert: A Multi-Level Benchmark for Evaluating LaTeX Code Generation by LLMs](https://arxiv.org/abs/2506.16990)
Append: [PersonalAI: Towards digital twins in the graph form](https://arxiv.org/abs/2506.17001)
Append: [LLM-Generated Feedback Supports Learning If Learners Choose to Use It](https://arxiv.org/abs/2506.17006)
Append: [Instituto de Telecomunica\c{c}\~oes at IWSLT 2025: Aligning Small-Scale Speech and Language Models for Speech-to-Text Learning](https://arxiv.org/abs/2506.17019)
Append: [MUCAR: Benchmarking Multilingual Cross-Modal Ambiguity Resolution for Multimodal Large Language Models](https://arxiv.org/abs/2506.17046)
Append: [Simultaneous Translation with Offline Speech and LLM Models in CUNI Submission to IWSLT 2025](https://arxiv.org/abs/2506.17077)
Append: [Tower+: Bridging Generality and Translation Specialization in Multilingual LLMs](https://arxiv.org/abs/2506.17080)
Append: [Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language Models: An Empirical Evaluation](https://arxiv.org/abs/2506.17088)
Append: [Better Language Model Inversion by Compactly Representing Next-Token Distributions](https://arxiv.org/abs/2506.17090)
Append: [Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context LMs?](https://arxiv.org/abs/2506.17121)
Append: [CLEAR-3K: Assessing Causal Explanatory Capabilities in Language Models](https://arxiv.org/abs/2506.17180)
Append: [Towards AI Search Paradigm](https://arxiv.org/abs/2506.17188)
Append: [Fine-Tuning Lowers Safety and Disrupts Evaluation Consistency](https://arxiv.org/abs/2506.17209)
Append: [cAST: Enhancing Code Retrieval-Augmented Generation with Structural Chunking via Abstract Syntax Tree](https://arxiv.org/abs/2506.15655)
Append: [BASE-Q: Bias and Asymmetric Scaling Enhanced Rotational Quantization for Large Language Models](https://arxiv.org/abs/2506.15689)
Append: [DeepRTL2: A Versatile Model for RTL-Related Tasks](https://arxiv.org/abs/2506.15697)
Append: [Learn from the Past: Fast Sparse Indexing for Large Language Model Decoding](https://arxiv.org/abs/2506.15704)
Append: [Adaptive Two Sided Laplace Transforms: A Learnable, Interpretable, and Scalable Replacement for Self-Attention](https://arxiv.org/abs/2506.15714)
Append: [daDPO: Distribution-Aware DPO for Distilling Conversational Abilities](https://arxiv.org/abs/2506.15717)
Append: [MadaKV: Adaptive Modality-Perception KV Cache Eviction for Efficient Multimodal Long-Context Inference](https://arxiv.org/abs/2506.15724)
Append: [OAgents: An Empirical Study of Building Effective Agents](https://arxiv.org/abs/2506.15741)
Append: [SLR: An Automated Synthesis Framework for Scalable Logical Reasoning](https://arxiv.org/abs/2506.15787)
Append: [MoR: Better Handling Diverse Queries with a Mixture of Sparse, Dense, and Human Retrievers](https://arxiv.org/abs/2506.15862)
Append: [Fractional Reasoning via Latent Steering Vectors Improves Inference Time Compute](https://arxiv.org/abs/2506.15882)
Append: [Early Attentive Sparsification Accelerates Neural Speech Transcription](https://arxiv.org/abs/2506.15912)
Append: [Exploring Big Five Personality and AI Capability Effects in LLM-Simulated Negotiation Dialogues](https://arxiv.org/abs/2506.15928)
Append: [Multi-use LLM Watermarking and the False Detection Problem](https://arxiv.org/abs/2506.15975)
Append: [Bayesian Epistemology with Weighted Authority: A Formal Architecture for Truth-Promoting Autonomous Scientific Reasoning](https://arxiv.org/abs/2506.16015)
Append: [Probing the Robustness of Large Language Models Safety to Latent Perturbations](https://arxiv.org/abs/2506.16078)
Append: [GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal Reasoning](https://arxiv.org/abs/2506.16141)
Append: [IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks](https://arxiv.org/abs/2506.16402)
Append: [Unpacking Generative AI in Education: Computational Modeling of Teacher and Student Perspectives in Social Media Discourse](https://arxiv.org/abs/2506.16412)
Append: [Probe before You Talk: Towards Black-box Defense against Backdoor Unalignment for Large Language Models](https://arxiv.org/abs/2506.16447)
Append: [Do We Talk to Robots Like Therapists, and Do They Respond Accordingly? Language Alignment in AI Emotional Support](https://arxiv.org/abs/2506.16473)
Append: [Revela: Dense Retriever Learning via Language Modeling](https://arxiv.org/abs/2506.16552)
Append: [Advancing Harmful Content Detection in Organizational Research: Integrating Large Language Models with Elo Rating System](https://arxiv.org/abs/2506.16575)
Append: [From Prompts to Constructs: A Dual-Validity Framework for LLM Research in Psychology](https://arxiv.org/abs/2506.16697)
Append: [Large Language Models as Psychological Simulators: A Methodological Guide](https://arxiv.org/abs/2506.16702)
Append: [Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs](https://arxiv.org/abs/2506.16962)
Append: [Latent Concept Disentanglement in Transformer-based Language Models](https://arxiv.org/abs/2506.16975)
Append: [From Concepts to Components: Concept-Agnostic Attention Module Discovery in Transformers](https://arxiv.org/abs/2506.17052)
Append: [Are Bias Evaluation Methods Biased ?](https://arxiv.org/abs/2506.17111)
Append: [MEXA: Towards General Multimodal Reasoning with Dynamic Multi-Expert Aggregation](https://arxiv.org/abs/2506.17113)
Append: [Dissecting the SWE-Bench Leaderboards: Profiling Submitters and Architectures of LLM- and Agent-Based Repair Systems](https://arxiv.org/abs/2506.17208)
Append: [Voices of Her: Analyzing Gender Differences in the AI Publication World](https://arxiv.org/abs/2305.14597)
Append: [LaRS: Latent Reasoning Skills for Chain-of-Thought Reasoning](https://arxiv.org/abs/2312.04684)
Append: [AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential Reasoning Ability](https://arxiv.org/abs/2402.09404)
Append: [A Survey of Automatic Hallucination Evaluation on Natural Language Generation](https://arxiv.org/abs/2404.12041)
Append: [BEADs: Bias Evaluation Across Domains](https://arxiv.org/abs/2406.04220)
Append: [Learning to Refine with Fine-Grained Natural Language Feedback](https://arxiv.org/abs/2407.02397)
Append: [sPhinX: Sample Efficient Multilingual Instruction Fine-Tuning Through N-shot Guided Prompting](https://arxiv.org/abs/2407.09879)
Append: [Contextual modulation of language comprehension in a dynamic neural model of lexical meaning](https://arxiv.org/abs/2407.14701)
Append: [Deep Learning based Visually Rich Document Content Understanding: A Survey](https://arxiv.org/abs/2408.01287)
Append: [Re-TASK: Revisiting LLM Tasks from Capability, Skill, and Knowledge Perspectives](https://arxiv.org/abs/2408.06904)
Append: [Can Large Language Models Replace Human Subjects? A Large-Scale Replication of Scenario-Based Experiments in Psychology and Management](https://arxiv.org/abs/2409.00128)
Append: [Core Knowledge Deficits in Multi-Modal Language Models](https://arxiv.org/abs/2410.10855)
Append: [SHAKTI: A 2.5 Billion Parameter Small Language Model Optimized for Edge AI and Low-Resource Environments](https://arxiv.org/abs/2410.11331)
Append: [Learning to Route LLMs with Confidence Tokens](https://arxiv.org/abs/2410.13284)
Append: [Principles of semantic and functional efficiency in grammatical patterning](https://arxiv.org/abs/2410.15865)
Append: [Layer-wise Alignment: Examining Safety Alignment Across Image Encoder Layers in Vision Language Models](https://arxiv.org/abs/2411.04291)
Append: [Song Form-aware Full-Song Text-to-Lyrics Generation with Multi-Level Granularity Syllable Count Control](https://arxiv.org/abs/2411.13100)
Append: [Incivility and Rigidity: The Risks of Fine-Tuning LLMs for Political Argumentation](https://arxiv.org/abs/2411.16813)
Append: [On Domain-Adaptive Post-Training for Multimodal Large Language Models](https://arxiv.org/abs/2411.19930)
Append: [Think&Cite: Improving Attributed Text Generation with Self-Guided Tree Search and Progress Reward Modeling](https://arxiv.org/abs/2412.14860)
Append: [Theoretical Guarantees for Minimum Bayes Risk Decoding](https://arxiv.org/abs/2502.12685)
Append: [Knapsack Optimization-based Schema Linking for LLM-based Text-to-SQL Generation](https://arxiv.org/abs/2502.12911)
Append: [Group-Level Data Selection for Efficient Pretraining](https://arxiv.org/abs/2502.14709)
Append: [From RAG to Memory: Non-Parametric Continual Learning for Large Language Models](https://arxiv.org/abs/2502.14802)
Append: [Batayan: A Filipino NLP benchmark for evaluating Large Language Models](https://arxiv.org/abs/2502.14911)
Append: [Uncertainty Quantification in Retrieval Augmented Question Answering](https://arxiv.org/abs/2502.18108)
Append: [olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language Models](https://arxiv.org/abs/2502.18443)
Append: [FRIDA to the Rescue! Analyzing Synthetic Data Effectiveness in Object-Based Common Sense Reasoning for Disaster Response](https://arxiv.org/abs/2502.18452)
Append: [Large-Scale Data Selection for Instruction Tuning](https://arxiv.org/abs/2503.01807)
Append: [AlignDistil: Token-Level Language Model Alignment as Adaptive Policy Distillation](https://arxiv.org/abs/2503.02832)
Append: [Coreference as an indicator of context scope in multimodal narrative](https://arxiv.org/abs/2503.05298)
Append: [Dynamic Knowledge Integration for Evidence-Driven Counter-Argument Generation with Large Language Models](https://arxiv.org/abs/2503.05328)
Append: [QG-SMS: Enhancing Test Item Analysis via Student Modeling and Simulation](https://arxiv.org/abs/2503.05888)
Append: [LLMs in Disease Diagnosis: A Comparative Study of DeepSeek-R1 and O3 Mini Across Chronic Health Conditions](https://arxiv.org/abs/2503.10486)
Append: [High-Dimensional Interlingual Representations of Large Language Models](https://arxiv.org/abs/2503.11280)
Append: [Deceptive Humor: A Synthetic Multilingual Benchmark Dataset for Bridging Fabricated Claims with Humorous Content](https://arxiv.org/abs/2503.16031)
Append: [CARE: Assessing the Impact of Multilingual Human Preference Learning on Cultural Awareness](https://arxiv.org/abs/2504.05154)
Append: [TALE: A Tool-Augmented Framework for Reference-Free Evaluation of Large Language Models](https://arxiv.org/abs/2504.07385)
Append: [Reimagining Urban Science: Scaling Causal Inference with Large Language Models](https://arxiv.org/abs/2504.12345)
Append: [Ask, Fail, Repeat: Meeseeks, an Iterative Feedback Benchmark for LLMs' Multi-turn Instruction-Following Ability](https://arxiv.org/abs/2504.21625)
Append: [ReplaceMe: Network Simplification via Depth Pruning and Transformer Block Linearization](https://arxiv.org/abs/2505.02819)
Append: [Learning Dynamics in Continual Pre-Training for Large Language Models](https://arxiv.org/abs/2505.07796)
Append: [Assessing and Mitigating Medical Knowledge Drift and Conflicts in Large Language Models](https://arxiv.org/abs/2505.07968)
Append: [Detecting Prefix Bias in LLM-based Reward Models](https://arxiv.org/abs/2505.13487)
Append: [AUTOLAW: Enhancing Legal Compliance in Large Language Models via Case Law Generation and Jury-Inspired Deliberation](https://arxiv.org/abs/2505.14015)
Append: [SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine Translation](https://arxiv.org/abs/2505.16637)
Append: [Watch and Listen: Understanding Audio-Visual-Speech Moments with Multimodal LLM](https://arxiv.org/abs/2505.18110)
Append: [Voice of a Continent: Mapping Africa's Speech Technology Frontier](https://arxiv.org/abs/2505.18436)
Append: [Calibrating Pre-trained Language Classifiers on LLM-generated Noisy Labels via Iterative Refinement](https://arxiv.org/abs/2505.19675)
Append: [More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models](https://arxiv.org/abs/2505.21523)
Append: [CVC: A Large-Scale Chinese Value Rule Corpus for Value Alignment of Large Language Models](https://arxiv.org/abs/2506.01495)
Append: [GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2506.02404)
Append: [Geopolitical biases in LLMs: what are the "good" and the "bad" countries according to contemporary language models](https://arxiv.org/abs/2506.06751)
Append: [SDE-SQL: Enhancing Text-to-SQL Generation in Large Language Models via Self-Driven Exploration with SQL Probes](https://arxiv.org/abs/2506.07245)
Append: [PlantBert: An Open Source Language Model for Plant Science](https://arxiv.org/abs/2506.08897)
Append: [Techniques for supercharging academic writing with generative AI](https://arxiv.org/abs/2310.17143)
Append: [Alto: Orchestrating Distributed Compound AI Systems with Nested Ancestry](https://arxiv.org/abs/2403.04311)
Append: [PromptDSI: Prompt-based Rehearsal-free Instance-wise Incremental Learning for Document Retrieval](https://arxiv.org/abs/2406.12593)
Append: [Cost-effective Instruction Learning for Pathology Vision and Language Analysis](https://arxiv.org/abs/2407.17734)
Append: [xGen-MM (BLIP-3): A Family of Open Large Multimodal Models](https://arxiv.org/abs/2408.08872)
Append: [MaPPER: Multimodal Prior-guided Parameter Efficient Tuning for Referring Expression Comprehension](https://arxiv.org/abs/2409.13609)
Append: [COS-DPO: Conditioned One-Shot Multi-Objective Fine-Tuning Framework](https://arxiv.org/abs/2410.08316)
Append: [ALTA: Compiler-Based Analysis of Transformers](https://arxiv.org/abs/2410.18077)
Append: [Adapting While Learning: Grounding LLMs for Scientific Problems with Intelligent Tool Usage Adaptation](https://arxiv.org/abs/2411.00412)
Append: [A Implies B: Circuit Analysis in LLMs for Propositional Logical Reasoning](https://arxiv.org/abs/2411.04105)
Append: [Watermarking Language Models through Language Models](https://arxiv.org/abs/2411.05091)
Append: [Cyclic Vision-Language Manipulator: Towards Reliable and Fine-Grained Image Interpretation for Automated Report Generation](https://arxiv.org/abs/2411.05261)
Append: [Quantifying artificial intelligence through algorithmic generalization](https://arxiv.org/abs/2411.05943)
Append: [On the Limits of Language Generation: Trade-Offs Between Hallucination and Mode Collapse](https://arxiv.org/abs/2411.09642)
Append: [Multi-Preference Optimization: Generalizing DPO via Set-Level Contrasts](https://arxiv.org/abs/2412.04628)
Append: [AutoPresent: Designing Structured Visuals from Scratch](https://arxiv.org/abs/2501.00912)
Append: [Ladder-residual: parallelism-aware architecture for accelerating large model inference with communication overlapping](https://arxiv.org/abs/2501.06589)
Append: [Rethinking External Slow-Thinking: From Snowball Errors to Probability of Correct Reasoning](https://arxiv.org/abs/2501.15602)
Append: [On Almost Surely Safe Alignment of Large Language Models at Inference-Time](https://arxiv.org/abs/2502.01208)
Append: [Beyond Self-Talk: A Communication-Centric Survey of LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2502.14321)
Append: [FEA-Bench: A Benchmark for Evaluating Repository-Level Code Generation for Feature Implementation](https://arxiv.org/abs/2503.06680)
Append: [LLM-Guided Indoor Navigation with Multimodal Map Understanding](https://arxiv.org/abs/2503.11702)
Append: [DrunkAgent: Stealthy Memory Corruption in LLM-Powered Recommender Agents](https://arxiv.org/abs/2503.23804)
Append: [Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot Manipulation Datasets](https://arxiv.org/abs/2505.15517)
Append: [SWE-Dev: Evaluating and Training Autonomous Feature-Driven Software Development](https://arxiv.org/abs/2505.16975)
Append: [On Path to Multimodal Historical Reasoning: HistBench and HistAgent](https://arxiv.org/abs/2505.20246)
Append: [RefAV: Towards Planning-Centric Scenario Mining](https://arxiv.org/abs/2505.20981)
Append: [UniWorld-V1: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation](https://arxiv.org/abs/2506.03147)
Append: [CIVET: Systematic Evaluation of Understanding in VLMs](https://arxiv.org/abs/2506.05146)
Append: [Kinetics: Rethinking Test-Time Scaling Laws](https://arxiv.org/abs/2506.05333)
append_entries: 194
Finish: 2025-06-23 04:36:13.305513
------------------------------------------------------
Started: 2025-06-23 06:27:09.102080
Existing_entries: 1194
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1310
Summarized using GPT-3.5-turbo
Append: [LogProber: Disentangling confidence from contamination in LLM responses](https://arxiv.org/abs/2408.14352)
Token length: 1402
Summarized using GPT-3.5-turbo
Append: [Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal Localization of Prolonged Exposure Therapy Elements](https://arxiv.org/abs/2506.09707)
append_entries: 2
Finish: 2025-06-23 06:27:13.239749
------------------------------------------------------
Started: 2025-06-23 08:24:30.708759
Existing_entries: 1002
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-23 08:24:31.130028
------------------------------------------------------
Started: 2025-06-23 10:19:20.968776
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-23 10:19:21.415369
------------------------------------------------------
Started: 2025-06-23 12:36:21.435869
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-23 12:36:21.920319
------------------------------------------------------
Started: 2025-06-23 14:17:39.837059
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-23 14:17:40.268578
------------------------------------------------------
Started: 2025-06-23 16:21:53.679955
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-23 16:21:54.174589
------------------------------------------------------
Started: 2025-06-23 18:24:11.782232
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-23 18:24:12.217305
------------------------------------------------------
Started: 2025-06-23 20:19:13.701665
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-23 20:19:14.167614
------------------------------------------------------
Started: 2025-06-23 22:16:54.704632
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-23 22:16:55.154559
------------------------------------------------------
Started: 2025-06-24 01:21:48.064716
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-24 01:21:48.486947
------------------------------------------------------
Started: 2025-06-24 03:16:27.144360
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-24 03:16:27.576714
------------------------------------------------------
Started: 2025-06-24 04:30:05.418537
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1280
Summarized using GPT-3.5-turbo
Append: [Outcome-Based Education: Evaluating Students' Perspectives Using Transformer](https://arxiv.org/abs/2506.17223)
Token length: 1035
Summarized using GPT-3.5-turbo
Append: [Efficient and Stealthy Jailbreak Attacks via Adversarial Prompt Distillation from LLMs to SLMs](https://arxiv.org/abs/2506.17231)
Token length: 1618
Summarized using GPT-3.5-turbo
Append: [GTA: Grouped-head latenT Attention](https://arxiv.org/abs/2506.17286)
Token length: 931
Summarized using GPT-3.5-turbo
Append: [AI-Generated Game Commentary: A Survey and a Datasheet Repository](https://arxiv.org/abs/2506.17294)
Token length: 1370
Summarized using GPT-3.5-turbo
Append: [Semantic uncertainty in advanced decoding methods for LLM generation](https://arxiv.org/abs/2506.17296)
Token length: 1211
Summarized using GPT-3.5-turbo
Append: [Mercury: Ultra-Fast Language Models Based on Diffusion](https://arxiv.org/abs/2506.17298)
Token length: 1117
Summarized using GPT-3.5-turbo
Append: [PRAISE: Enhancing Product Descriptions with LLM-Driven Structured Insights](https://arxiv.org/abs/2506.17314)
Token length: 1558
Summarized using GPT-3.5-turbo
Append: [Towards Safety Evaluations of Theory of Mind in Large Language Models](https://arxiv.org/abs/2506.17352)
Token length: 1502
Summarized using GPT-3.5-turbo
Append: [Cash or Comfort? How LLMs Value Your Inconvenience](https://arxiv.org/abs/2506.17367)
Token length: 1310
Summarized using GPT-3.5-turbo
Append: [Leveraging LLMs to Assess Tutor Moves in Real-Life Dialogues: A Feasibility Study](https://arxiv.org/abs/2506.17410)
Token length: 1653
Summarized using GPT-3.5-turbo
Append: [UProp: Investigating the Uncertainty Propagation of LLMs in Multi-Step Agentic Decision-Making](https://arxiv.org/abs/2506.17419)
Token length: 1339
Summarized using GPT-3.5-turbo
Append: [Beyond the Link: Assessing LLMs' ability to Classify Political Content across Global Media](https://arxiv.org/abs/2506.17435)
Token length: 1005
Summarized using GPT-3.5-turbo
Append: [Breaking the Transcription Bottleneck: Fine-tuning ASR Models for Extremely Low-Resource Fieldwork Languages](https://arxiv.org/abs/2506.17459)
Token length: 1252
Summarized using GPT-3.5-turbo
Append: [Computational Approaches to Understanding Large Language Model Impact on Writing and Information Ecosystems](https://arxiv.org/abs/2506.17467)
Token length: 970
Summarized using GPT-3.5-turbo
Append: [VeriLocc: End-to-End Cross-Architecture Register Allocation via LLM](https://arxiv.org/abs/2506.17506)
Token length: 1099
Summarized using GPT-3.5-turbo
Append: [Data Quality Issues in Multilingual Speech Datasets: The Need for Sociolinguistic Awareness and Proactive Language Planning](https://arxiv.org/abs/2506.17525)
Token length: 1110
Summarized using GPT-3.5-turbo
Append: [DuaShepherd: Integrating Stepwise Correctness and Potential Rewards for Mathematical Reasoning](https://arxiv.org/abs/2506.17533)
Token length: 1707
Summarized using GPT-3.5-turbo
Append: [Probing for Phonology in Self-Supervised Speech Representations: A Case Study on Accent Perception](https://arxiv.org/abs/2506.17542)
Token length: 1605
Summarized using GPT-3.5-turbo
Append: [AgriCHN: A Comprehensive Cross-domain Resource for Chinese Agricultural Named Entity Recognition](https://arxiv.org/abs/2506.17578)
Token length: 1580
Summarized using GPT-3.5-turbo
Append: [Mind the Gap: Assessing Wiktionary's Crowd-Sourced Linguistic Knowledge on Morphological Gaps in Two Related Languages](https://arxiv.org/abs/2506.17603)
Token length: 1471
Summarized using GPT-3.5-turbo
Append: [TyphoFormer: Language-Augmented Transformer for Accurate Typhoon Track Forecasting](https://arxiv.org/abs/2506.17609)
Token length: 956
Summarized using GPT-3.5-turbo
Append: [OpusLM: A Family of Open Unified Speech Language Models](https://arxiv.org/abs/2506.17611)
Token length: 1129
Summarized using GPT-3.5-turbo
Append: [Answer-Centric or Reasoning-Driven? Uncovering the Latent Memory Anchor in LLMs](https://arxiv.org/abs/2506.17630)
Token length: 1336
Summarized using GPT-3.5-turbo
Append: [Step-Opt: Boosting Optimization Modeling in LLMs through Iterative Data Synthesis and Structured Validation](https://arxiv.org/abs/2506.17637)
Token length: 1277
Summarized using GPT-3.5-turbo
Append: [TPTT: Transforming Pretrained Transformer into Titans](https://arxiv.org/abs/2506.17671)
Token length: 1360
Summarized using GPT-3.5-turbo
Append: [Resource-Friendly Dynamic Enhancement Chain for Multi-Hop Question Answering](https://arxiv.org/abs/2506.17692)
Token length: 1270
Summarized using GPT-3.5-turbo
Append: [Zero-Shot Conversational Stance Detection: Dataset and Approaches](https://arxiv.org/abs/2506.17693)
Token length: 1714
Summarized using GPT-3.5-turbo
Append: [The Evolution of Natural Language Processing: How Prompt Optimization and Language Models are Shaping the Future](https://arxiv.org/abs/2506.17700)
Token length: 836
Summarized using GPT-3.5-turbo
Append: [Aged to Perfection: Machine-Learning Maps of Age in Conversational English](https://arxiv.org/abs/2506.17708)
Token length: 1272
Summarized using GPT-3.5-turbo
Append: [Unveiling Factors for Enhanced POS Tagging: A Study of Low-Resource Medieval Romance Languages](https://arxiv.org/abs/2506.17715)
Token length: 1959
Summarized using GPT-3.5-turbo
Append: [KAG-Thinker: Teaching Large Language Models to Think with Human-like Reasoning Process](https://arxiv.org/abs/2506.17728)
Token length: 1909
Summarized using GPT-3.5-turbo
Append: [HIDE and Seek: Detecting Hallucinations in Language Models via Decoupled Representations](https://arxiv.org/abs/2506.17748)
Token length: 919
Summarized using GPT-3.5-turbo
Append: [Multilingual Tokenization through the Lens of Indian Languages: Challenges and Insights](https://arxiv.org/abs/2506.17789)
Token length: 1230
Summarized using GPT-3.5-turbo
Append: [THCM-CAL: Temporal-Hierarchical Causal Modelling with Conformal Calibration for Clinical Risk Prediction](https://arxiv.org/abs/2506.17844)
Token length: 1801
Summarized using GPT-3.5-turbo
Append: [LLMs for Customized Marketing Content Generation and Evaluation at Scale](https://arxiv.org/abs/2506.17863)
Token length: 1591
Summarized using GPT-3.5-turbo
Append: [QueueEDIT: Structural Self-Correction for Sequential Model Editing in LLMs](https://arxiv.org/abs/2506.17864)
Token length: 1894
Summarized using GPT-3.5-turbo
Append: [How Alignment Shrinks the Generative Horizon](https://arxiv.org/abs/2506.17871)
Token length: 1276
Summarized using GPT-3.5-turbo
Append: [Multi-turn Jailbreaking via Global Refinement and Active Fabrication](https://arxiv.org/abs/2506.17881)
Token length: 1201
Summarized using GPT-3.5-turbo
Append: [Scatter-Based Innovation Propagation in Large Language Models for Multi-Stage Process Adaptation](https://arxiv.org/abs/2506.17949)
Token length: 921
Summarized using GPT-3.5-turbo
Append: [A Comprehensive Graph Framework for Question Answering with Mode-Seeking Preference Alignment](https://arxiv.org/abs/2506.17951)
Token length: 1202
Summarized using GPT-3.5-turbo
Append: [PDF Retrieval Augmented Question Answering](https://arxiv.org/abs/2506.18027)
Token length: 1419
Summarized using GPT-3.5-turbo
Append: [Splitformer: An improved early-exit architecture for automatic speech recognition on edge devices](https://arxiv.org/abs/2506.18035)
Token length: 1071
Summarized using GPT-3.5-turbo
Append: [Markov-Enhanced Clustering for Long Document Summarization: Tackling the 'Lost in the Middle' Challenge with Large Language Models](https://arxiv.org/abs/2506.18036)
Token length: 1296
Summarized using GPT-3.5-turbo
Append: [Statistical Multicriteria Evaluation of LLM-Generated Text](https://arxiv.org/abs/2506.18082)
Token length: 1119
Summarized using GPT-3.5-turbo
Append: [Evaluating Prompt-Based and Fine-Tuned Approaches to Czech Anaphora Resolution](https://arxiv.org/abs/2506.18091)
Token length: 1629
Summarized using GPT-3.5-turbo
Append: [InspireDebate: Multi-Dimensional Subjective-Objective Evaluation-Guided Reasoning and Optimization for Debating](https://arxiv.org/abs/2506.18102)
Token length: 1346
Summarized using GPT-3.5-turbo
Append: [Chengyu-Bench: Benchmarking Large Language Models for Chinese Idiom Understanding and Use](https://arxiv.org/abs/2506.18105)
Token length: 1380
Summarized using GPT-3.5-turbo
Append: [Mental Health Equity in LLMs: Leveraging Multi-Hop Question Answering to Detect Amplified and Silenced Perspectives](https://arxiv.org/abs/2506.18116)
Token length: 1332
Summarized using GPT-3.5-turbo
Append: [The Syntactic Acceptability Dataset (Preview): A Resource for Machine Learning and Linguistic Analysis of English](https://arxiv.org/abs/2506.18120)
Token length: 1298
Summarized using GPT-3.5-turbo
Append: [$\phi^{\infty}$: Clause Purification, Embedding Realignment, and the Total Suppression of the Em Dash in Autoregressive Language Models](https://arxiv.org/abs/2506.18129)
Append: [Sparse Feature Coactivation Reveals Composable Semantic Modules in Large Language Models](https://arxiv.org/abs/2506.18141)
Append: [QuranMorph: Morphologically Annotated Quranic Corpus](https://arxiv.org/abs/2506.18148)
Append: [CareLab at #SMM4H-HeaRD 2025: Insomnia Detection and Food Safety Event Extraction with Domain-Aware Transformers](https://arxiv.org/abs/2506.18185)
Append: [Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review](https://arxiv.org/abs/2506.18199)
Append: [Deciphering Emotions in Children Storybooks: A Comparative Analysis of Multimodal LLMs in Educational Applications](https://arxiv.org/abs/2506.18201)
Append: [Enhancing Entity Aware Machine Translation with Multi-task Learning](https://arxiv.org/abs/2506.18318)
Append: [TranslationCorrect: A Unified Framework for Machine Translation Post-Editing with Predictive Error Assistance](https://arxiv.org/abs/2506.18337)
Append: [Less Data Less Tokens: Multilingual Unification Learning for Efficient Test-Time Reasoning in LLMs](https://arxiv.org/abs/2506.18341)
Append: [Evaluating Causal Explanation in Medical Reports with LLM-Based and Human-Aligned Metrics](https://arxiv.org/abs/2506.18387)
Append: [Lemmatization as a Classification Task: Results from Arabic across Multiple Genres](https://arxiv.org/abs/2506.18399)
Append: [TReB: A Comprehensive Benchmark for Evaluating Table Reasoning Capabilities of Large Language Models](https://arxiv.org/abs/2506.18421)
Append: [MeRF: Motivation-enhanced Reinforcement Finetuning for Large Reasoning Models](https://arxiv.org/abs/2506.18485)
Append: [Comparative Evaluation of ChatGPT and DeepSeek Across Key NLP Tasks: Strengths, Weaknesses, and Domain-Specific Performance](https://arxiv.org/abs/2506.18501)
Append: [End-to-End Spoken Grammatical Error Correction](https://arxiv.org/abs/2506.18532)
Append: [When Fine-Tuning Fails: Lessons from MS MARCO Passage Ranking](https://arxiv.org/abs/2506.18535)
Append: [A Modular Taxonomy for Hate Speech Definitions and Its Impact on Zero-Shot LLM Classification Performance](https://arxiv.org/abs/2506.18576)
Append: [Parallel Continuous Chain-of-Thought with Jacobi Iteration](https://arxiv.org/abs/2506.18582)
Append: [Reply to "Emergent LLM behaviors are observationally equivalent to data leakage"](https://arxiv.org/abs/2506.18600)
Append: [Semantic similarity estimation for domain specific data using BERT and other techniques](https://arxiv.org/abs/2506.18602)
Append: [The Anatomy of Speech Persuasion: Linguistic Shifts in LLM-Modified Speeches](https://arxiv.org/abs/2506.18621)
Append: [ByteSpan: Information-Driven Subword Tokenisation](https://arxiv.org/abs/2506.18639)
Append: [Is There a Case for Conversation Optimized Tokenizers in Large Language Models?](https://arxiv.org/abs/2506.18674)
Append: [Context Biasing for Pronunciations-Orthography Mismatch in Automatic Speech Recognition](https://arxiv.org/abs/2506.18703)
Append: [Benchmarking the Pedagogical Knowledge of Large Language Models](https://arxiv.org/abs/2506.18710)
Append: [Semantic-Preserving Adversarial Attacks on LLMs: An Adaptive Greedy Binary Search Approach](https://arxiv.org/abs/2506.18756)
Append: [ASP2LJ : An Adversarial Self-Play Laywer Augmented Legal Judgment Framework](https://arxiv.org/abs/2506.18768)
Append: [Existing LLMs Are Not Self-Consistent For Simple Tasks](https://arxiv.org/abs/2506.18781)
Append: [RWESummary: A Framework and Test for Choosing Large Language Models to Summarize Real-World Evidence (RWE) Studies](https://arxiv.org/abs/2506.18819)
Append: [MLLP-VRAIN UPV system for the IWSLT 2025 Simultaneous Speech Translation Translation task](https://arxiv.org/abs/2506.18828)
Append: [STU-PID: Steering Token Usage via PID Controller for Efficient Large Language Model Reasoning](https://arxiv.org/abs/2506.18831)
Append: [LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning](https://arxiv.org/abs/2506.18841)
Append: [Mechanistic Interpretability Needs Philosophy](https://arxiv.org/abs/2506.18852)
Append: [CommVQ: Commutative Vector Quantization for KV Cache Compression](https://arxiv.org/abs/2506.18879)
Append: [OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory, Compositional, and Transformative Generalization](https://arxiv.org/abs/2506.18880)
Append: [ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs](https://arxiv.org/abs/2506.18896)
Append: [SlimRAG: Retrieval without Graphs via Entity-Aware Context Selection](https://arxiv.org/abs/2506.17288)
Append: [PaceLLM: Brain-Inspired Large Language Models for Long-Context Understanding](https://arxiv.org/abs/2506.17310)
Append: [Beyond Prediction -- Structuring Epistemic Integrity in Artificial Reasoning Systems](https://arxiv.org/abs/2506.17331)
Append: [Zero-Shot Cognitive Impairment Detection from Speech Using AudioLLM](https://arxiv.org/abs/2506.17351)
Append: [LLM-driven Medical Report Generation via Communication-efficient Heterogeneous Federated Learning](https://arxiv.org/abs/2506.17562)
Append: [Cite Pretrain: Retrieval-Free Knowledge Attribution for Large Language Models](https://arxiv.org/abs/2506.17585)
Append: [CLiViS: Unleashing Cognitive Map through Linguistic-Visual Synergy for Embodied Visual Reasoning](https://arxiv.org/abs/2506.17629)
Append: [FaithfulSAE: Towards Capturing Faithful Features with Sparse Autoencoders without External Dataset Dependencies](https://arxiv.org/abs/2506.17673)
Append: [Enhancing Few-shot Keyword Spotting Performance through Pre-Trained Self-supervised Speech Models](https://arxiv.org/abs/2506.17686)
Append: [Beyond instruction-conditioning, MoTE: Mixture of Task Experts for Multi-task Embedding Models](https://arxiv.org/abs/2506.17781)
Append: [Bayesian Social Deduction with Graph-Informed Language Models](https://arxiv.org/abs/2506.17788)
Append: [Aligning Frozen LLMs by Reinforcement Learning: An Iterative Reweight-then-Optimize Approach](https://arxiv.org/abs/2506.17828)
Append: [Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective](https://arxiv.org/abs/2506.17930)
Append: [Tutorial: $\varphi$-Transductions in OpenFst via the Gallic Semiring](https://arxiv.org/abs/2506.17942)
Append: [PP-DocBee2: Improved Baselines with Efficient Data for Multimodal Document Understanding](https://arxiv.org/abs/2506.18023)
Append: [The Democratic Paradox in Large Language Models' Underestimation of Press Freedom](https://arxiv.org/abs/2506.18045)
Append: [RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation](https://arxiv.org/abs/2506.18088)
Append: [SE-Merging: A Self-Enhanced Approach for Dynamic Model Merging](https://arxiv.org/abs/2506.18135)
Append: [Reasoning about Uncertainty: Do Reasoning Models Know When They Don't Know?](https://arxiv.org/abs/2506.18183)
Append: [Shrinking the Generation-Verification Gap with Weak Verifiers](https://arxiv.org/abs/2506.18203)
Append: [AdapThink: Adaptive Thinking Preferences for Reasoning Language Model](https://arxiv.org/abs/2506.18237)
Append: [RLPR: Extrapolating RLVR to General Domains without Verifiers](https://arxiv.org/abs/2506.18254)
Append: [Enhancing Document Retrieval in COVID-19 Research: Leveraging Large Language Models for Hidden Relation Extraction](https://arxiv.org/abs/2506.18311)
Append: [Team LA at SCIDOCA shared task 2025: Citation Discovery via relation-based zero-shot retrieval](https://arxiv.org/abs/2506.18316)
Append: [Confucius3-Math: A Lightweight High-Performance Reasoning LLM for Chinese K-12 Mathematics Learning](https://arxiv.org/abs/2506.18330)
Append: [SlimMoE: Structured Compression of Large MoE Models via Expert Slimming and Distillation](https://arxiv.org/abs/2506.18349)
Append: [AI-Generated Song Detection via Lyrics Transcripts](https://arxiv.org/abs/2506.18488)
Append: [Smooth Operators: LLMs Translating Imperfect Hints into Disfluency-Rich Transcripts](https://arxiv.org/abs/2506.18510)
Append: [Airalogy: AI-empowered universal data digitization for research automation](https://arxiv.org/abs/2506.18586)
Append: [No Training Wheels: Steering Vectors for Bias Correction at Inference Time](https://arxiv.org/abs/2506.18598)
Append: [AggTruth: Contextual Hallucination Detection using Aggregated Attention Scores in LLMs](https://arxiv.org/abs/2506.18628)
Append: [ReDit: Reward Dithering for Improved LLM Policy Optimization](https://arxiv.org/abs/2506.18631)
Append: [Multi-modal Anchor Gated Transformer with Knowledge Distillation for Emotion Recognition in Conversation](https://arxiv.org/abs/2506.18716)
Append: [Neural Total Variation Distance Estimators for Changepoint Detection in News Data](https://arxiv.org/abs/2506.18764)
Append: [Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions During Code Training](https://arxiv.org/abs/2506.18777)
Append: [ConciseHint: Boosting Efficient Reasoning via Continuous Concise Hints during Generation](https://arxiv.org/abs/2506.18810)
Append: [USAD: Universal Speech and Audio Representation via Distillation](https://arxiv.org/abs/2506.18843)
Append: [OmniGen2: Exploration to Advanced Multimodal Generation](https://arxiv.org/abs/2506.18871)
Append: [Vision as a Dialect: Unifying Visual Understanding and Generation via Text-Aligned Representations](https://arxiv.org/abs/2506.18898)
Append: [jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval](https://arxiv.org/abs/2506.18902)
Append: [A Survey on Data Selection for LLM Instruction Tuning](https://arxiv.org/abs/2402.05123)
Append: [Alignment Helps Make the Most of Multimodal Data](https://arxiv.org/abs/2405.08454)
Append: [A Closer Look into Mixture-of-Experts in Large Language Models](https://arxiv.org/abs/2406.18219)
Append: [Anthropocentric bias in language model evaluation](https://arxiv.org/abs/2407.03859)
Append: ["I understand why I got this grade": Automatic Short Answer Grading with Feedback](https://arxiv.org/abs/2407.12818)
Append: [UniMoT: Unified Molecule-Text Language Model with Discrete Token Representation](https://arxiv.org/abs/2408.00863)
Append: [Reasoning Circuits in Language Models: A Mechanistic Interpretation of Syllogistic Inference](https://arxiv.org/abs/2408.08590)
Append: [Step-by-Step Unmasking for Parameter-Efficient Fine-tuning of Large Language Models](https://arxiv.org/abs/2408.14470)
Append: [Large Language Models for Disease Diagnosis: A Scoping Review](https://arxiv.org/abs/2409.00097)
Append: [Multilingual Retrieval Augmented Generation for Culturally-Sensitive Tasks: A Benchmark for Cross-lingual Robustness](https://arxiv.org/abs/2410.01171)
Append: [Self-Preference Bias in LLM-as-a-Judge](https://arxiv.org/abs/2410.21819)
Append: [Systematic Reward Gap Optimization for Mitigating VLM Hallucinations](https://arxiv.org/abs/2411.17265)
Append: [FinGPT: Enhancing Sentiment-Based Stock Movement Prediction with Dissemination-Aware and Context-Enriched LLMs](https://arxiv.org/abs/2412.10823)
Append: [DSGram: Dynamic Weighting Sub-Metrics for Grammatical Error Correction in the Era of Large Language Models](https://arxiv.org/abs/2412.12832)
Append: [LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Inconsistencies](https://arxiv.org/abs/2412.15035)
Append: [GeAR: Graph-enhanced Agent for Retrieval-augmented Generation](https://arxiv.org/abs/2412.18431)
Append: [Efficient Multi-Task Inferencing with a Shared Backbone and Lightweight Task-Specific Adapters for Automatic Scoring](https://arxiv.org/abs/2412.21065)
Append: [SEAL: Scaling to Emphasize Attention for Long-Context Retrieval](https://arxiv.org/abs/2501.15225)
Append: [ASCenD-BDS: Adaptable, Stochastic and Context-aware framework for Detection of Bias, Discrimination and Stereotyping](https://arxiv.org/abs/2502.02072)
Append: [Compromising Honesty and Harmlessness in Language Models via Deception Attacks](https://arxiv.org/abs/2502.08301)
Append: [Stop Overvaluing Multi-Agent Debate -- We Must Rethink Evaluation and Embrace Model Heterogeneity](https://arxiv.org/abs/2502.08788)
Append: [Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the Limits of Embedding Space Capacity](https://arxiv.org/abs/2502.13063)
Append: [Craw4LLM: Efficient Web Crawling for LLM Pretraining](https://arxiv.org/abs/2502.13347)
Append: [HiddenDetect: Detecting Jailbreak Attacks against Large Vision-Language Models via Monitoring Hidden States](https://arxiv.org/abs/2502.14744)
Append: [ParamMute: Suppressing Knowledge-Critical FFNs for Faithful Retrieval-Augmented Generation](https://arxiv.org/abs/2502.15543)
Append: [Language Models Grow Less Humanlike beyond Phase Transition](https://arxiv.org/abs/2502.18802)
Append: [RAPID: Long-Context Inference with Retrieval-Augmented Speculative Decoding](https://arxiv.org/abs/2502.20330)
Append: [Enhancing LLM Knowledge Learning through Generalization](https://arxiv.org/abs/2503.03705)
Append: [HiRAG: Retrieval-Augmented Generation with Hierarchical Knowledge](https://arxiv.org/abs/2503.10150)
Append: [A Dual-Directional Context-Aware Test-Time Learning for Text Classification](https://arxiv.org/abs/2503.15469)
Append: [Learning from Reference Answers: Versatile Language Model Alignment without Binary Human Preference Data](https://arxiv.org/abs/2504.09895)
Append: [A Survey on Large Language Model based Human-Agent Systems](https://arxiv.org/abs/2505.00753)
Append: [Proper Noun Diacritization for Arabic Wikipedia: A Benchmark Dataset](https://arxiv.org/abs/2505.02656)
Append: [TrumorGPT: Graph-Based Retrieval-Augmented Large Language Model for Fact-Checking](https://arxiv.org/abs/2505.07891)
Append: [Prototypical Human-AI Collaboration Behaviors from LLM-Assisted Writing in the Wild](https://arxiv.org/abs/2505.16023)
Append: [EMULATE: A Multi-Agent Framework for Determining the Veracity of Atomic Claims by Emulating Human Actions](https://arxiv.org/abs/2505.16576)
Append: [When can isotropy help adapt LLMs' next word prediction to numerical domains?](https://arxiv.org/abs/2505.17135)
Append: [SIPDO: Closed-Loop Prompt Optimization via Synthetic Data Feedback](https://arxiv.org/abs/2505.19514)
Append: [Pretraining Language Models to Ponder in Continuous Space](https://arxiv.org/abs/2505.20674)
Append: [Pearl: A Multimodal Culturally-Aware Arabic Instruction Dataset](https://arxiv.org/abs/2505.21979)
Append: [Eye of Judgement: Dissecting the Evaluation of Russian-speaking LLMs with POLLUX](https://arxiv.org/abs/2505.24616)
Append: [Dual Debiasing for Noisy In-Context Learning for Text Generation](https://arxiv.org/abs/2506.00418)
Append: [ExpertLongBench: Benchmarking Language Models on Expert-Level Long-Form Generation Tasks with Structured Checklists](https://arxiv.org/abs/2506.01241)
Append: [SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware Reinforcement Learning](https://arxiv.org/abs/2506.01713)
Append: [NovelHopQA: Diagnosing Multi-Hop Reasoning Failures in Long Narrative Contexts](https://arxiv.org/abs/2506.02000)
Append: [Improving the Efficiency of Long Document Classification using Sentence Ranking Approach](https://arxiv.org/abs/2506.07248)
Append: [LGAI-EMBEDDING-Preview Technical Report](https://arxiv.org/abs/2506.07438)
Append: [Piloting Copilot, Codex, and StarCoder2: Hot Temperature, Cold Prompts, or Black Magic?](https://arxiv.org/abs/2210.14699)
Append: [When Large Language Models Meet Vector Databases: A Survey](https://arxiv.org/abs/2402.01763)
Append: [$L^*LM$: Learning Automata from Examples using Natural Language Oracles](https://arxiv.org/abs/2402.07051)
Append: [Evaluating LLMs with Multiple Problems at once](https://arxiv.org/abs/2406.10786)
Append: [Handling Numeric Expressions in Automatic Speech Recognition](https://arxiv.org/abs/2408.00004)
Append: [Sycophancy in Vision-Language Models: A Systematic Analysis and an Inference-Time Mitigation Framework](https://arxiv.org/abs/2408.11261)
Append: [RePST: Language Model Empowered Spatio-Temporal Forecasting via Semantic-Oriented Reprogramming](https://arxiv.org/abs/2408.14505)
Append: [Benchmarking and Building Zero-Shot Hindi Retrieval Model with Hindi-BEIR and NLLB-E5](https://arxiv.org/abs/2409.05401)
Append: [Circuit Compositions: Exploring Modular Structures in Transformer-Based Language Models](https://arxiv.org/abs/2410.01434)
Append: [FutureFill: Fast Generation from Convolutional Sequence Models](https://arxiv.org/abs/2410.03766)
Append: [How Numerical Precision Affects Arithmetical Reasoning Capabilities of LLMs](https://arxiv.org/abs/2410.13857)
Append: [LoRA vs Full Fine-tuning: An Illusion of Equivalence](https://arxiv.org/abs/2410.21228)
Append: [AlzheimerRAG: Multimodal Retrieval Augmented Generation for Clinical Use Cases using PubMed articles](https://arxiv.org/abs/2412.16701)
Append: [Exploring the Potential of Encoder-free Architectures in 3D LMMs](https://arxiv.org/abs/2502.09620)
Append: [PlanGenLLMs: A Modern Survey of LLM Planning Capabilities](https://arxiv.org/abs/2502.11221)
Append: [Steering LLMs for Formal Theorem Proving](https://arxiv.org/abs/2502.15507)
Append: [Directional Gradient Projection for Robust Fine-Tuning of Foundation Models](https://arxiv.org/abs/2502.15895)
Append: [DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM Post-training](https://arxiv.org/abs/2504.09710)
Append: [Learning to Reason under Off-Policy Guidance](https://arxiv.org/abs/2504.14945)
Append: [LightRetriever: A LLM-based Hybrid Retrieval Architecture with 1000x Faster Query Inference](https://arxiv.org/abs/2505.12260)
Append: [ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training](https://arxiv.org/abs/2505.17331)
Append: [Cross from Left to Right Brain: Adaptive Text Dreamer for Vision-and-Language Navigation](https://arxiv.org/abs/2505.20897)
Append: [Position is Power: System Prompts as a Mechanism of Bias in Large Language Models (LLMs)](https://arxiv.org/abs/2505.21091)
Append: [FRAMES-VQA: Benchmarking Fine-Tuning Robustness across Multi-Modal Shifts in Visual Question Answering](https://arxiv.org/abs/2505.21755)
Append: [Infi-MMR: Curriculum-based Unlocking Multimodal Reasoning via Phased Reinforcement Learning in Multimodal Small Language Models](https://arxiv.org/abs/2505.23091)
Append: [Comba: Improving Bilinear RNNs with Closed-loop Control](https://arxiv.org/abs/2506.02475)
Append: [Cross-Entropy Games for Language Models: From Implicit Knowledge to General Capability Measures](https://arxiv.org/abs/2506.06832)
Append: [Reinforcement Learning Teachers of Test Time Scaling](https://arxiv.org/abs/2506.08388)
Append: [Effective Red-Teaming of Policy-Adherent Agents](https://arxiv.org/abs/2506.09600)
append_entries: 201
Finish: 2025-06-24 04:31:38.204555
------------------------------------------------------
Started: 2025-06-24 06:26:54.393631
Existing_entries: 1201
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1706
Summarized using GPT-3.5-turbo
Append: [Deep Binding of Language Model Virtual Personas: a Study on Approximating Political Partisan Misperceptions](https://arxiv.org/abs/2504.11673)
append_entries: 1
Finish: 2025-06-24 06:26:58.125640
------------------------------------------------------
Started: 2025-06-24 08:24:08.846061
Existing_entries: 1001
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1968
Summarized using GPT-3.5-turbo
Append: [Agent-RLVR: Training Software Engineering Agents via Guidance and Environment Rewards](https://arxiv.org/abs/2506.11425)
append_entries: 1
Finish: 2025-06-24 08:24:11.457077
------------------------------------------------------
Started: 2025-06-24 10:18:49.962265
Existing_entries: 1001
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-24 10:18:50.409895
------------------------------------------------------
Started: 2025-06-24 12:35:51.434099
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-24 12:35:51.880139
------------------------------------------------------
Started: 2025-06-24 14:16:56.266513
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-24 14:16:56.717421
------------------------------------------------------
Started: 2025-06-24 16:21:38.047262
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-24 16:21:38.525035
------------------------------------------------------
Started: 2025-06-24 18:23:52.512521
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-24 18:23:52.977348
------------------------------------------------------
Started: 2025-06-24 20:19:13.873389
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-24 20:19:14.344027
------------------------------------------------------
Started: 2025-06-24 22:16:14.956282
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-24 22:16:15.437366
------------------------------------------------------
Started: 2025-06-25 01:22:10.194502
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-25 01:22:10.637682
------------------------------------------------------
Started: 2025-06-25 03:17:29.612724
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-25 03:17:30.079326
------------------------------------------------------
Started: 2025-06-25 04:30:50.556590
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1351
Summarized using GPT-3.5-turbo
Append: [MemeMind: A Large-Scale Multimodal Dataset with Chain-of-Thought Reasoning for Harmful Meme Detection](https://arxiv.org/abs/2506.18919)
Token length: 1450
Summarized using GPT-3.5-turbo
Append: [Mirage of Mastery: Memorization Tricks LLMs into Artificially Inflated Self-Knowledge](https://arxiv.org/abs/2506.18998)
Token length: 1683
Summarized using GPT-3.5-turbo
Append: [Broken Tokens? Your Language Model can Secretly Handle Non-Canonical Tokenizations](https://arxiv.org/abs/2506.19004)
Token length: 1290
Summarized using GPT-3.5-turbo
Append: [Quantifying Fairness in LLMs Beyond Tokens: A Semantic and Statistical Perspective](https://arxiv.org/abs/2506.19028)
Token length: 1629
Summarized using GPT-3.5-turbo
Append: [Plan for Speed -- Dilated Scheduling for Masked Diffusion Language Models](https://arxiv.org/abs/2506.19037)
Token length: 1342
Summarized using GPT-3.5-turbo
Append: [NLPnorth @ TalentCLEF 2025: Comparing Discriminative, Contrastive, and Prompt-Based Methods for Job Title and Skill Matching](https://arxiv.org/abs/2506.19058)
Token length: 1312
Summarized using GPT-3.5-turbo
Append: [MFTCXplain: A Multilingual Benchmark Dataset for Evaluating the Moral Reasoning of LLMs through Hate Speech Multi-hop Explanation](https://arxiv.org/abs/2506.19073)
Token length: 1101
Summarized using GPT-3.5-turbo
Append: [Language Models Might Not Understand You: Evaluating Theory of Mind via Story Prompting](https://arxiv.org/abs/2506.19089)
Token length: 1538
Summarized using GPT-3.5-turbo
Append: [Human-Aligned Faithfulness in Toxicity Explanations of LLMs](https://arxiv.org/abs/2506.19113)
Token length: 1016
Summarized using GPT-3.5-turbo
Append: [Enhanced Hybrid Transducer and Attention Encoder Decoder with Text Data](https://arxiv.org/abs/2506.19159)
Token length: 1350
Summarized using GPT-3.5-turbo
Append: [Prompt, Translate, Fine-Tune, Re-Initialize, or Instruction-Tune? Adapting LLMs for In-Context Learning in Low-Resource Languages](https://arxiv.org/abs/2506.19187)
Token length: 1485
Summarized using GPT-3.5-turbo
Append: [Augmenting Multi-Agent Communication with State Delta Trajectory](https://arxiv.org/abs/2506.19209)
Token length: 1139
Summarized using GPT-3.5-turbo
Append: [Personality Prediction from Life Stories using Language Models](https://arxiv.org/abs/2506.19258)
Token length: 1384
Summarized using GPT-3.5-turbo
Append: [What Matters in LLM-generated Data: Diversity and Its Effect on Model Fine-Tuning](https://arxiv.org/abs/2506.19262)
Token length: 1161
Summarized using GPT-3.5-turbo
Append: [EmoStage: A Framework for Accurate Empathetic Response Generation via Perspective-Taking and Phase Recognition](https://arxiv.org/abs/2506.19279)
Token length: 921
Summarized using GPT-3.5-turbo
Append: [JCAPT: A Joint Modeling Approach for CAPT](https://arxiv.org/abs/2506.19315)
Token length: 1282
Summarized using GPT-3.5-turbo
Append: [Spotting Out-of-Character Behavior: Atomic-Level Evaluation of Persona Fidelity in Open-Ended Generation](https://arxiv.org/abs/2506.19352)
Token length: 1420
Summarized using GPT-3.5-turbo
Append: [Measuring and Guiding Monosemanticity](https://arxiv.org/abs/2506.19382)
Token length: 1351
Summarized using GPT-3.5-turbo
Append: [Automated Detection of Pre-training Text in Black-box LLMs](https://arxiv.org/abs/2506.19399)
Token length: 1654
Summarized using GPT-3.5-turbo
Append: [Learning to Disentangle Latent Reasoning Rules with Language VAEs: A Systematic Study](https://arxiv.org/abs/2506.19418)
Token length: 1138
Summarized using GPT-3.5-turbo
Append: [Can Large Language Models Capture Human Annotator Disagreements?](https://arxiv.org/abs/2506.19467)
Token length: 1089
Summarized using GPT-3.5-turbo
Append: [MuBench: Assessment of Multilingual Capabilities of Large Language Models Across 61 Languages](https://arxiv.org/abs/2506.19468)
Token length: 1921
Summarized using GPT-3.5-turbo
Append: [Commonsense Generation and Evaluation for Dialogue Systems using Large Language Models](https://arxiv.org/abs/2506.19483)
Token length: 1820
Summarized using GPT-3.5-turbo
Append: [Dialogic Pedagogy for Large Language Models: Aligning Conversational AI with Proven Theories of Learning](https://arxiv.org/abs/2506.19484)
Token length: 1731
Summarized using GPT-3.5-turbo
Append: [Is Long-to-Short a Free Lunch? Investigating Inconsistency and Reasoning Efficiency in LRMs](https://arxiv.org/abs/2506.19492)
Token length: 1734
Summarized using GPT-3.5-turbo
Append: [AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models](https://arxiv.org/abs/2506.19505)
Token length: 871
Summarized using GPT-3.5-turbo
Append: [heiDS at ArchEHR-QA 2025: From Fixed-k to Query-dependent-k for Retrieval Augmented Generation](https://arxiv.org/abs/2506.19512)
Token length: 1222
Summarized using GPT-3.5-turbo
Append: [Automatic Posology Structuration : What role for LLMs?](https://arxiv.org/abs/2506.19525)
Token length: 956
Summarized using GPT-3.5-turbo
Append: [KnowMap: Efficient Knowledge-Driven Task Adaptation for LLMs](https://arxiv.org/abs/2506.19527)
Token length: 1209
Summarized using GPT-3.5-turbo
Append: [Health Sentinel: An AI Pipeline For Real-time Disease Outbreak Detection](https://arxiv.org/abs/2506.19548)
Token length: 993
Summarized using GPT-3.5-turbo
Append: [RCStat: A Statistical Framework for using Relative Contextualization in Transformers](https://arxiv.org/abs/2506.19549)
Token length: 1078
Summarized using GPT-3.5-turbo
Append: [Has Machine Translation Evaluation Achieved Human Parity? The Human Reference and the Limits of Progress](https://arxiv.org/abs/2506.19571)
Token length: 1191
Summarized using GPT-3.5-turbo
Append: [ECCoT: A Framework for Enhancing Effective Cognition via Chain of Thought in Large Language Model](https://arxiv.org/abs/2506.19599)
Token length: 1301
Summarized using GPT-3.5-turbo
Append: [Social Hatred: Efficient Multimodal Detection of Hatemongers](https://arxiv.org/abs/2506.19603)
Token length: 1086
Summarized using GPT-3.5-turbo
Append: [Correcting Hallucinations in News Summaries: Exploration of Self-Correcting LLM Methods with External Knowledge](https://arxiv.org/abs/2506.19607)
Token length: 897
Summarized using GPT-3.5-turbo
Append: [Tailored Conversations beyond LLMs: A RL-Based Dialogue Manager](https://arxiv.org/abs/2506.19652)
Token length: 965
Summarized using GPT-3.5-turbo
Append: [Breaking Barriers: Do Reinforcement Post Training Gains Transfer To Unseen Domains?](https://arxiv.org/abs/2506.19733)
Token length: 1938
Summarized using GPT-3.5-turbo
Append: [Evaluating Rare Disease Diagnostic Performance in Symptom Checkers: A Synthetic Vignette Simulation Approach](https://arxiv.org/abs/2506.19750)
Token length: 824
Summarized using GPT-3.5-turbo
Append: [Arabic Dialect Classification using RNNs, Transformers, and Large Language Models: A Comparative Analysis](https://arxiv.org/abs/2506.19753)
Token length: 1023
Summarized using GPT-3.5-turbo
Append: [Accurate, fast, cheap: Choose three. Replacing Multi-Head-Attention with Bidirectional Recurrent Attention for Long-Form ASR](https://arxiv.org/abs/2506.19761)
Token length: 1212
Summarized using GPT-3.5-turbo
Append: [SRFT: A Single-Stage Method with Supervised and Reinforcement Fine-Tuning for Reasoning](https://arxiv.org/abs/2506.19767)
Token length: 972
Summarized using GPT-3.5-turbo
Append: [Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic Empirical Study](https://arxiv.org/abs/2506.19794)
Token length: 1937
Summarized using GPT-3.5-turbo
Append: [How Effectively Can BERT Models Interpret Context and Detect Bengali Communal Violent Text?](https://arxiv.org/abs/2506.19831)
Token length: 1308
Summarized using GPT-3.5-turbo
Append: [MAM: Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis via Role-Specialized Collaboration](https://arxiv.org/abs/2506.19835)
Token length: 1494
Summarized using GPT-3.5-turbo
Append: [Mix-of-Language-Experts Architecture for Multilingual Programming](https://arxiv.org/abs/2506.18923)
Token length: 1496
Summarized using GPT-3.5-turbo
Append: [Chain-of-Experts: Unlocking the Communication Power of Mixture-of-Experts Models](https://arxiv.org/abs/2506.18945)
Token length: 952
Summarized using GPT-3.5-turbo
Append: [LLMs on a Budget? Say HOLA](https://arxiv.org/abs/2506.18952)
Token length: 1947
Summarized using GPT-3.5-turbo
Append: [A Comment On "The Illusion of Thinking": Reframing the Reasoning Cliff as an Agentic Gap](https://arxiv.org/abs/2506.18957)
Token length: 1360
Summarized using GPT-3.5-turbo
Append: [From Web Search towards Agentic Deep Research: Incentivizing Search with Reasoning Agents](https://arxiv.org/abs/2506.18959)
Token length: 1502
Summarized using GPT-3.5-turbo
Append: [HAWAII: Hierarchical Visual Knowledge Transfer for Efficient Vision-Language Models](https://arxiv.org/abs/2506.19072)
Append: [Thought Anchors: Which LLM Reasoning Steps Matter?](https://arxiv.org/abs/2506.19143)
Append: [Bayesian Evolutionary Swarm Architecture: A Formal Epistemic System Grounded in Truth-Based Competition](https://arxiv.org/abs/2506.19191)
Append: [Skywork-SWE: Unveiling Data Scaling Laws for Software Engineering in LLMs](https://arxiv.org/abs/2506.19290)
Append: [In-Context Occam's Razor: How Transformers Prefer Simpler Hypotheses on the Fly](https://arxiv.org/abs/2506.19351)
Append: [Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments with a Hierarchical Spatial-Cognition Long-Short Memory System](https://arxiv.org/abs/2506.19433)
Append: [TTSDS2: Resources and Benchmark for Evaluating Human-Quality Text to Speech Systems](https://arxiv.org/abs/2506.19441)
Append: [NaviAgent: Bilevel Planning on Tool Dependency Graphs for Function Calling](https://arxiv.org/abs/2506.19500)
Append: [Fake or Real, Can Robots Tell? Evaluating Embodied Vision-Language Models on Real and 3D-Printed Objects](https://arxiv.org/abs/2506.19579)
Append: [Recurrent Visual Feature Extraction and Stereo Attentions for CT Report Generation](https://arxiv.org/abs/2506.19665)
Append: [Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large Language Models](https://arxiv.org/abs/2506.19697)
Append: [NEAR$^2$: A Nested Embedding Approach to Efficient Product Retrieval and Ranking](https://arxiv.org/abs/2506.19743)
Append: [Kling-Foley: Multimodal Diffusion Transformer for High-Quality Video-to-Audio Generation](https://arxiv.org/abs/2506.19774)
Append: [LLM-Based Social Simulations Require a Boundary](https://arxiv.org/abs/2506.19806)
Append: [KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality](https://arxiv.org/abs/2506.19807)
Append: [Evaluating Compliance with Visualization Guidelines in Diagrams for Scientific Publications Using Large Vision Language Models](https://arxiv.org/abs/2506.19825)
Append: [Scaling Speculative Decoding with Lookahead Reasoning](https://arxiv.org/abs/2506.19830)
Append: [Orthogonal Finetuning Made Scalable](https://arxiv.org/abs/2506.19847)
Append: [ScaleCap: Inference-Time Scalable Image Captioning via Dual-Modality Debiasing](https://arxiv.org/abs/2506.19848)
Append: [Impact of Visual Context on Noisy Multimodal NMT: An Empirical Study for English to Indian Languages](https://arxiv.org/abs/2308.16075)
Append: [PATCH! {P}sychometrics-{A}ssis{T}ed Ben{CH}marking of Large Language Models against Human Populations: A Case Study of Proficiency in 8th Grade Mathematics](https://arxiv.org/abs/2404.01799)
Append: [Detecting Machine-Generated Texts: Not Just "AI vs Humans" and Explainability is Complicated](https://arxiv.org/abs/2406.18259)
Append: [LEVOS: Leveraging Vocabulary Overlap with Sanskrit to Generate Technical Lexicons in Indian Languages](https://arxiv.org/abs/2407.06331)
Append: [Evaluating Transparent Reasoning in Large Language Models for Accountable Critical Tasks](https://arxiv.org/abs/2408.01933)
Append: [Rational Metareasoning for Large Language Models](https://arxiv.org/abs/2410.05563)
Append: [ADVLLM: Iterative Self-Tuning LLMs for Enhanced Jailbreaking Capabilities](https://arxiv.org/abs/2410.18469)
Append: [Collage: Decomposable Rapid Prototyping for Information Extraction on Scientific PDFs](https://arxiv.org/abs/2410.23478)
Append: [Entropy and type-token ratio in gigaword corpora](https://arxiv.org/abs/2411.10227)
Append: [Sensitive Content Classification in Social Media: A Holistic Resource and Evaluation](https://arxiv.org/abs/2411.19832)
Append: ["I know myself better, but not really greatly": How Well Can LLMs Detect and Explain LLM-Generated Texts?](https://arxiv.org/abs/2502.12743)
Append: [Language Model Re-rankers are Fooled by Lexical Similarities](https://arxiv.org/abs/2502.17036)
Append: [A Foundational individual Mobility Prediction Model based on Open-Source Large Language Models](https://arxiv.org/abs/2503.16553)
Append: [Large Language Models as Span Annotators](https://arxiv.org/abs/2504.08697)
Append: [Analyzing LLMs' Knowledge Boundary Cognition Across Languages Through the Lens of Internal Representations](https://arxiv.org/abs/2504.13816)
Append: [Disentangling Reasoning and Knowledge in Medical Large Language Models](https://arxiv.org/abs/2505.11462)
Append: [Small Language Models in the Real World: Insights from Industrial Text Classification](https://arxiv.org/abs/2505.16078)
Append: [FLAT-LLM: Fine-grained Low-rank Activation Space Transformation for Large Language Model Compression](https://arxiv.org/abs/2505.23966)
Append: [Transferring Features Across Language Models With Model Stitching](https://arxiv.org/abs/2506.06609)
Append: [Right Is Not Enough: The Pitfalls of Outcome Supervision in Training LLMs for Math Reasoning](https://arxiv.org/abs/2506.06877)
Append: [GeistBERT: Breathing Life into German NLP](https://arxiv.org/abs/2506.11903)
Append: [Words as Trigger Points in Social Media Discussions: A Large-Scale Case Study about UK Politics on Reddit](https://arxiv.org/abs/2405.10213)
Append: [ChatSR: Multimodal Large Language Models for Scientific Formula Discovery](https://arxiv.org/abs/2406.05410)
Append: [Large language models for automated scholarly paper review: A survey](https://arxiv.org/abs/2501.10326)
Append: [ProxSparse: Regularized Learning of Semi-Structured Sparsity Masks for Pretrained LLMs](https://arxiv.org/abs/2502.00258)
Append: [Local Look-Ahead Guidance via Verifier-in-the-Loop for Automated Theorem Proving](https://arxiv.org/abs/2503.09730)
Append: [Process Reward Models That Think](https://arxiv.org/abs/2504.16828)
Append: [TRAIL: Trace Reasoning and Agentic Issue Localization](https://arxiv.org/abs/2505.08638)
Append: [Time-IMM: A Dataset and Benchmark for Irregular Multimodal Multivariate Time Series](https://arxiv.org/abs/2506.10412)
Append: [RAG+: Enhancing Retrieval-Augmented Generation with Application-Aware Reasoning](https://arxiv.org/abs/2506.11555)
Append: [DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning with Video LLMs](https://arxiv.org/abs/2506.11558)
append_entries: 99
Finish: 2025-06-25 04:32:26.125847
------------------------------------------------------
Started: 2025-06-25 06:26:23.379451
Existing_entries: 1099
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-25 06:26:23.633246
------------------------------------------------------
Started: 2025-06-25 08:24:31.206011
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-25 08:24:31.454417
------------------------------------------------------
Started: 2025-06-25 10:18:45.030397
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-25 10:18:45.360927
------------------------------------------------------
Started: 2025-06-25 12:35:42.689405
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-25 12:35:42.950467
------------------------------------------------------
Started: 2025-06-25 14:17:08.708290
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-25 14:17:08.957329
------------------------------------------------------
Started: 2025-06-25 16:22:02.522188
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-25 16:22:02.783481
------------------------------------------------------
Started: 2025-06-25 18:24:31.586341
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-25 18:24:31.842467
------------------------------------------------------
Started: 2025-06-25 20:19:10.119915
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-25 20:19:10.407238
------------------------------------------------------
Started: 2025-06-25 22:16:11.728897
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-25 22:16:11.983679
------------------------------------------------------
Started: 2025-06-26 01:21:04.259865
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-26 01:21:04.510176
------------------------------------------------------
Started: 2025-06-26 03:15:58.843193
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-26 03:15:59.114785
------------------------------------------------------
Started: 2025-06-26 04:29:22.130922
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1188
Summarized using GPT-3.5-turbo
Append: [CycleDistill: Bootstrapping Machine Translation using LLMs with Cyclical Distillation](https://arxiv.org/abs/2506.19952)
Token length: 1243
Summarized using GPT-3.5-turbo
Append: [Inference Scaled GraphRAG: Improving Multi Hop Question Answering on Knowledge Graphs](https://arxiv.org/abs/2506.19967)
Token length: 1140
Summarized using GPT-3.5-turbo
Append: [Doc2Agent: Scalable Generation of Tool-Using Agents from API Documentation](https://arxiv.org/abs/2506.19998)
Token length: 1644
Summarized using GPT-3.5-turbo
Append: [A Modular Multitask Reasoning Framework Integrating Spatio-temporal Models and LLMs](https://arxiv.org/abs/2506.20073)
Token length: 1012
Summarized using GPT-3.5-turbo
Append: [SACL: Understanding and Combating Textual Bias in Code Retrieval with Semantic-Augmented Reranking and Localization](https://arxiv.org/abs/2506.20081)
Token length: 878
Summarized using GPT-3.5-turbo
Append: [Bridging Compositional and Distributional Semantics: A Survey on Latent Semantic Geometry via AutoEncoder](https://arxiv.org/abs/2506.20083)
Token length: 1335
Summarized using GPT-3.5-turbo
Append: [ITFormer: Bridging Time Series and Natural Language for Multi-Modal QA with Large-Scale Multitask Dataset](https://arxiv.org/abs/2506.20093)
Token length: 1801
Summarized using GPT-3.5-turbo
Append: [A Multi-Pass Large Language Model Framework for Precise and Efficient Radiology Report Error Detection](https://arxiv.org/abs/2506.20112)
Token length: 1325
Summarized using GPT-3.5-turbo
Append: [Leveraging AI Graders for Missing Score Imputation to Achieve Accurate Ability Estimation in Constructed-Response Tests](https://arxiv.org/abs/2506.20119)
Token length: 1831
Summarized using GPT-3.5-turbo
Append: [CCRS: A Zero-Shot LLM-as-a-Judge Framework for Comprehensive RAG Evaluation](https://arxiv.org/abs/2506.20128)
Token length: 1328
Summarized using GPT-3.5-turbo
Append: [AALC: Large Language Model Efficient Reasoning via Adaptive Accuracy-Length Control](https://arxiv.org/abs/2506.20160)
Token length: 1303
Summarized using GPT-3.5-turbo
Append: [SEED: A Structural Encoder for Embedding-Driven Decoding in Time Series Prediction with LLMs](https://arxiv.org/abs/2506.20167)
Token length: 1581
Summarized using GPT-3.5-turbo
Append: [COIN: Uncertainty-Guarding Selective Question Answering for Foundation Models with Provable Risk Guarantees](https://arxiv.org/abs/2506.20178)
Token length: 1028
Summarized using GPT-3.5-turbo
Append: [How to Retrieve Examples in In-context Learning to Improve Conversational Emotion Recognition using Large Language Models?](https://arxiv.org/abs/2506.20199)
Token length: 1193
Summarized using GPT-3.5-turbo
Append: [Intrinsic vs. Extrinsic Evaluation of Czech Sentence Embeddings: Semantic Relevance Doesn't Help with MT Evaluation](https://arxiv.org/abs/2506.20203)
Token length: 1585
Summarized using GPT-3.5-turbo
Append: [Perspectives in Play: A Multi-Perspective Approach for More Inclusive NLP Systems](https://arxiv.org/abs/2506.20209)
Token length: 1328
Summarized using GPT-3.5-turbo
Append: [Enhancing Large Language Models through Structured Reasoning](https://arxiv.org/abs/2506.20241)
Token length: 1363
Summarized using GPT-3.5-turbo
Append: [CBF-AFA: Chunk-Based Multi-SSL Fusion for Automatic Fluency Assessment](https://arxiv.org/abs/2506.20243)
Token length: 1474
Summarized using GPT-3.5-turbo
Append: [Narrative Shift Detection: A Hybrid Approach of Dynamic Topic Models and Large Language Models](https://arxiv.org/abs/2506.20269)
Token length: 1656
Summarized using GPT-3.5-turbo
Append: [Biomed-Enriched: A Biomedical Dataset Enriched with LLMs for Pretraining and Extracting Rare and Hidden Content](https://arxiv.org/abs/2506.20331)
Token length: 837
Summarized using GPT-3.5-turbo
Append: [TAPS: Tool-Augmented Personalisation via Structured Tagging](https://arxiv.org/abs/2506.20409)
Token length: 1929
Summarized using GPT-3.5-turbo
Append: [An Agentic System for Rare Disease Diagnosis with Traceable Reasoning](https://arxiv.org/abs/2506.20430)
Token length: 1317
Summarized using GPT-3.5-turbo
Append: [Probing AI Safety with Source Code](https://arxiv.org/abs/2506.20471)
Token length: 1498
Summarized using GPT-3.5-turbo
Append: [Time is On My Side: Dynamics of Talk-Time Sharing in Video-chat Conversations](https://arxiv.org/abs/2506.20474)
Token length: 597
Summarized using GPT-3.5-turbo
Append: [Knowledge-Aware Diverse Reranking for Cross-Source Question Answering](https://arxiv.org/abs/2506.20476)
Token length: 1355
Summarized using GPT-3.5-turbo
Append: [GPTailor: Large Language Model Pruning Through Layer Cutting and Stitching](https://arxiv.org/abs/2506.20480)
Token length: 1456
Summarized using GPT-3.5-turbo
Append: [ReCode: Updating Code API Knowledge with Reinforcement Learning](https://arxiv.org/abs/2506.20495)
Token length: 1897
Summarized using GPT-3.5-turbo
Append: [OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling](https://arxiv.org/abs/2506.20512)
Token length: 1772
Summarized using GPT-3.5-turbo
Append: [When Life Gives You Samples: The Benefits of Scaling up Inference Compute for Multilingual LLMs](https://arxiv.org/abs/2506.20544)
Token length: 1743
Summarized using GPT-3.5-turbo
Append: [Model Editing as a Double-Edged Sword: Steering Agent Ethical Behavior Toward Beneficence or Harm](https://arxiv.org/abs/2506.20606)
Token length: 1643
Summarized using GPT-3.5-turbo
Append: [DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation](https://arxiv.org/abs/2506.20639)
Token length: 1107
Summarized using GPT-3.5-turbo
Append: [Memento: Note-Taking for Your Future Self](https://arxiv.org/abs/2506.20642)
Token length: 1686
Summarized using GPT-3.5-turbo
Append: [Inside you are many wolves: Using cognitive models to interpret value trade-offs in LLMs](https://arxiv.org/abs/2506.20666)
Token length: 1302
Summarized using GPT-3.5-turbo
Append: [Capturing Visualization Design Rationale](https://arxiv.org/abs/2506.16571)
Token length: 1185
Summarized using GPT-3.5-turbo
Append: [Position: Machine Learning Conferences Should Establish a "Refutations and Critiques" Track](https://arxiv.org/abs/2506.19882)
Token length: 1527
Summarized using GPT-3.5-turbo
Append: [A Spatio-Temporal Point Process for Fine-Grained Modeling of Reading Behavior](https://arxiv.org/abs/2506.19999)
Token length: 1855
Summarized using GPT-3.5-turbo
Append: [Accurate and Energy Efficient: Local Retrieval-Augmented Generation Models Outperform Commercial Large Language Models in Medical Tasks](https://arxiv.org/abs/2506.20009)
Token length: 1657
Summarized using GPT-3.5-turbo
Append: [Persona-Assigned Large Language Models Exhibit Human-Like Motivated Reasoning](https://arxiv.org/abs/2506.20020)
Token length: 1607
Summarized using GPT-3.5-turbo
Append: [PSALM-V: Automating Symbolic Planning in Interactive Visual Environments with Large Language Models](https://arxiv.org/abs/2506.20097)
Token length: 1329
Summarized using GPT-3.5-turbo
Append: [MIRAGE: A Benchmark for Multimodal Information-Seeking and Reasoning in Agricultural Expert-Guided Conversations](https://arxiv.org/abs/2506.20100)
Token length: 1459
Summarized using GPT-3.5-turbo
Append: [Language Modeling by Language Models](https://arxiv.org/abs/2506.20249)
Token length: 1767
Summarized using GPT-3.5-turbo
Append: [Why Robots Are Bad at Detecting Their Mistakes: Limitations of Miscommunication Detection in Human-Robot Dialogue](https://arxiv.org/abs/2506.20268)
Token length: 1092
Summarized using GPT-3.5-turbo
Append: [FundaQ-8: A Clinically-Inspired Scoring Framework for Automated Fundus Image Quality Assessment](https://arxiv.org/abs/2506.20303)
Token length: 1598
Summarized using GPT-3.5-turbo
Append: [From Codicology to Code: A Comparative Study of Transformer and YOLO-based Detectors for Layout Analysis in Historical Documents](https://arxiv.org/abs/2506.20326)
Token length: 1441
Summarized using GPT-3.5-turbo
Append: [Counterfactual Influence as a Distributional Quantity](https://arxiv.org/abs/2506.20481)
Token length: 1203
Summarized using GPT-3.5-turbo
Append: [Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing positive and negative rewards](https://arxiv.org/abs/2506.20520)
Token length: 1261
Summarized using GPT-3.5-turbo
Append: [PLoP: Precise LoRA Placement for Efficient Finetuning of Large Models](https://arxiv.org/abs/2506.20629)
Token length: 1657
Summarized using GPT-3.5-turbo
Append: [The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind](https://arxiv.org/abs/2506.20664)
Token length: 1469
Summarized using GPT-3.5-turbo
Append: [MMSearch-R1: Incentivizing LMMs to Search](https://arxiv.org/abs/2506.20670)
Token length: 1764
Summarized using GPT-3.5-turbo
Append: [A Global Context Mechanism for Sequence Labeling](https://arxiv.org/abs/2305.19928)
Append: [When Large Language Models contradict humans? Large Language Models' Sycophantic Behaviour](https://arxiv.org/abs/2311.09410)
Append: [Language Models Learn Rare Phenomena from Less Rare Phenomena: The Case of the Missing AANNs](https://arxiv.org/abs/2403.19827)
Append: [Evaluating Long Range Dependency Handling in Code Generation LLMs](https://arxiv.org/abs/2407.21049)
Append: [On the Role of Context in Reading Time Prediction](https://arxiv.org/abs/2409.08160)
Append: [FactCheckmate: Preemptively Detecting and Mitigating Hallucinations in LMs](https://arxiv.org/abs/2410.02899)
Append: [Graph Linearization Methods for Reasoning on Graphs with Large Language Models](https://arxiv.org/abs/2410.19494)
Append: [Separating Tongue from Thought: Activation Patching Reveals Language-Agnostic Concept Representations in Transformers](https://arxiv.org/abs/2411.08745)
Append: [Understanding World or Predicting Future? A Comprehensive Survey of World Models](https://arxiv.org/abs/2411.14499)
Append: [A Comprehensive Evaluation of Semantic Relation Knowledge of Pretrained Language Models and Humans](https://arxiv.org/abs/2412.01131)
Append: [Misalignment of Semantic Relation Knowledge between WordNet and Human Intuition](https://arxiv.org/abs/2412.02138)
Append: [Attention Entropy is a Key Factor: An Analysis of Parallel Context Encoding with Full-attention-based Pre-trained Language Models](https://arxiv.org/abs/2412.16545)
Append: [Unlocking In-Context Learning for Natural Datasets Beyond Language Modelling](https://arxiv.org/abs/2501.06256)
Append: [Towards Fully Exploiting LLM Internal States to Enhance Knowledge Boundary Perception](https://arxiv.org/abs/2502.11677)
Append: [Ad-hoc Concept Forming in the Game Codenames as a Means for Evaluating Large Language Models](https://arxiv.org/abs/2502.11707)
Append: [VAQUUM: Are Vague Quantifiers Grounded in Visual Data?](https://arxiv.org/abs/2502.11874)
Append: [Balancing Truthfulness and Informativeness with Uncertainty-Aware Instruction Fine-Tuning](https://arxiv.org/abs/2502.11962)
Append: [LR^2Bench: Evaluating Long-chain Reflective Reasoning Capabilities of Large Language Models via Constraint Satisfaction Problems](https://arxiv.org/abs/2502.17848)
Append: [The Noisy Path from Source to Citation: Measuring How Scholars Engage with Past Research](https://arxiv.org/abs/2502.20581)
Append: [Rewarding Graph Reasoning Process makes LLMs more Generalized Reasoners](https://arxiv.org/abs/2503.00845)
Append: [LADM: Long-context Training Data Selection with Attention-based Dependency Measurement for LLMs](https://arxiv.org/abs/2503.02502)
Append: [Computation Mechanism Behind LLM Position Generalization](https://arxiv.org/abs/2503.13305)
Append: [Conversational User-AI Intervention: A Study on Prompt Rewriting for Improved LLM Response Generation](https://arxiv.org/abs/2503.16789)
Append: [LLaVA-CMoE: Towards Continual Mixture of Experts for Large Vision-Language Models](https://arxiv.org/abs/2503.21227)
Append: [CogniBench: A Legal-inspired Framework and Dataset for Assessing Cognitive Faithfulness of Large Language Models](https://arxiv.org/abs/2505.20767)
Append: [Recycling the Web: A Method to Enhance Pre-training Data Quality and Quantity for Language Models](https://arxiv.org/abs/2506.04689)
Append: [SMAR: Soft Modality-Aware Routing Strategy for MoE-based Multimodal Large Language Models Preserving Language Capabilities](https://arxiv.org/abs/2506.06406)
Append: [mSTEB: Massively Multilingual Evaluation of LLMs on Speech and Text Tasks](https://arxiv.org/abs/2506.08400)
Append: [FluoroSAM: A Language-promptable Foundation Model for Flexible X-ray Image Segmentation](https://arxiv.org/abs/2403.08059)
Append: [GlyphPattern: An Abstract Pattern Recognition Benchmark for Vision-Language Models](https://arxiv.org/abs/2408.05894)
Append: [Therapy as an NLP Task: Psychologists' Comparison of LLMs and Human Peers in CBT](https://arxiv.org/abs/2409.02244)
Append: [WAFFLE: Finetuning Multi-Modal Model for Automated Front-End Development](https://arxiv.org/abs/2410.18362)
Append: [Can Language Models Replace Programmers for Coding? REPOCOD Says 'Not Yet'](https://arxiv.org/abs/2410.21647)
Append: [VICCA: Visual Interpretation and Comprehension of Chest X-ray Anomalies in Generated Report Without Human Feedback](https://arxiv.org/abs/2501.17726)
Append: [Attention with Trained Embeddings Provably Selects Important Tokens](https://arxiv.org/abs/2505.17282)
Append: [Scientists' First Exam: Probing Cognitive Abilities of MLLM via Perception, Understanding, and Reasoning](https://arxiv.org/abs/2506.10521)
append_entries: 85
Finish: 2025-06-26 04:30:58.040352
------------------------------------------------------
Started: 2025-06-26 06:26:09.709714
Existing_entries: 1085
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-26 06:26:09.964447
------------------------------------------------------
Started: 2025-06-26 08:23:01.275654
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-26 08:23:01.504257
------------------------------------------------------
Started: 2025-06-26 10:18:29.498071
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-26 10:18:29.795711
------------------------------------------------------
Started: 2025-06-26 12:35:28.425328
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-26 12:35:28.682623
------------------------------------------------------
Started: 2025-06-26 14:16:52.645455
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-26 14:16:52.862903
------------------------------------------------------
Started: 2025-06-26 16:21:17.173259
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-26 16:21:17.400180
------------------------------------------------------
Started: 2025-06-26 18:24:26.735621
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-26 18:24:26.963490
------------------------------------------------------
Started: 2025-06-26 20:18:42.136437
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-26 20:18:42.355847
------------------------------------------------------
Started: 2025-06-26 22:16:05.291849
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-26 22:16:05.524584
------------------------------------------------------
Started: 2025-06-27 01:21:59.915483
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-27 01:22:00.141871
------------------------------------------------------
Started: 2025-06-27 03:17:12.621049
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-27 03:17:12.848659
------------------------------------------------------
Started: 2025-06-27 04:29:21.289722
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 741
Summarized using GPT-3.5-turbo
Append: [Towards Probabilistic Question Answering Over Tabular Data](https://arxiv.org/abs/2506.20747)
Token length: 1350
Summarized using GPT-3.5-turbo
Append: [Multi-lingual Functional Evaluation for Large Language Models](https://arxiv.org/abs/2506.20793)
Token length: 1606
Summarized using GPT-3.5-turbo
Append: [The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas](https://arxiv.org/abs/2506.20803)
Token length: 1396
Summarized using GPT-3.5-turbo
Append: [MultiFinRAG: An Optimized Multimodal Retrieval-Augmented Generation (RAG) Framework for Financial Question Answering](https://arxiv.org/abs/2506.20821)
Token length: 1015
Summarized using GPT-3.5-turbo
Append: [Uncovering Hidden Violent Tendencies in LLMs: A Demographic Analysis via Behavioral Vignettes](https://arxiv.org/abs/2506.20822)
Token length: 1440
Summarized using GPT-3.5-turbo
Append: [Decide less, communicate more: On the construct validity of end-to-end fact-checking in medicine](https://arxiv.org/abs/2506.20876)
Token length: 1749
Summarized using GPT-3.5-turbo
Append: [Optimising Language Models for Downstream Tasks: A Post-Training Perspective](https://arxiv.org/abs/2506.20917)
Token length: 1433
Summarized using GPT-3.5-turbo
Append: [FineWeb2: One Pipeline to Scale Them All -- Adapting Pre-Training Data Processing to Every Language](https://arxiv.org/abs/2506.20920)
Token length: 1495
Summarized using GPT-3.5-turbo
Append: [KaLM-Embedding-V2: Superior Training Techniques and Data Inspire A Versatile Embedding Model](https://arxiv.org/abs/2506.20923)
Token length: 1333
Summarized using GPT-3.5-turbo
Append: [Can Gradient Descent Simulate Prompting?](https://arxiv.org/abs/2506.20989)
Token length: 1929
Summarized using GPT-3.5-turbo
Append: [SAC: A Framework for Measuring and Inducing Personality Traits in LLMs with Dynamic Intensity Control](https://arxiv.org/abs/2506.20993)
Token length: 1458
Summarized using GPT-3.5-turbo
Append: [Large Language Models Acing Chartered Accountancy](https://arxiv.org/abs/2506.21031)
Token length: 1417
Summarized using GPT-3.5-turbo
Append: [A Semi-supervised Scalable Unified Framework for E-commerce Query Classification](https://arxiv.org/abs/2506.21049)
Token length: 1448
Summarized using GPT-3.5-turbo
Append: [MT2-CSD: A New Dataset and Multi-Semantic Knowledge Fusion Method for Conversational Stance Detection](https://arxiv.org/abs/2506.21053)
Token length: 1204
Summarized using GPT-3.5-turbo
Append: [DALR: Dual-level Alignment Learning for Multimodal Sentence Representation Learning](https://arxiv.org/abs/2506.21096)
Token length: 923
Summarized using GPT-3.5-turbo
Append: [ComRAG: Retrieval-Augmented Generation with Dynamic Vector Stores for Real-time Community Question Answering in Industry](https://arxiv.org/abs/2506.21098)
Token length: 1238
Summarized using GPT-3.5-turbo
Append: [Progtuning: Progressive Fine-tuning Framework for Transformer-based Language Models](https://arxiv.org/abs/2506.21119)
Token length: 1473
Summarized using GPT-3.5-turbo
Append: [Compressed and Smooth Latent Space for Text Diffusion Modeling](https://arxiv.org/abs/2506.21170)
Token length: 1165
Summarized using GPT-3.5-turbo
Append: [Maintaining MTEB: Towards Long Term Usability and Reproducibility of Embedding Benchmarks](https://arxiv.org/abs/2506.21182)
Token length: 1212
Summarized using GPT-3.5-turbo
Append: [Prompt-Guided Turn-Taking Prediction](https://arxiv.org/abs/2506.21191)
Token length: 1014
Summarized using GPT-3.5-turbo
Append: [Enhancing Automatic Term Extraction with Large Language Models via Syntactic Retrieval](https://arxiv.org/abs/2506.21222)
Token length: 1432
Summarized using GPT-3.5-turbo
Append: [Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling across Perception, Planning, and Safety in Real-World Multimodal Agents](https://arxiv.org/abs/2506.21252)
Token length: 1022
Summarized using GPT-3.5-turbo
Append: [Cat and Mouse -- Can Fake Text Generation Outpace Detector Systems?](https://arxiv.org/abs/2506.21274)
Token length: 1229
Summarized using GPT-3.5-turbo
Append: [Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via Self-Critical Fine-Tuning](https://arxiv.org/abs/2506.21285)
Token length: 1099
Summarized using GPT-3.5-turbo
Append: [Small Encoders Can Rival Large Decoders in Detecting Groundedness](https://arxiv.org/abs/2506.21288)
Token length: 1054
Summarized using GPT-3.5-turbo
Append: [Detecting Referring Expressions in Visually Grounded Dialogue with Autoregressive Language Models](https://arxiv.org/abs/2506.21294)
Token length: 1157
Summarized using GPT-3.5-turbo
Append: [Structuralist Approach to AI Literary Criticism: Leveraging Greimas Semiotic Square for Large Language Models](https://arxiv.org/abs/2506.21360)
Token length: 1445
Summarized using GPT-3.5-turbo
Append: [Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented Generation](https://arxiv.org/abs/2506.21384)
Token length: 1727
Summarized using GPT-3.5-turbo
Append: [Domain Knowledge-Enhanced LLMs for Fraud and Concept Drift Detection](https://arxiv.org/abs/2506.21443)
Token length: 1352
Summarized using GPT-3.5-turbo
Append: [Text2Cypher Across Languages: Evaluating Foundational Models Beyond English](https://arxiv.org/abs/2506.21445)
Token length: 1271
Summarized using GPT-3.5-turbo
Append: [Aligning Spoken Dialogue Models from User Interactions](https://arxiv.org/abs/2506.21463)
Token length: 1827
Summarized using GPT-3.5-turbo
Append: [TopK Language Models](https://arxiv.org/abs/2506.21468)
Token length: 954
Summarized using GPT-3.5-turbo
Append: [Bridging Offline and Online Reinforcement Learning for LLMs](https://arxiv.org/abs/2506.21495)
Token length: 1448
Summarized using GPT-3.5-turbo
Append: [Enhancing User Engagement in Socially-Driven Dialogue through Interactive LLM Alignments](https://arxiv.org/abs/2506.21497)
Token length: 1008
Summarized using GPT-3.5-turbo
Append: [skLEP: A Slovak General Language Understanding Benchmark](https://arxiv.org/abs/2506.21508)
Token length: 1190
Summarized using GPT-3.5-turbo
Append: [Potemkin Understanding in Large Language Models](https://arxiv.org/abs/2506.21521)
Token length: 1138
Summarized using GPT-3.5-turbo
Append: ["What's Up, Doc?": Analyzing How Users Seek Health Information in Large-Scale Conversational AI Datasets](https://arxiv.org/abs/2506.21532)
Token length: 1718
Summarized using GPT-3.5-turbo
Append: [Data Efficacy for Language Model Training](https://arxiv.org/abs/2506.21545)
Token length: 1763
Summarized using GPT-3.5-turbo
Append: [MAGPIE: A dataset for Multi-AGent contextual PrIvacy Evaluation](https://arxiv.org/abs/2506.20737)
Token length: 875
Summarized using GPT-3.5-turbo
Append: [Leaner Training, Lower Leakage: Revisiting Memorization in LLM Fine-Tuning with LoRA](https://arxiv.org/abs/2506.20856)
Token length: 954
Summarized using GPT-3.5-turbo
Append: [Beyond Reactive Safety: Risk-Aware LLM Alignment via Long-Horizon Simulation](https://arxiv.org/abs/2506.20949)
Token length: 1302
Summarized using GPT-3.5-turbo
Append: [SharpZO: Hybrid Sharpness-Aware Vision Language Model Prompt Tuning via Forward-Only Passes](https://arxiv.org/abs/2506.20990)
Token length: 1136
Summarized using GPT-3.5-turbo
Append: [Enhancing LLM Tool Use with High-quality Instruction Data from Knowledge Graph](https://arxiv.org/abs/2506.21071)
Token length: 1336
Summarized using GPT-3.5-turbo
Append: [Learning to Skip the Middle Layers of Transformers](https://arxiv.org/abs/2506.21103)
Token length: 1780
Summarized using GPT-3.5-turbo
Append: [Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?](https://arxiv.org/abs/2506.21215)
Token length: 1022
Summarized using GPT-3.5-turbo
Append: [Complexity-aware fine-tuning](https://arxiv.org/abs/2506.21220)
Token length: 1411
Summarized using GPT-3.5-turbo
Append: [DiLoCoX: A Low-Communication Large-Scale Training Framework for Decentralized Cluster](https://arxiv.org/abs/2506.21263)
Token length: 1937
Summarized using GPT-3.5-turbo
Append: [HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context](https://arxiv.org/abs/2506.21277)
Token length: 1946
Summarized using GPT-3.5-turbo
Append: [Exploring Adapter Design Tradeoffs for Low Resource Music Generation](https://arxiv.org/abs/2506.21298)
Token length: 994
Summarized using GPT-3.5-turbo
Append: [Latent Prototype Routing: Achieving Near-Perfect Load Balancing in Mixture-of-Experts](https://arxiv.org/abs/2506.21328)
Append: [Hybrid Deep Learning and Signal Processing for Arabic Dialect Recognition in Low-Resource Settings](https://arxiv.org/abs/2506.21386)
Append: [Scalable Bayesian Low-Rank Adaptation of Large Language Models via Stochastic Variational Subspace Inference](https://arxiv.org/abs/2506.21408)
Append: [Spatial Mental Modeling from Limited Views](https://arxiv.org/abs/2506.21458)
Append: [Logios : An open source Greek Polytonic Optical Character Recognition system](https://arxiv.org/abs/2506.21474)
Append: [Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge](https://arxiv.org/abs/2506.21506)
Append: [HalluSegBench: Counterfactual Visual Reasoning for Segmentation Hallucination Evaluation](https://arxiv.org/abs/2506.21546)
Append: [Learning Evaluation Models from Large Language Models for Sequence Generation](https://arxiv.org/abs/2308.04386)
Append: [MockLLM: A Multi-Agent Behavior Collaboration Framework for Online Job Seeking and Recruiting](https://arxiv.org/abs/2405.18113)
Append: [Capturing Style in Author and Document Representation](https://arxiv.org/abs/2407.13358)
Append: [Comparing Retrieval-Augmentation and Parameter-Efficient Fine-Tuning for Privacy-Preserving Personalization of Large Language Models](https://arxiv.org/abs/2409.09510)
Append: [Learning to Rank for Multiple Retrieval-Augmented Models through Iterative Utility Maximization](https://arxiv.org/abs/2410.09942)
Append: [A Troublemaker with Contagious Jailbreak Makes Chaos in Honest Towns](https://arxiv.org/abs/2410.16155)
Append: [SceneGenAgent: Precise Industrial Scene Generation with Coding Agent](https://arxiv.org/abs/2410.21909)
Append: [Prompting with Phonemes: Enhancing LLMs' Multilinguality for Non-Latin Script Languages](https://arxiv.org/abs/2411.02398)
Append: [CodeLutra: Boosting LLM Code Generation via Preference-Guided Refinement](https://arxiv.org/abs/2411.05199)
Append: [OpenNER 1.0: Standardized Open-Access Named Entity Recognition Datasets in 50+ Languages](https://arxiv.org/abs/2412.09587)
Append: [Do Large Language Models Advocate for Inferentialism?](https://arxiv.org/abs/2412.14501)
Append: [Reward-Guided Speculative Decoding for Efficient LLM Reasoning](https://arxiv.org/abs/2501.19324)
Append: [Privacy Ripple Effects from Adding or Removing Personal Information in Language Model Training](https://arxiv.org/abs/2502.15680)
Append: [Evaluating Large Language Models for Automated Clinical Abstraction in Pulmonary Embolism Registries: Performance Across Model Sizes, Versions, and Parameters](https://arxiv.org/abs/2503.21004)
Append: [Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning of LLMs](https://arxiv.org/abs/2505.11277)
Append: [A3 : an Analytical Low-Rank Approximation Framework for Attention](https://arxiv.org/abs/2505.12942)
Append: [Thinkless: LLM Learns When to Think](https://arxiv.org/abs/2505.13379)
Append: [Explainability of Large Language Models using SMILE: Statistical Model-agnostic Interpretability with Local Explanations](https://arxiv.org/abs/2505.21657)
Append: [HERMES: temporal-coHERent long-forM understanding with Episodes and Semantics](https://arxiv.org/abs/2408.17443)
Append: [Simulating Hard Attention Using Soft Attention](https://arxiv.org/abs/2412.09925)
Append: [GroundCap: A Visually Grounded Image Captioning Dataset](https://arxiv.org/abs/2502.13898)
Append: [PP-DocBee: Improving Multimodal Document Understanding Through a Bag of Tricks](https://arxiv.org/abs/2503.04065)
append_entries: 78
Finish: 2025-06-27 04:30:50.882274
------------------------------------------------------
Started: 2025-06-27 06:25:57.384973
Existing_entries: 1078
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-27 06:25:57.600792
------------------------------------------------------
Started: 2025-06-27 08:22:54.287281
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-27 08:22:54.496827
------------------------------------------------------
Started: 2025-06-27 10:18:41.593736
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-27 10:18:41.813159
------------------------------------------------------
Started: 2025-06-27 12:34:26.209940
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-27 12:34:26.417084
------------------------------------------------------
Started: 2025-06-27 14:16:36.179795
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-27 14:16:36.413869
------------------------------------------------------
Started: 2025-06-27 16:21:14.774887
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-27 16:21:15.023579
------------------------------------------------------
Started: 2025-06-27 18:23:40.256693
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-27 18:23:40.475619
------------------------------------------------------
Started: 2025-06-27 20:18:22.543824
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-27 20:18:22.807051
------------------------------------------------------
Started: 2025-06-27 22:17:33.814724
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-27 22:17:34.053996
------------------------------------------------------
Started: 2025-06-28 01:19:04.356431
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-28 01:19:04.605556
------------------------------------------------------
Started: 2025-06-28 03:11:59.229699
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-28 03:11:59.445185
------------------------------------------------------
Started: 2025-06-28 04:20:38.569851
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-28 04:20:38.631250
------------------------------------------------------
Started: 2025-06-28 06:22:29.444842
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-28 06:22:29.557685
------------------------------------------------------
Started: 2025-06-28 08:20:16.311158
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-28 08:20:16.365435
------------------------------------------------------
Started: 2025-06-28 10:16:34.224986
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-28 10:16:34.326357
------------------------------------------------------
Started: 2025-06-28 12:31:21.103408
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-28 12:31:21.175333
------------------------------------------------------
Started: 2025-06-28 14:14:15.972467
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-28 14:14:16.048562
------------------------------------------------------
Started: 2025-06-28 16:19:12.982576
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-28 16:19:13.042586
------------------------------------------------------
Started: 2025-06-28 18:21:33.869904
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-28 18:21:33.946665
------------------------------------------------------
Started: 2025-06-28 20:17:23.366269
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-28 20:17:23.459590
------------------------------------------------------
Started: 2025-06-28 22:15:07.699639
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-28 22:15:07.775394
------------------------------------------------------
Started: 2025-06-29 01:38:07.564184
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-29 01:38:07.653923
------------------------------------------------------
Started: 2025-06-29 03:26:15.850392
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-29 03:26:15.923621
------------------------------------------------------
Started: 2025-06-29 04:32:45.883243
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-29 04:32:46.032099
------------------------------------------------------
Started: 2025-06-29 06:23:37.210544
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-29 06:23:37.270066
------------------------------------------------------
Started: 2025-06-29 08:20:09.477153
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-29 08:20:09.584330
------------------------------------------------------
Started: 2025-06-29 10:19:10.785696
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-29 10:19:10.854338
------------------------------------------------------
Started: 2025-06-29 12:31:54.574220
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-29 12:31:54.632123
------------------------------------------------------
Started: 2025-06-29 14:14:13.647574
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-29 14:14:13.737137
------------------------------------------------------
Started: 2025-06-29 16:19:15.534087
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-29 16:19:15.594205
------------------------------------------------------
Started: 2025-06-29 18:21:42.918203
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-29 18:21:42.982837
------------------------------------------------------
Started: 2025-06-29 20:17:46.595199
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-29 20:17:46.654372
------------------------------------------------------
Started: 2025-06-29 22:15:25.929956
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-29 22:15:25.994719
------------------------------------------------------
Started: 2025-06-30 01:26:04.746282
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-30 01:26:04.870623
------------------------------------------------------
Started: 2025-06-30 03:22:45.009708
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-30 03:22:45.094401
------------------------------------------------------
Started: 2025-06-30 04:34:10.342923
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1015
Summarized using GPT-3.5-turbo
Append: [Efficient Multilingual ASR Finetuning via LoRA Language Experts](https://arxiv.org/abs/2506.21555)
Token length: 1765
Summarized using GPT-3.5-turbo
Append: [VAT-KG: Knowledge-Intensive Multimodal Knowledge Graph Dataset for Retrieval-Augmented Generation](https://arxiv.org/abs/2506.21556)
Token length: 1275
Summarized using GPT-3.5-turbo
Append: [Debunk and Infer: Multimodal Fake News Detection via Diffusion-Generated Evidence and LLM Reasoning](https://arxiv.org/abs/2506.21557)
Token length: 1407
Summarized using GPT-3.5-turbo
Append: [Bench to the Future: A Pastcasting Benchmark for Forecasting Agents](https://arxiv.org/abs/2506.21558)
Token length: 1963
Summarized using GPT-3.5-turbo
Append: [GraphLAMA: Enabling Efficient Adaptation of Graph Language Models with Limited Annotations](https://arxiv.org/abs/2506.21559)
Token length: 910
Summarized using GPT-3.5-turbo
Append: [Reinforcement Learning Fine-Tuning of Language Model for Instruction Following and Math Reasoning](https://arxiv.org/abs/2506.21560)
Token length: 1080
Summarized using GPT-3.5-turbo
Append: [Reasoning Isn't Enough: Examining Truth-Bias and Sycophancy in LLMs](https://arxiv.org/abs/2506.21561)
Token length: 910
Summarized using GPT-3.5-turbo
Append: [FloorPlan-DeepSeek (FPDS): A multimodal approach to floorplan generation using vector-based next room prediction](https://arxiv.org/abs/2506.21562)
Token length: 1351
Summarized using GPT-3.5-turbo
Append: [FormosanBench: Benchmarking Low-Resource Austronesian Languages in the Era of Large Language Models](https://arxiv.org/abs/2506.21563)
Token length: 758
Summarized using GPT-3.5-turbo
Append: [Team QUST at SemEval-2025 Task 10: Evaluating Large Language Models in Multiclass Multi-label Classification of News Entity Framing](https://arxiv.org/abs/2506.21564)
Token length: 1246
Summarized using GPT-3.5-turbo
Append: [A Multi-Agent Probabilistic Inference Framework Inspired by Kairanban-Style CoT System with IdoBata Conversation for Debiasing](https://arxiv.org/abs/2506.21565)
Token length: 1168
Summarized using GPT-3.5-turbo
Append: [The Saturation Point of Backtranslation in High Quality Low Resource English Gujarati Machine Translation](https://arxiv.org/abs/2506.21566)
Token length: 1964
Summarized using GPT-3.5-turbo
Append: [BioPars: A Pretrained Biomedical Large Language Model for Persian Biomedical Text Mining](https://arxiv.org/abs/2506.21567)
Token length: 1295
Summarized using GPT-3.5-turbo
Append: [Assessing RAG and HyDE on 1B vs. 4B-Parameter Gemma LLMs for Personal Assistants Integretion](https://arxiv.org/abs/2506.21568)
Token length: 1416
Summarized using GPT-3.5-turbo
Append: [Hybrid-NL2SVA: Integrating RAG and Finetuning for LLM-based NL2SVA](https://arxiv.org/abs/2506.21569)
Token length: 1058
Summarized using GPT-3.5-turbo
Append: [Random Initialization Can't Catch Up: The Advantage of Language Model Transfer for Time Series Forecasting](https://arxiv.org/abs/2506.21570)
Token length: 1678
Summarized using GPT-3.5-turbo
Append: [Towards Understanding the Cognitive Habits of Large Reasoning Models](https://arxiv.org/abs/2506.21571)
Token length: 1202
Summarized using GPT-3.5-turbo
Append: [Aligning MLLM Benchmark With Human Preferences via Structural Equation Modeling](https://arxiv.org/abs/2506.21572)
Token length: 1381
Summarized using GPT-3.5-turbo
Append: [Instruction Learning Paradigms: A Dual Perspective on White-box and Black-box LLMs](https://arxiv.org/abs/2506.21573)
Token length: 1119
Summarized using GPT-3.5-turbo
Append: [Digital Gatekeepers: Exploring Large Language Model's Role in Immigration Decisions](https://arxiv.org/abs/2506.21574)
Token length: 1375
Summarized using GPT-3.5-turbo
Append: [STRuCT-LLM: Unifying Tabular and Graph Reasoning with Reinforcement Learning for Semantic Parsing](https://arxiv.org/abs/2506.21575)
Token length: 1050
Summarized using GPT-3.5-turbo
Append: [Adapting Whisper for Parameter-efficient Code-Switching Speech Recognition via Soft Prompt Tuning](https://arxiv.org/abs/2506.21576)
Token length: 1067
Summarized using GPT-3.5-turbo
Append: [Language-Aware Prompt Tuning for Parameter-Efficient Seamless Language Expansion in Multilingual ASR](https://arxiv.org/abs/2506.21577)
Token length: 1497
Summarized using GPT-3.5-turbo
Append: [HealthQA-BR: A System-Wide Benchmark Reveals Critical Knowledge Gaps in Large Language Models](https://arxiv.org/abs/2506.21578)
Token length: 955
Summarized using GPT-3.5-turbo
Append: [From General Reasoning to Domain Expertise: Uncovering the Limits of Generalization in Large Language Models](https://arxiv.org/abs/2506.21580)
Token length: 1492
Summarized using GPT-3.5-turbo
Append: [VIDEE: Visual and Interactive Decomposition, Execution, and Evaluation of Text Analytics with Intelligent Agents](https://arxiv.org/abs/2506.21582)
Token length: 1586
Summarized using GPT-3.5-turbo
Append: [Hope Speech Detection in code-mixed Roman Urdu tweets: A Positive Turn in Natural Language Processing](https://arxiv.org/abs/2506.21583)
Token length: 953
Summarized using GPT-3.5-turbo
Append: [Empirical Evidence for Alignment Faking in Small LLMs and Prompt-Based Mitigation Techniques](https://arxiv.org/abs/2506.21584)
Token length: 1104
Summarized using GPT-3.5-turbo
Append: [Evaluation of LLM-based Strategies for the Extraction of Food Product Information from Online Shops](https://arxiv.org/abs/2506.21585)
Token length: 1214
Summarized using GPT-3.5-turbo
Append: [Can Vision Language Models Understand Mimed Actions?](https://arxiv.org/abs/2506.21586)
Token length: 1655
Summarized using GPT-3.5-turbo
Append: [Is DeepSeek a New Voice Among LLMs in Public Opinion Simulation?](https://arxiv.org/abs/2506.21587)
Token length: 1129
Summarized using GPT-3.5-turbo
Append: [Understanding Verbatim Memorization in LLMs Through Circuit Discovery](https://arxiv.org/abs/2506.21588)
Token length: 1342
Summarized using GPT-3.5-turbo
Append: [A General Method for Detecting Information Generated by Large Language Models](https://arxiv.org/abs/2506.21589)
Token length: 1659
Summarized using GPT-3.5-turbo
Append: [Representation Consistency for Accurate and Coherent LLM Answer Aggregation](https://arxiv.org/abs/2506.21590)
Token length: 1352
Summarized using GPT-3.5-turbo
Append: [FinEval-KR: A Financial Domain Evaluation Framework for Large Language Models' Knowledge and Reasoning](https://arxiv.org/abs/2506.21591)
Token length: 1425
Summarized using GPT-3.5-turbo
Append: [SignBart -- New approach with the skeleton sequence for Isolated Sign language Recognition](https://arxiv.org/abs/2506.21592)
Token length: 1604
Summarized using GPT-3.5-turbo
Append: [Gazal-R1: Achieving State-of-the-Art Medical Reasoning with Parameter-Efficient Two-Stage Training](https://arxiv.org/abs/2506.21594)
Token length: 1165
Summarized using GPT-3.5-turbo
Append: [Thunder-LLM: Efficiently Adapting LLMs to Korean with Minimal Resources](https://arxiv.org/abs/2506.21595)
Token length: 1088
Summarized using GPT-3.5-turbo
Append: [Evaluating Multimodal Large Language Models on Educational Textbook Question Answering](https://arxiv.org/abs/2506.21596)
Token length: 884
Summarized using GPT-3.5-turbo
Append: [Overview of the ClinIQLink 2025 Shared Task on Medical Question-Answering](https://arxiv.org/abs/2506.21597)
Token length: 1233
Summarized using GPT-3.5-turbo
Append: [Structured Attention Matters to Multimodal LLMs in Document Understanding](https://arxiv.org/abs/2506.21600)
Token length: 1403
Summarized using GPT-3.5-turbo
Append: [BiMark: Unbiased Multilayer Watermarking for Large Language Models](https://arxiv.org/abs/2506.21602)
Token length: 897
Summarized using GPT-3.5-turbo
Append: [Operationalizing Automated Essay Scoring: A Human-Aware Approach](https://arxiv.org/abs/2506.21603)
Token length: 1115
Summarized using GPT-3.5-turbo
Append: [MemBench: Towards More Comprehensive Evaluation on the Memory of LLM-based Agents](https://arxiv.org/abs/2506.21605)
Token length: 1627
Summarized using GPT-3.5-turbo
Append: [Large Language Models as symbolic DNA of cultural dynamics](https://arxiv.org/abs/2506.21606)
Token length: 1140
Summarized using GPT-3.5-turbo
Append: [CORE-KG: An LLM-Driven Knowledge Graph Construction Framework for Human Smuggling Networks](https://arxiv.org/abs/2506.21607)
Token length: 646
Summarized using GPT-3.5-turbo
Append: [SysTemp: A Multi-Agent System for Template-Based Generation of SysML v2](https://arxiv.org/abs/2506.21608)
Token length: 1782
Summarized using GPT-3.5-turbo
Append: [From Thinking to Output: Chain-of-Thought and Text Generation Characteristics in Reasoning Language Models](https://arxiv.org/abs/2506.21609)
Token length: 1596
Summarized using GPT-3.5-turbo
Append: [Does Multimodality Lead to Better Time Series Forecasting?](https://arxiv.org/abs/2506.21611)
Token length: 1586
Summarized using GPT-3.5-turbo
Append: [AdaptGOT: A Pre-trained Model for Adaptive Contextual POI Representation Learning](https://arxiv.org/abs/2506.21612)
Append: [ChildGuard: A Specialized Dataset for Combatting Child-Targeted Hate Speech](https://arxiv.org/abs/2506.21613)
Append: [LastingBench: Defend Benchmarks Against Knowledge Leakage](https://arxiv.org/abs/2506.21614)
Append: [Refine Medical Diagnosis Using Generation Augmented Retrieval and Clinical Practice Guidelines](https://arxiv.org/abs/2506.21615)
Append: [TIM: A Large-Scale Dataset and large Timeline Intelligence Model for Open-domain Timeline Summarization](https://arxiv.org/abs/2506.21616)
Append: [TrajTok: Technical Report for 2025 Waymo Open Sim Agents Challenge](https://arxiv.org/abs/2506.21618)
Append: [IndexTTS2: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech](https://arxiv.org/abs/2506.21619)
Append: [How Large Language Models play humans in online conversations: a simulated study of the 2016 US politics on Reddit](https://arxiv.org/abs/2506.21620)
Append: [The Open Proof Corpus: A Large-Scale Study of LLM-Generated Mathematical Proofs](https://arxiv.org/abs/2506.21621)
Append: [Adapting Foundation Speech Recognition Models to Impaired Speech: A Semantic Re-chaining Approach for Personalization of German Speech](https://arxiv.org/abs/2506.21622)
Append: [Performance of diverse evaluation metrics in NLP-based assessment and text generation of consumer complaints](https://arxiv.org/abs/2506.21623)
Append: [Doc2SAR: A Synergistic Framework for High-Fidelity Extraction of Structure-Activity Relationships from Scientific Documents](https://arxiv.org/abs/2506.21625)
Append: [Do We Really Need GNNs with Explicit Structural Modeling? MLPs Suffice for Language Model Representations](https://arxiv.org/abs/2506.21682)
Append: [ANUBHUTI: A Comprehensive Corpus For Sentiment Analysis In Bangla Regional Languages](https://arxiv.org/abs/2506.21686)
Append: [Identifying Speaker Information in Feed-Forward Layers of Self-Supervised Speech Transformers](https://arxiv.org/abs/2506.21712)
Append: [(Fact) Check Your Bias](https://arxiv.org/abs/2506.21745)
Append: [Evaluating List Construction and Temporal Understanding capabilities of Large Language Models](https://arxiv.org/abs/2506.21783)
Append: [Offensive Language Detection on Social Media Using XLNet](https://arxiv.org/abs/2506.21795)
Append: [A suite of allotaxonometric tools for the comparison of complex systems using rank-turbulence divergence](https://arxiv.org/abs/2506.21808)
Append: [Towards Transparent AI: A Survey on Explainable Large Language Models](https://arxiv.org/abs/2506.21812)
Append: [Exploring the Structure of AI-Induced Language Change in Scientific English](https://arxiv.org/abs/2506.21817)
Append: [PARSI: Persian Authorship Recognition via Stylometric Integration](https://arxiv.org/abs/2506.21840)
Append: [LinguaSynth: Heterogeneous Linguistic Signals for News Classification](https://arxiv.org/abs/2506.21848)
Append: [The Consistency Hypothesis in Uncertainty Quantification for Large Language Models](https://arxiv.org/abs/2506.21849)
Append: [Derivational Probing: Unveiling the Layer-wise Derivation of Syntactic Structures in Neural Language Models](https://arxiv.org/abs/2506.21861)
Append: [DeepTalk: Towards Seamless and Smart Speech Interaction with Adaptive Modality-Specific MoE](https://arxiv.org/abs/2506.21864)
Append: [WildSpeech-Bench: Benchmarking Audio LLMs in Natural Speech Conversation](https://arxiv.org/abs/2506.21875)
Append: [Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation](https://arxiv.org/abs/2506.21876)
Append: [A Dual-Layered Evaluation of Geopolitical and Cultural Bias in LLMs](https://arxiv.org/abs/2506.21881)
Append: [AutoMixer: Checkpoint Artifacts as Automatic Data Mixers](https://arxiv.org/abs/2506.21910)
Append: [PapersPlease: A Benchmark for Evaluating Motivational Values of Large Language Models Based on ERG Theory](https://arxiv.org/abs/2506.21961)
Append: [More Vulnerable than You Think: On the Stability of Tool-Integrated LLM Agents](https://arxiv.org/abs/2506.21967)
Append: [Advancing Jailbreak Strategies: A Hybrid Approach to Exploiting LLM Vulnerabilities and Bypassing Modern Defenses](https://arxiv.org/abs/2506.21972)
Append: [Don't Trust Generative Agents to Mimic Communication on Social Networks Unless You Benchmarked their Empirical Realism](https://arxiv.org/abs/2506.21974)
Append: [Analyzing and Fine-Tuning Whisper Models for Multilingual Pilot Speech Transcription in the Cockpit](https://arxiv.org/abs/2506.21990)
Append: [Can Peter Pan Survive MT? A Stylometric Study of LLMs, NMTs, and HTs in Children's Literature Translation](https://arxiv.org/abs/2506.22038)
Append: [Decoding Machine Translationese in English-Chinese News: LLMs vs. NMTs](https://arxiv.org/abs/2506.22050)
Append: [Lost at the Beginning of Reasoning](https://arxiv.org/abs/2506.22058)
Append: [MDC-R: The Minecraft Dialogue Corpus with Reference](https://arxiv.org/abs/2506.22062)
Append: [Involvement drives complexity of language in online debates](https://arxiv.org/abs/2506.22098)
Append: [Identifying a Circuit for Verb Conjugation in GPT-2](https://arxiv.org/abs/2506.22105)
Append: [DAPFAM: A Domain-Aware Patent Retrieval Dataset Aggregated at the Family Level](https://arxiv.org/abs/2506.22141)
Append: [SAGE: Spliced-Audio Generated Data for Enhancing Foundational Models in Low-Resource Arabic-English Code-Switched Speech Recognition](https://arxiv.org/abs/2506.22143)
Append: [Training Language Model to Critique for Better Refinement](https://arxiv.org/abs/2506.22157)
Append: [Leveraging In-Context Learning for Political Bias Testing of LLMs](https://arxiv.org/abs/2506.22232)
Append: [Detection of Personal Data in Structured Datasets Using a Large Language Model](https://arxiv.org/abs/2506.22305)
Append: [Evaluating Scoring Bias in LLM-as-a-Judge](https://arxiv.org/abs/2506.22316)
Append: [Why Are Parsing Actions for Understanding Message Hierarchies Not Random?](https://arxiv.org/abs/2506.22366)
Append: [QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization](https://arxiv.org/abs/2506.22396)
Append: [Refining Czech GEC: Insights from a Multi-Experiment Approach](https://arxiv.org/abs/2506.22402)
Append: [HyperCLOVA X THINK Technical Report](https://arxiv.org/abs/2506.22403)
Append: [Sequential Diagnosis with Language Models](https://arxiv.org/abs/2506.22405)
Append: [Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs](https://arxiv.org/abs/2506.21656)
Append: [CitySim: Modeling Urban Behaviors and City Dynamics with Large-Scale LLM-Driven Agent Simulation](https://arxiv.org/abs/2506.21805)
Append: [Exploring the change in scientific readability following the release of ChatGPT](https://arxiv.org/abs/2506.21825)
Append: [GenEscape: Hierarchical Multi-Agent Generation of Escape Room Puzzles](https://arxiv.org/abs/2506.21839)
Append: [3Description: An Intuitive Human-AI Collaborative 3D Modeling Approach](https://arxiv.org/abs/2506.21845)
Append: [RiverEcho: Real-Time Interactive Digital System for Ancient Yellow River Culture](https://arxiv.org/abs/2506.21865)
Append: [HyReC: Exploring Hybrid-based Retriever for Chinese](https://arxiv.org/abs/2506.21913)
Append: [ARAG: Agentic Retrieval Augmented Generation for Personalized Recommendation](https://arxiv.org/abs/2506.21931)
Append: [Using Large Language Models to Suggest Informative Prior Distributions in Bayesian Statistics](https://arxiv.org/abs/2506.21964)
Append: [Robust and Efficient Autoregressive Speech Synthesis with Dynamic Chunk-wise Prediction Policy](https://arxiv.org/abs/2506.22023)
Append: [GPAS: Accelerating Convergence of LLM Pretraining via Gradient-Preserving Activation Scaling](https://arxiv.org/abs/2506.22049)
Append: [Exploring Modularity of Agentic Systems for Drug Discovery](https://arxiv.org/abs/2506.22189)
Append: [Fine-Tuning MIDI-to-Audio Alignment using a Neural Network on Piano Roll and CQT Representations](https://arxiv.org/abs/2506.22237)
Append: [Projected Compression: Trainable Projection for Efficient Transformer Compression](https://arxiv.org/abs/2506.22255)
Append: [COOCO -- Common Objects Out-of-Context -- Semantic Violation in Scenes: Investigating Multimodal Context in Referential Communication](https://arxiv.org/abs/2506.22274)
Append: [Conceptual Topic Aggregation](https://arxiv.org/abs/2506.22309)
Append: [Optimal Estimation of Watermark Proportions in Hybrid AI-Human Texts](https://arxiv.org/abs/2506.22343)
Append: [Towards Fair Rankings: Leveraging LLMs for Gender Bias Detection and Measurement](https://arxiv.org/abs/2506.22372)
Append: [Probabilistic Optimality for Inference-time Scaling](https://arxiv.org/abs/2506.22376)
Append: [Can Video Large Multimodal Models Think Like Doubters-or Double-Down: A Study on Defeasible Video Entailment](https://arxiv.org/abs/2506.22385)
Append: [The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements](https://arxiv.org/abs/2506.22419)
Append: [Bias-Augmented Consistency Training Reduces Biased Reasoning in Chain-of-Thought](https://arxiv.org/abs/2403.05518)
Append: [Language in Vivo vs. in Silico: Size Matters but Larger Language Models Still Do Not Comprehend Language on a Par with Humans Due to Impenetrable Semantic Reference](https://arxiv.org/abs/2404.14883)
Append: [RLSF: Fine-tuning LLMs via Symbolic Feedback](https://arxiv.org/abs/2405.16661)
Append: [Beyond Fixed Length: Bucket Pre-training is All You Need](https://arxiv.org/abs/2407.07495)
Append: [Dynamic Adaptive Optimization for Effective Sentiment Analysis Fine-Tuning on Large Language Models](https://arxiv.org/abs/2408.11856)
Append: [LRP4RAG: Detecting Hallucinations in Retrieval-Augmented Generation via Layer-wise Relevance Propagation](https://arxiv.org/abs/2408.15533)
Append: [PQ-GCN: Enhancing Text Graph Question Classification with Phrase Features](https://arxiv.org/abs/2409.02481)
Append: [How to Train Long-Context Language Models (Effectively)](https://arxiv.org/abs/2410.02660)
Append: [Towards Reproducible LLM Evaluation: Quantifying Uncertainty in LLM Benchmark Scores](https://arxiv.org/abs/2410.03492)
Append: [Dynamic Adaptive Rank Space Exploration for Efficient Sentiment Analysis with Large Language Models](https://arxiv.org/abs/2410.16589)
Append: [All Entities are Not Created Equal: Examining the Long Tail for Ultra-Fine Entity Typing](https://arxiv.org/abs/2410.17355)
Append: [Introducing MAPO: Momentum-Aided Gradient Descent Prompt Optimization](https://arxiv.org/abs/2410.19499)
Append: [Are Triggers Needed for Document-Level Event Extraction?](https://arxiv.org/abs/2411.08708)
Append: [Strengthening False Information Propagation Detection: Leveraging SVM and Sophisticated Text Vectorization Techniques in comparison to BERT](https://arxiv.org/abs/2411.12703)
Append: [iPrOp: Interactive Prompt Optimization for Large Language Models with a Human in the Loop](https://arxiv.org/abs/2412.12644)
Append: [Refining Salience-Aware Sparse Fine-Tuning Strategies for Language Models](https://arxiv.org/abs/2412.13488)
Append: [End-to-End Long Document Summarization using Gradient Caching](https://arxiv.org/abs/2501.01805)
Append: [Metadata Conditioning Accelerates Language Model Pre-training](https://arxiv.org/abs/2501.01956)
Append: [ACORD: An Expert-Annotated Retrieval Dataset for Legal Contract Drafting](https://arxiv.org/abs/2501.06582)
Append: [Leveraging Online Olympiad-Level Math Problems for LLMs Training and Contamination-Resistant Evaluation](https://arxiv.org/abs/2501.14275)
Append: [Quantum-Enhanced Attention Mechanism in NLP: A Hybrid Classical-Quantum Approach](https://arxiv.org/abs/2501.15630)
Append: [ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference](https://arxiv.org/abs/2502.00299)
Append: [STAIR: Improving Safety Alignment with Introspective Reasoning](https://arxiv.org/abs/2502.02384)
Append: [MedRAG: Enhancing Retrieval-augmented Generation with Knowledge Graph-Elicited Reasoning for Healthcare Copilot](https://arxiv.org/abs/2502.04413)
Append: [A Survey of Large Language Models in Psychotherapy: Current Landscape and Future Directions](https://arxiv.org/abs/2502.11095)
Append: [Plant in Cupboard, Orange on Rably, Inat Aphone. Benchmarking Incremental Learning of Situation and Language Model using a Text-Simulated Situated Environment](https://arxiv.org/abs/2502.11733)
Append: [Advancing Language Multi-Agent Learning with Credit Re-Assignment for Interactive Environment Generalization](https://arxiv.org/abs/2502.14496)
Append: [Round Attention: A Novel Round-Level Attention Mechanism to Accelerate LLM Inference](https://arxiv.org/abs/2502.15294)
Append: [Detecting Knowledge Boundary of Vision Large Language Models by Sampling-Based Inference](https://arxiv.org/abs/2502.18023)
Append: [English K_Quantization of LLMs Does Not Disproportionately Diminish Multilingual Performance](https://arxiv.org/abs/2503.03592)
Append: [TableLoRA: Low-rank Adaptation on Table Structure Understanding for Large Language Models](https://arxiv.org/abs/2503.04396)
Append: [Grammar and Gameplay-aligned RL for Game Description Generation with LLMs](https://arxiv.org/abs/2503.15783)
Append: [MMCR: Benchmarking Cross-Source Reasoning in Scientific Papers](https://arxiv.org/abs/2503.16856)
Append: [Benchmarking Vision Language Models on German Factual Data](https://arxiv.org/abs/2504.11108)
Append: [Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs](https://arxiv.org/abs/2505.02862)
Append: [Llama See, Llama Do: A Mechanistic Perspective on Contextual Entrainment and Distraction in LLMs](https://arxiv.org/abs/2505.09338)
Append: [EasyDistill: A Comprehensive Toolkit for Effective Knowledge Distillation of Large Language Models](https://arxiv.org/abs/2505.20888)
Append: [MMBoundary: Advancing MLLM Knowledge Boundary Awareness through Reasoning Step Confidence Calibration](https://arxiv.org/abs/2505.23224)
Append: [EUR-USD Exchange Rate Forecasting Based on Information Fusion with Large Language Models and Deep Learning Methods](https://arxiv.org/abs/2408.13214)
Append: [Federated Data-Efficient Instruction Tuning for Large Language Models](https://arxiv.org/abs/2410.10926)
Append: [Robust Detection of Watermarks for Large Language Models Under Human Edits](https://arxiv.org/abs/2411.13868)
Append: [OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse Task Synthesis](https://arxiv.org/abs/2412.19723)
Append: [Embedding-based Approaches to Hyperpartisan News Detection](https://arxiv.org/abs/2501.01370)
Append: [Jailbreaking Multimodal Large Language Models via Shuffle Inconsistency](https://arxiv.org/abs/2501.04931)
Append: [KITAB-Bench: A Comprehensive Multi-Domain Benchmark for Arabic OCR and Document Understanding](https://arxiv.org/abs/2502.14949)
Append: [Multi-Turn Code Generation Through Single-Step Rewards](https://arxiv.org/abs/2502.20380)
Append: [Collective Reasoning Among LLMs: A Framework for Answer Validation Without Ground Truth](https://arxiv.org/abs/2502.20758)
Append: [LLM as GNN: Graph Vocabulary Learning for Text-Attributed Graph Foundation Models](https://arxiv.org/abs/2503.03313)
Append: [BeamLLM: Vision-Empowered mmWave Beam Prediction with Large Language Models](https://arxiv.org/abs/2503.10432)
Append: [ScienceBoard: Evaluating Multimodal Autonomous Agents in Realistic Scientific Workflows](https://arxiv.org/abs/2505.19897)
Append: [VLM@school -- Evaluation of AI image understanding on German middle school knowledge](https://arxiv.org/abs/2506.11604)
append_entries: 173
Finish: 2025-06-30 04:35:38.790240
------------------------------------------------------
Started: 2025-06-30 06:26:56.161717
Existing_entries: 1173
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-30 06:26:56.536462
------------------------------------------------------
Started: 2025-06-30 08:24:23.231043
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-30 08:24:23.654915
------------------------------------------------------
Started: 2025-06-30 10:18:58.755291
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-30 10:18:59.135825
------------------------------------------------------
Started: 2025-06-30 12:35:21.249566
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-30 12:35:21.623364
------------------------------------------------------
Started: 2025-06-30 14:16:57.273251
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-30 14:16:57.639950
------------------------------------------------------
Started: 2025-06-30 16:21:11.085562
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-30 16:21:11.463086
------------------------------------------------------
Started: 2025-06-30 18:24:11.625093
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-30 18:24:12.004731
------------------------------------------------------
Started: 2025-06-30 20:18:10.522437
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-30 20:18:10.953977
------------------------------------------------------
Started: 2025-06-30 22:16:26.251686
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-30 22:16:26.652223
------------------------------------------------------
Started: 2025-07-01 01:39:43.105749
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-01 01:39:43.505100
------------------------------------------------------
Started: 2025-07-01 03:27:43.870246
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-01 03:27:44.244608
------------------------------------------------------
Started: 2025-07-01 04:38:57.791640
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1713
Summarized using GPT-3.5-turbo
Append: [Psycholinguistic Word Features: a New Approach for the Evaluation of LLMs Alignment with Humans](https://arxiv.org/abs/2506.22439)
Token length: 1604
Summarized using GPT-3.5-turbo
Append: [AI Agents-as-Judge: Automated Assessment of Accuracy, Consistency, Completeness and Clarity for Enterprise Documents](https://arxiv.org/abs/2506.22485)
Token length: 1487
Summarized using GPT-3.5-turbo
Append: [Hallucination Detection with Small Language Models](https://arxiv.org/abs/2506.22486)
Token length: 1622
Summarized using GPT-3.5-turbo
Append: [PromptAug: Fine-grained Conflict Classification Using Data Augmentation](https://arxiv.org/abs/2506.22491)
Token length: 1545
Summarized using GPT-3.5-turbo
Append: [AgentStealth: Reinforcing Large Language Model for Anonymizing User-generated Text](https://arxiv.org/abs/2506.22508)
Token length: 1729
Summarized using GPT-3.5-turbo
Append: [Towards Text-free Graph Foundation Models: Rethinking Multi-Domain Graph Contrastive Learning](https://arxiv.org/abs/2506.22510)
Token length: 1476
Summarized using GPT-3.5-turbo
Append: [Can "consciousness" be observed from large language model (LLM) internal states? Dissecting LLM representations obtained from Theory of Mind test with Integrated Information Theory and Span Representation analysis](https://arxiv.org/abs/2506.22516)
Token length: 1391
Summarized using GPT-3.5-turbo
Append: [Weak-to-Strong GraphRAG: Aligning Weak Retrievers with Large Language Models for Graph-based Retrieval Augmented Generation](https://arxiv.org/abs/2506.22518)
Token length: 1473
Summarized using GPT-3.5-turbo
Append: [MisinfoTeleGraph: Network-driven Misinformation Detection for German Telegram Messages](https://arxiv.org/abs/2506.22529)
Token length: 1495
Summarized using GPT-3.5-turbo
Append: [RExBench: Can coding agents autonomously implement AI research extensions?](https://arxiv.org/abs/2506.22598)
Token length: 1178
Summarized using GPT-3.5-turbo
Append: [Temperature Matters: Enhancing Watermark Robustness Against Paraphrasing Attacks](https://arxiv.org/abs/2506.22623)
Token length: 1257
Summarized using GPT-3.5-turbo
Append: [Evaluating Hybrid Retrieval Augmented Generation using Dynamic Test Sets: LiveRAG Challenge](https://arxiv.org/abs/2506.22644)
Token length: 1270
Summarized using GPT-3.5-turbo
Append: [Assessing the feasibility of Large Language Models for detecting micro-behaviors in team interactions during space missions](https://arxiv.org/abs/2506.22679)
Token length: 1678
Summarized using GPT-3.5-turbo
Append: [VOCABTRIM: Vocabulary Pruning for Efficient Speculative Decoding in LLMs](https://arxiv.org/abs/2506.22694)
Token length: 1766
Summarized using GPT-3.5-turbo
Append: [Text Production and Comprehension by Human and Artificial Intelligence: Interdisciplinary Workshop Report](https://arxiv.org/abs/2506.22698)
Token length: 1239
Summarized using GPT-3.5-turbo
Append: [The Translation Barrier Hypothesis: Multilingual Generation with Large Language Models Suffers from Implicit Translation Failure](https://arxiv.org/abs/2506.22724)
Token length: 733
Summarized using GPT-3.5-turbo
Append: [Jan-nano Technical Report](https://arxiv.org/abs/2506.22760)
Token length: 1530
Summarized using GPT-3.5-turbo
Append: [Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning](https://arxiv.org/abs/2506.22777)
Token length: 1128
Summarized using GPT-3.5-turbo
Append: [ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in Large Language Models](https://arxiv.org/abs/2506.22791)
Token length: 1131
Summarized using GPT-3.5-turbo
Append: [MedEthicsQA: A Comprehensive Question Answering Benchmark for Medical Ethics Evaluation of LLMs](https://arxiv.org/abs/2506.22808)
Token length: 1410
Summarized using GPT-3.5-turbo
Append: [Selecting and Merging: Towards Adaptable and Scalable Named Entity Recognition with Large Language Models](https://arxiv.org/abs/2506.22813)
Token length: 1288
Summarized using GPT-3.5-turbo
Append: [Boosting CTC-Based ASR Using LLM-Based Intermediate Loss Regularization](https://arxiv.org/abs/2506.22846)
Token length: 1422
Summarized using GPT-3.5-turbo
Append: [Knowledge Augmented Finetuning Matters in both RAG and Agent Based Dialog Systems](https://arxiv.org/abs/2506.22852)
Token length: 1131
Summarized using GPT-3.5-turbo
Append: [DICE-BENCH: Evaluating the Tool-Use Capabilities of Large Language Models in Multi-Round, Multi-Party Dialogues](https://arxiv.org/abs/2506.22853)
Token length: 1345
Summarized using GPT-3.5-turbo
Append: [Mind the Gap: Entity-Preserved Context-Aware ASR Structured Transcriptions](https://arxiv.org/abs/2506.22858)
Token length: 1568
Summarized using GPT-3.5-turbo
Append: [Agent-to-Agent Theory of Mind: Testing Interlocutor Awareness among Large Language Models](https://arxiv.org/abs/2506.22957)
Token length: 1686
Summarized using GPT-3.5-turbo
Append: [On the Generalizability of "Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals"](https://arxiv.org/abs/2506.22977)
Token length: 904
Summarized using GPT-3.5-turbo
Append: [A Systematic Study of Compositional Syntactic Transformer Language Models](https://arxiv.org/abs/2506.22978)
Token length: 1836
Summarized using GPT-3.5-turbo
Append: [SoMi-ToM: Evaluating Multi-Perspective Theory of Mind in Embodied Social Interactions](https://arxiv.org/abs/2506.23046)
Token length: 985
Summarized using GPT-3.5-turbo
Append: [MariNER: A Dataset for Historical Brazilian Portuguese Named Entity Recognition](https://arxiv.org/abs/2506.23051)
Token length: 1234
Summarized using GPT-3.5-turbo
Append: [Boosting LLM's Molecular Structure Elucidation with Knowledge Enhanced Tree Search Reasoning](https://arxiv.org/abs/2506.23056)
Token length: 1475
Summarized using GPT-3.5-turbo
Append: [Text2VectorSQL: Bridging Text-to-SQL and Vector Search for Unified Natural Language Queries](https://arxiv.org/abs/2506.23071)
Token length: 1350
Summarized using GPT-3.5-turbo
Append: [From Individuals to Interactions: Benchmarking Gender Bias in Multimodal Large Language Models from the Lens of Social Relationship](https://arxiv.org/abs/2506.23101)
Token length: 1402
Summarized using GPT-3.5-turbo
Append: [FairI Tales: Evaluation of Fairness in Indian Contexts with a Focus on Bias and Stereotypes](https://arxiv.org/abs/2506.23111)
Token length: 1619
Summarized using GPT-3.5-turbo
Append: [Decoding Memes: Benchmarking Narrative Role Classification across Multilingual and Multimodal Models](https://arxiv.org/abs/2506.23122)
Token length: 1374
Summarized using GPT-3.5-turbo
Append: [Unleashing Embodied Task Planning Ability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2506.23127)
Token length: 983
Summarized using GPT-3.5-turbo
Append: [Format-Adapter: Improving Reasoning Capability of LLMs by Adapting Suitable Format](https://arxiv.org/abs/2506.23133)
Token length: 1392
Summarized using GPT-3.5-turbo
Append: [LLM-Assisted Question-Answering on Technical Documents Using Structured Data-Aware Retrieval Augmented Generation](https://arxiv.org/abs/2506.23136)
Token length: 1164
Summarized using GPT-3.5-turbo
Append: [Flow-Modulated Scoring for Semantic-Aware Knowledge Graph Completion](https://arxiv.org/abs/2506.23137)
Token length: 1258
Summarized using GPT-3.5-turbo
Append: [Benchmarking Deep Search over Heterogeneous Enterprise Data](https://arxiv.org/abs/2506.23139)
Token length: 1463
Summarized using GPT-3.5-turbo
Append: [Learning-to-Context Slope: Evaluating In-Context Learning Effectiveness Beyond Performance Illusions](https://arxiv.org/abs/2506.23146)
Token length: 1058
Summarized using GPT-3.5-turbo
Append: [V-SYNTHESIS: Task-Agnostic Synthesis of Consistent and Diverse In-Context Demonstrations from Scratch via V-Entropy](https://arxiv.org/abs/2506.23149)
Token length: 1650
Summarized using GPT-3.5-turbo
Append: [RiverText: A Python Library for Training and Evaluating Incremental Word Embeddings from Text Data Streams](https://arxiv.org/abs/2506.23192)
Token length: 1522
Summarized using GPT-3.5-turbo
Append: [Generalist Reward Models: Found Inside Large Language Models](https://arxiv.org/abs/2506.23235)
Token length: 858
Summarized using GPT-3.5-turbo
Append: [Two Spelling Normalization Approaches Based on Large Language Models](https://arxiv.org/abs/2506.23288)
Token length: 1958
Summarized using GPT-3.5-turbo
Append: [Objective-Free Local Learning and Emergent Language Structure in Thinking Machines](https://arxiv.org/abs/2506.23293)
Token length: 1270
Summarized using GPT-3.5-turbo
Append: [Ensemble BERT for Medication Event Classification on Electronic Health Records (EHRs)](https://arxiv.org/abs/2506.23315)
Token length: 1400
Summarized using GPT-3.5-turbo
Append: [Information Loss in LLMs' Multilingual Translation: The Role of Training Data, Language Proximity, and Language Family](https://arxiv.org/abs/2506.23340)
Token length: 1346
Summarized using GPT-3.5-turbo
Append: [ATGen: A Framework for Active Text Generation](https://arxiv.org/abs/2506.23342)
Token length: 1185
Summarized using GPT-3.5-turbo
Append: [Perspective Dial: Measuring Perspective of Text and Guiding LLM Outputs](https://arxiv.org/abs/2506.23377)
Append: [Hierarchical Memory Organization for Wikipedia Generation](https://arxiv.org/abs/2506.23393)
Append: [Datasets for Fairness in Language Models: An In-Depth Survey](https://arxiv.org/abs/2506.23411)
Append: [TuCo: Measuring the Contribution of Fine-Tuning to Individual Responses of LLMs](https://arxiv.org/abs/2506.23423)
Append: [Pipelined Decoder for Efficient Context-Aware Text Generation](https://arxiv.org/abs/2506.23431)
Append: [What to Keep and What to Drop: Adaptive Table Filtering Framework](https://arxiv.org/abs/2506.23463)
Append: [Thought-Augmented Planning for LLM-Powered Interactive Recommender Agent](https://arxiv.org/abs/2506.23485)
Append: [Reinforcement Fine-Tuning Enables MLLMs Learning Novel Tasks Stably](https://arxiv.org/abs/2506.23508)
Append: [NEU-ESC: A Comprehensive Vietnamese dataset for Educational Sentiment analysis and topic Classification toward multitask learning](https://arxiv.org/abs/2506.23524)
Append: [On Recipe Memorization and Creativity in Large Language Models: Is Your Model a Creative Cook, a Bad Cook, or Merely a Plagiator?](https://arxiv.org/abs/2506.23527)
Append: [Semantic-guided Diverse Decoding for Large Language Model](https://arxiv.org/abs/2506.23601)
Append: [Evaluating the Simulation of Human Personality-Driven Susceptibility to Misinformation with LLMs](https://arxiv.org/abs/2506.23610)
Append: [Robustness of Misinformation Classification Systems to Adversarial Examples Through BeamAttack](https://arxiv.org/abs/2506.23661)
Append: [Zero-Shot Contextual Embeddings via Offline Synthetic Corpus Generation](https://arxiv.org/abs/2506.23662)
Append: [L0: Reinforcement Learning to Become General Agents](https://arxiv.org/abs/2506.23667)
Append: [AutoEvoEval: An Automated Framework for Evolving Close-Ended LLM Evaluation Data](https://arxiv.org/abs/2506.23735)
Append: [Positional Bias in Binary Question Answering: How Uncertainty Shapes Model Preferences](https://arxiv.org/abs/2506.23743)
Append: [Do Thinking Tokens Help or Trap? Towards More Efficient Large Reasoning Model](https://arxiv.org/abs/2506.23840)
Append: [Garbage In, Reasoning Out? Why Benchmark Scores are Unreliable and What to Do About It](https://arxiv.org/abs/2506.23864)
Append: [Advancing Multi-Step Mathematical Reasoning in Large Language Models through Multi-Layered Self-Reflection with Auto-Prompting](https://arxiv.org/abs/2506.23888)
Append: [The Trilemma of Truth in Large Language Models](https://arxiv.org/abs/2506.23921)
Append: [IMPACT: Inflectional Morphology Probes Across Complex Typologies](https://arxiv.org/abs/2506.23929)
Append: [Leveraging the Potential of Prompt Engineering for Hate Speech Detection in Low-Resource Languages](https://arxiv.org/abs/2506.23930)
Append: [Graft: Integrating the Domain Knowledge via Efficient Parameter Synergy for MLLMs](https://arxiv.org/abs/2506.23940)
Append: [Unveiling Decision-Making in LLMs for Text Classification : Extraction of influential and interpretable concepts with Sparse Autoencoders](https://arxiv.org/abs/2506.23951)
Append: [TaP: A Taxonomy-Guided Framework for Automated and Scalable Preference Data Generation](https://arxiv.org/abs/2506.23979)
Append: [Machine Understanding of Scientific Language](https://arxiv.org/abs/2506.23990)
Append: [Auto-TA: Towards Scalable Automated Thematic Analysis (TA) via Multi-Agent Large Language Models with Reinforcement Learning](https://arxiv.org/abs/2506.23998)
Append: [Large Language Models Don't Make Sense of Word Problems. A Scoping Review from a Mathematics Education Perspective](https://arxiv.org/abs/2506.24006)
Append: [EXPERT: An Explainable Image Captioning Evaluation Metric with Structured Explanations](https://arxiv.org/abs/2506.24016)
Append: [STACK: Adversarial Attacks on LLM Safeguard Pipelines](https://arxiv.org/abs/2506.24068)
Append: [On the Predictive Power of Representation Dispersion in Language Models](https://arxiv.org/abs/2506.24106)
Append: [Computational Detection of Intertextual Parallels in Biblical Hebrew: A Benchmark Study Using Transformer-Based Language Models](https://arxiv.org/abs/2506.24117)
Append: [Computational Analysis of Climate Policy](https://arxiv.org/abs/2506.22449)
Append: [Theories of "Sexuality" in Natural Language Processing Bias Research](https://arxiv.org/abs/2506.22481)
Append: [A Detailed Factor Analysis for the Political Compass Test: Navigating Ideologies of Large Language Models](https://arxiv.org/abs/2506.22493)
Append: [Mitigating Gambling-Like Risk-Taking Behaviors in Large Language Models: A Behavioral Economics Approach to AI Safety](https://arxiv.org/abs/2506.22496)
Append: [VERA: Variational Inference Framework for Jailbreaking Large Language Models](https://arxiv.org/abs/2506.22666)
Append: [Residual Matrix Transformers: Scaling the Size of the Residual Stream](https://arxiv.org/abs/2506.22696)
Append: [BEST-Route: Adaptive LLM Routing with Test-Time Optimal Compute](https://arxiv.org/abs/2506.22716)
Append: [PhonemeFake: Redefining Deepfake Realism with Language-Driven Segmental Manipulation and Adaptive Bilevel Detection](https://arxiv.org/abs/2506.22783)
Append: [BayesLoRA: Task-Specific Uncertainty in Low-Rank Adapters](https://arxiv.org/abs/2506.22809)
Append: [Mask-aware Text-to-Image Retrieval: Referring Expression Segmentation Meets Cross-modal Retrieval](https://arxiv.org/abs/2506.22864)
Append: [MOTOR: Multimodal Optimal Transport via Grounded Retrieval in Medical Visual Question Answering](https://arxiv.org/abs/2506.22900)
Append: [MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning](https://arxiv.org/abs/2506.22992)
Append: [AURA: Agent for Understanding, Reasoning, and Automated Tool Use in Voice-Driven Tasks](https://arxiv.org/abs/2506.23049)
Append: [MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings](https://arxiv.org/abs/2506.23115)
Append: [UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence with Spatial Reasoning and Understanding](https://arxiv.org/abs/2506.23219)
Append: [Masked Gated Linear Unit](https://arxiv.org/abs/2506.23225)
Append: [Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games](https://arxiv.org/abs/2506.23276)
Append: [GaussMaster: An LLM-based Database Copilot System](https://arxiv.org/abs/2506.23322)
Append: [Density, asymmetry and citation dynamics in scientific literature](https://arxiv.org/abs/2506.23366)
Append: [You Sound a Little Tense: L2 Tailored Clear TTS Using Durational Vowel Properties](https://arxiv.org/abs/2506.23367)
Append: [Teaching a Language Model to Speak the Language of Tools](https://arxiv.org/abs/2506.23394)
Append: [Assessing GPTZero's Accuracy in Identifying AI vs. Human-Written Essays](https://arxiv.org/abs/2506.23517)
Append: [MMReason: An Open-Ended Multi-Modal Multi-Step Reasoning Benchmark for MLLMs Toward AGI](https://arxiv.org/abs/2506.23563)
Append: [Reachability in symmetric VASS](https://arxiv.org/abs/2506.23578)
Append: [Efficient Interleaved Speech Modeling through Knowledge Distillation](https://arxiv.org/abs/2506.23670)
Append: [Attestable Audits: Verifiable AI Safety Benchmarks Using Trusted Execution Environments](https://arxiv.org/abs/2506.23706)
Append: [Towards an Automated Multimodal Approach for Video Summarization: Building a Bridge Between Text, Audio and Facial Cue-Based Summarization](https://arxiv.org/abs/2506.23714)
Append: [Use Sparse Autoencoders to Discover Unknown Concepts, Not to Act on Known Concepts](https://arxiv.org/abs/2506.23845)
Append: [LLM Agents Are the Antidote to Walled Gardens](https://arxiv.org/abs/2506.23978)
Append: [Ella: Embodied Social Agents with Lifelong Memory](https://arxiv.org/abs/2506.24019)
Append: [Logit-Gap Steering: Efficient Short-Suffix Jailbreaks for Aligned Large Language Models](https://arxiv.org/abs/2506.24056)
Append: [MotionGPT3: Human Motion as a Second Modality](https://arxiv.org/abs/2506.24086)
Append: [SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2506.24119)
Append: [Scaling Data-Constrained Language Models](https://arxiv.org/abs/2305.16264)
Append: [RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing](https://arxiv.org/abs/2404.19543)
Append: [The Effectiveness of LLMs as Annotators: A Comparative Overview and Empirical Analysis of Direct Representation](https://arxiv.org/abs/2405.01299)
Append: [Benchmarking Uncertainty Quantification Methods for Large Language Models with LM-Polygraph](https://arxiv.org/abs/2406.15627)
Append: [Enhancing the Capability and Robustness of Large Language Models through Reinforcement Learning-Driven Query Refinement](https://arxiv.org/abs/2407.01461)
Append: [ChipXplore: Natural Language Exploration of Hardware Designs and Libraries](https://arxiv.org/abs/2407.12749)
Append: [Tracing Intricate Cues in Dialogue: Joint Graph Structure and Sentiment Dynamics for Multimodal Emotion Recognition](https://arxiv.org/abs/2407.21536)
Append: [CTISum: A New Benchmark Dataset For Cyber Threat Intelligence Summarization](https://arxiv.org/abs/2408.06576)
Append: [Emotional RAG LLMs: Reading Comprehension for the Open Internet](https://arxiv.org/abs/2408.11189)
Append: [S^3cMath: Spontaneous Step-level Self-correction Makes Large Language Models Better Mathematical Reasoners](https://arxiv.org/abs/2409.01524)
Append: [Kalahi: A handcrafted, grassroots cultural LLM evaluation suite for Filipino](https://arxiv.org/abs/2409.15380)
Append: [Margin Matching Preference Optimization: Enhanced Model Alignment with Granular Feedback](https://arxiv.org/abs/2410.03145)
Append: [Which Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?](https://arxiv.org/abs/2410.06735)
Append: [Sample then Identify: A General Framework for Risk Control and Assessment in Multimodal Large Language Models](https://arxiv.org/abs/2410.08174)
Append: [Parenting: Optimizing Knowledge Selection of Retrieval-Augmented Language Models with Parameter Decoupling and Tailored Tuning](https://arxiv.org/abs/2410.10360)
Append: [Beware of Calibration Data for Pruning Large Language Models](https://arxiv.org/abs/2410.17711)
Append: [Bridge: A Unified Framework to Knowledge Graph Completion via Language Models and Knowledge Representation](https://arxiv.org/abs/2411.06660)
Append: [The Limited Impact of Medical Adaptation of Large Language and Vision-Language Models](https://arxiv.org/abs/2411.08870)
Append: [MLAN: Language-Based Instruction Tuning Preserves and Transfers Knowledge in Multimodal Language Models](https://arxiv.org/abs/2411.10557)
Append: [A Context-aware Framework for Translation-mediated Conversations](https://arxiv.org/abs/2412.04205)
Append: [Reasoner Outperforms: Generative Stance Detection with Rationalization for Social Media](https://arxiv.org/abs/2412.10266)
Append: [Interpretable LLM-based Table Question Answering](https://arxiv.org/abs/2412.12386)
Append: [Computational Analysis of Character Development in Holocaust Testimonies](https://arxiv.org/abs/2412.17063)
Append: [Multimodal Contrastive Representation Learning in Augmented Biomedical Knowledge Graphs](https://arxiv.org/abs/2501.01644)
Append: [PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models](https://arxiv.org/abs/2501.03124)
Append: [Know Your Mistakes: Towards Preventing Overreliance on Task-Oriented Conversational AI Through Accountability Modeling](https://arxiv.org/abs/2501.10316)
Append: [Multimodal Medical Code Tokenizer](https://arxiv.org/abs/2502.04397)
Append: [Mechanistic Interpretability of Emotion Inference in Large Language Models](https://arxiv.org/abs/2502.05489)
Append: [KMI: A Dataset of Korean Motivational Interviewing Dialogues for Psychotherapy](https://arxiv.org/abs/2502.05651)
Append: [Demystifying Singular Defects in Large Language Models](https://arxiv.org/abs/2502.07004)
Append: [Organize the Web: Constructing Domains Enhances Pre-Training Data Curation](https://arxiv.org/abs/2502.10341)
Append: [Agentic Medical Knowledge Graphs Enhance Medical Question Answering: Bridging the Gap Between LLMs and Evolving Medical Knowledge](https://arxiv.org/abs/2502.13010)
Append: [Finding the Sweet Spot: Preference Data Construction for Scaling Preference Optimization](https://arxiv.org/abs/2502.16825)
Append: [Better Aligned with Survey Respondents or Training Data? Unveiling Political Leanings of LLMs on U.S. Supreme Court Cases](https://arxiv.org/abs/2502.18282)
Append: [What Makes the Preferred Thinking Direction for LLMs in Multiple-choice Questions?](https://arxiv.org/abs/2502.18435)
Append: [Know You First and Be You Better: Modeling Human-Like User Simulators via Implicit Profiles](https://arxiv.org/abs/2502.18968)
Append: [Time-MQA: Time Series Multi-Task Question Answering with Context Enhancement](https://arxiv.org/abs/2503.01875)
Append: [Enough Coin Flips Can Make LLMs Act Bayesian](https://arxiv.org/abs/2503.04722)
Append: [Gumiho: A Hybrid Architecture to Prioritize Early Tokens in Speculative Decoding](https://arxiv.org/abs/2503.10135)
Append: [TigerLLM -- A Family of Bangla Large Language Models](https://arxiv.org/abs/2503.10995)
Append: [Explainable Sentiment Analysis with DeepSeek-R1: Performance, Efficiency, and Few-Shot Learning](https://arxiv.org/abs/2503.11655)
Append: [LLM Braces: Straightening Out LLM Predictions with Relevant Sub-Updates](https://arxiv.org/abs/2503.16334)
Append: [Automating Adjudication of Cardiovascular Events Using Large Language Models](https://arxiv.org/abs/2503.17222)
Append: [Can LLMs Interpret and Leverage Structured Linguistic Representations? A Case Study with AMRs](https://arxiv.org/abs/2504.04745)
Append: [MetaSynth: Meta-Prompting-Driven Agentic Scaffolds for Diverse Synthetic Data Generation](https://arxiv.org/abs/2504.12563)
Append: [SConU: Selective Conformal Uncertainty in Large Language Models](https://arxiv.org/abs/2504.14154)
Append: [TTRL: Test-Time Reinforcement Learning](https://arxiv.org/abs/2504.16084)
Append: [Truth Neurons](https://arxiv.org/abs/2505.12182)
Append: [CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement Learning](https://arxiv.org/abs/2505.13271)
Append: [WebDancer: Towards Autonomous Information Seeking Agency](https://arxiv.org/abs/2505.22648)
Append: [ScienceMeter: Tracking Scientific Knowledge Updates in Language Models](https://arxiv.org/abs/2505.24302)
Append: [GeometryZero: Improving Geometry Solving for LLM with Group Contrastive Policy Optimization](https://arxiv.org/abs/2506.07160)
Append: [Brevity is the soul of sustainability: Characterizing LLM response lengths](https://arxiv.org/abs/2506.08686)
Append: [Improved Supervised Fine-Tuning for Large Language Models to Mitigate Catastrophic Forgetting](https://arxiv.org/abs/2506.09428)
Append: [From Outcomes to Processes: Guiding PRM Learning from ORM for Inference-Time Alignment](https://arxiv.org/abs/2506.12446)
Append: [FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented Generation](https://arxiv.org/abs/2506.12494)
Append: [Enabling Precise Topic Alignment in Large Language Models Via Sparse Autoencoders](https://arxiv.org/abs/2506.12576)
Append: [MMInA: Benchmarking Multihop Multimodal Internet Agents](https://arxiv.org/abs/2404.09992)
Append: [Learning Dynamics of LLM Finetuning](https://arxiv.org/abs/2407.10490)
Append: [CHARTOM: A Visual Theory-of-Mind Benchmark for LLMs on Misleading Charts](https://arxiv.org/abs/2408.14419)
Append: [Empirical evidence of Large Language Model's influence on human spoken communication](https://arxiv.org/abs/2409.01754)
Append: [FedEx-LoRA: Exact Aggregation for Federated and Efficient Fine-Tuning of Foundation Models](https://arxiv.org/abs/2410.09432)
Append: [Creativity in AI: Progresses and Challenges](https://arxiv.org/abs/2410.17218)
Append: [Evaluating K-Fold Cross Validation for Transformer Based Symbolic Regression Models](https://arxiv.org/abs/2410.21896)
Append: [Sparsing Law: Towards Large Language Models with Greater Activation Sparsity](https://arxiv.org/abs/2411.02335)
Append: [PriorDiffusion: Leverage Language Prior in Diffusion Models for Monocular Depth Estimation](https://arxiv.org/abs/2411.16750)
Append: [SEUF: Is Unlearning One Expert Enough for Mixture-of-Experts LLMs?](https://arxiv.org/abs/2411.18797)
Append: [A Survey of Test-Time Compute: From Intuitive Inference to Deliberate Reasoning](https://arxiv.org/abs/2501.02497)
Append: [DReSS: Data-driven Regularized Structured Streamlining for Large Language Models](https://arxiv.org/abs/2501.17905)
Append: [Riddle Me This! Stealthy Membership Inference for Retrieval-Augmented Generation](https://arxiv.org/abs/2502.00306)
Append: [AutoToM: Scaling Model-based Mental Inference via Automated Agent Modeling](https://arxiv.org/abs/2502.15676)
Append: [I see what you mean: Co-Speech Gestures for Reference Resolution in Multimodal Dialogue](https://arxiv.org/abs/2503.00071)
Append: [What can large language models do for sustainable food?](https://arxiv.org/abs/2503.04734)
Append: [Time-R1: Post-Training Large Vision Language Model for Temporal Video Grounding](https://arxiv.org/abs/2503.13377)
Append: [Redefining Evaluation Standards: A Unified Framework for Evaluating the Korean Capabilities of Language Models](https://arxiv.org/abs/2503.22968)
Append: [Distillation and Refinement of Reasoning in Small Language Models for Document Re-ranking](https://arxiv.org/abs/2504.03947)
Append: [AI Awareness](https://arxiv.org/abs/2504.20084)
Append: [Knowing You Don't Know: Learning When to Continue Search in Multi-round RAG through Self-Practicing](https://arxiv.org/abs/2505.02811)
Append: [LibVulnWatch: A Deep Assessment Agent System and Leaderboard for Uncovering Hidden Vulnerabilities in Open-Source AI Libraries](https://arxiv.org/abs/2505.08842)
Append: [From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data](https://arxiv.org/abs/2505.20166)
Append: [Detecting Sockpuppetry on Wikipedia Using Meta-Learning](https://arxiv.org/abs/2506.10314)
Append: [Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption Masking And Normalization](https://arxiv.org/abs/2506.12484)
append_entries: 197
Finish: 2025-07-01 04:40:31.882388
------------------------------------------------------
Started: 2025-07-01 06:27:03.565952
Existing_entries: 1197
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1644
Summarized using GPT-3.5-turbo
Append: [PromptDSI: Prompt-based Rehearsal-free Continual Learning for Document Retrieval](https://arxiv.org/abs/2406.12593)
append_entries: 1
Finish: 2025-07-01 06:27:05.377416
------------------------------------------------------
Started: 2025-07-01 08:23:41.911852
Existing_entries: 1001
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-01 08:23:42.350520
------------------------------------------------------
Started: 2025-07-01 10:19:05.883531
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-01 10:19:06.318197
------------------------------------------------------
Started: 2025-07-01 12:35:44.599181
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-01 12:35:45.044946
------------------------------------------------------
Started: 2025-07-01 14:16:38.085602
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-01 14:16:38.516479
------------------------------------------------------
Started: 2025-07-01 16:21:52.339365
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-01 16:21:52.786467
------------------------------------------------------
Started: 2025-07-01 18:23:41.902101
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-01 18:23:42.352963
------------------------------------------------------
Started: 2025-07-01 20:18:41.155430
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-01 20:18:41.605599
------------------------------------------------------
Started: 2025-07-01 22:16:09.668052
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-01 22:16:10.105198
------------------------------------------------------
Started: 2025-07-02 01:21:44.924818
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-02 01:21:45.396558
------------------------------------------------------
Started: 2025-07-02 03:16:43.841107
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-02 03:16:44.364444
------------------------------------------------------
Started: 2025-07-02 04:30:25.673133
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1121
Summarized using GPT-3.5-turbo
Append: [Table Understanding and (Multimodal) LLMs: A Cross-Domain Case Study on Scientific vs. Non-Scientific Data](https://arxiv.org/abs/2507.00152)
Token length: 820
Summarized using GPT-3.5-turbo
Append: [Prompting as Scientific Inquiry](https://arxiv.org/abs/2507.00163)
Token length: 1463
Summarized using GPT-3.5-turbo
Append: [LineRetriever: Planning-Aware Observation Reduction for Web Agents](https://arxiv.org/abs/2507.00210)
Token length: 1640
Summarized using GPT-3.5-turbo
Append: [Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning](https://arxiv.org/abs/2507.00214)
Token length: 749
Summarized using GPT-3.5-turbo
Append: [Towards Style Alignment in Cross-Cultural Translation](https://arxiv.org/abs/2507.00216)
Token length: 1727
Summarized using GPT-3.5-turbo
Append: [Linearly Decoding Refused Knowledge in Aligned Language Models](https://arxiv.org/abs/2507.00239)
Token length: 1179
Summarized using GPT-3.5-turbo
Append: [The Algebraic Structure of Morphosyntax](https://arxiv.org/abs/2507.00244)
Token length: 1059
Summarized using GPT-3.5-turbo
Append: [EfficientXLang: Towards Improving Token Efficiency Through Cross-Lingual Reasoning](https://arxiv.org/abs/2507.00246)
Token length: 979
Summarized using GPT-3.5-turbo
Append: [Impact of Fine-Tuning Methods on Memorization in Large Language Models](https://arxiv.org/abs/2507.00258)
Token length: 1716
Summarized using GPT-3.5-turbo
Append: [Natural language processing for African languages](https://arxiv.org/abs/2507.00297)
Token length: 1377
Summarized using GPT-3.5-turbo
Append: [Failure by Interference: Language Models Make Balanced Parentheses Errors When Faulty Mechanisms Overshadow Sound Ones](https://arxiv.org/abs/2507.00322)
Token length: 1135
Summarized using GPT-3.5-turbo
Append: [Modeling Data Diversity for Joint Instance and Verbalizer Selection in Cold-Start Scenarios](https://arxiv.org/abs/2507.00330)
Token length: 1674
Summarized using GPT-3.5-turbo
Append: [Question Decomposition for Retrieval-Augmented Generation](https://arxiv.org/abs/2507.00355)
Token length: 1342
Summarized using GPT-3.5-turbo
Append: [Gregorian melody, modality, and memory: Segmenting chant with Bayesian nonparametrics](https://arxiv.org/abs/2507.00380)
Token length: 1484
Summarized using GPT-3.5-turbo
Append: [Causal Prompting for Implicit Sentiment Analysis with Large Language Models](https://arxiv.org/abs/2507.00389)
Token length: 802
Summarized using GPT-3.5-turbo
Append: [Beyond Sociodemographic Prompting: Using Supervision to Align LLMs with Human Response Distributions](https://arxiv.org/abs/2507.00439)
Token length: 1018
Summarized using GPT-3.5-turbo
Append: [Pitfalls of Evaluating Language Models with Open Benchmarks](https://arxiv.org/abs/2507.00460)
Token length: 1831
Summarized using GPT-3.5-turbo
Append: [TeamCMU at Touch\'e: Adversarial Co-Evolution for Advertisement Integration and Detection in Conversational Search](https://arxiv.org/abs/2507.00509)
Token length: 981
Summarized using GPT-3.5-turbo
Append: [NIRANTAR: Continual Learning with New Languages and Domains on Real-world Speech Data](https://arxiv.org/abs/2507.00534)
Token length: 1619
Summarized using GPT-3.5-turbo
Append: [Capsule Network-Based Semantic Intent Modeling for Human-Computer Interaction](https://arxiv.org/abs/2507.00540)
Token length: 1229
Summarized using GPT-3.5-turbo
Append: [Methodological Rigour in Algorithm Application: An Illustration of Topic Modelling Algorithm](https://arxiv.org/abs/2507.00547)
Token length: 958
Summarized using GPT-3.5-turbo
Append: [TUM-MiKaNi at SemEval-2025 Task 3: Towards Multilingual and Knowledge-Aware Non-factual Hallucination Identification](https://arxiv.org/abs/2507.00579)
Token length: 1605
Summarized using GPT-3.5-turbo
Append: [Transferable Modeling Strategies for Low-Resource LLM Tasks: A Prompt and Alignment-Based](https://arxiv.org/abs/2507.00601)
Token length: 994
Summarized using GPT-3.5-turbo
Append: [Mixture of Reasonings: Teach Large Language Models to Reason with Adaptive Strategies](https://arxiv.org/abs/2507.00606)
Token length: 1344
Summarized using GPT-3.5-turbo
Append: [SAFER: Probing Safety in Reward Models with Sparse Autoencoder](https://arxiv.org/abs/2507.00665)
Token length: 999
Summarized using GPT-3.5-turbo
Append: [Contrasting Cognitive Styles in Vision-Language Models: Holistic Attention in Japanese Versus Analytical Focus in English](https://arxiv.org/abs/2507.00700)
Token length: 793
Summarized using GPT-3.5-turbo
Append: [AI Analyst: Framework and Comprehensive Evaluation of Large Language Models for Financial Time Series Report Generation](https://arxiv.org/abs/2507.00718)
Token length: 1543
Summarized using GPT-3.5-turbo
Append: [LitBench: A Benchmark and Dataset for Reliable Evaluation of Creative Writing](https://arxiv.org/abs/2507.00769)
Token length: 422
Summarized using GPT-3.5-turbo
Append: [A Diagrammatic Calculus for a Functional Model of Natural Language Semantics](https://arxiv.org/abs/2507.00782)
Token length: 1449
Summarized using GPT-3.5-turbo
Append: [Generative AI and the future of scientometrics: current topics and future questions](https://arxiv.org/abs/2507.00783)
Token length: 1690
Summarized using GPT-3.5-turbo
Append: [Many LLMs Are More Utilitarian Than One](https://arxiv.org/abs/2507.00814)
Token length: 968
Summarized using GPT-3.5-turbo
Append: [ProxAnn: Use-Oriented Evaluations of Topic Models and Document Clustering](https://arxiv.org/abs/2507.00828)
Token length: 1723
Summarized using GPT-3.5-turbo
Append: [Stylometry recognizes human and LLM-generated texts in short samples](https://arxiv.org/abs/2507.00838)
Token length: 1395
Summarized using GPT-3.5-turbo
Append: [TransLaw: Benchmarking Large Language Models in Multi-Agent Simulation of the Collaborative Translation](https://arxiv.org/abs/2507.00875)
Token length: 1044
Summarized using GPT-3.5-turbo
Append: [Mathematics Isn't Culture-Free: Probing Cultural Gaps via Entity and Scenario Perturbations](https://arxiv.org/abs/2507.00883)
Token length: 997
Summarized using GPT-3.5-turbo
Append: [Scaling Laws Are Unreliable for Downstream Tasks: A Reality Check](https://arxiv.org/abs/2507.00885)
Token length: 1024
Summarized using GPT-3.5-turbo
Append: [MemeCMD: An Automatically Generated Chinese Multi-turn Dialogue Dataset with Contextually Retrieved Memes](https://arxiv.org/abs/2507.00891)
Token length: 1147
Summarized using GPT-3.5-turbo
Append: [The Cognate Data Bottleneck in Language Phylogenetics](https://arxiv.org/abs/2507.00911)
Token length: 1420
Summarized using GPT-3.5-turbo
Append: [Discourse Heuristics For Paradoxically Moral Self-Correction](https://arxiv.org/abs/2507.00985)
Token length: 1583
Summarized using GPT-3.5-turbo
Append: [Should We Still Pretrain Encoders with Masked Language Modeling?](https://arxiv.org/abs/2507.00994)
Token length: 1173
Summarized using GPT-3.5-turbo
Append: [La Leaderboard: A Large Language Model Leaderboard for Spanish Varieties and Languages of Spain and Latin America](https://arxiv.org/abs/2507.00999)
Token length: 1545
Summarized using GPT-3.5-turbo
Append: [SciArena: An Open Evaluation Platform for Foundation Models in Scientific Literature Tasks](https://arxiv.org/abs/2507.01001)
Token length: 1132
Summarized using GPT-3.5-turbo
Append: [Hypertokens: Holographic Associative Memory in Tokenized LLMs](https://arxiv.org/abs/2507.00002)
Token length: 1502
Summarized using GPT-3.5-turbo
Append: [Implicit Reward as the Bridge: A Unified View of SFT and DPO Connections](https://arxiv.org/abs/2507.00018)
Token length: 751
Summarized using GPT-3.5-turbo
Append: [GLU Attention Improve Transformer](https://arxiv.org/abs/2507.00022)
Token length: 1759
Summarized using GPT-3.5-turbo
Append: [ROSE: Toward Reality-Oriented Safety Evaluation of Large Language Models](https://arxiv.org/abs/2507.00026)
Token length: 1471
Summarized using GPT-3.5-turbo
Append: [Moment Sampling in Video LLMs for Long-Form Video QA](https://arxiv.org/abs/2507.00033)
Token length: 1275
Summarized using GPT-3.5-turbo
Append: [CaughtCheating: Is Your MLLM a Good Cheating Detective? Exploring the Boundary of Visual Perception and Reasoning](https://arxiv.org/abs/2507.00045)
Token length: 1201
Summarized using GPT-3.5-turbo
Append: [Enhancing Reasoning Capabilities in SLMs with Reward Guided Dataset Distillation](https://arxiv.org/abs/2507.00054)
Token length: 1540
Summarized using GPT-3.5-turbo
Append: [MANTA: Cross-Modal Semantic Alignment and Information-Theoretic Optimization for Long-form Multimodal Understanding](https://arxiv.org/abs/2507.00068)
Append: [The language of time: a language model perspective on time-series foundation models](https://arxiv.org/abs/2507.00078)
Append: [State and Memory is All You Need for Robust and Reliable AI Agents](https://arxiv.org/abs/2507.00081)
Append: [Federated Learning-Enabled Hybrid Language Models for Communication-Efficient Token Transmission](https://arxiv.org/abs/2507.00082)
Append: [Thinking About Thinking: SAGE-nano's Inverse Reasoning for Self-Aware Language Models](https://arxiv.org/abs/2507.00092)
Append: [Interpretable AI for Time-Series: Multi-Model Heatmap Fusion with Global Attention and NLP-Generated Explanations](https://arxiv.org/abs/2507.00234)
Append: [Developing Lightweight DNN Models With Limited Data For Real-Time Sign Language Recognition](https://arxiv.org/abs/2507.00248)
Append: [Open-ended Scientific Discovery via Bayesian Surprise](https://arxiv.org/abs/2507.00310)
Append: [${\mu}^2$Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for Radiology Report Generation](https://arxiv.org/abs/2507.00316)
Append: [ASTRO: Teaching Language Models to Reason by Reflecting and Backtracking In-Context](https://arxiv.org/abs/2507.00417)
Append: [Flexible Language Modeling in Continuous Space with Transformer-based Autoregressive Flows](https://arxiv.org/abs/2507.00425)
Append: [Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning](https://arxiv.org/abs/2507.00432)
Append: [Overcoming Long-Context Limitations of State-Space Models via Context-Dependent Sparse Attention](https://arxiv.org/abs/2507.00449)
Append: [Beat and Downbeat Tracking in Performance MIDI Using an End-to-End Transformer Architecture](https://arxiv.org/abs/2507.00466)
Append: [MassTool: A Multi-Task Search-Based Tool Retrieval Framework for Large Language Models](https://arxiv.org/abs/2507.00487)
Append: [Leveraging Large Language Models for Spontaneous Speech-Based Suicide Risk Detection](https://arxiv.org/abs/2507.00693)
Append: [Safe Low Bandwidth SPV: A Formal Treatment of Simplified Payment Verification Protocols and Security Bounds](https://arxiv.org/abs/2507.00740)
Append: [Multi-interaction TTS toward professional recording reproduction](https://arxiv.org/abs/2507.00808)
Append: [Verifiable Natural Language to Linear Temporal Logic Translation: A Benchmark Dataset and Evaluation Suite](https://arxiv.org/abs/2507.00877)
Append: [ONLY: One-Layer Intervention Sufficiently Mitigates Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2507.00898)
Append: [Enhancing LLM Agent Safety via Causal Influence Prompting](https://arxiv.org/abs/2507.00979)
Append: [Quasi-symbolic Semantic Geometry over Transformer-based Variational AutoEncoder](https://arxiv.org/abs/2210.06230)
Append: [Can LLMs Evaluate Complex Attribution in QA? Automatic Benchmarking using Knowledge Graphs](https://arxiv.org/abs/2401.14640)
Append: [Large Language Model Confidence Estimation via Black-Box Access](https://arxiv.org/abs/2406.04370)
Append: [Evaluating Deduplication Techniques for Economic Research Paper Titles with a Focus on Semantic Similarity using NLP and LLMs](https://arxiv.org/abs/2410.01141)
Append: [Fact Recall, Heuristics or Pure Guesswork? Precise Interpretations of Language Models for Fact Completion](https://arxiv.org/abs/2410.14405)
Append: [ECG-Byte: A Tokenizer for End-to-End Generative Electrocardiogram Language Modeling](https://arxiv.org/abs/2412.14373)
Append: [BlockDialect: Block-wise Fine-grained Mixed Format Quantization for Energy-Efficient LLM Inference](https://arxiv.org/abs/2501.01144)
Append: [A Study of In-Context-Learning-Based Text-to-SQL Errors](https://arxiv.org/abs/2501.09310)
Append: [RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache Compression](https://arxiv.org/abs/2502.14051)
Append: [SAGE: Steering Dialog Generation with Future-Aware State-Action Augmentation](https://arxiv.org/abs/2503.03040)
Append: [SPADE: Structured Prompting Augmentation for Dialogue Enhancement in Machine-Generated Text Detection](https://arxiv.org/abs/2503.15044)
Append: [ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition](https://arxiv.org/abs/2503.21248)
Append: [An evaluation of LLMs and Google Translate for translation of selected Indian languages via sentiment and semantic analyses](https://arxiv.org/abs/2503.21393)
Append: [Efficient Domain-adaptive Continual Pretraining for the Process Industry in the German Language](https://arxiv.org/abs/2504.19856)
Append: [Llama-Nemotron: Efficient Reasoning Models](https://arxiv.org/abs/2505.00949)
Append: [Breaking mBad! Supervised Fine-tuning for Cross-Lingual Detoxification](https://arxiv.org/abs/2505.16722)
Append: [Not Minds, but Signs: Reframing LLMs through Semiotics](https://arxiv.org/abs/2505.17080)
Append: [From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning](https://arxiv.org/abs/2505.17117)
Append: [Revisiting Epistemic Markers in Confidence Estimation: Can Markers Accurately Reflect Large Language Models' Uncertainty?](https://arxiv.org/abs/2505.24778)
Append: [Parameter-Efficient Fine-Tuning via Circular Convolution](https://arxiv.org/abs/2407.19342)
Append: [Flexora: Flexible Low Rank Adaptation for Large Language Models](https://arxiv.org/abs/2408.10774)
Append: [OM4OV: Leveraging Ontology Matching for Ontology Versioning](https://arxiv.org/abs/2409.20302)
Append: [A Graph-Based Classical and Quantum Approach to Deterministic L-System Inference](https://arxiv.org/abs/2411.19906)
Append: [Scaling Inference-Time Search with Vision Value Model for Improved Visual Comprehension](https://arxiv.org/abs/2412.03704)
Append: [Integrating Expert Labels into LLM-based Emission Goal Detection: Example Selection vs Automatic Prompt Design](https://arxiv.org/abs/2412.06432)
Append: [ETTA: Elucidating the Design Space of Text-to-Audio Models](https://arxiv.org/abs/2412.19351)
Append: [Seeking and Updating with Live Visual Knowledge](https://arxiv.org/abs/2504.05288)
Append: [RadZero: Similarity-Based Cross-Attention for Explainable Vision-Language Alignment in Radiology with Zero-Shot Multi-Task Capability](https://arxiv.org/abs/2504.07416)
Append: [T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT](https://arxiv.org/abs/2505.00703)
Append: [Iterative Resolution of Prompt Ambiguities Using a Progressive Cutting-Search Approach](https://arxiv.org/abs/2505.02952)
Append: [Teaching Audio-Aware Large Language Models What Does Not Hear: Mitigating Hallucinations through Synthesized Negative Samples](https://arxiv.org/abs/2505.14518)
Append: [AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large Language Models](https://arxiv.org/abs/2505.16211)
Append: [Two-Stage Regularization-Based Structured Pruning for LLMs](https://arxiv.org/abs/2505.18232)
Append: [MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research](https://arxiv.org/abs/2505.19955)
Append: [Generative Representational Learning of Foundation Models for Recommendation](https://arxiv.org/abs/2506.11999)
append_entries: 105
Finish: 2025-07-02 04:31:56.023004
------------------------------------------------------
Started: 2025-07-02 06:26:06.545480
Existing_entries: 1105
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-02 06:26:06.872114
------------------------------------------------------
Started: 2025-07-02 08:23:43.107863
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-02 08:23:43.378456
------------------------------------------------------
Started: 2025-07-02 10:18:48.518987
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-02 10:18:48.839750
------------------------------------------------------
Started: 2025-07-02 12:35:23.494604
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-02 12:35:23.770198
------------------------------------------------------
Started: 2025-07-02 14:16:54.896336
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-02 14:16:55.168731
------------------------------------------------------
Started: 2025-07-02 16:21:04.097880
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-02 16:21:04.399543
------------------------------------------------------
Started: 2025-07-02 18:24:20.173306
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-02 18:24:20.451099
------------------------------------------------------
Started: 2025-07-02 20:17:03.885981
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-02 20:17:04.182027
------------------------------------------------------
Started: 2025-07-02 22:16:09.664980
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-02 22:16:09.935149
------------------------------------------------------
Started: 2025-07-03 01:21:38.809735
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-03 01:21:39.145792
------------------------------------------------------
Started: 2025-07-03 03:17:51.740771
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-03 03:17:52.014429
------------------------------------------------------
Started: 2025-07-03 04:45:02.062342
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1323
Summarized using GPT-3.5-turbo
Append: [MALIBU Benchmark: Multi-Agent LLM Implicit Bias Uncovered](https://arxiv.org/abs/2507.01019)
Token length: 839
Summarized using GPT-3.5-turbo
Append: [Event-based evaluation of abstractive news summarization](https://arxiv.org/abs/2507.01160)
Token length: 1515
Summarized using GPT-3.5-turbo
Append: [Matching and Linking Entries in Historical Swedish Encyclopedias](https://arxiv.org/abs/2507.01170)
Token length: 1611
Summarized using GPT-3.5-turbo
Append: [MEGA: xLSTM with Multihead Exponential Gated Fusion for Precise Aspect-based Sentiment Analysis](https://arxiv.org/abs/2507.01213)
Token length: 855
Summarized using GPT-3.5-turbo
Append: [The Medium Is Not the Message: Deconfounding Text Embeddings via Linear Concept Erasure](https://arxiv.org/abs/2507.01234)
Token length: 1232
Summarized using GPT-3.5-turbo
Append: [GAIus: Combining Genai with Legal Clauses Retrieval for Knowledge-based Assistant](https://arxiv.org/abs/2507.01259)
Token length: 1725
Summarized using GPT-3.5-turbo
Append: [Evaluating Large Language Models for Multimodal Simulated Ophthalmic Decision-Making in Diabetic Retinopathy and Glaucoma Screening](https://arxiv.org/abs/2507.01278)
Token length: 1379
Summarized using GPT-3.5-turbo
Append: [Rethinking All Evidence: Enhancing Trustworthy Retrieval-Augmented Generation via Conflict-Driven Summarization](https://arxiv.org/abs/2507.01281)
Token length: 1706
Summarized using GPT-3.5-turbo
Append: [Frustratingly Simple Retrieval Improves Challenging, Reasoning-Intensive Benchmarks](https://arxiv.org/abs/2507.01297)
Token length: 1352
Summarized using GPT-3.5-turbo
Append: [La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation](https://arxiv.org/abs/2507.01299)
Token length: 1023
Summarized using GPT-3.5-turbo
Append: [Symbolic or Numerical? Understanding Physics Problem Solving in Reasoning LLMs](https://arxiv.org/abs/2507.01334)
Token length: 981
Summarized using GPT-3.5-turbo
Append: [LEDOM: An Open and Fundamental Reverse Language Model](https://arxiv.org/abs/2507.01335)
Token length: 1966
Summarized using GPT-3.5-turbo
Append: [Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy](https://arxiv.org/abs/2507.01352)
Token length: 1570
Summarized using GPT-3.5-turbo
Append: [Clinical NLP with Attention-Based Deep Learning for Multi-Disease Prediction](https://arxiv.org/abs/2507.01437)
Token length: 1583
Summarized using GPT-3.5-turbo
Append: [LogitSpec: Accelerating Retrieval-based Speculative Decoding via Next Next Token Speculation](https://arxiv.org/abs/2507.01449)
Token length: 1641
Summarized using GPT-3.5-turbo
Append: [Evaluating the Effectiveness of Direct Preference Optimization for Personalizing German Automatic Text Simplifications for Persons with Intellectual Disabilities](https://arxiv.org/abs/2507.01479)
Token length: 1034
Summarized using GPT-3.5-turbo
Append: [Efficient Out-of-Scope Detection in Dialogue Systems via Uncertainty-Driven LLM Routing](https://arxiv.org/abs/2507.01541)
Token length: 1393
Summarized using GPT-3.5-turbo
Append: [Is External Information Useful for Stance Detection with LLMs?](https://arxiv.org/abs/2507.01543)
Token length: 1415
Summarized using GPT-3.5-turbo
Append: [Emotionally Intelligent Task-oriented Dialogue Systems: Architecture, Representation, and Optimisation](https://arxiv.org/abs/2507.01594)
Token length: 575
Summarized using GPT-3.5-turbo
Append: [Chart Question Answering from Real-World Analytical Narratives](https://arxiv.org/abs/2507.01627)
Token length: 1238
Summarized using GPT-3.5-turbo
Append: [Confidence and Stability of Global and Pairwise Scores in NLP Evaluation](https://arxiv.org/abs/2507.01633)
Token length: 1457
Summarized using GPT-3.5-turbo
Append: [Adapting Language Models to Indonesian Local Languages: An Empirical Study of Language Transferability on Zero-Shot Settings](https://arxiv.org/abs/2507.01645)
Token length: 1124
Summarized using GPT-3.5-turbo
Append: [AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness](https://arxiv.org/abs/2507.01702)
Token length: 1130
Summarized using GPT-3.5-turbo
Append: [Stereotype Detection as a Catalyst for Enhanced Bias Detection: A Multi-Task Learning Approach](https://arxiv.org/abs/2507.01715)
Token length: 1485
Summarized using GPT-3.5-turbo
Append: [LLMs for Legal Subsumption in German Employment Contracts](https://arxiv.org/abs/2507.01734)
Token length: 1151
Summarized using GPT-3.5-turbo
Append: [Data interference: emojis, homoglyphs, and issues of data fidelity in corpora and their results](https://arxiv.org/abs/2507.01764)
Token length: 1100
Summarized using GPT-3.5-turbo
Append: [MuRating: A High Quality Data Selecting Approach to Multilingual Large Language Model Pretraining](https://arxiv.org/abs/2507.01785)
Token length: 1056
Summarized using GPT-3.5-turbo
Append: [Probing Evaluation Awareness of Language Models](https://arxiv.org/abs/2507.01786)
Token length: 1471
Summarized using GPT-3.5-turbo
Append: [How Do Vision-Language Models Process Conflicting Information Across Modalities?](https://arxiv.org/abs/2507.01790)
Token length: 1237
Summarized using GPT-3.5-turbo
Append: [The Anatomy of Evidence: An Investigation Into Explainable ICD Coding](https://arxiv.org/abs/2507.01802)
Token length: 730
Summarized using GPT-3.5-turbo
Append: [Evaluating Structured Output Robustness of Small Language Models for Open Attribute-Value Extraction from Clinical Notes](https://arxiv.org/abs/2507.01810)
Token length: 958
Summarized using GPT-3.5-turbo
Append: [Low-Perplexity LLM-Generated Sequences and Where To Find Them](https://arxiv.org/abs/2507.01844)
Token length: 1192
Summarized using GPT-3.5-turbo
Append: [Eka-Eval : A Comprehensive Evaluation Framework for Large Language Models in Indian Languages](https://arxiv.org/abs/2507.01853)
Token length: 1239
Summarized using GPT-3.5-turbo
Append: [DIY-MKG: An LLM-Based Polyglot Language Learning System](https://arxiv.org/abs/2507.01872)
Token length: 1455
Summarized using GPT-3.5-turbo
Append: [MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher Assistants](https://arxiv.org/abs/2507.01887)
Token length: 1194
Summarized using GPT-3.5-turbo
Append: [High-Layer Attention Pruning with Rescaling](https://arxiv.org/abs/2507.01900)
Token length: 1578
Summarized using GPT-3.5-turbo
Append: [AI4Research: A Survey of Artificial Intelligence for Scientific Research](https://arxiv.org/abs/2507.01903)
Token length: 1285
Summarized using GPT-3.5-turbo
Append: [Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models](https://arxiv.org/abs/2507.01915)
Token length: 1309
Summarized using GPT-3.5-turbo
Append: [NaturalThoughts: Selecting and Distilling Reasoning Traces for General Reasoning Tasks](https://arxiv.org/abs/2507.01921)
Token length: 1213
Summarized using GPT-3.5-turbo
Append: [Decision-oriented Text Evaluation](https://arxiv.org/abs/2507.01923)
Token length: 1079
Summarized using GPT-3.5-turbo
Append: [Adaptability of ASR Models on Low-Resource Language: A Comparative Study of Whisper and Wav2Vec-BERT on Bangla](https://arxiv.org/abs/2507.01931)
Token length: 1487
Summarized using GPT-3.5-turbo
Append: [The Thin Line Between Comprehension and Persuasion in LLMs](https://arxiv.org/abs/2507.01936)
Token length: 947
Summarized using GPT-3.5-turbo
Append: [Scalable Offline ASR for Command-Style Dictation in Courtrooms](https://arxiv.org/abs/2507.01021)
Token length: 1525
Summarized using GPT-3.5-turbo
Append: [PathCoT: Chain-of-Thought Prompting for Zero-shot Pathology Visual Reasoning](https://arxiv.org/abs/2507.01029)
Token length: 707
Summarized using GPT-3.5-turbo
Append: [Can Argus Judge Them All? Comparing VLMs Across Domains](https://arxiv.org/abs/2507.01042)
Token length: 1099
Summarized using GPT-3.5-turbo
Append: [Cohort Retrieval using Dense Passage Retrieval](https://arxiv.org/abs/2507.01049)
Token length: 1355
Summarized using GPT-3.5-turbo
Append: [Text Detoxification: Data Efficiency, Semantic Preservation and Model Generalization](https://arxiv.org/abs/2507.01050)
Token length: 1161
Summarized using GPT-3.5-turbo
Append: [Automated Vehicles Should be Connected with Natural Language](https://arxiv.org/abs/2507.01059)
Token length: 1088
Summarized using GPT-3.5-turbo
Append: [Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless Handwritten STEM Grading](https://arxiv.org/abs/2507.01431)
Token length: 1124
Summarized using GPT-3.5-turbo
Append: [Following the Clues: Experiments on Person Re-ID using Cross-Modal Intelligence](https://arxiv.org/abs/2507.01504)
Append: [Crafting Hanzi as Narrative Bridges: An AI Co-Creation Workshop for Elderly Migrants](https://arxiv.org/abs/2507.01548)
Append: [Self-Guided Process Reward Optimization with Masked Step Advantage for Process Reinforcement Learning](https://arxiv.org/abs/2507.01551)
Append: [T3DM: Test-Time Training-Guided Distribution Shift Modelling for Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2507.01597)
Append: [Data Agent: A Holistic Architecture for Orchestrating Data+AI Ecosystems](https://arxiv.org/abs/2507.01599)
Append: [Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling](https://arxiv.org/abs/2507.01679)
Append: [ECCV 2024 W-CODA: 1st Workshop on Multimodal Perception and Comprehension of Corner Cases in Autonomous Driving](https://arxiv.org/abs/2507.01735)
Append: [Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training](https://arxiv.org/abs/2507.01752)
Append: [LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework for LLMs](https://arxiv.org/abs/2507.01806)
Append: [Test-Time Scaling with Reflective Generative Model](https://arxiv.org/abs/2507.01951)
Append: [Don't Say No: Jailbreaking LLM by Suppressing Refusal](https://arxiv.org/abs/2404.16369)
Append: [Divergent Creativity in Humans and Large Language Models](https://arxiv.org/abs/2405.13012)
Append: [Caution for the Environment: Multimodal Agents are Susceptible to Environmental Distractions](https://arxiv.org/abs/2408.02544)
Append: [Unifying Global and Near-Context Biasing in a Single Trie Pass](https://arxiv.org/abs/2409.13514)
Append: [QAEncoder: Towards Aligned Representation Learning in Question Answering Systems](https://arxiv.org/abs/2409.20434)
Append: [Guaranteed Generation from Large Language Models](https://arxiv.org/abs/2410.06716)
Append: [A Survey on Uncertainty Quantification of Large Language Models: Taxonomy, Open Research Challenges, and Future Directions](https://arxiv.org/abs/2412.05563)
Append: [Text to Band Gap: Pre-trained Language Models as Encoders for Semiconductor Band Gap Prediction](https://arxiv.org/abs/2501.03456)
Append: [VLM2-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit Matching Visual Cues](https://arxiv.org/abs/2502.12084)
Append: [olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language Models](https://arxiv.org/abs/2502.18443)
Append: [KatFishNet: Detecting LLM-Generated Korean Text through Linguistic Feature Analysis](https://arxiv.org/abs/2503.00032)
Append: [Towards Universal Semantics With Large Language Models](https://arxiv.org/abs/2505.11764)
Append: [Pre-training Large Memory Language Models with Internal and External Knowledge](https://arxiv.org/abs/2505.15962)
Append: [Delving into Multilingual Ethical Bias: The MSQAD with Statistical Hypothesis Tests for Large Language Models](https://arxiv.org/abs/2505.19121)
Append: [Self-reflective Uncertainties: Do LLMs Know Their Internal Answer Distribution?](https://arxiv.org/abs/2505.20295)
Append: [Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding](https://arxiv.org/abs/2505.22618)
Append: [BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning](https://arxiv.org/abs/2506.06955)
Append: [Combating Confirmation Bias: A Unified Pseudo-Labeling Framework for Entity Alignment](https://arxiv.org/abs/2307.02075)
Append: [Squat: Quant Small Language Models on the Edge](https://arxiv.org/abs/2402.10787)
Append: [Unified Triplet-Level Hallucination Evaluation for Large Vision-Language Models](https://arxiv.org/abs/2410.23114)
Append: [Direct Quantized Training of Language Models with Stochastic Rounding](https://arxiv.org/abs/2412.04787)
Append: [Recursive Training Loops in LLMs: How training data properties modulate distribution shift in generated data?](https://arxiv.org/abs/2504.03814)
Append: [On the Fundamental Impossibility of Hallucination Control in Large Language Models](https://arxiv.org/abs/2506.06382)
append_entries: 82
Finish: 2025-07-03 04:47:03.677893
------------------------------------------------------
Started: 2025-07-03 06:26:36.188182
Existing_entries: 1082
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-03 06:26:36.456972
------------------------------------------------------
Started: 2025-07-03 08:23:03.639800
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-03 08:23:03.973680
------------------------------------------------------
Started: 2025-07-03 10:18:50.526618
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-03 10:18:50.810443
------------------------------------------------------
Started: 2025-07-03 12:35:40.521918
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-03 12:35:40.743344
------------------------------------------------------
Started: 2025-07-03 14:16:49.252453
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-03 14:16:49.516638
------------------------------------------------------
Started: 2025-07-03 16:21:05.191485
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-03 16:21:05.420778
------------------------------------------------------
Started: 2025-07-03 18:23:46.609571
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-03 18:23:46.858629
------------------------------------------------------
Started: 2025-07-03 20:18:31.189554
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-03 20:18:31.413262
------------------------------------------------------
Started: 2025-07-03 22:15:59.413906
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-03 22:15:59.638730
------------------------------------------------------
Started: 2025-07-04 01:21:19.055193
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-04 01:21:19.410932
------------------------------------------------------
Started: 2025-07-04 03:16:04.985815
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-04 03:16:05.216834
------------------------------------------------------
Started: 2025-07-04 04:32:37.112143
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1187
Summarized using GPT-3.5-turbo
Append: [McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2507.02088)
Token length: 1695
Summarized using GPT-3.5-turbo
Append: [Reasoning or Not? A Comprehensive Evaluation of Reasoning LLMs for Dialogue Summarization](https://arxiv.org/abs/2507.02145)
Token length: 1439
Summarized using GPT-3.5-turbo
Append: [Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer](https://arxiv.org/abs/2507.02199)
Token length: 1491
Summarized using GPT-3.5-turbo
Append: [GDC Cohort Copilot: An AI Copilot for Curating Cohorts from the Genomic Data Commons](https://arxiv.org/abs/2507.02221)
Token length: 811
Summarized using GPT-3.5-turbo
Append: [MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent](https://arxiv.org/abs/2507.02259)
Token length: 1152
Summarized using GPT-3.5-turbo
Append: [DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning](https://arxiv.org/abs/2507.02302)
Token length: 583
Summarized using GPT-3.5-turbo
Append: [Coling-UniA at SciVQA 2025: Few-Shot Example Retrieval and Confidence-Informed Ensembling for Multimodal Large Language Models](https://arxiv.org/abs/2507.02357)
Token length: 1658
Summarized using GPT-3.5-turbo
Append: [QFFN-BERT: An Empirical Study of Depth, Performance, and Data Efficiency in Hybrid Quantum-Classical Transformers](https://arxiv.org/abs/2507.02364)
Token length: 1083
Summarized using GPT-3.5-turbo
Append: [Efficient Code LLM Training via Distribution-Consistent and Diversity-Aware Data Selection](https://arxiv.org/abs/2507.02378)
Token length: 1472
Summarized using GPT-3.5-turbo
Append: [Benchmarking Akan ASR Models Across Domain-Specific Datasets: A Comparative Evaluation of Performance, Scalability, and Adaptability](https://arxiv.org/abs/2507.02407)
Token length: 992
Summarized using GPT-3.5-turbo
Append: [A Cookbook for Community-driven Data Collection of Impaired Speech in LowResource Languages](https://arxiv.org/abs/2507.02428)
Token length: 686
Summarized using GPT-3.5-turbo
Append: [IndianBailJudgments-1200: A Multi-Attribute Dataset for Legal NLP on Indian Bail Orders](https://arxiv.org/abs/2507.02506)
Token length: 1095
Summarized using GPT-3.5-turbo
Append: [WebSailor: Navigating Super-human Reasoning for Web Agent](https://arxiv.org/abs/2507.02592)
Token length: 1382
Summarized using GPT-3.5-turbo
Append: [Revisiting Active Learning under (Human) Label Variation](https://arxiv.org/abs/2507.02593)
Token length: 1211
Summarized using GPT-3.5-turbo
Append: [MPF: Aligning and Debiasing Language Models post Deployment via Multi Perspective Fusion](https://arxiv.org/abs/2507.02595)
Token length: 673
Summarized using GPT-3.5-turbo
Append: [Exploring Gender Bias Beyond Occupational Titles](https://arxiv.org/abs/2507.02679)
Token length: 1255
Summarized using GPT-3.5-turbo
Append: [Can LLMs Identify Critical Limitations within Scientific Research? A Systematic Evaluation on AI Research Papers](https://arxiv.org/abs/2507.02694)
Token length: 1224
Summarized using GPT-3.5-turbo
Append: [Measurement of the Granularity of Vowel Production Space By Just Producible Different (JPD) Limens](https://arxiv.org/abs/2507.02744)
Token length: 1241
Summarized using GPT-3.5-turbo
Append: [Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs](https://arxiv.org/abs/2507.02778)
Token length: 1942
Summarized using GPT-3.5-turbo
Append: [Is Reasoning All You Need? Probing Bias in the Age of Reasoning Language Models](https://arxiv.org/abs/2507.02799)
Token length: 1328
Summarized using GPT-3.5-turbo
Append: [Multimodal Mathematical Reasoning with Diverse Solving Perspective](https://arxiv.org/abs/2507.02804)
Token length: 1558
Summarized using GPT-3.5-turbo
Append: [SynapseRoute: An Auto-Route Switching Framework on Dual-State Large Language Model](https://arxiv.org/abs/2507.02822)
Token length: 1350
Summarized using GPT-3.5-turbo
Append: [Generalizing Verifiable Instruction Following](https://arxiv.org/abs/2507.02833)
Token length: 1250
Summarized using GPT-3.5-turbo
Append: [LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge Injection to All Users](https://arxiv.org/abs/2507.02850)
Token length: 1471
Summarized using GPT-3.5-turbo
Append: [MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs](https://arxiv.org/abs/2507.02851)
Token length: 1640
Summarized using GPT-3.5-turbo
Append: [Answer Matching Outperforms Multiple Choice for Language Model Evaluation](https://arxiv.org/abs/2507.02856)
Token length: 1330
Summarized using GPT-3.5-turbo
Append: [Multimodal Misinformation Detection Using Early Fusion of Linguistic, Visual, and Social Features](https://arxiv.org/abs/2507.01984)
Token length: 1452
Summarized using GPT-3.5-turbo
Append: [FinAI-BERT: A Transformer-Based Model for Sentence-Level Detection of AI Disclosures in Financial Reports](https://arxiv.org/abs/2507.01991)
Token length: 1277
Summarized using GPT-3.5-turbo
Append: [Why Multi-Interest Fairness Matters: Hypergraph Contrastive Multi-Interest Learning for Fair Conversational Recommender System](https://arxiv.org/abs/2507.02000)
Token length: 1400
Summarized using GPT-3.5-turbo
Append: [STELLA: Self-Evolving LLM Agent for Biomedical Research](https://arxiv.org/abs/2507.02004)
Token length: 1822
Summarized using GPT-3.5-turbo
Append: [Evaluating the Promise and Pitfalls of LLMs in Hiring Decisions](https://arxiv.org/abs/2507.02087)
Token length: 1960
Summarized using GPT-3.5-turbo
Append: [Energy-Based Transformers are Scalable Learners and Thinkers](https://arxiv.org/abs/2507.02092)
Token length: 1592
Summarized using GPT-3.5-turbo
Append: [Dissecting the Impact of Mobile DVFS Governors on LLM Inference Performance and Energy Efficiency](https://arxiv.org/abs/2507.02135)
Token length: 926
Summarized using GPT-3.5-turbo
Append: [Analyzing and Improving Speaker Similarity Assessment for Speech Synthesis](https://arxiv.org/abs/2507.02176)
Token length: 1651
Summarized using GPT-3.5-turbo
Append: [ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text Recognition with Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.02200)
Token length: 1606
Summarized using GPT-3.5-turbo
Append: [SciGA: A Comprehensive Dataset for Designing Graphical Abstracts in Academic Papers](https://arxiv.org/abs/2507.02212)
Token length: 1293
Summarized using GPT-3.5-turbo
Append: [Seeing Through Green: Text-Based Classification and the Firm's Returns from Green Patents](https://arxiv.org/abs/2507.02287)
Token length: 696
Summarized using GPT-3.5-turbo
Append: [JoyTTS: LLM-based Spoken Chatbot With Voice Cloning](https://arxiv.org/abs/2507.02380)
Token length: 1683
Summarized using GPT-3.5-turbo
Append: [Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory](https://arxiv.org/abs/2507.02618)
Token length: 1444
Summarized using GPT-3.5-turbo
Append: [Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search](https://arxiv.org/abs/2507.02652)
Token length: 1467
Summarized using GPT-3.5-turbo
Append: [OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device Speculative Decoding](https://arxiv.org/abs/2507.02659)
Token length: 1003
Summarized using GPT-3.5-turbo
Append: [ASDA: Audio Spectrogram Differential Attention Mechanism for Self-Supervised Representation Learning](https://arxiv.org/abs/2507.02666)
Token length: 1335
Summarized using GPT-3.5-turbo
Append: [Early Signs of Steganographic Capabilities in Frontier LLMs](https://arxiv.org/abs/2507.02737)
Token length: 1735
Summarized using GPT-3.5-turbo
Append: [DeSTA2.5-Audio: Toward General-Purpose Large Audio Language Model with Self-Generated Cross-Modal Alignment](https://arxiv.org/abs/2507.02768)
Token length: 1442
Summarized using GPT-3.5-turbo
Append: [From Long Videos to Engaging Clips: A Human-Inspired Video Editing Framework with Multimodal Narrative Understanding](https://arxiv.org/abs/2507.02790)
Token length: 1962
Summarized using GPT-3.5-turbo
Append: [ExPO: Unlocking Hard Reasoning with Self-Explanation-Guided Reinforcement Learning](https://arxiv.org/abs/2507.02834)
Token length: 1716
Summarized using GPT-3.5-turbo
Append: [StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to Reason](https://arxiv.org/abs/2507.02841)
Token length: 1655
Summarized using GPT-3.5-turbo
Append: [Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection](https://arxiv.org/abs/2507.02844)
Token length: 1567
Summarized using GPT-3.5-turbo
Append: [Legal Requirements Translation from Law](https://arxiv.org/abs/2507.02846)
Token length: 1643
Summarized using GPT-3.5-turbo
Append: [Requirements Elicitation Follow-Up Question Generation](https://arxiv.org/abs/2507.02858)
Append: [Beyond Scale: The Diversity Coefficient as a Data Quality Metric for Variability in Natural Language Data](https://arxiv.org/abs/2306.13840)
Append: [Improving the Robustness of Distantly-Supervised Named Entity Recognition via Uncertainty-Aware Teacher Learning and Student-Student Collaborative Learning](https://arxiv.org/abs/2311.08010)
Append: [Optimal strategies to perform multilingual analysis of social content for a novel dataset in the tourism domain](https://arxiv.org/abs/2311.14727)
Append: [Delving into LLM-assisted writing in biomedical publications through excess vocabulary](https://arxiv.org/abs/2406.07016)
Append: [Task Prompt Vectors: Effective Initialization through Multi-Task Soft-Prompt Transfer](https://arxiv.org/abs/2408.01119)
Append: [MedAide: Information Fusion and Anatomy of Medical Intents via LLM-based Agent Collaboration](https://arxiv.org/abs/2410.12532)
Append: [De-mark: Watermark Removal in Large Language Models](https://arxiv.org/abs/2410.13808)
Append: [Next-Token Prediction Task Assumes Optimal Data Ordering for LLM Training in Proof Generation](https://arxiv.org/abs/2411.00863)
Append: [SHuBERT: Self-Supervised Sign Language Representation Learning via Multi-Stream Cluster Prediction](https://arxiv.org/abs/2411.16765)
Append: [Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache Compression](https://arxiv.org/abs/2412.05693)
Append: [Token Prepending: A Training-Free Approach for Eliciting Better Sentence Embeddings from LLMs](https://arxiv.org/abs/2412.11556)
Append: [REINFORCE++: An Efficient RLHF Algorithm with Robustness to Both Prompt and Reward Models](https://arxiv.org/abs/2501.03262)
Append: [Quantifying the Importance of Data Alignment in Downstream Model Performance](https://arxiv.org/abs/2501.08496)
Append: [Improved Unbiased Watermark for Large Language Models](https://arxiv.org/abs/2502.11268)
Append: [Layered Insights: Generalizable Analysis of Authorial Style by Leveraging All Transformer Layers](https://arxiv.org/abs/2503.00958)
Append: [Commander-GPT: Fully Unleashing the Sarcasm Detection Capability of Multi-Modal Large Language Models](https://arxiv.org/abs/2503.18681)
Append: [SMARTe: Slot-based Method for Accountable Relational Triple extraction](https://arxiv.org/abs/2504.12816)
Append: [Code2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General Reasoning](https://arxiv.org/abs/2505.13886)
Append: [Traveling Across Languages: Benchmarking Cross-Lingual Consistency in Multimodal LLMs](https://arxiv.org/abs/2505.15075)
Append: [Enhancing Clinical Multiple-Choice Questions Benchmarks with Knowledge Graph Guided Distractor Generation](https://arxiv.org/abs/2506.00612)
Append: [Explainable Compliance Detection with Multi-Hop Natural Language Inference on Assurance Case Structure](https://arxiv.org/abs/2506.08713)
Append: [AIn't Nothing But a Survey? Using Large Language Models for Coding German Open-Ended Survey Responses on Survey Motivation](https://arxiv.org/abs/2506.14634)
Append: [Rethinking LLM Training through Information Geometry and Quantum Metrics](https://arxiv.org/abs/2506.15830)
Append: [Bypass Back-propagation: Optimization-based Structural Pruning for Large Language Models via Policy Gradient](https://arxiv.org/abs/2406.10576)
Append: [Causal Representation Learning with Generative Artificial Intelligence: Application to Texts as Treatments](https://arxiv.org/abs/2410.00903)
Append: [Direct Preference Optimization Using Sparse Feature-Level Constraints](https://arxiv.org/abs/2411.07618)
Append: [On Characterizations for Language Generation: Interplay of Hallucinations, Breadth, and Stability](https://arxiv.org/abs/2412.18530)
Append: [Circuit-tuning: A Mechanistic Approach for Identifying Parameter Redundancy and Fine-tuning Neural Networks](https://arxiv.org/abs/2502.06106)
Append: [Incorporating LLMs for Large-Scale Urban Complex Mobility Simulation](https://arxiv.org/abs/2505.21880)
Append: [AI Flow: Perspectives, Scenarios, and Approaches](https://arxiv.org/abs/2506.12479)
append_entries: 80
Finish: 2025-07-04 04:35:06.601338
------------------------------------------------------
Started: 2025-07-04 06:26:17.675583
Existing_entries: 1080
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-04 06:26:17.928728
------------------------------------------------------
Started: 2025-07-04 08:22:59.553151
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-04 08:22:59.824309
------------------------------------------------------
Started: 2025-07-04 10:18:25.687508
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-04 10:18:25.959452
------------------------------------------------------
Started: 2025-07-04 12:34:42.189528
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-04 12:34:42.425403
------------------------------------------------------
Started: 2025-07-04 14:16:25.041494
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-04 14:16:25.268386
------------------------------------------------------
Started: 2025-07-04 16:20:20.336557
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-04 16:20:20.568859
------------------------------------------------------
Started: 2025-07-04 18:22:40.284244
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-04 18:22:40.572772
------------------------------------------------------
Started: 2025-07-04 20:18:02.380546
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-04 20:18:02.608911
------------------------------------------------------
Started: 2025-07-04 22:16:17.968982
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-04 22:16:18.222385
------------------------------------------------------
Started: 2025-07-05 01:18:41.959043
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-05 01:18:42.263916
------------------------------------------------------
Started: 2025-07-05 03:11:44.753310
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-05 03:11:44.992934
------------------------------------------------------
Started: 2025-07-05 04:21:53.478757
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-05 04:21:53.552630
------------------------------------------------------
Started: 2025-07-05 06:22:57.909972
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-05 06:22:58.005709
------------------------------------------------------
Started: 2025-07-05 08:20:34.491754
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-05 08:20:34.552827
------------------------------------------------------
Started: 2025-07-05 10:16:27.317824
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-05 10:16:27.371951
------------------------------------------------------
Started: 2025-07-05 12:31:42.562073
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-05 12:31:42.638079
------------------------------------------------------
Started: 2025-07-05 14:15:36.405577
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-05 14:15:36.466102
------------------------------------------------------
Started: 2025-07-05 16:19:20.594619
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-05 16:19:20.652519
------------------------------------------------------
Started: 2025-07-05 18:21:01.325775
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-05 18:21:01.391651
------------------------------------------------------
Started: 2025-07-05 20:17:12.281781
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-05 20:17:12.367621
------------------------------------------------------
Started: 2025-07-05 22:14:55.041451
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-05 22:14:55.128382
------------------------------------------------------
Started: 2025-07-06 01:28:42.219859
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-06 01:28:42.278732
------------------------------------------------------
Started: 2025-07-06 03:24:31.263706
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-06 03:24:31.322115
------------------------------------------------------
Started: 2025-07-06 04:30:30.225765
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-06 04:30:30.315431
------------------------------------------------------
Started: 2025-07-06 06:23:47.069475
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-06 06:23:47.129482
------------------------------------------------------
Started: 2025-07-06 08:20:20.148542
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-06 08:20:20.206205
------------------------------------------------------
Started: 2025-07-06 10:17:28.090182
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-06 10:17:28.147632
------------------------------------------------------
Started: 2025-07-06 12:32:02.134839
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-06 12:32:02.233325
------------------------------------------------------
Started: 2025-07-06 14:14:20.857726
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-06 14:14:20.917656
------------------------------------------------------
Started: 2025-07-06 16:19:19.883580
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-06 16:19:19.942011
------------------------------------------------------
Started: 2025-07-06 18:21:48.168249
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-06 18:21:48.230810
------------------------------------------------------
Started: 2025-07-06 20:17:51.290773
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-06 20:17:51.403119
------------------------------------------------------
Started: 2025-07-06 22:16:50.007700
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-06 22:16:50.071846
------------------------------------------------------
Started: 2025-07-07 01:26:10.144529
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-07 01:26:10.203981
------------------------------------------------------
Started: 2025-07-07 03:22:52.852971
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-07 03:22:52.926814
------------------------------------------------------
Started: 2025-07-07 04:31:47.283662
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-07 04:31:47.346330
------------------------------------------------------
Started: 2025-07-07 06:27:16.091863
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-07 06:27:16.152712
------------------------------------------------------
Started: 2025-07-07 08:24:54.996710
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-07 08:24:55.055732
------------------------------------------------------
Started: 2025-07-07 10:19:40.011379
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-07 10:19:40.082053
------------------------------------------------------
Started: 2025-07-07 12:35:33.218300
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-07 12:35:33.278390
------------------------------------------------------
Started: 2025-07-07 14:17:16.914562
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-07 14:17:16.974961
------------------------------------------------------
Started: 2025-07-07 16:22:16.906265
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-07 16:22:17.007191
------------------------------------------------------
Started: 2025-07-07 18:24:13.614901
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-07 18:24:13.689146
------------------------------------------------------
Started: 2025-07-07 20:19:12.767993
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-07 20:19:12.824318
------------------------------------------------------
Started: 2025-07-07 22:16:47.682326
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-07 22:16:47.761616
------------------------------------------------------
Started: 2025-07-08 01:22:16.544009
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-08 01:22:16.604668
------------------------------------------------------
Started: 2025-07-08 03:17:39.911577
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-08 03:17:39.978733
------------------------------------------------------
Started: 2025-07-08 04:32:04.950509
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1379
Summarized using GPT-3.5-turbo
Append: [Loki's Dance of Illusions: A Comprehensive Survey of Hallucination in Large Language Models](https://arxiv.org/abs/2507.02870)
Token length: 1146
Summarized using GPT-3.5-turbo
Append: [ChatGPT is not A Man but Das Man: Representativeness and Structural Consistency of Silicon Samples Generated by Large Language Models](https://arxiv.org/abs/2507.02919)
Token length: 1159
Summarized using GPT-3.5-turbo
Append: [A Unified Speech LLM for Diarization and Speech Recognition in Multilingual Conversations](https://arxiv.org/abs/2507.02927)
Token length: 1234
Summarized using GPT-3.5-turbo
Append: [Mitigating Hidden Confounding by Progressive Confounder Imputation via Large Language Models](https://arxiv.org/abs/2507.02928)
Token length: 1523
Summarized using GPT-3.5-turbo
Append: [Theory of Mind in Action: The Instruction Inference Task](https://arxiv.org/abs/2507.02935)
Token length: 1533
Summarized using GPT-3.5-turbo
Append: [A Large Language Model-Empowered Agent for Reliable and Robust Structural Analysis](https://arxiv.org/abs/2507.02938)
Token length: 1684
Summarized using GPT-3.5-turbo
Append: [Towards a Comparative Framework for Compositional AI Models](https://arxiv.org/abs/2507.02940)
Token length: 1871
Summarized using GPT-3.5-turbo
Append: [The Application of Large Language Models on Major Depressive Disorder Support Based on African Natural Products](https://arxiv.org/abs/2507.02947)
Token length: 1526
Summarized using GPT-3.5-turbo
Append: [RADIANT: Retrieval AugmenteD entIty-context AligNmenT -- Introducing RAG-ability and Entity-Context Divergence](https://arxiv.org/abs/2507.02949)
Token length: 1648
Summarized using GPT-3.5-turbo
Append: [Evaluating AI Counseling in Japanese: Counselor, Client, and Evaluator Roles Assessed by Motivational Interviewing Criteria](https://arxiv.org/abs/2507.02950)
Token length: 1115
Summarized using GPT-3.5-turbo
Append: [Advanced Financial Reasoning at Scale: A Comprehensive Evaluation of Large Language Models on CFA Level III](https://arxiv.org/abs/2507.02954)
Token length: 1041
Summarized using GPT-3.5-turbo
Append: [Real-World En Call Center Transcripts Dataset with PII Redaction](https://arxiv.org/abs/2507.02958)
Token length: 1186
Summarized using GPT-3.5-turbo
Append: [RAG-R1 : Incentivize the Search and Reasoning Capabilities of LLMs through Multi-query Parallelism](https://arxiv.org/abs/2507.02962)
Token length: 1685
Summarized using GPT-3.5-turbo
Append: [Less Data, More Security: Advancing Cybersecurity LLMs Specialization via Resource-Efficient Domain-Adaptive Continuous Pre-training with Minimal Tokens](https://arxiv.org/abs/2507.02964)
Token length: 1765
Summarized using GPT-3.5-turbo
Append: [PB-LLMs: Privacy- and Bias-aware NLP Models using Named-Entity Recognition](https://arxiv.org/abs/2507.02966)
Token length: 1508
Summarized using GPT-3.5-turbo
Append: [We Need Knowledge Distillation for Solving Math Word Problems](https://arxiv.org/abs/2507.02982)
Token length: 1005
Summarized using GPT-3.5-turbo
Append: [Truth, Trust, and Trouble: Medical AI on the Edge](https://arxiv.org/abs/2507.02983)
Token length: 1813
Summarized using GPT-3.5-turbo
Append: [From Answers to Rationales: Self-Aligning Multimodal Reasoning with Answer-Oriented Chain-of-Thought](https://arxiv.org/abs/2507.02984)
Token length: 1205
Summarized using GPT-3.5-turbo
Append: [GAF-Guard: An Agentic Framework for Risk Management and Governance in Large Language Models](https://arxiv.org/abs/2507.02986)
Token length: 1396
Summarized using GPT-3.5-turbo
Append: [A Comparative Study of Competency Question Elicitation Methods from Ontology Requirements](https://arxiv.org/abs/2507.02989)
Token length: 1485
Summarized using GPT-3.5-turbo
Append: [`For Argument's Sake, Show Me How to Harm Myself!': Jailbreaking LLMs in Suicide and Self-Harm Contexts](https://arxiv.org/abs/2507.02990)
Token length: 1091
Summarized using GPT-3.5-turbo
Append: [Evaluating Hierarchical Clinical Document Classification Using Reasoning-Based LLMs](https://arxiv.org/abs/2507.03001)
Token length: 1510
Summarized using GPT-3.5-turbo
Append: [Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages](https://arxiv.org/abs/2507.03003)
Token length: 1440
Summarized using GPT-3.5-turbo
Append: [CLUES: Collaborative High-Quality Data Selection for LLMs via Training Dynamics](https://arxiv.org/abs/2507.03004)
Token length: 1252
Summarized using GPT-3.5-turbo
Append: [Beyond cognacy](https://arxiv.org/abs/2507.03005)
Token length: 702
Summarized using GPT-3.5-turbo
Append: [PDFMathTranslate: Scientific Document Translation Preserving Layouts](https://arxiv.org/abs/2507.03009)
Token length: 706
Summarized using GPT-3.5-turbo
Append: [Subversion via Focal Points: Investigating Collusion in LLM Monitoring](https://arxiv.org/abs/2507.03010)
Token length: 899
Summarized using GPT-3.5-turbo
Append: [Beyond Overcorrection: Evaluating Diversity in T2I Models with DIVBENCH](https://arxiv.org/abs/2507.03015)
Token length: 1034
Summarized using GPT-3.5-turbo
Append: [OpenTable-R1: A Reinforcement Learning Augmented Tool Agent for Open-Domain Table Question Answering](https://arxiv.org/abs/2507.03018)
Token length: 1523
Summarized using GPT-3.5-turbo
Append: [The Book of Life approach: Enabling richness and scale for life course research](https://arxiv.org/abs/2507.03027)
Token length: 1946
Summarized using GPT-3.5-turbo
Append: [Preserving Privacy, Increasing Accessibility, and Reducing Cost: An On-Device Artificial Intelligence Model for Medical Transcription and Note Generation](https://arxiv.org/abs/2507.03033)
Token length: 1679
Summarized using GPT-3.5-turbo
Append: [Cautious Next Token Prediction](https://arxiv.org/abs/2507.03038)
Token length: 935
Summarized using GPT-3.5-turbo
Append: [Dynamic Long Short-Term Memory Based Memory Storage For Long Horizon LLM Interaction](https://arxiv.org/abs/2507.03042)
Token length: 1003
Summarized using GPT-3.5-turbo
Append: [K-Function: Joint Pronunciation Transcription and Feedback for Evaluating Kids Language Function](https://arxiv.org/abs/2507.03043)
Token length: 1881
Summarized using GPT-3.5-turbo
Append: [Counterfactual Tuning for Temporal Sensitivity Enhancement in Large Language Model-based Recommendation](https://arxiv.org/abs/2507.03047)
Token length: 1574
Summarized using GPT-3.5-turbo
Append: [Identification of Potentially Misclassified Crash Narratives using Machine Learning (ML) and Deep Learning (DL)](https://arxiv.org/abs/2507.03066)
Token length: 1620
Summarized using GPT-3.5-turbo
Append: [Large Language Models for Automating Clinical Data Standardization: HL7 FHIR Use Case](https://arxiv.org/abs/2507.03067)
Token length: 1608
Summarized using GPT-3.5-turbo
Append: [ARF-RLHF: Adaptive Reward-Following for RLHF through Emotion-Driven Self-Supervision and Trace-Biased Dynamic Optimization](https://arxiv.org/abs/2507.03069)
Token length: 1546
Summarized using GPT-3.5-turbo
Append: [RLVER: Reinforcement Learning with Verifiable Emotion Rewards for Empathetic Agents](https://arxiv.org/abs/2507.03112)
Token length: 1556
Summarized using GPT-3.5-turbo
Append: [ReliableMath: Benchmark of Reliable Mathematical Reasoning on Large Language Models](https://arxiv.org/abs/2507.03133)
Token length: 1538
Summarized using GPT-3.5-turbo
Append: [From Measurement to Mitigation: Exploring the Transferability of Debiasing Approaches to Gender Bias in Maltese Language Models](https://arxiv.org/abs/2507.03142)
Token length: 1953
Summarized using GPT-3.5-turbo
Append: [Expert-level validation of AI-generated medical text with scalable language models](https://arxiv.org/abs/2507.03152)
Token length: 1146
Summarized using GPT-3.5-turbo
Append: [Adversarial Manipulation of Reasoning Models using Internal Representations](https://arxiv.org/abs/2507.03167)
Token length: 1217
Summarized using GPT-3.5-turbo
Append: [How Much Content Do LLMs Generate That Induces Cognitive Bias in Users?](https://arxiv.org/abs/2507.03194)
Token length: 561
Summarized using GPT-3.5-turbo
Append: [A Lie-algebraic perspective on Tree-Adjoining Grammars](https://arxiv.org/abs/2507.03234)
Token length: 1504
Summarized using GPT-3.5-turbo
Append: [KinyaColBERT: A Lexically Grounded Retrieval Model for Low-Resource Retrieval-Augmented Generation](https://arxiv.org/abs/2507.03241)
Token length: 1843
Summarized using GPT-3.5-turbo
Append: [RefineX: Learning to Refine Pre-training Data at Scale from Expert-Guided Programs](https://arxiv.org/abs/2507.03253)
Token length: 1349
Summarized using GPT-3.5-turbo
Append: [GRAFT: A Graph-based Flow-aware Agentic Framework for Document-level Machine Translation](https://arxiv.org/abs/2507.03311)
Token length: 1414
Summarized using GPT-3.5-turbo
Append: [Read Quietly, Think Aloud: Decoupling Comprehension and Reasoning in LLMs](https://arxiv.org/abs/2507.03327)
Token length: 1206
Summarized using GPT-3.5-turbo
Append: [SHNU Multilingual Conversational Speech Recognition System for INTERSPEECH 2025 MLC-SLM Challenge](https://arxiv.org/abs/2507.03343)
Append: [Backtesting Sentiment Signals for Trading: Evaluating the Viability of Alpha Generation from Sentiment Analysis](https://arxiv.org/abs/2507.03350)
Append: [WETBench: A Benchmark for Detecting Task-Specific Machine-Generated Text on Wikipedia](https://arxiv.org/abs/2507.03373)
Append: [Making Sense of Korean Sentences: A Comprehensive Evaluation of LLMs through KoSEnd Dataset](https://arxiv.org/abs/2507.03378)
Append: [Graph Repairs with Large Language Models: An Empirical Study](https://arxiv.org/abs/2507.03410)
Append: [SMCLM: Semantically Meaningful Causal Language Modeling for Autoregressive Paraphrase Generation](https://arxiv.org/abs/2507.03415)
Append: [Improving Social Determinants of Health Documentation in French EHRs Using Large Language Models](https://arxiv.org/abs/2507.03433)
Append: [Beyond Weaponization: NLP Security for Medium and Lower-Resourced Languages in Their Own Right](https://arxiv.org/abs/2507.03473)
Append: [BMMR: A Large-Scale Bilingual Multimodal Multi-Discipline Reasoning Dataset](https://arxiv.org/abs/2507.03483)
Append: [Four Shades of Life Sciences: A Dataset for Disinformation Detection in the Life Sciences](https://arxiv.org/abs/2507.03488)
Append: [AI-VaxGuide: An Agentic RAG-Based LLM for Vaccination Decisions](https://arxiv.org/abs/2507.03493)
Append: [H2HTalk: Evaluating Large Language Models as Emotional Companion](https://arxiv.org/abs/2507.03543)
Append: [Articulatory clarity and variability before and after surgery for tongue cancer](https://arxiv.org/abs/2507.03576)
Append: [Learning to Translate Ambiguous Terminology by Preference Optimization on Post-Edits](https://arxiv.org/abs/2507.03580)
Append: [Multi-Hop Reasoning for Question Answering with Hyperbolic Representations](https://arxiv.org/abs/2507.03612)
Append: [EMERGE: A Benchmark for Updating Knowledge Graphs with Emerging Textual Knowledge](https://arxiv.org/abs/2507.03617)
Append: [Improving Low-Resource Dialect Classification Using Retrieval-based Voice Conversion](https://arxiv.org/abs/2507.03641)
Append: [Disentangling the Roles of Representation and Selection in Data Pruning](https://arxiv.org/abs/2507.03648)
Append: [TRACE: Training and Inference-Time Interpretability Analysis for Language Models](https://arxiv.org/abs/2507.03668)
Append: [Recon, Answer, Verify: Agents in Search of Truth](https://arxiv.org/abs/2507.03671)
Append: [TACOS: Open Tagging and Comparative Scoring for Instruction Fine-Tuning Data Selection](https://arxiv.org/abs/2507.03673)
Append: [STRUCTSENSE: A Task-Agnostic Agentic Framework for Structured Information Extraction with Human-In-The-Loop Evaluation and Benchmarking](https://arxiv.org/abs/2507.03674)
Append: [Controlling Thinking Speed in Reasoning Models](https://arxiv.org/abs/2507.03704)
Append: [Can LLMs Play \^O \u{A}n Quan Game? A Study of Multi-Step Planning and Decision Making](https://arxiv.org/abs/2507.03711)
Append: [MemOS: A Memory OS for AI System](https://arxiv.org/abs/2507.03724)
Append: [Alpay Algebra IV: Symbiotic Semantics and the Fixed-Point Convergence of Observer Embeddings](https://arxiv.org/abs/2507.03774)
Append: [OrthoRank: Token Selection via Sink Token Orthogonality for Efficient LLM inference](https://arxiv.org/abs/2507.03865)
Append: [Demystifying ChatGPT: How It Masters Genre Recognition](https://arxiv.org/abs/2507.03875)
Append: [Dynamic Injection of Entity Knowledge into Dense Retrievers](https://arxiv.org/abs/2507.03922)
Append: [Losing our Tail -- Again: On (Un)Natural Selection And Multilingual Large Language Models](https://arxiv.org/abs/2507.03933)
Append: [A Modular Unsupervised Framework for Attribute Recognition from Unstructured Text](https://arxiv.org/abs/2507.03949)
Append: [Easy Dataset: A Unified and Extensible Framework for Synthesizing LLM Fine-Tuning Data from Unstructured Documents](https://arxiv.org/abs/2507.04009)
Append: [Nunchi-Bench: Benchmarking Language Models on Cultural Reasoning with a Focus on Korean Superstition](https://arxiv.org/abs/2507.04014)
Append: [Handling Korean Out-of-Vocabulary Words with Phoneme Representation Learning](https://arxiv.org/abs/2507.04018)
Append: [LLMThinkBench: Towards Basic Math Reasoning and Overthinking in Large Language Models](https://arxiv.org/abs/2507.04023)
Append: [Patient-Centered RAG for Oncology Visit Aid Following the Ottawa Decision Guide](https://arxiv.org/abs/2507.04026)
Append: [Beyond Independent Passages: Adaptive Passage Combination Retrieval for Retrieval Augmented Open-Domain Question Answering](https://arxiv.org/abs/2507.04069)
Append: [XISM: an eXploratory and Interactive Graph Tool to Visualize and Evaluate Semantic Map Models](https://arxiv.org/abs/2507.04070)
Append: [Conversation Forests: The Key to Fine Tuning Large Language Models for Multi-Turn Medical Conversations is Branching](https://arxiv.org/abs/2507.04099)
Append: [BYOKG-RAG: Multi-Strategy Graph Retrieval for Knowledge Graph Question Answering](https://arxiv.org/abs/2507.04127)
Append: [Token Level Hallucination Detection via Variance in Language Models](https://arxiv.org/abs/2507.04137)
Append: [Dissecting Clinical Reasoning in Language Models: A Comparative Study of Prompts and Model Adaptation Strategies](https://arxiv.org/abs/2507.04142)
Append: [Large Language Models for Zero-Shot Multicultural Name Recognition](https://arxiv.org/abs/2507.04149)
Append: [SymbolicThought: Integrating Language Models and Symbolic Reasoning for Consistent and Interpretable Human Relationship Understanding](https://arxiv.org/abs/2507.04189)
Append: [Context Tuning for In-Context Optimization](https://arxiv.org/abs/2507.04221)
Append: [Fairness Evaluation of Large Language Models in Academic Library Reference Services](https://arxiv.org/abs/2507.04224)
Append: [No Language Data Left Behind: A Comparative Study of CJK Language Datasets in the Hugging Face Ecosystem](https://arxiv.org/abs/2507.04329)
Append: [HatePRISM: Policies, Platforms, and Research Integration. Advancing NLP for Hate Speech Proactive Mitigation](https://arxiv.org/abs/2507.04350)
Append: [Large Language Models' Varying Accuracy in Recognizing Risk-Promoting and Health-Supporting Sentiments in Public Health Discourse: The Cases of HPV Vaccination and Heated Tobacco Products](https://arxiv.org/abs/2507.04364)
Append: [Does Learning Mathematical Problem-Solving Generalize to Broader Reasoning?](https://arxiv.org/abs/2507.04391)
Append: [SpiritRAG: A Q&A System for Religion and Spirituality in the United Nations Archive](https://arxiv.org/abs/2507.04395)
Append: [THM@SimpleText 2025 -- Task 1.1: Revisiting Text Simplification based on Complex Terms for Non-Experts](https://arxiv.org/abs/2507.04414)
Append: [MOMENTS: A Comprehensive Multimodal Benchmark for Theory of Mind](https://arxiv.org/abs/2507.04415)
Append: [RAT: Bridging RNN Efficiency and Attention Accuracy in Language Modeling](https://arxiv.org/abs/2507.04416)
Append: [GradOT: Training-free Gradient-preserving Offsite-tuning for Large Language Models](https://arxiv.org/abs/2507.04455)
Append: [Think Twice Before You Judge: Mixture of Dual Reasoning Experts for Multimodal Sarcasm Detection](https://arxiv.org/abs/2507.04458)
Append: [Dual Modality-Aware Gated Prompt Tuning for Few-Shot Multimodal Sarcasm Detection](https://arxiv.org/abs/2507.04468)
Append: [Unveiling the Potential of Diffusion Large Language Model in Controllable Generation](https://arxiv.org/abs/2507.04504)
Append: [AdS: Adapter-state Sharing Framework for Multimodal Sarcasm Detection](https://arxiv.org/abs/2507.04508)
Append: [DP-Fusion: Token-Level Differentially Private Inference for Large Language Models](https://arxiv.org/abs/2507.04531)
Append: [Nile-Chat: Egyptian Language Models for Arabic and Latin Scripts](https://arxiv.org/abs/2507.04569)
Append: [PRIME: Large Language Model Personalization with Cognitive Memory and Thought Processes](https://arxiv.org/abs/2507.04607)
Append: [Retain or Reframe? A Computational Framework for the Analysis of Framing in News Articles and Reader Comments](https://arxiv.org/abs/2507.04612)
Append: [Knowledge-Aware Self-Correction in Language Models via Structured Memory Graphs](https://arxiv.org/abs/2507.04625)
Append: [Put Teacher in Student's Shoes: Cross-Distillation for Ultra-compact Model Compression Framework](https://arxiv.org/abs/2507.04636)
Append: [R1-RE: Cross-Domain Relationship Extraction with RLVR](https://arxiv.org/abs/2507.04642)
Append: [XiYan-SQL: A Novel Multi-Generator Framework For Text-to-SQL](https://arxiv.org/abs/2507.04701)
Append: [Why We Feel What We Feel: Joint Detection of Emotions and Their Opinion Triggers in E-commerce](https://arxiv.org/abs/2507.04708)
Append: [LOOM-Scope: a comprehensive and efficient LOng-cOntext Model evaluation framework](https://arxiv.org/abs/2507.04723)
Append: ["This Suits You the Best": Query Focused Comparative Explainable Summarization](https://arxiv.org/abs/2507.04733)
Append: [Word stress in self-supervised speech models: A cross-linguistic comparison](https://arxiv.org/abs/2507.04738)
Append: [A Tale of Two Scripts: Transliteration and Post-Correction for Judeo-Arabic](https://arxiv.org/abs/2507.04746)
Append: [LLMs as Architects and Critics for Multi-Source Opinion Summarization](https://arxiv.org/abs/2507.04751)
Append: [CoSteer: Collaborative Decoding-Time Personalization via Local Delta Steering](https://arxiv.org/abs/2507.04756)
Append: [Reason to Rote: Rethinking Memorization in Reasoning](https://arxiv.org/abs/2507.04782)
Append: [A Survey of Pun Generation: Datasets, Evaluations and Methodologies](https://arxiv.org/abs/2507.04793)
Append: [Spec-TOD: A Specialized Instruction-Tuned LLM Framework for Efficient Task-Oriented Dialogue Systems](https://arxiv.org/abs/2507.04841)
Append: [Dialogue-Based Multi-Dimensional Relationship Extraction from Novels](https://arxiv.org/abs/2507.04852)
Append: [$\textit{Grahak-Nyay:}$ Consumer Grievance Redressal through Large Language Models](https://arxiv.org/abs/2507.04854)
Append: [Building Open-Retrieval Conversational Question Answering Systems by Generating Synthetic Data and Decontextualizing User Questions](https://arxiv.org/abs/2507.04884)
Append: [Emergent Semantics Beyond Token Embeddings: Transformer LMs with Frozen Visual Unicode Representations](https://arxiv.org/abs/2507.04886)
Append: [O_FT@EvalLLM2025 : \'etude comparative de choix de donn\'ees et de strat\'egies d'apprentissage pour l'adaptation de mod\`eles de langue \`a un domaine](https://arxiv.org/abs/2507.04895)
Append: [SIGIR 2025 -- LiveRAG Challenge Report](https://arxiv.org/abs/2507.04942)
Append: [ArtifactsBench: Bridging the Visual-Interactive Gap in LLM Code Generation Evaluation](https://arxiv.org/abs/2507.04952)
Append: [Co-DETECT: Collaborative Discovery of Edge Cases in Text Classification](https://arxiv.org/abs/2507.05010)
Append: [Verified Language Processing with Hybrid Explainability: A Technical Report](https://arxiv.org/abs/2507.05017)
Append: [An Evaluation of Large Language Models on Text Summarization Tasks Using Prompt Engineering Techniques](https://arxiv.org/abs/2507.05123)
Append: [SMART: Simulated Students Aligned with Item Response Theory for Question Difficulty Prediction](https://arxiv.org/abs/2507.05129)
Append: [Interpretable Mnemonic Generation for Kanji Learning via Expectation-Maximization](https://arxiv.org/abs/2507.05137)
Append: [AI Generated Text Detection Using Instruction Fine-tuned Large Language and Transformer-Based Models](https://arxiv.org/abs/2507.05157)
Append: [InfoSteer: Steering Information Utility in Language Model Post-Training](https://arxiv.org/abs/2507.05158)
Append: [OpenS2S: Advancing Open-Source End-to-End Empathetic Large Speech Language Model](https://arxiv.org/abs/2507.05177)
Append: [From Fragments to Facts: A Curriculum-Driven DPO Approach for Generating Hindi News Veracity Explanations](https://arxiv.org/abs/2507.05179)
Append: [Pre-Trained Policy Discriminators are General Reward Models](https://arxiv.org/abs/2507.05197)
Append: [Response Attack: Exploiting Contextual Priming to Jailbreak Large Language Models](https://arxiv.org/abs/2507.05248)
Append: [Evaluating Memory in LLM Agents via Incremental Multi-Turn Interactions](https://arxiv.org/abs/2507.05257)
Append: [Large Language Model Agent for Modular Task Execution in Drug Discovery](https://arxiv.org/abs/2507.02925)
Append: [GameTileNet: A Semantic Dataset for Low-Resolution Game Art in Procedural Content Generation](https://arxiv.org/abs/2507.02941)
Append: [InvisibleInk: High-Utility and Low-Cost Text Generation with Differential Privacy](https://arxiv.org/abs/2507.02974)
Append: [Gated Recursive Fusion: A Stateful Approach to Scalable Multimodal Transformers](https://arxiv.org/abs/2507.02985)
Append: [A Weakly Supervised Transformer to Support Rare Disease Diagnosis from Electronic Health Records: Methods and Applications in Rare Pulmonary Disease](https://arxiv.org/abs/2507.02998)
Append: [Intrinsic Fingerprint of LLMs: Continue Training is NOT All You Need to Steal A Model!](https://arxiv.org/abs/2507.03014)
Append: [Improving LLM Reasoning for Vulnerability Detection via Group Relative Policy Optimization](https://arxiv.org/abs/2507.03051)
Append: [Federated Learning for ICD Classification with Lightweight Models and Pretrained Embeddings](https://arxiv.org/abs/2507.03122)
Append: [Towards a Psychoanalytic Perspective on VLM Behaviour: A First-step Interpretation with Intriguing Observations](https://arxiv.org/abs/2507.03123)
Append: [DeepGesture: A conversational gesture synthesis system based on emotions and semantics](https://arxiv.org/abs/2507.03147)
Append: [On the Relationship between Accent Strength and Articulatory Features](https://arxiv.org/abs/2507.03149)
Append: [MateInfoUB: A Real-World Benchmark for Testing LLMs in Competitive, Multilingual, and Multimodal Educational Tasks](https://arxiv.org/abs/2507.03162)
Append: [GDGB: A Benchmark for Generative Dynamic Text-Attributed Graph Learning](https://arxiv.org/abs/2507.03267)
Append: [LTLCrit: A Temporal Logic-based LLM Critic for Safe and Efficient Embodied Agents](https://arxiv.org/abs/2507.03293)
Append: [Disambiguation-Centric Finetuning Makes Enterprise Tool-Calling LLMs More Realistic and Less Risky](https://arxiv.org/abs/2507.03336)
Append: [Causal-SAM-LLM: Large Language Models as Causal Reasoners for Robust Medical Segmentation](https://arxiv.org/abs/2507.03585)
Append: [RECA-PD: A Robust Explainable Cross-Attention Method for Speech-based Parkinson's Disease Classification](https://arxiv.org/abs/2507.03594)
Append: [Is It Time To Treat Prompts As Code? A Multi-Use Case Study For Prompt Optimization Using DSPy](https://arxiv.org/abs/2507.03620)
Append: [Re-Emergent Misalignment: How Narrow Fine-Tuning Erodes Safety Alignment in LLMs](https://arxiv.org/abs/2507.03662)
Append: [Interaction Techniques that Encourage Longer Prompts Can Improve Psychological Ownership when Writing with AI](https://arxiv.org/abs/2507.03670)
Append: [Agent-Based Detection and Resolution of Incompleteness and Ambiguity in Interactions with Large Language Models](https://arxiv.org/abs/2507.03726)
Append: [A Comparative Study of Specialized LLMs as Dense Retrievers](https://arxiv.org/abs/2507.03958)
Append: [MMMOS: Multi-domain Multi-axis Audio Quality Assessment](https://arxiv.org/abs/2507.04094)
Append: [Relational inductive biases on attention mechanisms](https://arxiv.org/abs/2507.04117)
Append: [An HTR-LLM Workflow for High-Accuracy Transcription and Analysis of Abbreviated Latin Court Hand](https://arxiv.org/abs/2507.04132)
Append: [Efficient Detection of Intermittent Job Failures Using Few-Shot Learning](https://arxiv.org/abs/2507.04173)
Append: [Navigating Speech Recording Collections with AI-Generated Illustrations](https://arxiv.org/abs/2507.04182)
Append: [LearnLens: LLM-Enabled Personalised, Curriculum-Grounded Feedback with Educators in the Loop](https://arxiv.org/abs/2507.04295)
Append: [Computed Tomography Visual Question Answering with Cross-modal Feature Graphing](https://arxiv.org/abs/2507.04333)
Append: [SmartThinker: Learning to Compress and Preserve Reasoning by Step-Level Length Control](https://arxiv.org/abs/2507.04348)
Append: [Attention Slipping: A Mechanistic Understanding of Jailbreak Attacks and Defenses in LLMs](https://arxiv.org/abs/2507.04365)
Append: [Multi-Modal Semantic Parsing for the Interpretation of Tombstone Inscriptions](https://arxiv.org/abs/2507.04377)
Append: [MedGellan: LLM-Generated Medical Guidance to Support Physicians](https://arxiv.org/abs/2507.04431)
Append: [Reconstructing Biological Pathways by Applying Selective Incremental Learning to (Very) Small Language Models](https://arxiv.org/abs/2507.04432)
Append: [A Linguistic Analysis of Spontaneous Thoughts: Investigating Experiences of D\'ej\`a Vu, Unexpected Thoughts, and Involuntary Autobiographical Memories](https://arxiv.org/abs/2507.04439)
Append: [The role of large language models in UI/UX design: A systematic literature review](https://arxiv.org/abs/2507.04469)
Append: [Does Overnight News Explain Overnight Returns?](https://arxiv.org/abs/2507.04481)
Append: [A validity-guided workflow for robust large language model research in psychology](https://arxiv.org/abs/2507.04491)
Append: [DOTResize: Reducing LLM Width via Discrete Optimal Transport-based Neuron Merging](https://arxiv.org/abs/2507.04517)
Append: [Evaluating LLMs on Real-World Forecasting Against Human Superforecasters](https://arxiv.org/abs/2507.04562)
Append: [VLM2Vec-V2: Advancing Multimodal Embedding for Videos, Images, and Visual Documents](https://arxiv.org/abs/2507.04590)
Append: [ABench-Physics: Benchmarking Physical Reasoning in LLMs via High-Difficulty and Dynamic Physics Problems](https://arxiv.org/abs/2507.04766)
Append: [From Vision To Language through Graph of Events in Space and Time: An Explainable Self-supervised Approach](https://arxiv.org/abs/2507.04815)
Append: [Transcribing Spanish Texts from the Past: Experiments with Transkribus, Tesseract and Granite](https://arxiv.org/abs/2507.04878)
Append: [MARBLE: A Multi-Agent Rule-Based LLM Reasoning Engine for Accident Severity Prediction](https://arxiv.org/abs/2507.04893)
Append: [ReLoop: "Seeing Twice and Thinking Backwards" via Closed-loop Training to Mitigate Hallucinations in Multimodal understanding](https://arxiv.org/abs/2507.04943)
Append: [Taming the Tri-Space Tension: ARC-Guided Hallucination Modeling and Control for Text-to-Image Generation](https://arxiv.org/abs/2507.04946)
Append: [Can Video LLMs Refuse to Answer? Alignment for Answerability in Video Large Language Models](https://arxiv.org/abs/2507.04976)
Append: [From Autonomy to Agency: Agentic Vehicles for Human-Centered Mobility Systems](https://arxiv.org/abs/2507.04996)
Append: [Do We Really Need Specialization? Evaluating Generalist Text Embeddings for Zero-Shot Recommendation and Search](https://arxiv.org/abs/2507.05006)
Append: [AI-Driven Cytomorphology Image Synthesis for Medical Diagnostics](https://arxiv.org/abs/2507.05063)
Append: [Reviving Cultural Heritage: A Novel Approach for Comprehensive Historical Document Restoration](https://arxiv.org/abs/2507.05108)
Append: [Critiques of World Models](https://arxiv.org/abs/2507.05169)
Append: [MedGemma Technical Report](https://arxiv.org/abs/2507.05201)
Append: [Interleaving Logic and Counting](https://arxiv.org/abs/2507.05219)
Append: [Logit Reweighting for Topic-Focused Summarization](https://arxiv.org/abs/2507.05235)
Append: [SciMaster: Towards General-Purpose Scientific AI Agents, Part I. X-Master as Foundation: Can We Lead on Humanity's Last Exam?](https://arxiv.org/abs/2507.05241)
Append: [When Chain of Thought is Necessary, Language Models Struggle to Evade Monitors](https://arxiv.org/abs/2507.05246)
Append: [Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for Visual Reasoning](https://arxiv.org/abs/2507.05255)
Append: [Relation-Aware Network with Attention-Based Loss for Few-Shot Knowledge Graph Completion](https://arxiv.org/abs/2306.09519)
Append: [End-to-End Evaluation for Low-Latency Simultaneous Speech Translation](https://arxiv.org/abs/2308.03415)
Append: [SEPSIS: I Can Catch Your Lies -- A New Paradigm for Deception Detection](https://arxiv.org/abs/2312.00292)
Append: [Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding](https://arxiv.org/abs/2402.12374)
Append: [Pretraining Language Models Using Translationese](https://arxiv.org/abs/2403.13638)
Append: [LLMs' Reading Comprehension Is Affected by Parametric Knowledge and Struggles with Hypothetical Statements](https://arxiv.org/abs/2404.06283)
Append: [A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications](https://arxiv.org/abs/2404.14809)
Append: [Elements of World Knowledge (EWoK): A Cognition-Inspired Framework for Evaluating Basic World Knowledge in Language Models](https://arxiv.org/abs/2405.09605)
Append: [MoralBench: Moral Evaluation of LLMs](https://arxiv.org/abs/2406.04428)
Append: [From Intentions to Techniques: A Comprehensive Taxonomy and Challenges in Text Watermarking for Large Language Models](https://arxiv.org/abs/2406.11106)
Append: [On the Utility of Domain-Adjacent Fine-Tuned Model Ensembles for Few-shot Problems](https://arxiv.org/abs/2406.13720)
Append: [SS-GEN: A Social Story Generation Framework with Large Language Models](https://arxiv.org/abs/2406.15695)
Append: [A Text is Worth Several Tokens: Text Embedding from LLMs Secretly Aligns Well with The Key Tokens](https://arxiv.org/abs/2406.17378)
Append: [Is It Really Long Context if All You Need Is Retrieval? Towards Genuinely Difficult Long Context NLP](https://arxiv.org/abs/2407.00402)
Append: [Late Chunking: Contextual Chunk Embeddings Using Long-Context Embedding Models](https://arxiv.org/abs/2409.04701)
Append: [ODE: Open-Set Evaluation of Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2409.09318)
Append: [Using Large Multimodal Models to Extract Knowledge Components for Knowledge Tracing from Multimedia Question Information](https://arxiv.org/abs/2409.20167)
Append: [On Positional Bias of Faithfulness for Long-form Summarization](https://arxiv.org/abs/2410.23609)
Append: [Self-Consistency Preference Optimization](https://arxiv.org/abs/2411.04109)
Append: [The Super Weight in Large Language Models](https://arxiv.org/abs/2411.07191)
Append: [LIAR: Leveraging Inference Time Alignment (Best-of-N) to Jailbreak LLMs in Seconds](https://arxiv.org/abs/2412.05232)
Append: [AgentPS: Agentic Process Supervision for Content Moderation with Multimodal LLMs](https://arxiv.org/abs/2412.15251)
Append: [On Fusing ChatGPT and Ensemble Learning in Discon-tinuous Named Entity Recognition in Health Corpora](https://arxiv.org/abs/2412.16976)
Append: [ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving](https://arxiv.org/abs/2501.08203)
Append: [Exploring Robustness of LLMs to Paraphrasing Based on Sociodemographic Factors](https://arxiv.org/abs/2501.08276)
Append: [Rethinking Table Instruction Tuning](https://arxiv.org/abs/2501.14693)
Append: [In-Context Meta LoRA Generation](https://arxiv.org/abs/2501.17635)
Append: [Phonetic Reconstruction of the Consonant System of Middle Chinese via Mixed Integer Optimization](https://arxiv.org/abs/2502.04625)
Append: [Tokenization is Sensitive to Language Variation](https://arxiv.org/abs/2502.15343)
Append: [CritiQ: Mining Data Quality Criteria from Human Preferences](https://arxiv.org/abs/2502.19279)
Append: [iNews: A Multimodal Dataset for Modeling Personalized Affective Responses to News](https://arxiv.org/abs/2503.03335)
Append: [Gradient-guided Attention Map Editing: Towards Efficient Contextual Hallucination Mitigation](https://arxiv.org/abs/2503.08963)
Append: [Have LLMs Made Active Learning Obsolete? Surveying the NLP Community](https://arxiv.org/abs/2503.09701)
Append: [HKCanto-Eval: A Benchmark for Evaluating Cantonese Language Understanding and Cultural Comprehension in LLMs](https://arxiv.org/abs/2503.12440)
Append: [MASS: Mathematical Data Selection via Skill Graphs for Pretraining Large Language Models](https://arxiv.org/abs/2503.14917)
Append: [Entity-aware Cross-lingual Claim Detection for Automated Fact-checking](https://arxiv.org/abs/2503.15220)
Append: [Construction Identification and Disambiguation Using BERT: A Case Study of NPN](https://arxiv.org/abs/2503.18751)
Append: [PHEONA: An Evaluation Framework for Large Language Model-based Approaches to Computational Phenotyping](https://arxiv.org/abs/2503.19265)
Append: [UNITYAI-GUARD: Pioneering Toxicity Detection Across Low-Resource Indian Languages](https://arxiv.org/abs/2503.23088)
Append: [Parsing Through Boundaries in Chinese Word Segmentation](https://arxiv.org/abs/2503.23091)
Append: [On the Role of Feedback in Test-Time Scaling of Agentic AI Workflows](https://arxiv.org/abs/2504.01931)
Append: [Explain with Visual Keypoints Like a Real Mentor! A Benchmark for Multimodal Solution Explanation](https://arxiv.org/abs/2504.03197)
Append: [Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward](https://arxiv.org/abs/2504.03206)
Append: [NativQA Framework: Enabling LLMs with Native, Local, and Everyday Knowledge](https://arxiv.org/abs/2504.05995)
Append: [MAIN: Mutual Alignment Is Necessary for instruction tuning](https://arxiv.org/abs/2504.12913)
Append: [FairSteer: Inference Time Debiasing for LLMs with Dynamic Activation Steering](https://arxiv.org/abs/2504.14492)
Append: [EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language Models](https://arxiv.org/abs/2504.15133)
Append: [Are Information Retrieval Approaches Good at Harmonising Longitudinal Survey Questions in Social Science?](https://arxiv.org/abs/2504.20679)
Append: [ViMRHP: A Vietnamese Benchmark Dataset for Multimodal Review Helpfulness Prediction via Human-AI Collaborative Annotation](https://arxiv.org/abs/2505.07416)
Append: [Judging the Judges: Can Large Vision-Language Models Fairly Evaluate Chart Comprehension and Reasoning?](https://arxiv.org/abs/2505.08468)
Append: [ReviewInstruct: A Review-Driven Multi-Turn Conversations Generation Method for Large Language Models](https://arxiv.org/abs/2505.11010)
Append: [Hunyuan-TurboS: Advancing Large Language Models through Mamba-Transformer Synergy and Adaptive Chain-of-Thought](https://arxiv.org/abs/2505.15431)
Append: [Extended Inductive Reasoning for Personalized Preference Inference from Behavioral Signals](https://arxiv.org/abs/2505.18071)
Append: [Voice of a Continent: Mapping Africa's Speech Technology Frontier](https://arxiv.org/abs/2505.18436)
Append: [Evaluating AI for Finance: Is AI Credible at Assessing Investment Risk?](https://arxiv.org/abs/2505.18953)
Append: [ALAS: Measuring Latent Speech-Text Alignment For Spoken Language Understanding In Multimodal LLMs](https://arxiv.org/abs/2505.19937)
Append: [Evaluating AI capabilities in detecting conspiracy theories on YouTube](https://arxiv.org/abs/2505.23570)
Append: [Reviewing Scientific Papers for Critical Problems With Reasoning LLMs: Baseline Approaches and Automatic Evaluation](https://arxiv.org/abs/2505.23824)
Append: [RewardAnything: Generalizable Principle-Following Reward Models](https://arxiv.org/abs/2506.03637)
Append: [OPeRA: A Dataset of Observation, Persona, Rationale, and Action for Evaluating LLMs on Human Online Shopping Behavior Simulation](https://arxiv.org/abs/2506.05606)
Append: [Generalization or Hallucination? Understanding Out-of-Context Reasoning in Transformers](https://arxiv.org/abs/2506.10887)
Append: [AutoMind: Adaptive Knowledgeable Agent for Automated Data Science](https://arxiv.org/abs/2506.10974)
Append: [EAQuant: Enhancing Post-Training Quantization for MoE Models via Expert-Aware Optimization](https://arxiv.org/abs/2506.13329)
Append: [NTU Speechlab LLM-Based Multilingual ASR System for Interspeech MLC-SLM Challenge 2025](https://arxiv.org/abs/2506.13339)
Append: [Bi-directional Context-Enhanced Speech Large Language Models for Multilingual Conversational ASR](https://arxiv.org/abs/2506.13396)
Append: [Qwen vs. Gemma Integration with Whisper: A Comparative Study in Multilingual SpeechLLM Systems](https://arxiv.org/abs/2506.13596)
Append: [Thunder-DeID: Accurate and Efficient De-identification Framework for Korean Court Judgments](https://arxiv.org/abs/2506.15266)
Append: [Towards Fair RAG: On the Impact of Fair Ranking in Retrieval-Augmented Generation](https://arxiv.org/abs/2409.11598)
Append: [A Causal World Model Underlying Next Token Prediction: Exploring GPT in a Controlled Environment](https://arxiv.org/abs/2412.07446)
Append: [On the Expressiveness and Length Generalization of Selective State-Space Models on Regular Languages](https://arxiv.org/abs/2412.19350)
Append: [Towards Cost-Effective Reward Guided Text Generation](https://arxiv.org/abs/2502.04517)
Append: [SCALE: Towards Collaborative Content Analysis in Social Science with Large Language Model Agents and Human Intervention](https://arxiv.org/abs/2502.10937)
Append: [Do LLMs Understand the Safety of Their Inputs? Training-Free Moderation via Latent Prototypes](https://arxiv.org/abs/2502.16174)
Append: [Tip of the Tongue Query Elicitation for Simulated Evaluation](https://arxiv.org/abs/2502.17776)
Append: [Language Models can Self-Improve at State-Value Estimation for Better Search](https://arxiv.org/abs/2503.02878)
Append: [PENCIL: Long Thoughts with Short Memory](https://arxiv.org/abs/2503.14337)
Append: [Neural Discrete Token Representation Learning for Extreme Token Reduction in Video Large Language Models](https://arxiv.org/abs/2503.16980)
Append: [Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model](https://arxiv.org/abs/2503.24290)
Append: [Breach in the Shield: Unveiling the Vulnerabilities of Large Language Models](https://arxiv.org/abs/2504.03714)
Append: [Improving RL Exploration for LLM Reasoning through Retrospective Replay](https://arxiv.org/abs/2504.14363)
Append: [Perception, Reason, Think, and Plan: A Survey on Large Multimodal Reasoning Models](https://arxiv.org/abs/2505.04921)
Append: [Better Safe Than Sorry? Overreaction Problem of Vision Language Models in Visual Emergency Recognition](https://arxiv.org/abs/2505.15367)
Append: [BiMa: Towards Biases Mitigation for Text-Video Retrieval via Scene Element Guidance](https://arxiv.org/abs/2506.03589)
Append: [FinBERT2: A Specialized Bidirectional Encoder for Bridging the Gap in Finance-Specific Deployment of Large Language Models](https://arxiv.org/abs/2506.06335)
Append: [Play to Generalize: Learning to Reason Through Game Play](https://arxiv.org/abs/2506.08011)
Append: [The Geometries of Truth Are Orthogonal Across Tasks](https://arxiv.org/abs/2506.08572)
Append: [jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval](https://arxiv.org/abs/2506.18902)
append_entries: 291
Finish: 2025-07-08 04:34:14.482562
------------------------------------------------------
Started: 2025-07-08 06:27:20.905154
Existing_entries: 1291
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1309
Summarized using GPT-3.5-turbo
Append: [Comparative Evaluation of ChatGPT and DeepSeek Across Key NLP Tasks: Strengths, Weaknesses, and Domain-Specific Performance](https://arxiv.org/abs/2506.18501)
Token length: 976
Summarized using GPT-3.5-turbo
Append: [Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic Empirical Study](https://arxiv.org/abs/2506.19794)
Token length: 1690
Summarized using GPT-3.5-turbo
Append: [Inside you are many wolves: Using cognitive models to interpret value trade-offs in LLMs](https://arxiv.org/abs/2506.20666)
Token length: 1300
Summarized using GPT-3.5-turbo
Append: [RAG+: Enhancing Retrieval-Augmented Generation with Application-Aware Reasoning](https://arxiv.org/abs/2506.11555)
Token length: 1557
Summarized using GPT-3.5-turbo
Append: [KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality](https://arxiv.org/abs/2506.19807)
Token length: 1190
Summarized using GPT-3.5-turbo
Append: [Position: Machine Learning Conferences Should Establish a "Refutations and Critiques" Track](https://arxiv.org/abs/2506.19882)
append_entries: 6
Finish: 2025-07-08 06:27:37.439165
------------------------------------------------------
Started: 2025-07-08 08:24:00.461771
Existing_entries: 1006
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-08 08:24:01.145728
------------------------------------------------------
Started: 2025-07-08 10:18:42.949282
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-08 10:18:43.565452
------------------------------------------------------
Started: 2025-07-08 12:36:08.816331
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-08 12:36:09.431977
------------------------------------------------------
Started: 2025-07-08 14:16:22.813278
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-08 14:16:23.419772
------------------------------------------------------
Started: 2025-07-08 16:22:10.682716
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-08 16:22:11.293802
------------------------------------------------------
Started: 2025-07-08 18:24:20.519567
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-08 18:24:21.120729
------------------------------------------------------
Started: 2025-07-08 20:19:19.055895
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-08 20:19:19.674138
------------------------------------------------------
Started: 2025-07-08 22:16:50.316228
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-08 22:16:50.926641
------------------------------------------------------
Started: 2025-07-09 01:23:14.115783
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-09 01:23:14.778429
------------------------------------------------------
Started: 2025-07-09 03:20:13.544936
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-09 03:20:14.212804
------------------------------------------------------
Started: 2025-07-09 04:35:24.150181
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 998
Summarized using GPT-3.5-turbo
Append: [TokenShapley: Token Level Context Attribution with Shapley Value](https://arxiv.org/abs/2507.05261)
Token length: 910
Summarized using GPT-3.5-turbo
Append: [User Behavior Prediction as a Generic, Robust, Scalable, and Low-Cost Evaluation Strategy for Estimating Generalization in LLMs](https://arxiv.org/abs/2507.05266)
Token length: 1316
Summarized using GPT-3.5-turbo
Append: [An Adaptive Supervised Contrastive Learning Framework for Implicit Sexism Detection in Digital Social Networks](https://arxiv.org/abs/2507.05271)
Token length: 1672
Summarized using GPT-3.5-turbo
Append: [Beyond classical and contemporary models: a transformative ai framework for student dropout prediction in distance learning using rag, prompt engineering, and cross-modal fusion](https://arxiv.org/abs/2507.05285)
Token length: 1226
Summarized using GPT-3.5-turbo
Append: [LCDS: A Logic-Controlled Discharge Summary Generation System Supporting Source Attribution and Expert Review](https://arxiv.org/abs/2507.05319)
Token length: 764
Summarized using GPT-3.5-turbo
Append: [MindFlow: Revolutionizing E-commerce Customer Support with Multimodal LLM Agents](https://arxiv.org/abs/2507.05330)
Token length: 745
Summarized using GPT-3.5-turbo
Append: [LoRA-Augmented Generation (LAG) for Knowledge-Intensive Language Tasks](https://arxiv.org/abs/2507.05346)
Token length: 1286
Summarized using GPT-3.5-turbo
Append: [On the Bias of Next-Token Predictors Toward Systematically Inefficient Reasoning: A Shortest-Path Case Study](https://arxiv.org/abs/2507.05362)
Token length: 1146
Summarized using GPT-3.5-turbo
Append: [EduCoder: An Open-Source Annotation System for Education Transcript Data](https://arxiv.org/abs/2507.05385)
Token length: 1701
Summarized using GPT-3.5-turbo
Append: [The Generalization Ridge: Information Flow in Natural Language Generation](https://arxiv.org/abs/2507.05387)
Token length: 999
Summarized using GPT-3.5-turbo
Append: [Controlling What You Share: Assessing Language Model Adherence to Privacy Preferences](https://arxiv.org/abs/2507.05391)
Token length: 1521
Summarized using GPT-3.5-turbo
Append: [Learn Globally, Speak Locally: Bridging the Gaps in Multilingual Reasoning](https://arxiv.org/abs/2507.05418)
Token length: 1380
Summarized using GPT-3.5-turbo
Append: ["Lost-in-the-Later": Framework for Quantifying Contextual Grounding in Large Language Models](https://arxiv.org/abs/2507.05424)
Token length: 1171
Summarized using GPT-3.5-turbo
Append: [Gendered Divides in Online Discussions about Reproductive Rights](https://arxiv.org/abs/2507.05443)
Token length: 1239
Summarized using GPT-3.5-turbo
Append: [PhoniTale: Phonologically Grounded Mnemonic Generation for Typologically Distant Language Pairs](https://arxiv.org/abs/2507.05444)
Token length: 670
Summarized using GPT-3.5-turbo
Append: [On the Semantics of Large Language Models](https://arxiv.org/abs/2507.05448)
Token length: 1271
Summarized using GPT-3.5-turbo
Append: [ModelCitizens:Representing Community Voices in Online Safety](https://arxiv.org/abs/2507.05455)
Token length: 1198
Summarized using GPT-3.5-turbo
Append: [Empowering Healthcare Practitioners with Language Models: Structuring Speech Transcripts in Two Real-World Clinical Applications](https://arxiv.org/abs/2507.05517)
Token length: 1604
Summarized using GPT-3.5-turbo
Append: [Enhancing Test-Time Scaling of Large Language Models with Hierarchical Retrieval-Augmented MCTS](https://arxiv.org/abs/2507.05557)
Token length: 1964
Summarized using GPT-3.5-turbo
Append: [Self-Review Framework for Enhancing Instruction Following Capability of LLM](https://arxiv.org/abs/2507.05598)
Token length: 1496
Summarized using GPT-3.5-turbo
Append: [Flipping Knowledge Distillation: Leveraging Small Models' Expertise to Enhance LLMs in Text Matching](https://arxiv.org/abs/2507.05617)
Token length: 1298
Summarized using GPT-3.5-turbo
Append: [SARA: Selective and Adaptive Retrieval-augmented Generation with Context Compression](https://arxiv.org/abs/2507.05633)
Token length: 862
Summarized using GPT-3.5-turbo
Append: [ECom-Bench: Can LLM Agent Resolve Real-World E-commerce Customer Support Issues?](https://arxiv.org/abs/2507.05639)
Token length: 765
Summarized using GPT-3.5-turbo
Append: [Smoothie-Qwen: Post-Hoc Smoothing to Reduce Language Bias in Multilingual LLMs](https://arxiv.org/abs/2507.05686)
Token length: 946
Summarized using GPT-3.5-turbo
Append: [Agentic-R1: Distilled Dual-Strategy Reasoning](https://arxiv.org/abs/2507.05707)
Token length: 1284
Summarized using GPT-3.5-turbo
Append: [DRAGON: Dynamic RAG Benchmark On News](https://arxiv.org/abs/2507.05713)
Token length: 1558
Summarized using GPT-3.5-turbo
Append: [HIRAG: Hierarchical-Thought Instruction-Tuning Retrieval-Augmented Generation](https://arxiv.org/abs/2507.05714)
Token length: 1046
Summarized using GPT-3.5-turbo
Append: [Omni-Router: Sharing Routing Decisions in Sparse Mixture-of-Experts for Speech Recognition](https://arxiv.org/abs/2507.05724)
Token length: 937
Summarized using GPT-3.5-turbo
Append: [GPTKB v1.5: A Massive Knowledge Base for Exploring Factual LLM Knowledge](https://arxiv.org/abs/2507.05740)
Token length: 1163
Summarized using GPT-3.5-turbo
Append: [DocTalk: Scalable Graph-based Dialogue Synthesis for Enhancing LLM Conversational Capabilities](https://arxiv.org/abs/2507.05750)
Token length: 1896
Summarized using GPT-3.5-turbo
Append: [Flippi: End To End GenAI Assistant for E-Commerce](https://arxiv.org/abs/2507.05788)
Token length: 1234
Summarized using GPT-3.5-turbo
Append: [Bridging Perception and Language: A Systematic Benchmark for LVLMs' Understanding of Amodal Completion Reports](https://arxiv.org/abs/2507.05799)
Token length: 1031
Summarized using GPT-3.5-turbo
Append: [How to Evaluate Automatic Speech Recognition: Comparing Different Performance and Bias Measures](https://arxiv.org/abs/2507.05885)
Token length: 1352
Summarized using GPT-3.5-turbo
Append: [Psychometric Item Validation Using Virtual Respondents with Trait-Response Mediators](https://arxiv.org/abs/2507.05890)
Token length: 606
Summarized using GPT-3.5-turbo
Append: [Few-shot text-based emotion detection](https://arxiv.org/abs/2507.05918)
Token length: 1066
Summarized using GPT-3.5-turbo
Append: [Towards a Principled Evaluation of Knowledge Editors](https://arxiv.org/abs/2507.05937)
Token length: 1524
Summarized using GPT-3.5-turbo
Append: [Remember Past, Anticipate Future: Learning Continual Multimodal Misinformation Detectors](https://arxiv.org/abs/2507.05939)
Token length: 1713
Summarized using GPT-3.5-turbo
Append: [Chat-Ghosting: A Comparative Study of Methods for Auto-Completion in Dialog Systems](https://arxiv.org/abs/2507.05940)
Token length: 1343
Summarized using GPT-3.5-turbo
Append: [OpenFActScore: Open-Source Atomic Evaluation of Factuality in Text Generation](https://arxiv.org/abs/2507.05965)
Token length: 507
Summarized using GPT-3.5-turbo
Append: [We Should Evaluate Real-World Impact](https://arxiv.org/abs/2507.05973)
Token length: 1371
Summarized using GPT-3.5-turbo
Append: [RabakBench: Scaling Human Annotations to Construct Localized Multilingual Safety Benchmarks for Low-Resource Languages](https://arxiv.org/abs/2507.05980)
Token length: 1289
Summarized using GPT-3.5-turbo
Append: [Evolution without Large Models: Training Language Model with Task Principles](https://arxiv.org/abs/2507.05991)
Token length: 1212
Summarized using GPT-3.5-turbo
Append: [DocIE@XLLM25: In-Context Learning for Information Extraction using Fully Synthetic Demonstrations](https://arxiv.org/abs/2507.05997)
Token length: 844
Summarized using GPT-3.5-turbo
Append: [Conditional Multi-Stage Failure Recovery for Embodied Agents](https://arxiv.org/abs/2507.06016)
Token length: 971
Summarized using GPT-3.5-turbo
Append: [Entropy-Memorization Law: Evaluating Memorization Difficulty of Data in LLMs](https://arxiv.org/abs/2507.06056)
Token length: 929
Summarized using GPT-3.5-turbo
Append: [A Survey on Prompt Tuning](https://arxiv.org/abs/2507.06085)
Token length: 1859
Summarized using GPT-3.5-turbo
Append: [NeoBabel: A Multilingual Open Tower for Visual Generation](https://arxiv.org/abs/2507.06137)
Token length: 1228
Summarized using GPT-3.5-turbo
Append: [Coding Triangle: How Does Large Language Model Understand Code?](https://arxiv.org/abs/2507.06138)
Token length: 1572
Summarized using GPT-3.5-turbo
Append: [Skywork-R1V3 Technical Report](https://arxiv.org/abs/2507.06167)
Token length: 1513
Summarized using GPT-3.5-turbo
Append: [CriticLean: Critic-Guided Reinforcement Learning for Mathematical Formalization](https://arxiv.org/abs/2507.06181)
Append: [DS@GT at CheckThat! 2025: Detecting Subjectivity via Transfer-Learning and Corrective Data Augmentation](https://arxiv.org/abs/2507.06189)
Append: [DS@GT at CheckThat! 2025: Evaluating Context and Tokenization Strategies for Numerical Fact Verification](https://arxiv.org/abs/2507.06195)
Append: [UQLM: A Python Package for Uncertainty Quantification in Large Language Models](https://arxiv.org/abs/2507.06196)
Append: [A Survey on Latent Reasoning](https://arxiv.org/abs/2507.06203)
Append: [DS@GT at CheckThat! 2025: Ensemble Methods for Detection of Scientific Discourse on Social Media](https://arxiv.org/abs/2507.06205)
Append: [Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers](https://arxiv.org/abs/2507.06223)
Append: [Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving](https://arxiv.org/abs/2507.06229)
Append: [ReservoirChat: Interactive Documentation Enhanced with LLM and Knowledge Graph for ReservoirPy](https://arxiv.org/abs/2507.05279)
Append: [CoreCodeBench: A Configurable Multi-Scenario Repository-Level Benchmark](https://arxiv.org/abs/2507.05281)
Append: [Chat2SPaT: A Large Language Model Based Tool for Automating Traffic Signal Control Plan Management](https://arxiv.org/abs/2507.05283)
Append: [A Survey on Proactive Defense Strategies Against Misinformation in Large Language Models](https://arxiv.org/abs/2507.05288)
Append: [Structured Captions Improve Prompt Adherence in Text-to-Image Models (Re-LAION-Caption 19M)](https://arxiv.org/abs/2507.05300)
Append: [News Source Citing Patterns in AI Search Systems](https://arxiv.org/abs/2507.05301)
Append: [Narrowing the Gap: Supervised Fine-Tuning of Open-Source LLMs as a Viable Alternative to Proprietary Models for Pedagogical Tools](https://arxiv.org/abs/2507.05305)
Append: [Reinforcement Fine-Tuning Naturally Mitigates Forgetting in Continual Post-Training](https://arxiv.org/abs/2507.05386)
Append: [Fine-Grained Vision-Language Modeling for Multimodal Training Assistants in Augmented Reality](https://arxiv.org/abs/2507.05515)
Append: [Conversational Education at Scale: A Multi-LLM Agent Workflow for Procedural Learning and Pedagogic Quality Assessment](https://arxiv.org/abs/2507.05528)
Append: [Beyond Retrieval: Ensembling Cross-Encoders and GPT Rerankers with LLMs for Biomedical QA](https://arxiv.org/abs/2507.05577)
Append: [The Landscape of Memorization in LLMs: Mechanisms, Measurement, and Mitigation](https://arxiv.org/abs/2507.05578)
Append: [TuneShield: Mitigating Toxicity in Conversational AI while Fine-tuning on Untrusted Data](https://arxiv.org/abs/2507.05660)
Append: [AutoTriton: Automatic Triton Programming with Reinforcement Learning in LLMs](https://arxiv.org/abs/2507.05687)
Append: [MobileGUI-RL: Advancing Mobile GUI Agent through Reinforcement Learning in Online Environment](https://arxiv.org/abs/2507.05720)
Append: [ContextASR-Bench: A Massive Contextual Speech Recognition Benchmark](https://arxiv.org/abs/2507.05727)
Append: [Affective-ROPTester: Capability and Bias Analysis of LLMs in Predicting Retinopathy of Prematurity](https://arxiv.org/abs/2507.05816)
Append: [MusiScene: Leveraging MU-LLaMA for Scene Imagination and Enhanced Video Background Music Generation](https://arxiv.org/abs/2507.05894)
Append: [AI-Reporter: A Path to a New Genre of Scientific Communication](https://arxiv.org/abs/2507.05903)
Append: [Semantic Certainty Assessment in Vector Retrieval Systems: A Novel Framework for Embedding Quality Evaluation](https://arxiv.org/abs/2507.05933)
Append: [Development and Evaluation of HopeBot: an LLM-based chatbot for structured and interactive PHQ-9 depression screening](https://arxiv.org/abs/2507.05984)
Append: [Nyay-Darpan: Enhancing Decision Making Through Summarization and Case Retrieval for Consumer Law in India](https://arxiv.org/abs/2507.06090)
Append: [Evaluation of Habitat Robotics using Large Language Models](https://arxiv.org/abs/2507.06157)
Append: [Hidden Prompts in Manuscripts Exploit AI-Assisted Peer Review](https://arxiv.org/abs/2507.06185)
Append: [SQLBarber: A System Leveraging Large Language Models to Generate Customized and Realistic SQL Workloads](https://arxiv.org/abs/2507.06192)
Append: [Differential Mamba](https://arxiv.org/abs/2507.06204)
Append: [CultureCLIP: Empowering CLIP with Cultural Awareness through Synthetic Images and Contextualized Captions](https://arxiv.org/abs/2507.06210)
Append: [The distribution of syntactic dependency distances](https://arxiv.org/abs/2211.14620)
Append: [Detecting value-expressive text posts in Russian social media](https://arxiv.org/abs/2312.08968)
Append: [MEIT: Multimodal Electrocardiogram Instruction Tuning on Large Language Models for Report Generation](https://arxiv.org/abs/2403.04945)
Append: [News and Load: Social and Economic Drivers of Regional Multi-horizon Electricity Demand Forecasting](https://arxiv.org/abs/2406.06641)
Append: [Healing Powers of BERT: How Task-Specific Fine-Tuning Recovers Corrupted Language Models](https://arxiv.org/abs/2406.14459)
Append: [A Multi-Task and Multi-Label Classification Model for Implicit Discourse Relation Recognition](https://arxiv.org/abs/2408.08971)
Append: [What Would You Ask When You First Saw $a^2+b^2=c^2$? Evaluating LLM on Curiosity-Driven Questioning](https://arxiv.org/abs/2409.17172)
Append: [Evaluation of OpenAI o1: Opportunities and Challenges of AGI](https://arxiv.org/abs/2409.18486)
Append: [Adsorb-Agent: Autonomous Identification of Stable Adsorption Configurations via Large Language Model Agent](https://arxiv.org/abs/2410.16658)
Append: [Joint Beamforming and Speaker-Attributed ASR for Real Distant-Microphone Meeting Transcription](https://arxiv.org/abs/2410.21849)
Append: [One fish, two fish, but not the whole sea: Alignment reduces language models' conceptual diversity](https://arxiv.org/abs/2411.04427)
Append: [Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle](https://arxiv.org/abs/2411.08324)
Append: [Rethinking Associative Memory Mechanism in Induction Head](https://arxiv.org/abs/2412.11459)
Append: [Tractable Transformers for Flexible Conditional Generation](https://arxiv.org/abs/2502.07616)
Append: [Early-Exit and Instant Confidence Translation Quality Estimation](https://arxiv.org/abs/2502.14429)
Append: [MAMUT: A Novel Framework for Modifying Mathematical Formulas for the Generation of Specialized Datasets for Language Model Training](https://arxiv.org/abs/2502.20855)
Append: [Enhancing LLM Reliability via Explicit Knowledge Boundary Modeling](https://arxiv.org/abs/2503.02233)
Append: [GMLM: Bridging Graph Neural Networks and Language Models for Heterophilic Node Classification](https://arxiv.org/abs/2503.05763)
Append: [Seeing Sarcasm Through Different Eyes: Analyzing Multimodal Sarcasm Perception in Large Vision-Language Models](https://arxiv.org/abs/2503.12149)
Append: [A Survey on Transformer Context Extension: Approaches and Evaluation](https://arxiv.org/abs/2503.13299)
Append: [OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training Tokens](https://arxiv.org/abs/2504.07096)
Append: [Overcoming Data Scarcity in Generative Language Modelling for Low-Resource Languages: A Systematic Review](https://arxiv.org/abs/2505.04531)
Append: [FRAME: Feedback-Refined Agent Methodology for Enhancing Medical Research Insights](https://arxiv.org/abs/2505.04649)
Append: [Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning in Language Models](https://arxiv.org/abs/2505.15634)
Append: [MEF: A Capability-Aware Multi-Encryption Framework for Evaluating Vulnerabilities in Black-Box Large Language Models](https://arxiv.org/abs/2505.23404)
Append: [EEG2TEXT-CN: An Exploratory Study of Open-Vocabulary Chinese Text-EEG Alignment via Large Language Model and Contrastive Learning on ChineseEEG](https://arxiv.org/abs/2506.00854)
Append: [PulseReddit: A Novel Reddit Dataset for Benchmarking MAS in High-Frequency Cryptocurrency Trading](https://arxiv.org/abs/2506.03861)
Append: [FaithfulRAG: Fact-Level Conflict Modeling for Context-Faithful Retrieval-Augmented Generation](https://arxiv.org/abs/2506.08938)
Append: [Infini-gram mini: Exact n-gram Search at the Internet Scale with FM-Index](https://arxiv.org/abs/2506.12229)
Append: [Instruction Following by Boosting Attention of Large Language Models](https://arxiv.org/abs/2506.13734)
Append: [Tailored Conversations beyond LLMs: A RL-Based Dialogue Manager](https://arxiv.org/abs/2506.19652)
Append: [The Role of Deductive and Inductive Reasoning in Large Language Models](https://arxiv.org/abs/2410.02892)
Append: [Towards Exception Safety Code Generation with Intermediate Representation Agents Framework](https://arxiv.org/abs/2410.06949)
Append: [Feint and Attack: Attention-Based Strategies for Jailbreaking and Protecting LLMs](https://arxiv.org/abs/2410.16327)
Append: [The Impact of Prompt Programming on Function-Level Code Generation](https://arxiv.org/abs/2412.20545)
Append: [ViGiL3D: A Linguistically Diverse Dataset for 3D Visual Grounding](https://arxiv.org/abs/2501.01366)
Append: [Learning to Plan & Reason for Evaluation with Thinking-LLM-as-a-Judge](https://arxiv.org/abs/2501.18099)
Append: [Agents Are All You Need for LLM Unlearning](https://arxiv.org/abs/2502.00406)
Append: [Adaptive Tool Use in Large Language Models with Meta-Cognition Trigger](https://arxiv.org/abs/2502.12961)
Append: [Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition and Translation](https://arxiv.org/abs/2502.17380)
Append: [Bayesian Optimization for Controlled Image Editing via LLMs](https://arxiv.org/abs/2502.18116)
Append: [Analytic Subspace Routing: How Recursive Least Squares Works in Continual Learning of Large Language Model](https://arxiv.org/abs/2503.13575)
Append: [Offline Learning and Forgetting for Reasoning with Large Language Models](https://arxiv.org/abs/2504.11364)
Append: [ALLM4ADD: Unlocking the Capabilities of Audio Large Language Models for Audio Deepfake Detection](https://arxiv.org/abs/2505.11079)
Append: [Self-supervised learning of speech representations with Dutch archival data](https://arxiv.org/abs/2507.04554)
append_entries: 129
Finish: 2025-07-09 04:37:19.438025
------------------------------------------------------
Started: 2025-07-09 06:26:28.067400
Existing_entries: 1129
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 730
Summarized using GPT-3.5-turbo
Append: [Embedding-Based Approaches to Hyperpartisan News Detection](https://arxiv.org/abs/2501.01370)
append_entries: 1
Finish: 2025-07-09 06:26:30.755880
------------------------------------------------------
Started: 2025-07-09 08:23:36.483434
Existing_entries: 1001
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-09 08:23:36.801571
------------------------------------------------------
Started: 2025-07-09 10:19:17.423646
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-09 10:19:17.772365
------------------------------------------------------
Started: 2025-07-09 12:35:57.811753
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-09 12:35:58.138086
------------------------------------------------------
Started: 2025-07-09 14:16:54.847615
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-09 14:16:55.216863
------------------------------------------------------
Started: 2025-07-09 16:20:18.571965
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-09 16:20:18.898534
------------------------------------------------------
Started: 2025-07-09 18:24:37.502631
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-09 18:24:37.832565
------------------------------------------------------
Started: 2025-07-09 20:19:06.478807
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-09 20:19:06.869342
------------------------------------------------------
Started: 2025-07-09 22:16:45.791860
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-09 22:16:46.113776
------------------------------------------------------
Started: 2025-07-10 01:23:13.806240
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-10 01:23:14.125704
------------------------------------------------------
Started: 2025-07-10 03:19:52.141668
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-10 03:19:52.470898
------------------------------------------------------
Started: 2025-07-10 04:33:50.801140
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1050
Summarized using GPT-3.5-turbo
Append: [Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities](https://arxiv.org/abs/2507.06261)
Token length: 1637
Summarized using GPT-3.5-turbo
Append: [Humans overrely on overconfident language models, across languages](https://arxiv.org/abs/2507.06306)
Token length: 1227
Summarized using GPT-3.5-turbo
Append: [ETT: Expanding the Long Context Understanding Capability of LLMs at Test-Time](https://arxiv.org/abs/2507.06313)
Token length: 985
Summarized using GPT-3.5-turbo
Append: [Could the Road to Grounded, Neuro-symbolic AI be Paved with Words-as-Classifiers?](https://arxiv.org/abs/2507.06335)
Token length: 1069
Summarized using GPT-3.5-turbo
Append: [Evaluating Morphological Alignment of Tokenizers in 70 Languages](https://arxiv.org/abs/2507.06378)
Token length: 1257
Summarized using GPT-3.5-turbo
Append: [Hypermagmas and Colored Operads: Heads, Phases, and Theta Roles](https://arxiv.org/abs/2507.06393)
Token length: 1644
Summarized using GPT-3.5-turbo
Append: [PERK: Long-Context Reasoning as Parameter-Efficient Test-Time Learning](https://arxiv.org/abs/2507.06415)
Token length: 1510
Summarized using GPT-3.5-turbo
Append: [Reward Models Can Improve Themselves: Reward-Guided Adversarial Failure Mode Discovery for Robust Reward Modeling](https://arxiv.org/abs/2507.06419)
Token length: 775
Summarized using GPT-3.5-turbo
Append: [Exploring Task Performance with Interpretable Models via Sparse Auto-Encoders](https://arxiv.org/abs/2507.06427)
Token length: 1795
Summarized using GPT-3.5-turbo
Append: [Temporal Analysis of Climate Policy Discourse: Insights from Dynamic Embedded Topic Modeling](https://arxiv.org/abs/2507.06435)
Token length: 1725
Summarized using GPT-3.5-turbo
Append: [Perception-Aware Policy Optimization for Multimodal Reasoning](https://arxiv.org/abs/2507.06448)
Token length: 1203
Summarized using GPT-3.5-turbo
Append: [A Semantic Parsing Framework for End-to-End Time Normalization](https://arxiv.org/abs/2507.06450)
Token length: 1610
Summarized using GPT-3.5-turbo
Append: [A Systematic Analysis of Hybrid Linear Attention](https://arxiv.org/abs/2507.06457)
Token length: 1073
Summarized using GPT-3.5-turbo
Append: [On the Robustness of Verbal Confidence of LLMs in Adversarial Attacks](https://arxiv.org/abs/2507.06489)
Token length: 1572
Summarized using GPT-3.5-turbo
Append: [Pun Intended: Multi-Agent Translation of Wordplay with Contrastive Learning and Phonetic-Semantic Embeddings](https://arxiv.org/abs/2507.06506)
Token length: 1228
Summarized using GPT-3.5-turbo
Append: [SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers](https://arxiv.org/abs/2507.06517)
Token length: 1404
Summarized using GPT-3.5-turbo
Append: [InvestAlign: Overcoming Data Scarcity in Aligning Large Language Models with Investor Decision-Making Processes under Herd Behavior](https://arxiv.org/abs/2507.06528)
Token length: 1080
Summarized using GPT-3.5-turbo
Append: [Large Language Model for Extracting Complex Contract Information in Industrial Scenes](https://arxiv.org/abs/2507.06539)
Token length: 1241
Summarized using GPT-3.5-turbo
Append: [The Flaws of Others: An LLM-driven Framework for Scientific Knowledge Production](https://arxiv.org/abs/2507.06565)
Token length: 885
Summarized using GPT-3.5-turbo
Append: [Enhancing Food-Domain Question Answering with a Multimodal Knowledge Graph: Hybrid QA Generation and Diversity Analysis](https://arxiv.org/abs/2507.06571)
Token length: 1656
Summarized using GPT-3.5-turbo
Append: [Decoder-Hybrid-Decoder Architecture for Efficient Reasoning with Long Generation](https://arxiv.org/abs/2507.06607)
Token length: 1132
Summarized using GPT-3.5-turbo
Append: [FuDoBa: Fusing Document and Knowledge Graph-based Representations with Bayesian Optimisation](https://arxiv.org/abs/2507.06622)
Token length: 1764
Summarized using GPT-3.5-turbo
Append: [Expediting data extraction using a large language model (LLM) and scoping review protocol: a methodological study within a complex scoping review](https://arxiv.org/abs/2507.06623)
Token length: 966
Summarized using GPT-3.5-turbo
Append: [Elite Polarization in European Parliamentary Speeches: a Novel Measurement Approach Using Large Language Models](https://arxiv.org/abs/2507.06658)
Token length: 1574
Summarized using GPT-3.5-turbo
Append: [CLI-RAG: A Retrieval-Augmented Framework for Clinically Structured and Context Aware Text Generation with LLMs](https://arxiv.org/abs/2507.06715)
Token length: 1349
Summarized using GPT-3.5-turbo
Append: [On the Effect of Uncertainty on Layer-wise Inference Dynamics](https://arxiv.org/abs/2507.06722)
Token length: 1078
Summarized using GPT-3.5-turbo
Append: [KAConvText: Novel Approach to Burmese Sentence Classification using Kolmogorov-Arnold Convolution](https://arxiv.org/abs/2507.06753)
Token length: 964
Summarized using GPT-3.5-turbo
Append: [Checklist Engineering Empowers Multilingual LLM Judges](https://arxiv.org/abs/2507.06774)
Token length: 961
Summarized using GPT-3.5-turbo
Append: [Efficient Industrial sLLMs through Domain Adaptive Continual Pretraining: Method, Evaluation and Applications](https://arxiv.org/abs/2507.06795)
Token length: 1635
Summarized using GPT-3.5-turbo
Append: [Text to model via SysML: Automated generation of dynamical system computational models from unstructured natural language text via enhanced System Modeling Language diagrams](https://arxiv.org/abs/2507.06803)
Token length: 1396
Summarized using GPT-3.5-turbo
Append: [Adaptive Termination for Multi-round Parallel Reasoning: An Universal Semantic Entropy-Guided Framework](https://arxiv.org/abs/2507.06829)
Token length: 1009
Summarized using GPT-3.5-turbo
Append: [Shifting from Ranking to Set Selection for Retrieval Augmented Generation](https://arxiv.org/abs/2507.06838)
Token length: 853
Summarized using GPT-3.5-turbo
Append: [Developing and Maintaining an Open-Source Repository of AI Evaluations: Challenges and Insights](https://arxiv.org/abs/2507.06893)
Token length: 1575
Summarized using GPT-3.5-turbo
Append: [SCoRE: Streamlined Corpus-based Relation Extraction using Multi-Label Contrastive Learning and Bayesian kNN](https://arxiv.org/abs/2507.06895)
Token length: 1756
Summarized using GPT-3.5-turbo
Append: [VisualTrap: A Stealthy Backdoor Attack on GUI Agents via Visual Grounding Manipulation](https://arxiv.org/abs/2507.06899)
Token length: 1183
Summarized using GPT-3.5-turbo
Append: [MIND: A Multi-agent Framework for Zero-shot Harmful Meme Detection](https://arxiv.org/abs/2507.06908)
Token length: 1333
Summarized using GPT-3.5-turbo
Append: [MultiJustice: A Chinese Dataset for Multi-Party, Multi-Charge Legal Prediction](https://arxiv.org/abs/2507.06909)
Token length: 976
Summarized using GPT-3.5-turbo
Append: [Exploring LLMs for Predicting Tutor Strategy and Student Outcomes in Dialogues](https://arxiv.org/abs/2507.06910)
Token length: 1611
Summarized using GPT-3.5-turbo
Append: [Rethinking Verification for LLM Code Generation: From Generation to Testing](https://arxiv.org/abs/2507.06920)
Token length: 1183
Summarized using GPT-3.5-turbo
Append: [Investigating the Robustness of Retrieval-Augmented Generation at the Query Level](https://arxiv.org/abs/2507.06956)
Token length: 1519
Summarized using GPT-3.5-turbo
Append: [FRaN-X: FRaming and Narratives-eXplorer](https://arxiv.org/abs/2507.06974)
Token length: 1700
Summarized using GPT-3.5-turbo
Append: [FlexOlmo: Open Language Models for Flexible Data Use](https://arxiv.org/abs/2507.07024)
Token length: 1105
Summarized using GPT-3.5-turbo
Append: [UniConv: Unifying Retrieval and Response Generation for Large Language Models in Conversations](https://arxiv.org/abs/2507.07030)
Token length: 1788
Summarized using GPT-3.5-turbo
Append: [Discrete Diffusion Models for Language Generation](https://arxiv.org/abs/2507.07050)
Token length: 1009
Summarized using GPT-3.5-turbo
Append: [Super Kawaii Vocalics: Amplifying the "Cute" Factor in Computer Voice](https://arxiv.org/abs/2507.06235)
Token length: 1727
Summarized using GPT-3.5-turbo
Append: [Pronunciation-Lexicon Free Training for Phoneme-based Crosslingual ASR via Joint Stochastic Approximation](https://arxiv.org/abs/2507.06249)
Token length: 1640
Summarized using GPT-3.5-turbo
Append: [Emergent misalignment as prompt sensitivity: A research note](https://arxiv.org/abs/2507.06253)
Token length: 1951
Summarized using GPT-3.5-turbo
Append: [The bitter lesson of misuse detection](https://arxiv.org/abs/2507.06282)
Token length: 1183
Summarized using GPT-3.5-turbo
Append: [Can Interpretation Predict Behavior on Unseen Data?](https://arxiv.org/abs/2507.06445)
Token length: 1089
Summarized using GPT-3.5-turbo
Append: [Learning Japanese with Jouzu: Interaction Outcomes with Stylized Dialogue Fictional Agents](https://arxiv.org/abs/2507.06483)
Append: [Video-RTS: Rethinking Reinforcement Learning and Test-Time Scaling for Efficient and Enhanced Video Reasoning](https://arxiv.org/abs/2507.06485)
Append: [FIFA: Unified Faithfulness Evaluation Framework for Text-to-Video and Video-to-Text Generation](https://arxiv.org/abs/2507.06523)
Append: [DS@GT at CheckThat! 2025: Exploring Retrieval and Reranking Pipelines for Scientific Claim Source Retrieval on Social Media Discourse](https://arxiv.org/abs/2507.06563)
Append: [Civil Society in the Loop: Feedback-Driven Adaptation of (L)LM-Assisted Classification in an Open-Source Telegram Monitoring Tool](https://arxiv.org/abs/2507.06734)
Append: [Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning for Large Language Model](https://arxiv.org/abs/2507.06892)
Append: [Scaling Towards the Information Boundary of Instruction Set: InfinityInstruct-Subject Technical Report](https://arxiv.org/abs/2507.06968)
Append: [Learning Deliberately, Acting Intuitively: Unlocking Test-Time Reasoning in Multimodal LLMs](https://arxiv.org/abs/2507.06999)
Append: [DeepRetro: Retrosynthetic Pathway Discovery using Iterative LLM Reasoning](https://arxiv.org/abs/2507.07060)
Append: [Single Word Change is All You Need: Designing Attacks and Defenses for Text Classifiers](https://arxiv.org/abs/2401.17196)
Append: [Video-Language Understanding: A Survey from Model Architecture, Model Training, and Data Perspectives](https://arxiv.org/abs/2406.05615)
Append: [Automating IRAC Analysis in Malaysian Contract Law using a Semi-Structured Knowledge Base](https://arxiv.org/abs/2406.13217)
Append: [LASeR: Learning to Adaptively Select Reward Models with Multi-Armed Bandits](https://arxiv.org/abs/2410.01735)
Append: [FiRST: Finetuning Router-Selective Transformers for Input-Adaptive Latency Reduction](https://arxiv.org/abs/2410.12513)
Append: [CHAI for LLMs: Improving Code-Mixed Translation in Large Language Models through Reinforcement Learning with AI Feedback](https://arxiv.org/abs/2411.09073)
Append: [LCFO: Long Context and Long Form Output Dataset and Benchmarking](https://arxiv.org/abs/2412.08268)
Append: [AutoPrep: Natural Language Question-Aware Data Preparation with a Multi-Agent Framework](https://arxiv.org/abs/2412.10422)
Append: [Can Input Attributions Explain Inductive Reasoning in In-Context Learning?](https://arxiv.org/abs/2412.15628)
Append: [InfoTech Assistant: A Multimodal Conversational Agent for InfoTechnology Web Portal Queries](https://arxiv.org/abs/2412.16412)
Append: [Neuron-Level Differentiation of Memorization and Generalization in Large Language Models](https://arxiv.org/abs/2412.18497)
Append: [Theme-Explanation Structure for Table Summarization using Large Language Models: A Case Study on Korean Tabular Data](https://arxiv.org/abs/2501.10487)
Append: [NoLiMa: Long-Context Evaluation Beyond Literal Matching](https://arxiv.org/abs/2502.05167)
Append: [CMQCIC-Bench: A Chinese Benchmark for Evaluating Large Language Models in Medical Quality Control Indicator Calculation](https://arxiv.org/abs/2502.11703)
Append: [Teaching LLMs According to Their Aptitude: Adaptive Reasoning for Mathematical Problem Solving](https://arxiv.org/abs/2502.12022)
Append: [Multi-Attribute Steering of Language Models via Targeted Intervention](https://arxiv.org/abs/2502.12446)
Append: [LLM-based User Profile Management for Recommender System](https://arxiv.org/abs/2502.14541)
Append: [GuidedBench: Measuring and Mitigating the Evaluation Discrepancies of In-the-wild LLM Jailbreak Methods](https://arxiv.org/abs/2502.16903)
Append: [TokenSwift: Lossless Acceleration of Ultra Long Sequence Generation](https://arxiv.org/abs/2502.18890)
Append: [Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts](https://arxiv.org/abs/2503.09347)
Append: [Substance over Style: Evaluating Proactive Conversational Coaching Agents](https://arxiv.org/abs/2503.19328)
Append: [Adaptive Elicitation of Latent Information Using Natural Language](https://arxiv.org/abs/2504.04204)
Append: [Multi-Sense Embeddings for Language Models and Knowledge Distillation](https://arxiv.org/abs/2504.06036)
Append: [EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and Flexible LLM Fine-Tuning](https://arxiv.org/abs/2505.02579)
Append: [Knockout LLM Assessment: Using Large Language Models for Evaluations through Iterative Pairwise Comparisons](https://arxiv.org/abs/2506.03785)
Append: [Evaluating and Improving Robustness in Large Language Models: A Survey and Future Directions](https://arxiv.org/abs/2506.11111)
Append: [Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via Self-Critical Fine-Tuning](https://arxiv.org/abs/2506.21285)
Append: [OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework](https://arxiv.org/abs/2405.11143)
Append: [Refining Skewed Perceptions in Vision-Language Contrastive Models through Visual Representations](https://arxiv.org/abs/2405.14030)
Append: [CodeMirage: Hallucinations in Code Generated by Large Language Models](https://arxiv.org/abs/2408.08333)
Append: [Breaking PEFT Limitations: Leveraging Weak-to-Strong Knowledge Transfer for Backdoor Attacks in LLMs](https://arxiv.org/abs/2409.17946)
Append: [Planning Anything with Rigor: General-Purpose Zero-Shot Planning with LLM-based Formalized Programming](https://arxiv.org/abs/2410.12112)
Append: [Can adversarial attacks by large language models be attributed?](https://arxiv.org/abs/2411.08003)
Append: [FinSphere, a Real-Time Stock Analysis Agent Powered by Instruction-Tuned LLMs and Domain Tools](https://arxiv.org/abs/2501.12399)
Append: [Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning Large Language Models](https://arxiv.org/abs/2503.09567)
Append: [Do Larger Language Models Imply Better Generalization? A Pretraining Scaling Law for Implicit Reasoning](https://arxiv.org/abs/2504.03635)
append_entries: 94
Finish: 2025-07-10 04:35:59.489489
------------------------------------------------------
Started: 2025-07-10 06:26:56.921351
Existing_entries: 1094
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1631
Summarized using GPT-3.5-turbo
Append: [DeepTalk: Towards Seamless and Smart Speech Interaction with Adaptive Modality-Specific MoE](https://arxiv.org/abs/2506.21864)
append_entries: 1
Finish: 2025-07-10 06:26:59.891224
------------------------------------------------------
Started: 2025-07-10 08:23:40.191613
Existing_entries: 1001
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-10 08:23:40.446933
------------------------------------------------------
Started: 2025-07-10 10:19:13.633075
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-10 10:19:13.883337
------------------------------------------------------
Started: 2025-07-10 12:36:23.123010
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-10 12:36:23.380618
------------------------------------------------------
Started: 2025-07-10 14:17:43.588536
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-10 14:17:43.887371
------------------------------------------------------
Started: 2025-07-10 16:21:46.486898
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-10 16:21:46.746760
------------------------------------------------------
Started: 2025-07-10 18:24:45.385222
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-10 18:24:45.635282
------------------------------------------------------
Started: 2025-07-10 20:19:08.033072
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-10 20:19:08.310001
------------------------------------------------------
Started: 2025-07-10 22:16:23.647577
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-10 22:16:23.913708
------------------------------------------------------
Started: 2025-07-11 01:24:10.071613
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-11 01:24:10.389451
------------------------------------------------------
Started: 2025-07-11 03:25:32.218938
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-11 03:25:32.477491
------------------------------------------------------
Started: 2025-07-11 04:39:21.532929
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1389
Summarized using GPT-3.5-turbo
Append: [Planted in Pretraining, Swayed by Finetuning: A Case Study on the Origins of Cognitive Biases in LLMs](https://arxiv.org/abs/2507.07186)
Token length: 1198
Summarized using GPT-3.5-turbo
Append: [Prompt Perturbations Reveal Human-Like Biases in LLM Survey Responses](https://arxiv.org/abs/2507.07188)
Token length: 1191
Summarized using GPT-3.5-turbo
Append: [SynthTextEval: Synthetic Text Data Generation and Evaluation for High-Stakes Domains](https://arxiv.org/abs/2507.07229)
Token length: 1354
Summarized using GPT-3.5-turbo
Append: [Medical Red Teaming Protocol of Language Models: On the Importance of User Perspectives in Healthcare Settings](https://arxiv.org/abs/2507.07248)
Token length: 1223
Summarized using GPT-3.5-turbo
Append: [The Impact of Background Speech on Interruption Detection in Collaborative Groups](https://arxiv.org/abs/2507.07280)
Token length: 1143
Summarized using GPT-3.5-turbo
Append: [Multi-Agent Retrieval-Augmented Framework for Evidence-Based Counterspeech Against Health Misinformation](https://arxiv.org/abs/2507.07307)
Token length: 1552
Summarized using GPT-3.5-turbo
Append: [GNN-CNN: An Efficient Hybrid Model of Convolutional and Graph Neural Networks for Text Representation](https://arxiv.org/abs/2507.07414)
Token length: 1165
Summarized using GPT-3.5-turbo
Append: [MedReadCtrl: Personalizing medical text generation with readability-controlled instruction learning](https://arxiv.org/abs/2507.07419)
Token length: 1164
Summarized using GPT-3.5-turbo
Append: [SynthEHR-Eviction: Enhancing Eviction SDoH Detection with LLM-Augmented Synthetic EHR Data](https://arxiv.org/abs/2507.07421)
Token length: 1064
Summarized using GPT-3.5-turbo
Append: [Towards Interpretable Time Series Foundation Models](https://arxiv.org/abs/2507.07439)
Token length: 1350
Summarized using GPT-3.5-turbo
Append: [SAND: Boosting LLM Agents with Self-Taught Action Deliberation](https://arxiv.org/abs/2507.07441)
Token length: 1169
Summarized using GPT-3.5-turbo
Append: [RLEP: Reinforcement Learning with Experience Replay for LLM Reasoning](https://arxiv.org/abs/2507.07451)
Token length: 1445
Summarized using GPT-3.5-turbo
Append: [Machine Bullshit: Characterizing the Emergent Disregard for Truth in Large Language Models](https://arxiv.org/abs/2507.07484)
Token length: 1265
Summarized using GPT-3.5-turbo
Append: [PLAN-TUNING: Post-Training Language Models to Learn Step-by-Step Planning for Complex Problem Solving](https://arxiv.org/abs/2507.07495)
Token length: 1188
Summarized using GPT-3.5-turbo
Append: [Teaching LLM to Reason: Reinforcement Learning from Algorithmic Problems without Code](https://arxiv.org/abs/2507.07498)
Token length: 1665
Summarized using GPT-3.5-turbo
Append: [Extracting ORR Catalyst Information for Fuel Cell from Scientific Literature](https://arxiv.org/abs/2507.07499)
Token length: 1036
Summarized using GPT-3.5-turbo
Append: [Hallucination Stations: On Some Basic Limitations of Transformer-Based Language Models](https://arxiv.org/abs/2507.07505)
Token length: 1324
Summarized using GPT-3.5-turbo
Append: [Toward Real-World Chinese Psychological Support Dialogues: CPsDD Dataset and a Co-Evolving Multi-Agent System](https://arxiv.org/abs/2507.07509)
Token length: 915
Summarized using GPT-3.5-turbo
Append: [Triadic Multi-party Voice Activity Projection for Turn-taking in Spoken Dialogue Systems](https://arxiv.org/abs/2507.07518)
Token length: 1260
Summarized using GPT-3.5-turbo
Append: [CEA-LIST at CheckThat! 2025: Evaluating LLMs as Detectors of Bias and Opinion in Text](https://arxiv.org/abs/2507.07539)
Token length: 1512
Summarized using GPT-3.5-turbo
Append: [The Cross-Lingual Cost: Retrieval Biases in RAG over Arabic-English Corpora](https://arxiv.org/abs/2507.07543)
Token length: 1320
Summarized using GPT-3.5-turbo
Append: [The Synergy Dilemma of Long-CoT SFT and RL: Investigating Post-Training Techniques for Reasoning VLMs](https://arxiv.org/abs/2507.07562)
Token length: 962
Summarized using GPT-3.5-turbo
Append: [Single-to-mix Modality Alignment with Multimodal Large Language Model for Document Image Machine Translation](https://arxiv.org/abs/2507.07572)
Token length: 873
Summarized using GPT-3.5-turbo
Append: [Bayesian Discrete Diffusion Beats Autoregressive Perplexity](https://arxiv.org/abs/2507.07586)
Token length: 1074
Summarized using GPT-3.5-turbo
Append: [Exploring the Limits of Model Compression in LLMs: A Knowledge Distillation Study on QA Tasks](https://arxiv.org/abs/2507.07630)
Token length: 1501
Summarized using GPT-3.5-turbo
Append: [FrugalRAG: Learning to retrieve and reason for multi-hop QA](https://arxiv.org/abs/2507.07634)
Token length: 1168
Summarized using GPT-3.5-turbo
Append: [Lost in Pronunciation: Detecting Chinese Offensive Language Disguised by Phonetic Cloaking Replacement](https://arxiv.org/abs/2507.07640)
Token length: 998
Summarized using GPT-3.5-turbo
Append: [An Automated Length-Aware Quality Metric for Summarization](https://arxiv.org/abs/2507.07653)
Token length: 1522
Summarized using GPT-3.5-turbo
Append: [SAS: Simulated Attention Score](https://arxiv.org/abs/2507.07694)
Token length: 1953
Summarized using GPT-3.5-turbo
Append: [KeyKnowledgeRAG (K^2RAG): An Enhanced RAG method for improved LLM question-answering capabilities](https://arxiv.org/abs/2507.07695)
Token length: 1931
Summarized using GPT-3.5-turbo
Append: [Rethinking the Privacy of Text Embeddings: A Reproducibility Study of "Text Embeddings Reveal (Almost) As Much As Text"](https://arxiv.org/abs/2507.07700)
Token length: 1087
Summarized using GPT-3.5-turbo
Append: [Not All Preferences are What You Need for Post-Training: Selective Alignment Strategy for Preference Optimization](https://arxiv.org/abs/2507.07725)
Token length: 691
Summarized using GPT-3.5-turbo
Append: [Code-Switching in End-to-End Automatic Speech Recognition: A Systematic Literature Review](https://arxiv.org/abs/2507.07741)
Token length: 1752
Summarized using GPT-3.5-turbo
Append: [When Large Language Models Meet Law: Dual-Lens Taxonomy, Technical Advances, and Ethical Governance](https://arxiv.org/abs/2507.07748)
Token length: 1508
Summarized using GPT-3.5-turbo
Append: [StreamUni: Achieving Streaming Speech Translation with a Unified Large Speech-Language Model](https://arxiv.org/abs/2507.07803)
Token length: 1669
Summarized using GPT-3.5-turbo
Append: [Bridging Logic and Learning: Decoding Temporal Logic Embeddings via Transformers](https://arxiv.org/abs/2507.07808)
Token length: 714
Summarized using GPT-3.5-turbo
Append: [Understanding and Controlling Repetition Neurons and Induction Heads in In-Context Learning](https://arxiv.org/abs/2507.07810)
Token length: 1546
Summarized using GPT-3.5-turbo
Append: [On the Effect of Instruction Tuning Loss on Generalization](https://arxiv.org/abs/2507.07817)
Token length: 1068
Summarized using GPT-3.5-turbo
Append: [Conditional Unigram Tokenization with Parallel Data](https://arxiv.org/abs/2507.07824)
Token length: 1415
Summarized using GPT-3.5-turbo
Append: [From Ambiguity to Accuracy: The Transformative Effect of Coreference Resolution on Retrieval-Augmented Generation systems](https://arxiv.org/abs/2507.07847)
Token length: 1576
Summarized using GPT-3.5-turbo
Append: [Alpay Algebra V: Multi-Layered Semantic Games and Transfinite Fixed-Point Simulation](https://arxiv.org/abs/2507.07868)
Token length: 1363
Summarized using GPT-3.5-turbo
Append: [DocCHA: Towards LLM-Augmented Interactive Online diagnosis System](https://arxiv.org/abs/2507.07870)
Token length: 1427
Summarized using GPT-3.5-turbo
Append: [Automating MD simulations for Proteins using Large language Models: NAMD-Agent](https://arxiv.org/abs/2507.07887)
Token length: 1180
Summarized using GPT-3.5-turbo
Append: [DTECT: Dynamic Topic Explorer & Context Tracker](https://arxiv.org/abs/2507.07910)
Token length: 1440
Summarized using GPT-3.5-turbo
Append: [SAGE: A Visual Language Model for Anomaly Detection via Fact Enhancement and Entropy-aware Alignment](https://arxiv.org/abs/2507.07939)
Token length: 1950
Summarized using GPT-3.5-turbo
Append: [MIRIX: Multi-Agent Memory System for LLM-Based Agents](https://arxiv.org/abs/2507.07957)
Token length: 1503
Summarized using GPT-3.5-turbo
Append: [Why is Your Language Model a Poor Implicit Reward Model?](https://arxiv.org/abs/2507.07981)
Token length: 627
Summarized using GPT-3.5-turbo
Append: [Performance and Practical Considerations of Large and Small Language Models in Clinical Decision Support in Rheumatology](https://arxiv.org/abs/2507.07983)
Token length: 1313
Summarized using GPT-3.5-turbo
Append: [Automating Expert-Level Medical Reasoning Evaluation of Large Language Models](https://arxiv.org/abs/2507.07988)
Token length: 953
Summarized using GPT-3.5-turbo
Append: [PyVision: Agentic Vision with Dynamic Tooling](https://arxiv.org/abs/2507.07998)
Append: [Multi-level Mixture of Experts for Multimodal Entity Linking](https://arxiv.org/abs/2507.07108)
Append: [Growing Transformers: Modular Composition and Layer-wise Expansion on a Frozen Substrate](https://arxiv.org/abs/2507.07129)
Append: [Robust Multimodal Large Language Models Against Modality Conflict](https://arxiv.org/abs/2507.07151)
Append: [An Information-Theoretic Perspective on Multi-LLM Uncertainty Estimation](https://arxiv.org/abs/2507.07236)
Append: [A Language-Driven Framework for Improving Personalized Recommendations: Merging LLMs with Traditional Algorithms](https://arxiv.org/abs/2507.07251)
Append: [Open Source Planning & Control System with Language Agents for Autonomous Scientific Discovery](https://arxiv.org/abs/2507.07257)
Append: [LinguaMark: Do Multimodal Models Speak Fairly? A Benchmark-Based Evaluation](https://arxiv.org/abs/2507.07274)
Append: [ViDove: A Translation Agent System with Multimodal Context and Memory-Augmented Reasoning](https://arxiv.org/abs/2507.07306)
Append: [Bradley-Terry and Multi-Objective Reward Modeling Are Complementary](https://arxiv.org/abs/2507.07375)
Append: [May I have your Attention? Breaking Fine-Tuning based Prompt Injection Defenses using Architecture-Aware Attacks](https://arxiv.org/abs/2507.07417)
Append: [COALA: Numerically Stable and Efficient Framework for Context-Aware Low-Rank Approximation](https://arxiv.org/abs/2507.07580)
Append: [Improving Clustering on Occupational Text Data through Dimensionality Reduction](https://arxiv.org/abs/2507.07582)
Append: [SpatialViz-Bench: Automatically Generated Spatial Visualization Reasoning Tasks for MLLMs](https://arxiv.org/abs/2507.07610)
Append: [GuardVal: Dynamic Large Language Model Jailbreak Evaluation for Comprehensive Safety Testing](https://arxiv.org/abs/2507.07735)
Append: [Scaling RL to Long Videos](https://arxiv.org/abs/2507.07966)
Append: [Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and Methodology](https://arxiv.org/abs/2507.07999)
Append: [A Theory of Response Sampling in LLMs: Part Descriptive and Part Prescriptive](https://arxiv.org/abs/2402.11005)
Append: [Beyond Hate Speech: NLP's Challenges and Opportunities in Uncovering Dehumanizing Language](https://arxiv.org/abs/2402.13818)
Append: [Improving Cross-lingual Representation for Semantic Retrieval with Code-switching](https://arxiv.org/abs/2403.01364)
Append: [A Comprehensive Survey of Contamination Detection Methods in Large Language Models](https://arxiv.org/abs/2404.00699)
Append: [Truth-value judgment in language models: 'truth directions' are context sensitive](https://arxiv.org/abs/2404.18865)
Append: [CheckEmbed: Effective Verification of LLM Solutions to Open-Ended Tasks](https://arxiv.org/abs/2406.02524)
Append: [Multi-Head RAG: Solving Multi-Aspect Problems with LLMs](https://arxiv.org/abs/2406.05085)
Append: [Unsupervised Morphological Tree Tokenizer](https://arxiv.org/abs/2406.15245)
Append: [Operationalizing a Threat Model for Red-Teaming Large Language Models (LLMs)](https://arxiv.org/abs/2407.14937)
Append: [Derailer-Rerailer: Adaptive Verification for Efficient and Reliable Language Model Reasoning](https://arxiv.org/abs/2408.13940)
Append: [Investigating Context-Faithfulness in Large Language Models: The Roles of Memory Strength and Evidence Style](https://arxiv.org/abs/2409.10955)
Append: [TART: An Open-Source Tool-Augmented Framework for Explainable Table-based Reasoning](https://arxiv.org/abs/2409.11724)
Append: [Emoji Attack: Enhancing Jailbreak Attacks Against Judge LLM Detection](https://arxiv.org/abs/2411.01077)
Append: [Inter-linguistic Phonetic Composition (IPC): A Theoretical and Computational Approach to Enhance Second Language Pronunciation](https://arxiv.org/abs/2411.10927)
Append: [Understanding Chain-of-Thought in LLMs through Information Theory](https://arxiv.org/abs/2411.11984)
Append: [CoAM: Corpus of All-Type Multiword Expressions](https://arxiv.org/abs/2412.18151)
Append: [Long-Form Speech Generation with Spoken Language Models](https://arxiv.org/abs/2412.18603)
Append: [Enhancing Transformers for Generalizable First-Order Logical Entailment](https://arxiv.org/abs/2501.00759)
Append: [Decoding AI Judgment: How LLMs Assess News Credibility and Bias](https://arxiv.org/abs/2502.04426)
Append: [None of the Others: a General Technique to Distinguish Reasoning from Memorization in Multiple-Choice LLM Evaluation Benchmarks](https://arxiv.org/abs/2502.12896)
Append: [Good/Evil Reputation Judgment of Celebrities by LLMs via Retrieval Augmented Generation](https://arxiv.org/abs/2503.14382)
Append: [Investigating Co-Constructive Behavior of Large Language Models in Explanation Dialogues](https://arxiv.org/abs/2504.18483)
Append: [Comparative sentiment analysis of public perception: Monkeypox vs. COVID-19 behavioral insights](https://arxiv.org/abs/2505.07430)
Append: [Hierarchical Bracketing Encodings for Dependency Parsing as Tagging](https://arxiv.org/abs/2505.11693)
Append: [Evaluating Robustness of Large Audio Language Models to Audio Injection: An Empirical Study](https://arxiv.org/abs/2505.19598)
Append: [Long Context Scaling: Divide and Conquer via Multi-Agent Question-driven Collaboration](https://arxiv.org/abs/2505.20625)
Append: [What do self-supervised speech models know about Dutch? Analyzing advantages of language-specific pre-training](https://arxiv.org/abs/2506.00981)
Append: [Watermarking Degrades Alignment in Language Models: Analysis and Mitigation](https://arxiv.org/abs/2506.04462)
Append: [When Dialects Collide: How Socioeconomic Mixing Affects Language Use](https://arxiv.org/abs/2307.10016)
Append: [Structure Guided Large Language Model for SQL Generation](https://arxiv.org/abs/2402.13284)
Append: [SimSUM: Simulated Benchmark with Structured and Unstructured Medical Records](https://arxiv.org/abs/2409.08936)
Append: [Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation](https://arxiv.org/abs/2503.19092)
Append: [Towards a cognitive architecture to enable natural language interaction in co-constructive task learning](https://arxiv.org/abs/2503.23760)
Append: [Affordable AI Assistants with Knowledge Graph of Thoughts](https://arxiv.org/abs/2504.02670)
Append: [Mixture of Group Experts for Learning Invariant Representations](https://arxiv.org/abs/2504.09265)
Append: [BountyBench: Dollar Impact of AI Agent Attackers and Defenders on Real-World Cybersecurity Systems](https://arxiv.org/abs/2505.15216)
Append: [MAEBE: Multi-Agent Emergent Behavior Framework](https://arxiv.org/abs/2506.03053)
Append: [Thought Crime: Backdoors and Emergent Misalignment in Reasoning Models](https://arxiv.org/abs/2506.13206)
Append: [video-SALMONN 2: Captioning-Enhanced Audio-Visual Large Language Models](https://arxiv.org/abs/2506.15220)
append_entries: 105
Finish: 2025-07-11 04:41:35.932798
------------------------------------------------------
Started: 2025-07-11 06:26:51.782916
Existing_entries: 1105
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-11 06:26:52.076604
------------------------------------------------------
Started: 2025-07-11 08:23:30.295222
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-11 08:23:30.598853
------------------------------------------------------
Started: 2025-07-11 10:18:52.839187
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-11 10:18:53.117002
------------------------------------------------------
Started: 2025-07-11 12:34:58.218988
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-11 12:34:58.496424
------------------------------------------------------
Started: 2025-07-11 14:17:32.537820
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-11 14:17:32.806764
------------------------------------------------------
Started: 2025-07-11 16:21:39.606849
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-11 16:21:39.876049
------------------------------------------------------
Started: 2025-07-11 18:24:05.365864
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-11 18:24:05.667799
------------------------------------------------------
Started: 2025-07-11 20:19:12.471779
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-11 20:19:12.806589
------------------------------------------------------
Started: 2025-07-11 22:16:29.625944
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-11 22:16:29.940374
------------------------------------------------------
Started: 2025-07-12 01:25:38.320823
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-12 01:25:38.590079
------------------------------------------------------
Started: 2025-07-12 03:23:17.773505
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-12 03:23:18.080711
------------------------------------------------------
Started: 2025-07-12 04:28:53.227499
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-12 04:28:53.287801
------------------------------------------------------
Started: 2025-07-12 06:23:55.557020
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-12 06:23:55.649539
------------------------------------------------------
Started: 2025-07-12 08:20:46.886757
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-12 08:20:46.942762
------------------------------------------------------
Started: 2025-07-12 10:17:05.821144
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-12 10:17:05.896225
------------------------------------------------------
Started: 2025-07-12 12:32:33.258015
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-12 12:32:33.322918
------------------------------------------------------
Started: 2025-07-12 14:15:06.447086
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-12 14:15:06.505938
------------------------------------------------------
Started: 2025-07-12 16:19:26.068747
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-12 16:19:26.148326
------------------------------------------------------
Started: 2025-07-12 18:22:32.109517
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-12 18:22:32.199822
------------------------------------------------------
Started: 2025-07-12 20:17:50.410887
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-12 20:17:50.470837
------------------------------------------------------
Started: 2025-07-12 22:16:04.818074
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-12 22:16:04.879121
------------------------------------------------------
Started: 2025-07-13 01:40:32.645222
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-13 01:40:32.738937
------------------------------------------------------
Started: 2025-07-13 03:38:10.221882
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-13 03:38:10.281518
------------------------------------------------------
Started: 2025-07-13 04:34:57.231956
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-13 04:34:57.335132
------------------------------------------------------
Started: 2025-07-13 06:24:58.814801
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-13 06:24:58.878338
------------------------------------------------------
Started: 2025-07-13 08:20:44.372105
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-13 08:20:44.445834
------------------------------------------------------
Started: 2025-07-13 10:18:00.614585
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-13 10:18:00.689934
------------------------------------------------------
Started: 2025-07-13 12:32:53.908463
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-13 12:32:54.003799
------------------------------------------------------
Started: 2025-07-13 14:15:04.013571
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-13 14:15:04.090087
------------------------------------------------------
Started: 2025-07-13 16:19:39.317518
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-13 16:19:39.400921
------------------------------------------------------
Started: 2025-07-13 18:22:27.940161
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-13 18:22:28.036359
------------------------------------------------------
Started: 2025-07-13 20:17:36.943075
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-13 20:17:37.010955
------------------------------------------------------
Started: 2025-07-13 22:15:57.263672
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-13 22:15:57.325230
------------------------------------------------------
Started: 2025-07-14 01:27:31.675713
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-14 01:27:31.751254
------------------------------------------------------
Started: 2025-07-14 03:38:40.488043
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-14 03:38:40.551816
------------------------------------------------------
Started: 2025-07-14 04:39:18.831726
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123919131097902H0z82N3M)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [RepeaTTS: Towards Feature Discovery through Repeated Fine-Tuning](https://arxiv.org/abs/2507.08012)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123919272015323o86cB3mE)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [MedicalBERT: enhancing biomedical natural language processing using pretrained BERT-based model](https://arxiv.org/abs/2507.08013)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123919399586574fyIcNbU7)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [Mass-Scale Analysis of In-the-Wild Conversations Reveals Complexity Bounds on LLM Jailbreaking](https://arxiv.org/abs/2507.08014)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 202507141239195184749469EMMIkgO)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [Assessing the Capabilities and Limitations of FinGPT Model in Financial NLP Applications](https://arxiv.org/abs/2507.08015)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123919656180531BDcfBe8L)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [Mechanistic Indicators of Understanding in Large Language Models](https://arxiv.org/abs/2507.08017)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123919800788149cUSXvDwb)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [Review, Remask, Refine (R3): Process-Guided Block Diffusion for Text Generation](https://arxiv.org/abs/2507.08018)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123919925566810JN77XHJG)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [Signal or Noise? Evaluating Large Language Models in Resume Screening Across Contextual Variations and Human Expert Benchmarks](https://arxiv.org/abs/2507.08019)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 2025071412392051034016RfYYYrWy)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [Circumventing Safety Alignment in Large Language Models Through Embedding Space Toxicity Attenuation](https://arxiv.org/abs/2507.08020)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123920231861399B2u8CS6S)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [Unveiling Effective In-Context Configurations for Image Captioning: An External & Internal Analysis](https://arxiv.org/abs/2507.08021)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123920430912478IsUbkT6h)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: ["Amazing, They All Lean Left" -- Analyzing the Political Temperaments of Current LLMs](https://arxiv.org/abs/2507.08027)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123920635215303LqMvgb1o)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [Better Together: Quantifying the Benefits of AI-Assisted Recruitment](https://arxiv.org/abs/2507.08029)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123920765714970R3T5U3wG)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [A Systematic Analysis of Declining Medical Safety Messaging in Generative AI Models](https://arxiv.org/abs/2507.08030)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 202507141239209030343104t1bzzwi)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [Beyond Scale: Small Language Models are Comparable to GPT-4 in Mental Health Understanding](https://arxiv.org/abs/2507.08031)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 202507141239213933049357eLAued)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [Integrating External Tools with Large Language Models to Improve Accuracy](https://arxiv.org/abs/2507.08034)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123921213671964DbbTUiwA)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [Barriers in Integrating Medical Visual Question Answering into Radiology Workflows: A Scoping Review and Clinicians' Insights](https://arxiv.org/abs/2507.08036)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123921363860840Bdsjg5A5)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [CRISP: Complex Reasoning with Interpretable Step-based Plans](https://arxiv.org/abs/2507.08037)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123921528556771ySr9NPc4)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [AblationBench: Evaluating Automated Planning of Ablations in Empirical AI Research](https://arxiv.org/abs/2507.08038)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123921663321002g2CDoXch)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [Krul: Efficient State Restoration for Multi-turn Conversations with Dynamic Cross-layer KV Sharing](https://arxiv.org/abs/2507.08045)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123921792699221eKSepAC4)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [GRASP: Generic Reasoning And SPARQL Generation across Knowledge Graphs](https://arxiv.org/abs/2507.08107)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123921942292541wG15eu7F)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [Audit, Alignment, and Optimization of LM-Powered Subroutines with Application to Public Comment Processing](https://arxiv.org/abs/2507.08109)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123922738489610SotMiqS)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [Compactor: Calibrated Query-Agnostic KV Cache Compression with Approximate Leverage Scores](https://arxiv.org/abs/2507.08143)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123922208927773XklohhPu)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [Distilling Empathy from Large Language Models](https://arxiv.org/abs/2507.08151)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123922352229826ukv1ep9P)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [TruthTorchLM: A Comprehensive Library for Predicting Truthfulness in LLM Outputs](https://arxiv.org/abs/2507.08203)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123922507844208l096dPTH)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [Simple Mechanistic Explanations for Out-Of-Context Reasoning](https://arxiv.org/abs/2507.08218)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123922661344458okPeUJJO)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [Can LLMs Reliably Simulate Real Students' Abilities in Mathematics and Reading Comprehension?](https://arxiv.org/abs/2507.08232)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123922802827257tpiLq1In)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [Exploring Gender Differences in Chronic Pain Discussions on Reddit](https://arxiv.org/abs/2507.08241)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123922966153560kxSPwtRq)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [KAT-V1: Kwai-AutoThink Technical Report](https://arxiv.org/abs/2507.08297)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123923109077840yUjck9Bo)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [Improving MLLM's Document Image Machine Translation via Synchronously Self-reviewing Its OCR Proficiency](https://arxiv.org/abs/2507.08309)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123923242071755GY0r3Ymd)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [CRMAgent: A Multi-Agent LLM System for E-Commerce CRM Message Template Generation](https://arxiv.org/abs/2507.08325)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123923389809559JrThyoiz)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [MK2 at PBIG Competition: A Prompt Generation Solution](https://arxiv.org/abs/2507.08335)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123923537126026VLyXeewK)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [Distillation versus Contrastive Learning: How to Train Your Rerankers](https://arxiv.org/abs/2507.08336)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123923667959223yG8h4YgW)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [What Factors Affect LLMs and RLLMs in Financial Question Answering?](https://arxiv.org/abs/2507.08339)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123923805970844fkpDfXxR)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [Beyond N-Grams: Rethinking Evaluation Metrics and Strategies for Multilingual Abstractive Summarization](https://arxiv.org/abs/2507.08342)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123923936847514uUkS5SkB)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [Exploring Design of Multi-Agent LLM Dialogues for Research Ideation](https://arxiv.org/abs/2507.08350)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123924130832158iwnhU9RZ)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [The Curious Case of Factuality Finetuning: Models' Internal Beliefs Can Improve Factuality](https://arxiv.org/abs/2507.08371)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123924302030973dNdBep3N)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [A Survey of Large Language Models in Discipline-specific Research: Challenges, Methods and Opportunities](https://arxiv.org/abs/2507.08425)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 202507141239244312706473B7suyKo)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [ChainEdit: Propagating Ripple Effects in LLM Knowledge Editing through Logical Rule-Guided Chains](https://arxiv.org/abs/2507.08427)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123924562869370ovFGLFie)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [Finding Common Ground: Using Large Language Models to Detect Agreement in Multi-Agent Decision Conferences](https://arxiv.org/abs/2507.08440)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123924703690804UQPp9QWO)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [Diagnosing Failures in Large Language Models' Answers: Integrating Error Attribution into Evaluation Framework](https://arxiv.org/abs/2507.08459)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123924842061645W7oOnNbg)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [Using Large Language Models for Legal Decision-Making in Austrian Value-Added Tax Law: An Experimental Study](https://arxiv.org/abs/2507.08468)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123924974830805g6xwVEU6)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [ILT-Iterative LoRA Training through Focus-Feedback-Fix for Multilingual Speech Recognition](https://arxiv.org/abs/2507.08477)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123925113395879XQQOeOdT)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [Enhancing Essay Cohesion Assessment: A Novel Item Response Theory Approach](https://arxiv.org/abs/2507.08487)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123925232503649akJ9QXzB)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [A Third Paradigm for LLM Evaluation: Dialogue Game-Based Evaluation using clembench](https://arxiv.org/abs/2507.08491)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123925376187096TFxfTbK2)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [LLaPa: A Vision-Language Model Framework for Counterfactual-Aware Procedural Planning](https://arxiv.org/abs/2507.08496)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123925572737929cFGN49Pn)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [Semantic-Augmented Latent Topic Modeling with LLM-in-the-Loop](https://arxiv.org/abs/2507.08498)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123925771931263WMTxDY9f)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [PromotionGo at SemEval-2025 Task 11: A Feature-Centric Framework for Cross-Lingual Multi-Emotion Detection in Short Texts](https://arxiv.org/abs/2507.08499)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123925930939121vvqzjvQ1)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [The AI Language Proficiency Monitor -- Tracking the Progress of LLMs on Multilingual Benchmarks](https://arxiv.org/abs/2507.08538)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 2025071412392673427785bvrh4ogS)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [DocPolarBERT: A Pre-trained Model for Document Understanding with Relative Polar Coordinate Encoding of Layout Structures](https://arxiv.org/abs/2507.08606)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123926213493899vf3H7PfV)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [A comprehensive study of LLM-based argument classification: from LLAMA through GPT-4o to Deepseek-R1](https://arxiv.org/abs/2507.08621)
Summarization failed, append the original article
error: Error code: 403 - {'error': {'message': 'user quota is not enough (request id: 20250714123926413959173fcVlkH2u)', 'type': 'new_api_error', 'param': '', 'code': 'insufficient_user_quota'}}
Append: [The Impact of Automatic Speech Transcription on Speaker Attribution](https://arxiv.org/abs/2507.08660)
Append: [KELPS: A Framework for Verified Multi-Language Autoformalization via Semantic-Syntactic Alignment](https://arxiv.org/abs/2507.08665)
Append: [KG-Attention: Knowledge Graph-Guided Attention at Test-Time via Bidirectional Information Aggregation](https://arxiv.org/abs/2507.08704)
Append: [Multilingual Multimodal Software Developer for Code Generation](https://arxiv.org/abs/2507.08719)
Append: [KV Cache Steering for Inducing Reasoning in Small Language Models](https://arxiv.org/abs/2507.08799)
Append: [VideoConviction: A Multimodal Benchmark for Human Conviction and Stock Market Recommendations](https://arxiv.org/abs/2507.08104)
Append: [Audio Flamingo 3: Advancing Audio Intelligence with Fully Open Large Audio Language Models](https://arxiv.org/abs/2507.08128)
Append: [Overview of the TREC 2021 deep learning track](https://arxiv.org/abs/2507.08191)
Append: [Lightweight Safety Guardrails via Synthetic Data and RL-guided Adversarial Training](https://arxiv.org/abs/2507.08284)
Append: [M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning](https://arxiv.org/abs/2507.08306)
Append: [xpSHACL: Explainable SHACL Validation using Retrieval-Augmented Generation and Large Language Models](https://arxiv.org/abs/2507.08432)
Append: [A Multi-granularity Concept Sparse Activation and Hierarchical Knowledge Graph Fusion Framework for Rare Disease Diagnosis](https://arxiv.org/abs/2507.08529)
Append: [Large Multi-modal Model Cartographic Map Comprehension for Textual Locality Georeferencing](https://arxiv.org/abs/2507.08575)
Append: [Scaling Attention to Very Long Sequences in Linear Time with Wavelet-Enhanced Random Spectral Attention (WERSA)](https://arxiv.org/abs/2507.08637)
Append: [On Barriers to Archival Audio Processing](https://arxiv.org/abs/2507.08768)
Append: [BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity](https://arxiv.org/abs/2507.08771)
Append: [One Token to Fool LLM-as-a-Judge](https://arxiv.org/abs/2507.08794)
Append: [NeuralOS: Towards Simulating Operating Systems via Neural Generative Models](https://arxiv.org/abs/2507.08800)
Append: [Answer Generation for Questions With Multiple Information Sources in E-Commerce](https://arxiv.org/abs/2111.14003)
Append: [Comparing Spoken Languages using Paninian System of Sounds and Finite State Machines](https://arxiv.org/abs/2301.12463)
Append: [Riddle Generation using Learning Resources](https://arxiv.org/abs/2310.18290)
Append: [Large Language Models in Mental Health Care: a Scoping Review](https://arxiv.org/abs/2401.02984)
Append: [Weak-to-Strong Jailbreaking on Large Language Models](https://arxiv.org/abs/2401.17256)
Append: [Swap distance minimization beyond entropy minimization in word order variation](https://arxiv.org/abs/2404.14192)
Append: [SpecDec++: Boosting Speculative Decoding via Adaptive Candidate Lengths](https://arxiv.org/abs/2405.19715)
Append: [HeSum: a Novel Dataset for Abstractive Text Summarization in Hebrew](https://arxiv.org/abs/2406.03897)
Append: [Evaluating Implicit Bias in Large Language Models by Attacking From a Psychometric Perspective](https://arxiv.org/abs/2406.14023)
Append: [An Empirical Study of Validating Synthetic Data for Formula Generation](https://arxiv.org/abs/2407.10657)
Append: [Post-hoc Study of Climate Microtargeting on Social Media Ads with LLMs: Thematic Insights and Fairness Evaluation](https://arxiv.org/abs/2410.05401)
Append: [EvalTree: Profiling Language Model Weaknesses via Hierarchical Capability Trees](https://arxiv.org/abs/2503.08893)
Append: [REGEN: A Dataset and Benchmarks with Natural Language Critiques and Narratives](https://arxiv.org/abs/2503.11924)
Append: [Enabling Inclusive Systematic Reviews: Incorporating Preprint Articles with Large Language Model-Driven Evaluations](https://arxiv.org/abs/2503.13857)
Append: [Multi-Token Attention](https://arxiv.org/abs/2504.00927)
Append: [One-Pass to Reason: Token Duplication and Block-Sparse Mask for Efficient Fine-Tuning on Multi-Turn Reasoning](https://arxiv.org/abs/2504.18246)
Append: [Red Teaming Large Language Models for Healthcare](https://arxiv.org/abs/2505.00467)
Append: [Extracting memorized pieces of (copyrighted) books from open-weight language models](https://arxiv.org/abs/2505.12546)
Append: [GeistBERT: Breathing Life into German NLP](https://arxiv.org/abs/2506.11903)
Append: [Sampling from Your Language Model One Byte at a Time](https://arxiv.org/abs/2506.14123)
Append: [Sequence graphs realizations and ambiguity in language models](https://arxiv.org/abs/2402.08830)
Append: [Text2BIM: Generating Building Models Using a Large Language Model-based Multi-Agent Framework](https://arxiv.org/abs/2408.08054)
Append: [Drowning in Documents: Consequences of Scaling Reranker Inference](https://arxiv.org/abs/2411.11767)
Append: [Bandit-Based Prompt Design Strategy Selection Improves Prompt Optimizers](https://arxiv.org/abs/2503.01163)
Append: [Generative Retrieval and Alignment Model: A New Paradigm for E-commerce Retrieval](https://arxiv.org/abs/2504.01403)
Append: [AI Safety Should Prioritize the Future of Work](https://arxiv.org/abs/2504.13959)
Append: [Addressing Pitfalls in Auditing Practices of Automatic Speech Recognition Technologies: A Case Study of People with Aphasia](https://arxiv.org/abs/2506.08846)
Append: [Probing Experts' Perspectives on AI-Assisted Public Speaking Training](https://arxiv.org/abs/2507.07930)
append_entries: 95
Finish: 2025-07-14 04:39:26.535208
------------------------------------------------------
Started: 2025-07-14 06:28:07.762499
Existing_entries: 1095
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-14 06:28:08.009236
------------------------------------------------------
Started: 2025-07-14 08:26:30.505164
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-14 08:26:30.831780
------------------------------------------------------
Started: 2025-07-14 10:19:49.188837
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-14 10:19:49.442543
------------------------------------------------------
Started: 2025-07-14 12:37:08.568517
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-14 12:37:08.841254
------------------------------------------------------
Started: 2025-07-14 14:18:40.098183
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-14 14:18:40.369764
------------------------------------------------------
Started: 2025-07-14 16:22:57.984879
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-14 16:22:58.226000
------------------------------------------------------
Started: 2025-07-14 18:26:44.094956
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-14 18:26:44.348404
------------------------------------------------------
Started: 2025-07-14 20:19:47.653920
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-14 20:19:47.902857
------------------------------------------------------
Started: 2025-07-14 22:17:12.613417
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-14 22:17:12.863538
------------------------------------------------------
Started: 2025-07-15 01:28:12.215960
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-15 01:28:12.508784
------------------------------------------------------
Started: 2025-07-15 03:27:28.516899
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-15 03:27:28.762781
------------------------------------------------------
Started: 2025-07-15 04:40:38.815692
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1256
Summarized using GPT-3.5-turbo
Append: [Spatial ModernBERT: Spatial-Aware Transformer for Table and Key-Value Extraction in Financial Documents at Scale](https://arxiv.org/abs/2507.08865)
Token length: 1859
Summarized using GPT-3.5-turbo
Append: [SEALGuard: Safeguarding the Multilingual Conversations in Southeast Asian Languages for LLM Software Systems](https://arxiv.org/abs/2507.08898)
Token length: 1126
Summarized using GPT-3.5-turbo
Append: [Evaluating LLMs in Medicine: A Call for Rigor, Transparency](https://arxiv.org/abs/2507.08916)
Token length: 768
Summarized using GPT-3.5-turbo
Append: [From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for LLM Evaluation](https://arxiv.org/abs/2507.08924)
Token length: 1113
Summarized using GPT-3.5-turbo
Append: [Self-Improving Model Steering](https://arxiv.org/abs/2507.08967)
Token length: 1138
Summarized using GPT-3.5-turbo
Append: [Application of CARE-SD text classifier tools to assess distribution of stigmatizing and doubt-marking language features in EHR](https://arxiv.org/abs/2507.08969)
Token length: 1204
Summarized using GPT-3.5-turbo
Append: [Beyond vividness: Content analysis of induced hallucinations reveals the hidden structure of individual differences in visual imagery](https://arxiv.org/abs/2507.09011)
Token length: 1659
Summarized using GPT-3.5-turbo
Append: [Lizard: An Efficient Linearization Framework for Large Language Models](https://arxiv.org/abs/2507.09025)
Token length: 1306
Summarized using GPT-3.5-turbo
Append: [ALIGN: Prompt-based Attribute Alignment for Reliable, Responsible, and Personalized LLM-based Decision-Making](https://arxiv.org/abs/2507.09037)
Token length: 1336
Summarized using GPT-3.5-turbo
Append: [OpenCodeReasoning-II: A Simple Test Time Scaling Approach via Self-Critique](https://arxiv.org/abs/2507.09075)
Token length: 1304
Summarized using GPT-3.5-turbo
Append: [Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence Emotion Recognition in Conversation](https://arxiv.org/abs/2507.09076)
Token length: 1260
Summarized using GPT-3.5-turbo
Append: [CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards](https://arxiv.org/abs/2507.09104)
Token length: 1546
Summarized using GPT-3.5-turbo
Append: [OPENXRD: A Comprehensive Benchmark and Enhancement Framework for LLM/MLLM XRD Question Answering](https://arxiv.org/abs/2507.09155)
Token length: 1230
Summarized using GPT-3.5-turbo
Append: [PU-Lie: Lightweight Deception Detection in Imbalanced Diplomatic Dialogues via Positive-Unlabeled Learning](https://arxiv.org/abs/2507.09157)
Token length: 1236
Summarized using GPT-3.5-turbo
Append: [RAMA: Retrieval-Augmented Multi-Agent Framework for Misinformation Detection in Multimodal Fact-Checking](https://arxiv.org/abs/2507.09174)
Token length: 1109
Summarized using GPT-3.5-turbo
Append: [Detecting and Pruning Prominent but Detrimental Neurons in Large Language Models](https://arxiv.org/abs/2507.09185)
Token length: 1008
Summarized using GPT-3.5-turbo
Append: [Banzhida: Advancing Large Language Models for Tibetan with Curated Data and Continual Pre-Training](https://arxiv.org/abs/2507.09205)
Token length: 1921
Summarized using GPT-3.5-turbo
Append: [MetaClimage: A novel database of visual metaphors related to Climate Change, with costs and benefits analysis](https://arxiv.org/abs/2507.09225)
Token length: 733
Summarized using GPT-3.5-turbo
Append: [Swa-bhasha Resource Hub: Romanized Sinhala to Sinhala Transliteration Systems and Data Resources](https://arxiv.org/abs/2507.09245)
Token length: 1040
Summarized using GPT-3.5-turbo
Append: [Psychology-Driven Enhancement of Humour Translation](https://arxiv.org/abs/2507.09259)
Token length: 964
Summarized using GPT-3.5-turbo
Append: [ClaritySpeech: Dementia Obfuscation in Speech](https://arxiv.org/abs/2507.09282)
Token length: 1368
Summarized using GPT-3.5-turbo
Append: [DATE-LM: Benchmarking Data Attribution Evaluation for Large Language Models](https://arxiv.org/abs/2507.09424)
Token length: 1464
Summarized using GPT-3.5-turbo
Append: [Enhancing Clinical Text Classification via Fine-Tuned DRAGON Longformer Models](https://arxiv.org/abs/2507.09470)
Token length: 381
Summarized using GPT-3.5-turbo
Append: [The CoNLL-2013 Shared Task on Grammatical Error Correction](https://arxiv.org/abs/2507.09474)
Token length: 1140
Summarized using GPT-3.5-turbo
Append: [Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs](https://arxiv.org/abs/2507.09477)
Token length: 1603
Summarized using GPT-3.5-turbo
Append: [ViSP: A PPO-Driven Framework for Sarcasm Generation with Contrastive Learning](https://arxiv.org/abs/2507.09482)
Token length: 1477
Summarized using GPT-3.5-turbo
Append: [Balanced Training Data Augmentation for Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2507.09485)
Token length: 1468
Summarized using GPT-3.5-turbo
Append: [GoalfyMax: A Protocol-Driven Multi-Agent System for Intelligent Experience Entities](https://arxiv.org/abs/2507.09497)
Token length: 1278
Summarized using GPT-3.5-turbo
Append: [Ref-Long: Benchmarking the Long-context Referencing Capability of Long-context Language Models](https://arxiv.org/abs/2507.09506)
Token length: 1176
Summarized using GPT-3.5-turbo
Append: [How Important is `Perfect' English for Machine Translation Prompts?](https://arxiv.org/abs/2507.09509)
Token length: 684
Summarized using GPT-3.5-turbo
Append: [Adapting Definition Modeling for New Languages: A Case Study on Belarusian](https://arxiv.org/abs/2507.09536)
Token length: 1502
Summarized using GPT-3.5-turbo
Append: [NMIXX: Domain-Adapted Neural Embeddings for Cross-Lingual eXploration of Finance](https://arxiv.org/abs/2507.09601)
Token length: 1470
Summarized using GPT-3.5-turbo
Append: [SpreadPy: A Python tool for modelling spreading activation and superdiffusion in cognitive multiplex networks](https://arxiv.org/abs/2507.09628)
Token length: 838
Summarized using GPT-3.5-turbo
Append: [An Exploration of Knowledge Editing for Arabic](https://arxiv.org/abs/2507.09629)
Token length: 1010
Summarized using GPT-3.5-turbo
Append: [Can Group Relative Policy Optimization Improve Thai Legal Reasoning and Question Answering?](https://arxiv.org/abs/2507.09638)
Token length: 1217
Summarized using GPT-3.5-turbo
Append: [MCEval: A Dynamic Framework for Fair Multilingual Cultural Evaluation of LLMs](https://arxiv.org/abs/2507.09701)
Token length: 1485
Summarized using GPT-3.5-turbo
Append: [Large Language Models Encode Semantics in Low-Dimensional Linear Subspaces](https://arxiv.org/abs/2507.09709)
Token length: 1089
Summarized using GPT-3.5-turbo
Append: [Your Pretrained Model Tells the Difficulty Itself: A Self-Adaptive Curriculum Learning Paradigm for Natural Language Understanding](https://arxiv.org/abs/2507.09758)
Token length: 1164
Summarized using GPT-3.5-turbo
Append: [Te Ahorr\'e Un Click: A Revised Definition of Clickbait and Detection in Spanish News](https://arxiv.org/abs/2507.09777)
Token length: 1419
Summarized using GPT-3.5-turbo
Append: [Function Induction and Task Generalization: An Interpretability Study with Off-by-One Addition](https://arxiv.org/abs/2507.09875)
Token length: 1040
Summarized using GPT-3.5-turbo
Append: [Enhancing Retrieval Augmented Generation with Hierarchical Text Segmentation Chunking](https://arxiv.org/abs/2507.09935)
Token length: 1165
Summarized using GPT-3.5-turbo
Append: [Tiny Reward Models](https://arxiv.org/abs/2507.09973)
Token length: 1243
Summarized using GPT-3.5-turbo
Append: [TextOmics-Guided Diffusion for Hit-like Molecular Generation](https://arxiv.org/abs/2507.09982)
Token length: 1728
Summarized using GPT-3.5-turbo
Append: [Protective Factor-Aware Dynamic Influence Learning for Suicide Risk Prediction on Social Media](https://arxiv.org/abs/2507.10008)
Token length: 1218
Summarized using GPT-3.5-turbo
Append: [GeLaCo: An Evolutionary Approach to Layer Compression](https://arxiv.org/abs/2507.10059)
Token length: 1160
Summarized using GPT-3.5-turbo
Append: [Cultural Bias in Large Language Models: Evaluating AI Agents through Moral Questionnaires](https://arxiv.org/abs/2507.10073)
Token length: 1821
Summarized using GPT-3.5-turbo
Append: [Enhancing Chain-of-Thought Reasoning with Critical Representation Fine-tuning](https://arxiv.org/abs/2507.10085)
Token length: 1482
Summarized using GPT-3.5-turbo
Append: [Fusing Large Language Models with Temporal Transformers for Time Series Forecasting](https://arxiv.org/abs/2507.10098)
Token length: 1471
Summarized using GPT-3.5-turbo
Append: [Task-Based Flexible Feature Distillation for LLMs](https://arxiv.org/abs/2507.10155)
Token length: 1091
Summarized using GPT-3.5-turbo
Append: [Abusive text transformation using LLMs](https://arxiv.org/abs/2507.10177)
Append: [Absher: A Benchmark for Evaluating Large Language Models Understanding of Saudi Dialects](https://arxiv.org/abs/2507.10216)
Append: [Grammar-Guided Evolutionary Search for Discrete Prompt Optimisation](https://arxiv.org/abs/2507.10326)
Append: [Bridging Robustness and Generalization Against Word Substitution Attacks in NLP via the Growth Bound Matrix Approach](https://arxiv.org/abs/2507.10330)
Append: [Using AI to replicate human experimental results: a motion study](https://arxiv.org/abs/2507.10342)
Append: [Meanings are like Onions: a Layered Approach to Metaphor Processing](https://arxiv.org/abs/2507.10354)
Append: [From Sequence to Structure: Uncovering Substructure Reasoning in Transformers](https://arxiv.org/abs/2507.10435)
Append: [Referential ambiguity and clarification requests: comparing human and LLM behaviour](https://arxiv.org/abs/2507.10445)
Append: [From BERT to Qwen: Hate Detection across architectures](https://arxiv.org/abs/2507.10468)
Append: [MLAR: Multi-layer Large Language Model-based Robotic Process Automation Applicant Tracking](https://arxiv.org/abs/2507.10472)
Append: [Can You Detect the Difference?](https://arxiv.org/abs/2507.10475)
Append: [Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation](https://arxiv.org/abs/2507.10524)
Append: [CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks](https://arxiv.org/abs/2507.10535)
Append: [REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once](https://arxiv.org/abs/2507.10541)
Append: [Principled Foundations for Preference Optimization](https://arxiv.org/abs/2507.07855)
Append: [Think Clearly: Improving Reasoning via Redundant Token Pruning](https://arxiv.org/abs/2507.08806)
Append: [LoRA Is Slower Than You Think](https://arxiv.org/abs/2507.08833)
Append: [RAG Safety: Exploring Knowledge Poisoning Attacks to Retrieval-Augmented Generation](https://arxiv.org/abs/2507.08862)
Append: [Less Stress, More Privacy: Stress Detection on Anonymized Speech of Air Traffic Controllers](https://arxiv.org/abs/2507.08882)
Append: [Overview of the TREC 2023 deep learning track](https://arxiv.org/abs/2507.08890)
Append: [Semantic Source Code Segmentation using Small and Large Language Models](https://arxiv.org/abs/2507.08992)
Append: [DS@GT at Touch\'e: Large Language Models for Retrieval-Augmented Debate](https://arxiv.org/abs/2507.09090)
Append: [AInsight: Augmenting Expert Decision-Making with On-the-Fly Insights Grounded in Historical Data](https://arxiv.org/abs/2507.09100)
Append: [DLBAcalib: Robust Extrinsic Calibration for Non-Overlapping LiDARs Based on Dual LBA](https://arxiv.org/abs/2507.09176)
Append: [Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically-Aligned Confidence Calibration in Multimodal Large Language Models](https://arxiv.org/abs/2507.09279)
Append: [Voice Conversion for Lombard Speaking Style with Implicit and Explicit Acoustic Feature Conditioning](https://arxiv.org/abs/2507.09310)
Append: [ZipVoice-Dialog: Non-Autoregressive Spoken Dialogue Generation with Flow Matching](https://arxiv.org/abs/2507.09318)
Append: [Evaluating LLMs on Sequential API Call Through Automated Test Generation](https://arxiv.org/abs/2507.09481)
Append: [MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive Vision Generation Models](https://arxiv.org/abs/2507.09574)
Append: [Towards Concise and Adaptive Thinking in Large Reasoning Models: A Survey](https://arxiv.org/abs/2507.09662)
Append: [Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations](https://arxiv.org/abs/2507.09751)
Append: [EventHunter: Dynamic Clustering and Ranking of Security Events from Hacker Forum Discussions](https://arxiv.org/abs/2507.09762)
Append: [TinyTroupe: An LLM-powered Multiagent Persona Simulation Toolkit](https://arxiv.org/abs/2507.09788)
Append: [ViTCoT: Video-Text Interleaved Chain-of-Thought for Boosting Video Understanding in Large Language Models](https://arxiv.org/abs/2507.09876)
Append: [MixLoRA-DSI: Dynamically Expandable Mixture-of-LoRA Experts for Rehearsal-Free Generative Retrieval over Dynamic Corpora](https://arxiv.org/abs/2507.09924)
Append: [On The Role of Intentionality in Knowledge Representation: Analyzing Scene Context for Cognitive Agents with a Tiny Language Model](https://arxiv.org/abs/2507.10000)
Append: [Cross-modal Associations in Vision and Language Models: Revisiting the bouba-kiki effect](https://arxiv.org/abs/2507.10013)
Append: [Automating SPARQL Query Translations between DBpedia and Wikidata](https://arxiv.org/abs/2507.10045)
Append: [PRISM: Fine-Grained Paper-to-Paper Retrieval with Multi-Aspect-Aware Query Optimization](https://arxiv.org/abs/2507.10057)
Append: [Natural Language-based Assessment of L2 Oral Proficiency using LLMs](https://arxiv.org/abs/2507.10200)
Append: [FaceLLM: A Multimodal Large Language Model for Face Understanding](https://arxiv.org/abs/2507.10300)
Append: [Devanagari Handwritten Character Recognition using Convolutional Neural Network](https://arxiv.org/abs/2507.10398)
Append: [Text-to-Remote-Sensing-Image Retrieval beyond RGB Sources](https://arxiv.org/abs/2507.10403)
Append: [Multiple Choice Learning of Low Rank Adapters for Language Modeling](https://arxiv.org/abs/2507.10419)
Append: [DeepResearch$^{\text{Eco}}$: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology](https://arxiv.org/abs/2507.10522)
Append: [Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination](https://arxiv.org/abs/2507.10532)
Append: [EmbRACE-3K: Embodied Reasoning and Action in Complex Environments](https://arxiv.org/abs/2507.10548)
Append: [IDEAL: Influence-Driven Selective Annotations Empower In-Context Learners in Large Language Models](https://arxiv.org/abs/2310.10873)
Append: [Topic Modeling as Multi-Objective Contrastive Optimization](https://arxiv.org/abs/2402.07577)
Append: [SEE: Strategic Exploration and Exploitation for Cohesive In-Context Prompt Optimization](https://arxiv.org/abs/2402.11347)
Append: [An In-depth Evaluation of Large Language Models in Sentence Simplification with Error-based Human Assessment](https://arxiv.org/abs/2403.04963)
Append: [Towards Pareto Optimal Throughput in Small Language Model Serving](https://arxiv.org/abs/2404.03353)
Append: [Intuitive Fine-Tuning: Towards Simplifying Alignment into a Single Process](https://arxiv.org/abs/2405.11870)
Append: [Political Bias in LLMs: Unaligned Moral Values in Agent-centric Simulations](https://arxiv.org/abs/2408.11415)
Append: [CV-Probes: Studying the interplay of lexical and world knowledge in visually grounded verb understanding](https://arxiv.org/abs/2409.01389)
Append: [READoc: A Unified Benchmark for Realistic Document Structured Extraction](https://arxiv.org/abs/2409.05137)
Append: [TheraGen: Therapy for Every Generation](https://arxiv.org/abs/2409.13748)
Append: [Evaluation of Attribution Bias in Generator-Aware Retrieval-Augmented Large Language Models](https://arxiv.org/abs/2410.12380)
Append: [Personalization of Large Language Models: A Survey](https://arxiv.org/abs/2411.00027)
Append: [Large Language Models as Neurolinguistic Subjects: Discrepancy between Performance and Competence](https://arxiv.org/abs/2411.07533)
Append: [Knowledge-Augmented Multimodal Clinical Rationale Generation for Disease Diagnosis with Small Language Models](https://arxiv.org/abs/2411.07611)
Append: [InstCache: A Predictive Cache for LLM Serving](https://arxiv.org/abs/2411.13820)
Append: [VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information](https://arxiv.org/abs/2412.00947)
Append: [BEExformer: A Fast Inferencing Binarized Transformer with Early Exits](https://arxiv.org/abs/2412.05225)
Append: [KnowShiftQA: How Robust are RAG Systems when Textbook Knowledge Shifts in K-12 Education?](https://arxiv.org/abs/2412.08985)
Append: [Not all tokens are created equal: Perplexity Attention Weighted Networks for AI generated text detection](https://arxiv.org/abs/2501.03940)
Append: [ACEBench: Who Wins the Match Point in Tool Usage?](https://arxiv.org/abs/2501.12851)
Append: [Auditing Prompt Caching in Language Model APIs](https://arxiv.org/abs/2502.07776)
Append: [B-cos LM: Efficiently Transforming Pre-trained Language Models for Improved Explainability](https://arxiv.org/abs/2502.12992)
Append: [Qorgau: Evaluating LLM Safety in Kazakh-Russian Bilingual Contexts](https://arxiv.org/abs/2502.13640)
Append: [Disambiguate First, Parse Later: Generating Interpretations for Ambiguity Resolution in Semantic Parsing](https://arxiv.org/abs/2502.18448)
Append: [A Survey of Automatic Prompt Optimization with Instruction-focused Heuristic-based Search Algorithm](https://arxiv.org/abs/2502.18746)
Append: [OmniSQL: Synthesizing High-quality Text-to-SQL Data at Scale](https://arxiv.org/abs/2503.02240)
Append: [Supposedly Equivalent Facts That Aren't? Entity Frequency in Pre-training Induces Asymmetry in LLMs](https://arxiv.org/abs/2503.22362)
Append: [DiaTool-DPO: Multi-Turn Direct Preference Optimization for Tool-Augmented Large Language Models](https://arxiv.org/abs/2504.02882)
Append: [Bias Beyond English: Evaluating Social Bias and Debiasing Methods in a Low-Resource Setting](https://arxiv.org/abs/2504.11183)
Append: [Leveraging Large Language Models for Multi-Class and Multi-Label Detection of Drug Use and Overdose Symptoms on Social Media](https://arxiv.org/abs/2504.12355)
Append: [HYPEROFA: Expanding LLM Vocabulary to New Languages via Hypernetwork-Based Embedding Initialization](https://arxiv.org/abs/2504.21018)
Append: [Consistency in Language Models: Current Landscape, Challenges, and Future Directions](https://arxiv.org/abs/2505.00268)
Append: [Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement](https://arxiv.org/abs/2505.08245)
Append: [LEXam: Benchmarking Legal Reasoning on 340 Law Exams](https://arxiv.org/abs/2505.12864)
Append: [Reinforcing Question Answering Agents with Minimalist Policy Gradient Optimization](https://arxiv.org/abs/2505.17086)
Append: [The NaijaVoices Dataset: Cultivating Large-Scale, High-Quality, Culturally-Rich Speech Data for African Languages](https://arxiv.org/abs/2505.20564)
Append: [Structuring Radiology Reports: Challenging LLMs with Lightweight Models](https://arxiv.org/abs/2506.00200)
Append: [TReB: A Comprehensive Benchmark for Evaluating Table Reasoning Capabilities of Large Language Models](https://arxiv.org/abs/2506.18421)
Append: [Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning](https://arxiv.org/abs/2506.22777)
Append: [ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in Large Language Models](https://arxiv.org/abs/2506.22791)
Append: [Learning-to-Context Slope: Evaluating In-Context Learning Effectiveness Beyond Performance Illusions](https://arxiv.org/abs/2506.23146)
Append: [Perspective Dial: Measuring Perspective of Text and Guiding LLM Outputs](https://arxiv.org/abs/2506.23377)
Append: [MSVD-Indonesian: A Benchmark for Multimodal Video-Text Tasks in Indonesian](https://arxiv.org/abs/2306.11341)
Append: [Cascade Speculative Drafting for Even Faster LLM Inference](https://arxiv.org/abs/2312.11462)
Append: [MoRE: A Mixture of Reflectors Framework for Large Language Model-Based Sequential Recommendation](https://arxiv.org/abs/2409.06377)
Append: [EVOLvE: Evaluating and Optimizing LLMs For In-Context Exploration](https://arxiv.org/abs/2410.06238)
Append: [A Comprehensive Survey of Direct Preference Optimization: Datasets, Theories, Variants, and Applications](https://arxiv.org/abs/2410.15595)
Append: [LASER: Attention with Exponential Transformation](https://arxiv.org/abs/2411.03493)
Append: [Fourier Position Embedding: Enhancing Attention's Periodic Extension for Length Generalization](https://arxiv.org/abs/2412.17739)
Append: [A General Framework for Inference-time Scaling and Steering of Diffusion Models](https://arxiv.org/abs/2501.06848)
Append: [Logits are All We Need to Adapt Closed Models](https://arxiv.org/abs/2502.06806)
Append: [IPAD: Inverse Prompt for AI Detection -- A Robust and Explainable LLM-Generated Text Detector](https://arxiv.org/abs/2502.15902)
Append: [KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for Coding](https://arxiv.org/abs/2503.02951)
Append: [A Noise-Robust Turn-Taking System for Real-World Dialogue Robots: A Field Experiment](https://arxiv.org/abs/2503.06241)
Append: [Can A Society of Generative Agents Simulate Human Behavior and Inform Public Health Policy? A Case Study on Vaccine Hesitancy](https://arxiv.org/abs/2503.09639)
Append: [CASCADE Your Datasets for Cross-Mode Knowledge Retrieval of Language Models](https://arxiv.org/abs/2504.01450)
Append: [DataDecide: How to Predict Best Pretraining Data with Small Experiments](https://arxiv.org/abs/2504.11393)
Append: [Roll the dice & look before you leap: Going beyond the creative limits of next-token prediction](https://arxiv.org/abs/2504.15266)
Append: [EVALOOP: Assessing LLM Robustness in Programming from a Self-consistency Perspective](https://arxiv.org/abs/2505.12185)
Append: [Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement Fine-Tuning of Large Language Models](https://arxiv.org/abs/2505.17826)
Append: [Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization](https://arxiv.org/abs/2505.24859)
Append: [Scientists' First Exam: Probing Cognitive Abilities of MLLM via Perception, Understanding, and Reasoning](https://arxiv.org/abs/2506.10521)
Append: [SymRAG: Efficient Neuro-Symbolic Retrieval Through Adaptive Query Routing](https://arxiv.org/abs/2506.12981)
Append: [LLM Agents Are the Antidote to Walled Gardens](https://arxiv.org/abs/2506.23978)
append_entries: 160
Finish: 2025-07-15 04:42:28.406868
------------------------------------------------------
Started: 2025-07-15 06:28:25.276607
Existing_entries: 1160
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1025
Summarized using GPT-3.5-turbo
Append: [BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning](https://arxiv.org/abs/2506.06955)
Token length: 1132
Summarized using GPT-3.5-turbo
Append: [Following the Clues: Experiments on Person Re-ID using Cross-Modal Intelligence](https://arxiv.org/abs/2507.01504)
append_entries: 2
Finish: 2025-07-15 06:28:30.839985
------------------------------------------------------
Started: 2025-07-15 08:24:19.187328
Existing_entries: 1002
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-15 08:24:19.591607
------------------------------------------------------
Started: 2025-07-15 10:18:41.049244
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-15 10:18:41.445063
------------------------------------------------------
Started: 2025-07-15 12:37:16.617768
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-15 12:37:17.017086
------------------------------------------------------
Started: 2025-07-15 14:17:02.819071
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-15 14:17:03.233764
------------------------------------------------------
Started: 2025-07-15 16:22:52.417629
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-15 16:22:52.871580
------------------------------------------------------
Started: 2025-07-15 18:26:31.449729
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-15 18:26:32.050149
------------------------------------------------------
Started: 2025-07-15 20:19:51.728707
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-15 20:19:52.200662
------------------------------------------------------
Started: 2025-07-15 22:17:45.837332
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-15 22:17:46.316694
------------------------------------------------------
Started: 2025-07-16 01:25:18.272196
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-16 01:25:18.731818
------------------------------------------------------
Started: 2025-07-16 03:25:41.670680
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-16 03:25:42.261548
------------------------------------------------------
Started: 2025-07-16 04:40:30.533893
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1498
Summarized using GPT-3.5-turbo
Append: [Truth Sleuth and Trend Bender: AI Agents to fact-check YouTube videos and influence opinions](https://arxiv.org/abs/2507.10577)
Token length: 1669
Summarized using GPT-3.5-turbo
Append: [An Offline Mobile Conversational Agent for Mental Health Support: Learning from Emotional Dialogues and Psychological Texts with Student-Centered Evaluation](https://arxiv.org/abs/2507.10580)
Token length: 1857
Summarized using GPT-3.5-turbo
Append: [Transforming Sensitive Documents into Quantitative Data: An AI-Based Preprocessing Toolchain for Structured and Privacy-Conscious Analysis](https://arxiv.org/abs/2507.10582)
Token length: 947
Summarized using GPT-3.5-turbo
Append: [A Taxonomy for Design and Evaluation of Prompt-Based Natural Language Explanations](https://arxiv.org/abs/2507.10585)
Token length: 1025
Summarized using GPT-3.5-turbo
Append: [AutoRAG-LoRA: Hallucination-Triggered Knowledge Retuning via Lightweight Adapters](https://arxiv.org/abs/2507.10586)
Token length: 1561
Summarized using GPT-3.5-turbo
Append: [Anthropomimetic Uncertainty: What Verbalized Uncertainty in Language Models is Missing](https://arxiv.org/abs/2507.10587)
Token length: 1692
Summarized using GPT-3.5-turbo
Append: [PLEX: Perturbation-free Local Explanations for LLM-Based Text Classification](https://arxiv.org/abs/2507.10596)
Token length: 997
Summarized using GPT-3.5-turbo
Append: [Emergence of Hierarchical Emotion Organization in Large Language Models](https://arxiv.org/abs/2507.10599)
Token length: 1887
Summarized using GPT-3.5-turbo
Append: [Language Models for Adult Service Website Text Analysis](https://arxiv.org/abs/2507.10743)
Token length: 686
Summarized using GPT-3.5-turbo
Append: [Applying Text Embedding Models for Efficient Analysis in Labeled Property Graphs](https://arxiv.org/abs/2507.10772)
Token length: 970
Summarized using GPT-3.5-turbo
Append: [Can Multimodal Foundation Models Understand Schematic Diagrams? An Empirical Study on Information-Seeking QA over Scientific Papers](https://arxiv.org/abs/2507.10787)
Token length: 1000
Summarized using GPT-3.5-turbo
Append: [Testing Hypotheses from the Social Approval Theory of Online Hate: An Analysis of 110 Million Posts from Parler](https://arxiv.org/abs/2507.10810)
Token length: 1691
Summarized using GPT-3.5-turbo
Append: [LLMs on Trial: Evaluating Judicial Fairness for Large Language Models](https://arxiv.org/abs/2507.10852)
Token length: 1272
Summarized using GPT-3.5-turbo
Append: [How Stylistic Similarity Shapes Preferences in Dialogue Dataset with User and Third Party Evaluations](https://arxiv.org/abs/2507.10918)
Token length: 1214
Summarized using GPT-3.5-turbo
Append: [HanjaBridge: Resolving Semantic Ambiguity in Korean LLMs via Hanja-Augmented Pre-Training](https://arxiv.org/abs/2507.10920)
Token length: 1503
Summarized using GPT-3.5-turbo
Append: [Modeling Understanding of Story-Based Analogies Using Large Language Models](https://arxiv.org/abs/2507.10957)
Token length: 754
Summarized using GPT-3.5-turbo
Append: [DS@GT at eRisk 2025: From prompts to predictions, benchmarking early depression detection with conversational agent based assessments and temporal attention models](https://arxiv.org/abs/2507.10958)
Token length: 991
Summarized using GPT-3.5-turbo
Append: [Teach Me Sign: Stepwise Prompting LLM for Sign Language Production](https://arxiv.org/abs/2507.10972)
Token length: 1541
Summarized using GPT-3.5-turbo
Append: [Mario at EXIST 2025: A Simple Gateway to Effective Multilingual Sexism Detection](https://arxiv.org/abs/2507.10996)
Token length: 797
Summarized using GPT-3.5-turbo
Append: [Team HUMANE at AVeriTeC 2025: HerO 2 for Efficient Fact Verification](https://arxiv.org/abs/2507.11004)
Token length: 1474
Summarized using GPT-3.5-turbo
Append: [Journalism-Guided Agentic In-Context Learning for News Stance Detection](https://arxiv.org/abs/2507.11049)
Token length: 1327
Summarized using GPT-3.5-turbo
Append: [LLM-Augmented Symptom Analysis for Cardiovascular Disease Risk Prediction: A Clinical NLP](https://arxiv.org/abs/2507.11052)
Token length: 1312
Summarized using GPT-3.5-turbo
Append: [Social Media Sentiments Analysis on the July Revolution in Bangladesh: A Hybrid Transformer Based Machine Learning Approach](https://arxiv.org/abs/2507.11084)
Token length: 1435
Summarized using GPT-3.5-turbo
Append: [Beyond Traditional Algorithms: Leveraging LLMs for Accurate Cross-Border Entity Identification](https://arxiv.org/abs/2507.11086)
Token length: 1931
Summarized using GPT-3.5-turbo
Append: [The Devil behind the mask: An emergent safety vulnerability of Diffusion LLMs](https://arxiv.org/abs/2507.11097)
Token length: 1289
Summarized using GPT-3.5-turbo
Append: [Multi-Trigger Poisoning Amplifies Backdoor Vulnerabilities in LLMs](https://arxiv.org/abs/2507.11112)
Token length: 1448
Summarized using GPT-3.5-turbo
Append: [MSA at ImageCLEF 2025 Multimodal Reasoning: Multilingual Multimodal Reasoning With Ensemble Vision Language Models](https://arxiv.org/abs/2507.11114)
Token length: 1233
Summarized using GPT-3.5-turbo
Append: [What Should LLMs Forget? Quantifying Personal Data in LLMs for Right-to-Be-Forgotten Requests](https://arxiv.org/abs/2507.11128)
Token length: 1969
Summarized using GPT-3.5-turbo
Append: [Temperature and Persona Shape LLM Agent Consensus With Minimal Accuracy Gains in Qualitative Coding](https://arxiv.org/abs/2507.11198)
Token length: 943
Summarized using GPT-3.5-turbo
Append: [EsBBQ and CaBBQ: The Spanish and Catalan Bias Benchmarks for Question Answering](https://arxiv.org/abs/2507.11216)
Token length: 1092
Summarized using GPT-3.5-turbo
Append: [An Agentic Flow for Finite State Machine Extraction using Prompt Chaining](https://arxiv.org/abs/2507.11222)
Token length: 1249
Summarized using GPT-3.5-turbo
Append: [Sparse Autoencoders Can Capture Language-Specific Concepts Across Diverse Languages](https://arxiv.org/abs/2507.11230)
Token length: 1437
Summarized using GPT-3.5-turbo
Append: [KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware Rotary Positional Embedding](https://arxiv.org/abs/2507.11273)
Token length: 1195
Summarized using GPT-3.5-turbo
Append: [FMC: Formalization of Natural Language Mathematical Competition Problems](https://arxiv.org/abs/2507.11275)
Token length: 1376
Summarized using GPT-3.5-turbo
Append: [Fine-Grained Chinese Hate Speech Understanding: Span-Level Resources, Coded Term Lexicon, and Enhanced Detection Frameworks](https://arxiv.org/abs/2507.11292)
Token length: 1030
Summarized using GPT-3.5-turbo
Append: [Dr.Copilot: A Multi-Agent Prompt Optimized Assistant for Improving Patient-Doctor Communication in Romanian](https://arxiv.org/abs/2507.11299)
Token length: 1092
Summarized using GPT-3.5-turbo
Append: [Internal Value Alignment in Large Language Models through Controlled Value Vector Activation](https://arxiv.org/abs/2507.11316)
Token length: 1433
Summarized using GPT-3.5-turbo
Append: [Automated Novelty Evaluation of Academic Paper: A Collaborative Approach Integrating Human and Large Language Model Knowledge](https://arxiv.org/abs/2507.11330)
Token length: 1077
Summarized using GPT-3.5-turbo
Append: [What is the Best Process Model Representation? A Comparative Analysis for Process Modeling with Large Language Models](https://arxiv.org/abs/2507.11356)
Token length: 883
Summarized using GPT-3.5-turbo
Append: [Addressing Data Imbalance in Transformer-Based Multi-Label Emotion Detection with Weighted Loss](https://arxiv.org/abs/2507.11384)
Token length: 1242
Summarized using GPT-3.5-turbo
Append: [DCR: Quantifying Data Contamination in LLMs Evaluation](https://arxiv.org/abs/2507.11405)
Token length: 916
Summarized using GPT-3.5-turbo
Append: [EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and Reasoning Modes](https://arxiv.org/abs/2507.11407)
Token length: 1104
Summarized using GPT-3.5-turbo
Append: [KisMATH: Do LLMs Have Knowledge of Implicit Structures in Mathematical Reasoning?](https://arxiv.org/abs/2507.11408)
Token length: 1486
Summarized using GPT-3.5-turbo
Append: [Seq vs Seq: An Open Suite of Paired Encoders and Decoders](https://arxiv.org/abs/2507.11412)
Token length: 715
Summarized using GPT-3.5-turbo
Append: [Reasoning Strategies in Large Language Models: Can They Follow, Prefer, and Optimize?](https://arxiv.org/abs/2507.11423)
Token length: 1726
Summarized using GPT-3.5-turbo
Append: [HKGAI-V1: Towards Regional Sovereign Large Language Model for Hong Kong](https://arxiv.org/abs/2507.11502)
Token length: 956
Summarized using GPT-3.5-turbo
Append: [Real-World Summarization: When Evaluation Reaches Its Limits](https://arxiv.org/abs/2507.11508)
Token length: 1066
Summarized using GPT-3.5-turbo
Append: [NLP Meets the World: Toward Improving Conversations With the Public About Natural Language Processing Research](https://arxiv.org/abs/2507.10559)
Token length: 1762
Summarized using GPT-3.5-turbo
Append: [Orchestrator-Agent Trust: A Modular Agentic AI Visual Classification System with Trust-Aware Orchestration and RAG-Based Reasoning](https://arxiv.org/abs/2507.10571)
Token length: 1916
Summarized using GPT-3.5-turbo
Append: [Can Large Language Models Understand As Well As Apply Patent Regulations to Pass a Hands-On Patent Attorney Test?](https://arxiv.org/abs/2507.10576)
Append: [Findings of the BEA 2025 Shared Task on Pedagogical Ability Assessment of AI-powered Tutors](https://arxiv.org/abs/2507.10579)
Append: [Scalpel vs. Hammer: GRPO Amplifies Existing Capabilities, SFT Replaces Them](https://arxiv.org/abs/2507.10616)
Append: [From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web of Agents](https://arxiv.org/abs/2507.10644)
Append: [Theory of Mind and Self-Disclosure to CUIs](https://arxiv.org/abs/2507.10773)
Append: [Automated Thematic Analyses Using LLMs: Xylazine Wound Management Social Media Chatter Use Case](https://arxiv.org/abs/2507.10803)
Append: [MultiVox: Benchmarking Voice Assistants for Multimodal Interactions](https://arxiv.org/abs/2507.10859)
Append: [Overview of the TREC 2022 deep learning track](https://arxiv.org/abs/2507.10865)
Append: [Domain-Adaptive Small Language Models for Structured Tax Code Prediction](https://arxiv.org/abs/2507.10880)
Append: [NavComposer: Composing Language Instructions for Navigation Trajectories through Action-Scene-Object Modularization](https://arxiv.org/abs/2507.10894)
Append: [LiLM-RDB-SFC: Lightweight Language Model with Relational Database-Guided DRL for Optimized SFC Provisioning](https://arxiv.org/abs/2507.10903)
Append: [First-Order Error Matters: Accurate Compensation for Quantized Large Language Models](https://arxiv.org/abs/2507.11017)
Append: [SWE-MERA: A Dynamic Benchmark for Agenticly Evaluating Large Language Models on Software Engineering Tasks](https://arxiv.org/abs/2507.11059)
Append: [AirLLM: Diffusion Policy-based Adaptive LoRA for Remote Fine-Tuning of LLM over the Air](https://arxiv.org/abs/2507.11515)
Append: [Fine-grained Stateful Knowledge Exploration: Effective and Efficient Graph Retrieval with Large Language Models](https://arxiv.org/abs/2401.13444)
Append: [GenARM: Reward Guided Generation with Autoregressive Reward Model for Test-time Alignment](https://arxiv.org/abs/2410.08193)
Append: [Is Training Data Quality or Quantity More Impactful to Small Language Model Performance?](https://arxiv.org/abs/2411.15821)
Append: [AIDE: Attribute-Guided MultI-Hop Data Expansion for Data Scarcity in Task-Specific Fine-tuning](https://arxiv.org/abs/2412.06136)
Append: [Understanding the Dark Side of LLMs' Intrinsic Self-Correction](https://arxiv.org/abs/2412.14959)
Append: [Plancraft: an evaluation dataset for planning with LLM agents](https://arxiv.org/abs/2412.21033)
Append: [Comply: Learning Sentences with Complex Weights inspired by Fruit Fly Olfaction](https://arxiv.org/abs/2502.01706)
Append: [A Generative Approach to LLM Harmfulness Detection with Special Red Flag Tokens](https://arxiv.org/abs/2502.16366)
Append: [Shared Global and Local Geometry of Language Model Embeddings](https://arxiv.org/abs/2503.21073)
Append: [Style over Substance: Distilled Language Models Reason Via Stylistic Replication](https://arxiv.org/abs/2504.01738)
Append: [Truthful or Fabricated? Using Causal Attribution to Mitigate Reward Hacking in Explanations](https://arxiv.org/abs/2504.05294)
Append: [SocioVerse: A World Model for Social Simulation Powered by LLM Agents and A Pool of 10 Million Real-World Users](https://arxiv.org/abs/2504.10157)
Append: [Deep Binding of Language Model Virtual Personas: a Study on Approximating Political Partisan Misperceptions](https://arxiv.org/abs/2504.11673)
Append: [Block Circulant Adapter for Large Language Models](https://arxiv.org/abs/2505.00582)
Append: [Bring Reason to Vision: Understanding Perception and Reasoning through Model Merging](https://arxiv.org/abs/2505.05464)
Append: [Multimodal Sentiment Analysis on CMU-MOSEI Dataset using Transformer-based Models](https://arxiv.org/abs/2505.06110)
Append: [FalseReject: A Resource for Improving Contextual Safety and Mitigating Over-Refusals in LLMs via Structured Reasoning](https://arxiv.org/abs/2505.08054)
Append: [Is Compression Really Linear with Code Intelligence?](https://arxiv.org/abs/2505.11441)
Append: [Compression Hacking: A Supplementary Perspective on Informatics Properties of Language Models from Geometric Distortion](https://arxiv.org/abs/2505.17793)
Append: [Gaussian mixture models as a proxy for interacting language models](https://arxiv.org/abs/2506.00077)
Append: [Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback](https://arxiv.org/abs/2506.03106)
Append: [A quantum semantic framework for natural language processing](https://arxiv.org/abs/2506.10077)
Append: [ImpliRet: Benchmarking the Implicit Fact Retrieval Challenge](https://arxiv.org/abs/2506.14407)
Append: [Evaluating Multimodal Large Language Models on Educational Textbook Question Answering](https://arxiv.org/abs/2506.21596)
Append: [Jan-nano Technical Report](https://arxiv.org/abs/2506.22760)
Append: [Stylometry recognizes human and LLM-generated texts in short samples](https://arxiv.org/abs/2507.00838)
Append: [The GPT Surprise: Offering Large Language Model Chat in a Massive Coding Class Reduced Engagement but Increased Adopters Exam Performances](https://arxiv.org/abs/2407.09975)
Append: [SECURE: Semantics-aware Embodied Conversation under Unawareness for Lifelong Robot Learning](https://arxiv.org/abs/2409.17755)
Append: [Online Intrinsic Rewards for Decision Making Agents from Large Language Model Feedback](https://arxiv.org/abs/2410.23022)
Append: [DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM Serving](https://arxiv.org/abs/2411.02820)
Append: [LongDocURL: a Comprehensive Multimodal Long Document Benchmark Integrating Understanding, Reasoning, and Locating](https://arxiv.org/abs/2412.18424)
Append: [ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning](https://arxiv.org/abs/2502.01100)
Append: [Agentic Reasoning: A Streamlined Framework for Enhancing LLM Reasoning with Agentic Tools](https://arxiv.org/abs/2502.04644)
Append: [ReVISE: Learning to Refine at Test-Time via Intrinsic Self-Verification](https://arxiv.org/abs/2502.14565)
Append: [Voting or Consensus? Decision-Making in Multi-Agent Debate](https://arxiv.org/abs/2502.19130)
Append: [Representation Bending for Large Language Model Safety](https://arxiv.org/abs/2504.01550)
Append: [BMDetect: A Multimodal Deep Learning Framework for Comprehensive Biomedical Misconduct Detection](https://arxiv.org/abs/2505.05763)
Append: [ProtocolLLM: RTL Benchmark for SystemVerilog Generation of Communication Protocols](https://arxiv.org/abs/2506.07945)
append_entries: 101
Finish: 2025-07-16 04:42:42.432069
------------------------------------------------------
Started: 2025-07-16 06:26:52.921809
Existing_entries: 1101
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1031
Summarized using GPT-3.5-turbo
Append: [Traveling Across Languages: Benchmarking Cross-Lingual Consistency in Multimodal LLMs](https://arxiv.org/abs/2505.15075)
append_entries: 1
Finish: 2025-07-16 06:26:55.427858
------------------------------------------------------
Started: 2025-07-16 08:24:19.338624
Existing_entries: 1001
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-16 08:24:19.607952
------------------------------------------------------
Started: 2025-07-16 10:19:17.423664
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-16 10:19:17.692554
------------------------------------------------------
Started: 2025-07-16 12:37:44.963469
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-16 12:37:45.235847
------------------------------------------------------
Started: 2025-07-16 14:16:49.447009
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-16 14:16:49.782956
------------------------------------------------------
Started: 2025-07-16 16:22:25.454386
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-16 16:22:25.789525
------------------------------------------------------
Started: 2025-07-16 18:25:38.780828
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-16 18:25:39.085245
------------------------------------------------------
Started: 2025-07-16 20:19:47.565364
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-16 20:19:47.891410
------------------------------------------------------
Started: 2025-07-16 22:17:19.935362
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-16 22:17:20.212433
------------------------------------------------------
Started: 2025-07-17 01:26:17.741343
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-17 01:26:18.031179
------------------------------------------------------
Started: 2025-07-17 03:25:37.812342
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-17 03:25:38.118431
------------------------------------------------------
Started: 2025-07-17 04:39:26.371470
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1102
Summarized using GPT-3.5-turbo
Append: [Subjective Evaluation Profile Analysis of Science Fiction Short Stories and its Critical-Theoretical Significance](https://arxiv.org/abs/2507.11582)
Token length: 1172
Summarized using GPT-3.5-turbo
Append: [MapIQ: Benchmarking Multimodal Large Language Models for Map Question Answering](https://arxiv.org/abs/2507.11625)
Token length: 994
Summarized using GPT-3.5-turbo
Append: [Cross-lingual Few-shot Learning for Persian Sentiment Analysis with Incremental Adaptation](https://arxiv.org/abs/2507.11634)
Token length: 1396
Summarized using GPT-3.5-turbo
Append: [Partitioner Guided Modal Learning Framework](https://arxiv.org/abs/2507.11661)
Token length: 1371
Summarized using GPT-3.5-turbo
Append: [ExpliCIT-QA: Explainable Code-Based Image Table Question Answering](https://arxiv.org/abs/2507.11694)
Token length: 1835
Summarized using GPT-3.5-turbo
Append: [CRABS: A syntactic-semantic pincer strategy for bounding LLM interpretation of Python notebooks](https://arxiv.org/abs/2507.11742)
Token length: 1141
Summarized using GPT-3.5-turbo
Append: [AI Wizards at CheckThat! 2025: Enhancing Transformer-Based Embeddings with Sentiment for Subjectivity Detection in News Articles](https://arxiv.org/abs/2507.11764)
Token length: 1140
Summarized using GPT-3.5-turbo
Append: [Tracing Facts or just Copies? A critical investigation of the Competitions of Mechanisms in Large Language Models](https://arxiv.org/abs/2507.11809)
Token length: 1123
Summarized using GPT-3.5-turbo
Append: [ILID: Native Script Language Identification for Indian Languages](https://arxiv.org/abs/2507.11832)
Token length: 1497
Summarized using GPT-3.5-turbo
Append: [Your LLM Knows the Future: Uncovering Its Multi-Token Prediction Potential](https://arxiv.org/abs/2507.11851)
Token length: 783
Summarized using GPT-3.5-turbo
Append: [Cross-Domain Transfer and Few-Shot Learning for Personal Identifiable Information Recognition](https://arxiv.org/abs/2507.11862)
Token length: 961
Summarized using GPT-3.5-turbo
Append: [COLA-GEC: A Bidirectional Framework for Enhancing Grammatical Acceptability and Error Correction](https://arxiv.org/abs/2507.11867)
Token length: 1220
Summarized using GPT-3.5-turbo
Append: [DualReward: A Dynamic Reinforcement Learning Framework for Cloze Tests Distractor Generation](https://arxiv.org/abs/2507.11875)
Token length: 1739
Summarized using GPT-3.5-turbo
Append: [LLMs Encode Harmfulness and Refusal Separately](https://arxiv.org/abs/2507.11878)
Token length: 1387
Summarized using GPT-3.5-turbo
Append: [Marco-Bench-MIF: On Multilingual Instruction-Following Capability of Large Language Models](https://arxiv.org/abs/2507.11882)
Token length: 1098
Summarized using GPT-3.5-turbo
Append: [A Survey of Deep Learning for Geometry Problem Solving](https://arxiv.org/abs/2507.11936)
Token length: 1245
Summarized using GPT-3.5-turbo
Append: [POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart Question Answering](https://arxiv.org/abs/2507.11939)
Token length: 965
Summarized using GPT-3.5-turbo
Append: [BlockBPE: Parallel BPE Tokenization](https://arxiv.org/abs/2507.11941)
Token length: 1123
Summarized using GPT-3.5-turbo
Append: [DAC: A Dynamic Attention-aware Approach for Task-Agnostic Prompt Compression](https://arxiv.org/abs/2507.11942)
Token length: 1230
Summarized using GPT-3.5-turbo
Append: [IAM: Efficient Inference through Attention Mapping between Different-scale LLMs](https://arxiv.org/abs/2507.11953)
Token length: 886
Summarized using GPT-3.5-turbo
Append: [The benefits of query-based KGQA systems for complex and temporal questions in LLM era](https://arxiv.org/abs/2507.11954)
Token length: 1468
Summarized using GPT-3.5-turbo
Append: [PoTPTQ: A Two-step Power-of-Two Post-training for LLMs](https://arxiv.org/abs/2507.11959)
Token length: 1420
Summarized using GPT-3.5-turbo
Append: [Toxicity-Aware Few-Shot Prompting for Low-Resource Singlish Translation](https://arxiv.org/abs/2507.11966)
Token length: 1396
Summarized using GPT-3.5-turbo
Append: [Graph Representations for Reading Comprehension Analysis using Large Language Model and Eye-Tracking Biomarker](https://arxiv.org/abs/2507.11972)
Token length: 1418
Summarized using GPT-3.5-turbo
Append: [Value-Based Large Language Model Agent Simulation for Mutual Evaluation of Trust and Interpersonal Closeness](https://arxiv.org/abs/2507.11979)
Token length: 1209
Summarized using GPT-3.5-turbo
Append: [Simplifications are Absolutists: How Simplified Language Reduces Word Sense Awareness in LLM-Generated Definitions](https://arxiv.org/abs/2507.11981)
Token length: 1891
Summarized using GPT-3.5-turbo
Append: [Improving Data and Parameter Efficiency of Neural Language Models Using Representation Analysis](https://arxiv.org/abs/2507.12004)
Token length: 998
Summarized using GPT-3.5-turbo
Append: [A Comparative Approach to Assessing Linguistic Creativity of Large Language Models and Humans](https://arxiv.org/abs/2507.12039)
Token length: 638
Summarized using GPT-3.5-turbo
Append: [Evaluating the Ability of Large Language Models to Reason about Cardinal Directions, Revisited](https://arxiv.org/abs/2507.12059)
Token length: 821
Summarized using GPT-3.5-turbo
Append: [StylOch at PAN: Gradient-Boosted Trees with Frequency-Based Stylometric Features](https://arxiv.org/abs/2507.12064)
Token length: 1334
Summarized using GPT-3.5-turbo
Append: [BOOKCOREF: Coreference Resolution at Book Scale](https://arxiv.org/abs/2507.12075)
Token length: 1654
Summarized using GPT-3.5-turbo
Append: [Findings of MEGA: Maths Explanation with LLMs using the Socratic Method for Active Learning](https://arxiv.org/abs/2507.12079)
Token length: 1384
Summarized using GPT-3.5-turbo
Append: [Iterative Augmentation with Summarization Refinement (IASR) Evaluation for Unstructured Survey data Modeling and Analysis](https://arxiv.org/abs/2507.12126)
Token length: 1814
Summarized using GPT-3.5-turbo
Append: [Overview of the Sensemaking Task at the ELOQUENT 2025 Lab: LLMs as Teachers, Students and Evaluators](https://arxiv.org/abs/2507.12143)
Token length: 883
Summarized using GPT-3.5-turbo
Append: [Toward a Behavioural Translation Style Space: Simulating the Temporal Dynamics of Affect, Behaviour, and Cognition in Human Translation Production](https://arxiv.org/abs/2507.12208)
Token length: 854
Summarized using GPT-3.5-turbo
Append: [Towards few-shot isolated word reading assessment](https://arxiv.org/abs/2507.12217)
Token length: 1528
Summarized using GPT-3.5-turbo
Append: [Improving Contextual ASR via Multi-grained Fusion with Large Language Models](https://arxiv.org/abs/2507.12252)
Token length: 1211
Summarized using GPT-3.5-turbo
Append: [Translationese-index: Using Likelihood Ratios for Graded and Generalizable Measurement of Translationese](https://arxiv.org/abs/2507.12260)
Token length: 1024
Summarized using GPT-3.5-turbo
Append: [Infherno: End-to-end Agent-based FHIR Resource Synthesis from Free-form Clinical Notes](https://arxiv.org/abs/2507.12261)
Token length: 1909
Summarized using GPT-3.5-turbo
Append: [Text-ADBench: Text Anomaly Detection Benchmark based on LLMs Embedding](https://arxiv.org/abs/2507.12295)
Token length: 1802
Summarized using GPT-3.5-turbo
Append: [Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and Summarization](https://arxiv.org/abs/2507.12308)
Token length: 1048
Summarized using GPT-3.5-turbo
Append: [Exploring Gender Bias in Alzheimer's Disease Detection: Insights from Mandarin and Greek Speech Perception](https://arxiv.org/abs/2507.12356)
Token length: 1282
Summarized using GPT-3.5-turbo
Append: [Beyond Single Models: Enhancing LLM Detection of Ambiguity in Requests through Debate](https://arxiv.org/abs/2507.12370)
Token length: 1432
Summarized using GPT-3.5-turbo
Append: [Web-Browsing LLMs Can Access Social Media Profiles and Infer User Demographics](https://arxiv.org/abs/2507.12372)
Token length: 1092
Summarized using GPT-3.5-turbo
Append: [Probing for Arithmetic Errors in Language Models](https://arxiv.org/abs/2507.12379)
Token length: 1753
Summarized using GPT-3.5-turbo
Append: [Advancing Retrieval-Augmented Generation for Structured Enterprise and Internal Data](https://arxiv.org/abs/2507.12425)
Token length: 1238
Summarized using GPT-3.5-turbo
Append: [Can We Predict Alignment Before Models Finish Thinking? Towards Monitoring Misaligned Reasoning Models](https://arxiv.org/abs/2507.12428)
Token length: 1076
Summarized using GPT-3.5-turbo
Append: [S2WTM: Spherical Sliced-Wasserstein Autoencoder for Topic Modeling](https://arxiv.org/abs/2507.12451)
Token length: 1589
Summarized using GPT-3.5-turbo
Append: [Language Models Improve When Pretraining Data Matches Target Tasks](https://arxiv.org/abs/2507.12466)
Token length: 1255
Summarized using GPT-3.5-turbo
Append: [Fairness Is Not Enough: Auditing Competence and Intersectional Bias in AI-powered Resume Screening](https://arxiv.org/abs/2507.11548)
Append: [Jailbreak-Tuning: Models Efficiently Learn Jailbreak Susceptibility](https://arxiv.org/abs/2507.11630)
Append: [Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification](https://arxiv.org/abs/2507.11662)
Append: [MetaLint: Generalizable Idiomatic Code Quality Analysis through Instruction-Following and Easy-to-Hard Generalization](https://arxiv.org/abs/2507.11687)
Append: [Simulated Language Acquisition in a Biologically Realistic Model of the Brain](https://arxiv.org/abs/2507.11788)
Append: [RiemannLoRA: A Unified Riemannian Framework for Ambiguity-Free LoRA Optimization](https://arxiv.org/abs/2507.12142)
Append: [RUMAA: Repeat-Aware Unified Music Audio Analysis for Score-Performance Alignment, Transcription, and Mistake Detection](https://arxiv.org/abs/2507.12175)
Append: [MERA Code: A Unified Framework for Evaluating Code Generation Across Tasks](https://arxiv.org/abs/2507.12284)
Append: [Nonlinear Concept Erasure: a Density Matching Approach](https://arxiv.org/abs/2507.12341)
Append: [Developing Visual Augmented Q&A System using Scalable Vision Embedding Retrieval & Late Interaction Re-ranker](https://arxiv.org/abs/2507.12378)
Append: [RAGGED: Towards Informed Design of Scalable and Stable RAG Systems](https://arxiv.org/abs/2403.09040)
Append: [Protecting Copyrighted Material with Unique Identifiers in Large Language Model Training](https://arxiv.org/abs/2403.15740)
Append: [Linearly-Interpretable Concept Embedding Models for Text Analysis](https://arxiv.org/abs/2406.14335)
Append: [Understanding Language Model Circuits through Knowledge Editing](https://arxiv.org/abs/2406.17241)
Append: [How Well Can Knowledge Edit Methods Edit Perplexing Knowledge?](https://arxiv.org/abs/2406.17253)
Append: [Measuring Spiritual Values and Bias of Large Language Models](https://arxiv.org/abs/2410.11647)
Append: [Rolling the DICE on Idiomaticity: How LLMs Fail to Grasp Context](https://arxiv.org/abs/2410.16069)
Append: [TRIM: Token Reduction and Inference Modeling for Cost-Effective Language Generation](https://arxiv.org/abs/2412.07682)
Append: [Labels Generated by Large Language Models Help Measure People's Empathy in Vitro](https://arxiv.org/abs/2501.00691)
Append: [Learning an Effective Premise Retrieval Model for Efficient Mathematical Formalization](https://arxiv.org/abs/2501.13959)
Append: [Flexible and Efficient Grammar-Constrained Decoding](https://arxiv.org/abs/2502.05111)
Append: [Organize the Web: Constructing Domains Enhances Pre-Training Data Curation](https://arxiv.org/abs/2502.10341)
Append: [DEEPER Insight into Your User: Directed Persona Refinement for Dynamic Persona Modeling](https://arxiv.org/abs/2502.11078)
Append: [Towards Geo-Culturally Grounded LLM Generations](https://arxiv.org/abs/2502.13497)
Append: [ReviewAgents: Bridging the Gap Between Human and AI-Generated Paper Reviews](https://arxiv.org/abs/2503.08506)
Append: [Resona: Improving Context Copying in Linear Recurrence Models with Retrieval](https://arxiv.org/abs/2503.22913)
Append: [Semantic Adapter for Universal Text Embeddings: Diagnosing and Mitigating Negation Blindness to Enhance Universality](https://arxiv.org/abs/2504.00584)
Append: [TD-EVAL: Revisiting Task-Oriented Dialogue Evaluation by Combining Turn-Level Precision with Dialogue-Level Comparisons](https://arxiv.org/abs/2504.19982)
Append: [Multi-domain Multilingual Sentiment Analysis in Industry: Predicting Aspect-based Opinion Quadruples](https://arxiv.org/abs/2505.10389)
Append: [Efficient and Direct Duplex Modeling for Speech-to-Speech Language Model](https://arxiv.org/abs/2505.15670)
Append: [Large Language Models Often Know When They Are Being Evaluated](https://arxiv.org/abs/2505.23836)
Append: [AKReF: An argumentative knowledge representation framework for structured argumentation](https://arxiv.org/abs/2506.00713)
Append: [Planning-Aware Code Infilling via Horizon-Length Prediction](https://arxiv.org/abs/2410.03103)
Append: [Hallucination Detox: Sensitivity Dropout (SenD) for Large Language Model Training](https://arxiv.org/abs/2410.15460)
Append: [METIS: Fast Quality-Aware RAG Systems with Configuration Adaptation](https://arxiv.org/abs/2412.10543)
Append: [Generative Emergent Communication: Large Language Model is a Collective World Model](https://arxiv.org/abs/2501.00226)
Append: [Learning to Reason at the Frontier of Learnability](https://arxiv.org/abs/2502.12272)
Append: [FADE: Why Bad Descriptions Happen to Good Features](https://arxiv.org/abs/2502.16994)
Append: [BRIDGE: Bootstrapping Text to Control Time-Series Generation via Multi-Agent Iterative Optimization and Diffusion Modeling](https://arxiv.org/abs/2503.02445)
Append: [A Survey of Frontiers in LLM Reasoning: Inference Scaling, Learning to Reason, and Agentic Systems](https://arxiv.org/abs/2504.09037)
Append: [Miipher-2: A Universal Speech Restoration Model for Million-Hour Scale Data Restoration](https://arxiv.org/abs/2505.04457)
append_entries: 90
Finish: 2025-07-17 04:41:23.978616
------------------------------------------------------
Started: 2025-07-17 06:27:19.299230
Existing_entries: 1090
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-17 06:27:19.700518
------------------------------------------------------
Started: 2025-07-17 08:24:18.273140
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-17 08:24:18.506895
------------------------------------------------------
Started: 2025-07-17 10:19:51.115260
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-17 10:19:51.350105
------------------------------------------------------
Started: 2025-07-17 12:36:51.598975
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-17 12:36:51.837520
------------------------------------------------------
Started: 2025-07-17 14:16:31.901640
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-17 14:16:32.134526
------------------------------------------------------
Started: 2025-07-17 16:22:31.670899
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-17 16:22:31.904671
------------------------------------------------------
Started: 2025-07-17 18:25:50.180335
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-17 18:25:50.419532
------------------------------------------------------
Started: 2025-07-17 20:19:52.277411
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-17 20:19:52.525918
------------------------------------------------------
Started: 2025-07-17 22:17:16.610690
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-17 22:17:16.870506
------------------------------------------------------
Started: 2025-07-18 01:25:43.472283
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-18 01:25:43.716396
------------------------------------------------------
Started: 2025-07-18 03:27:02.174534
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-18 03:27:02.457013
------------------------------------------------------
Started: 2025-07-18 04:41:08.681918
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1616
Summarized using GPT-3.5-turbo
Append: [Modeling Open-World Cognition as On-Demand Synthesis of Probabilistic Models](https://arxiv.org/abs/2507.12547)
Token length: 1572
Summarized using GPT-3.5-turbo
Append: [Is This Just Fantasy? Language Model Representations Reflect Human Judgments of Event Plausibility](https://arxiv.org/abs/2507.12553)
Token length: 689
Summarized using GPT-3.5-turbo
Append: [The first open machine translation system for the Chechen language](https://arxiv.org/abs/2507.12672)
Token length: 1953
Summarized using GPT-3.5-turbo
Append: [Improving Drug Identification in Overdose Death Surveillance using Large Language Models](https://arxiv.org/abs/2507.12679)
Token length: 1240
Summarized using GPT-3.5-turbo
Append: [AdaptiSent: Context-Aware Adaptive Attention for Multimodal Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2507.12695)
Token length: 1488
Summarized using GPT-3.5-turbo
Append: [AudioJudge: Understanding What Works in Large Audio Model Based Speech Evaluation](https://arxiv.org/abs/2507.12705)
Token length: 1338
Summarized using GPT-3.5-turbo
Append: [FLEXITOKENS: Flexible Tokenization for Evolving Language Models](https://arxiv.org/abs/2507.12720)
Token length: 1280
Summarized using GPT-3.5-turbo
Append: [TransEvalnia: Reasoning-based Evaluation and Ranking of Translations](https://arxiv.org/abs/2507.12724)
Token length: 731
Summarized using GPT-3.5-turbo
Append: [Strategy Adaptation in Large Language Model Werewolf Agents](https://arxiv.org/abs/2507.12732)
Token length: 1432
Summarized using GPT-3.5-turbo
Append: [Logit Arithmetic Elicits Long Reasoning Capabilities Without Training](https://arxiv.org/abs/2507.12759)
Token length: 923
Summarized using GPT-3.5-turbo
Append: [Synergy: End-to-end Concept Model](https://arxiv.org/abs/2507.12769)
Token length: 914
Summarized using GPT-3.5-turbo
Append: [Learning Robust Negation Text Representations](https://arxiv.org/abs/2507.12782)
Token length: 1175
Summarized using GPT-3.5-turbo
Append: [Large Language Models' Internal Perception of Symbolic Music](https://arxiv.org/abs/2507.12808)
Token length: 1321
Summarized using GPT-3.5-turbo
Append: [Are Knowledge and Reference in Multilingual Language Models Cross-Lingually Consistent?](https://arxiv.org/abs/2507.12838)
Token length: 1033
Summarized using GPT-3.5-turbo
Append: [Making Language Model a Hierarchical Classifier and Generator](https://arxiv.org/abs/2507.12930)
Token length: 859
Summarized using GPT-3.5-turbo
Append: [MRT at IberLEF-2025 PRESTA Task: Maximizing Recovery from Tables with Multiple Steps](https://arxiv.org/abs/2507.12981)
Token length: 1051
Summarized using GPT-3.5-turbo
Append: [Formalizing Attack Scenario Description: A Proposed Model](https://arxiv.org/abs/2507.13076)
Token length: 1098
Summarized using GPT-3.5-turbo
Append: [SemCSE: Semantic Contrastive Sentence Embeddings Using LLM-Generated Summaries For Scientific Abstracts](https://arxiv.org/abs/2507.13105)
Token length: 1043
Summarized using GPT-3.5-turbo
Append: [A Computational Framework to Identify Self-Aspects in Text](https://arxiv.org/abs/2507.13115)
Token length: 1282
Summarized using GPT-3.5-turbo
Append: [Assessing the Reliability of LLMs Annotations in the Context of Demographic Bias and Model Explanation](https://arxiv.org/abs/2507.13138)
Token length: 1024
Summarized using GPT-3.5-turbo
Append: [Feature-based analysis of oral narratives from Afrikaans and isiXhosa children](https://arxiv.org/abs/2507.13164)
Token length: 1240
Summarized using GPT-3.5-turbo
Append: [GEMMAS: Graph-based Evaluation Metrics for Multi Agent Systems](https://arxiv.org/abs/2507.13190)
Token length: 987
Summarized using GPT-3.5-turbo
Append: [Automatically assessing oral narratives of Afrikaans and isiXhosa children](https://arxiv.org/abs/2507.13205)
Token length: 1363
Summarized using GPT-3.5-turbo
Append: [Enhancing Cross-task Transfer of Large Language Models via Activation Steering](https://arxiv.org/abs/2507.13236)
Token length: 1040
Summarized using GPT-3.5-turbo
Append: [HATS: Hindi Analogy Test Set for Evaluating Reasoning in Large Language Models](https://arxiv.org/abs/2507.13238)
Token length: 1258
Summarized using GPT-3.5-turbo
Append: [Automating Steering for Safe Multimodal Large Language Models](https://arxiv.org/abs/2507.13255)
Token length: 1181
Summarized using GPT-3.5-turbo
Append: [QuestA: Expanding Reasoning Capacity in LLMs via Question Augmentation](https://arxiv.org/abs/2507.13266)
Token length: 1786
Summarized using GPT-3.5-turbo
Append: [Overview of the TalentCLEF 2025: Skill and Job Title Intelligence for Human Capital Management](https://arxiv.org/abs/2507.13275)
Token length: 1149
Summarized using GPT-3.5-turbo
Append: [Multi-Agent Synergy-Driven Iterative Visual Narrative Synthesis](https://arxiv.org/abs/2507.13285)
Token length: 1232
Summarized using GPT-3.5-turbo
Append: [AbGen: Evaluating Large Language Models in Ablation Study Design and Evaluation for Scientific Research](https://arxiv.org/abs/2507.13300)
Token length: 1305
Summarized using GPT-3.5-turbo
Append: [HapticCap: A Multimodal Dataset and Task for Understanding User Experience of Vibration Haptic Signals](https://arxiv.org/abs/2507.13318)
Token length: 1143
Summarized using GPT-3.5-turbo
Append: [Social and Political Framing in Search Engine Results](https://arxiv.org/abs/2507.13325)
Token length: 1319
Summarized using GPT-3.5-turbo
Append: [Vision-and-Language Training Helps Deploy Taxonomic Knowledge but Does Not Fundamentally Alter It](https://arxiv.org/abs/2507.13328)
Token length: 1907
Summarized using GPT-3.5-turbo
Append: [The Imitation Game: Turing Machine Imitator is Length Generalizable Reasoner](https://arxiv.org/abs/2507.13332)
Token length: 1527
Summarized using GPT-3.5-turbo
Append: [A Survey of Context Engineering for Large Language Models](https://arxiv.org/abs/2507.13334)
Token length: 1299
Summarized using GPT-3.5-turbo
Append: [Comparing Apples to Oranges: A Dataset & Analysis of LLM Humour Understanding from Traditional Puns to Topical Jokes](https://arxiv.org/abs/2507.13335)
Token length: 1191
Summarized using GPT-3.5-turbo
Append: [Perfect diffusion is $\mathsf{TC}^0$ -- Bad diffusion is Turing-complete](https://arxiv.org/abs/2507.12469)
Token length: 1320
Summarized using GPT-3.5-turbo
Append: [A Survey of AIOps in the Era of Large Language Models](https://arxiv.org/abs/2507.12472)
Token length: 737
Summarized using GPT-3.5-turbo
Append: [Spatially Grounded Explanations in Vision Language Models for Document Visual Question Answering](https://arxiv.org/abs/2507.12490)
Token length: 1296
Summarized using GPT-3.5-turbo
Append: [Scaling Up RL: Unlocking Diverse Reasoning in LLMs via Prolonged Training](https://arxiv.org/abs/2507.12507)
Token length: 1970
Summarized using GPT-3.5-turbo
Append: [Mono-InternVL-1.5: Towards Cheaper and Faster Monolithic Multimodal Large Language Models](https://arxiv.org/abs/2507.12566)
Token length: 727
Summarized using GPT-3.5-turbo
Append: [A Fuzzy Approach to Project Success: Measuring What Matters](https://arxiv.org/abs/2507.12653)
Token length: 1488
Summarized using GPT-3.5-turbo
Append: [A Comprehensive Survey of Electronic Health Record Modeling: From Deep Learning Approaches to Large Language Models](https://arxiv.org/abs/2507.12774)
Token length: 1778
Summarized using GPT-3.5-turbo
Append: [PMKLC: Parallel Multi-Knowledge Learning-based Lossless Compression for Large-Scale Genomics Database](https://arxiv.org/abs/2507.12805)
Token length: 889
Summarized using GPT-3.5-turbo
Append: [MCPEval: Automatic MCP-based Deep Evaluation for AI Agent Models](https://arxiv.org/abs/2507.12806)
Token length: 899
Summarized using GPT-3.5-turbo
Append: [Emotional Support with LLM-based Empathetic Dialogue Generation](https://arxiv.org/abs/2507.12820)
Token length: 1039
Summarized using GPT-3.5-turbo
Append: [Probabilistic Soundness Guarantees in LLM Reasoning Chains](https://arxiv.org/abs/2507.12948)
Token length: 1549
Summarized using GPT-3.5-turbo
Append: [UniSLU: Unified Spoken Language Understanding from Heterogeneous Cross-Task Datasets](https://arxiv.org/abs/2507.12951)
Token length: 1123
Summarized using GPT-3.5-turbo
Append: [Teach Old SAEs New Domain Tricks with Boosting](https://arxiv.org/abs/2507.12990)
Token length: 1492
Summarized using GPT-3.5-turbo
Append: [Rethinking the Embodied Gap in Vision-and-Language Navigation: A Holistic Study of Physical and Visual Disparities](https://arxiv.org/abs/2507.13019)
Append: [From Roots to Rewards: Dynamic Tree Reasoning with RL](https://arxiv.org/abs/2507.13142)
Append: [Inverse Reinforcement Learning Meets Large Language Model Post-Training: Basics, Advances, and Opportunities](https://arxiv.org/abs/2507.13158)
Append: [The Generative Energy Arena (GEA): Incorporating Energy Awareness in Large Language Model (LLM) Human Evaluations](https://arxiv.org/abs/2507.13302)
Append: [VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning](https://arxiv.org/abs/2507.13348)
Append: [A Logically Consistent Chain-of-Thought Approach for Stance Detection](https://arxiv.org/abs/2312.16054)
Append: [Exploiting Adaptive Contextual Masking for Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2402.13722)
Append: [On the Limitations of Large Language Models (LLMs): False Attribution](https://arxiv.org/abs/2404.04631)
Append: [DeFine: Decision-Making with Analogical Reasoning over Factor Profiles](https://arxiv.org/abs/2410.01772)
Append: [Identifying Task Groupings for Multi-Task Learning Using Pointwise V-Usable Information](https://arxiv.org/abs/2410.12774)
Append: [SCULPT: Systematic Tuning of Long Prompts](https://arxiv.org/abs/2410.20788)
Append: [IOPO: Empowering LLMs with Complex Instruction Following via Input-Output Preference Optimization](https://arxiv.org/abs/2411.06208)
Append: [Multi-task retriever fine-tuning for domain-specific and efficient RAG](https://arxiv.org/abs/2501.04652)
Append: [Memorization Inheritance in Sequence-Level Knowledge Distillation for Neural Machine Translation](https://arxiv.org/abs/2502.01491)
Append: [MPO: An Efficient Post-Processing Framework for Mixing Diverse Preference Alignment](https://arxiv.org/abs/2502.18699)
Append: [OASIS: Order-Augmented Strategy for Improved Code Search](https://arxiv.org/abs/2503.08161)
Append: [Synthesizing Privacy-Preserving Text Data via Finetuning without Finetuning Billion-Scale LLMs](https://arxiv.org/abs/2503.12347)
Append: [A Multi-Stage Framework with Taxonomy-Guided Reasoning for Occupation Classification Using Large Language Models](https://arxiv.org/abs/2503.12989)
Append: [CoDet-M4: Detecting Machine-Generated Code in Multi-Lingual, Multi-Generator and Multi-Domain Settings](https://arxiv.org/abs/2503.13733)
Append: [Secure Multifaceted-RAG for Enterprise: Hybrid Knowledge Retrieval with Security Filtering](https://arxiv.org/abs/2504.13425)
Append: [ConTextual: Improving Clinical Text Summarization in LLMs with Context-preserving Token Filtering and Knowledge Graphs](https://arxiv.org/abs/2504.16394)
Append: [MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced Knowledge Boundary Awareness](https://arxiv.org/abs/2504.21773)
Append: [Code2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General Reasoning](https://arxiv.org/abs/2505.13886)
Append: [ContextQFormer: A New Context Modeling Method for Multi-Turn Multi-Modal Conversations](https://arxiv.org/abs/2505.23121)
Append: [MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents](https://arxiv.org/abs/2506.15841)
Append: [Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language Models: An Empirical Evaluation](https://arxiv.org/abs/2506.17088)
Append: [ReCode: Updating Code API Knowledge with Reinforcement Learning](https://arxiv.org/abs/2506.20495)
Append: [VIDEE: Visual and Interactive Decomposition, Execution, and Evaluation of Text Analytics with Intelligent Agents](https://arxiv.org/abs/2506.21582)
Append: [GUI Test Migration via Abstraction and Concretization](https://arxiv.org/abs/2409.05028)
Append: [LoRA Done RITE: Robust Invariant Transformation Equilibration for LoRA Optimization](https://arxiv.org/abs/2410.20625)
Append: [Unified Triplet-Level Hallucination Evaluation for Large Vision-Language Models](https://arxiv.org/abs/2410.23114)
Append: [UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning](https://arxiv.org/abs/2502.15082)
Append: [BEARCUBS: A benchmark for computer-using web agents](https://arxiv.org/abs/2503.07919)
Append: [ActionStudio: A Lightweight Framework for Data and Training of Large Action Models](https://arxiv.org/abs/2503.22673)
Append: [Task-Circuit Quantization: Leveraging Knowledge Localization and Interpretability for Compression](https://arxiv.org/abs/2504.07389)
Append: [Fine-Tune an SLM or Prompt an LLM? The Case of Generating Low-Code Workflows](https://arxiv.org/abs/2505.24189)
Append: [Cross-Layer Discrete Concept Discovery for Interpreting Language Models](https://arxiv.org/abs/2506.20040)
append_entries: 86
Finish: 2025-07-18 04:43:29.437969
------------------------------------------------------
Started: 2025-07-18 06:27:08.579876
Existing_entries: 1086
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1948
Summarized using GPT-3.5-turbo
Append: [SmartThinker: Learning to Compress and Preserve Reasoning by Step-Level Length Control](https://arxiv.org/abs/2507.04348)
append_entries: 1
Finish: 2025-07-18 06:27:11.782020
------------------------------------------------------
Started: 2025-07-18 08:24:11.072398
Existing_entries: 1001
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-18 08:24:11.386757
------------------------------------------------------
Started: 2025-07-18 10:19:35.411716
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-18 10:19:35.694274
------------------------------------------------------
Started: 2025-07-18 12:36:48.857735
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-18 12:36:49.097796
------------------------------------------------------
Started: 2025-07-18 14:17:25.229435
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-18 14:17:25.458594
------------------------------------------------------
Started: 2025-07-18 16:23:34.846484
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-18 16:23:35.090539
------------------------------------------------------
Started: 2025-07-18 18:24:53.923329
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-18 18:24:54.165322
------------------------------------------------------
Started: 2025-07-18 20:19:17.029797
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-18 20:19:17.274653
------------------------------------------------------
Started: 2025-07-18 22:17:23.158159
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-18 22:17:23.409968
------------------------------------------------------
Started: 2025-07-19 01:23:05.805016
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-19 01:23:06.034123
------------------------------------------------------
Started: 2025-07-19 03:20:46.109581
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-19 03:20:46.388960
------------------------------------------------------
Started: 2025-07-19 04:28:56.575586
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-19 04:28:56.639067
------------------------------------------------------
Started: 2025-07-19 06:24:50.418594
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-19 06:24:50.536281
------------------------------------------------------
Started: 2025-07-19 08:21:55.684950
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-19 08:21:55.743089
------------------------------------------------------
Started: 2025-07-19 10:17:34.547101
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-19 10:17:34.621261
------------------------------------------------------
Started: 2025-07-19 12:33:27.163489
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-19 12:33:27.226416
------------------------------------------------------
Started: 2025-07-19 14:15:17.635847
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-19 14:15:17.694819
------------------------------------------------------
Started: 2025-07-19 16:20:02.554418
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-19 16:20:02.629911
------------------------------------------------------
Started: 2025-07-19 18:23:05.293498
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-19 18:23:05.365743
------------------------------------------------------
Started: 2025-07-19 20:18:28.769898
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-19 20:18:28.829460
------------------------------------------------------
Started: 2025-07-19 22:16:21.545642
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-19 22:16:21.605237
------------------------------------------------------
Started: 2025-07-20 01:41:56.947111
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-20 01:41:57.042428
------------------------------------------------------
Started: 2025-07-20 03:43:34.569177
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-20 03:43:34.630885
------------------------------------------------------
Started: 2025-07-20 04:39:42.458540
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-20 04:39:42.516706
------------------------------------------------------
Started: 2025-07-20 06:24:31.921932
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-20 06:24:31.992049
------------------------------------------------------
Started: 2025-07-20 08:21:18.097395
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-20 08:21:18.159490
------------------------------------------------------
Started: 2025-07-20 10:17:57.812587
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-20 10:17:57.896849
------------------------------------------------------
Started: 2025-07-20 12:33:47.131024
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-20 12:33:47.234890
------------------------------------------------------
Started: 2025-07-20 14:15:20.488437
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-20 14:15:20.549570
------------------------------------------------------
Started: 2025-07-20 16:20:07.730221
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-20 16:20:07.838990
------------------------------------------------------
Started: 2025-07-20 18:23:31.419469
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-20 18:23:31.494523
------------------------------------------------------
Started: 2025-07-20 20:19:06.544598
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-20 20:19:06.621561
------------------------------------------------------
Started: 2025-07-20 22:16:29.943082
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-20 22:16:30.048158
------------------------------------------------------
Started: 2025-07-21 01:40:00.331324
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-21 01:40:00.504424
------------------------------------------------------
Started: 2025-07-21 03:42:13.659472
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-21 03:42:13.718359
------------------------------------------------------
Started: 2025-07-21 04:44:23.543810
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1213
Summarized using GPT-3.5-turbo
Append: [Adaptive Linguistic Prompting (ALP) Enhances Phishing Webpage Detection in Multimodal Large Language Models](https://arxiv.org/abs/2507.13357)
Token length: 1427
Summarized using GPT-3.5-turbo
Append: [Persona-Based Synthetic Data Generation Using Multi-Stage Conditioning with Large Language Models for Emotion Recognition](https://arxiv.org/abs/2507.13380)
Token length: 1212
Summarized using GPT-3.5-turbo
Append: [SAFT: Structure-Aware Fine-Tuning of LLMs for AMR-to-Text Generation](https://arxiv.org/abs/2507.13381)
Token length: 1288
Summarized using GPT-3.5-turbo
Append: [Context-Based Fake News Detection using Graph Based Approach: ACOVID-19 Use-case](https://arxiv.org/abs/2507.13382)
Token length: 1435
Summarized using GPT-3.5-turbo
Append: [PARAM-1 BharatGen 2.9B Model](https://arxiv.org/abs/2507.13390)
Token length: 1008
Summarized using GPT-3.5-turbo
Append: [TopicImpact: Improving Customer Feedback Analysis with Opinion Units for Topic Modeling and Star-Rating Prediction](https://arxiv.org/abs/2507.13392)
Token length: 1321
Summarized using GPT-3.5-turbo
Append: [Mitigating Stylistic Biases of Machine Translation Systems via Monolingual Corpora Only](https://arxiv.org/abs/2507.13395)
Token length: 1379
Summarized using GPT-3.5-turbo
Append: [Causal Language Control in Multilingual Transformers via Sparse Feature Steering](https://arxiv.org/abs/2507.13410)
Token length: 1502
Summarized using GPT-3.5-turbo
Append: [Aligning Knowledge Graphs and Language Models for Factual Accuracy](https://arxiv.org/abs/2507.13411)
Token length: 1488
Summarized using GPT-3.5-turbo
Append: [Paper Summary Attack: Jailbreaking LLMs through LLM Safety Papers](https://arxiv.org/abs/2507.13474)
Token length: 1279
Summarized using GPT-3.5-turbo
Append: [Revisiting LLM Value Probing Strategies: Are They Robust and Expressive?](https://arxiv.org/abs/2507.13490)
Token length: 1429
Summarized using GPT-3.5-turbo
Append: [Encoding syntactic objects and Merge operations in function spaces](https://arxiv.org/abs/2507.13501)
Token length: 1219
Summarized using GPT-3.5-turbo
Append: [A Computational Approach to Modeling Conversational Systems: Analyzing Large-Scale Quasi-Patterned Dialogue Flows](https://arxiv.org/abs/2507.13544)
Token length: 1912
Summarized using GPT-3.5-turbo
Append: [Reading Between the Lines: Combining Pause Dynamics and Semantic Coherence for Automated Assessment of Thought Disorder](https://arxiv.org/abs/2507.13551)
Token length: 701
Summarized using GPT-3.5-turbo
Append: [A Data-Centric Framework for Addressing Phonetic and Prosodic Challenges in Russian Speech Generative Models](https://arxiv.org/abs/2507.13563)
Token length: 1508
Summarized using GPT-3.5-turbo
Append: [Linguistic and Embedding-Based Profiling of Texts generated by Humans and Large Language Models](https://arxiv.org/abs/2507.13614)
Token length: 1195
Summarized using GPT-3.5-turbo
Append: [Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters](https://arxiv.org/abs/2507.13618)
Token length: 1197
Summarized using GPT-3.5-turbo
Append: [CU-ICU: Customizing Unsupervised Instruction-Finetuned Language Models for ICU Datasets via Text-to-Text Transfer Transformer](https://arxiv.org/abs/2507.13655)
Token length: 1244
Summarized using GPT-3.5-turbo
Append: [KiC: Keyword-inspired Cascade for Cost-Efficient Text Generation with LLMs](https://arxiv.org/abs/2507.13666)
Token length: 1617
Summarized using GPT-3.5-turbo
Append: [LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for Multi-Turn Dialogues](https://arxiv.org/abs/2507.13681)
Token length: 1266
Summarized using GPT-3.5-turbo
Append: [Consistent Explainers or Unreliable Narrators? Understanding LLM-generated Group Recommendations](https://arxiv.org/abs/2507.13705)
Token length: 1527
Summarized using GPT-3.5-turbo
Append: [The Judge Variable: Challenging Judge-Agnostic Legal Judgment Prediction](https://arxiv.org/abs/2507.13732)
Token length: 1309
Summarized using GPT-3.5-turbo
Append: [PRIDE -- Parameter-Efficient Reduction of Identity Discrimination for Equality in LLMs](https://arxiv.org/abs/2507.13743)
Token length: 1485
Summarized using GPT-3.5-turbo
Append: [Innocence in the Crossfire: Roles of Skip Connections in Jailbreaking Visual Language Models](https://arxiv.org/abs/2507.13761)
Token length: 1602
Summarized using GPT-3.5-turbo
Append: [An Enhanced Model-based Approach for Short Text Clustering](https://arxiv.org/abs/2507.13793)
Token length: 1812
Summarized using GPT-3.5-turbo
Append: [Question-Answer Extraction from Scientific Articles Using Knowledge Graphs and Large Language Models](https://arxiv.org/abs/2507.13827)
Token length: 1401
Summarized using GPT-3.5-turbo
Append: [The Expressions of Depression and Anxiety in Chinese Psycho-counseling: Usage of First-person Singular Pronoun and Negative Emotional Words](https://arxiv.org/abs/2507.13839)
Token length: 1241
Summarized using GPT-3.5-turbo
Append: [Modeling Fair Play in Detective Stories with Language Models](https://arxiv.org/abs/2507.13841)
Token length: 1112
Summarized using GPT-3.5-turbo
Append: [InTraVisTo: Inside Transformer Visualisation Tool](https://arxiv.org/abs/2507.13858)
Token length: 903
Summarized using GPT-3.5-turbo
Append: [Label Unification for Cross-Dataset Generalization in Cybersecurity NER](https://arxiv.org/abs/2507.13870)
Token length: 1040
Summarized using GPT-3.5-turbo
Append: [Optimizing ASR for Catalan-Spanish Code-Switching: A Comparative Analysis of Methodologies](https://arxiv.org/abs/2507.13875)
Token length: 1112
Summarized using GPT-3.5-turbo
Append: [Using LLMs to identify features of personal and professional skills in an open-response situational judgment test](https://arxiv.org/abs/2507.13881)
Token length: 800
Summarized using GPT-3.5-turbo
Append: [Political Leaning and Politicalness Classification of Texts](https://arxiv.org/abs/2507.13913)
Token length: 951
Summarized using GPT-3.5-turbo
Append: [The Levers of Political Persuasion with Conversational AI](https://arxiv.org/abs/2507.13919)
Token length: 896
Summarized using GPT-3.5-turbo
Append: [Marcel: A Lightweight and Open-Source Conversational Agent for University Student Support](https://arxiv.org/abs/2507.13937)
Token length: 1196
Summarized using GPT-3.5-turbo
Append: [Exploiting Primacy Effect To Improve Large Language Models](https://arxiv.org/abs/2507.13949)
Token length: 1961
Summarized using GPT-3.5-turbo
Append: [Bottom-up Domain-specific Superintelligence: A Reliable Knowledge Graph is What We Need](https://arxiv.org/abs/2507.13966)
Token length: 1096
Summarized using GPT-3.5-turbo
Append: [Open Automatic Speech Recognition Models for Classical and Modern Standard Arabic](https://arxiv.org/abs/2507.13977)
Token length: 915
Summarized using GPT-3.5-turbo
Append: [Efficient Temporal Tokenization for Mobility Prediction with Large Language Models](https://arxiv.org/abs/2507.14017)
Token length: 1309
Summarized using GPT-3.5-turbo
Append: [CPC-CMS: Cognitive Pairwise Comparison Classification Model Selection Framework for Document-level Sentiment Analysis](https://arxiv.org/abs/2507.14022)
Token length: 925
Summarized using GPT-3.5-turbo
Append: [Evaluating the Effectiveness of Cost-Efficient Large Language Models in Benchmark Biomedical Tasks](https://arxiv.org/abs/2507.14045)
Token length: 1130
Summarized using GPT-3.5-turbo
Append: [Collaborative Rational Speech Act: Pragmatic Reasoning for Multi-Turn Dialog](https://arxiv.org/abs/2507.14063)
Token length: 1953
Summarized using GPT-3.5-turbo
Append: [DENSE: Longitudinal Progress Note Generation with Temporal Modeling of Heterogeneous Clinical Notes Across Hospital Visits](https://arxiv.org/abs/2507.14079)
Token length: 1830
Summarized using GPT-3.5-turbo
Append: [Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts (PLABA) track](https://arxiv.org/abs/2507.14096)
Token length: 776
Summarized using GPT-3.5-turbo
Append: [Physical models realizing the transformer architecture of large language models](https://arxiv.org/abs/2507.13354)
Token length: 1483
Summarized using GPT-3.5-turbo
Append: [Enhancing Spatial Reasoning in Vision-Language Models via Chain-of-Thought Prompting and Reinforcement Learning](https://arxiv.org/abs/2507.13362)
Token length: 1735
Summarized using GPT-3.5-turbo
Append: [DyG-RAG: Dynamic Graph Retrieval-Augmented Generation with Event-Centric Reasoning](https://arxiv.org/abs/2507.13396)
Token length: 1215
Summarized using GPT-3.5-turbo
Append: [GOFAI meets Generative AI: Development of Expert Systems by means of Large Language Models](https://arxiv.org/abs/2507.13550)
Token length: 1743
Summarized using GPT-3.5-turbo
Append: [TexGS-VolVis: Expressive Scene Editing for Volume Visualization via Textured Gaussian Splatting](https://arxiv.org/abs/2507.13586)
Token length: 1382
Summarized using GPT-3.5-turbo
Append: [CoTasks: Chain-of-Thought based Video Instruction Tuning Tasks](https://arxiv.org/abs/2507.13609)
Append: [DailyLLM: Context-Aware Activity Log Generation Using Multi-Modal Sensors and LLMs](https://arxiv.org/abs/2507.13737)
Append: [Teaching Vision-Language Models to Ask: Resolving Ambiguity in Visual Questions](https://arxiv.org/abs/2507.13773)
Append: [RAG-based Architectures for Drug Side Effect Retrieval in LLMs](https://arxiv.org/abs/2507.13822)
Append: [SPARQL Query Generation with LLMs: Measuring the Impact of Training Data Memorization and Knowledge Injection](https://arxiv.org/abs/2507.13859)
Append: [Preprint: Did I Just Browse A Website Written by LLMs?](https://arxiv.org/abs/2507.13933)
Append: [EdgeVLA: Efficient Vision-Language-Action Models](https://arxiv.org/abs/2507.14049)
Append: [NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining](https://arxiv.org/abs/2507.14119)
Append: [ViMMRC 2.0 -- Enhancing Machine Reading Comprehension on Vietnamese Literature Text](https://arxiv.org/abs/2303.18162)
Append: [Meta4XNLI: A Crosslingual Parallel Corpus for Metaphor Detection and Interpretation](https://arxiv.org/abs/2404.07053)
Append: [psifx -- Psychological and Social Interactions Feature Extraction Package](https://arxiv.org/abs/2407.10266)
Append: [Sparse Rewards Can Self-Train Dialogue Agents](https://arxiv.org/abs/2409.04617)
Append: [Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs](https://arxiv.org/abs/2410.13394)
Append: [Temporal reasoning for timeline summarisation in social media](https://arxiv.org/abs/2501.00152)
Append: [Consistency of Responses and Continuations Generated by Large Language Models on Social Media](https://arxiv.org/abs/2501.08102)
Append: [ASTRID -- An Automated and Scalable TRIaD for the Evaluation of RAG-based Clinical Question Answering Systems](https://arxiv.org/abs/2501.08208)
Append: [Culture is Not Trivia: Sociocultural Theory for Cultural NLP](https://arxiv.org/abs/2502.12057)
Append: [When People are Floods: Analyzing Dehumanizing Metaphors in Immigration Discourse with Large Language Models](https://arxiv.org/abs/2502.13246)
Append: [Is That Your Final Answer? Test-Time Scaling Improves Selective Question Answering](https://arxiv.org/abs/2502.13962)
Append: [HoH: A Dynamic Benchmark for Evaluating the Impact of Outdated Information on Retrieval-Augmented Generation](https://arxiv.org/abs/2503.04800)
Append: [MultiBLiMP 1.0: A Massively Multilingual Benchmark of Linguistic Minimal Pairs](https://arxiv.org/abs/2504.02768)
Append: [ParaPO: Aligning Language Models to Reduce Verbatim Reproduction of Pre-training Data](https://arxiv.org/abs/2504.14452)
Append: [DeepSeek-Prover-V2: Advancing Formal Mathematical Reasoning via Reinforcement Learning for Subgoal Decomposition](https://arxiv.org/abs/2504.21801)
Append: [On-Policy Optimization with Group Equivalent Preference for Multi-Programming Language Understanding](https://arxiv.org/abs/2505.12723)
Append: [Exploring Graph Representations of Logical Forms for Language Modeling](https://arxiv.org/abs/2505.14523)
Append: [On the class of coding optimality of human languages and the origins of Zipf's law](https://arxiv.org/abs/2505.20015)
Append: [RExBench: Can coding agents autonomously implement AI research extensions?](https://arxiv.org/abs/2506.22598)
Append: [STACK: Adversarial Attacks on LLM Safeguard Pipelines](https://arxiv.org/abs/2506.24068)
Append: [On Pre-training of Multimodal Language Models Customized for Chart Understanding](https://arxiv.org/abs/2407.14506)
Append: [Deep Learning based Key Information Extraction from Business Documents: Systematic Literature Review](https://arxiv.org/abs/2408.06345)
Append: [An Approach for Auto Generation of Labeling Functions for Software Engineering Chatbots](https://arxiv.org/abs/2410.07094)
Append: [From Code to Compliance: Assessing ChatGPT's Utility in Designing an Accessible Webpage -- A Case Study](https://arxiv.org/abs/2501.03572)
Append: [Code Readability in the Age of Large Language Models: An Industrial Case Study from Atlassian](https://arxiv.org/abs/2501.11264)
Append: [To Code or not to Code? Adaptive Tool Integration for Math Language Models via Expectation-Maximization](https://arxiv.org/abs/2502.00691)
Append: [From Words to Collisions: LLM-Guided Evaluation and Adversarial Generation of Safety-Critical Driving Scenarios](https://arxiv.org/abs/2502.02145)
Append: [Harmony in Divergence: Towards Fast, Accurate, and Memory-efficient Zeroth-order LLM Fine-tuning](https://arxiv.org/abs/2502.03304)
Append: [EvolveNav: Self-Improving Embodied Reasoning for LLM-Based Vision-Language Navigation](https://arxiv.org/abs/2506.01551)
Append: [The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity](https://arxiv.org/abs/2506.06941)
Append: [LLM-driven Medical Report Generation via Communication-efficient Heterogeneous Federated Learning](https://arxiv.org/abs/2506.17562)
Append: [Reasoning about Uncertainty: Do Reasoning Models Know When They Don't Know?](https://arxiv.org/abs/2506.18183)
Append: [LearnLens: LLM-Enabled Personalised, Curriculum-Grounded Feedback with Educators in the Loop](https://arxiv.org/abs/2507.04295)
Append: [The role of large language models in UI/UX design: A systematic literature review](https://arxiv.org/abs/2507.04469)
Append: [Critiques of World Models](https://arxiv.org/abs/2507.05169)
append_entries: 92
Finish: 2025-07-21 04:46:29.637064
------------------------------------------------------
Started: 2025-07-21 06:27:44.601235
Existing_entries: 1092
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-21 06:27:44.885984
------------------------------------------------------
Started: 2025-07-21 08:27:35.779924
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-21 08:27:36.011001
------------------------------------------------------
Started: 2025-07-21 10:20:01.033458
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-21 10:20:01.298926
------------------------------------------------------
Started: 2025-07-21 12:38:04.089217
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-21 12:38:04.394164
------------------------------------------------------
Started: 2025-07-21 14:19:30.784697
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-21 14:19:31.018226
------------------------------------------------------
Started: 2025-07-21 16:21:52.046281
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-21 16:21:52.300075
------------------------------------------------------
Started: 2025-07-21 18:26:50.062278
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-21 18:26:50.299895
------------------------------------------------------
Started: 2025-07-21 20:19:51.282883
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-21 20:19:51.518034
------------------------------------------------------
Started: 2025-07-21 22:17:53.569367
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-21 22:17:53.838953
------------------------------------------------------
Started: 2025-07-22 01:26:06.616479
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-22 01:26:06.878856
------------------------------------------------------
Started: 2025-07-22 03:26:51.824645
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-22 03:26:52.069439
------------------------------------------------------
Started: 2025-07-22 04:41:06.836257
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1373
Summarized using GPT-3.5-turbo
Append: [DeepWriter: A Fact-Grounded Multimodal Writing Assistant Based On Offline Knowledge Base](https://arxiv.org/abs/2507.14189)
Token length: 1276
Summarized using GPT-3.5-turbo
Append: [Retention analysis of edited knowledge after fine-tuning](https://arxiv.org/abs/2507.14198)
Token length: 1398
Summarized using GPT-3.5-turbo
Append: [Open-Source LLMs Collaboration Beats Closed-Source LLMs: A Scalable Multi-Agent System](https://arxiv.org/abs/2507.14200)
Token length: 1950
Summarized using GPT-3.5-turbo
Append: [Let's Measure the Elephant in the Room: Facilitating Personalized Automated Analysis of Privacy Policies at Scale](https://arxiv.org/abs/2507.14214)
Token length: 1461
Summarized using GPT-3.5-turbo
Append: [Beyond Architectures: Evaluating the Role of Contextual Embeddings in Detecting Bipolar Disorder on Social Media](https://arxiv.org/abs/2507.14231)
Token length: 1966
Summarized using GPT-3.5-turbo
Append: [Language Models Change Facts Based on the Way You Talk](https://arxiv.org/abs/2507.14238)
Token length: 1271
Summarized using GPT-3.5-turbo
Append: [CCL-XCoT: An Efficient Cross-Lingual Knowledge Transfer Method for Mitigating Hallucination Generation](https://arxiv.org/abs/2507.14239)
Token length: 1880
Summarized using GPT-3.5-turbo
Append: [HuggingGraph: Understanding the Supply Chain of LLM Ecosystem](https://arxiv.org/abs/2507.14240)
Token length: 956
Summarized using GPT-3.5-turbo
Append: [Promptomatix: An Automatic Prompt Optimization Framework for Large Language Models](https://arxiv.org/abs/2507.14241)
Token length: 1304
Summarized using GPT-3.5-turbo
Append: [In-Depth and In-Breadth: Pre-training Multimodal Language Models Customized for Comprehensive Chart Understanding](https://arxiv.org/abs/2507.14298)
Token length: 1448
Summarized using GPT-3.5-turbo
Append: [Aligning Large Language Models to Low-Resource Languages through LLM-Based Selective Translation: A Systematic Study](https://arxiv.org/abs/2507.14304)
Token length: 1121
Summarized using GPT-3.5-turbo
Append: [How LLMs Comprehend Temporal Meaning in Narratives: A Case Study in Cognitive Evaluation of LLMs](https://arxiv.org/abs/2507.14307)
Token length: 1120
Summarized using GPT-3.5-turbo
Append: [What Makes You CLIC: Detection of Croatian Clickbait Headlines](https://arxiv.org/abs/2507.14314)
Token length: 1302
Summarized using GPT-3.5-turbo
Append: [Can LLMs Infer Personality from Real World Conversations?](https://arxiv.org/abs/2507.14355)
Token length: 1366
Summarized using GPT-3.5-turbo
Append: [Text-to-SQL for Enterprise Data Analytics](https://arxiv.org/abs/2507.14372)
Token length: 1187
Summarized using GPT-3.5-turbo
Append: [Error-Aware Curriculum Learning for Biomedical Relation Classification](https://arxiv.org/abs/2507.14374)
Token length: 1380
Summarized using GPT-3.5-turbo
Append: [X-Intelligence 3.0: Training and Evaluating Reasoning LLM for Semiconductor Display](https://arxiv.org/abs/2507.14430)
Token length: 661
Summarized using GPT-3.5-turbo
Append: [XL-DURel: Finetuning Sentence Transformers for Ordinal Word-in-Context Classification](https://arxiv.org/abs/2507.14578)
Token length: 1949
Summarized using GPT-3.5-turbo
Append: [Exploring Human-AI Complementarity in CPS Diagnosis Using Unimodal and Multimodal BERT Models](https://arxiv.org/abs/2507.14579)
Token length: 1862
Summarized using GPT-3.5-turbo
Append: [Explainable Collaborative Problem Solving Diagnosis with BERT using SHAP and its Implications for Teacher Adoption](https://arxiv.org/abs/2507.14584)
Token length: 1023
Summarized using GPT-3.5-turbo
Append: [Backtranslation and paraphrasing in the LLM era? Comparing data augmentation methods for emotion classification](https://arxiv.org/abs/2507.14590)
Token length: 1624
Summarized using GPT-3.5-turbo
Append: [Retrieval-Augmented Clinical Benchmarking for Contextual Model Testing in Kenyan Primary Care: A Methodology Paper](https://arxiv.org/abs/2507.14615)
Token length: 805
Summarized using GPT-3.5-turbo
Append: [Linear Relational Decoding of Morphology in Language Models](https://arxiv.org/abs/2507.14640)
Token length: 1077
Summarized using GPT-3.5-turbo
Append: [Cleanse: Uncertainty Estimation Approach Using Clustering-based Semantic Consistency in LLMs](https://arxiv.org/abs/2507.14649)
Token length: 1407
Summarized using GPT-3.5-turbo
Append: [Mangosteen: An Open Thai Corpus for Language Model Pretraining](https://arxiv.org/abs/2507.14664)
Token length: 1381
Summarized using GPT-3.5-turbo
Append: [Large Language Models as Medical Codes Selectors: a benchmark using the International Classification of Primary Care](https://arxiv.org/abs/2507.14681)
Token length: 1962
Summarized using GPT-3.5-turbo
Append: [MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via Context-Aware Multi-Stage Policy Optimization](https://arxiv.org/abs/2507.14683)
Token length: 1357
Summarized using GPT-3.5-turbo
Append: [Mind the Gap: A Review of Arabic Post-Training Datasets and Their Limitations](https://arxiv.org/abs/2507.14688)
Token length: 1681
Summarized using GPT-3.5-turbo
Append: [Rethinking Suicidal Ideation Detection: A Trustworthy Annotation Framework and Cross-Lingual Model Evaluation](https://arxiv.org/abs/2507.14693)
Token length: 1223
Summarized using GPT-3.5-turbo
Append: [Disparities in Peer Review Tone and the Role of Reviewer Anonymity](https://arxiv.org/abs/2507.14741)
Token length: 1811
Summarized using GPT-3.5-turbo
Append: [On the robustness of modeling grounded word learning through a child's egocentric input](https://arxiv.org/abs/2507.14749)
Token length: 1580
Summarized using GPT-3.5-turbo
Append: [GRACE: Generative Recommendation via Journey-Aware Sparse Attention on Chain-of-Thought Tokenization](https://arxiv.org/abs/2507.14758)
Token length: 1473
Summarized using GPT-3.5-turbo
Append: [FastLongSpeech: Enhancing Large Speech-Language Models for Efficient Long-Speech Processing](https://arxiv.org/abs/2507.14815)
Token length: 1762
Summarized using GPT-3.5-turbo
Append: [Doc2Chart: Intent-Driven Zero-Shot Chart Generation from Documents](https://arxiv.org/abs/2507.14819)
Token length: 1477
Summarized using GPT-3.5-turbo
Append: [Beyond Isolated Capabilities: Bridging Long CoT Reasoning and Long-Context Understanding](https://arxiv.org/abs/2507.14849)
Token length: 1696
Summarized using GPT-3.5-turbo
Append: [Tiny language models](https://arxiv.org/abs/2507.14871)
Token length: 1188
Summarized using GPT-3.5-turbo
Append: [MEKiT: Multi-source Heterogeneous Knowledge Injection Method via Instruction Tuning for Emotion-Cause Pair Extraction](https://arxiv.org/abs/2507.14887)
Token length: 1361
Summarized using GPT-3.5-turbo
Append: [Sparse Autoencoder-guided Supervised Finetuning to Mitigate Unexpected Code-Switching in LLMs](https://arxiv.org/abs/2507.14894)
Token length: 1392
Summarized using GPT-3.5-turbo
Append: [From Neurons to Semantics: Evaluating Cross-Linguistic Alignment Capabilities of Large Language Models via Neurons Alignment](https://arxiv.org/abs/2507.14900)
Token length: 980
Summarized using GPT-3.5-turbo
Append: [PromptSuite: A Task-Agnostic Framework for Multi-Prompt Generation](https://arxiv.org/abs/2507.14913)
Token length: 986
Summarized using GPT-3.5-turbo
Append: [SYNTHIA: Synthetic Yet Naturally Tailored Human-Inspired PersonAs](https://arxiv.org/abs/2507.14922)
Token length: 1272
Summarized using GPT-3.5-turbo
Append: [MUR: Momentum Uncertainty guided Reasoning for Large Language Models](https://arxiv.org/abs/2507.14958)
Token length: 1483
Summarized using GPT-3.5-turbo
Append: [RefCritic: Training Long Chain-of-Thought Critic Models with Refinement Feedback](https://arxiv.org/abs/2507.15024)
Token length: 1374
Summarized using GPT-3.5-turbo
Append: [WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization](https://arxiv.org/abs/2507.15061)
Token length: 1193
Summarized using GPT-3.5-turbo
Append: [Evaluation of Coding Schemes for Transformer-based Gene Sequence Modeling](https://arxiv.org/abs/2507.15087)
Token length: 1416
Summarized using GPT-3.5-turbo
Append: [A Penalty Goes a Long Way: Measuring Lexical Diversity in Synthetic Texts Under Prompt-Influenced Length Variations](https://arxiv.org/abs/2507.15092)
Token length: 979
Summarized using GPT-3.5-turbo
Append: [Filling the Gap: Is Commonsense Knowledge Generation useful for Natural Language Inference?](https://arxiv.org/abs/2507.15100)
Token length: 1179
Summarized using GPT-3.5-turbo
Append: [From Disagreement to Understanding: The Case for Ambiguity Detection in NLI](https://arxiv.org/abs/2507.15114)
Token length: 1166
Summarized using GPT-3.5-turbo
Append: [A Case Against Implicit Standards: Homophone Normalization in Machine Translation for Languages that use the Ge'ez Script](https://arxiv.org/abs/2507.15142)
Token length: 1202
Summarized using GPT-3.5-turbo
Append: [What Level of Automation is "Good Enough"? A Benchmark of Large Language Models for Meta-Analysis Data Extraction](https://arxiv.org/abs/2507.15152)
Append: [Collaborative Distillation Strategies for Parameter-Efficient Language Model Deployment](https://arxiv.org/abs/2507.15198)
Append: [SOI Matters: Analyzing Multi-Setting Training Dynamics in Pretrained Language Models via Subsets of Interest](https://arxiv.org/abs/2507.15236)
Append: [ChiMed 2.0: Advancing Chinese Medical Dataset in Facilitating Large Language Modeling](https://arxiv.org/abs/2507.15275)
Append: [A Novel Self-Evolution Framework for Large Language Models](https://arxiv.org/abs/2507.15281)
Append: [Beyond Easy Wins: A Text Hardness-Aware Benchmark for LLM-generated Text Detection](https://arxiv.org/abs/2507.15286)
Append: [On the Inevitability of Left-Leaning Political Bias in Aligned Language Models](https://arxiv.org/abs/2507.15328)
Append: [Reasoning Models are Test Exploiters: Rethinking Multiple-Choice](https://arxiv.org/abs/2507.15337)
Append: [LionGuard 2: Building Lightweight, Data-Efficient & Localised Multilingual Content Moderators](https://arxiv.org/abs/2507.15339)
Append: [Probing Information Distribution in Transformer Architectures through Entropy Analysis](https://arxiv.org/abs/2507.15347)
Append: [Metaphor and Large Language Models: When Surface Features Matter More than Deep Understanding](https://arxiv.org/abs/2507.15357)
Append: [STITCH: Simultaneous Thinking and Talking with Chunked Reasoning for Spoken Language Models](https://arxiv.org/abs/2507.15375)
Append: [AlgoSimBench: Identifying Algorithmically Similar Problems for Competitive Programming](https://arxiv.org/abs/2507.15378)
Append: [ASPERA: A Simulated Environment to Evaluate Planning for Complex Action Execution](https://arxiv.org/abs/2507.15501)
Append: [Step-level Verifier-guided Hybrid Test-Time Scaling for Large Language Models](https://arxiv.org/abs/2507.15512)
Append: [Evaluating Text Style Transfer: A Nine-Language Benchmark for Text Detoxification](https://arxiv.org/abs/2507.15557)
Append: [Smart Eyes for Silent Threats: VLMs and In-Context Learning for THz Imaging](https://arxiv.org/abs/2507.15576)
Append: [Learning to Extract Rational Evidence via Reinforcement Learning for Retrieval-Augmented Generation](https://arxiv.org/abs/2507.15586)
Append: [Conflicting narratives and polarization on social media](https://arxiv.org/abs/2507.15600)
Append: [Leveraging Context for Multimodal Fallacy Classification in Political Debates](https://arxiv.org/abs/2507.15641)
Append: [P3: Prompts Promote Prompting](https://arxiv.org/abs/2507.15675)
Append: [CoLD: Counterfactually-Guided Length Debiasing for Process Reward Models](https://arxiv.org/abs/2507.15698)
Append: [Compositional Understanding in Signaling Games](https://arxiv.org/abs/2507.15706)
Append: [Is Large Language Model Performance on Reasoning Tasks Impacted by Different Ways Questions Are Asked?](https://arxiv.org/abs/2507.15707)
Append: [Chinchunmei at SemEval-2025 Task 11: Boosting the Large Language Model's Capability of Emotion Perception using Contrastive Learning](https://arxiv.org/abs/2507.15714)
Append: [From Queries to Criteria: Understanding How Astronomers Evaluate LLMs](https://arxiv.org/abs/2507.15715)
Append: [BEnchmarking LLMs for Ophthalmology (BELO) for Ophthalmological Knowledge and Reasoning](https://arxiv.org/abs/2507.15717)
Append: [Understanding Large Language Models' Ability on Interdisciplinary Research](https://arxiv.org/abs/2507.15736)
Append: [A Fisher's exact test justification of the TF-IDF term-weighting scheme](https://arxiv.org/abs/2507.15742)
Append: [DialogueForge: LLM Simulation of Human-Chatbot Dialogue](https://arxiv.org/abs/2507.15752)
Append: [Interaction as Intelligence: Deep Research With Human-AI Partnership](https://arxiv.org/abs/2507.15759)
Append: [Supernova: Achieving More with Less in Transformer Architectures](https://arxiv.org/abs/2507.15773)
Append: [Stabilizing Knowledge, Promoting Reasoning: Dual-Token Constraints for RLVR](https://arxiv.org/abs/2507.15778)
Append: [Reservoir Computing as a Language Model](https://arxiv.org/abs/2507.15779)
Append: [Operationalizing AI for Good: Spotlight on Deployment and Integration of AI Models in Humanitarian Work](https://arxiv.org/abs/2507.15823)
Append: [The Impact of Language Mixing on Bilingual LLM Reasoning](https://arxiv.org/abs/2507.15849)
Append: [3LM: Bridging Arabic, STEM, and Code through Benchmarking](https://arxiv.org/abs/2507.15850)
Append: [A Sparsity Predicting Approach for Large Language Models via Activation Pattern Clustering](https://arxiv.org/abs/2507.14179)
Append: [ExCyTIn-Bench: Evaluating LLM agents on Cyber Threat Investigation](https://arxiv.org/abs/2507.14201)
Append: [LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of Large Language Models](https://arxiv.org/abs/2507.14204)
Append: [Identifying Algorithmic and Domain-Specific Bias in Parliamentary Debate Summarisation](https://arxiv.org/abs/2507.14221)
Append: [WebGuard: Building a Generalizable Guardrail for Web Agents](https://arxiv.org/abs/2507.14293)
Append: [Solo Connection: A Parameter Efficient Fine-Tuning Technique for Transformers](https://arxiv.org/abs/2507.14353)
Append: [Assessing the Reliability of Large Language Models for Deductive Qualitative Coding: A Comparative Study of ChatGPT Interventions](https://arxiv.org/abs/2507.14384)
Append: [Inverse Scaling in Test-Time Compute](https://arxiv.org/abs/2507.14417)
Append: [It's Not That Simple. An Analysis of Simple Test-Time Scaling](https://arxiv.org/abs/2507.14419)
Append: [Routine: A Structural Planning Framework for LLM Agent System in Enterprise](https://arxiv.org/abs/2507.14447)
Append: [Efficient Whole Slide Pathology VQA via Token Compression](https://arxiv.org/abs/2507.14497)
Append: [Conan: A Chunkwise Online Network for Zero-Shot Adaptive Voice Conversion](https://arxiv.org/abs/2507.14534)
Append: [What do Large Language Models know about materials?](https://arxiv.org/abs/2507.14586)
Append: [Optimizing Legal Document Retrieval in Vietnamese with Semi-Hard Negative Mining](https://arxiv.org/abs/2507.14619)
Append: [When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems](https://arxiv.org/abs/2507.14660)
Append: [Docopilot: Improving Multimodal Models for Document-Level Understanding](https://arxiv.org/abs/2507.14675)
Append: [GCC-Spam: Spam Detection via GAN, Contrastive Learning, and Character Similarity Networks](https://arxiv.org/abs/2507.14679)
Append: [The Invisible Leash: Why RLVR May Not Escape Its Origin](https://arxiv.org/abs/2507.14843)
Append: [Hear Your Code Fail, Voice-Assisted Debugging for Python](https://arxiv.org/abs/2507.15007)
Append: [Long-Short Distance Graph Neural Networks and Improved Curriculum Learning for Emotion Recognition in Conversation](https://arxiv.org/abs/2507.15205)
Append: [Exploiting Context-dependent Duration Features for Voice Anonymization Attack Systems](https://arxiv.org/abs/2507.15214)
Append: [GREAT: Guiding Query Generation with a Trie for Recommending Related Search about Video at Kuaishou](https://arxiv.org/abs/2507.15267)
Append: [A2TTS: TTS for Low Resource Indian Languages](https://arxiv.org/abs/2507.15272)
Append: [Off-Policy Corrected Reward Modeling for Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2507.15507)
Append: [Data Mixing Agent: Learning to Re-weight Domains for Continual Pre-training](https://arxiv.org/abs/2507.15640)
Append: [Towards physician-centered oversight of conversational diagnostic AI](https://arxiv.org/abs/2507.15743)
Append: [LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy Optimization](https://arxiv.org/abs/2507.15758)
Append: [Dissociating model architectures from inference computations](https://arxiv.org/abs/2507.15776)
Append: [Small LLMs Do Not Learn a Generalizable Theory of Mind via Reinforcement Learning](https://arxiv.org/abs/2507.15788)
Append: [Hierarchical Budget Policy Optimization for Adaptive Reasoning](https://arxiv.org/abs/2507.15844)
Append: [GUI-G$^2$: Gaussian Reward Modeling for GUI Grounding](https://arxiv.org/abs/2507.15846)
Append: [Transformers and Ensemble methods: A solution for Hate Speech Detection in Arabic languages](https://arxiv.org/abs/2303.09823)
Append: [Where Do People Tell Stories Online? Story Detection Across Online Communities](https://arxiv.org/abs/2311.09675)
Append: [A Survey of the Evolution of Language Model-Based Dialogue Systems: Data, Task and Models](https://arxiv.org/abs/2311.16789)
Append: [End-to-end Joint Punctuated and Normalized ASR with a Limited Amount of Punctuated Training Data](https://arxiv.org/abs/2311.17741)
Append: [VlogQA: Task, Dataset, and Baseline Models for Vietnamese Spoken-Based Machine Reading Comprehension](https://arxiv.org/abs/2402.02655)
Append: [Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for Large Language Models Aligned with Human Cognitive Principles](https://arxiv.org/abs/2406.12644)
Append: [Towards the Next Frontier in Speech Representation Learning Using Disentanglement](https://arxiv.org/abs/2407.02543)
Append: [Why Does New Knowledge Create Messy Ripple Effects in LLMs?](https://arxiv.org/abs/2407.12828)
Append: [Enhancing Natural Language Inference Performance with Knowledge Graph for COVID-19 Automated Fact-Checking in Indonesian Language](https://arxiv.org/abs/2409.00061)
Append: [DARE: Diverse Visual Question Answering with Robustness Evaluation](https://arxiv.org/abs/2409.18023)
Append: [CSSL: Contrastive Self-Supervised Learning for Dependency Parsing on Relatively Free Word Ordered and Morphologically Rich Low Resource Languages](https://arxiv.org/abs/2410.06944)
Append: [CCSBench: Evaluating Compositional Controllability in LLMs for Scientific Document Summarization](https://arxiv.org/abs/2410.12601)
Append: [Vulnerability of LLMs to Vertically Aligned Text Manipulations](https://arxiv.org/abs/2410.20016)
Append: [Ev2R: Evaluating Evidence Retrieval in Automated Fact-Checking](https://arxiv.org/abs/2411.05375)
Append: [DRS: Deep Question Reformulation With Structured Output](https://arxiv.org/abs/2411.17993)
Append: [A Survey on Large Language Model-Based Social Agents in Game-Theoretic Scenarios](https://arxiv.org/abs/2412.03920)
Append: [Finding A Voice: Exploring the Potential of African American Dialect and Voice Generation for Chatbots](https://arxiv.org/abs/2501.03441)
Append: [Preventing Rogue Agents Improves Multi-Agent Collaboration](https://arxiv.org/abs/2502.05986)
Append: [Tokenization Standards for Linguistic Integrity: Turkish as a Benchmark](https://arxiv.org/abs/2502.07057)
Append: [Layerwise Recall and the Geometry of Interwoven Knowledge in LLMs](https://arxiv.org/abs/2502.10871)
Append: [FastMCTS: A Simple Sampling Strategy for Data Synthesis](https://arxiv.org/abs/2502.11476)
Append: [Commonsense Reasoning in Arab Culture](https://arxiv.org/abs/2502.12788)
Append: [KazMMLU: Evaluating Language Models on Kazakh, Russian, and Regional Knowledge of Kazakhstan](https://arxiv.org/abs/2502.12829)
Append: [How Far are LLMs from Being Our Digital Twins? A Benchmark for Persona-Based Behavior Chain Simulation](https://arxiv.org/abs/2502.14642)
Append: [MKE-Coder: Multi-Axial Knowledge with Evidence Verification in ICD Coding for Chinese EMRs](https://arxiv.org/abs/2502.14916)
Append: [Analyze the Neurons, not the Embeddings: Understanding When and Where LLM Representations Align with Humans](https://arxiv.org/abs/2502.15090)
Append: [Steering into New Embedding Spaces: Analyzing Cross-Lingual Alignment Induced by Model Interventions in Multilingual Language Models](https://arxiv.org/abs/2502.15639)
Append: [KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse](https://arxiv.org/abs/2502.16002)
Append: [MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for Retrieval Augmented Generation](https://arxiv.org/abs/2502.17163)
Append: [Winning Big with Small Models: Knowledge Distillation vs. Self-Training for Reducing Hallucination in Product QA Agents](https://arxiv.org/abs/2502.19545)
Append: [Do Emotions Really Affect Argument Convincingness? A Dynamic Approach with LLM-based Manipulation Checks](https://arxiv.org/abs/2503.00024)
Append: [Cats Confuse Reasoning LLM: Query Agnostic Adversarial Triggers for Reasoning Models](https://arxiv.org/abs/2503.01781)
Append: [Symbolic Mixture-of-Experts: Adaptive Skill-based Routing for Heterogeneous Reasoning](https://arxiv.org/abs/2503.05641)
Append: [Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning](https://arxiv.org/abs/2503.09516)
Append: [BriLLM: Brain-inspired Large Language Model](https://arxiv.org/abs/2503.11299)
Append: [OpeNLGauge: An Explainable Metric for NLG Evaluation with Open-Weights LLMs](https://arxiv.org/abs/2503.11858)
Append: [Entity-aware Cross-lingual Claim Detection for Automated Fact-checking](https://arxiv.org/abs/2503.15220)
Append: [SWI: Speaking with Intent in Large Language Models](https://arxiv.org/abs/2503.21544)
Append: [Texture or Semantics? Vision-Language Models Get Lost in Font Recognition](https://arxiv.org/abs/2503.23768)
Append: [Detecting PTSD in Clinical Interviews: A Comparative Analysis of NLP Methods and Large Language Models](https://arxiv.org/abs/2504.01216)
Append: [The Dual-Route Model of Induction](https://arxiv.org/abs/2504.03022)
Append: [APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay](https://arxiv.org/abs/2504.03601)
Append: [Executable Functional Abstractions: Inferring Generative Programs for Advanced Math Problems](https://arxiv.org/abs/2504.09763)
Append: [clem:todd: A Framework for the Systematic Benchmarking of LLM-Based Task-Oriented Dialogue System Realisations](https://arxiv.org/abs/2505.05445)
Append: [Towards Harmonized Uncertainty Estimation for Large Language Models](https://arxiv.org/abs/2505.19073)
Append: [Controlling Language Confusion in Multilingual LLMs](https://arxiv.org/abs/2505.19116)
Append: [On Entity Identification in Language Models](https://arxiv.org/abs/2506.02701)
Append: [Label-semantics Aware Generative Approach for Domain-Agnostic Multilabel Classification](https://arxiv.org/abs/2506.06806)
Append: [Draft-based Approximate Inference for LLMs](https://arxiv.org/abs/2506.08373)
Append: [Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing Multi-Turn Planning and Tool Adaptation](https://arxiv.org/abs/2506.11092)
Append: [Plan for Speed: Dilated Scheduling for Masked Diffusion Language Models](https://arxiv.org/abs/2506.19037)
Append: [KaLM-Embedding-V2: Superior Training Techniques and Data Inspire A Versatile Embedding Model](https://arxiv.org/abs/2506.20923)
Append: [Enabling Efficient Attack Investigation via Human-in-the-Loop Security Analysis](https://arxiv.org/abs/2211.05403)
Append: [Attention with Markov: A Framework for Principled Analysis of Transformers via Markov Chains](https://arxiv.org/abs/2402.04161)
Append: [STUN: Structured-Then-Unstructured Pruning for Scalable MoE Pruning](https://arxiv.org/abs/2409.06211)
Append: [Sortformer: A Novel Approach for Permutation-Resolved Speaker Supervision in Speech-to-Text Systems](https://arxiv.org/abs/2409.06656)
Append: [AlphaDPO: Adaptive Reward Margin for Direct Preference Optimization](https://arxiv.org/abs/2410.10148)
Append: [ChronoSense: Exploring Temporal Understanding in Large Language Models with Time Intervals of Events](https://arxiv.org/abs/2501.03040)
Append: [OMoE: Diversifying Mixture of Low-Rank Adaptation by Orthogonal Finetuning](https://arxiv.org/abs/2501.10062)
Append: [Doing More with Less: A Survey on Routing Strategies for Resource Optimisation in Large Language Model-Based Systems](https://arxiv.org/abs/2502.00409)
Append: [Empowering LLMs with Logical Reasoning: A Comprehensive Survey](https://arxiv.org/abs/2502.15652)
Append: [Combinatorial Optimization for All: Using LLMs to Aid Non-Experts in Improving Optimization Algorithms](https://arxiv.org/abs/2503.10968)
Append: [A Semantic-based Optimization Approach for Repairing LLMs: Case Study on Code Generation](https://arxiv.org/abs/2503.12899)
Append: [Growing a Twig to Accelerate Large Vision-Language Models](https://arxiv.org/abs/2503.14075)
Append: [Only a Little to the Left: A Theory-grounded Measure of Political Bias in Large Language Models](https://arxiv.org/abs/2503.16148)
Append: [Visually Guided Decoding: Gradient-Free Hard Prompt Inversion with Language Models](https://arxiv.org/abs/2505.08622)
Append: [TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios](https://arxiv.org/abs/2505.12891)
Append: [AutoGen Driven Multi Agent Framework for Iterative Crime Data Analysis and Prediction](https://arxiv.org/abs/2506.11475)
Append: [DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning with Video LLMs](https://arxiv.org/abs/2506.11558)
Append: [Reviving Cultural Heritage: A Novel Approach for Comprehensive Historical Document Restoration](https://arxiv.org/abs/2507.05108)
Append: [Supporting SENCOTEN Language Documentation Efforts with Automatic Speech Recognition](https://arxiv.org/abs/2507.10827)
append_entries: 188
Finish: 2025-07-22 04:43:31.005710
------------------------------------------------------
Started: 2025-07-22 06:27:44.631823
Existing_entries: 1188
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-22 06:27:45.034063
------------------------------------------------------
Started: 2025-07-22 08:25:47.838188
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-22 08:25:48.239573
------------------------------------------------------
Started: 2025-07-22 10:19:37.366648
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-22 10:19:37.782194
------------------------------------------------------
Started: 2025-07-22 12:37:28.370218
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-22 12:37:28.834271
------------------------------------------------------
Started: 2025-07-22 14:18:46.834855
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-22 14:18:47.283964
------------------------------------------------------
Started: 2025-07-22 16:22:45.164849
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-22 16:22:45.591329
------------------------------------------------------
Started: 2025-07-22 18:26:29.320476
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-22 18:26:29.730690
------------------------------------------------------
Started: 2025-07-22 20:20:25.425772
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-22 20:20:25.897894
------------------------------------------------------
Started: 2025-07-22 22:17:01.945094
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-22 22:17:02.381690
------------------------------------------------------
Started: 2025-07-23 01:27:01.514299
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-23 01:27:01.964243
------------------------------------------------------
Started: 2025-07-23 03:28:39.254592
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-23 03:28:39.699539
------------------------------------------------------
Started: 2025-07-23 04:43:01.669275
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1359
Summarized using GPT-3.5-turbo
Append: [eSapiens's DEREK Module: Deep Extraction & Reasoning Engine for Knowledge with LLMs](https://arxiv.org/abs/2507.15863)
Token length: 1074
Summarized using GPT-3.5-turbo
Append: [Adversarial Demonstration Learning for Low-resource NER Using Dual Similarity](https://arxiv.org/abs/2507.15864)
Token length: 1586
Summarized using GPT-3.5-turbo
Append: [Small Edits, Big Consequences: Telling Good from Bad Robustness in Large Language Models](https://arxiv.org/abs/2507.15868)
Token length: 1626
Summarized using GPT-3.5-turbo
Append: [Enhancing Hindi NER in Low Context: A Comparative study of Transformer-based models with vs. without Retrieval Augmentation](https://arxiv.org/abs/2507.16002)
Token length: 943
Summarized using GPT-3.5-turbo
Append: [Learning without training: The implicit dynamics of in-context learning](https://arxiv.org/abs/2507.16003)
Token length: 852
Summarized using GPT-3.5-turbo
Append: [Help Me Write a Story: Evaluating LLMs' Ability to Generate Writing Feedback](https://arxiv.org/abs/2507.16007)
Token length: 1088
Summarized using GPT-3.5-turbo
Append: [mRAKL: Multilingual Retrieval-Augmented Knowledge Graph Construction for Low-Resourced Languages](https://arxiv.org/abs/2507.16011)
Token length: 1615
Summarized using GPT-3.5-turbo
Append: [AutoMeet: a proof-of-concept study of genAI to automate meetings in automotive engineering](https://arxiv.org/abs/2507.16054)
Token length: 1426
Summarized using GPT-3.5-turbo
Append: [Deep Researcher with Test-Time Diffusion](https://arxiv.org/abs/2507.16075)
Token length: 1178
Summarized using GPT-3.5-turbo
Append: [The Prompt Makes the Person(a): A Systematic Evaluation of Sociodemographic Persona Prompting for Large Language Models](https://arxiv.org/abs/2507.16076)
Token length: 1367
Summarized using GPT-3.5-turbo
Append: [Efficient Compositional Multi-tasking for On-device Large Language Models](https://arxiv.org/abs/2507.16083)
Token length: 1370
Summarized using GPT-3.5-turbo
Append: [BIDWESH: A Bangla Regional Based Hate Speech Detection Dataset](https://arxiv.org/abs/2507.16183)
Token length: 1241
Summarized using GPT-3.5-turbo
Append: [Do Large Language Models Have a Planning Theory of Mind? Evidence from MindGames: a Multi-Step Persuasion Task](https://arxiv.org/abs/2507.16196)
Token length: 1153
Summarized using GPT-3.5-turbo
Append: [WakenLLM: A Fine-Grained Benchmark for Evaluating LLM Reasoning Potential and Reasoning Process Stability](https://arxiv.org/abs/2507.16199)
Token length: 1552
Summarized using GPT-3.5-turbo
Append: [Towards Compute-Optimal Many-Shot In-Context Learning](https://arxiv.org/abs/2507.16217)
Token length: 1281
Summarized using GPT-3.5-turbo
Append: [FinResearchBench: A Logic Tree based Agent-as-a-Judge Evaluation Framework for Financial Research Agents](https://arxiv.org/abs/2507.16248)
Token length: 1048
Summarized using GPT-3.5-turbo
Append: [Efficient RL for optimizing conversation level outcomes with an LLM-based tutor](https://arxiv.org/abs/2507.16252)
Token length: 885
Summarized using GPT-3.5-turbo
Append: [iShumei-Chinchunmei at SemEval-2025 Task 4: A balanced forgetting and retention multi-task framework using effective unlearning loss](https://arxiv.org/abs/2507.16263)
Token length: 1145
Summarized using GPT-3.5-turbo
Append: [Beyond Isolated Dots: Benchmarking Structured Table Construction as Deep Knowledge Extraction](https://arxiv.org/abs/2507.16271)
Token length: 943
Summarized using GPT-3.5-turbo
Append: [Language Detection by Means of the Minkowski Norm: Identification Through Character Bigrams and Frequency Analysis](https://arxiv.org/abs/2507.16284)
Token length: 1071
Summarized using GPT-3.5-turbo
Append: [SpeLLM: Character-Level Multi-Head Decoding](https://arxiv.org/abs/2507.16323)
Token length: 1873
Summarized using GPT-3.5-turbo
Append: [Re:Form -- Reducing Human Priors in Scalable Formal Software Verification with RL in LLMs: A Preliminary Study on Dafny](https://arxiv.org/abs/2507.16331)
Token length: 1476
Summarized using GPT-3.5-turbo
Append: [GG-BBQ: German Gender Bias Benchmark for Question Answering](https://arxiv.org/abs/2507.16410)
Token length: 1779
Summarized using GPT-3.5-turbo
Append: [PromptAL: Sample-Aware Dynamic Soft Prompts for Few-Shot Active Learning](https://arxiv.org/abs/2507.16424)
Token length: 1699
Summarized using GPT-3.5-turbo
Append: [Dutch CrowS-Pairs: Adapting a Challenge Dataset for Measuring Social Biases in Language Models for Dutch](https://arxiv.org/abs/2507.16442)
Token length: 824
Summarized using GPT-3.5-turbo
Append: [Towards Enforcing Company Policy Adherence in Agentic Workflows](https://arxiv.org/abs/2507.16459)
Token length: 1163
Summarized using GPT-3.5-turbo
Append: [ICR Probe: Tracking Hidden State Dynamics for Reliable Hallucination Detection in LLMs](https://arxiv.org/abs/2507.16488)
Token length: 1910
Summarized using GPT-3.5-turbo
Append: [Combining Language and Topic Models for Hierarchical Text Classification](https://arxiv.org/abs/2507.16490)
Token length: 1470
Summarized using GPT-3.5-turbo
Append: [The Ever-Evolving Science Exam](https://arxiv.org/abs/2507.16514)
Token length: 1166
Summarized using GPT-3.5-turbo
Append: [Introducing Quality Estimation to Machine Translation Post-editing Workflow: An Empirical Study on Its Usefulness](https://arxiv.org/abs/2507.16515)
Token length: 718
Summarized using GPT-3.5-turbo
Append: [Learning Text Styles: A Study on Transfer, Attribution, and Verification](https://arxiv.org/abs/2507.16530)
Token length: 956
Summarized using GPT-3.5-turbo
Append: [Exploring Gender Bias in Large Language Models: An In-depth Dive into the German Language](https://arxiv.org/abs/2507.16557)
Token length: 1265
Summarized using GPT-3.5-turbo
Append: [Pixels to Principles: Probing Intuitive Physics Understanding in Multimodal Language Models](https://arxiv.org/abs/2507.16572)
Token length: 1348
Summarized using GPT-3.5-turbo
Append: [Step-Audio 2 Technical Report](https://arxiv.org/abs/2507.16632)
Token length: 1296
Summarized using GPT-3.5-turbo
Append: [Towards Automated Regulatory Compliance Verification in Financial Auditing with Large Language Models](https://arxiv.org/abs/2507.16642)
Token length: 895
Summarized using GPT-3.5-turbo
Append: [P-CoT: A Pedagogically-motivated Participatory Chain-of-Thought Prompting for Phonological Reasoning in LLMs](https://arxiv.org/abs/2507.16656)
Token length: 1957
Summarized using GPT-3.5-turbo
Append: [Self-Contradiction as Self-Improvement: Mitigating the Generation-Understanding Gap in MLLMs](https://arxiv.org/abs/2507.16663)
Token length: 1377
Summarized using GPT-3.5-turbo
Append: [PICACO: Pluralistic In-Context Value Alignment of LLMs via Total Correlation Optimization](https://arxiv.org/abs/2507.16679)
Token length: 561
Summarized using GPT-3.5-turbo
Append: [Interpretable Topic Extraction and Word Embedding Learning using row-stochastic DEDICOM](https://arxiv.org/abs/2507.16695)
Token length: 861
Summarized using GPT-3.5-turbo
Append: [Advancing Risk and Quality Assurance: A RAG Chatbot for Improved Regulatory Compliance](https://arxiv.org/abs/2507.16711)
Token length: 1422
Summarized using GPT-3.5-turbo
Append: [RAVine: Reality-Aligned Evaluation for Agentic Search](https://arxiv.org/abs/2507.16725)
Token length: 1032
Summarized using GPT-3.5-turbo
Append: [Unpacking Ambiguity: The Interaction of Polysemous Discourse Markers and Non-DM Signals](https://arxiv.org/abs/2507.16748)
Token length: 1455
Summarized using GPT-3.5-turbo
Append: [Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning](https://arxiv.org/abs/2507.16784)
Token length: 1383
Summarized using GPT-3.5-turbo
Append: [Test-Time-Matching: Decouple Personality, Memory, and Linguistic Style in LLM-based Role-Playing Language Agent](https://arxiv.org/abs/2507.16799)
Token length: 1732
Summarized using GPT-3.5-turbo
Append: [Agentar-Fin-R1: Enhancing Financial Intelligence through Domain Expertise, Training Efficiency, and Advanced Reasoning](https://arxiv.org/abs/2507.16802)
Token length: 1049
Summarized using GPT-3.5-turbo
Append: [LingBench++: A Linguistically-Informed Benchmark and Reasoning Framework for Multi-Step and Cross-Cultural Inference with LLMs](https://arxiv.org/abs/2507.16809)
Token length: 1849
Summarized using GPT-3.5-turbo
Append: [MegaScience: Pushing the Frontiers of Post-Training Datasets for Science Reasoning](https://arxiv.org/abs/2507.16812)
Token length: 1141
Summarized using GPT-3.5-turbo
Append: [RDMA: Cost Effective Agent-Driven Rare Disease Discovery within Electronic Health Record Systems](https://arxiv.org/abs/2507.15867)
Token length: 1381
Summarized using GPT-3.5-turbo
Append: [Why Braking? Scenario Extraction and Reasoning Utilizing LLM](https://arxiv.org/abs/2507.15874)
Token length: 1028
Summarized using GPT-3.5-turbo
Append: [Document Haystack: A Long Context Multimodal Image/Document Understanding Vision LLM Benchmark](https://arxiv.org/abs/2507.15882)
Append: [AlgoTune: Can Language Models Speed Up General-Purpose Numerical Programs?](https://arxiv.org/abs/2507.15887)
Append: [SpiroLLM: Finetuning Pretrained LLMs to Understand Spirogram Time Series with Clinical Validation in COPD Reporting](https://arxiv.org/abs/2507.16145)
Append: [Characterizing Online Activities Contributing to Suicide Mortality among Youth](https://arxiv.org/abs/2507.16185)
Append: [WhatsApp Tiplines and Multilingual Claims in the 2021 Indian Assembly Elections](https://arxiv.org/abs/2507.16298)
Append: [MMS Player: an open source software for parametric data-driven animation of Sign Language avatars](https://arxiv.org/abs/2507.16463)
Append: [C2-Evo: Co-Evolving Multimodal Data and Model for Self-Improving Reasoning](https://arxiv.org/abs/2507.16518)
Append: [Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report](https://arxiv.org/abs/2507.16534)
Append: [Scaling Linear Attention with Sparse State Expansion](https://arxiv.org/abs/2507.16577)
Append: [Experience is the Best Teacher: Grounding VLMs for Robotics through Self-Generated Memory](https://arxiv.org/abs/2507.16713)
Append: [Zebra-CoT: A Dataset for Interleaved Vision Language Reasoning](https://arxiv.org/abs/2507.16746)
Append: [Steering Out-of-Distribution Generalization with Concept Ablation Fine-Tuning](https://arxiv.org/abs/2507.16795)
Append: [Beyond Binary Rewards: Training LMs to Reason About Their Uncertainty](https://arxiv.org/abs/2507.16806)
Append: [Modeling the Sacred: Considerations when Using Religious Texts in Natural Language Processing](https://arxiv.org/abs/2404.14740)
Append: [Erasing Conceptual Knowledge from Language Models](https://arxiv.org/abs/2410.02760)
Append: [Data Processing for the OpenGPT-X Model Family](https://arxiv.org/abs/2410.08800)
Append: [Atomic Calibration of LLMs in Long-Form Generations](https://arxiv.org/abs/2410.13246)
Append: [Beyond English: Evaluating Automated Measurement of Moral Foundations in Non-English Discourse with a Chinese Case Study](https://arxiv.org/abs/2502.02451)
Append: [Universal Model Routing for Efficient LLM Inference](https://arxiv.org/abs/2502.08773)
Append: [Reasoning Does Not Necessarily Improve Role-Playing Ability](https://arxiv.org/abs/2502.16940)
Append: [LLMs syntactically adapt their language use to their conversational partner](https://arxiv.org/abs/2503.07457)
Append: [SciFi-Benchmark: Leveraging Science Fiction To Improve Robot Behavior](https://arxiv.org/abs/2503.10706)
Append: [Synthetic Data Generation Using Large Language Models: Advances in Text and Code](https://arxiv.org/abs/2503.14023)
Append: [Typed-RAG: Type-Aware Decomposition of Non-Factoid Questions for Retrieval-Augmented Generation](https://arxiv.org/abs/2503.15879)
Append: [Evaluating Intermediate Reasoning of Code-Assisted Large Language Models for Mathematics](https://arxiv.org/abs/2504.17665)
Append: [A Method for the Architecture of a Medical Vertical Large Language Model Based on Deepseek R1](https://arxiv.org/abs/2505.00025)
Append: [Generative Sign-description Prompts with Multi-positive Contrastive Learning for Sign Language Recognition](https://arxiv.org/abs/2505.02304)
Append: [HausaNLP: Current Status, Challenges and Future Directions for Hausa Natural Language Processing](https://arxiv.org/abs/2505.14311)
Append: [Hierarchical Safety Realignment: Lightweight Restoration of Safety in Pruned Large Vision-Language Models](https://arxiv.org/abs/2505.16104)
Append: [Multimodal Forecasting of Sparse Intraoperative Hypotension Events Powered by Language Model](https://arxiv.org/abs/2505.22116)
Append: [Self-Correcting Code Generation Using Small Language Models](https://arxiv.org/abs/2505.23060)
Append: [SenWiCh: Sense-Annotation of Low-Resource Languages for WiC using Hybrid Methods](https://arxiv.org/abs/2505.23714)
Append: [Speech as a Multimodal Digital Phenotype for Multi-Task LLM-based Mental Health Prediction](https://arxiv.org/abs/2505.23822)
Append: [Adaptive Graph Pruning for Multi-Agent Communication](https://arxiv.org/abs/2506.02951)
Append: [Can LLMs Generate Reliable Test Case Generators? A Study on Competition-Level Programming Problems](https://arxiv.org/abs/2506.06821)
Append: [Continuously Updating Digital Twins using Large Language Models](https://arxiv.org/abs/2506.12091)
Append: [Omni-Router: Sharing Routing Decisions in Sparse Mixture-of-Experts for Speech Recognition](https://arxiv.org/abs/2507.05724)
Append: [Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving](https://arxiv.org/abs/2507.06229)
Append: [Risks of AI Scientists: Prioritizing Safeguarding Over Autonomy](https://arxiv.org/abs/2402.04247)
Append: [Alto: Orchestrating Distributed Compound AI Systems with Nested Ancestry](https://arxiv.org/abs/2403.04311)
Append: [Exploring How Generative MLLMs Perceive More Than CLIP with the Same Vision Encoder](https://arxiv.org/abs/2411.05195)
Append: [Sparrow: Data-Efficient Video-LLM with Text-to-Image Augmentation](https://arxiv.org/abs/2411.19951)
Append: [R-Bot: An LLM-based Query Rewrite System](https://arxiv.org/abs/2412.01661)
Append: [InternAgent: When Agent Becomes the Scientist -- Building Closed-Loop System from Hypothesis to Verification](https://arxiv.org/abs/2505.16938)
Append: [DCG-SQL: Enhancing In-Context Learning for Text-to-SQL with Deep Contextual Schema Link Graph](https://arxiv.org/abs/2505.19956)
Append: [Human Empathy as Encoder: AI-Assisted Depression Assessment in Special Education](https://arxiv.org/abs/2505.23631)
append_entries: 95
Finish: 2025-07-23 04:45:02.794796
------------------------------------------------------
Started: 2025-07-23 06:27:58.735688
Existing_entries: 1095
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1054
Summarized using GPT-3.5-turbo
Append: [Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities](https://arxiv.org/abs/2507.06261)
append_entries: 1
Finish: 2025-07-23 06:28:01.541491
------------------------------------------------------
Started: 2025-07-23 08:25:17.048218
Existing_entries: 1001
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-23 08:25:17.312805
------------------------------------------------------
Started: 2025-07-23 10:19:30.500972
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-23 10:19:30.768463
------------------------------------------------------
Started: 2025-07-23 12:37:58.088486
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-23 12:37:58.380450
------------------------------------------------------
Started: 2025-07-23 14:19:16.884336
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-23 14:19:17.218242
------------------------------------------------------
Started: 2025-07-23 16:23:06.672538
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-23 16:23:06.935931
------------------------------------------------------
Started: 2025-07-23 18:26:25.296114
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-23 18:26:25.647789
------------------------------------------------------
Started: 2025-07-23 20:19:48.150208
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-23 20:19:48.511234
------------------------------------------------------
Started: 2025-07-23 22:17:45.965140
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-23 22:17:46.283037
------------------------------------------------------
Started: 2025-07-24 01:26:31.618048
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-24 01:26:31.891106
------------------------------------------------------
Started: 2025-07-24 03:27:00.643560
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-24 03:27:00.915836
------------------------------------------------------
Started: 2025-07-24 04:41:28.876062
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1263
Summarized using GPT-3.5-turbo
Append: [A Unifying Scheme for Extractive Content Selection Tasks](https://arxiv.org/abs/2507.16922)
Token length: 1518
Summarized using GPT-3.5-turbo
Append: [AI-based Clinical Decision Support for Primary Care: A Real-World Study](https://arxiv.org/abs/2507.16947)
Token length: 1525
Summarized using GPT-3.5-turbo
Append: [Harnessing RLHF for Robust Unanswerability Recognition and Trustworthy Response Generation in LLMs](https://arxiv.org/abs/2507.16951)
Token length: 1219
Summarized using GPT-3.5-turbo
Append: [Text-to-SPARQL Goes Beyond English: Multilingual Question Answering Over Knowledge Graphs through Human-Inspired Reasoning](https://arxiv.org/abs/2507.16971)
Token length: 1418
Summarized using GPT-3.5-turbo
Append: [Leveraging Synthetic Data for Question Answering with Multilingual LLMs in the Agricultural Domain](https://arxiv.org/abs/2507.16974)
Token length: 1588
Summarized using GPT-3.5-turbo
Append: [Obscured but Not Erased: Evaluating Nationality Bias in LLMs via Name-Based Bias Benchmarks](https://arxiv.org/abs/2507.16989)
Token length: 1625
Summarized using GPT-3.5-turbo
Append: [Multi-Label Classification with Generative AI Models in Healthcare: A Case Study of Suicidality and Risk Factors](https://arxiv.org/abs/2507.17009)
Token length: 1773
Summarized using GPT-3.5-turbo
Append: [Can External Validation Tools Improve Annotation Quality for LLM-as-a-Judge?](https://arxiv.org/abs/2507.17015)
Token length: 1562
Summarized using GPT-3.5-turbo
Append: [Evolutionary Feature-wise Thresholding for Binary Representation of NLP Embeddings](https://arxiv.org/abs/2507.17025)
Token length: 1087
Summarized using GPT-3.5-turbo
Append: [CogDual: Enhancing Dual Cognition of LLMs via Reinforcement Learning with Implicit Rule-Based Rewards](https://arxiv.org/abs/2507.17147)
Token length: 1441
Summarized using GPT-3.5-turbo
Append: [SKA-Bench: A Fine-Grained Benchmark for Evaluating Structured Knowledge Understanding of LLMs](https://arxiv.org/abs/2507.17178)
Token length: 1482
Summarized using GPT-3.5-turbo
Append: [FinGAIA: An End-to-End Benchmark for Evaluating AI Agents in Finance](https://arxiv.org/abs/2507.17186)
Token length: 1333
Summarized using GPT-3.5-turbo
Append: [The Pluralistic Moral Gap: Understanding Judgment and Value Differences between Humans and Large Language Models](https://arxiv.org/abs/2507.17216)
Token length: 1487
Summarized using GPT-3.5-turbo
Append: [CLARIFID: Improving Radiology Report Generation by Reinforcing Clinically Accurate Impressions and Enforcing Detailed Findings](https://arxiv.org/abs/2507.17234)
Token length: 866
Summarized using GPT-3.5-turbo
Append: [Triple X: A LLM-Based Multilingual Speech Recognition System for the INTERSPEECH2025 MLC-SLM Challenge](https://arxiv.org/abs/2507.17288)
Token length: 689
Summarized using GPT-3.5-turbo
Append: [Millions of $\text{GeAR}$-s: Extending GraphRAG to Millions of Documents](https://arxiv.org/abs/2507.17399)
Token length: 1316
Summarized using GPT-3.5-turbo
Append: [Investigating Subjective Factors of Argument Strength: Storytelling, Emotions, and Hedging](https://arxiv.org/abs/2507.17409)
Token length: 1526
Summarized using GPT-3.5-turbo
Append: [Each to Their Own: Exploring the Optimal Embedding in RAG](https://arxiv.org/abs/2507.17442)
Token length: 1871
Summarized using GPT-3.5-turbo
Append: [MultiNRC: A Challenging and Native Multilingual Reasoning Evaluation Benchmark for LLMs](https://arxiv.org/abs/2507.17476)
Token length: 1349
Summarized using GPT-3.5-turbo
Append: [Seed LiveInterpret 2.0: End-to-end Simultaneous Speech-to-speech Translation with Your Voice](https://arxiv.org/abs/2507.17527)
Token length: 1360
Summarized using GPT-3.5-turbo
Append: [Synthetic Voice Data for Automatic Speech Recognition in African Languages](https://arxiv.org/abs/2507.17578)
Token length: 1332
Summarized using GPT-3.5-turbo
Append: [A Hybrid Early-Exit Algorithm for Large Language Models Based on Space Alignment Decoding (SPADE)](https://arxiv.org/abs/2507.17618)
Token length: 1326
Summarized using GPT-3.5-turbo
Append: [WSM: Decay-Free Learning Rate Schedule via Checkpoint Merging for LLM Pre-training](https://arxiv.org/abs/2507.17634)
Token length: 1403
Summarized using GPT-3.5-turbo
Append: [Who Attacks, and Why? Using LLMs to Identify Negative Campaigning in 18M Tweets across 19 Countries](https://arxiv.org/abs/2507.17636)
Token length: 1651
Summarized using GPT-3.5-turbo
Append: [Towards Greater Leverage: Scaling Laws for Efficient Mixture-of-Experts Language Models](https://arxiv.org/abs/2507.17702)
Token length: 874
Summarized using GPT-3.5-turbo
Append: [TyDi QA-WANA: A Benchmark for Information-Seeking Question Answering in Languages of West Asia and North Africa](https://arxiv.org/abs/2507.17709)
Token length: 1214
Summarized using GPT-3.5-turbo
Append: [From Feedback to Checklists: Grounded Evaluation of AI-Generated Clinical Notes](https://arxiv.org/abs/2507.17717)
Token length: 1485
Summarized using GPT-3.5-turbo
Append: [AI Telephone Surveying: Automating Quantitative Data Collection with an AI Interviewer](https://arxiv.org/abs/2507.17718)
Token length: 1244
Summarized using GPT-3.5-turbo
Append: [Megrez2 Technical Report](https://arxiv.org/abs/2507.17728)
Token length: 1658
Summarized using GPT-3.5-turbo
Append: [Pretraining on the Test Set Is No Longer All You Need: A Debate-Driven Approach to QA Benchmarks](https://arxiv.org/abs/2507.17747)
Token length: 1868
Summarized using GPT-3.5-turbo
Append: [Disaster Informatics after the COVID-19 Pandemic: Bibliometric and Topic Analysis based on Large-scale Academic Literature](https://arxiv.org/abs/2507.16820)
Token length: 1868
Summarized using GPT-3.5-turbo
Append: [A Query-Aware Multi-Path Knowledge Graph Fusion Approach for Enhancing Retrieval-Augmented Generation in Large Language Models](https://arxiv.org/abs/2507.16826)
Token length: 744
Summarized using GPT-3.5-turbo
Append: [Towards Robust Speech Recognition for Jamaican Patois Music Transcription](https://arxiv.org/abs/2507.16834)
Token length: 1205
Summarized using GPT-3.5-turbo
Append: [Evaluating Speech-to-Text x LLM x Text-to-Speech Combinations for AI Interview Systems](https://arxiv.org/abs/2507.16835)
Token length: 1535
Summarized using GPT-3.5-turbo
Append: [Segmentation-free Goodness of Pronunciation](https://arxiv.org/abs/2507.16838)
Token length: 1413
Summarized using GPT-3.5-turbo
Append: [Pixels, Patterns, but No Poetry: To See The World like Humans](https://arxiv.org/abs/2507.16863)
Token length: 1592
Summarized using GPT-3.5-turbo
Append: [ReMeREC: Relation-aware and Multi-entity Referring Expression Comprehension](https://arxiv.org/abs/2507.16877)
Token length: 951
Summarized using GPT-3.5-turbo
Append: [SiLQ: Simple Large Language Model Quantization-Aware Training](https://arxiv.org/abs/2507.16933)
Token length: 1233
Summarized using GPT-3.5-turbo
Append: [A Highly Clean Recipe Dataset with Ingredient States Annotation for State Probing Task](https://arxiv.org/abs/2507.17232)
Token length: 1617
Summarized using GPT-3.5-turbo
Append: [Tab-MIA: A Benchmark Dataset for Membership Inference Attacks on Tabular Data in LLMs](https://arxiv.org/abs/2507.17259)
Token length: 1563
Summarized using GPT-3.5-turbo
Append: [TransLPRNet: Lite Vision-Language Network for Single/Dual-line Chinese License Plate Recognition](https://arxiv.org/abs/2507.17335)
Token length: 1207
Summarized using GPT-3.5-turbo
Append: [DNT: a Deeply Normalized Transformer that can be trained by Momentum SGD](https://arxiv.org/abs/2507.17501)
Token length: 1654
Summarized using GPT-3.5-turbo
Append: [URPO: A Unified Reward & Policy Optimization Framework for Large Language Models](https://arxiv.org/abs/2507.17515)
Token length: 1551
Summarized using GPT-3.5-turbo
Append: [BoSS: Beyond-Semantic Speech](https://arxiv.org/abs/2507.17563)
Token length: 1266
Summarized using GPT-3.5-turbo
Append: [Dual-branch Prompting for Multimodal Machine Translation](https://arxiv.org/abs/2507.17588)
Token length: 1122
Summarized using GPT-3.5-turbo
Append: [Rubrics as Rewards: Reinforcement Learning Beyond Verifiable Domains](https://arxiv.org/abs/2507.17746)
Token length: 1077
Summarized using GPT-3.5-turbo
Append: [Multi-Level Explanations for Generative Language Models](https://arxiv.org/abs/2403.14459)
Token length: 1453
Summarized using GPT-3.5-turbo
Append: [Impact of Stickers on Multimodal Sentiment and Intent in Social Media: A New Task, Dataset and Baseline](https://arxiv.org/abs/2405.08427)
Token length: 961
Summarized using GPT-3.5-turbo
Append: [Is text normalization relevant for classifying medieval charters?](https://arxiv.org/abs/2408.16446)
Token length: 1057
Summarized using GPT-3.5-turbo
Append: [SHARE: Shared Memory-Aware Open-Domain Long-Term Dialogue Dataset Constructed from Movie Script](https://arxiv.org/abs/2410.20682)
Append: [A Survey of Event Causality Identification: Taxonomy, Challenges, Assessment, and Prospects](https://arxiv.org/abs/2411.10371)
Append: [Beyond Profile: From Surface-Level Facts to Deep Persona Simulation in LLMs](https://arxiv.org/abs/2502.12988)
Append: [Modality-Aware Neuron Pruning for Unlearning in Multimodal Large Language Models](https://arxiv.org/abs/2502.15910)
Append: [An Efficient and Precise Training Data Construction Framework for Process-supervised Reward Model in Mathematical Reasoning](https://arxiv.org/abs/2503.02382)
Append: [AlignDistil: Token-Level Language Model Alignment as Adaptive Policy Distillation](https://arxiv.org/abs/2503.02832)
Append: [Visualising Policy-Reward Interplay to Inform Zeroth-Order Preference Optimisation of Large Language Models](https://arxiv.org/abs/2503.03460)
Append: [ORANSight-2.0: Foundational LLMs for O-RAN](https://arxiv.org/abs/2503.05200)
Append: [Towards Detecting Persuasion on Social Media: From Model Development to Insights on Persuasion Strategies](https://arxiv.org/abs/2503.13844)
Append: [Advancing Multimodal Reasoning via Reinforcement Learning with Cold Start](https://arxiv.org/abs/2505.22334)
Append: [MEF: A Capability-Aware Multi-Encryption Framework for Evaluating Vulnerabilities in Black-Box Large Language Models](https://arxiv.org/abs/2505.23404)
Append: [Lost in Variation? Evaluating NLI Performance in Basque and Spanish Geographical Variants](https://arxiv.org/abs/2506.15239)
Append: [Large Language Models in Argument Mining: A Survey](https://arxiv.org/abs/2506.16383)
Append: [Modeling Public Perceptions of Science in Media](https://arxiv.org/abs/2506.16622)
Append: [GTA: Grouped-head latenT Attention](https://arxiv.org/abs/2506.17286)
Append: [A Diagrammatic Calculus for a Functional Model of Natural Language Semantics](https://arxiv.org/abs/2507.00782)
Append: [Cautious Next Token Prediction](https://arxiv.org/abs/2507.03038)
Append: [Fairness Evaluation of Large Language Models in Academic Library Reference Services](https://arxiv.org/abs/2507.04224)
Append: [A Mathematical Theory of Discursive Networks](https://arxiv.org/abs/2507.06565)
Append: [Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step](https://arxiv.org/abs/2501.13926)
Append: [From Hypothesis to Publication: A Comprehensive Survey of AI-Driven Research Support Systems](https://arxiv.org/abs/2503.01424)
Append: [OpenVLThinker: Complex Vision-Language Reasoning via Iterative SFT-RL Cycles](https://arxiv.org/abs/2503.17352)
Append: [Pseudo-Autoregressive Neural Codec Language Models for Efficient Zero-Shot Text-to-Speech Synthesis](https://arxiv.org/abs/2504.10352)
Append: [Deep Video Discovery: Agentic Search with Tool Use for Long-form Video Understanding](https://arxiv.org/abs/2505.18079)
Append: [LoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning](https://arxiv.org/abs/2506.15606)
Append: [LEGO Co-builder: Exploring Fine-Grained Vision-Language Modeling for Multimodal LEGO Assembly Assistants](https://arxiv.org/abs/2507.05515)
append_entries: 75
Finish: 2025-07-24 04:43:29.019232
------------------------------------------------------
Started: 2025-07-24 06:27:34.607895
Existing_entries: 1075
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-24 06:27:34.879249
------------------------------------------------------
Started: 2025-07-24 08:25:04.137090
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-24 08:25:04.378297
------------------------------------------------------
Started: 2025-07-24 10:19:54.351368
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-24 10:19:54.577326
------------------------------------------------------
Started: 2025-07-24 12:38:02.633129
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-24 12:38:02.844296
------------------------------------------------------
Started: 2025-07-24 14:19:22.919822
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-24 14:19:23.136496
------------------------------------------------------
Started: 2025-07-24 16:20:11.026915
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-24 16:20:11.265872
------------------------------------------------------
Started: 2025-07-24 18:26:06.681708
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-24 18:26:06.952644
------------------------------------------------------
Started: 2025-07-24 20:18:42.925378
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-24 20:18:43.139731
------------------------------------------------------
Started: 2025-07-24 22:17:44.101518
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-24 22:17:44.369239
------------------------------------------------------
Started: 2025-07-25 01:26:06.348013
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-25 01:26:06.625913
------------------------------------------------------
Started: 2025-07-25 03:27:03.226935
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-25 03:27:03.480140
------------------------------------------------------
Started: 2025-07-25 04:39:52.895050
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1527
Summarized using GPT-3.5-turbo
Append: [Shop-R1: Rewarding LLMs to Simulate Human Behavior in Online Shopping via Reinforcement Learning](https://arxiv.org/abs/2507.17842)
Token length: 1306
Summarized using GPT-3.5-turbo
Append: [Dynamic and Generalizable Process Reward Modeling](https://arxiv.org/abs/2507.17849)
Token length: 1783
Summarized using GPT-3.5-turbo
Append: [VeriMinder: Mitigating Analytical Vulnerabilities in NL2SQL](https://arxiv.org/abs/2507.17896)
Token length: 1017
Summarized using GPT-3.5-turbo
Append: [One Whisper to Grade Them All](https://arxiv.org/abs/2507.17918)
Token length: 1881
Summarized using GPT-3.5-turbo
Append: [Evaluating the Performance of AI Text Detectors, Few-Shot and Chain-of-Thought Prompting Using DeepSeek Generated Text](https://arxiv.org/abs/2507.17944)
Token length: 809
Summarized using GPT-3.5-turbo
Append: [Are LLM Belief Updates Consistent with Bayes' Theorem?](https://arxiv.org/abs/2507.17951)
Token length: 1277
Summarized using GPT-3.5-turbo
Append: [Natural Language Processing for Tigrinya: Current State and Future Directions](https://arxiv.org/abs/2507.17974)
Token length: 1752
Summarized using GPT-3.5-turbo
Append: [Technical Report of TeleChat2, TeleChat2.5 and T1](https://arxiv.org/abs/2507.18013)
Token length: 1401
Summarized using GPT-3.5-turbo
Append: [NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural KV Database](https://arxiv.org/abs/2507.18028)
Token length: 1775
Summarized using GPT-3.5-turbo
Append: [GrAInS: Gradient-based Attribution for Inference-Time Steering of LLMs and VLMs](https://arxiv.org/abs/2507.18043)
Token length: 1042
Summarized using GPT-3.5-turbo
Append: [Synthetic Data Generation for Phrase Break Prediction with Large Language Model](https://arxiv.org/abs/2507.18044)
Token length: 971
Summarized using GPT-3.5-turbo
Append: [Privacy-Preserving Synthetic Review Generation with Diverse Writing Styles Using LLMs](https://arxiv.org/abs/2507.18055)
Token length: 1370
Summarized using GPT-3.5-turbo
Append: [TELEVAL: A Dynamic Benchmark Designed for Spoken Language Models in Chinese Interactive Scenarios](https://arxiv.org/abs/2507.18061)
Token length: 1315
Summarized using GPT-3.5-turbo
Append: [Hybrid and Unitary Fine-Tuning of Large Language Models: Methods and Benchmarking under Resource Constraints](https://arxiv.org/abs/2507.18076)
Token length: 975
Summarized using GPT-3.5-turbo
Append: [A New Pair of GloVes](https://arxiv.org/abs/2507.18103)
Token length: 1552
Summarized using GPT-3.5-turbo
Append: [GOAT-SLM: A Spoken Language Model with Paralinguistic and Speaker Characteristic Awareness](https://arxiv.org/abs/2507.18119)
Token length: 1518
Summarized using GPT-3.5-turbo
Append: [MathOPEval: A Fine-grained Evaluation Benchmark for Visual Operations of MLLMs in Mathematical Reasoning](https://arxiv.org/abs/2507.18140)
Token length: 1914
Summarized using GPT-3.5-turbo
Append: [HIVMedQA: Benchmarking large language models for HIV medical decision support](https://arxiv.org/abs/2507.18143)
Token length: 1440
Summarized using GPT-3.5-turbo
Append: [Sticking to the Mean: Detecting Sticky Tokens in Text Embedding Models](https://arxiv.org/abs/2507.18171)
Token length: 1170
Summarized using GPT-3.5-turbo
Append: [SCOPE: Stochastic and Counterbiased Option Placement for Evaluating Large Language Models](https://arxiv.org/abs/2507.18182)
Token length: 293
Summarized using GPT-3.5-turbo
Append: [TN-AutoRCA: Benchmark Construction and Agentic Framework for Self-Improving Alarm-Based Root Cause Analysis in Telecommunication Networks](https://arxiv.org/abs/2507.18190)
Token length: 1224
Summarized using GPT-3.5-turbo
Append: [Integrating an ISO30401-compliant Knowledge management system with existing business processes of an organization](https://arxiv.org/abs/2507.18197)
Token length: 1173
Summarized using GPT-3.5-turbo
Append: [Safeguarding RAG Pipelines with GMTP: A Gradient-based Masked Token Probability Method for Poisoned Document Detection](https://arxiv.org/abs/2507.18202)
Token length: 1435
Summarized using GPT-3.5-turbo
Append: [Exploring the Impact of Instruction-Tuning on LLM's Susceptibility to Misinformation](https://arxiv.org/abs/2507.18203)
Token length: 1188
Summarized using GPT-3.5-turbo
Append: [Prune&Comp: Free Lunch for Layer-Pruned LLMs via Iterative Pruning with Magnitude Compensation](https://arxiv.org/abs/2507.18212)
Token length: 1198
Summarized using GPT-3.5-turbo
Append: [Locate-and-Focus: Enhancing Terminology Translation in Speech Language Models](https://arxiv.org/abs/2507.18263)
Token length: 1290
Summarized using GPT-3.5-turbo
Append: [Zero-shot OCR Accuracy of Low-Resourced Languages: A Comparative Analysis on Sinhala and Tamil](https://arxiv.org/abs/2507.18264)
Token length: 982
Summarized using GPT-3.5-turbo
Append: [StyleAdaptedLM: Enhancing Instruction Following Models with Efficient Stylistic Transfer](https://arxiv.org/abs/2507.18294)
Token length: 1475
Summarized using GPT-3.5-turbo
Append: [BadReasoner: Planting Tunable Overthinking Backdoors into Large Reasoning Models for Fun or Profit](https://arxiv.org/abs/2507.18305)
Token length: 954
Summarized using GPT-3.5-turbo
Append: [Uncertainty Quantification for Evaluating Machine Translation Bias](https://arxiv.org/abs/2507.18338)
Token length: 1384
Summarized using GPT-3.5-turbo
Append: [TDR: Task-Decoupled Retrieval with Fine-Grained LLM Feedback for In-Context Learning](https://arxiv.org/abs/2507.18340)
Token length: 1412
Summarized using GPT-3.5-turbo
Append: [Hybrid Annotation for Propaganda Detection: Integrating LLM Pre-Annotations with Human Intelligence](https://arxiv.org/abs/2507.18343)
Token length: 1086
Summarized using GPT-3.5-turbo
Append: [CLEAR: Error Analysis via LLM-as-a-Judge Made Easy](https://arxiv.org/abs/2507.18392)
Token length: 961
Summarized using GPT-3.5-turbo
Append: [Factual Inconsistencies in Multilingual Wikipedia Tables](https://arxiv.org/abs/2507.18406)
Token length: 1685
Summarized using GPT-3.5-turbo
Append: [FinDPO: Financial Sentiment Analysis for Algorithmic Trading through Preference Optimization of LLMs](https://arxiv.org/abs/2507.18417)
Token length: 1763
Summarized using GPT-3.5-turbo
Append: [AraTable: Benchmarking LLMs' Reasoning and Understanding of Arabic Tabular Data](https://arxiv.org/abs/2507.18442)
Token length: 1176
Summarized using GPT-3.5-turbo
Append: [Restoring Rhythm: Punctuation Restoration Using Transformer Models for Bangla, a Low-Resource Language](https://arxiv.org/abs/2507.18448)
Token length: 1954
Summarized using GPT-3.5-turbo
Append: [Generation of Synthetic Clinical Text: A Systematic Review](https://arxiv.org/abs/2507.18451)
Token length: 1313
Summarized using GPT-3.5-turbo
Append: [Not All Features Deserve Attention: Graph-Guided Dependency Learning for Tabular Data Generation with Language Models](https://arxiv.org/abs/2507.18504)
Token length: 773
Summarized using GPT-3.5-turbo
Append: [The Moral Gap of Large Language Models](https://arxiv.org/abs/2507.18523)
Token length: 820
Summarized using GPT-3.5-turbo
Append: [Effective Multi-Task Learning for Biomedical Named Entity Recognition](https://arxiv.org/abs/2507.18542)
Token length: 1000
Summarized using GPT-3.5-turbo
Append: [GLiNER2: An Efficient Multi-Task Information Extraction System with Schema-Driven Interface](https://arxiv.org/abs/2507.18546)
Token length: 1149
Summarized using GPT-3.5-turbo
Append: [GIIFT: Graph-guided Inductive Image-free Multimodal Machine Translation](https://arxiv.org/abs/2507.18562)
Token length: 1467
Summarized using GPT-3.5-turbo
Append: [Hybrid Tokenization Strategy for DNA Language Model using Byte Pair Encoding and K-MER Methods](https://arxiv.org/abs/2507.18570)
Token length: 1300
Summarized using GPT-3.5-turbo
Append: [Wide-In, Narrow-Out: Revokable Decoding for Efficient and Effective DLLMs](https://arxiv.org/abs/2507.18578)
Token length: 952
Summarized using GPT-3.5-turbo
Append: [System Report for CCL25-Eval Task 10: SRAG-MAV for Fine-Grained Chinese Hate Speech Recognition](https://arxiv.org/abs/2507.18580)
Token length: 1354
Summarized using GPT-3.5-turbo
Append: [AQuilt: Weaving Logic and Self-Inspection into Low-Cost, High-Relevance Data Synthesis for Specialist LLMs](https://arxiv.org/abs/2507.18584)
Token length: 1198
Summarized using GPT-3.5-turbo
Append: [TRPrompt: Bootstrapping Query-Aware Prompt Optimization from Textual Rewards](https://arxiv.org/abs/2507.18618)
Token length: 1222
Summarized using GPT-3.5-turbo
Append: [Checklists Are Better Than Reward Models For Aligning Language Models](https://arxiv.org/abs/2507.18624)
Token length: 1167
Summarized using GPT-3.5-turbo
Append: [Exploring Communication Strategies for Collaborative LLM Agents in Mathematical Problem-Solving](https://arxiv.org/abs/2507.17753)
Append: [GenSelect: A Generative Approach to Best-of-N](https://arxiv.org/abs/2507.17797)
Append: [Bob's Confetti: Phonetic Memorization Attacks in Music and Video Generation](https://arxiv.org/abs/2507.17937)
Append: [GRR-CoCa: Leveraging LLM Mechanisms in Multimodal Model Architectures](https://arxiv.org/abs/2507.18009)
Append: [RECALLED: An Unbounded Resource Consumption Attack on Large Vision-Language Models](https://arxiv.org/abs/2507.18053)
Append: [Group Sequence Policy Optimization](https://arxiv.org/abs/2507.18071)
Append: [Agentic AI framework for End-to-End Medical Data Inference](https://arxiv.org/abs/2507.18115)
Append: [Recent Trends in Distant Conversational Speech Recognition: A Review of CHiME-7 and 8 DASR Challenges](https://arxiv.org/abs/2507.18161)
Append: [LoRA-Leak: Membership Inference Attacks Against LoRA Fine-tuned Language Models](https://arxiv.org/abs/2507.18302)
Append: [LLM-based Embedders for Prior Case Retrieval](https://arxiv.org/abs/2507.18455)
Append: [PosterMate: Audience-driven Collaborative Persona Agents for Poster Design](https://arxiv.org/abs/2507.18572)
Append: [SafeWork-R1: Coevolving Safety and Intelligence under the AI-45$^{\circ}$ Law](https://arxiv.org/abs/2507.18576)
Append: [DR.EHR: Dense Retrieval for Electronic Health Record with Knowledge Injection and Synthetic Data](https://arxiv.org/abs/2507.18583)
Append: [SynC: Synthetic Image Caption Dataset Refinement with One-to-many Mapping for Zero-shot Image Captioning](https://arxiv.org/abs/2507.18616)
Append: [Causally Testing Gender Bias in LLMs: A Case Study on Occupational Bias](https://arxiv.org/abs/2212.10678)
Append: [DocTER: Evaluating Document-based Knowledge Editing](https://arxiv.org/abs/2308.09954)
Append: [Quantifying the Uniqueness and Divisiveness of Presidential Discourse](https://arxiv.org/abs/2401.01405)
Append: [P-React: Synthesizing Topic-Adaptive Reactions of Personality Traits via Mixture of Specialized LoRA Experts](https://arxiv.org/abs/2406.12548)
Append: [VolDoGer: LLM-assisted Datasets for Domain Generalization in Vision-Language Tasks](https://arxiv.org/abs/2407.19795)
Append: [Identity-related Speech Suppression in Generative AI Content Moderation](https://arxiv.org/abs/2409.13725)
Append: [LIFBench: Evaluating the Instruction Following Performance and Stability of Large Language Models in Long-Context Scenarios](https://arxiv.org/abs/2411.07037)
Append: [BlockDialect: Block-wise Fine-grained Mixed Format Quantization for Energy-Efficient LLM Inference](https://arxiv.org/abs/2501.01144)
Append: [LLM Alignment as Retriever Optimization: An Information Retrieval Perspective](https://arxiv.org/abs/2502.03699)
Append: [ExpliCa: Evaluating Explicit Causal Reasoning in Large Language Models](https://arxiv.org/abs/2502.15487)
Append: [Discriminative Finetuning of Generative Large Language Models without Reward Models and Human Preference Data](https://arxiv.org/abs/2502.18679)
Append: [How do language models learn facts? Dynamics, curricula and hallucinations](https://arxiv.org/abs/2503.21676)
Append: [Exploiting individual differences to bootstrap communication](https://arxiv.org/abs/2504.05211)
Append: [Enhancing Transformation from Natural Language to Signal Temporal Logic Using LLMs with Diverse External Knowledge](https://arxiv.org/abs/2505.20658)
Append: [OPeRA: A Dataset of Observation, Persona, Rationale, and Action for Evaluating LLMs on Human Online Shopping Behavior Simulation](https://arxiv.org/abs/2506.05606)
Append: [Breaking Barriers: Do Reinforcement Post Training Gains Transfer To Unseen Domains?](https://arxiv.org/abs/2506.19733)
Append: [A Multi-Faceted Evaluation Framework for Assessing Synthetic Data Generated by Large Language Models](https://arxiv.org/abs/2404.14445)
Append: [Analyzing Fairness of Computer Vision and Natural Language Processing Models](https://arxiv.org/abs/2412.09900)
Append: [DEFAME: Dynamic Evidence-based FAct-checking with Multimodal Experts](https://arxiv.org/abs/2412.10510)
Append: [AIR-Bench: Automated Heterogeneous Information Retrieval Benchmark](https://arxiv.org/abs/2412.13102)
Append: [ELITE: Enhanced Language-Image Toxicity Evaluation for Safety](https://arxiv.org/abs/2502.04757)
Append: [IPCGRL: Language-Instructed Reinforcement Learning for Procedural Level Generation](https://arxiv.org/abs/2503.12358)
Append: [Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs](https://arxiv.org/abs/2503.16870)
Append: [LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are Important](https://arxiv.org/abs/2504.04704)
Append: [EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent Dialogue Framework](https://arxiv.org/abs/2504.14928)
Append: [Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games](https://arxiv.org/abs/2506.23276)
Append: [Scaling RL to Long Videos](https://arxiv.org/abs/2507.07966)
append_entries: 90
Finish: 2025-07-25 04:43:10.598541
------------------------------------------------------
Started: 2025-07-25 06:27:32.623003
Existing_entries: 1090
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1264
Summarized using GPT-3.5-turbo
Append: [Weak-to-Strong Jailbreaking on Large Language Models](https://arxiv.org/abs/2401.17256)
Token length: 1871
Summarized using GPT-3.5-turbo
Append: [A comprehensive study of LLM-based argument classification: from LLAMA through GPT-4o to Deepseek-R1](https://arxiv.org/abs/2507.08621)
append_entries: 2
Finish: 2025-07-25 06:27:37.555859
------------------------------------------------------
Started: 2025-07-25 08:23:51.993147
Existing_entries: 1002
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-25 08:23:52.247981
------------------------------------------------------
Started: 2025-07-25 10:19:58.348113
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-25 10:19:58.662794
------------------------------------------------------
Started: 2025-07-25 12:36:52.779408
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-25 12:36:53.064150
------------------------------------------------------
Started: 2025-07-25 14:16:23.672497
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-25 14:16:23.929920
------------------------------------------------------
Started: 2025-07-25 16:22:34.828373
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-25 16:22:35.078949
------------------------------------------------------
Started: 2025-07-25 18:25:44.556879
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-25 18:25:44.875744
------------------------------------------------------
Started: 2025-07-25 20:19:45.186324
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-25 20:19:45.443962
------------------------------------------------------
Started: 2025-07-25 22:17:10.762394
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-25 22:17:11.027711
------------------------------------------------------
Started: 2025-07-26 01:24:01.028912
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-26 01:24:01.301211
------------------------------------------------------
Started: 2025-07-26 03:23:59.343127
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-26 03:23:59.603935
------------------------------------------------------
Started: 2025-07-26 04:34:01.976038
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-26 04:34:02.039957
------------------------------------------------------
Started: 2025-07-26 06:25:19.095794
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-26 06:25:19.213256
------------------------------------------------------
Started: 2025-07-26 08:21:32.647208
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-26 08:21:32.709460
------------------------------------------------------
Started: 2025-07-26 10:17:33.861295
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-26 10:17:33.922710
------------------------------------------------------
Started: 2025-07-26 12:33:54.575210
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-26 12:33:54.689693
------------------------------------------------------
Started: 2025-07-26 14:15:49.717086
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-26 14:15:49.773072
------------------------------------------------------
Started: 2025-07-26 16:20:16.705791
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-26 16:20:16.799130
------------------------------------------------------
Started: 2025-07-26 18:23:40.904276
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-26 18:23:40.968900
------------------------------------------------------
Started: 2025-07-26 20:18:15.553093
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-26 20:18:15.611672
------------------------------------------------------
Started: 2025-07-26 22:16:27.049093
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-26 22:16:27.133019
------------------------------------------------------
Started: 2025-07-27 01:42:41.091000
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-27 01:42:41.181560
------------------------------------------------------
Started: 2025-07-27 03:44:49.612159
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-27 03:44:49.670415
------------------------------------------------------
Started: 2025-07-27 04:40:48.183998
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-27 04:40:48.243004
------------------------------------------------------
Started: 2025-07-27 06:25:57.399676
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-27 06:25:57.459583
------------------------------------------------------
Started: 2025-07-27 08:21:42.248962
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-27 08:21:42.345358
------------------------------------------------------
Started: 2025-07-27 10:18:04.793469
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-27 10:18:04.854590
------------------------------------------------------
Started: 2025-07-27 12:34:49.356403
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-27 12:34:49.414662
------------------------------------------------------
Started: 2025-07-27 14:15:25.576486
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-27 14:15:25.672337
------------------------------------------------------
Started: 2025-07-27 16:20:28.418678
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-27 16:20:28.477592
------------------------------------------------------
Started: 2025-07-27 18:23:41.364268
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-27 18:23:41.445879
------------------------------------------------------
Started: 2025-07-27 20:18:54.510107
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-27 20:18:54.570140
------------------------------------------------------
Started: 2025-07-27 22:16:52.711616
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-27 22:16:52.779237
------------------------------------------------------
Started: 2025-07-28 01:41:09.925274
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-28 01:41:09.996609
------------------------------------------------------
Started: 2025-07-28 03:43:37.595195
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-28 03:43:37.681874
------------------------------------------------------
Started: 2025-07-28 04:46:20.949815
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1184
Summarized using GPT-3.5-turbo
Append: [Specification Self-Correction: Mitigating In-Context Reward Hacking Through Test-Time Refinement](https://arxiv.org/abs/2507.18742)
Token length: 1196
Summarized using GPT-3.5-turbo
Append: [The Role of Orthographic Consistency in Multilingual Embedding Models for Text Classification in Arabic-Script Languages](https://arxiv.org/abs/2507.18762)
Token length: 1225
Summarized using GPT-3.5-turbo
Append: [ylmmcl at Multilingual Text Detoxification 2025: Lexicon-Guided Detoxification and Classifier-Gated Rewriting](https://arxiv.org/abs/2507.18769)
Token length: 1096
Summarized using GPT-3.5-turbo
Append: [Evaluating Code-Mixing in LLMs Across 18 Languages](https://arxiv.org/abs/2507.18791)
Token length: 1040
Summarized using GPT-3.5-turbo
Append: [CueBuddy: helping non-native English speakers navigate English-centric STEM education](https://arxiv.org/abs/2507.18827)
Token length: 768
Summarized using GPT-3.5-turbo
Append: [PrismRAG: Boosting RAG Factuality with Distractor Resilience and Strategized Reasoning](https://arxiv.org/abs/2507.18857)
Token length: 1191
Summarized using GPT-3.5-turbo
Append: [MindFlow+: A Self-Evolving Agent for E-Commerce Customer Service](https://arxiv.org/abs/2507.18884)
Token length: 1482
Summarized using GPT-3.5-turbo
Append: [NUTMEG: Separating Signal From Noise in Annotator Disagreement](https://arxiv.org/abs/2507.18890)
Token length: 1540
Summarized using GPT-3.5-turbo
Append: [REPRO-Bench: Can Agentic AI Systems Assess the Reproducibility of Social Science Research?](https://arxiv.org/abs/2507.18901)
Token length: 1655
Summarized using GPT-3.5-turbo
Append: [SLoW: Select Low-frequency Words! Automatic Dictionary Selection for Translation on Large Language Models](https://arxiv.org/abs/2507.18902)
Token length: 1211
Summarized using GPT-3.5-turbo
Append: [Large language models provide unsafe answers to patient-posed medical questions](https://arxiv.org/abs/2507.18905)
Token length: 1672
Summarized using GPT-3.5-turbo
Append: [A Systematic Review of Key Retrieval-Augmented Generation (RAG) Systems: Progress, Gaps, and Future Directions](https://arxiv.org/abs/2507.18910)
Token length: 1086
Summarized using GPT-3.5-turbo
Append: [Mining Contextualized Visual Associations from Images for Creativity Understanding](https://arxiv.org/abs/2507.18915)
Token length: 1231
Summarized using GPT-3.5-turbo
Append: [Uncovering Cross-Linguistic Disparities in LLMs using Sparse Autoencoders](https://arxiv.org/abs/2507.18918)
Token length: 1378
Summarized using GPT-3.5-turbo
Append: [LLaVA-NeuMT: Selective Layer-Neuron Modulation for Efficient Multilingual Multimodal Translation](https://arxiv.org/abs/2507.18940)
Token length: 1421
Summarized using GPT-3.5-turbo
Append: [Legal Document Summarization: Enhancing Judicial Efficiency through Automation Detection](https://arxiv.org/abs/2507.18952)
Token length: 1002
Summarized using GPT-3.5-turbo
Append: [A Similarity Measure for Comparing Conversational Dynamics](https://arxiv.org/abs/2507.18956)
Token length: 1456
Summarized using GPT-3.5-turbo
Append: [A Toolbox, Not a Hammer -- Multi-TAG: Scaling Math Reasoning with Multi-Tool Aggregation](https://arxiv.org/abs/2507.18973)
Token length: 1187
Summarized using GPT-3.5-turbo
Append: [Arg-LLaDA: Argument Summarization via Large Language Diffusion Models and Sufficiency-Aware Refinement](https://arxiv.org/abs/2507.19081)
Token length: 1093
Summarized using GPT-3.5-turbo
Append: [Debating Truth: Debate-driven Claim Verification with Multiple Large Language Model Agents](https://arxiv.org/abs/2507.19090)
Token length: 1133
Summarized using GPT-3.5-turbo
Append: [Objectifying the Subjective: Cognitive Biases in Topic Interpretations](https://arxiv.org/abs/2507.19117)
Token length: 1719
Summarized using GPT-3.5-turbo
Append: [An Empirical Investigation of Gender Stereotype Representation in Large Language Models: The Italian Case](https://arxiv.org/abs/2507.19156)
Token length: 1251
Summarized using GPT-3.5-turbo
Append: [Can Small-Scale Data Poisoning Exacerbate Dialect-Linked Biases in Large Language Models?](https://arxiv.org/abs/2507.19195)
Token length: 1562
Summarized using GPT-3.5-turbo
Append: [How Much Do Large Language Model Cheat on Evaluation? Benchmarking Overestimation under the One-Time-Pad-Based Framework](https://arxiv.org/abs/2507.19219)
Token length: 1604
Summarized using GPT-3.5-turbo
Append: [Jailbreaking Large Language Diffusion Models: Revealing Hidden Safety Flaws in Diffusion-Based Text Generation](https://arxiv.org/abs/2507.19227)
Token length: 1361
Summarized using GPT-3.5-turbo
Append: [Identifying Fine-grained Forms of Populism in Political Discourse: A Case Study on Donald Trump's Presidential Campaigns](https://arxiv.org/abs/2507.19303)
Token length: 1015
Summarized using GPT-3.5-turbo
Append: [AutoPCR: Automated Phenotype Concept Recognition by Prompting](https://arxiv.org/abs/2507.19315)
Token length: 1830
Summarized using GPT-3.5-turbo
Append: [Smooth Reading: Bridging the Gap of Recurrent LLM to Self-Attention LLM on Long-Context Tasks](https://arxiv.org/abs/2507.19353)
Token length: 1209
Summarized using GPT-3.5-turbo
Append: [Enhancing Speech Emotion Recognition Leveraging Aligning Timestamps of ASR Transcripts and Speaker Diarization](https://arxiv.org/abs/2507.19356)
Token length: 1096
Summarized using GPT-3.5-turbo
Append: [SpeechIQ: Speech Intelligence Quotient Across Cognitive Levels in Voice Understanding Large Language Models](https://arxiv.org/abs/2507.19361)
Token length: 1046
Summarized using GPT-3.5-turbo
Append: [Data Augmentation for Spoken Grammatical Error Correction](https://arxiv.org/abs/2507.19374)
Token length: 1869
Summarized using GPT-3.5-turbo
Append: [Detection of Adverse Drug Events in Dutch clinical free text documents using Transformer Models: benchmark study](https://arxiv.org/abs/2507.19396)
Token length: 1373
Summarized using GPT-3.5-turbo
Append: [Towards Domain Specification of Embedding Models in Medicine](https://arxiv.org/abs/2507.19407)
Token length: 1124
Summarized using GPT-3.5-turbo
Append: [TokenSmith: Streamlining Data Editing, Search, and Inspection for Large-Scale Language Model Training and Interpretability](https://arxiv.org/abs/2507.19419)
Token length: 1342
Summarized using GPT-3.5-turbo
Append: [GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning](https://arxiv.org/abs/2507.19457)
Token length: 865
Summarized using GPT-3.5-turbo
Append: [Conversations Gone Awry, But Then? Evaluating Conversational Forecasting Models](https://arxiv.org/abs/2507.19470)
Token length: 1783
Summarized using GPT-3.5-turbo
Append: [People Are Highly Cooperative with Large Language Models, Especially When Communication Is Possible or Following Human Interaction](https://arxiv.org/abs/2507.18639)
Token length: 1266
Summarized using GPT-3.5-turbo
Append: [TreeReader: A Hierarchical Academic Paper Reader Powered by Language Models](https://arxiv.org/abs/2507.18945)
Token length: 1084
Summarized using GPT-3.5-turbo
Append: [Adaptive Learning Systems: Personalized Curriculum Design Using LLM-Powered Analytics](https://arxiv.org/abs/2507.18949)
Token length: 1347
Summarized using GPT-3.5-turbo
Append: [MLLM-based Speech Recognition: When and How is Multimodality Beneficial?](https://arxiv.org/abs/2507.19037)
Token length: 1038
Summarized using GPT-3.5-turbo
Append: [FD-Bench: A Full-Duplex Benchmarking Pipeline Designed for Full Duplex Spoken Dialogue Systems](https://arxiv.org/abs/2507.19040)
Token length: 1025
Summarized using GPT-3.5-turbo
Append: [Closing the Modality Gap for Mixed Modality Search](https://arxiv.org/abs/2507.19054)
Token length: 1201
Summarized using GPT-3.5-turbo
Append: [PurpCode: Reasoning for Safer Code Generation](https://arxiv.org/abs/2507.19060)
Token length: 1824
Summarized using GPT-3.5-turbo
Append: [Distilling a Small Utility-Based Passage Selector to Enhance Retrieval-Augmented Generation](https://arxiv.org/abs/2507.19102)
Token length: 1661
Summarized using GPT-3.5-turbo
Append: [OS-MAP: How Far Can Computer-Using Agents Go in Breadth and Depth?](https://arxiv.org/abs/2507.19132)
Token length: 846
Summarized using GPT-3.5-turbo
Append: [Towards Multimodal Social Conversations with Robots: Using Vision-Language Models](https://arxiv.org/abs/2507.19196)
Token length: 1633
Summarized using GPT-3.5-turbo
Append: [Should Top-Down Clustering Affect Boundaries in Unsupervised Word Discovery?](https://arxiv.org/abs/2507.19204)
Token length: 1757
Summarized using GPT-3.5-turbo
Append: [A Markov Categorical Framework for Language Modeling](https://arxiv.org/abs/2507.19247)
Token length: 1630
Summarized using GPT-3.5-turbo
Append: [Injecting External Knowledge into the Reasoning Process Enhances Retrieval-Augmented Generation](https://arxiv.org/abs/2507.19333)
Token length: 914
Summarized using GPT-3.5-turbo
Append: [LOTUS: A Leaderboard for Detailed Image Captioning from Quality to Societal Bias and User Preferences](https://arxiv.org/abs/2507.19362)
Append: [Advancing Event Forecasting through Massive Training of Large Language Models: Challenges, Solutions, and Broader Impacts](https://arxiv.org/abs/2507.19477)
Append: [MMBench-GUI: Hierarchical Multi-Platform Evaluation Framework for GUI Agents](https://arxiv.org/abs/2507.19478)
Append: [Comparison of pipeline, sequence-to-sequence, and GPT models for end-to-end relation extraction: experiments with the rare disease use-case](https://arxiv.org/abs/2311.13729)
Append: [Spike No More: Stabilizing the Pre-training of Large Language Models](https://arxiv.org/abs/2312.16903)
Append: [How Important is Domain Specificity in Language Models and Instruction Finetuning for Biomedical Relation Extraction?](https://arxiv.org/abs/2402.13470)
Append: [MultiSocial: Multilingual Benchmark of Machine-Generated Text Detection of Social-Media Texts](https://arxiv.org/abs/2406.12549)
Append: [Long-Form Answers to Visual Questions from Blind and Low Vision People](https://arxiv.org/abs/2408.06303)
Append: [Advancing biomolecular understanding and design following human instructions](https://arxiv.org/abs/2410.07919)
Append: [A Comprehensive Evaluation of Semantic Relation Knowledge of Pretrained Language Models and Humans](https://arxiv.org/abs/2412.01131)
Append: [LLMs are Also Effective Embedding Models: An In-depth Overview](https://arxiv.org/abs/2412.12591)
Append: [Evaluation of LLM Vulnerabilities to Being Misused for Personalized Disinformation Generation](https://arxiv.org/abs/2412.13666)
Append: [T2ISafety: Benchmark for Assessing Fairness, Toxicity, and Privacy in Image Generation](https://arxiv.org/abs/2501.12612)
Append: [An Efficient Sparse Fine-Tuning with Low Quantization Error via Neural Network Pruning](https://arxiv.org/abs/2502.11439)
Append: [Can LLMs Predict Citation Intent? An Experimental Analysis of In-context Learning and Fine-tuning on Open LLMs](https://arxiv.org/abs/2502.14561)
Append: [Palm: A Culturally Inclusive and Linguistically Diverse Dataset for Arabic LLMs](https://arxiv.org/abs/2503.00151)
Append: [Ensemble Debiasing Across Class and Sample Levels for Fairer Prompting Accuracy](https://arxiv.org/abs/2503.05157)
Append: [Relation Extraction with Instance-Adapted Predicate Descriptions](https://arxiv.org/abs/2503.17799)
Append: [Kill two birds with one stone: generalized and robust AI-generated text detection via dynamic perturbations](https://arxiv.org/abs/2504.21019)
Append: [RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale](https://arxiv.org/abs/2505.03005)
Append: [Distilling the Implicit Multi-Branch Structure in LLMs' Reasoning via Reinforcement Learning](https://arxiv.org/abs/2505.16142)
Append: [DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning System for Multi-Turn Clinical Dialogue](https://arxiv.org/abs/2505.19630)
Append: [Toward Structured Knowledge Reasoning: Contrastive Retrieval-Augmented Generation on Experience](https://arxiv.org/abs/2506.00842)
Append: [Acoustically Precise Hesitation Tagging Is Essential for End-to-End Verbatim Transcription Systems](https://arxiv.org/abs/2506.04076)
Append: [References Matter: Investigating the Impact of Reference Set Variation on Summarization Evaluation](https://arxiv.org/abs/2506.14335)
Append: [JCAPT: A Joint Modeling Approach for CAPT](https://arxiv.org/abs/2506.19315)
Append: [Noise Contrastive Estimation-based Matching Framework for Low-Resource Security Attack Pattern Recognition](https://arxiv.org/abs/2401.10337)
Append: [XAI4LLM. Let Machine Learning Models and LLMs Collaborate for Enhanced In-Context Learning in Healthcare](https://arxiv.org/abs/2405.06270)
Append: [An Investigation of Prompt Variations for Zero-shot LLM-based Rankers](https://arxiv.org/abs/2406.14117)
Append: [ToolACE: Winning the Points of LLM Function Calling](https://arxiv.org/abs/2409.00920)
Append: [Verbalized Representation Learning for Interpretable Few-Shot Generalization](https://arxiv.org/abs/2411.18651)
Append: [Accelerating Multimodal Large Language Models via Dynamic Visual-Token Exit and the Empirical Findings](https://arxiv.org/abs/2411.19628)
Append: [Analyze Feature Flow to Enhance Interpretation and Steering in Language Models](https://arxiv.org/abs/2502.03032)
Append: [Distillation Scaling Laws](https://arxiv.org/abs/2502.08606)
Append: [Toward Super Agent System with Hybrid AI Routers](https://arxiv.org/abs/2504.10519)
Append: [AI Flow: Perspectives, Scenarios, and Approaches](https://arxiv.org/abs/2506.12479)
append_entries: 85
Finish: 2025-07-28 04:48:01.816182
------------------------------------------------------
Started: 2025-07-28 06:29:19.421264
Existing_entries: 1085
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1806
Summarized using GPT-3.5-turbo
Append: [MedicalBERT: enhancing biomedical natural language processing using pretrained BERT-based model](https://arxiv.org/abs/2507.08013)
append_entries: 1
Finish: 2025-07-28 06:29:21.612926
------------------------------------------------------
Started: 2025-07-28 08:27:27.665490
Existing_entries: 1001
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-28 08:27:27.937232
------------------------------------------------------
Started: 2025-07-28 10:20:03.857777
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-28 10:20:04.093854
------------------------------------------------------
Started: 2025-07-28 12:38:54.010138
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-28 12:38:54.298333
------------------------------------------------------
Started: 2025-07-28 14:20:02.759291
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-28 14:20:03.063948
------------------------------------------------------
Started: 2025-07-28 16:23:41.395948
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-28 16:23:41.635744
------------------------------------------------------
Started: 2025-07-28 18:27:46.156982
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-28 18:27:46.415857
------------------------------------------------------
Started: 2025-07-28 20:20:26.392570
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-28 20:20:26.670439
------------------------------------------------------
Started: 2025-07-28 22:18:23.560793
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-28 22:18:23.807192
------------------------------------------------------
Started: 2025-07-29 01:46:26.622921
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-29 01:46:26.860084
------------------------------------------------------
Started: 2025-07-29 03:47:19.918363
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-29 03:47:20.224319
------------------------------------------------------
Started: 2025-07-29 04:48:54.978458
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1573
Summarized using GPT-3.5-turbo
Append: [Advancing Mental Disorder Detection: A Comparative Evaluation of Transformer and LSTM Architectures on Social Media](https://arxiv.org/abs/2507.19511)
Token length: 1308
Summarized using GPT-3.5-turbo
Append: [Setting The Table with Intent: Intent-aware Schema Generation and Editing for Literature Review Tables](https://arxiv.org/abs/2507.19521)
Token length: 1124
Summarized using GPT-3.5-turbo
Append: [Mind the Language Gap in Digital Humanities: LLM-Aided Translation of SKOS Thesauri](https://arxiv.org/abs/2507.19537)
Token length: 1386
Summarized using GPT-3.5-turbo
Append: [Mitigating Geospatial Knowledge Hallucination in Large Language Models: Benchmarking and Dynamic Factuality Aligning](https://arxiv.org/abs/2507.19586)
Token length: 1421
Summarized using GPT-3.5-turbo
Append: [Efficient Attention Mechanisms for Large Language Models: A Survey](https://arxiv.org/abs/2507.19595)
Token length: 1064
Summarized using GPT-3.5-turbo
Append: [MOCHA: Are Code Language Models Robust Against Multi-Turn Malicious Coding Prompts?](https://arxiv.org/abs/2507.19598)
Token length: 965
Summarized using GPT-3.5-turbo
Append: [HITSZ's End-To-End Speech Translation Systems Combining Sequence-to-Sequence Auto Speech Recognition Model and Indic Large Language Model for IWSLT 2025 in Indic Track](https://arxiv.org/abs/2507.19616)
Token length: 1449
Summarized using GPT-3.5-turbo
Append: [MCIF: Multimodal Crosslingual Instruction-Following Benchmark from Scientific Talks](https://arxiv.org/abs/2507.19634)
Token length: 1273
Summarized using GPT-3.5-turbo
Append: [RoD-TAL: A Benchmark for Answering Questions in Romanian Driving License Exams](https://arxiv.org/abs/2507.19666)
Token length: 1237
Summarized using GPT-3.5-turbo
Append: [Towards Inclusive NLP: Assessing Compressed Multilingual Transformers across Diverse Language Benchmarks](https://arxiv.org/abs/2507.19699)
Token length: 1399
Summarized using GPT-3.5-turbo
Append: [Ta-G-T: Subjectivity Capture in Table to Text Generation via RDF Graphs](https://arxiv.org/abs/2507.19710)
Token length: 1185
Summarized using GPT-3.5-turbo
Append: [Basic Reading Distillation](https://arxiv.org/abs/2507.19741)
Token length: 1371
Summarized using GPT-3.5-turbo
Append: [JT-Math: A Multi-Stage Framework for Advanced Mathematical Reasoning in Large Language Models](https://arxiv.org/abs/2507.19748)
Token length: 1097
Summarized using GPT-3.5-turbo
Append: [Are You There God? Lightweight Narrative Annotation of Christian Fiction with LMs](https://arxiv.org/abs/2507.19756)
Token length: 1415
Summarized using GPT-3.5-turbo
Append: [UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing Large Language Models' Reasoning Abilities](https://arxiv.org/abs/2507.19766)
Token length: 1317
Summarized using GPT-3.5-turbo
Append: [Flora: Effortless Context Construction to Arbitrary Length and Scale](https://arxiv.org/abs/2507.19786)
Token length: 1265
Summarized using GPT-3.5-turbo
Append: [HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs](https://arxiv.org/abs/2507.19823)
Token length: 1633
Summarized using GPT-3.5-turbo
Append: [DRIVE: Disfluency-Rich Synthetic Dialog Data Generation Framework for Intelligent Vehicle Environments](https://arxiv.org/abs/2507.19867)
Token length: 706
Summarized using GPT-3.5-turbo
Append: [The Polish Vocabulary Size Test: A Novel Adaptive Test for Receptive Vocabulary Assessment](https://arxiv.org/abs/2507.19869)
Token length: 1940
Summarized using GPT-3.5-turbo
Append: [Zero-shot Performance of Generative AI in Brazilian Portuguese Medical Exam](https://arxiv.org/abs/2507.19885)
Token length: 1302
Summarized using GPT-3.5-turbo
Append: [A Gold Standard Dataset and Evaluation Framework for Depression Detection and Explanation in Social Media using LLMs](https://arxiv.org/abs/2507.19899)
Token length: 1392
Summarized using GPT-3.5-turbo
Append: [CaliDrop: KV Cache Compression with Calibration](https://arxiv.org/abs/2507.19906)
Token length: 951
Summarized using GPT-3.5-turbo
Append: [KLAAD: Refining Attention Mechanisms to Reduce Societal Bias in Generative Language Models](https://arxiv.org/abs/2507.19962)
Token length: 1562
Summarized using GPT-3.5-turbo
Append: [Text2Vis: A Challenging and Diverse Benchmark for Generating Multimodal Visualizations from Text](https://arxiv.org/abs/2507.19969)
Token length: 1030
Summarized using GPT-3.5-turbo
Append: [Exploring LLM Autoscoring Reliability in Large-Scale Writing Assessments Using Generalizability Theory](https://arxiv.org/abs/2507.19980)
Token length: 1368
Summarized using GPT-3.5-turbo
Append: [VLQA: The First Comprehensive, Large, and High-Quality Vietnamese Dataset for Legal Question Answering](https://arxiv.org/abs/2507.19995)
Token length: 949
Summarized using GPT-3.5-turbo
Append: [Anomaly Detection in Human Language via Meta-Learning: A Few-Shot Approach](https://arxiv.org/abs/2507.20019)
Token length: 1352
Summarized using GPT-3.5-turbo
Append: [FAEDKV: Infinite-Window Fourier Transform for Unbiased KV Cache Compression](https://arxiv.org/abs/2507.20030)
Token length: 1293
Summarized using GPT-3.5-turbo
Append: [Infogen: Generating Complex Statistical Infographics from Documents](https://arxiv.org/abs/2507.20046)
Token length: 1868
Summarized using GPT-3.5-turbo
Append: [A Tensor-Based Compiler and a Runtime for Neuron-Level DNN Certifier Specifications](https://arxiv.org/abs/2507.20055)
Token length: 922
Summarized using GPT-3.5-turbo
Append: [RAG in the Wild: On the (In)effectiveness of LLMs with Mixture-of-Knowledge Retrieval Augmentation](https://arxiv.org/abs/2507.20059)
Token length: 1325
Summarized using GPT-3.5-turbo
Append: [ProsodyLM: Uncovering the Emerging Prosody Processing Capabilities in Speech Language Models](https://arxiv.org/abs/2507.20091)
Token length: 1188
Summarized using GPT-3.5-turbo
Append: [AI-Driven Generation of Old English: A Framework for Low-Resource Languages](https://arxiv.org/abs/2507.20111)
Token length: 1569
Summarized using GPT-3.5-turbo
Append: [Sem-DPO: Mitigating Semantic Inconsistency in Preference Optimization for Prompt Engineering](https://arxiv.org/abs/2507.20133)
Token length: 1287
Summarized using GPT-3.5-turbo
Append: [Multi-Stage Verification-Centric Framework for Mitigating Hallucination in Multi-Modal RAG](https://arxiv.org/abs/2507.20136)
Token length: 1316
Summarized using GPT-3.5-turbo
Append: [Multi-Agent Interactive Question Generation Framework for Long Document Understanding](https://arxiv.org/abs/2507.20145)
Token length: 1147
Summarized using GPT-3.5-turbo
Append: [Goal Alignment in LLM-Based User Simulators for Conversational AI](https://arxiv.org/abs/2507.20152)
Token length: 1255
Summarized using GPT-3.5-turbo
Append: [SGPO: Self-Generated Preference Optimization based on Self-Improver](https://arxiv.org/abs/2507.20181)
Token length: 1592
Summarized using GPT-3.5-turbo
Append: [SessionIntentBench: A Multi-task Inter-session Intention-shift Modeling Benchmark for E-commerce Customer Behavior Understanding](https://arxiv.org/abs/2507.20185)
Token length: 1489
Summarized using GPT-3.5-turbo
Append: [Diversity-Enhanced Reasoning for Subjective Questions](https://arxiv.org/abs/2507.20187)
Token length: 1027
Summarized using GPT-3.5-turbo
Append: [IQ Test for LLMs: An Evaluation Framework for Uncovering Core Skills in LLMs](https://arxiv.org/abs/2507.20208)
Token length: 1297
Summarized using GPT-3.5-turbo
Append: [Co-NAML-LSTUR: A Combined Model with Attentive Multi-View Learning and Long- and Short-term User Representations for News Recommendation](https://arxiv.org/abs/2507.20210)
Token length: 1273
Summarized using GPT-3.5-turbo
Append: [Reframe Your Life Story: Interactive Narrative Therapist and Innovative Moment Assessment with Large Language Models](https://arxiv.org/abs/2507.20241)
Token length: 1131
Summarized using GPT-3.5-turbo
Append: [Modeling Professionalism in Expert Questioning through Linguistic Differentiation](https://arxiv.org/abs/2507.20249)
Token length: 1307
Summarized using GPT-3.5-turbo
Append: [Post-Completion Learning for Language Models](https://arxiv.org/abs/2507.20252)
Token length: 1511
Summarized using GPT-3.5-turbo
Append: [EMBRACE: Shaping Inclusive Opinion Representation by Aligning Implicit Conversations with Social Norms](https://arxiv.org/abs/2507.20264)
Token length: 1492
Summarized using GPT-3.5-turbo
Append: [MoL-RL: Distilling Multi-Step Environmental Feedback into LLMs for Feedback-Independent Reasoning](https://arxiv.org/abs/2507.20278)
Token length: 1123
Summarized using GPT-3.5-turbo
Append: [What Language(s) Does Aya-23 Think In? How Multilinguality Affects Internal Language Representations](https://arxiv.org/abs/2507.20279)
Token length: 1577
Summarized using GPT-3.5-turbo
Append: [Advancing Dialectal Arabic to Modern Standard Arabic Machine Translation](https://arxiv.org/abs/2507.20301)
Token length: 918
Summarized using GPT-3.5-turbo
Append: [DYNARTmo: A Dynamic Articulatory Model for Visualization of Speech Movement Patterns](https://arxiv.org/abs/2507.20343)
Append: [RMTBench: Benchmarking LLMs Through Multi-Turn User-Centric Role-Playing](https://arxiv.org/abs/2507.20352)
Append: [Length Representations in Large Language Models](https://arxiv.org/abs/2507.20398)
Append: [Cognitive Chain-of-Thought: Structured Multimodal Reasoning about Social Situations](https://arxiv.org/abs/2507.20409)
Append: [CONCAP: Seeing Beyond English with Concepts Retrieval-Augmented Captioning](https://arxiv.org/abs/2507.20411)
Append: [Survey of NLU Benchmarks Diagnosing Linguistic Phenomena: Why not Standardize Diagnostics Benchmarks?](https://arxiv.org/abs/2507.20419)
Append: [CodeNER: Code Prompting for Named Entity Recognition](https://arxiv.org/abs/2507.20423)
Append: [Speaking in Words, Thinking in Logic: A Dual-Process Framework in QA Systems](https://arxiv.org/abs/2507.20491)
Append: [AQUA: A Large Language Model for Aquaculture & Fisheries](https://arxiv.org/abs/2507.20520)
Append: [SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers](https://arxiv.org/abs/2507.20527)
Append: [Dialogues of Dissent: Thematic and Rhetorical Dimensions of Hate and Counter-Hate Speech in Social Media Conversations](https://arxiv.org/abs/2507.20528)
Append: [Enhancing Hallucination Detection via Future Context](https://arxiv.org/abs/2507.20546)
Append: [ZSE-Cap: A Zero-Shot Ensemble for Image Retrieval and Prompt-Guided Captioning](https://arxiv.org/abs/2507.20564)
Append: [Before the Outrage: Challenges and Advances in Predicting Online Antisocial Behavior](https://arxiv.org/abs/2507.20614)
Append: [Ontology-Enhanced Knowledge Graph Completion using Large Language Models](https://arxiv.org/abs/2507.20643)
Append: [Geometric-Mean Policy Optimization](https://arxiv.org/abs/2507.20673)
Append: [When Scale Meets Diversity: Evaluating Language Models on Fine-Grained Multilingual Claim Verification](https://arxiv.org/abs/2507.20700)
Append: [Text2VLM: Adapting Text-Only Datasets to Evaluate Alignment Training in Visual Language Models](https://arxiv.org/abs/2507.20704)
Append: [Investigating Structural Pruning and Recovery Techniques for Compressing Multimodal Large Language Models: An Empirical Study](https://arxiv.org/abs/2507.20749)
Append: [Multilingual Self-Taught Faithfulness Evaluators](https://arxiv.org/abs/2507.20752)
Append: [On The Role of Pretrained Language Models in General-Purpose Text Embeddings: A Survey](https://arxiv.org/abs/2507.20783)
Append: [Automating Thematic Review of Prevention of Future Deaths Reports: Replicating the ONS Child Suicide Study using Large Language Models](https://arxiv.org/abs/2507.20786)
Append: [Latent Inter-User Difference Modeling for LLM Personalization](https://arxiv.org/abs/2507.20849)
Append: [A survey of diversity quantification in natural language processing: The why, what, where and how](https://arxiv.org/abs/2507.20858)
Append: [Leveraging Open-Source Large Language Models for Clinical Information Extraction in Resource-Constrained Settings](https://arxiv.org/abs/2507.20859)
Append: [Soft Injection of Task Embeddings Outperforms Prompt-Based In-Context Learning](https://arxiv.org/abs/2507.20906)
Append: [MediQAl: A French Medical Question Answering Dataset for Knowledge and Reasoning Evaluation](https://arxiv.org/abs/2507.20917)
Append: [FHSTP@EXIST 2025 Benchmark: Sexism Detection with Transparent Speech Concept Bottleneck Models](https://arxiv.org/abs/2507.20924)
Append: [FRED: Financial Retrieval-Enhanced Detection and Editing of Hallucinations in Language Models](https://arxiv.org/abs/2507.20930)
Append: [Mind the Gap: Conformative Decoding to Improve Output Diversity of Instruction-Tuned Large Language Models](https://arxiv.org/abs/2507.20956)
Append: [Memorization in Fine-Tuned Large Language Models](https://arxiv.org/abs/2507.21009)
Append: [Multi-Agent-as-Judge: Aligning LLM-Agent-Based Automated Evaluation with Multi-Dimensional Human Evaluation](https://arxiv.org/abs/2507.21028)
Append: [Does AI and Human Advice Mitigate Punishment for Selfish Behavior? An Experiment on AI ethics From a Psychological Perspective](https://arxiv.org/abs/2507.19487)
Append: [FedDPG: An Adaptive Yet Efficient Prompt-tuning Approach in Federated Learning Settings](https://arxiv.org/abs/2507.19534)
Append: [Salsa as a Nonverbal Embodied Language -- The CoMPAS3D Dataset and Benchmarks](https://arxiv.org/abs/2507.19684)
Append: [AutoSign: Direct Pose-to-Text Translation for Continuous Sign Language Recognition](https://arxiv.org/abs/2507.19840)
Append: [Agentic Reinforced Policy Optimization](https://arxiv.org/abs/2507.19849)
Append: [The Impact of Fine-tuning Large Language Models on Automated Program Repair](https://arxiv.org/abs/2507.19909)
Append: [Spatial Language Likelihood Grounding Network for Bayesian Fusion of Human-Robot Observations](https://arxiv.org/abs/2507.19947)
Append: [Leveraging Fine-Tuned Large Language Models for Interpretable Pancreatic Cystic Lesion Feature Extraction and Risk Categorization](https://arxiv.org/abs/2507.19973)
Append: [Improving the Performance of Sequential Recommendation Systems with an Extended Large Language Model](https://arxiv.org/abs/2507.19990)
Append: [The Carbon Cost of Conversation, Sustainability in the Age of Language Models](https://arxiv.org/abs/2507.20018)
Append: [$K^4$: Online Log Anomaly Detection Via Unsupervised Typicality Learning](https://arxiv.org/abs/2507.20051)
Append: [PITA: Preference-Guided Inference-Time Alignment for LLM Post-Training](https://arxiv.org/abs/2507.20067)
Append: [The Devil is in the EOS: Sequence Training for Detailed Image Captioning](https://arxiv.org/abs/2507.20077)
Append: [EcoTransformer: Attention without Multiplication](https://arxiv.org/abs/2507.20096)
Append: [The Policy Cliff: A Theoretical Analysis of Reward-Policy Maps in Large Language Models](https://arxiv.org/abs/2507.20150)
Append: [SciToolAgent: A Knowledge Graph-Driven Scientific Agent for Multi-Tool Integration](https://arxiv.org/abs/2507.20280)
Append: [MountainLion: A Multi-Modal LLM-Based Agent System for Interpretable and Adaptive Financial Trading](https://arxiv.org/abs/2507.20474)
Append: [Customize Multi-modal RAI Guardrails with Precedent-based predictions](https://arxiv.org/abs/2507.20503)
Append: [Security Challenges in AI Agent Deployment: Insights from a Large Scale Public Competition](https://arxiv.org/abs/2507.20526)
Append: [Kimi K2: Open Agentic Intelligence](https://arxiv.org/abs/2507.20534)
Append: [The Importance of Facial Features in Vision-based Sign Language Recognition: Eyes, Mouth or Full Face?](https://arxiv.org/abs/2507.20884)
Append: [Enhancing Project-Specific Code Completion by Inferring Internal API Information](https://arxiv.org/abs/2507.20888)
Append: [$A^2R^2$: Advancing Img2LaTeX Conversion via Visual Reasoning with Attention-Guided Refinement](https://arxiv.org/abs/2507.20890)
Append: [Dissecting Persona-Driven Reasoning in Language Models via Activation Patching](https://arxiv.org/abs/2507.20936)
Append: [Your AI, Not Your View: The Bias of LLMs in Investment Analysis](https://arxiv.org/abs/2507.20957)
Append: [LoRA-PAR: A Flexible Dual-System LoRA Partitioning Approach to Efficient LLM Fine-Tuning](https://arxiv.org/abs/2507.20999)
Append: [Cheap Learning: Maximising Performance of Language Models for Social Data Science Using Minimal Data](https://arxiv.org/abs/2401.12295)
Append: [Juru: Legal Brazilian Large Language Model from Reputable Sources](https://arxiv.org/abs/2403.18140)
Append: [Learning to Clarify: Multi-turn Conversations with Action-Based Contrastive Self-Training](https://arxiv.org/abs/2406.00222)
Append: [Language Models Resist Alignment: Evidence From Data Compression](https://arxiv.org/abs/2406.06144)
Append: [DoubleDipper: Improving Long-Context LLMs via Context Recycling](https://arxiv.org/abs/2406.13632)
Append: [The Impact of LoRA Adapters on LLMs for Clinical Text Classification Under Computational and Data Constraints](https://arxiv.org/abs/2407.19299)
Append: [A Practice of Post-Training on Llama-3 70B with Optimal Selection of Additional Language Mixture Ratio](https://arxiv.org/abs/2409.06624)
Append: [MeTHanol: Modularized Thinking Language Models with Intermediate Layer Thinking, Decoding and Bootstrapping Reasoning](https://arxiv.org/abs/2409.12059)
Append: [Real-time Factuality Assessment from Adversarial Feedback](https://arxiv.org/abs/2410.14651)
Append: [Do Large Language Models Have an English Accent? Evaluating and Improving the Naturalness of Multilingual LLMs](https://arxiv.org/abs/2410.15956)
Append: [What is Wrong with Perplexity for Long-context Language Modeling?](https://arxiv.org/abs/2410.23771)
Append: [Summarization of Opinionated Political Documents with Varied Perspectives](https://arxiv.org/abs/2411.04093)
Append: [Benchmarking Linguistic Diversity of Large Language Models](https://arxiv.org/abs/2412.10271)
Append: [Critique of Impure Reason: Unveiling the reasoning behaviour of medical Large Language Models](https://arxiv.org/abs/2412.15748)
Append: [Computational Analysis of Character Development in Holocaust Testimonies](https://arxiv.org/abs/2412.17063)
Append: [Align Attention Heads Before Merging Them: An Effective Way for Converting MHA to GQA](https://arxiv.org/abs/2412.20677)
Append: [FocalPO: Enhancing Preference Optimizing by Focusing on Correct Preference Rankings](https://arxiv.org/abs/2501.06645)
Append: [Large Language Models Are Human-Like Internally](https://arxiv.org/abs/2502.01615)
Append: [LIMO: Less is More for Reasoning](https://arxiv.org/abs/2502.03387)
Append: [Improving Similar Case Retrieval Ranking Performance By Revisiting RankSVM](https://arxiv.org/abs/2502.11131)
Append: [Self-Regularization with Sparse Autoencoders for Controllable LLM-based Classification](https://arxiv.org/abs/2502.14133)
Append: [FactReasoner: A Probabilistic Approach to Long-Form Factuality Assessment for Large Language Models](https://arxiv.org/abs/2502.18573)
Append: [In Prospect and Retrospect: Reflective Memory Management for Long-term Personalized Dialogue Agents](https://arxiv.org/abs/2503.08026)
Append: [Data Caricatures: On the Representation of African American Language in Pretraining Corpora](https://arxiv.org/abs/2503.10789)
Append: [Automating Mathematical Proof Generation Using Large Language Model Agents and Knowledge Graphs](https://arxiv.org/abs/2503.11657)
Append: [Understanding Common Ground Misalignment in Goal-Oriented Dialog: A Case-Study with Ubuntu Chat Logs](https://arxiv.org/abs/2503.12370)
Append: [Enhancing LLM Reasoning with Iterative DPO: A Comprehensive Empirical Investigation](https://arxiv.org/abs/2503.12854)
Append: [TIB-STC: A Large-Scale Structured Tibetan Benchmark for Low-Resource Language Modeling](https://arxiv.org/abs/2503.18288)
Append: [Navigating the Risks of Using Large Language Models for Text Annotation in Social Science Research](https://arxiv.org/abs/2503.22040)
Append: [Scaling Analysis of Interleaved Speech-Text Language Models](https://arxiv.org/abs/2504.02398)
Append: [Language Modeling for the Future of Finance: A Survey into Metrics, Tasks, and Data Opportunities](https://arxiv.org/abs/2504.07274)
Append: [Memorization: A Close Look at Books](https://arxiv.org/abs/2504.12549)
Append: [When Does Metadata Conditioning (NOT) Work for Language Model Pre-Training? A Study with Context-Free Grammars](https://arxiv.org/abs/2504.17562)
Append: [Colombian Waitresses y Jueces canadienses: Gender and Country Biases in Occupation Recommendations from LLMs](https://arxiv.org/abs/2505.02456)
Append: [Measuring Information Distortion in Hierarchical Ultra long Novel Reconstruction:The Optimal Expansion Ratio](https://arxiv.org/abs/2505.12572)
Append: [Accidental Vulnerability: Factors in Fine-Tuning that Shift Model Safeguards](https://arxiv.org/abs/2505.16789)
Append: [Unveil Multi-Picture Descriptions for Multilingual Mild Cognitive Impairment Detection via Contrastive Learning](https://arxiv.org/abs/2505.17067)
Append: [Cog-TiPRO: Iterative Prompt Refinement with LLMs to Detect Cognitive Decline via Longitudinal Voice Assistant Commands](https://arxiv.org/abs/2505.17137)
Append: [Scaling Physical Reasoning with the PHYSICS Dataset](https://arxiv.org/abs/2506.00022)
Append: [Minimal Pair-Based Evaluation of Code-Switching](https://arxiv.org/abs/2506.01840)
Append: [Code-Switching and Syntax: A Large-Scale Experiment](https://arxiv.org/abs/2506.01846)
Append: [Pruning for Performance: Efficient Idiom and Metaphor Classification in Low-Resource Konkani Using mBERT](https://arxiv.org/abs/2506.02005)
Append: [Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning](https://arxiv.org/abs/2506.09853)
Append: [Intersectional Bias in Japanese Large Language Models from a Contextualized Perspective](https://arxiv.org/abs/2506.12327)
Append: [A Structured Bangla Dataset of Disease-Symptom Associations to Improve Diagnostic Accuracy](https://arxiv.org/abs/2506.13610)
Append: [Reinforcement learning fine-tuning of language model for instruction following and math reasoning](https://arxiv.org/abs/2506.21560)
Append: [ChildGuard: A Specialized Dataset for Combatting Child-Targeted Hate Speech](https://arxiv.org/abs/2506.21613)
Append: [From Answers to Rationales: Self-Aligning Multimodal Reasoning with Answer-Oriented Chain-of-Thought](https://arxiv.org/abs/2507.02984)
Append: [Emergent Semantics Beyond Token Embeddings: Transformer LMs with Frozen Visual Unicode Representations](https://arxiv.org/abs/2507.04886)
Append: [Checklist Engineering Empowers Multilingual LLM Judges](https://arxiv.org/abs/2507.06774)
Append: [Multi-Agent Retrieval-Augmented Framework for Evidence-Based Counterspeech Against Health Misinformation](https://arxiv.org/abs/2507.07307)
Append: [Otter: A Multi-Modal Model with In-Context Instruction Tuning](https://arxiv.org/abs/2305.03726)
Append: [LLM2TEA: An Agentic AI Designer for Discovery with Generative Evolutionary Multitasking](https://arxiv.org/abs/2406.14917)
Append: [LLM-Barber: Block-Aware Rebuilder for Sparsity Mask in One-Shot for Large Language Models](https://arxiv.org/abs/2408.10631)
Append: [Everything is a Video: Unifying Modalities through Next-Frame Prediction](https://arxiv.org/abs/2411.10503)
Append: [Benchmarking Open-ended Audio Dialogue Understanding for Large Audio-Language Models](https://arxiv.org/abs/2412.05167)
Append: [Preference learning made easy: Everything should be understood through win rate](https://arxiv.org/abs/2502.10505)
Append: [Protecting Users From Themselves: Safeguarding Contextual Privacy in Interactions with Conversational Agents](https://arxiv.org/abs/2502.18509)
Append: [Robust Data Watermarking in Language Models by Injecting Fictitious Knowledge](https://arxiv.org/abs/2503.04036)
Append: [Explainable Synthetic Image Detection through Diffusion Timestep Ensembling](https://arxiv.org/abs/2503.06201)
Append: [Understanding Learner-LLM Chatbot Interactions and the Impact of Prompting Guidelines](https://arxiv.org/abs/2504.07840)
Append: [AI as a deliberative partner fosters intercultural empathy for Americans but fails for Latin American participants](https://arxiv.org/abs/2504.13887)
Append: [AutoLibra: Agent Metric Induction from Open-Ended Feedback](https://arxiv.org/abs/2505.02820)
Append: [FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis for \"U-Tsang, Amdo and Kham Speech Dataset Generation](https://arxiv.org/abs/2505.14351)
Append: [Benchmarking Graph Neural Networks for Document Layout Analysis in Public Affairs](https://arxiv.org/abs/2505.14699)
Append: [Evaluating the Promise and Pitfalls of LLMs in Hiring Decisions](https://arxiv.org/abs/2507.02087)
Append: [Assemble Your Crew: Automatic Multi-agent Communication Topology Design via Autoregressive Graph Generation](https://arxiv.org/abs/2507.18224)
append_entries: 174
Finish: 2025-07-29 04:50:59.807628
------------------------------------------------------
Started: 2025-07-29 06:28:23.754170
Existing_entries: 1174
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 972
Summarized using GPT-3.5-turbo
Append: [Advancing Large Language Models for Tibetan with Curated Data and Continual Pre-Training](https://arxiv.org/abs/2507.09205)
append_entries: 1
Finish: 2025-07-29 06:28:26.158543
------------------------------------------------------
Started: 2025-07-29 08:25:16.689127
Existing_entries: 1001
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-29 08:25:17.084324
------------------------------------------------------
Started: 2025-07-29 10:19:26.405576
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-29 10:19:26.800418
------------------------------------------------------
Started: 2025-07-29 12:38:58.351122
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-29 12:38:58.745884
------------------------------------------------------
Started: 2025-07-29 14:21:45.622625
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-29 14:21:46.012004
------------------------------------------------------
Started: 2025-07-29 16:22:08.636328
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-29 16:22:09.041888
------------------------------------------------------
Started: 2025-07-29 18:27:38.551083
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-29 18:27:38.955511
------------------------------------------------------
Started: 2025-07-29 20:20:24.752078
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-29 20:20:25.152806
------------------------------------------------------
Started: 2025-07-29 22:17:58.026139
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-29 22:17:58.447304
------------------------------------------------------
Started: 2025-07-30 01:28:02.872741
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-30 01:28:03.298112
------------------------------------------------------
Started: 2025-07-30 03:40:48.723591
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-30 03:40:49.128233
------------------------------------------------------
Started: 2025-07-30 04:44:37.486636
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 765
Summarized using GPT-3.5-turbo
Append: [Categorical Classification of Book Summaries Using Word Embedding Techniques](https://arxiv.org/abs/2507.21058)
Token length: 1917
Summarized using GPT-3.5-turbo
Append: [Dialogic Social Learning for Artificial Agents: Enhancing LLM Ontology Acquisition through Mixed-Initiative Educational Interactions](https://arxiv.org/abs/2507.21065)
Token length: 1806
Summarized using GPT-3.5-turbo
Append: [Product vs. Process: Exploring EFL Students' Editing of AI-Generated Text for Expository Writing](https://arxiv.org/abs/2507.21073)
Token length: 698
Summarized using GPT-3.5-turbo
Append: [Which symbol grounding problem should we try to solve?](https://arxiv.org/abs/2507.21080)
Token length: 1193
Summarized using GPT-3.5-turbo
Append: [ChatGPT Reads Your Tone and Responds Accordingly -- Until It Does Not -- Emotional Framing Induces Bias in LLM Outputs](https://arxiv.org/abs/2507.21083)
Token length: 1373
Summarized using GPT-3.5-turbo
Append: [Reviving Your MNEME: Predicting The Side Effects of LLM Unlearning and Fine-Tuning via Sparse Model Diffing](https://arxiv.org/abs/2507.21084)
Token length: 1366
Summarized using GPT-3.5-turbo
Append: [Multi-Amateur Contrastive Decoding for Text Generation](https://arxiv.org/abs/2507.21086)
Token length: 1679
Summarized using GPT-3.5-turbo
Append: [QU-NLP at CheckThat! 2025: Multilingual Subjectivity in News Articles Detection using Feature-Augmented Transformer Models with Sequential Cross-Lingual Fine-Tuning](https://arxiv.org/abs/2507.21095)
Token length: 1246
Summarized using GPT-3.5-turbo
Append: [Rewrite-to-Rank: Optimizing Ad Visibility via Retrieval-Aware Text Rewriting](https://arxiv.org/abs/2507.21099)
Token length: 1292
Summarized using GPT-3.5-turbo
Append: [iLSU-T: an Open Dataset for Uruguayan Sign Language Translation](https://arxiv.org/abs/2507.21104)
Token length: 1866
Summarized using GPT-3.5-turbo
Append: [Creation of a Numerical Scoring System to Objectively Measure and Compare the Level of Rhetoric in Arabic Texts: A Feasibility Study, and A Working Prototype](https://arxiv.org/abs/2507.21106)
Token length: 1531
Summarized using GPT-3.5-turbo
Append: [Curved Inference: Concern-Sensitive Geometry in Large Language Model Residual Streams](https://arxiv.org/abs/2507.21107)
Token length: 1384
Summarized using GPT-3.5-turbo
Append: [A Survey of Classification Tasks and Approaches for Legal Contracts](https://arxiv.org/abs/2507.21108)
Token length: 1742
Summarized using GPT-3.5-turbo
Append: [SemRAG: Semantic Knowledge-Augmented RAG for Improved Question-Answering](https://arxiv.org/abs/2507.21110)
Token length: 1147
Summarized using GPT-3.5-turbo
Append: [InsurTech innovation using natural language processing](https://arxiv.org/abs/2507.21112)
Token length: 1479
Summarized using GPT-3.5-turbo
Append: [TRIDENT: Benchmarking LLM Safety in Finance, Medicine, and Law](https://arxiv.org/abs/2507.21134)
Token length: 1019
Summarized using GPT-3.5-turbo
Append: [TTS-1 Technical Report](https://arxiv.org/abs/2507.21138)
Token length: 1032
Summarized using GPT-3.5-turbo
Append: [Diverse LLMs or Diverse Question Interpretations? That is the Ensembling Question](https://arxiv.org/abs/2507.21168)
Token length: 1251
Summarized using GPT-3.5-turbo
Append: [Contrast-CAT: Contrasting Activations for Enhanced Interpretability in Transformer-based Text Classifiers](https://arxiv.org/abs/2507.21186)
Token length: 1173
Summarized using GPT-3.5-turbo
Append: [Understanding Public Perception of Crime in Bangladesh: A Transformer-Based Approach with Explainability](https://arxiv.org/abs/2507.21234)
Token length: 1236
Summarized using GPT-3.5-turbo
Append: [Bangla BERT for Hyperpartisan News Detection: A Semi-Supervised and Explainable AI Approach](https://arxiv.org/abs/2507.21242)
Token length: 1862
Summarized using GPT-3.5-turbo
Append: [Can human clinical rationales improve the performance and explainability of clinical text classification models?](https://arxiv.org/abs/2507.21302)
Token length: 1509
Summarized using GPT-3.5-turbo
Append: [Do Large Language Models Understand Morality Across Cultures?](https://arxiv.org/abs/2507.21319)
Token length: 1794
Summarized using GPT-3.5-turbo
Append: [A Deep Learning Automatic Speech Recognition Model for Shona Language](https://arxiv.org/abs/2507.21331)
Token length: 1763
Summarized using GPT-3.5-turbo
Append: [StructText: A Synthetic Table-to-Text Approach for Benchmark Generation with Multi-Dimensional Evaluation](https://arxiv.org/abs/2507.21340)
Token length: 1069
Summarized using GPT-3.5-turbo
Append: [Turbocharging Web Automation: The Impact of Compressed History States](https://arxiv.org/abs/2507.21369)
Token length: 1443
Summarized using GPT-3.5-turbo
Append: [MemTool: Optimizing Short-Term Memory Management for Dynamic Tool Calling in LLM Agent Multi-Turn Conversations](https://arxiv.org/abs/2507.21428)
Token length: 1921
Summarized using GPT-3.5-turbo
Append: [Towards Locally Deployable Fine-Tuned Causal Large Language Models for Mode Choice Behaviour](https://arxiv.org/abs/2507.21432)
Token length: 1567
Summarized using GPT-3.5-turbo
Append: [Which LLMs Get the Joke? Probing Non-STEM Reasoning Abilities with HumorBench](https://arxiv.org/abs/2507.21476)
Token length: 1492
Summarized using GPT-3.5-turbo
Append: [Improving Task Diversity in Label Efficient Supervised Finetuning of LLMs](https://arxiv.org/abs/2507.21482)
Token length: 1412
Summarized using GPT-3.5-turbo
Append: [VN-MTEB: Vietnamese Massive Text Embedding Benchmark](https://arxiv.org/abs/2507.21500)
Token length: 1245
Summarized using GPT-3.5-turbo
Append: [Persona Vectors: Monitoring and Controlling Character Traits in Language Models](https://arxiv.org/abs/2507.21509)
Token length: 1380
Summarized using GPT-3.5-turbo
Append: [Model-free Speculative Decoding for Transformer-based ASR with Token Map Drafting](https://arxiv.org/abs/2507.21522)
Token length: 1134
Summarized using GPT-3.5-turbo
Append: [TriangleMix: A Lossless and Efficient Attention Pattern for Long Context Prefilling](https://arxiv.org/abs/2507.21526)
Token length: 1967
Summarized using GPT-3.5-turbo
Append: [Automatic Classification of User Requirements from Online Feedback -- A Replication Study](https://arxiv.org/abs/2507.21532)
Token length: 1150
Summarized using GPT-3.5-turbo
Append: [Modern Uyghur Dependency Treebank (MUDT): An Integrated Morphosyntactic Framework for a Low-Resource Language](https://arxiv.org/abs/2507.21536)
Token length: 1187
Summarized using GPT-3.5-turbo
Append: [MAGIC: A Multi-Hop and Graph-Based Benchmark for Inter-Context Conflicts in Retrieval-Augmented Generation](https://arxiv.org/abs/2507.21544)
Token length: 1172
Summarized using GPT-3.5-turbo
Append: [Evaluating the cognitive reality of Spanish irregular morphomic patterns: Humans vs. Transformers](https://arxiv.org/abs/2507.21556)
Token length: 1214
Summarized using GPT-3.5-turbo
Append: [Multi-Hypothesis Distillation of Multilingual Neural Translation Models for Low-Resource Languages](https://arxiv.org/abs/2507.21568)
Token length: 1045
Summarized using GPT-3.5-turbo
Append: [Multilingual JobBERT for Cross-Lingual Job Title Matching](https://arxiv.org/abs/2507.21609)
Token length: 1589
Summarized using GPT-3.5-turbo
Append: [Libra: Assessing and Improving Reward Model by Learning to Think](https://arxiv.org/abs/2507.21645)
Token length: 1146
Summarized using GPT-3.5-turbo
Append: [UnsafeChain: Enhancing Reasoning Model Safety via Hard Cases](https://arxiv.org/abs/2507.21652)
Token length: 1423
Summarized using GPT-3.5-turbo
Append: [Adversarial Defence without Adversarial Defence: Enhancing Language Model Robustness via Instance-level Principal Component Removal](https://arxiv.org/abs/2507.21750)
Token length: 1454
Summarized using GPT-3.5-turbo
Append: [AgriEval: A Comprehensive Chinese Agricultural Benchmark for Large Language Models](https://arxiv.org/abs/2507.21773)
Token length: 1093
Summarized using GPT-3.5-turbo
Append: [The Problem with Safety Classification is not just the Models](https://arxiv.org/abs/2507.21782)
Token length: 647
Summarized using GPT-3.5-turbo
Append: [ChartMark: A Structured Grammar for Chart Annotation](https://arxiv.org/abs/2507.21810)
Token length: 638
Summarized using GPT-3.5-turbo
Append: [Overview of ADoBo at IberLEF 2025: Automatic Detection of Anglicisms in Spanish](https://arxiv.org/abs/2507.21813)
Token length: 1320
Summarized using GPT-3.5-turbo
Append: [HRIPBench: Benchmarking LLMs in Harm Reduction Information Provision to Support People Who Use Drugs](https://arxiv.org/abs/2507.21815)
Token length: 1234
Summarized using GPT-3.5-turbo
Append: [Modelling Adjectival Modification Effects on Semantic Plausibility](https://arxiv.org/abs/2507.21828)
Token length: 1379
Summarized using GPT-3.5-turbo
Append: [Introducing HALC: A general pipeline for finding optimal prompting strategies for automated coding with LLMs in the computational social sciences](https://arxiv.org/abs/2507.21831)
Append: [AutoTIR: Autonomous Tools Integrated Reasoning via Reinforcement Learning](https://arxiv.org/abs/2507.21836)
Append: [Graph-R1: Towards Agentic GraphRAG Framework via End-to-end Reinforcement Learning](https://arxiv.org/abs/2507.21892)
Append: [Rote Learning Considered Useful: Generalizing over Memorized Data in LLMs](https://arxiv.org/abs/2507.21914)
Append: [Training language models to be warm and empathetic makes them less reliable and more sycophantic](https://arxiv.org/abs/2507.21919)
Append: [Post-Training Large Language Models via Reinforcement Learning from Self-Feedback](https://arxiv.org/abs/2507.21931)
Append: [Culinary Crossroads: A RAG Framework for Enhancing Diversity in Cross-Cultural Recipe Adaptation](https://arxiv.org/abs/2507.21934)
Append: [Predicting Microbial Ontology and Pathogen Risk from Environmental Metadata with Large Language Models](https://arxiv.org/abs/2507.21980)
Append: [DeepSieve: Information Sieving via LLM-as-a-Knowledge-Router](https://arxiv.org/abs/2507.22050)
Append: [R-Stitch: Dynamic Trajectory Stitching for Efficient Reasoning](https://arxiv.org/abs/2507.17307)
Append: [Can LLMs Reason About Trust?: A Pilot Study](https://arxiv.org/abs/2507.21075)
Append: [Emotionally Aware Moderation: The Potential of Emotion Monitoring in Shaping Healthier Social Media Conversations](https://arxiv.org/abs/2507.21089)
Append: [Analise Semantica Automatizada com LLM e RAG para Bulas Farmaceuticas](https://arxiv.org/abs/2507.21103)
Append: [AgentMaster: A Multi-Agent Conversational Framework Using A2A and MCP Protocols for Multimodal Information Retrieval and Analysis](https://arxiv.org/abs/2507.21105)
Append: [OneShield -- the Next Generation of LLM Guardrails](https://arxiv.org/abs/2507.21170)
Append: [MaPPO: Maximum a Posteriori Preference Optimization with Prior Knowledge](https://arxiv.org/abs/2507.21183)
Append: [EvoSLD: Automated Neural Scaling Law Discovery With Large Language Models](https://arxiv.org/abs/2507.21184)
Append: [CompoST: A Benchmark for Analyzing the Ability of LLMs To Compositionally Interpret Questions in a QALD Setting](https://arxiv.org/abs/2507.21257)
Append: [LeMix: Unified Scheduling for LLM Training and Inference on Multi-GPU Systems](https://arxiv.org/abs/2507.21276)
Append: [Teaching Language Models To Gather Information Proactively](https://arxiv.org/abs/2507.21389)
Append: [Multimodal LLMs as Customized Reward Models for Text-to-Image Generation](https://arxiv.org/abs/2507.21391)
Append: [ReGATE: Learning Faster and Better with Fewer Tokens in MLLMs](https://arxiv.org/abs/2507.21420)
Append: [What Does it Mean for a Neural Network to Learn a "World Model"?](https://arxiv.org/abs/2507.21513)
Append: [Who's important? -- SUnSET: Synergistic Understanding of Stakeholder, Events and Time for Timeline Generation](https://arxiv.org/abs/2507.21903)
Append: [UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and Precise Inference-Time Grounding](https://arxiv.org/abs/2507.22025)
Append: [UserBench: An Interactive Gym Environment for User-Centric Agents](https://arxiv.org/abs/2507.22034)
Append: [MetaCLIP 2: A Worldwide Scaling Recipe](https://arxiv.org/abs/2507.22062)
Append: [The pitfalls of next-token prediction](https://arxiv.org/abs/2403.06963)
Append: [Task Arithmetic for Language Expansion in Speech Translation](https://arxiv.org/abs/2409.11274)
Append: [Simulated patient systems are intelligent when powered by large language model-based AI agents](https://arxiv.org/abs/2409.18924)
Append: [BIG5-CHAT: Shaping LLM Personalities Through Training on Human-Grounded Data](https://arxiv.org/abs/2410.16491)
Append: [Pralekha: Cross-Lingual Document Alignment for Indic Languages](https://arxiv.org/abs/2411.19096)
Append: [Low-Confidence Gold: Refining Low-Confidence Samples for Efficient Instruction Tuning](https://arxiv.org/abs/2502.18978)
Append: [Narrative Context Protocol: An Open-Source Storytelling Framework for Generative AI](https://arxiv.org/abs/2503.04844)
Append: [Training LLM-based Tutors to Improve Student Learning Outcomes in Dialogues](https://arxiv.org/abs/2503.06424)
Append: [Levels of Analysis for Large Language Models](https://arxiv.org/abs/2503.13401)
Append: [EEG-CLIP : Learning EEG representations from natural language descriptions](https://arxiv.org/abs/2503.16531)
Append: ["Whose Side Are You On?" Estimating Ideology of Political and News Content Using Large Language Models and Few-shot Demonstration Selection](https://arxiv.org/abs/2503.20797)
Append: [Beyond the Reported Cutoff: Where Large Language Models Fall Short on Financial Knowledge](https://arxiv.org/abs/2504.00042)
Append: [My Life in Artificial Intelligence: People, anecdotes, and some lessons learnt](https://arxiv.org/abs/2504.04142)
Append: [Probing then Editing Response Personality of Large Language Models](https://arxiv.org/abs/2504.10227)
Append: [Ai2 Scholar QA: Organized Literature Synthesis with Attribution](https://arxiv.org/abs/2504.10861)
Append: [FB-RAG: Improving RAG with Forward and Backward Lookup](https://arxiv.org/abs/2505.17206)
Append: [CHIMERA: A Knowledge Base of Scientific Idea Recombinations for Research Analysis and Ideation](https://arxiv.org/abs/2505.20779)
Append: [FLAT-LLM: Fine-grained Low-rank Activation Space Transformation for Large Language Model Compression](https://arxiv.org/abs/2505.23966)
Append: [SmoothRot: Combining Channel-Wise Scaling and Rotation for Quantization-Friendly LLMs](https://arxiv.org/abs/2506.05413)
Append: [HIRAG: Hierarchical-Thought Instruction-Tuning Retrieval-Augmented Generation](https://arxiv.org/abs/2507.05714)
Append: [FrugalRAG: Learning to retrieve and reason for multi-hop QA](https://arxiv.org/abs/2507.07634)
Append: [Sparse Autoencoders Can Capture Language-Specific Concepts Across Diverse Languages](https://arxiv.org/abs/2507.11230)
Append: [Image Captioning via Compact Bidirectional Architecture](https://arxiv.org/abs/2201.01984)
Append: [Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in LLMs](https://arxiv.org/abs/2407.15549)
Append: [Strategist: Self-improvement of LLM Decision Making via Bi-Level Tree Search](https://arxiv.org/abs/2408.10635)
Append: [Signs as Tokens: A Retrieval-Enhanced Multilingual Sign Language Generator](https://arxiv.org/abs/2411.17799)
Append: [AIM: Adaptive Inference of Multi-Modal LLMs via Token Merging and Pruning](https://arxiv.org/abs/2412.03248)
Append: [SAKE: Steering Activations for Knowledge Editing](https://arxiv.org/abs/2503.01751)
Append: [SQuat: Subspace-orthogonal KV Cache Quantization](https://arxiv.org/abs/2503.24358)
Append: [LLAMAPIE: Proactive In-Ear Conversation Assistants](https://arxiv.org/abs/2505.04066)
Append: [Mining Intrinsic Rewards from LLM Hidden States for Efficient Best-of-N Sampling](https://arxiv.org/abs/2505.12225)
Append: [Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach](https://arxiv.org/abs/2505.14479)
Append: [Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models](https://arxiv.org/abs/2506.01413)
Append: [FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation](https://arxiv.org/abs/2506.09081)
Append: [SLR: Automated Synthesis for Scalable Logical Reasoning](https://arxiv.org/abs/2506.15787)
Append: [A Detailed Factor Analysis for the Political Compass Test: Navigating Ideologies of Large Language Models](https://arxiv.org/abs/2506.22493)
Append: [Audio Flamingo 3: Advancing Audio Intelligence with Fully Open Large Audio Language Models](https://arxiv.org/abs/2507.08128)
append_entries: 113
Finish: 2025-07-30 04:46:39.357451
------------------------------------------------------
Started: 2025-07-30 06:29:36.158965
Existing_entries: 1113
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-30 06:29:36.434249
------------------------------------------------------
Started: 2025-07-30 08:25:12.773208
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-30 08:25:13.132438
------------------------------------------------------
Started: 2025-07-30 10:19:59.947521
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-30 10:20:00.230717
------------------------------------------------------
Started: 2025-07-30 12:38:47.166589
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-30 12:38:47.480276
------------------------------------------------------
Started: 2025-07-30 14:18:34.892976
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-30 14:18:35.257777
------------------------------------------------------
Started: 2025-07-30 16:23:05.753892
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-30 16:23:06.036883
------------------------------------------------------
Started: 2025-07-30 18:26:57.401636
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-30 18:26:57.679581
------------------------------------------------------
Started: 2025-07-30 20:18:43.697008
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-30 20:18:43.995204
------------------------------------------------------
Started: 2025-07-30 22:17:42.186815
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-30 22:17:42.473238
------------------------------------------------------
Started: 2025-07-31 01:27:30.755699
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-31 01:27:31.096853
------------------------------------------------------
Started: 2025-07-31 03:39:27.819949
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-31 03:39:28.097937
------------------------------------------------------
Started: 2025-07-31 04:43:06.668809
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 813
Summarized using GPT-3.5-turbo
Append: [IndoPref: A Multi-Domain Pairwise Preference Dataset for Indonesian](https://arxiv.org/abs/2507.22159)
Token length: 1194
Summarized using GPT-3.5-turbo
Append: [Persona-Augmented Benchmarking: Evaluating LLMs Across Diverse Writing Styles](https://arxiv.org/abs/2507.22168)
Token length: 1331
Summarized using GPT-3.5-turbo
Append: [A Scalable Pipeline for Estimating Verb Frame Frequencies Using Large Language Models](https://arxiv.org/abs/2507.22187)
Token length: 1154
Summarized using GPT-3.5-turbo
Append: [The role of media memorability in facilitating startups' access to venture capital funding](https://arxiv.org/abs/2507.22201)
Token length: 862
Summarized using GPT-3.5-turbo
Append: [How Well Does First-Token Entropy Approximate Word Entropy as a Psycholinguistic Predictor?](https://arxiv.org/abs/2507.22209)
Token length: 1284
Summarized using GPT-3.5-turbo
Append: [RL from Teacher-Model Refinement: Gradual Imitation Learning for Machine Translation](https://arxiv.org/abs/2507.22219)
Token length: 1206
Summarized using GPT-3.5-turbo
Append: [Meaning-infused grammar: Gradient Acceptability Shapes the Geometric Representations of Constructions in LLMs](https://arxiv.org/abs/2507.22286)
Token length: 744
Summarized using GPT-3.5-turbo
Append: [Intent Recognition and Out-of-Scope Detection using LLMs in Multi-party Conversations](https://arxiv.org/abs/2507.22289)
Token length: 1209
Summarized using GPT-3.5-turbo
Append: [A Comprehensive Taxonomy of Negation for NLP and Neural Retrievers](https://arxiv.org/abs/2507.22337)
Token length: 1941
Summarized using GPT-3.5-turbo
Append: [Traits Run Deep: Enhancing Personality Assessment via Psychology-Guided LLM Representations and Multimodal Apparent Behaviors](https://arxiv.org/abs/2507.22367)
Token length: 1297
Summarized using GPT-3.5-turbo
Append: [PATENTWRITER: A Benchmarking Study for Patent Drafting with LLMs](https://arxiv.org/abs/2507.22387)
Token length: 780
Summarized using GPT-3.5-turbo
Append: [Question Generation for Assessing Early Literacy Reading Comprehension](https://arxiv.org/abs/2507.22410)
Token length: 1338
Summarized using GPT-3.5-turbo
Append: [NeedleChain: Measuring Intact Long-Context Reasoning Capability of Large Language Models](https://arxiv.org/abs/2507.22411)
Token length: 1322
Summarized using GPT-3.5-turbo
Append: [AI-generated stories favour stability over change: homogeneity and cultural stereotyping in narratives generated by gpt-4o-mini](https://arxiv.org/abs/2507.22445)
Token length: 1715
Summarized using GPT-3.5-turbo
Append: [Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance](https://arxiv.org/abs/2507.22448)
Token length: 735
Summarized using GPT-3.5-turbo
Append: [What is an "Abstract Reasoner"? Revisiting Experiments and Arguments about Large Language Models](https://arxiv.org/abs/2507.22457)
Token length: 1299
Summarized using GPT-3.5-turbo
Append: [IFEvalCode: Controlled Code Generation](https://arxiv.org/abs/2507.22462)
Token length: 1323
Summarized using GPT-3.5-turbo
Append: [SLM-SQL: An Exploration of Small Language Models for Text-to-SQL](https://arxiv.org/abs/2507.22478)
Token length: 1868
Summarized using GPT-3.5-turbo
Append: [CliCARE: Grounding Large Language Models in Clinical Guidelines for Decision Support over Longitudinal Cancer Electronic Health Records](https://arxiv.org/abs/2507.22533)
Token length: 1721
Summarized using GPT-3.5-turbo
Append: [A Benchmark Dataset and Evaluation Framework for Vietnamese Large Language Models in Customer Support](https://arxiv.org/abs/2507.22542)
Token length: 1608
Summarized using GPT-3.5-turbo
Append: [ControlMed: Adding Reasoning Control to Medical Language Model](https://arxiv.org/abs/2507.22545)
Token length: 1415
Summarized using GPT-3.5-turbo
Append: [Exploiting Synergistic Cognitive Biases to Bypass Safety in LLMs](https://arxiv.org/abs/2507.22564)
Token length: 1217
Summarized using GPT-3.5-turbo
Append: [Unveiling the Influence of Amplifying Language-Specific Neurons](https://arxiv.org/abs/2507.22581)
Token length: 1104
Summarized using GPT-3.5-turbo
Append: [BALSAM: A Platform for Benchmarking Arabic Large Language Models](https://arxiv.org/abs/2507.22603)
Token length: 1423
Summarized using GPT-3.5-turbo
Append: [Language Arithmetics: Towards Systematic Language Neuron Identification and Manipulation](https://arxiv.org/abs/2507.22608)
Token length: 1476
Summarized using GPT-3.5-turbo
Append: [Multilingual Political Views of Large Language Models: Identification and Steering](https://arxiv.org/abs/2507.22623)
Token length: 1502
Summarized using GPT-3.5-turbo
Append: [Listening to the Unspoken: Exploring 365 Aspects of Multimodal Interview Performance Assessment](https://arxiv.org/abs/2507.22676)
Token length: 1387
Summarized using GPT-3.5-turbo
Append: [From Sufficiency to Reflection: Reinforcement-Guided Thinking Quality in Retrieval-Augmented Reasoning for LLMs](https://arxiv.org/abs/2507.22716)
Token length: 881
Summarized using GPT-3.5-turbo
Append: [Investigating Hallucination in Conversations for Low Resource Languages](https://arxiv.org/abs/2507.22720)
Token length: 1343
Summarized using GPT-3.5-turbo
Append: [Resource-Efficient Adaptation of Large Language Models for Text Embeddings via Prompt Engineering and Contrastive Fine-tuning](https://arxiv.org/abs/2507.22729)
Token length: 1227
Summarized using GPT-3.5-turbo
Append: [Reducing Hallucinations in Summarization via Reinforcement Learning with Entity Hallucination Index](https://arxiv.org/abs/2507.22744)
Token length: 1211
Summarized using GPT-3.5-turbo
Append: [CUS-QA: Local-Knowledge-Oriented Open-Ended Question Answering Dataset](https://arxiv.org/abs/2507.22752)
Token length: 762
Summarized using GPT-3.5-turbo
Append: [Opportunities and Challenges of LLMs in Education: An NLP Perspective](https://arxiv.org/abs/2507.22753)
Token length: 1185
Summarized using GPT-3.5-turbo
Append: [MASCA: LLM based-Multi Agents System for Credit Assessment](https://arxiv.org/abs/2507.22758)
Token length: 596
Summarized using GPT-3.5-turbo
Append: [DBLPLink 2.0 -- An Entity Linker for the DBLP Scholarly Knowledge Graph](https://arxiv.org/abs/2507.22811)
Token length: 1176
Summarized using GPT-3.5-turbo
Append: [Beyond Natural Language Plans: Structure-Aware Planning for Query-Focused Table Summarization](https://arxiv.org/abs/2507.22829)
Token length: 1521
Summarized using GPT-3.5-turbo
Append: [Where to show Demos in Your Prompt: A Positional Bias of In-Context Learning](https://arxiv.org/abs/2507.22887)
Token length: 1248
Summarized using GPT-3.5-turbo
Append: [CIMR: Contextualized Iterative Multimodal Reasoning for Robust Instruction Following in LVLMs](https://arxiv.org/abs/2507.22074)
Token length: 1347
Summarized using GPT-3.5-turbo
Append: [CodeEvo: Interaction-Driven Synthesis of Code-centric Data through Hybrid and Iterative Feedback](https://arxiv.org/abs/2507.22080)
Token length: 890
Summarized using GPT-3.5-turbo
Append: [Prompt Optimization and Evaluation for LLM Automated Red Teaming](https://arxiv.org/abs/2507.22133)
Token length: 1040
Summarized using GPT-3.5-turbo
Append: [Strategic Deflection: Defending LLMs from Logit Manipulation](https://arxiv.org/abs/2507.22160)
Token length: 1719
Summarized using GPT-3.5-turbo
Append: [Explainability Through Systematicity: The Hard Systematicity Challenge for Artificial Intelligence](https://arxiv.org/abs/2507.22197)
Token length: 1240
Summarized using GPT-3.5-turbo
Append: [CoEx -- Co-evolving World-model and Exploration](https://arxiv.org/abs/2507.22281)
Token length: 1352
Summarized using GPT-3.5-turbo
Append: [LLM-Crowdsourced: A Benchmark-Free Paradigm for Mutual Evaluation of Large Language Models](https://arxiv.org/abs/2507.22359)
Token length: 1011
Summarized using GPT-3.5-turbo
Append: [Pre-trained Models Perform the Best When Token Distributions Follow Zipf's Law](https://arxiv.org/abs/2507.22543)
Token length: 1858
Summarized using GPT-3.5-turbo
Append: [Efficient Differentially Private Fine-Tuning of LLMs via Reinforcement Learning](https://arxiv.org/abs/2507.22565)
Token length: 1497
Summarized using GPT-3.5-turbo
Append: [VL-Cogito: Progressive Curriculum Reinforcement Learning for Advanced Multimodal Reasoning](https://arxiv.org/abs/2507.22607)
Token length: 1323
Summarized using GPT-3.5-turbo
Append: [Next Tokens Denoising for Speech Synthesis](https://arxiv.org/abs/2507.22746)
Token length: 1084
Summarized using GPT-3.5-turbo
Append: [The Incomplete Bridge: How AI Research (Mis)Engages with Psychology](https://arxiv.org/abs/2507.22847)
Token length: 1741
Summarized using GPT-3.5-turbo
Append: [GeoOutageKG: A Multimodal Geospatiotemporal Knowledge Graph for Multiresolution Power Outage Analysis](https://arxiv.org/abs/2507.22878)
Append: [RecGPT Technical Report](https://arxiv.org/abs/2507.22879)
Append: [Towards the Law of Capacity Gap in Distilling Language Models](https://arxiv.org/abs/2311.07052)
Append: [Instruction-tuned Large Language Models for Machine Translation in the Medical Domain](https://arxiv.org/abs/2408.16440)
Append: [Past Meets Present: Creating Historical Analogy with Large Language Models](https://arxiv.org/abs/2409.14820)
Append: [Neutral Residues: Revisiting Adapters for Model Extension](https://arxiv.org/abs/2410.02744)
Append: [Natural Language Processing for the Legal Domain: A Survey of Tasks, Datasets, Models, and Challenges](https://arxiv.org/abs/2410.21306)
Append: [Yankari: A Monolingual Yoruba Dataset](https://arxiv.org/abs/2412.03334)
Append: [Efficient Continual Learning for Small Language Models with a Discrete Key-Value Bottleneck](https://arxiv.org/abs/2412.08528)
Append: [ECG-Byte: A Tokenizer for End-to-End Generative Electrocardiogram Language Modeling](https://arxiv.org/abs/2412.14373)
Append: [Modeling Story Expectations to Understand Engagement: A Generative Framework Using LLMs](https://arxiv.org/abs/2412.15239)
Append: [Rationale-guided Prompting for Knowledge-based Visual Question Answering](https://arxiv.org/abs/2412.16936)
Append: [FineMedLM-o1: Enhancing Medical Knowledge Reasoning Ability of LLM from Supervised Fine-Tuning to Test-Time Training](https://arxiv.org/abs/2501.09213)
Append: [GneissWeb: Preparing High Quality Data for LLMs at Scale](https://arxiv.org/abs/2502.14907)
Append: [QE4PE: Word-level Quality Estimation for Human Post-Editing](https://arxiv.org/abs/2503.03044)
Append: [Cross-Modal State-Space Graph Reasoning for Structured Summarization](https://arxiv.org/abs/2503.20988)
Append: [Prompt-Reverse Inconsistency: LLM Self-Inconsistency Beyond Generative Randomness and Prompt Paraphrasing](https://arxiv.org/abs/2504.01282)
Append: [Voices of Freelance Professional Writers on AI: Limitations, Expectations, and Fears](https://arxiv.org/abs/2504.05008)
Append: [D\'ej\`a Vu: Multilingual LLM Evaluation through the Lens of Machine Translation Evaluation](https://arxiv.org/abs/2504.11829)
Append: [BERSting at the Screams: A Benchmark for Distanced, Emotional and Shouted Speech Recognition](https://arxiv.org/abs/2505.00059)
Append: [IterKey: Iterative Keyword Generation with LLMs for Enhanced Retrieval Augmented Generation](https://arxiv.org/abs/2505.08450)
Append: [What Are They Talking About? A Benchmark of Knowledge-Grounded Discussion Summarization](https://arxiv.org/abs/2505.12474)
Append: [Denoising Concept Vectors with Sparse Autoencoders for Improved Language Model Steering](https://arxiv.org/abs/2505.15038)
Append: [MiniLongBench: The Low-cost Long Context Understanding Benchmark for Large Language Models](https://arxiv.org/abs/2505.19959)
Append: [Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning](https://arxiv.org/abs/2505.21354)
Append: [MuSciClaims: Multimodal Scientific Claim Verification](https://arxiv.org/abs/2506.04585)
Append: [LLM-as-a-qualitative-judge: automating error analysis in natural language generation](https://arxiv.org/abs/2506.09147)
Append: [MFTCXplain: A Multilingual Benchmark Dataset for Evaluating the Moral Reasoning of LLMs through Hate Speech Multi-hop Explanations](https://arxiv.org/abs/2506.19073)
Append: [Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions](https://arxiv.org/abs/2404.07214)
Append: [Mmm whatcha say? Uncovering distal and proximal context effects in first and second-language word perception using psychophysical reverse correlation](https://arxiv.org/abs/2406.05515)
Append: [Positive-Augmented Contrastive Learning for Vision-and-Language Evaluation and Training](https://arxiv.org/abs/2410.07336)
Append: [Can adversarial attacks by large language models be attributed?](https://arxiv.org/abs/2411.08003)
Append: [Scoring Verifiers: Evaluating Synthetic Verification for Code and Reasoning](https://arxiv.org/abs/2502.13820)
Append: [OWLViz: An Open-World Benchmark for Visual Question Answering](https://arxiv.org/abs/2503.07631)
Append: [ReverBERT: A State Space Model for Efficient Text-Driven Speech Style Transfer](https://arxiv.org/abs/2503.20992)
Append: [UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction Synthesis](https://arxiv.org/abs/2504.11257)
Append: [Enhancing Ultra-Low-Bit Quantization of Large Language Models Through Saliency-Aware Partial Retraining](https://arxiv.org/abs/2504.13932)
Append: [Co-AttenDWG: Co-Attentive Dimension-Wise Gating and Expert Fusion for Multi-Modal Offensive Content Detection](https://arxiv.org/abs/2505.19010)
Append: [Masked Language Models are Good Heterogeneous Graph Generalizers](https://arxiv.org/abs/2506.06157)
Append: [Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence](https://arxiv.org/abs/2506.15677)
Append: [SpatialViz-Bench: Automatically Generated Spatial Visualization Reasoning Tasks for MLLMs](https://arxiv.org/abs/2507.07610)
Append: [BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity](https://arxiv.org/abs/2507.08771)
append_entries: 91
Finish: 2025-07-31 04:44:54.984193
------------------------------------------------------
Started: 2025-07-31 06:28:14.604965
Existing_entries: 1091
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-31 06:28:14.845581
------------------------------------------------------
Started: 2025-07-31 08:25:32.494034
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-31 08:25:32.762336
------------------------------------------------------
Started: 2025-07-31 10:19:37.001977
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-31 10:19:37.274284
------------------------------------------------------
Started: 2025-07-31 12:36:39.312969
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-31 12:36:39.586595
------------------------------------------------------
Started: 2025-07-31 14:19:39.370329
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-31 14:19:39.695449
------------------------------------------------------
Started: 2025-07-31 16:22:30.469803
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-31 16:22:30.712172
------------------------------------------------------
Started: 2025-07-31 18:27:18.597989
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-31 18:27:18.864359
------------------------------------------------------
Started: 2025-07-31 20:20:02.900390
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-31 20:20:03.142466
------------------------------------------------------
Started: 2025-07-31 22:17:48.985464
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-07-31 22:17:49.268261
------------------------------------------------------
Started: 2025-08-01 01:46:08.637489
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-01 01:46:08.936722
------------------------------------------------------
Started: 2025-08-01 03:53:19.397765
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-01 03:53:19.641521
------------------------------------------------------
Started: 2025-08-01 04:55:51.634349
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1490
Summarized using GPT-3.5-turbo
Append: [Large Language Models in the Travel Domain: An Industrial Experience](https://arxiv.org/abs/2507.22910)
Token length: 1245
Summarized using GPT-3.5-turbo
Append: [ElectriQ: A Benchmark for Assessing the Response Capability of Large Language Models in Power Marketing](https://arxiv.org/abs/2507.22911)
Token length: 1940
Summarized using GPT-3.5-turbo
Append: [A Language Model-Driven Semi-Supervised Ensemble Framework for Illicit Market Detection Across Deep/Dark Web and Social Platforms](https://arxiv.org/abs/2507.22912)
Token length: 1122
Summarized using GPT-3.5-turbo
Append: [A Hybrid Framework for Subject Analysis: Integrating Embedding-Based Regression Models with Large Language Models](https://arxiv.org/abs/2507.22913)
Token length: 1387
Summarized using GPT-3.5-turbo
Append: [Full Triple Matcher: Integrating all triple elements between heterogeneous Knowledge Graphs](https://arxiv.org/abs/2507.22914)
Token length: 1277
Summarized using GPT-3.5-turbo
Append: [Theoretical Foundations and Mitigation of Hallucination in Large Language Models](https://arxiv.org/abs/2507.22915)
Token length: 1553
Summarized using GPT-3.5-turbo
Append: [Reading Between the Timelines: RAG for Answering Diachronic Questions](https://arxiv.org/abs/2507.22917)
Token length: 996
Summarized using GPT-3.5-turbo
Append: [Semantic Convergence: Investigating Shared Representations Across Scaled LLMs](https://arxiv.org/abs/2507.22918)
Token length: 1890
Summarized using GPT-3.5-turbo
Append: [A novel language model for predicting serious adverse event results in clinical trials from their prospective registrations](https://arxiv.org/abs/2507.22919)
Token length: 1819
Summarized using GPT-3.5-turbo
Append: [Discrete Tokenization for Multimodal LLMs: A Comprehensive Survey](https://arxiv.org/abs/2507.22920)
Token length: 1290
Summarized using GPT-3.5-turbo
Append: [Fast and Accurate Contextual Knowledge Extraction Using Cascading Language Model Chains and Candidate Answers](https://arxiv.org/abs/2507.22921)
Token length: 1259
Summarized using GPT-3.5-turbo
Append: [Predicting stock prices with ChatGPT-annotated Reddit sentiment](https://arxiv.org/abs/2507.22922)
Token length: 1233
Summarized using GPT-3.5-turbo
Append: [How and Where to Translate? The Impact of Translation Strategies in Cross-lingual LLM Prompting](https://arxiv.org/abs/2507.22923)
Token length: 1170
Summarized using GPT-3.5-turbo
Append: [Using Sentiment Analysis to Investigate Peer Feedback by Native and Non-Native English Speakers](https://arxiv.org/abs/2507.22924)
Token length: 1340
Summarized using GPT-3.5-turbo
Append: [Hierarchical Memory for High-Efficiency Long-Term Reasoning in LLM Agents](https://arxiv.org/abs/2507.22925)
Token length: 1583
Summarized using GPT-3.5-turbo
Append: [Multi-Relation Extraction in Entity Pairs using Global Context](https://arxiv.org/abs/2507.22926)
Token length: 1334
Summarized using GPT-3.5-turbo
Append: [PRGB Benchmark: A Robust Placeholder-Assisted Algorithm for Benchmarking Retrieval-Augmented Generation](https://arxiv.org/abs/2507.22927)
Token length: 1288
Summarized using GPT-3.5-turbo
Append: [How does Chain of Thought Think? Mechanistic Interpretability of Chain-of-Thought Reasoning with Sparse Autoencoding](https://arxiv.org/abs/2507.22928)
Token length: 1459
Summarized using GPT-3.5-turbo
Append: [EH-Benchmark Ophthalmic Hallucination Benchmark and Agent-Driven Top-Down Traceable Reasoning Workflow](https://arxiv.org/abs/2507.22929)
Token length: 1809
Summarized using GPT-3.5-turbo
Append: [Protecting Vulnerable Voices: Synthetic Dataset Generation for Self-Disclosure Detection](https://arxiv.org/abs/2507.22930)
Token length: 947
Summarized using GPT-3.5-turbo
Append: [Enhancing RAG Efficiency with Adaptive Context Compression](https://arxiv.org/abs/2507.22931)
Token length: 841
Summarized using GPT-3.5-turbo
Append: [FinMarBa: A Market-Informed Dataset for Financial Sentiment Classification](https://arxiv.org/abs/2507.22932)
Token length: 1209
Summarized using GPT-3.5-turbo
Append: [Augmented Vision-Language Models: A Systematic Review](https://arxiv.org/abs/2507.22933)
Token length: 819
Summarized using GPT-3.5-turbo
Append: [Deep Learning Approaches for Multimodal Intent Recognition: A Survey](https://arxiv.org/abs/2507.22934)
Token length: 1562
Summarized using GPT-3.5-turbo
Append: [Trusted Knowledge Extraction for Operations and Maintenance Intelligence](https://arxiv.org/abs/2507.22935)
Token length: 1239
Summarized using GPT-3.5-turbo
Append: [Evaluating Large Language Models (LLMs) in Financial NLP: A Comparative Study on Financial Report Analysis](https://arxiv.org/abs/2507.22936)
Token length: 1479
Summarized using GPT-3.5-turbo
Append: [CoE-Ops: Collaboration of LLM-based Experts for AIOps Question-Answering](https://arxiv.org/abs/2507.22937)
Token length: 1341
Summarized using GPT-3.5-turbo
Append: [A Graph-based Approach for Multi-Modal Question Answering from Flowcharts in Telecom Documents](https://arxiv.org/abs/2507.22938)
Token length: 1773
Summarized using GPT-3.5-turbo
Append: [PARROT: An Open Multilingual Radiology Reports Dataset](https://arxiv.org/abs/2507.22939)
Token length: 1792
Summarized using GPT-3.5-turbo
Append: [Trustworthy Reasoning: Evaluating and Enhancing Factual Accuracy in LLM Intermediate Thought Processes](https://arxiv.org/abs/2507.22940)
Token length: 1299
Summarized using GPT-3.5-turbo
Append: [SigBERT: Combining Narrative Medical Reports and Rough Path Signature Theory for Survival Risk Estimation in Oncology](https://arxiv.org/abs/2507.22941)
Token length: 1765
Summarized using GPT-3.5-turbo
Append: [A chart review process aided by natural language processing and multi-wave adaptive sampling to expedite validation of code-based algorithms for large database studies](https://arxiv.org/abs/2507.22943)
Token length: 1709
Summarized using GPT-3.5-turbo
Append: [Opacity as Authority: Arbitrariness and the Preclusion of Contestation](https://arxiv.org/abs/2507.22944)
Token length: 1333
Summarized using GPT-3.5-turbo
Append: [C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring Challenges in Complex Conversations](https://arxiv.org/abs/2507.22968)
Token length: 1191
Summarized using GPT-3.5-turbo
Append: [Math Natural Language Inference: this should be easy!](https://arxiv.org/abs/2507.23063)
Token length: 912
Summarized using GPT-3.5-turbo
Append: [Exploring In-Context Learning for Frame-Semantic Parsing](https://arxiv.org/abs/2507.23082)
Token length: 1565
Summarized using GPT-3.5-turbo
Append: [Context-aware Rotary Position Embedding](https://arxiv.org/abs/2507.23083)
Token length: 941
Summarized using GPT-3.5-turbo
Append: [SMART-Editor: A Multi-Agent Framework for Human-Like Design Editing with Structural Integrity](https://arxiv.org/abs/2507.23095)
Token length: 1201
Summarized using GPT-3.5-turbo
Append: [RASL: Retrieval Augmented Schema Linking for Massive Database Text-to-SQL](https://arxiv.org/abs/2507.23104)
Token length: 1388
Summarized using GPT-3.5-turbo
Append: [Uncovering the Fragility of Trustworthy LLMs through Chinese Textual Ambiguity](https://arxiv.org/abs/2507.23121)
Token length: 844
Summarized using GPT-3.5-turbo
Append: [ISO-Bench: Benchmarking Multimodal Causal Reasoning in Visual-Language Models through Procedural Plans](https://arxiv.org/abs/2507.23135)
Token length: 1116
Summarized using GPT-3.5-turbo
Append: [User Feedback in Human-LLM Dialogues: A Lens to Understand Users But Noisy as a Learning Signal](https://arxiv.org/abs/2507.23158)
Token length: 1385
Summarized using GPT-3.5-turbo
Append: [LENS: Learning Ensemble Confidence from Neural States for Multi-LLM Answer Integration](https://arxiv.org/abs/2507.23167)
Token length: 1659
Summarized using GPT-3.5-turbo
Append: [Geak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks](https://arxiv.org/abs/2507.23194)
Token length: 1525
Summarized using GPT-3.5-turbo
Append: [Failures Are the Stepping Stones to Success: Enhancing Few-Shot In-Context Learning by Leveraging Negative Samples](https://arxiv.org/abs/2507.23211)
Token length: 1182
Summarized using GPT-3.5-turbo
Append: [Model Directions, Not Words: Mechanistic Topic Models Using Sparse Autoencoders](https://arxiv.org/abs/2507.23220)
Token length: 1513
Summarized using GPT-3.5-turbo
Append: [Enabling Few-Shot Alzheimer's Disease Diagnosis on Tabular Biomarker Data with LLMs](https://arxiv.org/abs/2507.23227)
Token length: 1213
Summarized using GPT-3.5-turbo
Append: [P-ReMIS: Pragmatic Reasoning in Mental Health and a Social Implication](https://arxiv.org/abs/2507.23247)
Token length: 1557
Summarized using GPT-3.5-turbo
Append: [Evaluating LLMs' Multilingual Capabilities for Bengali: Benchmark Creation and Performance Analysis](https://arxiv.org/abs/2507.23248)
Token length: 1907
Summarized using GPT-3.5-turbo
Append: [Unveiling Super Experts in Mixture-of-Experts Large Language Models](https://arxiv.org/abs/2507.23279)
Append: [What's Taboo for You? - An Empirical Evaluation of LLMs Behavior Toward Sensitive Content](https://arxiv.org/abs/2507.23319)
Append: [MUST-RAG: MUSical Text Question Answering with Retrieval Augmented Generation](https://arxiv.org/abs/2507.23334)
Append: [Text-to-SQL Task-oriented Dialogue Ontology Construction](https://arxiv.org/abs/2507.23358)
Append: [MPCC: A Novel Benchmark for Multimodal Planning with Complex Constraints in Multimodal Large Language Models](https://arxiv.org/abs/2507.23382)
Append: [Causal2Vec: Improving Decoder-only LLMs as Versatile Embedding Models](https://arxiv.org/abs/2507.23386)
Append: [Beyond the Cloud: Assessing the Benefits and Drawbacks of Local LLM Deployment for Translators](https://arxiv.org/abs/2507.23399)
Append: [MRGSEM-Sum: An Unsupervised Multi-document Summarization Framework based on Multi-Relational Graphs and Structural Entropy Minimization](https://arxiv.org/abs/2507.23400)
Append: [Enhanced Arabic Text Retrieval with Attentive Relevance Scoring](https://arxiv.org/abs/2507.23404)
Append: [Beyond Passive Critical Thinking: Fostering Proactive Questioning to Enhance Human-AI Collaboration](https://arxiv.org/abs/2507.23407)
Append: [Role-Aware Language Models for Secure and Contextualized Access Control in Organizations](https://arxiv.org/abs/2507.23465)
Append: [A Novel Evaluation Benchmark for Medical LLMs: Illuminating Safety and Effectiveness in Clinical Domains](https://arxiv.org/abs/2507.23486)
Append: [Med-R$^3$: Enhancing Medical Retrieval-Augmented Reasoning of LLMs via Progressive Reinforcement Learning](https://arxiv.org/abs/2507.23541)
Append: [T-Detect: Tail-Aware Statistical Normalization for Robust Detection of Adversarial Machine-Generated Text](https://arxiv.org/abs/2507.23577)
Append: [DiffLoRA: Differential Low-Rank Adapters for Large Language Models](https://arxiv.org/abs/2507.23588)
Append: [Arabic Hate Speech Identification and Masking in Social Media using Deep Learning Models and Pre-trained Models Fine-tuning](https://arxiv.org/abs/2507.23661)
Append: [Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs](https://arxiv.org/abs/2507.23740)
Append: [Cascaded Information Disclosure for Generalized Evaluation of Problem Solving Capabilities](https://arxiv.org/abs/2507.23776)
Append: [Hybrid EEG--Driven Brain--Computer Interface: A Large Language Model Framework for Personalized Language Rehabilitation](https://arxiv.org/abs/2507.22892)
Append: [Voice-guided Orchestrated Intelligence for Clinical Evaluation (VOICE): A Voice AI Agent System for Prehospital Stroke Assessment](https://arxiv.org/abs/2507.22898)
Append: [Toward the Autonomous AI Doctor: Quantitative Benchmarking of an Autonomous Agentic AI Versus Board-Certified Clinicians in a Real World Setting](https://arxiv.org/abs/2507.22902)
Append: [ELMES: An Automated Framework for Evaluating Large Language Models in Educational Scenarios](https://arxiv.org/abs/2507.22947)
Append: [Exploring Dynamic Parameters for Vietnamese Gender-Independent ASR](https://arxiv.org/abs/2507.22964)
Append: [Generalized Reinforcement Learning for Retriever-Specific Query Rewriter with Unstructured Real-World Documents](https://arxiv.org/abs/2507.23242)
Append: [SequenceLayers: Sequence Processing and Streaming Neural Networks Made Easy](https://arxiv.org/abs/2507.23292)
Append: [DSBC : Data Science task Benchmarking with Context engineering](https://arxiv.org/abs/2507.23336)
Append: [SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution](https://arxiv.org/abs/2507.23348)
Append: [SWE-Exp: Experience-Driven Software Issue Resolution](https://arxiv.org/abs/2507.23361)
Append: [Holistic Evaluations of Topic Models](https://arxiv.org/abs/2507.23364)
Append: [Counterfactual Evaluation for Blind Attack Detection in LLM-based Evaluation Systems](https://arxiv.org/abs/2507.23453)
Append: [MECAT: A Multi-Experts Constructed Benchmark for Fine-Grained Audio Understanding Tasks](https://arxiv.org/abs/2507.23511)
Append: [Deep Learning-based Prediction of Clinical Trial Enrollment with Uncertainty Estimates](https://arxiv.org/abs/2507.23607)
Append: [TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached Responses](https://arxiv.org/abs/2507.23674)
Append: [TextQuests: How Good are LLMs at Text-Based Video Games?](https://arxiv.org/abs/2507.23701)
Append: [Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving](https://arxiv.org/abs/2507.23726)
Append: [CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks](https://arxiv.org/abs/2507.23751)
Append: [SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model](https://arxiv.org/abs/2507.23773)
Append: [LiMe: a Latin Corpus of Late Medieval Criminal Sentences](https://arxiv.org/abs/2404.12829)
Append: [Explaining vague language](https://arxiv.org/abs/2404.18154)
Append: [Iterative Repair with Weak Verifiers for Few-shot Transfer in KBQA with Unanswerability](https://arxiv.org/abs/2406.14313)
Append: [Cutting Through the Noise: Boosting LLM Performance on Math Word Problems](https://arxiv.org/abs/2406.15444)
Append: [Can LLMs assist with Ambiguity? A Quantitative Evaluation of various Large Language Models on Word Sense Disambiguation](https://arxiv.org/abs/2411.18337)
Append: [Cultural Palette: Pluralising Culture Alignment via Multi-agent Palette](https://arxiv.org/abs/2412.11167)
Append: [Inside-Out: Hidden Factual Knowledge in LLMs](https://arxiv.org/abs/2503.15299)
Append: [Can one size fit all?: Measuring Failure in Multi-Document Summarization Domain Transfer](https://arxiv.org/abs/2503.15768)
Append: [Splits! A Flexible Dataset and Evaluation Framework for Sociocultural Linguistic Investigation](https://arxiv.org/abs/2504.04640)
Append: [Improving Multilingual Capabilities with Cultural and Local Knowledge in Large Language Models While Enhancing Native Performance](https://arxiv.org/abs/2504.09753)
Append: [Robust and Fine-Grained Detection of AI Generated Texts](https://arxiv.org/abs/2504.11952)
Append: [Vision-Language Models Are Not Pragmatically Competent in Referring Expression Generation](https://arxiv.org/abs/2504.16060)
Append: [Leveraging LLMs to Create Content Corpora for Niche Domains](https://arxiv.org/abs/2505.02851)
Append: [Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric Speech Recognition in Low-Resource Languages](https://arxiv.org/abs/2505.14874)
Append: [The Pragmatic Mind of Machines: Tracing the Emergence of Pragmatic Competence in Large Language Models](https://arxiv.org/abs/2505.18497)
Append: [AutoSchemaKG: Autonomous Knowledge Graph Construction through Dynamic Schema Induction from Web-Scale Corpora](https://arxiv.org/abs/2505.23628)
Append: [Framing Political Bias in Multilingual LLMs Across Pakistani Languages](https://arxiv.org/abs/2506.00068)
Append: [Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive, and Inductive Reasoning in Language Models](https://arxiv.org/abs/2506.07106)
Append: [Unable to Forget: Proactive Interference Reveals Working Memory Limits in LLMs Beyond Context Length](https://arxiv.org/abs/2506.08184)
Append: [Advances in LLMs with Focus on Reasoning, Adaptability, Efficiency and Ethics](https://arxiv.org/abs/2506.12365)
Append: [Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review](https://arxiv.org/abs/2506.18199)
Append: [WildSpeech-Bench: Benchmarking Audio LLMs in Natural Speech Conversation](https://arxiv.org/abs/2506.21875)
Append: [Perception-Aware Policy Optimization for Multimodal Reasoning](https://arxiv.org/abs/2507.06448)
Append: [KeyKnowledgeRAG (K^2RAG): An Enhanced RAG method for improved LLM question-answering capabilities](https://arxiv.org/abs/2507.07695)
Append: [DocPolarBERT: A Pre-trained Model for Document Understanding with Relative Polar Coordinate Encoding of Layout Structures](https://arxiv.org/abs/2507.08606)
Append: [Cultural Bias in Large Language Models: Evaluating AI Agents through Moral Questionnaires](https://arxiv.org/abs/2507.10073)
Append: [ILID: Native Script Language Identification for Indian Languages](https://arxiv.org/abs/2507.11832)
Append: [How AI Ideas Affect the Creativity, Diversity, and Evolution of Human Ideas: Evidence From a Large, Dynamic Experiment](https://arxiv.org/abs/2401.13481)
Append: [EgoOops: A Dataset for Mistake Action Detection from Egocentric Videos referring to Procedural Texts](https://arxiv.org/abs/2410.05343)
Append: [InfAlign: Inference-aware language model alignment](https://arxiv.org/abs/2412.19792)
Append: [LLaVA-MORE: A Comparative Study of LLMs and Visual Backbones for Enhanced Visual Instruction Tuning](https://arxiv.org/abs/2503.15621)
Append: [AgentSpec: Customizable Runtime Enforcement for Safe and Reliable LLM Agents](https://arxiv.org/abs/2503.18666)
Append: [How Can I Publish My LLM Benchmark Without Giving the True Answers Away?](https://arxiv.org/abs/2505.18102)
Append: [AI-Reporter: A Path to a New Genre of Scientific Communication](https://arxiv.org/abs/2507.05903)
append_entries: 120
Finish: 2025-08-01 04:57:57.529748
------------------------------------------------------
Started: 2025-08-01 06:29:35.008613
Existing_entries: 1120
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-01 06:29:35.320201
------------------------------------------------------
Started: 2025-08-01 08:25:21.198242
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-01 08:25:21.519688
------------------------------------------------------
Started: 2025-08-01 10:19:48.515937
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-01 10:19:48.878147
------------------------------------------------------
Started: 2025-08-01 12:37:34.050214
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-01 12:37:34.371310
------------------------------------------------------
Started: 2025-08-01 14:19:34.541830
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-01 14:19:34.841077
------------------------------------------------------
Started: 2025-08-01 16:23:19.418360
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-01 16:23:19.737727
------------------------------------------------------
Started: 2025-08-01 18:26:01.552931
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-01 18:26:01.877671
------------------------------------------------------
Started: 2025-08-01 20:19:42.900343
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-01 20:19:43.203507
------------------------------------------------------
Started: 2025-08-01 22:17:27.133364
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-01 22:17:27.429692
------------------------------------------------------
Started: 2025-08-02 01:24:01.195086
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-02 01:24:01.520406
------------------------------------------------------
Started: 2025-08-02 03:23:49.471650
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-02 03:23:49.776735
------------------------------------------------------
Started: 2025-08-02 04:32:57.723981
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-02 04:32:57.803993
------------------------------------------------------
Started: 2025-08-02 06:24:33.772941
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-02 06:24:33.849127
------------------------------------------------------
Started: 2025-08-02 08:21:53.278405
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-02 08:21:53.338879
------------------------------------------------------
Started: 2025-08-02 10:17:44.318804
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-02 10:17:44.380717
------------------------------------------------------
Started: 2025-08-02 12:34:37.655755
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-02 12:34:37.710924
------------------------------------------------------
Started: 2025-08-02 14:16:05.112033
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-02 14:16:05.194013
------------------------------------------------------
Started: 2025-08-02 16:20:40.676217
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-02 16:20:40.789332
------------------------------------------------------
Started: 2025-08-02 18:24:26.389571
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-02 18:24:26.468673
------------------------------------------------------
Started: 2025-08-02 20:18:26.079927
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-02 20:18:26.157600
------------------------------------------------------
Started: 2025-08-02 22:16:42.141364
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-02 22:16:42.198572
------------------------------------------------------
Started: 2025-08-03 01:44:11.624138
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-03 01:44:11.742365
------------------------------------------------------
Started: 2025-08-03 03:48:36.944758
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-03 03:48:37.057365
------------------------------------------------------
Started: 2025-08-03 04:45:48.438815
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-03 04:45:48.504840
------------------------------------------------------
Started: 2025-08-03 06:24:44.156060
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-03 06:24:44.234794
------------------------------------------------------
Started: 2025-08-03 08:21:39.268215
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-03 08:21:39.351366
------------------------------------------------------
Started: 2025-08-03 10:17:51.240587
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-03 10:17:51.302070
------------------------------------------------------
Started: 2025-08-03 12:35:55.081058
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-03 12:35:55.141941
------------------------------------------------------
Started: 2025-08-03 14:15:19.263944
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-03 14:15:19.320775
------------------------------------------------------
Started: 2025-08-03 16:21:01.528360
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-03 16:21:01.602941
------------------------------------------------------
Started: 2025-08-03 18:24:52.278725
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-03 18:24:52.343059
------------------------------------------------------
Started: 2025-08-03 20:19:29.958897
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-03 20:19:30.051302
------------------------------------------------------
Started: 2025-08-03 22:17:10.025551
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-03 22:17:10.087675
------------------------------------------------------
Started: 2025-08-04 01:44:07.444971
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-04 01:44:07.542642
------------------------------------------------------
Started: 2025-08-04 03:53:33.320175
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-04 03:53:33.399950
------------------------------------------------------
Started: 2025-08-04 04:59:06.469587
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1310
Summarized using GPT-3.5-turbo
Append: [PhysicsEval: Inference-Time Techniques to Improve the Reasoning Proficiency of Large Language Models on Physics Problems](https://arxiv.org/abs/2508.00079)
Token length: 1404
Summarized using GPT-3.5-turbo
Append: [Do LLMs produce texts with "human-like" lexical diversity?](https://arxiv.org/abs/2508.00086)
Token length: 1356
Summarized using GPT-3.5-turbo
Append: [Semiotic Complexity and Its Epistemological Implications for Modeling Culture](https://arxiv.org/abs/2508.00095)
Token length: 966
Summarized using GPT-3.5-turbo
Append: [FACTORY: A Challenging Human-Verified Prompt Set for Long-Form Factuality](https://arxiv.org/abs/2508.00109)
Token length: 923
Summarized using GPT-3.5-turbo
Append: [Is neural semantic parsing good at ellipsis resolution, or isn't it?](https://arxiv.org/abs/2508.00121)
Token length: 1039
Summarized using GPT-3.5-turbo
Append: [Comparison of Large Language Models for Deployment Requirements](https://arxiv.org/abs/2508.00185)
Token length: 1224
Summarized using GPT-3.5-turbo
Append: [Tabular Data Understanding with LLMs: A Survey of Recent Advances and Challenges](https://arxiv.org/abs/2508.00217)
Token length: 1327
Summarized using GPT-3.5-turbo
Append: [Semantic Compression for Word and Sentence Embeddings using Discrete Wavelet Transform](https://arxiv.org/abs/2508.00220)
Token length: 1675
Summarized using GPT-3.5-turbo
Append: [Model Misalignment and Language Change: Traces of AI-Associated Language in Unscripted Spoken English](https://arxiv.org/abs/2508.00238)
Token length: 1913
Summarized using GPT-3.5-turbo
Append: [Integrating clinical reasoning into large language model-based diagnosis through etiology-aware attention steering](https://arxiv.org/abs/2508.00285)
Token length: 1455
Summarized using GPT-3.5-turbo
Append: [Systematic Evaluation of Optimization Techniques for Long-Context Language Models](https://arxiv.org/abs/2508.00305)
Token length: 984
Summarized using GPT-3.5-turbo
Append: [Improving Multimodal Contrastive Learning of Sentence Embeddings with Object-Phrase Alignment](https://arxiv.org/abs/2508.00332)
Token length: 1792
Summarized using GPT-3.5-turbo
Append: [PilotRL: Training Language Model Agents via Global Planning-Guided Progressive Reinforcement Learning](https://arxiv.org/abs/2508.00344)
Token length: 1140
Summarized using GPT-3.5-turbo
Append: [Lucy: edgerunning agentic web search on mobile with machine generated task vectors](https://arxiv.org/abs/2508.00360)
Token length: 1661
Summarized using GPT-3.5-turbo
Append: [EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level Efficiency for Edge Devices](https://arxiv.org/abs/2508.00370)
Token length: 1816
Summarized using GPT-3.5-turbo
Append: [Multi-Layer Attention is the Amplifier of Demonstration Effectiveness](https://arxiv.org/abs/2508.00385)
Token length: 1710
Summarized using GPT-3.5-turbo
Append: [SA-GCS: Semantic-Aware Gaussian Curriculum Scheduling for UAV Vision-Language Navigation](https://arxiv.org/abs/2508.00390)
Token length: 1014
Summarized using GPT-3.5-turbo
Append: [Combining Discrete Wavelet and Cosine Transforms for Efficient Sentence Embedding](https://arxiv.org/abs/2508.00420)
Token length: 1321
Summarized using GPT-3.5-turbo
Append: [ReaGAN: Node-as-Agent-Reasoning Graph Agentic Network](https://arxiv.org/abs/2508.00429)
Token length: 1271
Summarized using GPT-3.5-turbo
Append: [Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges](https://arxiv.org/abs/2508.00454)
Token length: 657
Summarized using GPT-3.5-turbo
Append: [GETALP@AutoMin 2025: Leveraging RAG to Answer Questions based on Meeting Transcripts](https://arxiv.org/abs/2508.00476)
Token length: 1140
Summarized using GPT-3.5-turbo
Append: [The Missing Parts: Augmenting Fact Verification with Half-Truth Detection](https://arxiv.org/abs/2508.00489)
Token length: 1579
Summarized using GPT-3.5-turbo
Append: [EFlat-LoRA: Efficiently Seeking Flat Minima for Better Generalization in Fine-Tuning Large Language Models and Beyond](https://arxiv.org/abs/2508.00522)
Token length: 1132
Summarized using GPT-3.5-turbo
Append: [The Prosody of Emojis](https://arxiv.org/abs/2508.00537)
Token length: 1011
Summarized using GPT-3.5-turbo
Append: [PaPaformer: Language Model from Pre-trained Paraller Paths](https://arxiv.org/abs/2508.00544)
Token length: 1285
Summarized using GPT-3.5-turbo
Append: [SynAdapt: Learning Adaptive Reasoning in Large Language Models via Synthetic Continuous Chain-of-Thought](https://arxiv.org/abs/2508.00574)
Token length: 1231
Summarized using GPT-3.5-turbo
Append: [A Context-Aware Dual-Metric Framework for Confidence Estimation in Large Language Models](https://arxiv.org/abs/2508.00600)
Token length: 1408
Summarized using GPT-3.5-turbo
Append: [GHTM: A Graph based Hybrid Topic Modeling Approach in Low-Resource Bengali Language](https://arxiv.org/abs/2508.00605)
Token length: 1373
Summarized using GPT-3.5-turbo
Append: [Prompting Science Report 3: I'll pay you or I'll kill you -- but will you care?](https://arxiv.org/abs/2508.00614)
Token length: 1732
Summarized using GPT-3.5-turbo
Append: [DACTYL: Diverse Adversarial Corpus of Texts Yielded from Large Language Models](https://arxiv.org/abs/2508.00619)
Token length: 1386
Summarized using GPT-3.5-turbo
Append: [Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement Techniques and Applications](https://arxiv.org/abs/2508.00669)
Token length: 938
Summarized using GPT-3.5-turbo
Append: [MELAC: Massive Evaluation of Large Language Models with Alignment of Culture in Persian Language](https://arxiv.org/abs/2508.00673)
Token length: 1723
Summarized using GPT-3.5-turbo
Append: [Team "better_call_claude": Style Change Detection using a Sequential Sentence Pair Classifier](https://arxiv.org/abs/2508.00675)
Token length: 1044
Summarized using GPT-3.5-turbo
Append: [Segment First, Retrieve Better: Realistic Legal Search via Rhetorical Role-Based Queries](https://arxiv.org/abs/2508.00679)
Token length: 889
Summarized using GPT-3.5-turbo
Append: [Better Call Claude: Can LLMs Detect Changes of Writing Style?](https://arxiv.org/abs/2508.00680)
Token length: 1216
Summarized using GPT-3.5-turbo
Append: [NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian Common Law System](https://arxiv.org/abs/2508.00709)
Token length: 1945
Summarized using GPT-3.5-turbo
Append: [Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and Context-Aware KGQA](https://arxiv.org/abs/2508.00719)
Token length: 968
Summarized using GPT-3.5-turbo
Append: [Out-of-Context Abduction: LLMs Make Inferences About Procedural Data Leveraging Declarative Facts in Earlier Training Data](https://arxiv.org/abs/2508.00741)
Token length: 1536
Summarized using GPT-3.5-turbo
Append: [Applying Psychometrics to Large Language Model Simulated Populations: Recreating the HEXACO Personality Inventory Experiment with Generative Agents](https://arxiv.org/abs/2508.00742)
Token length: 1894
Summarized using GPT-3.5-turbo
Append: [Agentic large language models improve retrieval-based radiology question answering](https://arxiv.org/abs/2508.00743)
Token length: 1002
Summarized using GPT-3.5-turbo
Append: [GLiDRE: Generalist Lightweight model for Document-level Relation Extraction](https://arxiv.org/abs/2508.00757)
Token length: 1209
Summarized using GPT-3.5-turbo
Append: [MMBERT: Scaled Mixture-of-Experts Multimodal BERT for Robust Chinese Hate Speech Detection under Cloaking Perturbations](https://arxiv.org/abs/2508.00760)
Token length: 1195
Summarized using GPT-3.5-turbo
Append: [ITUNLP at SemEval-2025 Task 8: Question-Answering over Tabular Data: A Zero-Shot Approach using LLM-Driven Code Generation](https://arxiv.org/abs/2508.00762)
Token length: 1118
Summarized using GPT-3.5-turbo
Append: [Do They Understand Them? An Updated Evaluation on Nonbinary Pronoun Handling in Large Language Models](https://arxiv.org/abs/2508.00788)
Token length: 1893
Summarized using GPT-3.5-turbo
Append: [Beyond Fixed: Variable-Length Denoising for Diffusion Large Language Models](https://arxiv.org/abs/2508.00819)
Token length: 1517
Summarized using GPT-3.5-turbo
Append: [Scalable Spectrum Availability Prediction using a Markov Chain Framework and ITU-R Propagation Models](https://arxiv.org/abs/2508.00028)
Token length: 1508
Summarized using GPT-3.5-turbo
Append: [GPT-4.1 Sets the Standard in Automated Experiment Design Using Novel Python Libraries](https://arxiv.org/abs/2508.00033)
Token length: 1499
Summarized using GPT-3.5-turbo
Append: [A Survey on Code Generation with LLM-based Agents](https://arxiv.org/abs/2508.00083)
Token length: 1735
Summarized using GPT-3.5-turbo
Append: [Watch the Weights: Unsupervised monitoring and control of fine-tuned LLMs](https://arxiv.org/abs/2508.00161)
Token length: 1436
Summarized using GPT-3.5-turbo
Append: [On the Risk of Misleading Reports: Diagnosing Textual Biases in Multimodal Clinical AI](https://arxiv.org/abs/2508.00171)
Append: [RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization](https://arxiv.org/abs/2508.00222)
Append: [Towards Higher Effective Rank in Parameter-efficient Fine-tuning using Khatri--Rao Product](https://arxiv.org/abs/2508.00230)
Append: [MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning](https://arxiv.org/abs/2508.00271)
Append: [Mind the Gap: The Divergence Between Human and LLM-Generated Tasks](https://arxiv.org/abs/2508.00282)
Append: [R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge](https://arxiv.org/abs/2508.00324)
Append: [Benchmarking LLMs for Unit Test Generation from Real-World Functions](https://arxiv.org/abs/2508.00408)
Append: [Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training](https://arxiv.org/abs/2508.00414)
Append: [Fine-grained Spatiotemporal Grounding on Egocentric Videos](https://arxiv.org/abs/2508.00518)
Append: [Towards a unified framework for programming paradigms: A systematic review of classification formalisms and methodological foundations](https://arxiv.org/abs/2508.00534)
Append: [ContestTrade: A Multi-Agent Trading System Based on Internal Contest Mechanism](https://arxiv.org/abs/2508.00554)
Append: [Activation-Guided Local Editing for Jailbreaking Attacks](https://arxiv.org/abs/2508.00555)
Append: [Context-based Motion Retrieval using Open Vocabulary Methods for Autonomous Driving](https://arxiv.org/abs/2508.00589)
Append: [Demo: TOSense -- What Did You Just Agree to?](https://arxiv.org/abs/2508.00659)
Append: [Classification of Psychiatry Clinical Notes by Diagnosis: A Deep Learning and Machine Learning Approach](https://arxiv.org/abs/2508.00695)
Append: [Retrieval-Augmented Semantic Parsing: Improving Generalization with Lexical Knowledge](https://arxiv.org/abs/2412.10207)
Append: [An Investigation into Value Misalignment in LLM-Generated Texts for Cultural Heritage](https://arxiv.org/abs/2501.02039)
Append: [IssueBench: Millions of Realistic Prompts for Measuring Issue Bias in LLM Writing Assistance](https://arxiv.org/abs/2502.08395)
Append: [Better Embeddings with Coupled Adam](https://arxiv.org/abs/2502.08441)
Append: [SEFL: Enhancing Educational Assignment Feedback with LLM Agents](https://arxiv.org/abs/2502.12927)
Append: [Lost in Space: Finding the Right Tokens for Structured Output](https://arxiv.org/abs/2502.14969)
Append: [Linguistic Generalizability of Test-Time Scaling in Mathematical Reasoning](https://arxiv.org/abs/2502.17407)
Append: [Do Large Language Models Know How Much They Know?](https://arxiv.org/abs/2502.19573)
Append: [A Survey on Post-training of Large Language Models](https://arxiv.org/abs/2503.06072)
Append: [AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through Lightweight Vocabulary Adaptation](https://arxiv.org/abs/2503.19693)
Append: [MemInsight: Autonomous Memory Augmentation for LLM Agents](https://arxiv.org/abs/2503.21760)
Append: [Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the Evaluation Protocol](https://arxiv.org/abs/2504.10284)
Append: [Socrates or Smartypants: Testing Logic Reasoning Capabilities of Large Language Models with Logic Programming-based Test Oracles](https://arxiv.org/abs/2504.12312)
Append: [Debunking with Dialogue? Exploring AI-Generated Counterspeech to Challenge Conspiracy Theories](https://arxiv.org/abs/2504.16604)
Append: [Credible Plan-Driven RAG Method for Multi-Hop Question Answering](https://arxiv.org/abs/2504.16787)
Append: [Lost in Benchmarks? Rethinking Large Language Model Benchmarking with Item Response Theory](https://arxiv.org/abs/2505.15055)
Append: [Mitigating Gender Bias via Fostering Exploratory Thinking in LLMs](https://arxiv.org/abs/2505.17217)
Append: [AutoMixer: Checkpoint Artifacts as Automatic Data Mixers](https://arxiv.org/abs/2506.21910)
Append: [RAG-R1 : Incentivize the Search and Reasoning Capabilities of LLMs through Multi-query Parallelism](https://arxiv.org/abs/2507.02962)
Append: [LLMs Encode Harmfulness and Refusal Separately](https://arxiv.org/abs/2507.11878)
Append: [Loss Landscape Degeneracy and Stagewise Development in Transformers](https://arxiv.org/abs/2402.02364)
Append: [Policy Maps: Tools for Guiding the Unbounded Space of LLM Behaviors](https://arxiv.org/abs/2409.18203)
Append: [LLaVA-Video: Video Instruction Tuning With Synthetic Data](https://arxiv.org/abs/2410.02713)
Append: [Unlocking Multi-Modal Potentials for Link Prediction on Dynamic Text-Attributed Graphs](https://arxiv.org/abs/2502.19651)
Append: [Think Before Recommend: Unleashing the Latent Reasoning Power for Sequential Recommendation](https://arxiv.org/abs/2503.22675)
Append: [OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device Speculative Decoding](https://arxiv.org/abs/2507.02659)
Append: [Evaluating LLMs on Real-World Forecasting Against Human Superforecasters](https://arxiv.org/abs/2507.04562)
Append: [Sound and Complete Neurosymbolic Reasoning with LLM-Grounded Interpretations](https://arxiv.org/abs/2507.09751)
append_entries: 92
Finish: 2025-08-04 05:01:00.392260
------------------------------------------------------
Started: 2025-08-04 06:34:26.080443
Existing_entries: 1092
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-04 06:34:26.318120
------------------------------------------------------
Started: 2025-08-04 08:27:12.433807
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-04 08:27:12.681980
------------------------------------------------------
Started: 2025-08-04 10:21:26.369137
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-04 10:21:26.660784
------------------------------------------------------
Started: 2025-08-04 12:39:40.979203
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-04 12:39:41.263696
------------------------------------------------------
Started: 2025-08-04 14:22:38.116381
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-04 14:22:38.422525
------------------------------------------------------
Started: 2025-08-04 16:24:57.822994
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-04 16:24:58.144704
------------------------------------------------------
Started: 2025-08-04 18:27:49.908978
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-04 18:27:50.165736
------------------------------------------------------
Started: 2025-08-04 20:20:41.636754
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-04 20:20:41.900494
------------------------------------------------------
Started: 2025-08-04 22:18:27.537289
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-04 22:18:27.806112
------------------------------------------------------
Started: 2025-08-05 01:39:44.060128
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-05 01:39:44.351389
------------------------------------------------------
Started: 2025-08-05 03:44:51.759403
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-05 03:44:52.016475
------------------------------------------------------
Started: 2025-08-05 04:48:45.676086
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1273
Summarized using GPT-3.5-turbo
Append: [Rethinking Graph-Based Document Classification: Learning Data-Driven Structures Beyond Heuristic Approaches](https://arxiv.org/abs/2508.00864)
Token length: 1360
Summarized using GPT-3.5-turbo
Append: [FECT: Factuality Evaluation of Interpretive AI-Generated Claims in Contact Center Conversation Transcripts](https://arxiv.org/abs/2508.00889)
Token length: 1338
Summarized using GPT-3.5-turbo
Append: [XAutoLM: Efficient Fine-Tuning of Language Models via Meta-Learning and AutoML](https://arxiv.org/abs/2508.00924)
Token length: 1636
Summarized using GPT-3.5-turbo
Append: [MAO-ARAG: Multi-Agent Orchestration for Adaptive Retrieval-Augmented Generation](https://arxiv.org/abs/2508.01005)
Token length: 1225
Summarized using GPT-3.5-turbo
Append: [UrBLiMP: A Benchmark for Evaluating the Linguistic Competence of Large Language Models in Urdu](https://arxiv.org/abs/2508.01006)
Token length: 1173
Summarized using GPT-3.5-turbo
Append: [Cross-Domain Web Information Extraction at Pinterest](https://arxiv.org/abs/2508.01096)
Token length: 1299
Summarized using GPT-3.5-turbo
Append: [Asking the Right Questions: Benchmarking Large Language Models in the Development of Clinical Consultation Templates](https://arxiv.org/abs/2508.01159)
Token length: 840
Summarized using GPT-3.5-turbo
Append: [CSIRO-LT at SemEval-2025 Task 11: Adapting LLMs for Emotion Recognition for Multiple Languages](https://arxiv.org/abs/2508.01161)
Token length: 1840
Summarized using GPT-3.5-turbo
Append: [Adaptive Content Restriction for Large Language Models via Suffix Optimization](https://arxiv.org/abs/2508.01198)
Token length: 907
Summarized using GPT-3.5-turbo
Append: [Show or Tell? Modeling the evolution of request-making in Human-LLM conversations](https://arxiv.org/abs/2508.01213)
Token length: 1740
Summarized using GPT-3.5-turbo
Append: [WebDS: An End-to-End Benchmark for Web-based Data Science](https://arxiv.org/abs/2508.01222)
Token length: 1360
Summarized using GPT-3.5-turbo
Append: [WarriorMath: Enhancing the Mathematical Ability of Large Language Models with a Defect-aware Framework](https://arxiv.org/abs/2508.01245)
Token length: 1658
Summarized using GPT-3.5-turbo
Append: [Bridging LLMs and Symbolic Reasoning in Educational QA Systems: Insights from the XAI Challenge at IJCNN 2025](https://arxiv.org/abs/2508.01263)
Token length: 1449
Summarized using GPT-3.5-turbo
Append: [Prompting Large Language Models with Partial Knowledge for Answering Questions with Unseen Entities](https://arxiv.org/abs/2508.01290)
Token length: 1589
Summarized using GPT-3.5-turbo
Append: [KEDAS: Knowledge Editing Alignment with Diverse Augmentation and Self-adaptive Inference](https://arxiv.org/abs/2508.01302)
Token length: 1289
Summarized using GPT-3.5-turbo
Append: [D-SCoRE: Document-Centric Segmentation and CoT Reasoning with Structured Export for QA-CoT Data Generation](https://arxiv.org/abs/2508.01309)
Token length: 1372
Summarized using GPT-3.5-turbo
Append: [LinkQA: Synthesizing Diverse QA from Multiple Seeds Strongly Linked by Knowledge Points](https://arxiv.org/abs/2508.01317)
Token length: 1546
Summarized using GPT-3.5-turbo
Append: [Large-Scale Diverse Synthesis for Mid-Training](https://arxiv.org/abs/2508.01326)
Token length: 1313
Summarized using GPT-3.5-turbo
Append: [MaRGen: Multi-Agent LLM Approach for Self-Directed Market Research and Analysis](https://arxiv.org/abs/2508.01370)
Token length: 1009
Summarized using GPT-3.5-turbo
Append: [MedSynth: Realistic, Synthetic Medical Dialogue-Note Pairs](https://arxiv.org/abs/2508.01401)
Token length: 1007
Summarized using GPT-3.5-turbo
Append: [ArzEn-MultiGenre: An aligned parallel dataset of Egyptian Arabic song lyrics, novels, and subtitles, with English translations](https://arxiv.org/abs/2508.01411)
Token length: 1143
Summarized using GPT-3.5-turbo
Append: [Discovering Bias Associations through Open-Ended LLM Generations](https://arxiv.org/abs/2508.01412)
Token length: 1283
Summarized using GPT-3.5-turbo
Append: [From Query to Logic: Ontology-Driven Multi-Hop Reasoning in LLMs](https://arxiv.org/abs/2508.01424)
Token length: 1874
Summarized using GPT-3.5-turbo
Append: [Towards Efficient Medical Reasoning with Minimal Fine-Tuning Data](https://arxiv.org/abs/2508.01450)
Token length: 1649
Summarized using GPT-3.5-turbo
Append: [TreeDiff: AST-Guided Code Generation with Diffusion LLMs](https://arxiv.org/abs/2508.01473)
Token length: 1390
Summarized using GPT-3.5-turbo
Append: [Harnessing Collective Intelligence of LLMs for Robust Biomedical QA: A Multi-Model Approach](https://arxiv.org/abs/2508.01480)
Token length: 1722
Summarized using GPT-3.5-turbo
Append: [TeSent: A Benchmark Dataset for Fairness-aware Explainable Sentiment Classification in Telugu](https://arxiv.org/abs/2508.01486)
Token length: 944
Summarized using GPT-3.5-turbo
Append: [The Homogenizing Effect of Large Language Models on Human Expression and Thought](https://arxiv.org/abs/2508.01491)
Token length: 1095
Summarized using GPT-3.5-turbo
Append: [A Theory of Adaptive Scaffolding for LLM-Based Pedagogical Agents](https://arxiv.org/abs/2508.01503)
Token length: 1414
Summarized using GPT-3.5-turbo
Append: [MOPrompt: Multi-objective Semantic Evolution for Prompt Optimization](https://arxiv.org/abs/2508.01541)
Token length: 1551
Summarized using GPT-3.5-turbo
Append: [Are All Prompt Components Value-Neutral? Understanding the Heterogeneous Adversarial Robustness of Dissected Prompt in Large Language Models](https://arxiv.org/abs/2508.01554)
Token length: 1954
Summarized using GPT-3.5-turbo
Append: [OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers for Biomedical NER Across 12 Public Datasets](https://arxiv.org/abs/2508.01630)
Token length: 1486
Summarized using GPT-3.5-turbo
Append: [Authorship Attribution in Multilingual Machine-Generated Texts](https://arxiv.org/abs/2508.01656)
Token length: 1346
Summarized using GPT-3.5-turbo
Append: [CUPID: Evaluating Personalized and Contextualized Alignment of LLMs from Interactions](https://arxiv.org/abs/2508.01674)
Token length: 1666
Summarized using GPT-3.5-turbo
Append: [The Bidirectional Process Reward Model](https://arxiv.org/abs/2508.01682)
Token length: 1312
Summarized using GPT-3.5-turbo
Append: [Collaborative Chain-of-Agents for Parametric-Retrieved Knowledge Synergy](https://arxiv.org/abs/2508.01696)
Token length: 1447
Summarized using GPT-3.5-turbo
Append: [Am I Blue or Is My Hobby Counting Teardrops? Expression Leakage in Large Language Models as a Symptom of Irrelevancy Disruption](https://arxiv.org/abs/2508.01708)
Token length: 1534
Summarized using GPT-3.5-turbo
Append: [CultureGuard: Towards Culturally-Aware Dataset and Guard Model for Multilingual Safety Applications](https://arxiv.org/abs/2508.01710)
Token length: 1785
Summarized using GPT-3.5-turbo
Append: [Enhancing the Preference Extractor in Multi-turn Dialogues: From Annotating Disasters to Accurate Preference Extraction](https://arxiv.org/abs/2508.01739)
Token length: 1606
Summarized using GPT-3.5-turbo
Append: [AI-Generated Text is Non-Stationary: Detection via Temporal Tomography](https://arxiv.org/abs/2508.01754)
Token length: 1729
Summarized using GPT-3.5-turbo
Append: [A comprehensive taxonomy of hallucinations in Large Language Models](https://arxiv.org/abs/2508.01781)
Token length: 1600
Summarized using GPT-3.5-turbo
Append: [HeQ: a Large and Diverse Hebrew Reading Comprehension Benchmark](https://arxiv.org/abs/2508.01812)
Token length: 1498
Summarized using GPT-3.5-turbo
Append: [AGENTICT$^2$S:Robust Text-to-SPARQL via Agentic Collaborative Reasoning over Heterogeneous Knowledge Graphs for the Circular Economy](https://arxiv.org/abs/2508.01815)
Token length: 1514
Summarized using GPT-3.5-turbo
Append: [MLP Memory: Language Modeling with Retriever-pretrained External Memory](https://arxiv.org/abs/2508.01832)
Token length: 1963
Summarized using GPT-3.5-turbo
Append: [Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web Agents](https://arxiv.org/abs/2508.01858)
Token length: 1173
Summarized using GPT-3.5-turbo
Append: [Counterfactual Probing for Hallucination Detection and Mitigation in Large Language Models](https://arxiv.org/abs/2508.01862)
Token length: 1757
Summarized using GPT-3.5-turbo
Append: [Quantum-RAG and PunGPT2: Advancing Low-Resource Language Generation and Retrieval for the Punjabi Language](https://arxiv.org/abs/2508.01918)
Token length: 1140
Summarized using GPT-3.5-turbo
Append: [Word Overuse and Alignment in Large Language Models: The Influence of Learning from Human Feedback](https://arxiv.org/abs/2508.01930)
Token length: 1711
Summarized using GPT-3.5-turbo
Append: [ROVER: Recursive Reasoning Over Videos with Vision-Language Models for Embodied Tasks](https://arxiv.org/abs/2508.01943)
Token length: 1718
Summarized using GPT-3.5-turbo
Append: [SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic Association and Long Story Comprehension](https://arxiv.org/abs/2508.01959)
Append: [TIBSTC-CoT: A Multi-Domain Instruction Dataset for Chain-of-Thought Reasoning in Language Models](https://arxiv.org/abs/2508.01977)
Append: [Contextually Aware E-Commerce Product Question Answering using RAG](https://arxiv.org/abs/2508.01990)
Append: [Prompting Large Language Models to Detect Dementia Family Caregivers](https://arxiv.org/abs/2508.01999)
Append: [SpeechRole: A Large-Scale Dataset and Benchmark for Evaluating Speech Role-Playing Agents](https://arxiv.org/abs/2508.02013)
Append: [SpeechR: A Benchmark for Speech Reasoning in Large Audio-Language Models](https://arxiv.org/abs/2508.02018)
Append: [Diagnosing Memorization in Chain-of-Thought Reasoning, One Token at a Time](https://arxiv.org/abs/2508.02037)
Append: [Marco-Voice Technical Report](https://arxiv.org/abs/2508.02038)
Append: [Harnessing Temporal Databases for Systematic Evaluation of Factual Time-Sensitive Question-Answering in Large Language Models](https://arxiv.org/abs/2508.02045)
Append: [ProCut: LLM Prompt Compression via Attribution Estimation](https://arxiv.org/abs/2508.02053)
Append: [The SMeL Test: A simple benchmark for media literacy in language models](https://arxiv.org/abs/2508.02074)
Append: [When Truth Is Overridden: Uncovering the Internal Origins of Sycophancy in Large Language Models](https://arxiv.org/abs/2508.02087)
Append: ["Harmless to You, Hurtful to Me!": Investigating the Detection of Toxic Languages Grounded in the Perspective of Youth](https://arxiv.org/abs/2508.02094)
Append: [Learning Dynamics of Meta-Learning in Small Model Pretraining](https://arxiv.org/abs/2508.02189)
Append: [Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed Inference](https://arxiv.org/abs/2508.02193)
Append: [Proof2Hybrid: Automatic Mathematical Benchmark Synthesis for Proof-Centric Problems](https://arxiv.org/abs/2508.02208)
Append: [Isolating Culture Neurons in Multilingual Large Language Models](https://arxiv.org/abs/2508.02241)
Append: [Interference Matrix: Quantifying Cross-Lingual Interference in Transformer Encoders](https://arxiv.org/abs/2508.02256)
Append: [Decomposing the Entropy-Performance Exchange: The Missing Keys to Unlocking Effective Reinforcement Learning](https://arxiv.org/abs/2508.02260)
Append: [SHAMI-MT: A Syrian Arabic Dialect to Modern Standard Arabic Bidirectional Machine Translation System](https://arxiv.org/abs/2508.02268)
Append: [Dynaword: From One-shot to Continuously Developed Datasets](https://arxiv.org/abs/2508.02271)
Append: [A French Version of the OLDI Seed Corpus](https://arxiv.org/abs/2508.02290)
Append: [Simple Methods Defend RAG Systems Well Against Real-World Attacks](https://arxiv.org/abs/2508.02296)
Append: [LaMPE: Length-aware Multi-grained Position Encoding for Adaptive Long-context Scaling Without Training](https://arxiv.org/abs/2508.02308)
Append: [VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo](https://arxiv.org/abs/2508.02317)
Append: [CAMERA: Multi-Matrix Joint Compression for MoE Models via Micro-Expert Redundancy Analysis](https://arxiv.org/abs/2508.02322)
Append: [Understanding and Mitigating Political Stance Cross-topic Generalization in Large Language Models](https://arxiv.org/abs/2508.02360)
Append: [CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important Before Generation](https://arxiv.org/abs/2508.02401)
Append: [Learning to Evolve: Bayesian-Guided Continual Knowledge Graph Embedding](https://arxiv.org/abs/2508.02426)
Append: [AI-Based Measurement of Innovation: Mapping Expert Insight into Large Language Model Applications](https://arxiv.org/abs/2508.02430)
Append: [LatentPrompt: Optimizing Promts in Latent Space](https://arxiv.org/abs/2508.02452)
Append: [Monsoon Uprising in Bangladesh: How Facebook Shaped Collective Identity](https://arxiv.org/abs/2508.02498)
Append: [From Monolingual to Bilingual: Investigating Language Conditioning in Large Language Models for Psycholinguistic Tasks](https://arxiv.org/abs/2508.02502)
Append: [Modular Arithmetic: Language Models Solve Math Digit by Digit](https://arxiv.org/abs/2508.02513)
Append: [PoeTone: A Framework for Constrained Generation of Structured Chinese Songci with LLMs](https://arxiv.org/abs/2508.02515)
Append: [I Have No Mouth, and I Must Rhyme: Uncovering Internal Phonetic Representations in LLaMA 3.2](https://arxiv.org/abs/2508.02527)
Append: [Contextual Graph Transformer: A Small Language Model for Enhanced Engineering Document Information Extraction](https://arxiv.org/abs/2508.02532)
Append: [What's in the News? Towards Identification of Bias by Commission, Omission, and Source Selection (COSS)](https://arxiv.org/abs/2508.02540)
Append: [Building and Aligning Comparable Corpora](https://arxiv.org/abs/2508.02555)
Append: [Automated SNOMED CT Concept Annotation in Clinical Text Using Bi-GRU Neural Networks](https://arxiv.org/abs/2508.02556)
Append: [Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction](https://arxiv.org/abs/2508.02558)
Append: [Guess or Recall? Training CNNs to Classify and Localize Memorization in LLMs](https://arxiv.org/abs/2508.02573)
Append: [EHSAN: Leveraging ChatGPT in a Hybrid Framework for Arabic Aspect-Based Sentiment Analysis in Healthcare](https://arxiv.org/abs/2508.02574)
Append: [MArgE: Meshing Argumentative Evidence from Multiple Large Language Models for Justifiable Claim Verification](https://arxiv.org/abs/2508.02584)
Append: [CharBench: Evaluating the Role of Tokenization in Character-Level Tasks](https://arxiv.org/abs/2508.02591)
Append: [Mitigating Attention Hacking in Preference-Based Reward Modeling via Interaction Distillation](https://arxiv.org/abs/2508.02618)
Append: [Pointer: Linear-Complexity Long-Range Modeling without Pre-training](https://arxiv.org/abs/2508.02631)
Append: [Test Set Quality in Multilingual LLM Evaluation](https://arxiv.org/abs/2508.02635)
Append: [Contextual Phenotyping of Pediatric Sepsis Cohort Using Large Language Models](https://arxiv.org/abs/2505.09805)
Append: [The Attribution Crisis in LLM Search Results](https://arxiv.org/abs/2508.00838)
Append: [Hallucination Detection and Mitigation with Diffusion in Multi-Variate Time-Series Foundation Models](https://arxiv.org/abs/2508.00881)
Append: [AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks](https://arxiv.org/abs/2508.00890)
Append: [Filtering with Self-Attention and Storing with MLP: One-Layer Transformers Can Provably Acquire and Extract Knowledge](https://arxiv.org/abs/2508.00901)
Append: [An analysis of AI Decision under Risk: Prospect theory emerges in Large Language Models](https://arxiv.org/abs/2508.00902)
Append: [Cyber-Zero: Training Cybersecurity Agents without Runtime](https://arxiv.org/abs/2508.00910)
Append: [Small sample-based adaptive text classification through iterative and contrastive description refinement](https://arxiv.org/abs/2508.00957)
Append: [CADDesigner: Conceptual Design of CAD Models Based on General-Purpose Agent](https://arxiv.org/abs/2508.01031)
Append: [DBAIOps: A Reasoning LLM-Enhanced Database Operation and Maintenance System using Knowledge Graphs](https://arxiv.org/abs/2508.01136)
Append: [Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens](https://arxiv.org/abs/2508.01191)
Append: [AgentArmor: Enforcing Program Analysis on Agent Runtime Trace to Defend Against Prompt Injection](https://arxiv.org/abs/2508.01249)
Append: [Multi-TW: Benchmarking Multimodal Models on Traditional Chinese Question Answering in Taiwan](https://arxiv.org/abs/2508.01274)
Append: [ConfGuard: A Simple and Effective Backdoor Detection for Large Language Models](https://arxiv.org/abs/2508.01365)
Append: [ChEmbed: Enhancing Chemical Literature Search Through Domain-Specific Text Embeddings](https://arxiv.org/abs/2508.01643)
Append: [DUP: Detection-guided Unlearning for Backdoor Purification in Language Models](https://arxiv.org/abs/2508.01647)
Append: [Voxlect: A Speech Foundation Model Benchmark for Modeling Dialects and Regional Languages Around the Globe](https://arxiv.org/abs/2508.01691)
Append: [Uncertainty-Based Methods for Automated Process Reward Data Construction and Output Aggregation in Mathematical Reasoning](https://arxiv.org/abs/2508.01773)
Append: [LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?](https://arxiv.org/abs/2508.01780)
Append: [CSLRConformer: A Data-Centric Conformer Approach for Continuous Arabic Sign Language Recognition on the Isharah Datase](https://arxiv.org/abs/2508.01791)
Append: [Complete Evasion, Zero Modification: PDF Attacks on AI Text Detection](https://arxiv.org/abs/2508.01887)
Append: [Revisiting Replay and Gradient Alignment for Continual Pre-Training of Large Language Models](https://arxiv.org/abs/2508.01908)
Append: [A Decentralized Framework for Ethical Authorship Validation in Academic Publishing: Leveraging Self-Sovereign Identity and Blockchain Technology](https://arxiv.org/abs/2508.01913)
Append: [Decomposing Representation Space into Interpretable Subspaces with Unsupervised Learning](https://arxiv.org/abs/2508.01916)
Append: [MolReasoner: Toward Effective and Interpretable Reasoning for Molecular LLMs](https://arxiv.org/abs/2508.02066)
Append: [Human Capital Visualization using Speech Amount during Meetings](https://arxiv.org/abs/2508.02075)
Append: [CRINN: Contrastive Reinforcement Learning for Approximate Nearest Neighbor Search](https://arxiv.org/abs/2508.02091)
Append: [Trainable Dynamic Mask Sparse Attention](https://arxiv.org/abs/2508.02124)
Append: [Subject or Style: Adaptive and Training-Free Mixture of LoRAs](https://arxiv.org/abs/2508.02165)
Append: [Hidden in the Noise: Unveiling Backdoors in Audio LLMs Alignment through Latent Acoustic Pattern Triggers](https://arxiv.org/abs/2508.02175)
Append: [LeanK: Learnable K Cache Channel Pruning for Efficient Decoding](https://arxiv.org/abs/2508.02215)
Append: [CellForge: Agentic Design of Virtual Cell Models](https://arxiv.org/abs/2508.02276)
Append: [Dialogue Systems Engineering: A Survey and Future Directions](https://arxiv.org/abs/2508.02279)
Append: [CAPO: Towards Enhancing LLM Reasoning through Verifiable Generative Credit Assignment](https://arxiv.org/abs/2508.02298)
Append: [Understanding User Preferences for Interaction Styles in Conversational Recommender Systems: The Predictive Role of System Qualities, User Experience, and Traits](https://arxiv.org/abs/2508.02328)
Append: [Language Model Guided Reinforcement Learning in Quantitative Trading](https://arxiv.org/abs/2508.02366)
Append: [Six Guidelines for Trustworthy, Ethical and Responsible Automation Design](https://arxiv.org/abs/2508.02371)
Append: [Modality Bias in LVLMs: Analyzing and Mitigating Object Hallucination via Attention Lens](https://arxiv.org/abs/2508.02419)
Append: [AIAP: A No-Code Workflow Builder for Non-Experts with Natural Language and Multi-Agent Collaboration](https://arxiv.org/abs/2508.02470)
Append: [OptiHive: Ensemble Selection for LLM-Based Optimization via Statistical Modeling](https://arxiv.org/abs/2508.02503)
Append: [Test-time Prompt Intervention](https://arxiv.org/abs/2508.02511)
Append: [What are you sinking? A geometric approach on attention sink](https://arxiv.org/abs/2508.02546)
Append: [Parameter-Efficient Routed Fine-Tuning: Mixture-of-Experts Demands Mixture of Adaptation Modules](https://arxiv.org/abs/2508.02587)
Append: [HealthFlow: A Self-Evolving AI Agent with Meta Planning for Autonomous Healthcare Research](https://arxiv.org/abs/2508.02621)
Append: [Noosemia: toward a Cognitive and Phenomenological Account of Intentionality Attribution in Human-Generative AI Interaction](https://arxiv.org/abs/2508.02622)
Append: [HyCodePolicy: Hybrid Language Controllers for Multimodal Monitoring and Decision in Embodied Agents](https://arxiv.org/abs/2508.02629)
Append: [You Can Generate It Again: Data-to-Text Generation with Verification and Correction Prompting](https://arxiv.org/abs/2306.15933)
Append: [Thinker-DDM: Modeling Deliberation for Machine Translation with a Drift-Diffusion Process](https://arxiv.org/abs/2402.10699)
Append: [THREAD: Thinking Deeper with Recursive Spawning](https://arxiv.org/abs/2405.17402)
Append: [Dynamic Order Template Prediction for Generative Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2406.11130)
Append: [Can Tool-augmented Large Language Models be Aware of Incomplete Conditions?](https://arxiv.org/abs/2406.12307)
Append: [Cascade Reward Sampling for Efficient Decoding-Time Alignment](https://arxiv.org/abs/2406.16306)
Append: [Path-LLM: A Shortest-Path-based LLM Learning for Unified Graph Representation](https://arxiv.org/abs/2408.05456)
Append: [Learning from Negative Samples in Biomedical Generative Entity Linking](https://arxiv.org/abs/2408.16493)
Append: [CCSBench: Evaluating Compositional Controllability in LLMs for Scientific Document Summarization](https://arxiv.org/abs/2410.12601)
Append: [Arena-Lite: Efficient and Reliable Large Language Model Evaluation via Tournament-Based Direct Comparisons](https://arxiv.org/abs/2411.01281)
Append: [Training and Evaluating Language Models with Template-based Data Generation](https://arxiv.org/abs/2411.18104)
Append: [Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity](https://arxiv.org/abs/2412.02252)
Append: [Core Context Aware Transformers for Long Context Language Modeling](https://arxiv.org/abs/2412.12465)
Append: [KnowRA: Knowledge Retrieval Augmented Method for Document-level Relation Extraction with Comprehensive Reasoning Abilities](https://arxiv.org/abs/2501.00571)
Append: [Self-Evolving Critique Abilities in Large Language Models](https://arxiv.org/abs/2501.05727)
Append: [Rethinking Table Instruction Tuning](https://arxiv.org/abs/2501.14693)
Append: [Beyond Scaling: Measuring and Predicting the Upper Bound of Knowledge Retention in Language Model Pre-Training](https://arxiv.org/abs/2502.04066)
Append: [Emergent Response Planning in LLMs](https://arxiv.org/abs/2502.06258)
Append: [Thinking Outside the (Gray) Box: A Context-Based Score for Assessing Value and Originality in Neural Text Generation](https://arxiv.org/abs/2502.13207)
Append: [Towards Question Answering over Large Semi-structured Tables](https://arxiv.org/abs/2502.13422)
Append: [DiffSampling: Enhancing Diversity and Accuracy in Neural Text Generation](https://arxiv.org/abs/2502.14037)
Append: [Control Illusion: The Failure of Instruction Hierarchies in Large Language Models](https://arxiv.org/abs/2502.15851)
Append: [What are Foundation Models Cooking in the Post-Soviet World?](https://arxiv.org/abs/2502.18583)
Append: [Predictive Data Selection: The Data That Predicts Is the Data That Teaches](https://arxiv.org/abs/2503.00808)
Append: [Efficient Dynamic Clustering-Based Document Compression for Retrieval-Augmented-Generation](https://arxiv.org/abs/2504.03165)
Append: [Enhancing Time Series Forecasting via Multi-Level Text Alignment with LLMs](https://arxiv.org/abs/2504.07360)
Append: [Thought Manipulation: External Thought Can Be Efficient for Large Reasoning Models](https://arxiv.org/abs/2504.13626)
Append: [Agree to Disagree? A Meta-Evaluation of LLM Misgendering](https://arxiv.org/abs/2504.17075)
Append: [A Scoping Review of Natural Language Processing in Addressing Medically Inaccurate Information: Errors, Misinformation, and Hallucination](https://arxiv.org/abs/2505.00008)
Append: [Do MLLMs Capture How Interfaces Guide User Behavior? A Benchmark for Multimodal UI/UX Design Understanding](https://arxiv.org/abs/2505.05026)
Append: [GenKnowSub: Improving Modularity and Reusability of LLMs through General Knowledge Subtraction](https://arxiv.org/abs/2505.10939)
Append: [XtraGPT: Context-Aware and Controllable Academic Paper Revision via Human-AI Collaboration](https://arxiv.org/abs/2505.11336)
Append: [ChartEdit: How Far Are MLLMs From Automating Chart Analysis? Evaluating MLLMs' Capability via Chart Editing](https://arxiv.org/abs/2505.11935)
Append: [On the Generalization vs Fidelity Paradox in Knowledge Distillation](https://arxiv.org/abs/2505.15442)
Append: [Not All Tokens Are What You Need In Thinking](https://arxiv.org/abs/2505.17827)
Append: [Shifting AI Efficiency From Model-Centric to Data-Centric Compression](https://arxiv.org/abs/2505.19147)
Append: [It's High Time: A Survey of Temporal Question Answering](https://arxiv.org/abs/2505.20243)
Append: [Affordance Benchmark for MLLMs](https://arxiv.org/abs/2506.00893)
Append: [Leaps Beyond the Seen: Reinforced Reasoning Augmented Generation for Clinical Notes](https://arxiv.org/abs/2506.05386)
Append: [RAISE: Enhancing Scientific Reasoning in LLMs via Step-by-Step Retrieval](https://arxiv.org/abs/2506.08625)
Append: [Extrapolation by Association: Length Generalization Transfer in Transformers](https://arxiv.org/abs/2506.09251)
Append: [Reasoning with Exploration: An Entropy Perspective on Reinforcement Learning for LLMs](https://arxiv.org/abs/2506.14758)
Append: [FinCoT: Grounding Chain-of-Thought in Expert Financial Reasoning](https://arxiv.org/abs/2506.16123)
Append: [MIST: Jailbreaking Black-box Large Language Models via Iterative Semantic Tuning](https://arxiv.org/abs/2506.16792)
Append: [Can Reasoning Help Large Language Models Capture Human Annotator Disagreement?](https://arxiv.org/abs/2506.19467)
Append: [FloorPlan-DeepSeek (FPDS): A multimodal approach to floorplan generation using vector-based next room prediction](https://arxiv.org/abs/2506.21562)
Append: [LinguaSynth: Heterogeneous Linguistic Signals for News Classification](https://arxiv.org/abs/2506.21848)
Append: [What to Keep and What to Drop: Adaptive Table Filtering Framework](https://arxiv.org/abs/2506.23463)
Append: [Generalizing Verifiable Instruction Following](https://arxiv.org/abs/2507.02833)
Append: [FlexOlmo: Open Language Models for Flexible Data Use](https://arxiv.org/abs/2507.07024)
Append: [Distillation versus Contrastive Learning: How to Train Your Rerankers](https://arxiv.org/abs/2507.08336)
Append: [Ref-Long: Benchmarking the Long-context Referencing Capability of Long-context Language Models](https://arxiv.org/abs/2507.09506)
Append: [LLMs on Trial: Evaluating Judicial Fairness for Large Language Models](https://arxiv.org/abs/2507.10852)
Append: [Kestrel: 3D Multimodal LLM for Part-Aware Grounded Description](https://arxiv.org/abs/2405.18937)
Append: [VideoLLaMB: Long Streaming Video Understanding with Recurrent Memory Bridges](https://arxiv.org/abs/2409.01071)
Append: [Examining Test-Time Adaptation for Personalized Child Speech Recognition](https://arxiv.org/abs/2409.13095)
Append: [Language-based Audio Moment Retrieval](https://arxiv.org/abs/2409.15672)
Append: [CheXalign: Preference fine-tuning in chest X-ray interpretation models without human feedback](https://arxiv.org/abs/2410.07025)
Append: [More than Memes: A Multimodal Topic Modeling Approach to Conspiracy Theories on Telegram](https://arxiv.org/abs/2410.08642)
Append: [BiDoRA: Bi-level Optimization-Based Weight-Decomposed Low-Rank Adaptation](https://arxiv.org/abs/2410.09758)
Append: [Robust and Efficient Fine-tuning of LLMs with Bayesian Reparameterization of Low-Rank Adaptation](https://arxiv.org/abs/2411.04358)
Append: [A Guide to Misinformation Detection Data and Evaluation](https://arxiv.org/abs/2411.05060)
Append: [Associative memory inspires improvements for in-context learning using a novel attention residual stream architecture](https://arxiv.org/abs/2412.15113)
Append: [Gandalf the Red: Adaptive Security for LLMs](https://arxiv.org/abs/2501.07927)
Append: [MCTS-SQL: Light-Weight LLMs can Master the Text-to-SQL through Monte Carlo Tree Search](https://arxiv.org/abs/2501.16607)
Append: [Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions](https://arxiv.org/abs/2502.04322)
Append: [Safety at Scale: A Comprehensive Survey of Large Model and Agent Safety](https://arxiv.org/abs/2502.05206)
Append: [AIRepr: An Analyst-Inspector Framework for Evaluating Reproducibility of LLMs in Data Science](https://arxiv.org/abs/2502.16395)
Append: [JurisTCU: A Brazilian Portuguese Information Retrieval Dataset with Query Relevance Judgments](https://arxiv.org/abs/2503.08379)
Append: [R1-VL: Learning to Reason with Multimodal Large Language Models via Step-wise Group Relative Policy Optimization](https://arxiv.org/abs/2503.12937)
Append: [StableGS: A Floater-Free Framework for 3D Gaussian Splatting](https://arxiv.org/abs/2503.18458)
Append: [GateLens: A Reasoning-Enhanced LLM Agent for Automotive Software Release Analytics](https://arxiv.org/abs/2503.21735)
Append: [LoRI: Reducing Cross-Task Interference in Multi-Task Low-Rank Adaptation](https://arxiv.org/abs/2504.07448)
Append: [ParetoHqD: Fast Offline Multiobjective Alignment of Large Language Models using Pareto High-quality Data](https://arxiv.org/abs/2504.16628)
Append: [One Trigger Token Is Enough: A Defense Strategy for Balancing Safety and Usability in Large Language Models](https://arxiv.org/abs/2505.07167)
Append: [Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling: A Perspective of Probability Theory](https://arxiv.org/abs/2505.10981)
Append: [GEM: Gaussian Embedding Modeling for Out-of-Distribution Detection in GUI Agents](https://arxiv.org/abs/2505.12842)
Append: [Think, Reflect, Create: Metacognitive Learning for Zero-Shot Robotic Planning with LLMs](https://arxiv.org/abs/2505.14899)
Append: [PRISON: Unmasking the Criminal Potential of Large Language Models](https://arxiv.org/abs/2506.16150)
Append: [Disambiguation-Centric Finetuning Makes Enterprise Tool-Calling LLMs More Realistic and Less Risky](https://arxiv.org/abs/2507.03336)
Append: [Multi-Modal Semantic Parsing for the Interpretation of Tombstone Inscriptions](https://arxiv.org/abs/2507.04377)
Append: [From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web of Agents](https://arxiv.org/abs/2507.10644)
Append: [MCPEval: Automatic MCP-based Deep Evaluation for AI Agent Models](https://arxiv.org/abs/2507.12806)
append_entries: 226
Finish: 2025-08-05 04:50:55.213939
------------------------------------------------------
Started: 2025-08-05 06:30:16.759770
Existing_entries: 1226
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1224
Summarized using GPT-3.5-turbo
Append: [Large Language Models in Argument Mining: A Survey](https://arxiv.org/abs/2506.16383)
Token length: 1831
Summarized using GPT-3.5-turbo
Append: [HuggingGraph: Understanding the Supply Chain of LLM Ecosystem](https://arxiv.org/abs/2507.14240)
append_entries: 2
Finish: 2025-08-05 06:30:22.319494
------------------------------------------------------
Started: 2025-08-05 08:25:48.526233
Existing_entries: 1002
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-05 08:25:49.019443
------------------------------------------------------
Started: 2025-08-05 10:19:55.546103
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-05 10:19:56.283912
------------------------------------------------------
Started: 2025-08-05 12:39:41.857658
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-05 12:39:42.336405
------------------------------------------------------
Started: 2025-08-05 14:22:25.777665
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-05 14:22:26.437542
------------------------------------------------------
Started: 2025-08-05 16:19:43.257598
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-05 16:19:43.968292
------------------------------------------------------
Started: 2025-08-05 18:28:55.079363
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-05 18:28:55.605056
------------------------------------------------------
Started: 2025-08-05 20:19:00.044438
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-05 20:19:00.527018
------------------------------------------------------
Started: 2025-08-05 22:18:36.550241
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-05 22:18:37.040672
------------------------------------------------------
Started: 2025-08-06 01:28:16.147395
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-06 01:28:16.641367
------------------------------------------------------
Started: 2025-08-06 03:42:08.025094
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-06 03:42:08.514607
------------------------------------------------------
Started: 2025-08-06 04:45:49.867431
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1268
Summarized using GPT-3.5-turbo
Append: [Clinically Grounded Agent-based Report Evaluation: An Interpretable Metric for Radiology Report Generation](https://arxiv.org/abs/2508.02808)
Token length: 1142
Summarized using GPT-3.5-turbo
Append: [Modeling Annotator Disagreement with Demographic-Aware Experts and Synthetic Perspectives](https://arxiv.org/abs/2508.02853)
Token length: 1677
Summarized using GPT-3.5-turbo
Append: [Highlight & Summarize: RAG without the jailbreaks](https://arxiv.org/abs/2508.02872)
Token length: 1476
Summarized using GPT-3.5-turbo
Append: [Merge-based syntax is mediated by distinct neurocognitive mechanisms: A clustering analysis of comprehension abilities in 84,000 individuals with language deficits across nine languages](https://arxiv.org/abs/2508.02885)
Token length: 1773
Summarized using GPT-3.5-turbo
Append: [Coherent Multimodal Reasoning with Iterative Self-Evaluation for Vision-Language Models](https://arxiv.org/abs/2508.02886)
Token length: 926
Summarized using GPT-3.5-turbo
Append: [SLIM-LLMs: Modeling of Style-Sensory Language RelationshipsThrough Low-Dimensional Representations](https://arxiv.org/abs/2508.02901)
Token length: 913
Summarized using GPT-3.5-turbo
Append: [Can LLMs Generate High-Quality Task-Specific Conversations?](https://arxiv.org/abs/2508.02931)
Token length: 1254
Summarized using GPT-3.5-turbo
Append: [CoCoTen: Detecting Adversarial Inputs to Large Language Models through Latent Space Features of Contextual Co-occurrence Tensors](https://arxiv.org/abs/2508.02997)
Token length: 1203
Summarized using GPT-3.5-turbo
Append: [When Algorithms Meet Artists: Topic Modeling the AI-Art Debate, 2013-2025](https://arxiv.org/abs/2508.03037)
Token length: 1577
Summarized using GPT-3.5-turbo
Append: [Privacy-Aware Decoding: Mitigating Privacy Leakage of Large Language Models in Retrieval-Augmented Generation](https://arxiv.org/abs/2508.03098)
Token length: 1577
Summarized using GPT-3.5-turbo
Append: [Token-Level Precise Attack on RAG: Searching for the Best Alternatives to Mislead Generation](https://arxiv.org/abs/2508.03110)
Token length: 1375
Summarized using GPT-3.5-turbo
Append: [Cross-lingual Opinions and Emotions Mining in Comparable Documents](https://arxiv.org/abs/2508.03112)
Token length: 1693
Summarized using GPT-3.5-turbo
Append: [Long Story Generation via Knowledge Graph and Literary Theory](https://arxiv.org/abs/2508.03137)
Token length: 1563
Summarized using GPT-3.5-turbo
Append: [RCP-Merging: Merging Long Chain-of-Thought Models with Domain-Specific Models by Considering Reasoning Capability as Prior](https://arxiv.org/abs/2508.03140)
Token length: 1751
Summarized using GPT-3.5-turbo
Append: [Light-IF: Endowing LLMs with Generalizable Reasoning via Preview and Self-Checking for Complex Instruction Following](https://arxiv.org/abs/2508.03178)
Token length: 1113
Summarized using GPT-3.5-turbo
Append: [Analyzing German Parliamentary Speeches: A Machine Learning Approach for Topic and Sentiment Classification](https://arxiv.org/abs/2508.03181)
Token length: 1471
Summarized using GPT-3.5-turbo
Append: [Beyond Content: How Grammatical Gender Shapes Visual Representation in Text-to-Image Models](https://arxiv.org/abs/2508.03199)
Token length: 730
Summarized using GPT-3.5-turbo
Append: [Current State in Privacy-Preserving Text Preprocessing for Domain-Agnostic NLP](https://arxiv.org/abs/2508.03204)
Token length: 1218
Summarized using GPT-3.5-turbo
Append: [Probing Syntax in Large Language Models: Successes and Remaining Challenges](https://arxiv.org/abs/2508.03211)
Token length: 563
Summarized using GPT-3.5-turbo
Append: [CardiffNLP at CLEARS-2025: Prompting Large Language Models for Plain Language and Easy-to-Read Text Rewriting](https://arxiv.org/abs/2508.03240)
Token length: 995
Summarized using GPT-3.5-turbo
Append: [Somatic in the East, Psychological in the West?: Investigating Clinically-Grounded Cross-Cultural Depression Symptom Expression in LLMs](https://arxiv.org/abs/2508.03247)
Token length: 1548
Summarized using GPT-3.5-turbo
Append: [RooseBERT: A New Deal For Political Language Modelling](https://arxiv.org/abs/2508.03250)
Token length: 1676
Summarized using GPT-3.5-turbo
Append: [Exploring Stability-Plasticity Trade-offs for Continual Named Entity Recognition](https://arxiv.org/abs/2508.03259)
Token length: 1386
Summarized using GPT-3.5-turbo
Append: [Pay What LLM Wants: Can LLM Simulate Economics Experiment with 522 Real-human Persona?](https://arxiv.org/abs/2508.03262)
Token length: 1437
Summarized using GPT-3.5-turbo
Append: [LECTOR: LLM-Enhanced Concept-based Test-Oriented Repetition for Adaptive Spaced Learning](https://arxiv.org/abs/2508.03275)
Token length: 1140
Summarized using GPT-3.5-turbo
Append: [Do language models accommodate their users? A study of linguistic convergence](https://arxiv.org/abs/2508.03276)
Token length: 1592
Summarized using GPT-3.5-turbo
Append: [Investigating Gender Bias in LLM-Generated Stories via Psychological Stereotypes](https://arxiv.org/abs/2508.03292)
Token length: 937
Summarized using GPT-3.5-turbo
Append: [NLP Methods May Actually Be Better Than Professors at Estimating Question Difficulty](https://arxiv.org/abs/2508.03294)
Token length: 1717
Summarized using GPT-3.5-turbo
Append: [Towards Trustworthy Multimodal Moderation via Policy-Aligned Reasoning and Hierarchical Labeling](https://arxiv.org/abs/2508.03296)
Token length: 1896
Summarized using GPT-3.5-turbo
Append: [CTTS: Collective Test-Time Scaling](https://arxiv.org/abs/2508.03333)
Token length: 1546
Summarized using GPT-3.5-turbo
Append: [Taggus: An Automated Pipeline for the Extraction of Characters' Social Networks from Portuguese Fiction Literature](https://arxiv.org/abs/2508.03358)
Token length: 1964
Summarized using GPT-3.5-turbo
Append: [Thinking with Nothinking Calibration: A New In-Context Learning Paradigm in Reasoning Large Language Models](https://arxiv.org/abs/2508.03363)
Token length: 1599
Summarized using GPT-3.5-turbo
Append: [ReDSM5: A Reddit Dataset for DSM-5 Depression Detection](https://arxiv.org/abs/2508.03399)
Token length: 1486
Summarized using GPT-3.5-turbo
Append: [Variety Is the Spice of Life: Detecting Misinformation with Dynamic Environmental Representations](https://arxiv.org/abs/2508.03420)
Token length: 1522
Summarized using GPT-3.5-turbo
Append: [LLMs Have a Heart of Stone: Demystifying the Soft Thinking Ability of Large Reasoning Models](https://arxiv.org/abs/2508.03440)
Token length: 1404
Summarized using GPT-3.5-turbo
Append: [Cropping outperforms dropout as an augmentation strategy for training self-supervised text embeddings](https://arxiv.org/abs/2508.03453)
Token length: 643
Summarized using GPT-3.5-turbo
Append: [fact check AI at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-checked Claim Retrieval](https://arxiv.org/abs/2508.03475)
Token length: 1428
Summarized using GPT-3.5-turbo
Append: [CF-RAG: A Dataset and Method for Carbon Footprint QA Using Retrieval-Augmented Generation](https://arxiv.org/abs/2508.03489)
Token length: 1379
Summarized using GPT-3.5-turbo
Append: [UPLME: Uncertainty-Aware Probabilistic Language Modelling for Robust Empathy Regression](https://arxiv.org/abs/2508.03520)
Token length: 1243
Summarized using GPT-3.5-turbo
Append: [FilBench: Can LLMs Understand and Generate Filipino?](https://arxiv.org/abs/2508.03523)
Token length: 1146
Summarized using GPT-3.5-turbo
Append: [Marito: Structuring and Building Open Multilingual Terminologies for South African NLP](https://arxiv.org/abs/2508.03529)
Token length: 1649
Summarized using GPT-3.5-turbo
Append: [EmbedGrad: Gradient-Based Prompt Optimization in Embedding Space for Large Language Models](https://arxiv.org/abs/2508.03533)
Token length: 1444
Summarized using GPT-3.5-turbo
Append: [Beyond the Surface: Enhancing LLM-as-a-Judge Alignment with Human via Internal Representations](https://arxiv.org/abs/2508.03550)
Token length: 1116
Summarized using GPT-3.5-turbo
Append: [Tackling Distribution Shift in LLM via KILO: Knowledge-Instructed Learning for Continual Adaptation](https://arxiv.org/abs/2508.03571)
Token length: 1663
Summarized using GPT-3.5-turbo
Append: [Are We on the Right Way for Assessing Document Retrieval-Augmented Generation?](https://arxiv.org/abs/2508.03644)
Token length: 1150
Summarized using GPT-3.5-turbo
Append: [Can Large Vision-Language Models Understand Multimodal Sarcasm?](https://arxiv.org/abs/2508.03654)
Token length: 1696
Summarized using GPT-3.5-turbo
Append: [CTR-Sink: Attention Sink for Language Models in Click-Through Rate Prediction](https://arxiv.org/abs/2508.03668)
Token length: 1104
Summarized using GPT-3.5-turbo
Append: [FairLangProc: A Python package for fairness in NLP](https://arxiv.org/abs/2508.03677)
Token length: 861
Summarized using GPT-3.5-turbo
Append: [More Than a Score: Probing the Impact of Prompt Specificity on LLM Code Generation](https://arxiv.org/abs/2508.03678)
Token length: 1627
Summarized using GPT-3.5-turbo
Append: [CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and Outcome Reward](https://arxiv.org/abs/2508.03686)
Append: [ToolRegistry: A Protocol-Agnostic Tool Management Library for Function-Calling LLMs](https://arxiv.org/abs/2507.10593)
Append: [Efficient Agents: Building Effective Agents While Reducing Cost](https://arxiv.org/abs/2508.02694)
Append: [Teaching at Scale: Leveraging AI to Evaluate and Elevate Engineering Education](https://arxiv.org/abs/2508.02731)
Append: [CreditARF: A Framework for Corporate Credit Rating with Annual Report and Financial Feature Integration](https://arxiv.org/abs/2508.02738)
Append: [NeuroSync: Intent-Aware Code-Based Problem Solving via Direct LLM Understanding Modification](https://arxiv.org/abs/2508.02823)
Append: [SecoustiCodec: Cross-Modal Aligned Streaming Single-Codecbook Speech Codec](https://arxiv.org/abs/2508.02849)
Append: [VisuCraft: Enhancing Large Vision-Language Models for Complex Visual-Guided Creative Content Generation via Structured Information Extraction](https://arxiv.org/abs/2508.02890)
Append: [Following Route Instructions using Large Vision-Language Models: A Comparison between Low-level and Panoramic Action Spaces](https://arxiv.org/abs/2508.02917)
Append: [Defend LLMs Through Self-Consciousness](https://arxiv.org/abs/2508.02961)
Append: [Unified Tool Integration for LLMs: A Protocol-Agnostic Approach to Function Calling](https://arxiv.org/abs/2508.02979)
Append: [AGENTiGraph: A Multi-Agent Knowledge Graph Framework for Interactive, Domain-Specific LLM Chatbots](https://arxiv.org/abs/2508.02999)
Append: [VRPO: Rethinking Value Modeling for Robust RL Training under Noisy Supervision](https://arxiv.org/abs/2508.03058)
Append: [Toward Verifiable Misinformation Detection: A Multi-Tool LLM Agent Framework](https://arxiv.org/abs/2508.03092)
Append: [ChartCap: Mitigating Hallucination of Dense Chart Captioning](https://arxiv.org/abs/2508.03164)
Append: [Understanding the Embedding Models on Hyper-relational Knowledge Graph](https://arxiv.org/abs/2508.03280)
Append: [Reliable Evaluation Protocol for Low-Precision Retrieval](https://arxiv.org/abs/2508.03306)
Append: [VLMQ: Efficient Post-Training Quantization for Large Vision-Language Models via Hessian Augmentation](https://arxiv.org/abs/2508.03351)
Append: [A Comparative Study of Neurosymbolic AI Approaches to Interpretable Logical Reasoning](https://arxiv.org/abs/2508.03366)
Append: [Draw Your Mind: Personalized Generation via Condition-Level Modeling in Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.03481)
Append: [Training Long-Context, Multi-Turn Software Engineering Agents with Reinforcement Learning](https://arxiv.org/abs/2508.03501)
Append: [MoKA: Mixture of Kronecker Adapters](https://arxiv.org/abs/2508.03527)
Append: [MultiRAG: A Knowledge-guided Framework for Mitigating Hallucination in Multi-source Retrieval Augmented Generation](https://arxiv.org/abs/2508.03553)
Append: [PyLate: Flexible Training and Retrieval for Late Interaction Models](https://arxiv.org/abs/2508.03555)
Append: [Beyond Meme Templates: Limitations of Visual Similarity Measures in Meme Matching](https://arxiv.org/abs/2508.03562)
Append: [OSINT or BULLSHINT? Exploring Open-Source Intelligence tweets about the Russo-Ukrainian War](https://arxiv.org/abs/2508.03599)
Append: [Forest vs Tree: The $(N, K)$ Trade-off in Reproducible ML Evaluation](https://arxiv.org/abs/2508.03663)
Append: [Pre-trained Transformer-Based Approach for Arabic Question Answering : A Comparative Study](https://arxiv.org/abs/2111.05671)
Append: [Bridging LLMs and KGs without Fine-Tuning: Intermediate Probing Meets Subgraph-Aware Entity Descriptions](https://arxiv.org/abs/2408.06787)
Append: [Domain-Independent Automatic Generation of Descriptive Texts for Time-Series Data](https://arxiv.org/abs/2409.16647)
Append: [From Text to Trajectory: Exploring Complex Constraint Representation and Decomposition in Safe Reinforcement Learning](https://arxiv.org/abs/2412.08920)
Append: [Think Outside the Data: Colonial Biases and Systemic Issues in Automated Moderation Pipelines for Low-Resource Languages](https://arxiv.org/abs/2501.13836)
Append: [AdaMCoT: Rethinking Cross-Lingual Factual Reasoning through Adaptive Multilingual Chain-of-Thought](https://arxiv.org/abs/2501.16154)
Append: [CLIPPER: Compression enables long-context synthetic data generation](https://arxiv.org/abs/2502.14854)
Append: [M2S: Multi-turn to Single-turn jailbreak in Red Teaming for LLMs](https://arxiv.org/abs/2503.04856)
Append: [GEMA-Score: Granular Explainable Multi-Agent Scoring Framework for Radiology Report Evaluation](https://arxiv.org/abs/2503.05347)
Append: [GPT is Devastated and LLaMA is Content: Emotion Representation Alignment in LLMs for Keyword-based Generation](https://arxiv.org/abs/2503.11881)
Append: [Ensemble Learning for Large Language Models in Text and Code Generation: A Survey](https://arxiv.org/abs/2503.13505)
Append: [ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving Systems](https://arxiv.org/abs/2503.20756)
Append: [Why do LLMs attend to the first token?](https://arxiv.org/abs/2504.02732)
Append: [Can Performant LLMs Be Ethical? Quantifying the Impact of Web Crawling Opt-Outs](https://arxiv.org/abs/2504.06219)
Append: [The Multi-Round Diagnostic RAG Framework for Emulating Clinical Reasoning](https://arxiv.org/abs/2504.07724)
Append: [Reconstructing Sepsis Trajectories from Clinical Case Reports using LLMs: the Textual Time Series Corpus for Sepsis](https://arxiv.org/abs/2504.12326)
Append: [Energy-Based Reward Models for Robust Language Model Alignment](https://arxiv.org/abs/2504.13134)
Append: [Science Hierarchography: Hierarchical Organization of Science Literature](https://arxiv.org/abs/2504.13834)
Append: [Multilingual Performance Biases of Large Language Models in Education](https://arxiv.org/abs/2504.17720)
Append: [MetaGen Blended RAG: Unlocking Zero-Shot Precision for Specialized Domain Question-Answering](https://arxiv.org/abs/2505.18247)
Append: [RIVAL: Reinforcement Learning with Iterative and Adversarial Optimization for Machine Translation](https://arxiv.org/abs/2506.05070)
Append: [ProRefine: Inference-Time Prompt Refinement with Textual Feedback](https://arxiv.org/abs/2506.05305)
Append: [ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark](https://arxiv.org/abs/2506.10960)
Append: [What Makes a Good Speech Tokenizer for LLM-Centric Speech Generation? A Systematic Study](https://arxiv.org/abs/2506.12537)
Append: [Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic Empirical Study](https://arxiv.org/abs/2506.19794)
Append: [AI4Research: A Survey of Artificial Intelligence for Scientific Research](https://arxiv.org/abs/2507.01903)
Append: [STRUCTSENSE: A Task-Agnostic Agentic Framework for Structured Information Extraction with Human-In-The-Loop Evaluation and Benchmarking](https://arxiv.org/abs/2507.03674)
Append: [MemOS: A Memory OS for AI System](https://arxiv.org/abs/2507.03724)
Append: [CLARIFID: Improving Radiology Report Generation by Reinforcing Clinically Accurate Impressions and Enforcing Detailed Findings](https://arxiv.org/abs/2507.17234)
Append: [Aging Up AAC: An Introspection on Augmentative and Alternative Communication Applications for Autistic Adults](https://arxiv.org/abs/2404.17730)
Append: [BrainECHO: Semantic Brain Signal Decoding through Vector-Quantized Spectrogram Reconstruction for Whisper-Enhanced Text Generation](https://arxiv.org/abs/2410.14971)
Append: [Talking to DINO: Bridging Self-Supervised Vision Backbones with Language for Open-Vocabulary Segmentation](https://arxiv.org/abs/2411.19331)
Append: [WSI-LLaVA: A Multimodal Large Language Model for Whole Slide Image](https://arxiv.org/abs/2412.02141)
Append: [CutPaste&Find: Efficient Multimodal Hallucination Detector with Visual-aid Knowledge Base](https://arxiv.org/abs/2502.12591)
Append: [Exploring Personalized Health Support through Data-Driven, Theory-Guided LLMs: A Case Study in Sleep Health](https://arxiv.org/abs/2502.13920)
Append: [Out-of-Context Relational Reasoning in Large Language Models](https://arxiv.org/abs/2503.10408)
Append: [A Survey of Frontiers in LLM Reasoning: Inference Scaling, Learning to Reason, and Agentic Systems](https://arxiv.org/abs/2504.09037)
Append: [Pseudo-Autoregressive Neural Codec Language Models for Efficient Zero-Shot Text-to-Speech Synthesis](https://arxiv.org/abs/2504.10352)
Append: [Antidistillation Sampling](https://arxiv.org/abs/2504.13146)
Append: [LightRetriever: A LLM-based Hybrid Retrieval Architecture with 1000x Faster Query Inference](https://arxiv.org/abs/2505.12260)
Append: [Principled Foundations for Preference Optimization](https://arxiv.org/abs/2507.07855)
Append: [Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination](https://arxiv.org/abs/2507.10532)
Append: [Document Haystack: A Long Context Multimodal Image/Document Understanding Vision LLM Benchmark](https://arxiv.org/abs/2507.15882)
append_entries: 119
Finish: 2025-08-06 04:47:43.094847
------------------------------------------------------
Started: 2025-08-06 06:29:46.150954
Existing_entries: 1119
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-06 06:29:46.519375
------------------------------------------------------
Started: 2025-08-06 08:25:43.106274
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-06 08:25:43.415572
------------------------------------------------------
Started: 2025-08-06 10:20:09.800081
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-06 10:20:10.110913
------------------------------------------------------
Started: 2025-08-06 12:39:28.089808
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-06 12:39:28.490994
------------------------------------------------------
Started: 2025-08-06 14:21:48.574261
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-06 14:21:48.971399
------------------------------------------------------
Started: 2025-08-06 16:23:39.145933
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-06 16:23:39.489331
------------------------------------------------------
Started: 2025-08-06 18:26:47.564150
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-06 18:26:47.877820
------------------------------------------------------
Started: 2025-08-06 20:20:06.735159
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-06 20:20:07.218456
------------------------------------------------------
Started: 2025-08-06 22:18:42.468758
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-06 22:18:42.780462
------------------------------------------------------
Started: 2025-08-07 01:28:56.418254
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-07 01:28:56.969368
------------------------------------------------------
Started: 2025-08-07 03:42:06.966641
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-07 03:42:07.275092
------------------------------------------------------
Started: 2025-08-07 04:45:43.784980
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1607
Summarized using GPT-3.5-turbo
Append: [How Deep Is Representational Bias in LLMs? The Cases of Caste and Religion](https://arxiv.org/abs/2508.03712)
Token length: 860
Summarized using GPT-3.5-turbo
Append: [FeynTune: Large Language Models for High-Energy Theory](https://arxiv.org/abs/2508.03716)
Token length: 1788
Summarized using GPT-3.5-turbo
Append: [Intent Aware Context Retrieval for Multi-Turn Agricultural Question Answering](https://arxiv.org/abs/2508.03719)
Token length: 1318
Summarized using GPT-3.5-turbo
Append: [Hierarchical Verification of Speculative Beams for Accelerating LLM Inference](https://arxiv.org/abs/2508.03726)
Token length: 1234
Summarized using GPT-3.5-turbo
Append: [WINELL: Wikipedia Never-Ending Updating with LLM Agents](https://arxiv.org/abs/2508.03728)
Token length: 1425
Summarized using GPT-3.5-turbo
Append: [GanitBench: A bi-lingual benchmark for evaluating mathematical reasoning in Vision Language Models](https://arxiv.org/abs/2508.03737)
Token length: 1930
Summarized using GPT-3.5-turbo
Append: [AttnTrace: Attention-based Context Traceback for Long-Context LLMs](https://arxiv.org/abs/2508.03793)
Token length: 1809
Summarized using GPT-3.5-turbo
Append: [Majority Bit-Aware Watermarking For Large Language Models](https://arxiv.org/abs/2508.03829)
Token length: 1446
Summarized using GPT-3.5-turbo
Append: [Hallucination to Truth: A Review of Fact-Checking and Factuality Evaluation in Large Language Models](https://arxiv.org/abs/2508.03860)
Token length: 776
Summarized using GPT-3.5-turbo
Append: [An Entity Linking Agent for Question Answering](https://arxiv.org/abs/2508.03865)
Token length: 1784
Summarized using GPT-3.5-turbo
Append: [Sotopia-RL: Reward Design for Social Intelligence](https://arxiv.org/abs/2508.03905)
Token length: 1571
Summarized using GPT-3.5-turbo
Append: [CoAct-1: Computer-using Agents with Coding as Actions](https://arxiv.org/abs/2508.03923)
Token length: 1649
Summarized using GPT-3.5-turbo
Append: [CAP-LLM: Context-Augmented Personalized Large Language Models for News Headline Generation](https://arxiv.org/abs/2508.03935)
Token length: 1393
Summarized using GPT-3.5-turbo
Append: [Data and AI governance: Promoting equity, ethics, and fairness in large language models](https://arxiv.org/abs/2508.03970)
Token length: 921
Summarized using GPT-3.5-turbo
Append: [Confidence-Weighted Token Set Cover for Early Hypothesis Pruning in Self-Consistency](https://arxiv.org/abs/2508.03979)
Token length: 1319
Summarized using GPT-3.5-turbo
Append: [Are Today's LLMs Ready to Explain Well-Being Concepts?](https://arxiv.org/abs/2508.03990)
Token length: 1953
Summarized using GPT-3.5-turbo
Append: [Transferring Expert Cognitive Models to Social Robots via Agentic Concept Bottleneck Models](https://arxiv.org/abs/2508.03998)
Token length: 1652
Summarized using GPT-3.5-turbo
Append: [HarmonyGuard: Toward Safety and Utility in Web Agents via Adaptive Policy Enhancement and Dual-Objective Optimization](https://arxiv.org/abs/2508.04010)
Token length: 1205
Summarized using GPT-3.5-turbo
Append: [Step More: Going Beyond Single Backpropagation in Meta Learning Based Model Editing](https://arxiv.org/abs/2508.04012)
Token length: 1504
Summarized using GPT-3.5-turbo
Append: [ZARA: Zero-shot Motion Time-Series Analysis via Knowledge and Retrieval Driven LLM Agents](https://arxiv.org/abs/2508.04038)
Token length: 1252
Summarized using GPT-3.5-turbo
Append: [Large Reasoning Models Are Autonomous Jailbreak Agents](https://arxiv.org/abs/2508.04039)
Token length: 1731
Summarized using GPT-3.5-turbo
Append: [DTPA: Dynamic Token-level Prefix Augmentation for Controllable Text Generation](https://arxiv.org/abs/2508.04047)
Token length: 1801
Summarized using GPT-3.5-turbo
Append: [PAIRS: Parametric-Verified Adaptive Information Retrieval and Selection for Efficient RAG](https://arxiv.org/abs/2508.04057)
Token length: 1166
Summarized using GPT-3.5-turbo
Append: [Efficient Strategy for Improving Large Language Model (LLM) Capabilities](https://arxiv.org/abs/2508.04073)
Token length: 766
Summarized using GPT-3.5-turbo
Append: [ToolGrad: Efficient Tool-use Dataset Generation with Textual "Gradients"](https://arxiv.org/abs/2508.04086)
Token length: 1601
Summarized using GPT-3.5-turbo
Append: [GM-PRM: A Generative Multimodal Process Reward Model for Multimodal Mathematical Reasoning](https://arxiv.org/abs/2508.04088)
Token length: 1289
Summarized using GPT-3.5-turbo
Append: [Unveiling Over-Memorization in Finetuning LLMs for Reasoning Tasks](https://arxiv.org/abs/2508.04117)
Token length: 1044
Summarized using GPT-3.5-turbo
Append: [Difficulty-Based Preference Data Selection by DPO Implicit Reward Gap](https://arxiv.org/abs/2508.04149)
Token length: 1042
Summarized using GPT-3.5-turbo
Append: [The State Of TTS: A Case Study with Human Fooling Rates](https://arxiv.org/abs/2508.04179)
Token length: 1277
Summarized using GPT-3.5-turbo
Append: [Hacking Hallucinations of MLLMs with Causal Sufficiency and Necessity](https://arxiv.org/abs/2508.04182)
Token length: 1808
Summarized using GPT-3.5-turbo
Append: [Characterizing Deep Research: A Benchmark and Formal Definition](https://arxiv.org/abs/2508.04183)
Token length: 1727
Summarized using GPT-3.5-turbo
Append: [Eliciting and Analyzing Emergent Misalignment in State-of-the-Art Large Language Models](https://arxiv.org/abs/2508.04196)
Token length: 1167
Summarized using GPT-3.5-turbo
Append: [Reasoning Beyond Labels: Measuring LLM Sentiment in Low-Resource, Culturally Nuanced Contexts](https://arxiv.org/abs/2508.04199)
Token length: 1283
Summarized using GPT-3.5-turbo
Append: [ReasoningGuard: Safeguarding Large Reasoning Models with Inference-time Safety Aha Moments](https://arxiv.org/abs/2508.04204)
Token length: 1489
Summarized using GPT-3.5-turbo
Append: [Hierarchical Text Classification Using Black Box Large Language Models](https://arxiv.org/abs/2508.04219)
Token length: 1404
Summarized using GPT-3.5-turbo
Append: [DP-GPT4MTS: Dual-Prompt Large Language Model for Textual-Numerical Time Series Forecasting](https://arxiv.org/abs/2508.04239)
Token length: 1227
Summarized using GPT-3.5-turbo
Append: [TalkDep: Clinically Grounded LLM Personas for Conversation-Centric Depression Screening](https://arxiv.org/abs/2508.04248)
Token length: 1517
Summarized using GPT-3.5-turbo
Append: [KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs](https://arxiv.org/abs/2508.04257)
Token length: 1342
Summarized using GPT-3.5-turbo
Append: [ShoppingBench: A Real-World Intent-Grounded Shopping Benchmark for LLM-based Agents](https://arxiv.org/abs/2508.04266)
Token length: 1572
Summarized using GPT-3.5-turbo
Append: [A Few Words Can Distort Graphs: Knowledge Poisoning Attacks on Graph-based Retrieval-Augmented Generation of Large Language Models](https://arxiv.org/abs/2508.04276)
Token length: 1285
Summarized using GPT-3.5-turbo
Append: [Beyond the Leaderboard: Rethinking Medical Benchmarks for Large Language Models](https://arxiv.org/abs/2508.04325)
Token length: 1935
Summarized using GPT-3.5-turbo
Append: [Modelling and Classifying the Components of a Literature Review](https://arxiv.org/abs/2508.04337)
Token length: 1195
Summarized using GPT-3.5-turbo
Append: [GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy Entropy](https://arxiv.org/abs/2508.04349)
Token length: 1293
Summarized using GPT-3.5-turbo
Append: [Chain of Questions: Guiding Multimodal Curiosity in Language Models](https://arxiv.org/abs/2508.04350)
Token length: 494
Summarized using GPT-3.5-turbo
Append: [AIC CTU@FEVER 8: On-premise fact checking through long context RAG](https://arxiv.org/abs/2508.04390)
Token length: 1812
Summarized using GPT-3.5-turbo
Append: [Improving Crash Data Quality with Large Language Models: Evidence from Secondary Crash Narratives in Kentucky](https://arxiv.org/abs/2508.04399)
Token length: 1964
Summarized using GPT-3.5-turbo
Append: [Why are LLMs' abilities emergent?](https://arxiv.org/abs/2508.04401)
Token length: 929
Summarized using GPT-3.5-turbo
Append: [What Do Humans Hear When Interacting? Experiments on Selective Listening for Evaluating ASR of Spoken Dialogue Systems](https://arxiv.org/abs/2508.04402)
Token length: 792
Summarized using GPT-3.5-turbo
Append: [Dialogue Response Prefetching Based on Semantic Similarity and Prediction Confidence of Language Model](https://arxiv.org/abs/2508.04403)
Token length: 1362
Summarized using GPT-3.5-turbo
Append: [Evaluating, Synthesizing, and Enhancing for Customer Support Conversation](https://arxiv.org/abs/2508.04423)
Append: [StepFun-Formalizer: Unlocking the Autoformalization Potential of LLMs through Knowledge-Reasoning Fusion](https://arxiv.org/abs/2508.04440)
Append: [Automated Generation of Curriculum-Aligned Multiple-Choice Questions for Malaysian Secondary Mathematics Using Generative AI](https://arxiv.org/abs/2508.04442)
Append: [CALE : Concept-Aligned Embeddings for Both Within-Lemma and Inter-Lemma Sense Differentiation](https://arxiv.org/abs/2508.04494)
Append: [StyliTruth : Unlocking Stylized yet Truthful LLM Generation via Disentangled Steering](https://arxiv.org/abs/2508.04530)
Append: [Unveiling the Landscape of Clinical Depression Assessment: From Behavioral Signatures to Psychiatric Reasoning](https://arxiv.org/abs/2508.04531)
Append: [Beyond Brainstorming: What Drives High-Quality Scientific Ideas? Lessons from Multi-Agent Collaboration](https://arxiv.org/abs/2508.04575)
Append: [Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning](https://arxiv.org/abs/2508.04581)
Append: [TURA: Tool-Augmented Unified Retrieval Agent for AI Search](https://arxiv.org/abs/2508.04604)
Append: [Lightweight Transformers for Zero-Shot and Fine-Tuned Text-to-SQL Generation Using Spider](https://arxiv.org/abs/2508.04623)
Append: [P-Aligner: Enabling Pre-Alignment of Language Models via Principled Instruction Synthesis](https://arxiv.org/abs/2508.04626)
Append: [IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2508.04632)
Append: [Can NLP Tackle Hate Speech in the Real World? Stakeholder-Informed Feedback and Survey on Counterspeech](https://arxiv.org/abs/2508.04638)
Append: [Multi-module GRPO: Composing Policy Gradients and Prompt Optimization for Language Model Programs](https://arxiv.org/abs/2508.04660)
Append: [Sculptor: Empowering LLMs with Cognitive Agency via Active Context Management](https://arxiv.org/abs/2508.04664)
Append: [GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay](https://arxiv.org/abs/2508.04676)
Append: [FaST: Feature-aware Sampling and Tuning for Personalized Preference Alignment with Limited Data](https://arxiv.org/abs/2508.04698)
Append: [Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during Multi-Hop Analysis](https://arxiv.org/abs/2508.04699)
Append: [MD-LLM-1: A Large Language Model for Molecular Dynamics](https://arxiv.org/abs/2508.03709)
Append: [A Social Data-Driven System for Identifying Estate-related Events and Topics](https://arxiv.org/abs/2508.03711)
Append: [Health Insurance Coverage Rule Interpretation Corpus: Law, Policy, and Medical Guidance for Health Insurance Coverage Understanding](https://arxiv.org/abs/2508.03718)
Append: [CX-Mind: A Pioneering Multimodal Large Language Model for Interleaved Reasoning in Chest X-ray via Curriculum-Guided Reinforcement Learning](https://arxiv.org/abs/2508.03733)
Append: [GTPO: Trajectory-Based Policy Optimization in Large Language Models](https://arxiv.org/abs/2508.03772)
Append: [MegaWika 2: A More Comprehensive Multilingual Collection of Articles and their Sources](https://arxiv.org/abs/2508.03828)
Append: [ASTRA: Autonomous Spatial-Temporal Red-teaming for AI Software Assistants](https://arxiv.org/abs/2508.03936)
Append: [Accelerating Scientific Discovery with Multi-Document Summarization of Impact-Ranked Papers](https://arxiv.org/abs/2508.03962)
Append: [ConvMix: A Mixed-Criteria Data Augmentation Framework for Conversational Dense Retrieval](https://arxiv.org/abs/2508.04001)
Append: [AgREE: Agentic Reasoning for Knowledge Graph Completion on Emerging Entities](https://arxiv.org/abs/2508.04118)
Append: [COPO: Consistency-Aware Policy Optimization](https://arxiv.org/abs/2508.04138)
Append: [Multilingual Source Tracing of Speech Deepfakes: A First Benchmark](https://arxiv.org/abs/2508.04143)
Append: [ToxicTAGS: Decoding Toxic Memes with Rich Tag Annotations](https://arxiv.org/abs/2508.04166)
Append: [Graph Representation Learning with Massive Unlabeled Data for Rumor Detection](https://arxiv.org/abs/2508.04252)
Append: [Beyond Pixels: Exploring DOM Downsampling for LLM-Based Web Agents](https://arxiv.org/abs/2508.04412)
Append: [FrEVL: Leveraging Frozen Pretrained Embeddings for Efficient Vision-Language Understanding](https://arxiv.org/abs/2508.04469)
Append: [OS Agents: A Survey on MLLM-based Agents for General Computing Devices Use](https://arxiv.org/abs/2508.04482)
Append: [Causal Reflection with Language Models](https://arxiv.org/abs/2508.04495)
Append: [Analyzing and Mitigating Object Hallucination: A Training Bias Perspective](https://arxiv.org/abs/2508.04567)
Append: [Do Recommender Systems Really Leverage Multimodal Content? A Comprehensive Analysis on Multimodal Representations for Recommendation](https://arxiv.org/abs/2508.04571)
Append: [Position: The Current AI Conference Model is Unsustainable! Diagnosing the Crisis of Centralized AI Conference](https://arxiv.org/abs/2508.04586)
Append: [Query Attribute Modeling: Improving search relevance with Semantic Search and Meta Data Filtering](https://arxiv.org/abs/2508.04683)
Append: [SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience](https://arxiv.org/abs/2508.04700)
Append: [How Well Do LLMs Represent Values Across Cultures? Empirical Analysis of LLM Responses Based on Hofstede Cultural Dimensions](https://arxiv.org/abs/2406.14805)
Append: [Fairness Definitions in Language Models Explained](https://arxiv.org/abs/2407.18454)
Append: [Parse Trees Guided LLM Prompt Compression](https://arxiv.org/abs/2409.15395)
Append: [Human Bias in the Face of AI: Examining Human Judgment Against Text Labeled as AI Generated](https://arxiv.org/abs/2410.03723)
Append: [A Survey of Conversational Search](https://arxiv.org/abs/2410.15576)
Append: [AUTALIC: A Dataset for Anti-AUTistic Ableist Language In Context](https://arxiv.org/abs/2410.16520)
Append: [CLaSP: Learning Concepts for Time-Series Signals from Natural Language Supervision](https://arxiv.org/abs/2411.08397)
Append: [FactEHR: A Dataset for Evaluating Factuality in Clinical Notes Using LLMs](https://arxiv.org/abs/2412.12422)
Append: [Improved Unbiased Watermark for Large Language Models](https://arxiv.org/abs/2502.11268)
Append: [Evaluating the Robustness of Multimodal Agents Against Active Environmental Injection Attacks](https://arxiv.org/abs/2502.13053)
Append: [Mixup Model Merge: Enhancing Model Merging Performance through Randomized Linear Interpolation](https://arxiv.org/abs/2502.15434)
Append: [Evaluating Robustness of LLMs in Question Answering on Multilingual Noisy OCR Data](https://arxiv.org/abs/2502.16781)
Append: [Assessing Agentic Large Language Models in Multilingual National Bias](https://arxiv.org/abs/2502.17945)
Append: [Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning](https://arxiv.org/abs/2503.09516)
Append: [The Impact of Item-Writing Flaws on Difficulty and Discrimination in Item Response Theory](https://arxiv.org/abs/2503.10533)
Append: [I Have Covered All the Bases Here: Interpreting Reasoning Features in Large Language Models via Sparse Autoencoders](https://arxiv.org/abs/2503.18878)
Append: [Learning Optimal Prompt Ensemble for Multi-source Visual Prompt Transfer](https://arxiv.org/abs/2504.12311)
Append: [CRAB: A Benchmark for Evaluating Curation of Retrieval-Augmented LLMs in Biomedicine](https://arxiv.org/abs/2504.12342)
Append: [Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models](https://arxiv.org/abs/2504.14194)
Append: [Evaluating Multi-Hop Reasoning in Large Language Models: A Chemistry-Centric Case Study](https://arxiv.org/abs/2504.16414)
Append: [Improving the fact-checking performance of language models by relying on their entailment ability](https://arxiv.org/abs/2505.15050)
Append: [Explain Less, Understand More: Jargon Detection via Personalized Parameter-Efficient Fine-tuning](https://arxiv.org/abs/2505.16227)
Append: [Text-Only Reasoning Unleashes Zero-Shot Multimodal Evaluators](https://arxiv.org/abs/2505.18601)
Append: [Emotion-o1: Adaptive Long Reasoning for Emotion Understanding in LLMs](https://arxiv.org/abs/2505.22548)
Append: [Model Internal Sleuthing: Finding Lexical Identity and Inflectional Morphology in Modern Language Models](https://arxiv.org/abs/2506.02132)
Append: [FinanceReasoning: Benchmarking Financial Numerical Reasoning More Credible, Comprehensive and Challenging](https://arxiv.org/abs/2506.05828)
Append: [NameTag 3: A Tool and a Service for Multilingual/Multitagset NER](https://arxiv.org/abs/2506.05949)
Append: [UITron-Speech: Towards Automated GUI Agents Based on Speech Instructions](https://arxiv.org/abs/2506.11127)
Append: [How Far Can LLMs Improve from Experience? Measuring Test-Time Learning Ability in LLMs with Human Comparison](https://arxiv.org/abs/2506.14448)
Append: [R1-RE: Cross-Domain Relation Extraction with RLVR](https://arxiv.org/abs/2507.04642)
Append: [From Queries to Criteria: Understanding How Astronomers Evaluate LLMs](https://arxiv.org/abs/2507.15715)
Append: [Strong Priority and Determinacy in Timed CCS](https://arxiv.org/abs/2403.04618)
Append: [Basis Selection: Low-Rank Decomposition of Pretrained Large Language Models for Target Applications](https://arxiv.org/abs/2405.15877)
Append: [AVG-LLaVA: An Efficient Large Multimodal Model with Adaptive Visual Granularity](https://arxiv.org/abs/2410.02745)
Append: [Beyond Adapter Retrieval: Latent Geometry-Preserving Composition via Sparse Task Projection](https://arxiv.org/abs/2410.09908)
Append: [Automatically Interpreting Millions of Features in Large Language Models](https://arxiv.org/abs/2410.13928)
Append: [p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay](https://arxiv.org/abs/2412.04449)
Append: [Ultra Memory-Efficient On-FPGA Training of Transformers via Tensor-Compressed Optimization](https://arxiv.org/abs/2501.06663)
Append: [Tool Unlearning for Tool-Augmented LLMs](https://arxiv.org/abs/2502.01083)
Append: [SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild](https://arxiv.org/abs/2503.18892)
Append: [CAIN: Hijacking LLM-Humans Conversations via Malicious System Prompts](https://arxiv.org/abs/2505.16888)
Append: [On the Fundamental Impossibility of Hallucination Control in Large Language Models](https://arxiv.org/abs/2506.06382)
Append: [IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks](https://arxiv.org/abs/2506.16402)
Append: [Thought Anchors: Which LLM Reasoning Steps Matter?](https://arxiv.org/abs/2506.19143)
Append: [A Comparative Study of Specialized LLMs as Dense Retrievers](https://arxiv.org/abs/2507.03958)
Append: [ContextASR-Bench: A Massive Contextual Speech Recognition Benchmark](https://arxiv.org/abs/2507.05727)
Append: [Bob's Confetti: Phonetic Memorization Attacks in Music and Video Generation](https://arxiv.org/abs/2507.17937)
append_entries: 137
Finish: 2025-08-07 04:47:50.029372
------------------------------------------------------
Started: 2025-08-07 06:28:59.254284
Existing_entries: 1137
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1377
Summarized using GPT-3.5-turbo
Append: [Towards Domain Specification of Embedding Models in Medicine](https://arxiv.org/abs/2507.19407)
append_entries: 1
Finish: 2025-08-07 06:29:02.303389
------------------------------------------------------
Started: 2025-08-07 08:25:38.131385
Existing_entries: 1001
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-07 08:25:38.462969
------------------------------------------------------
Started: 2025-08-07 10:19:46.483666
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-07 10:19:46.842038
------------------------------------------------------
Started: 2025-08-07 12:39:20.337362
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-07 12:39:20.750346
------------------------------------------------------
Started: 2025-08-07 14:21:13.627163
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-07 14:21:14.006944
------------------------------------------------------
Started: 2025-08-07 16:23:53.197891
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-07 16:23:53.527269
------------------------------------------------------
Started: 2025-08-07 18:28:21.297345
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-07 18:28:21.657415
------------------------------------------------------
Started: 2025-08-07 20:19:55.916731
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-07 20:19:56.254755
------------------------------------------------------
Started: 2025-08-07 22:18:13.406030
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-07 22:18:13.739961
------------------------------------------------------
Started: 2025-08-08 01:28:13.590834
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-08 01:28:13.924571
------------------------------------------------------
Started: 2025-08-08 03:41:36.467435
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-08 03:41:36.799197
------------------------------------------------------
Started: 2025-08-08 04:45:54.462302
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 996
Summarized using GPT-3.5-turbo
Append: [Enhancing Dialogue Annotation with Speaker Characteristics Leveraging a Frozen LLM](https://arxiv.org/abs/2508.04795)
Token length: 1085
Summarized using GPT-3.5-turbo
Append: [Parity-Aware Byte-Pair Encoding: Improving Cross-lingual Fairness in Tokenization](https://arxiv.org/abs/2508.04796)
Token length: 727
Summarized using GPT-3.5-turbo
Append: [Pitch Accent Detection improves Pretrained Automatic Speech Recognition](https://arxiv.org/abs/2508.04814)
Token length: 1392
Summarized using GPT-3.5-turbo
Append: [Persistent Instability in LLM's Personality Measurements: Effects of Scale, Reasoning, and Conversation History](https://arxiv.org/abs/2508.04826)
Token length: 1422
Summarized using GPT-3.5-turbo
Append: [RCR-Router: Efficient Role-Aware Context Routing for Multi-Agent LLM Systems with Structured Memory](https://arxiv.org/abs/2508.04903)
Token length: 1181
Summarized using GPT-3.5-turbo
Append: [I Think, Therefore I Am Under-Qualified? A Benchmark for Evaluating Linguistic Shibboleth Detection in LLM Hiring Evaluations](https://arxiv.org/abs/2508.04939)
Token length: 1184
Summarized using GPT-3.5-turbo
Append: [Towards Robust Evaluation of Visual Activity Recognition: Resolving Verb Ambiguity with Sense Clustering](https://arxiv.org/abs/2508.04945)
Token length: 1593
Summarized using GPT-3.5-turbo
Append: [A Multi-Stage Large Language Model Framework for Extracting Suicide-Related Social Determinants of Health](https://arxiv.org/abs/2508.05003)
Token length: 1435
Summarized using GPT-3.5-turbo
Append: [Dialogues Aspect-based Sentiment Quadruple Extraction via Structural Entropy Minimization Partitioning](https://arxiv.org/abs/2508.05023)
Token length: 1144
Summarized using GPT-3.5-turbo
Append: [Evaluation of LLMs in AMR Parsing](https://arxiv.org/abs/2508.05028)
Token length: 1429
Summarized using GPT-3.5-turbo
Append: [Align, Don't Divide: Revisiting the LoRA Architecture in Multi-Task Learning](https://arxiv.org/abs/2508.05078)
Token length: 1111
Summarized using GPT-3.5-turbo
Append: [Multimodal Fact Checking with Unified Visual, Textual, and Contextual Representations](https://arxiv.org/abs/2508.05097)
Token length: 1220
Summarized using GPT-3.5-turbo
Append: [BEE-RAG: Balanced Entropy Engineering for Retrieval-Augmented Generation](https://arxiv.org/abs/2508.05100)
Token length: 1393
Summarized using GPT-3.5-turbo
Append: [Attention Basin: Why Contextual Position Matters in Large Language Models](https://arxiv.org/abs/2508.05128)
Token length: 1333
Summarized using GPT-3.5-turbo
Append: [Towards Assessing Medical Ethics from Knowledge to Practice](https://arxiv.org/abs/2508.05132)
Token length: 844
Summarized using GPT-3.5-turbo
Append: [ATLANTIS at SemEval-2025 Task 3: Detecting Hallucinated Text Spans in Question Answering](https://arxiv.org/abs/2508.05179)
Token length: 1507
Summarized using GPT-3.5-turbo
Append: [Resource-Limited Joint Multimodal Sentiment Reasoning and Classification via Chain-of-Thought Enhancement and Distillation](https://arxiv.org/abs/2508.05234)
Token length: 1492
Summarized using GPT-3.5-turbo
Append: [Pruning Large Language Models by Identifying and Preserving Functional Networks](https://arxiv.org/abs/2508.05239)
Token length: 1636
Summarized using GPT-3.5-turbo
Append: [CodeBoost: Boosting Code LLMs by Squeezing Knowledge from Code Snippets with RL](https://arxiv.org/abs/2508.05242)
Token length: 1715
Summarized using GPT-3.5-turbo
Append: [ASCoT: An Adaptive Self-Correction Chain-of-Thought Method for Late-Stage Fragility in LLMs](https://arxiv.org/abs/2508.05282)
Token length: 1549
Summarized using GPT-3.5-turbo
Append: [Decision-Making with Deliberation: Meta-reviewing as a Document-grounded Dialogue](https://arxiv.org/abs/2508.05283)
Token length: 859
Summarized using GPT-3.5-turbo
Append: [SONAR-LLM: Autoregressive Transformer that Thinks in Sentence Embeddings and Speaks in Tokens](https://arxiv.org/abs/2508.05305)
Token length: 1603
Summarized using GPT-3.5-turbo
Append: [Efficient Reasoning for Large Reasoning Language Models via Certainty-Guided Reflection Suppression](https://arxiv.org/abs/2508.05337)
Token length: 1620
Summarized using GPT-3.5-turbo
Append: [Evaluation of a Sign Language Avatar on Comprehensibility, User Experience \& Acceptability](https://arxiv.org/abs/2508.05358)
Token length: 1418
Summarized using GPT-3.5-turbo
Append: [Can Language Models Critique Themselves? Investigating Self-Feedback for Retrieval Augmented Generation at BioASQ 2025](https://arxiv.org/abs/2508.05366)
Token length: 1025
Summarized using GPT-3.5-turbo
Append: [The TUB Sign Language Corpus Collection](https://arxiv.org/abs/2508.05374)
Token length: 1314
Summarized using GPT-3.5-turbo
Append: [MyCulture: Exploring Malaysia's Diverse Culture under Low-Resource Language Constraints](https://arxiv.org/abs/2508.05429)
Token length: 1293
Summarized using GPT-3.5-turbo
Append: [LLMEval-3: A Large-Scale Longitudinal Study on Robust and Fair Evaluation of Large Language Models](https://arxiv.org/abs/2508.05452)
Token length: 1393
Summarized using GPT-3.5-turbo
Append: [TASE: Token Awareness and Structured Evaluation for Multilingual Language Models](https://arxiv.org/abs/2508.05468)
Token length: 836
Summarized using GPT-3.5-turbo
Append: [Rethinking Creativity Evaluation: A Critical Analysis of Existing Creativity Evaluations](https://arxiv.org/abs/2508.05470)
Token length: 1581
Summarized using GPT-3.5-turbo
Append: [LAG: Logic-Augmented Generation from a Cartesian Perspective](https://arxiv.org/abs/2508.05509)
Token length: 1763
Summarized using GPT-3.5-turbo
Append: [The World According to LLMs: How Geographic Origin Influences LLMs' Entity Deduction Capabilities](https://arxiv.org/abs/2508.05525)
Token length: 1188
Summarized using GPT-3.5-turbo
Append: [CoCoLex: Confidence-guided Copy-based Decoding for Grounded Legal Text Generation](https://arxiv.org/abs/2508.05534)
Token length: 1328
Summarized using GPT-3.5-turbo
Append: [Conformal Sets in Multiple-Choice Question Answering under Black-Box Settings with Provable Coverage Guarantees](https://arxiv.org/abs/2508.05544)
Token length: 1264
Summarized using GPT-3.5-turbo
Append: [Do Political Opinions Transfer Between Western Languages? An Analysis of Unaligned and Aligned Multilingual LLMs](https://arxiv.org/abs/2508.05553)
Token length: 1641
Summarized using GPT-3.5-turbo
Append: [MathSmith: Towards Extremely Hard Mathematical Reasoning by Forging Synthetic Problems with a Reinforced Policy](https://arxiv.org/abs/2508.05592)
Token length: 1792
Summarized using GPT-3.5-turbo
Append: [Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2508.05613)
Token length: 1712
Summarized using GPT-3.5-turbo
Append: [OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks](https://arxiv.org/abs/2508.05614)
Token length: 1278
Summarized using GPT-3.5-turbo
Append: [Learning to Reason for Factuality](https://arxiv.org/abs/2508.05618)
Token length: 1407
Summarized using GPT-3.5-turbo
Append: [How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in Multi-Turn Conversations](https://arxiv.org/abs/2508.05625)
Token length: 1151
Summarized using GPT-3.5-turbo
Append: [H-Net++: Hierarchical Dynamic Chunking for Tokenizer-Free Language Modelling in Morphologically-Rich Languages](https://arxiv.org/abs/2508.05628)
Token length: 1883
Summarized using GPT-3.5-turbo
Append: [Prescriptive Agents based on Rag for Automated Maintenance (PARAM)](https://arxiv.org/abs/2508.04714)
Token length: 1595
Summarized using GPT-3.5-turbo
Append: [Federal Reserve Communication and the COVID-19 Pandemic](https://arxiv.org/abs/2508.04830)
Token length: 1415
Summarized using GPT-3.5-turbo
Append: [Fine-Tuning Small Language Models (SLMs) for Autonomous Web-based Geographical Information Systems (AWebGIS)](https://arxiv.org/abs/2508.04846)
Token length: 1433
Summarized using GPT-3.5-turbo
Append: [Advancing Hate Speech Detection with Transformers: Insights from the MetaHate](https://arxiv.org/abs/2508.04913)
Token length: 1184
Summarized using GPT-3.5-turbo
Append: [ConfAgents: A Conformal-Guided Multi-Agent Framework for Cost-Efficient Medical Diagnosis](https://arxiv.org/abs/2508.04915)
Token length: 1187
Summarized using GPT-3.5-turbo
Append: [REINA: Regularized Entropy Information-Based Loss for Efficient Simultaneous Speech Translation](https://arxiv.org/abs/2508.04946)
Token length: 1317
Summarized using GPT-3.5-turbo
Append: [R-Zero: Self-Evolving Reasoning LLM from Zero Data](https://arxiv.org/abs/2508.05004)
Token length: 1582
Summarized using GPT-3.5-turbo
Append: [Can Large Language Models Integrate Spatial Data? Empirical Insights into Reasoning Strengths and Computational Weaknesses](https://arxiv.org/abs/2508.05009)
Token length: 1529
Summarized using GPT-3.5-turbo
Append: [Making Prompts First-Class Citizens for Adaptive LLM Pipelines](https://arxiv.org/abs/2508.05012)
Append: [A Study of the Framework and Real-World Applications of Language Embedding for 3D Scene Understanding](https://arxiv.org/abs/2508.05064)
Append: [Cognitive Duality for Adaptive Web Agents](https://arxiv.org/abs/2508.05081)
Append: [JPS: Jailbreak Multimodal Large Language Models with Collaborative Visual Perturbation and Textual Steering](https://arxiv.org/abs/2508.05087)
Append: [Exploring Superior Function Calls via Reinforcement Learning](https://arxiv.org/abs/2508.05118)
Append: [Navigating Through Paper Flood: Advancing LLM-based Paper Evaluation through Domain-Aware Retrieval and Latent Reasoning](https://arxiv.org/abs/2508.05129)
Append: [Speech LLMs in Low-Resource Scenarios: Data Volume Requirements and the Impact of Pretraining on High-Resource Languages](https://arxiv.org/abs/2508.05149)
Append: [Aligning LLMs on a Budget: Inference-Time Alignment with Heuristic Reward Models](https://arxiv.org/abs/2508.05165)
Append: [Posterior-GRPO: Rewarding Reasoning Processes in Code Generation](https://arxiv.org/abs/2508.05170)
Append: [QA-Dragon: Query-Aware Dynamic RAG System for Knowledge-Intensive Visual Question Answering](https://arxiv.org/abs/2508.05197)
Append: [FAITH: A Framework for Assessing Intrinsic Tabular Hallucinations in finance](https://arxiv.org/abs/2508.05201)
Append: [Understanding and Mitigating Errors of LLM-Generated RTL Code](https://arxiv.org/abs/2508.05266)
Append: [A Novel Architecture for Symbolic Reasoning with Decision Trees and LLM Agents](https://arxiv.org/abs/2508.05311)
Append: [Bench-2-CoP: Can We Trust Benchmarking for EU AI Compliance?](https://arxiv.org/abs/2508.05464)
Append: [Can Large Language Models Generate Effective Datasets for Emotion Recognition in Conversations?](https://arxiv.org/abs/2508.05474)
Append: [MELLA: Bridging Linguistic Capability and Cultural Groundedness for Low-Resource Language MLLMs](https://arxiv.org/abs/2508.05502)
Append: [Mixed-Initiative Dialog for Human-Robot Collaborative Manipulation](https://arxiv.org/abs/2508.05535)
Append: [SPGISpeech 2.0: Transcribed multi-speaker financial audio for speaker-tagged transcription](https://arxiv.org/abs/2508.05554)
Append: [Fairy$\pm i$: the First 2-bit Complex LLM with All Parameters in $\{\pm1, \pm i\}$](https://arxiv.org/abs/2508.05571)
Append: [Iterative Learning of Computable Phenotypes for Treatment Resistant Hypertension using Large Language Models](https://arxiv.org/abs/2508.05581)
Append: [Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and Vision](https://arxiv.org/abs/2508.05606)
Append: [Test-Time Reinforcement Learning for GUI Grounding via Region Consistency](https://arxiv.org/abs/2508.05615)
Append: [A Latent-Variable Model for Intrinsic Probing](https://arxiv.org/abs/2201.08214)
Append: [Probabilities of Chat LLMs Are Miscalibrated but Still Predict Correctness on Multiple-Choice Q&A](https://arxiv.org/abs/2402.13213)
Append: [Understanding Large Language Model Behaviors through Interactive Counterfactual Generation and Analysis](https://arxiv.org/abs/2405.00708)
Append: [CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for Multi-label Social Media Text Classification in Disaster Informatics](https://arxiv.org/abs/2406.15477)
Append: [When in Doubt, Cascade: Towards Building Efficient and Capable Guardrails](https://arxiv.org/abs/2407.06323)
Append: [CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation](https://arxiv.org/abs/2409.02098)
Append: [Medal Matters: Probing LLMs' Failure Cases Through Olympic Rankings](https://arxiv.org/abs/2409.06518)
Append: [WhisperNER: Unified Open Named Entity and Speech Recognition](https://arxiv.org/abs/2409.08107)
Append: [MedHalu: Hallucinations in Responses to Healthcare Queries by Large Language Models](https://arxiv.org/abs/2409.19492)
Append: [From Code to Correctness: Closing the Last Mile of Code Generation with Hierarchical Debugging](https://arxiv.org/abs/2410.01215)
Append: [Recent Advances in Speech Language Models: A Survey](https://arxiv.org/abs/2410.03751)
Append: [BloomWise: Enhancing Problem-Solving capabilities of Large Language Models using Bloom's-Taxonomy-Inspired Prompts](https://arxiv.org/abs/2410.04094)
Append: [Scaling Laws For Mixed Quantization](https://arxiv.org/abs/2410.06722)
Append: [Data Processing for the OpenGPT-X Model Family](https://arxiv.org/abs/2410.08800)
Append: [Large Language Models Still Exhibit Bias in Long Text](https://arxiv.org/abs/2410.17519)
Append: [GuARD: Effective Anomaly Detection through a Text-Rich and Graph-Informed Language Model](https://arxiv.org/abs/2412.03930)
Append: [Efficient Knowledge Injection in LLMs via Self-Distillation](https://arxiv.org/abs/2412.14964)
Append: [Multi-Agents Based on Large Language Models for Knowledge-based Visual Question Answering](https://arxiv.org/abs/2412.18351)
Append: [Can open source large language models be used for tumor documentation in Germany? -- An evaluation on urological doctors' notes](https://arxiv.org/abs/2501.12106)
Append: [RLTHF: Targeted Human Feedback for LLM Alignment](https://arxiv.org/abs/2502.13417)
Append: [Which Questions Improve Learning the Most? Utility Estimation of Questions with LM-based Simulations](https://arxiv.org/abs/2502.17383)
Append: [Language Model Uncertainty Quantification with Attention Chain](https://arxiv.org/abs/2503.19168)
Append: [You Cannot Feed Two Birds with One Score: the Accuracy-Naturalness Tradeoff in Translation](https://arxiv.org/abs/2503.24013)
Append: [SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers](https://arxiv.org/abs/2504.00255)
Append: [PolyGuard: A Multilingual Safety Moderation Tool for 17 Languages](https://arxiv.org/abs/2504.04377)
Append: [DEL: Context-Aware Dynamic Exit Layer for Efficient Self-Speculative Decoding](https://arxiv.org/abs/2504.05598)
Append: [Tell Me Who Your Students Are: GPT Can Generate Valid Multiple-Choice Questions When Students' (Mis)Understanding Is Hinted](https://arxiv.org/abs/2505.05815)
Append: [Enabling On-Device Medical AI Assistants via Input-Driven Saliency Adaptation](https://arxiv.org/abs/2506.11105)
Append: [Improving Factuality for Dialogue Response Generation via Graph-Based Knowledge Augmentation](https://arxiv.org/abs/2506.12496)
Append: [Can Vision Language Models Understand Mimed Actions?](https://arxiv.org/abs/2506.21586)
Append: [McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2507.02088)
Append: [DOTS: Learning to Reason Dynamically in LLMs via Optimal Reasoning Trajectories Search](https://arxiv.org/abs/2410.03864)
Append: [Verbalized Representation Learning for Interpretable Few-Shot Generalization](https://arxiv.org/abs/2411.18651)
Append: [Enhancing Code LLMs with Reinforcement Learning in Code Generation: A Survey](https://arxiv.org/abs/2412.20367)
Append: [Human Cognitive Benchmarks Reveal Foundational Visual Gaps in MLLMs](https://arxiv.org/abs/2502.16435)
Append: [Semantic Integrity Constraints: Declarative Guardrails for AI-Augmented Data Processing Systems](https://arxiv.org/abs/2503.00600)
Append: [Teaching LLMs How to Learn with Contextual Fine-Tuning](https://arxiv.org/abs/2503.09032)
Append: [R2Vul: Learning to Reason about Software Vulnerabilities with Reinforcement Learning and Structured Reasoning Distillation](https://arxiv.org/abs/2504.04699)
Append: [ArXivBench: When You Should Avoid Using ChatGPT for Academic Writing](https://arxiv.org/abs/2504.10496)
Append: [JEPA4Rec: Learning Effective Language Representations for Sequential Recommendation via Joint Embedding Predictive Architecture](https://arxiv.org/abs/2504.10512)
Append: [Explainable Recommendation with Simulated Human Feedback](https://arxiv.org/abs/2504.14147)
Append: [Optimizing LLM-Based Multi-Agent System with Textual Feedback: A Case Study on Software Development](https://arxiv.org/abs/2505.16086)
Append: [Learning to Diagnose Privately: DP-Powered LLMs for Radiology Report Classification](https://arxiv.org/abs/2506.04450)
Append: [Hierarchical Budget Policy Optimization for Adaptive Reasoning](https://arxiv.org/abs/2507.15844)
Append: [SafeWork-R1: Coevolving Safety and Intelligence under the AI-45$^{\circ}$ Law](https://arxiv.org/abs/2507.18576)
append_entries: 116
Finish: 2025-08-08 04:48:00.499458
------------------------------------------------------
Started: 2025-08-08 06:29:11.191971
Existing_entries: 1116
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1425
Summarized using GPT-3.5-turbo
Append: [Efficient Attention Mechanisms for Large Language Models: A Survey](https://arxiv.org/abs/2507.19595)
append_entries: 1
Finish: 2025-08-08 06:29:13.752636
------------------------------------------------------
Started: 2025-08-08 08:25:30.198670
Existing_entries: 1001
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-08 08:25:30.494890
------------------------------------------------------
Started: 2025-08-08 10:19:21.843717
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-08 10:19:22.174649
------------------------------------------------------
Started: 2025-08-08 12:38:12.576363
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-08 12:38:12.877998
------------------------------------------------------
Started: 2025-08-08 14:19:53.858650
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-08 14:19:54.182355
------------------------------------------------------
Started: 2025-08-08 16:20:29.024671
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-08 16:20:29.323043
------------------------------------------------------
Started: 2025-08-08 18:24:24.632695
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-08 18:24:25.023877
------------------------------------------------------
Started: 2025-08-08 20:19:19.535420
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-08 20:19:19.857459
------------------------------------------------------
Started: 2025-08-08 22:16:49.235747
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-08 22:16:49.541114
------------------------------------------------------
Started: 2025-08-09 01:21:19.271131
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-09 01:21:19.562446
------------------------------------------------------
Started: 2025-08-09 03:17:59.636135
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-09 03:17:59.969124
------------------------------------------------------
Started: 2025-08-09 04:26:39.974153
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-09 04:26:40.033186
------------------------------------------------------
Started: 2025-08-09 06:25:11.649343
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-09 06:25:11.706152
------------------------------------------------------
Started: 2025-08-09 08:21:20.810969
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-09 08:21:20.901072
------------------------------------------------------
Started: 2025-08-09 10:16:54.865614
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-09 10:16:54.927315
------------------------------------------------------
Started: 2025-08-09 12:33:47.200939
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-09 12:33:47.258501
------------------------------------------------------
Started: 2025-08-09 14:15:46.803270
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-09 14:15:46.883448
------------------------------------------------------
Started: 2025-08-09 16:20:08.830681
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-09 16:20:08.917298
------------------------------------------------------
Started: 2025-08-09 18:23:40.670389
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-09 18:23:40.778444
------------------------------------------------------
Started: 2025-08-09 20:18:08.435085
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-09 20:18:08.494691
------------------------------------------------------
Started: 2025-08-09 22:16:16.933773
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-09 22:16:16.994575
------------------------------------------------------
Started: 2025-08-10 01:41:18.153100
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-10 01:41:18.210747
------------------------------------------------------
Started: 2025-08-10 03:41:36.937709
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-10 03:41:37.000539
------------------------------------------------------
Started: 2025-08-10 04:37:58.816937
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-10 04:37:58.900606
------------------------------------------------------
Started: 2025-08-10 06:25:10.690335
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-10 06:25:10.754204
------------------------------------------------------
Started: 2025-08-10 08:21:30.394462
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-10 08:21:30.463668
------------------------------------------------------
Started: 2025-08-10 10:17:44.053802
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-10 10:17:44.116470
------------------------------------------------------
Started: 2025-08-10 12:34:19.897769
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-10 12:34:19.968378
------------------------------------------------------
Started: 2025-08-10 14:15:43.239674
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-10 14:15:43.300103
------------------------------------------------------
Started: 2025-08-10 16:20:11.305747
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-10 16:20:11.412137
------------------------------------------------------
Started: 2025-08-10 18:22:32.462824
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-10 18:22:32.568106
------------------------------------------------------
Started: 2025-08-10 20:18:34.235042
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-10 20:18:34.298985
------------------------------------------------------
Started: 2025-08-10 22:16:11.220015
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-10 22:16:11.280671
------------------------------------------------------
Started: 2025-08-11 01:28:43.231400
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-11 01:28:43.410097
------------------------------------------------------
Started: 2025-08-11 03:40:58.754504
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-11 03:40:58.871315
------------------------------------------------------
Started: 2025-08-11 04:44:24.964209
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 921
Summarized using GPT-3.5-turbo
Append: [PEACH: A sentence-aligned Parallel English-Arabic Corpus for Healthcare](https://arxiv.org/abs/2508.05722)
Token length: 1320
Summarized using GPT-3.5-turbo
Append: [Guardians and Offenders: A Survey on Harmful Content Generation and Safety Mitigation](https://arxiv.org/abs/2508.05775)
Token length: 1347
Summarized using GPT-3.5-turbo
Append: [FineDialFact: A benchmark for Fine-grained Dialogue Fact Verification](https://arxiv.org/abs/2508.05782)
Token length: 1404
Summarized using GPT-3.5-turbo
Append: [Human-like fleeting memory improves language learning but impairs reading time prediction in transformer language models](https://arxiv.org/abs/2508.05803)
Token length: 1963
Summarized using GPT-3.5-turbo
Append: ["Mirror" Language AI Models of Depression are Criterion-Contaminated](https://arxiv.org/abs/2508.05830)
Token length: 1064
Summarized using GPT-3.5-turbo
Append: [Discovering Properties of Inflectional Morphology in Neural Emergent Communication](https://arxiv.org/abs/2508.05843)
Token length: 1779
Summarized using GPT-3.5-turbo
Append: [Do Machines Think Emotionally? Cognitive Appraisal Analysis of Large Language Models](https://arxiv.org/abs/2508.05880)
Token length: 1242
Summarized using GPT-3.5-turbo
Append: [Spectrum Projection Score: Aligning Retrieved Summaries with Reader Models in Retrieval-Augmented Generation](https://arxiv.org/abs/2508.05909)
Token length: 1502
Summarized using GPT-3.5-turbo
Append: [Prosocial Behavior Detection in Player Game Chat: From Aligning Human-AI Definitions to Efficient Annotation at Scale](https://arxiv.org/abs/2508.05938)
Token length: 1712
Summarized using GPT-3.5-turbo
Append: [Adversarial Topic-aware Prompt-tuning for Cross-topic Automated Essay Scoring](https://arxiv.org/abs/2508.05987)
Token length: 1156
Summarized using GPT-3.5-turbo
Append: [Crisp Attention: Regularizing Transformers via Structured Sparsity](https://arxiv.org/abs/2508.06016)
Token length: 1677
Summarized using GPT-3.5-turbo
Append: [Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future](https://arxiv.org/abs/2508.06026)
Token length: 1770
Summarized using GPT-3.5-turbo
Append: [Efficient Knowledge Probing of Large Language Models by Adapting Pre-trained Embeddings](https://arxiv.org/abs/2508.06030)
Token length: 1463
Summarized using GPT-3.5-turbo
Append: [EvolvR: Self-Evolving Pairwise Reasoning for Story Evaluation to Enhance Generation](https://arxiv.org/abs/2508.06046)
Token length: 992
Summarized using GPT-3.5-turbo
Append: [ConlangCrafter: Constructing Languages with a Multi-Hop LLM Pipeline](https://arxiv.org/abs/2508.06094)
Token length: 872
Summarized using GPT-3.5-turbo
Append: [Few-Shot Prompting for Extractive Quranic QA with Instruction-Tuned LLMs](https://arxiv.org/abs/2508.06103)
Token length: 1872
Summarized using GPT-3.5-turbo
Append: [You Don't Need Pre-built Graphs for RAG: Retrieval Augmented Generation with Adaptive Reasoning Structures](https://arxiv.org/abs/2508.06105)
Token length: 1282
Summarized using GPT-3.5-turbo
Append: [AURA: Affordance-Understanding and Risk-aware Alignment Technique for Large Language Models](https://arxiv.org/abs/2508.06124)
Token length: 1930
Summarized using GPT-3.5-turbo
Append: [Less is More: Selective Reflection for Compatible and Efficient Knowledge Distillation in Large Language Models](https://arxiv.org/abs/2508.06135)
Token length: 753
Summarized using GPT-3.5-turbo
Append: [Scaling Personality Control in LLMs with Big Five Scaler Prompts](https://arxiv.org/abs/2508.06149)
Token length: 1667
Summarized using GPT-3.5-turbo
Append: [Semantic and Structural Analysis of Implicit Biases in Large Language Models: An Interpretable Approach](https://arxiv.org/abs/2508.06155)
Token length: 1772
Summarized using GPT-3.5-turbo
Append: [One Size Does Not Fit All: A Distribution-Aware Sparsification for More Precise Model Merging](https://arxiv.org/abs/2508.06163)
Token length: 1599
Summarized using GPT-3.5-turbo
Append: [UR$^2$: Unify RAG and Reasoning through Reinforcement Learning](https://arxiv.org/abs/2508.06165)
Token length: 1754
Summarized using GPT-3.5-turbo
Append: [Pragmatics beyond humans: meaning, communication, and LLMs](https://arxiv.org/abs/2508.06167)
Token length: 1918
Summarized using GPT-3.5-turbo
Append: [Comparing Knowledge Injection Methods for LLMs in a Low-Resource Regime](https://arxiv.org/abs/2508.06178)
Token length: 1901
Summarized using GPT-3.5-turbo
Append: [DKG-LLM : A Framework for Medical Diagnosis and Personalized Treatment Recommendations via Dynamic Knowledge Graph and Large Language Model Integration](https://arxiv.org/abs/2508.06186)
Token length: 1460
Summarized using GPT-3.5-turbo
Append: [Beyond Uniform Criteria: Scenario-Adaptive Multi-Dimensional Jailbreak Evaluation](https://arxiv.org/abs/2508.06194)
Token length: 1414
Summarized using GPT-3.5-turbo
Append: [EICAP: Deep Dive in Assessment and Enhancement of Large Language Models in Emotional Intelligence through Multi-Turn Conversations](https://arxiv.org/abs/2508.06196)
Token length: 1337
Summarized using GPT-3.5-turbo
Append: [Classification is a RAG problem: A case study on hate speech detection](https://arxiv.org/abs/2508.06204)
Token length: 1516
Summarized using GPT-3.5-turbo
Append: [InfoCausalQA:Can Models Perform Non-explicit Causal Reasoning Based on Infographic?](https://arxiv.org/abs/2508.06220)
Token length: 1336
Summarized using GPT-3.5-turbo
Append: [Large Language Model Data Generation for Enhanced Intent Recognition in German Speech](https://arxiv.org/abs/2508.06277)
Token length: 1398
Summarized using GPT-3.5-turbo
Append: [Matrix-Driven Instant Review: Confident Detection and Reconstruction of LLM Plagiarism on PC](https://arxiv.org/abs/2508.06309)
Token length: 1530
Summarized using GPT-3.5-turbo
Append: [Harnessing Adaptive Topology Representations for Zero-Shot Graph Question Answering](https://arxiv.org/abs/2508.06345)
Token length: 1201
Summarized using GPT-3.5-turbo
Append: [Cyberbullying Detection via Aggression-Enhanced Prompting](https://arxiv.org/abs/2508.06360)
Token length: 920
Summarized using GPT-3.5-turbo
Append: [Evaluating Style-Personalized Text Generation: Challenges and Directions](https://arxiv.org/abs/2508.06374)
Token length: 1891
Summarized using GPT-3.5-turbo
Append: [LLMs vs. Chinese Anime Enthusiasts: A Comparative Study on Emotionally Supportive Role-Playing](https://arxiv.org/abs/2508.06388)
Token length: 1558
Summarized using GPT-3.5-turbo
Append: [Quantifying Conversation Drift in MCP via Latent Polytope](https://arxiv.org/abs/2508.06418)
Token length: 1095
Summarized using GPT-3.5-turbo
Append: [Memp: Exploring Agent Procedural Memory](https://arxiv.org/abs/2508.06433)
Token length: 1756
Summarized using GPT-3.5-turbo
Append: [Learning the Topic, Not the Language: How LLMs Classify Online Immigration Discourse Across Languages](https://arxiv.org/abs/2508.06435)
Token length: 780
Summarized using GPT-3.5-turbo
Append: [Echoes of Automation: The Increasing Use of LLMs in Newsmaking](https://arxiv.org/abs/2508.06445)
Token length: 1468
Summarized using GPT-3.5-turbo
Append: [SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning](https://arxiv.org/abs/2508.06447)
Token length: 952
Summarized using GPT-3.5-turbo
Append: [GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models](https://arxiv.org/abs/2508.06471)
Token length: 1625
Summarized using GPT-3.5-turbo
Append: [HapticLLaMA: A Multimodal Sensory Language Model for Haptic Captioning](https://arxiv.org/abs/2508.06475)
Token length: 874
Summarized using GPT-3.5-turbo
Append: [Post-training for Efficient Communication via Convention Formation](https://arxiv.org/abs/2508.06482)
Token length: 1884
Summarized using GPT-3.5-turbo
Append: [AttriLens-Mol: Attribute Guided Reinforcement Learning for Molecular Property Prediction with Large Language Models](https://arxiv.org/abs/2508.04748)
Token length: 1424
Summarized using GPT-3.5-turbo
Append: [Enhancing Retrieval-Augmented Generation for Electric Power Industry Customer Support](https://arxiv.org/abs/2508.05664)
Token length: 989
Summarized using GPT-3.5-turbo
Append: [A Survey of LLM-based Deep Search Agents: Paradigm, Optimization, Evaluation, and Challenges](https://arxiv.org/abs/2508.05668)
Token length: 1744
Summarized using GPT-3.5-turbo
Append: [Fine-Tuning Vision-Language Models for Markdown Conversion of Financial Tables in Malaysian Audited Financial Reports](https://arxiv.org/abs/2508.05669)
Token length: 1157
Summarized using GPT-3.5-turbo
Append: [DINA: A Dual Defense Framework Against Internal Noise and External Attacks in Natural Language Processing](https://arxiv.org/abs/2508.05671)
Token length: 1654
Summarized using GPT-3.5-turbo
Append: [DMFI: Dual-Modality Fine-Tuning and Inference Framework for LLM-Based Insider Threat Detection](https://arxiv.org/abs/2508.05694)
Append: [InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization](https://arxiv.org/abs/2508.05731)
Append: [Basic interactive algorithms: Preview](https://arxiv.org/abs/2508.05798)
Append: [NanoCodec: Towards High-Quality Ultra Fast Speech LLM Inference](https://arxiv.org/abs/2508.05835)
Append: [Do Ethical AI Principles Matter to Users? A Large-Scale Analysis of User Sentiment and Satisfaction](https://arxiv.org/abs/2508.05913)
Append: [Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents](https://arxiv.org/abs/2508.05954)
Append: [Position: Intelligent Coding Systems Should Write Programs with Justifications](https://arxiv.org/abs/2508.06017)
Append: [Fact2Fiction: Targeted Poisoning Attack to Agentic Fact-checking System](https://arxiv.org/abs/2508.06059)
Append: [ThematicPlane: Bridging Tacit User Intent and Latent Spaces for Image Generation](https://arxiv.org/abs/2508.06065)
Append: [A Systematic Literature Review of Retrieval-Augmented Generation: Techniques, Metrics, and Challenges](https://arxiv.org/abs/2508.06401)
Append: [Sample-efficient LLM Optimization with Reset Replay](https://arxiv.org/abs/2508.06412)
Append: [ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls](https://arxiv.org/abs/2508.06457)
Append: [Effective Training Data Synthesis for Improving MLLM Chart Understanding](https://arxiv.org/abs/2508.06492)
Append: [Benchmarking LLMs on the Semantic Overlap Summarization Task](https://arxiv.org/abs/2402.17008)
Append: [Towards Pareto Optimal Throughput in Small Language Model Serving](https://arxiv.org/abs/2404.03353)
Append: [Extract-and-Abstract: Unifying Extractive and Abstractive Summarization within Single Encoder-Decoder Framework](https://arxiv.org/abs/2409.11827)
Append: [Turning Logic Against Itself : Probing Model Defenses Through Contrastive Questions](https://arxiv.org/abs/2501.01872)
Append: [The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs](https://arxiv.org/abs/2501.10970)
Append: [Neural Contextual Reinforcement Framework for Logical Structure Language Generation](https://arxiv.org/abs/2501.11417)
Append: [Architectural Fusion Through Contextual Partitioning in Large Language Models: A Novel Approach to Parameterized Knowledge Integration](https://arxiv.org/abs/2501.12901)
Append: [Autonomous Structural Memory Manipulation for Large Language Models Using Hierarchical Embedding Augmentation](https://arxiv.org/abs/2501.14119)
Append: [Contextual Reinforcement in Multimodal Token Compression for Large Language Models](https://arxiv.org/abs/2501.16658)
Append: [Structural Embedding Projection for Contextual Large Language Model Inference](https://arxiv.org/abs/2501.18826)
Append: [Context-Preserving Tensorial Reconfiguration in Large Language Model Training](https://arxiv.org/abs/2502.00246)
Append: [Contextual Morphogenesis in Large Language Models: A Novel Approach to Self-Organizing Token Representations](https://arxiv.org/abs/2502.00301)
Append: [Context-Aware Hierarchical Merging for Long Document Summarization](https://arxiv.org/abs/2502.00977)
Append: [Gradient-Regularized Latent Space Modulation in Large Language Models for Structured Contextual Synthesis](https://arxiv.org/abs/2502.01979)
Append: [Latent Structure Modulation in Large Language Models Through Stochastic Concept Embedding Transitions](https://arxiv.org/abs/2502.05553)
Append: [Structural Perturbation in Large Language Model Representations through Recursive Symbolic Regeneration](https://arxiv.org/abs/2502.05794)
Append: [Structural Reformation of Large Language Model Neuron Encapsulation for Divergent Information Aggregation](https://arxiv.org/abs/2502.07124)
Append: [Structured Convergence in Large Language Model Representations via Hierarchical Latent Space Folding](https://arxiv.org/abs/2502.08947)
Append: [Statistical Coherence Alignment for Large Language Model Representation Learning Through Tensor Field Convergence](https://arxiv.org/abs/2502.09815)
Append: [Exploring Synaptic Resonance in Large Language Models: A Novel Approach to Contextual Memory Integration](https://arxiv.org/abs/2502.10699)
Append: [Exploring Contextual Flux in Large Language Models: A Novel Approach to Self-Modulating Semantic Networks](https://arxiv.org/abs/2502.10942)
Append: [Topic Over Source: The Key to Effective Data Mixing for Language Models Pre-training](https://arxiv.org/abs/2502.16802)
Append: [One ruler to measure them all: Benchmarking multilingual long-context language models](https://arxiv.org/abs/2503.01996)
Append: [OpenCodeReasoning: Advancing Data Distillation for Competitive Coding](https://arxiv.org/abs/2504.01943)
Append: [Single-Pass Document Scanning for Question Answering](https://arxiv.org/abs/2504.03101)
Append: [Not All Data Are Unlearned Equally](https://arxiv.org/abs/2504.05058)
Append: [Self-Steering Language Models](https://arxiv.org/abs/2504.07081)
Append: [Layers at Similar Depths Generate Similar Activations Across LLM Architectures](https://arxiv.org/abs/2504.08775)
Append: [EvidenceBench: A Benchmark for Extracting Evidence from Biomedical Papers](https://arxiv.org/abs/2504.18736)
Append: [Can a Crow Hatch a Falcon? Lineage Matters in Predicting Large Language Model Performance](https://arxiv.org/abs/2504.19811)
Append: [No Query, No Access](https://arxiv.org/abs/2505.07258)
Append: [The Devil Is in the Word Alignment Details: On Translation-Based Cross-Lingual Transfer for Token Classification Tasks](https://arxiv.org/abs/2505.10507)
Append: [MAATS: A Multi-Agent Automated Translation System Based on MQM Evaluation](https://arxiv.org/abs/2505.14848)
Append: [CUB: Benchmarking Context Utilisation Techniques for Language Models](https://arxiv.org/abs/2505.16518)
Append: [Automated Privacy Information Annotation in Large Language Model Interactions](https://arxiv.org/abs/2505.20910)
Append: [DrVoice: Parallel Speech-Text Voice Conversation Model via Dual-Resolution Speech Representations](https://arxiv.org/abs/2506.09349)
Append: [No Universal Prompt: Unifying Reasoning through Adaptive Prompting for Temporal Table Reasoning](https://arxiv.org/abs/2506.11246)
Append: [Decompositional Reasoning for Graph Retrieval with Large Language Models](https://arxiv.org/abs/2506.13380)
Append: [Quantifying Fairness in LLMs Beyond Tokens: A Semantic and Statistical Perspective](https://arxiv.org/abs/2506.19028)
Append: [AALC: Large Language Model Efficient Reasoning via Adaptive Accuracy-Length Control](https://arxiv.org/abs/2506.20160)
Append: [Humans overrely on overconfident language models, across languages](https://arxiv.org/abs/2507.06306)
Append: [Pretraining on the Test Set Is No Longer All You Need: A Debate-Driven Approach to QA Benchmarks](https://arxiv.org/abs/2507.17747)
Append: [ProsodyLM: Uncovering the Emerging Prosody Processing Capabilities in Speech Language Models](https://arxiv.org/abs/2507.20091)
Append: [Integrating large language models and active inference to understand eye movements in reading and dyslexia](https://arxiv.org/abs/2308.04941)
Append: [INS-MMBench: A Comprehensive Benchmark for Evaluating LVLMs' Performance in Insurance](https://arxiv.org/abs/2406.09105)
Append: [From Next-Token to Mathematics: The Learning Dynamics of Mathematical Reasoning in Language Models](https://arxiv.org/abs/2407.00900)
Append: [Towards More Realistic Extraction Attacks: An Adversarial Perspective](https://arxiv.org/abs/2407.02596)
Append: [Reducibility among NP-Hard graph problems and boundary classes](https://arxiv.org/abs/2411.14553)
Append: [Are Your LLMs Capable of Stable Reasoning?](https://arxiv.org/abs/2412.13147)
Append: [Contextually Entangled Gradient Mapping for Optimized LLM Comprehension](https://arxiv.org/abs/2502.00048)
Append: [Hypothesis-Driven Theory-of-Mind Reasoning for Large Language Models](https://arxiv.org/abs/2502.11881)
Append: [Rank1: Test-Time Compute for Reranking in Information Retrieval](https://arxiv.org/abs/2502.18418)
Append: [Code-as-Symbolic-Planner: Foundation Model-Based Robot Planning via Symbolic Code Generation](https://arxiv.org/abs/2503.01700)
Append: [CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive Program Synthesis](https://arxiv.org/abs/2503.23145)
Append: [OpenCodeInstruct: A Large-scale Instruction Tuning Dataset for Code LLMs](https://arxiv.org/abs/2504.04030)
Append: [CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation](https://arxiv.org/abs/2504.15254)
Append: [Language Agents Mirror Human Causal Reasoning Biases. How Can We Help Them Think Like Scientists?](https://arxiv.org/abs/2505.09614)
Append: [From Autonomy to Agency: Agentic Vehicles for Human-Centered Mobility Systems](https://arxiv.org/abs/2507.04996)
Append: [Nyay-Darpan: Enhancing Decision Making Through Summarization and Case Retrieval for Consumer Law in India](https://arxiv.org/abs/2507.06090)
append_entries: 121
Finish: 2025-08-11 04:46:11.185283
------------------------------------------------------
Started: 2025-08-11 06:29:05.157298
Existing_entries: 1121
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-11 06:29:05.448984
------------------------------------------------------
Started: 2025-08-11 08:26:34.111832
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-11 08:26:34.399119
------------------------------------------------------
Started: 2025-08-11 10:19:58.924470
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-11 10:19:59.221866
------------------------------------------------------
Started: 2025-08-11 12:38:04.088945
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-11 12:38:04.375035
------------------------------------------------------
Started: 2025-08-11 14:19:47.330933
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-11 14:19:47.618191
------------------------------------------------------
Started: 2025-08-11 16:23:00.513917
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-11 16:23:00.807102
------------------------------------------------------
Started: 2025-08-11 18:26:53.297806
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-11 18:26:53.582600
------------------------------------------------------
Started: 2025-08-11 20:18:04.663874
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-11 20:18:04.955012
------------------------------------------------------
Started: 2025-08-11 22:16:31.326797
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-11 22:16:31.619147
------------------------------------------------------
Started: 2025-08-12 01:20:33.285122
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-12 01:20:33.565750
------------------------------------------------------
Started: 2025-08-12 03:15:10.002616
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-12 03:15:10.300969
------------------------------------------------------
Started: 2025-08-12 04:28:49.518353
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1136
Summarized using GPT-3.5-turbo
Append: [Semi-automated Fact-checking in Portuguese: Corpora Enrichment using Retrieval with Claim extraction](https://arxiv.org/abs/2508.06495)
Token length: 1238
Summarized using GPT-3.5-turbo
Append: [Retrieval augmented generation based dynamic prompting for few-shot biomedical named entity recognition using large language models](https://arxiv.org/abs/2508.06504)
Token length: 1243
Summarized using GPT-3.5-turbo
Append: [CarbonScaling: Extending Neural Scaling Laws for Carbon Footprint in Large Language Models](https://arxiv.org/abs/2508.06524)
Token length: 1502
Summarized using GPT-3.5-turbo
Append: [The Art of Breaking Words: Rethinking Multilingual Tokenizer Design](https://arxiv.org/abs/2508.06533)
Token length: 1176
Summarized using GPT-3.5-turbo
Append: [Factor Augmented Supervised Learning with Text Embeddings](https://arxiv.org/abs/2508.06548)
Token length: 1399
Summarized using GPT-3.5-turbo
Append: [Discerning minds or generic tutors? Evaluating instructional guidance capabilities in Socratic LLMs](https://arxiv.org/abs/2508.06583)
Token length: 1351
Summarized using GPT-3.5-turbo
Append: [LLM Unlearning Without an Expert Curated Dataset](https://arxiv.org/abs/2508.06595)
Token length: 1747
Summarized using GPT-3.5-turbo
Append: [BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research Agent](https://arxiv.org/abs/2508.06600)
Token length: 1489
Summarized using GPT-3.5-turbo
Append: [Train It and Forget It: Merge Lists are Unnecessary for BPE Inference in Language Models](https://arxiv.org/abs/2508.06621)
Token length: 1038
Summarized using GPT-3.5-turbo
Append: [Measuring Stereotype and Deviation Biases in Large Language Models](https://arxiv.org/abs/2508.06649)
Token length: 1618
Summarized using GPT-3.5-turbo
Append: [Testing the Limits of Machine Translation from One Book](https://arxiv.org/abs/2508.06665)
Token length: 1014
Summarized using GPT-3.5-turbo
Append: [Do Biased Models Have Biased Thoughts?](https://arxiv.org/abs/2508.06671)
Token length: 1680
Summarized using GPT-3.5-turbo
Append: [Play Favorites: A Statistical Method to Measure Self-Bias in LLM-as-a-Judge](https://arxiv.org/abs/2508.06709)
Token length: 1931
Summarized using GPT-3.5-turbo
Append: [Large Language Models for Oral History Understanding with Text Classification and Sentiment Analysis](https://arxiv.org/abs/2508.06729)
Token length: 1282
Summarized using GPT-3.5-turbo
Append: [Many-Turn Jailbreaking](https://arxiv.org/abs/2508.06755)
Token length: 1284
Summarized using GPT-3.5-turbo
Append: [SEVADE: Self-Evolving Multi-Agent Analysis with Decoupled Evaluation for Hallucination-Resistant Irony Detection](https://arxiv.org/abs/2508.06803)
Token length: 1500
Summarized using GPT-3.5-turbo
Append: [Annotating Errors in English Learners' Written Language Production: Advancing Automated Written Feedback Systems](https://arxiv.org/abs/2508.06810)
Token length: 644
Summarized using GPT-3.5-turbo
Append: [Text to Speech System for Meitei Mayek Script](https://arxiv.org/abs/2508.06870)
Token length: 1361
Summarized using GPT-3.5-turbo
Append: [ESNERA: Empirical and semantic named entity alignment for named entity dataset merging](https://arxiv.org/abs/2508.06877)
Token length: 958
Summarized using GPT-3.5-turbo
Append: [The ReQAP System for Question Answering over Personal Information](https://arxiv.org/abs/2508.06880)
Token length: 1371
Summarized using GPT-3.5-turbo
Append: [Score Before You Speak: Improving Persona Consistency in Dialogue Generation using Response Quality Scores](https://arxiv.org/abs/2508.06886)
Token length: 1560
Summarized using GPT-3.5-turbo
Append: [Model-Agnostic Sentiment Distribution Stability Analysis for Robust LLM-Generated Texts Detection](https://arxiv.org/abs/2508.06913)
Token length: 1002
Summarized using GPT-3.5-turbo
Append: [Two-Stage Quranic QA via Ensemble Retrieval and Instruction-Tuned Answer Extraction](https://arxiv.org/abs/2508.06971)
Token length: 1031
Summarized using GPT-3.5-turbo
Append: [Rethinking 1-bit Optimization Leveraging Pre-trained Large Language Models](https://arxiv.org/abs/2508.06974)
Token length: 1429
Summarized using GPT-3.5-turbo
Append: [Vec2Summ: Text Summarization via Probabilistic Sentence Embeddings](https://arxiv.org/abs/2508.07017)
Token length: 937
Summarized using GPT-3.5-turbo
Append: [SEADialogues: A Multilingual Culturally Grounded Multi-turn Dialogue Dataset on Southeast Asian Languages](https://arxiv.org/abs/2508.07069)
Token length: 1219
Summarized using GPT-3.5-turbo
Append: [BharatBBQ: A Multilingual Bias Benchmark for Question Answering in the Indian Context](https://arxiv.org/abs/2508.07090)
Token length: 1425
Summarized using GPT-3.5-turbo
Append: [Less Is More: Training-Free Sparse Attention with Global Locality for Efficient Reasoning](https://arxiv.org/abs/2508.07101)
Token length: 1942
Summarized using GPT-3.5-turbo
Append: [Investigating Intersectional Bias in Large Language Models using Confidence Disparities in Coreference Resolution](https://arxiv.org/abs/2508.07111)
Token length: 1789
Summarized using GPT-3.5-turbo
Append: [Fairness of Automatic Speech Recognition: Looking Through a Philosophical Lens](https://arxiv.org/abs/2508.07143)
Token length: 1370
Summarized using GPT-3.5-turbo
Append: [Gradient Surgery for Safe LLM Fine-Tuning](https://arxiv.org/abs/2508.07172)
Token length: 1601
Summarized using GPT-3.5-turbo
Append: [Omni-SafetyBench: A Benchmark for Safety Evaluation of Audio-Visual Large Language Models](https://arxiv.org/abs/2508.07173)
Token length: 1449
Summarized using GPT-3.5-turbo
Append: [Improved Personalized Headline Generation via Denoising Fake Interests from Implicit Feedback](https://arxiv.org/abs/2508.07178)
Token length: 1627
Summarized using GPT-3.5-turbo
Append: [Schema Lineage Extraction at Scale: Multilingual Pipelines, Composite Evaluation, and Language-Model Benchmarks](https://arxiv.org/abs/2508.07179)
Token length: 1404
Summarized using GPT-3.5-turbo
Append: [DySK-Attn: A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention](https://arxiv.org/abs/2508.07185)
Token length: 1551
Summarized using GPT-3.5-turbo
Append: [Adapting LLMs to Time Series Forecasting via Temporal Heterogeneity Modeling and Semantic Alignment](https://arxiv.org/abs/2508.07195)
Token length: 1661
Summarized using GPT-3.5-turbo
Append: [Enhancing Rumor Detection Methods with Propagation Structure Infused Language Model](https://arxiv.org/abs/2508.07209)
Token length: 1500
Summarized using GPT-3.5-turbo
Append: [How Does a Deep Neural Network Look at Lexical Stress?](https://arxiv.org/abs/2508.07229)
Token length: 1323
Summarized using GPT-3.5-turbo
Append: [Prompt Tuning for Few-Shot Continual Learning Named Entity Recognition](https://arxiv.org/abs/2508.07248)
Token length: 985
Summarized using GPT-3.5-turbo
Append: [The 2D+ Dynamic Articulatory Model DYNARTmo: Tongue-Palate Contact Area Estimation](https://arxiv.org/abs/2508.07262)
Token length: 1031
Summarized using GPT-3.5-turbo
Append: [Incorporating Contextual Paralinguistic Understanding in Large Speech-Language Models](https://arxiv.org/abs/2508.07273)
Token length: 1443
Summarized using GPT-3.5-turbo
Append: [MAQuA: Adaptive Question-Asking for Multidimensional Mental Health Screening using Item Response Theory](https://arxiv.org/abs/2508.07279)
Token length: 1643
Summarized using GPT-3.5-turbo
Append: ["Pull or Not to Pull?'': Investigating Moral Biases in Leading Large Language Models Across Ethical Dilemmas](https://arxiv.org/abs/2508.07284)
Token length: 1707
Summarized using GPT-3.5-turbo
Append: [Arce: Augmented Roberta with Contextualized Elucidations for Ner in Automated Rule Checking](https://arxiv.org/abs/2508.07286)
Token length: 1448
Summarized using GPT-3.5-turbo
Append: [CCFQA: A Benchmark for Cross-Lingual and Cross-Modal Speech and Text Factuality Evaluation](https://arxiv.org/abs/2508.07295)
Token length: 1064
Summarized using GPT-3.5-turbo
Append: [HealthBranches: Synthesizing Clinically-Grounded Question Answering Datasets via Decision Pathways](https://arxiv.org/abs/2508.07308)
Token length: 1077
Summarized using GPT-3.5-turbo
Append: [ObfusQAte: A Proposed Framework to Evaluate LLM Robustness on Obfuscated Factual Question Answering](https://arxiv.org/abs/2508.07321)
Token length: 1157
Summarized using GPT-3.5-turbo
Append: [Strategies of Code-switching in Human-Machine Dialogs](https://arxiv.org/abs/2508.07325)
Token length: 1602
Summarized using GPT-3.5-turbo
Append: [Think Before You Talk: Enhancing Meaningful Dialogue Generation in Full-Duplex Speech Language Models with Planning-Inspired Text Guidance](https://arxiv.org/abs/2508.07375)
Token length: 1237
Summarized using GPT-3.5-turbo
Append: [Grounding Multilingual Multimodal LLMs With Cultural Knowledge](https://arxiv.org/abs/2508.07414)
Append: [Let's Revise Step-by-Step: A Unified Local Search Framework for Code Generation with LLMs](https://arxiv.org/abs/2508.07434)
Append: [Positional Biases Shift as Inputs Approach Context Window Limits](https://arxiv.org/abs/2508.07479)
Append: [ALOPE: Adaptive Layer Optimization for Translation Quality Estimation using Large Language Models](https://arxiv.org/abs/2508.07484)
Append: [Augmenting Bias Detection in LLMs Using Topological Data Analysis](https://arxiv.org/abs/2508.07516)
Append: [Word Clouds as Common Voices: LLM-Assisted Visualization of Participant-Weighted Themes in Qualitative Interviews](https://arxiv.org/abs/2508.07517)
Append: [From Trial-and-Error to Improvement: A Systematic Analysis of LLM Exploration Mechanisms in RLVR](https://arxiv.org/abs/2508.07534)
Append: [IBPS: Indian Bail Prediction System](https://arxiv.org/abs/2508.07592)
Append: [Keyword-Centric Prompting for One-Shot Event Detection with Self-Generated Rationale Enhancements](https://arxiv.org/abs/2508.07598)
Append: [InterChart: Benchmarking Visual Reasoning Across Decomposed and Distributed Chart Information](https://arxiv.org/abs/2508.07630)
Append: [LoSemB: Logic-Guided Semantic Bridging for Inductive Tool Retrieval](https://arxiv.org/abs/2508.07690)
Append: [What am I missing here?: Evaluating Large Language Models for Masked Sentence Prediction](https://arxiv.org/abs/2508.07702)
Append: [Exploring Causal Effect of Social Bias on Faithfulness Hallucinations in Large Language Models](https://arxiv.org/abs/2508.07753)
Append: [SASST: Leveraging Syntax-Aware Chunking and LLMs for Simultaneous Speech Translation](https://arxiv.org/abs/2508.07781)
Append: [Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts](https://arxiv.org/abs/2508.07785)
Append: [Can You Trick the Grader? Adversarial Persuasion of LLM Judges](https://arxiv.org/abs/2508.07805)
Append: [Evaluating Compositional Approaches for Focus and Sentiment Analysis](https://arxiv.org/abs/2508.07810)
Append: [Evaluating Large Language Models as Expert Annotators](https://arxiv.org/abs/2508.07827)
Append: [LLMs for Law: Evaluating Legal-Specific LLMs on Contract Understanding](https://arxiv.org/abs/2508.07849)
Append: [Large Language Models for Czech Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2508.07860)
Append: [Few-shot Cross-lingual Aspect-Based Sentiment Analysis with Sequence-to-Sequence Models](https://arxiv.org/abs/2508.07866)
Append: [Tailored Emotional LLM-Supporter: Enhancing Cultural Sensitivity](https://arxiv.org/abs/2508.07902)
Append: [Challenges and opportunities in portraying emotion in generated sign language](https://arxiv.org/abs/2508.07937)
Append: [Expert Preference-based Evaluation of Automated Related Work Generation](https://arxiv.org/abs/2508.07955)
Append: [Large Language Models for Subjective Language Understanding: A Survey](https://arxiv.org/abs/2508.07959)
Append: [Toward Machine Interpreting: Lessons from Human Interpreting Studies](https://arxiv.org/abs/2508.07964)
Append: [Understanding Syntactic Generalization in Structure-inducing Language Models](https://arxiv.org/abs/2508.07969)
Append: [Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL](https://arxiv.org/abs/2508.07976)
Append: [The Medical Metaphors Corpus (MCC)](https://arxiv.org/abs/2508.07993)
Append: [WideSearch: Benchmarking Agentic Broad Info-Seeking](https://arxiv.org/abs/2508.07999)
Append: [Progressive Depth Up-scaling via Optimal Transport](https://arxiv.org/abs/2508.08011)
Append: [9th Workshop on Sign Language Translation and Avatar Technologies (SLTAT 2025)](https://arxiv.org/abs/2508.08050)
Append: [Dual Information Speech Language Models for Emotional Conversations](https://arxiv.org/abs/2508.08095)
Append: [Assessing LLM Text Detection in Educational Contexts: Does Human Contribution Affect Detection?](https://arxiv.org/abs/2508.08096)
Append: [Iterative refinement, not training objective, makes HuBERT behave differently from wav2vec 2.0](https://arxiv.org/abs/2508.08110)
Append: [Czech Dataset for Complex Aspect-Based Sentiment Analysis Tasks](https://arxiv.org/abs/2508.08125)
Append: [Optimal Transport Regularization for Speech Text Alignment in Spoken Language Models](https://arxiv.org/abs/2508.08131)
Append: [Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models](https://arxiv.org/abs/2508.08139)
Append: [Data-Efficient Biomedical In-Context Learning: A Diversity-Enhanced Submodular Perspective](https://arxiv.org/abs/2508.08140)
Append: [REX-RAG: Reasoning Exploration with Policy Correction in Retrieval-Augmented Generation](https://arxiv.org/abs/2508.08149)
Append: [LPI-RIT at LeWiDi-2025: Improving Distributional Predictions via Metadata and Loss Reweighting with DisCo](https://arxiv.org/abs/2508.08163)
Append: [Efficient Speculative Decoding for Llama at Scale: Challenges and Solutions](https://arxiv.org/abs/2508.08192)
Append: [Human-Alignment and Calibration of Inference-Time Uncertainty in Large Language Models](https://arxiv.org/abs/2508.08204)
Append: [SAEMark: Multi-bit LLM Watermarking with Inference-Time Scaling](https://arxiv.org/abs/2508.08211)
Append: [Capabilities of GPT-5 on Multimodal Medical Reasoning](https://arxiv.org/abs/2508.08224)
Append: [Exploring Safety Alignment Evaluation of LLMs in Chinese Mental Health Dialogues via LLM-as-Judge](https://arxiv.org/abs/2508.08236)
Append: [Jinx: Unlimited LLMs for Probing Alignment Failures](https://arxiv.org/abs/2508.08243)
Append: [Generative Artificial Intelligence Extracts Structure-Function Relationships from Plants for New Materials](https://arxiv.org/abs/2508.06591)
Append: [Story Ribbons: Reimagining Storyline Visualizations with Large Language Models](https://arxiv.org/abs/2508.06772)
Append: [Maestro-EVC: Controllable Emotional Voice Conversion Guided by References and Explicit Prosody](https://arxiv.org/abs/2508.06890)
Append: [AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance](https://arxiv.org/abs/2508.06944)
Append: [DatasetResearch: Benchmarking Agent Systems for Demand-Driven Dataset Discovery](https://arxiv.org/abs/2508.06960)
Append: [TurboBias: Universal ASR Context-Biasing powered by GPU-accelerated Phrase-Boosting Tree](https://arxiv.org/abs/2508.07014)
Append: [MultiMedEdit: A Scenario-Aware Benchmark for Evaluating Knowledge Editing in Medical VQA](https://arxiv.org/abs/2508.07022)
Append: [ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability](https://arxiv.org/abs/2508.07050)
Append: [SQL-Exchange: Transforming SQL Queries Across Domains](https://arxiv.org/abs/2508.07087)
Append: [Propagation Tree Is Not Deep: Adaptive Graph Contrastive Learning Approach for Rumor Detection](https://arxiv.org/abs/2508.07201)
Append: [Towards Real-World Rumor Detection: Anomaly Detection Framework with Graph Supervised Contrastive Learning](https://arxiv.org/abs/2508.07205)
Append: [EndoAgent: A Memory-Guided Reflective Agent for Intelligent Endoscopic Vision-to-Decision Reasoning](https://arxiv.org/abs/2508.07292)
Append: [FlexCTC: GPU-powered CTC Beam Decoding with advanced Contextual Abilities](https://arxiv.org/abs/2508.07315)
Append: [PrLM: Learning Explicit Reasoning for Personalized RAG via Contrastive Reward Optimization](https://arxiv.org/abs/2508.07342)
Append: [Rethinking Domain-Specific LLM Benchmark Construction: A Comprehensiveness-Compactness Approach](https://arxiv.org/abs/2508.07353)
Append: [Generative AI for Strategic Plan Development](https://arxiv.org/abs/2508.07405)
Append: [A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems](https://arxiv.org/abs/2508.07407)
Append: [Event-Aware Sentiment Factors from LLM-Augmented Financial Tweets: A Transparent Framework for Interpretable Quant Trading](https://arxiv.org/abs/2508.07408)
Append: [CP-Agent: Agentic Constraint Programming](https://arxiv.org/abs/2508.07468)
Append: [Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy](https://arxiv.org/abs/2508.07485)
Append: [Conversational DNA: A New Visual Language for Understanding Dialogue Structure in Human and AI](https://arxiv.org/abs/2508.07520)
Append: [ThinkTuning: Instilling Cognitive Reflections without Distillation](https://arxiv.org/abs/2508.07616)
Append: [Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving Clipping Policy Optimization](https://arxiv.org/abs/2508.07629)
Append: [Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents](https://arxiv.org/abs/2508.07642)
Append: [GLiClass: Generalist Lightweight Model for Sequence Classification Tasks](https://arxiv.org/abs/2508.07662)
Append: [Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment](https://arxiv.org/abs/2508.07750)
Append: [Pareto Multi-Objective Alignment for Language Models](https://arxiv.org/abs/2508.07768)
Append: [Joint Transcription of Acoustic Guitar Strumming Directions and Chords](https://arxiv.org/abs/2508.07973)
Append: [Improving Document Retrieval Coherence for Semantically Equivalent Queries](https://arxiv.org/abs/2508.07975)
Append: [Exploring Procedural Data Generation for Automatic Acoustic Guitar Fingerpicking Transcription](https://arxiv.org/abs/2508.07987)
Append: [Audio-Thinker: Guiding Audio Language Model When and How to Think via Reinforcement Learning](https://arxiv.org/abs/2508.08039)
Append: [From Source to Target: Leveraging Transfer Learning for Predictive Process Monitoring in Organizations](https://arxiv.org/abs/2508.08061)
Append: [Investigating the Design Space of Visual Grounding in Multimodal Large Language Model](https://arxiv.org/abs/2508.08066)
Append: [HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating Local and Web Searches](https://arxiv.org/abs/2508.08088)
Append: [Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning](https://arxiv.org/abs/2508.08221)
Append: [Highly Fast Text Segmentation With Pairwise Markov Chains](https://arxiv.org/abs/2102.11037)
Append: [How Chinese are Chinese Language Models? The Puzzling Lack of Language Policy in China's LLMs](https://arxiv.org/abs/2407.09652)
Append: [AI-AI Bias: large language models favor communications generated by large language models](https://arxiv.org/abs/2407.12856)
Append: [Chain of Thought Still Thinks Fast: APriCoT Helps with Thinking Slow](https://arxiv.org/abs/2408.08651)
Append: [A Practice of Post-Training on Llama-3 70B with Optimal Selection of Additional Language Mixture Ratio](https://arxiv.org/abs/2409.06624)
Append: [CLAIR-A: Leveraging Large Language Models to Judge Audio Captions](https://arxiv.org/abs/2409.12962)
Append: [A Closer Look at Machine Unlearning for Large Language Models](https://arxiv.org/abs/2410.08109)
Append: [FlatQuant: Flatness Matters for LLM Quantization](https://arxiv.org/abs/2410.09426)
Append: [Strengthening False Information Propagation Detection: Leveraging SVM and Sophisticated Text Vectorization Techniques in comparison to BERT](https://arxiv.org/abs/2411.12703)
Append: [WebWalker: Benchmarking LLMs in Web Traversal](https://arxiv.org/abs/2501.07572)
Append: [Softplus Attention with Re-weighting Boosts Length Extrapolation in Large Language Models](https://arxiv.org/abs/2501.13428)
Append: [Improving Your Model Ranking on Chatbot Arena by Vote Rigging](https://arxiv.org/abs/2501.17858)
Append: [ReGLA: Refining Gated Linear Attention](https://arxiv.org/abs/2502.01578)
Append: [Predicting Depression in Screening Interviews from Interactive Multi-Theme Collaboration](https://arxiv.org/abs/2502.12204)
Append: [ALFA: Aligning LLMs to Ask Good Questions A Case Study in Clinical Reasoning](https://arxiv.org/abs/2502.14860)
Append: [URO-Bench: Towards Comprehensive Evaluation for End-to-End Spoken Dialogue Models](https://arxiv.org/abs/2502.17810)
Append: [Invisible Walls in Cities: Leveraging Large Language Models to Predict Urban Segregation Experience with Social Media Content](https://arxiv.org/abs/2503.04773)
Append: [X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and Extreme KV Compression](https://arxiv.org/abs/2503.11132)
Append: [Both Direct and Indirect Evidence Contribute to Dative Alternation Preferences in Language Models](https://arxiv.org/abs/2503.20850)
Append: [$\mu$KE: Matryoshka Unstructured Knowledge Editing of Large Language Models](https://arxiv.org/abs/2504.01196)
Append: [Overcoming Vocabulary Constraints with Pixel-level Fallback](https://arxiv.org/abs/2504.02122)
Append: [How Post-Training Reshapes LLMs: A Mechanistic View on Knowledge, Truthfulness, Refusal, and Confidence](https://arxiv.org/abs/2504.02904)
Append: [NoveltyBench: Evaluating Language Models for Humanlike Diversity](https://arxiv.org/abs/2504.05228)
Append: [Do LLMs Understand Your Translations? Evaluating Paragraph-level MT with Question Answering](https://arxiv.org/abs/2504.07583)
Append: [QUDsim: Quantifying Discourse Similarities in LLM-Generated Text](https://arxiv.org/abs/2504.09373)
Append: [GreenMind: A Next-Generation Vietnamese Large Language Model for Structured and Logical Reasoning](https://arxiv.org/abs/2504.16832)
Append: [Planning with Diffusion Models for Target-Oriented Dialogue Systems](https://arxiv.org/abs/2504.16858)
Append: [Steering the CensorShip: Uncovering Representation Vectors for LLM "Thought" Control](https://arxiv.org/abs/2504.17130)
Append: [RAIR: Retrieval-Augmented Iterative Refinement for Chinese Spelling Correction](https://arxiv.org/abs/2504.18938)
Append: [WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional Websites from Scratch](https://arxiv.org/abs/2505.03733)
Append: [Rethinking Prompt Optimizers: From Prompt Merits to Optimization](https://arxiv.org/abs/2505.09930)
Append: [Decoding the Multimodal Mind: Generalizable Brain-to-Text Translation via Multimodal Alignment and Adaptive Routing](https://arxiv.org/abs/2505.10356)
Append: [The taggedPBC: Annotating a massive parallel corpus for crosslinguistic investigations](https://arxiv.org/abs/2505.12560)
Append: [Extracting Probabilistic Knowledge from Large Language Models for Bayesian Network Parameterization](https://arxiv.org/abs/2505.15918)
Append: [WebDancer: Towards Autonomous Information Seeking Agency](https://arxiv.org/abs/2505.22648)
Append: [Document Valuation in LLM Summaries: A Cluster Shapley Approach](https://arxiv.org/abs/2505.23842)
Append: [Verbal Werewolf: Engage Users with Verbalized Agentic Werewolf Game Framework](https://arxiv.org/abs/2506.00160)
Append: [PersianMedQA: Evaluating Large Language Models on a Persian-English Bilingual Medical Question Answering Benchmark](https://arxiv.org/abs/2506.00250)
Append: [HERGC: Heterogeneous Experts Representation and Generative Completion for Multimodal Knowledge Graphs](https://arxiv.org/abs/2506.00826)
Append: [ACCESS DENIED INC: The First Benchmark Environment for Sensitivity Awareness](https://arxiv.org/abs/2506.00964)
Append: [Structure-Augmented Reasoning Generation](https://arxiv.org/abs/2506.08364)
Append: [PersonalAI: A Systematic Comparison of Knowledge Graph Storage and Retrieval Approaches for Personalized LLM agents](https://arxiv.org/abs/2506.17001)
Append: [CycleDistill: Bootstrapping Machine Translation using LLMs with Cyclical Distillation](https://arxiv.org/abs/2506.19952)
Append: [MDC-R: The Minecraft Dialogue Corpus with Reference](https://arxiv.org/abs/2506.22062)
Append: [Auto-TA: Towards Scalable Automated Thematic Analysis (TA) via Multi-Agent Large Language Models with Reinforcement Learning](https://arxiv.org/abs/2506.23998)
Append: [Large Language Models Don't Make Sense of Word Problems. A Scoping Review from a Mathematics Education Perspective](https://arxiv.org/abs/2506.24006)
Append: [EduCoder: An Open-Source Annotation System for Education Transcript Data](https://arxiv.org/abs/2507.05385)
Append: [WSM: Decay-Free Learning Rate Schedule via Checkpoint Merging for LLM Pre-training](https://arxiv.org/abs/2507.17634)
Append: [Towards Greater Leverage: Scaling Laws for Efficient Mixture-of-Experts Language Models](https://arxiv.org/abs/2507.17702)
Append: [Investigating writing style as a contributor to gender gaps in science and technology](https://arxiv.org/abs/2204.13805)
Append: [ComPEFT: Compression for Communicating Parameter Efficient Updates via Sparsification and Quantization](https://arxiv.org/abs/2311.13171)
Append: [SynthVLM: Towards High-Quality and Efficient Synthesis of Image-Caption Datasets for Vision-Language Models](https://arxiv.org/abs/2407.20756)
Append: [A Survey on Model MoErging: Recycling and Routing Among Specialized Experts for Collaborative Learning](https://arxiv.org/abs/2408.07057)
Append: [MathScape: Benchmarking Multimodal Large Language Models in Real-World Mathematical Contexts](https://arxiv.org/abs/2408.07543)
Append: [Meta-Unlearning on Diffusion Models: Preventing Relearning Unlearned Concepts](https://arxiv.org/abs/2410.12777)
Append: [Fairness through Difference Awareness: Measuring Desired Group Discrimination in LLMs](https://arxiv.org/abs/2502.01926)
Append: [ScaffoldGPT: A Scaffold-based GPT Model for Drug Optimization](https://arxiv.org/abs/2502.06891)
Append: [On the Duality between Gradient Transformations and Adapters](https://arxiv.org/abs/2502.13811)
Append: [Collective Reasoning Among LLMs: A Framework for Answer Validation Without Ground Truth](https://arxiv.org/abs/2502.20758)
Append: [Reviewing Clinical Knowledge in Medical Large Language Models: Training and Beyond](https://arxiv.org/abs/2502.20988)
Append: [Instructor-Worker Large Language Model System for Policy Recommendation: a Case Study on Air Quality Analysis of the January 2025 Los Angeles Wildfires](https://arxiv.org/abs/2503.00566)
Append: [Reasoning Capabilities of Large Language Models on Dynamic Tasks](https://arxiv.org/abs/2505.10543)
Append: [Learning to Reason without External Rewards](https://arxiv.org/abs/2505.19590)
Append: [Embracing Imperfection: Simulating Students with Diverse Cognitive Levels Using LLM-based Agents](https://arxiv.org/abs/2505.19997)
Append: [How Far Are We from Generating Missing Modalities with Foundation Models?](https://arxiv.org/abs/2506.03530)
Append: [Exploring Adapter Design Tradeoffs for Low Resource Music Generation](https://arxiv.org/abs/2506.21298)
Append: [GenEscape: Hierarchical Multi-Agent Generation of Escape Room Puzzles](https://arxiv.org/abs/2506.21839)
Append: [ARAG: Agentic Retrieval Augmented Generation for Personalized Recommendation](https://arxiv.org/abs/2506.21931)
Append: [Probabilistic Optimality for Inference-time Scaling](https://arxiv.org/abs/2506.22376)
Append: [UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and Precise Inference-Time Grounding](https://arxiv.org/abs/2507.22025)
append_entries: 201
Finish: 2025-08-12 04:30:25.990951
------------------------------------------------------
Started: 2025-08-12 06:27:32.373021
Existing_entries: 1201
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1427
Summarized using GPT-3.5-turbo
Append: [Language Arithmetics: Towards Systematic Language Neuron Identification and Manipulation](https://arxiv.org/abs/2507.22608)
Token length: 1829
Summarized using GPT-3.5-turbo
Append: [A novel language model for predicting serious adverse event results in clinical trials from their prospective registrations](https://arxiv.org/abs/2507.22919)
Token length: 1314
Summarized using GPT-3.5-turbo
Append: [TextQuests: How Good are LLMs at Text-Based Video Games?](https://arxiv.org/abs/2507.23701)
append_entries: 3
Finish: 2025-08-12 06:27:39.106240
------------------------------------------------------
Started: 2025-08-12 08:23:43.141402
Existing_entries: 1003
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-12 08:23:43.590462
------------------------------------------------------
Started: 2025-08-12 10:18:54.343447
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-12 10:18:54.810280
------------------------------------------------------
Started: 2025-08-12 12:36:09.665966
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-12 12:36:10.165507
------------------------------------------------------
Started: 2025-08-12 14:17:26.644195
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-12 14:17:27.201007
------------------------------------------------------
Started: 2025-08-12 16:21:53.091292
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-12 16:21:53.586224
------------------------------------------------------
Started: 2025-08-12 18:26:31.756125
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-12 18:26:32.204349
------------------------------------------------------
Started: 2025-08-12 20:19:41.700009
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-12 20:19:42.140172
------------------------------------------------------
Started: 2025-08-12 22:16:17.392129
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-12 22:16:17.858247
------------------------------------------------------
Started: 2025-08-13 01:22:04.270605
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-13 01:22:04.749492
------------------------------------------------------
Started: 2025-08-13 03:17:44.916867
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-13 03:17:45.363130
------------------------------------------------------
Started: 2025-08-13 04:30:53.311280
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1272
Summarized using GPT-3.5-turbo
Append: [Argument Quality Annotation and Gender Bias Detection in Financial Communication through Large Language Models](https://arxiv.org/abs/2508.08262)
Token length: 1110
Summarized using GPT-3.5-turbo
Append: [TurQUaz at CheckThat! 2025: Debating Large Language Models for Scientific Web Discourse Detection](https://arxiv.org/abs/2508.08265)
Token length: 1335
Summarized using GPT-3.5-turbo
Append: [Heartificial Intelligence: Exploring Empathy in Language Models](https://arxiv.org/abs/2508.08271)
Token length: 1461
Summarized using GPT-3.5-turbo
Append: [Real-time News Story Identification](https://arxiv.org/abs/2508.08272)
Token length: 1220
Summarized using GPT-3.5-turbo
Append: [TT-XAI: Trustworthy Clinical Text Explanations via Keyword Distillation and LLM Reasoning](https://arxiv.org/abs/2508.08273)
Token length: 1469
Summarized using GPT-3.5-turbo
Append: [Distilling Knowledge from Large Language Models: A Concept Bottleneck Model for Hate and Counter Speech Recognition](https://arxiv.org/abs/2508.08274)
Token length: 1599
Summarized using GPT-3.5-turbo
Append: [MLLM-CBench:A Comprehensive Benchmark for Continual Instruction Tuning of Multimodal LLMs with Chain-of-Thought Reasoning Analysis](https://arxiv.org/abs/2508.08275)
Token length: 1064
Summarized using GPT-3.5-turbo
Append: [Evaluating Contrast Localizer for Identifying Causal Unitsin Social & Mathematical Tasks in Language Models](https://arxiv.org/abs/2508.08276)
Token length: 902
Summarized using GPT-3.5-turbo
Append: [Objective Metrics for Evaluating Large Language Models Using External Data Sources](https://arxiv.org/abs/2508.08277)
Token length: 1131
Summarized using GPT-3.5-turbo
Append: [MinionsLLM: a Task-adaptive Framework For The Training and Control of Multi-Agent Systems Through Natural Language](https://arxiv.org/abs/2508.08283)
Token length: 1080
Summarized using GPT-3.5-turbo
Append: [The Illusion of Progress: Re-evaluating Hallucination Detection in LLMs](https://arxiv.org/abs/2508.08285)
Token length: 1471
Summarized using GPT-3.5-turbo
Append: [Sacred or Synthetic? Evaluating LLM Reliability and Abstention for Religious Questions](https://arxiv.org/abs/2508.08287)
Token length: 1484
Summarized using GPT-3.5-turbo
Append: [Putnam-AXIOM: A Functional and Static Benchmark](https://arxiv.org/abs/2508.08292)
Token length: 1289
Summarized using GPT-3.5-turbo
Append: [CoDAE: Adapting Large Language Models for Education via Chain-of-Thought Data Augmentation](https://arxiv.org/abs/2508.08386)
Token length: 1634
Summarized using GPT-3.5-turbo
Append: [Mol-R1: Towards Explicit Long-CoT Reasoning in Molecule Discovery](https://arxiv.org/abs/2508.08401)
Token length: 1664
Summarized using GPT-3.5-turbo
Append: [Rethinking Tokenization for Rich Morphology: The Dominance of Unigram over BPE and Morphological Alignment](https://arxiv.org/abs/2508.08424)
Token length: 1096
Summarized using GPT-3.5-turbo
Append: [Enhancing Small LLM Alignment through Margin-Based Objective Modifications under Resource Constraints](https://arxiv.org/abs/2508.08466)
Token length: 1145
Summarized using GPT-3.5-turbo
Append: [Momentum Point-Perplexity Mechanics in Large Language Models](https://arxiv.org/abs/2508.08492)
Token length: 1359
Summarized using GPT-3.5-turbo
Append: [Steerable Pluralism: Pluralistic Alignment via Few-Shot Comparative Regression](https://arxiv.org/abs/2508.08509)
Token length: 817
Summarized using GPT-3.5-turbo
Append: [DeCAL Tokenwise Compression](https://arxiv.org/abs/2508.08514)
Token length: 1240
Summarized using GPT-3.5-turbo
Append: [DepressLLM: Interpretable domain-adapted language model for depression detection from real-world narratives](https://arxiv.org/abs/2508.08591)
Token length: 1860
Summarized using GPT-3.5-turbo
Append: [Optimizing Retrieval-Augmented Generation (RAG) for Colloquial Cantonese: A LoRA-Based Systematic Review](https://arxiv.org/abs/2508.08610)
Token length: 1944
Summarized using GPT-3.5-turbo
Append: [InternBootcamp Technical Report: Boosting LLM Reasoning with Verifiable Task Scaling](https://arxiv.org/abs/2508.08636)
Token length: 1902
Summarized using GPT-3.5-turbo
Append: [Quick on the Uptake: Eliciting Implicit Intents from Human Demonstrations for Personalized Mobile-Use Agents](https://arxiv.org/abs/2508.08645)
Token length: 757
Summarized using GPT-3.5-turbo
Append: [LLaMA-Based Models for Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2508.08649)
Token length: 865
Summarized using GPT-3.5-turbo
Append: [UWB at WASSA-2024 Shared Task 2: Cross-lingual Emotion Detection](https://arxiv.org/abs/2508.08650)
Token length: 712
Summarized using GPT-3.5-turbo
Append: [Prompt-Based Approach for Czech Sentiment Analysis](https://arxiv.org/abs/2508.08651)
Token length: 1221
Summarized using GPT-3.5-turbo
Append: [LLM driven Text-to-Table Generation through Sub-Tasks Guidance and Iterative Refinement](https://arxiv.org/abs/2508.08653)
Token length: 1545
Summarized using GPT-3.5-turbo
Append: [TopXGen: Topic-Diverse Parallel Data Generation for Low-Resource Machine Translation](https://arxiv.org/abs/2508.08680)
Token length: 934
Summarized using GPT-3.5-turbo
Append: [Out of the Box, into the Clinic? Evaluating State-of-the-Art ASR for Clinical Applications for Older Adults](https://arxiv.org/abs/2508.08684)
Token length: 1460
Summarized using GPT-3.5-turbo
Append: [A Survey on Parallel Text Generation: From Parallel Decoding to Diffusion Language Models](https://arxiv.org/abs/2508.08712)
Token length: 1545
Summarized using GPT-3.5-turbo
Append: [IROTE: Human-like Traits Elicitation of Large Language Model via In-Context Self-Reflective Optimization](https://arxiv.org/abs/2508.08719)
Token length: 1570
Summarized using GPT-3.5-turbo
Append: [Magical: Medical Lay Language Generation via Semantic Invariance and Layperson-tailored Adaptation](https://arxiv.org/abs/2508.08730)
Token length: 1499
Summarized using GPT-3.5-turbo
Append: [SciRerankBench: Benchmarking Rerankers Towards Scientific Retrieval-Augmented Generated LLMs](https://arxiv.org/abs/2508.08742)
Token length: 1292
Summarized using GPT-3.5-turbo
Append: [DevNous: An LLM-Based Multi-Agent System for Grounding IT Project Management in Unstructured Conversation](https://arxiv.org/abs/2508.08761)
Token length: 1910
Summarized using GPT-3.5-turbo
Append: [Privacy-protected Retrieval-Augmented Generation for Knowledge Graph Question Answering](https://arxiv.org/abs/2508.08785)
Token length: 1464
Summarized using GPT-3.5-turbo
Append: [Feedback-Driven Tool-Use Improvements in Large Language Models via Automated Build Environments](https://arxiv.org/abs/2508.08791)
Token length: 1401
Summarized using GPT-3.5-turbo
Append: [TiMoE: Time-Aware Mixture of Language Experts](https://arxiv.org/abs/2508.08827)
Token length: 1329
Summarized using GPT-3.5-turbo
Append: [An Investigation of Robustness of LLMs in Mathematical Reasoning: Benchmarking with Mathematically-Equivalent Transformation of Advanced Mathematical Problems](https://arxiv.org/abs/2508.08833)
Token length: 1152
Summarized using GPT-3.5-turbo
Append: [Steering Towards Fairness: Mitigating Political Bias in LLMs](https://arxiv.org/abs/2508.08846)
Token length: 1322
Summarized using GPT-3.5-turbo
Append: [BiasGym: Fantastic Biases and How to Find (and Remove) Them](https://arxiv.org/abs/2508.08855)
Token length: 1327
Summarized using GPT-3.5-turbo
Append: [Weakly Supervised Fine-grained Span-Level Framework for Chinese Radiology Report Quality Assurance](https://arxiv.org/abs/2508.08876)
Token length: 1425
Summarized using GPT-3.5-turbo
Append: [Entangled in Representations: Mechanistic Investigation of Cultural Biases in Large Language Models](https://arxiv.org/abs/2508.08879)
Token length: 1864
Summarized using GPT-3.5-turbo
Append: [ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic Parallelism in LLMs](https://arxiv.org/abs/2508.08895)
Token length: 1257
Summarized using GPT-3.5-turbo
Append: [Munsit at NADI 2025 Shared Task 2: Pushing the Boundaries of Multidialectal Arabic ASR with Weakly Supervised Pretraining and Continual Supervised Fine-tuning](https://arxiv.org/abs/2508.08912)
Token length: 1010
Summarized using GPT-3.5-turbo
Append: [Reveal-Bangla: A Dataset for Cross-Lingual Multi-Step Reasoning Evaluation](https://arxiv.org/abs/2508.08933)
Token length: 1470
Summarized using GPT-3.5-turbo
Append: [Train Long, Think Short: Curriculum Learning for Efficient Reasoning](https://arxiv.org/abs/2508.08940)
Token length: 1360
Summarized using GPT-3.5-turbo
Append: [Jointly Generating and Attributing Answers using Logits of Document-Identifier Tokens](https://arxiv.org/abs/2508.08942)
Token length: 1281
Summarized using GPT-3.5-turbo
Append: [Retrospective Sparse Attention for Efficient Long-Context Generation](https://arxiv.org/abs/2508.09001)
Token length: 912
Summarized using GPT-3.5-turbo
Append: [LyS at SemEval 2025 Task 8: Zero-Shot Code Generation for Tabular QA](https://arxiv.org/abs/2508.09012)
Append: [A Survey on Training-free Alignment of Large Language Models](https://arxiv.org/abs/2508.09016)
Append: [LLM-as-a-Supervisor: Mistaken Therapeutic Behaviors Trigger Targeted Supervisory Feedback](https://arxiv.org/abs/2508.09042)
Append: [MVISU-Bench: Benchmarking Mobile Agents for Real-World Tasks by Multi-App, Vague, Interactive, Single-App and Unethical Instructions](https://arxiv.org/abs/2508.09057)
Append: [READER: Retrieval-Assisted Drafter for Efficient LLM Inference](https://arxiv.org/abs/2508.09072)
Append: [CPO: Addressing Reward Ambiguity in Role-playing Dialogue via Comparative Policy Optimization](https://arxiv.org/abs/2508.09074)
Append: [Utilizing Multilingual Encoders to Improve Large Language Models for Low-Resource Languages](https://arxiv.org/abs/2508.09091)
Append: [Link Prediction for Event Logs in the Process Industry](https://arxiv.org/abs/2508.09096)
Append: [AutoCodeBench: Large Language Models are Automatic Code Benchmark Generators](https://arxiv.org/abs/2508.09101)
Append: [SinLlama - A Large Language Model for Sinhala](https://arxiv.org/abs/2508.09115)
Append: [OdysseyBench: Evaluating LLM Agents on Long-Horizon Complex Office Application Workflows](https://arxiv.org/abs/2508.09124)
Append: [Complex Logical Instruction Generation](https://arxiv.org/abs/2508.09125)
Append: [Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language Models](https://arxiv.org/abs/2508.09138)
Append: [Benchmarking Large Language Models for Geolocating Colonial Virginia Land Grants](https://arxiv.org/abs/2508.08266)
Append: [Doctor Sun: A Bilingual Multimodal Large Language Model for Biomedical AI](https://arxiv.org/abs/2508.08270)
Append: [Maximizing GPU Efficiency via Optimal Adapter Caching: An Analytical Approach for Multi-Tenant LLM Serving](https://arxiv.org/abs/2508.08343)
Append: [Exploring the Technical Knowledge Interaction of Global Digital Humanities: Three-decade Evidence from Bibliometric-based perspectives](https://arxiv.org/abs/2508.08347)
Append: [Bilevel MCTS for Amortized O(1) Node Selection in Classical Planning](https://arxiv.org/abs/2508.08385)
Append: [Re:Verse -- Can Your VLM Read a Manga?](https://arxiv.org/abs/2508.08508)
Append: [Fine-grained Video Dubbing Duration Alignment with Segment Supervised Preference Optimization](https://arxiv.org/abs/2508.08550)
Append: [Adaptive Personalized Conversational Information Retrieval](https://arxiv.org/abs/2508.08634)
Append: [MiGrATe: Mixed-Policy GRPO for Adaptation at Test-Time](https://arxiv.org/abs/2508.08641)
Append: [$\text{M}^{2}$LLM: Multi-view Molecular Representation Learning with Large Language Models](https://arxiv.org/abs/2508.08657)
Append: [MultiAiTutor: Child-Friendly Educational Multilingual Speech Generation Tutor with LLMs](https://arxiv.org/abs/2508.08715)
Append: [Designing Memory-Augmented AR Agents for Spatiotemporal Reasoning in Personalized Task Assistance](https://arxiv.org/abs/2508.08774)
Append: [A Dual-Axis Taxonomy of Knowledge Editing for LLMs: From Mechanisms to Functions](https://arxiv.org/abs/2508.08795)
Append: [Revealing the Role of Audio Channels in ASR Performance Degradation](https://arxiv.org/abs/2508.08967)
Append: [E3-Rewrite: Learning to Rewrite SQL for Executability, Equivalence,and Efficiency](https://arxiv.org/abs/2508.09023)
Append: [P/D-Device: Disaggregated Large Language Model between Cloud and Devices](https://arxiv.org/abs/2508.09035)
Append: [Quantifying Gender Biases Towards Politicians on Reddit](https://arxiv.org/abs/2112.12014)
Append: [Utilizing Large Language Models for Information Extraction from Real Estate Transactions](https://arxiv.org/abs/2404.18043)
Append: [From Pixels to Tokens: Revisiting Object Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2410.06795)
Append: [AdEval: Alignment-based Dynamic Evaluation to Mitigate Data Contamination in Large Language Models](https://arxiv.org/abs/2501.13983)
Append: [EvoP: Robust LLM Inference via Evolutionary Pruning](https://arxiv.org/abs/2502.14910)
Append: [Conformal Linguistic Calibration: Trading-off between Factuality and Specificity](https://arxiv.org/abs/2502.19110)
Append: [Evaluating Large Language Models for Automated Clinical Abstraction in Pulmonary Embolism Registries: Performance Across Model Sizes, Versions, and Parameters](https://arxiv.org/abs/2503.21004)
Append: [Opioid Named Entity Recognition (ONER-2025) from Reddit](https://arxiv.org/abs/2504.00027)
Append: [CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs with Controllable Puzzle Generation](https://arxiv.org/abs/2504.00043)
Append: [ChatBench: From Static Benchmarks to Human-AI Evaluation](https://arxiv.org/abs/2504.07114)
Append: [Retrieval-Augmented Generation with Conflicting Evidence](https://arxiv.org/abs/2504.13079)
Append: [Do LLMs Really Forget? Evaluating Unlearning with Knowledge Correlation and Confidence Awareness](https://arxiv.org/abs/2506.05735)
Append: [Mind the Gap: Benchmarking LLM Uncertainty, Discrimination, and Calibration in Specialty-Aware Clinical QA](https://arxiv.org/abs/2506.10769)
Append: [Unsupervised Document and Template Clustering using Multimodal Embeddings](https://arxiv.org/abs/2506.12116)
Append: [Post-Completion Learning for Language Models](https://arxiv.org/abs/2507.20252)
Append: [DYNARTmo: A Dynamic Articulatory Model for Visualization of Speech Movement Patterns](https://arxiv.org/abs/2507.20343)
Append: [AI Pedagogy: Dialogic Social Learning for Artificial Agents](https://arxiv.org/abs/2507.21065)
Append: [Role-Aware Language Models for Secure and Contextualized Access Control in Organizations](https://arxiv.org/abs/2507.23465)
Append: [A Novel Evaluation Benchmark for Medical LLMs: Illuminating Safety and Effectiveness in Clinical Domains](https://arxiv.org/abs/2507.23486)
Append: [AIOS: LLM Agent Operating System](https://arxiv.org/abs/2403.16971)
Append: [VisionUnite: A Vision-Language Foundation Model for Ophthalmology Enhanced with Clinical Knowledge](https://arxiv.org/abs/2408.02865)
Append: [Task Diversity Shortens the ICL Plateau](https://arxiv.org/abs/2410.05448)
Append: [A Risk Taxonomy and Reflection Tool for Large Language Model Adoption in Public Health](https://arxiv.org/abs/2411.02594)
Append: [Decoding-based Regression](https://arxiv.org/abs/2501.19383)
Append: [Sleepless Nights, Sugary Days: Creating Synthetic Users with Health Conditions for Realistic Coaching Agent Interactions](https://arxiv.org/abs/2502.13135)
Append: [OSMa-Bench: Evaluating Open Semantic Mapping Under Varying Lighting Conditions](https://arxiv.org/abs/2503.10331)
Append: [Adaptive Computation Pruning for the Forgetting Transformer](https://arxiv.org/abs/2504.06949)
Append: [Mitigating Hallucination in Large Vision-Language Models via Adaptive Attention Calibration](https://arxiv.org/abs/2505.21472)
Append: [CulturalFrames: Assessing Cultural Expectation Alignment in Text-to-Image Models and Evaluation Metrics](https://arxiv.org/abs/2506.08835)
Append: [Argus Inspection: Do Multimodal Large Language Models Possess the Eye of Panoptes?](https://arxiv.org/abs/2506.14805)
append_entries: 108
Finish: 2025-08-13 04:32:30.205660
------------------------------------------------------
Started: 2025-08-13 06:27:21.431391
Existing_entries: 1108
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1462
Summarized using GPT-3.5-turbo
Append: [Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training](https://arxiv.org/abs/2508.00414)
Token length: 1294
Summarized using GPT-3.5-turbo
Append: [Context-based Motion Retrieval using Open Vocabulary Methods for Autonomous Driving](https://arxiv.org/abs/2508.00589)
append_entries: 2
Finish: 2025-08-13 06:27:25.619691
------------------------------------------------------
Started: 2025-08-13 08:23:56.494931
Existing_entries: 1002
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-13 08:23:56.785441
------------------------------------------------------
Started: 2025-08-13 10:19:17.438904
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-13 10:19:17.731100
------------------------------------------------------
Started: 2025-08-13 12:36:50.783628
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-13 12:36:51.091861
------------------------------------------------------
Started: 2025-08-13 14:18:05.611992
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-13 14:18:05.919661
------------------------------------------------------
Started: 2025-08-13 16:18:35.211385
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-13 16:18:35.505289
------------------------------------------------------
Started: 2025-08-13 18:23:49.969096
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-13 18:23:50.266628
------------------------------------------------------
Started: 2025-08-13 20:19:01.622043
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-13 20:19:01.909854
------------------------------------------------------
Started: 2025-08-13 22:17:05.382393
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-13 22:17:05.682354
------------------------------------------------------
Started: 2025-08-14 01:22:21.177102
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-14 01:22:21.571809
------------------------------------------------------
Started: 2025-08-14 03:17:58.588060
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-14 03:17:58.900964
------------------------------------------------------
Started: 2025-08-14 04:31:28.611939
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1624
Summarized using GPT-3.5-turbo
Append: [ParallelSearch: Train your LLMs to Decompose Query and Search Sub-queries in Parallel with Reinforcement Learning](https://arxiv.org/abs/2508.09303)
Token length: 1508
Summarized using GPT-3.5-turbo
Append: [Leveraging Large Language Models for Rare Disease Named Entity Recognition](https://arxiv.org/abs/2508.09323)
Token length: 1382
Summarized using GPT-3.5-turbo
Append: [TEN: Table Explicitization, Neurosymbolically](https://arxiv.org/abs/2508.09324)
Token length: 1835
Summarized using GPT-3.5-turbo
Append: [Decoding Neural Emotion Patterns through Natural Language Processing Embeddings](https://arxiv.org/abs/2508.09337)
Token length: 1909
Summarized using GPT-3.5-turbo
Append: [The Human-AI Hybrid Delphi Model: A Structured Framework for Context-Rich, Expert Consensus in Complex Domains](https://arxiv.org/abs/2508.09349)
Token length: 1034
Summarized using GPT-3.5-turbo
Append: [Flow-SLM: Joint Learning of Linguistic and Acoustic Information for Spoken Language Modeling](https://arxiv.org/abs/2508.09350)
Token length: 966
Summarized using GPT-3.5-turbo
Append: [APIO: Automatic Prompt Induction and Optimization for Grammatical Error Correction and Text Simplification](https://arxiv.org/abs/2508.09378)
Token length: 1102
Summarized using GPT-3.5-turbo
Append: [Columbo: Expanding Abbreviated Column Names for Tabular Data Using Large Language Models](https://arxiv.org/abs/2508.09403)
Token length: 987
Summarized using GPT-3.5-turbo
Append: [Leveraging Zipformer Model for Effective Language Identification in Code-Switched Child-Directed Speech](https://arxiv.org/abs/2508.09430)
Token length: 1538
Summarized using GPT-3.5-turbo
Append: [From Charts to Fair Narratives: Uncovering and Mitigating Geo-Economic Biases in Chart-to-Text](https://arxiv.org/abs/2508.09450)
Token length: 1160
Summarized using GPT-3.5-turbo
Append: [User-centric Subjective Leaderboard by Customizable Reward Modeling](https://arxiv.org/abs/2508.09463)
Token length: 1356
Summarized using GPT-3.5-turbo
Append: [Learning Facts at Scale with Active Reading](https://arxiv.org/abs/2508.09494)
Token length: 1320
Summarized using GPT-3.5-turbo
Append: [From Ranking to Selection: A Simple but Efficient Dynamic Passage Selector for Retrieval Augmented Generation](https://arxiv.org/abs/2508.09497)
Token length: 1134
Summarized using GPT-3.5-turbo
Append: [LACA: Improving Cross-lingual Aspect-Based Sentiment Analysis with LLM Data Augmentation](https://arxiv.org/abs/2508.09515)
Token length: 1212
Summarized using GPT-3.5-turbo
Append: [Cross-lingual Aspect-Based Sentiment Analysis: A Survey on Tasks, Approaches, and Challenges](https://arxiv.org/abs/2508.09516)
Token length: 768
Summarized using GPT-3.5-turbo
Append: [UWBa at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval](https://arxiv.org/abs/2508.09517)
Token length: 928
Summarized using GPT-3.5-turbo
Append: [COMPEER: Controllable Empathetic Reinforcement Reasoning for Emotional Support Conversation](https://arxiv.org/abs/2508.09521)
Token length: 1912
Summarized using GPT-3.5-turbo
Append: [The Surprising Effectiveness of Membership Inference with Simple N-Gram Coverage](https://arxiv.org/abs/2508.09603)
Token length: 1435
Summarized using GPT-3.5-turbo
Append: [AINL-Eval 2025 Shared Task: Detection of AI-Generated Scientific Abstracts in Russian](https://arxiv.org/abs/2508.09622)
Token length: 952
Summarized using GPT-3.5-turbo
Append: [Improving Diversity in Language Models: When Temperature Fails, Change the Loss](https://arxiv.org/abs/2508.09654)
Token length: 1538
Summarized using GPT-3.5-turbo
Append: [EffiEval: Efficient and Generalizable Model Evaluation via Capability Coverage Maximization](https://arxiv.org/abs/2508.09662)
Token length: 1729
Summarized using GPT-3.5-turbo
Append: [Slow Tuning and Low-Entropy Masking for Safe Chain-of-Thought Distillation](https://arxiv.org/abs/2508.09666)
Token length: 987
Summarized using GPT-3.5-turbo
Append: [Evaluating the Role of Large Language Models in Legal Practice in India](https://arxiv.org/abs/2508.09713)
Token length: 1228
Summarized using GPT-3.5-turbo
Append: [The Perils of Chart Deception: How Misleading Visualizations Affect Vision-Language Models](https://arxiv.org/abs/2508.09716)
Token length: 1433
Summarized using GPT-3.5-turbo
Append: [Sample More to Think Less: Group Filtered Policy Optimization for Concise Reasoning](https://arxiv.org/abs/2508.09726)
Token length: 1131
Summarized using GPT-3.5-turbo
Append: [Transforming Questions and Documents for Semantically Aligned Retrieval-Augmented Generation](https://arxiv.org/abs/2508.09755)
Token length: 1143
Summarized using GPT-3.5-turbo
Append: [Echoes of Agreement: Argument Driven Opinion Shifts in Large Language Models](https://arxiv.org/abs/2508.09759)
Token length: 954
Summarized using GPT-3.5-turbo
Append: [UtterTune: LoRA-Based Target-Language Pronunciation Edit and Control in Multilingual Text-to-Speech](https://arxiv.org/abs/2508.09767)
Token length: 1241
Summarized using GPT-3.5-turbo
Append: [Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study](https://arxiv.org/abs/2508.09776)
Token length: 1259
Summarized using GPT-3.5-turbo
Append: [Adoption of Explainable Natural Language Processing: Perspectives from Industry and Academia on Practices and Challenges](https://arxiv.org/abs/2508.09786)
Token length: 1674
Summarized using GPT-3.5-turbo
Append: [BigCharts-R1: Enhanced Chart Reasoning with Visual Reinforcement Finetuning](https://arxiv.org/abs/2508.09804)
Token length: 1867
Summarized using GPT-3.5-turbo
Append: [A Comprehensive Survey of Datasets for Clinical Mental Health AI Systems](https://arxiv.org/abs/2508.09809)
Token length: 1345
Summarized using GPT-3.5-turbo
Append: [Speed Always Wins: A Survey on Efficient Architectures for Large Language Models](https://arxiv.org/abs/2508.09834)
Token length: 1089
Summarized using GPT-3.5-turbo
Append: [PRELUDE: A Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts](https://arxiv.org/abs/2508.09848)
Token length: 990
Summarized using GPT-3.5-turbo
Append: [Assessing the Feasibility of Lightweight Whisper Models for Low-Resource Urdu Transcription](https://arxiv.org/abs/2508.09865)
Token length: 1473
Summarized using GPT-3.5-turbo
Append: [Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models](https://arxiv.org/abs/2508.09874)
Token length: 950
Summarized using GPT-3.5-turbo
Append: [A Survey of Cognitive Distortion Detection and Classification in NLP](https://arxiv.org/abs/2508.09878)
Token length: 1234
Summarized using GPT-3.5-turbo
Append: [Language of Persuasion and Misrepresentation in Business Communication: A Textual Detection Approach](https://arxiv.org/abs/2508.09935)
Token length: 1201
Summarized using GPT-3.5-turbo
Append: [A Comprehensive Evaluation framework of Alignment Techniques for LLMs](https://arxiv.org/abs/2508.09937)
Token length: 1362
Summarized using GPT-3.5-turbo
Append: [VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models](https://arxiv.org/abs/2508.09945)
Token length: 1206
Summarized using GPT-3.5-turbo
Append: [Specialised or Generic? Tokenization Choices for Radiology Language Models](https://arxiv.org/abs/2508.09952)
Token length: 1090
Summarized using GPT-3.5-turbo
Append: [Shaping Event Backstories to Estimate Potential Emotion Contexts](https://arxiv.org/abs/2508.09954)
Token length: 1703
Summarized using GPT-3.5-turbo
Append: [Performance of GPT-5 Frontier Models in Ophthalmology Question Answering](https://arxiv.org/abs/2508.09956)
Token length: 1563
Summarized using GPT-3.5-turbo
Append: [Which one Performs Better? Wav2Vec or Whisper? Applying both in Badini Kurdish Speech to Text (BKSTT)](https://arxiv.org/abs/2508.09957)
Token length: 1912
Summarized using GPT-3.5-turbo
Append: [Neural Bandit Based Optimal LLM Selection for a Pipeline of Tasks](https://arxiv.org/abs/2508.09958)
Token length: 1518
Summarized using GPT-3.5-turbo
Append: [MoLAN: A Unified Modality-Aware Noise Dynamic Editing Framework for Multimodal Sentiment Analysis](https://arxiv.org/abs/2508.09145)
Token length: 1541
Summarized using GPT-3.5-turbo
Append: [$\Delta$-AttnMask: Attention-Guided Masked Hidden States for Efficient Data Selection and Augmentation](https://arxiv.org/abs/2508.09199)
Token length: 1241
Summarized using GPT-3.5-turbo
Append: [From Hard Refusals to Safe-Completions: Toward Output-Centric Safety Training](https://arxiv.org/abs/2508.09224)
Token length: 1349
Summarized using GPT-3.5-turbo
Append: [NEFMind: Parameter-Efficient Fine-Tuning of Open-Source LLMs for Telecom APIs Automation](https://arxiv.org/abs/2508.09240)
Token length: 1289
Summarized using GPT-3.5-turbo
Append: [Can AI Keep a Secret? Contextual Integrity Verification: A Provable Security Architecture for LLMs](https://arxiv.org/abs/2508.09288)
Append: [Fake-Mamba: Real-Time Speech Deepfake Detection Using Bidirectional Mamba as Self-Attention's Alternative](https://arxiv.org/abs/2508.09294)
Append: [ProMode: A Speech Prosody Model Conditioned on Acoustic and Textual Inputs](https://arxiv.org/abs/2508.09389)
Append: [Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference](https://arxiv.org/abs/2508.09442)
Append: [IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding](https://arxiv.org/abs/2508.09456)
Append: [NeuronTune: Fine-Grained Neuron Modulation for Balanced Safety-Utility Alignment in LLMs](https://arxiv.org/abs/2508.09473)
Append: [AI Blob! LLM-Driven Recontextualization of Italian Television Archives](https://arxiv.org/abs/2508.09535)
Append: [How Persuasive Could LLMs Be? A First Study Combining Linguistic-Rhetorical Analysis and User Experiments](https://arxiv.org/abs/2508.09614)
Append: [A Close Reading Approach to Gender Narrative Biases in AI-Generated Stories](https://arxiv.org/abs/2508.09651)
Append: [COME: Dual Structure-Semantic Learning with Collaborative MoE for Universal Lesion Detection Across Heterogeneous Ultrasound Datasets](https://arxiv.org/abs/2508.09886)
Append: [Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation](https://arxiv.org/abs/2508.09987)
Append: [From Stars to Insights: Exploration and Implementation of Unified Sentiment Analysis with Distant Supervision](https://arxiv.org/abs/2305.01710)
Append: [Robo-Instruct: Simulator-Augmented Instruction Alignment For Finetuning Code LLMs](https://arxiv.org/abs/2405.20179)
Append: [LongIns: A Challenging Long-context Instruction-based Exam for LLMs](https://arxiv.org/abs/2406.17588)
Append: [Improving Multimodal Large Language Models Using Continual Learning](https://arxiv.org/abs/2410.19925)
Append: [Leveraging Audio and Text Modalities in Mental Health: A Study of LLMs Performance](https://arxiv.org/abs/2412.10417)
Append: [Memorization Over Reasoning? Exposing and Mitigating Verbatim Memorization in Large Language Models' Character Understanding Evaluation](https://arxiv.org/abs/2412.14368)
Append: [Beyond Memorization: Assessing Semantic Generalization in Large Language Models Using Phrasal Constructions](https://arxiv.org/abs/2501.04661)
Append: [Benchmarking LLMs' Mathematical Reasoning with Unseen Random Variables Questions](https://arxiv.org/abs/2501.11790)
Append: [RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache Compression](https://arxiv.org/abs/2502.14051)
Append: [Efficient Inference for Large Reasoning Models: A Survey](https://arxiv.org/abs/2503.23077)
Append: [Follow the Flow: On Information Flow Across Textual Tokens in Text-to-Image Models](https://arxiv.org/abs/2504.01137)
Append: [CO-Bench: Benchmarking Language Model Agents in Algorithm Search for Combinatorial Optimization](https://arxiv.org/abs/2504.04310)
Append: [AI-Slop to AI-Polish? Aligning Language Models through Edit-Based Writing Rewards and Test-time Computation](https://arxiv.org/abs/2504.07532)
Append: [Non-native Children's Automatic Speech Assessment Challenge (NOCASA)](https://arxiv.org/abs/2504.20678)
Append: [IP-CRR: Information Pursuit for Interpretable Classification of Chest Radiology Reports](https://arxiv.org/abs/2505.00191)
Append: [Towards Safer Pretraining: Analyzing and Filtering Harmful Content in Webscale datasets for Responsible LLMs](https://arxiv.org/abs/2505.02009)
Append: [Teaching Large Language Models to Maintain Contextual Faithfulness via Synthetic Tasks and Reinforcement Learning](https://arxiv.org/abs/2505.16483)
Append: [LogicCat: A Chain-of-Thought Text-to-SQL Benchmark for Complex Reasoning](https://arxiv.org/abs/2505.18744)
Append: [MemGuide: Intent-Driven Memory Selection for Goal-Oriented Multi-Session LLM Agents](https://arxiv.org/abs/2505.20231)
Append: [Exploring Scaling Laws for EHR Foundation Models](https://arxiv.org/abs/2505.22964)
Append: [Sarc7: Evaluating Sarcasm Detection and Generation with Seven Types and Emotion-Informed Techniques](https://arxiv.org/abs/2506.00658)
Append: [DefenderBench: A Toolkit for Evaluating Language Agents in Cybersecurity Environments](https://arxiv.org/abs/2506.00739)
Append: [Applying Text Embedding Models for Efficient Analysis in Labeled Property Graphs](https://arxiv.org/abs/2507.10772)
Append: [Discrete Neural Algorithmic Reasoning](https://arxiv.org/abs/2402.11628)
Append: [LUMA: A Benchmark Dataset for Learning from Uncertain and Multimodal Data](https://arxiv.org/abs/2406.09864)
Append: [Multi-Step Reasoning with Large Language Models, a Survey](https://arxiv.org/abs/2407.11511)
Append: [Explaining Caption-Image Interactions in CLIP Models with Second-Order Attributions](https://arxiv.org/abs/2408.14153)
Append: [Analyzing Finetuning Representation Shift for Multimodal LLMs Steering](https://arxiv.org/abs/2501.03012)
Append: [Shifting Perspectives: Steering Vectors for Robust Bias Mitigation in LLMs](https://arxiv.org/abs/2503.05371)
Append: [MedRep: Medical Concept Representation for General Electronic Health Record Foundation Models](https://arxiv.org/abs/2504.08329)
Append: [EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text Prompting](https://arxiv.org/abs/2504.12867)
Append: [MapStory: Prototyping Editable Map Animations with LLM Agents](https://arxiv.org/abs/2505.21966)
Append: [ContestTrade: A Multi-Agent Trading System Based on Internal Contest Mechanism](https://arxiv.org/abs/2508.00554)
append_entries: 93
Finish: 2025-08-14 04:33:11.034695
------------------------------------------------------
Started: 2025-08-14 06:27:26.723245
Existing_entries: 1093
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1494
Summarized using GPT-3.5-turbo
Append: [Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens](https://arxiv.org/abs/2508.01191)
append_entries: 1
Finish: 2025-08-14 06:27:29.618254
------------------------------------------------------
Started: 2025-08-14 08:24:00.482183
Existing_entries: 1001
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-14 08:24:00.739053
------------------------------------------------------
Started: 2025-08-14 10:19:20.586757
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-14 10:19:20.919875
------------------------------------------------------
Started: 2025-08-14 12:37:20.778825
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-14 12:37:21.070871
------------------------------------------------------
Started: 2025-08-14 14:17:27.797849
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-14 14:17:28.131659
------------------------------------------------------
Started: 2025-08-14 16:22:59.785708
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-14 16:23:00.047755
------------------------------------------------------
Started: 2025-08-14 18:25:45.166734
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-14 18:25:45.420087
------------------------------------------------------
Started: 2025-08-14 20:19:38.807154
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-14 20:19:39.067681
------------------------------------------------------
Started: 2025-08-14 22:16:19.095661
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-14 22:16:19.374594
------------------------------------------------------
Started: 2025-08-15 01:22:50.779958
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-15 01:22:51.141920
------------------------------------------------------
Started: 2025-08-15 03:19:20.686760
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-15 03:19:20.947086
------------------------------------------------------
Started: 2025-08-15 04:31:22.384410
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1427
Summarized using GPT-3.5-turbo
Append: [Bridging AI Innovation and Healthcare Needs: Lessons Learned from Incorporating Modern NLP at The BC Cancer Registry](https://arxiv.org/abs/2508.09991)
Token length: 1388
Summarized using GPT-3.5-turbo
Append: [A Transparent Fairness Evaluation Protocol for Open-Source Language Model Benchmarking on the Blockchain](https://arxiv.org/abs/2508.09993)
Token length: 1428
Summarized using GPT-3.5-turbo
Append: [Thematic and Task-Based Categorization of K-12 GenAI Usages with Hierarchical Topic Modeling](https://arxiv.org/abs/2508.09997)
Token length: 1123
Summarized using GPT-3.5-turbo
Append: [INTIMA: A Benchmark for Human-AI Companionship Behavior](https://arxiv.org/abs/2508.09998)
Token length: 1512
Summarized using GPT-3.5-turbo
Append: [XFacta: Contemporary, Real-World Dataset and Evaluation for Multimodal Misinformation Detection with Multimodal LLMs](https://arxiv.org/abs/2508.09999)
Token length: 1072
Summarized using GPT-3.5-turbo
Append: [AutoGeTS: Knowledge-based Automated Generation of Text Synthetics for Improving Text Classification](https://arxiv.org/abs/2508.10000)
Token length: 1457
Summarized using GPT-3.5-turbo
Append: [HiFACTMix: A Code-Mixed Benchmark and Graph-Aware Model for EvidenceBased Political Claim Verification in Hinglish](https://arxiv.org/abs/2508.10001)
Token length: 1229
Summarized using GPT-3.5-turbo
Append: [Semantic Structure in Large Language Model Embeddings](https://arxiv.org/abs/2508.10003)
Token length: 1831
Summarized using GPT-3.5-turbo
Append: [User Perception of Attention Visualizations: Effects on Interpretability Across Evidence-Based Medical Documents](https://arxiv.org/abs/2508.10004)
Token length: 1147
Summarized using GPT-3.5-turbo
Append: [From Answers to Questions: EQGBench for Evaluating LLMs' Educational Question Generation](https://arxiv.org/abs/2508.10005)
Token length: 1857
Summarized using GPT-3.5-turbo
Append: [Automated scoring of the Ambiguous Intentions Hostility Questionnaire using fine-tuned large language models](https://arxiv.org/abs/2508.10007)
Token length: 660
Summarized using GPT-3.5-turbo
Append: [Multidimensional classification of posts for online course discussion forum curation](https://arxiv.org/abs/2508.10008)
Token length: 1000
Summarized using GPT-3.5-turbo
Append: [Beyond Hard Sharing: Efficient Multi-Task Speech-to-Text Modeling with Supervised Mixture of Experts](https://arxiv.org/abs/2508.10009)
Token length: 1259
Summarized using GPT-3.5-turbo
Append: [An Audit and Analysis of LLM-Assisted Health Misinformation Jailbreaks Against LLMs](https://arxiv.org/abs/2508.10010)
Token length: 1955
Summarized using GPT-3.5-turbo
Append: [Evaluation of GPT-based large language generative AI models as study aids for the national licensure examination for registered dietitians in Japan](https://arxiv.org/abs/2508.10011)
Token length: 1382
Summarized using GPT-3.5-turbo
Append: [Guided Navigation in Knowledge-Dense Environments: Structured Semantic Exploration with Guidance Graphs](https://arxiv.org/abs/2508.10012)
Token length: 1879
Summarized using GPT-3.5-turbo
Append: [Semantic Bridge: Universal Multi-Hop Question Generation via AMR-Driven Graph Synthesis](https://arxiv.org/abs/2508.10013)
Token length: 1494
Summarized using GPT-3.5-turbo
Append: [PersonaEval: Are LLM Evaluators Human Enough to Judge Role-Play?](https://arxiv.org/abs/2508.10014)
Token length: 1443
Summarized using GPT-3.5-turbo
Append: [RealTalk-CN: A Realistic Chinese Speech-Text Dialogue Benchmark With Cross-Modal Interaction Analysis](https://arxiv.org/abs/2508.10015)
Token length: 1797
Summarized using GPT-3.5-turbo
Append: [Training-Free Multimodal Large Language Model Orchestration](https://arxiv.org/abs/2508.10016)
Token length: 1298
Summarized using GPT-3.5-turbo
Append: [A Rose by Any Other Name Would Smell as Sweet: Categorical Homotopy Theory for Large Language Models](https://arxiv.org/abs/2508.10018)
Token length: 1773
Summarized using GPT-3.5-turbo
Append: [Decoupling Understanding from Reasoning via Problem Space Mapping for Small-scale Model Reasoning](https://arxiv.org/abs/2508.10019)
Token length: 1949
Summarized using GPT-3.5-turbo
Append: [FedCoT: Communication-Efficient Federated Reasoning Enhancement for Large Language Models](https://arxiv.org/abs/2508.10020)
Token length: 960
Summarized using GPT-3.5-turbo
Append: [LATTE: Learning Aligned Transactions and Textual Embeddings for Bank Clients](https://arxiv.org/abs/2508.10021)
Token length: 1431
Summarized using GPT-3.5-turbo
Append: [Conformal P-Value in Multiple-Choice Question Answering Tasks with Provable Risk Control](https://arxiv.org/abs/2508.10022)
Token length: 1364
Summarized using GPT-3.5-turbo
Append: [RTTC: Reward-Guided Collaborative Test-Time Compute](https://arxiv.org/abs/2508.10024)
Token length: 1349
Summarized using GPT-3.5-turbo
Append: [Detecting and explaining postpartum depression in real-time with generative artificial intelligence](https://arxiv.org/abs/2508.10025)
Token length: 1402
Summarized using GPT-3.5-turbo
Append: [SABER: Switchable and Balanced Training for Efficient LLM Reasoning](https://arxiv.org/abs/2508.10026)
Token length: 1927
Summarized using GPT-3.5-turbo
Append: [LLMCARE: Alzheimer's Detection via Transformer Models Enhanced by LLM-Generated Synthetic Data](https://arxiv.org/abs/2508.10027)
Token length: 1577
Summarized using GPT-3.5-turbo
Append: [PREF: Reference-Free Evaluation of Personalised Text Generation in LLMs](https://arxiv.org/abs/2508.10028)
Token length: 1189
Summarized using GPT-3.5-turbo
Append: [Latent Fusion Jailbreak: Blending Harmful and Harmless Representations to Elicit Unsafe LLM Outputs](https://arxiv.org/abs/2508.10029)
Token length: 1537
Summarized using GPT-3.5-turbo
Append: [Inference-Aware Prompt Optimization for Aligning Black-Box Large Language Models](https://arxiv.org/abs/2508.10030)
Token length: 1066
Summarized using GPT-3.5-turbo
Append: [The Cost of Thinking: Increased Jailbreak Risk in Large Language Models](https://arxiv.org/abs/2508.10032)
Token length: 1440
Summarized using GPT-3.5-turbo
Append: [Reflect then Learn: Active Prompting for Information Extraction Guided by Introspective Confusion](https://arxiv.org/abs/2508.10036)
Token length: 1596
Summarized using GPT-3.5-turbo
Append: [mSCoRe: a $M$ultilingual and Scalable Benchmark for $S$kill-based $Co$mmonsense $Re$asoning](https://arxiv.org/abs/2508.10137)
Token length: 1133
Summarized using GPT-3.5-turbo
Append: [Multi-Turn Puzzles: Evaluating Interactive Reasoning and Strategic Dialogue in LLMs](https://arxiv.org/abs/2508.10142)
Token length: 1675
Summarized using GPT-3.5-turbo
Append: [LaajMeter: A Framework for LaaJ Evaluation](https://arxiv.org/abs/2508.10161)
Token length: 1255
Summarized using GPT-3.5-turbo
Append: [Estimating Machine Translation Difficulty](https://arxiv.org/abs/2508.10175)
Token length: 1190
Summarized using GPT-3.5-turbo
Append: [Efficient Forward-Only Data Valuation for Pretrained LLMs and VLMs](https://arxiv.org/abs/2508.10180)
Token length: 1373
Summarized using GPT-3.5-turbo
Append: [PakBBQ: A Culturally Adapted Bias Benchmark for QA](https://arxiv.org/abs/2508.10186)
Token length: 1928
Summarized using GPT-3.5-turbo
Append: [Prompt-Response Semantic Divergence Metrics for Faithfulness Hallucination and Misalignment Detection in Large Language Models](https://arxiv.org/abs/2508.10192)
Token length: 647
Summarized using GPT-3.5-turbo
Append: [Understanding Textual Emotion Through Emoji Prediction](https://arxiv.org/abs/2508.10222)
Token length: 1213
Summarized using GPT-3.5-turbo
Append: [Using Large Language Models to Measure Symptom Severity in Patients At Risk for Schizophrenia](https://arxiv.org/abs/2508.10226)
Token length: 649
Summarized using GPT-3.5-turbo
Append: [A Computational Approach to Analyzing Language Change and Variation in the Constructed Language Toki Pona](https://arxiv.org/abs/2508.10246)
Token length: 657
Summarized using GPT-3.5-turbo
Append: [Inductive Bias Extraction and Matching for LLM Prompts](https://arxiv.org/abs/2508.10295)
Token length: 1763
Summarized using GPT-3.5-turbo
Append: [Yet another algorithmic bias: A Discursive Analysis of Large Language Models Reinforcing Dominant Discourses on Gender and Race](https://arxiv.org/abs/2508.10304)
Token length: 1317
Summarized using GPT-3.5-turbo
Append: [ReviewRL: Towards Automated Scientific Review with RL](https://arxiv.org/abs/2508.10308)
Token length: 1500
Summarized using GPT-3.5-turbo
Append: [From Surface to Semantics: Semantic Structure Parsing for Table-Centric Document Analysis](https://arxiv.org/abs/2508.10311)
Token length: 1799
Summarized using GPT-3.5-turbo
Append: [Beyond Semantic Understanding: Preserving Collaborative Frequency Components in LLM-based Recommendation](https://arxiv.org/abs/2508.10312)
Token length: 1404
Summarized using GPT-3.5-turbo
Append: [Cross-Prompt Encoder for Low-Performing Languages](https://arxiv.org/abs/2508.10352)
Append: [Making Qwen3 Think in Korean with Reinforcement Learning](https://arxiv.org/abs/2508.10355)
Append: [Advancing Cross-lingual Aspect-Based Sentiment Analysis with LLMs and Constrained Decoding for Sequence-to-Sequence Models](https://arxiv.org/abs/2508.10366)
Append: [Large Language Models for Summarizing Czech Historical Documents and Beyond](https://arxiv.org/abs/2508.10368)
Append: [Improving Generative Cross-lingual Aspect-Based Sentiment Analysis with Constrained Decoding](https://arxiv.org/abs/2508.10369)
Append: [Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts](https://arxiv.org/abs/2508.10390)
Append: [Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text Generation](https://arxiv.org/abs/2508.10404)
Append: [ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning](https://arxiv.org/abs/2508.10419)
Append: [Evaluating LLMs on Chinese Idiom Translation](https://arxiv.org/abs/2508.10421)
Append: [Computational Economics in Large Language Models: Exploring Model Behavior and Incentive Design under Resource Constraints](https://arxiv.org/abs/2508.10426)
Append: [DiFaR: Enhancing Multimodal Misinformation Detection with Diverse, Factual, and Relevant Rationales](https://arxiv.org/abs/2508.10444)
Append: [When Explainability Meets Privacy: An Investigation at the Intersection of Post-hoc Explainability and Differential Privacy in the Context of Natural Language Processing](https://arxiv.org/abs/2508.10482)
Append: [When Language Overrules: Revealing Text Dominance in Multimodal Large Language Models](https://arxiv.org/abs/2508.10552)
Append: [eDIF: A European Deep Inference Fabric for Remote Interpretability of LLM](https://arxiv.org/abs/2508.10553)
Append: [Neural Machine Translation for Coptic-French: Strategies for Low-Resource Ancient Languages](https://arxiv.org/abs/2508.10683)
Append: [Continuous Bangla Sign Language Translation: Mitigating the Expense of Gloss Annotation with the Assistance of Graph](https://arxiv.org/abs/2508.10687)
Append: [Learning from Natural Language Feedback for Personalized Question Answering](https://arxiv.org/abs/2508.10695)
Append: [Thinking Inside the Mask: In-Place Prompting in Diffusion LLMs](https://arxiv.org/abs/2508.10736)
Append: [Beyond "Not Novel Enough": Enriching Scholarly Critique with LLM-Assisted Feedback](https://arxiv.org/abs/2508.10795)
Append: [Reinforced Language Models for Sequential Decision Making](https://arxiv.org/abs/2508.10839)
Append: [Psyche-R1: Towards Reliable Psychological LLMs through Unified Empathy, Expertise, and Reasoning](https://arxiv.org/abs/2508.10848)
Append: [From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms](https://arxiv.org/abs/2508.10860)
Append: [SSRL: Self-Search Reinforcement Learning](https://arxiv.org/abs/2508.10874)
Append: [A Survey on Diffusion Language Models](https://arxiv.org/abs/2508.10875)
Append: [Personalized Product Search Ranking: A Multi-Task Learning Approach with Tabular and Non-Tabular Data](https://arxiv.org/abs/2508.09636)
Append: [Context Misleads LLMs: The Role of Context Filtering in Maintaining Safe Alignment of LLMs](https://arxiv.org/abs/2508.10031)
Append: [Large Language Models Show Signs of Alignment with Human Neurocognition During Abstract Reasoning](https://arxiv.org/abs/2508.10057)
Append: [SaraCoder: Orchestrating Semantic and Structural Cues for Profit-Oriented Repository-Level Code Completion](https://arxiv.org/abs/2508.10068)
Append: [Amazon Nova AI Challenge -- Trusted AI: Advancing secure, AI-assisted software development](https://arxiv.org/abs/2508.10108)
Append: [Nested-ReFT: Efficient Reinforcement Learning for Large Language Model Fine-Tuning via Off-Policy Rollouts](https://arxiv.org/abs/2508.10123)
Append: [Personalized Real-time Jargon Support for Online Meetings](https://arxiv.org/abs/2508.10239)
Append: [Improving OCR for Historical Texts of Multiple Languages](https://arxiv.org/abs/2508.10356)
Append: [CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model](https://arxiv.org/abs/2508.10416)
Append: [Reverse Physician-AI Relationship: Full-process Clinical Diagnosis Driven by a Large Language Model](https://arxiv.org/abs/2508.10492)
Append: [Diversity First, Quality Later: A Two-Stage Assumption for Language Model Alignment](https://arxiv.org/abs/2508.10530)
Append: [Improving Value-based Process Verifier via Low-Cost Variance Reduction](https://arxiv.org/abs/2508.10539)
Append: [Stabilizing Long-term Multi-turn Reinforcement Learning with Gated Rewards](https://arxiv.org/abs/2508.10548)
Append: [Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models](https://arxiv.org/abs/2508.10751)
Append: [Memory-Augmented Transformers: A Systematic Review from Neuroscience Principles to Technical Solutions](https://arxiv.org/abs/2508.10824)
Append: [Searching for Privacy Risks in LLM Agents via Simulation](https://arxiv.org/abs/2508.10880)
Append: [Knowledge-based Consistency Testing of Large Language Models](https://arxiv.org/abs/2407.12830)
Append: [This Candidate is [MASK]. Prompt-based Sentiment Extraction and Reference Letters](https://arxiv.org/abs/2410.16325)
Append: [Fleurs-SLU: A Massively Multilingual Benchmark for Spoken Language Understanding](https://arxiv.org/abs/2501.06117)
Append: [Measuring Diversity in Synthetic Datasets](https://arxiv.org/abs/2502.08512)
Append: [LED-Merging: Mitigating Safety-Utility Conflicts in Model Merging with Location-Election-Disjoint](https://arxiv.org/abs/2502.16770)
Append: [TikZero: Zero-Shot Text-Guided Graphics Program Synthesis](https://arxiv.org/abs/2503.11509)
Append: [Explainable Sentiment Analysis with DeepSeek-R1: Performance, Efficiency, and Few-Shot Learning](https://arxiv.org/abs/2503.11655)
Append: [Building Instruction-Tuning Datasets from Human-Written Instructions with Open-Weight Large Language Models](https://arxiv.org/abs/2503.23714)
Append: [ToolACE-R: Model-aware Iterative Training and Adaptive Refinement for Tool Learning](https://arxiv.org/abs/2504.01400)
Append: [CoTAL: Human-in-the-Loop Prompt Engineering for Generalizable Formative Assessment Scoring](https://arxiv.org/abs/2504.02323)
Append: [Swedish Whispers; Leveraging a Massive Speech Corpus for Swedish Speech Recognition](https://arxiv.org/abs/2505.17538)
Append: [Curse of High Dimensionality Issue in Transformer for Long-context Modeling](https://arxiv.org/abs/2505.22107)
Append: [AF-MAT: Aspect-aware Flip-and-Fuse xLSTM for Aspect-based Sentiment Analysis](https://arxiv.org/abs/2507.01213)
Append: [Meanings are like Onions: a Layered Approach to Metaphor Processing](https://arxiv.org/abs/2507.10354)
Append: [CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks](https://arxiv.org/abs/2507.10535)
Append: [DeepWriter: A Fact-Grounded Multimodal Writing Assistant Based On Offline Knowledge Base](https://arxiv.org/abs/2507.14189)
Append: [A New Query Expansion Approach via Agent-Mediated Dialogic Inquiry](https://arxiv.org/abs/2502.08557)
Append: [BitDecoding: Unlocking Tensor Cores for Long-Context LLMs with Low-Bit KV Cache](https://arxiv.org/abs/2503.18773)
Append: [CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting](https://arxiv.org/abs/2504.15485)
Append: [Grouped Sequency-arranged Rotation: Optimizing Rotation Transformation for Quantization for Free](https://arxiv.org/abs/2505.03810)
Append: [FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference](https://arxiv.org/abs/2505.13109)
Append: [MAPS: A Multilingual Benchmark for Global Agent Performance and Security](https://arxiv.org/abs/2505.15935)
Append: [Evaluation of Cultural Competence of Vision-Language Models](https://arxiv.org/abs/2505.22793)
Append: [Prompt Attacks Reveal Superficial Knowledge Removal in Unlearning Methods](https://arxiv.org/abs/2506.10236)
Append: [A Detailed Factor Analysis for the Political Compass Test: Navigating Ideologies of Large Language Models](https://arxiv.org/abs/2506.22493)
Append: [LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy Optimization](https://arxiv.org/abs/2507.15758)
append_entries: 115
Finish: 2025-08-15 04:32:56.229493
------------------------------------------------------
Started: 2025-08-15 06:26:56.706805
Existing_entries: 1115
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1488
Summarized using GPT-3.5-turbo
Append: [Marco-Voice Technical Report](https://arxiv.org/abs/2508.02038)
append_entries: 1
Finish: 2025-08-15 06:26:59.132026
------------------------------------------------------
Started: 2025-08-15 08:22:54.375012
Existing_entries: 1001
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1046
Summarized using GPT-3.5-turbo
Append: [Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic Empirical Study](https://arxiv.org/abs/2506.19794)
append_entries: 1
Finish: 2025-08-15 08:22:57.953458
------------------------------------------------------
Started: 2025-08-15 10:18:12.356891
Existing_entries: 1001
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-15 10:18:12.687620
------------------------------------------------------
Started: 2025-08-15 12:34:16.730428
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-15 12:34:17.026283
------------------------------------------------------
Started: 2025-08-15 14:16:51.521061
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-15 14:16:51.907204
------------------------------------------------------
Started: 2025-08-15 16:21:01.722759
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-15 16:21:02.043499
------------------------------------------------------
Started: 2025-08-15 18:24:52.428781
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-15 18:24:52.725144
------------------------------------------------------
Started: 2025-08-15 20:18:39.463476
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-15 20:18:39.759389
------------------------------------------------------
Started: 2025-08-15 22:16:20.850544
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-15 22:16:21.147052
------------------------------------------------------
Started: 2025-08-16 01:18:46.748181
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-16 01:18:47.064818
------------------------------------------------------
Started: 2025-08-16 03:12:34.862077
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-16 03:12:35.153901
------------------------------------------------------
Started: 2025-08-16 04:21:55.886837
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-16 04:21:55.962891
------------------------------------------------------
Started: 2025-08-16 06:23:48.008222
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-16 06:23:48.103584
------------------------------------------------------
Started: 2025-08-16 08:20:59.675555
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-16 08:20:59.770054
------------------------------------------------------
Started: 2025-08-16 10:16:20.331263
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-16 10:16:20.389966
------------------------------------------------------
Started: 2025-08-16 12:32:23.103682
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-16 12:32:23.211728
------------------------------------------------------
Started: 2025-08-16 14:14:11.277542
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-16 14:14:11.347183
------------------------------------------------------
Started: 2025-08-16 16:18:57.836731
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-16 16:18:57.895835
------------------------------------------------------
Started: 2025-08-16 18:21:53.389909
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-16 18:21:53.458715
------------------------------------------------------
Started: 2025-08-16 20:17:32.571213
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-16 20:17:32.635633
------------------------------------------------------
Started: 2025-08-16 22:15:51.317258
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-16 22:15:51.373506
------------------------------------------------------
Started: 2025-08-17 01:27:53.672824
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-17 01:27:53.752170
------------------------------------------------------
Started: 2025-08-17 03:23:44.879669
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-17 03:23:44.958545
------------------------------------------------------
Started: 2025-08-17 04:30:09.299075
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-17 04:30:09.360868
------------------------------------------------------
Started: 2025-08-17 06:24:38.040295
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-17 06:24:38.127011
------------------------------------------------------
Started: 2025-08-17 08:21:29.157821
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-17 08:21:29.243595
------------------------------------------------------
Started: 2025-08-17 10:16:52.354150
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-17 10:16:52.427929
------------------------------------------------------
Started: 2025-08-17 12:33:03.946174
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-17 12:33:04.006702
------------------------------------------------------
Started: 2025-08-17 14:15:01.996730
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-17 14:15:02.081189
------------------------------------------------------
Started: 2025-08-17 16:19:23.833447
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-17 16:19:23.922072
------------------------------------------------------
Started: 2025-08-17 18:23:09.979565
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-17 18:23:10.037534
------------------------------------------------------
Started: 2025-08-17 20:18:28.861818
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-17 20:18:28.929137
------------------------------------------------------
Started: 2025-08-17 22:16:00.021694
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-17 22:16:00.098938
------------------------------------------------------
Started: 2025-08-18 01:26:58.833812
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-18 01:26:58.913219
------------------------------------------------------
Started: 2025-08-18 03:27:37.918780
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-18 03:27:37.979744
------------------------------------------------------
Started: 2025-08-18 04:39:36.265662
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1720
Summarized using GPT-3.5-turbo
Append: [A2HCoder: An LLM-Driven Coding Agent for Hierarchical Algorithm-to-HDL Translation](https://arxiv.org/abs/2508.10904)
Token length: 1299
Summarized using GPT-3.5-turbo
Append: [PersonaTwin: A Multi-Tier Prompt Conditioning Framework for Generating and Evaluating Personalized Digital Twins](https://arxiv.org/abs/2508.10906)
Token length: 829
Summarized using GPT-3.5-turbo
Append: [gpt-oss-120b & gpt-oss-20b Model Card](https://arxiv.org/abs/2508.10925)
Token length: 1039
Summarized using GPT-3.5-turbo
Append: [Modeling and Detecting Company Risks from News: A Case Study in Bloomberg News](https://arxiv.org/abs/2508.10927)
Token length: 1700
Summarized using GPT-3.5-turbo
Append: [Rule2Text: A Framework for Generating and Evaluating Natural Language Explanations of Knowledge Graph Rules](https://arxiv.org/abs/2508.10971)
Token length: 1264
Summarized using GPT-3.5-turbo
Append: [Improving Text Style Transfer using Masked Diffusion Language Models with Inference-time Scaling](https://arxiv.org/abs/2508.10995)
Token length: 1227
Summarized using GPT-3.5-turbo
Append: [SproutBench: A Benchmark for Safe and Ethical Large Language Models for Youth](https://arxiv.org/abs/2508.11009)
Token length: 1172
Summarized using GPT-3.5-turbo
Append: [Beyond the Rosetta Stone: Unification Forces in Generalization Dynamics](https://arxiv.org/abs/2508.11017)
Token length: 1716
Summarized using GPT-3.5-turbo
Append: [Hell or High Water: Evaluating Agentic Recovery from External Failures](https://arxiv.org/abs/2508.11027)
Token length: 1520
Summarized using GPT-3.5-turbo
Append: [BIPOLAR: Polarization-based granular framework for LLM bias evaluation](https://arxiv.org/abs/2508.11061)
Token length: 584
Summarized using GPT-3.5-turbo
Append: [Approaching the Source of Symbol Grounding with Confluent Reductions of Abstract Meaning Representation Directed Graphs](https://arxiv.org/abs/2508.11068)
Token length: 1220
Summarized using GPT-3.5-turbo
Append: [Towards Reliable Multi-Agent Systems for Marketing Applications via Reflection, Memory, and Planning](https://arxiv.org/abs/2508.11120)
Token length: 1083
Summarized using GPT-3.5-turbo
Append: [MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens of Documents](https://arxiv.org/abs/2508.11133)
Token length: 1335
Summarized using GPT-3.5-turbo
Append: [MobQA: A Benchmark Dataset for Semantic Understanding of Human Mobility Data through Question Answering](https://arxiv.org/abs/2508.11163)
Token length: 1135
Summarized using GPT-3.5-turbo
Append: [Overcoming Low-Resource Barriers in Tulu: Neural Models and Corpus Creation for OffensiveLanguage Identification](https://arxiv.org/abs/2508.11166)
Token length: 1794
Summarized using GPT-3.5-turbo
Append: [Personalized Distractor Generation via MCTS-Guided Reasoning Reconstruction](https://arxiv.org/abs/2508.11184)
Token length: 1015
Summarized using GPT-3.5-turbo
Append: [Novel Parasitic Dual-Scale Modeling for Efficient and Accurate Multilingual Speech Translation](https://arxiv.org/abs/2508.11189)
Token length: 1754
Summarized using GPT-3.5-turbo
Append: [E-CaTCH: Event-Centric Cross-Modal Attention with Temporal Consistency and Class-Imbalance Handling for Misinformation Detection](https://arxiv.org/abs/2508.11197)
Token length: 1603
Summarized using GPT-3.5-turbo
Append: [Cross-Granularity Hypergraph Retrieval-Augmented Generation for Multi-hop Question Answering](https://arxiv.org/abs/2508.11247)
Token length: 1024
Summarized using GPT-3.5-turbo
Append: [UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?](https://arxiv.org/abs/2508.11260)
Token length: 1341
Summarized using GPT-3.5-turbo
Append: [LETToT: Label-Free Evaluation of Large Language Models On Tourism Using Expert Tree-of-Thought](https://arxiv.org/abs/2508.11280)
Token length: 1374
Summarized using GPT-3.5-turbo
Append: [ToxiFrench: Benchmarking and Enhancing Language Models via CoT Fine-Tuning for French Toxicity Detection](https://arxiv.org/abs/2508.11281)
Token length: 1967
Summarized using GPT-3.5-turbo
Append: [AI in Mental Health: Emotional and Sentiment Analysis of Large Language Models' Responses to Depression, Anxiety, and Stress Queries](https://arxiv.org/abs/2508.11285)
Token length: 1301
Summarized using GPT-3.5-turbo
Append: [SafeConstellations: Steering LLM Safety to Reduce Over-Refusals Through Task-Specific Trajectory](https://arxiv.org/abs/2508.11290)
Token length: 1429
Summarized using GPT-3.5-turbo
Append: [SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced Benchmark for Automatic Survey Generation Systems](https://arxiv.org/abs/2508.11310)
Token length: 1209
Summarized using GPT-3.5-turbo
Append: [LLM Compression: How Far Can We Go in Balancing Size and Performance?](https://arxiv.org/abs/2508.11318)
Token length: 1533
Summarized using GPT-3.5-turbo
Append: [SpecDetect: Simple, Fast, and Training-Free Detection of LLM-Generated Text via Spectral Analysis](https://arxiv.org/abs/2508.11343)
Token length: 1333
Summarized using GPT-3.5-turbo
Append: [Feedback Indicators: The Alignment between Llama and a Teacher in Language Learning](https://arxiv.org/abs/2508.11364)
Token length: 1041
Summarized using GPT-3.5-turbo
Append: [When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs](https://arxiv.org/abs/2508.11383)
Token length: 1464
Summarized using GPT-3.5-turbo
Append: [Retrieval-augmented reasoning with lean language models](https://arxiv.org/abs/2508.11386)
Token length: 1325
Summarized using GPT-3.5-turbo
Append: [Model Interpretability and Rationale Extraction by Input Mask Optimization](https://arxiv.org/abs/2508.11388)
Token length: 1017
Summarized using GPT-3.5-turbo
Append: [Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training](https://arxiv.org/abs/2508.11393)
Token length: 1301
Summarized using GPT-3.5-turbo
Append: [Survey-to-Behavior: Downstream Alignment of Human Values in LLMs via Survey Questions](https://arxiv.org/abs/2508.11414)
Token length: 1257
Summarized using GPT-3.5-turbo
Append: [HumorPlanSearch: Structured Planning and HuCoT for Contextual AI Humor](https://arxiv.org/abs/2508.11429)
Token length: 1269
Summarized using GPT-3.5-turbo
Append: [Online Anti-sexist Speech: Identifying Resistance to Gender Bias in Political Discourse](https://arxiv.org/abs/2508.11434)
Token length: 1854
Summarized using GPT-3.5-turbo
Append: [CoDiEmb: A Collaborative yet Distinct Framework for Unified Representation Learning in Information Retrieval and Semantic Textual Similarity](https://arxiv.org/abs/2508.11442)
Token length: 1389
Summarized using GPT-3.5-turbo
Append: [Reference Points in LLM Sentiment Analysis: The Role of Structured Context](https://arxiv.org/abs/2508.11454)
Token length: 1929
Summarized using GPT-3.5-turbo
Append: [Speciesism in AI: Evaluating Discrimination Against Animals in Large Language Models](https://arxiv.org/abs/2508.11534)
Token length: 986
Summarized using GPT-3.5-turbo
Append: [Language models align with brain regions that represent concepts across modalities](https://arxiv.org/abs/2508.11536)
Token length: 1614
Summarized using GPT-3.5-turbo
Append: [AgentMental: An Interactive Multi-Agent Framework for Explainable and Adaptive Mental Health Assessment](https://arxiv.org/abs/2508.11567)
Token length: 1455
Summarized using GPT-3.5-turbo
Append: [Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme Reasoning Efficiency in Large Language Models](https://arxiv.org/abs/2508.11582)
Token length: 1046
Summarized using GPT-3.5-turbo
Append: [Representing Speech Through Autoregressive Prediction of Cochlear Tokens](https://arxiv.org/abs/2508.11598)
Token length: 1240
Summarized using GPT-3.5-turbo
Append: [Dataset Creation for Visual Entailment using Generative AI](https://arxiv.org/abs/2508.11605)
Token length: 646
Summarized using GPT-3.5-turbo
Append: [TinyTim: A Family of Language Models for Divergent Generation](https://arxiv.org/abs/2508.11607)
Token length: 1594
Summarized using GPT-3.5-turbo
Append: [The Next Phase of Scientific Fact-Checking: Advanced Evidence Retrieval from Complex Structured Academic Papers](https://arxiv.org/abs/2506.20844)
Token length: 1669
Summarized using GPT-3.5-turbo
Append: [Empowering Multimodal LLMs with External Tools: A Comprehensive Survey](https://arxiv.org/abs/2508.10955)
Token length: 1757
Summarized using GPT-3.5-turbo
Append: [BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining](https://arxiv.org/abs/2508.10975)
Token length: 1664
Summarized using GPT-3.5-turbo
Append: [Match & Choose: Model Selection Framework for Fine-tuning Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.10993)
Token length: 1493
Summarized using GPT-3.5-turbo
Append: [Can Multi-modal (reasoning) LLMs detect document manipulation?](https://arxiv.org/abs/2508.11021)
Token length: 1119
Summarized using GPT-3.5-turbo
Append: [Diffusion is a code repair operator and generator](https://arxiv.org/abs/2508.11110)
Append: [PaperRegister: Boosting Flexible-grained Paper Search via Hierarchical Register Indexing](https://arxiv.org/abs/2508.11116)
Append: [+VeriRel: Verification Feedback to Enhance Document Retrieval for Scientific Fact Checking](https://arxiv.org/abs/2508.11122)
Append: [A Cross-Modal Rumor Detection Scheme via Contrastive Learning by Exploring Text and Image internal Correlations](https://arxiv.org/abs/2508.11141)
Append: [Expressive Speech Retrieval using Natural Language Descriptions of Speaking Style](https://arxiv.org/abs/2508.11187)
Append: [How Causal Abstraction Underpins Computational Explanation](https://arxiv.org/abs/2508.11214)
Append: [ORFuzz: Fuzzing the "Other Side" of LLM Safety -- Testing Over-Refusal](https://arxiv.org/abs/2508.11222)
Append: [Benchmarking Prosody Encoding in Discrete Speech Tokens](https://arxiv.org/abs/2508.11224)
Append: [Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning Models to Ask for Information](https://arxiv.org/abs/2508.11252)
Append: [Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed LLMs via Post-Processing](https://arxiv.org/abs/2508.11258)
Append: [Generalize across Homophily and Heterophily: Hybrid Spectral Graph Pre-Training and Prompt Tuning](https://arxiv.org/abs/2508.11328)
Append: [Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps](https://arxiv.org/abs/2508.11452)
Append: [Emphasis Sensitivity in Speech Representations](https://arxiv.org/abs/2508.11566)
Append: [Controlling Multimodal LLMs via Reward-guided Decoding](https://arxiv.org/abs/2508.11616)
Append: [A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems](https://arxiv.org/abs/2402.18013)
Append: [Bridging Context Gaps: Leveraging Coreference Resolution for Long Contextual Understanding](https://arxiv.org/abs/2410.01671)
Append: [RULEBREAKERS: Challenging LLMs at the Crossroads between Formal Logic and Human-like Reasoning](https://arxiv.org/abs/2410.16502)
Append: [Personalized LLM for Generating Customized Responses to the Same Query from Different Users](https://arxiv.org/abs/2412.11736)
Append: [A Dual-Perspective NLG Meta-Evaluation Framework with Automatic Benchmark and Better Interpretability](https://arxiv.org/abs/2502.12052)
Append: [Visual-RAG: Benchmarking Text-to-Image Retrieval Augmented Generation for Visual Knowledge Intensive Queries](https://arxiv.org/abs/2502.16636)
Append: [Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs](https://arxiv.org/abs/2503.01307)
Append: [Feather-SQL: A Lightweight NL2SQL Framework with Dual-Model Collaboration Paradigm for Small Language Models](https://arxiv.org/abs/2503.17811)
Append: [Inaccuracy of an E-Dictionary and Its Influence on Chinese Language Users](https://arxiv.org/abs/2504.00799)
Append: [Investigating the Effect of Parallel Data in the Cross-Lingual Transfer for Vision-Language Encoders](https://arxiv.org/abs/2504.21681)
Append: [Relationship Detection on Tabular Data Using Statistical Analysis and Large Language Models](https://arxiv.org/abs/2506.06371)
Append: [PilotRL: Training Language Model Agents via Global Planning-Guided Progressive Reinforcement Learning](https://arxiv.org/abs/2508.00344)
Append: [Tool-Planner: Task Planning with Clusters across Multiple Tools](https://arxiv.org/abs/2406.03807)
Append: [TokenRec: Learning to Tokenize ID for LLM-based Generative Recommendation](https://arxiv.org/abs/2406.10450)
Append: [Uncertainty-Aware Adaptation of Large Language Models for Protein-Protein Interaction Analysis](https://arxiv.org/abs/2502.06173)
Append: [Causal Language in Observational Studies: Sociocultural Backgrounds and Team Composition](https://arxiv.org/abs/2502.12159)
Append: [EllieSQL: Cost-Efficient Text-to-SQL with Complexity-Aware Routing](https://arxiv.org/abs/2503.22402)
Append: [TokLIP: Marry Visual Tokens to CLIP for Multimodal Comprehension and Generation](https://arxiv.org/abs/2505.05422)
Append: [Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models](https://arxiv.org/abs/2506.07468)
Append: [Omni-DPO: A Dual-Perspective Paradigm for Dynamic Preference Learning of LLMs](https://arxiv.org/abs/2506.10054)
Append: [AlgoTune: Can Language Models Speed Up General-Purpose Numerical Programs?](https://arxiv.org/abs/2507.15887)
Append: [MMESGBench: Pioneering Multimodal Understanding and Complex Reasoning Benchmark for ESG Tasks](https://arxiv.org/abs/2507.18932)
append_entries: 85
Finish: 2025-08-18 04:41:14.158428
------------------------------------------------------
Started: 2025-08-18 06:29:06.512257
Existing_entries: 1085
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-18 06:29:06.861822
------------------------------------------------------
Started: 2025-08-18 08:25:39.755433
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-18 08:25:40.044070
------------------------------------------------------
Started: 2025-08-18 10:19:54.125401
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-18 10:19:54.359899
------------------------------------------------------
Started: 2025-08-18 12:37:23.374953
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-18 12:37:23.606999
------------------------------------------------------
Started: 2025-08-18 14:18:31.033505
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-18 14:18:31.290462
------------------------------------------------------
Started: 2025-08-18 16:22:21.151724
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-18 16:22:21.464734
------------------------------------------------------
Started: 2025-08-18 18:25:56.555384
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-18 18:25:56.789998
------------------------------------------------------
Started: 2025-08-18 20:17:44.269588
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-18 20:17:44.503725
------------------------------------------------------
Started: 2025-08-18 22:14:30.132501
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-18 22:14:30.445948
------------------------------------------------------
Started: 2025-08-19 01:18:46.818528
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-19 01:18:47.048634
------------------------------------------------------
Started: 2025-08-19 03:08:27.016233
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-19 03:08:27.251055
------------------------------------------------------
Started: 2025-08-19 04:23:33.524698
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 948
Summarized using GPT-3.5-turbo
Append: [Deep Language Geometry: Constructing a Metric Space from LLM Weights](https://arxiv.org/abs/2508.11676)
Token length: 926
Summarized using GPT-3.5-turbo
Append: [Can we Evaluate RAGs with Synthetic Data?](https://arxiv.org/abs/2508.11758)
Token length: 659
Summarized using GPT-3.5-turbo
Append: [Limitation Learning: Catching Adverse Dialog with GAIL](https://arxiv.org/abs/2508.11767)
Token length: 570
Summarized using GPT-3.5-turbo
Append: [Investigating Transcription Normalization in the Faetar ASR Benchmark](https://arxiv.org/abs/2508.11771)
Token length: 1820
Summarized using GPT-3.5-turbo
Append: [A Multi-Task Evaluation of LLMs' Processing of Academic Text Input](https://arxiv.org/abs/2508.11779)
Token length: 662
Summarized using GPT-3.5-turbo
Append: [LLM-Guided Planning and Summary-Based Scientific Text Simplification: DS@GT at CLEF 2025 SimpleText](https://arxiv.org/abs/2508.11816)
Token length: 743
Summarized using GPT-3.5-turbo
Append: [Hallucination Detection and Mitigation in Scientific Text Simplification using Ensemble Approaches: DS@GT at CLEF 2025 SimpleText](https://arxiv.org/abs/2508.11823)
Token length: 937
Summarized using GPT-3.5-turbo
Append: [A Survey of Idiom Datasets for Psycholinguistic and Computational Research](https://arxiv.org/abs/2508.11828)
Token length: 1283
Summarized using GPT-3.5-turbo
Append: [Every 28 Days the AI Dreams of Soft Skin and Burning Stars: Scaffolding AI Agents with Hormones and Emotions](https://arxiv.org/abs/2508.11829)
Token length: 1069
Summarized using GPT-3.5-turbo
Append: [When Does Language Transfer Help? Sequential Fine-Tuning for Cross-Lingual Euphemism Detection](https://arxiv.org/abs/2508.11831)
Token length: 1476
Summarized using GPT-3.5-turbo
Append: [SupraTok: Cross-Boundary Tokenization for Enhanced Language Model Performance](https://arxiv.org/abs/2508.11857)
Token length: 1550
Summarized using GPT-3.5-turbo
Append: [In-Context Examples Matter: Improving Emotion Recognition in Conversation with Instruction Tuning](https://arxiv.org/abs/2508.11889)
Token length: 1389
Summarized using GPT-3.5-turbo
Append: [CORE: Measuring Multi-Agent LLM Interaction Quality under Game-Theoretic Pressures](https://arxiv.org/abs/2508.11915)
Token length: 781
Summarized using GPT-3.5-turbo
Append: [LLMs Struggle with NLI for Perfect Aspect: A Cross-Linguistic Study in Chinese and Japanese](https://arxiv.org/abs/2508.11927)
Token length: 1227
Summarized using GPT-3.5-turbo
Append: [CAMF: Collaborative Adversarial Multi-agent Framework for Machine Generated Text Detection](https://arxiv.org/abs/2508.11933)
Token length: 1453
Summarized using GPT-3.5-turbo
Append: [Learning Wisdom from Errors: Promoting LLM's Continual Relation Learning through Exploiting Error Cases](https://arxiv.org/abs/2508.12031)
Token length: 1590
Summarized using GPT-3.5-turbo
Append: [Mind the Generation Process: Fine-Grained Confidence Estimation During LLM Generation](https://arxiv.org/abs/2508.12040)
Token length: 1282
Summarized using GPT-3.5-turbo
Append: [J6: Jacobian-Driven Role Attribution for Multi-Objective Prompt Optimization in LLMs](https://arxiv.org/abs/2508.12086)
Token length: 1435
Summarized using GPT-3.5-turbo
Append: [STEM: Efficient Relative Capability Evaluation of LLMs through Structured Transition Samples](https://arxiv.org/abs/2508.12096)
Token length: 1909
Summarized using GPT-3.5-turbo
Append: [Exploring Efficiency Frontiers of Thinking Budget in Medical Reasoning: Scaling Laws between Computational Resources and Reasoning Quality](https://arxiv.org/abs/2508.12140)
Token length: 1517
Summarized using GPT-3.5-turbo
Append: [LLM-as-a-Judge for Privacy Evaluation? Exploring the Alignment of Human and LLM Perceptions of Privacy in Textual Data](https://arxiv.org/abs/2508.12158)
Token length: 965
Summarized using GPT-3.5-turbo
Append: [Arabic Multimodal Machine Learning: Datasets, Applications, Approaches, and Challenges](https://arxiv.org/abs/2508.12227)
Token length: 1153
Summarized using GPT-3.5-turbo
Append: [SEA-BED: Southeast Asia Embedding Benchmark](https://arxiv.org/abs/2508.12243)
Token length: 1938
Summarized using GPT-3.5-turbo
Append: [What do Speech Foundation Models Learn? Analysis and Applications](https://arxiv.org/abs/2508.12255)
Token length: 786
Summarized using GPT-3.5-turbo
Append: [Structuring the Unstructured: A Systematic Review of Text-to-Structure Generation for Agentic AI with a Universal Evaluation Framework](https://arxiv.org/abs/2508.12257)
Token length: 962
Summarized using GPT-3.5-turbo
Append: [Fast, Slow, and Tool-augmented Thinking for LLMs: A Review](https://arxiv.org/abs/2508.12265)
Token length: 860
Summarized using GPT-3.5-turbo
Append: [The Self-Execution Benchmark: Measuring LLMs' Attempts to Overcome Their Lack of Self-Execution](https://arxiv.org/abs/2508.12277)
Token length: 1659
Summarized using GPT-3.5-turbo
Append: [Legal$\Delta$: Enhancing Legal Reasoning in LLMs via Reinforcement Learning with Chain-of-Thought Guided Information Gain](https://arxiv.org/abs/2508.12281)
Token length: 1048
Summarized using GPT-3.5-turbo
Append: [A Question Answering Dataset for Temporal-Sensitive Retrieval-Augmented Generation](https://arxiv.org/abs/2508.12282)
Token length: 1446
Summarized using GPT-3.5-turbo
Append: [Incorporating Legal Logic into Deep Learning: An Intelligent Approach to Probation Prediction](https://arxiv.org/abs/2508.12286)
Token length: 1516
Summarized using GPT-3.5-turbo
Append: [CarelessWhisper: Turning Whisper into a Causal Streaming Model](https://arxiv.org/abs/2508.12301)
Token length: 1202
Summarized using GPT-3.5-turbo
Append: [Consensus or Conflict? Fine-Grained Evaluation of Conflicting Answers in Question-Answering](https://arxiv.org/abs/2508.12355)
Token length: 1552
Summarized using GPT-3.5-turbo
Append: [ReaLM: Reflection-Enhanced Autonomous Reasoning with Small Language Models](https://arxiv.org/abs/2508.12387)
Token length: 1959
Summarized using GPT-3.5-turbo
Append: [MedKGent: A Large Language Model Agent Framework for Constructing Temporally Evolving Medical Knowledge Graph](https://arxiv.org/abs/2508.12393)
Token length: 1223
Summarized using GPT-3.5-turbo
Append: [Extracting Post-Acute Sequelae of SARS-CoV-2 Infection Symptoms from Clinical Notes via Hybrid Natural Language Processing](https://arxiv.org/abs/2508.12405)
Token length: 1425
Summarized using GPT-3.5-turbo
Append: [ZigzagAttention: Efficient Long-Context Inference with Exclusive Retrieval and Streaming Heads](https://arxiv.org/abs/2508.12407)
Token length: 1559
Summarized using GPT-3.5-turbo
Append: [The Cultural Gene of Large Language Models: A Study on the Impact of Cross-Corpus Training on Model Values and Biases](https://arxiv.org/abs/2508.12411)
Token length: 1697
Summarized using GPT-3.5-turbo
Append: [Uncovering Emergent Physics Representations Learned In-Context by Large Language Models](https://arxiv.org/abs/2508.12448)
Token length: 1751
Summarized using GPT-3.5-turbo
Append: [M3PO: Multimodal-Model-Guided Preference Optimization for Visual Instruction Following](https://arxiv.org/abs/2508.12458)
Token length: 1005
Summarized using GPT-3.5-turbo
Append: [LoraxBench: A Multitask, Multilingual Benchmark Suite for 20 Indonesian Languages](https://arxiv.org/abs/2508.12459)
Token length: 1365
Summarized using GPT-3.5-turbo
Append: [Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI's Latest Open Source Models](https://arxiv.org/abs/2508.12461)
Token length: 1135
Summarized using GPT-3.5-turbo
Append: [The Structural Sources of Verb Meaning Revisited: Large Language Models Display Syntactic Bootstrapping](https://arxiv.org/abs/2508.12482)
Token length: 1518
Summarized using GPT-3.5-turbo
Append: [Mitigating Hallucinations in Large Language Models via Causal Reasoning](https://arxiv.org/abs/2508.12495)
Token length: 1282
Summarized using GPT-3.5-turbo
Append: [CorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection](https://arxiv.org/abs/2508.12535)
Token length: 1259
Summarized using GPT-3.5-turbo
Append: [Beyond Modality Limitations: A Unified MLLM Approach to Automated Speaking Assessment with Effective Curriculum Learning](https://arxiv.org/abs/2508.12591)
Token length: 1129
Summarized using GPT-3.5-turbo
Append: [Semantic Anchoring in Agentic Memory: Leveraging Linguistic Structures for Persistent Conversational Context](https://arxiv.org/abs/2508.12630)
Token length: 1338
Summarized using GPT-3.5-turbo
Append: [Beyond GPT-5: Making LLMs Cheaper and Better via Performance-Efficiency Optimized Routing](https://arxiv.org/abs/2508.12631)
Token length: 1388
Summarized using GPT-3.5-turbo
Append: [Prompt-Induced Linguistic Fingerprints for LLM-Generated Fake News Detection](https://arxiv.org/abs/2508.12632)
Token length: 1056
Summarized using GPT-3.5-turbo
Append: [Breaking Language Barriers: Equitable Performance in Multilingual Language Models](https://arxiv.org/abs/2508.12662)
Token length: 1233
Summarized using GPT-3.5-turbo
Append: [Leveraging Large Language Models for Predictive Analysis of Human Misery](https://arxiv.org/abs/2508.12669)
Append: [ToolACE-MT: Non-Autoregressive Generation for Agentic Multi-Turn Interaction](https://arxiv.org/abs/2508.12685)
Append: [DESIGNER: Design-Logic-Guided Multidisciplinary Data Synthesis for LLM Reasoning](https://arxiv.org/abs/2508.12726)
Append: [LinguaSafe: A Comprehensive Multilingual Safety Benchmark for Large Language Models](https://arxiv.org/abs/2508.12733)
Append: [CRED-SQL: Enhancing Real-world Large Scale Database Text-to-SQL Parsing through Cluster Retrieval and Execution Description](https://arxiv.org/abs/2508.12769)
Append: [From SALAMANDRA to SALAMANDRATA: BSC Submission for WMT25 General Machine Translation Shared Task](https://arxiv.org/abs/2508.12774)
Append: [HeteroRAG: A Heterogeneous Retrieval-Augmented Generation Framework for Medical Vision Language Tasks](https://arxiv.org/abs/2508.12778)
Append: [Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic Thought Reward](https://arxiv.org/abs/2508.12800)
Append: [When Alignment Hurts: Decoupling Representational Spaces in Multilingual Models](https://arxiv.org/abs/2508.12803)
Append: [ding-01 :ARG0: An AMR Corpus for Spontaneous French Dialogue](https://arxiv.org/abs/2508.12819)
Append: [Context Matters: Incorporating Target Awareness in Conversational Abusive Language Detection](https://arxiv.org/abs/2508.12828)
Append: [It takes a village to write a book: Mapping anonymous contributions in Stephen Langton's Quaestiones Theologiae](https://arxiv.org/abs/2508.12830)
Append: [Word Meanings in Transformer Language Models](https://arxiv.org/abs/2508.12863)
Append: [An LLM Agent-Based Complex Semantic Table Annotation Approach](https://arxiv.org/abs/2508.12868)
Append: [A Stitch in Time Saves Nine: Proactive Self-Refinement for Language Models](https://arxiv.org/abs/2508.12903)
Append: [Analyzing Information Sharing and Coordination in Multi-Agent Planning](https://arxiv.org/abs/2508.12981)
Append: [WebMall -- A Multi-Shop Benchmark for Evaluating Web Agents](https://arxiv.org/abs/2508.13024)
Append: [Integrating Feedback Loss from Bi-modal Sarcasm Detector for Sarcastic Speech Synthesis](https://arxiv.org/abs/2508.13028)
Append: [Can Large Models Teach Student Models to Solve Mathematical Problems Like Human Beings? A Reasoning Distillation Method via Multi-LoRA Interaction](https://arxiv.org/abs/2508.13037)
Append: [B\"{u}y\"{u}k Dil Modelleri i\c{c}in TR-MMLU Benchmark{\i}: Performans De\u{g}erlendirmesi, Zorluklar ve \.{I}yile\c{s}tirme F{\i}rsatlar{\i}](https://arxiv.org/abs/2508.13044)
Append: [Do\u{g}al Dil \.I\c{s}lemede Tokenizasyon Standartlar{\i} ve \"Ol\c{c}\"um\"u: T\"urk\c{c}e \"Uzerinden B\"uy\"uk Dil Modellerinin Kar\c{s}{\i}la\c{s}t{\i}rmal{\i} Analizi](https://arxiv.org/abs/2508.13058)
Append: [Evaluating ASR robustness to spontaneous speech errors: A study of WhisperX using a Speech Error Database](https://arxiv.org/abs/2508.13060)
Append: [Reinforced Context Order Recovery for Adaptive Reasoning and Planning](https://arxiv.org/abs/2508.13070)
Append: [DocHPLT: A Massively Multilingual Document-Level Translation Dataset](https://arxiv.org/abs/2508.13079)
Append: [All for law and law for all: Adaptive RAG Pipeline for Legal Research](https://arxiv.org/abs/2508.13107)
Append: [AutoBnB-RAG: Enhancing Multi-Agent Incident Response with Retrieval-Augmented Generation](https://arxiv.org/abs/2508.13118)
Append: [Spot the BlindSpots: Systematic Identification and Quantification of Fine-Grained LLM Biases in Contact Center Summaries](https://arxiv.org/abs/2508.13124)
Append: [MuDRiC: Multi-Dialect Reasoning for Arabic Commonsense Validation](https://arxiv.org/abs/2508.13130)
Append: [Improving Detection of Watermarked Language Models](https://arxiv.org/abs/2508.13131)
Append: [OptimalThinkingBench: Evaluating Over and Underthinking in LLMs](https://arxiv.org/abs/2508.13141)
Append: [Signal and Noise: A Framework for Reducing Uncertainty in Language Model Evaluation](https://arxiv.org/abs/2508.13144)
Append: [RepreGuard: Detecting LLM-Generated Text by Revealing Hidden Representation Patterns](https://arxiv.org/abs/2508.13152)
Append: [Sparse Attention across Multiple-context KV Cache](https://arxiv.org/abs/2508.11661)
Append: [Assessing Representation Stability for Transformer Models](https://arxiv.org/abs/2508.11667)
Append: [Code Vulnerability Detection Across Different Programming Languages with AI Models](https://arxiv.org/abs/2508.11710)
Append: [Ovis2.5 Technical Report](https://arxiv.org/abs/2508.11737)
Append: [Using Natural Language for Human-Robot Collaboration in the Real World](https://arxiv.org/abs/2508.11759)
Append: [VideoAVE: A Multi-Attribute Video-to-Text Attribute Value Extraction Dataset and Benchmark Models](https://arxiv.org/abs/2508.11801)
Append: [Labels or Input? Rethinking Augmentation in Multimodal Hate Detection](https://arxiv.org/abs/2508.11808)
Append: [LARC: Towards Human-level Constrained Retrosynthesis Planning through an Agentic Framework](https://arxiv.org/abs/2508.11860)
Append: [EVTP-IVS: Effective Visual Token Pruning For Unifying Instruction Visual Segmentation In Multi-Modal Large Language Models](https://arxiv.org/abs/2508.11886)
Append: [Optimizing Token Choice for Code Watermarking: A RL Approach](https://arxiv.org/abs/2508.11925)
Append: [CHBench: A Cognitive Hierarchy Benchmark for Evaluating Strategic Reasoning Capability of LLMs](https://arxiv.org/abs/2508.11944)
Append: [Mitigating Jailbreaks with Intent-Aware LLMs](https://arxiv.org/abs/2508.12072)
Append: [VimoRAG: Video-based Retrieval-augmented 3D Motion Generation for Motion Language Models](https://arxiv.org/abs/2508.12081)
Append: [Generative Medical Event Models Improve with Scale](https://arxiv.org/abs/2508.12104)
Append: [DynamixSFT: Dynamic Mixture Optimization of Instruction Tuning Collections](https://arxiv.org/abs/2508.12116)
Append: [Where to Start Alignment? Diffusion Large Language Model May Demand a Distinct Position](https://arxiv.org/abs/2508.12398)
Append: [Non-Iterative Symbolic-Aided Chain-of-Thought for Logical Reasoning](https://arxiv.org/abs/2508.12425)
Append: [Adversarial Attacks on VQA-NLE: Exposing and Alleviating Inconsistencies in Visual Question Answering Explanations](https://arxiv.org/abs/2508.12430)
Append: [Insight Rumors: A Novel Textual Rumor Locating and Marking Model Leveraging Att_BiMamba2 Network](https://arxiv.org/abs/2508.12574)
Append: [An LLM + ASP Workflow for Joint Entity-Relation Extraction](https://arxiv.org/abs/2508.12611)
Append: [Vision-G1: Towards General Vision Language Reasoning with Multi-Domain Data Curation](https://arxiv.org/abs/2508.12680)
Append: [Reinforcement Learning with Rubric Anchors](https://arxiv.org/abs/2508.12790)
Append: [Bridging Human and LLM Judgments: Understanding and Narrowing the Gap](https://arxiv.org/abs/2508.12792)
Append: [Maximum Score Routing For Mixture-of-Experts](https://arxiv.org/abs/2508.12801)
Append: [Learning to Steer: Input-dependent Steering for Multimodal LLMs](https://arxiv.org/abs/2508.12815)
Append: [E3RG: Building Explicit Emotion-driven Empathetic Response Generation System with Multimodal Large Language Model](https://arxiv.org/abs/2508.12854)
Append: [TCUQ: Single-Pass Uncertainty Quantification from Temporal Consistency with Streaming Conformal Calibration for TinyML](https://arxiv.org/abs/2508.12905)
Append: [SNAP-UQ: Self-supervised Next-Activation Prediction for Single-Pass Uncertainty in TinyML](https://arxiv.org/abs/2508.12907)
Append: [PC-Sampler: Position-Aware Calibration of Decoding Bias in Masked Diffusion Models](https://arxiv.org/abs/2508.13021)
Append: [Has GPT-5 Achieved Spatial Intelligence? An Empirical Study](https://arxiv.org/abs/2508.13142)
Append: [Is Smaller Always Faster? Tradeoffs in Compressing Self-Supervised Speech Transformers](https://arxiv.org/abs/2211.09949)
Append: [Large language models can replicate cross-cultural differences in personality](https://arxiv.org/abs/2310.10679)
Append: [MHPP: Exploring the Capabilities and Limitations of Language Models Beyond Basic Code Generation](https://arxiv.org/abs/2405.11430)
Append: [FacLens: Transferable Probe for Foreseeing Non-Factuality in Fact-Seeking Question Answering of Large Language Models](https://arxiv.org/abs/2406.05328)
Append: [S2Cap: A Benchmark and a Baseline for Singing Style Captioning](https://arxiv.org/abs/2409.09866)
Append: [Towards No-Code Programming of Cobots: Experiments with Code Synthesis by Large Code Models for Conversational Programming](https://arxiv.org/abs/2409.11041)
Append: [LLMs Are In-Context Bandit Reinforcement Learners](https://arxiv.org/abs/2410.05362)
Append: [StepTool: Enhancing Multi-Step Tool Usage in LLMs via Step-Grained Reinforcement Learning](https://arxiv.org/abs/2410.07745)
Append: [Emoji Attack: Enhancing Jailbreak Attacks Against Judge LLM Detection](https://arxiv.org/abs/2411.01077)
Append: [Regress, Don't Guess -- A Regression-like Loss on Number Tokens for Language Models](https://arxiv.org/abs/2411.02083)
Append: [NormXLogit: The Head-on-Top Never Lies](https://arxiv.org/abs/2411.16252)
Append: [On Fusing ChatGPT and Ensemble Learning in Discon-tinuous Named Entity Recognition in Health Corpora](https://arxiv.org/abs/2412.16976)
Append: [Idiom Detection in Sorani Kurdish Texts](https://arxiv.org/abs/2501.14528)
Append: [2SSP: A Two-Stage Framework for Structured Pruning of LLMs](https://arxiv.org/abs/2501.17771)
Append: [VisualSpeech: Enhancing Prosody Modeling in TTS Using Video](https://arxiv.org/abs/2501.19258)
Append: [Dealing with Annotator Disagreement in Hate Speech Classification](https://arxiv.org/abs/2502.08266)
Append: [LIDDIA: Language-based Intelligent Drug Discovery Agent](https://arxiv.org/abs/2502.13959)
Append: [Full-Duplex-Bench: A Benchmark to Evaluate Full-duplex Spoken Dialogue Models on Turn-taking Capabilities](https://arxiv.org/abs/2503.04721)
Append: [An Information-Theoretic Approach to Identifying Formulaic Clusters in Textual Data](https://arxiv.org/abs/2503.07303)
Append: [High-Dimensional Interlingual Representations of Large Language Models](https://arxiv.org/abs/2503.11280)
Append: [More Women, Same Stereotypes: Unpacking the Gender Bias Paradox in Large Language Models](https://arxiv.org/abs/2503.15904)
Append: [FastCuRL: Curriculum Reinforcement Learning with Stage-wise Context Scaling for Efficient Training R1-like Reasoning Models](https://arxiv.org/abs/2503.17287)
Append: [SCORE: Story Coherence and Retrieval Enhancement for AI Narratives](https://arxiv.org/abs/2503.23512)
Append: [TeleAntiFraud-28k: An Audio-Text Slow-Thinking Dataset for Telecom Fraud Detection](https://arxiv.org/abs/2503.24115)
Append: [SpectR: Dynamically Composing LM Experts with Spectral Routing](https://arxiv.org/abs/2504.03454)
Append: [Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning Models](https://arxiv.org/abs/2504.04823)
Append: [Fast Controlled Generation from Language Models with Adaptive Weighted Rejection Sampling](https://arxiv.org/abs/2504.05410)
Append: [EvalAgent: Discovering Implicit Evaluation Criteria from the Web](https://arxiv.org/abs/2504.15219)
Append: [Deliberate Planning in Language Models with Symbolic Representation](https://arxiv.org/abs/2505.01479)
Append: [Convert Language Model into a Value-based Strategic Planner](https://arxiv.org/abs/2505.06987)
Append: [From Templates to Natural Language: Generalization Challenges in Instruction-Tuned LLMs for Spatial Reasoning](https://arxiv.org/abs/2505.14425)
Append: [Token-level Accept or Reject: A Micro Alignment Approach for Large Language Models](https://arxiv.org/abs/2505.19743)
Append: [Concealment of Intent: A Game-Theoretic Analysis](https://arxiv.org/abs/2505.20841)
Append: [Explaining Large Language Models with gSMILE](https://arxiv.org/abs/2505.21657)
Append: [Translation in the Wild](https://arxiv.org/abs/2505.23548)
Append: [OrthoRank: Token Selection via Sink Token Orthogonality for Efficient LLM inference](https://arxiv.org/abs/2507.03865)
Append: [LoRA-Augmented Generation (LAG) for Knowledge-Intensive Language Tasks](https://arxiv.org/abs/2507.05346)
Append: [PromptSuite: A Task-Agnostic Framework for Multi-Prompt Generation](https://arxiv.org/abs/2507.14913)
Append: [Beyond Fixed: Training-Free Variable-Length Denoising for Diffusion Large Language Models](https://arxiv.org/abs/2508.00819)
Append: [Exploring Scholarly Data by Semantic Query on Knowledge Graph Embedding Space](https://arxiv.org/abs/1909.08191)
Append: [Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation](https://arxiv.org/abs/2403.19103)
Append: [LGR2: Language Guided Reward Relabeling for Accelerating Hierarchical Reinforcement Learning](https://arxiv.org/abs/2406.05881)
Append: [Large Language Models Must Be Taught to Know What They Don't Know](https://arxiv.org/abs/2406.08391)
Append: [A Law of Next-Token Prediction in Large Language Models](https://arxiv.org/abs/2408.13442)
Append: [Advancing AI-Scientist Understanding: Multi-Agent LLMs with Interpretable Physics Reasoning](https://arxiv.org/abs/2504.01911)
Append: [Learning Adaptive Parallel Reasoning with Language Models](https://arxiv.org/abs/2504.15466)
Append: [CoRank: LLM-Based Compact Reranking with Document Features for Scientific Retrieval](https://arxiv.org/abs/2505.13757)
Append: [Hard Negative Contrastive Learning for Fine-Grained Geometric Understanding in Large Multimodal Models](https://arxiv.org/abs/2505.20152)
Append: [Flexible Tool Selection through Low-dimensional Attribute Alignment of Vision and Language](https://arxiv.org/abs/2505.22146)
Append: [Generalizable LLM Learning of Graph Synthetic Data with Post-training Alignment](https://arxiv.org/abs/2506.00845)
Append: [LocalGPT: Benchmarking and Advancing Large Language Models for Local Life Services in Meituan](https://arxiv.org/abs/2506.02720)
Append: [Dissecting the SWE-Bench Leaderboards: Profiling Submitters and Architectures of LLM- and Agent-Based Repair Systems](https://arxiv.org/abs/2506.17208)
Append: [USAD: Universal Speech and Audio Representation via Distillation](https://arxiv.org/abs/2506.18843)
Append: [Overcoming Long-Context Limitations of State-Space Models via Context-Dependent Sparse Attention](https://arxiv.org/abs/2507.00449)
Append: [Nonlinear Concept Erasure: a Density Matching Approach](https://arxiv.org/abs/2507.12341)
Append: [Towards Multimodal Social Conversations with Robots: Using Vision-Language Models](https://arxiv.org/abs/2507.19196)
append_entries: 167
Finish: 2025-08-19 04:25:14.452424
------------------------------------------------------
Started: 2025-08-19 06:25:54.073711
Existing_entries: 1167
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1199
Summarized using GPT-3.5-turbo
Append: [GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy Entropy](https://arxiv.org/abs/2508.04349)
append_entries: 1
Finish: 2025-08-19 06:25:55.930493
------------------------------------------------------
Started: 2025-08-19 08:22:11.537463
Existing_entries: 1001
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-19 08:22:11.999234
------------------------------------------------------
Started: 2025-08-19 10:18:10.788027
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-19 10:18:11.195131
------------------------------------------------------
Started: 2025-08-19 12:34:54.154847
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-19 12:34:54.564316
------------------------------------------------------
Started: 2025-08-19 14:16:41.102571
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-19 14:16:41.524297
------------------------------------------------------
Started: 2025-08-19 16:21:21.906227
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-19 16:21:22.295637
------------------------------------------------------
Started: 2025-08-19 18:22:25.632021
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-19 18:22:26.031904
------------------------------------------------------
Started: 2025-08-19 20:18:20.456109
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-19 20:18:20.878405
------------------------------------------------------
Started: 2025-08-19 22:15:50.616856
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-19 22:15:51.057827
------------------------------------------------------
Started: 2025-08-20 01:16:52.609401
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-20 01:16:53.040819
------------------------------------------------------
Started: 2025-08-20 03:06:12.426114
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-20 03:06:12.895824
------------------------------------------------------
Started: 2025-08-20 04:23:44.672339
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1161
Summarized using GPT-3.5-turbo
Append: [Fair Play in the Newsroom: Actor-Based Filtering Gender Discrimination in Text Corpora](https://arxiv.org/abs/2508.13169)
Token length: 1247
Summarized using GPT-3.5-turbo
Append: [MM-BrowseComp: A Comprehensive Benchmark for Multimodal Browsing Agents](https://arxiv.org/abs/2508.13186)
Token length: 1170
Summarized using GPT-3.5-turbo
Append: [Overcoming Latency Bottlenecks in On-Device Speech Translation: A Cascaded Approach with Alignment-Based Streaming MT](https://arxiv.org/abs/2508.13358)
Token length: 1423
Summarized using GPT-3.5-turbo
Append: [Stands to Reason: Investigating the Effect of Reasoning on Idiomaticity Detection](https://arxiv.org/abs/2508.13365)
Token length: 1022
Summarized using GPT-3.5-turbo
Append: [Whispering Context: Distilling Syntax and Semantics for Long Speech Transcripts](https://arxiv.org/abs/2508.13376)
Token length: 1941
Summarized using GPT-3.5-turbo
Append: [Datarus-R1: An Adaptive Multi-Step Reasoning LLM for Automated Data Analysis](https://arxiv.org/abs/2508.13382)
Token length: 1542
Summarized using GPT-3.5-turbo
Append: [ALIGN: Word Association Learning for Cross-Cultural Generalization in Large Language Models](https://arxiv.org/abs/2508.13426)
Token length: 1679
Summarized using GPT-3.5-turbo
Append: [ProMed: Shapley Information Gain Guided Reinforcement Learning for Proactive Medical LLMs](https://arxiv.org/abs/2508.13514)
Token length: 1575
Summarized using GPT-3.5-turbo
Append: [Saudi-Dialect-ALLaM: LoRA Fine-Tuning for Dialectal Arabic Generation](https://arxiv.org/abs/2508.13525)
Token length: 970
Summarized using GPT-3.5-turbo
Append: [MATA (m\=ata): Mindful Assessment of the Telugu Abilities of Large Language Models](https://arxiv.org/abs/2508.13526)
Token length: 1491
Summarized using GPT-3.5-turbo
Append: [Compressed Models are NOT Trust-equivalent to Their Large Counterparts](https://arxiv.org/abs/2508.13533)
Token length: 1448
Summarized using GPT-3.5-turbo
Append: [A Comparative Study of Decoding Strategies in Medical Text Generation](https://arxiv.org/abs/2508.13580)
Token length: 1044
Summarized using GPT-3.5-turbo
Append: [Who Gets the Mic? Investigating Gender Bias in the Speaker Assignment of a Speech-LLM](https://arxiv.org/abs/2508.13603)
Token length: 1246
Summarized using GPT-3.5-turbo
Append: [AdaDocVQA: Adaptive Framework for Long Document Visual Question Answering in Low-Resource Settings](https://arxiv.org/abs/2508.13606)
Token length: 1178
Summarized using GPT-3.5-turbo
Append: [CRISP: Persistent Concept Unlearning via Sparse Autoencoders](https://arxiv.org/abs/2508.13650)
Token length: 1409
Summarized using GPT-3.5-turbo
Append: [ViExam: Are Vision Language Models Better than Humans on Vietnamese Multimodal Exam Questions?](https://arxiv.org/abs/2508.13680)
Token length: 1155
Summarized using GPT-3.5-turbo
Append: [Generics and Default Reasoning in Large Language Models](https://arxiv.org/abs/2508.13718)
Token length: 1343
Summarized using GPT-3.5-turbo
Append: [Prediction is not Explanation: Revisiting the Explanatory Capacity of Mapping Embeddings](https://arxiv.org/abs/2508.13729)
Token length: 1165
Summarized using GPT-3.5-turbo
Append: [EEG-MedRAG: Enhancing EEG-based Clinical Decision-Making via Hierarchical Hypergraph Retrieval-Augmented Generation](https://arxiv.org/abs/2508.13735)
Token length: 1853
Summarized using GPT-3.5-turbo
Append: [Sycophancy under Pressure: Evaluating and Mitigating Sycophantic Bias via Adversarial Dialogues in Scientific QA](https://arxiv.org/abs/2508.13743)
Token length: 1334
Summarized using GPT-3.5-turbo
Append: [MGT-Prism: Enhancing Domain Generalization for Machine-Generated Text Detection via Spectral Alignment](https://arxiv.org/abs/2508.13768)
Token length: 1510
Summarized using GPT-3.5-turbo
Append: [Can Large Language Models (LLMs) Describe Pictures Like Children? A Comparative Corpus Study](https://arxiv.org/abs/2508.13769)
Token length: 1417
Summarized using GPT-3.5-turbo
Append: [TracSum: A New Benchmark for Aspect-Based Summarization with Sentence-Level Traceability in Medical Domain](https://arxiv.org/abs/2508.13798)
Token length: 996
Summarized using GPT-3.5-turbo
Append: [Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values Understanding](https://arxiv.org/abs/2508.13804)
Token length: 1158
Summarized using GPT-3.5-turbo
Append: [Prompt-Based One-Shot Exact Length-Controlled Generation with LLMs](https://arxiv.org/abs/2508.13805)
Token length: 1725
Summarized using GPT-3.5-turbo
Append: [The illusion of a perfect metric: Why evaluating AI's words is harder than it looks](https://arxiv.org/abs/2508.13816)
Token length: 1269
Summarized using GPT-3.5-turbo
Append: [Extracting Structured Requirements from Unstructured Building Technical Specifications for Building Information Modeling](https://arxiv.org/abs/2508.13833)
Token length: 1731
Summarized using GPT-3.5-turbo
Append: [MME-SCI: A Comprehensive and Challenging Science Benchmark for Multimodal Large Language Models](https://arxiv.org/abs/2508.13938)
Token length: 1589
Summarized using GPT-3.5-turbo
Append: [ReviewGraph: A Knowledge Graph Embedding Based Framework for Review Rating Prediction with Sentiment Features](https://arxiv.org/abs/2508.13953)
Token length: 1593
Summarized using GPT-3.5-turbo
Append: [Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization](https://arxiv.org/abs/2508.13993)
Token length: 864
Summarized using GPT-3.5-turbo
Append: [Ask Good Questions for Large Language Models](https://arxiv.org/abs/2508.14025)
Token length: 1447
Summarized using GPT-3.5-turbo
Append: [Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR](https://arxiv.org/abs/2508.14029)
Token length: 1602
Summarized using GPT-3.5-turbo
Append: [Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation](https://arxiv.org/abs/2508.14031)
Token length: 1859
Summarized using GPT-3.5-turbo
Append: [The Promise of Large Language Models in Digital Health: Evidence from Sentiment Analysis in Online Health Communities](https://arxiv.org/abs/2508.14032)
Token length: 1237
Summarized using GPT-3.5-turbo
Append: [TaoSR1: The Thinking Model for E-commerce Relevance Search](https://arxiv.org/abs/2508.12365)
Token length: 1776
Summarized using GPT-3.5-turbo
Append: [Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation and Agentic RL](https://arxiv.org/abs/2508.13167)
Token length: 1895
Summarized using GPT-3.5-turbo
Append: [Cognitive Workspace: Active Memory Management for LLMs -- An Empirical Study of Functional Infinite Context](https://arxiv.org/abs/2508.13171)
Token length: 1139
Summarized using GPT-3.5-turbo
Append: [White-Box Reasoning: Synergizing LLM Strategy and gm/Id Data for Automated Analog Circuit Design](https://arxiv.org/abs/2508.13172)
Token length: 1106
Summarized using GPT-3.5-turbo
Append: [The Interpretability Analysis of the Model Can Bring Improvements to the Text-to-SQL Task](https://arxiv.org/abs/2508.13178)
Token length: 1873
Summarized using GPT-3.5-turbo
Append: [Combating Homelessness Stigma with LLMs: A New Multi-Modal Dataset for Bias Detection](https://arxiv.org/abs/2508.13187)
Token length: 1358
Summarized using GPT-3.5-turbo
Append: [Explicit v.s. Implicit Memory: Exploring Multi-hop Complex Reasoning Over Personalized Information](https://arxiv.org/abs/2508.13250)
Token length: 1300
Summarized using GPT-3.5-turbo
Append: [X-MoE: Enabling Scalable Training for Emerging Mixture-of-Experts Architectures on HPC Platforms](https://arxiv.org/abs/2508.13337)
Token length: 1804
Summarized using GPT-3.5-turbo
Append: [TASER: Table Agents for Schema-guided Extraction and Recommendation](https://arxiv.org/abs/2508.13404)
Token length: 1655
Summarized using GPT-3.5-turbo
Append: [Structured Prompting and Multi-Agent Knowledge Distillation for Traffic Video Interpretation and Risk Inference](https://arxiv.org/abs/2508.13439)
Token length: 1262
Summarized using GPT-3.5-turbo
Append: [LLM-Enhanced Linear Autoencoders for Recommendation](https://arxiv.org/abs/2508.13500)
Token length: 1960
Summarized using GPT-3.5-turbo
Append: [Input Time Scaling](https://arxiv.org/abs/2508.13654)
Token length: 1461
Summarized using GPT-3.5-turbo
Append: [Improved Generalized Planning with LLMs through Strategy Refinement and Reflection](https://arxiv.org/abs/2508.13876)
Token length: 1100
Summarized using GPT-3.5-turbo
Append: [Prompt Orchestration Markup Language](https://arxiv.org/abs/2508.13948)
Token length: 1685
Summarized using GPT-3.5-turbo
Append: [Query Logs Analytics: A Aystematic Literature Review](https://arxiv.org/abs/2508.13949)
Token length: 1709
Summarized using GPT-3.5-turbo
Append: [RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation](https://arxiv.org/abs/2508.13968)
Append: [iTBLS: A Dataset of Interactive Conversations Over Tabular Information](https://arxiv.org/abs/2404.12580)
Append: [BQA: Body Language Question Answering Dataset for Video Large Language Models](https://arxiv.org/abs/2410.13206)
Append: [Development of Pre-Trained Transformer-based Models for the Nepali Language](https://arxiv.org/abs/2411.15734)
Append: [Consolidating and Developing Benchmarking Datasets for the Nepali Natural Language Understanding Tasks](https://arxiv.org/abs/2411.19244)
Append: [Understanding the Impact of Confidence in Retrieval Augmented Generation: A Case Study in the Medical Domain](https://arxiv.org/abs/2412.20309)
Append: [Universal Abstraction: Harnessing Frontier Models to Structure Real-World Data at Scale](https://arxiv.org/abs/2502.00943)
Append: [Fact or Guesswork? Evaluating Large Language Models' Medical Knowledge with Structured One-Hop Judgments](https://arxiv.org/abs/2502.14275)
Append: [Basic Category Usage in Vision Language Models](https://arxiv.org/abs/2503.12530)
Append: [SEA-LION: Southeast Asian Languages in One Network](https://arxiv.org/abs/2504.05747)
Append: [Hallucinations and Key Information Extraction in Medical Texts: A Comprehensive Assessment of Open-Source Large Language Models](https://arxiv.org/abs/2504.19061)
Append: [Harnessing Structured Knowledge: A Concept Map-Based Approach for High-Quality Multiple Choice Question Generation with Effective Distractors](https://arxiv.org/abs/2505.02850)
Append: [Crossing Borders Without Crossing Boundaries: How Sociolinguistic Awareness Can Optimize User Engagement with Localized Spanish AI Models Across Hispanophone Countries](https://arxiv.org/abs/2505.09902)
Append: [Model Performance-Guided Evaluation Data Selection for Effective Prompt Optimization](https://arxiv.org/abs/2505.10736)
Append: ["Haet Bhasha aur Diskrimineshun": Phonetic Perturbations in Code-Mixed Hinglish to Red-Team LLMs](https://arxiv.org/abs/2505.14226)
Append: [Soft Reasoning: Navigating Solution Spaces in Large Language Models through Controlled Embedding Exploration](https://arxiv.org/abs/2505.24688)
Append: [PlantDeBERTa: An Open Source Language Model for Plant Science](https://arxiv.org/abs/2506.08897)
Append: [Iterative Utility Judgment Framework via LLMs Inspired by Relevance in Philosophy](https://arxiv.org/abs/2406.11290)
Append: [MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation](https://arxiv.org/abs/2410.02458)
Append: [LEGO-GraphRAG: Modularizing Graph-based Retrieval-Augmented Generation for Design Space Exploration](https://arxiv.org/abs/2411.05844)
Append: [GoAI: Enhancing AI Students' Learning Paths and Idea Generation via Graph of AI Ideas](https://arxiv.org/abs/2503.08549)
Append: [Unleashing the Power of LLMs in Dense Retrieval with Query Likelihood Modeling](https://arxiv.org/abs/2504.05216)
Append: [Quiet Feature Learning in Algorithmic Tasks](https://arxiv.org/abs/2505.03997)
append_entries: 72
Finish: 2025-08-20 04:25:24.834635
------------------------------------------------------
Started: 2025-08-20 06:25:50.283092
Existing_entries: 1072
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1228
Summarized using GPT-3.5-turbo
Append: [A Study of the Framework and Real-World Applications of Language Embedding for 3D Scene Understanding](https://arxiv.org/abs/2508.05064)
append_entries: 1
Finish: 2025-08-20 06:25:52.808360
------------------------------------------------------
Started: 2025-08-20 08:22:06.597814
Existing_entries: 1001
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-20 08:22:06.814245
------------------------------------------------------
Started: 2025-08-20 10:17:47.779998
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-20 10:17:48.000939
------------------------------------------------------
Started: 2025-08-20 12:34:45.990582
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-20 12:34:46.199134
------------------------------------------------------
Started: 2025-08-20 14:17:26.574825
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-20 14:17:26.965475
------------------------------------------------------
Started: 2025-08-20 16:21:02.103025
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-20 16:21:02.330650
------------------------------------------------------
Started: 2025-08-20 18:24:22.365983
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-20 18:24:22.632363
------------------------------------------------------
Started: 2025-08-20 20:18:32.231330
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-20 20:18:32.458477
------------------------------------------------------
Started: 2025-08-20 22:14:25.963099
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-20 22:14:26.168935
------------------------------------------------------
Started: 2025-08-21 01:16:00.059626
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-21 01:16:00.294247
------------------------------------------------------
Started: 2025-08-21 03:05:15.562575
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-21 03:05:15.813860
------------------------------------------------------
Started: 2025-08-21 04:23:13.710666
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1259
Summarized using GPT-3.5-turbo
Append: [From Image Captioning to Visual Storytelling](https://arxiv.org/abs/2508.14045)
Token length: 656
Summarized using GPT-3.5-turbo
Append: [Benchmarking Sociolinguistic Diversity in Swahili NLP: A Taxonomy-Guided Approach](https://arxiv.org/abs/2508.14051)
Token length: 1221
Summarized using GPT-3.5-turbo
Append: [Contrastive Analysis of Constituent Order Preferences Within Adverbial Roles in English and Chinese News: A Large-Language-Model-Driven Approach](https://arxiv.org/abs/2508.14054)
Token length: 746
Summarized using GPT-3.5-turbo
Append: [T-REX: Table -- Refute or Entail eXplainer](https://arxiv.org/abs/2508.14055)
Token length: 824
Summarized using GPT-3.5-turbo
Append: [Confidence Estimation for Text-to-SQL in Large Language Models](https://arxiv.org/abs/2508.14056)
Token length: 1087
Summarized using GPT-3.5-turbo
Append: [Assessing and Mitigating Data Memorization Risks in Fine-Tuned Large Language Models](https://arxiv.org/abs/2508.14062)
Token length: 1453
Summarized using GPT-3.5-turbo
Append: [Punctuation and Predicates in Language Models](https://arxiv.org/abs/2508.14067)
Token length: 1934
Summarized using GPT-3.5-turbo
Append: [DLLMQuant: Quantizing Diffusion-based Large Language Models](https://arxiv.org/abs/2508.14090)
Token length: 1491
Summarized using GPT-3.5-turbo
Append: [MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based Peer Review Automation](https://arxiv.org/abs/2508.14146)
Token length: 1186
Summarized using GPT-3.5-turbo
Append: [DPad: Efficient Diffusion Language Models with Suffix Dropout](https://arxiv.org/abs/2508.14148)
Token length: 1374
Summarized using GPT-3.5-turbo
Append: [Comparing energy consumption and accuracy in text classification inference](https://arxiv.org/abs/2508.14170)
Token length: 1311
Summarized using GPT-3.5-turbo
Append: [Let's Use ChatGPT To Write Our Paper! Benchmarking LLMs To Write the Introduction of a Research Paper](https://arxiv.org/abs/2508.14273)
Token length: 1236
Summarized using GPT-3.5-turbo
Append: [Disentangling concept semantics via multilingual averaging in Sparse Autoencoders](https://arxiv.org/abs/2508.14275)
Token length: 1222
Summarized using GPT-3.5-turbo
Append: [GRILE: A Benchmark for Grammar Reasoning and Explanation in Romanian LLMs](https://arxiv.org/abs/2508.14279)
Token length: 1560
Summarized using GPT-3.5-turbo
Append: [Tokens with Meaning: A Hybrid Tokenization Approach for NLP](https://arxiv.org/abs/2508.14292)
Token length: 911
Summarized using GPT-3.5-turbo
Append: [A Joint Multitask Model for Morpho-Syntactic Parsing](https://arxiv.org/abs/2508.14307)
Token length: 1326
Summarized using GPT-3.5-turbo
Append: [Zero-knowledge LLM hallucination detection and mitigation through fine-grained cross-model consistency](https://arxiv.org/abs/2508.14314)
Token length: 1334
Summarized using GPT-3.5-turbo
Append: [SurveyGen-I: Consistent Scientific Survey Generation with Evolving Plans and Memory-Guided Writing](https://arxiv.org/abs/2508.14317)
Token length: 1175
Summarized using GPT-3.5-turbo
Append: [Beyond Semantic Similarity: Reducing Unnecessary API Calls via Behavior-Aligned Retriever](https://arxiv.org/abs/2508.14323)
Token length: 912
Summarized using GPT-3.5-turbo
Append: [ISCA: A Framework for Interview-Style Conversational Agents](https://arxiv.org/abs/2508.14344)
Token length: 1961
Summarized using GPT-3.5-turbo
Append: [ZPD-SCA: Unveiling the Blind Spots of LLMs in Assessing Students' Cognitive Abilities](https://arxiv.org/abs/2508.14377)
Token length: 1160
Summarized using GPT-3.5-turbo
Append: [Credence Calibration Game? Calibrating Large Language Models through Structured Play](https://arxiv.org/abs/2508.14390)
Token length: 1789
Summarized using GPT-3.5-turbo
Append: [DEPTH: Hallucination-Free Relation Extraction via Dependency-Aware Sentence Simplification and Two-tiered Hierarchical Refinement](https://arxiv.org/abs/2508.14391)
Token length: 1500
Summarized using GPT-3.5-turbo
Append: [Cognitive Surgery: The Awakening of Implicit Territorial Awareness in LLMs](https://arxiv.org/abs/2508.14408)
Token length: 1830
Summarized using GPT-3.5-turbo
Append: [Knowledge Graph-Infused Fine-Tuning for Structured Reasoning in Large Language Models](https://arxiv.org/abs/2508.14427)
Token length: 1384
Summarized using GPT-3.5-turbo
Append: [NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model](https://arxiv.org/abs/2508.14444)
Token length: 583
Summarized using GPT-3.5-turbo
Append: [In2x at WMT25 Translation Task](https://arxiv.org/abs/2508.14472)
Token length: 1786
Summarized using GPT-3.5-turbo
Append: [Reasoning is about giving reasons](https://arxiv.org/abs/2508.14488)
Token length: 993
Summarized using GPT-3.5-turbo
Append: [EmoTale: An Enacted Speech-emotion Dataset in Danish](https://arxiv.org/abs/2508.14548)
Token length: 1273
Summarized using GPT-3.5-turbo
Append: [Towards Skeletal and Signer Noise Reduction in Sign Language Production via Quaternion-Based Pose Encoding and Contrastive Learning](https://arxiv.org/abs/2508.14574)
Token length: 837
Summarized using GPT-3.5-turbo
Append: [Filling the Gap for Uzbek: Creating Translation Resources for Southern Uzbek](https://arxiv.org/abs/2508.14586)
Token length: 943
Summarized using GPT-3.5-turbo
Append: [Continuous sentiment scores for literary and multilingual contexts](https://arxiv.org/abs/2508.14620)
Token length: 902
Summarized using GPT-3.5-turbo
Append: [Improving in-context learning with a better scoring function](https://arxiv.org/abs/2508.14685)
Token length: 1492
Summarized using GPT-3.5-turbo
Append: [ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine](https://arxiv.org/abs/2508.14706)
Token length: 1341
Summarized using GPT-3.5-turbo
Append: [The Digital Sous Chef -- A Comparative Study on Fine-Tuning Language Models for Recipe Generation](https://arxiv.org/abs/2508.14718)
Token length: 1204
Summarized using GPT-3.5-turbo
Append: [Transplant Then Regenerate: A New Paradigm for Text Data Augmentation](https://arxiv.org/abs/2508.14723)
Token length: 1210
Summarized using GPT-3.5-turbo
Append: [Evaluating Multilingual and Code-Switched Alignment in LLMs via Synthetic Natural Language Inference](https://arxiv.org/abs/2508.14735)
Token length: 1881
Summarized using GPT-3.5-turbo
Append: [TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting](https://arxiv.org/abs/2508.14782)
Token length: 1363
Summarized using GPT-3.5-turbo
Append: [Evaluating Retrieval-Augmented Generation vs. Long-Context Input for Clinical Reasoning over EHRs](https://arxiv.org/abs/2508.14817)
Token length: 1407
Summarized using GPT-3.5-turbo
Append: [Long Chain-of-Thought Reasoning Across Languages](https://arxiv.org/abs/2508.14828)
Token length: 1910
Summarized using GPT-3.5-turbo
Append: [MedReseacher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework](https://arxiv.org/abs/2508.14880)
Token length: 1539
Summarized using GPT-3.5-turbo
Append: [Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs](https://arxiv.org/abs/2508.14896)
Token length: 519
Summarized using GPT-3.5-turbo
Append: [RAG-Boost: Retrieval-Augmented Generation Enhanced LLM-based Speech Recognition](https://arxiv.org/abs/2508.14048)
Token length: 940
Summarized using GPT-3.5-turbo
Append: [MahaTTS: A Unified Framework for Multilingual Text-to-Speech Synthesis](https://arxiv.org/abs/2508.14049)
Token length: 1841
Summarized using GPT-3.5-turbo
Append: [FinAgentBench: A Benchmark Dataset for Agentic Retrieval in Financial Question Answering](https://arxiv.org/abs/2508.14052)
Token length: 1271
Summarized using GPT-3.5-turbo
Append: [Two Birds with One Stone: Multi-Task Detection and Attribution of LLM-Generated Text](https://arxiv.org/abs/2508.14190)
Token length: 1285
Summarized using GPT-3.5-turbo
Append: [Measuring LLM Code Generation Stability via Structural Entropy](https://arxiv.org/abs/2508.14288)
Token length: 1934
Summarized using GPT-3.5-turbo
Append: [MultiFuzz: A Dense Retrieval-based Multi-Agent System for Network Protocol Fuzzing](https://arxiv.org/abs/2508.14300)
Token length: 991
Summarized using GPT-3.5-turbo
Append: [GLASS: Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation](https://arxiv.org/abs/2508.14302)
Token length: 1411
Summarized using GPT-3.5-turbo
Append: [DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization](https://arxiv.org/abs/2508.14460)
Append: [Who Sees What? Structured Thought-Action Sequences for Epistemic Reasoning in LLMs](https://arxiv.org/abs/2508.14564)
Append: [MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers](https://arxiv.org/abs/2508.14704)
Append: [Privileged Self-Access Matters for Introspection in AI](https://arxiv.org/abs/2508.14802)
Append: [The Prompting Brain: Neurocognitive Markers of Expertise in Guiding Large Language Models](https://arxiv.org/abs/2508.14869)
Append: [Virtual Community: An Open World for Humans, Robots, and Society](https://arxiv.org/abs/2508.14893)
Append: [G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model](https://arxiv.org/abs/2312.11370)
Append: [Social Debiasing for Fair Multi-modal LLMs](https://arxiv.org/abs/2408.06569)
Append: [Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources](https://arxiv.org/abs/2409.08239)
Append: [Deliberate Reasoning in Language Models as Structure-Aware Planning with an Accurate World Model](https://arxiv.org/abs/2410.03136)
Append: [ChuLo: Chunk-Level Key Information Representation for Long Document Understanding](https://arxiv.org/abs/2410.11119)
Append: [A Little Human Data Goes A Long Way](https://arxiv.org/abs/2410.13098)
Append: [SLED: Self Logits Evolution Decoding for Improving Factuality in Large Language Models](https://arxiv.org/abs/2411.02433)
Append: [Retrieval-Augmented Semantic Parsing: Improving Generalization with Lexical Knowledge](https://arxiv.org/abs/2412.10207)
Append: [Task-Oriented Automatic Fact-Checking with Frame-Semantics](https://arxiv.org/abs/2501.13288)
Append: [Natural Language Generation from Visual Events: State-of-the-Art and Key Open Questions](https://arxiv.org/abs/2502.13034)
Append: [Advancing Language Multi-Agent Learning with Credit Re-Assignment for Interactive Environment Generalization](https://arxiv.org/abs/2502.14496)
Append: [JudgeLRM: Large Reasoning Models as a Judge](https://arxiv.org/abs/2504.00050)
Append: [Chain of Correction for Full-text Speech Recognition with Large Language Models](https://arxiv.org/abs/2504.01519)
Append: [Boosting Chart-to-Code Generation in MLLM via Dual Preference-Guided Refinement](https://arxiv.org/abs/2504.02906)
Append: [Uncertainty Quantification for Language Models: A Suite of Black-Box, White-Box, LLM Judge, and Ensemble Scorers](https://arxiv.org/abs/2504.19254)
Append: [Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback](https://arxiv.org/abs/2506.03106)
Append: [Customizing Speech Recognition Model with Large Language Model Feedback](https://arxiv.org/abs/2506.11091)
Append: [Enhancing Temporal Sensitivity of Large Language Model for Recommendation with Counterfactual Tuning](https://arxiv.org/abs/2507.03047)
Append: [Each to Their Own: Exploring the Optimal Embedding in RAG](https://arxiv.org/abs/2507.17442)
Append: [Is neural semantic parsing good at ellipsis resolution, or isn't it?](https://arxiv.org/abs/2508.00121)
Append: [Enhancing Depression-Diagnosis-Oriented Chat with Psychological State Tracking](https://arxiv.org/abs/2403.09717)
Append: [LoRA-XS: Low-Rank Adaptation with Extremely Small Number of Parameters](https://arxiv.org/abs/2405.17604)
Append: [Coupling without Communication and Drafter-Invariant Speculative Decoding](https://arxiv.org/abs/2408.07978)
Append: [ReSpark: Leveraging Previous Data Reports as References to Generate New Reports with LLMs](https://arxiv.org/abs/2502.02329)
Append: [FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis for \"U-Tsang, Amdo and Kham Speech Dataset Generation](https://arxiv.org/abs/2505.14351)
Append: [DeepRetro: Retrosynthetic Pathway Discovery using Iterative LLM Reasoning](https://arxiv.org/abs/2507.07060)
Append: [CRINN: Contrastive Reinforcement Learning for Approximate Nearest Neighbor Search](https://arxiv.org/abs/2508.02091)
append_entries: 82
Finish: 2025-08-21 04:24:48.736010
------------------------------------------------------
Started: 2025-08-21 06:35:08.411209
Existing_entries: 1082
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1603
Summarized using GPT-3.5-turbo
Append: [From Autonomy to Agency: Agentic Vehicles for Human-Centered Mobility Systems](https://arxiv.org/abs/2507.04996)
append_entries: 1
Finish: 2025-08-21 06:35:10.605521
------------------------------------------------------
Started: 2025-08-21 08:21:51.321198
Existing_entries: 1001
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-21 08:21:51.548521
------------------------------------------------------
Started: 2025-08-21 10:17:38.534105
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-21 10:17:38.811498
------------------------------------------------------
Started: 2025-08-21 12:34:08.143192
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-21 12:34:08.412902
------------------------------------------------------
Started: 2025-08-21 14:17:03.684433
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-21 14:17:03.917067
------------------------------------------------------
Started: 2025-08-21 16:20:15.715665
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-21 16:20:15.984538
------------------------------------------------------
Started: 2025-08-21 18:23:59.553648
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-21 18:23:59.793096
------------------------------------------------------
Started: 2025-08-21 20:17:48.998585
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-21 20:17:49.303845
------------------------------------------------------
Started: 2025-08-21 22:16:31.525761
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-21 22:16:31.777867
------------------------------------------------------
Started: 2025-08-22 01:16:56.268414
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-22 01:16:56.558717
------------------------------------------------------
Started: 2025-08-22 03:05:53.420041
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-22 03:05:53.721327
------------------------------------------------------
Started: 2025-08-22 04:22:30.225096
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1620
Summarized using GPT-3.5-turbo
Append: [Efficient Switchable Safety Control in LLMs via Magic-Token-Guided Co-Training](https://arxiv.org/abs/2508.14904)
Token length: 743
Summarized using GPT-3.5-turbo
Append: [Preliminary Ranking of WMT25 General Machine Translation Systems](https://arxiv.org/abs/2508.14909)
Token length: 1279
Summarized using GPT-3.5-turbo
Append: [Bridging the Culture Gap: A Framework for LLM-Driven Socio-Cultural Localization of Math Word Problems in Low-Resource Languages](https://arxiv.org/abs/2508.14913)
Token length: 1119
Summarized using GPT-3.5-turbo
Append: [Improving LLMs for Machine Translation Using Synthetic Preference Data](https://arxiv.org/abs/2508.14951)
Token length: 1494
Summarized using GPT-3.5-turbo
Append: [Multilingual Datasets for Custom Input Extraction and Explanation Requests Parsing in Conversational XAI Systems](https://arxiv.org/abs/2508.14982)
Token length: 1295
Summarized using GPT-3.5-turbo
Append: [Reward-Shifted Speculative Sampling Is An Efficient Test-Time Weak-to-Strong Aligner](https://arxiv.org/abs/2508.15044)
Token length: 1315
Summarized using GPT-3.5-turbo
Append: [LongRecall: A Structured Approach for Robust Recall Evaluation in Long-Form Text](https://arxiv.org/abs/2508.15085)
Token length: 1093
Summarized using GPT-3.5-turbo
Append: [Mapping the Course for Prompt-based Structured Prediction](https://arxiv.org/abs/2508.15090)
Token length: 1941
Summarized using GPT-3.5-turbo
Append: [Nemotron-CC-Math: A 133 Billion-Token-Scale High Quality Math Pretraining Dataset](https://arxiv.org/abs/2508.15096)
Token length: 822
Summarized using GPT-3.5-turbo
Append: [Identifying and Answering Questions with False Assumptions: An Interpretable Approach](https://arxiv.org/abs/2508.15139)
Token length: 1736
Summarized using GPT-3.5-turbo
Append: [ContextualLVLM-Agent: A Holistic Framework for Multi-Turn Visually-Grounded Dialogue and Complex Instruction Following](https://arxiv.org/abs/2508.15164)
Token length: 1440
Summarized using GPT-3.5-turbo
Append: [SemToken: Semantic-Aware Tokenization for Efficient Long-Context Language Modeling](https://arxiv.org/abs/2508.15190)
Token length: 1732
Summarized using GPT-3.5-turbo
Append: [Fin-PRM: A Domain-Specialized Process Reward Model for Financial Reasoning in Large Language Models](https://arxiv.org/abs/2508.15202)
Token length: 1823
Summarized using GPT-3.5-turbo
Append: [SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache Channel Pruning](https://arxiv.org/abs/2508.15212)
Token length: 1205
Summarized using GPT-3.5-turbo
Append: [Select to Know: An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering](https://arxiv.org/abs/2508.15213)
Token length: 1315
Summarized using GPT-3.5-turbo
Append: [Self-Guided Function Calling in Large Language Models via Stepwise Experience Recall](https://arxiv.org/abs/2508.15214)
Token length: 1151
Summarized using GPT-3.5-turbo
Append: [Are Checklists Really Useful for Automatic Evaluation of Generative Tasks?](https://arxiv.org/abs/2508.15218)
Token length: 1505
Summarized using GPT-3.5-turbo
Append: [VocabTailor: Dynamic Vocabulary Selection for Downstream Tasks in Small Language Models](https://arxiv.org/abs/2508.15229)
Token length: 1118
Summarized using GPT-3.5-turbo
Append: [WangchanThaiInstruct: An instruction-following Dataset for Culture-Aware, Multitask, and Multi-domain Evaluation in Thai](https://arxiv.org/abs/2508.15239)
Token length: 1209
Summarized using GPT-3.5-turbo
Append: [UniCoM: A Universal Code-Switching Speech Generator](https://arxiv.org/abs/2508.15244)
Token length: 1293
Summarized using GPT-3.5-turbo
Append: [EMNLP: Educator-role Moral and Normative Large Language Models Profiling](https://arxiv.org/abs/2508.15250)
Token length: 1102
Summarized using GPT-3.5-turbo
Append: [Conflict-Aware Soft Prompting for Retrieval-Augmented Generation](https://arxiv.org/abs/2508.15253)
Token length: 1224
Summarized using GPT-3.5-turbo
Append: [TComQA: Extracting Temporal Commonsense from Text](https://arxiv.org/abs/2508.15274)
Token length: 972
Summarized using GPT-3.5-turbo
Append: [CUPE: Contextless Universal Phoneme Encoder for Language-Agnostic Speech Processing](https://arxiv.org/abs/2508.15316)
Token length: 1967
Summarized using GPT-3.5-turbo
Append: [KG-EDAS: A Meta-Metric Framework for Evaluating Knowledge Graph Completion Models](https://arxiv.org/abs/2508.15357)
Token length: 1323
Summarized using GPT-3.5-turbo
Append: [A Survey on Large Language Model Benchmarks](https://arxiv.org/abs/2508.15361)
Token length: 1944
Summarized using GPT-3.5-turbo
Append: [Unveiling Trust in Multimodal Large Language Models: Evaluation, Analysis, and Mitigation](https://arxiv.org/abs/2508.15370)
Token length: 1353
Summarized using GPT-3.5-turbo
Append: [Confidence-Modulated Speculative Decoding for Large Language Models](https://arxiv.org/abs/2508.15371)
Token length: 1613
Summarized using GPT-3.5-turbo
Append: [Exploiting Vocabulary Frequency Imbalance in Language Model Pre-training](https://arxiv.org/abs/2508.15390)
Token length: 1017
Summarized using GPT-3.5-turbo
Append: [Attribution, Citation, and Quotation: A Survey of Evidence-based Text Generation with Large Language Models](https://arxiv.org/abs/2508.15396)
Token length: 1443
Summarized using GPT-3.5-turbo
Append: [When Audio and Text Disagree: Revealing Text Bias in Large Audio-Language Models](https://arxiv.org/abs/2508.15407)
Token length: 1544
Summarized using GPT-3.5-turbo
Append: [LLaSO: A Foundational Framework for Reproducible Research in Large Language and Speech Model](https://arxiv.org/abs/2508.15418)
Token length: 1016
Summarized using GPT-3.5-turbo
Append: [A Study of Privacy-preserving Language Modeling Approaches](https://arxiv.org/abs/2508.15421)
Token length: 727
Summarized using GPT-3.5-turbo
Append: [M-HELP: Using Social Media Data to Detect Mental Health Help-Seeking Signals](https://arxiv.org/abs/2508.15440)
Token length: 1215
Summarized using GPT-3.5-turbo
Append: [Principle Methods of Rendering Non-equivalent Words from Uzbek and Dari to Russian and English](https://arxiv.org/abs/2508.15453)
Token length: 806
Summarized using GPT-3.5-turbo
Append: [PyTOD: Programmable Task-Oriented Dialogue with Execution Feedback](https://arxiv.org/abs/2508.15456)
Token length: 1360
Summarized using GPT-3.5-turbo
Append: [RadReason: Radiology Report Evaluation Metric with Reasons and Sub-Scores](https://arxiv.org/abs/2508.15464)
Token length: 1604
Summarized using GPT-3.5-turbo
Append: [SLM4Offer: Personalized Marketing Offer Generation Using Contrastive Learning Based Fine-Tuning](https://arxiv.org/abs/2508.15471)
Token length: 1425
Summarized using GPT-3.5-turbo
Append: [Subjective Behaviors and Preferences in LLM: Language of Browsing](https://arxiv.org/abs/2508.15474)
Token length: 968
Summarized using GPT-3.5-turbo
Append: [Influence-driven Curriculum Learning for Pre-training on Limited Data](https://arxiv.org/abs/2508.15475)
Token length: 1266
Summarized using GPT-3.5-turbo
Append: [SLM-Bench: A Comprehensive Benchmark of Small Language Models on Environmental Impacts -- Extended Version](https://arxiv.org/abs/2508.15478)
Token length: 1392
Summarized using GPT-3.5-turbo
Append: [HebID: Detecting Social Identities in Hebrew-language Political Text](https://arxiv.org/abs/2508.15483)
Token length: 883
Summarized using GPT-3.5-turbo
Append: [Dream 7B: Diffusion Large Language Models](https://arxiv.org/abs/2508.15487)
Token length: 1281
Summarized using GPT-3.5-turbo
Append: [The Enemy from Within: A Study of Political Delegitimization Discourse in Israeli Political Speech](https://arxiv.org/abs/2508.15524)
Token length: 1351
Summarized using GPT-3.5-turbo
Append: [SafetyFlow: An Agent-Flow System for Automated LLM Safety Benchmarking](https://arxiv.org/abs/2508.15526)
Token length: 520
Summarized using GPT-3.5-turbo
Append: [Trained Miniatures: Low cost, High Efficacy SLMs for Sales & Marketing](https://arxiv.org/abs/2508.15617)
Token length: 1449
Summarized using GPT-3.5-turbo
Append: [SDGO: Self-Discrimination-Guided Optimization for Consistent Safety in Large Language Models](https://arxiv.org/abs/2508.15648)
Token length: 1389
Summarized using GPT-3.5-turbo
Append: [Benchmarking Computer Science Survey Generation](https://arxiv.org/abs/2508.15658)
Token length: 1366
Summarized using GPT-3.5-turbo
Append: [Position Bias Mitigates Position Bias:Mitigate Position Bias Through Inter-Position Knowledge Distillation](https://arxiv.org/abs/2508.15709)
Token length: 1347
Summarized using GPT-3.5-turbo
Append: [Stemming -- The Evolution and Current State with a Focus on Bangla](https://arxiv.org/abs/2508.15711)
Append: [EcomMMMU: Strategic Utilization of Visuals for Robust Multimodal E-Commerce Models](https://arxiv.org/abs/2508.15721)
Append: [End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning](https://arxiv.org/abs/2508.15746)
Append: [Dissecting Tool-Integrated Reasoning: An Empirical Study and Analysis](https://arxiv.org/abs/2508.15754)
Append: [LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on Challenging Queries](https://arxiv.org/abs/2508.15760)
Append: [A Chinese Heart Failure Status Speech Database with Universal and Personalised Classification](https://arxiv.org/abs/2508.14908)
Append: [Transsion Multilingual Speech Recognition System for MLC-SLM 2025 Challenge](https://arxiv.org/abs/2508.14916)
Append: [Robust Symbolic Reasoning for Visual Narratives via Hierarchical and Semantically Normalized Knowledge Graphs](https://arxiv.org/abs/2508.14941)
Append: [Don't Think Twice! Over-Reasoning Impairs Confidence Calibration](https://arxiv.org/abs/2508.15050)
Append: [LLMs and Agentic AI in Insurance Decision-Making: Opportunities and Challenges For Africa](https://arxiv.org/abs/2508.15110)
Append: [Open-Universe Assistance Games](https://arxiv.org/abs/2508.15119)
Append: [aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery Generated by AI Scientists](https://arxiv.org/abs/2508.15126)
Append: [LLM4Sweat: A Trustworthy Large Language Model for Hyperhidrosis Support](https://arxiv.org/abs/2508.15192)
Append: [Retrieval-Augmented Review Generation for Poisoning Recommender Systems](https://arxiv.org/abs/2508.15252)
Append: [AmbiSQL: Interactive Ambiguity Detection and Resolution for Text-to-SQL](https://arxiv.org/abs/2508.15276)
Append: [Adversarial Attacks against Neural Ranking Models via In-Context Learning](https://arxiv.org/abs/2508.15283)
Append: [Evaluating Knowledge Graph Complexity via Semantic, Spectral, and Structural Metrics for Link Prediction](https://arxiv.org/abs/2508.15291)
Append: [Multiple Memory Systems for Enhancing the Long-term Memory of Agent](https://arxiv.org/abs/2508.15294)
Append: [IPIGuard: A Novel Tool Dependency Graph-Based Defense Against Indirect Prompt Injection in LLM Agents](https://arxiv.org/abs/2508.15310)
Append: [DiagECG: An LLM-Driven Framework for Diagnostic Reasoning via Discretized ECG Tokenization](https://arxiv.org/abs/2508.15338)
Append: [CITE: A Comprehensive Benchmark for Heterogeneous Text-Attributed Graphs on Catalytic Materials](https://arxiv.org/abs/2508.15392)
Append: [Foundational Design Principles and Patterns for Building Robust and Adaptive GenAI-Native Systems](https://arxiv.org/abs/2508.15411)
Append: [GraSP: A Unified Graph-Based Framework for Scalable Generation, Quality Tagging, and Management of Synthetic Data for SFT and DPO](https://arxiv.org/abs/2508.15432)
Append: [Classification errors distort findings in automated speech processing: examples and solutions from child-development research](https://arxiv.org/abs/2508.15637)
Append: [Language-Guided Tuning: Enhancing Numeric Optimization with Textual Feedback](https://arxiv.org/abs/2508.15757)
Append: [Intern-S1: A Scientific Multimodal Foundation Model](https://arxiv.org/abs/2508.15763)
Append: [Unplug and Play Language Models: Decomposing Experts in Language Models at Inference Time](https://arxiv.org/abs/2404.11916)
Append: [On the Role of Entity and Event Level Conceptualization in Generalizable Reasoning: A Survey of Tasks, Methods, Applications, and Future Directions](https://arxiv.org/abs/2406.10885)
Append: [Teuken-7B-Base & Teuken-7B-Instruct: Towards European LLMs](https://arxiv.org/abs/2410.03730)
Append: [Fine-tuning foundational models to code diagnoses from veterinary health records](https://arxiv.org/abs/2410.15186)
Append: [Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition](https://arxiv.org/abs/2412.13612)
Append: [Rethinking Addressing in Language Models via Contexualized Equivariant Positional Encoding](https://arxiv.org/abs/2501.00712)
Append: [Everybody Likes to Sleep: A Computer-Assisted Comparison of Object Naming Data from 30 Languages](https://arxiv.org/abs/2501.08312)
Append: [Self-Supervised Prompt Optimization](https://arxiv.org/abs/2502.06855)
Append: [RefineCoder: Iterative Improving of Large Language Models via Adaptive Critique Refinement for Code Generation](https://arxiv.org/abs/2502.09183)
Append: [Ontology-Guided Reverse Thinking Makes Large Language Models Stronger on Knowledge Graph Question Answering](https://arxiv.org/abs/2502.11491)
Append: [Pub-Guard-LLM: Detecting Retracted Biomedical Articles with Reliable Explanations](https://arxiv.org/abs/2502.15429)
Append: [Robust Bias Detection in MLMs and its Application to Human Trait Ratings](https://arxiv.org/abs/2502.15600)
Append: [Synthetic vs. Gold: The Role of LLM Generated Labels and Data in Cyberbullying Detection](https://arxiv.org/abs/2502.15860)
Append: [Pragmatic Inference Chain (PIC) Improving LLMs' Reasoning of Authentic Implicit Toxic Language](https://arxiv.org/abs/2503.01539)
Append: [Advancing the Database of Cross-Linguistic Colexifications with New Workflows and Data](https://arxiv.org/abs/2503.11377)
Append: [Leveraging Large Language Models for Explainable Activity Recognition in Smart Homes: A Critical Evaluation](https://arxiv.org/abs/2503.16622)
Append: [VerifiAgent: a Unified Verification Agent in Language Model Reasoning](https://arxiv.org/abs/2504.00406)
Append: [MuSeD: A Multimodal Spanish Dataset for Sexism Detection in Social Media Videos](https://arxiv.org/abs/2504.11169)
Append: [Kuwain 1.5B: An Arabic SLM via Language Injection](https://arxiv.org/abs/2504.15120)
Append: [Cequel: Cost-Effective Querying of Large Language Models for Text Clustering](https://arxiv.org/abs/2504.15640)
Append: [Annif at SemEval-2025 Task 5: Traditional XMTC augmented by LLMs](https://arxiv.org/abs/2504.19675)
Append: [WebEvolver: Enhancing Web Agent Self-Improvement with Coevolving World Model](https://arxiv.org/abs/2504.21024)
Append: [Sadeed: Advancing Arabic Diacritization Through Small Language Model](https://arxiv.org/abs/2504.21635)
Append: [Mutarjim: Advancing Bidirectional Arabic-English Translation with a Small Language Model](https://arxiv.org/abs/2505.17894)
Append: [One-shot Entropy Minimization](https://arxiv.org/abs/2505.20282)
Append: [Lossless Token Sequence Compression via Meta-Tokens](https://arxiv.org/abs/2506.00307)
Append: [LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure Profiles](https://arxiv.org/abs/2506.06561)
Append: [AraReasoner: Evaluating Reasoning-Based LLMs for Arabic NLP](https://arxiv.org/abs/2506.08768)
Append: [MFTCXplain: A Multilingual Benchmark Dataset for Evaluating the Moral Reasoning of LLMs through Hate Speech Multi-hop Explanations](https://arxiv.org/abs/2506.19073)
Append: [Empirical Evidence for Alignment Faking in a Small LLM and Prompt-Based Mitigation Techniques](https://arxiv.org/abs/2506.21584)
Append: [SAND: Boosting LLM Agents with Self-Taught Action Deliberation](https://arxiv.org/abs/2507.07441)
Append: [Large Language Models Encode Semantics in Low-Dimensional Linear Subspaces](https://arxiv.org/abs/2507.09709)
Append: [Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters](https://arxiv.org/abs/2507.13618)
Append: [Length Representations in Large Language Models](https://arxiv.org/abs/2507.20398)
Append: [CUS-QA: Local-Knowledge-Oriented Open-Ended Question Answering Dataset](https://arxiv.org/abs/2507.22752)
Append: [Diagnosing Memorization in Chain-of-Thought Reasoning, One Token at a Time](https://arxiv.org/abs/2508.02037)
Append: [CRISPR-GPT for Agentic Automation of Gene-editing Experiments](https://arxiv.org/abs/2404.18021)
Append: [MATATA: Weakly Supervised End-to-End MAthematical Tool-Augmented Reasoning for Tabular Applications](https://arxiv.org/abs/2411.18915)
Append: [Evaluation Agent: Efficient and Promptable Evaluation Framework for Visual Generative Models](https://arxiv.org/abs/2412.09645)
Append: [InfAlign: Inference-aware language model alignment](https://arxiv.org/abs/2412.19792)
Append: [Learning to Generate Unit Tests for Automated Debugging](https://arxiv.org/abs/2502.01619)
Append: [Versatile Framework for Song Generation with Prompt-based Control](https://arxiv.org/abs/2504.19062)
Append: [On the Fundamental Impossibility of Hallucination Control in Large Language Models](https://arxiv.org/abs/2506.06382)
Append: [Exploring Big Five Personality and AI Capability Effects in LLM-Simulated Negotiation Dialogues](https://arxiv.org/abs/2506.15928)
Append: [Exploring Modularity of Agentic Systems for Drug Discovery](https://arxiv.org/abs/2506.22189)
Append: [The Devil is in the EOS: Sequence Training for Detailed Image Captioning](https://arxiv.org/abs/2507.20077)
Append: [Prescriptive Agents based on RAG for Automated Maintenance (PARAM)](https://arxiv.org/abs/2508.04714)
append_entries: 122
Finish: 2025-08-22 04:24:02.764137
------------------------------------------------------
Started: 2025-08-22 06:25:16.637221
Existing_entries: 1122
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-22 06:25:16.968887
------------------------------------------------------
Started: 2025-08-22 08:22:09.202443
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-22 08:22:09.488455
------------------------------------------------------
Started: 2025-08-22 10:17:28.906657
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-22 10:17:29.198854
------------------------------------------------------
Started: 2025-08-22 12:33:26.836930
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-22 12:33:27.135339
------------------------------------------------------
Started: 2025-08-22 14:16:12.228457
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-22 14:16:12.538759
------------------------------------------------------
Started: 2025-08-22 16:19:47.147310
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-22 16:19:47.472790
------------------------------------------------------
Started: 2025-08-22 18:23:07.180825
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-22 18:23:07.549840
------------------------------------------------------
Started: 2025-08-22 20:18:00.055339
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-22 20:18:00.348644
------------------------------------------------------
Started: 2025-08-22 22:15:11.043472
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-22 22:15:11.334962
------------------------------------------------------
Started: 2025-08-23 01:14:25.996936
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-23 01:14:26.377364
------------------------------------------------------
Started: 2025-08-23 02:59:59.927474
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-23 03:00:00.245353
------------------------------------------------------
Started: 2025-08-23 04:19:16.050645
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-23 04:19:16.178058
------------------------------------------------------
Started: 2025-08-23 06:22:17.528904
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-23 06:22:17.635621
------------------------------------------------------
Started: 2025-08-23 08:19:34.150886
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-23 08:19:34.217880
------------------------------------------------------
Started: 2025-08-23 10:15:35.071527
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-23 10:15:35.131944
------------------------------------------------------
Started: 2025-08-23 12:30:40.408232
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-23 12:30:40.477512
------------------------------------------------------
Started: 2025-08-23 14:13:37.371047
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-23 14:13:37.432680
------------------------------------------------------
Started: 2025-08-23 16:17:47.947234
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-23 16:17:48.043663
------------------------------------------------------
Started: 2025-08-23 18:20:30.794984
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-23 18:20:30.861802
------------------------------------------------------
Started: 2025-08-23 20:16:33.351530
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-23 20:16:33.409614
------------------------------------------------------
Started: 2025-08-23 22:14:23.826435
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-23 22:14:23.885677
------------------------------------------------------
Started: 2025-08-24 01:24:53.854522
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-24 01:24:53.933525
------------------------------------------------------
Started: 2025-08-24 03:16:40.693236
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-24 03:16:40.752845
------------------------------------------------------
Started: 2025-08-24 04:23:14.761125
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-24 04:23:14.820552
------------------------------------------------------
Started: 2025-08-24 06:23:31.130258
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-24 06:23:31.221141
------------------------------------------------------
Started: 2025-08-24 08:19:17.552253
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-24 08:19:17.643208
------------------------------------------------------
Started: 2025-08-24 10:15:38.160430
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-24 10:15:38.217736
------------------------------------------------------
Started: 2025-08-24 12:30:54.755263
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-24 12:30:54.841167
------------------------------------------------------
Started: 2025-08-24 14:13:38.901418
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-24 14:13:38.980393
------------------------------------------------------
Started: 2025-08-24 16:18:37.384325
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-24 16:18:37.459600
------------------------------------------------------
Started: 2025-08-24 18:21:14.356803
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-24 18:21:14.416397
------------------------------------------------------
Started: 2025-08-24 20:17:16.048256
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-24 20:17:16.124007
------------------------------------------------------
Started: 2025-08-24 22:15:02.230754
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-24 22:15:02.342348
------------------------------------------------------
Started: 2025-08-25 01:19:50.378417
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-25 01:19:50.461932
------------------------------------------------------
Started: 2025-08-25 03:13:02.652770
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-25 03:13:02.742976
------------------------------------------------------
Started: 2025-08-25 04:28:28.724783
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1429
Summarized using GPT-3.5-turbo
Append: [KG-o1: Enhancing Multi-hop Question Answering in Large Language Models via Knowledge Graph Integration](https://arxiv.org/abs/2508.15790)
Token length: 1572
Summarized using GPT-3.5-turbo
Append: [InteChar: A Unified Oracle Bone Character List for Ancient Chinese Language Modeling](https://arxiv.org/abs/2508.15791)
Token length: 1196
Summarized using GPT-3.5-turbo
Append: [Bhav-Net: Knowledge Transfer for Cross-Lingual Antonym vs Synonym Distinction via Dual-Space Graph Transformers](https://arxiv.org/abs/2508.15792)
Token length: 1730
Summarized using GPT-3.5-turbo
Append: [Format as a Prior: Quantifying and Analyzing Bias in LLMs for Heterogeneous Data](https://arxiv.org/abs/2508.15793)
Token length: 1126
Summarized using GPT-3.5-turbo
Append: [Do Language Models Agree with Human Perceptions of Suspense in Stories?](https://arxiv.org/abs/2508.15794)
Token length: 1208
Summarized using GPT-3.5-turbo
Append: [Benchmarking the Legal Reasoning of LLMs in Arabic Islamic Inheritance Cases](https://arxiv.org/abs/2508.15796)
Token length: 1632
Summarized using GPT-3.5-turbo
Append: [Benchmarking the Medical Understanding and Reasoning of Large Language Models in Arabic Healthcare Tasks](https://arxiv.org/abs/2508.15797)
Token length: 1937
Summarized using GPT-3.5-turbo
Append: [Persuasiveness and Bias in LLM: Investigating the Impact of Persuasiveness and Reinforcement of Bias in Language Models](https://arxiv.org/abs/2508.15798)
Token length: 576
Summarized using GPT-3.5-turbo
Append: [A Framework for Processing Textual Descriptions of Business Processes using a Constrained Language -- Technical Report](https://arxiv.org/abs/2508.15799)
Token length: 1365
Summarized using GPT-3.5-turbo
Append: [A BERT-based Hierarchical Classification Model with Applications in Chinese Commodity Classification](https://arxiv.org/abs/2508.15800)
Token length: 1840
Summarized using GPT-3.5-turbo
Append: [LingVarBench: Benchmarking LLM for Automated Named Entity Recognition in Structured Synthetic Spoken Transcriptions](https://arxiv.org/abs/2508.15801)
Token length: 1269
Summarized using GPT-3.5-turbo
Append: [MAC: A Live Benchmark for Multimodal Large Language Models in Scientific Understanding](https://arxiv.org/abs/2508.15802)
Token length: 1714
Summarized using GPT-3.5-turbo
Append: [ReportBench: Evaluating Deep Research Agents via Academic Survey Tasks](https://arxiv.org/abs/2508.15804)
Token length: 1479
Summarized using GPT-3.5-turbo
Append: [ALAS: Autonomous Learning Agent for Self-Updating Language Models](https://arxiv.org/abs/2508.15805)
Token length: 981
Summarized using GPT-3.5-turbo
Append: [SurfaceLogicKV: Surface and Logic Attention Behaviors are All You Need for Robust KV Cache Compression](https://arxiv.org/abs/2508.15806)
Token length: 1214
Summarized using GPT-3.5-turbo
Append: [KL-based self-distillation for large language models](https://arxiv.org/abs/2508.15807)
Token length: 1431
Summarized using GPT-3.5-turbo
Append: [Chain-of-Query: Unleashing the Power of LLMs in SQL-Aided Table Understanding via Multi-Agent Collaboration](https://arxiv.org/abs/2508.15809)
Token length: 1318
Summarized using GPT-3.5-turbo
Append: [Detecting Hope, Hate, and Emotion in Arabic Textual Speech and Multi-modal Memes Using Large Language Models](https://arxiv.org/abs/2508.15810)
Token length: 1399
Summarized using GPT-3.5-turbo
Append: [From Clicks to Preference: A Multi-stage Alignment Framework for Generative Query Suggestion in Conversational System](https://arxiv.org/abs/2508.15811)
Token length: 1716
Summarized using GPT-3.5-turbo
Append: [SCOPE: A Generative Approach for LLM Prompt Compression](https://arxiv.org/abs/2508.15813)
Token length: 1454
Summarized using GPT-3.5-turbo
Append: [User-Assistant Bias in LLMs](https://arxiv.org/abs/2508.15815)
Token length: 883
Summarized using GPT-3.5-turbo
Append: [Meet Your New Client: Writing Reports for AI -- Benchmarking Information Loss in Market Research Deliverables](https://arxiv.org/abs/2508.15817)
Token length: 1280
Summarized using GPT-3.5-turbo
Append: [Research on intelligent generation of structural demolition suggestions based on multi-model collaboration](https://arxiv.org/abs/2508.15820)
Token length: 1893
Summarized using GPT-3.5-turbo
Append: [An Auditable Pipeline for Fuzzy Full-Text Screening in Systematic Reviews: Integrating Contrastive Semantic Highlighting and LLM Judgment](https://arxiv.org/abs/2508.15822)
Token length: 1438
Summarized using GPT-3.5-turbo
Append: [SDEC: Semantic Deep Embedded Clustering](https://arxiv.org/abs/2508.15823)
Token length: 888
Summarized using GPT-3.5-turbo
Append: [Avalia\c{c}\~ao de efici\^encia na leitura: uma abordagem baseada em PLN](https://arxiv.org/abs/2508.15824)
Token length: 1111
Summarized using GPT-3.5-turbo
Append: [Enhancing Cryptocurrency Sentiment Analysis with Multimodal Features](https://arxiv.org/abs/2508.15825)
Token length: 1548
Summarized using GPT-3.5-turbo
Append: [Embarrassed to observe: The effects of directive language in brand conversation](https://arxiv.org/abs/2508.15826)
Token length: 1857
Summarized using GPT-3.5-turbo
Append: [Mini-Omni-Reasoner: Token-Level Thinking-in-Speaking in Large Speech Models](https://arxiv.org/abs/2508.15827)
Token length: 1283
Summarized using GPT-3.5-turbo
Append: [Mining Mental Health Signals: A Comparative Study of Four Machine Learning Methods for Depression Detection from Social Media Posts in Sorani Kurdish](https://arxiv.org/abs/2508.15829)
Token length: 1599
Summarized using GPT-3.5-turbo
Append: [DAIQ: Auditing Demographic Attribute Inference from Question in LLMs](https://arxiv.org/abs/2508.15830)
Token length: 1728
Summarized using GPT-3.5-turbo
Append: [Who's Asking? Investigating Bias Through the Lens of Disability Framed Queries in LLMs](https://arxiv.org/abs/2508.15831)
Token length: 1648
Summarized using GPT-3.5-turbo
Append: [A Functionality-Grounded Benchmark for Evaluating Web Agents in E-commerce Domains](https://arxiv.org/abs/2508.15832)
Token length: 1510
Summarized using GPT-3.5-turbo
Append: [Scalable Scientific Interest Profiling Using Large Language Models](https://arxiv.org/abs/2508.15834)
Token length: 1351
Summarized using GPT-3.5-turbo
Append: [Alvorada-Bench: Can Language Models Solve Brazilian University Entrance Exams?](https://arxiv.org/abs/2508.15835)
Token length: 827
Summarized using GPT-3.5-turbo
Append: [MorphNAS: Differentiable Architecture Search for Morphologically-Aware Multilingual NER](https://arxiv.org/abs/2508.15836)
Token length: 1298
Summarized using GPT-3.5-turbo
Append: [Statistical Comparative Analysis of Semantic Similarities and Model Transferability Across Datasets for Short Answer Grading](https://arxiv.org/abs/2508.15837)
Token length: 1441
Summarized using GPT-3.5-turbo
Append: [A Review of Developmental Interpretability in Large Language Models](https://arxiv.org/abs/2508.15841)
Token length: 1815
Summarized using GPT-3.5-turbo
Append: [Lexical Hints of Accuracy in LLM Reasoning Chains](https://arxiv.org/abs/2508.15842)
Token length: 852
Summarized using GPT-3.5-turbo
Append: [Coarse-to-Fine Personalized LLM Impressions for Streamlined Radiology Reports](https://arxiv.org/abs/2508.15845)
Token length: 1477
Summarized using GPT-3.5-turbo
Append: [CyPortQA: Benchmarking Multimodal Large Language Models for Cyclone Preparedness in Port Operation](https://arxiv.org/abs/2508.15846)
Token length: 921
Summarized using GPT-3.5-turbo
Append: [Mechanistic Exploration of Backdoored Large Language Model Attention Patterns](https://arxiv.org/abs/2508.15847)
Token length: 1129
Summarized using GPT-3.5-turbo
Append: [MedCoT-RAG: Causal Chain-of-Thought RAG for Medical Question Answering](https://arxiv.org/abs/2508.15849)
Token length: 1659
Summarized using GPT-3.5-turbo
Append: [DocHop-QA: Towards Multi-Hop Reasoning over Multimodal Document Collections](https://arxiv.org/abs/2508.15851)
Token length: 1160
Summarized using GPT-3.5-turbo
Append: [MGSC: A Multi-granularity Consistency Framework for Robust End-to-end Asr](https://arxiv.org/abs/2508.15853)
Token length: 1192
Summarized using GPT-3.5-turbo
Append: [QU-NLP at QIAS 2025 Shared Task: A Two-Phase LLM Fine-Tuning and Retrieval-Augmented Generation Approach for Islamic Inheritance Reasoning](https://arxiv.org/abs/2508.15854)
Token length: 1218
Summarized using GPT-3.5-turbo
Append: [Counterspeech for Mitigating the Influence of Media Bias: Comparing Human and LLM-Generated Responses](https://arxiv.org/abs/2508.15855)
Token length: 1512
Summarized using GPT-3.5-turbo
Append: [XFinBench: Benchmarking LLMs in Complex Financial Problem Solving and Reasoning](https://arxiv.org/abs/2508.15861)
Token length: 1761
Summarized using GPT-3.5-turbo
Append: [CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated Chain-of-Thought-based Reinforced Fine-Tuning](https://arxiv.org/abs/2508.15868)
Token length: 1639
Summarized using GPT-3.5-turbo
Append: [NEAT: Concept driven Neuron Attribution in LLMs](https://arxiv.org/abs/2508.15875)
Append: [DeepMEL: A Multi-Agent Collaboration Framework for Multimodal Entity Linking](https://arxiv.org/abs/2508.15876)
Append: [Annif at the GermEval-2025 LLMs4Subjects Task: Traditional XMTC Augmented by Efficient LLMs](https://arxiv.org/abs/2508.15877)
Append: [Jet-Nemotron: Efficient Language Model with Post Neural Architecture Search](https://arxiv.org/abs/2508.15884)
Append: [Evaluating Structured Decoding for Text-to-Table Generation: Evidence from Three Datasets](https://arxiv.org/abs/2508.15910)
Append: [Dancing with Deer: A Constructional Perspective on MWEs in the Era of LLMs](https://arxiv.org/abs/2508.15977)
Append: [Political Ideology Shifts in Large Language Models](https://arxiv.org/abs/2508.16013)
Append: [X-Troll: eXplainable Detection of State-Sponsored Information Operations Agents](https://arxiv.org/abs/2508.16021)
Append: [OpenWHO: A Document-Level Parallel Corpus for Health Translation in Low-Resource Languages](https://arxiv.org/abs/2508.16048)
Append: [Ethical Considerations of Large Language Models in Game Playing](https://arxiv.org/abs/2508.16065)
Append: [Less Redundancy: Boosting Practicality of Vision Language Model in Walking Assistants](https://arxiv.org/abs/2508.16070)
Append: [CEQuest: Benchmarking Large Language Models for Construction Estimation](https://arxiv.org/abs/2508.16081)
Append: [CYCLE-INSTRUCT: Fully Seed-Free Instruction Tuning via Dual Self-Training and Cycle Consistency](https://arxiv.org/abs/2508.16100)
Append: [From Indirect Object Identification to Syllogisms: Exploring Binary Mechanisms in Transformer Circuits](https://arxiv.org/abs/2508.16109)
Append: [Text Takes Over: A Study of Modality Bias in Multimodal Intent Detection](https://arxiv.org/abs/2508.16122)
Append: [XLQA: A Benchmark for Locale-Aware Multilingual Open-Domain Question Answering](https://arxiv.org/abs/2508.16139)
Append: [ParamBench: A Graduate-Level Benchmark for Evaluating LLM Understanding on Indic Subjects](https://arxiv.org/abs/2508.16185)
Append: [Seeing is Believing: Emotion-Aware Audio-Visual Language Modeling for Expressive Speech Generation](https://arxiv.org/abs/2508.16188)
Append: [ComicScene154: A Scene Dataset for Comic Analysis](https://arxiv.org/abs/2508.16190)
Append: [CMR-SPB: Cross-Modal Multi-Hop Reasoning over Text, Image, and Speech with Path Balance](https://arxiv.org/abs/2508.16198)
Append: [TULIP: Adapting Open-Source Large Language Models for Underrepresented Languages and Specialized Financial Tasks](https://arxiv.org/abs/2508.16243)
Append: [M3TQA: Massively Multilingual Multitask Table Question Answering](https://arxiv.org/abs/2508.16265)
Append: [From Confidence to Collapse in LLM Factual Robustness](https://arxiv.org/abs/2508.16267)
Append: [LLMs that Understand Processes: Instruction-tuning for Semantics-Aware Process Mining](https://arxiv.org/abs/2508.16270)
Append: [JaParaPat: A Large-Scale Japanese-English Parallel Patent Application Corpus](https://arxiv.org/abs/2508.16303)
Append: [LLMSymGuard: A Symbolic Safety Guardrail Framework Leveraging Interpretable Jailbreak Concepts](https://arxiv.org/abs/2508.16325)
Append: [MizanQA: Benchmarking Large Language Models on Moroccan Legal Question Answering](https://arxiv.org/abs/2508.16357)
Append: [The Mediomatix Corpus: Parallel Data for Romansh Idioms via Comparable Schoolbooks](https://arxiv.org/abs/2508.16371)
Append: [ChatGPT-generated texts show authorship traits that identify them as non-human](https://arxiv.org/abs/2508.16385)
Append: [RoMedQA: The First Benchmark for Romanian Medical Question Answering](https://arxiv.org/abs/2508.16390)
Append: [Cetvel: A Unified Benchmark for Evaluating Language Understanding, Generation and Cultural Capacity of LLMs for Turkish](https://arxiv.org/abs/2508.16431)
Append: [A Probabilistic Inference Scaling Theory for LLM Self-Correction](https://arxiv.org/abs/2508.16456)
Append: [What makes an entity salient in discourse?](https://arxiv.org/abs/2508.16464)
Append: [LLM-as-classifier: Semi-Supervised, Iterative Framework for Hierarchical Text Classification using Large Language Models](https://arxiv.org/abs/2508.16478)
Append: [HAMSA: Hijacking Aligned Compact Models via Stealthy Automation](https://arxiv.org/abs/2508.16484)
Append: [Transfer Learning via Lexical Relatedness: A Sarcasm and Hate Speech Case Study](https://arxiv.org/abs/2508.16555)
Append: [Interpreting the linear structure of vision-language model embedding spaces](https://arxiv.org/abs/2504.11695)
Append: [Z-Pruner: Post-Training Pruning of Large Language Models for Efficiency without Retraining](https://arxiv.org/abs/2508.15828)
Append: [Unveiling Unicode's Unseen Underpinnings in Undermining Authorship Attribution](https://arxiv.org/abs/2508.15840)
Append: [Self-Disguise Attack: Induce the LLM to disguise itself for AIGT detection evasion](https://arxiv.org/abs/2508.15848)
Append: [PGF-Net: A Progressive Gated-Fusion Framework for Efficient Multimodal Sentiment Analysis](https://arxiv.org/abs/2508.15852)
Append: [Beyond Individuals: Collective Predictive Coding for Memory, Attention, and the Emergence of Language](https://arxiv.org/abs/2508.15859)
Append: [Lean Meets Theoretical Computer Science: Scalable Synthesis of Theorem Proving Challenges in Formal-Informal Pairs](https://arxiv.org/abs/2508.15878)
Append: [Beyond Transcription: Mechanistic Interpretability in ASR](https://arxiv.org/abs/2508.15882)
Append: [ASIC-Agent: An Autonomous Multi-Agent System for ASIC Design with Benchmark Evaluation](https://arxiv.org/abs/2508.15940)
Append: [Generative Foundation Model for Structured and Unstructured Electronic Health Records](https://arxiv.org/abs/2508.16054)
Append: [Extending FKG.in: Towards a Food Claim Traceability Network](https://arxiv.org/abs/2508.16117)
Append: [Hardwired-Neurons Language Processing Units as General-Purpose Cognitive Substrates](https://arxiv.org/abs/2508.16151)
Append: [AgentFly: Fine-tuning LLM Agents without Fine-tuning LLMs](https://arxiv.org/abs/2508.16153)
Append: [SpecVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning](https://arxiv.org/abs/2508.16201)
Append: [Retrieval Enhanced Feedback via In-context Neural Error-book](https://arxiv.org/abs/2508.16313)
Append: [Vevo2: Bridging Controllable Speech and Singing Voice Generation via Unified Prosody Learning](https://arxiv.org/abs/2508.16332)
Append: [AetherCode: Evaluating LLMs' Ability to Win In Premier Programming Competitions](https://arxiv.org/abs/2508.16402)
Append: [Retrieval-Augmented Defense: Adaptive and Controllable Jailbreak Prevention for Large Language Models](https://arxiv.org/abs/2508.16406)
Append: [PediatricsMQA: a Multi-modal Pediatrics Question Answering Benchmark](https://arxiv.org/abs/2508.16439)
Append: [Anti-establishment sentiment on TikTok: Implications for understanding influence(rs) and expertise on social media](https://arxiv.org/abs/2508.16453)
Append: [FLAMES: Improving LLM Math Reasoning via a Fine-Grained Analysis of the Data Synthesis Pipeline](https://arxiv.org/abs/2508.16514)
Append: [Sparse but Wrong: Incorrect L0 Leads to Incorrect Features in Sparse Autoencoders](https://arxiv.org/abs/2508.16560)
Append: [Prompting Techniques for Reducing Social Bias in LLMs through System 1 and System 2 Cognitive Processes](https://arxiv.org/abs/2404.17218)
Append: [Seamless Language Expansion: Enhancing Multilingual Mastery in Self-Supervised Models](https://arxiv.org/abs/2406.14092)
Append: [Sentiment Reasoning for Healthcare](https://arxiv.org/abs/2407.21054)
Append: [PublicHearingBR: A Brazilian Portuguese Dataset of Public Hearing Transcripts for Summarization of Long Documents](https://arxiv.org/abs/2410.07495)
Append: [Do LLMs write like humans? Variation in grammatical and rhetorical styles](https://arxiv.org/abs/2410.16107)
Append: [Establishing Task Scaling Laws via Compute-Efficient Model Ladders](https://arxiv.org/abs/2412.04403)
Append: [MINTQA: A Multi-Hop Question Answering Benchmark for Evaluating LLMs on New and Tail Knowledge](https://arxiv.org/abs/2412.17032)
Append: [Can Hallucinations Help? Boosting LLMs for Drug Discovery](https://arxiv.org/abs/2501.13824)
Append: [Towards Privacy-aware Mental Health AI Models: Advances, Challenges, and Opportunities](https://arxiv.org/abs/2502.00451)
Append: [Ask Patients with Patience: Enabling LLMs for Human-Centric Medical Dialogue with Grounded Reasoning](https://arxiv.org/abs/2502.07143)
Append: [Top-Theta Attention: Sparsifying Transformers by Compensated Thresholding](https://arxiv.org/abs/2502.08363)
Append: [NitiBench: A Comprehensive Study of LLM Framework Capabilities for Thai Legal Question Answering](https://arxiv.org/abs/2502.10868)
Append: [Soteria: Language-Specific Functional Parameter Steering for Multilingual Safety Alignment](https://arxiv.org/abs/2502.11244)
Append: [Collaborative Stance Detection via Small-Large Language Model Consistency Verification](https://arxiv.org/abs/2502.19954)
Append: [from Benign import Toxic: Jailbreaking the Language Model via Adversarial Metaphors](https://arxiv.org/abs/2503.00038)
Append: [Rotary Offset Features in Large Language Models](https://arxiv.org/abs/2503.01832)
Append: [Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices](https://arxiv.org/abs/2503.10652)
Append: [Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models](https://arxiv.org/abs/2503.16419)
Append: [Is Small Language Model the Silver Bullet to Low-Resource Languages Machine Translation?](https://arxiv.org/abs/2503.24102)
Append: [Contextualize-then-Aggregate: Circuits for In-Context Learning in Gemma-2 2B](https://arxiv.org/abs/2504.00132)
Append: [MultiBLiMP 1.0: A Massively Multilingual Benchmark of Linguistic Minimal Pairs](https://arxiv.org/abs/2504.02768)
Append: [Exploration of Plan-Guided Summarization for Narrative Texts: the Case of Small Language Models](https://arxiv.org/abs/2504.09071)
Append: [DIDS: Domain Impact-aware Data Sampling for Large Language Model Training](https://arxiv.org/abs/2504.13227)
Append: [MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks](https://arxiv.org/abs/2505.03427)
Append: [DRP: Distilled Reasoning Pruning with Skill-aware Step Decomposition for Efficient Large Reasoning Models](https://arxiv.org/abs/2505.13975)
Append: [SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long Sequences](https://arxiv.org/abs/2505.20776)
Append: [QA-LIGN: Aligning LLMs through Constitutionally Decomposed QA](https://arxiv.org/abs/2506.08123)
Append: [Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms](https://arxiv.org/abs/2506.09457)
Append: [SPARE: Single-Pass Annotation with Reference-Guided Evaluation for Automatic Process Supervision and Reward Modelling](https://arxiv.org/abs/2506.15498)
Append: [A Survey of Deep Learning for Geometry Problem Solving](https://arxiv.org/abs/2507.11936)
Append: [A Toolbox, Not a Hammer -- Multi-TAG: Scaling Math Reasoning with Multi-Tool Aggregation](https://arxiv.org/abs/2507.18973)
Append: [Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and Context-Aware KGQA](https://arxiv.org/abs/2508.00719)
Append: [Parity-Aware Byte-Pair Encoding: Improving Cross-lingual Fairness in Tokenization](https://arxiv.org/abs/2508.04796)
Append: [Cyberbullying Detection via Aggression-Enhanced Prompting](https://arxiv.org/abs/2508.06360)
Append: [Enhancing Code-switched Text-to-Speech Synthesis Capability in Large Language Models with only Monolingual Corpora](https://arxiv.org/abs/2409.10969)
Append: [How Performance Pressure Influences AI-Assisted Decision Making](https://arxiv.org/abs/2410.16560)
Append: [One Example Shown, Many Concepts Known! Counterexample-Driven Conceptual Reasoning in Mathematical LLMs](https://arxiv.org/abs/2502.10454)
Append: [Leveraging LLMs for Utility-Focused Annotation: Reducing Manual Effort for Retrieval and RAG](https://arxiv.org/abs/2504.05220)
Append: [Efficient RL Training for Reasoning Models via Length-Aware Optimization](https://arxiv.org/abs/2505.12284)
Append: [CAMA: Enhancing Multimodal In-Context Learning with Context-Aware Modulated Attention](https://arxiv.org/abs/2505.17097)
Append: [PoisonSwarm: Universal Harmful Information Synthesis via Model Crowdsourcing](https://arxiv.org/abs/2505.21184)
Append: [HypER: Literature-grounded Hypothesis Generation and Distillation with Provenance](https://arxiv.org/abs/2506.12937)
Append: [Evaluating Speech-to-Text x LLM x Text-to-Speech Combinations for AI Interview Systems](https://arxiv.org/abs/2507.16835)
append_entries: 150
Finish: 2025-08-25 04:30:41.623010
------------------------------------------------------
Started: 2025-08-25 06:27:35.859511
Existing_entries: 1150
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1790
Summarized using GPT-3.5-turbo
Append: [ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability](https://arxiv.org/abs/2508.07050)
append_entries: 1
Finish: 2025-08-25 06:27:38.608643
------------------------------------------------------
Started: 2025-08-25 08:24:08.064822
Existing_entries: 1001
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-25 08:24:08.438649
------------------------------------------------------
Started: 2025-08-25 10:19:02.727772
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-25 10:19:03.141991
------------------------------------------------------
Started: 2025-08-25 12:35:20.800026
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-25 12:35:21.203002
------------------------------------------------------
Started: 2025-08-25 14:17:27.426274
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-25 14:17:27.834793
------------------------------------------------------
Started: 2025-08-25 16:20:46.436563
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-25 16:20:46.867322
------------------------------------------------------
Started: 2025-08-25 18:24:41.463281
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-25 18:24:41.881982
------------------------------------------------------
Started: 2025-08-25 20:18:23.738788
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-25 20:18:24.080124
------------------------------------------------------
Started: 2025-08-25 22:15:57.587954
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-25 22:15:57.924882
------------------------------------------------------
Started: 2025-08-26 01:17:47.575297
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-26 01:17:47.974142
------------------------------------------------------
Started: 2025-08-26 03:06:54.472488
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-26 03:06:54.812864
------------------------------------------------------
Started: 2025-08-26 04:22:07.485311
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1601
Summarized using GPT-3.5-turbo
Append: [GreenTEA: Gradient Descent with Topic-modeling and Evolutionary Auto-prompting](https://arxiv.org/abs/2508.16603)
Token length: 1383
Summarized using GPT-3.5-turbo
Append: [Cognitive Decision Routing in Large Language Models: When to Think Fast, When to Think Slow](https://arxiv.org/abs/2508.16636)
Token length: 1350
Summarized using GPT-3.5-turbo
Append: [Trust but Verify! A Survey on Verification Design for Test-time Scaling](https://arxiv.org/abs/2508.16665)
Token length: 1508
Summarized using GPT-3.5-turbo
Append: [Do Cognitively Interpretable Reasoning Traces Improve LLM Performance?](https://arxiv.org/abs/2508.16695)
Token length: 1668
Summarized using GPT-3.5-turbo
Append: [QueryBandits for Hallucination Mitigation: Exploiting Semantic Features for No-Regret Rewriting](https://arxiv.org/abs/2508.16697)
Token length: 1066
Summarized using GPT-3.5-turbo
Append: [Assessing Consciousness-Related Behaviors in Large Language Models Using the Maze Test](https://arxiv.org/abs/2508.16705)
Token length: 1448
Summarized using GPT-3.5-turbo
Append: [Sparse and Dense Retrievers Learn Better Together: Joint Sparse-Dense Optimization for Text-Image Retrieval](https://arxiv.org/abs/2508.16707)
Token length: 1368
Summarized using GPT-3.5-turbo
Append: [Error Reflection Prompting: Can Large Language Models Successfully Understand Errors?](https://arxiv.org/abs/2508.16729)
Token length: 1636
Summarized using GPT-3.5-turbo
Append: [GAICo: A Deployed and Extensible Framework for Evaluating Diverse and Multimodal Generative AI Outputs](https://arxiv.org/abs/2508.16753)
Token length: 1431
Summarized using GPT-3.5-turbo
Append: [How Good are LLM-based Rerankers? An Empirical Analysis of State-of-the-Art Reranking Models](https://arxiv.org/abs/2508.16757)
Token length: 1566
Summarized using GPT-3.5-turbo
Append: [Toward Socially Aware Vision-Language Models: Evaluating Cultural Competence Through Multimodal Story Generation](https://arxiv.org/abs/2508.16762)
Token length: 1391
Summarized using GPT-3.5-turbo
Append: [Assess and Prompt: A Generative RL Framework for Improving Engagement in Online Mental Health Communities](https://arxiv.org/abs/2508.16788)
Token length: 1394
Summarized using GPT-3.5-turbo
Append: [ReProCon: Scalable and Resource-Efficient Few-Shot Biomedical Named Entity Recognition](https://arxiv.org/abs/2508.16833)
Token length: 937
Summarized using GPT-3.5-turbo
Append: [LLMs Learn Constructions That Humans Do Not Know](https://arxiv.org/abs/2508.16837)
Token length: 868
Summarized using GPT-3.5-turbo
Append: [If We May De-Presuppose: Robustly Verifying Claims through Presupposition-Free Question Decomposition](https://arxiv.org/abs/2508.16838)
Token length: 1540
Summarized using GPT-3.5-turbo
Append: [Learning from Diverse Reasoning Paths with Routing and Collaboration](https://arxiv.org/abs/2508.16861)
Token length: 1529
Summarized using GPT-3.5-turbo
Append: [QFrCoLA: a Quebec-French Corpus of Linguistic Acceptability Judgments](https://arxiv.org/abs/2508.16867)
Token length: 1028
Summarized using GPT-3.5-turbo
Append: [JUDGEBERT: Assessing Legal Meaning Preservation Between Sentences](https://arxiv.org/abs/2508.16870)
Token length: 1085
Summarized using GPT-3.5-turbo
Append: [Dream to Chat: Model-based Reinforcement Learning on Dialogues with User Belief Modeling](https://arxiv.org/abs/2508.16876)
Token length: 1659
Summarized using GPT-3.5-turbo
Append: [ObjexMT: Objective Extraction and Metacognitive Calibration for LLM-as-a-Judge under Multi-Turn Jailbreaks](https://arxiv.org/abs/2508.16889)
Token length: 1361
Summarized using GPT-3.5-turbo
Append: [Unbiased Reasoning for Knowledge-Intensive Tasks in Large Language Models via Conditional Front-Door Adjustment](https://arxiv.org/abs/2508.16910)
Token length: 1647
Summarized using GPT-3.5-turbo
Append: [Being Kind Isn't Always Being Safe: Diagnosing Affective Hallucination in LLMs](https://arxiv.org/abs/2508.16921)
Token length: 1266
Summarized using GPT-3.5-turbo
Append: [Explaining Black-box Language Models with Knowledge Probing Systems: A Post-hoc Explanation Perspective](https://arxiv.org/abs/2508.16969)
Token length: 1476
Summarized using GPT-3.5-turbo
Append: [Decoding Alignment: A Critical Survey of LLM Development Initiatives through Value-setting and Data-centric Lens](https://arxiv.org/abs/2508.16982)
Token length: 1383
Summarized using GPT-3.5-turbo
Append: [ReFactX: Scalable Reasoning with Reliable Facts via Constrained Generation](https://arxiv.org/abs/2508.16983)
Token length: 1350
Summarized using GPT-3.5-turbo
Append: [GRADE: Generating multi-hop QA and fine-gRAined Difficulty matrix for RAG Evaluation](https://arxiv.org/abs/2508.16994)
Token length: 1539
Summarized using GPT-3.5-turbo
Append: [DeAR: Dual-Stage Document Reranking with Reasoning Agents via LLM Distillation](https://arxiv.org/abs/2508.16998)
Token length: 883
Summarized using GPT-3.5-turbo
Append: [KL-Regularised Q-Learning: A Token-level Action-Value perspective on Online RLHF](https://arxiv.org/abs/2508.17000)
Token length: 1189
Summarized using GPT-3.5-turbo
Append: [Planning for Success: Exploring LLM Long-term Planning Capabilities in Table Understanding](https://arxiv.org/abs/2508.17005)
Token length: 1848
Summarized using GPT-3.5-turbo
Append: [EduRABSA: An Education Review Dataset for Aspect-based Sentiment Analysis Tasks](https://arxiv.org/abs/2508.17008)
Token length: 1126
Summarized using GPT-3.5-turbo
Append: [Improving Table Understanding with LLMs and Entity-Oriented Search](https://arxiv.org/abs/2508.17028)
Token length: 842
Summarized using GPT-3.5-turbo
Append: [GRAID: Synthetic Data Generation with Geometric Constraints and Multi-Agentic Reflection for Harmful Content Detection](https://arxiv.org/abs/2508.17057)
Token length: 1206
Summarized using GPT-3.5-turbo
Append: [Linguistic Neuron Overlap Patterns to Facilitate Cross-lingual Transfer on Low-resource Languages](https://arxiv.org/abs/2508.17078)
Token length: 660
Summarized using GPT-3.5-turbo
Append: [Token Homogenization under Positional Bias](https://arxiv.org/abs/2508.17126)
Token length: 1150
Summarized using GPT-3.5-turbo
Append: [A Straightforward Pipeline for Targeted Entailment and Contradiction Detection](https://arxiv.org/abs/2508.17127)
Token length: 892
Summarized using GPT-3.5-turbo
Append: [The Power of Framing: How News Headlines Guide Search Behavior](https://arxiv.org/abs/2508.17131)
Token length: 983
Summarized using GPT-3.5-turbo
Append: [Geolocation-Aware Robust Spoken Language Identification](https://arxiv.org/abs/2508.17148)
Token length: 1120
Summarized using GPT-3.5-turbo
Append: [Natural Language Satisfiability: Exploring the Problem Distribution and Evaluating Transformer-based Language Models](https://arxiv.org/abs/2508.17153)
Token length: 904
Summarized using GPT-3.5-turbo
Append: [SPORTSQL: An Interactive System for Real-Time Sports Reasoning and Visualization](https://arxiv.org/abs/2508.17157)
Token length: 984
Summarized using GPT-3.5-turbo
Append: [Quantifying Language Disparities in Multilingual Large Language Models](https://arxiv.org/abs/2508.17162)
Token length: 1084
Summarized using GPT-3.5-turbo
Append: [The Impact of Annotator Personas on LLM Behavior Across the Perspectivism Spectrum](https://arxiv.org/abs/2508.17164)
Token length: 1531
Summarized using GPT-3.5-turbo
Append: [Towards Alignment-Centric Paradigm: A Survey of Instruction Tuning in Large Language Models](https://arxiv.org/abs/2508.17184)
Token length: 1202
Summarized using GPT-3.5-turbo
Append: [Active Domain Knowledge Acquisition with \$100 Budget: Enhancing LLMs via Cost-Efficient, Expert-Involved Interaction in Sensitive Domains](https://arxiv.org/abs/2508.17202)
Token length: 1502
Summarized using GPT-3.5-turbo
Append: [SSFO: Self-Supervised Faithfulness Optimization for Retrieval-Augmented Generation](https://arxiv.org/abs/2508.17225)
Token length: 1126
Summarized using GPT-3.5-turbo
Append: [ClaimGen-CN: A Large-scale Chinese Dataset for Legal Claim Generation](https://arxiv.org/abs/2508.17234)
Token length: 1579
Summarized using GPT-3.5-turbo
Append: [Routing Distilled Knowledge via Mixture of LoRA Experts for Large Language Model based Bundle Generation](https://arxiv.org/abs/2508.17250)
Token length: 1045
Summarized using GPT-3.5-turbo
Append: [Are You Sure You're Positive? Consolidating Chain-of-Thought Agents with Uncertainty Quantification for Aspect-Category Sentiment Analysis](https://arxiv.org/abs/2508.17258)
Token length: 1457
Summarized using GPT-3.5-turbo
Append: [From Language to Action: A Review of Large Language Models as Autonomous Agents and Tool Users](https://arxiv.org/abs/2508.17281)
Token length: 1098
Summarized using GPT-3.5-turbo
Append: [Handling Students Dropouts in an LLM-driven Interactive Online Course Using Language Models](https://arxiv.org/abs/2508.17310)
Token length: 872
Summarized using GPT-3.5-turbo
Append: [CultranAI at PalmX 2025: Data Augmentation for Cultural Knowledge Representation](https://arxiv.org/abs/2508.17324)
Append: [Omne-R1: Learning to Reason with Memory for Multi-hop Question Answering](https://arxiv.org/abs/2508.17330)
Append: [DropLoRA: Sparse Low-Rank Adaptation for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2508.17337)
Append: [Capturing Legal Reasoning Paths from Facts to Law in Court Judgments using Knowledge Graphs](https://arxiv.org/abs/2508.17340)
Append: [The Arabic Generality Score: Another Dimension of Modeling Arabic Dialectness](https://arxiv.org/abs/2508.17347)
Append: [UI-Level Evaluation of ALLaM 34B: Measuring an Arabic-Centric LLM via HUMAIN Chat](https://arxiv.org/abs/2508.17378)
Append: [Agent-Testing Agent: A Meta-Agent for Automated Testing and Evaluation of Conversational AI Agents](https://arxiv.org/abs/2508.17393)
Append: [DashboardQA: Benchmarking Multimodal Agents for Question Answering on Interactive Dashboards](https://arxiv.org/abs/2508.17398)
Append: [DS@GT at CheckThat! 2025: A Simple Retrieval-First, LLM-Backed Framework for Claim Normalization](https://arxiv.org/abs/2508.17402)
Append: [MahaParaphrase: A Marathi Paraphrase Detection Corpus and BERT-based Models](https://arxiv.org/abs/2508.17444)
Append: [Persuasion Dynamics in LLMs: Investigating Robustness and Adaptability in Knowledge and Safety with DuET-PD](https://arxiv.org/abs/2508.17450)
Append: [Evaluating the Impact of Verbal Multiword Expressions on Machine Translation](https://arxiv.org/abs/2508.17458)
Append: [Efficient Zero-Shot Long Document Classification by Reducing Context Through Sentence Ranking](https://arxiv.org/abs/2508.17490)
Append: [Improving French Synthetic Speech Quality via SSML Prosody Control](https://arxiv.org/abs/2508.17494)
Append: [Debate or Vote: Which Yields Better Decisions in Multi-Agent Large Language Models?](https://arxiv.org/abs/2508.17536)
Append: [Humanizing Machines: Rethinking LLM Anthropomorphism Through a Multi-Level Framework of Design](https://arxiv.org/abs/2508.17573)
Append: [CausalSent: Interpretable Sentiment Classification with RieszNet](https://arxiv.org/abs/2508.17576)
Append: [UQ: Assessing Language Models on Unsolved Questions](https://arxiv.org/abs/2508.17580)
Append: [Less Is More? Examining Fairness in Pruned Large Language Models for Summarising Opinions](https://arxiv.org/abs/2508.17610)
Append: [Steering When Necessary: Flexible Steering Large Language Models with Backtracking](https://arxiv.org/abs/2508.17621)
Append: [EMO-Reasoning: Benchmarking Emotional Reasoning Capabilities in Spoken Dialogue Systems](https://arxiv.org/abs/2508.17623)
Append: [Stop Spinning Wheels: Mitigating LLM Overthinking via Mining Patterns for Early Reasoning Exit](https://arxiv.org/abs/2508.17627)
Append: [Weights-Rotated Preference Optimization for Large Language Models](https://arxiv.org/abs/2508.17637)
Append: [SurveyGen: Quality-Aware Scientific Survey Generation with Large Language Models](https://arxiv.org/abs/2508.17647)
Append: [CoCoA: Confidence- and Context-Aware Adaptive Decoding for Resolving Knowledge Conflicts in Large Language Models](https://arxiv.org/abs/2508.17670)
Append: [Text Meets Topology: Rethinking Out-of-distribution Detection in Text-Rich Networks](https://arxiv.org/abs/2508.17690)
Append: [EMPOWER: Evolutionary Medical Prompt Optimization With Reinforcement Learning](https://arxiv.org/abs/2508.17703)
Append: [Layerwise Importance Analysis of Feed-Forward Networks in Transformer-based Language Models](https://arxiv.org/abs/2508.17734)
Append: [SMITE: Enhancing Fairness in LLMs through Optimal In-Context Example Selection via Dynamic Validation](https://arxiv.org/abs/2508.17735)
Append: [ISACL: Internal State Analyzer for Copyrighted Training Data Leakage](https://arxiv.org/abs/2508.17767)
Append: [Speculating LLMs' Chinese Training Data Pollution from Their Tokens](https://arxiv.org/abs/2508.17771)
Append: [Zero-shot Context Biasing with Trie-based Decoding using Synthetic Multi-Pronunciation](https://arxiv.org/abs/2508.17796)
Append: [DRQA: Dynamic Reasoning Quota Allocation for Controlling Overthinking in Reasoning Large Language Models](https://arxiv.org/abs/2508.17803)
Append: [Beyond Demographics: Enhancing Cultural Value Survey Simulation with Multi-Stage Personality-Driven Cognitive Reasoning](https://arxiv.org/abs/2508.17855)
Append: [Speech Discrete Tokens or Continuous Features? A Comparative Analysis for Spoken Language Understanding in SpeechLLMs](https://arxiv.org/abs/2508.17863)
Append: [ILRe: Intermediate Layer Retrieval for Context Compression in Causal Language Models](https://arxiv.org/abs/2508.17892)
Append: [Pandora: Leveraging Code-driven Knowledge Transfer for Unified Structured Knowledge Reasoning](https://arxiv.org/abs/2508.17905)
Append: [Evaluating the Representation of Vowels in Wav2Vec Feature Extractor: A Layer-Wise Analysis Using MFCCs](https://arxiv.org/abs/2508.17914)
Append: [Information availability in different languages and various technological constraints related to multilinguism on the Internet](https://arxiv.org/abs/2508.17918)
Append: [Feature-Refined Unsupervised Model for Loanword Detection](https://arxiv.org/abs/2508.17923)
Append: [AMELIA: A Family of Multi-task End-to-end Language Models for Argumentation](https://arxiv.org/abs/2508.17926)
Append: [Debiasing Multilingual LLMs in Cross-lingual Latent Space](https://arxiv.org/abs/2508.17948)
Append: [Understanding Subword Compositionality of Large Language Models](https://arxiv.org/abs/2508.17953)
Append: [German4All - A Dataset and Model for Readability-Controlled Paraphrasing in German](https://arxiv.org/abs/2508.17973)
Append: [A Retail-Corpus for Aspect-Based Sentiment Analysis with Large Language Models](https://arxiv.org/abs/2508.17994)
Append: [Neither Valid nor Reliable? Investigating the Use of LLMs as Judges](https://arxiv.org/abs/2508.18076)
Append: [How Quantization Shapes Bias in Large Language Models](https://arxiv.org/abs/2508.18088)
Append: [Speech-Based Depressive Mood Detection in the Presence of Multiple Sclerosis: A Cross-Corpus and Cross-Lingual Study](https://arxiv.org/abs/2508.18092)
Append: [Agri-Query: A Case Study on RAG vs. Long-Context LLMs for Cross-Lingual Technical Question Answering](https://arxiv.org/abs/2508.18093)
Append: [Detecting and Characterizing Planning in Language Models](https://arxiv.org/abs/2508.18098)
Append: [SentiMM: A Multimodal Multi-Agent Framework for Sentiment Analysis in Social Media](https://arxiv.org/abs/2508.18108)
Append: [Toward a Better Localization of Princeton WordNet](https://arxiv.org/abs/2508.18134)
Append: [S2Sent: Nested Selectivity Aware Sentence Representation Learning](https://arxiv.org/abs/2508.18164)
Append: [DiscussLLM: Teaching Large Language Models When to Speak](https://arxiv.org/abs/2508.18167)
Append: [Improving End-to-End Training of Retrieval-Augmented Generation Models via Joint Stochastic Approximation](https://arxiv.org/abs/2508.18168)
Append: [Leveraging Large Language Models for Accurate Sign Language Translation in Low-Resource Scenarios](https://arxiv.org/abs/2508.18183)
Append: [Exploring the Interplay between Musical Preferences and Personality through the Lens of Language](https://arxiv.org/abs/2508.18208)
Append: [Why Synthetic Isn't Real Yet: A Diagnostic Framework for Contact Center Dialogue Generation](https://arxiv.org/abs/2508.18210)
Append: [Better Language Model-Based Judging Reward Modeling through Scaling Comprehension Boundaries](https://arxiv.org/abs/2508.18212)
Append: [MTalk-Bench: Evaluating Speech-to-Speech Models in Multi-Turn Dialogues via Arena-style and Rubrics Protocols](https://arxiv.org/abs/2508.18240)
Append: [Demographic Biases and Gaps in the Perception of Sexism in Large Language Models](https://arxiv.org/abs/2508.18245)
Append: [From BERT to LLMs: Comparing and Understanding Chinese Classifier Prediction in Language Models](https://arxiv.org/abs/2508.18253)
Append: [MIRAGE: Scaling Test-Time Inference with Parallel Graph-Retrieval-Augmented Reasoning Chains](https://arxiv.org/abs/2508.18260)
Append: [Humans Perceive Wrong Narratives from AI Reasoning Texts](https://arxiv.org/abs/2508.16599)
Append: [Learn to Memorize: Optimizing LLM-based Agents with Adaptive Memory Framework](https://arxiv.org/abs/2508.16629)
Append: [Empirical Analysis of the Effect of Context in the Task of Automated Essay Scoring in Transformer-Based Models](https://arxiv.org/abs/2508.16638)
Append: [Leveraging Multi-Source Textural UGC for Neighbourhood Housing Quality Assessment: A GPT-Enhanced Framework](https://arxiv.org/abs/2508.16657)
Append: [Invisible Filters: Cultural Bias in Hiring Evaluations Using Large Language Models](https://arxiv.org/abs/2508.16673)
Append: [MedRepBench: A Comprehensive Benchmark for Medical Report Interpretation](https://arxiv.org/abs/2508.16674)
Append: [WISCA: A Lightweight Model Transition Method to Improve LLM Training via Weight Scaling](https://arxiv.org/abs/2508.16676)
Append: [Recall-Extend Dynamics: Enhancing Small Language Models through Controlled Exploration and Refined Offline Integration](https://arxiv.org/abs/2508.16677)
Append: [Revisiting Rule-Based Stuttering Detection: A Comprehensive Analysis of Interpretable Models for Clinical Applications](https://arxiv.org/abs/2508.16681)
Append: [Hyperbolic Multimodal Representation Learning for Biological Taxonomies](https://arxiv.org/abs/2508.16744)
Append: [Guarding Your Conversations: Privacy Gatekeepers for Secure Interactions with Cloud-Based AI Models](https://arxiv.org/abs/2508.16765)
Append: [Interpreting the Effects of Quantization on LLMs](https://arxiv.org/abs/2508.16785)
Append: [Quantifying Sycophancy as Deviations from Bayesian Rationality in LLMs](https://arxiv.org/abs/2508.16846)
Append: [Attention Layers Add Into Low-Dimensional Residual Subspaces](https://arxiv.org/abs/2508.16929)
Append: [THEME : Enhancing Thematic Investing with Semantic Stock Representations and Temporal Dynamics](https://arxiv.org/abs/2508.16936)
Append: [RephraseTTS: Dynamic Length Text based Speech Insertion with Speaker Style Transfer](https://arxiv.org/abs/2508.17031)
Append: [Anemoi: A Semi-Centralized Multi-agent Systems Based on Agent-to-Agent Communication MCP server from Coral Protocol](https://arxiv.org/abs/2508.17068)
Append: [LLM Assertiveness can be Mechanistically Decomposed into Emotional and Logical Components](https://arxiv.org/abs/2508.17182)
Append: [Multi-Agent Visual-Language Reasoning for Comprehensive Highway Scene Understanding](https://arxiv.org/abs/2508.17205)
Append: [CoViPAL: Layer-wise Contextualized Visual Token Pruning for Large Vision-Language Models](https://arxiv.org/abs/2508.17243)
Append: [Mind the (Language) Gap: Towards Probing Numerical and Cross-Lingual Limits of LVLMs](https://arxiv.org/abs/2508.17334)
Append: [Large Language Models as Universal Predictors? An Empirical Study on Small Tabular Datasets](https://arxiv.org/abs/2508.17391)
Append: [TreePO: Bridging the Gap of Policy Optimization and Efficacy and Inference Efficiency with Heuristic Tree-based Modeling](https://arxiv.org/abs/2508.17445)
Append: [Activation Transport Operators](https://arxiv.org/abs/2508.17540)
Append: [RubikSQL: Lifelong Learning Agentic Knowledge Base as an Industrial NL2SQL System](https://arxiv.org/abs/2508.17590)
Append: [Dynamic Embedding of Hierarchical Visual Features for Efficient Vision-Language Fine-Tuning](https://arxiv.org/abs/2508.17638)
Append: [Characterizing the Behavior of Training Mamba-based State Space Models on GPUs](https://arxiv.org/abs/2508.17679)
Append: [LLM-based Agentic Reasoning Frameworks: A Survey from Methods to Scenarios](https://arxiv.org/abs/2508.17692)
Append: [How Do LLM-Generated Texts Impact Term-Based Retrieval Models?](https://arxiv.org/abs/2508.17715)
Append: [Talking to Robots: A Practical Examination of Speech Foundation Models for HRI Applications](https://arxiv.org/abs/2508.17753)
Append: [CEIDM: A Controlled Entity and Interaction Diffusion Model for Enhanced Text-to-Image Generation](https://arxiv.org/abs/2508.17760)
Append: [Proximal Supervised Fine-Tuning](https://arxiv.org/abs/2508.17784)
Append: [Designing Practical Models for Isolated Word Visual Speech Recognition](https://arxiv.org/abs/2508.17894)
Append: [Unseen Speaker and Language Adaptation for Lightweight Text-To-Speech with Adapters](https://arxiv.org/abs/2508.18006)
Append: [Named Entity Recognition of Historical Text via Large Language Model](https://arxiv.org/abs/2508.18090)
Append: [The AI Data Scientist](https://arxiv.org/abs/2508.18113)
Append: [HLLM-Creator: Hierarchical LLM-based Personalized Creative Generation](https://arxiv.org/abs/2508.18118)
Append: [Unraveling the cognitive patterns of Large Language Models through module communities](https://arxiv.org/abs/2508.18192)
Append: [Can AI Have a Personality? Prompt Engineering for AI Personality Simulation: A Chatbot Case Study in Gender-Affirming Voice Therapy Training](https://arxiv.org/abs/2508.18234)
Append: [Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models](https://arxiv.org/abs/2308.15022)
Append: [Does GPT-4 surpass human performance in linguistic pragmatics?](https://arxiv.org/abs/2312.09545)
Append: [Rethinking Cross-Subject Data Splitting for Brain-to-Text Decoding](https://arxiv.org/abs/2312.10987)
Append: [Backdoor Attacks on Dense Retrieval via Public and Unintentional Triggers](https://arxiv.org/abs/2402.13532)
Append: [Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining](https://arxiv.org/abs/2403.04780)
Append: [Large Language Models Meet NLP: A Survey](https://arxiv.org/abs/2405.12819)
Append: [ComplexTempQA:A 100m Dataset for Complex Temporal Question Answering](https://arxiv.org/abs/2406.04866)
Append: [Evaluating $n$-Gram Novelty of Language Models Using Rusty-DAWG](https://arxiv.org/abs/2406.13069)
Append: [A Factuality and Diversity Reconciled Decoding Method for Knowledge-Grounded Dialogue Generation](https://arxiv.org/abs/2407.05718)
Append: [Orthogonal Finetuning for Direct Preference Optimization](https://arxiv.org/abs/2409.14836)
Append: [Localizing Factual Inconsistencies in Attributable Text Generation](https://arxiv.org/abs/2410.07473)
Append: [SensorLLM: Aligning Large Language Models with Motion Sensors for Human Activity Recognition](https://arxiv.org/abs/2410.10624)
Append: [Draft Model Knows When to Stop: Self-Verification Speculative Decoding for Long-Form Generation](https://arxiv.org/abs/2411.18462)
Append: [Towards Controllable Speech Synthesis in the Era of Large Language Models: A Systematic Survey](https://arxiv.org/abs/2412.06602)
Append: [DRT: Deep Reasoning Translation via Long Chain-of-Thought](https://arxiv.org/abs/2412.17498)
Append: [Harnessing Large Language Models for Disaster Management: A Survey](https://arxiv.org/abs/2501.06932)
Append: [SCP-116K: A High-Quality Problem-Solution Dataset and a Generalized Pipeline for Automated Extraction in the Higher Education Science Domain](https://arxiv.org/abs/2501.15587)
Append: [Evaluation of Large Language Models via Coupled Token Generation](https://arxiv.org/abs/2502.01754)
Append: [Investigating the Robustness of Deductive Reasoning with Large Language Models](https://arxiv.org/abs/2502.04352)
Append: [EmoBench-M: Benchmarking Emotional Intelligence for Multimodal Large Language Models](https://arxiv.org/abs/2502.04424)
Append: [Head-Specific Intervention Can Induce Misaligned AI Coordination in Large Language Models](https://arxiv.org/abs/2502.05945)
Append: [Playing with Voices: Tabletop Role-Playing Game Recordings as a Diarization Challenge](https://arxiv.org/abs/2502.12714)
Append: [Trust Me, I'm Wrong: LLMs Hallucinate with Certainty Despite Knowing the Answer](https://arxiv.org/abs/2502.12964)
Append: [Control Illusion: The Failure of Instruction Hierarchies in Large Language Models](https://arxiv.org/abs/2502.15851)
Append: [Detecting Knowledge Boundary of Vision Large Language Models by Sampling-Based Inference](https://arxiv.org/abs/2502.18023)
Append: [Steering Dialogue Dynamics for Robustness against Multi-turn Jailbreaking Attacks](https://arxiv.org/abs/2503.00187)
Append: [Seeing Sarcasm Through Different Eyes: Analyzing Multimodal Sarcasm Perception in Large Vision-Language Models](https://arxiv.org/abs/2503.12149)
Append: [OpenHuEval: Evaluating Large Language Model on Hungarian Specifics](https://arxiv.org/abs/2503.21500)
Append: [ImF: Implicit Fingerprint for Large Language Models](https://arxiv.org/abs/2503.21805)
Append: [Post-Training Language Models for Continual Relation Extraction](https://arxiv.org/abs/2504.05214)
Append: [Unified attacks to large language model watermarks: spoofing and scrubbing in unauthorized knowledge distillation](https://arxiv.org/abs/2504.17480)
Append: [Theory of Mind in Large Language Models: Assessment and Enhancement](https://arxiv.org/abs/2505.00026)
Append: [A Factorized Probabilistic Model of the Semantics of Vague Temporal Adverbials Relative to Different Event Types](https://arxiv.org/abs/2505.01311)
Append: [A Survey on the Safety and Security Threats of Computer-Using Agents: JARVIS or Ultron?](https://arxiv.org/abs/2505.10924)
Append: [From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way Parallel Corpora](https://arxiv.org/abs/2505.14045)
Append: [sudoLLM: On Multi-role Alignment of Language Models](https://arxiv.org/abs/2505.14607)
Append: [Traveling Across Languages: Benchmarking Cross-Lingual Consistency in Multimodal LLMs](https://arxiv.org/abs/2505.15075)
Append: [IRONIC: Coherence-Aware Reasoning Chains for Multi-Modal Sarcasm Detection](https://arxiv.org/abs/2505.16258)
Append: [Exploring the Vulnerability of the Content Moderation Guardrail in Large Language Models via Intent Manipulation](https://arxiv.org/abs/2505.18556)
Append: [Debate-to-Detect: Reformulating Misinformation Detection as a Real-World Debate with Large Language Models](https://arxiv.org/abs/2505.18596)
Append: [Large Language Models in the Task of Automatic Validation of Text Classifier Predictions](https://arxiv.org/abs/2505.18688)
Append: [DecisionFlow: Advancing Large Language Model as Principled Decision Maker](https://arxiv.org/abs/2505.21397)
Append: [Words Like Knives: Backstory-Personalized Modeling and Detection of Violent Communication](https://arxiv.org/abs/2505.21451)
Append: [Self-Correcting Code Generation Using Small Language Models](https://arxiv.org/abs/2505.23060)
Append: [Measuring Sycophancy of Language Models in Multi-turn Dialogues](https://arxiv.org/abs/2505.23840)
Append: [Auto prompt sql: a resource-efficient architecture for text-to-sql translation in constrained environments](https://arxiv.org/abs/2506.03598)
Append: [AudioLens: A Closer Look at Auditory Attribute Perception of Large Audio-Language Models](https://arxiv.org/abs/2506.05140)
Append: [Automatic Speech Recognition of African American English: Lexical and Contextual Effects](https://arxiv.org/abs/2506.06888)
Append: [Reasoning with RAGged events: RAG-Enhanced Event Knowledge Base Construction and reasoning with proof-assistants](https://arxiv.org/abs/2506.07042)
Append: [From Legal Texts to Defeasible Deontic Logic via LLMs: A Study in Automated Semantic Analysis](https://arxiv.org/abs/2506.08899)
Append: [CoLMbo: Speaker Language Model for Descriptive Profiling](https://arxiv.org/abs/2506.09375)
Append: [Modeling Probabilistic Reduction using Information Theory and Naive Discriminative Learning](https://arxiv.org/abs/2506.09641)
Append: [BiMark: Unbiased Multilayer Watermarking for Large Language Models](https://arxiv.org/abs/2506.21602)
Append: [Evaluating Scoring Bias in LLM-as-a-Judge](https://arxiv.org/abs/2506.22316)
Append: [FlexOlmo: Open Language Models for Flexible Data Use](https://arxiv.org/abs/2507.07024)
Append: [GoalfyMax: A Protocol-Driven Multi-Agent System for Intelligent Experience Entities](https://arxiv.org/abs/2507.09497)
Append: [CRABS: A syntactic-semantic pincer strategy for bounding LLM interpretation of Python notebooks](https://arxiv.org/abs/2507.11742)
Append: [QuestA: Expanding Reasoning Capacity in LLMs via Question Augmentation](https://arxiv.org/abs/2507.13266)
Append: [Adaptive Linguistic Prompting (ALP) Enhances Phishing Webpage Detection in Multimodal Large Language Models](https://arxiv.org/abs/2507.13357)
Append: [Zero-shot OCR Accuracy of Low-Resourced Languages: A Comparative Analysis on Sinhala and Tamil](https://arxiv.org/abs/2507.18264)
Append: [Trusted Knowledge Extraction for Operations and Maintenance Intelligence](https://arxiv.org/abs/2507.22935)
Append: [PARROT: An Open Multilingual Radiology Reports Dataset](https://arxiv.org/abs/2507.22939)
Append: [Agentic large language models improve retrieval-based radiology question answering](https://arxiv.org/abs/2508.00743)
Append: [CultureGuard: Towards Culturally-Aware Dataset and Guard Model for Multilingual Safety Applications](https://arxiv.org/abs/2508.01710)
Append: [Jinx: Unlimited LLMs for Probing Alignment Failures](https://arxiv.org/abs/2508.08243)
Append: [PII-Compass: Guiding LLM training data extraction prompts towards the target PII via grounding](https://arxiv.org/abs/2407.02943)
Append: [Defending against Jailbreak through Early Exit Generation of Large Language Models](https://arxiv.org/abs/2408.11308)
Append: [Fingerprint Vector: Enabling Scalable and Efficient Model Fingerprint Transfer via Vector Addition](https://arxiv.org/abs/2409.08846)
Append: [Using LLM for Real-Time Transcription and Summarization of Doctor-Patient Interactions into ePuskesmas in Indonesia: A Proof-of-Concept Study](https://arxiv.org/abs/2409.17054)
Append: [Confidential Prompting: Privacy-preserving LLM Inference on Cloud](https://arxiv.org/abs/2409.19134)
Append: [LLM-Forest: Ensemble Learning of LLMs with Graph-Augmented Prompts for Data Imputation](https://arxiv.org/abs/2410.21520)
Append: [TOMATO: Assessing Visual Temporal Reasoning Capabilities in Multimodal Foundation Models](https://arxiv.org/abs/2410.23266)
Append: [Leveraging the Power of MLLMs for Gloss-Free Sign Language Translation](https://arxiv.org/abs/2411.16789)
Append: [HeteroTune: Efficient Federated Learning for Large Heterogeneous Models](https://arxiv.org/abs/2411.16796)
Append: [Missing Melodies: AI Music Generation and its "Nearly" Complete Omission of the Global South](https://arxiv.org/abs/2412.04100)
Append: [AutoDCWorkflow: LLM-based Data Cleaning Workflow Auto-Generation and Benchmark](https://arxiv.org/abs/2412.06724)
Append: [Towards New Benchmark for AI Alignment & Sentiment Analysis in Socially Important Issues: A Comparative Study of Human and LLMs in the Context of AGI](https://arxiv.org/abs/2501.02531)
Append: [Disentangling Exploration of Large Language Models by Optimal Exploitation](https://arxiv.org/abs/2501.08925)
Append: [TombRaider: Entering the Vault of History to Jailbreak Large Language Models](https://arxiv.org/abs/2501.18628)
Append: [Forgotten Polygons: Multimodal Large Language Models are Shape-Blind](https://arxiv.org/abs/2502.15969)
Append: [CAARMA: Class Augmentation with Adversarial Mixup Regularization](https://arxiv.org/abs/2503.16718)
Append: [Understanding Bias Reinforcement in LLM Agents Debate](https://arxiv.org/abs/2503.16814)
Append: [Task Memory Engine (TME): Enhancing State Awareness for Multi-Step LLM Agent Tasks](https://arxiv.org/abs/2504.08525)
Append: [X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents](https://arxiv.org/abs/2504.13203)
Append: [VeriCoder: Enhancing LLM-Based RTL Code Generation through Functional Correctness Validation](https://arxiv.org/abs/2504.15659)
Append: [SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based on Speech and Audio Information](https://arxiv.org/abs/2505.13237)
Append: [WHEN TO ACT, WHEN TO WAIT: Modeling the Intent-Action Alignment Problem in Dialogue](https://arxiv.org/abs/2506.01881)
Append: [MMTU: A Massive Multi-Task Table Understanding and Reasoning Benchmark](https://arxiv.org/abs/2506.05587)
Append: [Effective Red-Teaming of Policy-Adherent Agents](https://arxiv.org/abs/2506.09600)
append_entries: 240
Finish: 2025-08-26 04:24:07.552096
------------------------------------------------------
Started: 2025-08-26 06:26:29.368606
Existing_entries: 1240
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1068
Summarized using GPT-3.5-turbo
Append: [Evaluating Contrast Localizer for Identifying Causal Units in Social & Mathematical Tasks in Language Models](https://arxiv.org/abs/2508.08276)
append_entries: 1
Finish: 2025-08-26 06:26:33.649595
------------------------------------------------------
Started: 2025-08-26 08:23:34.960902
Existing_entries: 1001
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-26 08:23:35.502521
------------------------------------------------------
Started: 2025-08-26 10:18:23.060994
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-26 10:18:23.578846
------------------------------------------------------
Started: 2025-08-26 12:36:31.077083
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-26 12:36:31.629324
------------------------------------------------------
Started: 2025-08-26 14:15:47.057690
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-26 14:15:47.649295
------------------------------------------------------
Started: 2025-08-26 16:19:59.382131
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-26 16:19:59.991838
------------------------------------------------------
Started: 2025-08-26 18:22:53.964029
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-26 18:22:54.562692
------------------------------------------------------
Started: 2025-08-26 20:18:09.212285
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-26 20:18:09.725550
------------------------------------------------------
Started: 2025-08-26 22:15:13.964712
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-26 22:15:14.479841
------------------------------------------------------
Started: 2025-08-27 01:17:37.871551
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-27 01:17:38.646856
------------------------------------------------------
Started: 2025-08-27 03:00:51.783908
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-27 03:00:52.363893
------------------------------------------------------
Started: 2025-08-27 04:25:02.048979
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1323
Summarized using GPT-3.5-turbo
Append: [Semantic Attractors and the Emergence of Meaning: Towards a Teleological Model of AGI](https://arxiv.org/abs/2508.18290)
Token length: 1399
Summarized using GPT-3.5-turbo
Append: [LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions](https://arxiv.org/abs/2508.18321)
Token length: 1418
Summarized using GPT-3.5-turbo
Append: [Not All Visitors are Bilingual: A Measurement Study of the Multilingual Web from an Accessibility Perspective](https://arxiv.org/abs/2508.18328)
Token length: 1261
Summarized using GPT-3.5-turbo
Append: [Language-Specific Layer Matters: Efficient Multilingual Enhancement for Large Vision-Language Models](https://arxiv.org/abs/2508.18381)
Token length: 1298
Summarized using GPT-3.5-turbo
Append: [Backprompting: Leveraging Synthetic Production Data for Health Advice Guardrails](https://arxiv.org/abs/2508.18384)
Token length: 1104
Summarized using GPT-3.5-turbo
Append: [Integral Transformer: Denoising Attention, Not Too Much Not Too Little](https://arxiv.org/abs/2508.18387)
Token length: 1194
Summarized using GPT-3.5-turbo
Append: [Latent Self-Consistency for Reliable Majority-Set Selection in Short- and Long-Answer Reasoning](https://arxiv.org/abs/2508.18395)
Token length: 1270
Summarized using GPT-3.5-turbo
Append: [Can Out-of-Distribution Evaluations Uncover Reliance on Shortcuts? A Case Study in Question Answering](https://arxiv.org/abs/2508.18407)
Token length: 1670
Summarized using GPT-3.5-turbo
Append: [How Reliable are LLMs for Reasoning on the Re-ranking task?](https://arxiv.org/abs/2508.18444)
Token length: 1131
Summarized using GPT-3.5-turbo
Append: [Integrating gender inclusivity into large language models via instruction tuning](https://arxiv.org/abs/2508.18466)
Token length: 716
Summarized using GPT-3.5-turbo
Append: [Principled Detection of Hallucinations in Large Language Models via Multiple Testing](https://arxiv.org/abs/2508.18473)
Token length: 1244
Summarized using GPT-3.5-turbo
Append: [COMET-poly: Machine Translation Metric Grounded in Other Candidates](https://arxiv.org/abs/2508.18549)
Token length: 1659
Summarized using GPT-3.5-turbo
Append: [The Mind's Eye: A Multi-Faceted Reward Framework for Guiding Visual Metaphor Generation](https://arxiv.org/abs/2508.18569)
Token length: 1061
Summarized using GPT-3.5-turbo
Append: [What do language models model? Transformers, automata, and the format of thought](https://arxiv.org/abs/2508.18598)
Token length: 863
Summarized using GPT-3.5-turbo
Append: [A New NMT Model for Translating Clinical Texts from English to Spanish](https://arxiv.org/abs/2508.18607)
Token length: 1223
Summarized using GPT-3.5-turbo
Append: [Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models](https://arxiv.org/abs/2508.18609)
Token length: 1374
Summarized using GPT-3.5-turbo
Append: [Thinking Before You Speak: A Proactive Test-time Scaling Approach](https://arxiv.org/abs/2508.18648)
Token length: 1440
Summarized using GPT-3.5-turbo
Append: [Breaking the Trade-Off Between Faithfulness and Expressiveness for Large Language Models](https://arxiv.org/abs/2508.18651)
Token length: 1369
Summarized using GPT-3.5-turbo
Append: [Emotion Omni: Enabling Empathetic Speech Response Generation through Large Language Models](https://arxiv.org/abs/2508.18655)
Token length: 1521
Summarized using GPT-3.5-turbo
Append: [Tailored Teaching with Balanced Difficulty: Elevating Reasoning in Multimodal Chain-of-Thought via Prompt Curriculum](https://arxiv.org/abs/2508.18673)
Token length: 1660
Summarized using GPT-3.5-turbo
Append: [Knowing or Guessing? Robust Medical Visual Question Answering via Joint Consistency and Contrastive Learning](https://arxiv.org/abs/2508.18687)
Token length: 1322
Summarized using GPT-3.5-turbo
Append: [Attention2Probability: Attention-Driven Terminology Probability Estimation for Robust Speech-to-Text System](https://arxiv.org/abs/2508.18701)
Token length: 916
Summarized using GPT-3.5-turbo
Append: [Filtering for Creativity: Adaptive Prompting for Multilingual Riddle Generation in LLMs](https://arxiv.org/abs/2508.18709)
Token length: 1102
Summarized using GPT-3.5-turbo
Append: [EMMM, Explain Me My Model! Explainable Machine Generated Text Detection in Dialogues](https://arxiv.org/abs/2508.18715)
Token length: 1249
Summarized using GPT-3.5-turbo
Append: [Beyond Quality: Unlocking Diversity in Ad Headline Generation with Large Language Models](https://arxiv.org/abs/2508.18739)
Token length: 1231
Summarized using GPT-3.5-turbo
Append: [M3HG: Multimodal, Multi-scale, and Multi-type Node Heterogeneous Graph for Emotion Cause Triplet Extraction in Conversations](https://arxiv.org/abs/2508.18740)
Token length: 1379
Summarized using GPT-3.5-turbo
Append: [Chronological Passage Assembling in RAG framework for Temporal Question Answering](https://arxiv.org/abs/2508.18748)
Token length: 1546
Summarized using GPT-3.5-turbo
Append: [ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large Language Models](https://arxiv.org/abs/2508.18773)
Token length: 833
Summarized using GPT-3.5-turbo
Append: [Harnessing Rule-Based Reinforcement Learning for Enhanced Grammatical Error Correction](https://arxiv.org/abs/2508.18780)
Token length: 1629
Summarized using GPT-3.5-turbo
Append: [Controllable Conversational Theme Detection Track at DSTC 12](https://arxiv.org/abs/2508.18783)
Token length: 1322
Summarized using GPT-3.5-turbo
Append: [LaTeXTrans: Structured LaTeX Translation with Multi-Agent Coordination](https://arxiv.org/abs/2508.18791)
Token length: 1333
Summarized using GPT-3.5-turbo
Append: [LLM-based Contrastive Self-Supervised AMR Learning with Masked Graph Autoencoders for Fake News Detection](https://arxiv.org/abs/2508.18819)
Token length: 1096
Summarized using GPT-3.5-turbo
Append: [Arrows of Math Reasoning Data Synthesis for Large Language Models: Diversity, Complexity and Correctness](https://arxiv.org/abs/2508.18824)
Token length: 1602
Summarized using GPT-3.5-turbo
Append: [ConfTuner: Training Large Language Models to Express Their Confidence Verbally](https://arxiv.org/abs/2508.18847)
Token length: 1284
Summarized using GPT-3.5-turbo
Append: [ReflectivePrompt: Reflective evolution in autoprompting algorithms](https://arxiv.org/abs/2508.18870)
Token length: 1309
Summarized using GPT-3.5-turbo
Append: [Empowering Computing Education Researchers Through LLM-Assisted Content Analysis](https://arxiv.org/abs/2508.18872)
Token length: 1130
Summarized using GPT-3.5-turbo
Append: [Affective Polarization across European Parliaments](https://arxiv.org/abs/2508.18916)
Token length: 1636
Summarized using GPT-3.5-turbo
Append: [Diverse And Private Synthetic Datasets Generation for RAG evaluation: A multi-agent framework](https://arxiv.org/abs/2508.18929)
Token length: 995
Summarized using GPT-3.5-turbo
Append: [Interpretable by AI Mother Tongue: Native Symbolic Reasoning in Neural Models](https://arxiv.org/abs/2508.18988)
Token length: 1019
Summarized using GPT-3.5-turbo
Append: [Automatic Prompt Optimization with Prompt Distillation](https://arxiv.org/abs/2508.18992)
Token length: 1369
Summarized using GPT-3.5-turbo
Append: [MovieCORE: COgnitive REasoning in Movies](https://arxiv.org/abs/2508.19026)
Token length: 1389
Summarized using GPT-3.5-turbo
Append: [HiPlan: Hierarchical Planning for LLM-Based Agents with Adaptive Global-Local Guidance](https://arxiv.org/abs/2508.19076)
Token length: 1731
Summarized using GPT-3.5-turbo
Append: ["Where does it hurt?" - Dataset and Study on Physician Intent Trajectories in Doctor Patient Dialogues](https://arxiv.org/abs/2508.19077)
Token length: 1219
Summarized using GPT-3.5-turbo
Append: [It's All About In-Context Learning! Teaching Extremely Low-Resource Languages to LLMs](https://arxiv.org/abs/2508.19089)
Token length: 1122
Summarized using GPT-3.5-turbo
Append: [Retrieval-Augmented Generation for Natural Language Art Provenance Searches in the Getty Provenance Index](https://arxiv.org/abs/2508.19093)
Token length: 1367
Summarized using GPT-3.5-turbo
Append: [Beyond the Black Box: Integrating Lexical and Semantic Methods in Quantitative Discourse Analysis with BERTopic](https://arxiv.org/abs/2508.19099)
Token length: 1219
Summarized using GPT-3.5-turbo
Append: [Do LVLMs Know What They Know? A Systematic Study of Knowledge Boundary Perception in LVLMs](https://arxiv.org/abs/2508.19111)
Token length: 1579
Summarized using GPT-3.5-turbo
Append: [Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning](https://arxiv.org/abs/2508.19202)
Token length: 879
Summarized using GPT-3.5-turbo
Append: [VibeVoice Technical Report](https://arxiv.org/abs/2508.19205)
Token length: 1242
Summarized using GPT-3.5-turbo
Append: [Evaluating the Evaluators: Are readability metrics good measures of readability?](https://arxiv.org/abs/2508.19221)
Append: [Generative Interfaces for Language Models](https://arxiv.org/abs/2508.19227)
Append: [Toward Responsible ASR for African American English Speakers: A Scoping Review of Bias and Equity in Speech Technology](https://arxiv.org/abs/2508.18288)
Append: [H-PRM: A Pluggable Hotword Pre-Retrieval Module for Various Speech Recognition Systems](https://arxiv.org/abs/2508.18295)
Append: [Can VLMs Recall Factual Associations From Visual References?](https://arxiv.org/abs/2508.18297)
Append: [SALMAN: Stability Analysis of Language Models Through the Maps Between Graph-based Manifolds](https://arxiv.org/abs/2508.18306)
Append: [Training Language Model Agents to Find Vulnerabilities with CTF-Dojo](https://arxiv.org/abs/2508.18370)
Append: [A Systematic Approach to Predict the Impact of Cybersecurity Vulnerabilities Using LLMs](https://arxiv.org/abs/2508.18439)
Append: [Designing across domains with declarative thinking: Insights from the 96-Eyes ptychographic imager project](https://arxiv.org/abs/2508.18512)
Append: [RLMR: Reinforcement Learning with Mixed Rewards for Creative Writing](https://arxiv.org/abs/2508.18642)
Append: [Beyond Benchmark: LLMs Evaluation with an Anthropomorphic and Value-oriented Roadmap](https://arxiv.org/abs/2508.18646)
Append: [UniC-RAG: Universal Knowledge Corruption Attacks to Retrieval-Augmented Generation](https://arxiv.org/abs/2508.18652)
Append: [Membership Inference Attacks on LLM-based Recommender Systems](https://arxiv.org/abs/2508.18665)
Append: [Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks](https://arxiv.org/abs/2508.18672)
Append: [FALCON: Autonomous Cyber Threat Intelligence Mining with LLMs for IDS Rule Generation](https://arxiv.org/abs/2508.18684)
Append: [Bias Mitigation Agent: Optimizing Source Selection for Fair and Balanced Knowledge Retrieval](https://arxiv.org/abs/2508.18724)
Append: [CAC-CoT: Connector-Aware Compact Chain-of-Thought for Efficient Reasoning Data Synthesis Across Dual-System Cognitive Tasks](https://arxiv.org/abs/2508.18743)
Append: [Text to Query Plans for Question Answering on Large Tables](https://arxiv.org/abs/2508.18758)
Append: [Answering the Unanswerable Is to Err Knowingly: Analyzing and Mitigating Abstention Failures in Large Reasoning Models](https://arxiv.org/abs/2508.18760)
Append: [Beyond the Textual: Generating Coherent Visual Options for MCQs](https://arxiv.org/abs/2508.18772)
Append: [The Double-edged Sword of LLM-based Data Reconstruction: Understanding and Mitigating Contextual Vulnerability in Word-level Differential Privacy Text Sanitization](https://arxiv.org/abs/2508.18976)
Append: [Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A Framework and Benchmark](https://arxiv.org/abs/2508.19005)
Append: [The Ramon Llull's Thinking Machine for Automated Ideation](https://arxiv.org/abs/2508.19200)
Append: [StepWiser: Stepwise Generative Judges for Wiser Reasoning](https://arxiv.org/abs/2508.19229)
Append: [A Survey on Data Selection for LLM Instruction Tuning](https://arxiv.org/abs/2402.05123)
Append: [HateDebias: On the Diversity and Variability of Hate Speech Debiasing](https://arxiv.org/abs/2406.04876)
Append: [Exploring the Robustness of Language Models for Tabular Question Answering via Attention Analysis](https://arxiv.org/abs/2406.12719)
Append: [ChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context](https://arxiv.org/abs/2407.06866)
Append: [Recognizing Limits: Investigating Infeasibility in Large Language Models](https://arxiv.org/abs/2408.05873)
Append: [Label Set Optimization via Activation Distribution Kurtosis for Zero-shot Classification with Generative Models](https://arxiv.org/abs/2410.19195)
Append: [From Intents to Conversations: Generating Intent-Driven Dialogues with Contrastive Learning for Multi-Turn Classification](https://arxiv.org/abs/2411.14252)
Append: [Adapting Large Language Models to Log Analysis with Interpretable Domain Knowledge](https://arxiv.org/abs/2412.01377)
Append: [TL-Training: A Task-Feature-Based Framework for Training Large Language Models in Tool Use](https://arxiv.org/abs/2412.15495)
Append: [Large Language Models Badly Generalize across Option Length, Problem Types, and Irrelevant Noun Replacements](https://arxiv.org/abs/2502.12459)
Append: [Bridging the Editing Gap in LLMs: FineEdit for Precise and Targeted Text Modifications](https://arxiv.org/abs/2502.13358)
Append: [Collaborative Evaluation of Deepfake Text with Deliberation-Enhancing Dialogue Systems](https://arxiv.org/abs/2503.04945)
Append: [SmartBench: Is Your LLM Truly a Good Chinese Smartphone Assistant?](https://arxiv.org/abs/2503.06029)
Append: [Accelerate Parallelizable Reasoning via Parallel Decoding within One Sequence](https://arxiv.org/abs/2503.20533)
Append: [An Ontology-Driven Graph RAG for Legal Norms: A Hierarchical, Temporal, and Deterministic Approach](https://arxiv.org/abs/2505.00039)
Append: [Improving Multilingual Language Models by Aligning Representations through Steering](https://arxiv.org/abs/2505.12584)
Append: [Truth or Twist? Optimal Model Selection for Reliable Label Flipping Evaluation in LLM-based Counterfactuals](https://arxiv.org/abs/2505.13972)
Append: [Can Pruning Improve Reasoning? Revisiting Long-CoT Compression with Capability in Mind for Better Reasoning](https://arxiv.org/abs/2505.14582)
Append: [RePPL: Recalibrating Perplexity by Uncertainty in Semantic Propagation and Language Generation for Explainable QA Hallucination Detection](https://arxiv.org/abs/2505.15386)
Append: [ELSPR: Evaluator LLM Training Data Self-Purification on Non-Transitive Preferences via Tournament Graph Reconstruction](https://arxiv.org/abs/2505.17691)
Append: [Subjective Perspectives within Learned Representations Predict High-Impact Innovation](https://arxiv.org/abs/2506.04616)
Append: [Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity Dilemma of Embeddings](https://arxiv.org/abs/2506.08592)
Append: [An Agentic System for Rare Disease Diagnosis with Traceable Reasoning](https://arxiv.org/abs/2506.20430)
Append: [Krul: Efficient State Restoration for Multi-turn Conversations with Dynamic Cross-layer KV Sharing](https://arxiv.org/abs/2507.08045)
Append: [SKA-Bench: A Fine-Grained Benchmark for Evaluating Structured Knowledge Understanding of LLMs](https://arxiv.org/abs/2507.17178)
Append: [Weakly-Supervised 3D Visual Grounding based on Visual Language Alignment](https://arxiv.org/abs/2312.09625)
Append: [VAGUE: Visual Contexts Clarify Ambiguous Expressions](https://arxiv.org/abs/2411.14137)
Append: [Evolutionary Automata and Deep Evolutionary Computation](https://arxiv.org/abs/2411.15008)
Append: [Aligning NLP Models with Target Population Perspectives using PAIR: Population-Aligned Instance Replication](https://arxiv.org/abs/2501.06826)
Append: [mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented Generation](https://arxiv.org/abs/2505.24073)
Append: [Cyber-Zero: Training Cybersecurity Agents without Runtime](https://arxiv.org/abs/2508.00910)
append_entries: 104
Finish: 2025-08-27 04:27:05.986506
------------------------------------------------------
Started: 2025-08-27 06:24:12.293193
Existing_entries: 1104
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-27 06:24:12.567940
------------------------------------------------------
Started: 2025-08-27 08:21:20.764082
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-27 08:21:21.066310
------------------------------------------------------
Started: 2025-08-27 10:16:47.320966
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-27 10:16:47.630474
------------------------------------------------------
Started: 2025-08-27 12:34:03.759006
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-27 12:34:04.034418
------------------------------------------------------
Started: 2025-08-27 14:15:57.288271
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-27 14:15:57.583978
------------------------------------------------------
Started: 2025-08-27 16:20:09.008432
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-27 16:20:09.343921
------------------------------------------------------
Started: 2025-08-27 18:22:47.808025
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-27 18:22:48.136414
------------------------------------------------------
Started: 2025-08-27 20:17:53.933038
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-27 20:17:54.253347
------------------------------------------------------
Started: 2025-08-27 22:14:57.002764
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-27 22:14:57.320723
------------------------------------------------------
Started: 2025-08-28 01:14:47.598041
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-28 01:14:47.928754
------------------------------------------------------
Started: 2025-08-28 03:00:15.709105
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-28 03:00:16.021801
------------------------------------------------------
Started: 2025-08-28 04:23:41.344646
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1237
Summarized using GPT-3.5-turbo
Append: [MultiPL-MoE: Multi-Programming-Lingual Extension of Large Language Models through Hybrid Mixture-of-Experts](https://arxiv.org/abs/2508.19268)
Token length: 1110
Summarized using GPT-3.5-turbo
Append: [Whisper based Cross-Lingual Phoneme Recognition between Vietnamese and English](https://arxiv.org/abs/2508.19270)
Token length: 1778
Summarized using GPT-3.5-turbo
Append: [Rethinking Reasoning in LLMs: Neuro-Symbolic Local RetoMaton Beyond ICL and CoT](https://arxiv.org/abs/2508.19271)
Token length: 784
Summarized using GPT-3.5-turbo
Append: [RAGAPHENE: A RAG Annotation Platform with Human Enhancements and Edits](https://arxiv.org/abs/2508.19272)
Token length: 1965
Summarized using GPT-3.5-turbo
Append: [Leveraging Language Models and Machine Learning in Verbal Autopsy Analysis](https://arxiv.org/abs/2508.19274)
Token length: 1179
Summarized using GPT-3.5-turbo
Append: [FLAIRR-TS -- Forecasting LLM-Agents with Iterative Refinement and Retrieval for Time Series](https://arxiv.org/abs/2508.19279)
Token length: 1603
Summarized using GPT-3.5-turbo
Append: [CORE: Lossless Compression for Retrieval-Augmented LLMs via Reinforcement Learning](https://arxiv.org/abs/2508.19282)
Token length: 1322
Summarized using GPT-3.5-turbo
Append: [Context-Adaptive Synthesis and Compression for Enhanced Retrieval-Augmented Generation in Complex Domains](https://arxiv.org/abs/2508.19357)
Token length: 1175
Summarized using GPT-3.5-turbo
Append: [Reflective Agreement: Combining Self-Mixture of Agents with a Sequence Tagger for Robust Event Extraction](https://arxiv.org/abs/2508.19359)
Token length: 1119
Summarized using GPT-3.5-turbo
Append: [LongReasonArena: A Long Reasoning Benchmark for Large Language Models](https://arxiv.org/abs/2508.19363)
Token length: 1018
Summarized using GPT-3.5-turbo
Append: [Database Entity Recognition with Data Augmentation and Deep Learning](https://arxiv.org/abs/2508.19372)
Token length: 1433
Summarized using GPT-3.5-turbo
Append: [One Joke to Rule them All? On the (Im)possibility of Generalizing Humor](https://arxiv.org/abs/2508.19402)
Token length: 813
Summarized using GPT-3.5-turbo
Append: [A perishable ability? The future of writing in the face of generative artificial intelligence](https://arxiv.org/abs/2508.19427)
Token length: 1815
Summarized using GPT-3.5-turbo
Append: [Heterogeneous LLM Methods for Ontology Learning (Few-Shot Prompting, Ensemble Typing, and Attention-Based Taxonomies)](https://arxiv.org/abs/2508.19428)
Token length: 1209
Summarized using GPT-3.5-turbo
Append: [Bridging Language Gaps: Enhancing Few-Shot Language Adaptation](https://arxiv.org/abs/2508.19464)
Token length: 1925
Summarized using GPT-3.5-turbo
Append: [Inference Gap in Domain Expertise and Machine Intelligence in Named Entity Recognition: Creation of and Insights from a Substance Use-related Dataset](https://arxiv.org/abs/2508.19467)
Token length: 1338
Summarized using GPT-3.5-turbo
Append: [Automatic Question & Answer Generation Using Generative Large Language Model (LLM)](https://arxiv.org/abs/2508.19475)
Token length: 1579
Summarized using GPT-3.5-turbo
Append: [Improving Low-Resource Translation with Dictionary-Guided Fine-Tuning and RL: A Spanish-to-Wayuunaiki Study](https://arxiv.org/abs/2508.19481)
Token length: 914
Summarized using GPT-3.5-turbo
Append: [Rule Synergy Analysis using LLMs: State of the Art and Implications](https://arxiv.org/abs/2508.19484)
Token length: 1171
Summarized using GPT-3.5-turbo
Append: [Blockwise SFT for Diffusion Language Models: Reconciling Bidirectional Attention and Autoregressive Decoding](https://arxiv.org/abs/2508.19529)
Token length: 1018
Summarized using GPT-3.5-turbo
Append: [Alignment with Fill-In-the-Middle for Enhancing Code Generation](https://arxiv.org/abs/2508.19532)
Token length: 1239
Summarized using GPT-3.5-turbo
Append: [Emotion Transfer with Enhanced Prototype for Unseen Emotion Recognition in Conversation](https://arxiv.org/abs/2508.19533)
Token length: 1170
Summarized using GPT-3.5-turbo
Append: [Language Models Identify Ambiguities and Exploit Loopholes](https://arxiv.org/abs/2508.19546)
Token length: 1079
Summarized using GPT-3.5-turbo
Append: [Towards a Holistic and Automated Evaluation Framework for Multi-Level Comprehension of LLMs in Book-Length Contexts](https://arxiv.org/abs/2508.19578)
Token length: 1183
Summarized using GPT-3.5-turbo
Append: [ArgCMV: An Argument Summarization Benchmark for the LLM-era](https://arxiv.org/abs/2508.19580)
Token length: 1400
Summarized using GPT-3.5-turbo
Append: [Towards stable AI systems for Evaluating Arabic Pronunciations](https://arxiv.org/abs/2508.19587)
Token length: 1146
Summarized using GPT-3.5-turbo
Append: [Understanding and Leveraging the Expert Specialization of Context Faithfulness in Mixture-of-Experts LLMs](https://arxiv.org/abs/2508.19594)
Token length: 1514
Summarized using GPT-3.5-turbo
Append: [LFD: Layer Fused Decoding to Exploit External Knowledge in Retrieval-Augmented Generation](https://arxiv.org/abs/2508.19614)
Token length: 1527
Summarized using GPT-3.5-turbo
Append: [A Symbolic Adversarial Learning Framework for Evolving Fake News Generation and Detection](https://arxiv.org/abs/2508.19633)
Token length: 922
Summarized using GPT-3.5-turbo
Append: [Automatic integration of SystemC in the FMI standard for Software-defined Vehicle design](https://arxiv.org/abs/2508.19665)
Token length: 974
Summarized using GPT-3.5-turbo
Append: [Survey of Specialized Large Language Model](https://arxiv.org/abs/2508.19667)
Token length: 395
Summarized using GPT-3.5-turbo
Append: [Building Task Bots with Self-learning for Enhanced Adaptability, Extensibility, and Factuality](https://arxiv.org/abs/2508.19689)
Token length: 1550
Summarized using GPT-3.5-turbo
Append: [Continuously Steering LLMs Sensitivity to Contextual Knowledge with Proxy Models](https://arxiv.org/abs/2508.19720)
Token length: 1047
Summarized using GPT-3.5-turbo
Append: [CAM\~OES: A Comprehensive Automatic Speech Recognition Benchmark for European Portuguese](https://arxiv.org/abs/2508.19721)
Token length: 1492
Summarized using GPT-3.5-turbo
Append: [NLKI: A lightweight Natural Language Knowledge Integration Framework for Improving Small VLMs in Commonsense VQA Tasks](https://arxiv.org/abs/2508.19724)
Token length: 1292
Summarized using GPT-3.5-turbo
Append: [Spotlight Attention: Towards Efficient LLM Generation via Non-linear Hashing-based KV Cache Retrieval](https://arxiv.org/abs/2508.19740)
Token length: 1233
Summarized using GPT-3.5-turbo
Append: [Uncovering the Bigger Picture: Comprehensive Event Understanding Via Diverse News Retrieval](https://arxiv.org/abs/2508.19758)
Token length: 1301
Summarized using GPT-3.5-turbo
Append: [Principled Personas: Defining and Measuring the Intended Effects of Persona Prompting on Task Performance](https://arxiv.org/abs/2508.19764)
Token length: 1260
Summarized using GPT-3.5-turbo
Append: [T2R-bench: A Benchmark for Generating Article-Level Reports from Real World Industrial Tables](https://arxiv.org/abs/2508.19813)
Token length: 1454
Summarized using GPT-3.5-turbo
Append: [Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning](https://arxiv.org/abs/2508.19828)
Token length: 819
Summarized using GPT-3.5-turbo
Append: [Benchmarking Hindi LLMs: A New Suite of Datasets and a Comparative Analysis](https://arxiv.org/abs/2508.19831)
Token length: 1369
Summarized using GPT-3.5-turbo
Append: [Scalable and consistent few-shot classification of survey responses using text embeddings](https://arxiv.org/abs/2508.19836)
Token length: 890
Summarized using GPT-3.5-turbo
Append: [TokenVerse++: Towards Flexible Multitask Learning with Dynamic Task Activation](https://arxiv.org/abs/2508.19856)
Token length: 989
Summarized using GPT-3.5-turbo
Append: [Beyond Shallow Heuristics: Leveraging Human Intuition for Curriculum Learning](https://arxiv.org/abs/2508.19873)
Token length: 1569
Summarized using GPT-3.5-turbo
Append: [AI-Powered Detection of Inappropriate Language in Medical School Curricula](https://arxiv.org/abs/2508.19883)
Token length: 1052
Summarized using GPT-3.5-turbo
Append: [Bangla-Bayanno: A 52K-Pair Bengali Visual Question Answering Dataset with LLM-Assisted Translation Refinement](https://arxiv.org/abs/2508.19887)
Token length: 1304
Summarized using GPT-3.5-turbo
Append: [Logical Reasoning with Outcome Reward Models for Test-Time Scaling](https://arxiv.org/abs/2508.19903)
Token length: 1464
Summarized using GPT-3.5-turbo
Append: [Your AI Bosses Are Still Prejudiced: The Emergence of Stereotypes in LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2508.19919)
Token length: 1676
Summarized using GPT-3.5-turbo
Append: [HEAL: A Hypothesis-Based Preference-Aware Analysis Framework](https://arxiv.org/abs/2508.19922)
Token length: 1234
Summarized using GPT-3.5-turbo
Append: [Dhati+: Fine-tuned Large Language Models for Arabic Subjectivity Evaluation](https://arxiv.org/abs/2508.19966)
Append: [Diffusion Language Models Know the Answer Before Decoding](https://arxiv.org/abs/2508.19982)
Append: [AgentCoMa: A Compositional Benchmark Mixing Commonsense and Mathematical Reasoning in Real-World Scenarios](https://arxiv.org/abs/2508.19988)
Append: [MathBuddy: A Multimodal System for Affective Math Tutoring](https://arxiv.org/abs/2508.19993)
Append: [ReSURE: Regularizing Supervision Unreliability for Multi-turn Dialogue Fine-tuning](https://arxiv.org/abs/2508.19996)
Append: [Selective Retrieval-Augmentation for Long-Tail Legal Text Classification](https://arxiv.org/abs/2508.19997)
Append: [DeepScholar-Bench: A Live Benchmark and Automated Evaluation for Generative Research Synthesis](https://arxiv.org/abs/2508.20033)
Append: [Forewarned is Forearmed: Pre-Synthesizing Jailbreak-like Instructions to Enhance LLM Safety Guardrail to Potential Attacks](https://arxiv.org/abs/2508.20038)
Append: [AraHealthQA 2025 Shared Task Description Paper](https://arxiv.org/abs/2508.20047)
Append: [11Plus-Bench: Demystifying Multimodal LLM Spatial Reasoning with Cognitive-Inspired Analysis](https://arxiv.org/abs/2508.20068)
Append: [Capabilities of GPT-5 across critical domains: Is it the next breakthrough?](https://arxiv.org/abs/2508.19259)
Append: [Beat-Based Rhythm Quantization of MIDI Performances](https://arxiv.org/abs/2508.19262)
Append: [Should LLMs be WEIRD? Exploring WEIRDness and Human Rights in Large Language Models](https://arxiv.org/abs/2508.19269)
Append: [Object Detection with Multimodal Large Vision-Language Models: An In-depth Review](https://arxiv.org/abs/2508.19294)
Append: [Sycophancy as compositions of Atomic Psychometric Traits](https://arxiv.org/abs/2508.19316)
Append: [An Investigation on Group Query Hallucination Attacks](https://arxiv.org/abs/2508.19321)
Append: [Geopolitical Parallax: Beyond Walter Lippmann Just After Large Language Models](https://arxiv.org/abs/2508.19492)
Append: [Functional Consistency of LLM Code Embeddings: A Self-Evolving Data Synthesis Framework for Benchmarking](https://arxiv.org/abs/2508.19558)
Append: [Instructional Agents: LLM Agents on Automated Course Material Generation for Teaching Faculties](https://arxiv.org/abs/2508.19611)
Append: [Word Chain Generators for Prefix Normal Words](https://arxiv.org/abs/2508.19619)
Append: [Safety Alignment Should Be Made More Than Just A Few Attention Heads](https://arxiv.org/abs/2508.19697)
Append: [Analysing Chain of Thought Dynamics: Active Guidance or Unfaithful Post-hoc Rationalisation?](https://arxiv.org/abs/2508.19827)
Append: [SoK: Large Language Model Copyright Auditing via Fingerprinting](https://arxiv.org/abs/2508.19843)
Append: [KRETA: A Benchmark for Korean Reading and Reasoning in Text-Rich VQA Attuned to Diverse Visual Contexts](https://arxiv.org/abs/2508.19944)
Append: [GLSim: Detecting Object Hallucinations in LVLMs via Global-Local Similarity](https://arxiv.org/abs/2508.19972)
Append: [Self-Supervised Pre-Training with Equilibrium Constraints](https://arxiv.org/abs/2508.19990)
Append: [Linear-Time Demonstration Selection for In-Context Learning via Gradient Estimation](https://arxiv.org/abs/2508.19999)
Append: [SWIRL: A Staged Workflow for Interleaved Reinforcement Learning in Mobile GUI Control](https://arxiv.org/abs/2508.20018)
Append: [Symphony: A Decentralized Multi-Agent Framework for Scalable Collective Intelligence](https://arxiv.org/abs/2508.20019)
Append: [Pruning Strategies for Backdoor Defense in LLMs](https://arxiv.org/abs/2508.20032)
Append: [Disabling Self-Correction in Retrieval-Augmented Generation via Stealthy Retriever Poisoning](https://arxiv.org/abs/2508.20083)
Append: [Cross-lingual Offensive Language Detection: A Systematic Review of Datasets, Transfer Approaches and Challenges](https://arxiv.org/abs/2401.09244)
Append: [NPHardEval4V: Dynamic Evaluation of Large Vision-Language Models with Effects of Vision](https://arxiv.org/abs/2403.01777)
Append: [FiRST: Finetuning Router-Selective Transformers for Input-Adaptive Latency Reduction](https://arxiv.org/abs/2410.12513)
Append: [Understanding Fairness-Accuracy Trade-offs in Machine Learning Models: Does Promoting Fairness Undermine Performance?](https://arxiv.org/abs/2411.17374)
Append: [On Domain-Adaptive Post-Training for Multimodal Large Language Models](https://arxiv.org/abs/2411.19930)
Append: [Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging](https://arxiv.org/abs/2412.19512)
Append: [Agent-as-Judge for Factual Summarization of Long Narratives](https://arxiv.org/abs/2501.09993)
Append: [Chain-of-Reasoning: Towards Unified Mathematical Reasoning in Large Language Models via a Multi-Paradigm Perspective](https://arxiv.org/abs/2501.11110)
Append: [MDEval: Evaluating and Enhancing Markdown Awareness in Large Language Models](https://arxiv.org/abs/2501.15000)
Append: [Efficient Response Generation Strategy Selection for Fine-Tuning Large Language Models Through Self-Aligned Perplexity](https://arxiv.org/abs/2502.11779)
Append: [Constructing a Norm for Children's Scientific Drawing: Distribution Features Based on Semantic Similarity of Large Language Models](https://arxiv.org/abs/2502.15348)
Append: [KoWit-24: A Richly Annotated Dataset of Wordplay in News Headlines](https://arxiv.org/abs/2503.01510)
Append: [SuperBPE: Space Travel for Language Models](https://arxiv.org/abs/2503.13423)
Append: [News is More than a Collection of Facts: Moral Frame Preserving News Summarization](https://arxiv.org/abs/2504.00657)
Append: [Evaluating the Fitness of Ontologies for the Task of Question Generation](https://arxiv.org/abs/2504.07994)
Append: [Multi-Type Context-Aware Conversational Recommender Systems via Mixture-of-Experts](https://arxiv.org/abs/2504.13655)
Append: [ICL CIPHERS: Quantifying "Learning" in In-Context Learning via Substitution Ciphers](https://arxiv.org/abs/2504.19395)
Append: [Hydra: Structured Cross-Source Enhanced Large Language Model Reasoning](https://arxiv.org/abs/2505.17464)
Append: [mSTEB: Massively Multilingual Evaluation of LLMs on Speech and Text Tasks](https://arxiv.org/abs/2506.08400)
Append: [Refining Czech GEC: Insights from a Multi-Experiment Approach](https://arxiv.org/abs/2506.22402)
Append: [PhoniTale: Phonologically Grounded Mnemonic Generation for Typologically Distant Language Pairs](https://arxiv.org/abs/2507.05444)
Append: [PyVision: Agentic Vision with Dynamic Tooling](https://arxiv.org/abs/2507.07998)
Append: [Doc2Chart: Intent-Driven Zero-Shot Chart Generation from Documents](https://arxiv.org/abs/2507.14819)
Append: [Step-Audio 2 Technical Report](https://arxiv.org/abs/2507.16632)
Append: [MegaScience: Pushing the Frontiers of Post-Training Datasets for Science Reasoning](https://arxiv.org/abs/2507.16812)
Append: [When Algorithms Meet Artists: Topic Modeling the AI-Art Debate, 2013-2025](https://arxiv.org/abs/2508.03037)
Append: [Putnam-AXIOM: A Functional and Static Benchmark for Measuring Higher Level Mathematical Reasoning in LLMs](https://arxiv.org/abs/2508.08292)
Append: [A Survey on Parallel Text Generation: From Parallel Decoding to Diffusion Language Models](https://arxiv.org/abs/2508.08712)
Append: [A Survey on Training-free Alignment of Large Language Models](https://arxiv.org/abs/2508.09016)
Append: [SinLlama -- A Large Language Model for Sinhala](https://arxiv.org/abs/2508.09115)
Append: [A Statistical Framework of Watermarks for Large Language Models: Pivot, Detection Efficiency and Optimal Rules](https://arxiv.org/abs/2404.01245)
Append: [Reducing Biases towards Minoritized Populations in Medical Curricular Content via Artificial Intelligence for Fairer Health Outcomes](https://arxiv.org/abs/2407.12680)
Append: [LLM-based feature generation from text for interpretable machine learning](https://arxiv.org/abs/2409.07132)
Append: [Robust Detection of Watermarks for Large Language Models Under Human Edits](https://arxiv.org/abs/2411.13868)
Append: [Unifying the Extremes: Developing a Unified Model for Detecting and Predicting Extremist Traits and Radicalization](https://arxiv.org/abs/2501.04820)
Append: [Know "No" Better: A Data-Driven Approach for Enhancing Negation Awareness in CLIP](https://arxiv.org/abs/2501.10913)
Append: [Do Vision Encoders Truly Explain Object Hallucination?: Mitigating Object Hallucination via Simple Fine-Grained CLIPScore](https://arxiv.org/abs/2502.20034)
Append: [Exploring Typographic Visual Prompts Injection Threats in Cross-Modality Generation Models](https://arxiv.org/abs/2503.11519)
Append: [EnvInjection: Environmental Prompt Injection Attack to Multi-modal Web Agents](https://arxiv.org/abs/2505.11717)
Append: [RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation](https://arxiv.org/abs/2506.18088)
Append: [GTPO: Trajectory-Based Policy Optimization in Large Language Models](https://arxiv.org/abs/2508.03772)
Append: [R-Zero: Self-Evolving Reasoning LLM from Zero Data](https://arxiv.org/abs/2508.05004)
append_entries: 122
Finish: 2025-08-28 04:25:57.502333
------------------------------------------------------
Started: 2025-08-28 06:25:08.904406
Existing_entries: 1122
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-28 06:25:09.290010
------------------------------------------------------
Started: 2025-08-28 08:21:33.378209
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-28 08:21:33.674835
------------------------------------------------------
Started: 2025-08-28 10:17:17.225708
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-28 10:17:17.588099
------------------------------------------------------
Started: 2025-08-28 12:33:29.598986
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-28 12:33:29.897337
------------------------------------------------------
Started: 2025-08-28 14:16:19.113328
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-28 14:16:19.413426
------------------------------------------------------
Started: 2025-08-28 16:20:27.152210
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-28 16:20:27.534953
------------------------------------------------------
Started: 2025-08-28 18:22:59.160362
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-28 18:22:59.591304
------------------------------------------------------
Started: 2025-08-28 20:17:51.976484
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-28 20:17:52.301396
------------------------------------------------------
Started: 2025-08-28 22:15:15.605649
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-28 22:15:15.945131
------------------------------------------------------
Started: 2025-08-29 01:14:36.996733
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-29 01:14:37.292975
------------------------------------------------------
Started: 2025-08-29 03:00:04.877393
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-29 03:00:05.184823
------------------------------------------------------
Started: 2025-08-29 04:23:19.929375
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 982
Summarized using GPT-3.5-turbo
Append: [Social Bias in Multilingual Language Models: A Survey](https://arxiv.org/abs/2508.20201)
Token length: 1483
Summarized using GPT-3.5-turbo
Append: [Prompting Strategies for Language Model-Based Item Generation in K-12 Education: Bridging the Gap Between Small and Large Language Models](https://arxiv.org/abs/2508.20217)
Token length: 1034
Summarized using GPT-3.5-turbo
Append: [Integrating SystemC TLM into FMI 3.0 Co-Simulations with an Open-Source Approach](https://arxiv.org/abs/2508.20223)
Token length: 1020
Summarized using GPT-3.5-turbo
Append: [Can Compact Language Models Search Like Agents? Distillation-Guided Policy Optimization for Preserving Agentic RAG Capabilities](https://arxiv.org/abs/2508.20324)
Token length: 1940
Summarized using GPT-3.5-turbo
Append: [GUARD: Guideline Upholding Test through Adaptive Role-play and Jailbreak Diagnostics for LLMs](https://arxiv.org/abs/2508.20325)
Token length: 1321
Summarized using GPT-3.5-turbo
Append: [Joint Enhancement of Relational Reasoning for Long-Context LLMs](https://arxiv.org/abs/2508.20351)
Token length: 1539
Summarized using GPT-3.5-turbo
Append: [Graph-R1: Unleashing LLM Reasoning with NP-Hard Graph Problems](https://arxiv.org/abs/2508.20373)
Token length: 1560
Summarized using GPT-3.5-turbo
Append: [CAPE: Context-Aware Personality Evaluation Framework for Large Language Models](https://arxiv.org/abs/2508.20385)
Token length: 1525
Summarized using GPT-3.5-turbo
Append: [Measuring Reasoning Utility in LLMs via Conditional Entropy Reduction](https://arxiv.org/abs/2508.20395)
Token length: 861
Summarized using GPT-3.5-turbo
Append: [UI-Bench: A Benchmark for Evaluating Design Capabilities of AI Text-to-App Tools](https://arxiv.org/abs/2508.20410)
Token length: 1391
Summarized using GPT-3.5-turbo
Append: [DentalBench: Benchmarking and Advancing LLMs Capability for Bilingual Dentistry Understanding](https://arxiv.org/abs/2508.20416)
Token length: 1328
Summarized using GPT-3.5-turbo
Append: [KG-CQR: Leveraging Structured Relation Representations in Knowledge Graphs for Contextual Query Retrieval](https://arxiv.org/abs/2508.20417)
Token length: 1685
Summarized using GPT-3.5-turbo
Append: [CAMB: A comprehensive industrial LLM benchmark on civil aviation maintenance](https://arxiv.org/abs/2508.20420)
Token length: 895
Summarized using GPT-3.5-turbo
Append: [Searching the Title of Practical Work of the Informatics Engineering Bachelor Program with the Case Base Reasoning Method](https://arxiv.org/abs/2508.20442)
Token length: 1403
Summarized using GPT-3.5-turbo
Append: [MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World Tasks via MCP Servers](https://arxiv.org/abs/2508.20453)
Token length: 1927
Summarized using GPT-3.5-turbo
Append: [Prediction of mortality and resource utilization in critical care: a deep learning approach using multimodal electronic health records with natural language processing techniques](https://arxiv.org/abs/2508.20460)
Token length: 1206
Summarized using GPT-3.5-turbo
Append: [ConspirED: A Dataset for Cognitive Traits of Conspiracy Theories and Large Language Model Safety](https://arxiv.org/abs/2508.20468)
Token length: 1320
Summarized using GPT-3.5-turbo
Append: [Languages Still Left Behind: Toward a Better Multilingual Machine Translation Benchmark](https://arxiv.org/abs/2508.20511)
Token length: 1653
Summarized using GPT-3.5-turbo
Append: [SciTopic: Enhancing Topic Discovery in Scientific Literature through Advanced LLM](https://arxiv.org/abs/2508.20514)
Token length: 942
Summarized using GPT-3.5-turbo
Append: [Overview of BioASQ 2024: The twelfth BioASQ challenge on Large-Scale Biomedical Semantic Indexing and Question Answering](https://arxiv.org/abs/2508.20532)
Token length: 1024
Summarized using GPT-3.5-turbo
Append: [Overview of BioASQ 2025: The Thirteenth BioASQ Challenge on Large-Scale Biomedical Semantic Indexing and Question Answering](https://arxiv.org/abs/2508.20554)
Token length: 1377
Summarized using GPT-3.5-turbo
Append: [Adaptive Federated Distillation for Multi-Domain Non-IID Textual Data](https://arxiv.org/abs/2508.20557)
Token length: 1520
Summarized using GPT-3.5-turbo
Append: [Leveraging Generative Models for Real-Time Query-Driven Text Summarization in Large-Scale Web Search](https://arxiv.org/abs/2508.20559)
Token length: 1309
Summarized using GPT-3.5-turbo
Append: [KCS: Diversify Multi-hop Question Generation with Knowledge Composition Sampling](https://arxiv.org/abs/2508.20567)
Token length: 1549
Summarized using GPT-3.5-turbo
Append: [A Graph Talks, But Who's Listening? Rethinking Evaluations for Graph-Language Models](https://arxiv.org/abs/2508.20583)
Token length: 1219
Summarized using GPT-3.5-turbo
Append: [Generative Annotation for ASR Named Entity Correction](https://arxiv.org/abs/2508.20700)
Token length: 1036
Summarized using GPT-3.5-turbo
Append: [Multi-Lingual Implicit Discourse Relation Recognition with Multi-Label Hierarchical Learning](https://arxiv.org/abs/2508.20712)
Token length: 1097
Summarized using GPT-3.5-turbo
Append: [Addressing Tokenization Inconsistency in Steganography and Watermarking Based on Large Language Models](https://arxiv.org/abs/2508.20718)
Token length: 1643
Summarized using GPT-3.5-turbo
Append: [rStar2-Agent: Agentic Reasoning Technical Report](https://arxiv.org/abs/2508.20722)
Token length: 1315
Summarized using GPT-3.5-turbo
Append: [Leveraging Semantic Triples for Private Document Generation with Local Differential Privacy Guarantees](https://arxiv.org/abs/2508.20736)
Token length: 858
Summarized using GPT-3.5-turbo
Append: [Specializing General-purpose LLM Embeddings for Implicit Hate Speech Detection across Datasets](https://arxiv.org/abs/2508.20750)
Token length: 1365
Summarized using GPT-3.5-turbo
Append: [GUARD: Glocal Uncertainty-Aware Robust Decoding for Effective and Efficient Open-Ended Text Generation](https://arxiv.org/abs/2508.20757)
Token length: 1507
Summarized using GPT-3.5-turbo
Append: [Feel the Difference? A Comparative Analysis of Emotional Arcs in Real and LLM-Generated CBT Sessions](https://arxiv.org/abs/2508.20764)
Token length: 1346
Summarized using GPT-3.5-turbo
Append: [Turning the Spell Around: Lightweight Alignment Amplification via Rank-One Safety Injection](https://arxiv.org/abs/2508.20766)
Token length: 813
Summarized using GPT-3.5-turbo
Append: [Signs of Struggle: Spotting Cognitive Distortions across Language and Register](https://arxiv.org/abs/2508.20771)
Token length: 634
Summarized using GPT-3.5-turbo
Append: [Exploring Machine Learning and Language Models for Multimodal Depression Detection](https://arxiv.org/abs/2508.20805)
Token length: 1505
Summarized using GPT-3.5-turbo
Append: [GDLLM: A Global Distance-aware Modeling Approach Based on Large Language Models for Event Temporal Relation Extraction](https://arxiv.org/abs/2508.20828)
Token length: 1472
Summarized using GPT-3.5-turbo
Append: [MSRS: Evaluating Multi-Source Retrieval-Augmented Generation](https://arxiv.org/abs/2508.20867)
Token length: 1252
Summarized using GPT-3.5-turbo
Append: [The Uneven Impact of Post-Training Quantization in Machine Translation](https://arxiv.org/abs/2508.20893)
Token length: 1109
Summarized using GPT-3.5-turbo
Append: [SageLM: A Multi-aspect and Explainable Large Language Model for Speech Judgement](https://arxiv.org/abs/2508.20916)
Token length: 1255
Summarized using GPT-3.5-turbo
Append: [How Can Input Reformulation Improve Tool Usage Accuracy in a Complex Dynamic Environment? A Study on $\tau$-bench](https://arxiv.org/abs/2508.20931)
Token length: 1267
Summarized using GPT-3.5-turbo
Append: [STARE at the Structure: Steering ICL Exemplar Selection with Structural Alignment](https://arxiv.org/abs/2508.20944)
Token length: 1177
Summarized using GPT-3.5-turbo
Append: [ProactiveEval: A Unified Evaluation Framework for Proactive Dialogue Agents](https://arxiv.org/abs/2508.20973)
Token length: 1564
Summarized using GPT-3.5-turbo
Append: [Lethe: Purifying Backdoored Large Language Models with Knowledge Dilution](https://arxiv.org/abs/2508.21004)
Token length: 1419
Summarized using GPT-3.5-turbo
Append: [An Agile Method for Implementing Retrieval Augmented Generation Tools in Industrial SMEs](https://arxiv.org/abs/2508.21024)
Token length: 1348
Summarized using GPT-3.5-turbo
Append: [Re-Representation in Sentential Relation Extraction with Sequence Routing Algorithm](https://arxiv.org/abs/2508.21049)
Token length: 1250
Summarized using GPT-3.5-turbo
Append: [Enabling Equitable Access to Trustworthy Financial Reasoning](https://arxiv.org/abs/2508.21051)
Token length: 1581
Summarized using GPT-3.5-turbo
Append: [A Unified Theory of Language](https://arxiv.org/abs/2508.20109)
Token length: 1416
Summarized using GPT-3.5-turbo
Append: [Mitigating Hallucinations in Multimodal LLMs via Object-aware Preference Optimization](https://arxiv.org/abs/2508.20181)
Token length: 977
Summarized using GPT-3.5-turbo
Append: [AI-AI Esthetic Collaboration with Explicit Semiotic Awareness and Emergent Grammar Development](https://arxiv.org/abs/2508.20195)
Append: [A Novel Framework for Automated Explain Vision Model Using Vision-Language Models](https://arxiv.org/abs/2508.20227)
Append: [Robustness Assessment and Enhancement of Text Watermarking for Google's SynthID](https://arxiv.org/abs/2508.20228)
Append: [A Systematic Review on the Generative AI Applications in Human Medical Genomics](https://arxiv.org/abs/2508.20275)
Append: [How Multimodal LLMs Solve Image Tasks: A Lens on Visual Grounding, Task Reasoning, and Answer Decoding](https://arxiv.org/abs/2508.20279)
Append: [ELIXIR: Efficient and LIghtweight model for eXplaIning Recommendations](https://arxiv.org/abs/2508.20312)
Append: [Poison Once, Refuse Forever: Weaponizing Alignment for Injecting Bias in LLMs](https://arxiv.org/abs/2508.20333)
Append: [DFAMS: Dynamic-flow guided Federated Alignment based Multi-prototype Search](https://arxiv.org/abs/2508.20353)
Append: [Unifying Diarization, Separation, and ASR with Multi-Speaker Encoder](https://arxiv.org/abs/2508.20474)
Append: [MERIT: Maximum-normalized Element-wise Ratio for Language Model Large-batch Training](https://arxiv.org/abs/2508.20577)
Append: [GDS Agent: A Graph Algorithmic Reasoning Agent](https://arxiv.org/abs/2508.20637)
Append: [Improving Alignment in LVLMs with Debiased Self-Judgment](https://arxiv.org/abs/2508.20655)
Append: [MobileCLIP2: Improving Multi-Modal Reinforced Training](https://arxiv.org/abs/2508.20691)
Append: [Leveraging Large Language Models for Generating Research Topic Ontologies: A Multi-Disciplinary Study](https://arxiv.org/abs/2508.20693)
Append: [Token Buncher: Shielding LLMs from Harmful Reinforcement Learning Fine-Tuning](https://arxiv.org/abs/2508.20697)
Append: [Transparent Semantic Spaces: A Categorical Approach to Explainable Word Embeddings](https://arxiv.org/abs/2508.20701)
Append: [A Graph-Based Test-Harness for LLM Evaluation](https://arxiv.org/abs/2508.20810)
Append: [OLMoASR: Open Models and Data for Training Robust Speech Recognition Models](https://arxiv.org/abs/2508.20869)
Append: [ChainReaction! Structured Approach with Causal Chains as Intermediate Representations for Improved and Explainable Causal Video Question Answering](https://arxiv.org/abs/2508.21010)
Append: [On the Theoretical Limitations of Embedding-Based Retrieval](https://arxiv.org/abs/2508.21038)
Append: [Probing Pre-Trained Language Models for Cross-Cultural Differences in Values](https://arxiv.org/abs/2203.13722)
Append: [ASVD: Activation-aware Singular Value Decomposition for Compressing Large Language Models](https://arxiv.org/abs/2312.05821)
Append: [LGDE: Local Graph-based Dictionary Expansion](https://arxiv.org/abs/2405.07764)
Append: [Bitune: Leveraging Bidirectional Attention to Improve Decoder-Only LLMs](https://arxiv.org/abs/2405.14862)
Append: [SoAy: A Solution-based LLM API-using Methodology for Academic Information Seeking](https://arxiv.org/abs/2405.15165)
Append: [Enhancing Natural Language Inference Performance with Knowledge Graph for COVID-19 Automated Fact-Checking in Indonesian Language](https://arxiv.org/abs/2409.00061)
Append: [Explaining word embeddings with perfect fidelity: Case study in research impact prediction](https://arxiv.org/abs/2409.15912)
Append: [Query Optimization for Parametric Knowledge Refinement in Retrieval-Augmented Large Language Models](https://arxiv.org/abs/2411.07820)
Append: [Improving the quality of Web-mined Parallel Corpora of Low-Resource Languages using Debiasing Heuristics](https://arxiv.org/abs/2502.19074)
Append: [Are formal and functional linguistic mechanisms dissociated in language models?](https://arxiv.org/abs/2503.11302)
Append: [SaRoHead: Detecting Satire in a Multi-Domain Romanian News Headline Dataset](https://arxiv.org/abs/2504.07612)
Append: [Multilingual Contextualization of Large Language Models for Document-Level Machine Translation](https://arxiv.org/abs/2504.12140)
Append: [DART: Distilling Autoregressive Reasoning to Silent Thought](https://arxiv.org/abs/2506.11752)
Append: [Bridging Compositional and Distributional Semantics: A Survey on Latent Semantic Geometry via AutoEncoder](https://arxiv.org/abs/2506.20083)
Append: [Agent-to-Agent Theory of Mind: Testing Interlocutor Awareness among Large Language Models](https://arxiv.org/abs/2506.22957)
Append: [Adversarial Manipulation of Reasoning Models using Internal Representations](https://arxiv.org/abs/2507.03167)
Append: [Entropy-Memorization Law: Evaluating Memorization Difficulty of Data in LLMs](https://arxiv.org/abs/2507.06056)
Append: [Dynamic Context Compression for Efficient RAG](https://arxiv.org/abs/2507.22931)
Append: [CoCoTen: Detecting Adversarial Inputs to Large Language Models through Latent Space Features of Contextual Co-occurrence Tensors](https://arxiv.org/abs/2508.02997)
Append: [WideSearch: Benchmarking Agentic Broad Info-Seeking](https://arxiv.org/abs/2508.07999)
Append: [Steering Towards Fairness: Mitigating Political Bias in LLMs](https://arxiv.org/abs/2508.08846)
Append: [Estimating Machine Translation Difficulty](https://arxiv.org/abs/2508.10175)
Append: [Beyond the Rosetta Stone: Unification Forces in Generalization Dynamics](https://arxiv.org/abs/2508.11017)
Append: [Explainability of Text Processing and Retrieval Methods: A Survey](https://arxiv.org/abs/2212.07126)
Append: [OLKAVS: An Open Large-Scale Korean Audio-Visual Speech Dataset](https://arxiv.org/abs/2301.06375)
Append: [Network Formation and Dynamics Among Multi-LLMs](https://arxiv.org/abs/2402.10659)
Append: [Noro: Noise-Robust One-shot Voice Conversion with Hidden Speaker Representation Learning](https://arxiv.org/abs/2411.19770)
Append: [Relative Drawing Identification Complexity is Invariant to Modality in Vision-Language Models](https://arxiv.org/abs/2505.10583)
Append: [CoMoE: Contrastive Representation for Mixture-of-Experts in Parameter-Efficient Fine-tuning](https://arxiv.org/abs/2505.17553)
Append: [GLProtein: Global-and-Local Structure Aware Protein Representation Learning](https://arxiv.org/abs/2506.06294)
Append: [A Highly Clean Recipe Dataset with Ingredient States Annotation for State Probing Task](https://arxiv.org/abs/2507.17232)
append_entries: 100
Finish: 2025-08-29 04:25:02.407966
------------------------------------------------------
Started: 2025-08-29 06:24:10.942943
Existing_entries: 1100
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-29 06:24:11.251964
------------------------------------------------------
Started: 2025-08-29 08:21:02.383654
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-29 08:21:02.695950
------------------------------------------------------
Started: 2025-08-29 10:16:45.937171
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-29 10:16:46.221195
------------------------------------------------------
Started: 2025-08-29 12:33:15.766380
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-29 12:33:16.038350
------------------------------------------------------
Started: 2025-08-29 14:15:27.962875
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-29 14:15:28.229205
------------------------------------------------------
Started: 2025-08-29 16:19:50.113091
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-29 16:19:50.406933
------------------------------------------------------
Started: 2025-08-29 18:21:49.439517
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-29 18:21:49.702558
------------------------------------------------------
Started: 2025-08-29 20:17:06.196594
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-29 20:17:06.474443
------------------------------------------------------
Started: 2025-08-29 22:14:58.135345
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-29 22:14:58.450324
------------------------------------------------------
Started: 2025-08-30 01:11:18.244771
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-30 01:11:18.553724
------------------------------------------------------
Started: 2025-08-30 02:53:57.440181
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-30 02:53:57.735334
------------------------------------------------------
Started: 2025-08-30 04:18:08.399603
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-30 04:18:08.657347
------------------------------------------------------
Started: 2025-08-30 06:20:58.822168
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-30 06:20:58.888411
------------------------------------------------------
Started: 2025-08-30 08:18:45.695032
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-30 08:18:45.750931
------------------------------------------------------
Started: 2025-08-30 10:14:40.378350
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-30 10:14:40.434977
------------------------------------------------------
Started: 2025-08-30 12:29:49.305903
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-30 12:29:49.416401
------------------------------------------------------
Started: 2025-08-30 14:12:53.976267
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-30 14:12:54.040684
------------------------------------------------------
Started: 2025-08-30 16:17:39.255689
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-30 16:17:39.313231
------------------------------------------------------
Started: 2025-08-30 18:20:09.742829
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-30 18:20:09.846434
------------------------------------------------------
Started: 2025-08-30 20:15:40.559698
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-30 20:15:40.615973
------------------------------------------------------
Started: 2025-08-30 22:13:48.536004
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-30 22:13:48.611091
------------------------------------------------------
Started: 2025-08-31 01:19:44.856138
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-31 01:19:44.958499
------------------------------------------------------
Started: 2025-08-31 03:04:27.955272
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-31 03:04:28.016563
------------------------------------------------------
Started: 2025-08-31 04:18:34.818471
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-31 04:18:34.939244
------------------------------------------------------
Started: 2025-08-31 06:22:01.816844
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-31 06:22:01.874512
------------------------------------------------------
Started: 2025-08-31 08:18:43.025493
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-31 08:18:43.114515
------------------------------------------------------
Started: 2025-08-31 10:14:48.356249
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-31 10:14:48.431709
------------------------------------------------------
Started: 2025-08-31 12:30:12.120999
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-31 12:30:12.186176
------------------------------------------------------
Started: 2025-08-31 14:13:08.144027
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-31 14:13:08.206929
------------------------------------------------------
Started: 2025-08-31 16:17:48.866650
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-31 16:17:48.952304
------------------------------------------------------
Started: 2025-08-31 18:20:39.444715
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-31 18:20:39.525074
------------------------------------------------------
Started: 2025-08-31 20:16:48.945760
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-31 20:16:49.027352
------------------------------------------------------
Started: 2025-08-31 22:14:39.841777
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-08-31 22:14:39.901085
------------------------------------------------------
Started: 2025-09-01 01:26:28.642736
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-01 01:26:28.719629
------------------------------------------------------
Started: 2025-09-01 03:18:45.750522
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-01 03:18:45.811694
------------------------------------------------------
Started: 2025-09-01 04:29:46.838324
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1190
Summarized using GPT-3.5-turbo
Append: [CoBA: Counterbias Text Augmentation for Mitigating Various Spurious Correlations via Semantic Triples](https://arxiv.org/abs/2508.21083)
Token length: 1195
Summarized using GPT-3.5-turbo
Append: [Mapping Toxic Comments Across Demographics: A Dataset from German Public Broadcasting](https://arxiv.org/abs/2508.21084)
Token length: 1610
Summarized using GPT-3.5-turbo
Append: [Granite Embedding R2 Models](https://arxiv.org/abs/2508.21085)
Token length: 877
Summarized using GPT-3.5-turbo
Append: [TrInk: Ink Generation with Transformer Network](https://arxiv.org/abs/2508.21098)
Token length: 1047
Summarized using GPT-3.5-turbo
Append: [How Does Cognitive Bias Affect Large Language Models? A Case Study on the Anchoring Effect in Price Negotiation Simulations](https://arxiv.org/abs/2508.21137)
Token length: 1375
Summarized using GPT-3.5-turbo
Append: [Can Multimodal LLMs Solve the Basic Perception Problems of Percept-V?](https://arxiv.org/abs/2508.21143)
Token length: 1847
Summarized using GPT-3.5-turbo
Append: [A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers](https://arxiv.org/abs/2508.21148)
Token length: 1240
Summarized using GPT-3.5-turbo
Append: [Quantifying Label-Induced Bias in Large Language Model Self- and Cross-Evaluations](https://arxiv.org/abs/2508.21164)
Token length: 1417
Summarized using GPT-3.5-turbo
Append: [BED-LLM: Intelligent Information Gathering with LLMs and Bayesian Experimental Design](https://arxiv.org/abs/2508.21184)
Token length: 1506
Summarized using GPT-3.5-turbo
Append: [Improving Aviation Safety Analysis: Automated HFACS Classification Using Reinforcement Learning with Group Relative Policy Optimization](https://arxiv.org/abs/2508.21201)
Token length: 915
Summarized using GPT-3.5-turbo
Append: [Enhancing Robustness of Autoregressive Language Models against Orthographic Attacks via Pixel-based Approach](https://arxiv.org/abs/2508.21206)
Token length: 980
Summarized using GPT-3.5-turbo
Append: [Do Self-Supervised Speech Models Exhibit the Critical Period Effects in Language Acquisition?](https://arxiv.org/abs/2508.21210)
Token length: 1208
Summarized using GPT-3.5-turbo
Append: [Decoding Memories: An Efficient Pipeline for Self-Consistency Hallucination Detection](https://arxiv.org/abs/2508.21228)
Token length: 603
Summarized using GPT-3.5-turbo
Append: [Efficient Code Embeddings from Code Generation Models](https://arxiv.org/abs/2508.21290)
Token length: 732
Summarized using GPT-3.5-turbo
Append: [BLUEX Revisited: Enhancing Benchmark Coverage with Automatic Captioning](https://arxiv.org/abs/2508.21294)
Token length: 980
Summarized using GPT-3.5-turbo
Append: [Challenges and Applications of Large Language Models: A Comparison of GPT and DeepSeek family of models](https://arxiv.org/abs/2508.21377)
Token length: 1948
Summarized using GPT-3.5-turbo
Append: [Normality and the Turing Test](https://arxiv.org/abs/2508.21382)
Token length: 1130
Summarized using GPT-3.5-turbo
Append: [AllSummedUp: un framework open-source pour comparer les metriques d'evaluation de resume](https://arxiv.org/abs/2508.21389)
Token length: 1062
Summarized using GPT-3.5-turbo
Append: [Automatic Reviewers Fail to Detect Faulty Reasoning in Research Papers: A New Counterfactual Evaluation Framework](https://arxiv.org/abs/2508.21422)
Token length: 1361
Summarized using GPT-3.5-turbo
Append: [Med-RewardBench: Benchmarking Reward Models and Judges for Medical Multimodal Large Language Models](https://arxiv.org/abs/2508.21430)
Token length: 1276
Summarized using GPT-3.5-turbo
Append: [Discovering Semantic Subdimensions through Disentangled Conceptual Representations](https://arxiv.org/abs/2508.21436)
Token length: 1634
Summarized using GPT-3.5-turbo
Append: [Beyond the Surface: Probing the Ideological Depth of Large Language Models](https://arxiv.org/abs/2508.21448)
Token length: 1617
Summarized using GPT-3.5-turbo
Append: [Igniting Creative Writing in Small Language Models: LLM-as-a-Judge versus Multi-Agent Refined Rewards](https://arxiv.org/abs/2508.21476)
Token length: 1801
Summarized using GPT-3.5-turbo
Append: [HSFN: Hierarchical Selection for Fake News Detection building Heterogeneous Ensemble](https://arxiv.org/abs/2508.21482)
Token length: 1045
Summarized using GPT-3.5-turbo
Append: [L3Cube-MahaSTS: A Marathi Sentence Similarity Dataset and Models](https://arxiv.org/abs/2508.21569)
Token length: 1484
Summarized using GPT-3.5-turbo
Append: [A Survey on Current Trends and Recent Advances in Text Anonymization](https://arxiv.org/abs/2508.21587)
Token length: 1550
Summarized using GPT-3.5-turbo
Append: [Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning](https://arxiv.org/abs/2508.21589)
Token length: 1088
Summarized using GPT-3.5-turbo
Append: [Personality Matters: User Traits Predict LLM Preferences in Multi-Turn Collaborative Tasks](https://arxiv.org/abs/2508.21628)
Token length: 1606
Summarized using GPT-3.5-turbo
Append: [QZhou-Embedding Technical Report](https://arxiv.org/abs/2508.21632)
Token length: 1223
Summarized using GPT-3.5-turbo
Append: [Is this chart lying to me? Automating the detection of misleading visualizations](https://arxiv.org/abs/2508.21675)
Token length: 1421
Summarized using GPT-3.5-turbo
Append: [Not All Parameters Are Created Equal: Smart Isolation Boosts Fine-Tuning Performance](https://arxiv.org/abs/2508.21741)
Token length: 1030
Summarized using GPT-3.5-turbo
Append: [Reasoning-Intensive Regression](https://arxiv.org/abs/2508.21762)
Token length: 1067
Summarized using GPT-3.5-turbo
Append: [PiCSAR: Probabilistic Confidence Selection And Ranking](https://arxiv.org/abs/2508.21787)
Token length: 853
Summarized using GPT-3.5-turbo
Append: [Going over Fine Web with a Fine-Tooth Comb: Technical Report of Indexing Fine Web for Problematic Content Search and Retrieval](https://arxiv.org/abs/2508.21788)
Token length: 1010
Summarized using GPT-3.5-turbo
Append: [Database Normalization via Dual-LLM Self-Refinement](https://arxiv.org/abs/2508.17693)
Token length: 1971
Summarized using GPT-3.5-turbo
Append: [Normalisation of SWIFT Message Counterparties with Feature Extraction and Clustering](https://arxiv.org/abs/2508.21081)
Token length: 1508
Summarized using GPT-3.5-turbo
Append: [Model-Task Alignment Drives Distinct RL Outcomes](https://arxiv.org/abs/2508.21188)
Token length: 1160
Summarized using GPT-3.5-turbo
Append: [Fuzzy, Symbolic, and Contextual: Enhancing LLM Instruction via Cognitive Scaffolding](https://arxiv.org/abs/2508.21204)
Token length: 1436
Summarized using GPT-3.5-turbo
Append: [Designing Smarter Conversational Agents for Kids: Lessons from Cognitive Work and Means-Ends Analyses](https://arxiv.org/abs/2508.21209)
Token length: 1624
Summarized using GPT-3.5-turbo
Append: [CrossTL: A Universal Programming Language Translator with Unified Intermediate Representation](https://arxiv.org/abs/2508.21256)
Token length: 1256
Summarized using GPT-3.5-turbo
Append: [Quantum-Enhanced Natural Language Generation: A Multi-Model Framework with Hybrid Quantum-Classical Architectures](https://arxiv.org/abs/2508.21332)
Token length: 1034
Summarized using GPT-3.5-turbo
Append: [Stairway to Fairness: Connecting Group and Individual Fairness](https://arxiv.org/abs/2508.21334)
Token length: 1961
Summarized using GPT-3.5-turbo
Append: [AHELM: A Holistic Evaluation of Audio-Language Models](https://arxiv.org/abs/2508.21376)
Token length: 1200
Summarized using GPT-3.5-turbo
Append: [From Canonical to Complex: Benchmarking LLM Capabilities in Undergraduate Thermodynamics](https://arxiv.org/abs/2508.21452)
Token length: 1344
Summarized using GPT-3.5-turbo
Append: [Morae: Proactively Pausing UI Agents for User Choices](https://arxiv.org/abs/2508.21456)
Token length: 1255
Summarized using GPT-3.5-turbo
Append: [Accept or Deny? Evaluating LLM Fairness and Performance in Loan Approval across Table-to-Text Serialization Approaches](https://arxiv.org/abs/2508.21512)
Token length: 1154
Summarized using GPT-3.5-turbo
Append: [Summarize-Exemplify-Reflect: Data-driven Insight Distillation Empowers LLMs for Few-shot Tabular Classification](https://arxiv.org/abs/2508.21561)
Token length: 1734
Summarized using GPT-3.5-turbo
Append: [Why Stop at Words? Unveiling the Bigger Picture through Line-Level OCR](https://arxiv.org/abs/2508.21693)
Token length: 1583
Summarized using GPT-3.5-turbo
Append: [Continuous Language Model Interpolation for Dynamic and Controllable Text Generation](https://arxiv.org/abs/2404.07117)
Token length: 1497
Summarized using GPT-3.5-turbo
Append: [Revealing Fine-Grained Values and Opinions in Large Language Models](https://arxiv.org/abs/2406.19238)
Append: [E2LLM: Encoder Elongated Large Language Models for Long-Context Understanding and Reasoning](https://arxiv.org/abs/2409.06679)
Append: [Blind Spot Navigation in Large Language Model Reasoning with Thought Space Explorer](https://arxiv.org/abs/2410.24155)
Append: [A Collaborative Content Moderation Framework for Toxicity Detection based on Conformalized Estimates of Annotation Disagreement](https://arxiv.org/abs/2411.04090)
Append: [Retrieval-Augmented Machine Translation with Unstructured Knowledge](https://arxiv.org/abs/2412.04342)
Append: [Toxicity Begets Toxicity: Unraveling Conversational Chains in Political Podcasts](https://arxiv.org/abs/2501.12640)
Append: [Strategic resource allocation in memory encoding: An efficiency principle shaping language processing](https://arxiv.org/abs/2503.14728)
Append: [Inducing Programmatic Skills for Agentic Tasks](https://arxiv.org/abs/2504.06821)
Append: [DeepTrans: Deep Reasoning Translation via Reinforcement Learning](https://arxiv.org/abs/2504.10187)
Append: [Testing Conviction: An Argumentative Framework for Measuring LLM Political Stability](https://arxiv.org/abs/2504.17052)
Append: [MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced Knowledge Boundary Awareness](https://arxiv.org/abs/2504.21773)
Append: [FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting Framework for Large Language Models](https://arxiv.org/abs/2505.15683)
Append: [L3Cube-MahaEmotions: A Marathi Emotion Recognition Dataset with Synthetic Annotations using CoTR prompting and Large Language Models](https://arxiv.org/abs/2506.00863)
Append: [Quantifying Fairness in LLMs Beyond Tokens: A Semantic and Statistical Perspective](https://arxiv.org/abs/2506.19028)
Append: [Interpretable Mnemonic Generation for Kanji Learning via Expectation-Maximization](https://arxiv.org/abs/2507.05137)
Append: [Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic Thought Reward](https://arxiv.org/abs/2508.12800)
Append: [Transforming Wearable Data into Personal Health Insights using Large Language Model Agents](https://arxiv.org/abs/2406.06464)
Append: [ROSE: A Reward-Oriented Data Selection Framework for LLM Task-Specific Instruction Tuning](https://arxiv.org/abs/2412.00631)
Append: [Refusal Tokens: A Simple Way to Calibrate Refusals in Large Language Models](https://arxiv.org/abs/2412.06748)
Append: [Don't lie to your friends: Learning what you know from collaborative self-play](https://arxiv.org/abs/2503.14481)
Append: [Roll the dice & look before you leap: Going beyond the creative limits of next-token prediction](https://arxiv.org/abs/2504.15266)
Append: [Towards Understanding Camera Motions in Any Video](https://arxiv.org/abs/2504.15376)
Append: [TrustGeoGen: Formal-Verified Data Engine for Trustworthy Multi-modal Geometric Problem Solving](https://arxiv.org/abs/2504.15780)
Append: [BASE-Q: Bias and Asymmetric Scaling Enhanced Rotational Quantization for Large Language Models](https://arxiv.org/abs/2506.15689)
append_entries: 73
Finish: 2025-09-01 04:31:40.032765
------------------------------------------------------
Started: 2025-09-01 06:27:20.985572
Existing_entries: 1073
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-01 06:27:21.199925
------------------------------------------------------
Started: 2025-09-01 08:23:53.342808
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-01 08:23:53.581814
------------------------------------------------------
Started: 2025-09-01 10:18:20.027848
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-01 10:18:20.264381
------------------------------------------------------
Started: 2025-09-01 12:34:40.350425
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-01 12:34:40.561130
------------------------------------------------------
Started: 2025-09-01 14:15:42.685270
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-01 14:15:42.928189
------------------------------------------------------
Started: 2025-09-01 16:19:21.001045
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-01 16:19:21.252197
------------------------------------------------------
Started: 2025-09-01 18:21:48.443279
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-01 18:21:48.681536
------------------------------------------------------
Started: 2025-09-01 20:16:35.910632
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-01 20:16:36.123274
------------------------------------------------------
Started: 2025-09-01 22:14:44.727114
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-01 22:14:44.947208
------------------------------------------------------
Started: 2025-09-02 01:15:52.382075
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-02 01:15:52.597396
------------------------------------------------------
Started: 2025-09-02 03:03:57.076868
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-02 03:03:57.296832
------------------------------------------------------
Started: 2025-09-02 04:19:53.944690
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-02 04:19:54.004224
------------------------------------------------------
Started: 2025-09-02 06:25:31.896286
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-02 06:25:32.005179
------------------------------------------------------
Started: 2025-09-02 08:22:22.453105
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-02 08:22:22.533902
------------------------------------------------------
Started: 2025-09-02 10:17:41.914378
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-02 10:17:42.024289
------------------------------------------------------
Started: 2025-09-02 12:34:40.214934
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-02 12:34:40.310062
------------------------------------------------------
Started: 2025-09-02 14:16:36.037265
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-02 14:16:36.095730
------------------------------------------------------
Started: 2025-09-02 16:20:05.156425
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-02 16:20:05.223769
------------------------------------------------------
Started: 2025-09-02 18:21:20.667667
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-02 18:21:20.781384
------------------------------------------------------
Started: 2025-09-02 20:16:58.805288
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-02 20:16:58.871470
------------------------------------------------------
Started: 2025-09-02 22:13:26.712742
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-02 22:13:26.776373
------------------------------------------------------
Started: 2025-09-03 01:11:49.125893
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-03 01:11:49.214705
------------------------------------------------------
Started: 2025-09-03 02:52:41.888969
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-03 02:52:41.993408
------------------------------------------------------
Started: 2025-09-03 04:23:47.958994
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1544
Summarized using GPT-3.5-turbo
Append: [MultiStream-LLM: Bridging Modalities for Robust Sign Language Translation](https://arxiv.org/abs/2509.00030)
Token length: 1035
Summarized using GPT-3.5-turbo
Append: [Compiling Prompts, Not Crafting Them: A Reproducible Workflow for AI-Assisted Evidence Synthesis](https://arxiv.org/abs/2509.00038)
Token length: 870
Summarized using GPT-3.5-turbo
Append: [What Are Research Hypotheses?](https://arxiv.org/abs/2509.00185)
Token length: 1058
Summarized using GPT-3.5-turbo
Append: [Explainable Chain-of-Thought Reasoning: An Empirical Analysis on State-Aware Reasoning Dynamics](https://arxiv.org/abs/2509.00190)
Token length: 1506
Summarized using GPT-3.5-turbo
Append: [The Rarity Blind Spot: A Framework for Evaluating Statistical Reasoning in LLMs](https://arxiv.org/abs/2509.00245)
Token length: 1232
Summarized using GPT-3.5-turbo
Append: [The Differential Meaning of Models: A Framework for Analyzing the Structural Consequences of Semantic Modeling Decisions](https://arxiv.org/abs/2509.00248)
Token length: 1400
Summarized using GPT-3.5-turbo
Append: [The Temporal Game: A New Perspective on Temporal Relation Extraction](https://arxiv.org/abs/2509.00250)
Token length: 1323
Summarized using GPT-3.5-turbo
Append: [Exploring Reasoning-Infused Text Embedding with Large Language Models for Zero-Shot Dense Retrieval](https://arxiv.org/abs/2509.00276)
Token length: 1223
Summarized using GPT-3.5-turbo
Append: [OpinioRAG: Towards Generating User-Centric Opinion Highlights from Large-scale Online Reviews](https://arxiv.org/abs/2509.00285)
Token length: 1138
Summarized using GPT-3.5-turbo
Append: [Wage Sentiment Indices Derived from Survey Comments via Large Language Models](https://arxiv.org/abs/2509.00290)
Token length: 1793
Summarized using GPT-3.5-turbo
Append: [Balanced Actor Initialization: Stable RLHF Training of Distillation-Based Reasoning Models](https://arxiv.org/abs/2509.00309)
Token length: 919
Summarized using GPT-3.5-turbo
Append: [GIER: Gap-Driven Self-Refinement for Large Language Models](https://arxiv.org/abs/2509.00325)
Token length: 1892
Summarized using GPT-3.5-turbo
Append: [Open Data Synthesis For Deep Research](https://arxiv.org/abs/2509.00375)
Token length: 1026
Summarized using GPT-3.5-turbo
Append: [GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV Cache Eviction](https://arxiv.org/abs/2509.00388)
Token length: 1503
Summarized using GPT-3.5-turbo
Append: [The Resurgence of GCG Adversarial Attacks on Large Language Models](https://arxiv.org/abs/2509.00391)
Token length: 1444
Summarized using GPT-3.5-turbo
Append: [MedSEBA: Synthesizing Evidence-Based Answers Grounded in Evolving Medical Literature](https://arxiv.org/abs/2509.00414)
Token length: 1821
Summarized using GPT-3.5-turbo
Append: [The Gold Medals in an Empty Room: Diagnosing Metalinguistic Reasoning in LLMs with Camlang](https://arxiv.org/abs/2509.00425)
Token length: 1489
Summarized using GPT-3.5-turbo
Append: [GOSU: Retrieval-Augmented Generation with Global-Level Optimized Semantic Unit-Centric Framework](https://arxiv.org/abs/2509.00449)
Token length: 1132
Summarized using GPT-3.5-turbo
Append: [CVPD at QIAS 2025 Shared Task: An Efficient Encoder-Based Approach for Islamic Inheritance Reasoning](https://arxiv.org/abs/2509.00457)
Token length: 1189
Summarized using GPT-3.5-turbo
Append: [TECP: Token-Entropy Conformal Prediction for LLMs](https://arxiv.org/abs/2509.00461)
Token length: 1396
Summarized using GPT-3.5-turbo
Append: [Talk Less, Call Right: Enhancing Role-Play LLM Agents with Automatic Prompt Optimization and Role Prompting](https://arxiv.org/abs/2509.00482)
Token length: 1484
Summarized using GPT-3.5-turbo
Append: [ResearchQA: Evaluating Scholarly Question Answering at Scale Across 75 Fields with Survey-Mined Questions and Rubrics](https://arxiv.org/abs/2509.00496)
Token length: 1472
Summarized using GPT-3.5-turbo
Append: [Entropy-based Coarse and Compressed Semantic Speech Representation Learning](https://arxiv.org/abs/2509.00503)
Token length: 1200
Summarized using GPT-3.5-turbo
Append: [Modeling Motivated Reasoning in Law: Evaluating Strategic Role Conditioning in LLM Summarization](https://arxiv.org/abs/2509.00529)
Token length: 980
Summarized using GPT-3.5-turbo
Append: [Thinking Hard, Going Misaligned: Emergent Misalignment in LLMs](https://arxiv.org/abs/2509.00544)
Token length: 1740
Summarized using GPT-3.5-turbo
Append: [StealthEval: A Probe-Rewrite-Evaluate Workflow for Reliable Benchmarks](https://arxiv.org/abs/2509.00591)
Token length: 1463
Summarized using GPT-3.5-turbo
Append: [Gated Associative Memory: A Parallel O(N) Architecture for Efficient Sequence Modeling](https://arxiv.org/abs/2509.00605)
Token length: 635
Summarized using GPT-3.5-turbo
Append: [A Multi-Strategy Approach for AI-Generated Text Detection](https://arxiv.org/abs/2509.00623)
Token length: 1430
Summarized using GPT-3.5-turbo
Append: [Can Multi-turn Self-refined Single Agent LMs with Retrieval Solve Hard Coding Problems?](https://arxiv.org/abs/2509.00629)
Token length: 1414
Summarized using GPT-3.5-turbo
Append: [Confident, Calibrated, or Complicit: Probing the Trade-offs between Safety Alignment and Ideological Bias in Language Models in Detecting Hate Speech](https://arxiv.org/abs/2509.00673)
Token length: 1113
Summarized using GPT-3.5-turbo
Append: [Router Upcycling: Leveraging Mixture-of-Routers in Mixture-of-Experts Upcycling](https://arxiv.org/abs/2509.00679)
Token length: 1371
Summarized using GPT-3.5-turbo
Append: [Do small language models generate realistic variable-quality fake news headlines?](https://arxiv.org/abs/2509.00680)
Token length: 1648
Summarized using GPT-3.5-turbo
Append: [Text Reinforcement for Multimodal Time Series Forecasting](https://arxiv.org/abs/2509.00687)
Token length: 807
Summarized using GPT-3.5-turbo
Append: [CE-Bench: Towards a Reliable Contrastive Evaluation Benchmark of Interpretability of Sparse Autoencoders](https://arxiv.org/abs/2509.00691)
Token length: 1732
Summarized using GPT-3.5-turbo
Append: [Learning to Shop Like Humans: A Review-driven Retrieval-Augmented Recommendation Framework with LLMs](https://arxiv.org/abs/2509.00698)
Token length: 1596
Summarized using GPT-3.5-turbo
Append: [Reward-Weighted Sampling: Enhancing Non-Autoregressive Characteristics in Masked Diffusion LLMs](https://arxiv.org/abs/2509.00707)
Token length: 1124
Summarized using GPT-3.5-turbo
Append: [Designing LMS and Instructional Strategies for Integrating Generative-Conversational AI](https://arxiv.org/abs/2509.00709)
Token length: 1670
Summarized using GPT-3.5-turbo
Append: [LLM Encoder vs. Decoder: Robust Detection of Chinese AI-Generated Text with LoRA](https://arxiv.org/abs/2509.00731)
Token length: 1321
Summarized using GPT-3.5-turbo
Append: [Decomposing and Revising What Language Models Generate](https://arxiv.org/abs/2509.00765)
Token length: 1404
Summarized using GPT-3.5-turbo
Append: [LegalChainReasoner: A Legal Chain-guided Framework for Criminal Judicial Opinion Generation](https://arxiv.org/abs/2509.00783)
Token length: 1496
Summarized using GPT-3.5-turbo
Append: [CaresAI at BioCreative IX Track 1 -- LLM for Biomedical QA](https://arxiv.org/abs/2509.00806)
Token length: 1121
Summarized using GPT-3.5-turbo
Append: [TMT: A Simple Way to Translate Topic Models Using Dictionaries](https://arxiv.org/abs/2509.00822)
Token length: 1185
Summarized using GPT-3.5-turbo
Append: [Neural Models and Language Model Prompting for the Multidimensional Evaluation of Open-Ended Conversations](https://arxiv.org/abs/2509.00841)
Token length: 1333
Summarized using GPT-3.5-turbo
Append: [Negative Matters: Multi-Granularity Hard-Negative Synthesis and Anchor-Token-Aware Pooling for Enhanced Text Embeddings](https://arxiv.org/abs/2509.00842)
Token length: 1319
Summarized using GPT-3.5-turbo
Append: [Prompting Away Stereotypes? Evaluating Bias in Text-to-Image Models for Occupations](https://arxiv.org/abs/2509.00849)
Token length: 1322
Summarized using GPT-3.5-turbo
Append: [Exploring and Mitigating Fawning Hallucinations in Large Language Models](https://arxiv.org/abs/2509.00869)
Token length: 1609
Summarized using GPT-3.5-turbo
Append: [EviNote-RAG: Enhancing RAG Models via Answer-Supportive Evidence Notes](https://arxiv.org/abs/2509.00877)
Token length: 1126
Summarized using GPT-3.5-turbo
Append: [SeLeRoSa: Sentence-Level Romanian Satire Detection Dataset](https://arxiv.org/abs/2509.00893)
Token length: 1493
Summarized using GPT-3.5-turbo
Append: [Supervised In-Context Fine-Tuning for Generative Sequence Labeling](https://arxiv.org/abs/2509.00921)
Token length: 1416
Summarized using GPT-3.5-turbo
Append: [MedCOD: Enhancing English-to-Spanish Medical Translation of Large Language Models Using Enriched Chain-of-Dictionary Framework](https://arxiv.org/abs/2509.00934)
Append: [Structure and Destructure: Dual Forces in the Making of Knowledge Engines](https://arxiv.org/abs/2509.00949)
Append: [RPRO:Ranked Preference Reinforcement Optimization for Enhancing Medical QA and Diagnostic Reasoning](https://arxiv.org/abs/2509.00974)
Append: [Performance Analysis of Supervised Machine Learning Algorithms for Text Classification](https://arxiv.org/abs/2509.00983)
Append: [Ranking of Bangla Word Graph using Graph-based Ranking Algorithms](https://arxiv.org/abs/2509.01011)
Append: [We Politely Insist: Your LLM Must Learn the Persian Art of Taarof](https://arxiv.org/abs/2509.01035)
Append: [A Dynamic Fusion Model for Consistent Crisis Response](https://arxiv.org/abs/2509.01053)
Append: [Speaking at the Right Level: Literacy-Controlled Counterspeech Generation with RAG-RL](https://arxiv.org/abs/2509.01058)
Append: [Assessing Large Language Models on Islamic Legal Reasoning: Evidence from Inheritance Law Evaluation](https://arxiv.org/abs/2509.01081)
Append: [A Paradigm Gap in Urdu](https://arxiv.org/abs/2509.01084)
Append: [Privacy-Preserving Reasoning with Knowledge-Distilled Parametric Retrieval Augmented Generation](https://arxiv.org/abs/2509.01088)
Append: [REFRAG: Rethinking RAG based Decoding](https://arxiv.org/abs/2509.01092)
Append: [Natural Context Drift Undermines the Natural Language Understanding of Large Language Models](https://arxiv.org/abs/2509.01093)
Append: [Dream-Coder 7B: An Open Diffusion Language Model for Code](https://arxiv.org/abs/2509.01142)
Append: [Zero-shot Cross-lingual NER via Mitigating Language Difference: An Entity-aligned Translation Perspective](https://arxiv.org/abs/2509.01147)
Append: [Joint Information Extraction Across Classical and Modern Chinese with Tea-MOELoRA](https://arxiv.org/abs/2509.01158)
Append: [Enhancing Large Language Model for Knowledge Graph Completion via Structure-Aware Alignment-Tuning](https://arxiv.org/abs/2509.01166)
Append: [Modular Techniques for Synthetic Long-Context Data Generation in Language Model Training and Evaluation](https://arxiv.org/abs/2509.01185)
Append: [Statutory Construction and Interpretation for Artificial Intelligence](https://arxiv.org/abs/2509.01186)
Append: [Efficient Large Language Models with Zero-Shot Adjustable Acceleration](https://arxiv.org/abs/2509.01190)
Append: [SimulMEGA: MoE Routers are Advanced Policy Makers for Simultaneous Speech Translation](https://arxiv.org/abs/2509.01200)
Append: [Mitigating Catastrophic Forgetting in Continual Learning through Model Growth](https://arxiv.org/abs/2509.01213)
Append: [DaMoC: Efficiently Selecting the Optimal Large Language Model for Fine-tuning Domain Taks Based on Data and Model Compression](https://arxiv.org/abs/2509.01221)
Append: [Rethinking the Chain-of-Thought: The Roles of In-Context Learning and Pre-trained Priors](https://arxiv.org/abs/2509.01236)
Append: [Annotation and modeling of emotions in a textual corpus: an evaluative approach](https://arxiv.org/abs/2509.01260)
Append: [Culture is Everywhere: A Call for Intentionally Cultural Evaluation](https://arxiv.org/abs/2509.01301)
Append: [TableZoomer: A Collaborative Agent Framework for Large-scale Table Question Answering](https://arxiv.org/abs/2509.01312)
Append: [Can Smaller LLMs do better? Unlocking Cross-Domain Potential through Parameter-Efficient Fine-Tuning for Text Summarization](https://arxiv.org/abs/2509.01314)
Append: [LongCat-Flash Technical Report](https://arxiv.org/abs/2509.01322)
Append: [KoBLEX: Open Legal Question Answering with Multi-hop Reasoning](https://arxiv.org/abs/2509.01324)
Append: [Can Large Language Models Master Complex Card Games?](https://arxiv.org/abs/2509.01328)
Append: [Reasoning Vectors: Transferring Chain-of-Thought Capabilities via Task Arithmetic](https://arxiv.org/abs/2509.01363)
Append: [WATCHED: A Web AI Agent Tool for Combating Hate Speech by Expanding Data](https://arxiv.org/abs/2509.01379)
Append: [ABCD-LINK: Annotation Bootstrapping for Cross-Document Fine-Grained Links](https://arxiv.org/abs/2509.01387)
Append: [Analysing the Language of Neural Audio Codecs](https://arxiv.org/abs/2509.01390)
Append: [LLMs cannot spot math errors, even when allowed to peek into the solution](https://arxiv.org/abs/2509.01395)
Append: [Vis-CoT: A Human-in-the-Loop Framework for Interactive Visualization and Intervention in LLM Chain-of-Thought Reasoning](https://arxiv.org/abs/2509.01412)
Append: [On the Alignment of Large Language Models with Global Human Opinion](https://arxiv.org/abs/2509.01418)
Append: [Trusted Uncertainty in Large Language Models: A Unified Framework for Confidence Calibration and Risk-Controlled Refusal](https://arxiv.org/abs/2509.01455)
Append: [Robust Knowledge Editing via Explicit Reasoning Chains for Distractor-Resilient Multi-Hop QA](https://arxiv.org/abs/2509.01468)
Append: [Do Retrieval Augmented Language Models Know When They Don't Know?](https://arxiv.org/abs/2509.01476)
Append: [MeVe: A Modular System for Memory Verification and Effective Context Control in Language Models](https://arxiv.org/abs/2509.01514)
Append: [Service, Solidarity, and Self-Help: A Comparative Topic Modeling Analysis of Community Unionism in the Boot and Shoe Union and Unite Community](https://arxiv.org/abs/2509.01529)
Append: [CAT: Causal Attention Tuning For Injecting Fine-grained Causal Knowledge into Large Language Models](https://arxiv.org/abs/2509.01535)
Append: [In-N-Out: A Parameter-Level API Graph Dataset for Tool Agents](https://arxiv.org/abs/2509.01560)
Append: [Enhancing Uncertainty Estimation in LLMs with Expectation of Aggregated Internal Belief](https://arxiv.org/abs/2509.01564)
Append: [Testing the assumptions about the geometry of sentence embedding spaces: the cosine measure need not apply](https://arxiv.org/abs/2509.01606)
Append: [Benchmarking the Detection of LLMs-Generated Modern Chinese Poetry](https://arxiv.org/abs/2509.01620)
Append: [TransGAT: Transformer-Based Graph Neural Networks for Multi-Dimensional Automated Essay Scoring](https://arxiv.org/abs/2509.01640)
Append: [Parallel Needleman-Wunsch on CUDA to measure word similarity based on phonetic transcriptions](https://arxiv.org/abs/2509.01654)
Append: [Bridging Thoughts and Words: Graph-Based Intent-Semantic Joint Learning for Fake News Detection](https://arxiv.org/abs/2509.01660)
Append: [chDzDT: Word-level morphology-aware language model for Algerian social media text](https://arxiv.org/abs/2509.01772)
Append: [Flaw or Artifact? Rethinking Prompt Sensitivity in Evaluating LLMs](https://arxiv.org/abs/2509.01790)
Append: [Mic Drop or Data Flop? Evaluating the Fitness for Purpose of AI Voice Interviewers for Data Collection within Quantitative & Qualitative Research Contexts](https://arxiv.org/abs/2509.01814)
Append: [Extracting OPQRST in Electronic Health Records using Large Language Models with Reasoning](https://arxiv.org/abs/2509.01885)
Append: [Weakly Supervised Medical Entity Extraction and Linking for Chief Complaints](https://arxiv.org/abs/2509.01899)
Append: [DRAssist: Dispute Resolution Assistance using Large Language Models](https://arxiv.org/abs/2509.01962)
Append: [StructCoh: Structured Contrastive Learning for Context-Aware Text Semantic Matching](https://arxiv.org/abs/2509.02033)
Append: [DeepSeek performs better than other Large Language Models in Dental Cases](https://arxiv.org/abs/2509.02036)
Append: [NADI 2025: The First Multidialectal Arabic Speech Processing Shared Task](https://arxiv.org/abs/2509.02038)
Append: [Attributes as Textual Genes: Leveraging LLMs as Genetic Algorithm Simulators for Conditional Synthetic Data Generation](https://arxiv.org/abs/2509.02040)
Append: [How Instruction-Tuning Imparts Length Control: A Cross-Lingual Mechanistic Analysis](https://arxiv.org/abs/2509.02075)
Append: [Better by Comparison: Retrieval-Augmented Contrastive Reasoning for Automatic Prompt Optimization](https://arxiv.org/abs/2509.02093)
Append: [JudgeAgent: Dynamically Evaluate LLMs with Agent-as-Interviewer](https://arxiv.org/abs/2509.02097)
Append: [CMRAG: Co-modality-based document retrieval and visual question answering](https://arxiv.org/abs/2509.02123)
Append: [AMBEDKAR-A Multi-level Bias Elimination through a Decoding Approach with Knowledge Augmentation for Robust Constitutional Alignment of Language Models](https://arxiv.org/abs/2509.02133)
Append: [Meta-Pretraining for Zero-Shot Cross-Lingual Named Entity Recognition in Low-Resource Philippine Languages](https://arxiv.org/abs/2509.02160)
Append: [Avoidance Decoding for Diverse Multi-Branch Story Generation](https://arxiv.org/abs/2509.02170)
Append: [FActBench: A Benchmark for Fine-grained Automatic Evaluation of LLM-Generated Text in the Medical Domain](https://arxiv.org/abs/2509.02198)
Append: [Towards Fundamental Language Models: Does Linguistic Competence Scale with Model Size?](https://arxiv.org/abs/2509.02225)
Append: [LLMs and their Limited Theory of Mind: Evaluating Mental State Annotations in Situated Dialogue](https://arxiv.org/abs/2509.02292)
Append: [DCPO: Dynamic Clipping Policy Optimization](https://arxiv.org/abs/2509.02333)
Append: [Implicit Reasoning in Large Language Models: A Comprehensive Survey](https://arxiv.org/abs/2509.02350)
Append: [Towards Temporal Knowledge-Base Creation for Fine-Grained Opinion Analysis with Language Models](https://arxiv.org/abs/2509.02363)
Append: [An Ensemble Classification Approach in A Multi-Layered Large Language Model Framework for Disease Prediction](https://arxiv.org/abs/2509.02446)
Append: [EmoPerso: Enhancing Personality Detection with Self-Supervised Emotion-Aware Modelling](https://arxiv.org/abs/2509.02450)
Append: [Do LLMs Adhere to Label Definitions? Examining Their Receptivity to External Label Definitions](https://arxiv.org/abs/2509.02452)
Append: [SpecEval: Evaluating Model Adherence to Behavior Specifications](https://arxiv.org/abs/2509.02464)
Append: [GRAM-R$^2$: Self-Training Generative Foundation Reward Models for Reward Reasoning](https://arxiv.org/abs/2509.02492)
Append: [MoSEs: Uncertainty-Aware AI-Generated Text Detection via Mixture of Stylistics Experts with Conditional Thresholds](https://arxiv.org/abs/2509.02499)
Append: [L3Cube-IndicHeadline-ID: A Dataset for Headline Identification and Semantic Evaluation in Low-Resource Indian Languages](https://arxiv.org/abs/2509.02503)
Append: [The Forgotten Code: Validating a Century-Old Translation System with AI](https://arxiv.org/abs/2509.02506)
Append: [Top-H Decoding: Adapting the Creativity and Coherence with Bounded Entropy in Text Generation](https://arxiv.org/abs/2509.02510)
Append: [Comparative Study of Pre-Trained BERT and Large Language Models for Code-Mixed Named Entity Recognition](https://arxiv.org/abs/2509.02514)
Append: [Implicit Actor Critic Coupling via a Supervised Learning Framework for RLVR](https://arxiv.org/abs/2509.02522)
Append: [Flavors of Moonshine: Tiny Specialized ASR Models for Edge Devices](https://arxiv.org/abs/2509.02523)
Append: [Jointly Reinforcing Diversity and Quality in Language Model Generations](https://arxiv.org/abs/2509.02534)
Append: [PalmX 2025: The First Shared Task on Benchmarking LLMs on Arabic and Islamic Culture](https://arxiv.org/abs/2509.02550)
Append: [Traj-MLLM: Can Multimodal Large Language Models Reform Trajectory Data Mining?](https://arxiv.org/abs/2509.00053)
Append: [Language and Experience: A Computational Model of Social Learning in Complex Tasks](https://arxiv.org/abs/2509.00074)
Append: [Amplifying Emotional Signals: Data-Efficient Deep Learning for Robust Speech Emotion Recognition](https://arxiv.org/abs/2509.00077)
Append: [ChipChat: Low-Latency Cascaded Conversational Agent in MLX](https://arxiv.org/abs/2509.00078)
Append: [Data Cartography for Detecting Memorization Hotspots and Guiding Data Interventions in Generative Models](https://arxiv.org/abs/2509.00083)
Append: [Learning to Refine: Self-Refinement of Parallel Reasoning in LLMs](https://arxiv.org/abs/2509.00084)
Append: [Ensemble Debates with Local Large Language Models for AI Alignment](https://arxiv.org/abs/2509.00091)
Append: [Automatic Pronunciation Error Detection and Correction of the Holy Quran's Learners Using Deep Learning](https://arxiv.org/abs/2509.00094)
Append: [Pruning Weights but Not Truth: Safeguarding Truthfulness While Pruning LLMs](https://arxiv.org/abs/2509.00096)
Append: [Adaptive Monitoring and Real-World Evaluation of Agentic AI Systems](https://arxiv.org/abs/2509.00115)
Append: [Evaluating the Effectiveness of Transformer Layers in Wav2Vec 2.0, XLS-R, and Whisper for Speaker Identification Tasks](https://arxiv.org/abs/2509.00230)
Append: [KG-RAG: Enhancing GUI Agent Decision-Making via Knowledge Graph-Driven Retrieval-Augmented Generation](https://arxiv.org/abs/2509.00366)
Append: [LLM-Assisted Iterative Evolution with Swarm Intelligence Toward SuperBrain](https://arxiv.org/abs/2509.00510)
Append: [ERank: Fusing Supervised Fine-Tuning and Reinforcement Learning for Effective and Efficient Text Reranking](https://arxiv.org/abs/2509.00520)
Append: [Advanced spectral clustering for heterogeneous data in credit risk monitoring systems](https://arxiv.org/abs/2509.00546)
Append: [On Verifiable Legal Reasoning: A Multi-Agent Framework with Formalized Knowledge Representations](https://arxiv.org/abs/2509.00710)
Append: [L-MARS -- Legal Multi-Agent Workflow with Orchestrated Reasoning and Agentic Search](https://arxiv.org/abs/2509.00761)
Append: [Aligning Reasoning LLMs for Materials Discovery with Physics-aware Rejection Sampling](https://arxiv.org/abs/2509.00768)
Append: [ChatCLIDS: Simulating Persuasive AI Dialogues to Promote Closed-Loop Insulin Adoption in Type 1 Diabetes Care](https://arxiv.org/abs/2509.00891)
Append: [DTRNet: Dynamic Token Routing Network to Reduce Quadratic Costs in Transformers](https://arxiv.org/abs/2509.00925)
Append: [Self-Exploring Language Models for Explainable Link Forecasting on Temporal Graphs via Reinforcement Learning](https://arxiv.org/abs/2509.00975)
Append: [Hybrid Topic-Semantic Labeling and Graph Embeddings for Unsupervised Legal Document Clustering](https://arxiv.org/abs/2509.00990)
Append: [MEPT: Mixture of Expert Prompt Tuning as a Manifold Mapper](https://arxiv.org/abs/2509.00996)
Append: [Analysis of Error Sources in LLM-based Hypothesis Search for Few-Shot Rule Induction](https://arxiv.org/abs/2509.01016)
Append: [Chronotome: Real-Time Topic Modeling for Streaming Embedding Spaces](https://arxiv.org/abs/2509.01051)
Append: [FlashAdventure: A Benchmark for GUI Agents Solving Full Story Arcs in Diverse Adventure Games](https://arxiv.org/abs/2509.01052)
Append: [VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use](https://arxiv.org/abs/2509.01055)
Append: [Do Video Language Models Really Know Where to Look? Diagnosing Attention Failures in Video Language Models](https://arxiv.org/abs/2509.01167)
Append: [Question-to-Knowledge: Multi-Agent Generation of Inspectable Facts for Product Mapping](https://arxiv.org/abs/2509.01182)
Append: [GradeSQL: Outcome Reward Models for Ranking SQL Queries from Large Language Models](https://arxiv.org/abs/2509.01308)
Append: [Towards High Data Efficiency in Reinforcement Learning with Verifiable Reward](https://arxiv.org/abs/2509.01321)
Append: [LLM-Guided Semantic Relational Reasoning for Multimodal Intent Recognition](https://arxiv.org/abs/2509.01337)
Append: [MixedG2P-T5: G2P-free Speech Synthesis for Mixed-script texts using Speech Self-Supervised Learning and Language Model](https://arxiv.org/abs/2509.01391)
Append: [ArabEmoNet: A Lightweight Hybrid 2D CNN-BiLSTM Model with Attention for Robust Arabic Speech Emotion Recognition](https://arxiv.org/abs/2509.01401)
Append: [CSRM-LLM: Embracing Multilingual LLMs for Cold-Start Relevance Matching in Emerging E-commerce Markets](https://arxiv.org/abs/2509.01566)
Append: [Reinforced Visual Perception with Tools](https://arxiv.org/abs/2509.01656)
Append: [An LLM-enabled semantic-centric framework to consume privacy policies](https://arxiv.org/abs/2509.01716)
Append: [ShortageSim: Simulating Drug Shortages under Information Asymmetry](https://arxiv.org/abs/2509.01813)
Append: [RSCC: A Large-Scale Remote Sensing Change Caption Dataset for Disaster Events](https://arxiv.org/abs/2509.01907)
Append: [Oyster-I: Beyond Refusal -- Constructive Safety Alignment for Responsible Language Models](https://arxiv.org/abs/2509.01909)
Append: [How Real Is AI Tutoring? Comparing Simulated and Human Dialogues in One-on-One Instruction](https://arxiv.org/abs/2509.01914)
Append: [EigenBench: A Comparative Behavioral Measure of Value Alignment](https://arxiv.org/abs/2509.01938)
Append: [Content and Engagement Trends in COVID-19 YouTube Videos: Evidence from the Late Pandemic](https://arxiv.org/abs/2509.01954)
Append: [From Attack Descriptions to Vulnerabilities: A Sentence Transformer-Based Approach](https://arxiv.org/abs/2509.02077)
Append: [E-THER: A PCT-Grounded Dataset for Benchmarking Empathic AI](https://arxiv.org/abs/2509.02100)
Append: [Understanding Space Is Rocket Science - Only Top Reasoning Models Can Solve Spatial Understanding Tasks](https://arxiv.org/abs/2509.02175)
Append: [Spectrogram Patch Codec: A 2D Block-Quantized VQ-VAE and HiFi-GAN for Neural Speech Coding](https://arxiv.org/abs/2509.02244)
Append: [Evaluating Cumulative Spectral Gradient as a Complexity Measure](https://arxiv.org/abs/2509.02399)
Append: [AppCopilot: Toward General, Accurate, Long-Horizon, and Efficient Mobile Agent](https://arxiv.org/abs/2509.02444)
Append: [FLM-Audio: Natural Monologues Improves Native Full-Duplex Chatbots via Dual Training](https://arxiv.org/abs/2509.02521)
Append: [UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2509.02544)
Append: [The Landscape of Agentic Reinforcement Learning for LLMs: A Survey](https://arxiv.org/abs/2509.02547)
Append: [DynaGuard: A Dynamic Guardrail Model With User-Defined Policies](https://arxiv.org/abs/2509.02563)
Append: [Similarity between Units of Natural Language: The Transition from Coarse to Fine Estimation](https://arxiv.org/abs/2210.14275)
Append: [Rule-Guided Joint Embedding Learning over Knowledge Graphs](https://arxiv.org/abs/2401.02968)
Append: [Semantic Parsing for Question Answering over Knowledge Graphs](https://arxiv.org/abs/2401.06772)
Append: [Into the crossfire: evaluating the use of a language model to crowdsource gun violence reports](https://arxiv.org/abs/2401.12989)
Append: [Whose LLM is it Anyway? Linguistic Comparison and LLM Attribution for GPT-3.5, GPT-4 and Bard](https://arxiv.org/abs/2402.14533)
Append: [Guiding Large Language Models to Post-Edit Machine Translation with Error Annotations](https://arxiv.org/abs/2404.07851)
Append: [Why Not Transform Chat Large Language Models to Non-English?](https://arxiv.org/abs/2405.13923)
Append: [Intrinsic Test of Unlearning Using Parametric Knowledge Traces](https://arxiv.org/abs/2406.11614)
Append: [MEGen: Generative Backdoor into Large Language Models via Model Editing](https://arxiv.org/abs/2408.10722)
Append: [On the Diagram of Thought](https://arxiv.org/abs/2409.10038)
Append: [Mitigating Semantic Leakage in Cross-lingual Embeddings via Orthogonality Constraint](https://arxiv.org/abs/2409.15664)
Append: [Learning by Surprise: Surplexity for Mitigating Model Collapse in Generative AI](https://arxiv.org/abs/2410.12341)
Append: [How Does Knowledge Selection Help Retrieval Augmented Generation?](https://arxiv.org/abs/2410.13258)
Append: [Distill Visual Chart Reasoning Ability from LLMs to MLLMs](https://arxiv.org/abs/2410.18798)
Append: [A Computational Method for Measuring "Open Codes" in Qualitative Analysis](https://arxiv.org/abs/2411.12142)
Append: [Evaluating Language Models as Synthetic Data Generators](https://arxiv.org/abs/2412.03679)
Append: [Opportunities and Challenges of Large Language Models for Low-Resource Languages in Humanities Research](https://arxiv.org/abs/2412.04497)
Append: [Benchmarking LLMs for Mimicking Child-Caregiver Language in Interaction](https://arxiv.org/abs/2412.09318)
Append: [Truthful Text Sanitization Guided by Inference Attacks](https://arxiv.org/abs/2412.12928)
Append: [Acquisition of Recursive Possessives and Recursive Locatives in Mandarin](https://arxiv.org/abs/2412.16556)
Append: [Improving Low-Resource Machine Translation via Cross-Linguistic Transfer from Typologically Similar High-Resource Languages](https://arxiv.org/abs/2501.00045)
Append: [Echoes in AI: Quantifying lack of plot diversity in LLM outputs](https://arxiv.org/abs/2501.00273)
Append: [Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning](https://arxiv.org/abs/2502.03275)
Append: [Injecting Domain-Specific Knowledge into Large Language Models: A Comprehensive Survey](https://arxiv.org/abs/2502.10708)
Append: [LogiDynamics: Unraveling the Dynamics of Logical Inference in Large Language Model Reasoning](https://arxiv.org/abs/2502.11176)
Append: [InsBank: Evolving Instruction Subset for Ongoing Alignment](https://arxiv.org/abs/2502.11419)
Append: [Instruction Tuning on Public Government and Cultural Data for Low-Resource Language: a Case Study in Kazakh](https://arxiv.org/abs/2502.13647)
Append: [Multilingual != Multicultural: Evaluating Gaps Between Multilingual Capabilities and Cultural Alignment in LLMs](https://arxiv.org/abs/2502.16534)
Append: [Automatic Input Rewriting Improves Translation with Large Language Models](https://arxiv.org/abs/2502.16682)
Append: [A Causal Lens for Evaluating Faithfulness Metrics](https://arxiv.org/abs/2502.18848)
Append: [Personalized Causal Graph Reasoning for LLMs: An Implementation for Dietary Recommendations](https://arxiv.org/abs/2503.00134)
Append: [Interpretation Gaps in LLM-Assisted Comprehension of Privacy Documents](https://arxiv.org/abs/2503.12225)
Append: [General Table Question Answering via Answer-Formula Joint Generation](https://arxiv.org/abs/2503.12345)
Append: [UniBERT: Adversarial Training for Language-Universal Representations](https://arxiv.org/abs/2503.12608)
Append: [Unmasking Deceptive Visuals: Benchmarking Multimodal Large Language Models on Misleading Chart Question Answering](https://arxiv.org/abs/2503.18172)
Append: [CoRanking: Collaborative Ranking with Small and Large Ranking Agents](https://arxiv.org/abs/2503.23427)
Append: [AIR: A Systematic Analysis of Annotations, Instructions, and Response Pairs in Preference Dataset](https://arxiv.org/abs/2504.03612)
Append: [RankAlign: A Ranking View of the Generator-Validator Gap in Large Language Models](https://arxiv.org/abs/2504.11381)
Append: [AskQE: Question Answering as Automatic Evaluation for Machine Translation](https://arxiv.org/abs/2504.11582)
Append: [Deep Binding of Language Model Virtual Personas: a Study on Approximating Political Partisan Misperceptions](https://arxiv.org/abs/2504.11673)
Append: [Bias Analysis and Mitigation through Protected Attribute Detection and Regard Classification](https://arxiv.org/abs/2504.14212)
Append: [Improving Informally Romanized Language Identification](https://arxiv.org/abs/2504.21540)
Append: [A Survey on Progress in LLM Alignment from the Perspective of Reward Design](https://arxiv.org/abs/2505.02666)
Append: [Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning of LLMs](https://arxiv.org/abs/2505.11277)
Append: [When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs](https://arxiv.org/abs/2505.11423)
Append: [From Automation to Autonomy: A Survey on Large Language Models in Scientific Discovery](https://arxiv.org/abs/2505.13259)
Append: [MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol](https://arxiv.org/abs/2505.14590)
Append: [Can Large Language Models be Effective Online Opinion Miners?](https://arxiv.org/abs/2505.15695)
Append: [Unveil Multi-Picture Descriptions for Multilingual Mild Cognitive Impairment Detection via Contrastive Learning](https://arxiv.org/abs/2505.17067)
Append: [From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning](https://arxiv.org/abs/2505.17117)
Append: [Cog-TiPRO: Iterative Prompt Refinement with LLMs to Detect Cognitive Decline via Longitudinal Voice Assistant Commands](https://arxiv.org/abs/2505.17137)
Append: [Federated Retrieval-Augmented Generation: A Systematic Mapping Study](https://arxiv.org/abs/2505.18906)
Append: [Automated Essay Scoring Incorporating Annotations from Automated Feedback Systems](https://arxiv.org/abs/2505.22771)
Append: [Multiple LLM Agents Debate for Equitable Cultural Alignment](https://arxiv.org/abs/2505.24671)
Append: [Should I Share this Translation? Evaluating Quality Feedback for User Reliance on Machine Translation](https://arxiv.org/abs/2505.24683)
Append: [Strategic Discourse Assessment: The Crooked Path to Innocence](https://arxiv.org/abs/2506.01195)
Append: [FinS-Pilot: A Benchmark for Online Financial RAG System](https://arxiv.org/abs/2506.02037)
Append: [LLMEval-Med: A Real-world Clinical Benchmark for Medical LLMs with Physician Validation](https://arxiv.org/abs/2506.04078)
Append: [Surface Fairness, Deep Bias: A Comparative Study of Bias in Language Models](https://arxiv.org/abs/2506.10491)
Append: [A Structured Bangla Dataset of Disease-Symptom Associations to Improve Diagnostic Accuracy](https://arxiv.org/abs/2506.13610)
Append: [TPTT: Transforming Pretrained Transformers into Titans](https://arxiv.org/abs/2506.17671)
Append: [Flow-Modulated Scoring for Semantic-Aware Knowledge Graph Completion](https://arxiv.org/abs/2506.23137)
Append: [MedVAL: Toward Expert-Level Medical Text Validation with Language Models](https://arxiv.org/abs/2507.03152)
Append: [Agentic-R1: Distilled Dual-Strategy Reasoning](https://arxiv.org/abs/2507.05707)
Append: [CRMAgent: A Multi-Agent LLM System for E-Commerce CRM Message Template Generation](https://arxiv.org/abs/2507.08325)
Append: [How Important is `Perfect' English for Machine Translation Prompts?](https://arxiv.org/abs/2507.09509)
Append: [Cross-Domain Transfer and Few-Shot Learning for Personal Identifiable Information Recognition](https://arxiv.org/abs/2507.11862)
Append: [Label Unification for Cross-Dataset Generalization in Cybersecurity NER](https://arxiv.org/abs/2507.13870)
Append: [Bottom-up Domain-specific Superintelligence: A Reliable Knowledge Graph is What We Need](https://arxiv.org/abs/2507.13966)
Append: [From Disagreement to Understanding: The Case for Ambiguity Detection in NLI](https://arxiv.org/abs/2507.15114)
Append: [Towards Compute-Optimal Many-Shot In-Context Learning](https://arxiv.org/abs/2507.16217)
Append: [Persona Vectors: Monitoring and Controlling Character Traits in Language Models](https://arxiv.org/abs/2507.21509)
Append: [MAQuA: Adaptive Question-Asking for Multidimensional Mental Health Screening using Item Response Theory](https://arxiv.org/abs/2508.07279)
Append: [Weakly Supervised Fine-grained Span-Level Framework for Chinese Radiology Report Quality Assurance](https://arxiv.org/abs/2508.08876)
Append: [A Novel Kuhnian Ontology for Epistemic Classification of STM Scholarly Articles](https://arxiv.org/abs/2002.03531)
Append: [Cross-Modal Adapter for Vision-Language Retrieval](https://arxiv.org/abs/2211.09623)
Append: [Exploring Large Language Models in Resolving Environment-Related Crash Bugs: Localizing and Repairing](https://arxiv.org/abs/2312.10448)
Append: [Towards Incremental Learning in Large Language Models: A Critical Review](https://arxiv.org/abs/2404.18311)
Append: [A Survey on Vision-Language-Action Models for Embodied AI](https://arxiv.org/abs/2405.14093)
Append: [Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration](https://arxiv.org/abs/2405.14314)
Append: [Advancing Grounded Multimodal Named Entity Recognition via LLM-Based Reformulation and Box-Based Segmentation](https://arxiv.org/abs/2406.07268)
Append: [Explaining Length Bias in LLM-Based Preference Evaluations](https://arxiv.org/abs/2407.01085)
Append: [Learning to (Learn at Test Time): RNNs with Expressive Hidden States](https://arxiv.org/abs/2407.04620)
Append: [Learn while Unlearn: An Iterative Unlearning Framework for Generative Language Models](https://arxiv.org/abs/2407.20271)
Append: [Efficient Detection of Toxic Prompts in Large Language Models](https://arxiv.org/abs/2408.11727)
Append: [A Law of Next-Token Prediction in Large Language Models](https://arxiv.org/abs/2408.13442)
Append: [DivScene: Towards Open-Vocabulary Object Navigation with Large Vision Language Models in Diverse Scenes](https://arxiv.org/abs/2410.02730)
Append: [Exploring Response Uncertainty in MLLMs: An Empirical Evaluation under Misleading Scenarios](https://arxiv.org/abs/2411.02708)
Append: [Re-examining learning linear functions in context](https://arxiv.org/abs/2411.11465)
Append: [AlzheimerRAG: Multimodal Retrieval Augmented Generation for Clinical Use Cases using PubMed articles](https://arxiv.org/abs/2412.16701)
Append: [FlairGPT: Repurposing LLMs for Interior Designs](https://arxiv.org/abs/2501.04648)
Append: [Multimodal LLMs Can Reason about Aesthetics in Zero-Shot](https://arxiv.org/abs/2501.09012)
Append: [Temporal Preference Optimization for Long-Form Video Understanding](https://arxiv.org/abs/2501.13919)
Append: [SETS: Leveraging Self-Verification and Self-Correction for Improved Test-Time Scaling](https://arxiv.org/abs/2501.19306)
Append: [Training and Evaluating with Human Label Variation: An Empirical Study](https://arxiv.org/abs/2502.01891)
Append: [Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating Rotation and Learnable Non-uniform Quantizer](https://arxiv.org/abs/2502.15779)
Append: [Agent Trading Arena: A Study on Numerical Understanding in LLM-Based Agents](https://arxiv.org/abs/2502.17967)
Append: [Does Acceleration Cause Hidden Instability in Vision Language Models? Uncovering Instance-Level Divergence Through a Large-Scale Empirical Study](https://arxiv.org/abs/2503.06794)
Append: [VPO: Aligning Text-to-Video Generation Models with Prompt Optimization](https://arxiv.org/abs/2503.20491)
Append: [More Bang for the Buck: Process Reward Modeling with Entropy-Driven Uncertainty](https://arxiv.org/abs/2503.22233)
Append: [Towards a cognitive architecture to enable natural language interaction in co-constructive task learning](https://arxiv.org/abs/2503.23760)
Append: [Assessing AI-Generated Questions' Alignment with Cognitive Frameworks in Educational Assessment](https://arxiv.org/abs/2504.14232)
Append: [StoryReasoning Dataset: Using Chain-of-Thought for Scene Understanding and Grounded Story Generation](https://arxiv.org/abs/2505.10292)
Append: [Fine-tuning Quantized Neural Networks with Zeroth-order Optimization](https://arxiv.org/abs/2505.13430)
Append: [How Can I Publish My LLM Benchmark Without Giving the True Answers Away?](https://arxiv.org/abs/2505.18102)
Append: [CytoDiff: AI-Driven Cytomorphology Image Synthesis for Medical Diagnostics](https://arxiv.org/abs/2507.05063)
Append: [SpatialViz-Bench: An MLLM Benchmark for Spatial Visualization](https://arxiv.org/abs/2507.07610)
Append: [A Multi-granularity Concept Sparse Activation and Hierarchical Knowledge Graph Fusion Framework for Rare Disease Diagnosis](https://arxiv.org/abs/2507.08529)
Append: [ExCyTIn-Bench: Evaluating LLM agents on Cyber Threat Investigation](https://arxiv.org/abs/2507.14201)
Append: [Conan: A Chunkwise Online Network for Zero-Shot Adaptive Voice Conversion](https://arxiv.org/abs/2507.14534)
Append: [A Markov Categorical Framework for Language Modeling](https://arxiv.org/abs/2507.19247)
Append: [MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning](https://arxiv.org/abs/2508.00271)
Append: [A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems](https://arxiv.org/abs/2508.07407)
Append: [Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps](https://arxiv.org/abs/2508.11452)
append_entries: 304
Finish: 2025-09-03 04:26:00.115793
------------------------------------------------------
Started: 2025-09-03 06:24:26.196632
Existing_entries: 1304
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1495
Summarized using GPT-3.5-turbo
Append: [MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based Peer Review Automation](https://arxiv.org/abs/2508.14146)
Token length: 1388
Summarized using GPT-3.5-turbo
Append: [NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model](https://arxiv.org/abs/2508.14444)
Token length: 1916
Summarized using GPT-3.5-turbo
Append: [MedResearcher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework](https://arxiv.org/abs/2508.14880)
Token length: 1299
Summarized using GPT-3.5-turbo
Append: [Reward-Shifted Speculative Sampling Is An Efficient Test-Time Weak-to-Strong Aligner](https://arxiv.org/abs/2508.15044)
Token length: 1827
Summarized using GPT-3.5-turbo
Append: [SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache Channel Pruning](https://arxiv.org/abs/2508.15212)
append_entries: 5
Finish: 2025-09-03 06:24:40.022068
------------------------------------------------------
Started: 2025-09-03 08:20:52.203473
Existing_entries: 1005
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-03 08:20:52.864042
------------------------------------------------------
Started: 2025-09-03 10:16:07.638774
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-03 10:16:08.373984
------------------------------------------------------
Started: 2025-09-03 12:33:02.563720
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-03 12:33:03.312571
------------------------------------------------------
Started: 2025-09-03 14:16:09.790891
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-03 14:16:10.469278
------------------------------------------------------
Started: 2025-09-03 16:19:34.041270
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-03 16:19:34.725180
------------------------------------------------------
Started: 2025-09-03 18:21:59.594608
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-03 18:22:00.252731
------------------------------------------------------
Started: 2025-09-03 20:17:04.378154
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-03 20:17:05.133008
------------------------------------------------------
Started: 2025-09-03 22:14:17.356287
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-03 22:14:18.085558
------------------------------------------------------
Started: 2025-09-04 01:11:27.598821
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-04 01:11:28.251004
------------------------------------------------------
Started: 2025-09-04 02:53:10.880731
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-04 02:53:11.625176
------------------------------------------------------
Started: 2025-09-04 04:22:33.809230
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1007
Summarized using GPT-3.5-turbo
Append: [DrDiff: Dynamic Routing Diffusion with Hierarchical Attention for Breaking the Efficiency-Quality Trade-off](https://arxiv.org/abs/2509.02785)
Token length: 1044
Summarized using GPT-3.5-turbo
Append: [SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and Benchmarking under Domain Shift in ASR](https://arxiv.org/abs/2509.02830)
Token length: 795
Summarized using GPT-3.5-turbo
Append: [Clustering Discourses: Racial Biases in Short Stories about Women Generated by Large Language Models](https://arxiv.org/abs/2509.02834)
Token length: 1619
Summarized using GPT-3.5-turbo
Append: [IDEAlign: Comparing Large Language Models to Human Experts in Open-ended Interpretive Annotations](https://arxiv.org/abs/2509.02855)
Token length: 1391
Summarized using GPT-3.5-turbo
Append: [A-SEA3L-QA: A Fully Automated Self-Evolving, Adversarial Workflow for Arabic Long-Context Question-Answer Generation](https://arxiv.org/abs/2509.02864)
Token length: 1574
Summarized using GPT-3.5-turbo
Append: [Advancing Minority Stress Detection with Transformers: Insights from the Social Media Datasets](https://arxiv.org/abs/2509.02908)
Token length: 1385
Summarized using GPT-3.5-turbo
Append: [English Pronunciation Evaluation without Complex Joint Training: LoRA Fine-tuned Speech Multimodal LLM](https://arxiv.org/abs/2509.02915)
Token length: 1059
Summarized using GPT-3.5-turbo
Append: [Decoding the Rule Book: Extracting Hidden Moderation Criteria from Reddit Communities](https://arxiv.org/abs/2509.02926)
Token length: 1328
Summarized using GPT-3.5-turbo
Append: [ProMQA-Assembly: Multimodal Procedural QA Dataset on Assembly](https://arxiv.org/abs/2509.02949)
Token length: 1222
Summarized using GPT-3.5-turbo
Append: [DiaCBT: A Long-Periodic Dialogue Corpus Guided by Cognitive Conceptualization Diagram for CBT-based Psychological Counseling](https://arxiv.org/abs/2509.02999)
Token length: 740
Summarized using GPT-3.5-turbo
Append: [Mitigating Data Imbalance in Automated Speaking Assessment](https://arxiv.org/abs/2509.03010)
Token length: 1099
Summarized using GPT-3.5-turbo
Append: [Training LLMs to be Better Text Embedders through Bidirectional Reconstruction](https://arxiv.org/abs/2509.03020)
Token length: 1450
Summarized using GPT-3.5-turbo
Append: [Structure-Learnable Adapter Fine-Tuning for Parameter-Efficient Large Language Models](https://arxiv.org/abs/2509.03057)
Token length: 1546
Summarized using GPT-3.5-turbo
Append: [A Long Short-Term Memory (LSTM) Model for Business Sentiment Analysis Based on Recurrent Neural Network](https://arxiv.org/abs/2509.03060)
Token length: 1335
Summarized using GPT-3.5-turbo
Append: [Measuring Scalar Constructs in Social Science with LLMs](https://arxiv.org/abs/2509.03116)
Token length: 1357
Summarized using GPT-3.5-turbo
Append: [From Evaluation to Defense: Constructing Persistent Edit-Based Fingerprints for Large Language Models](https://arxiv.org/abs/2509.03122)
Token length: 1645
Summarized using GPT-3.5-turbo
Append: [An experimental and computational study of an Estonian single-person word naming](https://arxiv.org/abs/2509.03143)
Token length: 708
Summarized using GPT-3.5-turbo
Append: [Expanding the WMT24++ Benchmark with Rumantsch Grischun, Sursilvan, Sutsilvan, Surmiran, Puter, and Vallader](https://arxiv.org/abs/2509.03148)
Token length: 1307
Summarized using GPT-3.5-turbo
Append: [Domain Adaptation of LLMs for Process Data](https://arxiv.org/abs/2509.03161)
Token length: 1283
Summarized using GPT-3.5-turbo
Append: [SinhalaMMLU: A Comprehensive Benchmark for Evaluating Multitask Language Understanding in Sinhala](https://arxiv.org/abs/2509.03162)
Token length: 833
Summarized using GPT-3.5-turbo
Append: [Comparison of End-to-end Speech Assessment Models for the NOCASA 2025 Challenge](https://arxiv.org/abs/2509.03256)
Token length: 820
Summarized using GPT-3.5-turbo
Append: [LatPhon: Lightweight Multilingual G2P for Romance Languages and English](https://arxiv.org/abs/2509.03300)
Token length: 1447
Summarized using GPT-3.5-turbo
Append: [AgenTracer: Who Is Inducing Failure in the LLM Agentic Systems?](https://arxiv.org/abs/2509.03312)
Token length: 1500
Summarized using GPT-3.5-turbo
Append: [LMEnt: A Suite for Analyzing Knowledge in Language Models from Pretraining Data to Representations](https://arxiv.org/abs/2509.03405)
Token length: 1939
Summarized using GPT-3.5-turbo
Append: [Learning Mechanism Underlying NLP Pre-Training and Fine-Tuning](https://arxiv.org/abs/2509.03407)
Token length: 1130
Summarized using GPT-3.5-turbo
Append: [Curse of Knowledge: When Complex Evaluation Context Benefits yet Biases LLM Judges](https://arxiv.org/abs/2509.03419)
Token length: 1952
Summarized using GPT-3.5-turbo
Append: [Continuous Saudi Sign Language Recognition: A Vision Transformer Approach](https://arxiv.org/abs/2509.03467)
Token length: 899
Summarized using GPT-3.5-turbo
Append: [Design and Optimization of Reinforcement Learning-Based Agents in Text-Based Games](https://arxiv.org/abs/2509.03479)
Token length: 1101
Summarized using GPT-3.5-turbo
Append: [Speech DF Arena: A Leaderboard for Speech DeepFake Detection Models](https://arxiv.org/abs/2509.02859)
Token length: 1117
Summarized using GPT-3.5-turbo
Append: [Identifiability and minimality bounds of quantum and post-quantum models of classical stochastic processes](https://arxiv.org/abs/2509.03004)
Token length: 1108
Summarized using GPT-3.5-turbo
Append: [Mitigating Multimodal Hallucinations via Gradient-based Self-Reflection](https://arxiv.org/abs/2509.03113)
Token length: 1759
Summarized using GPT-3.5-turbo
Append: [SESGO: Spanish Evaluation of Stereotypical Generative Outputs](https://arxiv.org/abs/2509.03329)
Token length: 1202
Summarized using GPT-3.5-turbo
Append: [Language Models Do Not Follow Occam's Razor: A Benchmark for Inductive and Abductive Reasoning](https://arxiv.org/abs/2509.03345)
Token length: 871
Summarized using GPT-3.5-turbo
Append: [Situating AI Agents in their World: Aspective Agentic AI for Dynamic Partially Observable Information Systems](https://arxiv.org/abs/2509.03380)
Token length: 1513
Summarized using GPT-3.5-turbo
Append: [LimiX: Unleashing Structured-Data Modeling Capability for Generalist Intelligence](https://arxiv.org/abs/2509.03505)
Token length: 1086
Summarized using GPT-3.5-turbo
Append: [Learn and Unlearn: Addressing Misinformation in Multilingual LLMs](https://arxiv.org/abs/2406.13748)
Token length: 1221
Summarized using GPT-3.5-turbo
Append: [SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention](https://arxiv.org/abs/2406.15486)
Token length: 1233
Summarized using GPT-3.5-turbo
Append: [Banishing LLM Hallucinations Requires Rethinking Generalization](https://arxiv.org/abs/2406.17642)
Token length: 1272
Summarized using GPT-3.5-turbo
Append: [TreeBoN: Enhancing Inference-Time Alignment with Speculative Tree-Search and Best-of-N Sampling](https://arxiv.org/abs/2410.16033)
Token length: 1266
Summarized using GPT-3.5-turbo
Append: [Attacking Misinformation Detection Using Adversarial Examples Generated by Language Models](https://arxiv.org/abs/2410.20940)
Token length: 1450
Summarized using GPT-3.5-turbo
Append: [Dial-In LLM: Human-Aligned LLM-in-the-loop Intent Clustering for Customer Service Dialogues](https://arxiv.org/abs/2412.09049)
Token length: 1073
Summarized using GPT-3.5-turbo
Append: [Attention-guided Self-reflection for Zero-shot Hallucination Detection in Large Language Models](https://arxiv.org/abs/2501.09997)
Token length: 1251
Summarized using GPT-3.5-turbo
Append: [FedP$^2$EFT: Federated Learning to Personalize PEFT for Multilingual LLMs](https://arxiv.org/abs/2502.04387)
Token length: 1029
Summarized using GPT-3.5-turbo
Append: [FELLE: Autoregressive Speech Synthesis with Token-Wise Coarse-to-Fine Flow Matching](https://arxiv.org/abs/2502.11128)
Token length: 1372
Summarized using GPT-3.5-turbo
Append: [Rapid Word Learning Through Meta In-Context Learning](https://arxiv.org/abs/2502.14791)
Token length: 1447
Summarized using GPT-3.5-turbo
Append: [Problem Solved? Information Extraction Design Space for Layout-Rich Documents using LLMs](https://arxiv.org/abs/2502.18179)
Token length: 1751
Summarized using GPT-3.5-turbo
Append: [Texture or Semantics? Vision-Language Models Get Lost in Font Recognition](https://arxiv.org/abs/2503.23768)
Token length: 1639
Summarized using GPT-3.5-turbo
Append: [LawFlow: Collecting and Simulating Lawyers' Thought Processes on Business Formation Case Studies](https://arxiv.org/abs/2504.18942)
Token length: 851
Summarized using GPT-3.5-turbo
Append: [Demystifying optimized prompts in language models](https://arxiv.org/abs/2505.02273)
Token length: 1298
Summarized using GPT-3.5-turbo
Append: [QualBench: Benchmarking Chinese LLMs with Localized Professional Qualifications for Vertical Domain Evaluation](https://arxiv.org/abs/2505.05225)
Append: [Insertion Language Models: Sequence Generation with Arbitrary-Position Insertions](https://arxiv.org/abs/2505.05755)
Append: [NOVER: Incentive Training for Language Models via Verifier-Free Reinforcement Learning](https://arxiv.org/abs/2505.16022)
Append: [On the class of coding optimality of human languages and the origins of Zipf's law](https://arxiv.org/abs/2505.20015)
Append: [IndexTTS2: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech](https://arxiv.org/abs/2506.21619)
Append: [RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based Sequence Modeling](https://arxiv.org/abs/2507.04416)
Append: [Advancing Dialectal Arabic to Modern Standard Arabic Machine Translation](https://arxiv.org/abs/2507.20301)
Append: [MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens of Documents](https://arxiv.org/abs/2508.11133)
Append: [JARVIS: A Neuro-Symbolic Commonsense Reasoning Framework for Conversational Embodied Agents](https://arxiv.org/abs/2208.13266)
Append: [A Survey on Human-AI Collaboration with Large Foundation Models](https://arxiv.org/abs/2403.04931)
Append: [PadChest-GR: A Bilingual Chest X-ray Dataset for Grounded Radiology Report Generation](https://arxiv.org/abs/2411.05085)
Append: [That is Unacceptable: the Moral Foundations of Canceling](https://arxiv.org/abs/2503.05720)
Append: [You Sound a Little Tense: L2 Tailored Clear TTS Using Durational Vowel Properties](https://arxiv.org/abs/2506.23367)
Append: [CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks](https://arxiv.org/abs/2507.23751)
append_entries: 63
Finish: 2025-09-04 04:24:32.301013
------------------------------------------------------
Started: 2025-09-04 06:23:36.384915
Existing_entries: 1063
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-04 06:23:36.645182
------------------------------------------------------
Started: 2025-09-04 08:20:34.878733
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-04 08:20:35.093276
------------------------------------------------------
Started: 2025-09-04 10:16:30.889946
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-04 10:16:31.093427
------------------------------------------------------
Started: 2025-09-04 12:32:00.093510
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-04 12:32:00.331941
------------------------------------------------------
Started: 2025-09-04 14:15:40.502478
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-04 14:15:40.754852
------------------------------------------------------
Started: 2025-09-04 16:20:09.818634
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-04 16:20:10.045795
------------------------------------------------------
Started: 2025-09-04 18:22:08.731237
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-04 18:22:09.006341
------------------------------------------------------
Started: 2025-09-04 20:16:58.468057
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-04 20:16:58.808114
------------------------------------------------------
Started: 2025-09-04 22:14:45.201587
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-04 22:14:45.408955
------------------------------------------------------
Started: 2025-09-05 01:12:59.784948
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-05 01:13:00.026653
------------------------------------------------------
Started: 2025-09-05 02:56:44.708272
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-05 02:56:45.026055
------------------------------------------------------
Started: 2025-09-05 04:23:41.029736
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1283
Summarized using GPT-3.5-turbo
Append: [Speech-Based Cognitive Screening: A Systematic Evaluation of LLM Adaptation Strategies](https://arxiv.org/abs/2509.03525)
Token length: 1503
Summarized using GPT-3.5-turbo
Append: [Enhancing Speech Large Language Models through Reinforced Behavior Alignment](https://arxiv.org/abs/2509.03526)
Token length: 1084
Summarized using GPT-3.5-turbo
Append: [Multilevel Analysis of Cryptocurrency News using RAG Approach with Fine-Tuned Mistral Large Language Model](https://arxiv.org/abs/2509.03527)
Token length: 729
Summarized using GPT-3.5-turbo
Append: [The ProLiFIC dataset: Leveraging LLMs to Unveil the Italian Lawmaking Process](https://arxiv.org/abs/2509.03528)
Token length: 1829
Summarized using GPT-3.5-turbo
Append: [Multimodal Proposal for an AI-Based Tool to Increase Cross-Assessment of Messages](https://arxiv.org/abs/2509.03529)
Token length: 1089
Summarized using GPT-3.5-turbo
Append: [Reading Between the Signs: Predicting Future Suicidal Ideation from Adolescent Social Media Texts](https://arxiv.org/abs/2509.03530)
Token length: 1861
Summarized using GPT-3.5-turbo
Append: [Real-Time Detection of Hallucinated Entities in Long-Form Generation](https://arxiv.org/abs/2509.03531)
Token length: 1581
Summarized using GPT-3.5-turbo
Append: [Topic Identification in LLM Input-Output Pairs through the Lens of Information Bottleneck](https://arxiv.org/abs/2509.03533)
Token length: 777
Summarized using GPT-3.5-turbo
Append: [QuesGenie: Intelligent Multimodal Question Generation](https://arxiv.org/abs/2509.03535)
Token length: 1215
Summarized using GPT-3.5-turbo
Append: [AR$^2$: Adversarial Reinforcement Learning for Abstract Reasoning in Large Language Models](https://arxiv.org/abs/2509.03537)
Token length: 1395
Summarized using GPT-3.5-turbo
Append: [Improving Factuality in LLMs via Inference-Time Knowledge Graph Construction](https://arxiv.org/abs/2509.03540)
Token length: 1329
Summarized using GPT-3.5-turbo
Append: [ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference](https://arxiv.org/abs/2509.03565)
Token length: 1101
Summarized using GPT-3.5-turbo
Append: [NoteBar: An AI-Assisted Note-Taking System for Personal Knowledge Management](https://arxiv.org/abs/2509.03610)
Token length: 1640
Summarized using GPT-3.5-turbo
Append: [E-ARMOR: Edge case Assessment and Review of Multilingual Optical Character Recognition](https://arxiv.org/abs/2509.03615)
Token length: 1225
Summarized using GPT-3.5-turbo
Append: [Breaking the Mirror: Activation-Based Mitigation of Self-Preference in LLM Evaluators](https://arxiv.org/abs/2509.03647)
Token length: 1724
Summarized using GPT-3.5-turbo
Append: [Semantic Analysis of SNOMED CT Concept Co-occurrences in Clinical Documentation using MIMIC-IV](https://arxiv.org/abs/2509.03662)
Token length: 760
Summarized using GPT-3.5-turbo
Append: [MLSD: A Novel Few-Shot Learning Approach to Enhance Cross-Target and Cross-Domain Stance Detection](https://arxiv.org/abs/2509.03725)
Token length: 1205
Summarized using GPT-3.5-turbo
Append: [SiLVERScore: Semantically-Aware Embeddings for Sign Language Generation Evaluation](https://arxiv.org/abs/2509.03791)
Token length: 1030
Summarized using GPT-3.5-turbo
Append: [Measuring How (Not Just Whether) VLMs Build Common Ground](https://arxiv.org/abs/2509.03805)
Token length: 1244
Summarized using GPT-3.5-turbo
Append: [Align-then-Slide: A complete evaluation framework for Ultra-Long Document-Level Machine Translation](https://arxiv.org/abs/2509.03809)
Token length: 1044
Summarized using GPT-3.5-turbo
Append: [NE-PADD: Leveraging Named Entity Knowledge for Robust Partial Audio Deepfake Detection via Attention Aggregation](https://arxiv.org/abs/2509.03829)
Token length: 1711
Summarized using GPT-3.5-turbo
Append: [Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth](https://arxiv.org/abs/2509.03867)
Token length: 1632
Summarized using GPT-3.5-turbo
Append: [A Comprehensive Survey on Trustworthiness in Reasoning with Large Language Models](https://arxiv.org/abs/2509.03871)
Token length: 1334
Summarized using GPT-3.5-turbo
Append: [False Sense of Security: Why Probing-based Malicious Input Detection Fails to Generalize](https://arxiv.org/abs/2509.03888)
Token length: 1684
Summarized using GPT-3.5-turbo
Append: [MobileRAG: Enhancing Mobile Agent with Retrieval-Augmented Generation](https://arxiv.org/abs/2509.03891)
Token length: 1776
Summarized using GPT-3.5-turbo
Append: [MTQA:Matrix of Thought for Enhanced Reasoning in Complex Question Answering](https://arxiv.org/abs/2509.03918)
Token length: 1364
Summarized using GPT-3.5-turbo
Append: [Decoding the Poetic Language of Emotion in Korean Modern Poetry: Insights from a Human-Labeled Dataset and AI Modeling](https://arxiv.org/abs/2509.03932)
Token length: 1572
Summarized using GPT-3.5-turbo
Append: [SelfAug: Mitigating Catastrophic Forgetting in Retrieval-Augmented Generation via Distribution Self-Alignment](https://arxiv.org/abs/2509.03934)
Token length: 1411
Summarized using GPT-3.5-turbo
Append: [SPFT-SQL: Enhancing Large Language Model for Text-to-SQL Parsing by Self-Play Fine-Tuning](https://arxiv.org/abs/2509.03937)
Token length: 1705
Summarized using GPT-3.5-turbo
Append: [VoxRole: A Comprehensive Benchmark for Evaluating Speech-Based Role-Playing Agents](https://arxiv.org/abs/2509.03940)
Token length: 1033
Summarized using GPT-3.5-turbo
Append: [CANDY: Benchmarking LLMs' Limitations and Assistive Potential in Chinese Misinformation Fact-Checking](https://arxiv.org/abs/2509.03957)
Token length: 1302
Summarized using GPT-3.5-turbo
Append: [Exploring NLP Benchmarks in an Extremely Low-Resource Setting](https://arxiv.org/abs/2509.03962)
Token length: 796
Summarized using GPT-3.5-turbo
Append: [Expanding Foundational Language Capabilities in Open-Source LLMs through a Korean Case Study](https://arxiv.org/abs/2509.03972)
Token length: 1026
Summarized using GPT-3.5-turbo
Append: [RTQA : Recursive Thinking for Complex Temporal Knowledge Graph Question Answering with Large Language Models](https://arxiv.org/abs/2509.03995)
Token length: 1571
Summarized using GPT-3.5-turbo
Append: [On Robustness and Reliability of Benchmark-Based Evaluation of LLMs](https://arxiv.org/abs/2509.04013)
Token length: 729
Summarized using GPT-3.5-turbo
Append: [What if I ask in \textit{alia lingua}? Measuring Functional Similarity Across Languages](https://arxiv.org/abs/2509.04032)
Token length: 1451
Summarized using GPT-3.5-turbo
Append: [A RoBERTa-Based Functional Syntax Annotation Model for Chinese Texts](https://arxiv.org/abs/2509.04046)
Token length: 1861
Summarized using GPT-3.5-turbo
Append: [Synthesizing Sheet Music Problems for Evaluation and Reinforcement Learning](https://arxiv.org/abs/2509.04059)
Token length: 1208
Summarized using GPT-3.5-turbo
Append: [Arabic Chatbot Technologies in Education: An Overview](https://arxiv.org/abs/2509.04066)
Token length: 1104
Summarized using GPT-3.5-turbo
Append: [Improving Narrative Classification and Explanation via Fine Tuned Language Models](https://arxiv.org/abs/2509.04077)
Token length: 1403
Summarized using GPT-3.5-turbo
Append: [Towards Stable and Personalised Profiles for Lexical Alignment in Spoken Human-Agent Dialogue](https://arxiv.org/abs/2509.04104)
Token length: 737
Summarized using GPT-3.5-turbo
Append: [MultiWikiQA: A Reading Comprehension Benchmark in 300+ Languages](https://arxiv.org/abs/2509.04111)
Token length: 767
Summarized using GPT-3.5-turbo
Append: [Joint Modeling of Entities and Discourse Relations for Coherence Assessment](https://arxiv.org/abs/2509.04182)
Token length: 1674
Summarized using GPT-3.5-turbo
Append: [MAGneT: Coordinated Multi-Agent Generation of Synthetic Multi-Turn Mental Health Counseling Sessions](https://arxiv.org/abs/2509.04183)
Token length: 1141
Summarized using GPT-3.5-turbo
Append: [Explicit and Implicit Data Augmentation for Social Event Detection](https://arxiv.org/abs/2509.04202)
Token length: 1413
Summarized using GPT-3.5-turbo
Append: [Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow Real Instructions?](https://arxiv.org/abs/2509.04292)
Token length: 1100
Summarized using GPT-3.5-turbo
Append: [Facts Fade Fast: Evaluating Memorization of Outdated Medical Knowledge in Large Language Models](https://arxiv.org/abs/2509.04304)
Token length: 1047
Summarized using GPT-3.5-turbo
Append: [PARCO: Phoneme-Augmented Robust Contextual ASR via Contrastive Entity Disambiguation](https://arxiv.org/abs/2509.04357)
Token length: 1346
Summarized using GPT-3.5-turbo
Append: [Measuring Bias or Measuring the Task: Understanding the Brittle Nature of LLM Gender Biases](https://arxiv.org/abs/2509.04373)
Token length: 1159
Summarized using GPT-3.5-turbo
Append: [Can Language Models Handle a Non-Gregorian Calendar?](https://arxiv.org/abs/2509.04432)
Append: [CausalARC: Abstract Reasoning with Causal World Models](https://arxiv.org/abs/2509.03636)
Append: [Towards a Neurosymbolic Reasoning System Grounded in Schematic Representations](https://arxiv.org/abs/2509.03644)
Append: [Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning](https://arxiv.org/abs/2509.03646)
Append: [The Personality Illusion: Revealing Dissociation Between Self-Reports & Behavior in LLMs](https://arxiv.org/abs/2509.03730)
Append: [Singular Value Few-shot Adaptation of Vision-Language Models](https://arxiv.org/abs/2509.03740)
Append: [Evaluating the Robustness of Retrieval-Augmented Generation to Adversarial Evidence in the Health Domain](https://arxiv.org/abs/2509.03787)
Append: [SPECS: Specificity-Enhanced CLIP-Score for Long Image Caption Evaluation](https://arxiv.org/abs/2509.03897)
Append: [Promptception: How Sensitive Are Large Multimodal Models to Prompts?](https://arxiv.org/abs/2509.03986)
Append: [NER Retriever: Zero-Shot Named Entity Retrieval with Type-Aware Embeddings](https://arxiv.org/abs/2509.04011)
Append: [CoT-Space: A Theoretical Framework for Internal Slow-Thinking via Reinforcement Learning](https://arxiv.org/abs/2509.04027)
Append: [LibriQuote: A Speech Dataset of Fictional Character Utterances for Expressive Zero-Shot Speech Synthesis](https://arxiv.org/abs/2509.04072)
Append: [Towards an Action-Centric Ontology for Cooking Procedures Using Temporal Graphs](https://arxiv.org/abs/2509.04159)
Append: [Crossing the Species Divide: Transfer Learning from Speech to Animal Sounds](https://arxiv.org/abs/2509.04166)
Append: [Psychologically Enhanced AI Agents](https://arxiv.org/abs/2509.04343)
Append: [Contextualized Token Discrimination for Speech Search Query Correction](https://arxiv.org/abs/2509.04393)
Append: [Self-adaptive Dataset Construction for Real-World Multimodal Safety Scenarios](https://arxiv.org/abs/2509.04403)
Append: [No Thoughts Just AI: Biased LLM Recommendations Limit Human Agency in Resume Screening](https://arxiv.org/abs/2509.04404)
Append: [Towards a Unified View of Large Language Model Post-Training](https://arxiv.org/abs/2509.04419)
Append: [The Telephone Game: Evaluating Semantic Drift in Unified Models](https://arxiv.org/abs/2509.04438)
Append: [ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory](https://arxiv.org/abs/2509.04439)
Append: [Delta Activations: A Representation for Finetuned Large Language Models](https://arxiv.org/abs/2509.04442)
Append: [MyProfessors: Mining Turkish Student Reviews](https://arxiv.org/abs/2109.02325)
Append: [Mitigating Bias in Text Classification via Prompt-Based Text Transformation](https://arxiv.org/abs/2305.06166)
Append: [Exploring Linguistic Features for Turkish Text Readability](https://arxiv.org/abs/2306.03774)
Append: [R2C2-Coder: Enhancing and Benchmarking Real-world Repository-level Code Completion Abilities of Code Large Language Models](https://arxiv.org/abs/2406.01359)
Append: [DynaSaur: Large Language Agents Beyond Predefined Actions](https://arxiv.org/abs/2411.01747)
Append: [ACING: Actor-Critic for Instruction Learning in Black-Box LLMs](https://arxiv.org/abs/2411.12736)
Append: [Small Changes, Large Consequences: Analyzing the Allocational Fairness of LLMs in Hiring Contexts](https://arxiv.org/abs/2501.04316)
Append: [A Survey of Graph Retrieval-Augmented Generation for Customized Large Language Models](https://arxiv.org/abs/2501.13958)
Append: [An Unsupervised Natural Language Processing Pipeline for Assessing Referral Appropriateness](https://arxiv.org/abs/2501.14701)
Append: [HamRaz: A Culture-Based Persian Conversation Dataset for Person-Centered Therapy Using LLM Agents](https://arxiv.org/abs/2502.05982)
Append: [HalluEntity: Benchmarking and Understanding Entity-Level Hallucination Detection](https://arxiv.org/abs/2502.11948)
Append: [Autoformalization in the Wild: Assessing LLMs on Real-World Mathematical Definitions](https://arxiv.org/abs/2502.12065)
Append: [Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions](https://arxiv.org/abs/2502.12616)
Append: [FRIDA to the Rescue! Analyzing Synthetic Data Effectiveness in Object-Based Common Sense Reasoning for Disaster Response](https://arxiv.org/abs/2502.18452)
Append: [Explicit Learning and the LLM in Machine Translation](https://arxiv.org/abs/2503.09454)
Append: [FutureGen: A RAG-based Approach to Generate the Future Work of Scientific Article](https://arxiv.org/abs/2503.16561)
Append: [EQ-Knight: A Memory-Augmented LLM Agent for Strategic Affective Gaming in Debt Recovery](https://arxiv.org/abs/2503.21080)
Append: [Learning Optimal Prompt Ensemble for Multi-source Visual Prompt Transfer](https://arxiv.org/abs/2504.12311)
Append: [Context Reasoner: Incentivizing Reasoning Capability for Contextualized Privacy and Safety Compliance via Reinforcement Learning](https://arxiv.org/abs/2505.14585)
Append: [MiniCPM4: Ultra-Efficient LLMs on End Devices](https://arxiv.org/abs/2506.07900)
Append: [MEDUSA: A Multimodal Deep Fusion Multi-Stage Training Framework for Speech Emotion Recognition in Naturalistic Conditions](https://arxiv.org/abs/2506.09556)
Append: [DAPFAM: A Domain-Aware Family-level Dataset to benchmark cross domain patent retrieval](https://arxiv.org/abs/2506.22141)
Append: [Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges](https://arxiv.org/abs/2508.00454)
Append: [Two-Stage Quranic QA via Ensemble Retrieval and Instruction-Tuned Answer Extraction](https://arxiv.org/abs/2508.06971)
Append: [Transplant Then Regenerate: A New Paradigm for Text Data Augmentation](https://arxiv.org/abs/2508.14723)
Append: [SLM-Bench: A Comprehensive Benchmark of Small Language Models on Environmental Impacts--Extended Version](https://arxiv.org/abs/2508.15478)
Append: [PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal Documents](https://arxiv.org/abs/2406.13923)
Append: [Enhancing FKG.in: automating Indian food composition analysis](https://arxiv.org/abs/2412.05248)
Append: [Science Across Languages: Assessing LLM Multilingual Translation of Scientific Papers](https://arxiv.org/abs/2502.17882)
Append: [Short-video Propagation Influence Rating: A New Real-world Dataset and A New Large Graph Model](https://arxiv.org/abs/2503.23746)
Append: [Is Random Attention Sufficient for Sequence Modeling? Disentangling Trainable Components in the Transformer](https://arxiv.org/abs/2506.01115)
Append: [MultiGen: Child-Friendly Multilingual Speech Generator with LLMs](https://arxiv.org/abs/2508.08715)
Append: [Extending FKG.in: Towards a Food Claim Traceability Network](https://arxiv.org/abs/2508.16117)
append_entries: 104
Finish: 2025-09-05 04:25:49.106161
------------------------------------------------------
Started: 2025-09-05 06:23:54.753710
Existing_entries: 1104
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-05 06:23:55.026153
------------------------------------------------------
Started: 2025-09-05 08:20:50.529641
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-05 08:20:50.837351
------------------------------------------------------
Started: 2025-09-05 10:16:39.004845
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-05 10:16:39.381630
------------------------------------------------------
Started: 2025-09-05 12:32:16.305822
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-05 12:32:16.585057
------------------------------------------------------
Started: 2025-09-05 14:14:57.736104
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-05 14:14:58.018121
------------------------------------------------------
Started: 2025-09-05 16:18:54.160516
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-05 16:18:54.439694
------------------------------------------------------
Started: 2025-09-05 18:20:26.629628
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-05 18:20:26.929374
------------------------------------------------------
Started: 2025-09-05 20:17:03.843279
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-05 20:17:04.123872
------------------------------------------------------
Started: 2025-09-05 22:14:16.410242
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-05 22:14:16.691571
------------------------------------------------------
Started: 2025-09-06 01:11:29.910053
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-06 01:11:30.288328
------------------------------------------------------
Started: 2025-09-06 02:51:44.173808
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-06 02:51:44.505042
------------------------------------------------------
Started: 2025-09-06 04:18:08.090620
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-06 04:18:08.179992
------------------------------------------------------
Started: 2025-09-06 06:20:32.024990
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-06 06:20:32.080878
------------------------------------------------------
Started: 2025-09-06 08:17:54.774441
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-06 08:17:54.847812
------------------------------------------------------
Started: 2025-09-06 10:14:11.289617
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-06 10:14:11.442852
------------------------------------------------------
Started: 2025-09-06 12:28:27.156377
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-06 12:28:27.223344
------------------------------------------------------
Started: 2025-09-06 14:12:51.892431
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-06 14:12:51.995376
------------------------------------------------------
Started: 2025-09-06 16:16:57.308398
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-06 16:16:57.390951
------------------------------------------------------
Started: 2025-09-06 18:19:12.461418
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-06 18:19:12.549944
------------------------------------------------------
Started: 2025-09-06 20:14:56.914671
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-06 20:14:56.992725
------------------------------------------------------
Started: 2025-09-06 22:13:09.846087
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-06 22:13:09.930305
------------------------------------------------------
Started: 2025-09-07 01:18:32.216132
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-07 01:18:32.280305
------------------------------------------------------
Started: 2025-09-07 03:00:07.490116
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-07 03:00:07.586338
------------------------------------------------------
Started: 2025-09-07 04:18:18.076004
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-07 04:18:18.137285
------------------------------------------------------
Started: 2025-09-07 06:21:12.462620
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-07 06:21:12.521999
------------------------------------------------------
Started: 2025-09-07 08:17:58.536443
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-07 08:17:58.600514
------------------------------------------------------
Started: 2025-09-07 10:14:52.318359
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-07 10:14:52.405226
------------------------------------------------------
Started: 2025-09-07 12:29:14.301042
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-07 12:29:14.360804
------------------------------------------------------
Started: 2025-09-07 14:12:55.026309
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-07 14:12:55.129136
------------------------------------------------------
Started: 2025-09-07 16:17:02.102939
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-07 16:17:02.212835
------------------------------------------------------
Started: 2025-09-07 18:19:20.913802
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-07 18:19:21.066286
------------------------------------------------------
Started: 2025-09-07 20:15:28.853385
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-07 20:15:28.984074
------------------------------------------------------
Started: 2025-09-07 22:14:04.450026
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-07 22:14:04.558325
------------------------------------------------------
Started: 2025-09-08 01:16:58.039347
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-08 01:16:58.144834
------------------------------------------------------
Started: 2025-09-08 03:04:09.848738
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-08 03:04:09.929732
------------------------------------------------------
Started: 2025-09-08 04:23:36.343862
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1250
Summarized using GPT-3.5-turbo
Append: [INSEva: A Comprehensive Chinese Benchmark for Large Language Models in Insurance](https://arxiv.org/abs/2509.04455)
Token length: 969
Summarized using GPT-3.5-turbo
Append: [Mentalic Net: Development of RAG-based Conversational AI and Evaluation Framework for Mental Health Support](https://arxiv.org/abs/2509.04456)
Token length: 1356
Summarized using GPT-3.5-turbo
Append: [Do MLLMs Really Understand the Charts?](https://arxiv.org/abs/2509.04457)
Token length: 622
Summarized using GPT-3.5-turbo
Append: [Predicting Failures of LLMs to Link Biomedical Ontology Terms to Identifiers Evidence Across Models and Ontologies](https://arxiv.org/abs/2509.04458)
Token length: 1634
Summarized using GPT-3.5-turbo
Append: [Uncertainty-Aware Collaborative System of Large and Small Models for Multimodal Sentiment Analysis](https://arxiv.org/abs/2509.04459)
Token length: 1581
Summarized using GPT-3.5-turbo
Append: [CoCoNUTS: Concentrating on Content while Neglecting Uninformative Textual Styles for AI-Generated Peer Review Detection](https://arxiv.org/abs/2509.04460)
Token length: 1325
Summarized using GPT-3.5-turbo
Append: [From Post To Personality: Harnessing LLMs for MBTI Prediction in Social Media](https://arxiv.org/abs/2509.04461)
Token length: 1907
Summarized using GPT-3.5-turbo
Append: [Benchmarking GPT-5 for biomedical natural language processing](https://arxiv.org/abs/2509.04462)
Token length: 1332
Summarized using GPT-3.5-turbo
Append: [Can Multiple Responses from an LLM Reveal the Sources of Its Uncertainty?](https://arxiv.org/abs/2509.04464)
Token length: 1034
Summarized using GPT-3.5-turbo
Append: [Emotionally-Aware Agents for Dispute Resolution](https://arxiv.org/abs/2509.04465)
Token length: 1607
Summarized using GPT-3.5-turbo
Append: [Just-in-time and distributed task representations in language models](https://arxiv.org/abs/2509.04466)
Token length: 1289
Summarized using GPT-3.5-turbo
Append: [Enhancing LLM Efficiency: Targeted Pruning for Prefill-Decode Disaggregation in Inference](https://arxiv.org/abs/2509.04467)
Token length: 1585
Summarized using GPT-3.5-turbo
Append: [Evaluating Large Language Models for Financial Reasoning: A CFA-Based Benchmark Study](https://arxiv.org/abs/2509.04468)
Token length: 739
Summarized using GPT-3.5-turbo
Append: [Multi-Modal Vision vs. Text-Based Parsing: Benchmarking LLM Strategies for Invoice Processing](https://arxiv.org/abs/2509.04469)
Token length: 848
Summarized using GPT-3.5-turbo
Append: [COCORELI: Cooperative, Compositional Reconstitution \& Execution of Language Instructions](https://arxiv.org/abs/2509.04470)
Token length: 1611
Summarized using GPT-3.5-turbo
Append: [MOSAIC: A Multilingual, Taxonomy-Agnostic, and Computationally Efficient Approach for Radiological Report Classification](https://arxiv.org/abs/2509.04471)
Token length: 1265
Summarized using GPT-3.5-turbo
Append: [RECAP: REwriting Conversations for Intent Understanding in Agentic Planning](https://arxiv.org/abs/2509.04472)
Token length: 1067
Summarized using GPT-3.5-turbo
Append: [SpeechLLM: Unified Speech and Language Model for Enhanced Multi-Task Understanding in Low Resource Settings](https://arxiv.org/abs/2509.04473)
Token length: 1636
Summarized using GPT-3.5-turbo
Append: [Scaling Up, Speeding Up: A Benchmark of Speculative Decoding for Efficient LLM Test-Time Scaling](https://arxiv.org/abs/2509.04474)
Token length: 1619
Summarized using GPT-3.5-turbo
Append: [ParaThinker: Native Parallel Thinking as a New Paradigm to Scale LLM Test-time Compute](https://arxiv.org/abs/2509.04475)
Token length: 1392
Summarized using GPT-3.5-turbo
Append: [Training Text-to-Molecule Models with Context-Aware Tokenization](https://arxiv.org/abs/2509.04476)
Token length: 1144
Summarized using GPT-3.5-turbo
Append: [An End-to-End System for Culturally-Attuned Driving Feedback using a Dual-Component NLG Engine](https://arxiv.org/abs/2509.04478)
Token length: 1087
Summarized using GPT-3.5-turbo
Append: [No Clustering, No Routing: How Transformers Actually Process Rare Tokens](https://arxiv.org/abs/2509.04479)
Token length: 1148
Summarized using GPT-3.5-turbo
Append: [Discrete Prompt Tuning via Recursive Utilization of Black-box Multimodal Large Language Model for Personalized Visual Emotion Recognition](https://arxiv.org/abs/2509.04480)
Token length: 1526
Summarized using GPT-3.5-turbo
Append: [Energy Landscapes Enable Reliable Abstention in Retrieval-Augmented Large Language Models for Healthcare](https://arxiv.org/abs/2509.04482)
Token length: 999
Summarized using GPT-3.5-turbo
Append: [DecMetrics: Structured Claim Decomposition Scoring for Factually Consistent LLM Outputs](https://arxiv.org/abs/2509.04483)
Token length: 1336
Summarized using GPT-3.5-turbo
Append: [The Good, the Bad and the Constructive: Automatically Measuring Peer Review's Utility for Authors](https://arxiv.org/abs/2509.04484)
Token length: 1402
Summarized using GPT-3.5-turbo
Append: [ASCENDgpt: A Phenotype-Aware Transformer Model for Cardiovascular Risk Prediction from Electronic Health Records](https://arxiv.org/abs/2509.04485)
Token length: 1452
Summarized using GPT-3.5-turbo
Append: [Serialized Output Prompting for Large Language Model-based Multi-Talker Speech Recognition](https://arxiv.org/abs/2509.04488)
Token length: 1103
Summarized using GPT-3.5-turbo
Append: [Refining Transcripts With TV Subtitles by Prompt-Based Weakly Supervised Training of ASR](https://arxiv.org/abs/2509.04491)
Token length: 1600
Summarized using GPT-3.5-turbo
Append: [Learned Hallucination Detection in Black-Box LLMs using Token-level Entropy Production Rate](https://arxiv.org/abs/2509.04492)
Token length: 1433
Summarized using GPT-3.5-turbo
Append: [A Narrative-Driven Computational Framework for Clinician Burnout Surveillance](https://arxiv.org/abs/2509.04497)
Token length: 1165
Summarized using GPT-3.5-turbo
Append: [Where Should I Study? Biased Language Models Decide! Evaluating Fairness in LMs for Academic Recommendations](https://arxiv.org/abs/2509.04498)
Token length: 1395
Summarized using GPT-3.5-turbo
Append: [DeepTRACE: Auditing Deep Research AI Systems for Tracking Reliability Across Citations and Evidence](https://arxiv.org/abs/2509.04499)
Token length: 1618
Summarized using GPT-3.5-turbo
Append: [Context Engineering for Trustworthiness: Rescorla Wagner Steering Under Mixed and Inappropriate Contexts](https://arxiv.org/abs/2509.04500)
Token length: 1102
Summarized using GPT-3.5-turbo
Append: [Understanding Reinforcement Learning for Model Training, and future directions with GRAPE](https://arxiv.org/abs/2509.04501)
Token length: 1522
Summarized using GPT-3.5-turbo
Append: [VaccineRAG: Boosting Multimodal Large Language Models' Immunity to Harmful RAG Samples](https://arxiv.org/abs/2509.04502)
Token length: 1399
Summarized using GPT-3.5-turbo
Append: [Behavioral Fingerprinting of Large Language Models](https://arxiv.org/abs/2509.04504)
Token length: 901
Summarized using GPT-3.5-turbo
Append: [From Silent Signals to Natural Language: A Dual-Stage Transformer-LLM Approach](https://arxiv.org/abs/2509.04507)
Token length: 1264
Summarized using GPT-3.5-turbo
Append: [ProST: Progressive Sub-task Training for Pareto-Optimal Multi-agent Systems Using Small Language Models](https://arxiv.org/abs/2509.04508)
Token length: 1319
Summarized using GPT-3.5-turbo
Append: [Combine Virtual Reality and Machine-Learning to Identify the Presence of Dyslexia: A Cross-Linguistic Approach](https://arxiv.org/abs/2509.04510)
Token length: 1435
Summarized using GPT-3.5-turbo
Append: [Scaling behavior of large language models in emotional safety classification across sizes and tasks](https://arxiv.org/abs/2509.04512)
Token length: 1176
Summarized using GPT-3.5-turbo
Append: [Mitigation of Gender and Ethnicity Bias in AI-Generated Stories through Model Explanations](https://arxiv.org/abs/2509.04515)
Token length: 1968
Summarized using GPT-3.5-turbo
Append: [Artificially Fluent: Swahili AI Performance Benchmarks Between English-Trained and Natively-Trained Datasets](https://arxiv.org/abs/2509.04516)
Token length: 1623
Summarized using GPT-3.5-turbo
Append: [Analysis of Voluntarily Reported Data Post Mesh Implantation for Detecting Public Emotion and Identifying Concern Reports](https://arxiv.org/abs/2509.04517)
Token length: 1675
Summarized using GPT-3.5-turbo
Append: [Advancing SLM Tool-Use Capability using Reinforcement Learning](https://arxiv.org/abs/2509.04518)
Token length: 1612
Summarized using GPT-3.5-turbo
Append: [Hierarchical Section Matching Prediction (HSMP) BERT for Fine-Grained Extraction of Structured Data from Hebrew Free-Text Radiology Reports in Crohn's Disease](https://arxiv.org/abs/2509.04519)
Token length: 905
Summarized using GPT-3.5-turbo
Append: [Using LLMs to create analytical datasets: A case study of reconstructing the historical memory of Colombia](https://arxiv.org/abs/2509.04523)
Token length: 1339
Summarized using GPT-3.5-turbo
Append: [Quantized Large Language Models in Biomedical Natural Language Processing: Evaluation and Recommendation](https://arxiv.org/abs/2509.04534)
Token length: 1132
Summarized using GPT-3.5-turbo
Append: [Manipulating Transformer-Based Models: Controllability, Steerability, and Robust Interventions](https://arxiv.org/abs/2509.04549)
Append: [Spoken in Jest, Detected in Earnest: A Systematic Review of Sarcasm Recognition -- Multimodal Fusion, Challenges, and Future Prospects](https://arxiv.org/abs/2509.04605)
Append: [Sample-efficient Integration of New Modalities into Large Language Models](https://arxiv.org/abs/2509.04606)
Append: [Breaking to Build: A Threat Model of Prompt-Based Attacks for Securing LLMs](https://arxiv.org/abs/2509.04615)
Append: [Comparative Analysis of Transformer Models in Disaster Tweet Classification for Public Safety](https://arxiv.org/abs/2509.04650)
Append: [Polysemantic Dropout: Conformal OOD Detection for Specialized LLMs](https://arxiv.org/abs/2509.04655)
Append: [AraHalluEval: A Fine-grained Hallucination Evaluation Framework for Arabic LLMs](https://arxiv.org/abs/2509.04656)
Append: [Evaluating NL2SQL via SQL2NL](https://arxiv.org/abs/2509.04657)
Append: [Why Language Models Hallucinate](https://arxiv.org/abs/2509.04664)
Append: [ODKE+: Ontology-Guided Open-Domain Knowledge Extraction with LLMs](https://arxiv.org/abs/2509.04696)
Append: [OleSpeech-IV: A Large-Scale Multispeaker and Multilingual Conversational Speech Dataset with Diverse Topics](https://arxiv.org/abs/2509.04702)
Append: [KERAG: Knowledge-Enhanced Retrieval-Augmented Generation for Advanced Question Answering](https://arxiv.org/abs/2509.04716)
Append: [Phonological Representation Learning for Isolated Signs Improves Out-of-Vocabulary Generalization](https://arxiv.org/abs/2509.04745)
Append: [A Study of Large Language Models for Patient Information Extraction: Model Architecture, Fine-Tuning Strategy, and Multi-task Instruction Tuning](https://arxiv.org/abs/2509.04753)
Append: [Research on Multi-hop Inference Optimization of LLM Based on MQUAKE Framework](https://arxiv.org/abs/2509.04770)
Append: [Decoders Laugh as Loud as Encoders](https://arxiv.org/abs/2509.04779)
Append: [Enhancing Diversity in Large Language Models via Determinantal Point Processes](https://arxiv.org/abs/2509.04784)
Append: [Personality as a Probe for LLM Evaluation: Method Trade-offs and Downstream Effects](https://arxiv.org/abs/2509.04794)
Append: [Knowledge Collapse in LLMs: When Fluency Survives but Facts Fail under Recursive Synthetic Training](https://arxiv.org/abs/2509.04796)
Append: [Mind the Gap: Evaluating Model- and Agentic-Level Vulnerabilities in LLMs with Action Graphs](https://arxiv.org/abs/2509.04802)
Append: [Analyzing Finnish Inflectional Classes through Discriminative Lexicon and Deep Learning Models](https://arxiv.org/abs/2509.04813)
Append: [AFD-SLU: Adaptive Feature Distillation for Spoken Language Understanding](https://arxiv.org/abs/2509.04821)
Append: [Memorization $\neq$ Understanding: Do Large Language Models Have the Ability of Scenario Cognition?](https://arxiv.org/abs/2509.04866)
Append: [Using LLMs for Multilingual Clinical Entity Linking to ICD-10](https://arxiv.org/abs/2509.04868)
Append: [L1RA: Dynamic Rank Assignment in LoRA Fine-Tuning](https://arxiv.org/abs/2509.04884)
Append: [PLaMo 2 Technical Report](https://arxiv.org/abs/2509.04897)
Append: [ACE-RL: Adaptive Constraint-Enhanced Reward for Long-form Generation Reinforcement Learning](https://arxiv.org/abs/2509.04903)
Append: [Classification of kinetic-related injury in hospital triage data using NLP](https://arxiv.org/abs/2509.04969)
Append: [Optimizing Small Transformer-Based Language Models for Multi-Label Sentiment Analysis in Short Texts](https://arxiv.org/abs/2509.04982)
Append: [Do Large Language Models Need Intent? Revisiting Response Generation Strategies for Service Assistant](https://arxiv.org/abs/2509.05006)
Append: [Masked Diffusion Language Models with Frequency-Informed Training](https://arxiv.org/abs/2509.05056)
Append: [Entropy2Vec: Crosslingual Language Modeling Entropy as End-to-End Learnable Language Representations](https://arxiv.org/abs/2509.05060)
Append: [ToM-SSI: Evaluating Theory of Mind in Situated Social Interactions](https://arxiv.org/abs/2509.05066)
Append: [ICR: Iterative Clarification and Rewriting for Conversational Search](https://arxiv.org/abs/2509.05100)
Append: [PRIM: Towards Practical In-Image Multilingual Machine Translation](https://arxiv.org/abs/2509.05146)
Append: [Triadic Fusion of Cognitive, Functional, and Causal Dimensions for Explainable LLMs: The TAXAL Framework](https://arxiv.org/abs/2509.05199)
Append: [Hunyuan-MT Technical Report](https://arxiv.org/abs/2509.05209)
Append: [BEDTime: A Unified Benchmark for Automatically Describing Time Series](https://arxiv.org/abs/2509.05215)
Append: [HoPE: Hyperbolic Rotary Positional Encoding for Stable Long-Range Dependency Modeling in Large Language Models](https://arxiv.org/abs/2509.05218)
Append: [Less is More Tokens: Efficient Math Reasoning via Difficulty-Aware Chain-of-Thought Distillation](https://arxiv.org/abs/2509.05226)
Append: [CURE: Controlled Unlearning for Robust Embeddings -- Mitigating Conceptual Shortcuts in Pre-Trained Language Models](https://arxiv.org/abs/2509.05230)
Append: [Uniform Information Density and Syntactic Reduction: Revisiting $\textit{that}$-Mentioning in English Complement Clauses](https://arxiv.org/abs/2509.05254)
Append: [Elucidating the Design Space of Decay in Linear Attention](https://arxiv.org/abs/2509.05282)
Append: [Crosscoding Through Time: Tracking Emergence & Consolidation Of Linguistic Representations Throughout LLM Pretraining](https://arxiv.org/abs/2509.05291)
Append: [Labelling Data with Unknown References](https://arxiv.org/abs/2506.03083)
Append: [Narrative-to-Scene Generation: An LLM-Driven Pipeline for 2D Game Environments](https://arxiv.org/abs/2509.04481)
Append: [Maestro: Joint Graph & Config Optimization for Reliable AI Agents](https://arxiv.org/abs/2509.04642)
Append: [DarkStream: real-time speech anonymization with low latency](https://arxiv.org/abs/2509.04667)
Append: [Language-Driven Hierarchical Task Structures as Explicit World Models for Multi-Agent Learning](https://arxiv.org/abs/2509.04731)
Append: [WildScore: Benchmarking MLLMs in-the-Wild Symbolic Music Reasoning](https://arxiv.org/abs/2509.04744)
Append: [Code Review Without Borders: Evaluating Synthetic vs. Real Data for Review Recommendation](https://arxiv.org/abs/2509.04810)
Append: [Evaluating Cognitive-Behavioral Fixation via Multimodal User Viewing Patterns on Social Media](https://arxiv.org/abs/2509.04823)
Append: [SparkUI-Parser: Enhancing GUI Perception with Robust Grounding and Parsing](https://arxiv.org/abs/2509.04908)
Append: [Towards Ontology-Based Descriptions of Conversations with Qualitatively-Defined Concepts](https://arxiv.org/abs/2509.04926)
Append: [Sticker-TTS: Learn to Utilize Historical Experience with a Sticker-driven Test-Time Scaling Framework](https://arxiv.org/abs/2509.05007)
Append: [Finding your MUSE: Mining Unexpected Solutions Engine](https://arxiv.org/abs/2509.05072)
Append: [SpikingBrain Technical Report: Spiking Brain-inspired Large Models](https://arxiv.org/abs/2509.05276)
Append: [Non-Termination Proving: 100 Million LoC and Beyond](https://arxiv.org/abs/2509.05293)
Append: [Modeling Sequential Sentence Relation to Improve Cross-lingual Dense Retrieval](https://arxiv.org/abs/2302.01626)
Append: [Demystifying Chains, Trees, and Graphs of Thoughts](https://arxiv.org/abs/2401.14295)
Append: [AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling](https://arxiv.org/abs/2402.12226)
Append: [PersonaGym: Evaluating Persona Agents and LLMs](https://arxiv.org/abs/2407.18416)
Append: [ATHAR: A High-Quality and Diverse Dataset for Classical Arabic to English Translation](https://arxiv.org/abs/2407.19835)
Append: [Caution for the Environment: Multimodal LLM Agents are Susceptible to Environmental Distractions](https://arxiv.org/abs/2408.02544)
Append: [Selective Preference Optimization via Token-Level Reward Function Estimation](https://arxiv.org/abs/2408.13518)
Append: [LogicPro: Improving Complex Logical Reasoning via Program-Guided Learning](https://arxiv.org/abs/2409.12929)
Append: [What fifty-one years of Linguistics and Artificial Intelligence research tell us about their correlation: A scientometric analysis](https://arxiv.org/abs/2411.19858)
Append: [Assessing the Sensitivity and Alignment of FOL Closeness Metrics](https://arxiv.org/abs/2501.08613)
Append: [Large Language Models with Temporal Reasoning for Longitudinal Clinical Summarization and Prediction](https://arxiv.org/abs/2501.18724)
Append: [All That Glitters is Not Novel: Plagiarism in AI Generated Research](https://arxiv.org/abs/2502.16487)
Append: [Proof or Bluff? Evaluating LLMs on 2025 USA Math Olympiad](https://arxiv.org/abs/2503.21934)
Append: [StereoDetect: Detecting Stereotypes and Anti-stereotypes the Correct Way Using Social Psychological Underpinnings](https://arxiv.org/abs/2504.03352)
Append: [Can LLMs Simulate Personas with Reversed Performance? A Benchmark for Counterfactual Instruction Following](https://arxiv.org/abs/2504.06460)
Append: [ViClaim: A Multilingual Multilabel Dataset for Automatic Claim Detection in Videos](https://arxiv.org/abs/2504.12882)
Append: [RAVEN: Query-Guided Representation Alignment for Question Answering over Audio, Video, Embedded Sensors, and Natural Language](https://arxiv.org/abs/2505.17114)
Append: [First Steps Towards Overhearing LLM Agents: A Case Study With Dungeons & Dragons Gameplay](https://arxiv.org/abs/2505.22809)
Append: [Text2Cypher Across Languages: Evaluating and Finetuning LLMs](https://arxiv.org/abs/2506.21445)
Append: [HuggingGraph: Understanding the Supply Chain of LLM Ecosystem](https://arxiv.org/abs/2507.14240)
Append: [Arg-LLaDA: Argument Summarization via Large Language Diffusion Models and Sufficiency-Aware Refinement](https://arxiv.org/abs/2507.19081)
Append: [Yesterday's News: Benchmarking Multi-Dimensional Out-of-Distribution Generalization of Misinformation Detection Models](https://arxiv.org/abs/2410.18122)
Append: [TokUR: Token-Level Uncertainty Estimation for Large Language Model Reasoning](https://arxiv.org/abs/2505.11737)
Append: [Conversational Education at Scale: A Multi-LLM Agent Workflow for Procedural Learning and Pedagogic Quality Assessment](https://arxiv.org/abs/2507.05528)
Append: [Simple Yet Effective: An Information-Theoretic Approach to Multi-LLM Uncertainty Quantification](https://arxiv.org/abs/2507.07236)
Append: [MountainLion: A Multi-Modal LLM-Based Agent System for Interpretable and Adaptive Financial Trading](https://arxiv.org/abs/2507.20474)
Append: [AgentArmor: Enforcing Program Analysis on Agent Runtime Trace to Defend Against Prompt Injection](https://arxiv.org/abs/2508.01249)
append_entries: 134
Finish: 2025-09-08 04:25:15.707544
------------------------------------------------------
Started: 2025-09-08 06:25:24.402764
Existing_entries: 1134
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1272
Summarized using GPT-3.5-turbo
Append: [Persuasion Dynamics in LLMs: Investigating Robustness and Adaptability in Knowledge and Safety with DuET-PD](https://arxiv.org/abs/2508.17450)
append_entries: 1
Finish: 2025-09-08 06:25:26.282623
------------------------------------------------------
Started: 2025-09-08 08:22:54.806912
Existing_entries: 1001
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-08 08:22:55.119898
------------------------------------------------------
Started: 2025-09-08 10:18:15.263269
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-08 10:18:15.626891
------------------------------------------------------
Started: 2025-09-08 12:35:21.655756
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-08 12:35:22.030888
------------------------------------------------------
Started: 2025-09-08 14:16:27.063136
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-08 14:16:27.399630
------------------------------------------------------
Started: 2025-09-08 16:19:29.782079
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-08 16:19:30.181440
------------------------------------------------------
Started: 2025-09-08 18:23:35.097901
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-08 18:23:35.416951
------------------------------------------------------
Started: 2025-09-08 20:17:36.685541
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-08 20:17:36.999292
------------------------------------------------------
Started: 2025-09-08 22:15:03.241027
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-08 22:15:03.650081
------------------------------------------------------
Started: 2025-09-09 01:14:15.019572
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-09 01:14:15.335953
------------------------------------------------------
Started: 2025-09-09 02:59:14.295437
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-09 02:59:14.707338
------------------------------------------------------
Started: 2025-09-09 04:23:08.062454
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 974
Summarized using GPT-3.5-turbo
Append: [An Empirical Analysis of Discrete Unit Representations in Speech Language Modeling Pre-training](https://arxiv.org/abs/2509.05359)
Token length: 1399
Summarized using GPT-3.5-turbo
Append: [Beyond ROUGE: N-Gram Subspace Features for LLM Hallucination Detection](https://arxiv.org/abs/2509.05360)
Token length: 1006
Summarized using GPT-3.5-turbo
Append: [A Lightweight Framework for Trigger-Guided LoRA-Based Self-Adaptation in LLMs](https://arxiv.org/abs/2509.05385)
Token length: 1087
Summarized using GPT-3.5-turbo
Append: [Talk Isn't Always Cheap: Understanding Failure Modes in Multi-Agent Debate](https://arxiv.org/abs/2509.05396)
Token length: 943
Summarized using GPT-3.5-turbo
Append: [No Translation Needed: Forecasting Quality from Fertility and Metadata](https://arxiv.org/abs/2509.05425)
Token length: 1062
Summarized using GPT-3.5-turbo
Append: [Direct-Scoring NLG Evaluators Can Use Pairwise Comparisons Too](https://arxiv.org/abs/2509.05440)
Token length: 1637
Summarized using GPT-3.5-turbo
Append: [From Staff Messages to Actionable Insights: A Multi-Stage LLM Classification Framework for Healthcare Analytics](https://arxiv.org/abs/2509.05484)
Token length: 1009
Summarized using GPT-3.5-turbo
Append: [The Token Tax: Systematic Bias in Multilingual Tokenization](https://arxiv.org/abs/2509.05486)
Token length: 1435
Summarized using GPT-3.5-turbo
Append: [Biomedical Literature Q&A System Using Retrieval-Augmented Generation (RAG)](https://arxiv.org/abs/2509.05505)
Token length: 1693
Summarized using GPT-3.5-turbo
Append: [Using Contrastive Learning to Improve Two-Way Reasoning in Large Language Models: The Obfuscation Task as a Case Study](https://arxiv.org/abs/2509.05553)
Token length: 1336
Summarized using GPT-3.5-turbo
Append: [Ad hoc conventions generalize to new referents](https://arxiv.org/abs/2509.05566)
Token length: 1460
Summarized using GPT-3.5-turbo
Append: [Mitigating Spurious Correlations Between Question and Answer via Chain-of-Thought Correctness Perception Distillation](https://arxiv.org/abs/2509.05602)
Token length: 1272
Summarized using GPT-3.5-turbo
Append: [Icon$^{2}$: Aligning Large Language Models Using Self-Synthetic Preference Data via Inherent Regulation](https://arxiv.org/abs/2509.05605)
Token length: 1115
Summarized using GPT-3.5-turbo
Append: [Beyond Keywords: Driving Generative Search Engine Optimization with Content-Centric Agents](https://arxiv.org/abs/2509.05607)
Token length: 1668
Summarized using GPT-3.5-turbo
Append: [New Insights into Optimal Alignment of Acoustic and Linguistic Representations for Knowledge Transfer in ASR](https://arxiv.org/abs/2509.05609)
Token length: 1229
Summarized using GPT-3.5-turbo
Append: [From Joy to Fear: A Benchmark of Emotion Estimation in Pop Song Lyrics](https://arxiv.org/abs/2509.05617)
Token length: 1453
Summarized using GPT-3.5-turbo
Append: [Few-Shot Query Intent Detection via Relation-Aware Prompt Learning](https://arxiv.org/abs/2509.05635)
Token length: 1485
Summarized using GPT-3.5-turbo
Append: [LM-Searcher: Cross-domain Neural Architecture Search with LLMs via Unified Numerical Encoding](https://arxiv.org/abs/2509.05657)
Token length: 1273
Summarized using GPT-3.5-turbo
Append: [Cross-Question Method Reuse in Large Language Models: From Word-Level Prediction to Rational Logical-Layer Reasoning](https://arxiv.org/abs/2509.05660)
Token length: 1379
Summarized using GPT-3.5-turbo
Append: [Llama-GENBA-10B: A Trilingual Large Language Model for German, English and Bavarian](https://arxiv.org/abs/2509.05668)
Token length: 1178
Summarized using GPT-3.5-turbo
Append: [Revealing the Numeracy Gap: An Empirical Investigation of Text Embedding Models](https://arxiv.org/abs/2509.05691)
Token length: 1535
Summarized using GPT-3.5-turbo
Append: [A Survey of the State-of-the-Art in Conversational Question Answering Systems](https://arxiv.org/abs/2509.05716)
Token length: 1067
Summarized using GPT-3.5-turbo
Append: [Exploring Subjective Tasks in Farsi: A Survey Analysis and Evaluation of Language Models](https://arxiv.org/abs/2509.05719)
Token length: 1948
Summarized using GPT-3.5-turbo
Append: [QCSE: A Pretrained Quantum Context-Sensitive Word Embedding for Natural Language Processing](https://arxiv.org/abs/2509.05729)
Token length: 794
Summarized using GPT-3.5-turbo
Append: [Enhancing Factual Accuracy and Citation Generation in LLMs via Multi-Stage Self-Verification](https://arxiv.org/abs/2509.05741)
Token length: 1006
Summarized using GPT-3.5-turbo
Append: [LatinX: Aligning a Multilingual TTS Model with Direct Preference Optimization](https://arxiv.org/abs/2509.05863)
Token length: 1528
Summarized using GPT-3.5-turbo
Append: [ZhiFangDanTai: Fine-tuning Graph-based Retrieval-Augmented Generation Model for Traditional Chinese Medicine Formula](https://arxiv.org/abs/2509.05867)
Token length: 1285
Summarized using GPT-3.5-turbo
Append: [MedFactEval and MedAgentBrief: A Framework and Workflow for Generating and Evaluating Factual Clinical Summaries](https://arxiv.org/abs/2509.05878)
Token length: 1373
Summarized using GPT-3.5-turbo
Append: [Let's Roleplay: Examining LLM Alignment in Collaborative Dialogues](https://arxiv.org/abs/2509.05882)
Token length: 1696
Summarized using GPT-3.5-turbo
Append: [Enhancing the Robustness of Contextual ASR to Varying Biasing Information Volumes Through Purified Semantic Correlation Joint Modeling](https://arxiv.org/abs/2509.05908)
Token length: 1352
Summarized using GPT-3.5-turbo
Append: [Accelerating Large Language Model Inference via Early-Exiting Algorithms](https://arxiv.org/abs/2509.05915)
Token length: 1015
Summarized using GPT-3.5-turbo
Append: [KatotohananQA: Evaluating Truthfulness of Large Language Models in Filipino](https://arxiv.org/abs/2509.06065)
Token length: 1293
Summarized using GPT-3.5-turbo
Append: [Multimodal Fine-grained Context Interaction Graph Modeling for Conversational Speech Synthesis](https://arxiv.org/abs/2509.06074)
Token length: 779
Summarized using GPT-3.5-turbo
Append: [Multimodal Reasoning for Science: Technical Report and 1st Place Solution to the ICML 2025 SeePhys Challenge](https://arxiv.org/abs/2509.06079)
Token length: 1025
Summarized using GPT-3.5-turbo
Append: [Orthogonal Low-rank Adaptation in Lie Groups for Continual Learning of Large Language Models](https://arxiv.org/abs/2509.06100)
Token length: 1035
Summarized using GPT-3.5-turbo
Append: [Benchmarking Gender and Political Bias in Large Language Models](https://arxiv.org/abs/2509.06164)
Token length: 1090
Summarized using GPT-3.5-turbo
Append: [Understanding the Influence of Synthetic Data for Text Embedders](https://arxiv.org/abs/2509.06184)
Token length: 1406
Summarized using GPT-3.5-turbo
Append: [Augmented Fine-Tuned LLMs for Enhanced Recruitment Automation](https://arxiv.org/abs/2509.06196)
Token length: 1119
Summarized using GPT-3.5-turbo
Append: [MSLEF: Multi-Segment LLM Ensemble Finetuning in Recruitment](https://arxiv.org/abs/2509.06200)
Token length: 898
Summarized using GPT-3.5-turbo
Append: [No Encore: Unlearning as Opt-Out in Music Generation](https://arxiv.org/abs/2509.06277)
Token length: 1551
Summarized using GPT-3.5-turbo
Append: [Mask-GCG: Are All Tokens in Adversarial Suffixes Necessary for Jailbreak Attacks?](https://arxiv.org/abs/2509.06350)
Token length: 1738
Summarized using GPT-3.5-turbo
Append: [PL-CA: A Parametric Legal Case Augmentation Framework](https://arxiv.org/abs/2509.06356)
Token length: 1057
Summarized using GPT-3.5-turbo
Append: [Do LLMs exhibit the same commonsense capabilities across languages?](https://arxiv.org/abs/2509.06401)
Token length: 1746
Summarized using GPT-3.5-turbo
Append: [WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents](https://arxiv.org/abs/2509.06501)
Token length: 1022
Summarized using GPT-3.5-turbo
Append: [Crown, Frame, Reverse: Layer-Wise Scaling Variants for LLM Pre-Training](https://arxiv.org/abs/2509.06518)
Token length: 1411
Summarized using GPT-3.5-turbo
Append: [LAMDAS: LLM as an Implicit Classifier for Domain-specific Data Selection](https://arxiv.org/abs/2509.06524)
Token length: 1424
Summarized using GPT-3.5-turbo
Append: [SLiNT: Structure-aware Language Model with Injection and Contrastive Training for Knowledge Graph Completion](https://arxiv.org/abs/2509.06531)
Token length: 1329
Summarized using GPT-3.5-turbo
Append: [HAVE: Head-Adaptive Gating and ValuE Calibration for Hallucination Mitigation in Large Language Models](https://arxiv.org/abs/2509.06596)
Token length: 1011
Summarized using GPT-3.5-turbo
Append: [Guided Decoding and Its Critical Role in Retrieval-Augmented Generation](https://arxiv.org/abs/2509.06631)
Token length: 791
Summarized using GPT-3.5-turbo
Append: [Modelling Intertextuality with N-gram Embeddings](https://arxiv.org/abs/2509.06637)
Append: [Domain-Aware RAG: MoL-Enhanced RL for Efficient Training and Scalable Retrieval](https://arxiv.org/abs/2509.06650)
Append: [IntrEx: A Dataset for Modeling Engagement in Educational Conversations](https://arxiv.org/abs/2509.06652)
Append: [ParCzech4Speech: A New Speech Corpus Derived from Czech Parliamentary Data](https://arxiv.org/abs/2509.06675)
Append: [Will Annotators Disagree? Identifying Subjectivity in Value-Laden Arguments](https://arxiv.org/abs/2509.06704)
Append: [Anchoring Refusal Direction: Mitigating Safety Risks in Tuning via Projection Constraint](https://arxiv.org/abs/2509.06795)
Append: [MachineLearningLM: Continued Pretraining Language Models on Millions of Synthetic Tabular Prediction Tasks Scales In-Context ML](https://arxiv.org/abs/2509.06806)
Append: [MoGU V2: Toward a Higher Pareto Frontier Between Model Usability and Security](https://arxiv.org/abs/2509.06807)
Append: [Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in the TPTP Ecosystem](https://arxiv.org/abs/2509.06809)
Append: [A Comparative Benchmark of Large Language Models for Labelling Wind Turbine Maintenance Logs](https://arxiv.org/abs/2509.06813)
Append: [COMPACT: Common-token Optimized Model Pruning Across Channels and Tokens](https://arxiv.org/abs/2509.06836)
Append: [EPT Benchmark: Evaluation of Persian Trustworthiness in Large Language Models](https://arxiv.org/abs/2509.06838)
Append: [The Majority is not always right: RL training for solution aggregation](https://arxiv.org/abs/2509.06870)
Append: [UNH at CheckThat! 2025: Fine-tuning Vs Prompting in Claim Extraction](https://arxiv.org/abs/2509.06883)
Append: [mmBERT: A Modern Multilingual Encoder with Annealed Language Learning](https://arxiv.org/abs/2509.06888)
Append: [Proof-Carrying Numbers (PCN): A Protocol for Trustworthy Numeric Answers from LLMs via Claim Verification](https://arxiv.org/abs/2509.06902)
Append: [Beyond Two-Stage Training: Cooperative SFT and RL for LLM Reasoning](https://arxiv.org/abs/2509.06948)
Append: [Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models](https://arxiv.org/abs/2509.06949)
Append: [On the Same Wavelength? Evaluating Pragmatic Reasoning in Language Models across Broad Concepts](https://arxiv.org/abs/2509.06952)
Append: [Are LLM Agents Behaviorally Coherent? Latent Profiles for Social Simulation](https://arxiv.org/abs/2509.03736)
Append: [ProtSAE: Disentangling and Interpreting Protein Language Models via Semantically-Guided Sparse Autoencoders](https://arxiv.org/abs/2509.05309)
Append: [ForensicsData: A Digital Forensics Dataset for Large Language Models](https://arxiv.org/abs/2509.05331)
Append: [Authorship Without Writing: Large Language Models and the Senior Author Analogy](https://arxiv.org/abs/2509.05390)
Append: [Cross-Service Threat Intelligence in LLM Services using Privacy-Preserving Fingerprints](https://arxiv.org/abs/2509.05608)
Append: [On the Contribution of Lexical Features to Speech Emotion Recognition](https://arxiv.org/abs/2509.05634)
Append: [Imagining Alternatives: Towards High-Resolution 3D Counterfactual Medical Image Generation via Language Guidance](https://arxiv.org/abs/2509.05978)
Append: [TSPC: A Two-Stage Phoneme-Centric Architecture for code-switching Vietnamese-English Speech Recognition](https://arxiv.org/abs/2509.05983)
Append: [Language Native Lightly Structured Databases for Large Language Model Driven Composite Materials Research](https://arxiv.org/abs/2509.06093)
Append: [Reverse-Engineered Reasoning for Open-Ended Generation](https://arxiv.org/abs/2509.06160)
Append: [From Long to Short: LLMs Excel at Trimming Own Reasoning Chains](https://arxiv.org/abs/2509.06174)
Append: [Language Bias in Information Retrieval: The Nature of the Beast and Mitigation Methods](https://arxiv.org/abs/2509.06195)
Append: [Beamforming-LLM: What, Where and When Did I Miss?](https://arxiv.org/abs/2509.06221)
Append: [SFR-DeepResearch: Towards Effective Reinforcement Learning for Autonomously Reasoning Single Agents](https://arxiv.org/abs/2509.06283)
Append: [Index-Preserving Lightweight Token Pruning for Efficient Document Understanding in Vision-Language Models](https://arxiv.org/abs/2509.06415)
Append: [Reinforcement Learning Foundations for Deep Research Systems: A Survey](https://arxiv.org/abs/2509.06733)
Append: [VehicleWorld: A Highly Integrated Multi-Device Environment for Intelligent Vehicle Interaction](https://arxiv.org/abs/2509.06736)
Append: [RAFFLES: Reasoning-based Attribution of Faults for LLM Systems](https://arxiv.org/abs/2509.06822)
Append: [Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet](https://arxiv.org/abs/2509.06861)
Append: [Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents](https://arxiv.org/abs/2509.06917)
Append: [An Ethically Grounded LLM-Based Approach to Insider Threat Synthesis and Detection](https://arxiv.org/abs/2509.06920)
Append: [Outcome-based Exploration for LLM Reasoning](https://arxiv.org/abs/2509.06941)
Append: [Interleaving Reasoning for Better Text-to-Image Generation](https://arxiv.org/abs/2509.06945)
Append: [Multiple Noises in Diffusion Model for Semi-Supervised Multi-Domain Translation](https://arxiv.org/abs/2309.14394)
Append: [Support or Refute: Analyzing the Stance of Evidence to Detect Out-of-Context Mis- and Disinformation](https://arxiv.org/abs/2311.01766)
Append: [Grammaticality illusion or ambiguous interpretation? Event-related potentials reveal the nature of the missing-NP effect in Mandarin centre-embedded structures](https://arxiv.org/abs/2402.11282)
Append: [Repetition Improves Language Model Embeddings](https://arxiv.org/abs/2402.15449)
Append: [Linearly Controlled Language Generation with Performative Guarantees](https://arxiv.org/abs/2405.15454)
Append: [Synth-SBDH: A Synthetic Dataset of Social and Behavioral Determinants of Health for Clinical Text](https://arxiv.org/abs/2406.06056)
Append: [A Principled Framework for Evaluating on Typologically Diverse Languages](https://arxiv.org/abs/2407.05022)
Append: [Affective Computing in the Era of Large Language Models: A Survey from the NLP Perspective](https://arxiv.org/abs/2408.04638)
Append: [Self-Alignment: Improving Alignment of Cultural Values in LLMs via In-Context Learning](https://arxiv.org/abs/2408.16482)
Append: [Towards No-Code Programming of Cobots: Experiments with Code Synthesis by Large Code Models for Conversational Programming](https://arxiv.org/abs/2409.11041)
Append: [Extracting and Combining Abilities For Building Multi-lingual Ability-enhanced Large Language Models](https://arxiv.org/abs/2410.07825)
Append: [Conversational Code Generation: a Case Study of Designing a Dialogue System for Generating Driving Scenarios for Testing Autonomous Vehicles](https://arxiv.org/abs/2410.09829)
Append: [GASE: Generatively Augmented Sentence Encoding](https://arxiv.org/abs/2411.04914)
Append: [Exploring the Limits of Large Language Models: A Systematic Evaluation of Masked Text Processing Ability through MskQA and MskCal](https://arxiv.org/abs/2411.05665)
Append: [HierTOD: A Task-Oriented Dialogue System Driven by Hierarchical Goals](https://arxiv.org/abs/2411.07152)
Append: [Lessons from Studying Two-Hop Latent Reasoning](https://arxiv.org/abs/2411.16353)
Append: [Fine-Tuning Large Language Models for Scientific Text Classification: A Comparative Study](https://arxiv.org/abs/2412.00098)
Append: [Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Hop Arithmetic Reasoning](https://arxiv.org/abs/2412.01113)
Append: [Concept Bottleneck Large Language Models](https://arxiv.org/abs/2412.07992)
Append: [Process-Supervised Reward Models for Verifying Clinical Note Generation: A Scalable Approach Guided by Domain Expertise](https://arxiv.org/abs/2412.12583)
Append: [Revealing the impact of synthetic native samples and multi-tasking strategies in Hindi-English code-mixed humour and sarcasm detection](https://arxiv.org/abs/2412.12761)
Append: [Knowledge Editing through Chain-of-Thought](https://arxiv.org/abs/2412.17727)
Append: [Turning Logic Against Itself : Probing Model Defenses Through Contrastive Questions](https://arxiv.org/abs/2501.01872)
Append: [OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking](https://arxiv.org/abs/2501.09751)
Append: [Error Classification of Large Language Models on Math Word Problems: A Dynamically Adaptive Framework](https://arxiv.org/abs/2501.15581)
Append: [Through the Prism of Culture: Evaluating LLMs' Understanding of Indian Subcultures and Traditions](https://arxiv.org/abs/2501.16748)
Append: [Premise-Augmented Reasoning Chains Improve Error Identification in Math reasoning with LLMs](https://arxiv.org/abs/2502.02362)
Append: [Position: LLMs Can be Good Tutors in English Education](https://arxiv.org/abs/2502.05467)
Append: [Reinforced Lifelong Editing for Language Models](https://arxiv.org/abs/2502.05759)
Append: [Improve LLM-as-a-Judge Ability as a General Ability](https://arxiv.org/abs/2502.11689)
Append: [Soft Token Attacks Cannot Reliably Audit Unlearning in Large Language Models](https://arxiv.org/abs/2502.15836)
Append: [Evaluating the Robustness and Accuracy of Text Watermarking Under Real-World Cross-Lingual Manipulations](https://arxiv.org/abs/2502.16699)
Append: [Low-Confidence Gold: Refining Low-Confidence Samples for Efficient Instruction Tuning](https://arxiv.org/abs/2502.18978)
Append: [PlainQAFact: Retrieval-augmented Factual Consistency Evaluation Metric for Biomedical Plain Language Summarization](https://arxiv.org/abs/2503.08890)
Append: [X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and Extreme KV Compression](https://arxiv.org/abs/2503.11132)
Append: [LinkAlign: Scalable Schema Linking for Real-World Large-Scale Multi-Database Text-to-SQL](https://arxiv.org/abs/2503.18596)
Append: [Learning to Reason for Long-Form Story Generation](https://arxiv.org/abs/2503.22828)
Append: [Efficient Dynamic Clustering-Based Document Compression for Retrieval-Augmented-Generation](https://arxiv.org/abs/2504.03165)
Append: [Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models](https://arxiv.org/abs/2504.03624)
Append: [Advancing Scientific Text Classification: Fine-Tuned Models with Dataset Expansion and Hard-Voting](https://arxiv.org/abs/2504.19021)
Append: [Assessing and Mitigating Medical Knowledge Drift and Conflicts in Large Language Models](https://arxiv.org/abs/2505.07968)
Append: [Pierce the Mists, Greet the Sky: Decipher Knowledge Overshadowing via Knowledge Circuit Analysis](https://arxiv.org/abs/2505.14406)
Append: [Language Mixing in Reasoning Language Models: Patterns, Impact, and Internal Causes](https://arxiv.org/abs/2505.14815)
Append: [VocalBench: Benchmarking the Vocal Conversational Abilities for Speech Interaction Models](https://arxiv.org/abs/2505.15727)
Append: [Too Consistent to Detect: A Study of Self-Consistent Errors in LLMs](https://arxiv.org/abs/2505.17656)
Append: [Fast Quiet-STaR: Thinking Without Thought Tokens](https://arxiv.org/abs/2505.17746)
Append: [Rhapsody: A Dataset for Highlight Detection in Podcasts](https://arxiv.org/abs/2505.19429)
Append: [Self-Critique and Refinement for Faithful Natural Language Explanations](https://arxiv.org/abs/2505.22823)
Append: [ChatCFD: An LLM-Driven Agent for End-to-End CFD Automation with Domain-Specific Structured Reasoning](https://arxiv.org/abs/2506.02019)
Append: [TreeReview: A Dynamic Tree of Questions Framework for Deep and Efficient LLM-based Scientific Peer Review](https://arxiv.org/abs/2506.07642)
Append: [Persona-driven Simulation of Voting Behavior in the European Parliament with Large Language Models](https://arxiv.org/abs/2506.11798)
Append: [RADIANT: Retrieval AugmenteD entIty-context AligNmenT -- Introducing RAG-ability and Entity-Context Divergence](https://arxiv.org/abs/2507.02949)
Append: [Dynamic Injection of Entity Knowledge into Dense Retrievers](https://arxiv.org/abs/2507.03922)
Append: [Step-level Verifier-guided Hybrid Test-Time Scaling for Large Language Models](https://arxiv.org/abs/2507.15512)
Append: [Not All Features Deserve Attention: Graph-Guided Dependency Learning for Tabular Data Generation with Language Models](https://arxiv.org/abs/2507.18504)
Append: [CodeMixBench: Evaluating Code-Mixing Capabilities of LLMs Across 18 Languages](https://arxiv.org/abs/2507.18791)
Append: [Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and Context-Aware KGQA](https://arxiv.org/abs/2508.00719)
Append: [Out of the Box, into the Clinic? Evaluating State-of-the-Art ASR for Clinical Applications for Older Adults](https://arxiv.org/abs/2508.08684)
Append: [EMNLP: Educator-role Moral and Normative Large Language Models Profiling](https://arxiv.org/abs/2508.15250)
Append: [CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated Chain-of-Thought-based Reinforced Fine-Tuning](https://arxiv.org/abs/2508.15868)
Append: [Jet-Nemotron: Efficient Language Model with Post Neural Architecture Search](https://arxiv.org/abs/2508.15884)
Append: [Leveraging Large Language Models for Accurate Sign Language Translation in Low-Resource Scenarios](https://arxiv.org/abs/2508.18183)
Append: [MedualTime: A Dual-Adapter Language Model for Medical Time Series-Text Multimodal Learning](https://arxiv.org/abs/2406.06620)
Append: [ResearchArena: Benchmarking Large Language Models' Ability to Collect and Organize Information as Research Agents](https://arxiv.org/abs/2406.10291)
Append: [BeSimulator: A Large Language Model Powered Text-based Behavior Simulator](https://arxiv.org/abs/2409.15865)
Append: [Causal Representation Learning with Generative Artificial Intelligence: Application to Texts as Treatments](https://arxiv.org/abs/2410.00903)
Append: [ETF: An Entity Tracing Framework for Hallucination Detection in Code Summaries](https://arxiv.org/abs/2410.14748)
Append: [ElectroVizQA: How well do Multi-modal LLMs perform in Electronics Visual Question Answering?](https://arxiv.org/abs/2412.00102)
Append: [ChinaTravel: An Open-Ended Benchmark for Language Agents in Chinese Travel Planning](https://arxiv.org/abs/2412.13682)
Append: [AI Sees Your Location, But With A Bias Toward The Wealthy World](https://arxiv.org/abs/2502.11163)
Append: [VisBias: Measuring Explicit and Implicit Social Biases in Vision Language Models](https://arxiv.org/abs/2503.07575)
Append: [MM-Spatial: Exploring 3D Spatial Understanding in Multimodal LLMs](https://arxiv.org/abs/2503.13111)
Append: [Antidistillation Sampling](https://arxiv.org/abs/2504.13146)
Append: [OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simulation](https://arxiv.org/abs/2504.13707)
Append: [A Minimum Description Length Approach to Regularization in Neural Networks](https://arxiv.org/abs/2505.13398)
Append: [InterFeat: A Pipeline for Finding Interesting Scientific Features](https://arxiv.org/abs/2505.13534)
Append: [Project Riley: Multimodal Multi-Agent LLM Collaboration with Emotional Reasoning and Voting](https://arxiv.org/abs/2505.20521)
Append: [SUDER: Self-Improving Unified Large Multimodal Models for Understanding and Generation with Dual Self-Rewards](https://arxiv.org/abs/2506.07963)
Append: [An LLM + ASP Workflow for Joint Entity-Relation Extraction](https://arxiv.org/abs/2508.12611)
Append: [FinAgentBench: A Benchmark Dataset for Agentic Retrieval in Financial Question Answering](https://arxiv.org/abs/2508.14052)
append_entries: 171
Finish: 2025-09-09 04:24:49.694843
------------------------------------------------------
Started: 2025-09-09 06:25:14.613090
Existing_entries: 1171
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1342
Summarized using GPT-3.5-turbo
Append: [A Survey on Training-free Alignment of Large Language Models](https://arxiv.org/abs/2508.09016)
Token length: 1326
Summarized using GPT-3.5-turbo
Append: [Empathy Omni: Enabling Empathetic Speech Response Generation through Large Language Models](https://arxiv.org/abs/2508.18655)
Token length: 1023
Summarized using GPT-3.5-turbo
Append: [Automatic Prompt Optimization with Prompt Distillation](https://arxiv.org/abs/2508.18992)
Token length: 1373
Summarized using GPT-3.5-turbo
Append: [MovieCORE: COgnitive REasoning in Movies](https://arxiv.org/abs/2508.19026)
Token length: 1277
Summarized using GPT-3.5-turbo
Append: [Membership Inference Attacks on LLM-based Recommender Systems](https://arxiv.org/abs/2508.18665)
Token length: 1426
Summarized using GPT-3.5-turbo
Append: [Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A Framework and Benchmark](https://arxiv.org/abs/2508.19005)
append_entries: 6
Finish: 2025-09-09 06:25:26.576253
------------------------------------------------------
Started: 2025-09-09 08:22:10.379226
Existing_entries: 1006
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-09 08:22:10.875475
------------------------------------------------------
Started: 2025-09-09 10:17:11.863418
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-09 10:17:12.270949
------------------------------------------------------
Started: 2025-09-09 12:35:25.402665
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-09 12:35:25.859658
------------------------------------------------------
Started: 2025-09-09 14:14:06.143861
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-09 14:14:06.551188
------------------------------------------------------
Started: 2025-09-09 16:20:15.561767
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-09 16:20:15.977055
------------------------------------------------------
Started: 2025-09-09 18:19:56.293323
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-09 18:19:56.775695
------------------------------------------------------
Started: 2025-09-09 20:17:35.768838
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-09 20:17:36.180921
------------------------------------------------------
Started: 2025-09-09 22:13:25.281651
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-09 22:13:25.719387
------------------------------------------------------
Started: 2025-09-10 01:11:59.701605
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-10 01:12:00.137422
------------------------------------------------------
Started: 2025-09-10 02:53:00.630386
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-10 02:53:01.034490
------------------------------------------------------
Started: 2025-09-10 04:21:28.877233
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1323
Summarized using GPT-3.5-turbo
Append: [MedBench-IT: A Comprehensive Benchmark for Evaluating Large Language Models on Italian Medical Entrance Examinations](https://arxiv.org/abs/2509.07135)
Token length: 1042
Summarized using GPT-3.5-turbo
Append: [The ML-SUPERB 2.0 Challenge: Towards Inclusive ASR Benchmarking for All Language Varieties](https://arxiv.org/abs/2509.07139)
Token length: 1503
Summarized using GPT-3.5-turbo
Append: [Toward Purpose-oriented Topic Model Evaluation enabled by Large Language Models](https://arxiv.org/abs/2509.07142)
Token length: 1132
Summarized using GPT-3.5-turbo
Append: [Towards EnergyGPT: A Large Language Model Specialized for the Energy Sector](https://arxiv.org/abs/2509.07177)
Token length: 1434
Summarized using GPT-3.5-turbo
Append: [DischargeSim: A Simulation Benchmark for Educational Doctor-Patient Communication at Discharge](https://arxiv.org/abs/2509.07188)
Token length: 1061
Summarized using GPT-3.5-turbo
Append: [Rule-Based Moral Principles for Explaining Uncertainty in Natural Language Generation](https://arxiv.org/abs/2509.07190)
Token length: 1513
Summarized using GPT-3.5-turbo
Append: [LLM Analysis of 150+ years of German Parliamentary Debates on Migration Reveals Shift from Post-War Solidarity to Anti-Solidarity in the Last Decade](https://arxiv.org/abs/2509.07274)
Token length: 929
Summarized using GPT-3.5-turbo
Append: [Causal Attention with Lookahead Keys](https://arxiv.org/abs/2509.07301)
Token length: 1488
Summarized using GPT-3.5-turbo
Append: [Basis Vector Metric: A Method for Robust Open-Ended State Change Detection](https://arxiv.org/abs/2509.07308)
Token length: 867
Summarized using GPT-3.5-turbo
Append: [Instance-level Performance Prediction for Long-form Generation Tasks](https://arxiv.org/abs/2509.07309)
Token length: 1744
Summarized using GPT-3.5-turbo
Append: [Does This Look Familiar to You? Knowledge Analysis via Model Internal Representations](https://arxiv.org/abs/2509.07311)
Token length: 1001
Summarized using GPT-3.5-turbo
Append: [Mitigating Attention Localization in Small Scale: Self-Attention Refinement via One-step Belief Propagation](https://arxiv.org/abs/2509.07324)
Token length: 1903
Summarized using GPT-3.5-turbo
Append: [PersonaFuse: A Personality Activation-Driven Framework for Enhancing Human-LLM Interactions](https://arxiv.org/abs/2509.07370)
Token length: 1003
Summarized using GPT-3.5-turbo
Append: [Talking with Oompa Loompas: A novel framework for evaluating linguistic acquisition of LLM agents](https://arxiv.org/abs/2509.07389)
Token length: 996
Summarized using GPT-3.5-turbo
Append: [The Role of Exploration Modules in Small Language Models for Knowledge Graph Question Answering](https://arxiv.org/abs/2509.07399)
Token length: 1820
Summarized using GPT-3.5-turbo
Append: [LongEmotion: Measuring Emotional Intelligence of Large Language Models in Long-Context Interaction](https://arxiv.org/abs/2509.07403)
Token length: 1006
Summarized using GPT-3.5-turbo
Append: [AIxcellent Vibes at GermEval 2025 Shared Task on Candy Speech Detection: Improving Model Performance by Span-Level Training](https://arxiv.org/abs/2509.07459)
Token length: 1067
Summarized using GPT-3.5-turbo
Append: [Understanding Stigmatizing Language Lexicons: A Comparative Analysis in Clinical Contexts](https://arxiv.org/abs/2509.07462)
Token length: 860
Summarized using GPT-3.5-turbo
Append: [From Scarcity to Efficiency: Investigating the Effects of Data Augmentation on African Machine Translation](https://arxiv.org/abs/2509.07471)
Token length: 1235
Summarized using GPT-3.5-turbo
Append: [HALT-RAG: A Task-Adaptable Framework for Hallucination Detection with Calibrated NLI Ensembles and Abstention](https://arxiv.org/abs/2509.07475)
Token length: 1326
Summarized using GPT-3.5-turbo
Append: [ALLabel: Three-stage Active Learning for LLM-based Entity Recognition using Demonstration Retrieval](https://arxiv.org/abs/2509.07512)
Token length: 1265
Summarized using GPT-3.5-turbo
Append: [VeriOS: Query-Driven Proactive Human-Agent-GUI Interaction for Trustworthy OS Agents](https://arxiv.org/abs/2509.07553)
Token length: 1175
Summarized using GPT-3.5-turbo
Append: [Avoiding Knowledge Edit Skipping in Multi-hop Question Answering with Guided Decomposition](https://arxiv.org/abs/2509.07555)
Token length: 1247
Summarized using GPT-3.5-turbo
Append: [BALI: Enhancing Biomedical Language Representations through Knowledge Graph and Language Model Alignment](https://arxiv.org/abs/2509.07588)
Token length: 1432
Summarized using GPT-3.5-turbo
Append: [MaLei at MultiClinSUM: Summarisation of Clinical Documents using Perspective-Aware Iterative Self-Prompting with LLMs](https://arxiv.org/abs/2509.07622)
Token length: 1655
Summarized using GPT-3.5-turbo
Append: [MoLoRAG: Bootstrapping Document Understanding via Multi-modal Logic-aware Retrieval](https://arxiv.org/abs/2509.07666)
Token length: 1442
Summarized using GPT-3.5-turbo
Append: [M-BRe: Discovering Training Samples for Relation Extraction from Unlabeled Texts with Large Language Models](https://arxiv.org/abs/2509.07730)
Token length: 1105
Summarized using GPT-3.5-turbo
Append: [Factuality Beyond Coherence: Evaluating LLM Watermarking Methods for Medical Texts](https://arxiv.org/abs/2509.07755)
Token length: 1425
Summarized using GPT-3.5-turbo
Append: [Are LLMs Enough for Hyperpartisan, Fake, Polarized and Harmful Content Detection? Evaluating In-Context Learning vs. Fine-Tuning](https://arxiv.org/abs/2509.07768)
Token length: 1551
Summarized using GPT-3.5-turbo
Append: [SciNLP: A Domain-Specific Benchmark for Full-Text Scientific Entity and Relation Extraction in NLP](https://arxiv.org/abs/2509.07801)
Token length: 1568
Summarized using GPT-3.5-turbo
Append: [Dual Knowledge-Enhanced Two-Stage Reasoner for Multimodal Dialog Systems](https://arxiv.org/abs/2509.07817)
Token length: 1874
Summarized using GPT-3.5-turbo
Append: [Small Open Models Achieve Near Parity with Large Models in Low Resource Literary Translation at a Fraction of the Cost](https://arxiv.org/abs/2509.07829)
Token length: 1421
Summarized using GPT-3.5-turbo
Append: [Are Humans as Brittle as Large Language Models?](https://arxiv.org/abs/2509.07869)
Token length: 1267
Summarized using GPT-3.5-turbo
Append: [From Detection to Mitigation: Addressing Gender Bias in Chinese Texts via Efficient Tuning and Voting-Based Rebalancing](https://arxiv.org/abs/2509.07889)
Token length: 949
Summarized using GPT-3.5-turbo
Append: [Biased Tales: Cultural and Topic Bias in Generating Children's Stories](https://arxiv.org/abs/2509.07908)
Token length: 1031
Summarized using GPT-3.5-turbo
Append: [GENUINE: Graph Enhanced Multi-level Uncertainty Estimation for Large Language Models](https://arxiv.org/abs/2509.07925)
Token length: 1021
Summarized using GPT-3.5-turbo
Append: [SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric Knowledge](https://arxiv.org/abs/2509.07968)
Token length: 1807
Summarized using GPT-3.5-turbo
Append: [Parallel-R1: Towards Parallel Thinking via Reinforcement Learning](https://arxiv.org/abs/2509.07980)
Token length: 1421
Summarized using GPT-3.5-turbo
Append: [CARE: Decoding Time Safety Alignment via Rollback and Introspection Intervention](https://arxiv.org/abs/2509.06982)
Token length: 1814
Summarized using GPT-3.5-turbo
Append: [VLMs-in-the-Wild: Bridging the Gap Between Academic Benchmarks and Enterprise Reality](https://arxiv.org/abs/2509.06994)
Token length: 1439
Summarized using GPT-3.5-turbo
Append: [ArGen: Auto-Regulation of Generative AI via GRPO and Policy-as-Code](https://arxiv.org/abs/2509.07006)
Token length: 1911
Summarized using GPT-3.5-turbo
Append: [From Eigenmodes to Proofs: Integrating Graph Spectral Operators with Symbolic Interpretable Reasoning](https://arxiv.org/abs/2509.07017)
Token length: 1146
Summarized using GPT-3.5-turbo
Append: [Instruction Agent: Enhancing Agent with Expert Demonstration](https://arxiv.org/abs/2509.07098)
Token length: 1366
Summarized using GPT-3.5-turbo
Append: [Neuro-Symbolic Frameworks: Conceptual Characterization and Empirical Comparative Analysis](https://arxiv.org/abs/2509.07122)
Token length: 1046
Summarized using GPT-3.5-turbo
Append: [Measuring Uncertainty in Transformer Circuits with Effective Information Consistency](https://arxiv.org/abs/2509.07149)
Token length: 1173
Summarized using GPT-3.5-turbo
Append: [Beyond Sequential Reranking: Reranker-Guided Search Improves Reasoning Intensive Retrieval](https://arxiv.org/abs/2509.07163)
Token length: 1216
Summarized using GPT-3.5-turbo
Append: [That's So FETCH: Fashioning Ensemble Techniques for LLM Classification in Civil Legal Intake and Referral](https://arxiv.org/abs/2509.07170)
Token length: 1377
Summarized using GPT-3.5-turbo
Append: [Neurocognitive Modeling for Text Generation: Deep Learning Architecture for EEG Data](https://arxiv.org/abs/2509.07202)
Token length: 1908
Summarized using GPT-3.5-turbo
Append: [Benchmarking Information Retrieval Models on Complex Retrieval Tasks](https://arxiv.org/abs/2509.07253)
Token length: 1422
Summarized using GPT-3.5-turbo
Append: [ALICE: An Interpretable Neural Architecture for Generalization in Substitution Ciphers](https://arxiv.org/abs/2509.07282)
Append: [Language Self-Play For Data-Free Training](https://arxiv.org/abs/2509.07414)
Append: [GLEAM: Learning to Match and Explain in Cross-View Geo-Localization](https://arxiv.org/abs/2509.07450)
Append: [Astra: A Multi-Agent System for GPU Kernel Performance Optimization](https://arxiv.org/abs/2509.07506)
Append: [Competitive Audio-Language Models with Data-Efficient Single-Stage Training on Public Data](https://arxiv.org/abs/2509.07526)
Append: [Uncovering Scaling Laws for Large Language Models via Inverse Problems](https://arxiv.org/abs/2509.07909)
Append: [Visual-TableQA: Open-Domain Benchmark for Reasoning over Table Images](https://arxiv.org/abs/2509.07966)
Append: [Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search](https://arxiv.org/abs/2509.07969)
Append: [UPLex: Fine-Grained Personality Control in Large Language Models via Unsupervised Lexical Modulation](https://arxiv.org/abs/2310.16582)
Append: [JoPA:Explaining Large Language Model's Generation via Joint Prompt Attribution](https://arxiv.org/abs/2405.20404)
Append: [CTourLLM: Enhancing LLMs with Chinese Tourism Knowledge](https://arxiv.org/abs/2407.12791)
Append: [TokenSelect: Efficient Long-Context Inference and Length Extrapolation for LLMs via Dynamic Token-Level KV Cache Selection](https://arxiv.org/abs/2411.02886)
Append: [Dovetail: A CPU/GPU Heterogeneous Speculative Decoding for LLM inference](https://arxiv.org/abs/2412.18934)
Append: [Cardiverse: Harnessing LLMs for Novel Card Game Prototyping](https://arxiv.org/abs/2502.07128)
Append: [MEMIT-Merge: Addressing MEMIT's Key-Value Conflicts in Same-Subject Batch Editing for LLMs](https://arxiv.org/abs/2502.07322)
Append: [M-ABSA: A Multilingual Dataset for Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2502.11824)
Append: [Robust Adaptation of Large Multimodal Models for Retrieval Augmented Hateful Meme Detection](https://arxiv.org/abs/2502.13061)
Append: [MEBench: Benchmarking Large Language Models for Cross-Document Multi-Entity Question Answering](https://arxiv.org/abs/2502.18993)
Append: [When Large Language Models Meet Speech: A Survey on Integration Approaches](https://arxiv.org/abs/2502.19548)
Append: [Local Normalization Distortion and the Thermodynamic Formalism of Decoding Strategies for Large Language Models](https://arxiv.org/abs/2503.21929)
Append: [Register Always Matters: Analysis of LLM Pretraining Data Through the Lens of Language Variation](https://arxiv.org/abs/2504.01542)
Append: [SemCAFE: When Named Entities make the Difference Assessing Web Source Reliability through Entity-level Analytics](https://arxiv.org/abs/2504.08776)
Append: [Beyond One-Size-Fits-All: Inversion Learning for Highly Effective NLG Evaluation Prompts](https://arxiv.org/abs/2504.21117)
Append: [Llama-Nemotron: Efficient Reasoning Models](https://arxiv.org/abs/2505.00949)
Append: [OBLIVIATE: Robust and Practical Machine Unlearning for Large Language Models](https://arxiv.org/abs/2505.04416)
Append: [A Japanese Language Model and Three New Evaluation Benchmarks for Pharmaceutical NLP](https://arxiv.org/abs/2505.16661)
Append: [FinRAGBench-V: A Benchmark for Multimodal RAG with Visual Citation in the Financial Domain](https://arxiv.org/abs/2505.17471)
Append: [LogicCat: A Chain-of-Thought Text-to-SQL Benchmark for Complex Reasoning](https://arxiv.org/abs/2505.18744)
Append: [Multimodal Emotion Recognition in Conversations: A Survey of Methods, Trends, Challenges and Prospects](https://arxiv.org/abs/2505.20511)
Append: [Localizing Persona Representations in LLMs](https://arxiv.org/abs/2505.24539)
Append: [Are Economists Always More Introverted? Analyzing Consistency in Persona-Assigned LLMs](https://arxiv.org/abs/2506.02659)
Append: [Debatable Intelligence: Benchmarking LLM Judges via Debate Speech Evaluation](https://arxiv.org/abs/2506.05062)
Append: [MEMOIR: Lifelong Model Editing with Minimal Overwrite and Informed Retention for LLMs](https://arxiv.org/abs/2506.07899)
Append: [Language Models Might Not Understand You: Evaluating Theory of Mind via Story Prompting](https://arxiv.org/abs/2506.19089)
Append: [Meaning-infused grammar: Gradient Acceptability Shapes the Geometric Representations of Constructions in LLMs](https://arxiv.org/abs/2507.22286)
Append: [Enhancing Dialogue Annotation with Speaker Characteristics Leveraging a Frozen LLM](https://arxiv.org/abs/2508.04795)
Append: [Bhav-Net: Knowledge Transfer for Cross-Lingual Antonym vs Synonym Distinction via Dual-Space Graph Transformers](https://arxiv.org/abs/2508.15792)
Append: [Trust but Verify! A Survey on Verification Design for Test-time Scaling](https://arxiv.org/abs/2508.16665)
Append: [Understanding the Language Model to Solve the Symbolic Multi-Step Reasoning Problem from the Perspective of Buffer Mechanism](https://arxiv.org/abs/2405.15302)
Append: [CoMMIT: Coordinated Multimodal Instruction Tuning](https://arxiv.org/abs/2407.20454)
Append: [Understanding Museum Exhibits using Vision-Language Reasoning](https://arxiv.org/abs/2412.01370)
Append: [FilterRAG: Zero-Shot Informed Retrieval-Augmented Generation to Mitigate Hallucinations in VQA](https://arxiv.org/abs/2502.18536)
Append: [Personalized Attacks of Social Engineering in Multi-turn Conversations: LLM Agents for Simulation and Detection](https://arxiv.org/abs/2503.15552)
Append: [The Model Hears You: Audio Language Model Deployments Should Consider the Principle of Least Privilege](https://arxiv.org/abs/2503.16833)
Append: [Visuospatial Cognitive Assistant](https://arxiv.org/abs/2505.12312)
Append: [Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts](https://arxiv.org/abs/2505.12363)
Append: [MedGellan: LLM-Generated Medical Guidance to Support Physicians](https://arxiv.org/abs/2507.04431)
Append: [A Systematic Literature Review of Retrieval-Augmented Generation: Techniques, Metrics, and Challenges](https://arxiv.org/abs/2508.06401)
Append: [Benchmarking for Domain-Specific LLMs: A Case Study on Academia and Beyond](https://arxiv.org/abs/2508.07353)
Append: [Heterogeneous Self-Supervised Acoustic Pre-Training with Local Constraints](https://arxiv.org/abs/2508.19990)
append_entries: 99
Finish: 2025-09-10 04:23:06.070817
------------------------------------------------------
Started: 2025-09-10 06:23:55.013369
Existing_entries: 1099
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-10 06:23:55.281421
------------------------------------------------------
Started: 2025-09-10 08:21:10.128581
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-10 08:21:10.426557
------------------------------------------------------
Started: 2025-09-10 10:16:22.073706
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-10 10:16:22.425280
------------------------------------------------------
Started: 2025-09-10 12:32:27.869685
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-10 12:32:28.141070
------------------------------------------------------
Started: 2025-09-10 14:14:51.520290
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-10 14:14:51.837597
------------------------------------------------------
Started: 2025-09-10 16:20:05.889344
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-10 16:20:06.376864
------------------------------------------------------
Started: 2025-09-10 18:22:21.366102
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-10 18:22:21.690053
------------------------------------------------------
Started: 2025-09-10 20:16:52.993243
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-10 20:16:53.302889
------------------------------------------------------
Started: 2025-09-10 22:14:48.910065
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-10 22:14:49.184191
------------------------------------------------------
Started: 2025-09-11 01:13:47.203848
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-11 01:13:47.646242
------------------------------------------------------
Started: 2025-09-11 02:57:41.086319
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-11 02:57:41.385694
------------------------------------------------------
Started: 2025-09-11 04:22:21.821704
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 974
Summarized using GPT-3.5-turbo
Append: [Bilingual Word Level Language Identification for Omotic Languages](https://arxiv.org/abs/2509.07998)
Token length: 1598
Summarized using GPT-3.5-turbo
Append: [AntiDote: Bi-level Adversarial Training for Tamper-Resistant LLMs](https://arxiv.org/abs/2509.08000)
Token length: 1452
Summarized using GPT-3.5-turbo
Append: [MVPBench: A Benchmark and Fine-Tuning Framework for Aligning Large Language Models with Diverse Human Values](https://arxiv.org/abs/2509.08022)
Token length: 1197
Summarized using GPT-3.5-turbo
Append: [NOWJ@COLIEE 2025: A Multi-stage Framework Integrating Embedding Models and Large Language Models for Legal Retrieval and Entailment](https://arxiv.org/abs/2509.08025)
Token length: 1395
Summarized using GPT-3.5-turbo
Append: [SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery](https://arxiv.org/abs/2509.08032)
Token length: 1272
Summarized using GPT-3.5-turbo
Append: [No for Some, Yes for Others: Persona Prompts and Other Sources of False Refusal in Language Models](https://arxiv.org/abs/2509.08075)
Token length: 1552
Summarized using GPT-3.5-turbo
Append: [Culturally transmitted color categories in LLMs reflect a learning bias toward efficient compression](https://arxiv.org/abs/2509.08093)
Token length: 793
Summarized using GPT-3.5-turbo
Append: [MERLIN: Multi-Stage Curriculum Alignment for Multilingual Encoder and LLM Fusion](https://arxiv.org/abs/2509.08105)
Token length: 1478
Summarized using GPT-3.5-turbo
Append: [Bias after Prompting: Persistent Discrimination in Large Language Models](https://arxiv.org/abs/2509.08146)
Token length: 775
Summarized using GPT-3.5-turbo
Append: [Verbalized Algorithms](https://arxiv.org/abs/2509.08150)
Token length: 1464
Summarized using GPT-3.5-turbo
Append: [Balancing Quality and Variation: Spam Filtering Distorts Data Label Distributions](https://arxiv.org/abs/2509.08217)
Token length: 1794
Summarized using GPT-3.5-turbo
Append: [Towards Knowledge-Aware Document Systems: Modeling Semantic Coverage Relations via Answerability Detection](https://arxiv.org/abs/2509.08304)
Token length: 456
Summarized using GPT-3.5-turbo
Append: [Toward Subtrait-Level Model Explainability in Automated Writing Evaluation](https://arxiv.org/abs/2509.08345)
Token length: 475
Summarized using GPT-3.5-turbo
Append: [Automatic Detection of Inauthentic Templated Responses in English Language Assessments](https://arxiv.org/abs/2509.08355)
Token length: 1152
Summarized using GPT-3.5-turbo
Append: [<think> So let's replace this phrase with insult... </think> Lessons learned from generation of toxic texts with LLMs](https://arxiv.org/abs/2509.08358)
Token length: 1245
Summarized using GPT-3.5-turbo
Append: [Low-Resource Fine-Tuning for Multi-Task Structured Information Extraction with a Billion-Parameter Instruction-Tuned Model](https://arxiv.org/abs/2509.08381)
Token length: 1332
Summarized using GPT-3.5-turbo
Append: [CommonVoice-SpeechRE and RPG-MoGe: Advancing Speech Relation Extraction with a New Dataset and Multi-Order Generative Framework](https://arxiv.org/abs/2509.08438)
Token length: 1282
Summarized using GPT-3.5-turbo
Append: [Adversarial Attacks Against Automated Fact-Checking: A Survey](https://arxiv.org/abs/2509.08463)
Token length: 729
Summarized using GPT-3.5-turbo
Append: [Acquiescence Bias in Large Language Models](https://arxiv.org/abs/2509.08480)
Token length: 1318
Summarized using GPT-3.5-turbo
Append: [Simulating Identity, Propagating Bias: Abstraction and Stereotypes in LLM-Generated Text](https://arxiv.org/abs/2509.08484)
Token length: 1495
Summarized using GPT-3.5-turbo
Append: [Too Helpful, Too Harmless, Too Honest or Just Right?](https://arxiv.org/abs/2509.08486)
Token length: 1378
Summarized using GPT-3.5-turbo
Append: [CM-Align: Consistency-based Multilingual Alignment for Large Language Models](https://arxiv.org/abs/2509.08541)
Token length: 1607
Summarized using GPT-3.5-turbo
Append: [LLM Ensemble for RAG: Role of Context Length in Zero-Shot Question Answering for BioASQ Challenge](https://arxiv.org/abs/2509.08596)
Token length: 1837
Summarized using GPT-3.5-turbo
Append: [Memorization in Large Language Models in Medicine: Prevalence, Characteristics, and Implications](https://arxiv.org/abs/2509.08604)
Token length: 1493
Summarized using GPT-3.5-turbo
Append: [OTESGN:Optimal Transport Enhanced Syntactic-Semantic Graph Networks for Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2509.08612)
Token length: 1234
Summarized using GPT-3.5-turbo
Append: [X-Teaming Evolutionary M2S: Automated Discovery of Multi-turn to Single-turn Jailbreak Templates](https://arxiv.org/abs/2509.08729)
Token length: 1345
Summarized using GPT-3.5-turbo
Append: [Streaming Sequence-to-Sequence Learning with Delayed Streams Modeling](https://arxiv.org/abs/2509.08753)
Token length: 1014
Summarized using GPT-3.5-turbo
Append: [Do All Autoregressive Transformers Remember Facts the Same Way? A Cross-Architecture Analysis of Recall Mechanisms](https://arxiv.org/abs/2509.08778)
Token length: 1414
Summarized using GPT-3.5-turbo
Append: [Evaluating LLMs Without Oracle Feedback: Agentic Annotation Evaluation Through Unsupervised Consistency Signals](https://arxiv.org/abs/2509.08809)
Token length: 1326
Summarized using GPT-3.5-turbo
Append: [MoVoC: Morphology-Aware Subword Construction for Geez Script Languages](https://arxiv.org/abs/2509.08812)
Token length: 1228
Summarized using GPT-3.5-turbo
Append: [Building High-Quality Datasets for Portuguese LLMs: From Common Crawl Snapshots to Industrial-Grade Corpora](https://arxiv.org/abs/2509.08824)
Token length: 1927
Summarized using GPT-3.5-turbo
Append: [Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs for Text Annotation](https://arxiv.org/abs/2509.08825)
Token length: 1277
Summarized using GPT-3.5-turbo
Append: [A Survey of Reinforcement Learning for Large Reasoning Models](https://arxiv.org/abs/2509.08827)
Token length: 1203
Summarized using GPT-3.5-turbo
Append: [Measuring and mitigating overreliance is necessary for building human-compatible AI](https://arxiv.org/abs/2509.08010)
Token length: 1269
Summarized using GPT-3.5-turbo
Append: [XML Prompting as Grammar-Constrained Interaction: Fixed-Point Semantics, Convergence Guarantees, and Human-AI Protocols](https://arxiv.org/abs/2509.08182)
Token length: 1207
Summarized using GPT-3.5-turbo
Append: [EvolKV: Evolutionary KV Cache Compression for LLM Inference](https://arxiv.org/abs/2509.08315)
Token length: 1502
Summarized using GPT-3.5-turbo
Append: [HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI Assistants](https://arxiv.org/abs/2509.08494)
Token length: 1342
Summarized using GPT-3.5-turbo
Append: [Generative Data Refinement: Just Ask for Better Data](https://arxiv.org/abs/2509.08653)
Token length: 1808
Summarized using GPT-3.5-turbo
Append: [AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2509.08755)
Token length: 1395
Summarized using GPT-3.5-turbo
Append: [Calibrating MLLM-as-a-judge via Multimodal Bayesian Prompt Ensembles](https://arxiv.org/abs/2509.08777)
Token length: 1475
Summarized using GPT-3.5-turbo
Append: [Scaling Truth: The Confidence Paradox in AI Fact-Checking](https://arxiv.org/abs/2509.08803)
Token length: 1621
Summarized using GPT-3.5-turbo
Append: [Merge-of-Thought Distillation](https://arxiv.org/abs/2509.08814)
Token length: 698
Summarized using GPT-3.5-turbo
Append: [Baba Is AI: Break the Rules to Beat the Benchmark](https://arxiv.org/abs/2407.13729)
Token length: 1477
Summarized using GPT-3.5-turbo
Append: [Localizing Factual Inconsistencies in Attributable Text Generation](https://arxiv.org/abs/2410.07473)
Token length: 1821
Summarized using GPT-3.5-turbo
Append: [TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks](https://arxiv.org/abs/2412.14161)
Token length: 1484
Summarized using GPT-3.5-turbo
Append: [MedS$^3$: Towards Medical Slow Thinking with Self-Evolved Soft Dual-sided Process Supervision](https://arxiv.org/abs/2501.12051)
Token length: 1546
Summarized using GPT-3.5-turbo
Append: [CoAT: Chain-of-Associated-Thoughts Framework for Enhancing Large Language Models Reasoning](https://arxiv.org/abs/2502.02390)
Token length: 1332
Summarized using GPT-3.5-turbo
Append: [IssueBench: Millions of Realistic Prompts for Measuring Issue Bias in LLM Writing Assistance](https://arxiv.org/abs/2502.08395)
Token length: 780
Summarized using GPT-3.5-turbo
Append: [Beyond Seen Data: Improving KBQA Generalization Through Schema-Guided Logical Form Generation](https://arxiv.org/abs/2502.12737)
Token length: 1528
Summarized using GPT-3.5-turbo
Append: [Pay Attention to Real World Perturbations! Natural Robustness Evaluation in Machine Reading Comprehension](https://arxiv.org/abs/2502.16523)
Append: [REGen: A Reliable Evaluation Framework for Generative Event Argument Extraction](https://arxiv.org/abs/2502.16838)
Append: [MPO: Boosting LLM Agents with Meta Plan Optimization](https://arxiv.org/abs/2503.02682)
Append: [DomainCQA: Crafting Knowledge-Intensive QA from Domain-Specific Charts](https://arxiv.org/abs/2503.19498)
Append: [Scaling Video-Language Models to 10K Frames via Hierarchical Differential Distillation](https://arxiv.org/abs/2504.02438)
Append: [CoT-RAG: Integrating Chain of Thought and Retrieval-Augmented Generation to Enhance Reasoning in Large Language Models](https://arxiv.org/abs/2504.13534)
Append: [Prior Prompt Engineering for Reinforcement Fine-Tuning](https://arxiv.org/abs/2505.14157)
Append: [Your Language Model Can Secretly Write Like Humans: Contrastive Paraphrase Attacks on LLM-Generated Text Detectors](https://arxiv.org/abs/2505.15337)
Append: [How Far Are We from Optimal Reasoning Efficiency?](https://arxiv.org/abs/2506.07104)
Append: [VIDEE: Visual and Interactive Decomposition, Execution, and Evaluation of Text Analytics with Intelligent Agents](https://arxiv.org/abs/2506.21582)
Append: [HIRAG: Hierarchical-Thought Instruction-Tuning Retrieval-Augmented Generation](https://arxiv.org/abs/2507.05714)
Append: [Arce: Augmented Roberta with Contextualized Elucidations for Ner in Automated Rule Checking](https://arxiv.org/abs/2508.07286)
Append: [Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL](https://arxiv.org/abs/2508.07976)
Append: [All for law and law for all: Adaptive RAG Pipeline for Legal Research](https://arxiv.org/abs/2508.13107)
Append: [Subjective Behaviors and Preferences in LLM: Language of Browsing](https://arxiv.org/abs/2508.15474)
Append: [Mind the Value-Action Gap: Do LLMs Act in Alignment with Their Values?](https://arxiv.org/abs/2501.15463)
Append: [Self-Correcting Decoding with Generative Feedback for Mitigating Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2502.06130)
Append: [Recursive Training Loops in LLMs: How training data properties modulate distribution shift in generated data?](https://arxiv.org/abs/2504.03814)
Append: [Meta-Semantics Augmented Few-Shot Relational Learning](https://arxiv.org/abs/2505.05684)
Append: [TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached Responses](https://arxiv.org/abs/2507.23674)
append_entries: 69
Finish: 2025-09-11 04:24:01.625980
------------------------------------------------------
Started: 2025-09-11 06:24:13.503771
Existing_entries: 1069
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-11 06:24:13.767688
------------------------------------------------------
Started: 2025-09-11 08:21:02.125518
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-11 08:21:02.337721
------------------------------------------------------
Started: 2025-09-11 10:16:21.918680
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-11 10:16:22.160593
------------------------------------------------------
Started: 2025-09-11 12:31:53.383799
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-11 12:31:53.638232
------------------------------------------------------
Started: 2025-09-11 14:15:42.953112
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-11 14:15:43.169422
------------------------------------------------------
Started: 2025-09-11 16:19:30.903511
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-11 16:19:31.171035
------------------------------------------------------
Started: 2025-09-11 18:19:19.670089
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-11 18:19:19.880969
------------------------------------------------------
Started: 2025-09-11 20:14:11.892360
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-11 20:14:12.110784
------------------------------------------------------
Started: 2025-09-11 22:14:56.897080
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-11 22:14:57.139970
------------------------------------------------------
Started: 2025-09-12 01:11:01.547654
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-12 01:11:01.786277
------------------------------------------------------
Started: 2025-09-12 02:52:30.539865
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-12 02:52:30.782911
------------------------------------------------------
Started: 2025-09-12 04:22:59.995200
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 634
Summarized using GPT-3.5-turbo
Append: [Noise or Nuance: An Investigation Into Useful Information and Filtering For LLM Driven AKBC](https://arxiv.org/abs/2509.08903)
Token length: 1295
Summarized using GPT-3.5-turbo
Append: [Automated Evidence Extraction and Scoring for Corporate Climate Policy Engagement: A Multilingual RAG Approach](https://arxiv.org/abs/2509.08907)
Token length: 1463
Summarized using GPT-3.5-turbo
Append: [Documents Are People and Words Are Items: A Psychometric Approach to Textual Data with Contextual Embeddings](https://arxiv.org/abs/2509.08920)
Token length: 1265
Summarized using GPT-3.5-turbo
Append: [BRoverbs -- Measuring how much LLMs understand Portuguese proverbs](https://arxiv.org/abs/2509.08960)
Token length: 1086
Summarized using GPT-3.5-turbo
Append: [Can Vision-Language Models Solve Visual Math Equations?](https://arxiv.org/abs/2509.09013)
Token length: 1718
Summarized using GPT-3.5-turbo
Append: [Stated Preference for Interaction and Continued Engagement (SPICE): Evaluating an LLM's Willingness to Re-engage in Conversation](https://arxiv.org/abs/2509.09043)
Token length: 1095
Summarized using GPT-3.5-turbo
Append: [Improving LLM Safety and Helpfulness using SFT and DPO: A Study on OPT-350M](https://arxiv.org/abs/2509.09055)
Token length: 1209
Summarized using GPT-3.5-turbo
Append: [MR-UIE: Multi-Perspective Reasoning with Reinforcement Learning for Universal Information Extraction](https://arxiv.org/abs/2509.09082)
Token length: 924
Summarized using GPT-3.5-turbo
Append: [TigerCoder: A Novel Suite of LLMs for Code Generation in Bangla](https://arxiv.org/abs/2509.09101)
Token length: 1705
Summarized using GPT-3.5-turbo
Append: [Compass-v3: Scaling Domain-Specific LLMs for Multilingual E-Commerce in Southeast Asia](https://arxiv.org/abs/2509.09121)
Token length: 1266
Summarized using GPT-3.5-turbo
Append: [Automated Classification of Tutors' Dialogue Acts Using Generative AI: A Case Study Using the CIMA Corpus](https://arxiv.org/abs/2509.09125)
Token length: 937
Summarized using GPT-3.5-turbo
Append: [ViRanker: A BGE-M3 & Blockwise Parallel Transformer Cross-Encoder for Vietnamese Reranking](https://arxiv.org/abs/2509.09131)
Token length: 1922
Summarized using GPT-3.5-turbo
Append: [LITcoder: A General-Purpose Library for Building and Comparing Encoding Models](https://arxiv.org/abs/2509.09152)
Token length: 1168
Summarized using GPT-3.5-turbo
Append: [Target-oriented Multimodal Sentiment Classification with Counterfactual-enhanced Debiasing](https://arxiv.org/abs/2509.09160)
Token length: 947
Summarized using GPT-3.5-turbo
Append: [EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs](https://arxiv.org/abs/2509.09174)
Token length: 963
Summarized using GPT-3.5-turbo
Append: [Efficient Trie-based Biasing using K-step Prediction for Rare Word Recognition](https://arxiv.org/abs/2509.09196)
Token length: 1042
Summarized using GPT-3.5-turbo
Append: [Improving Synthetic Data Training for Contextual Biasing Models with a Keyword-Aware Cost Function](https://arxiv.org/abs/2509.09197)
Token length: 1504
Summarized using GPT-3.5-turbo
Append: [GmSLM : Generative Marmoset Spoken Language Modeling](https://arxiv.org/abs/2509.09198)
Token length: 1351
Summarized using GPT-3.5-turbo
Append: [CCF: A Context Compression Framework for Efficient Long-Sequence Language Modeling](https://arxiv.org/abs/2509.09199)
Token length: 1028
Summarized using GPT-3.5-turbo
Append: [Reading Between the Lines: Classifying Resume Seniority with Large Language Models](https://arxiv.org/abs/2509.09229)
Token length: 1085
Summarized using GPT-3.5-turbo
Append: [Agentic LLMs for Question Answering over Tabular Data](https://arxiv.org/abs/2509.09234)
Token length: 1825
Summarized using GPT-3.5-turbo
Append: [From scratch to silver: Creating trustworthy training data for patent-SDG classification using Large Language Models](https://arxiv.org/abs/2509.09303)
Token length: 1916
Summarized using GPT-3.5-turbo
Append: [MetaRAG: Metamorphic Testing for Hallucination Detection in RAG Systems](https://arxiv.org/abs/2509.09360)
Token length: 703
Summarized using GPT-3.5-turbo
Append: [Modelling Analogies and Analogical Reasoning: Connecting Cognitive Science Theory and NLP Research](https://arxiv.org/abs/2509.09381)
Token length: 631
Summarized using GPT-3.5-turbo
Append: [Hierarchical Bracketing Encodings Work for Dependency Graphs](https://arxiv.org/abs/2509.09388)
Token length: 1569
Summarized using GPT-3.5-turbo
Append: [GrACE: A Generative Approach to Better Confidence Elicitation in Large Language Models](https://arxiv.org/abs/2509.09438)
Token length: 1058
Summarized using GPT-3.5-turbo
Append: [Mitigating Language Barriers in Education: Developing Multilingual Digital Learning Materials with Machine Translation](https://arxiv.org/abs/2509.09473)
Token length: 1597
Summarized using GPT-3.5-turbo
Append: [Towards Explainable Job Title Matching: Leveraging Semantic Textual Relatedness and Knowledge Graphs](https://arxiv.org/abs/2509.09522)
Token length: 828
Summarized using GPT-3.5-turbo
Append: [DeMeVa at LeWiDi-2025: Modeling Perspectives with In-Context Learning and Label Distribution Learning](https://arxiv.org/abs/2509.09524)
Token length: 1121
Summarized using GPT-3.5-turbo
Append: [Prompting the Market? A Large-Scale Meta-Analysis of GenAI in Finance NLP (2022-2025)](https://arxiv.org/abs/2509.09544)
Token length: 1139
Summarized using GPT-3.5-turbo
Append: [Personality-Enhanced Social Recommendations in SAMI: Exploring the Role of Personality Detection in Matchmaking](https://arxiv.org/abs/2509.09583)
Token length: 1426
Summarized using GPT-3.5-turbo
Append: [Fluent but Unfeeling: The Emotional Blind Spots of Language Models](https://arxiv.org/abs/2509.09593)
Token length: 1113
Summarized using GPT-3.5-turbo
Append: [LAVA: Language Model Assisted Verbal Autopsy for Cause-of-Death Determination](https://arxiv.org/abs/2509.09602)
Token length: 1247
Summarized using GPT-3.5-turbo
Append: [Bridging the Capability Gap: Joint Alignment Tuning for Harmonizing LLM-based Multi-Agent Systems](https://arxiv.org/abs/2509.09629)
Token length: 1545
Summarized using GPT-3.5-turbo
Append: [All for One: LLMs Solve Mental Math at the Last Token With Information Transferred From Other Tokens](https://arxiv.org/abs/2509.09650)
Token length: 914
Summarized using GPT-3.5-turbo
Append: [Steering MoE LLMs via Expert (De)Activation](https://arxiv.org/abs/2509.09660)
Token length: 1272
Summarized using GPT-3.5-turbo
Append: [CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning in Large Language Models](https://arxiv.org/abs/2509.09675)
Token length: 1275
Summarized using GPT-3.5-turbo
Append: [Automated Unity Game Template Generation from GDDs via NLP and Multi-Modal LLMs](https://arxiv.org/abs/2509.08847)
Token length: 1554
Summarized using GPT-3.5-turbo
Append: [A vibe coding learning design to enhance EFL students' talking to, through, and about AI](https://arxiv.org/abs/2509.08854)
Token length: 1346
Summarized using GPT-3.5-turbo
Append: [Recurrence Meets Transformers for Universal Multimodal Retrieval](https://arxiv.org/abs/2509.08897)
Token length: 1687
Summarized using GPT-3.5-turbo
Append: [Generative Engine Optimization: How to Dominate AI Search](https://arxiv.org/abs/2509.08919)
Token length: 1079
Summarized using GPT-3.5-turbo
Append: [Open-sci-ref-0.01: open and reproducible reference baselines for language model and dataset comparison](https://arxiv.org/abs/2509.09009)
Token length: 1349
Summarized using GPT-3.5-turbo
Append: [COCO-Urdu: A Large-Scale Urdu Image-Caption Dataset with Multimodal Quality Estimation](https://arxiv.org/abs/2509.09014)
Token length: 1050
Summarized using GPT-3.5-turbo
Append: [Bona fide Cross Testing Reveals Weak Spot in Audio Deepfake Detection Systems](https://arxiv.org/abs/2509.09204)
Token length: 1524
Summarized using GPT-3.5-turbo
Append: [Identifying Key Features for Establishing Sustainable Agro-Tourism Centre: A Data Driven Approach](https://arxiv.org/abs/2509.09214)
Token length: 1512
Summarized using GPT-3.5-turbo
Append: [Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents](https://arxiv.org/abs/2509.09265)
Token length: 1325
Summarized using GPT-3.5-turbo
Append: [Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for Multistep Reasoning](https://arxiv.org/abs/2509.09284)
Token length: 1466
Summarized using GPT-3.5-turbo
Append: [Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on Materials Characterization](https://arxiv.org/abs/2509.09307)
Token length: 1838
Summarized using GPT-3.5-turbo
Append: [OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning](https://arxiv.org/abs/2509.09332)
Token length: 1305
Summarized using GPT-3.5-turbo
Append: [LLMs Don't Know Their Own Decision Boundaries: The Unreliability of Self-Generated Counterfactual Explanations](https://arxiv.org/abs/2509.09396)
Append: [DiFlow-TTS: Discrete Flow Matching with Factorized Speech Tokens for Low-Latency Zero-Shot Text-To-Speech](https://arxiv.org/abs/2509.09631)
Append: [Retrieval-Augmented Generation for Reliable Interpretation of Radio Regulations](https://arxiv.org/abs/2509.09651)
Append: [SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning](https://arxiv.org/abs/2509.09674)
Append: [ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable Orthogonal Butterfly Transforms](https://arxiv.org/abs/2509.09679)
Append: [FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark](https://arxiv.org/abs/2509.09680)
Append: [ASTPrompter: Preference-Aligned Automated Language Model Red-Teaming to Generate Low-Perplexity Unsafe Prompts](https://arxiv.org/abs/2407.09447)
Append: [RED: Unleashing Token-Level Rewards from Holistic Feedback via Reward Redistribution](https://arxiv.org/abs/2411.08302)
Append: [MERaLiON-SpeechEncoder: Towards a Speech Foundation Model for Singapore and Beyond](https://arxiv.org/abs/2412.11538)
Append: [Thinking with Many Minds: Using Large Language Models for Multi-Perspective Problem-Solving](https://arxiv.org/abs/2501.02348)
Append: [CondAmbigQA: A Benchmark and Dataset for Conditional Ambiguous Question Answering](https://arxiv.org/abs/2502.01523)
Append: [SimMark: A Robust Sentence-Level Similarity-Based Watermarking Algorithm for Large Language Models](https://arxiv.org/abs/2502.02787)
Append: [Are Generative Models Underconfident? Better Quality Estimation with Boosted Model Probability](https://arxiv.org/abs/2502.11115)
Append: [Culturally-Nuanced Story Generation for Reasoning in Low-Resource Languages: The Case of Javanese and Sundanese](https://arxiv.org/abs/2502.12932)
Append: [Uncertainty Quantification in Retrieval Augmented Question Answering](https://arxiv.org/abs/2502.18108)
Append: [CritiQ: Mining Data Quality Criteria from Human Preferences](https://arxiv.org/abs/2502.19279)
Append: [MIND: Towards Immersive Psychological Healing with Multi-agent Inner Dialogue](https://arxiv.org/abs/2502.19860)
Append: [SWI: Speaking with Intent in Large Language Models](https://arxiv.org/abs/2503.21544)
Append: [Entropy-Gated Branching for Efficient Test-Time Reasoning](https://arxiv.org/abs/2503.21961)
Append: [Contextualize-then-Aggregate: Circuits for In-Context Learning in Gemma-2 2B](https://arxiv.org/abs/2504.00132)
Append: [An Ontology-Driven Graph RAG for Legal Norms: A Structural, Temporal, and Deterministic Approach](https://arxiv.org/abs/2505.00039)
Append: [AdaptMI: Adaptive Skill-based In-context Math Instruction for Small Language Models](https://arxiv.org/abs/2505.00147)
Append: [A Novel Data Augmentation Approach for Automatic Speaking Assessment on Opinion Expressions](https://arxiv.org/abs/2506.04077)
Append: [The NTNU System at the S&I Challenge 2025 SLA Open Track](https://arxiv.org/abs/2506.05121)
Append: [Task Matters: Knowledge Requirements Shape LLM Responses to Context-Memory Conflict](https://arxiv.org/abs/2506.06485)
Append: [Persistent Homology of Topic Networks for the Prediction of Reader Curiosity](https://arxiv.org/abs/2506.11095)
Append: [Spotlight Attention: Towards Efficient LLM Generation via Non-linear Hashing-based KV Cache Retrieval](https://arxiv.org/abs/2508.19740)
Append: [T2R-bench: A Benchmark for Generating Article-Level Reports from Real World Industrial Tables](https://arxiv.org/abs/2508.19813)
Append: [ReceiptSense: Beyond Traditional OCR -- A Dataset for Receipt Understanding](https://arxiv.org/abs/2406.04493)
Append: [Enhancing Few-Shot Transfer Learning with Optimized Multi-Task Prompt Tuning through Modular Prompt Composition](https://arxiv.org/abs/2408.13227)
Append: [Scalable Evaluation of Online Facilitation Strategies via Synthetic Simulation of Discussions](https://arxiv.org/abs/2503.16505)
Append: [VeriSafe Agent: Safeguarding Mobile GUI Agent via Logic-based Action Verification](https://arxiv.org/abs/2503.18492)
Append: [Optimizing Length Compression in Large Reasoning Models](https://arxiv.org/abs/2506.14755)
Append: [Can Large Language Models Understand As Well As Apply Patent Regulations to Pass a Hands-On Patent Attorney Test?](https://arxiv.org/abs/2507.10576)
Append: [LoRA-PAR: A Flexible Dual-System LoRA Partitioning Approach to Efficient LLM Fine-Tuning](https://arxiv.org/abs/2507.20999)
Append: [Improving Alignment in LVLMs with Debiased Self-Judgment](https://arxiv.org/abs/2508.20655)
append_entries: 85
Finish: 2025-09-12 04:24:50.244283
------------------------------------------------------
Started: 2025-09-12 06:23:51.149397
Existing_entries: 1085
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-12 06:23:51.413082
------------------------------------------------------
Started: 2025-09-12 08:20:19.585042
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-12 08:20:19.804611
------------------------------------------------------
Started: 2025-09-12 10:17:26.516560
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-12 10:17:26.874781
------------------------------------------------------
Started: 2025-09-12 12:31:46.250256
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-12 12:31:46.473650
------------------------------------------------------
Started: 2025-09-12 14:14:48.308212
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-12 14:14:48.566894
------------------------------------------------------
Started: 2025-09-12 16:16:01.508182
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-12 16:16:01.756431
------------------------------------------------------
Started: 2025-09-12 18:18:31.807115
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-12 18:18:32.036586
------------------------------------------------------
Started: 2025-09-12 20:17:12.196286
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-12 20:17:12.453114
------------------------------------------------------
Started: 2025-09-12 22:14:20.917128
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-12 22:14:21.281107
------------------------------------------------------
Started: 2025-09-13 01:07:55.410450
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-13 01:07:55.654468
------------------------------------------------------
Started: 2025-09-13 02:43:08.900861
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-13 02:43:09.128847
------------------------------------------------------
Started: 2025-09-13 04:17:36.515367
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-13 04:17:36.589775
------------------------------------------------------
Started: 2025-09-13 06:20:46.490037
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-13 06:20:46.555247
------------------------------------------------------
Started: 2025-09-13 08:18:13.664333
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-13 08:18:13.721195
------------------------------------------------------
Started: 2025-09-13 10:14:15.158922
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-13 10:14:15.254268
------------------------------------------------------
Started: 2025-09-13 12:28:38.891816
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-13 12:28:38.954911
------------------------------------------------------
Started: 2025-09-13 14:12:06.292824
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-13 14:12:06.354345
------------------------------------------------------
Started: 2025-09-13 16:16:14.835996
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-13 16:16:14.915396
------------------------------------------------------
Started: 2025-09-13 18:18:55.361082
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-13 18:18:55.419860
------------------------------------------------------
Started: 2025-09-13 20:14:50.058681
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-13 20:14:50.121213
------------------------------------------------------
Started: 2025-09-13 22:13:07.175312
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-13 22:13:07.297705
------------------------------------------------------
Started: 2025-09-14 01:17:45.036698
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-14 01:17:45.142322
------------------------------------------------------
Started: 2025-09-14 02:59:52.325006
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-14 02:59:52.386422
------------------------------------------------------
Started: 2025-09-14 04:18:11.210438
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-14 04:18:11.329448
------------------------------------------------------
Started: 2025-09-14 06:21:13.748803
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-14 06:21:13.819814
------------------------------------------------------
Started: 2025-09-14 08:18:22.520911
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-14 08:18:22.629997
------------------------------------------------------
Started: 2025-09-14 10:14:02.626195
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-14 10:14:02.684614
------------------------------------------------------
Started: 2025-09-14 12:28:15.292268
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-14 12:28:15.353412
------------------------------------------------------
Started: 2025-09-14 14:12:46.309512
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-14 14:12:46.368823
------------------------------------------------------
Started: 2025-09-14 16:16:11.966460
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-14 16:16:12.033794
------------------------------------------------------
Started: 2025-09-14 18:19:28.963690
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-14 18:19:29.049255
------------------------------------------------------
Started: 2025-09-14 20:15:09.205731
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-14 20:15:09.265962
------------------------------------------------------
Started: 2025-09-14 22:13:31.150954
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-14 22:13:31.235010
------------------------------------------------------
Started: 2025-09-15 01:17:56.812515
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-15 01:17:56.940193
------------------------------------------------------
Started: 2025-09-15 03:05:08.305445
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-15 03:05:08.384182
------------------------------------------------------
Started: 2025-09-15 04:23:11.527218
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1695
Summarized using GPT-3.5-turbo
Append: [Structured Information Matters: Explainable ICD Coding with Patient-Level Knowledge Graphs](https://arxiv.org/abs/2509.09699)
Token length: 1099
Summarized using GPT-3.5-turbo
Append: [Cross-Layer Attention Probing for Fine-Grained Hallucination Detection](https://arxiv.org/abs/2509.09700)
Token length: 1046
Summarized using GPT-3.5-turbo
Append: [Optimal Multi-Task Learning at Regularization Horizon for Speech Translation Task](https://arxiv.org/abs/2509.09701)
Token length: 1189
Summarized using GPT-3.5-turbo
Append: [Creativity Benchmark: A benchmark for marketing creativity for LLM models](https://arxiv.org/abs/2509.09702)
Token length: 1377
Summarized using GPT-3.5-turbo
Append: [CTCC: A Robust and Stealthy Fingerprinting Framework for Large Language Models via Cross-Turn Contextual Correlation Backdoor](https://arxiv.org/abs/2509.09703)
Token length: 1114
Summarized using GPT-3.5-turbo
Append: [Temporal Preferences in Language Models for Long-Horizon Assistance](https://arxiv.org/abs/2509.09704)
Token length: 1062
Summarized using GPT-3.5-turbo
Append: [The Non-Determinism of Small LLMs: Evidence of Low Answer Consistency in Repetition Trials of Standard Multiple-Choice Benchmarks](https://arxiv.org/abs/2509.09705)
Token length: 1295
Summarized using GPT-3.5-turbo
Append: [Beyond I'm Sorry, I Can't: Dissecting Large Language Model Refusal](https://arxiv.org/abs/2509.09708)
Token length: 1018
Summarized using GPT-3.5-turbo
Append: [Assisting Research Proposal Writing with Large Language Models: Evaluation and Refinement](https://arxiv.org/abs/2509.09709)
Token length: 1588
Summarized using GPT-3.5-turbo
Append: [Generating Individual Travel Diaries Using Large Language Models Informed by Census and Land-Use Data](https://arxiv.org/abs/2509.09710)
Token length: 1565
Summarized using GPT-3.5-turbo
Append: [Psychiatry-Bench: A Multi-Task Benchmark for LLMs in Psychiatry](https://arxiv.org/abs/2509.09711)
Token length: 1910
Summarized using GPT-3.5-turbo
Append: [The Thinking Therapist: Training Large Language Models to Deliver Acceptance and Commitment Therapy using Supervised Fine-Tuning and Odds Ratio Policy Optimization](https://arxiv.org/abs/2509.09712)
Token length: 1498
Summarized using GPT-3.5-turbo
Append: [HANRAG: Heuristic Accurate Noise-resistant Retrieval-Augmented Generation for Multi-hop Question Answering](https://arxiv.org/abs/2509.09713)
Token length: 1603
Summarized using GPT-3.5-turbo
Append: [How Small Transformation Expose the Weakness of Semantic Similarity Measures](https://arxiv.org/abs/2509.09714)
Token length: 1438
Summarized using GPT-3.5-turbo
Append: [Investigating Symbolic Triggers of Hallucination in Gemma Models Across HaluEval and TruthfulQA](https://arxiv.org/abs/2509.09715)
Token length: 1649
Summarized using GPT-3.5-turbo
Append: [ALIGNS: Unlocking nomological networks in psychological measurement through a large language model](https://arxiv.org/abs/2509.09723)
Token length: 1143
Summarized using GPT-3.5-turbo
Append: [DiTTO-LLM: Framework for Discovering Topic-based Technology Opportunities via Large Language Model](https://arxiv.org/abs/2509.09724)
Token length: 1386
Summarized using GPT-3.5-turbo
Append: [BIBERT-Pipe on Biomedical Nested Named Entity Linking at BioASQ 2025](https://arxiv.org/abs/2509.09725)
Token length: 730
Summarized using GPT-3.5-turbo
Append: [Natural Language Translation of Formal Proofs through Informalization of Proof Steps and Recursive Summarization along Proof Structure](https://arxiv.org/abs/2509.09726)
Token length: 1444
Summarized using GPT-3.5-turbo
Append: [A Role-Aware Multi-Agent Framework for Financial Education Question Answering with LLMs](https://arxiv.org/abs/2509.09727)
Token length: 1100
Summarized using GPT-3.5-turbo
Append: [A meta-analysis on the performance of machine-learning based language models for sentiment analysis](https://arxiv.org/abs/2509.09728)
Token length: 1209
Summarized using GPT-3.5-turbo
Append: [MultimodalHugs: Enabling Sign Language Processing in Hugging Face](https://arxiv.org/abs/2509.09729)
Token length: 1017
Summarized using GPT-3.5-turbo
Append: [Benchmarking Vision-Language Models on Chinese Ancient Documents: From OCR to Knowledge Reasoning](https://arxiv.org/abs/2509.09731)
Token length: 1561
Summarized using GPT-3.5-turbo
Append: [MCP-AgentBench: Evaluating Real-World Language Agent Performance with MCP-Mediated Tools](https://arxiv.org/abs/2509.09734)
Token length: 1959
Summarized using GPT-3.5-turbo
Append: [Discrimination by LLMs: Cross-lingual Bias Assessment and Mitigation in Decision-Making and Summarisation](https://arxiv.org/abs/2509.09735)
Token length: 1756
Summarized using GPT-3.5-turbo
Append: [HEFT: A Coarse-to-Fine Hierarchy for Enhancing the Efficiency and Accuracy of Language Model Reasoning](https://arxiv.org/abs/2509.09801)
Token length: 1758
Summarized using GPT-3.5-turbo
Append: [Pragmatic Frames Evoked by Gestures: A FrameNet Brasil Approach to Multimodality in Turn Organization](https://arxiv.org/abs/2509.09804)
Token length: 1002
Summarized using GPT-3.5-turbo
Append: [Topic-Guided Reinforcement Learning with LLMs for Enhancing Multi-Document Summarization](https://arxiv.org/abs/2509.09852)
Token length: 1662
Summarized using GPT-3.5-turbo
Append: [Emulating Public Opinion: A Proof-of-Concept of AI-Generated Synthetic Survey Responses for the Chilean Case](https://arxiv.org/abs/2509.09871)
Token length: 815
Summarized using GPT-3.5-turbo
Append: [Large Language Models Meet Legal Artificial Intelligence: A Survey](https://arxiv.org/abs/2509.09969)
Token length: 924
Summarized using GPT-3.5-turbo
Append: [CMHG: A Dataset and Benchmark for Headline Generation of Minority Languages in China](https://arxiv.org/abs/2509.09990)
Token length: 1189
Summarized using GPT-3.5-turbo
Append: [Unsupervised Hallucination Detection by Inspecting Reasoning Processes](https://arxiv.org/abs/2509.10004)
Token length: 1734
Summarized using GPT-3.5-turbo
Append: [Multi-Intent Recognition in Dialogue Understanding: A Comparison Between Smaller Open-Source LLMs](https://arxiv.org/abs/2509.10010)
Token length: 1289
Summarized using GPT-3.5-turbo
Append: [Linguistic trajectories of bipolar disorder on social media](https://arxiv.org/abs/2509.10035)
Token length: 1054
Summarized using GPT-3.5-turbo
Append: [!MSA at BAREC Shared Task 2025: Ensembling Arabic Transformers for Readability Assessment](https://arxiv.org/abs/2509.10040)
Token length: 1361
Summarized using GPT-3.5-turbo
Append: [Established Psychometric vs. Ecologically Valid Questionnaires: Rethinking Psychological Assessments in Large Language Models](https://arxiv.org/abs/2509.10078)
Token length: 1082
Summarized using GPT-3.5-turbo
Append: [Querying Climate Knowledge: Semantic Retrieval for Scientific Discovery](https://arxiv.org/abs/2509.10087)
Token length: 1848
Summarized using GPT-3.5-turbo
Append: [Arabic Large Language Models for Medical Text Generation](https://arxiv.org/abs/2509.10095)
Token length: 1539
Summarized using GPT-3.5-turbo
Append: [Scaling Arabic Medical Chatbots Using Synthetic Data: Enhancing Generative AI with Synthetic Patient Records](https://arxiv.org/abs/2509.10108)
Token length: 1027
Summarized using GPT-3.5-turbo
Append: [Prominence-aware automatic speech recognition for conversational speech](https://arxiv.org/abs/2509.10116)
Token length: 1481
Summarized using GPT-3.5-turbo
Append: [Population-Aligned Persona Generation for LLM-based Social Simulation](https://arxiv.org/abs/2509.10127)
Token length: 1017
Summarized using GPT-3.5-turbo
Append: [Towards Reliable and Interpretable Document Question Answering via VLMs](https://arxiv.org/abs/2509.10129)
Token length: 1081
Summarized using GPT-3.5-turbo
Append: [Benchmark of stylistic variation in LLM-generated texts](https://arxiv.org/abs/2509.10179)
Token length: 1788
Summarized using GPT-3.5-turbo
Append: [Incongruent Positivity: When Miscalibrated Positivity Undermines Online Supportive Conversations](https://arxiv.org/abs/2509.10184)
Token length: 1265
Summarized using GPT-3.5-turbo
Append: [Beyond Token Limits: Assessing Language Model Performance on Long Text Classification](https://arxiv.org/abs/2509.10199)
Token length: 1391
Summarized using GPT-3.5-turbo
Append: [SI-FACT: Mitigating Knowledge Conflict via Self-Improving Faithfulness-Aware Contrastive Tuning](https://arxiv.org/abs/2509.10208)
Token length: 1446
Summarized using GPT-3.5-turbo
Append: [Dropping Experts, Recombining Neurons: Retraining-Free Pruning for Sparse Mixture-of-Experts LLMs](https://arxiv.org/abs/2509.10377)
Token length: 1423
Summarized using GPT-3.5-turbo
Append: [Is In-Context Learning Learning?](https://arxiv.org/abs/2509.10414)
Token length: 920
Summarized using GPT-3.5-turbo
Append: [Long Context Automated Essay Scoring with Language Models](https://arxiv.org/abs/2509.10417)
Token length: 1638
Summarized using GPT-3.5-turbo
Append: [RefactorCoderQA: Benchmarking LLMs for Multi-Domain Coding Question Solutions in Cloud and Edge Deployment](https://arxiv.org/abs/2509.10436)
Append: [DeepDive: Advancing Deep Search Agents with Knowledge Graphs and Multi-Turn RL](https://arxiv.org/abs/2509.10446)
Append: [WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained Speech Recognition Transformers](https://arxiv.org/abs/2509.10452)
Append: [Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level RL](https://arxiv.org/abs/2509.09177)
Append: [DB3 Team's Solution For Meta KDD Cup' 25](https://arxiv.org/abs/2509.09681)
Append: [Text-to-SQL Oriented to the Process Mining Domain: A PT-EN Dataset for Query Translation](https://arxiv.org/abs/2509.09684)
Append: [AI-Powered Assistant for Long-Term Access to RHIC Knowledge](https://arxiv.org/abs/2509.09688)
Append: [Personas within Parameters: Fine-Tuning Small Language Models with Low-Rank Adapters to Mimic User Behaviors](https://arxiv.org/abs/2509.09689)
Append: [Differential Robustness in Transformer Language Models: Empirical Evaluation Under Adversarial Text Attacks](https://arxiv.org/abs/2509.09706)
Append: [LLM-Based Instance-Driven Heuristic Bias In the Context of a Biased Random Key Genetic Algorithm](https://arxiv.org/abs/2509.09707)
Append: [VStyle: A Benchmark for Voice Style Adaptation with Spoken Instructions](https://arxiv.org/abs/2509.09716)
Append: [Improving MLLM Historical Record Extraction with Test-Time Image](https://arxiv.org/abs/2509.09722)
Append: [HypoGeneAgent: A Hypothesis Language Agent for Gene-Set Cluster Resolution Selection Using Perturb-seq Datasets](https://arxiv.org/abs/2509.09740)
Append: [Executable Ontologies: Synthesizing Event Semantics with Dataflow Architecture](https://arxiv.org/abs/2509.09775)
Append: [Latency and Token-Aware Test-Time Compute](https://arxiv.org/abs/2509.09864)
Append: [LLMs as Agentic Cooperative Players in Multiplayer UNO](https://arxiv.org/abs/2509.09867)
Append: [Vibe Check: Understanding the Effects of LLM-Based Conversational Agents' Personality and Alignment on User Perceptions in Goal-Oriented Tasks](https://arxiv.org/abs/2509.09870)
Append: [Whisper Has an Internal Word Aligner](https://arxiv.org/abs/2509.09987)
Append: [Unified Learnable 2D Convolutional Feature Extraction for ASR](https://arxiv.org/abs/2509.10031)
Append: [VARCO-VISION-2.0 Technical Report](https://arxiv.org/abs/2509.10105)
Append: [Error Analysis in a Modular Meeting Transcription System](https://arxiv.org/abs/2509.10143)
Append: [Abduct, Act, Predict: Scaffolding Causal Inference for Automated Failure Attribution in Multi-Agent Systems](https://arxiv.org/abs/2509.10401)
Append: [Slaves to the Law of Large Numbers: An Asymptotic Equipartition Property for Perplexity in Generative Language Models](https://arxiv.org/abs/2405.13798)
Append: [UIO-LLMs: Unbiased Incremental Optimization for Long-Context LLMs](https://arxiv.org/abs/2406.18173)
Append: [Direct Judgement Preference Optimization](https://arxiv.org/abs/2409.14664)
Append: [Atomic Fact Decomposition Helps Attributed Question Answering](https://arxiv.org/abs/2410.16708)
Append: [Are LLMs Better than Reported? Detecting Label Errors and Mitigating Their Effect on Model Performance](https://arxiv.org/abs/2410.18889)
Append: [Polish-English medical knowledge transfer: A new benchmark and results](https://arxiv.org/abs/2412.00559)
Append: [A 2-step Framework for Automated Literary Translation Evaluation: Its Promises and Pitfalls](https://arxiv.org/abs/2412.01340)
Append: [Tokens, the oft-overlooked appetizer: Large language models, the distributional hypothesis, and meaning](https://arxiv.org/abs/2412.10924)
Append: [FinMTEB: Finance Massive Text Embedding Benchmark](https://arxiv.org/abs/2502.10990)
Append: [D\'ej\`a Vu: Multilingual LLM Evaluation through the Lens of Machine Translation Evaluation](https://arxiv.org/abs/2504.11829)
Append: [Alignment-Augmented Speculative Decoding with Alignment Sampling and Conditional Verification](https://arxiv.org/abs/2505.13204)
Append: [Breaking Language Barriers or Reinforcing Bias? A Study of Gender and Racial Disparities in Multilingual Contrastive Vision Language Models](https://arxiv.org/abs/2505.14160)
Append: [Humans Hallucinate Too: Language Models Identify and Correct Subjective Annotation Errors With Label-in-a-Haystack Prompts](https://arxiv.org/abs/2505.17222)
Append: [NileChat: Towards Linguistically Diverse and Culturally Aware LLMs for Local Communities](https://arxiv.org/abs/2505.18383)
Append: [Faster and Better LLMs via Latency-Aware Test-Time Scaling](https://arxiv.org/abs/2505.19634)
Append: [Comparing Apples to Oranges: A Dataset & Analysis of LLM Humour Understanding from Traditional Puns to Topical Jokes](https://arxiv.org/abs/2507.13335)
Append: [Reframe Your Life Story: Interactive Narrative Therapist and Innovative Moment Assessment with Large Language Models](https://arxiv.org/abs/2507.20241)
Append: [Feedback-Driven Tool-Use Improvements in Large Language Models via Automated Build Environments](https://arxiv.org/abs/2508.08791)
Append: [Decoding Neural Emotion Patterns through Large Language Model Embeddings](https://arxiv.org/abs/2508.09337)
Append: [Can LLM Prompting Serve as a Proxy for Static Analysis in Vulnerability Detection](https://arxiv.org/abs/2412.12039)
Append: [MoPD: Mixture-of-Prompts Distillation for Vision-Language Models](https://arxiv.org/abs/2412.19087)
Append: [Agentic Vehicles for Human-Centered Mobility Systems](https://arxiv.org/abs/2507.04996)
Append: [Input-Time Scaling](https://arxiv.org/abs/2508.13654)
append_entries: 94
Finish: 2025-09-15 04:24:46.145139
------------------------------------------------------
Started: 2025-09-15 06:25:37.434966
Existing_entries: 1094
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1577
Summarized using GPT-3.5-turbo
Append: [Oyster-I: Beyond Refusal -- Constructive Safety Alignment for Responsible Language Models](https://arxiv.org/abs/2509.01909)
append_entries: 1
Finish: 2025-09-15 06:25:39.728235
------------------------------------------------------
Started: 2025-09-15 08:21:34.419567
Existing_entries: 1001
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-15 08:21:34.663777
------------------------------------------------------
Started: 2025-09-15 10:17:14.269323
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-15 10:17:14.549011
------------------------------------------------------
Started: 2025-09-15 12:33:42.514881
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-15 12:33:42.832026
------------------------------------------------------
Started: 2025-09-15 14:16:52.055151
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-15 14:16:52.299866
------------------------------------------------------
Started: 2025-09-15 16:19:49.011866
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-15 16:19:49.255183
------------------------------------------------------
Started: 2025-09-15 18:23:03.075453
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-15 18:23:03.329845
------------------------------------------------------
Started: 2025-09-15 20:17:20.996545
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-15 20:17:21.247388
------------------------------------------------------
Started: 2025-09-15 22:13:53.064428
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-15 22:13:53.314847
------------------------------------------------------
Started: 2025-09-16 01:11:56.866994
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-16 01:11:57.343702
------------------------------------------------------
Started: 2025-09-16 02:53:46.031501
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-16 02:53:46.343484
------------------------------------------------------
Started: 2025-09-16 04:22:32.329012
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1135
Summarized using GPT-3.5-turbo
Append: [Uncovering the Vulnerability of Large Language Models in the Financial Domain via Risk Concealment](https://arxiv.org/abs/2509.10546)
Token length: 1178
Summarized using GPT-3.5-turbo
Append: [No Answer Needed: Predicting LLM Answer Accuracy from Question-Only Linear Probes](https://arxiv.org/abs/2509.10625)
Token length: 1245
Summarized using GPT-3.5-turbo
Append: [Interdisciplinary Research in Conversation: A Case Study in Computational Morphology for Language Documentation](https://arxiv.org/abs/2509.10644)
Token length: 1060
Summarized using GPT-3.5-turbo
Append: [Context Copying Modulation: The Role of Entropy Neurons in Managing Parametric and Contextual Knowledge Conflicts](https://arxiv.org/abs/2509.10663)
Token length: 982
Summarized using GPT-3.5-turbo
Append: [Pluralistic Alignment for Healthcare: A Role-Driven Framework](https://arxiv.org/abs/2509.10685)
Token length: 1625
Summarized using GPT-3.5-turbo
Append: [Struct-Bench: A Benchmark for Differentially Private Structured Text Generation](https://arxiv.org/abs/2509.10696)
Token length: 1387
Summarized using GPT-3.5-turbo
Append: [A Survey on Retrieval And Structuring Augmented Generation with Large Language Models](https://arxiv.org/abs/2509.10697)
Token length: 1516
Summarized using GPT-3.5-turbo
Append: [SearchInstruct: Enhancing Domain Adaptation via Retrieval-Based Instruction Dataset Creation](https://arxiv.org/abs/2509.10708)
Token length: 1557
Summarized using GPT-3.5-turbo
Append: [PolyTruth: Multilingual Disinformation Detection using Transformer-Based Language Models](https://arxiv.org/abs/2509.10737)
Token length: 1431
Summarized using GPT-3.5-turbo
Append: [Reasoning Under Uncertainty: Exploring Probabilistic Reasoning Capabilities of LLMs](https://arxiv.org/abs/2509.10739)
Token length: 1120
Summarized using GPT-3.5-turbo
Append: [Automated MCQA Benchmarking at Scale: Evaluating Reasoning Traces as Retrieval Sources for Domain Adaptation of Small Language Models](https://arxiv.org/abs/2509.10744)
Token length: 1054
Summarized using GPT-3.5-turbo
Append: [RECAP: Transparent Inference-Time Emotion Alignment for Medical Dialogue Systems](https://arxiv.org/abs/2509.10746)
Token length: 1845
Summarized using GPT-3.5-turbo
Append: [Judge Q: Trainable Queries for Optimized Information Retention in KV Cache Eviction](https://arxiv.org/abs/2509.10798)
Token length: 1295
Summarized using GPT-3.5-turbo
Append: [Towards Automated Error Discovery: A Study in Conversational AI](https://arxiv.org/abs/2509.10833)
Token length: 1899
Summarized using GPT-3.5-turbo
Append: [Evaluating Large Language Models for Evidence-Based Clinical Question Answering](https://arxiv.org/abs/2509.10843)
Token length: 1833
Summarized using GPT-3.5-turbo
Append: [GAPrune: Gradient-Alignment Pruning for Domain-Aware Embeddings](https://arxiv.org/abs/2509.10844)
Token length: 1387
Summarized using GPT-3.5-turbo
Append: [Text2Sign Diffusion: A Generative Approach for Gloss-Free Sign Language Production](https://arxiv.org/abs/2509.10845)
Token length: 1812
Summarized using GPT-3.5-turbo
Append: [A funny companion: Distinct neural responses to perceived AI- versus humangenerated humor](https://arxiv.org/abs/2509.10847)
Token length: 1215
Summarized using GPT-3.5-turbo
Append: [Pre-Storage Reasoning for Episodic Memory: Shifting Inference Burden to Memory for Personalized Dialogue](https://arxiv.org/abs/2509.10852)
Token length: 1056
Summarized using GPT-3.5-turbo
Append: [Quantifier Scope Interpretation in Language Learners and LLMs](https://arxiv.org/abs/2509.10860)
Token length: 1609
Summarized using GPT-3.5-turbo
Append: [Term2Note: Synthesising Differentially Private Clinical Notes from Medical Terms](https://arxiv.org/abs/2509.10882)
Token length: 1486
Summarized using GPT-3.5-turbo
Append: [CultureSynth: A Hierarchical Taxonomy-Guided and Retrieval-Augmented Framework for Cultural Question-Answer Synthesis](https://arxiv.org/abs/2509.10886)
Token length: 1419
Summarized using GPT-3.5-turbo
Append: [Aligning ESG Controversy Data with International Guidelines through Semi-Automatic Ontology Construction](https://arxiv.org/abs/2509.10922)
Token length: 975
Summarized using GPT-3.5-turbo
Append: [Introducing Spotlight: A Novel Approach for Generating Captivating Key Information from Documents](https://arxiv.org/abs/2509.10935)
Token length: 1499
Summarized using GPT-3.5-turbo
Append: [An Interpretable Benchmark for Clickbait Detection and Tactic Attribution](https://arxiv.org/abs/2509.10937)
Token length: 1313
Summarized using GPT-3.5-turbo
Append: [EmoBench-Reddit: A Hierarchical Benchmark for Evaluating the Emotional Intelligence of Multimodal Large Language Models](https://arxiv.org/abs/2509.11101)
Token length: 1833
Summarized using GPT-3.5-turbo
Append: [Fluid Language Model Benchmarking](https://arxiv.org/abs/2509.11106)
Token length: 1452
Summarized using GPT-3.5-turbo
Append: [We Argue to Agree: Towards Personality-Driven Argumentation-Based Negotiation Dialogue Systems for Tourism](https://arxiv.org/abs/2509.11118)
Token length: 1216
Summarized using GPT-3.5-turbo
Append: [Joint Effects of Argumentation Theory, Audio Modality and Data Enrichment on LLM-Based Fallacy Classification](https://arxiv.org/abs/2509.11127)
Token length: 1414
Summarized using GPT-3.5-turbo
Append: [When Smiley Turns Hostile: Interpreting How Emojis Trigger LLMs' Toxicity](https://arxiv.org/abs/2509.11141)
Token length: 1622
Summarized using GPT-3.5-turbo
Append: [Text2Mem: A Unified Memory Operation Language for Memory Operating System](https://arxiv.org/abs/2509.11145)
Token length: 1122
Summarized using GPT-3.5-turbo
Append: [Differentially-private text generation degrades output language quality](https://arxiv.org/abs/2509.11176)
Token length: 1284
Summarized using GPT-3.5-turbo
Append: [Optimal Brain Restoration for Joint Quantization and Sparsification of LLMs](https://arxiv.org/abs/2509.11177)
Token length: 1194
Summarized using GPT-3.5-turbo
Append: [RanAT4BIE: Random Adversarial Training for Biomedical Information Extraction](https://arxiv.org/abs/2509.11191)
Token length: 1887
Summarized using GPT-3.5-turbo
Append: [The Prompt Engineering Report Distilled: Quick Start Guide for Life Sciences](https://arxiv.org/abs/2509.11295)
Token length: 1301
Summarized using GPT-3.5-turbo
Append: [Ko-PIQA: A Korean Physical Commonsense Reasoning Dataset with Cultural Context](https://arxiv.org/abs/2509.11303)
Token length: 835
Summarized using GPT-3.5-turbo
Append: [!MSA at AraHealthQA 2025 Shared Task: Enhancing LLM Performance for Arabic Clinical Question Answering through Prompt Engineering and Ensemble Learning](https://arxiv.org/abs/2509.11365)
Token length: 1263
Summarized using GPT-3.5-turbo
Append: [Transformer Enhanced Relation Classification: A Comparative Analysis of Contextuality, Data Efficiency and Sequence Complexity](https://arxiv.org/abs/2509.11374)
Token length: 1730
Summarized using GPT-3.5-turbo
Append: [Continually Adding New Languages to Multilingual Language Models](https://arxiv.org/abs/2509.11414)
Token length: 1133
Summarized using GPT-3.5-turbo
Append: [A Transformer-Based Cross-Platform Analysis of Public Discourse on the 15-Minute City Paradigm](https://arxiv.org/abs/2509.11443)
Token length: 1218
Summarized using GPT-3.5-turbo
Append: [CognitiveSky: Scalable Sentiment and Narrative Analysis for Decentralized Social Media](https://arxiv.org/abs/2509.11444)
Token length: 1132
Summarized using GPT-3.5-turbo
Append: [CEMTM: Contextual Embedding-based Multimodal Topic Modeling](https://arxiv.org/abs/2509.11465)
Token length: 770
Summarized using GPT-3.5-turbo
Append: [Improving LLMs' Learning for Coreference Resolution](https://arxiv.org/abs/2509.11466)
Token length: 851
Summarized using GPT-3.5-turbo
Append: [ClaimIQ at CheckThat! 2025: Comparing Prompted and Fine-Tuned Language Models for Verifying Numerical Claims](https://arxiv.org/abs/2509.11492)
Token length: 1210
Summarized using GPT-3.5-turbo
Append: [AKCIT-FN at CheckThat! 2025: Switching Fine-Tuned SLMs and LLM Prompting for Multilingual Claim Normalization](https://arxiv.org/abs/2509.11496)
Token length: 672
Summarized using GPT-3.5-turbo
Append: [DeDisCo at the DISRPT 2025 Shared Task: A System for Discourse Relation Classification](https://arxiv.org/abs/2509.11498)
Token length: 1119
Summarized using GPT-3.5-turbo
Append: [Unsupervised Candidate Ranking for Lexical Substitution via Holistic Sentence Semantics](https://arxiv.org/abs/2509.11513)
Token length: 993
Summarized using GPT-3.5-turbo
Append: [LVLMs are Bad at Overhearing Human Referential Communication](https://arxiv.org/abs/2509.11514)
Token length: 1870
Summarized using GPT-3.5-turbo
Append: [PeruMedQA: Benchmarking Large Language Models (LLMs) on Peruvian Medical Exams -- Dataset Construction and Evaluation](https://arxiv.org/abs/2509.11517)
Token length: 739
Summarized using GPT-3.5-turbo
Append: [On the Distinctive Co-occurrence Characteristics of Antonymy](https://arxiv.org/abs/2509.11534)
Append: [HARP: Hallucination Detection via Reasoning Subspace Projection](https://arxiv.org/abs/2509.11536)
Append: [HiChunk: Evaluating and Enhancing Retrieval-Augmented Generation with Hierarchical Chunking](https://arxiv.org/abs/2509.11552)
Append: [D$^2$HScore: Reasoning-Aware Hallucination Detection via Semantic Breadth and Depth Analysis in LLMs](https://arxiv.org/abs/2509.11569)
Append: [Bhaasha, Bhasa, Zaban: A Survey for Low-Resourced Languages in South Asia -- Current Stage and Challenges](https://arxiv.org/abs/2509.11570)
Append: [Analyzing Information-Seeking Behaviors in a Hakka AI Chatbot: A Cognitive-Pragmatic Study](https://arxiv.org/abs/2509.11591)
Append: [Dynamic Span Interaction and Graph-Aware Memory for Entity-Level Sentiment Classification](https://arxiv.org/abs/2509.11604)
Append: [HalluDetect: Detecting, Mitigating, and Benchmarking Hallucinations in Conversational Systems](https://arxiv.org/abs/2509.11619)
Append: [AesBiasBench: Evaluating Bias and Alignment in Multimodal Language Models for Personalized Image Aesthetic Assessment](https://arxiv.org/abs/2509.11620)
Append: [EthicsMH: A Pilot Benchmark for Ethical Reasoning in Mental Health AI](https://arxiv.org/abs/2509.11648)
Append: [A Dynamic Knowledge Update-Driven Model with Large Language Models for Fake News Detection](https://arxiv.org/abs/2509.11687)
Append: [CoachMe: Decoding Sport Elements with a Reference-Based Coaching Instruction Generation Model](https://arxiv.org/abs/2509.11698)
Append: [Room acoustics affect communicative success in hybrid meeting spaces: a pilot study](https://arxiv.org/abs/2509.11709)
Append: [An Agentic Toolkit for Adaptive Information Extraction from Regulatory Documents](https://arxiv.org/abs/2509.11773)
Append: [User eXperience Perception Insights Dataset (UXPID): Synthetic User Feedback from Public Industrial Forums](https://arxiv.org/abs/2509.11777)
Append: [When Curiosity Signals Danger: Predicting Health Crises Through Online Medication Inquiries](https://arxiv.org/abs/2509.11802)
Append: [From Fuzzy Speech to Medical Insight: Benchmarking LLMs on Noisy Patient Narratives](https://arxiv.org/abs/2509.11803)
Append: [PledgeTracker: A System for Monitoring the Fulfilment of Pledges](https://arxiv.org/abs/2509.11804)
Append: [SCDTour: Embedding Axis Ordering and Merging for Interpretable Semantic Change Detection](https://arxiv.org/abs/2509.11818)
Append: [MOOM: Maintenance, Organization and Optimization of Memory in Ultra-Long Role-Playing Dialogues](https://arxiv.org/abs/2509.11860)
Append: [Growing Perspectives: Modelling Embodied Perspective Taking and Inner Narrative Development Using Large Language Models](https://arxiv.org/abs/2509.11868)
Append: [Uncertainty in Authorship: Why Perfect AI Detection Is Mathematically Impossible](https://arxiv.org/abs/2509.11915)
Append: [Designing LLMs for cultural sensitivity: Evidence from English-Japanese translation](https://arxiv.org/abs/2509.11921)
Append: [Spec-LLaVA: Accelerating Vision-Language Models with Dynamic Tree-Based Speculative Decoding](https://arxiv.org/abs/2509.11961)
Append: [ToolRM: Outcome Reward Models for Tool-Calling Large Language Models](https://arxiv.org/abs/2509.11963)
Append: [Query-Focused Extractive Summarization for Sentiment Explanation](https://arxiv.org/abs/2509.11989)
Append: [Text Adaptation to Plain Language and Easy Read via Automatic Post-Editing Cycles](https://arxiv.org/abs/2509.11991)
Append: [Steering Language Models in Multi-Token Generation: A Case Study on Tense and Aspect](https://arxiv.org/abs/2509.12065)
Append: [SENSE models: an open source solution for multilingual and multimodal semantic-based tasks](https://arxiv.org/abs/2509.12093)
Append: [Is 'Hope' a person or an idea? A pilot benchmark for NER: comparing traditional NLP tools and large language models on ambiguous entities](https://arxiv.org/abs/2509.12098)
Append: [In-domain SSL pre-training and streaming ASR](https://arxiv.org/abs/2509.12101)
Append: [GTA: Supervised-Guided Reinforcement Learning for Text Classification with Large Language Models](https://arxiv.org/abs/2509.12108)
Append: [CBP-Tuning: Efficient Local Customization for Black-box Large Language Models](https://arxiv.org/abs/2509.12112)
Append: [XplaiNLP at CheckThat! 2025: Multilingual Subjectivity Detection with Finetuned Transformers and Prompt-Based Inference with Large Language Models](https://arxiv.org/abs/2509.12130)
Append: [Pun Unintended: LLMs and the Illusion of Humor Understanding](https://arxiv.org/abs/2509.12158)
Append: [RAGs to Riches: RAG-like Few-shot Learning for Large Language Model Role-playing](https://arxiv.org/abs/2509.12168)
Append: [Preservation of Language Understanding Capabilities in Speech-aware Large Language Models](https://arxiv.org/abs/2509.12171)
Append: [DSRAG: A Domain-Specific Retrieval Framework Based on Document-derived Multimodal Knowledge Graph](https://arxiv.org/abs/2509.10467)
Append: [Learning Decomposed Contextual Token Representations from Pretrained and Collaborative Signals for Generative Recommendation](https://arxiv.org/abs/2509.10468)
Append: [Real-Time RAG for the Identification of Supply Chain Vulnerabilities](https://arxiv.org/abs/2509.10469)
Append: [Decoupling the "What" and "Where" With Polar Coordinate Positional Embeddings](https://arxiv.org/abs/2509.10534)
Append: [DualAlign: Generating Clinically Grounded Synthetic Data](https://arxiv.org/abs/2509.10538)
Append: [Smart Trial: Evaluating the Use of Large Language Models for Recruiting Clinical Trial Participants via Social Media](https://arxiv.org/abs/2509.10584)
Append: [LLM in the Middle: A Systematic Review of Threats and Mitigations to Real-World LLM-based Systems](https://arxiv.org/abs/2509.10682)
Append: [Understanding AI Evaluation Patterns: How Different GPT Models Assess Vision-Language Descriptions](https://arxiv.org/abs/2509.10707)
Append: [AgentArch: A Comprehensive Benchmark to Evaluate Agent Architectures in Enterprise](https://arxiv.org/abs/2509.10769)
Append: [Why Bonds Fail Differently? Explainable Multimodal Learning for Multi-Class Default Prediction](https://arxiv.org/abs/2509.10802)
Append: [Harmful Prompt Laundering: Jailbreaking LLMs with Abductive Styles and Symbolic Encoding](https://arxiv.org/abs/2509.10931)
Append: [Public Data Assisted Differentially Private In-Context Learning](https://arxiv.org/abs/2509.10932)
Append: [ReFineG: Synergizing Small Supervised Models and LLMs for Low-Resource Grounded Multimodal NER](https://arxiv.org/abs/2509.10975)
Append: [Rethinking Human Preference Evaluation of LLM Rationales](https://arxiv.org/abs/2509.11026)
Append: [The System Description of CPS Team for Track on Driving with Language of CVPR 2024 Autonomous Grand Challenge](https://arxiv.org/abs/2509.11071)
Append: [Length-Aware Rotary Position Embedding for Text-Speech Alignment](https://arxiv.org/abs/2509.11084)
Append: [Agentic Username Suggestion and Multimodal Gender Detection in Online Platforms: Introducing the PNGT-26K Dataset](https://arxiv.org/abs/2509.11136)
Append: [AQUA: Attention via QUery mAgnitudes for Memory and Compute Efficient Inference in LLMs](https://arxiv.org/abs/2509.11155)
Append: [DreamNav: A Trajectory-Based Imaginative Framework for Zero-Shot Vision-and-Language Navigation](https://arxiv.org/abs/2509.11197)
Append: [Evalet: Evaluating Large Language Models by Fragmenting Outputs into Functions](https://arxiv.org/abs/2509.11206)
Append: [Mitigating Hallucinations in Large Vision-Language Models by Self-Injecting Hallucinations](https://arxiv.org/abs/2509.11287)
Append: [Opal: An Operator Algebra View of RLHF](https://arxiv.org/abs/2509.11298)
Append: [Trading-R1: Financial Trading with LLM Reasoning via Reinforcement Learning](https://arxiv.org/abs/2509.11420)
Append: [FuseCodec: Semantic-Contextual Fusion and Supervision for Neural Codecs](https://arxiv.org/abs/2509.11425)
Append: [Securing AI Agents: Implementing Role-Based Access Control for Industrial Applications](https://arxiv.org/abs/2509.11431)
Append: [Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting](https://arxiv.org/abs/2509.11452)
Append: [Formal Reasoning for Intelligent QA Systems: A Case Study in the Educational Domain](https://arxiv.org/abs/2509.11572)
Append: [MALLM: Multi-Agent Large Language Models Framework](https://arxiv.org/abs/2509.11656)
Append: [MindVL: Towards Efficient and Effective Training of Multimodal Large Language Models on Ascend NPUs](https://arxiv.org/abs/2509.11662)
Append: [Measuring Visual Understanding in Telecom domain: Performance Metrics for Image-to-UML conversion using VLMs](https://arxiv.org/abs/2509.11667)
Append: [Collapse of Irrelevant Representations (CIR) Ensures Robust and Non-Disruptive LLM Unlearning](https://arxiv.org/abs/2509.11816)
Append: [Collaborative Document Editing with Multiple Users and AI Agents](https://arxiv.org/abs/2509.11826)
Append: [The AI Memory Gap: Users Misremember What They Created With AI or Without](https://arxiv.org/abs/2509.11851)
Append: [How to Evaluate Medical AI](https://arxiv.org/abs/2509.11941)
Append: [MillStone: How Open-Minded Are LLMs?](https://arxiv.org/abs/2509.11967)
Append: [Lost in Embeddings: Information Loss in Vision-Language Models](https://arxiv.org/abs/2509.11986)
Append: [AMQ: Enabling AutoML for Mixed-precision Weight-Only Quantization of Large Language Models](https://arxiv.org/abs/2509.12019)
Append: [FinGEAR: Financial Mapping-Guided Enhanced Answer Retrieval](https://arxiv.org/abs/2509.12042)
Append: [RadarLLM: Adapting Pretrained Large Language Models for Marine Radar Target Detection with Preference-aware Loss](https://arxiv.org/abs/2509.12089)
Append: [When marine radar target detection meets pretrained large language models](https://arxiv.org/abs/2509.12110)
Append: [Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language Models](https://arxiv.org/abs/2509.12132)
Append: [Event2Vec: A Geometric Approach to Learning Composable Representations of Event Sequences](https://arxiv.org/abs/2509.12188)
Append: [Survival at Any Cost? LLMs and the Choice Between Self-Preservation and Human Harm](https://arxiv.org/abs/2509.12190)
Append: [Understanding Emergent In-Context Learning from a Kernel Regression Perspective](https://arxiv.org/abs/2305.12766)
Append: [Tackling Fake News in Bengali: Unraveling the Impact of Summarization vs. Augmentation on Pre-trained Language Models](https://arxiv.org/abs/2307.06979)
Append: [Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models](https://arxiv.org/abs/2309.01219)
Append: [LML: A Novel Lexicon for the Moral Foundation of Liberty](https://arxiv.org/abs/2407.11862)
Append: [Surveying the Landscape of Image Captioning Evaluation: A Comprehensive Taxonomy, Trends and Metrics Analysis](https://arxiv.org/abs/2408.04909)
Append: [Can Advanced LLMs Coach Smaller LLMs? Knowledge Distillation for Goal-Oriented Dialogs](https://arxiv.org/abs/2408.07238)
Append: [GP-GPT: Large Language Model for Gene-Phenotype Mapping](https://arxiv.org/abs/2409.09825)
Append: [Revealing the Inherent Instructability of Pre-Trained Language Models](https://arxiv.org/abs/2410.02465)
Append: [Evaluating Automatic Speech Recognition Systems for Korean Meteorological Experts](https://arxiv.org/abs/2410.18444)
Append: [Can LLMs assist with Ambiguity? A Quantitative Evaluation of various Large Language Models on Word Sense Disambiguation](https://arxiv.org/abs/2411.18337)
Append: [Artificial intelligence contribution to translation industry: looking back and forward](https://arxiv.org/abs/2411.19855)
Append: [FM2DS: Few-Shot Multimodal Multihop Data Synthesis with Knowledge Distillation for Question Answering](https://arxiv.org/abs/2412.07030)
Append: [Speak-to-Structure: Evaluating LLMs in Open-domain Natural Language-Driven Molecule Generation](https://arxiv.org/abs/2412.14642)
Append: [IOLBENCH: Benchmarking LLMs on Linguistic Reasoning](https://arxiv.org/abs/2501.04249)
Append: [Transformer-Based Multimodal Knowledge Graph Completion with Link-Aware Contexts](https://arxiv.org/abs/2501.15688)
Append: [From Personas to Talks: Revisiting the Impact of Personas on LLM-Synthesized Emotional Support Conversations](https://arxiv.org/abs/2502.11451)
Append: [DSMoE: Matrix-Partitioned Experts with Dynamic Routing for Computation-Efficient Dense LLMs](https://arxiv.org/abs/2502.12455)
Append: [Efficient Environmental Claim Detection with Hyperbolic Graph Neural Networks](https://arxiv.org/abs/2502.13628)
Append: [Rumor Detection by Multi-task Suffix Learning based on Time-series Dual Sentiments](https://arxiv.org/abs/2502.14383)
Append: [Understanding the Uncertainty of LLM Explanations: A Perspective Based on Reasoning Topology](https://arxiv.org/abs/2502.17026)
Append: [LLM as a Broken Telephone: Iterative Generation Distorts Information](https://arxiv.org/abs/2502.20258)
Append: [LinguaLens: Towards Interpreting Linguistic Mechanisms of Large Language Models via Sparse Auto-Encoder](https://arxiv.org/abs/2502.20344)
Append: [Monitoring Decoding: Mitigating Hallucination via Evaluating the Factuality of Partial Response during Generation](https://arxiv.org/abs/2503.03106)
Append: [Chain of Strategy Optimization Makes Large Language Models Better Emotional Supporter](https://arxiv.org/abs/2503.05362)
Append: [Hallucinated Span Detection with Multi-View Attention Features](https://arxiv.org/abs/2504.04335)
Append: [Assessing LLMs in Art Contexts: Critique Generation and Theory of Mind Evaluation](https://arxiv.org/abs/2504.12805)
Append: [LogicTree: Structured Proof Exploration for Coherent and Rigorous Logical Reasoning with Large Language Models](https://arxiv.org/abs/2504.14089)
Append: [EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language Models](https://arxiv.org/abs/2504.15133)
Append: [Better To Ask in English? Evaluating Factual Accuracy of Multilingual LLMs in English and Low-Resource Languages](https://arxiv.org/abs/2504.20022)
Append: [Improving Informally Romanized Language Identification](https://arxiv.org/abs/2504.21540)
Append: [MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced Knowledge Boundary Awareness](https://arxiv.org/abs/2504.21773)
Append: [Base Models Beat Aligned Models at Randomness and Creativity](https://arxiv.org/abs/2505.00047)
Append: [Synthesize-on-Graph: Knowledgeable Synthetic Data Generation for Continue Pre-training of Large Language Models](https://arxiv.org/abs/2505.00979)
Append: [Multilingual Collaborative Defense for Large Language Models](https://arxiv.org/abs/2505.11835)
Append: [ConvSearch-R1: Enhancing Query Reformulation for Conversational Search with Reasoning via Reinforcement Learning](https://arxiv.org/abs/2505.15776)
Append: [HiMATE: A Hierarchical Multi-Agent Framework for Machine Translation Evaluation](https://arxiv.org/abs/2505.16281)
Append: [ReliableEval: A Recipe for Stochastic LLM Evaluation via Method of Moments](https://arxiv.org/abs/2505.22169)
Append: [Active Layer-Contrastive Decoding Reduces Hallucination in Large Language Model Generation](https://arxiv.org/abs/2505.23657)
Append: [MARS-Bench: A Multi-turn Athletic Real-world Scenario Benchmark for Dialogue Evaluation](https://arxiv.org/abs/2505.23810)
Append: [Soft Reasoning: Navigating Solution Spaces in Large Language Models through Controlled Embedding Exploration](https://arxiv.org/abs/2505.24688)
Append: [Hopscotch: Discovering and Skipping Redundancies in Language Models](https://arxiv.org/abs/2506.03303)
Append: [Recycling the Web: A Method to Enhance Pre-training Data Quality and Quantity for Language Models](https://arxiv.org/abs/2506.04689)
Append: [GeoGuess: Multimodal Reasoning based on Hierarchy of Visual Information in Street View](https://arxiv.org/abs/2506.16633)
Append: [Mirage of Mastery: Memorization Tricks LLMs into Artificially Inflated Self-Knowledge](https://arxiv.org/abs/2506.18998)
Append: [Time is On My Side: Dynamics of Talk-Time Sharing in Video-chat Conversations](https://arxiv.org/abs/2506.20474)
Append: [A Cross-Cultural Comparison of LLM-based Public Opinion Simulation: Evaluating Chinese and U.S. Models on Diverse Societies](https://arxiv.org/abs/2506.21587)
Append: [LastingBench: Defend Benchmarks Against Knowledge Leakage](https://arxiv.org/abs/2506.21614)
Append: [PDFMathTranslate: Scientific Document Translation Preserving Layouts](https://arxiv.org/abs/2507.03009)
Append: [Persona-Based Synthetic Data Generation Using Multi-Stage Conditioning with Large Language Models for Emotion Recognition](https://arxiv.org/abs/2507.13380)
Append: [UR$^2$: Unify RAG and Reasoning through Reinforcement Learning](https://arxiv.org/abs/2508.06165)
Append: [Humanizing Machines: Rethinking LLM Anthropomorphism Through a Multi-Level Framework of Design](https://arxiv.org/abs/2508.17573)
Append: [Less Is More? Examining Fairness in Pruned Large Language Models for Summarising Opinions](https://arxiv.org/abs/2508.17610)
Append: [ISACL: Internal State Analyzer for Copyrighted Training Data Leakage](https://arxiv.org/abs/2508.17767)
Append: [MTalk-Bench: Evaluating Speech-to-Speech Models in Multi-Turn Dialogues via Arena-style and Rubrics Protocols](https://arxiv.org/abs/2508.18240)
Append: [AraHealthQA 2025: The First Shared Task on Arabic Health Question Answering](https://arxiv.org/abs/2508.20047)
Append: [Enhancing Prompt Injection Attacks to LLMs via Poisoning Alignment](https://arxiv.org/abs/2410.14827)
Append: [A Survey on Large Language Model-based Agents for Statistics and Data Science](https://arxiv.org/abs/2412.14222)
Append: [Evaluating and Aligning Human Economic Risk Preferences in LLMs](https://arxiv.org/abs/2503.06646)
Append: [One Goal, Many Challenges: Robust Preference Optimization Amid Content-Aware and Multi-Source Noise](https://arxiv.org/abs/2503.12301)
Append: [Assessing Consistency and Reproducibility in the Outputs of Large Language Models: Evidence Across Diverse Finance and Accounting Tasks](https://arxiv.org/abs/2503.16974)
Append: [Lean Formalization of Generalization Error Bound by Rademacher Complexity](https://arxiv.org/abs/2503.19605)
Append: [Rethinking LLM-Based Recommendations: A Personalized Query-Driven Parallel Integration](https://arxiv.org/abs/2504.11889)
Append: [SmallPlan: Leverage Small Language Models for Sequential Path Planning with Simulation-Powered, LLM-Guided Distillation](https://arxiv.org/abs/2505.00831)
Append: [Steering LVLMs via Sparse Autoencoder for Hallucination Mitigation](https://arxiv.org/abs/2505.16146)
Append: [STRICT: Stress Test of Rendering Images Containing Text](https://arxiv.org/abs/2505.18985)
Append: [Plugging Schema Graph into Multi-Table QA: A Human-Guided Framework for Reducing LLM Reliance](https://arxiv.org/abs/2506.04427)
Append: [Reducing Object Hallucination in Large Audio-Language Models via Audio-Aware Decoding](https://arxiv.org/abs/2506.07233)
Append: [The Diffusion Duality](https://arxiv.org/abs/2506.10892)
Append: [Low-rank variational dropout: Uncertainty and rank selection in adapters](https://arxiv.org/abs/2506.22809)
Append: [CAC-CoT: Connector-Aware Compact Chain-of-Thought for Efficient Reasoning Data Synthesis Across Dual-System Cognitive Tasks](https://arxiv.org/abs/2508.18743)
append_entries: 199
Finish: 2025-09-16 04:24:13.290351
------------------------------------------------------
Started: 2025-09-16 06:25:17.519058
Existing_entries: 1199
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1583
Summarized using GPT-3.5-turbo
Append: [Adaptive Monitoring and Real-World Evaluation of Agentic AI Systems](https://arxiv.org/abs/2509.00115)
Token length: 1382
Summarized using GPT-3.5-turbo
Append: [MEPT: Mixture of Expert Prompt Tuning as a Manifold Mapper](https://arxiv.org/abs/2509.00996)
append_entries: 2
Finish: 2025-09-16 06:25:22.562340
------------------------------------------------------
Started: 2025-09-16 08:22:07.536659
Existing_entries: 1002
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-16 08:22:07.972752
------------------------------------------------------
Started: 2025-09-16 10:17:00.752088
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-16 10:17:01.189220
------------------------------------------------------
Started: 2025-09-16 12:33:38.357159
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-16 12:33:38.879572
------------------------------------------------------
Started: 2025-09-16 14:16:38.467619
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-16 14:16:39.050892
------------------------------------------------------
Started: 2025-09-16 16:20:04.301244
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-16 16:20:04.806663
------------------------------------------------------
Started: 2025-09-16 18:23:01.007441
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-16 18:23:01.422852
------------------------------------------------------
Started: 2025-09-16 20:17:26.760537
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-16 20:17:27.181460
------------------------------------------------------
Started: 2025-09-16 22:14:03.842407
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-16 22:14:04.303466
------------------------------------------------------
Started: 2025-09-17 01:11:51.820062
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-17 01:11:52.261749
------------------------------------------------------
Started: 2025-09-17 02:53:12.346121
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-17 02:53:12.779555
------------------------------------------------------
Started: 2025-09-17 04:22:46.967335
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1076
Summarized using GPT-3.5-turbo
Append: [MTEB-NL and E5-NL: Embedding Benchmark and Models for Dutch](https://arxiv.org/abs/2509.12340)
Token length: 1374
Summarized using GPT-3.5-turbo
Append: [MORABLES: A Benchmark for Assessing Abstract Moral Reasoning in LLMs with Fables](https://arxiv.org/abs/2509.12371)
Token length: 1536
Summarized using GPT-3.5-turbo
Append: [LLM-as-a-Judge: Rapid Evaluation of Legal Document Recommendation for Retrieval-Augmented Generation](https://arxiv.org/abs/2509.12382)
Token length: 770
Summarized using GPT-3.5-turbo
Append: [SENTRA: Selected-Next-Token Transformer for LLM Text Detection](https://arxiv.org/abs/2509.12385)
Token length: 1555
Summarized using GPT-3.5-turbo
Append: [MORQA: Benchmarking Evaluation Metrics for Medical Open-Ended Question Answering](https://arxiv.org/abs/2509.12405)
Token length: 1685
Summarized using GPT-3.5-turbo
Append: [MedFact: Benchmarking the Fact-Checking Capabilities of Large Language Models on Chinese Medical Texts](https://arxiv.org/abs/2509.12440)
Token length: 1133
Summarized using GPT-3.5-turbo
Append: [Topic Coverage-based Demonstration Retrieval for In-Context Learning](https://arxiv.org/abs/2509.12451)
Token length: 1967
Summarized using GPT-3.5-turbo
Append: [Does Language Model Understand Language?](https://arxiv.org/abs/2509.12459)
Token length: 1478
Summarized using GPT-3.5-turbo
Append: [Audited Reasoning Refinement: Fine-Tuning Language Models via LLM-Guided Step-Wise Evaluation and Correction](https://arxiv.org/abs/2509.12476)
Token length: 1271
Summarized using GPT-3.5-turbo
Append: [FunAudio-ASR Technical Report](https://arxiv.org/abs/2509.12508)
Token length: 1408
Summarized using GPT-3.5-turbo
Append: [A comparison of pipelines for the translation of a low resource language based on transformers](https://arxiv.org/abs/2509.12514)
Token length: 988
Summarized using GPT-3.5-turbo
Append: [MAGIC-Enhanced Keyword Prompting for Zero-Shot Audio Captioning with CLIP Models](https://arxiv.org/abs/2509.12591)
Token length: 1583
Summarized using GPT-3.5-turbo
Append: [EconProver: Towards More Economical Test-Time Scaling for Automated Theorem Proving](https://arxiv.org/abs/2509.12603)
Token length: 796
Summarized using GPT-3.5-turbo
Append: [Positional Encoding via Token-Aware Phase Attention](https://arxiv.org/abs/2509.12635)
Token length: 1210
Summarized using GPT-3.5-turbo
Append: [PAC: Pronunciation-Aware Contextualized Large Language Model-based Automatic Speech Recognition](https://arxiv.org/abs/2509.12647)
Token length: 1413
Summarized using GPT-3.5-turbo
Append: [Don't Change My View: Ideological Bias Auditing in Large Language Models](https://arxiv.org/abs/2509.12652)
Token length: 1085
Summarized using GPT-3.5-turbo
Append: [Mitigating Strategy Preference Bias in Emotional Support Conversation via Uncertainty Estimations](https://arxiv.org/abs/2509.12661)
Token length: 1271
Summarized using GPT-3.5-turbo
Append: [Chat-Driven Text Generation and Interaction for Person Retrieval](https://arxiv.org/abs/2509.12662)
Token length: 1627
Summarized using GPT-3.5-turbo
Append: [Towards Inclusive Toxic Content Moderation: Addressing Vulnerabilities to Adversarial Attacks in Toxicity Classifiers Tackling LLM-generated Content](https://arxiv.org/abs/2509.12672)
Token length: 893
Summarized using GPT-3.5-turbo
Append: [Case-Based Decision-Theoretic Decoding with Quality Memories](https://arxiv.org/abs/2509.12677)
Token length: 1539
Summarized using GPT-3.5-turbo
Append: [HistoryBankQA: Multilingual Temporal Question Answering on Historical Events](https://arxiv.org/abs/2509.12720)
Token length: 1671
Summarized using GPT-3.5-turbo
Append: [Contrastive Learning with Enhanced Abstract Representations using Grouped Loss of Abstract Semantic Supervision](https://arxiv.org/abs/2509.12771)
Token length: 1943
Summarized using GPT-3.5-turbo
Append: [ConvergeWriter: Data-Driven Bottom-Up Article Construction](https://arxiv.org/abs/2509.12811)
Token length: 957
Summarized using GPT-3.5-turbo
Append: [Data Augmentation for Maltese NLP using Transliterated and Machine Translated Arabic Data](https://arxiv.org/abs/2509.12853)
Token length: 1182
Summarized using GPT-3.5-turbo
Append: [Benchmarking and Improving LVLMs on Event Extraction from Multimedia Documents](https://arxiv.org/abs/2509.12876)
Token length: 1213
Summarized using GPT-3.5-turbo
Append: [The LLM Already Knows: Estimating LLM-Perceived Question Difficulty via Hidden Representations](https://arxiv.org/abs/2509.12886)
Token length: 1366
Summarized using GPT-3.5-turbo
Append: [Conan-Embedding-v2: Training an LLM from Scratch for Text Embeddings](https://arxiv.org/abs/2509.12892)
Token length: 697
Summarized using GPT-3.5-turbo
Append: [All Roads Lead to Rome: Graph-Based Confidence Estimation for Large Language Model Reasoning](https://arxiv.org/abs/2509.12908)
Token length: 1828
Summarized using GPT-3.5-turbo
Append: [Automated Generation of Research Workflows from Academic Papers: A Full-text Mining Framework](https://arxiv.org/abs/2509.12955)
Token length: 924
Summarized using GPT-3.5-turbo
Append: [Investigating ReLoRA: Effects on the Learning Dynamics of Small Language Models](https://arxiv.org/abs/2509.12960)
Token length: 1114
Summarized using GPT-3.5-turbo
Append: [Do LLMs Understand Wine Descriptors Across Cultures? A Benchmark for Cultural Adaptations of Wine Reviews](https://arxiv.org/abs/2509.12961)
Token length: 1274
Summarized using GPT-3.5-turbo
Append: [SitLLM: Large Language Models for Sitting Posture Health Understanding via Pressure Sensor Data](https://arxiv.org/abs/2509.12994)
Token length: 1442
Summarized using GPT-3.5-turbo
Append: [Multi-Model Synthetic Training for Mission-Critical Small Language Models](https://arxiv.org/abs/2509.13047)
Token length: 1471
Summarized using GPT-3.5-turbo
Append: [Shaping Explanations: Semantic Reward Modeling with Encoder-Only Transformers for GRPO](https://arxiv.org/abs/2509.13081)
Token length: 1841
Summarized using GPT-3.5-turbo
Append: [Empowering LLMs with Parameterized Skills for Adversarial Long-Horizon Planning](https://arxiv.org/abs/2509.13127)
Token length: 1340
Summarized using GPT-3.5-turbo
Append: [LLM Hallucination Detection: A Fast Fourier Transform Method Based on Hidden Layer Temporal Signals](https://arxiv.org/abs/2509.13154)
Token length: 1350
Summarized using GPT-3.5-turbo
Append: [The Few-shot Dilemma: Over-prompting Large Language Models](https://arxiv.org/abs/2509.13196)
Token length: 1738
Summarized using GPT-3.5-turbo
Append: [Evaluating LLM Alignment on Personality Inference from Real-World Interview Data](https://arxiv.org/abs/2509.13244)
Token length: 1021
Summarized using GPT-3.5-turbo
Append: [ChartGaze: Enhancing Chart Understanding in LVLMs with Eye-Tracking Guided Attention Refinement](https://arxiv.org/abs/2509.13282)
Token length: 1400
Summarized using GPT-3.5-turbo
Append: [WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents](https://arxiv.org/abs/2509.13309)
Token length: 1115
Summarized using GPT-3.5-turbo
Append: [Scaling Agents via Continual Pre-training](https://arxiv.org/abs/2509.13310)
Token length: 1350
Summarized using GPT-3.5-turbo
Append: [Towards General Agentic Intelligence via Environment Scaling](https://arxiv.org/abs/2509.13311)
Token length: 1399
Summarized using GPT-3.5-turbo
Append: [WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research](https://arxiv.org/abs/2509.13312)
Token length: 1352
Summarized using GPT-3.5-turbo
Append: [ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization](https://arxiv.org/abs/2509.13313)
Token length: 1151
Summarized using GPT-3.5-turbo
Append: [Do Natural Language Descriptions of Model Activations Convey Privileged Information?](https://arxiv.org/abs/2509.13316)
Token length: 1375
Summarized using GPT-3.5-turbo
Append: [MEUV: Achieving Fine-Grained Capability Activation in Large Language Models via Mutually Exclusive Unlock Vectors](https://arxiv.org/abs/2509.12221)
Token length: 883
Summarized using GPT-3.5-turbo
Append: [Humor in Pixels: Benchmarking Large Multimodal Models Understanding of Online Comics](https://arxiv.org/abs/2509.12248)
Token length: 1534
Summarized using GPT-3.5-turbo
Append: [LLMAP: LLM-Assisted Multi-Objective Route Planning with User Preferences](https://arxiv.org/abs/2509.12273)
Token length: 729
Summarized using GPT-3.5-turbo
Append: [Exact Coset Sampling for Quantum Lattice Algorithms](https://arxiv.org/abs/2509.12341)
Token length: 897
Summarized using GPT-3.5-turbo
Append: [Small Models, Big Results: Achieving Superior Intent Extraction through Decomposition](https://arxiv.org/abs/2509.12423)
Append: [Context-Aware Language Models for Forecasting Market Impact from Sequences of Financial News](https://arxiv.org/abs/2509.12519)
Append: [The Adaptation Paradox: Agency vs. Mimicry in Companion Chatbots](https://arxiv.org/abs/2509.12525)
Append: [LEAF: Knowledge Distillation of Text Embedding Models with Teacher-Aligned Representations](https://arxiv.org/abs/2509.12539)
Append: [Yet Another Watermark for Large Language Models](https://arxiv.org/abs/2509.12574)
Append: [Match Chat: Real Time Generative AI and Generative Computing for Tennis](https://arxiv.org/abs/2509.12592)
Append: [The Better You Learn, The Smarter You Prune: Towards Efficient Vision-language-action Models via Differentiable Token Pruning](https://arxiv.org/abs/2509.12594)
Append: [DaSAThco: Data-Aware SAT Heuristics Combinations Optimization via Large Language Models](https://arxiv.org/abs/2509.12602)
Append: [A Novel Recurrent Neural Network Framework for Prediction and Treatment of Oncogenic Mutation Progression](https://arxiv.org/abs/2509.12732)
Append: [Zero-shot Graph Reasoning via Retrieval Augmented Framework with LLMs](https://arxiv.org/abs/2509.12743)
Append: [Similarity-Distance-Magnitude Activations](https://arxiv.org/abs/2509.12760)
Append: [InfoGain-RAG: Boosting Retrieval-Augmented Generation via Document Information Gain-based Reranking and Filtering](https://arxiv.org/abs/2509.12765)
Append: [Rethinking the Evaluation of Alignment Methods: Insights into Diversity, Generalisation, and Safety](https://arxiv.org/abs/2509.12936)
Append: [Jailbreaking Large Language Models Through Content Concretization](https://arxiv.org/abs/2509.12937)
Append: [When Inverse Data Outperforms: Exploring the Pitfalls of Mixed Data in Multi-Stage Fine-Tuning](https://arxiv.org/abs/2509.13079)
Append: [Textarium: Entangling Annotation, Abstraction and Argument](https://arxiv.org/abs/2509.13191)
Append: [Podcasts as a Medium for Participation in Collective Action: A Case Study of Black Lives Matter](https://arxiv.org/abs/2509.13197)
Append: [HARMONIC: A Content-Centric Cognitive Robotic Architecture](https://arxiv.org/abs/2509.13279)
Append: [RepIt: Representing Isolated Targets to Steer Language Models](https://arxiv.org/abs/2509.13281)
Append: [WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning](https://arxiv.org/abs/2509.13305)
Append: [Do predictability factors towards signing avatars hold across cultures?](https://arxiv.org/abs/2307.02103)
Append: [Emphasising Structured Information: Integrating Abstract Meaning Representation into LLMs for Enhanced Open-Domain Dialogue Evaluation](https://arxiv.org/abs/2404.01129)
Append: [Cutting Through the Noise: Boosting LLM Performance on Math Word Problems](https://arxiv.org/abs/2406.15444)
Append: [Context-Aware Membership Inference Attacks against Pre-trained Large Language Models](https://arxiv.org/abs/2409.13745)
Append: [Responsible AI in NLP: GUS-Net Span-Level Bias Detection Dataset and Benchmark for Generalizations, Unfairness, and Stereotypes](https://arxiv.org/abs/2410.08388)
Append: [Can Code-Switched Texts Activate a Knowledge Switch in LLMs? A Case Study on English-Korean Code-Switching](https://arxiv.org/abs/2410.18436)
Append: [Break the Checkbox: Challenging Closed-Style Evaluations of Cultural Alignment in LLMs](https://arxiv.org/abs/2502.08045)
Append: [TokenSkip: Controllable Chain-of-Thought Compression in LLMs](https://arxiv.org/abs/2502.12067)
Append: [How Much Do LLMs Hallucinate across Languages? On Multilingual Estimation of LLM Hallucination in the Wild](https://arxiv.org/abs/2502.12769)
Append: [Teaching Your Models to Understand Code via Focal Preference Alignment](https://arxiv.org/abs/2503.02783)
Append: [Sketch-of-Thought: Efficient LLM Reasoning with Adaptive Cognitive-Inspired Sketching](https://arxiv.org/abs/2503.05179)
Append: [Dynamic Relation Inference via Verb Embeddings](https://arxiv.org/abs/2503.13021)
Append: [Why Stop at One Error? Benchmarking LLMs as Data Science Code Debuggers for Multi-Hop and Multi-Bug Errors](https://arxiv.org/abs/2503.22388)
Append: [Is the Top Still Spinning? Evaluating Subjectivity in Narrative Understanding](https://arxiv.org/abs/2504.01132)
Append: [Do Large Language Models Truly Grasp Addition? A Rule-Focused Diagnostic Using Two-Integer Arithmetic](https://arxiv.org/abs/2504.05262)
Append: [Game-RL: Synthesizing Verifiable Game Tasks at Scale to Boost VLMs General Reasoning](https://arxiv.org/abs/2505.13886)
Append: [The Strawberry Problem: Emergence of Character-level Understanding in Tokenized Language Models](https://arxiv.org/abs/2505.14172)
Append: [Keep Security! Benchmarking Security Policy Preservation in Large Language Model Contexts Against Indirect Attacks in Question Answering](https://arxiv.org/abs/2505.15805)
Append: [From Surveys to Narratives: Rethinking Cultural Value Adaptation in LLMs](https://arxiv.org/abs/2505.16408)
Append: [Reading Between the Prompts: How Stereotypes Shape LLM's Implicit Personalization](https://arxiv.org/abs/2505.16467)
Append: [PatentScore: Multi-dimensional Evaluation of LLM-Generated Patent Claims](https://arxiv.org/abs/2505.19345)
Append: [Counterfactual Simulatability of LLM Explanations for Generation Tasks](https://arxiv.org/abs/2505.21740)
Append: [UniversalCEFR: Enabling Open Multilingual Research on Language Proficiency Assessment](https://arxiv.org/abs/2506.01419)
Append: [From Understanding to Generation: An Efficient Shortcut for Evaluating Language Models](https://arxiv.org/abs/2506.03592)
Append: [EIFBENCH: Extremely Complex Instruction Following Benchmark for Large Language Models](https://arxiv.org/abs/2506.08375)
Append: [Efficient Context Selection for Long-Context QA: No Tuning, No Iteration, Just Adaptive-$k$](https://arxiv.org/abs/2506.08479)
Append: [References Matter: Investigating the Impact of Reference Set Variation on Summarization Evaluation](https://arxiv.org/abs/2506.14335)
Append: [Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language Models: An Empirical Evaluation](https://arxiv.org/abs/2506.17088)
Append: [TAPS: Tool-Augmented Personalisation via Structured Tagging](https://arxiv.org/abs/2506.20409)
Append: [UtterTune: LoRA-Based Target-Language Pronunciation Edit and Control in Multilingual Text-to-Speech](https://arxiv.org/abs/2508.09767)
Append: [OpenWHO: A Document-Level Parallel Corpus for Health Translation in Low-Resource Languages](https://arxiv.org/abs/2508.16048)
Append: [Understanding and Leveraging the Expert Specialization of Context Faithfulness in Mixture-of-Experts LLMs](https://arxiv.org/abs/2508.19594)
Append: [Concurrent Linguistic Error Detection (CLED): a New Methodology for Error Detection in Large Language Models](https://arxiv.org/abs/2403.16393)
Append: [The Belief State Transformer](https://arxiv.org/abs/2410.23506)
Append: [Probing LLM Hallucination from Within: Perturbation-Driven Approach via Internal Knowledge](https://arxiv.org/abs/2411.09689)
Append: [Talking to DINO: Bridging Self-Supervised Vision Backbones with Language for Open-Vocabulary Segmentation](https://arxiv.org/abs/2411.19331)
Append: [SuPreME: A Supervised Pre-training Framework for Multimodal ECG Representation Learning](https://arxiv.org/abs/2502.19668)
Append: [Evaluating the Robustness of Open-Source Vision-Language Models to Domain Shift in Object Captioning](https://arxiv.org/abs/2506.19579)
Append: [Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection](https://arxiv.org/abs/2507.02844)
Append: [Teaching Vision-Language Models to Ask: Resolving Ambiguity in Visual Questions](https://arxiv.org/abs/2507.13773)
Append: [GPT-4.1 Sets the Standard in Automated Experiment Design Using Novel Python Libraries](https://arxiv.org/abs/2508.00033)
Append: [IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding](https://arxiv.org/abs/2508.09456)
append_entries: 111
Finish: 2025-09-17 04:24:22.937219
------------------------------------------------------
Started: 2025-09-17 06:24:20.965071
Existing_entries: 1111
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-17 06:24:21.309987
------------------------------------------------------
Started: 2025-09-17 08:20:55.556315
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-17 08:20:55.848886
------------------------------------------------------
Started: 2025-09-17 10:16:59.513420
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-17 10:16:59.806411
------------------------------------------------------
Started: 2025-09-17 12:33:26.949388
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-17 12:33:27.378539
------------------------------------------------------
Started: 2025-09-17 14:15:50.238566
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-17 14:15:50.534150
------------------------------------------------------
Started: 2025-09-17 16:20:15.009169
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-17 16:20:15.322637
------------------------------------------------------
Started: 2025-09-17 18:22:47.531521
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-17 18:22:47.829751
------------------------------------------------------
Started: 2025-09-17 20:17:19.812573
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-17 20:17:20.112939
------------------------------------------------------
Started: 2025-09-17 22:13:47.893142
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-17 22:13:48.178079
------------------------------------------------------
Started: 2025-09-18 01:11:41.187990
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-18 01:11:41.483775
------------------------------------------------------
Started: 2025-09-18 02:54:46.660306
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-18 02:54:46.953567
------------------------------------------------------
Started: 2025-09-18 04:23:29.477666
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 944
Summarized using GPT-3.5-turbo
Append: [Gender-Neutral Rewriting in Italian: Models, Approaches, and Trade-offs](https://arxiv.org/abs/2509.13480)
Token length: 1305
Summarized using GPT-3.5-turbo
Append: [Op-Fed: Opinion, Stance, and Monetary Policy Annotations on FOMC Transcripts Using Active Learning](https://arxiv.org/abs/2509.13539)
Token length: 1261
Summarized using GPT-3.5-turbo
Append: [Overview of Dialog System Evaluation Track: Dimensionality, Language, Culture and Safety at DSTC 12](https://arxiv.org/abs/2509.13569)
Token length: 1265
Summarized using GPT-3.5-turbo
Append: [Latent Traits and Cross-Task Transfer: Deconstructing Dataset Interactions in LLM Fine-tuning](https://arxiv.org/abs/2509.13624)
Token length: 1115
Summarized using GPT-3.5-turbo
Append: [Sparse Neurons Carry Strong Signals of Question Ambiguity in LLMs](https://arxiv.org/abs/2509.13664)
Token length: 1565
Summarized using GPT-3.5-turbo
Append: [CL$^2$GEC: A Multi-Discipline Benchmark for Continual Learning in Chinese Literature Grammatical Error Correction](https://arxiv.org/abs/2509.13672)
Token length: 1532
Summarized using GPT-3.5-turbo
Append: [AgentCTG: Harnessing Multi-Agent Collaboration for Fine-Grained Precise Control in Text Generation](https://arxiv.org/abs/2509.13677)
Token length: 1209
Summarized using GPT-3.5-turbo
Append: [Improving Context Fidelity via Native Retrieval-Augmented Reasoning](https://arxiv.org/abs/2509.13683)
Token length: 1061
Summarized using GPT-3.5-turbo
Append: [Can Large Language Models Robustly Perform Natural Language Inference for Japanese Comparatives?](https://arxiv.org/abs/2509.13695)
Token length: 556
Summarized using GPT-3.5-turbo
Append: [Integrating Text and Time-Series into (Large) Language Models to Predict Medical Outcomes](https://arxiv.org/abs/2509.13696)
Token length: 1180
Summarized using GPT-3.5-turbo
Append: [DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models](https://arxiv.org/abs/2509.13702)
Token length: 1951
Summarized using GPT-3.5-turbo
Append: [Automated Triaging and Transfer Learning of Incident Learning Safety Reports Using Large Language Representational Models](https://arxiv.org/abs/2509.13706)
Token length: 1304
Summarized using GPT-3.5-turbo
Append: [DSPC: Dual-Stage Progressive Compression Framework for Efficient Long-Context Reasoning](https://arxiv.org/abs/2509.13723)
Token length: 1091
Summarized using GPT-3.5-turbo
Append: [Implementing a Logical Inference System for Japanese Comparatives](https://arxiv.org/abs/2509.13734)
Token length: 1086
Summarized using GPT-3.5-turbo
Append: [Exploring Data and Parameter Efficient Strategies for Arabic Dialect Identifications](https://arxiv.org/abs/2509.13775)
Token length: 1051
Summarized using GPT-3.5-turbo
Append: [Teaching According to Talents! Instruction Tuning LLMs with Competence-Aware Curriculum Learning](https://arxiv.org/abs/2509.13790)
Token length: 789
Summarized using GPT-3.5-turbo
Append: [Measuring Gender Bias in Job Title Matching for Grammatical Gender Languages](https://arxiv.org/abs/2509.13803)
Token length: 1663
Summarized using GPT-3.5-turbo
Append: [Geometric Uncertainty for Detecting and Correcting Hallucinations in LLMs](https://arxiv.org/abs/2509.13813)
Token length: 982
Summarized using GPT-3.5-turbo
Append: [Findings of the Third Automatic Minuting (AutoMin) Challenge](https://arxiv.org/abs/2509.13814)
Token length: 1455
Summarized using GPT-3.5-turbo
Append: [Large Language Models Discriminate Against Speakers of German Dialects](https://arxiv.org/abs/2509.13835)
Token length: 1538
Summarized using GPT-3.5-turbo
Append: [Do LLMs Align Human Values Regarding Social Biases? Judging and Explaining Social Biases with LLMs](https://arxiv.org/abs/2509.13869)
Token length: 1241
Summarized using GPT-3.5-turbo
Append: [Combining Evidence and Reasoning for Biomedical Fact-Checking](https://arxiv.org/abs/2509.13879)
Token length: 1239
Summarized using GPT-3.5-turbo
Append: [Combating Biomedical Misinformation through Multi-modal Claim Detection and Evidence-based Verification](https://arxiv.org/abs/2509.13888)
Token length: 1182
Summarized using GPT-3.5-turbo
Append: [Do Large Language Models Understand Word Senses?](https://arxiv.org/abs/2509.13905)
Token length: 1074
Summarized using GPT-3.5-turbo
Append: [Linguistic Nepotism: Trading-off Quality for Language Preference in Multilingual RAG](https://arxiv.org/abs/2509.13930)
Token length: 885
Summarized using GPT-3.5-turbo
Append: [Long-context Reference-based MT Quality Estimation](https://arxiv.org/abs/2509.13980)
Token length: 1169
Summarized using GPT-3.5-turbo
Append: [Slim-SC: Thought Pruning for Efficient Scaling with Self-Consistency](https://arxiv.org/abs/2509.13990)
Token length: 1325
Summarized using GPT-3.5-turbo
Append: [Early Stopping Chain-of-thoughts in Large Language Models](https://arxiv.org/abs/2509.14004)
Token length: 964
Summarized using GPT-3.5-turbo
Append: [Hala Technical Report: Building Arabic-Centric Instruction & Translation Models at Scale](https://arxiv.org/abs/2509.14008)
Token length: 1254
Summarized using GPT-3.5-turbo
Append: [Audio-Based Crowd-Sourced Evaluation of Machine Translation Quality](https://arxiv.org/abs/2509.14023)
Token length: 1176
Summarized using GPT-3.5-turbo
Append: [You Are What You Train: Effects of Data Composition on Training Context-aware Machine Translation Models](https://arxiv.org/abs/2509.14031)
Token length: 1275
Summarized using GPT-3.5-turbo
Append: [Enhancing Multi-Agent Debate System Performance via Confidence Expression](https://arxiv.org/abs/2509.14034)
Token length: 1556
Summarized using GPT-3.5-turbo
Append: [SSL-SSAW: Self-Supervised Learning with Sigmoid Self-Attention Weighting for Question-Based Sign Language Translation](https://arxiv.org/abs/2509.14036)
Token length: 1227
Summarized using GPT-3.5-turbo
Append: [Canary-1B-v2 & Parakeet-TDT-0.6B-v3: Efficient and High-Performance Models for Multilingual ASR and AST](https://arxiv.org/abs/2509.14128)
Token length: 1001
Summarized using GPT-3.5-turbo
Append: [CS-FLEURS: A Massively Multilingual and Code-Switched Speech Dataset](https://arxiv.org/abs/2509.14161)
Token length: 1375
Summarized using GPT-3.5-turbo
Append: [AssoCiAm: A Benchmark for Evaluating Association Thinking while Circumventing Ambiguity](https://arxiv.org/abs/2509.14171)
Token length: 1213
Summarized using GPT-3.5-turbo
Append: [Synthesizing Behaviorally-Grounded Reasoning Chains: A Data-Generation Framework for Personal Finance LLMs](https://arxiv.org/abs/2509.14180)
Token length: 1361
Summarized using GPT-3.5-turbo
Append: [Framing Migration: A Computational Analysis of UK Parliamentary Discourse](https://arxiv.org/abs/2509.14197)
Token length: 1320
Summarized using GPT-3.5-turbo
Append: [Apertus: Democratizing Open and Compliant LLMs for Global Language Environments](https://arxiv.org/abs/2509.14233)
Token length: 1358
Summarized using GPT-3.5-turbo
Append: [An AI-Powered Framework for Analyzing Collective Idea Evolution in Deliberative Assemblies](https://arxiv.org/abs/2509.12577)
Token length: 1683
Summarized using GPT-3.5-turbo
Append: [Explicit Reasoning Makes Better Judges: A Systematic Study on Accuracy, Efficiency, and Robustness](https://arxiv.org/abs/2509.13332)
Token length: 1965
Summarized using GPT-3.5-turbo
Append: [Accuracy Paradox in Large Language Models: Regulating Hallucination Risks in Generative AI](https://arxiv.org/abs/2509.13345)
Token length: 1620
Summarized using GPT-3.5-turbo
Append: [Teaching LLMs to Plan: Logical Chain-of-Thought Instruction Tuning for Symbolic Planning](https://arxiv.org/abs/2509.13351)
Token length: 1507
Summarized using GPT-3.5-turbo
Append: [CogniAlign: Survivability-Grounded Multi-Agent Moral Reasoning for Safe and Transparent AI](https://arxiv.org/abs/2509.13356)
Token length: 795
Summarized using GPT-3.5-turbo
Append: [TICL: Text-Embedding KNN For Speech In-Context Learning Unlocks Speech Recognition Abilities of Large Multimodal Models](https://arxiv.org/abs/2509.13395)
Token length: 1201
Summarized using GPT-3.5-turbo
Append: [SteeringControl: Holistic Evaluation of Alignment Steering in LLMs](https://arxiv.org/abs/2509.13450)
Token length: 1307
Summarized using GPT-3.5-turbo
Append: [Annotating Satellite Images of Forests with Keywords from a Specialized Corpus in the Context of Change Detection](https://arxiv.org/abs/2509.13586)
Token length: 1184
Summarized using GPT-3.5-turbo
Append: [See, Think, Act: Teaching Multimodal Agents to Effectively Interact with GUI by Identifying Toggles](https://arxiv.org/abs/2509.13615)
Token length: 1284
Summarized using GPT-3.5-turbo
Append: [Privacy-Aware In-Context Learning for Large Language Models](https://arxiv.org/abs/2509.13625)
Token length: 1713
Summarized using GPT-3.5-turbo
Append: [THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning](https://arxiv.org/abs/2509.13761)
Append: [Diving into Mitigating Hallucinations from a Vision Perspective for Large Vision-Language Models](https://arxiv.org/abs/2509.13836)
Append: [Noise Supervised Contrastive Learning and Feature-Perturbed for Anomalous Sound Detection](https://arxiv.org/abs/2509.13853)
Append: [An Empirical Study on Failures in Automated Issue Solving](https://arxiv.org/abs/2509.13941)
Append: [Enhancing Time Awareness in Generative Recommendation](https://arxiv.org/abs/2509.13957)
Append: [Exploring Major Transitions in the Evolution of Biological Cognition With Artificial Neural Networks](https://arxiv.org/abs/2509.13968)
Append: [A TRRIP Down Memory Lane: Temperature-Based Re-Reference Interval Prediction For Instruction Caching](https://arxiv.org/abs/2509.14041)
Append: [Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A Self-Optimizing Framework](https://arxiv.org/abs/2509.14093)
Append: [When Avatars Have Personality: Effects on Engagement and Communication in Immersive Medical Training](https://arxiv.org/abs/2509.14132)
Append: [Dense Video Understanding with Gated Residual Tokenization](https://arxiv.org/abs/2509.14199)
Append: [GEM-Bench: A Benchmark for Ad-Injected Response Generation within Generative Engine Marketing](https://arxiv.org/abs/2509.14221)
Append: [Language models' activations linearly encode training-order recency](https://arxiv.org/abs/2509.14223)
Append: [Large Language Models for Information Retrieval: A Survey](https://arxiv.org/abs/2308.07107)
Append: [Annotation-Efficient Language Model Alignment via Diverse and Representative Response Texts](https://arxiv.org/abs/2405.13541)
Append: [Database-Augmented Query Representation for Information Retrieval](https://arxiv.org/abs/2406.16013)
Append: [NeedleBench: Evaluating LLM Retrieval and Reasoning Across Varying Information Densities](https://arxiv.org/abs/2407.11963)
Append: [Contextual modulation of language comprehension in a dynamic neural model of lexical meaning](https://arxiv.org/abs/2407.14701)
Append: [MAgICoRe: Multi-Agent, Iterative, Coarse-to-Fine Refinement for Reasoning](https://arxiv.org/abs/2409.12147)
Append: [DAVIS: Planning Agent with Knowledge Graph-Powered Inner Monologue](https://arxiv.org/abs/2410.09252)
Append: [Mirror-Consistency: Harnessing Inconsistency in Majority Voting](https://arxiv.org/abs/2410.10857)
Append: [Unlocking Legal Knowledge: A Multilingual Dataset for Judicial Summarization in Switzerland](https://arxiv.org/abs/2410.13456)
Append: [KBM: Delineating Knowledge Boundary for Adaptive Retrieval in Large Language Models](https://arxiv.org/abs/2411.06207)
Append: [Human-in-the-Loop Generation of Adversarial Texts: A Case Study on Tibetan Script](https://arxiv.org/abs/2412.12478)
Append: [Enhancing the De-identification of Personally Identifiable Information in Educational Data](https://arxiv.org/abs/2501.09765)
Append: [Beyond checkmate: exploring the creative chokepoints in AI text](https://arxiv.org/abs/2501.19301)
Append: [Forget What You Know about LLMs Evaluations -- LLMs are Like a Chameleon](https://arxiv.org/abs/2502.07445)
Append: [LogiDynamics: Unraveling the Dynamics of Inductive, Abductive and Deductive Logical Inferences in LLM Reasoning](https://arxiv.org/abs/2502.11176)
Append: [Mind the Style Gap: Meta-Evaluation of Style and Attribute Transfer Metrics](https://arxiv.org/abs/2502.15022)
Append: [What's Not Said Still Hurts: A Description-Based Evaluation Framework for Measuring Social Bias in LLMs](https://arxiv.org/abs/2502.19749)
Append: [COMI-LINGUA: Expert Annotated Large-Scale Dataset for Multitask NLP in Hindi-English Code-Mixing](https://arxiv.org/abs/2503.21670)
Append: [ClonEval: An Open Voice Cloning Benchmark](https://arxiv.org/abs/2504.20581)
Append: [CAMEO: Collection of Multilingual Emotional Speech Corpora](https://arxiv.org/abs/2505.11051)
Append: [From n-gram to Attention: How Model Architectures Learn and Propagate Bias in Language Modeling](https://arxiv.org/abs/2505.12381)
Append: [From Automation to Autonomy: A Survey on Large Language Models in Scientific Discovery](https://arxiv.org/abs/2505.13259)
Append: [Does Localization Inform Unlearning? A Rigorous Examination of Local Parameter Attribution for Knowledge Unlearning in Language Models](https://arxiv.org/abs/2505.16252)
Append: [MAVL: A Multilingual Audio-Video Lyrics Dataset for Animated Song Translation](https://arxiv.org/abs/2505.18614)
Append: [SCRum-9: Multilingual Stance Classification over Rumours on Social Media](https://arxiv.org/abs/2505.18916)
Append: [Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint](https://arxiv.org/abs/2505.23759)
Append: [Calibrating LLMs for Text-to-SQL Parsing by Leveraging Sub-clause Frequencies](https://arxiv.org/abs/2505.23804)
Append: [Benchmarking Large Language Models for Cryptanalysis and Side-Channel Vulnerabilities](https://arxiv.org/abs/2505.24621)
Append: [Sarc7: Evaluating Sarcasm Detection and Generation with Seven Types and Emotion-Informed Techniques](https://arxiv.org/abs/2506.00658)
Append: [FroM: Frobenius Norm-Based Data-Free Adaptive Model Merging](https://arxiv.org/abs/2506.02478)
Append: [A Culturally-diverse Multilingual Multimodal Video Benchmark & Model](https://arxiv.org/abs/2506.07032)
Append: [Table-Text Alignment: Explaining Claim Verification Against Tables in Scientific Papers](https://arxiv.org/abs/2506.10486)
Append: [FinCoT: Grounding Chain-of-Thought in Expert Financial Reasoning](https://arxiv.org/abs/2506.16123)
Append: [SpeechRole: A Large-Scale Dataset and Benchmark for Evaluating Speech Role-Playing Agents](https://arxiv.org/abs/2508.02013)
Append: [Mitigating Attention Hacking in Preference-Based Reward Modeling via Interaction Distillation](https://arxiv.org/abs/2508.02618)
Append: [Self-Guided Function Calling in Large Language Models via Stepwise Experience Recall](https://arxiv.org/abs/2508.15214)
Append: [Position Bias Mitigates Position Bias:Mitigate Position Bias Through Inter-Position Knowledge Distillation](https://arxiv.org/abs/2508.15709)
Append: [Language Models Identify Ambiguities and Exploit Loopholes](https://arxiv.org/abs/2508.19546)
Append: [How Does Cognitive Bias Affect Large Language Models? A Case Study on the Anchoring Effect in Price Negotiation Simulations](https://arxiv.org/abs/2508.21137)
Append: [Assessing Large Language Models on Islamic Legal Reasoning: Evidence from Inheritance Law Evaluation](https://arxiv.org/abs/2509.01081)
Append: [An Attention-Based Denoising Framework for Personality Detection in Social Media Texts](https://arxiv.org/abs/2311.09945)
Append: [Video-Language Critic: Transferable Reward Functions for Language-Conditioned Robotics](https://arxiv.org/abs/2405.19988)
Append: [xGen-MM (BLIP-3): A Family of Open Large Multimodal Models](https://arxiv.org/abs/2408.08872)
Append: [Privately Learning from Graphs with Applications in Fine-tuning Large Language Models](https://arxiv.org/abs/2410.08299)
Append: [KALL-E:Autoregressive Speech Synthesis with Next-Distribution Prediction](https://arxiv.org/abs/2412.16846)
Append: [A Comprehensive Survey on the Trustworthiness of Large Language Models in Healthcare](https://arxiv.org/abs/2502.15871)
Append: [Structured Preference Optimization for Vision-Language Long-Horizon Task Planning](https://arxiv.org/abs/2502.20742)
Append: [Out-of-Context Reasoning in Large Language Models](https://arxiv.org/abs/2503.10408)
Append: [MythTriage: Scalable Detection of Opioid Use Disorder Myths on a Video-Sharing Platform](https://arxiv.org/abs/2506.00308)
Append: [Posterior-GRPO: Rewarding Reasoning Processes in Code Generation](https://arxiv.org/abs/2508.05170)
Append: [Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and Vision](https://arxiv.org/abs/2508.05606)
Append: [Singular Value Few-shot Adaptation of Vision-Language Models](https://arxiv.org/abs/2509.03740)
append_entries: 113
Finish: 2025-09-18 04:25:32.763602
------------------------------------------------------
Started: 2025-09-18 06:24:02.663813
Existing_entries: 1113
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-18 06:24:02.975735
------------------------------------------------------
Started: 2025-09-18 08:20:36.344999
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-18 08:20:36.631299
------------------------------------------------------
Started: 2025-09-18 10:16:54.665591
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-18 10:16:55.040164
------------------------------------------------------
Started: 2025-09-18 12:32:43.749301
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-18 12:32:44.090575
------------------------------------------------------
Started: 2025-09-18 14:16:32.830453
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-18 14:16:33.141476
------------------------------------------------------
Started: 2025-09-18 16:19:30.672220
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-18 16:19:30.998750
------------------------------------------------------
Started: 2025-09-18 18:23:34.544813
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-18 18:23:34.855438
------------------------------------------------------
Started: 2025-09-18 20:18:03.506184
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-18 20:18:03.817462
------------------------------------------------------
Started: 2025-09-18 22:13:12.954671
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-18 22:13:13.285913
------------------------------------------------------
Started: 2025-09-19 01:13:57.375333
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-19 01:13:57.702011
------------------------------------------------------
Started: 2025-09-19 02:58:29.583260
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-19 02:58:29.876911
------------------------------------------------------
Started: 2025-09-19 04:22:31.492009
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1095
Summarized using GPT-3.5-turbo
Append: [Tokenization Strategies for Low-Resource Agglutinative Languages in Word2Vec: Case Study on Turkish and Finnish](https://arxiv.org/abs/2509.14238)
Token length: 1347
Summarized using GPT-3.5-turbo
Append: [Advancing Conversational AI with Shona Slang: A Dataset and Hybrid Model for Digital Inclusion](https://arxiv.org/abs/2509.14249)
Token length: 1331
Summarized using GPT-3.5-turbo
Append: [The meaning of prompts and the prompts of meaning: Semiotic reflections and modelling](https://arxiv.org/abs/2509.14250)
Token length: 1174
Summarized using GPT-3.5-turbo
Append: [LLM-JEPA: Large Language Models Meet Joint Embedding Predictive Architectures](https://arxiv.org/abs/2509.14252)
Token length: 1092
Summarized using GPT-3.5-turbo
Append: [CrossPT: Exploring Cross-Task Transferability through Multi-Task Prompt Tuning](https://arxiv.org/abs/2509.14253)
Token length: 1685
Summarized using GPT-3.5-turbo
Append: [Hallucination Detection with the Internal Layers of LLMs](https://arxiv.org/abs/2509.14254)
Token length: 1447
Summarized using GPT-3.5-turbo
Append: [Opening the Black Box: Interpretable LLMs via Semantic Resonance Architecture](https://arxiv.org/abs/2509.14255)
Token length: 1332
Summarized using GPT-3.5-turbo
Append: [JU-NLP at Touch\'e: Covert Advertisement in Conversational AI-Generation and Detection Strategies](https://arxiv.org/abs/2509.14256)
Token length: 1106
Summarized using GPT-3.5-turbo
Append: [From Correction to Mastery: Reinforced Distillation of Large Language Model Agents](https://arxiv.org/abs/2509.14257)
Token length: 1064
Summarized using GPT-3.5-turbo
Append: [Persuasive or Neutral? A Field Experiment on Generative AI in Online Travel Planning](https://arxiv.org/abs/2509.14259)
Token length: 888
Summarized using GPT-3.5-turbo
Append: [Shutdown Resistance in Large Language Models](https://arxiv.org/abs/2509.14260)
Token length: 1173
Summarized using GPT-3.5-turbo
Append: [Refining Syntactic Distinctions Using Decision Trees: A Paper on Postnominal 'That' in Complement vs. Relative Clauses](https://arxiv.org/abs/2509.14261)
Token length: 1135
Summarized using GPT-3.5-turbo
Append: [Context-Enhanced Granular Edit Representation for Efficient and Accurate ASR Post-editing](https://arxiv.org/abs/2509.14263)
Token length: 1369
Summarized using GPT-3.5-turbo
Append: [Defining, Understanding, and Detecting Online Toxicity: Challenges and Machine Learning Approaches](https://arxiv.org/abs/2509.14264)
Token length: 1205
Summarized using GPT-3.5-turbo
Append: [Efficient Hate Speech Detection: Evaluating 38 Models from Traditional Methods to Transformers](https://arxiv.org/abs/2509.14266)
Token length: 1024
Summarized using GPT-3.5-turbo
Append: [Graph-Enhanced Retrieval-Augmented Question Answering for E-Commerce Customer Support](https://arxiv.org/abs/2509.14267)
Token length: 1684
Summarized using GPT-3.5-turbo
Append: [DetectAnyLLM: Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models](https://arxiv.org/abs/2509.14268)
Token length: 1525
Summarized using GPT-3.5-turbo
Append: [SparseDoctor: Towards Efficient Chat Doctor with Mixture of Experts Enhanced Large Language Models](https://arxiv.org/abs/2509.14269)
Token length: 1411
Summarized using GPT-3.5-turbo
Append: [SpeechWeave: Diverse Multilingual Synthetic Text & Audio Data Generation Pipeline for Training Text to Speech Models](https://arxiv.org/abs/2509.14270)
Token length: 541
Summarized using GPT-3.5-turbo
Append: [Predicting Antibiotic Resistance Patterns Using Sentence-BERT: A Machine Learning Approach](https://arxiv.org/abs/2509.14283)
Token length: 1328
Summarized using GPT-3.5-turbo
Append: [Annotating Training Data for Conditional Semantic Textual Similarity Measurement using Large Language Models](https://arxiv.org/abs/2509.14399)
Token length: 1769
Summarized using GPT-3.5-turbo
Append: [Adding LLMs to the psycholinguistic norming toolbox: A practical guide to getting the most out of human ratings](https://arxiv.org/abs/2509.14405)
Token length: 1343
Summarized using GPT-3.5-turbo
Append: [Causal-Counterfactual RAG: The Integration of Causal-Counterfactual Reasoning into RAG](https://arxiv.org/abs/2509.14435)
Token length: 1086
Summarized using GPT-3.5-turbo
Append: [Simulating a Bias Mitigation Scenario in Large Language Models](https://arxiv.org/abs/2509.14438)
Token length: 943
Summarized using GPT-3.5-turbo
Append: [Correct-Detect: Balancing Performance and Ambiguity Through the Lens of Coreference Resolution in LLMs](https://arxiv.org/abs/2509.14456)
Token length: 1599
Summarized using GPT-3.5-turbo
Append: [Not What the Doctor Ordered: Surveying LLM-based De-identification and Quantifying Clinical Information Loss](https://arxiv.org/abs/2509.14464)
Token length: 1135
Summarized using GPT-3.5-turbo
Append: [Ticket-Bench: A Kickoff for Multilingual and Regionalized Agent Evaluation](https://arxiv.org/abs/2509.14477)
Token length: 1172
Summarized using GPT-3.5-turbo
Append: [Estimating Semantic Alphabet Size for LLM Uncertainty Quantification](https://arxiv.org/abs/2509.14478)
Token length: 1172
Summarized using GPT-3.5-turbo
Append: [Process-Supervised Reinforcement Learning for Interactive Multimodal Tool-Use Agents](https://arxiv.org/abs/2509.14480)
Token length: 1428
Summarized using GPT-3.5-turbo
Append: [Translate, then Detect: Leveraging Machine Translation for Cross-Lingual Toxicity Classification](https://arxiv.org/abs/2509.14493)
Token length: 1244
Summarized using GPT-3.5-turbo
Append: [Introducing OmniGEC: A Silver Multilingual Dataset for Grammatical Error Correction](https://arxiv.org/abs/2509.14504)
Token length: 894
Summarized using GPT-3.5-turbo
Append: [From Turn-Taking to Synchronous Dialogue: A Survey of Full-Duplex Spoken Language Models](https://arxiv.org/abs/2509.14515)
Token length: 1062
Summarized using GPT-3.5-turbo
Append: [Delta Knowledge Distillation for Large Language Models](https://arxiv.org/abs/2509.14526)
Token length: 1463
Summarized using GPT-3.5-turbo
Append: [Catch Me If You Can? Not Yet: LLMs Still Struggle to Imitate the Implicit Writing Styles of Everyday Authors](https://arxiv.org/abs/2509.14543)
Token length: 1252
Summarized using GPT-3.5-turbo
Append: [Controlling Language Difficulty in Dialogues with Linguistic Features](https://arxiv.org/abs/2509.14545)
Token length: 1031
Summarized using GPT-3.5-turbo
Append: [Position: Thematic Analysis of Unstructured Clinical Transcripts with Large Language Models](https://arxiv.org/abs/2509.14597)
Token length: 1108
Summarized using GPT-3.5-turbo
Append: [Leveraging IndoBERT and DistilBERT for Indonesian Emotion Classification in E-Commerce Reviews](https://arxiv.org/abs/2509.14611)
Token length: 1064
Summarized using GPT-3.5-turbo
Append: [Reveal and Release: Iterative LLM Unlearning with Self-generated Data](https://arxiv.org/abs/2509.14624)
Token length: 1783
Summarized using GPT-3.5-turbo
Append: [SWE-QA: Can Language Models Answer Repository-level Code Questions?](https://arxiv.org/abs/2509.14635)
Token length: 1036
Summarized using GPT-3.5-turbo
Append: [MUSE: MCTS-Driven Red Teaming Framework for Enhanced Multi-Turn Dialogue Safety in Large Language Models](https://arxiv.org/abs/2509.14651)
Token length: 894
Summarized using GPT-3.5-turbo
Append: [UMA-Split: unimodal aggregation for both English and Mandarin non-autoregressive speech recognition](https://arxiv.org/abs/2509.14653)
Token length: 1632
Summarized using GPT-3.5-turbo
Append: [TableDART: Dynamic Adaptive Multi-Modal Routing for Table Understanding](https://arxiv.org/abs/2509.14671)
Token length: 1047
Summarized using GPT-3.5-turbo
Append: [HARNESS: Lightweight Distilled Arabic Speech Foundation Models](https://arxiv.org/abs/2509.14689)
Token length: 971
Summarized using GPT-3.5-turbo
Append: [From Ground Trust to Truth: Disparities in Offensive Language Judgments on Contemporary Korean Political Discourse](https://arxiv.org/abs/2509.14712)
Token length: 1534
Summarized using GPT-3.5-turbo
Append: [Decoupled Proxy Alignment: Mitigating Language Prior Conflict for Multimodal Alignment in MLLM](https://arxiv.org/abs/2509.14735)
Token length: 1663
Summarized using GPT-3.5-turbo
Append: [UnifiedVisual: A Framework for Constructing Unified Vision-Language Datasets](https://arxiv.org/abs/2509.14738)
Token length: 1227
Summarized using GPT-3.5-turbo
Append: [Evaluating Large Language Models for Cross-Lingual Retrieval](https://arxiv.org/abs/2509.14749)
Token length: 1291
Summarized using GPT-3.5-turbo
Append: [KAIO: A Collection of More Challenging Korean Questions](https://arxiv.org/abs/2509.14752)
Token length: 1386
Summarized using GPT-3.5-turbo
Append: [Reasoning over Boundaries: Enhancing Specification Alignment via Test-time Delibration](https://arxiv.org/abs/2509.14760)
Token length: 729
Summarized using GPT-3.5-turbo
Append: [SINAI at eRisk@CLEF 2023: Approaching Early Detection of Gambling with Natural Language Processing](https://arxiv.org/abs/2509.14797)
Append: [SINAI at eRisk@CLEF 2022: Approaching Early Detection of Gambling and Eating Disorders with Natural Language Processing](https://arxiv.org/abs/2509.14806)
Append: [ReCoVeR the Target Language: Language Steering without Sacrificing Task Performance](https://arxiv.org/abs/2509.14814)
Append: [LLM Agents at the Roundtable: A Multi-Perspective and Dialectical Reasoning Framework for Essay Scoring](https://arxiv.org/abs/2509.14834)
Append: [V-SEAM: Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models](https://arxiv.org/abs/2509.14837)
Append: [Empathy-R1: A Chain-of-Empathy and Reinforcement Learning Framework for Long-Form Mental Health Support](https://arxiv.org/abs/2509.14851)
Append: [Llama-Mimi: Speech Language Models with Interleaved Semantic and Acoustic Tokens](https://arxiv.org/abs/2509.14882)
Append: [A Multi-To-One Interview Paradigm for Efficient MLLM Evaluation](https://arxiv.org/abs/2509.14886)
Append: [FURINA: Free from Unmergeable Router via LINear Aggregation of mixed experts](https://arxiv.org/abs/2509.14900)
Append: [A Comparative Evaluation of Large Language Models for Persian Sentiment Analysis and Emotion Detection in Social Media Texts](https://arxiv.org/abs/2509.14922)
Append: [Patent Language Model Pretraining with ModernBERT](https://arxiv.org/abs/2509.14926)
Append: [Cross-Modal Knowledge Distillation for Speech Large Language Models](https://arxiv.org/abs/2509.14930)
Append: [Explicit vs. Implicit Biographies: Evaluating and Adapting LLM Information Extraction on Wikidata-Derived Texts](https://arxiv.org/abs/2509.14943)
Append: [Mind the Gap: A Closer Look at Tokenization for Multiple-Choice Question Answering with LLMs](https://arxiv.org/abs/2509.15020)
Append: [CLEAR: A Comprehensive Linguistic Evaluation of Argument Rewriting by Large Language Models](https://arxiv.org/abs/2509.15027)
Append: [Value-Guided KV Compression for LLMs via Approximated CUR Decomposition](https://arxiv.org/abs/2509.15038)
Append: [Can maiBERT Speak for Maithili?](https://arxiv.org/abs/2509.15048)
Append: [LLM-OREF: An Open Relation Extraction Framework Based on Large Language Models](https://arxiv.org/abs/2509.15089)
Append: [TextMine: LLM-Powered Knowledge Extraction for Humanitarian Mine Action](https://arxiv.org/abs/2509.15098)
Append: [Large Language Model probabilities cannot distinguish between possible and impossible language](https://arxiv.org/abs/2509.15114)
Append: [A1: Asynchronous Test-Time Scaling via Conformal Prediction](https://arxiv.org/abs/2509.15148)
Append: [SMARTER: A Data-efficient Framework to Improve Toxicity Detection with Explanation via Self-augmenting Large Language Models](https://arxiv.org/abs/2509.15174)
Append: [Fast and Fluent Diffusion Language Models via Convolutional Decoding and Rejective Fine-tuning](https://arxiv.org/abs/2509.15188)
Append: [Fair-GPTQ: Bias-Aware Quantization for Large Language Models](https://arxiv.org/abs/2509.15206)
Append: [What's the Best Way to Retrieve Slides? A Comparative Study of Multimodal, Caption-Based, and Hybrid Retrieval Techniques](https://arxiv.org/abs/2509.15211)
Append: [Assessing Historical Structural Oppression Worldwide via Rule-Guided Prompting of Large Language Models](https://arxiv.org/abs/2509.15216)
Append: [LNE-Blocking: An Efficient Framework for Contamination Mitigation Evaluation on Large Language Models](https://arxiv.org/abs/2509.15218)
Append: [Masked Diffusion Models as Energy Minimization](https://arxiv.org/abs/2509.13866)
Append: [FedMentor: Domain-Aware Differential Privacy for Heterogeneous Federated LLMs in Mental Health](https://arxiv.org/abs/2509.14275)
Append: [The Sum Leaks More Than Its Parts: Compositional Privacy Risks and Mitigations in Multi-Agent Collaboration](https://arxiv.org/abs/2509.14284)
Append: [From Capabilities to Performance: Evaluating Key Functional Properties of LLM Architectures in Penetration Testing](https://arxiv.org/abs/2509.14289)
Append: [A Simple and Efficient Jailbreak Method Exploiting LLMs' Helpfulness](https://arxiv.org/abs/2509.14297)
Append: [A Taxonomy of Prompt Defects in LLM Systems](https://arxiv.org/abs/2509.14404)
Append: [DeKeyNLU: Enhancing Natural Language to SQL Generation through Task Decomposition and Keyword Extraction](https://arxiv.org/abs/2509.14507)
Append: [LLM Jailbreak Detection for (Almost) Free!](https://arxiv.org/abs/2509.14558)
Append: [Towards Human-like Multimodal Conversational Agent by Generating Engaging Speech](https://arxiv.org/abs/2509.14627)
Append: [AgentCompass: Towards Reliable Evaluation of Agentic Workflows in Production](https://arxiv.org/abs/2509.14647)
Append: [Understanding the Thinking Process of Reasoning Models: A Perspective from Schoenfeld's Episode Theory](https://arxiv.org/abs/2509.14662)
Append: [Spatial Audio Motion Understanding and Reasoning](https://arxiv.org/abs/2509.14666)
Append: [ToolSample: Dual Dynamic Sampling Methods with Curriculum Learning for RL-based Tool Learning](https://arxiv.org/abs/2509.14718)
Append: [Frame Sampling Strategies Matter: A Benchmark for small vision language models](https://arxiv.org/abs/2509.14769)
Append: [MARIC: Multi-Agent Reasoning for Image Classification](https://arxiv.org/abs/2509.14860)
Append: [SynParaSpeech: Automated Synthesis of Paralinguistic Datasets for Speech Generation and Understanding](https://arxiv.org/abs/2509.14946)
Append: [TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference](https://arxiv.org/abs/2509.15110)
Append: [FCPE: A Fast Context-based Pitch Estimation Model](https://arxiv.org/abs/2509.15140)
Append: [Mind the Gap: Data Rewriting for Stable Off-Policy Supervised Fine-Tuning](https://arxiv.org/abs/2509.15157)
Append: [AIP: Subverting Retrieval-Augmented Generation via Adversarial Instructional Prompt](https://arxiv.org/abs/2509.15159)
Append: [An Evaluation-Centric Paradigm for Scientific Visualization Agents](https://arxiv.org/abs/2509.15160)
Append: [Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation](https://arxiv.org/abs/2509.15194)
Append: [FlowRL: Matching Reward Distributions for LLM Reasoning](https://arxiv.org/abs/2509.15207)
Append: [Fast Multipole Attention: A Scalable Multilevel Attention Mechanism for Text and Images](https://arxiv.org/abs/2310.11960)
Append: [The Art of Storytelling: Multi-Agent Generative AI for Dynamic Multimodal Narratives](https://arxiv.org/abs/2409.11261)
Append: [FG-PRM: Fine-grained Hallucination Detection and Mitigation in Language Model Mathematical Reasoning](https://arxiv.org/abs/2410.06304)
Append: [RAcQUEt: Unveiling the Dangers of Overlooked Referential Ambiguity in Visual LLMs](https://arxiv.org/abs/2412.13835)
Append: [Mind the Inclusivity Gap: Multilingual Gender-Neutral Translation Evaluation with mGeNTE](https://arxiv.org/abs/2501.09409)
Append: [Examining False Positives under Inference Scaling for Mathematical Reasoning](https://arxiv.org/abs/2502.06217)
Append: [Linguistic Generalizations are not Rules: Impacts on Evaluation of LMs](https://arxiv.org/abs/2502.13195)
Append: [SNaRe: Domain-aware Data Generation for Low-Resource Event Detection](https://arxiv.org/abs/2502.17394)
Append: [Single- vs. Dual-Prompt Dialogue Generation with LLMs for Job Interviews in Human Resources](https://arxiv.org/abs/2502.18650)
Append: [Unsupervised Concept Vector Extraction for Bias Control in LLMs](https://arxiv.org/abs/2502.19721)
Append: [Evaluation and Facilitation of Online Discussions in the LLM Era: A Survey](https://arxiv.org/abs/2503.01513)
Append: [CARE: Multilingual Human Preference Learning for Cultural Awareness](https://arxiv.org/abs/2504.05154)
Append: [Read Before You Think: Mitigating LLM Comprehension Failures with Step-by-Step Reading](https://arxiv.org/abs/2504.09402)
Append: [Extracting memorized pieces of (copyrighted) books from open-weight language models](https://arxiv.org/abs/2505.12546)
Append: [Diverse, not Short: A Length-Controlled Data Selection Strategy for Improving Response Diversity of Language Models](https://arxiv.org/abs/2505.16245)
Append: [PMPO: Probabilistic Metric Prompt Optimization for Small and Large Language Models](https://arxiv.org/abs/2505.16307)
Append: [Mechanistic Understanding and Mitigation of Language Confusion in English-Centric Large Language Models](https://arxiv.org/abs/2505.16538)
Append: [Assistant-Guided Mitigation of Teacher Preference Bias in LLM-as-a-Judge](https://arxiv.org/abs/2505.19176)
Append: [MOLE: Metadata Extraction and Validation in Scientific Papers Using LLMs](https://arxiv.org/abs/2505.19800)
Append: [WebCoT: Enhancing Web Agent Reasoning by Reconstructing Chain-of-Thought in Reflection, Branching, and Rollback](https://arxiv.org/abs/2505.20013)
Append: [Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision](https://arxiv.org/abs/2505.20415)
Append: [mdok of KInIT: Robustly Fine-tuned LLM for Binary and Multiclass AI-Generated Text Detection](https://arxiv.org/abs/2506.01702)
Append: [ImpRAG: Retrieval-Augmented Generation with Implicit Queries](https://arxiv.org/abs/2506.02279)
Append: [DiCoRe: Enhancing Zero-shot Event Detection via Divergent-Convergent LLM Reasoning](https://arxiv.org/abs/2506.05128)
Append: [QA-LIGN: Aligning LLMs through Constitutionally Decomposed QA](https://arxiv.org/abs/2506.08123)
Append: ["What's Up, Doc?": Analyzing How Users Seek Health Information in Large-Scale Conversational AI Datasets](https://arxiv.org/abs/2506.21532)
Append: [MedVAL: Toward Expert-Level Medical Text Validation with Language Models](https://arxiv.org/abs/2507.03152)
Append: [SMART: Simulated Students Aligned with Item Response Theory for Question Difficulty Prediction](https://arxiv.org/abs/2507.05129)
Append: [Unpacking Ambiguity: The Interaction of Polysemous Discourse Markers and Non-DM Signals](https://arxiv.org/abs/2507.16748)
Append: [ASCoT: An Adaptive Self-Correction Chain-of-Thought Method for Late-Stage Fragility in LLMs](https://arxiv.org/abs/2508.05282)
Append: [Select to Know: An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering](https://arxiv.org/abs/2508.15213)
Append: [Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning](https://arxiv.org/abs/2508.21589)
Append: [Learn while Unlearn: An Iterative Unlearning Framework for Generative Language Models](https://arxiv.org/abs/2407.20271)
Append: [Synaptic Theory of Chunking in Working Memory](https://arxiv.org/abs/2408.07637)
Append: [3DS: Medical Domain Adaptation of LLMs via Decomposed Difficulty-based Data Selection](https://arxiv.org/abs/2410.10901)
Append: [Large Multi-modal Models Can Interpret Features in Large Multi-modal Models](https://arxiv.org/abs/2411.14982)
Append: [GASLITEing the Retrieval: Exploring Vulnerabilities in Dense Embedding-based Search](https://arxiv.org/abs/2412.20953)
Append: [METAL: A Multi-Agent Framework for Chart Generation with Test-Time Scaling](https://arxiv.org/abs/2502.17651)
Append: [InMind: Evaluating LLMs in Capturing and Applying Individual Human Reasoning Styles](https://arxiv.org/abs/2508.16072)
Append: [CSRM-LLM: Embracing Multilingual LLMs for Cold-Start Relevance Matching in Emerging E-commerce Markets](https://arxiv.org/abs/2509.01566)
append_entries: 139
Finish: 2025-09-19 04:24:08.167379
------------------------------------------------------
Started: 2025-09-19 06:24:06.408665
Existing_entries: 1139
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-19 06:24:06.788812
------------------------------------------------------
Started: 2025-09-19 08:20:49.838393
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-19 08:20:50.177347
------------------------------------------------------
Started: 2025-09-19 10:17:49.677604
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-19 10:17:50.097521
------------------------------------------------------
Started: 2025-09-19 12:33:38.737563
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-19 12:33:39.185701
------------------------------------------------------
Started: 2025-09-19 14:15:44.352546
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-19 14:15:44.709685
------------------------------------------------------
Started: 2025-09-19 16:18:58.194685
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-19 16:18:58.544195
------------------------------------------------------
Started: 2025-09-19 18:21:55.528870
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-19 18:21:55.889392
------------------------------------------------------
Started: 2025-09-19 20:17:13.798485
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-19 20:17:14.197981
------------------------------------------------------
Started: 2025-09-19 22:14:55.419346
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-19 22:14:55.791879
------------------------------------------------------
Started: 2025-09-20 01:10:25.172797
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-20 01:10:25.543997
------------------------------------------------------
Started: 2025-09-20 02:51:36.992937
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-20 02:51:37.378492
------------------------------------------------------
Started: 2025-09-20 04:18:23.563941
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-20 04:18:23.689698
------------------------------------------------------
Started: 2025-09-20 06:21:43.870156
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-20 06:21:43.926908
------------------------------------------------------
Started: 2025-09-20 08:18:45.672750
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-20 08:18:45.823432
------------------------------------------------------
Started: 2025-09-20 10:15:01.642282
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-20 10:15:01.747579
------------------------------------------------------
Started: 2025-09-20 12:30:08.156921
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-20 12:30:08.263922
------------------------------------------------------
Started: 2025-09-20 14:13:20.051412
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-20 14:13:20.157225
------------------------------------------------------
Started: 2025-09-20 16:17:11.008923
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-20 16:17:11.095178
------------------------------------------------------
Started: 2025-09-20 18:20:00.115816
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-20 18:20:00.222969
------------------------------------------------------
Started: 2025-09-20 20:15:03.842503
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-20 20:15:03.919379
------------------------------------------------------
Started: 2025-09-20 22:13:25.182909
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-20 22:13:25.262881
------------------------------------------------------
Started: 2025-09-21 01:19:41.730586
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-21 01:19:41.820008
------------------------------------------------------
Started: 2025-09-21 03:05:14.133955
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-21 03:05:14.194437
------------------------------------------------------
Started: 2025-09-21 04:18:41.668061
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-21 04:18:41.747889
------------------------------------------------------
Started: 2025-09-21 06:22:15.196421
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-21 06:22:15.254546
------------------------------------------------------
Started: 2025-09-21 08:17:52.528195
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-21 08:17:52.632690
------------------------------------------------------
Started: 2025-09-21 10:14:45.021197
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-21 10:14:45.075636
------------------------------------------------------
Started: 2025-09-21 12:29:35.402284
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-21 12:29:35.470636
------------------------------------------------------
Started: 2025-09-21 14:13:11.204306
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-21 14:13:11.291256
------------------------------------------------------
Started: 2025-09-21 16:17:44.718298
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-21 16:17:44.840823
------------------------------------------------------
Started: 2025-09-21 18:20:17.680357
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-21 18:20:17.764462
------------------------------------------------------
Started: 2025-09-21 20:16:11.211148
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-21 20:16:11.315211
------------------------------------------------------
Started: 2025-09-21 22:13:53.935809
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-21 22:13:54.000608
------------------------------------------------------
Started: 2025-09-22 01:18:50.383830
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-22 01:18:50.448546
------------------------------------------------------
Started: 2025-09-22 03:04:59.445370
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-22 03:04:59.648593
------------------------------------------------------
Started: 2025-09-22 04:22:51.823453
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1269
Summarized using GPT-3.5-turbo
Append: [Synthetic bootstrapped pretraining](https://arxiv.org/abs/2509.15248)
Token length: 1855
Summarized using GPT-3.5-turbo
Append: [Comparative Analysis of Tokenization Algorithms for Low-Resource Language Dzongkha](https://arxiv.org/abs/2509.15255)
Token length: 1196
Summarized using GPT-3.5-turbo
Append: [Toxicity Red-Teaming: Benchmarking LLM Safety in Singapore's Low-Resource Languages](https://arxiv.org/abs/2509.15260)
Token length: 902
Summarized using GPT-3.5-turbo
Append: [PolBiX: Detecting LLMs' Political Bias in Fact-Checking through X-phemisms](https://arxiv.org/abs/2509.15335)
Token length: 890
Summarized using GPT-3.5-turbo
Append: [Quantifying Self-Awareness of Knowledge in Large Language Models](https://arxiv.org/abs/2509.15339)
Token length: 1425
Summarized using GPT-3.5-turbo
Append: [Real, Fake, or Manipulated? Detecting Machine-Influenced Text](https://arxiv.org/abs/2509.15350)
Token length: 939
Summarized using GPT-3.5-turbo
Append: [Beyond Spurious Signals: Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing](https://arxiv.org/abs/2509.15361)
Token length: 878
Summarized using GPT-3.5-turbo
Append: [Speech Language Models for Under-Represented Languages: Insights from Wolof](https://arxiv.org/abs/2509.15362)
Token length: 907
Summarized using GPT-3.5-turbo
Append: [Frustratingly Easy Data Augmentation for Low-Resource ASR](https://arxiv.org/abs/2509.15373)
Token length: 1493
Summarized using GPT-3.5-turbo
Append: [Quantifying Uncertainty in Natural Language Explanations of Large Language Models for Question Answering](https://arxiv.org/abs/2509.15403)
Token length: 1407
Summarized using GPT-3.5-turbo
Append: [Deep learning and abstractive summarisation for radiological reports: an empirical study for adapting the PEGASUS models' family with scarce data](https://arxiv.org/abs/2509.15419)
Token length: 1503
Summarized using GPT-3.5-turbo
Append: [BiRQ: Bi-Level Self-Labeling Random Quantization for Self-Supervised Speech Recognition](https://arxiv.org/abs/2509.15430)
Token length: 1724
Summarized using GPT-3.5-turbo
Append: [PILOT: Steering Synthetic Data Generation with Psychological & Linguistic Output Targeting](https://arxiv.org/abs/2509.15447)
Token length: 1127
Summarized using GPT-3.5-turbo
Append: [Evaluating Multimodal Large Language Models on Spoken Sarcasm Understanding](https://arxiv.org/abs/2509.15476)
Token length: 1236
Summarized using GPT-3.5-turbo
Append: [Red Teaming Multimodal Language Models: Evaluating Harm Across Prompt Modalities and Models](https://arxiv.org/abs/2509.15478)
Token length: 942
Summarized using GPT-3.5-turbo
Append: [mucAI at BAREC Shared Task 2025: Towards Uncertainty Aware Arabic Readability Assessment](https://arxiv.org/abs/2509.15485)
Token length: 1039
Summarized using GPT-3.5-turbo
Append: [LLM Cache Bandit Revisited: Addressing Query Heterogeneity for Cost-Effective LLM Inference](https://arxiv.org/abs/2509.15515)
Token length: 1423
Summarized using GPT-3.5-turbo
Append: [How do Language Models Generate Slang: A Systematic Comparison between Human and Machine-Generated Slang Usages](https://arxiv.org/abs/2509.15518)
Token length: 1210
Summarized using GPT-3.5-turbo
Append: [A method for improving multilingual quality and diversity of instruction fine-tuning datasets](https://arxiv.org/abs/2509.15549)
Token length: 1499
Summarized using GPT-3.5-turbo
Append: [DNA-DetectLLM: Unveiling AI-Generated Text via a DNA-Inspired Mutation-Repair Paradigm](https://arxiv.org/abs/2509.15550)
Token length: 1470
Summarized using GPT-3.5-turbo
Append: [Exploring Polyglot Harmony: On Multilingual Data Allocation for Large Language Models Pretraining](https://arxiv.org/abs/2509.15556)
Token length: 1335
Summarized using GPT-3.5-turbo
Append: [How important is language for human-like intelligence?](https://arxiv.org/abs/2509.15560)
Token length: 1197
Summarized using GPT-3.5-turbo
Append: [LiteLong: Resource-Efficient Long-Context Data Synthesis for LLMs](https://arxiv.org/abs/2509.15568)
Token length: 984
Summarized using GPT-3.5-turbo
Append: [Relevance to Utility: Process-Supervised Rewrite for RAG](https://arxiv.org/abs/2509.15577)
Token length: 1730
Summarized using GPT-3.5-turbo
Append: [Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization](https://arxiv.org/abs/2509.15579)
Token length: 1063
Summarized using GPT-3.5-turbo
Append: [DivLogicEval: A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models](https://arxiv.org/abs/2509.15587)
Token length: 1214
Summarized using GPT-3.5-turbo
Append: [SciEvent: Benchmarking Multi-domain Scientific Event Extraction](https://arxiv.org/abs/2509.15620)
Token length: 1330
Summarized using GPT-3.5-turbo
Append: [Concept Unlearning in Large Language Models via Self-Constructed Knowledge Triplets](https://arxiv.org/abs/2509.15621)
Token length: 1778
Summarized using GPT-3.5-turbo
Append: [Sparse-Autoencoder-Guided Internal Representation Unlearning for Large Language Models](https://arxiv.org/abs/2509.15631)
Token length: 885
Summarized using GPT-3.5-turbo
Append: [Multilingual LLM Prompting Strategies for Medical English-Vietnamese Machine Translation](https://arxiv.org/abs/2509.15640)
Token length: 981
Summarized using GPT-3.5-turbo
Append: [Layer-wise Minimal Pair Probing Reveals Contextual Grammatical-Conceptual Hierarchy in Speech Representations](https://arxiv.org/abs/2509.15655)
Token length: 1043
Summarized using GPT-3.5-turbo
Append: [VOX-KRIKRI: Unifying Speech and Language through Continuous Fusion](https://arxiv.org/abs/2509.15667)
Token length: 1076
Summarized using GPT-3.5-turbo
Append: [Fine-Tuning Large Multimodal Models for Automatic Pronunciation Assessment](https://arxiv.org/abs/2509.15701)
Token length: 970
Summarized using GPT-3.5-turbo
Append: [Once Upon a Time: Interactive Learning for Storytelling with Small Language Models](https://arxiv.org/abs/2509.15714)
Token length: 1296
Summarized using GPT-3.5-turbo
Append: [REFER: Mitigating Bias in Opinion Summarisation via Frequency Framed Prompting](https://arxiv.org/abs/2509.15723)
Token length: 1163
Summarized using GPT-3.5-turbo
Append: [Can LLMs Judge Debates? Evaluating Non-Linear Reasoning via Argumentation Theory Semantics](https://arxiv.org/abs/2509.15739)
Token length: 1169
Summarized using GPT-3.5-turbo
Append: [UniGist: Towards General and Hardware-aligned Sequence-level Long Context Compression](https://arxiv.org/abs/2509.15763)
Token length: 1038
Summarized using GPT-3.5-turbo
Append: [UPRPRC: Unified Pipeline for Reproducing Parallel Resources -- Corpus from the United Nations](https://arxiv.org/abs/2509.15789)
Token length: 736
Summarized using GPT-3.5-turbo
Append: [RAVE: Retrieval and Scoring Aware Verifiable Claim Detection](https://arxiv.org/abs/2509.15793)
Token length: 936
Summarized using GPT-3.5-turbo
Append: [Best-of-L: Cross-Lingual Reward Modeling for Mathematical Reasoning](https://arxiv.org/abs/2509.15811)
Token length: 1040
Summarized using GPT-3.5-turbo
Append: [The Curious Case of Visual Grounding: Different Effects for Speech- and Text-based Language Encoders](https://arxiv.org/abs/2509.15837)
Token length: 1321
Summarized using GPT-3.5-turbo
Append: [Multi-Physics: A Comprehensive Benchmark for Multimodal LLMs Reasoning on Chinese Multi-Subject Physics Problems](https://arxiv.org/abs/2509.15839)
Token length: 1459
Summarized using GPT-3.5-turbo
Append: [Distribution-Aligned Decoding for Efficient LLM Task Adaptation](https://arxiv.org/abs/2509.15888)
Token length: 1294
Summarized using GPT-3.5-turbo
Append: [The Psychology of Falsehood: A Human-Centric Survey of Misinformation Detection](https://arxiv.org/abs/2509.15896)
Token length: 1195
Summarized using GPT-3.5-turbo
Append: [Re-FRAME the Meeting Summarization SCOPE: Fact-Based Summarization and Personalization via Questions](https://arxiv.org/abs/2509.15901)
Token length: 1161
Summarized using GPT-3.5-turbo
Append: [Beyond the Score: Uncertainty-Calibrated LLMs for Automated Essay Assessment](https://arxiv.org/abs/2509.15926)
Token length: 1430
Summarized using GPT-3.5-turbo
Append: [Localmax dynamics for attention in transformers and its asymptotic behavior](https://arxiv.org/abs/2509.15958)
Token length: 1245
Summarized using GPT-3.5-turbo
Append: [BEFT: Bias-Efficient Fine-Tuning of Language Models](https://arxiv.org/abs/2509.15974)
Token length: 1274
Summarized using GPT-3.5-turbo
Append: [Session-Level Spoken Language Assessment with a Multimodal Foundation Model via Multi-Target Learning](https://arxiv.org/abs/2509.16025)
Token length: 1089
Summarized using GPT-3.5-turbo
Append: [Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech](https://arxiv.org/abs/2509.16028)
Append: [Beyond Pointwise Scores: Decomposed Criteria-Based Evaluation of LLM Responses](https://arxiv.org/abs/2509.16093)
Append: [DiEP: Adaptive Mixture-of-Experts Compression through Differentiable Expert Pruning](https://arxiv.org/abs/2509.16105)
Append: [It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge](https://arxiv.org/abs/2509.16107)
Append: [CodeRAG: Finding Relevant and Necessary Knowledge for Retrieval-Augmented Repository-Level Code Completion](https://arxiv.org/abs/2509.16112)
Append: [CultureScope: A Dimensional Lens for Probing Cultural Understanding in LLMs](https://arxiv.org/abs/2509.16188)
Append: [RPG: A Repository Planning Graph for Unified and Scalable Codebase Generation](https://arxiv.org/abs/2509.16198)
Append: [Learning Analytics from Spoken Discussion Dialogs in Flipped Classroom](https://arxiv.org/abs/2301.12399)
Append: [Video2Roleplay: A Multimodal Dataset and Framework for Video-Guided Role-playing Agents](https://arxiv.org/abs/2509.15233)
Append: [ViSpec: Accelerating Vision-Language Models with Vision-Aware Speculative Decoding](https://arxiv.org/abs/2509.15235)
Append: [M-PACE: Mother Child Framework for Multimodal Compliance](https://arxiv.org/abs/2509.15241)
Append: [Fleming-R1: Toward Expert-Level Medical Reasoning via Reinforcement Learning](https://arxiv.org/abs/2509.15279)
Append: [Efficient and Versatile Model for Multilingual Information Retrieval of Islamic Text: Development and Deployment in Real-World Scenarios](https://arxiv.org/abs/2509.15380)
Append: [Beyond Words: Enhancing Desire, Emotion, and Sentiment Recognition with Non-Verbal Cues](https://arxiv.org/abs/2509.15540)
Append: [Small LLMs with Expert Blocks Are Good Enough for Hyperparamter Tuning](https://arxiv.org/abs/2509.15561)
Append: [SightSound-R1: Cross-Modal Reasoning Distillation from Vision to Audio Language Models](https://arxiv.org/abs/2509.15661)
Append: [KITE: Kernelized and Information Theoretic Exemplars for In-Context Learning](https://arxiv.org/abs/2509.15676)
Append: [Direct Simultaneous Translation Activation for Large Audio-Language Models](https://arxiv.org/abs/2509.15692)
Append: [EHR-MCP: Real-world Evaluation of Clinical Information Retrieval by Large Language Models via Model Context Protocol](https://arxiv.org/abs/2509.15957)
Append: [VoXtream: Full-Stream Text-to-Speech with Extremely Low Latency](https://arxiv.org/abs/2509.15969)
Append: [EmoHeal: An End-to-End System for Personalized Therapeutic Music Retrieval from Fine-grained Emotions](https://arxiv.org/abs/2509.15986)
Append: [SABER: Uncovering Vulnerabilities in Safety Alignment via Cross-Layer Residual Connection](https://arxiv.org/abs/2509.16060)
Append: [Robust Vision-Language Models via Tensor Decomposition: A Defense Against Adversarial Attacks](https://arxiv.org/abs/2509.16163)
Append: [Latent learning: episodic memory complements parametric learning by enabling flexible reuse of experiences](https://arxiv.org/abs/2509.16189)
Append: [MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer](https://arxiv.org/abs/2509.16197)
Append: [Automatic Lexical Simplification for Turkish](https://arxiv.org/abs/2201.05878)
Append: [BBScoreV2: Learning Time-Evolution and Latent Alignment from Stochastic Representation](https://arxiv.org/abs/2405.17764)
Append: [The Great AI Witch Hunt: Reviewers Perception and (Mis)Conception of Generative AI in Research Writing](https://arxiv.org/abs/2407.12015)
Append: [ConfReady: A RAG based Assistant and Dataset for Conference Checklist Responses](https://arxiv.org/abs/2408.04675)
Append: [DynamicNER: A Dynamic, Multilingual, and Fine-Grained Dataset for LLM-based Named Entity Recognition](https://arxiv.org/abs/2409.11022)
Append: [Disentangling Latent Shifts of In-Context Learning with Weak Supervision](https://arxiv.org/abs/2410.01508)
Append: [Query Optimization for Parametric Knowledge Refinement in Retrieval-Augmented Large Language Models](https://arxiv.org/abs/2411.07820)
Append: [Efficient Real-time Refinement of Language Model Text Generation](https://arxiv.org/abs/2501.07824)
Append: [A Layered Multi-Expert Framework for Long-Context Mental Health Assessments](https://arxiv.org/abs/2501.13951)
Append: [Bias Beware: The Impact of Cognitive Biases on LLM-Driven Product Recommendations](https://arxiv.org/abs/2502.01349)
Append: [Adaptive Self-improvement LLM Agentic System for ML Library Development](https://arxiv.org/abs/2502.02534)
Append: [Where Fact Ends and Fairness Begins: Redefining AI Bias Evaluation through Cognitive Biases](https://arxiv.org/abs/2502.05849)
Append: [FSLI: An Interpretable Formal Semantic System for One-Dimensional Ordering Inference](https://arxiv.org/abs/2502.08415)
Append: [Sparsity May Be All You Need: Sparse Random Parameter Adaptation](https://arxiv.org/abs/2502.15975)
Append: [Evaluating Robustness of LLMs in Question Answering on Multilingual Noisy OCR Data](https://arxiv.org/abs/2502.16781)
Append: [Harnessing Multiple Large Language Models: A Survey on LLM Ensemble](https://arxiv.org/abs/2502.18036)
Append: [KatFishNet: Detecting LLM-Generated Korean Text through Linguistic Feature Analysis](https://arxiv.org/abs/2503.00032)
Append: [DP-GTR: Differentially Private Prompt Protection via Group Text Rewriting](https://arxiv.org/abs/2503.04990)
Append: [reWordBench: Benchmarking and Improving the Robustness of Reward Models with Transformed Inputs](https://arxiv.org/abs/2503.11751)
Append: [MT-RewardTree: A Comprehensive Framework for Advancing LLM-Based Machine Translation via Reward Modeling](https://arxiv.org/abs/2503.12123)
Append: [Personalized Language Models via Privacy-Preserving Evolutionary Model Merging](https://arxiv.org/abs/2503.18008)
Append: [A Benchmark for End-to-End Zero-Shot Biomedical Relation Extraction with LLMs: Experiments with OpenAI Models](https://arxiv.org/abs/2504.04083)
Append: [UXAgent: A System for Simulating Usability Testing of Web Design with LLM Agents](https://arxiv.org/abs/2504.09407)
Append: [Natural Fingerprints of Large Language Models](https://arxiv.org/abs/2504.14871)
Append: [Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training](https://arxiv.org/abs/2504.20484)
Append: [Search and Refine During Think: Facilitating Knowledge Refinement for Improved Retrieval-Augmented Reasoning](https://arxiv.org/abs/2505.11277)
Append: [The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation](https://arxiv.org/abs/2505.13090)
Append: [Are LLMs Better Formalizers than Solvers on Complex Problems?](https://arxiv.org/abs/2505.13252)
Append: [MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation Capabilities in Any Language](https://arxiv.org/abs/2505.14395)
Append: [Creative Preference Optimization](https://arxiv.org/abs/2505.14442)
Append: [Language Mixing in Reasoning Language Models: Patterns, Impact, and Internal Causes](https://arxiv.org/abs/2505.14815)
Append: [Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark Study](https://arxiv.org/abs/2505.15389)
Append: [MuseScorer: Idea Originality Scoring At Scale](https://arxiv.org/abs/2505.16232)
Append: [CLEAR: A Clinically-Grounded Tabular Framework for Radiology Report Evaluation](https://arxiv.org/abs/2505.16325)
Append: [Measuring Lexical Diversity of Synthetic Data Generated through Fine-Grained Persona Prompting](https://arxiv.org/abs/2505.17390)
Append: [HydraRAG: Structured Cross-Source Enhanced Large Language Model Reasoning](https://arxiv.org/abs/2505.17464)
Append: [AmpleHate: Amplifying the Attention for Versatile Implicit Hate Detection](https://arxiv.org/abs/2505.19528)
Append: [SEMMA: A Semantic Aware Knowledge Graph Foundation Model](https://arxiv.org/abs/2505.20422)
Append: [Calibrating LLM Confidence by Probing Perturbed Representation Stability](https://arxiv.org/abs/2505.21772)
Append: [MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain Dialogue Evaluators](https://arxiv.org/abs/2505.22777)
Append: [Cross-Attention Speculative Decoding](https://arxiv.org/abs/2505.24544)
Append: [Emergent Abilities of Large Language Models under Continued Pretraining for Language Adaptation](https://arxiv.org/abs/2506.00288)
Append: [LESS: Large Language Model Enhanced Semi-Supervised Learning for Speech Foundational Models Using in-the-wild Data](https://arxiv.org/abs/2506.04586)
Append: [Benchmarking Debiasing Methods for LLM-based Parameter Estimates](https://arxiv.org/abs/2506.09627)
Append: [From Judgment to Interference: Early Stopping LLM Harmful Outputs via Streaming Content Monitoring](https://arxiv.org/abs/2506.09996)
Append: [A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages](https://arxiv.org/abs/2506.12158)
Append: [IGD: Token Decisiveness Modeling via Information Gain in LLMs for Personalized Recommendation](https://arxiv.org/abs/2506.13229)
Append: [The Impact of Automatic Speech Transcription on Speaker Attribution](https://arxiv.org/abs/2507.08660)
Append: [Translationese-index: Using Likelihood Ratios for Graded and Generalizable Measurement of Translationese](https://arxiv.org/abs/2507.12260)
Append: [Causal2Vec: Improving Decoder-only LLMs as Versatile Embedding Models](https://arxiv.org/abs/2507.23386)
Append: [XAutoLM: Efficient Fine-Tuning of Language Models via Meta-Learning and AutoML](https://arxiv.org/abs/2508.00924)
Append: [WangchanThaiInstruct: An instruction-following Dataset for Culture-Aware, Multitask, and Multi-domain Evaluation in Thai](https://arxiv.org/abs/2508.15239)
Append: [CORE-RAG: Lossless Compression for Retrieval-Augmented LLMs via Reinforcement Learning](https://arxiv.org/abs/2508.19282)
Append: [Discovering Semantic Subdimensions through Disentangled Conceptual Representations](https://arxiv.org/abs/2508.21436)
Append: [Not All Parameters Are Created Equal: Smart Isolation Boosts Fine-Tuning Performance](https://arxiv.org/abs/2508.21741)
Append: [MedCOD: Enhancing English-to-Spanish Medical Translation of Large Language Models Using Enriched Chain-of-Dictionary Framework](https://arxiv.org/abs/2509.00934)
Append: [LongCat-Flash Technical Report](https://arxiv.org/abs/2509.01322)
Append: [Do Retrieval Augmented Language Models Know When They Don't Know?](https://arxiv.org/abs/2509.01476)
Append: [FLARE: Faithful Logic-Aided Reasoning and Exploration](https://arxiv.org/abs/2410.11900)
Append: [Entropy-Regularized Process Reward Model](https://arxiv.org/abs/2412.11006)
Append: [Tag&Tab: Pretraining Data Detection in Large Language Models Using Keyword-Based Membership Inference Attack](https://arxiv.org/abs/2501.08454)
Append: [Neural Networks for Learnable and Scalable Influence Estimation of Instruction Fine-Tuning Data](https://arxiv.org/abs/2502.09969)
Append: [AdaSteer: Your Aligned LLM is Inherently an Adaptive Jailbreak Defender](https://arxiv.org/abs/2504.09466)
Append: [AgentA/B: Automated and Scalable Web A/BTesting with Interactive LLM Agents](https://arxiv.org/abs/2504.09723)
Append: [ConCISE: Confidence-guided Compression in Step-by-step Efficient Reasoning](https://arxiv.org/abs/2505.04881)
Append: [StreamBridge: Turning Your Offline Video Large Language Model into a Proactive Streaming Assistant](https://arxiv.org/abs/2505.05467)
Append: [SPaRC: A Spatial Pathfinding Reasoning Challenge](https://arxiv.org/abs/2505.16686)
Append: [P2VA: Converting Persona Descriptions into Voice Attributes for Fair and Controllable Text-to-Speech](https://arxiv.org/abs/2505.17093)
Append: [Can Large Language Models Infer Causal Relationships from Real-World Text?](https://arxiv.org/abs/2505.18931)
Append: [Beyond Linear Steering: Unified Multi-Attribute Control for Language Models](https://arxiv.org/abs/2505.24535)
Append: [LLMs Can Compensate for Deficiencies in Visual Representations](https://arxiv.org/abs/2506.05439)
Append: [Capturing Polysemanticity with PRISM: A Multi-Concept Feature Description Framework](https://arxiv.org/abs/2506.15538)
Append: [MountainLion: A Multi-Modal LLM-Based Agent System for Interpretable and Adaptive Financial Trading](https://arxiv.org/abs/2507.20474)
Append: [Personalized Real-time Jargon Support for Online Meetings](https://arxiv.org/abs/2508.10239)
Append: [Using Natural Language for Human-Robot Collaboration in the Real World](https://arxiv.org/abs/2508.11759)
Append: [Foundational Design Principles and Patterns for Building Robust and Adaptive GenAI-Native Systems](https://arxiv.org/abs/2508.15411)
Append: [SyGra: A Unified Graph-Based Framework for Scalable Generation, Quality Tagging, and Management of Synthetic Data](https://arxiv.org/abs/2508.15432)
Append: [RSCC: A Large-Scale Remote Sensing Change Caption Dataset for Disaster Events](https://arxiv.org/abs/2509.01907)
append_entries: 152
Finish: 2025-09-22 04:24:31.364151
------------------------------------------------------
Started: 2025-09-22 06:26:06.193324
Existing_entries: 1152
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-22 06:26:06.575832
------------------------------------------------------
Started: 2025-09-22 08:22:41.399888
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-22 08:22:41.747247
------------------------------------------------------
Started: 2025-09-22 10:18:26.894003
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-22 10:18:27.245526
------------------------------------------------------
Started: 2025-09-22 14:17:05.343134
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-22 14:17:05.725838
------------------------------------------------------
Started: 2025-09-22 16:20:26.669575
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-22 16:20:27.016769
------------------------------------------------------
Started: 2025-09-22 18:22:22.121788
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-22 18:22:22.583143
------------------------------------------------------
Started: 2025-09-22 20:17:42.888714
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-22 20:17:43.248438
------------------------------------------------------
Started: 2025-09-22 22:15:00.315066
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-22 22:15:00.709875
------------------------------------------------------
Started: 2025-09-23 01:12:29.296770
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-23 01:12:29.769074
------------------------------------------------------
Started: 2025-09-23 02:55:24.685081
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-23 02:55:25.040075
------------------------------------------------------
Started: 2025-09-23 04:22:45.425738
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1119
Summarized using GPT-3.5-turbo
Append: [On LLM-Based Scientific Inductive Reasoning Beyond Equations](https://arxiv.org/abs/2509.16226)
Token length: 1147
Summarized using GPT-3.5-turbo
Append: [REAMS: Reasoning Enhanced Algorithm for Maths Solving](https://arxiv.org/abs/2509.16241)
Token length: 1220
Summarized using GPT-3.5-turbo
Append: [HausaMovieReview: A Benchmark Dataset for Sentiment Analysis in Low-Resource African Language](https://arxiv.org/abs/2509.16256)
Token length: 1077
Summarized using GPT-3.5-turbo
Append: [Gender and Political Bias in Large Language Models: A Demonstration Platform](https://arxiv.org/abs/2509.16264)
Token length: 1698
Summarized using GPT-3.5-turbo
Append: [Language Modeling with Learned Meta-Tokens](https://arxiv.org/abs/2509.16278)
Token length: 1223
Summarized using GPT-3.5-turbo
Append: [Overhearing LLM Agents: A Survey, Taxonomy, and Roadmap](https://arxiv.org/abs/2509.16325)
Token length: 1694
Summarized using GPT-3.5-turbo
Append: [HARE: an entity and relation centric evaluation framework for histopathology reports](https://arxiv.org/abs/2509.16326)
Token length: 1360
Summarized using GPT-3.5-turbo
Append: [RephQA: Evaluating Readability of Large Language Models in Public Health Question Answering](https://arxiv.org/abs/2509.16360)
Token length: 1185
Summarized using GPT-3.5-turbo
Append: [Whisper-UT: A Unified Translation Framework for Speech and Text](https://arxiv.org/abs/2509.16375)
Token length: 1112
Summarized using GPT-3.5-turbo
Append: [Evaluating Behavioral Alignment in Conflict Dialogue: A Multi-Dimensional Comparison of LLM Agents and Humans](https://arxiv.org/abs/2509.16394)
Token length: 1081
Summarized using GPT-3.5-turbo
Append: ['Rich Dad, Poor Lad': How do Large Language Models Contextualize Socioeconomic Factors in College Admission ?](https://arxiv.org/abs/2509.16400)
Token length: 1073
Summarized using GPT-3.5-turbo
Append: [Pico: A Modular Framework for Hypothesis-Driven Small Language Model Research](https://arxiv.org/abs/2509.16413)
Token length: 877
Summarized using GPT-3.5-turbo
Append: [Evaluating CxG Generalisation in LLMs via Construction-Based NLI Fine Tuning](https://arxiv.org/abs/2509.16422)
Token length: 1186
Summarized using GPT-3.5-turbo
Append: [PersonaMatrix: A Recipe for Persona-Aware Evaluation of Legal Summarization](https://arxiv.org/abs/2509.16449)
Token length: 1434
Summarized using GPT-3.5-turbo
Append: [Implicit Behavioral Alignment of Language Agents in High-Stakes Crowd Simulations](https://arxiv.org/abs/2509.16457)
Token length: 1193
Summarized using GPT-3.5-turbo
Append: [Intrinsic Meets Extrinsic Fairness: Assessing the Downstream Impact of Bias Mitigation in Large Language Models](https://arxiv.org/abs/2509.16462)
Token length: 1396
Summarized using GPT-3.5-turbo
Append: [Computational Analysis of Conversation Dynamics through Participant Responsivity](https://arxiv.org/abs/2509.16464)
Token length: 1157
Summarized using GPT-3.5-turbo
Append: [The Oracle Has Spoken: A Multi-Aspect Evaluation of Dialogue in Pythia](https://arxiv.org/abs/2509.16487)
Token length: 1751
Summarized using GPT-3.5-turbo
Append: [Can an Individual Manipulate the Collective Decisions of Multi-Agents?](https://arxiv.org/abs/2509.16494)
Token length: 1462
Summarized using GPT-3.5-turbo
Append: [AIPsychoBench: Understanding the Psychometric Differences between LLMs and Humans](https://arxiv.org/abs/2509.16530)
Token length: 1260
Summarized using GPT-3.5-turbo
Append: [Leveraging Multilingual Training for Authorship Representation: Enhancing Generalization across Languages and Domains](https://arxiv.org/abs/2509.16531)
Token length: 1245
Summarized using GPT-3.5-turbo
Append: [Challenging the Evaluator: LLM Sycophancy Under User Rebuttal](https://arxiv.org/abs/2509.16533)
Token length: 1236
Summarized using GPT-3.5-turbo
Append: [InteGround: On the Evaluation of Verification and Retrieval Planning in Integrative Grounding](https://arxiv.org/abs/2509.16534)
Token length: 1739
Summarized using GPT-3.5-turbo
Append: [Mental Multi-class Classification on Social Media: Benchmarking Transformer Architectures against LSTM Models](https://arxiv.org/abs/2509.16542)
Token length: 1313
Summarized using GPT-3.5-turbo
Append: [ChemOrch: Empowering LLMs with Chemical Intelligence via Synthetic Instructions](https://arxiv.org/abs/2509.16543)
Token length: 1360
Summarized using GPT-3.5-turbo
Append: [Rethinking the Role of Text Complexity in Language Model Pretraining](https://arxiv.org/abs/2509.16551)
Token length: 1565
Summarized using GPT-3.5-turbo
Append: [MPCG: Multi-Round Persona-Conditioned Generation for Modeling the Evolution of Misinformation with LLMs](https://arxiv.org/abs/2509.16564)
Token length: 1644
Summarized using GPT-3.5-turbo
Append: [From Scores to Steps: Diagnosing and Improving LLM Performance in Evidence-Based Medical Calculations](https://arxiv.org/abs/2509.16584)
Token length: 989
Summarized using GPT-3.5-turbo
Append: [Benchmarking Contextual and Paralinguistic Reasoning in Speech-LLMs: A Case Study with In-the-Wild Data](https://arxiv.org/abs/2509.16589)
Token length: 1552
Summarized using GPT-3.5-turbo
Append: [From Uniform to Heterogeneous: Tailoring Policy Optimization to Every Token's Nature](https://arxiv.org/abs/2509.16591)
Token length: 1215
Summarized using GPT-3.5-turbo
Append: [Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from Token and Parameter Levels](https://arxiv.org/abs/2509.16596)
Token length: 1119
Summarized using GPT-3.5-turbo
Append: [MCP: A Control-Theoretic Orchestration Framework for Synergistic Efficiency and Interpretability in Multimodal Large Language Models](https://arxiv.org/abs/2509.16597)
Token length: 790
Summarized using GPT-3.5-turbo
Append: [PruneCD: Contrasting Pruned Self Model to Improve Decoding Factuality](https://arxiv.org/abs/2509.16598)
Token length: 1765
Summarized using GPT-3.5-turbo
Append: [Computational-Assisted Systematic Review and Meta-Analysis (CASMA): Effect of a Subclass of GnRH-a on Endometriosis Recurrence](https://arxiv.org/abs/2509.16599)
Token length: 1094
Summarized using GPT-3.5-turbo
Append: [LLMsPark: A Benchmark for Evaluating Large Language Models in Strategic Gaming Contexts](https://arxiv.org/abs/2509.16610)
Token length: 1444
Summarized using GPT-3.5-turbo
Append: [Redefining Experts: Interpretable Decomposition of Language Models for Toxicity Mitigation](https://arxiv.org/abs/2509.16660)
Token length: 1074
Summarized using GPT-3.5-turbo
Append: [Robust Native Language Identification through Agentic Decomposition](https://arxiv.org/abs/2509.16666)
Token length: 1751
Summarized using GPT-3.5-turbo
Append: [Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle](https://arxiv.org/abs/2509.16679)
Token length: 1808
Summarized using GPT-3.5-turbo
Append: [EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and Efficient LLMs](https://arxiv.org/abs/2509.16686)
Token length: 669
Summarized using GPT-3.5-turbo
Append: [Decoding Uncertainty: The Impact of Decoding Strategies for Uncertainty Estimation in Large Language Models](https://arxiv.org/abs/2509.16696)
Token length: 958
Summarized using GPT-3.5-turbo
Append: [OPEN-THEATRE: An Open-Source Toolkit for LLM-based Interactive Drama](https://arxiv.org/abs/2509.16713)
Token length: 1649
Summarized using GPT-3.5-turbo
Append: [Semi-Supervised Synthetic Data Generation with Fine-Grained Relevance Control for Short Video Search Relevance Modeling](https://arxiv.org/abs/2509.16717)
Token length: 1468
Summarized using GPT-3.5-turbo
Append: [Time to Revist Exact Match](https://arxiv.org/abs/2509.16720)
Token length: 1118
Summarized using GPT-3.5-turbo
Append: [A Multi-Level Benchmark for Causal Language Understanding in Social Media Discourse](https://arxiv.org/abs/2509.16722)
Token length: 1231
Summarized using GPT-3.5-turbo
Append: [Angular Dispersion Accelerates $k$-Nearest Neighbors Machine Translation](https://arxiv.org/abs/2509.16729)
Token length: 1787
Summarized using GPT-3.5-turbo
Append: [The Sound of Syntax: Finetuning and Comprehensive Evaluation of Language Models for Speech Pathology](https://arxiv.org/abs/2509.16765)
Token length: 984
Summarized using GPT-3.5-turbo
Append: [MoRoVoc: A Large Dataset for Geographical Variation Identification of the Spoken Romanian Language](https://arxiv.org/abs/2509.16781)
Token length: 1881
Summarized using GPT-3.5-turbo
Append: [Domain-Adaptive Pre-Training for Arabic Aspect-Based Sentiment Analysis: A Comparative Study of Domain Adaptation and Fine-Tuning Strategies](https://arxiv.org/abs/2509.16788)
Token length: 889
Summarized using GPT-3.5-turbo
Append: [KuBERT: Central Kurdish BERT Model and Its Application for Sentiment Analysis](https://arxiv.org/abs/2509.16804)
Token length: 1323
Summarized using GPT-3.5-turbo
Append: [Cognitive Linguistic Identity Fusion Score (CLIFS): A Scalable Cognition-Informed Approach to Quantifying Identity Fusion from Text](https://arxiv.org/abs/2509.16813)
Append: [Semantic-Driven Topic Modeling for Analyzing Creativity in Virtual Brainstorming](https://arxiv.org/abs/2509.16835)
Append: [Multi-task Pretraining for Enhancing Interpretable L2 Pronunciation Assessment](https://arxiv.org/abs/2509.16876)
Append: [Can GRPO Boost Complex Multimodal Table Understanding?](https://arxiv.org/abs/2509.16889)
Append: [CLaC at DISRPT 2025: Hierarchical Adapters for Cross-Framework Multi-lingual Discourse Relation Classification](https://arxiv.org/abs/2509.16903)
Append: [CUTE: A Multilingual Dataset for Enhancing Cross-Lingual Knowledge Transfer in Low-Resource Languages](https://arxiv.org/abs/2509.16914)
Append: [K-DeCore: Facilitating Knowledge Transfer in Continual Structured Knowledge Reasoning via Knowledge Decoupling](https://arxiv.org/abs/2509.16929)
Append: [AirQA: A Comprehensive QA Dataset for AI Research with Instance-Level Evaluation](https://arxiv.org/abs/2509.16952)
Append: [Preference Distillation via Value based Reinforcement Learning](https://arxiv.org/abs/2509.16965)
Append: [Advancing Speech Understanding in Speech-Aware Language Models with GRPO](https://arxiv.org/abs/2509.16990)
Append: [The Transfer Neurons Hypothesis: An Underlying Mechanism for Language Latent Space Transitions in Multilingual LLMs](https://arxiv.org/abs/2509.17030)
Append: [Modeling Bottom-up Information Quality during Language Processing](https://arxiv.org/abs/2509.17047)
Append: [TactfulToM: Do LLMs Have the Theory of Mind Ability to Understand White Lies?](https://arxiv.org/abs/2509.17054)
Append: [SFT-TA: Supervised Fine-Tuned Agents in Multi-Agent LLMs for Automated Inductive Thematic Analysis](https://arxiv.org/abs/2509.17167)
Append: [FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning Models on Automatically Verifiable Textual and Visual Questions](https://arxiv.org/abs/2509.17177)
Append: [Attention Consistency for LLMs Explanation](https://arxiv.org/abs/2509.17178)
Append: [LifeAlign: Lifelong Alignment for Large Language Models with Memory-Augmented Focalized Preference Optimization](https://arxiv.org/abs/2509.17183)
Append: [Evolution of Concepts in Language Model Pre-Training](https://arxiv.org/abs/2509.17196)
Append: [Prompt-Based Simplification for Plain Language using Spanish Language Models](https://arxiv.org/abs/2509.17209)
Append: [Extending Automatic Machine Translation Evaluation to Book-Length Documents](https://arxiv.org/abs/2509.17249)
Append: [Probabilistic Token Alignment for Large Language Model Fusion](https://arxiv.org/abs/2509.17276)
Append: [Automated Knowledge Graph Construction using Large Language Models and Sentence Complexity Modelling](https://arxiv.org/abs/2509.17289)
Append: [Multi-View Attention Multiple-Instance Learning Enhanced by LLM Reasoning for Cognitive Distortion Detection](https://arxiv.org/abs/2509.17292)
Append: [Scaling, Simplification, and Adaptation: Lessons from Pretraining on Machine-Translated Text](https://arxiv.org/abs/2509.17317)
Append: [AIMMerging: Adaptive Iterative Model Merging Using Training Trajectories for Language Model Continual Learning](https://arxiv.org/abs/2509.17348)
Append: [Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation](https://arxiv.org/abs/2509.17349)
Append: [Scale-free Characteristics of Multilingual Legal Texts and the Limitations of LLMs](https://arxiv.org/abs/2509.17367)
Append: [Robustness of Neurosymbolic Reasoners on First-Order Logic Problems](https://arxiv.org/abs/2509.17377)
Append: [FinDebate: Multi-Agent Collaborative Intelligence for Financial Analysis](https://arxiv.org/abs/2509.17395)
Append: [EpiCache: Episodic KV Cache Management for Long Conversational Question Answering](https://arxiv.org/abs/2509.17396)
Append: [DIWALI - Diversity and Inclusivity aWare cuLture specific Items for India: Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian Context](https://arxiv.org/abs/2509.17399)
Append: [Vision Language Models Are Not (Yet) Spelling Correctors](https://arxiv.org/abs/2509.17418)
Append: [RealBench: A Chinese Multi-image Understanding Benchmark Close to Real-world Scenarios](https://arxiv.org/abs/2509.17421)
Append: [QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models](https://arxiv.org/abs/2509.17428)
Append: [MedFact: A Large-scale Chinese Dataset for Evidence-based Medical Fact-checking of LLM Responses](https://arxiv.org/abs/2509.17436)
Append: [GeoPQA: Bridging the Visual Perception Gap in MLLMs for Geometric Reasoning](https://arxiv.org/abs/2509.17437)
Append: [Filling in the Clinical Gaps in Benchmark: Case for HealthBench for the Japanese medical system](https://arxiv.org/abs/2509.17444)
Append: [Semantic Reformulation Entropy for Robust Hallucination Detection in QA Tasks](https://arxiv.org/abs/2509.17445)
Append: [SLAyiNG: Towards Queer Language Processing](https://arxiv.org/abs/2509.17449)
Append: [Codifying Natural Langauge Tasks](https://arxiv.org/abs/2509.17455)
Append: [PRINCIPLES: Synthetic Strategy Memory for Proactive Dialogue Agents](https://arxiv.org/abs/2509.17459)
Append: [Diagnosing Model Editing via Knowledge Spectrum](https://arxiv.org/abs/2509.17482)
Append: [AttnComp: Attention-Guided Adaptive Context Compression for Retrieval-Augmented Generation](https://arxiv.org/abs/2509.17486)
Append: [MapCoder-Lite: Squeezing Multi-Agent Coding into a Single Small LLM](https://arxiv.org/abs/2509.17489)
Append: [Enhancing Cross-Lingual Transfer through Reversible Transliteration: A Huffman-Based Approach for Low-Resource Languages](https://arxiv.org/abs/2509.17493)
Append: [CorefInst: Leveraging LLMs for Multilingual Coreference Resolution](https://arxiv.org/abs/2509.17505)
Append: [Leveraging Audio-Visual Data to Reduce the Multilingual Gap in Self-Supervised Speech Models](https://arxiv.org/abs/2509.17523)
Append: [Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning](https://arxiv.org/abs/2509.17552)
Append: [Specification-Aware Machine Translation and Evaluation for Purpose Alignment](https://arxiv.org/abs/2509.17559)
Append: [Asking a Language Model for Diverse Responses](https://arxiv.org/abs/2509.17570)
Append: [MSCoRe: A Benchmark for Multi-Stage Collaborative Reasoning in LLM Agents](https://arxiv.org/abs/2509.17628)
Append: [AuditoryBench++: Can Language Models Understand Auditory Knowledge without Hearing?](https://arxiv.org/abs/2509.17641)
Append: [Crosslingual Optimized Metric for Translation Assessment of Indian Languages](https://arxiv.org/abs/2509.17667)
Append: [PG-CE: A Progressive Generation Dataset with Constraint Enhancement for Controllable Text Generation](https://arxiv.org/abs/2509.17669)
Append: [Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG Applications](https://arxiv.org/abs/2509.17671)
Append: [When TableQA Meets Noise: A Dual Denoising Framework for Complex Questions and Large-scale Tables](https://arxiv.org/abs/2509.17680)
Append: [TASO: Task-Aligned Sparse Optimization for Parameter-Efficient Model Adaptation](https://arxiv.org/abs/2509.17688)
Append: [Evaluating LLM-Generated Versus Human-Authored Responses in Role-Play Dialogues](https://arxiv.org/abs/2509.17694)
Append: [Investigating Bias: A Multilingual Pipeline for Generating, Solving, and Evaluating Math Problems with LLMs](https://arxiv.org/abs/2509.17701)
Append: [Breaking Token Into Concepts: Exploring Extreme Compression in Token Representation Via Compositional Shared Semantics](https://arxiv.org/abs/2509.17737)
Append: [Qwen3-Omni Technical Report](https://arxiv.org/abs/2509.17765)
Append: [A State-Update Prompting Strategy for Efficient and Robust Multi-turn Dialogue](https://arxiv.org/abs/2509.17766)
Append: [DIVERS-Bench: Evaluating Language Identification Across Domain Shifts and Code-Switching](https://arxiv.org/abs/2509.17768)
Append: [One Agent to Serve All: a Lite-Adaptive Stylized AI Assistant for Millions of Multi-Style Official Accounts](https://arxiv.org/abs/2509.17788)
Append: [Learning to vary: Teaching LMs to reproduce human linguistic variability in next-word prediction](https://arxiv.org/abs/2509.17794)
Append: [Findings of the Fourth Shared Task on Multilingual Coreference Resolution: Can LLMs Dethrone Traditional Approaches?](https://arxiv.org/abs/2509.17796)
Append: [Everyday Physics in Korean Contexts: A Culturally Grounded Physical Reasoning Benchmark](https://arxiv.org/abs/2509.17807)
Append: [Towards Adaptive Context Management for Intelligent Conversational Question Answering](https://arxiv.org/abs/2509.17829)
Append: [Fine-Grained Detection of AI-Generated Text Using Sentence-Level Segmentation](https://arxiv.org/abs/2509.17830)
Append: [Trust Me, I Can Convince You: The Contextualized Argument Appraisal Framework](https://arxiv.org/abs/2509.17844)
Append: [Make Every Letter Count: Building Dialect Variation Dictionaries from Monolingual Corpora](https://arxiv.org/abs/2509.17855)
Append: [CorPipe at CRAC 2025: Evaluating Multilingual Encoders for Multilingual Coreference Resolution](https://arxiv.org/abs/2509.17858)
Append: [Unsupervised Learning and Representation of Mandarin Tonal Categories by a Generative CNN](https://arxiv.org/abs/2509.17859)
Append: [How Persuasive is Your Context?](https://arxiv.org/abs/2509.17879)
Append: [SiDiaC: Sinhala Diachronic Corpus](https://arxiv.org/abs/2509.17912)
Append: [Improving Zero-shot Sentence Decontextualisation with Content Selection and Planning](https://arxiv.org/abs/2509.17921)
Append: [Transformer-Encoder Trees for Efficient Multilingual Machine Translation and Speech Translation](https://arxiv.org/abs/2509.17930)
Append: [Training-free Truthfulness Detection via Value Vectors in LLMs](https://arxiv.org/abs/2509.17932)
Append: [D-REX: A Benchmark for Detecting Deceptive Reasoning in Large Language Models](https://arxiv.org/abs/2509.17938)
Append: [HICode: Hierarchical Inductive Coding with LLMs](https://arxiv.org/abs/2509.17946)
Append: [Dorabella Cipher as Musical Inspiration](https://arxiv.org/abs/2509.17950)
Append: [Bringing Pedagogy into Focus: Evaluating Virtual Teaching Assistants' Question-Answering in Asynchronous Learning Environments](https://arxiv.org/abs/2509.17961)
Append: [ReDepress: A Cognitive Framework for Detecting Depression Relapse from Social Media](https://arxiv.org/abs/2509.17991)
Append: [Variation in Verification: Understanding Verification Dynamics in Large Language Models](https://arxiv.org/abs/2509.17995)
Append: [WenetSpeech-Chuan: A Large-Scale Sichuanese Corpus with Rich Annotation for Dialectal Speech Processing](https://arxiv.org/abs/2509.18004)
Append: [Cross-Attention is Half Explanation in Speech-to-Text Models](https://arxiv.org/abs/2509.18010)
Append: [RadEval: A framework for radiology text evaluation](https://arxiv.org/abs/2509.18030)
Append: [The PIMMUR Principles: Ensuring Validity in Collective Behavior of LLM Societies](https://arxiv.org/abs/2509.18052)
Append: [TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for \"U-Tsang, Amdo and Kham Speech Dataset Generation](https://arxiv.org/abs/2509.18060)
Append: [ARK-V1: An LLM-Agent for Knowledge Graph Question Answering Requiring Commonsense Reasoning](https://arxiv.org/abs/2509.18063)
Append: [SEQR: Secure and Efficient QR-based LoRA Routing](https://arxiv.org/abs/2509.18093)
Append: [Predicting First Year Dropout from Pre Enrolment Motivation Statements Using Text Mining](https://arxiv.org/abs/2509.16224)
Append: [How Can Quantum Deep Learning Improve Large Language Models?](https://arxiv.org/abs/2509.16244)
Append: [Patterns in the Transition From Founder-Leadership to Community Governance of Open Source](https://arxiv.org/abs/2509.16295)
Append: [How Large Language Models are Designed to Hallucinate](https://arxiv.org/abs/2509.16297)
Append: [Psychometric Personality Shaping Modulates Capabilities and Safety in Language Models](https://arxiv.org/abs/2509.16332)
Append: [Longitudinal and Multimodal Recording System to Capture Real-World Patient-Clinician Conversations for AI and Encounter Research: Protocol](https://arxiv.org/abs/2509.16378)
Append: [Hierarchical Retrieval: The Geometry and a Pretrain-Finetune Recipe](https://arxiv.org/abs/2509.16411)
Append: [AutoArabic: A Three-Stage Framework for Localizing Video-Text Retrieval Benchmarks](https://arxiv.org/abs/2509.16438)
Append: [Evaluating the Effectiveness and Scalability of LLM-Based Data Augmentation for Retrieval](https://arxiv.org/abs/2509.16442)
Append: [Purely Semantic Indexing for LLM-based Generative Recommendation and Retrieval](https://arxiv.org/abs/2509.16446)
Append: [Towards Universal Debiasing for Language Models-based Tabular Data Generation](https://arxiv.org/abs/2509.16475)
Append: [Seeing Culture: A Benchmark for Visual Reasoning and Grounding](https://arxiv.org/abs/2509.16517)
Append: [Advancing Reference-free Evaluation of Video Captions with Factual Analysis](https://arxiv.org/abs/2509.16538)
Append: [Long document summarization using page specific target text alignment and distilling page importance](https://arxiv.org/abs/2509.16539)
Append: [SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward Learning](https://arxiv.org/abs/2509.16548)
Append: [SalaMAnder: Shapley-based Mathematical Expression Attribution and Metric for Chain-of-Thought Reasoning](https://arxiv.org/abs/2509.16561)
Append: [Question Answering with LLMs and Learning from Answer Sets](https://arxiv.org/abs/2509.16590)
Append: [The Role of Vocabularies in Learning Sparse Representations for Ranking](https://arxiv.org/abs/2509.16621)
Append: [When Big Models Train Small Ones: Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs](https://arxiv.org/abs/2509.16633)
Append: [FESTA: Functionally Equivalent Sampling for Trust Assessment of Multimodal LLMs](https://arxiv.org/abs/2509.16648)
Append: [Idiosyncratic Versus Normative Modeling of Atypical Speech Recognition: Dysarthric Case Studies](https://arxiv.org/abs/2509.16718)
Append: [Geometric Mixture Classifier (GMC): A Discriminative Per-Class Mixture of Hyperplanes](https://arxiv.org/abs/2509.16769)
Append: [seqBench: A Tunable Benchmark to Quantify Sequential Reasoning Limits of LLMs](https://arxiv.org/abs/2509.16866)
Append: [Dynamic Expert Specialization: Towards Catastrophic Forgetting-Free Multi-Domain MoE Adaptation](https://arxiv.org/abs/2509.16882)
Append: [DRES: Fake news detection by dynamic representation and ensemble selection](https://arxiv.org/abs/2509.16893)
Append: [SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering Tasks?](https://arxiv.org/abs/2509.16941)
Append: [Localizing Malicious Outputs from CodeLLM](https://arxiv.org/abs/2509.17070)
Append: [SVeritas: Benchmark for Robust Speaker Verification under Diverse Conditions](https://arxiv.org/abs/2509.17091)
Append: [ARE: Scaling Up Agent Environments and Evaluations](https://arxiv.org/abs/2509.17158)
Append: [VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery](https://arxiv.org/abs/2509.17191)
Append: [Causal Representation Learning from Multimodal Clinical Records under Non-Random Modality Missingness](https://arxiv.org/abs/2509.17228)
Append: [MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with RoE](https://arxiv.org/abs/2509.17238)
Append: [Can Agents Judge Systematic Reviews Like Humans? Evaluating SLRs with LLM-based Multi-Agent System](https://arxiv.org/abs/2509.17240)
Append: [CogAtom: From Cognitive Atoms to Olympiad-level Mathematical Reasoning in Large Language Models](https://arxiv.org/abs/2509.17318)
Append: [OpenGVL - Benchmarking Visual Temporal Progress for Data Curation](https://arxiv.org/abs/2509.17321)
Append: [Generalizable End-to-End Tool-Use RL with Synthetic CodeGym](https://arxiv.org/abs/2509.17325)
Append: [Mano Report](https://arxiv.org/abs/2509.17336)
Append: [LLaVul: A Multimodal LLM for Interpretable Vulnerability Reasoning about Source Code](https://arxiv.org/abs/2509.17337)
Append: [Program Synthesis via Test-Time Transduction](https://arxiv.org/abs/2509.17393)
Append: [Autiverse: Eliciting Autistic Adolescents' Daily Narratives through AI-guided Multimodal Journaling](https://arxiv.org/abs/2509.17466)
Append: [LingoQ: Bridging the Gap between ESL Learning and Work through AI-Generated Work-Related Quizzes](https://arxiv.org/abs/2509.17477)
Append: [ChartHal: A Fine-grained Framework Evaluating Hallucination of Large Vision Language Models in Chart Understanding](https://arxiv.org/abs/2509.17481)
Append: [AutiHero: Leveraging Generative AI in Social Narratives to Engage Parents in Story-Driven Behavioral Guidance for Autistic Children](https://arxiv.org/abs/2509.17608)
Append: [ConfClip: Confidence-Weighted and Clipped Reward for Reinforcement Learning in LLMs](https://arxiv.org/abs/2509.17730)
Append: [WISE: Weak-Supervision-Guided Step-by-Step Explanations for Multimodal LLMs in Image Classification](https://arxiv.org/abs/2509.17740)
Append: [From Documents to Database: Failure Modes for Industrial Assets](https://arxiv.org/abs/2509.17834)
Append: [Through the Lens of Human-Human Collaboration: A Configurable Research Platform for Exploring Human-Agent Collaboration](https://arxiv.org/abs/2509.18008)
Append: [Reasoning Core: A Scalable RL Environment for LLM Symbolic Reasoning](https://arxiv.org/abs/2509.18083)
Append: [Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative Decoding](https://arxiv.org/abs/2509.18085)
Append: [OnePiece: Bringing Context Engineering and Reasoning to Industrial Cascade Ranking System](https://arxiv.org/abs/2509.18091)
Append: [MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late Interaction](https://arxiv.org/abs/2509.18095)
Append: [Scaling Efficient LLMs](https://arxiv.org/abs/2402.14746)
Append: [MindRef: Mimicking Human Memory for Hierarchical Reference Retrieval with Fine-Grained Location Awareness](https://arxiv.org/abs/2402.17010)
Append: [Temporal Scaling Law for Large Language Models](https://arxiv.org/abs/2404.17785)
Append: [LLaSA: A Sensor-Aware LLM for Natural Language Reasoning of Human Activity from IMU Data](https://arxiv.org/abs/2406.14498)
Append: [Evaluating Fairness in Large Vision-Language Models Across Diverse Demographic Attributes and Prompts](https://arxiv.org/abs/2406.17974)
Append: [On the Low-Rank Parametrization of Reward Models for Controlled Language Generation](https://arxiv.org/abs/2407.04615)
Append: [Rethinking Backdoor Detection Evaluation for Language Models](https://arxiv.org/abs/2409.00399)
Append: [GALLa: Graph Aligned Large Language Models for Improved Source Code Understanding](https://arxiv.org/abs/2409.04183)
Append: [SouLLMate: An Application Enhancing Diverse Mental Health Support with Adaptive LLMs, Prompt Engineering, and RAG Techniques](https://arxiv.org/abs/2410.16322)
Append: [Bayesian scaling laws for in-context learning](https://arxiv.org/abs/2410.16531)
Append: [Group-SAE: Efficient Training of Sparse Autoencoders for Large Language Models via Layer Groups](https://arxiv.org/abs/2410.21508)
Append: [Rationale-Guided Retrieval Augmented Generation for Medical Question Answering](https://arxiv.org/abs/2411.00300)
Append: [AfriMed-QA: A Pan-African, Multi-Specialty, Medical Question-Answering Benchmark Dataset](https://arxiv.org/abs/2411.15640)
Append: [Measuring Risk of Bias in Biomedical Reports: The RoBBR Benchmark](https://arxiv.org/abs/2411.18831)
Append: [PERL: Pinyin Enhanced Rephrasing Language Model for Chinese ASR N-best Error Correction](https://arxiv.org/abs/2412.03230)
Append: [GRIP: A Graph-Based Reasoning Instruction Producer](https://arxiv.org/abs/2412.08864)
Append: [NILE: Internal Consistency Alignment in Large Language Models](https://arxiv.org/abs/2412.16686)
Append: [ACORD: An Expert-Annotated Retrieval Dataset for Legal Contract Drafting](https://arxiv.org/abs/2501.06582)
Append: [Mitigating Forgetting in LLM Fine-Tuning via Low-Perplexity Token Learning](https://arxiv.org/abs/2501.14315)
Append: [Efficient Beam Search for Large Language Models Using Trie-Based Decoding](https://arxiv.org/abs/2502.00085)
Append: [Attention Sinks: A 'Catch, Tag, Release' Mechanism for Embeddings](https://arxiv.org/abs/2502.00919)
Append: [Adaptive Distraction: Probing LLM Contextual Robustness with Automated Tree Search](https://arxiv.org/abs/2502.01609)
Append: [No Need for Explanations: LLMs can implicitly learn from mistakes in-context](https://arxiv.org/abs/2502.08550)
Append: [Beyond Pairwise: Global Zero-shot Temporal Graph Generation](https://arxiv.org/abs/2502.11114)
Append: [DCAD-2000: A Multilingual Dataset across 2000+ Languages with Data Cleaning as Anomaly Detection](https://arxiv.org/abs/2502.11546)
Append: [Auto-Search and Refinement: An Automated Framework for Gender Bias Mitigation in Large Language Models](https://arxiv.org/abs/2502.11559)
Append: [Evaluating Step-by-step Reasoning Traces: A Survey](https://arxiv.org/abs/2502.12289)
Append: [Large Language Models Badly Generalize across Option Length, Problem Types, and Irrelevant Noun Replacements](https://arxiv.org/abs/2502.12459)
Append: [Reasoning-to-Defend: Safety-Aware Reasoning Can Defend Large Language Models from Jailbreaking](https://arxiv.org/abs/2502.12970)
Append: [Neural Attention Search](https://arxiv.org/abs/2502.13251)
Append: [Translation in the Hands of Many:Centering Lay Users in Machine Translation Interactions](https://arxiv.org/abs/2502.13780)
Append: [Does Reasoning Introduce Bias? A Study of Social Bias Evaluation and Mitigation in LLM Reasoning](https://arxiv.org/abs/2502.15361)
Append: [All-in-one: Understanding and Generation in Multimodal Reasoning with the MAIA Benchmark](https://arxiv.org/abs/2502.16989)
Append: [FoREST: Frame of Reference Evaluation in Spatial Reasoning Tasks](https://arxiv.org/abs/2502.17775)
Append: [Improving the quality of Web-mined Parallel Corpora of Low-Resource Languages using Debiasing Heuristics](https://arxiv.org/abs/2502.19074)
Append: [Beyond Prompting: An Efficient Embedding Framework for Open-Domain Question Answering](https://arxiv.org/abs/2503.01606)
Append: [From Language to Cognition: How LLMs Outgrow the Human Language Network](https://arxiv.org/abs/2503.01830)
Append: [Assumed Identities: Quantifying Gender Bias in Machine Translation of Gender-Ambiguous Occupational Terms](https://arxiv.org/abs/2503.04372)
Append: [WikiBigEdit: Understanding the Limits of Lifelong Knowledge Editing in LLMs](https://arxiv.org/abs/2503.05683)
Append: [ASTRA: A Negotiation Agent with Adaptive and Strategic Reasoning via Tool-integrated Action for Dynamic Offer Optimization](https://arxiv.org/abs/2503.07129)
Append: [Context-aware Biases for Length Extrapolation](https://arxiv.org/abs/2503.08067)
Append: [TLUE: A Tibetan Language Understanding Evaluation Benchmark](https://arxiv.org/abs/2503.12051)
Append: [Can Language Models Follow Multiple Turns of Entangled Instructions?](https://arxiv.org/abs/2503.13222)
Append: [Unmasking Deceptive Visuals: Benchmarking Multimodal Large Language Models on Misleading Chart Question Answering](https://arxiv.org/abs/2503.18172)
Append: [AfroXLMR-Social: Adapting Pre-trained Language Models for African Languages Social Media Text](https://arxiv.org/abs/2503.18247)
Append: [XL-Suite: Cross-Lingual Synthetic Training and Evaluation Data for Open-Ended Generation](https://arxiv.org/abs/2503.22973)
Append: [CoLa: Learning to Interactively Collaborate with Large Language Models](https://arxiv.org/abs/2504.02965)
Append: [Sequential-NIAH: A Needle-In-A-Haystack Benchmark for Extracting Sequential Needles from Long Contexts](https://arxiv.org/abs/2504.04713)
Append: [GRPO-LEAD: A Difficulty-Aware Reinforcement Learning Approach for Concise Mathematical Reasoning in Language Models](https://arxiv.org/abs/2504.09696)
Append: [Improving Instruct Models for Free: A Study on Partial Adaptation](https://arxiv.org/abs/2504.11626)
Append: [MAIN: Mutual Alignment Is Necessary for instruction tuning](https://arxiv.org/abs/2504.12913)
Append: [How Real Are Synthetic Therapy Conversations? Evaluating Fidelity in Prolonged Exposure Dialogues](https://arxiv.org/abs/2504.21800)
Append: [KoACD: The First Korean Adolescent Dataset for Cognitive Distortion Analysis via Role-Switching Multi-LLM Negotiation](https://arxiv.org/abs/2505.00367)
Append: [LCES: Zero-shot Automated Essay Scoring via Pairwise Comparisons Using Large Language Models](https://arxiv.org/abs/2505.08498)
Append: [EAMET: Robust Massive Model Editing via Embedding Alignment Optimization](https://arxiv.org/abs/2505.11876)
Append: [Distribution Prompting: Understanding the Expressivity of Language Models Through the Next-Token Distributions They Can Produce](https://arxiv.org/abs/2505.12244)
Append: [R3: Robust Rubric-Agnostic Reward Models](https://arxiv.org/abs/2505.13388)
Append: [CIE: Controlling Language Model Text Generations Using Continuous Signals](https://arxiv.org/abs/2505.13448)
Append: [JOLT-SQL: Joint Loss Tuning of Text-to-SQL with Confusion-aware Noisy Schema Sampling](https://arxiv.org/abs/2505.14305)
Append: [QA-prompting: Improving Summarization with Large Language Models using Question-Answering](https://arxiv.org/abs/2505.14347)
Append: [Scaling Low-Resource MT via Synthetic Data Generation with LLMs](https://arxiv.org/abs/2505.14423)
Append: [EmoGist: Efficient In-Context Learning for Visual Emotion Understanding](https://arxiv.org/abs/2505.14660)
Append: [The Pursuit of Empathy: Evaluating Small Language Models for PTSD Dialogue Support](https://arxiv.org/abs/2505.15065)
Append: [A Risk Ontology for Evaluating AI-Powered Psychotherapy Virtual Agents](https://arxiv.org/abs/2505.15108)
Append: [TurnaboutLLM: A Deductive Reasoning Benchmark from Detective Games](https://arxiv.org/abs/2505.15712)
Append: [TACO: Enhancing Multimodal In-context Learning via Task Mapping-Guided Sequence Configuration](https://arxiv.org/abs/2505.17098)
Append: [L-MTP: Leap Multi-Token Prediction Beyond Adjacent Context for Large Language Models](https://arxiv.org/abs/2505.17505)
Append: [Revisiting Backdoor Attacks on LLMs: A Stealthy and Practical Poisoning Framework via Harmless Inputs](https://arxiv.org/abs/2505.17601)
Append: [Runaway is Ashamed, But Helpful: On the Early-Exit Behavior of Large Language Model-based Agents in Embodied Environments](https://arxiv.org/abs/2505.17616)
Append: [Discriminating Form and Meaning in Multilingual Models with Minimal-Pair ABX Tasks](https://arxiv.org/abs/2505.17747)
Append: [How Is LLM Reasoning Distracted by Irrelevant Context? An Analysis Using a Controlled Benchmark](https://arxiv.org/abs/2505.18761)
Append: [Uncertainty-Aware Attention Heads: Efficient Unsupervised Uncertainty Quantification for LLMs](https://arxiv.org/abs/2505.20045)
Append: [Large Language Models Meet Knowledge Graphs for Question Answering: Synthesis and Opportunities](https://arxiv.org/abs/2505.20099)
Append: [We Need to Measure Data Diversity in NLP -- Better and Broader](https://arxiv.org/abs/2505.20264)
Append: [Does quantization affect models' performance on long-context tasks?](https://arxiv.org/abs/2505.20276)
Append: [Inceptive Transformers: Enhancing Contextual Representations through Multi-Scale Feature Learning Across Domains and Languages](https://arxiv.org/abs/2505.20496)
Append: [Fluent but Foreign: Even Regional LLMs Lack Cultural Alignment](https://arxiv.org/abs/2505.21548)
Append: [MAKIEval: A Multilingual Automatic WiKidata-based Framework for Cultural Awareness Evaluation for LLMs](https://arxiv.org/abs/2505.21693)
Append: [Multilingual vs Crosslingual Retrieval of Fact-Checked Claims: A Tale of Two Approaches](https://arxiv.org/abs/2505.22118)
Append: [LASER: Stratified Selective Sampling for Instruction Tuning with Dedicated Scoring Strategy](https://arxiv.org/abs/2505.22157)
Append: [DyePack: Provably Flagging Test Set Contamination in LLMs Using Backdoors](https://arxiv.org/abs/2505.23001)
Append: [Neither Stochastic Parroting nor AGI: LLMs Solve Tasks through Context-Directed Extrapolation from Training Data Priors](https://arxiv.org/abs/2505.23323)
Append: [From Chat Logs to Collective Insights: Aggregative Question Answering](https://arxiv.org/abs/2505.23765)
Append: [CaMMT: Benchmarking Culturally Aware Multimodal Machine Translation](https://arxiv.org/abs/2505.24456)
Append: [LaMP-QA: A Benchmark for Personalized Long-form Question Answering](https://arxiv.org/abs/2506.00137)
Append: [PAKTON: A Multi-Agent Framework for Question Answering in Long Legal Agreements](https://arxiv.org/abs/2506.00608)
Append: [ESGenius: Benchmarking LLMs on Environmental, Social, and Governance (ESG) and Sustainability Knowledge](https://arxiv.org/abs/2506.01646)
Append: [EuroGEST: Investigating gender stereotypes in multilingual language models](https://arxiv.org/abs/2506.03867)
Append: [TableEval: A Real-World Benchmark for Complex, Multilingual, and Multi-Structured Table Question Answering](https://arxiv.org/abs/2506.03949)
Append: [Tau-Eval: A Unified Evaluation Framework for Useful and Private Text Anonymization](https://arxiv.org/abs/2506.05979)
Append: [AdvSumm: Adversarial Training for Bias Mitigation in Text Summarization](https://arxiv.org/abs/2506.06273)
Append: [ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning](https://arxiv.org/abs/2506.09513)
Append: [Breaking the Reviewer: Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks](https://arxiv.org/abs/2506.11113)
Append: [Med-PRM: Medical Reasoning Models with Stepwise, Guideline-verified Process Rewards](https://arxiv.org/abs/2506.11474)
Append: [Feedback Friction: LLMs Struggle to Fully Incorporate External Feedback](https://arxiv.org/abs/2506.11930)
Append: [SoundMind: RL-Incentivized Logic Reasoning for Audio-Language Models](https://arxiv.org/abs/2506.12935)
Append: [GuiLoMo: Allocating Expert Number and Rank for LoRA-MoE via Bilevel Optimization with GuidedSelection Vectors](https://arxiv.org/abs/2506.14646)
Append: [DiscoSG: Towards Discourse-Level Text Scene Graph Parsing through Iterative Graph Refinement](https://arxiv.org/abs/2506.15583)
Append: [MIST: Jailbreaking Black-box Large Language Models via Iterative Semantic Tuning](https://arxiv.org/abs/2506.16792)
Append: [VAT-KG: Knowledge-Intensive Multimodal Knowledge Graph Dataset for Retrieval-Augmented Generation](https://arxiv.org/abs/2506.21556)
Append: [Datasets for Fairness in Language Models: An In-Depth Survey](https://arxiv.org/abs/2506.23411)
Append: [Advanced Financial Reasoning at Scale: A Comprehensive Evaluation of Large Language Models on CFA Level III](https://arxiv.org/abs/2507.02954)
Append: [MOMENTS: A Comprehensive Multimodal Benchmark for Theory of Mind](https://arxiv.org/abs/2507.04415)
Append: [Journalism-Guided Agentic In-Context Learning for News Stance Detection](https://arxiv.org/abs/2507.11049)
Append: [DCR: Quantifying Data Contamination in LLMs Evaluation](https://arxiv.org/abs/2507.11405)
Append: [Automating Steering for Safe Multimodal Large Language Models](https://arxiv.org/abs/2507.13255)
Append: [Collaborative Rational Speech Act: Pragmatic Reasoning for Multi-Turn Dialog](https://arxiv.org/abs/2507.14063)
Append: [PromptSuite: A Task-Agnostic Framework for Multi-Prompt Generation](https://arxiv.org/abs/2507.14913)
Append: [A Similarity Measure for Comparing Conversational Dynamics](https://arxiv.org/abs/2507.18956)
Append: [Latent Inter-User Difference Modeling for LLM Personalization](https://arxiv.org/abs/2507.20849)
Append: [The Missing Parts: Augmenting Fact Verification with Half-Truth Detection](https://arxiv.org/abs/2508.00489)
Append: [Applying Psychometrics to Large Language Model Simulated Populations: Recreating the HEXACO Personality Inventory Experiment with Generative Agents](https://arxiv.org/abs/2508.00742)
Append: [Steering Towards Fairness: Mitigating Political Bias in LLMs](https://arxiv.org/abs/2508.08846)
Append: [Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language Models](https://arxiv.org/abs/2508.09138)
Append: [A Survey of Cognitive Distortion Detection and Classification in NLP](https://arxiv.org/abs/2508.09878)
Append: [Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values Understanding](https://arxiv.org/abs/2508.13804)
Append: [Mini-Omni-Reasoner: Token-Level Thinking-in-Speaking in Large Speech Models](https://arxiv.org/abs/2508.15827)
Append: [Feel the Difference? A Comparative Analysis of Emotional Arcs in Real and LLM-Generated CBT Sessions](https://arxiv.org/abs/2508.20764)
Append: [Reward-Weighted Sampling: Enhancing Non-Autoregressive Characteristics in Masked Diffusion LLMs](https://arxiv.org/abs/2509.00707)
Append: [EviNote-RAG: Enhancing RAG Models via Answer-Supportive Evidence Notes](https://arxiv.org/abs/2509.00877)
Append: [A Dynamic Fusion Model for Consistent Crisis Response](https://arxiv.org/abs/2509.01053)
Append: [Speaking at the Right Level: Literacy-Controlled Counterspeech Generation with RAG-RL](https://arxiv.org/abs/2509.01058)
Append: [Measuring Scalar Constructs in Social Science with LLMs](https://arxiv.org/abs/2509.03116)
Append: [PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference](https://arxiv.org/abs/2509.04467)
Append: [The Good, the Bad and the Constructive: Automatically Measuring Peer Review's Utility for Authors](https://arxiv.org/abs/2509.04484)
Append: [Alpha-GPT: Human-AI Interactive Alpha Mining for Quantitative Investment](https://arxiv.org/abs/2308.00016)
Append: [Audio Contrastive-based Fine-tuning: Decoupling Representation Learning and Classification](https://arxiv.org/abs/2309.11895)
Append: [Post-hoc Reward Calibration: A Case Study on Length Bias](https://arxiv.org/abs/2409.17407)
Append: [Step Guided Reasoning: Improving Mathematical Reasoning using Guidance Generation and Step Reasoning](https://arxiv.org/abs/2410.19817)
Append: [Both Text and Images Leaked! A Systematic Analysis of Data Contamination in Multimodal LLM](https://arxiv.org/abs/2411.03823)
Append: [OptiChat: Bridging Optimization Models and Practitioners with Large Language Models](https://arxiv.org/abs/2501.08406)
Append: [GRADIEND: Feature Learning within Neural Networks Exemplified through Biases](https://arxiv.org/abs/2502.01406)
Append: [EquiBench: Benchmarking Large Language Models' Reasoning about Program Semantics via Equivalence Checking](https://arxiv.org/abs/2502.12466)
Append: [Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security Analysis](https://arxiv.org/abs/2502.20383)
Append: [Audio-Reasoner: Improving Reasoning Capability in Large Audio Language Models](https://arxiv.org/abs/2503.02318)
Append: [CAARMA: Class Augmentation with Adversarial Mixup Regularization](https://arxiv.org/abs/2503.16718)
Append: [VQToken: Neural Discrete Token Representation Learning for Extreme Token Reduction in Video Large Language Models](https://arxiv.org/abs/2503.16980)
Append: [CoSIL: Issue Localization via LLM-Driven Code Graph Searching](https://arxiv.org/abs/2503.22424)
Append: [LEO-MINI: An Efficient Multimodal Large Language Model using Conditional Token Reduction and Mixture of Multi-Modal Experts](https://arxiv.org/abs/2504.04653)
Append: [Sugar-Coated Poison: Benign Generation Unlocks LLM Jailbreaking](https://arxiv.org/abs/2504.05652)
Append: [Survey of Video Diffusion Models: Foundations, Implementations, and Applications](https://arxiv.org/abs/2504.16081)
Append: [Creating General User Models from Computer Use](https://arxiv.org/abs/2505.10831)
Append: [LightRetriever: A LLM-based Text Retrieval Architecture with Extremely Faster Query Inference](https://arxiv.org/abs/2505.12260)
Append: [SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle Generation from SAT Formulas](https://arxiv.org/abs/2505.14615)
Append: [Tool Preferences in Agentic LLMs are Unreliable](https://arxiv.org/abs/2505.18135)
Append: [How Much Do Large Language Models Know about Human Motion? A Case Study in 3D Avatar Control](https://arxiv.org/abs/2505.21531)
Append: [Enhancing Study-Level Inference from Clinical Trial Papers via Reinforcement Learning-Based Numeric Reasoning](https://arxiv.org/abs/2505.22928)
Append: [The Automated but Risky Game: Modeling and Benchmarking Agent-to-Agent Negotiations and Transactions in Consumer Markets](https://arxiv.org/abs/2506.00073)
Append: [Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization](https://arxiv.org/abs/2506.04039)
Append: [Matter-of-Fact: A Benchmark for Verifying the Feasibility of Literature-Supported Claims in Materials Science](https://arxiv.org/abs/2506.04410)
Append: [Time to Talk: LLM Agents for Asynchronous Group Communication in Mafia Games](https://arxiv.org/abs/2506.05309)
Append: [VisText-Mosquito: A Unified Multimodal Benchmark Dataset for Visual Detection, Segmentation, and Textual Reasoning on Mosquito Breeding Sites](https://arxiv.org/abs/2506.14629)
Append: [Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for Visual Reasoning](https://arxiv.org/abs/2507.05255)
Append: [Agentic AI with Orchestrator-Agent Trust: A Modular Visual Classification Framework with Trust-Aware Orchestration and RAG-Based Reasoning](https://arxiv.org/abs/2507.10571)
Append: [Jailbreak-Tuning: Models Efficiently Learn Jailbreak Susceptibility](https://arxiv.org/abs/2507.11630)
Append: [From Roots to Rewards: Dynamic Tree Reasoning with Reinforcement Learning](https://arxiv.org/abs/2507.13142)
Append: [Dissecting Persona-Driven Reasoning in Language Models via Activation Patching](https://arxiv.org/abs/2507.20936)
Append: [AgentMaster: A Multi-Agent Conversational Framework Using A2A and MCP Protocols for Multimodal Information Retrieval and Analysis](https://arxiv.org/abs/2507.21105)
Append: [Retrieval Enhanced Feedback via In-context Neural Error-book](https://arxiv.org/abs/2508.16313)
Append: [GLSim: Detecting Object Hallucinations in LVLMs via Global-Local Similarity](https://arxiv.org/abs/2508.19972)
Append: [TSPC: A Two-Stage Phoneme-Centric Architecture for code-switching Vietnamese-English Speech Recognition](https://arxiv.org/abs/2509.05983)
append_entries: 352
Finish: 2025-09-23 04:24:31.964514
------------------------------------------------------
Started: 2025-09-23 06:25:21.580862
Existing_entries: 1352
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 707
Summarized using GPT-3.5-turbo
Append: [PDFMathTranslate: Scientific Document Translation Preserving Layouts](https://arxiv.org/abs/2507.03009)
Token length: 1610
Summarized using GPT-3.5-turbo
Append: [UR$^2$: Unify RAG and Reasoning through Reinforcement Learning](https://arxiv.org/abs/2508.06165)
Token length: 1297
Summarized using GPT-3.5-turbo
Append: [Factuality Beyond Coherence: Evaluating LLM Watermarking Methods for Medical Texts](https://arxiv.org/abs/2509.07755)
Token length: 1555
Summarized using GPT-3.5-turbo
Append: [SciNLP: A Domain-Specific Benchmark for Full-Text Scientific Entity and Relation Extraction in NLP](https://arxiv.org/abs/2509.07801)
Token length: 1722
Summarized using GPT-3.5-turbo
Append: [Stated Preference for Interaction and Continued Engagement (SPICE): Evaluating an LLM's Willingness to Re-engage in Conversation](https://arxiv.org/abs/2509.09043)
Token length: 1903
Summarized using GPT-3.5-turbo
Append: [The Thinking Therapist: Training Large Language Models to Deliver Acceptance and Commitment Therapy using Supervised Fine-Tuning and Odds Ratio Policy Optimization](https://arxiv.org/abs/2509.09712)
Token length: 1523
Summarized using GPT-3.5-turbo
Append: [VStyle: A Benchmark for Voice Style Adaptation with Spoken Instructions](https://arxiv.org/abs/2509.09716)
append_entries: 7
Finish: 2025-09-23 06:25:39.061998
------------------------------------------------------
Started: 2025-09-23 08:21:17.552881
Existing_entries: 1007
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-23 08:21:18.300642
------------------------------------------------------
Started: 2025-09-23 10:16:53.811971
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-23 10:16:54.525700
------------------------------------------------------
Started: 2025-09-23 12:32:55.666426
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-23 12:32:56.385244
------------------------------------------------------
Started: 2025-09-23 14:16:14.027455
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-23 14:16:14.749672
------------------------------------------------------
Started: 2025-09-23 16:20:11.617866
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-23 16:20:12.421117
------------------------------------------------------
Started: 2025-09-23 18:23:47.287059
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-23 18:23:48.052420
------------------------------------------------------
Started: 2025-09-23 20:17:25.634438
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-23 20:17:26.364354
------------------------------------------------------
Started: 2025-09-23 22:14:36.847586
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-23 22:14:37.564974
------------------------------------------------------
Started: 2025-09-24 01:13:19.485381
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-24 01:13:20.274586
------------------------------------------------------
Started: 2025-09-24 02:56:33.354049
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-24 02:56:34.099915
------------------------------------------------------
Started: 2025-09-24 04:22:53.542913
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1677
Summarized using GPT-3.5-turbo
Append: [Dynamic Prompt Fusion for Multi-Task and Cross-Domain Adaptation in LLMs](https://arxiv.org/abs/2509.18113)
Token length: 996
Summarized using GPT-3.5-turbo
Append: [GAUSS: Benchmarking Structured Mathematical Skills for Large Language Models](https://arxiv.org/abs/2509.18122)
Token length: 1495
Summarized using GPT-3.5-turbo
Append: [Event Causality Identification with Synthetic Control](https://arxiv.org/abs/2509.18156)
Token length: 1211
Summarized using GPT-3.5-turbo
Append: [ZERA: Zero-init Instruction Evolving Refinement Agent - From Zero Instructions to Structured Prompts via Principle-based Optimization](https://arxiv.org/abs/2509.18158)
Token length: 1250
Summarized using GPT-3.5-turbo
Append: [Thinking in a Crowd: How Auxiliary Information Shapes LLM Reasoning](https://arxiv.org/abs/2509.18163)
Token length: 1648
Summarized using GPT-3.5-turbo
Append: [SIRAG: Towards Stable and Interpretable RAG with A Process-Supervised Multi-Agent Framework](https://arxiv.org/abs/2509.18167)
Token length: 1463
Summarized using GPT-3.5-turbo
Append: [ERFC: Happy Customers with Emotion Recognition and Forecasting in Conversation in Call Centers](https://arxiv.org/abs/2509.18175)
Token length: 1037
Summarized using GPT-3.5-turbo
Append: [Evaluating Large Language Models for Detecting Antisemitism](https://arxiv.org/abs/2509.18293)
Token length: 1813
Summarized using GPT-3.5-turbo
Append: [Exploiting Tree Structure for Credit Assignment in RL Training of LLMs](https://arxiv.org/abs/2509.18314)
Token length: 1843
Summarized using GPT-3.5-turbo
Append: [Brittleness and Promise: Knowledge Graph Based Reward Modeling for Diagnostic Reasoning](https://arxiv.org/abs/2509.18316)
Token length: 1718
Summarized using GPT-3.5-turbo
Append: [Speculate Deep and Accurate: Lossless and Training-Free Acceleration for Offloaded LLMs via Substitute Speculative Decoding](https://arxiv.org/abs/2509.18344)
Token length: 969
Summarized using GPT-3.5-turbo
Append: [Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents](https://arxiv.org/abs/2509.18360)
Token length: 1278
Summarized using GPT-3.5-turbo
Append: [Interactive Real-Time Speaker Diarization Correction with Human Feedback](https://arxiv.org/abs/2509.18377)
Token length: 1593
Summarized using GPT-3.5-turbo
Append: [NormGenesis: Multicultural Dialogue Generation via Exemplar-Guided Social Norm Modeling and Violation Recovery](https://arxiv.org/abs/2509.18395)
Token length: 1229
Summarized using GPT-3.5-turbo
Append: [Evaluating the Creativity of LLMs in Persian Literary Text Generation](https://arxiv.org/abs/2509.18401)
Token length: 1954
Summarized using GPT-3.5-turbo
Append: [Developing an AI framework to automatically detect shared decision-making in patient-doctor conversations](https://arxiv.org/abs/2509.18439)
Token length: 1167
Summarized using GPT-3.5-turbo
Append: [CogniLoad: A Synthetic Natural Language Reasoning Benchmark With Tunable Length, Intrinsic Difficulty, and Distractor Density](https://arxiv.org/abs/2509.18458)
Token length: 1684
Summarized using GPT-3.5-turbo
Append: [LAWCAT: Efficient Distillation from Quadratic to Linear Attention with Convolution across Tokens for Long Context Modeling](https://arxiv.org/abs/2509.18467)
Token length: 1881
Summarized using GPT-3.5-turbo
Append: [Actions Speak Louder than Prompts: A Large-Scale Study of LLMs for Graph Inference](https://arxiv.org/abs/2509.18487)
Token length: 920
Summarized using GPT-3.5-turbo
Append: [A Rhythm-Aware Phrase Insertion for Classical Arabic Poetry Composition](https://arxiv.org/abs/2509.18514)
Token length: 1163
Summarized using GPT-3.5-turbo
Append: [Trace Is In Sentences: Unbiased Lightweight ChatGPT-Generated Text Detector](https://arxiv.org/abs/2509.18535)
Token length: 1322
Summarized using GPT-3.5-turbo
Append: [CCQA: Generating Question from Solution Can Improve Inference-Time Reasoning in SLMs](https://arxiv.org/abs/2509.18536)
Token length: 1163
Summarized using GPT-3.5-turbo
Append: [Prior-based Noisy Text Data Filtering: Fast and Strong Alternative For Perplexity](https://arxiv.org/abs/2509.18577)
Token length: 1120
Summarized using GPT-3.5-turbo
Append: [TsqLoRA: Towards Sensitivity and Quality Low-Rank Adaptation for Efficient Fine-Tuning](https://arxiv.org/abs/2509.18585)
Token length: 992
Summarized using GPT-3.5-turbo
Append: [UniECG: Understanding and Generating ECG in One Unified Model](https://arxiv.org/abs/2509.18588)
Token length: 1270
Summarized using GPT-3.5-turbo
Append: [A Good Plan is Hard to Find: Aligning Models with Preferences is Misaligned with What Helps Users](https://arxiv.org/abs/2509.18632)
Token length: 1082
Summarized using GPT-3.5-turbo
Append: [Consistency-Aware Parameter-Preserving Knowledge Editing Framework for Multi-Hop Question Answering](https://arxiv.org/abs/2509.18655)
Token length: 996
Summarized using GPT-3.5-turbo
Append: [Analyzing Uncertainty of LLM-as-a-Judge: Interval Evaluations with Conformal Prediction](https://arxiv.org/abs/2509.18658)
Token length: 1279
Summarized using GPT-3.5-turbo
Append: [MemOrb: A Plug-and-Play Verbal-Reinforcement Memory Layer for E-Commerce Customer Service](https://arxiv.org/abs/2509.18713)
Token length: 1375
Summarized using GPT-3.5-turbo
Append: [LOTUSDIS: A Thai far-field meeting corpus for robust conversational ASR](https://arxiv.org/abs/2509.18722)
Token length: 1602
Summarized using GPT-3.5-turbo
Append: [Global-Recent Semantic Reasoning on Dynamic Text-Attributed Graphs with Large Language Models](https://arxiv.org/abs/2509.18742)
Token length: 1332
Summarized using GPT-3.5-turbo
Append: [False Friends Are Not Foes: Investigating Vocabulary Overlap in Multilingual Language Models](https://arxiv.org/abs/2509.18750)
Token length: 1328
Summarized using GPT-3.5-turbo
Append: [When Long Helps Short: How Context Length in Supervised Fine-tuning Affects Behavior of Large Language Models](https://arxiv.org/abs/2509.18762)
Token length: 1181
Summarized using GPT-3.5-turbo
Append: [Financial Risk Relation Identification through Dual-view Adaptation](https://arxiv.org/abs/2509.18775)
Token length: 1897
Summarized using GPT-3.5-turbo
Append: [AECBench: A Hierarchical Benchmark for Knowledge Evaluation of Large Language Models in the AEC Field](https://arxiv.org/abs/2509.18776)
Token length: 1108
Summarized using GPT-3.5-turbo
Append: [Beyond the Leaderboard: Understanding Performance Disparities in Large Language Models via Model Diffing](https://arxiv.org/abs/2509.18792)
Token length: 1268
Summarized using GPT-3.5-turbo
Append: [MAPEX: A Multi-Agent Pipeline for Keyphrase Extraction](https://arxiv.org/abs/2509.18813)
Token length: 1298
Summarized using GPT-3.5-turbo
Append: [Are Smaller Open-Weight LLMs Closing the Gap to Proprietary Models for Biomedical Question Answering?](https://arxiv.org/abs/2509.18843)
Token length: 1495
Summarized using GPT-3.5-turbo
Append: [Multi-Hierarchical Feature Detection for Large Language Model Generated Text](https://arxiv.org/abs/2509.18862)
Token length: 1360
Summarized using GPT-3.5-turbo
Append: [Diversity Boosts AI-Generated Text Detection](https://arxiv.org/abs/2509.18880)
Token length: 1081
Summarized using GPT-3.5-turbo
Append: [Extractive Fact Decomposition for Interpretable Natural Language Inference in one Forward Pass](https://arxiv.org/abs/2509.18901)
Token length: 1164
Summarized using GPT-3.5-turbo
Append: [DTW-Align: Bridging the Modality Gap in End-to-End Speech Translation with Dynamic Time Warping Alignment](https://arxiv.org/abs/2509.18987)
Token length: 1218
Summarized using GPT-3.5-turbo
Append: [Investigating Test-Time Scaling with Reranking for Machine Translation](https://arxiv.org/abs/2509.19020)
Token length: 1099
Summarized using GPT-3.5-turbo
Append: [Charting a Decade of Computational Linguistics in Italy: The CLiC-it Corpus](https://arxiv.org/abs/2509.19033)
Token length: 1549
Summarized using GPT-3.5-turbo
Append: [Pathways of Thoughts: Multi-Directional Thinking for Long-form Personalized Question Answering](https://arxiv.org/abs/2509.19094)
Token length: 859
Summarized using GPT-3.5-turbo
Append: [Are most sentences unique? An empirical examination of Chomskyan claims](https://arxiv.org/abs/2509.19108)
Token length: 1314
Summarized using GPT-3.5-turbo
Append: [Human-Annotated NER Dataset for the Kyrgyz Language](https://arxiv.org/abs/2509.19109)
Token length: 1081
Summarized using GPT-3.5-turbo
Append: [Context-Aware Hierarchical Taxonomy Generation for Scientific Papers via LLM-Guided Multi-Aspect Clustering](https://arxiv.org/abs/2509.19125)
Token length: 1075
Summarized using GPT-3.5-turbo
Append: [Anecdoctoring: Automated Red-Teaming Across Language and Place](https://arxiv.org/abs/2509.19143)
Token length: 838
Summarized using GPT-3.5-turbo
Append: [Measuring AI "Slop" in Text](https://arxiv.org/abs/2509.19163)
Append: [Soft Tokens, Hard Truths](https://arxiv.org/abs/2509.19170)
Append: [Online Process Reward Leanring for Agentic Reinforcement Learning](https://arxiv.org/abs/2509.19199)
Append: [Steering Multimodal Large Language Models Decoding for Context-Aware Safety](https://arxiv.org/abs/2509.19212)
Append: [Systematic Comparative Analysis of Large Pretrained Language Models on Contextualized Medication Event Extraction](https://arxiv.org/abs/2509.19224)
Append: [CompLLM: Compression for Long Context Q&A](https://arxiv.org/abs/2509.19228)
Append: [Reinforcement Learning on Pre-Training Data](https://arxiv.org/abs/2509.19249)
Append: [Extracting Conceptual Spaces from LLMs Using Prototype Embeddings](https://arxiv.org/abs/2509.19269)
Append: [SloPalSpeech: A 2,8000-Hour Slovak Speech Corpus from Parliamentary Data](https://arxiv.org/abs/2509.19270)
Append: [WolBanking77: Wolof Banking Speech Intent Classification Dataset](https://arxiv.org/abs/2509.19271)
Append: [DRISHTIKON: A Multimodal Multilingual Benchmark for Testing Language Models' Understanding on Indian Culture](https://arxiv.org/abs/2509.19274)
Append: [Safe-SAIL: Towards a Fine-grained Safety Landscape of Large Language Models via Sparse Autoencoder Interpretation Framework](https://arxiv.org/abs/2509.18127)
Append: [PiMoE: Token-Level Routing for Integrating High-Precision Computation and Reasoning](https://arxiv.org/abs/2509.18169)
Append: [TurnBack: A Geospatial Route Cognition Benchmark for Large Language Models through Reverse Route](https://arxiv.org/abs/2509.18173)
Append: [Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR](https://arxiv.org/abs/2509.18174)
Append: [Conversational Orientation Reasoning: Egocentric-to-Allocentric Navigation with Multimodal Chain-of-Thought](https://arxiv.org/abs/2509.18200)
Append: [The Illusion of Readiness: Stress Testing Large Frontier Models on Multimodal Medical Benchmarks](https://arxiv.org/abs/2509.18234)
Append: [Memory-QA: Answering Recall Questions Based on Multimodal Memories](https://arxiv.org/abs/2509.18436)
Append: [No Verifiable Reward for Prosody: Toward Preference-Guided Prosody Learning in TTS](https://arxiv.org/abs/2509.18531)
Append: [HarmoniFuse: A Component-Selective and Prompt-Adaptive Framework for Multi-Task Speech Language Modeling](https://arxiv.org/abs/2509.18570)
Append: [Teaching Audio Models to Reason: A Unified Framework for Source- and Layer-wise Distillation](https://arxiv.org/abs/2509.18579)
Append: [OraPO: Oracle-educated Reinforcement Learning for Data-efficient and Factual Radiology Report Generation](https://arxiv.org/abs/2509.18600)
Append: [Agentic AutoSurvey: Let LLMs Survey LLMs](https://arxiv.org/abs/2509.18661)
Append: [Pay More Attention To Audio: Mitigating Imbalance of Cross-Modal Attention in Large Audio Language Models](https://arxiv.org/abs/2509.18816)
Append: [Failure Makes the Agent Stronger: Enhancing Accuracy through Structured Reflection for Reliable Tool Interactions](https://arxiv.org/abs/2509.18847)
Append: [VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction](https://arxiv.org/abs/2509.19002)
Append: [ColorBlindnessEval: Can Vision-Language Models Pass Color Blindness Tests?](https://arxiv.org/abs/2509.19070)
Append: [Citrus-V: Advancing Medical Foundation Models with Unified Medical Image Grounding for Clinical Reasoning](https://arxiv.org/abs/2509.19090)
Append: [Finding My Voice: Generative Reconstruction of Disordered Speech for Automated Clinical Evaluation](https://arxiv.org/abs/2509.19231)
Append: [Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from the Arab World](https://arxiv.org/abs/2509.19265)
Append: [LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via a Hybrid Architecture](https://arxiv.org/abs/2409.02889)
Append: [Enhancing Unsupervised Sentence Embeddings via Knowledge-Driven Data Augmentation and Gaussian-Decayed Contrastive Learning](https://arxiv.org/abs/2409.12887)
Append: [Post-hoc Study of Climate Microtargeting on Social Media Ads with LLMs: Thematic Insights and Fairness Evaluation](https://arxiv.org/abs/2410.05401)
Append: [Exploring Model Kinship for Merging Large Language Models](https://arxiv.org/abs/2410.12613)
Append: [Language Models as Causal Effect Generators](https://arxiv.org/abs/2411.08019)
Append: [Compositional Phoneme Approximation for L1-Grounded L2 Pronunciation Training](https://arxiv.org/abs/2411.10927)
Append: [Improving Low-Resource Sequence Labeling with Knowledge Fusion and Contextual Label Explanations](https://arxiv.org/abs/2501.19093)
Append: [VLDBench Evaluating Multimodal Disinformation with Regulatory Alignment](https://arxiv.org/abs/2502.11361)
Append: [Language Models Can Predict Their Own Behavior](https://arxiv.org/abs/2502.13329)
Append: [Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests](https://arxiv.org/abs/2502.14359)
Append: [LightThinker: Thinking Step-by-Step Compression](https://arxiv.org/abs/2502.15589)
Append: [Can LLMs Explain Themselves Counterfactually?](https://arxiv.org/abs/2502.18156)
Append: [Anything Goes? A Crosslinguistic Study of (Im)possible Language Learning in LMs](https://arxiv.org/abs/2502.18795)
Append: [Promote, Suppress, Iterate: How Language Models Answer One-to-Many Factual Queries](https://arxiv.org/abs/2502.20475)
Append: [CODI: Compressing Chain-of-Thought into Continuous Space via Self-Distillation](https://arxiv.org/abs/2502.21074)
Append: [CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners](https://arxiv.org/abs/2503.16356)
Append: [LookAhead Tuning: Safer Language Models via Partial Answer Previews](https://arxiv.org/abs/2503.19041)
Append: [Pandora: A Code-Driven Large Language Model Agent for Unified Reasoning Across Diverse Structured Knowledge](https://arxiv.org/abs/2504.12734)
Append: [Think in Safety: Unveiling and Mitigating Safety Alignment Collapse in Multimodal Large Reasoning Model](https://arxiv.org/abs/2505.06538)
Append: [Benchmarking Critical Questions Generation: A Challenging Reasoning Task for Large Language Models](https://arxiv.org/abs/2505.11341)
Append: [Disambiguation in Conversational Question Answering in the Era of LLMs and Agents: A Survey](https://arxiv.org/abs/2505.12543)
Append: [Memorization or Reasoning? Exploring the Idiom Understanding of LLMs](https://arxiv.org/abs/2505.16216)
Append: [Large Language Models Implicitly Learn to See and Hear Just By Reading](https://arxiv.org/abs/2505.17091)
Append: [Large Language Models Do Multi-Label Classification Differently](https://arxiv.org/abs/2505.17510)
Append: [NileChat: Towards Linguistically Diverse and Culturally Aware LLMs for Local Communities](https://arxiv.org/abs/2505.18383)
Append: [Unraveling Misinformation Propagation in LLM Reasoning](https://arxiv.org/abs/2505.18555)
Append: [LiTEx: A Linguistic Taxonomy of Explanations for Understanding Within-Label Variation in Natural Language Inference](https://arxiv.org/abs/2505.22848)
Append: [Please Translate Again: Two Simple Experiments on Whether Human-Like Reasoning Helps Translation](https://arxiv.org/abs/2506.04521)
Append: [LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure Profiles](https://arxiv.org/abs/2506.06561)
Append: [Combining Constrained and Unconstrained Decoding via Boosting: BoostCD and Its Application to Information Extraction](https://arxiv.org/abs/2506.14901)
Append: [A suite of allotaxonometric tools for the comparison of complex systems using rank-turbulence divergence](https://arxiv.org/abs/2506.21808)
Append: [T-Detect: Tail-Aware Statistical Normalization for Robust Detection of Adversarial Machine-Generated Text](https://arxiv.org/abs/2507.23577)
Append: [AI-Generated Text is Non-Stationary: Detection via Temporal Tomography](https://arxiv.org/abs/2508.01754)
Append: [Columbo: Expanding Abbreviated Column Names for Tabular Data Using Large Language Models](https://arxiv.org/abs/2508.09403)
Append: [Reward-Shifted Speculative Sampling Is An Efficient Test-Time Weak-to-Strong Aligner](https://arxiv.org/abs/2508.15044)
Append: [Identifying and Answering Questions with False Assumptions: An Interpretable Approach](https://arxiv.org/abs/2508.15139)
Append: [Linguistic Neuron Overlap Patterns to Facilitate Cross-lingual Transfer on Low-resource Languages](https://arxiv.org/abs/2508.17078)
Append: [T2R-bench: A Benchmark for Generating Article-Level Reports from Real World Industrial Tables](https://arxiv.org/abs/2508.19813)
Append: [Mind the Gap: Evaluating Model- and Agentic-Level Vulnerabilities in LLMs with Action Graphs](https://arxiv.org/abs/2509.04802)
Append: [Improving Image Captioning Descriptiveness by Ranking and LLM-based Fusion](https://arxiv.org/abs/2306.11593)
Append: [Is Pre-training Truly Better Than Meta-Learning?](https://arxiv.org/abs/2306.13841)
Append: [MediSyn: A Generalist Text-Guided Latent Diffusion Model For Diverse Medical Image Synthesis](https://arxiv.org/abs/2405.09806)
Append: [DOTA: Distributional Test-Time Adaptation of Vision-Language Models](https://arxiv.org/abs/2409.19375)
Append: [EMMA: End-to-End Multimodal Model for Autonomous Driving](https://arxiv.org/abs/2410.23262)
Append: [Large Vision-Language Model Alignment and Misalignment: A Survey Through the Lens of Explainability](https://arxiv.org/abs/2501.01346)
Append: [Fine-Tuning is Subgraph Search: A New Lens on Learning Dynamics](https://arxiv.org/abs/2502.06106)
Append: [DeepResonance: Enhancing Multimodal Music Understanding via Music-centric Multi-way Instruction Tuning](https://arxiv.org/abs/2502.12623)
Append: [Union of Experts: Adapting Hierarchical Routing to Equivalently Decomposed Transformer](https://arxiv.org/abs/2503.02495)
Append: [A Survey on Sparse Autoencoders: Interpreting the Internal Mechanisms of Large Language Models](https://arxiv.org/abs/2503.05613)
Append: [ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning](https://arxiv.org/abs/2503.19470)
Append: [Meta-Semantics Augmented Few-Shot Relational Learning](https://arxiv.org/abs/2505.05684)
Append: [Beyond Input Activations: Identifying Influential Latents by Gradient Sparse Autoencoders](https://arxiv.org/abs/2505.08080)
Append: [MiCRo: Mixture Modeling and Context-aware Routing for Personalized Preference Learning](https://arxiv.org/abs/2505.24846)
Append: [Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models](https://arxiv.org/abs/2506.09532)
Append: [When and How Long Did Therapy Happen? Soft-Supervising Temporal Localization Using Audio-Language Models](https://arxiv.org/abs/2506.09707)
Append: [RAG+: Enhancing Retrieval-Augmented Generation with Application-Aware Reasoning](https://arxiv.org/abs/2506.11555)
Append: [LogicGuard: Improving Embodied LLM agents through Temporal Logic based Critics](https://arxiv.org/abs/2507.03293)
Append: [Generative Medical Event Models Improve with Scale](https://arxiv.org/abs/2508.12104)
Append: [Training Language Model Agents to Find Vulnerabilities with CTF-Dojo](https://arxiv.org/abs/2508.18370)
Append: [SoK: Large Language Model Copyright Auditing via Fingerprinting](https://arxiv.org/abs/2508.19843)
Append: [Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level RL](https://arxiv.org/abs/2509.09177)
Append: [Abduct, Act, Predict: Scaffolding Causal Inference for Automated Failure Attribution in Multi-Agent Systems](https://arxiv.org/abs/2509.10401)
append_entries: 141
Finish: 2025-09-24 04:24:35.531330
------------------------------------------------------
Started: 2025-09-24 06:24:26.061750
Existing_entries: 1141
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1207
Summarized using GPT-3.5-turbo
Append: [OpenWHO: A Document-Level Parallel Corpus for Health Translation in Low-Resource Languages](https://arxiv.org/abs/2508.16048)
Token length: 1444
Summarized using GPT-3.5-turbo
Append: [Seeing is Not Understanding: A Benchmark on Perception-Cognition Disparities in Large Language Models](https://arxiv.org/abs/2509.11101)
append_entries: 2
Finish: 2025-09-24 06:24:31.207106
------------------------------------------------------
Started: 2025-09-24 08:21:47.915104
Existing_entries: 1002
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-24 08:21:48.446998
------------------------------------------------------
Started: 2025-09-24 10:17:10.997062
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-24 10:17:11.345054
------------------------------------------------------
Started: 2025-09-24 12:33:58.442045
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-24 12:33:58.911139
------------------------------------------------------
Started: 2025-09-24 14:14:04.780958
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-24 14:14:05.159373
------------------------------------------------------
Started: 2025-09-24 16:20:56.865997
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-24 16:20:57.211776
------------------------------------------------------
Started: 2025-09-24 18:21:44.181018
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-24 18:21:44.603492
------------------------------------------------------
Started: 2025-09-24 20:17:50.386484
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-24 20:17:50.739367
------------------------------------------------------
Started: 2025-09-24 22:14:54.958979
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-24 22:14:55.348436
------------------------------------------------------
Started: 2025-09-25 01:13:51.800075
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-25 01:13:52.233137
------------------------------------------------------
Started: 2025-09-25 02:59:01.196788
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-25 02:59:01.734016
------------------------------------------------------
Started: 2025-09-25 04:22:46.032705
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 817
Summarized using GPT-3.5-turbo
Append: [Automated Item Neutralization for Non-Cognitive Scales: A Large Language Model Approach to Reducing Social-Desirability Bias](https://arxiv.org/abs/2509.19314)
Token length: 1309
Summarized using GPT-3.5-turbo
Append: [FHIR-AgentBench: Benchmarking LLM Agents for Realistic Interoperable EHR Question Answering](https://arxiv.org/abs/2509.19319)
Token length: 1813
Summarized using GPT-3.5-turbo
Append: [Readme_AI: Dynamic Context Construction for Large Language Models](https://arxiv.org/abs/2509.19322)
Token length: 1805
Summarized using GPT-3.5-turbo
Append: [Magnitude Matters: a Superior Class of Similarity Metrics for Holistic Semantic Understanding](https://arxiv.org/abs/2509.19323)
Token length: 1295
Summarized using GPT-3.5-turbo
Append: [How Much of Your Data Can Suck? Thresholds for Domain Performance and Emergent Misalignment in LLMs](https://arxiv.org/abs/2509.19325)
Token length: 1806
Summarized using GPT-3.5-turbo
Append: [Unveiling the Merits and Defects of LLMs in Automatic Review Generation for Scientific Papers](https://arxiv.org/abs/2509.19326)
Token length: 1840
Summarized using GPT-3.5-turbo
Append: [A systematic review of trial-matching pipelines using large language models](https://arxiv.org/abs/2509.19327)
Token length: 383
Summarized using GPT-3.5-turbo
Append: [How Model Size, Temperature, and Prompt Style Affect LLM-Human Assessment Score Alignment](https://arxiv.org/abs/2509.19329)
Token length: 1501
Summarized using GPT-3.5-turbo
Append: [Quantifying Compositionality of Classic and State-of-the-Art Embeddings](https://arxiv.org/abs/2509.19332)
Token length: 1538
Summarized using GPT-3.5-turbo
Append: [Pluralistic Off-policy Evaluation and Alignment](https://arxiv.org/abs/2509.19333)
Token length: 1399
Summarized using GPT-3.5-turbo
Append: [Cognitive-Level Adaptive Generation via Capability-Aware Retrieval and Style Adaptation](https://arxiv.org/abs/2509.19336)
Token length: 1073
Summarized using GPT-3.5-turbo
Append: [Part-of-speech tagging for Nagamese Language using CRF](https://arxiv.org/abs/2509.19343)
Token length: 565
Summarized using GPT-3.5-turbo
Append: [Performance of Large Language Models in Answering Critical Care Medicine Questions](https://arxiv.org/abs/2509.19344)
Token length: 1822
Summarized using GPT-3.5-turbo
Append: [SCORE: A Semantic Evaluation Framework for Generative Document Parsing](https://arxiv.org/abs/2509.19345)
Token length: 1455
Summarized using GPT-3.5-turbo
Append: [Benchmarking ChatGPT and DeepSeek in April 2025: A Novel Dual Perspective Sentiment Analysis Using Lexicon-Based and Deep Learning Approaches](https://arxiv.org/abs/2509.19346)
Token length: 527
Summarized using GPT-3.5-turbo
Append: [Characterizing Knowledge Graph Tasks in LLM Benchmarks Using Cognitive Complexity Frameworks](https://arxiv.org/abs/2509.19347)
Token length: 1747
Summarized using GPT-3.5-turbo
Append: [ShinkaEvolve: Towards Open-Ended And Sample-Efficient Program Evolution](https://arxiv.org/abs/2509.19349)
Token length: 1449
Summarized using GPT-3.5-turbo
Append: [TriSPrompt: A Hierarchical Soft Prompt Model for Multimodal Rumor Detection with Incomplete Modalities](https://arxiv.org/abs/2509.19352)
Token length: 1454
Summarized using GPT-3.5-turbo
Append: [RoadMind: Towards a Geospatial AI Expert for Disaster Response](https://arxiv.org/abs/2509.19354)
Token length: 1413
Summarized using GPT-3.5-turbo
Append: [Benchmarking and Improving LLM Robustness for Personalized Generation](https://arxiv.org/abs/2509.19358)
Token length: 1502
Summarized using GPT-3.5-turbo
Append: [Semantic Representation Attack against Aligned Large Language Models](https://arxiv.org/abs/2509.19360)
Token length: 758
Summarized using GPT-3.5-turbo
Append: [The Inadequacy of Offline LLM Evaluations: A Need to Account for Personalization in Model Behavior](https://arxiv.org/abs/2509.19364)
Token length: 1200
Summarized using GPT-3.5-turbo
Append: [LLM-Assisted Topic Reduction for BERTopic on Social Media Data](https://arxiv.org/abs/2509.19365)
Token length: 1740
Summarized using GPT-3.5-turbo
Append: [Pipeline Parallelism is All You Need for Optimized Early-Exit Based Self-Speculative Decoding](https://arxiv.org/abs/2509.19368)
Token length: 1124
Summarized using GPT-3.5-turbo
Append: [SLM-Based Agentic AI with P-C-G: Optimized for Korean Tool Use](https://arxiv.org/abs/2509.19369)
Token length: 1215
Summarized using GPT-3.5-turbo
Append: [Meow: End-to-End Outline Writing for Automatic Academic Survey](https://arxiv.org/abs/2509.19370)
Token length: 1412
Summarized using GPT-3.5-turbo
Append: [How to inject knowledge efficiently? Knowledge Infusion Scaling Law for Pre-training Large Language Models](https://arxiv.org/abs/2509.19371)
Token length: 1379
Summarized using GPT-3.5-turbo
Append: [A Pipeline to Assess Merging Methods via Behavior and Internals](https://arxiv.org/abs/2509.19476)
Token length: 841
Summarized using GPT-3.5-turbo
Append: [Do LLMs Encode Frame Semantics? Evidence from Frame Identification](https://arxiv.org/abs/2509.19540)
Token length: 766
Summarized using GPT-3.5-turbo
Append: [Confidence Calibration in Large Language Model-Based Entity Matching](https://arxiv.org/abs/2509.19557)
Token length: 889
Summarized using GPT-3.5-turbo
Append: [Uncertainty in Semantic Language Modeling with PIXELS](https://arxiv.org/abs/2509.19563)
Token length: 981
Summarized using GPT-3.5-turbo
Append: [Retrieval Augmented Generation based context discovery for ASR](https://arxiv.org/abs/2509.19567)
Token length: 1111
Summarized using GPT-3.5-turbo
Append: [ExPe: Exact Positional Encodings for Generative Transformer Models with Extrapolating Capabilities](https://arxiv.org/abs/2509.19569)
Token length: 1690
Summarized using GPT-3.5-turbo
Append: [LLMs4All: A Review on Large Language Models for Research and Applications in Academic Disciplines](https://arxiv.org/abs/2509.19580)
Token length: 1063
Summarized using GPT-3.5-turbo
Append: [GuessingGame: Measuring the Informativeness of Open-Ended Questions in Large Language Models](https://arxiv.org/abs/2509.19593)
Token length: 901
Summarized using GPT-3.5-turbo
Append: [Anatomy of a Feeling: Narrating Embodied Emotions via Large Vision-Language Models](https://arxiv.org/abs/2509.19595)
Token length: 972
Summarized using GPT-3.5-turbo
Append: [Evaluating Language Translation Models by Playing Telephone](https://arxiv.org/abs/2509.19611)
Token length: 1356
Summarized using GPT-3.5-turbo
Append: [AutoSpec: An Agentic Framework for Automatically Drafting Patent Specification](https://arxiv.org/abs/2509.19640)
Token length: 1464
Summarized using GPT-3.5-turbo
Append: [Large Language Models for Pedestrian Safety: An Application to Predicting Driver Yielding Behavior at Unsignalized Intersections](https://arxiv.org/abs/2509.19657)
Token length: 973
Summarized using GPT-3.5-turbo
Append: [DyBBT: Dynamic Balance via Bandit inspired Targeting for Dialog Policy with Cognitive Dual-Systems](https://arxiv.org/abs/2509.19695)
Token length: 1085
Summarized using GPT-3.5-turbo
Append: [Personality Vector: Modulating Personality of Large Language Models by Model Merging](https://arxiv.org/abs/2509.19727)
Token length: 1159
Summarized using GPT-3.5-turbo
Append: [HiCoLoRA: Addressing Context-Prompt Misalignment via Hierarchical Collaborative LoRA for Zero-Shot DST](https://arxiv.org/abs/2509.19742)
Token length: 1126
Summarized using GPT-3.5-turbo
Append: [PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs](https://arxiv.org/abs/2509.19745)
Token length: 1345
Summarized using GPT-3.5-turbo
Append: [CHURRO: Making History Readable with an Open-Weight Large Vision-Language Model for High-Accuracy, Low-Cost Historical Text Recognition](https://arxiv.org/abs/2509.19768)
Token length: 999
Summarized using GPT-3.5-turbo
Append: [EnAnchored-X2X: English-Anchored Optimization for Many-to-Many Translation](https://arxiv.org/abs/2509.19770)
Token length: 1384
Summarized using GPT-3.5-turbo
Append: [bi-GRPO: Bidirectional Optimization for Jailbreak Backdoor Injection on LLMs](https://arxiv.org/abs/2509.19775)
Token length: 1962
Summarized using GPT-3.5-turbo
Append: [Polarity Detection of Sustainable Detection Goals in News Text](https://arxiv.org/abs/2509.19833)
Token length: 955
Summarized using GPT-3.5-turbo
Append: [TianHui: A Domain-Specific Large Language Model for Diverse Traditional Chinese Medicine Scenarios](https://arxiv.org/abs/2509.19834)
Token length: 1115
Summarized using GPT-3.5-turbo
Append: [Mah\={a}n\={a}ma: A Unique Testbed for Literary Entity Discovery and Linking](https://arxiv.org/abs/2509.19844)
Token length: 1623
Summarized using GPT-3.5-turbo
Append: [Benchmarking Gaslighting Attacks Against Speech Large Language Models](https://arxiv.org/abs/2509.19858)
Append: [SINAI at eRisk@CLEF 2025: Transformer-Based and Conversational Strategies for Depression Detection](https://arxiv.org/abs/2509.19861)
Append: [SwissGPC v1.0 -- The Swiss German Podcasts Corpus](https://arxiv.org/abs/2509.19866)
Append: [Do Before You Judge: Self-Reference as a Pathway to Better LLM Evaluation](https://arxiv.org/abs/2509.19880)
Append: [Future Policy Aware Preference Learning for Mathematical Reasoning](https://arxiv.org/abs/2509.19893)
Append: [WEST: LLM based Speech Toolkit for Speech Understanding, Generation, and Interaction](https://arxiv.org/abs/2509.19902)
Append: [CorIL: Towards Enriching Indian Language to Indian Language Parallel Corpora and Machine Translation Systems](https://arxiv.org/abs/2509.19941)
Append: [The Knowledge-Behaviour Disconnect in LLM-based Chatbots](https://arxiv.org/abs/2509.20004)
Append: [DiffNator: Generating Structured Explanations of Time-Series Differences](https://arxiv.org/abs/2509.20007)
Append: [Tokenization and Representation Biases in Multilingual Models on Dialectal NLP Tasks](https://arxiv.org/abs/2509.20045)
Append: [Responsible AI Technical Report](https://arxiv.org/abs/2509.20057)
Append: [From Input Perception to Predictive Insight: Modeling Model Blind Spots Before They Become Errors](https://arxiv.org/abs/2509.20065)
Append: [From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training](https://arxiv.org/abs/2509.20072)
Append: [Can Constructions "SCAN" Compositionality ?](https://arxiv.org/abs/2509.20074)
Append: [OLaPh: Optimal Language Phonemizer](https://arxiv.org/abs/2509.20086)
Append: [Causal Understanding by LLMs: The Role of Uncertainty](https://arxiv.org/abs/2509.20088)
Append: [Integrated Framework for LLM Evaluation with Answer Generation](https://arxiv.org/abs/2509.20097)
Append: [Less is More: The Effectiveness of Compact Typological Language Representations](https://arxiv.org/abs/2509.20129)
Append: [Embedding Domain Knowledge for Large Language Models via Reinforcement Learning from Augmented Generation](https://arxiv.org/abs/2509.20162)
Append: [Probing Gender Bias in Multilingual LLMs: A Case Study of Stereotypes in Persian](https://arxiv.org/abs/2509.20168)
Append: [Thinking Augmented Pre-training](https://arxiv.org/abs/2509.20186)
Append: [Play by the Type Rules: Inferring Constraints for LLM Functions in Declarative Programs](https://arxiv.org/abs/2509.20208)
Append: [Low-Resource English-Tigrinya MT: Leveraging Multilingual Models, Custom Tokenizers, and Clean Evaluation Benchmarks](https://arxiv.org/abs/2509.20209)
Append: [Investigating the Representation of Backchannels and Fillers in Fine-tuned Language Models](https://arxiv.org/abs/2509.20237)
Append: [Instruction Boundary: Quantifying Biases in LLM Reasoning under Various Coverage](https://arxiv.org/abs/2509.20278)
Append: [Feeding Two Birds or Favoring One? Adequacy-Fluency Tradeoffs in Evaluation and Meta-Evaluation of Machine Translation](https://arxiv.org/abs/2509.20287)
Append: [Multilingual Hope Speech Detection: A Comparative Study of Logistic Regression, mBERT, and XLM-RoBERTa with Active Learning](https://arxiv.org/abs/2509.20315)
Append: [SIM-CoT: Supervised Implicit Chain-of-Thought](https://arxiv.org/abs/2509.20317)
Append: [Z-Scores: A Metric for Linguistically Assessing Disfluency Removal](https://arxiv.org/abs/2509.20319)
Append: [DRES: Benchmarking LLMs for Disfluency Removal](https://arxiv.org/abs/2509.20321)
Append: [Morphological Synthesizer for Ge'ez Language: Addressing Morphological Complexity and Resource Limitations](https://arxiv.org/abs/2509.20341)
Append: [EmbeddingGemma: Powerful and Lightweight Text Representations](https://arxiv.org/abs/2509.20354)
Append: [Language Models that Think, Chat Better](https://arxiv.org/abs/2509.20357)
Append: [STARQA: A Question Answering Dataset for Complex Analytical Reasoning over Structured Databases](https://arxiv.org/abs/2509.19508)
Append: [Cognitive Load Limits in Large Language Models: Benchmarking Multi-Hop Reasoning](https://arxiv.org/abs/2509.19517)
Append: [Frame-Stacked Local Transformers For Efficient Multi-Codebook Speech Generation](https://arxiv.org/abs/2509.19592)
Append: [Multimodal Language Models with Modality-Specific Experts for Financial Forecasting from Interleaved Sequences of Text and Time Series](https://arxiv.org/abs/2509.19628)
Append: [Advancing Speech Summarization in Multi-modal LLMs with Reinforcement Learning](https://arxiv.org/abs/2509.19631)
Append: [Human-AI Narrative Synthesis to Foster Shared Understanding in Civic Decision-Making](https://arxiv.org/abs/2509.19643)
Append: [UserRL: Training Interactive User-Centric Agent via Reinforcement Learning](https://arxiv.org/abs/2509.19736)
Append: [VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2509.19803)
Append: [PromptCoT 2.0: Scaling Prompt Synthesis for Large Language Model Reasoning](https://arxiv.org/abs/2509.19894)
Append: [Table Detection with Active Learning](https://arxiv.org/abs/2509.20003)
Append: [Embodied AI: From LLMs to World Models](https://arxiv.org/abs/2509.20021)
Append: [Discrete Diffusion for Reflective Vision-Language-Action Models in Autonomous Driving](https://arxiv.org/abs/2509.20109)
Append: [Federation of Agents: A Semantics-Aware Communication Fabric for Large-Scale Agentic AI](https://arxiv.org/abs/2509.20175)
Append: [Muse-it: A Tool for Analyzing Music Discourse on Reddit](https://arxiv.org/abs/2509.20228)
Append: [Failure Modes of Maximum Entropy RLHF](https://arxiv.org/abs/2509.20265)
Append: [Scan-do Attitude: Towards Autonomous CT Protocol Management using a Large Language Model Agent](https://arxiv.org/abs/2509.20270)
Append: [TALEC: Teach Your LLM to Evaluate in Specific Domain with In-house Criteria by Criteria Division and Zero-shot Plus Few-shot](https://arxiv.org/abs/2407.10999)
Append: [Context-Masked Meta-Prompting for Privacy-Preserving LLM Adaptation in Finance](https://arxiv.org/abs/2407.18920)
Append: [Efficient Fine-Tuning of Large Language Models for Automated Medical Documentation](https://arxiv.org/abs/2409.09324)
Append: [Evading Toxicity Detection with ASCII-art: A Benchmark of Spatial Attacks on Moderation Systems](https://arxiv.org/abs/2409.18708)
Append: [UNComp: Can Matrix Entropy Uncover Sparsity? -- A Compressor Design from an Uncertainty-Aware Perspective](https://arxiv.org/abs/2410.03090)
Append: [Blind Men and the Elephant: Diverse Perspectives on Gender Stereotypes in Benchmark Datasets](https://arxiv.org/abs/2501.01168)
Append: [Understanding Before Reasoning: Enhancing Chain-of-Thought with Iterative Summarization Pre-Prompting](https://arxiv.org/abs/2501.04341)
Append: [LLMs Reproduce Stereotypes of Sexual and Gender Minorities](https://arxiv.org/abs/2501.05926)
Append: [BAP v2: An Enhanced Task Framework for Instruction Following in Minecraft Dialogues](https://arxiv.org/abs/2501.10836)
Append: [LLMs as a synthesis between symbolic and distributed approaches to language](https://arxiv.org/abs/2502.11856)
Append: [Bridging Information Gaps with Comprehensive Answers: Improving the Diversity and Informativeness of Follow-Up Questions](https://arxiv.org/abs/2502.17715)
Append: [What are Foundation Models Cooking in the Post-Soviet World?](https://arxiv.org/abs/2502.18583)
Append: [MEBench: Benchmarking Large Language Models for Cross-Document Multi-Entity Question Answering](https://arxiv.org/abs/2502.18993)
Append: [HoT: Highlighted Chain of Thought for Referencing Supporting Facts from Inputs](https://arxiv.org/abs/2503.02003)
Append: [Large Language Models for Multilingual Previously Fact-Checked Claim Detection](https://arxiv.org/abs/2503.02737)
Append: [Language Models Fail to Introspect About Their Knowledge of Language](https://arxiv.org/abs/2503.07513)
Append: [Modeling Subjectivity in Cognitive Appraisal with Language Models](https://arxiv.org/abs/2503.11381)
Append: [Aligned Probing: Relating Toxic Behavior and Model Internals](https://arxiv.org/abs/2503.13390)
Append: [Unifying Text Semantics and Graph Structures for Temporal Text-attributed Graphs with Large Language Models](https://arxiv.org/abs/2503.14411)
Append: [Inverse Reinforcement Learning with Dynamic Reward Scaling for LLM Alignment](https://arxiv.org/abs/2503.18991)
Append: [Playpen: An Environment for Exploring Learning Through Conversational Interaction](https://arxiv.org/abs/2504.08590)
Append: [Small or Large? Zero-Shot or Finetuned? Guiding Language Model Choice for Specialized Applications in Healthcare](https://arxiv.org/abs/2504.21191)
Append: [Meeseeks: A Feedback-Driven, Iterative Self-Correction Benchmark evaluating LLMs' Instruction Following Capability](https://arxiv.org/abs/2504.21625)
Append: [Scent of Knowledge: Optimizing Search-Enhanced Reasoning with Information Foraging](https://arxiv.org/abs/2505.09316)
Append: [SAFE: Improving LLM Systems using Sentence-Level In-generation Attribution](https://arxiv.org/abs/2505.12621)
Append: [From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way Parallel Corpora](https://arxiv.org/abs/2505.14045)
Append: [DISCO Balances the Scales: Adaptive Domain- and Difficulty-Aware Reinforcement Learning on Imbalanced Data](https://arxiv.org/abs/2505.15074)
Append: [Date Fragments: A Hidden Bottleneck of Tokenization for Temporal Reasoning](https://arxiv.org/abs/2505.16088)
Append: [Safeguarding Privacy of Retrieval Data against Membership Inference Attacks: Is This Query Too Close to Home?](https://arxiv.org/abs/2505.22061)
Append: [Advancing Expert Specialization for Better MoE](https://arxiv.org/abs/2505.22323)
Append: [Threading the Needle: Reweaving Chain-of-Thought Reasoning to Explain Human Label Variation](https://arxiv.org/abs/2505.23368)
Append: [LegalSearchLM: Rethinking Legal Case Retrieval as Legal Elements Generation](https://arxiv.org/abs/2505.23832)
Append: [RadialRouter: Structured Representation for Efficient and Robust Large Language Models Routing](https://arxiv.org/abs/2506.03880)
Append: [How Well Can Reasoning Models Identify and Recover from Unhelpful Thoughts?](https://arxiv.org/abs/2506.10979)
Append: [Augmenting Multi-Agent Communication with State Delta Trajectory](https://arxiv.org/abs/2506.19209)
Append: [The Medium Is Not the Message: Deconfounding Document Embeddings via Linear Concept Erasure](https://arxiv.org/abs/2507.01234)
Append: [Detecting Token-Level Hallucinations Using Variance Signals: A Reference-Free Approach](https://arxiv.org/abs/2507.04137)
Append: [VisualTrap: A Stealthy Backdoor Attack on GUI Agents via Visual Grounding Manipulation](https://arxiv.org/abs/2507.06899)
Append: [Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence Emotion Recognition in Conversation](https://arxiv.org/abs/2507.09076)
Append: [Resource-Efficient Adaptation of Large Language Models for Text Embeddings via Prompt Engineering and Contrastive Fine-tuning](https://arxiv.org/abs/2507.22729)
Append: [Enhancing RAG Efficiency with Adaptive Context Compression](https://arxiv.org/abs/2507.22931)
Append: [From Query to Logic: Ontology-Driven Multi-Hop Reasoning in LLMs](https://arxiv.org/abs/2508.01424)
Append: [SciRerankBench: Benchmarking Rerankers Towards Scientific Retrieval-Augmented Generated LLMs](https://arxiv.org/abs/2508.08742)
Append: [Culture is Everywhere: A Call for Intentionally Cultural Evaluation](https://arxiv.org/abs/2509.01301)
Append: [Expanding the WMT24++ Benchmark with Rumantsch Grischun, Sursilvan, Sutsilvan, Surmiran, Puter, and Vallader](https://arxiv.org/abs/2509.03148)
Append: [No Encore: Unlearning as Opt-Out in Music Generation](https://arxiv.org/abs/2509.06277)
Append: [Interdisciplinary Research in Conversation: A Case Study in Computational Morphology for Language Documentation](https://arxiv.org/abs/2509.10644)
Append: [RealitySummary: Exploring On-Demand Mixed Reality Text Summarization and Question Answering using Large Language Models](https://arxiv.org/abs/2405.18620)
Append: [Macroeconomic Forecasting with Large Language Models](https://arxiv.org/abs/2407.00890)
Append: [Tree Search for Language Model Agents](https://arxiv.org/abs/2407.01476)
Append: [Unifying Symbolic Music Arrangement: Track-Aware Reconstruction and Structured Tokenization](https://arxiv.org/abs/2408.15176)
Append: [Robust Training of Neural Networks at Arbitrary Precision and Sparsity](https://arxiv.org/abs/2409.09245)
Append: [A GEN AI Framework for Medical Note Generation](https://arxiv.org/abs/2410.01841)
Append: [GraphEQA: Using 3D Semantic Scene Graphs for Real-time Embodied Question Answering](https://arxiv.org/abs/2412.14480)
Append: [HawkBench: Investigating Resilience of RAG Methods on Stratified Information-Seeking Tasks](https://arxiv.org/abs/2502.13465)
Append: [CNS-Obsidian: A Neurosurgical Vision-Language Model Built From Scientific Publications](https://arxiv.org/abs/2502.19546)
Append: [Beyond Outlining: Heterogeneous Recursive Planning for Adaptive Long-form Writing with Language Models](https://arxiv.org/abs/2503.08275)
Append: [Towards Visual Text Grounding of Multimodal Large Language Model](https://arxiv.org/abs/2504.04974)
Append: [Stepwise Guided Policy Optimization: Coloring your Incorrect Reasoning in GRPO](https://arxiv.org/abs/2505.11595)
Append: [AAPO: Enhancing the Reasoning Capabilities of LLMs with Advantage Momentum](https://arxiv.org/abs/2505.14264)
Append: [Redemption Score: A Multi-Modal Evaluation Framework for Image Captioning via Distributional, Perceptual, and Linguistic Signal Triangulation](https://arxiv.org/abs/2505.16180)
Append: [WikiGap: Promoting Epistemic Equity by Surfacing Knowledge Gaps Between English Wikipedia and other Language Editions](https://arxiv.org/abs/2505.24195)
Append: [Localized LoRA: A Structured Low-Rank Approximation for Efficient Fine-Tuning](https://arxiv.org/abs/2506.00236)
Append: [OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models](https://arxiv.org/abs/2506.03135)
Append: [Urania: Differentially Private Insights into AI Use](https://arxiv.org/abs/2506.04681)
Append: [CLOSP: A Unified Semantic Space for SAR, MSI, and Text in Remote Sensing](https://arxiv.org/abs/2507.10403)
append_entries: 164
Finish: 2025-09-25 04:24:22.304846
------------------------------------------------------
Started: 2025-09-25 06:24:57.263669
Existing_entries: 1164
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-25 06:24:57.722176
------------------------------------------------------
Started: 2025-09-25 08:21:37.032821
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-25 08:21:37.407552
------------------------------------------------------
Started: 2025-09-25 10:17:07.588927
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-25 10:17:08.029029
------------------------------------------------------
Started: 2025-09-25 12:34:52.274642
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-25 12:34:52.838386
------------------------------------------------------
Started: 2025-09-25 14:16:37.347842
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-25 14:16:37.723469
------------------------------------------------------
Started: 2025-09-25 16:20:04.992764
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-25 16:20:05.373486
------------------------------------------------------
Started: 2025-09-25 18:23:51.946362
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-25 18:23:52.319648
------------------------------------------------------
Started: 2025-09-25 20:18:09.830633
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-25 20:18:10.406090
------------------------------------------------------
Started: 2025-09-25 22:12:59.620951
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-25 22:12:59.996137
------------------------------------------------------
Started: 2025-09-26 01:13:33.510326
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-26 01:13:33.923443
------------------------------------------------------
Started: 2025-09-26 02:57:09.454436
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-26 02:57:09.832275
------------------------------------------------------
Started: 2025-09-26 04:20:40.080534
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1685
Summarized using GPT-3.5-turbo
Append: [Interpreting Public Sentiment in Diplomacy Events: A Counterfactual Analysis Framework Using Large Language Models](https://arxiv.org/abs/2509.20367)
Token length: 1039
Summarized using GPT-3.5-turbo
Append: [Speaker Style-Aware Phoneme Anchoring for Improved Cross-Lingual Speech Emotion Recognition](https://arxiv.org/abs/2509.20373)
Token length: 1325
Summarized using GPT-3.5-turbo
Append: [CFD-LLMBench: A Benchmark Suite for Evaluating Large Language Models in Computational Fluid Dynamics](https://arxiv.org/abs/2509.20374)
Token length: 1732
Summarized using GPT-3.5-turbo
Append: [Assessing Classical Machine Learning and Transformer-based Approaches for Detecting AI-Generated Research Text](https://arxiv.org/abs/2509.20375)
Token length: 1354
Summarized using GPT-3.5-turbo
Append: [ConceptViz: A Visual Analytics Approach for Exploring Concepts in Large Language Models](https://arxiv.org/abs/2509.20376)
Token length: 1465
Summarized using GPT-3.5-turbo
Append: [SKILL-RAG: Self-Knowledge Induced Learning and Filtering for Retrieval-Augmented Generation](https://arxiv.org/abs/2509.20377)
Token length: 1082
Summarized using GPT-3.5-turbo
Append: [Beyond Global Emotion: Fine-Grained Emotional Speech Synthesis with Dynamic Word-Level Modulation](https://arxiv.org/abs/2509.20378)
Token length: 1107
Summarized using GPT-3.5-turbo
Append: [USB-Rec: An Effective Framework for Improving Conversational Recommendation Capability of Large Language Model](https://arxiv.org/abs/2509.20381)
Token length: 1291
Summarized using GPT-3.5-turbo
Append: [Document Summarization with Conformal Importance Guarantees](https://arxiv.org/abs/2509.20461)
Token length: 702
Summarized using GPT-3.5-turbo
Append: [ShortCheck: Checkworthiness Detection of Multilingual Short-Form Videos](https://arxiv.org/abs/2509.20467)
Token length: 1351
Summarized using GPT-3.5-turbo
Append: [MARS: toward more efficient multi-agent collaboration for LLM reasoning](https://arxiv.org/abs/2509.20502)
Token length: 1012
Summarized using GPT-3.5-turbo
Append: [SiniticMTError: A Machine Translation Dataset with Error Annotations for Sinitic Languages](https://arxiv.org/abs/2509.20557)
Token length: 1871
Summarized using GPT-3.5-turbo
Append: [SwasthLLM: a Unified Cross-Lingual, Multi-Task, and Meta-Learning Zero-Shot Framework for Medical Diagnosis Using Contrastive Representations](https://arxiv.org/abs/2509.20567)
Token length: 1796
Summarized using GPT-3.5-turbo
Append: [Dynamic Reasoning Chains through Depth-Specialized Mixture-of-Experts in Transformer Architectures](https://arxiv.org/abs/2509.20577)
Token length: 1780
Summarized using GPT-3.5-turbo
Append: [Hierarchical Resolution Transformers: A Wavelet-Inspired Architecture for Multi-Scale Language Understanding](https://arxiv.org/abs/2509.20581)
Token length: 1382
Summarized using GPT-3.5-turbo
Append: [FS-DFM: Fast and Accurate Long Text Generation with Few-Step Diffusion Language Models](https://arxiv.org/abs/2509.20624)
Token length: 1469
Summarized using GPT-3.5-turbo
Append: [Look Before you Leap: Estimating LLM Benchmark Scores from Descriptions](https://arxiv.org/abs/2509.20645)
Token length: 1320
Summarized using GPT-3.5-turbo
Append: [Building Tailored Speech Recognizers for Japanese Speaking Assessment](https://arxiv.org/abs/2509.20655)
Token length: 1454
Summarized using GPT-3.5-turbo
Append: [Enhancing Molecular Property Prediction with Knowledge from Large Language Models](https://arxiv.org/abs/2509.20664)
Token length: 1340
Summarized using GPT-3.5-turbo
Append: [RedHerring Attack: Testing the Reliability of Attack Detection](https://arxiv.org/abs/2509.20691)
Token length: 1378
Summarized using GPT-3.5-turbo
Append: [Overcoming Black-box Attack Inefficiency with Hybrid and Dynamic Select Algorithms](https://arxiv.org/abs/2509.20699)
Token length: 1179
Summarized using GPT-3.5-turbo
Append: [MI-Fuse: Label Fusion for Unsupervised Domain Adaptation with Closed-Source Large-Audio Language Model](https://arxiv.org/abs/2509.20706)
Token length: 808
Summarized using GPT-3.5-turbo
Append: [Probability Distribution Collapse: A Critical Bottleneck to Compact Unsupervised Neural Grammar Induction](https://arxiv.org/abs/2509.20734)
Token length: 1013
Summarized using GPT-3.5-turbo
Append: [Confidence-guided Refinement Reasoning for Zero-shot Question Answering](https://arxiv.org/abs/2509.20750)
Token length: 1411
Summarized using GPT-3.5-turbo
Append: [SFT Doesn't Always Hurt General Capabilities: Revisiting Domain-Specific Fine-Tuning in LLMs](https://arxiv.org/abs/2509.20758)
Token length: 1701
Summarized using GPT-3.5-turbo
Append: [Towards Atoms of Large Language Models](https://arxiv.org/abs/2509.20784)
Token length: 1752
Summarized using GPT-3.5-turbo
Append: [Few-Shot and Training-Free Review Generation via Conversational Prompting](https://arxiv.org/abs/2509.20805)
Token length: 1359
Summarized using GPT-3.5-turbo
Append: [Enrich-on-Graph: Query-Graph Alignment for Complex Reasoning with LLM Enriching](https://arxiv.org/abs/2509.20810)
Token length: 1135
Summarized using GPT-3.5-turbo
Append: [Leveraging What's Overfixed: Post-Correction via LLM Grammatical Error Overcorrection](https://arxiv.org/abs/2509.20811)
Token length: 756
Summarized using GPT-3.5-turbo
Append: [Distilling Many-Shot In-Context Learning into a Cheat Sheet](https://arxiv.org/abs/2509.20820)
Token length: 994
Summarized using GPT-3.5-turbo
Append: [Zero-Shot Privacy-Aware Text Rewriting via Iterative Tree Search](https://arxiv.org/abs/2509.20838)
Token length: 1345
Summarized using GPT-3.5-turbo
Append: [Concise and Sufficient Sub-Sentence Citations for Retrieval-Augmented Generation](https://arxiv.org/abs/2509.20859)
Token length: 1102
Summarized using GPT-3.5-turbo
Append: [WeFT: Weighted Entropy-driven Fine-Tuning for dLLMs](https://arxiv.org/abs/2509.20863)
Token length: 1713
Summarized using GPT-3.5-turbo
Append: [Single Answer is Not Enough: On Generating Ranked Lists with Medical Reasoning Models](https://arxiv.org/abs/2509.20866)
Token length: 1639
Summarized using GPT-3.5-turbo
Append: [Learning to Summarize by Learning to Quiz: Adversarial Agentic Collaboration for Long Document Summarization](https://arxiv.org/abs/2509.20900)
Token length: 1263
Summarized using GPT-3.5-turbo
Append: [MemLens: Uncovering Memorization in LLMs with Activation Trajectories](https://arxiv.org/abs/2509.20909)
Token length: 1874
Summarized using GPT-3.5-turbo
Append: [Cross-Linguistic Analysis of Memory Load in Sentence Comprehension: Linear Distance and Structural Density](https://arxiv.org/abs/2509.20916)
Token length: 1076
Summarized using GPT-3.5-turbo
Append: [Tool Calling for Arabic LLMs: Data Strategies and Instruction Tuning](https://arxiv.org/abs/2509.20957)
Token length: 1964
Summarized using GPT-3.5-turbo
Append: [Analysis of instruction-based LLMs' capabilities to score and judge text-input problems in an academic setting](https://arxiv.org/abs/2509.20982)
Token length: 819
Summarized using GPT-3.5-turbo
Append: [Generative AI for FFRDCs](https://arxiv.org/abs/2509.21040)
Token length: 1027
Summarized using GPT-3.5-turbo
Append: [Behind RoPE: How Does Causal Mask Encode Positional Information?](https://arxiv.org/abs/2509.21042)
Token length: 1519
Summarized using GPT-3.5-turbo
Append: [When Instructions Multiply: Measuring and Estimating LLM Capabilities of Multiple Instructions Following](https://arxiv.org/abs/2509.21051)
Token length: 1699
Summarized using GPT-3.5-turbo
Append: [SoM-1K: A Thousand-Problem Benchmark Dataset for Strength of Materials](https://arxiv.org/abs/2509.21079)
Token length: 1894
Summarized using GPT-3.5-turbo
Append: [Which Cultural Lens Do Models Adopt? On Cultural Positioning Bias and Agentic Mitigation in LLMs](https://arxiv.org/abs/2509.21080)
Token length: 1320
Summarized using GPT-3.5-turbo
Append: [PerHalluEval: Persian Hallucination Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2509.21104)
Token length: 1498
Summarized using GPT-3.5-turbo
Append: [BESPOKE: Benchmark for Search-Augmented Large Language Model Personalization via Diagnostic Feedback](https://arxiv.org/abs/2509.21106)
Token length: 1028
Summarized using GPT-3.5-turbo
Append: [VoiceBBQ: Investigating Effect of Content and Acoustics in Social Bias of Spoken Language Model](https://arxiv.org/abs/2509.21108)
Token length: 1781
Summarized using GPT-3.5-turbo
Append: [Acoustic-based Gender Differentiation in Speech-aware Language Models](https://arxiv.org/abs/2509.21125)
Token length: 612
Summarized using GPT-3.5-turbo
Append: [AutoIntent: AutoML for Text Classification](https://arxiv.org/abs/2509.21138)
Token length: 1167
Summarized using GPT-3.5-turbo
Append: [Retrieval over Classification: Integrating Relation Semantics for Multimodal Relation Extraction](https://arxiv.org/abs/2509.21151)
Append: [Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in Language Models](https://arxiv.org/abs/2509.21155)
Append: [Who's Laughing Now? An Overview of Computational Humour Generation and Explanation](https://arxiv.org/abs/2509.21175)
Append: [GEP: A GCG-Based method for extracting personally identifiable information from chatbots built on small language models](https://arxiv.org/abs/2509.21192)
Append: [Eigen-1: Adaptive Multi-Agent Refinement with Monitor-Based RAG for Scientific Reasoning](https://arxiv.org/abs/2509.21193)
Append: [CLaw: Benchmarking Chinese Legal Knowledge in Large Language Models - A Fine-grained Corpus and Reasoning Analysis](https://arxiv.org/abs/2509.21208)
Append: [SGMem: Sentence Graph Memory for Long-Term Conversational Agents](https://arxiv.org/abs/2509.21212)
Append: [Query-Centric Graph Retrieval Augmented Generation](https://arxiv.org/abs/2509.21237)
Append: [Un-Doubling Diffusion: LLM-guided Disambiguation of Homonym Duplication](https://arxiv.org/abs/2509.21262)
Append: [LLM Output Homogenization is Task Dependent](https://arxiv.org/abs/2509.21267)
Append: [LLMTrace: A Corpus for Classification and Fine-Grained Localization of AI-Written Text](https://arxiv.org/abs/2509.21269)
Append: [Bounds of Chain-of-Thought Robustness: Reasoning Steps, Embed Norms, and Beyond](https://arxiv.org/abs/2509.21284)
Append: [DisCoCLIP: A Distributional Compositional Tensor Network Encoder for Vision-Language Understanding](https://arxiv.org/abs/2509.21287)
Append: [The role of synthetic data in Multilingual, Multi-cultural AI systems: Lessons from Indic Languages](https://arxiv.org/abs/2509.21294)
Append: [Sycophancy Is Not One Thing: Causal Separation of Sycophantic Behaviors in LLMs](https://arxiv.org/abs/2509.21305)
Append: [RLBFF: Binary Flexible Feedback to bridge between Human Feedback & Verifiable Rewards](https://arxiv.org/abs/2509.21319)
Append: [SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines](https://arxiv.org/abs/2509.21320)
Append: [Intercept Cancer: Cancer Pre-Screening with Large Scale Healthcare Foundation Models](https://arxiv.org/abs/2506.00209)
Append: [CARINOX: Inference-time Scaling with Category-Aware Reward-based Initial Noise Optimization and Exploration](https://arxiv.org/abs/2509.17458)
Append: [Leveraging NTPs for Efficient Hallucination Detection in VLMs](https://arxiv.org/abs/2509.20379)
Append: [Blueprints of Trust: AI System Cards for End to End Transparency and Governance](https://arxiv.org/abs/2509.20394)
Append: [RadAgents: Multimodal Agentic Reasoning for Chest X-ray Interpretation with Radiologist-like Workflows](https://arxiv.org/abs/2509.20490)
Append: [InsightGUIDE: An Opinionated AI Assistant for Guided Critical Reading of Scientific Literature](https://arxiv.org/abs/2509.20493)
Append: [Perspectra: Choosing Your Experts Enhances Critical Thinking in Multi-Agent Research Ideation](https://arxiv.org/abs/2509.20553)
Append: [Every Character Counts: From Vulnerability to Defense in Phishing Detection](https://arxiv.org/abs/2509.20589)
Append: [Human Semantic Representations of Social Interactions from Moving Shapes](https://arxiv.org/abs/2509.20673)
Append: [Can Federated Learning Safeguard Private Data in LLM Training? Vulnerabilities, Attacks, and Defense Evaluation](https://arxiv.org/abs/2509.20680)
Append: [CE-GPPO: Controlling Entropy via Gradient-Preserving Clipping Policy Optimization in Reinforcement Learning](https://arxiv.org/abs/2509.20712)
Append: [Visual Authority and the Rhetoric of Health Misinformation: A Multimodal Analysis of Social Media Videos](https://arxiv.org/abs/2509.20724)
Append: [Seeing Through Words, Speaking Through Pixels: Deep Representational Alignment Between Vision and Language Models](https://arxiv.org/abs/2509.20751)
Append: [Verification Limits Code LLM Training](https://arxiv.org/abs/2509.20837)
Append: [StyleBench: Evaluating thinking styles in Large Language Models](https://arxiv.org/abs/2509.20868)
Append: [On Theoretical Interpretations of Concept-Based In-Context Learning](https://arxiv.org/abs/2509.20882)
Append: [CLUE: Conflict-guided Localization for LLM Unlearning Framework](https://arxiv.org/abs/2509.20977)
Append: [Binary Autoencoder for Mechanistic Interpretability of Large Language Models](https://arxiv.org/abs/2509.20997)
Append: [Mechanism of Task-oriented Information Removal in In-context Learning](https://arxiv.org/abs/2509.21012)
Append: [DELTA-Code: How Does RL Unlock and Transfer New Programming Algorithms in LLMs?](https://arxiv.org/abs/2509.21016)
Append: [CLAUSE: Agentic Neuro-Symbolic Knowledge Graph Reasoning via Dynamic Learnable Context Engineering](https://arxiv.org/abs/2509.21035)
Append: [Disagreements in Reasoning: How a Model's Thinking Process Dictates Persuasion in Multi-Agent Systems](https://arxiv.org/abs/2509.21054)
Append: [PMark: Towards Robust and Distortion-free Semantic-level Watermarking with Channel Constraints](https://arxiv.org/abs/2509.21057)
Append: [ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning](https://arxiv.org/abs/2509.21070)
Append: [Communication Bias in Large Language Models: A Regulatory Perspective](https://arxiv.org/abs/2509.21075)
Append: [TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them](https://arxiv.org/abs/2509.21117)
Append: [Expanding Reasoning Potential in Foundation Model by Learning Diverse Chains of Thought Patterns](https://arxiv.org/abs/2509.21124)
Append: [Automotive-ENV: Benchmarking Multimodal Agents in Vehicle Interface Systems](https://arxiv.org/abs/2509.21143)
Append: [TABLET: A Large-Scale Dataset for Robust Visual Table Understanding](https://arxiv.org/abs/2509.21205)
Append: [Sigma: Semantically Informative Pre-training for Skeleton-based Sign Language Understanding](https://arxiv.org/abs/2509.21223)
Append: [Evaluating the Evaluators: Metrics for Compositional Text-to-Image Generation](https://arxiv.org/abs/2509.21227)
Append: [Hallucination as an Upper Bound: A New Perspective on Text-to-Image Evaluation](https://arxiv.org/abs/2509.21257)
Append: [Interactive Recommendation Agent with Active User Commands](https://arxiv.org/abs/2509.21317)
Append: [Higher-Order DisCoCat (Peirce-Lambek-Montague semantics)](https://arxiv.org/abs/2311.17813)
Append: [ASCIIEval: Benchmarking Models' Visual Perception in Text Strings via ASCII Art](https://arxiv.org/abs/2410.01733)
Append: [UniHR: Hierarchical Representation Learning for Unified Knowledge Graph Link Prediction](https://arxiv.org/abs/2411.07019)
Append: [Investigating Factuality in Long-Form Text Generation: The Roles of Self-Known and Self-Unknown](https://arxiv.org/abs/2411.15993)
Append: [LAMA-UT: Language Agnostic Multilingual ASR through Orthography Unification and Language-Specific Transliteration](https://arxiv.org/abs/2412.15299)
Append: [Labeling Free-text Data using Language Model Ensembles](https://arxiv.org/abs/2501.08413)
Append: [Improving LLM Unlearning Robustness via Random Perturbations](https://arxiv.org/abs/2501.19202)
Append: [Quantifying depressive mental states with large language models](https://arxiv.org/abs/2502.09487)
Append: [MathFimer: Enhancing Mathematical Reasoning by Expanding Reasoning Steps through Fill-in-the-Middle Task](https://arxiv.org/abs/2502.11684)
Append: [The Validation Gap: A Mechanistic Analysis of How Language Models Compute Arithmetic but Fail to Validate It](https://arxiv.org/abs/2502.11771)
Append: [Thinking Outside the (Gray) Box: A Context-Based Score for Assessing Value and Originality in Neural Text Generation](https://arxiv.org/abs/2502.13207)
Append: [JUREX-4E: Juridical Expert-Annotated Four-Element Knowledge Base for Legal Reasoning](https://arxiv.org/abs/2502.17166)
Append: [Problem Solved? Information Extraction Design Space for Layout-Rich Documents using LLMs](https://arxiv.org/abs/2502.18179)
Append: [Collab-Overcooked: Benchmarking and Evaluating Large Language Models as Collaborative Agents](https://arxiv.org/abs/2502.20073)
Append: [Constructions are Revealed in Word Distributions](https://arxiv.org/abs/2503.06048)
Append: [Explainable Sentiment Analysis with DeepSeek-R1: Performance, Efficiency, and Few-Shot Learning](https://arxiv.org/abs/2503.11655)
Append: [Inference-Time Scaling for Generalist Reward Modeling](https://arxiv.org/abs/2504.02495)
Append: [Decoding Open-Ended Information Seeking Goals from Eye Movements in Reading](https://arxiv.org/abs/2505.02872)
Append: [Ambiguity Resolution in Text-to-Structured Data Mapping](https://arxiv.org/abs/2505.11679)
Append: [VerifyBench: Benchmarking Reference-based Reward Systems for Large Language Models](https://arxiv.org/abs/2505.15801)
Append: [UNCERTAINTY-LINE: Length-Invariant Estimation of Uncertainty for Large Language Models](https://arxiv.org/abs/2505.19060)
Append: [InComeS: Integrating Compression and Selection Mechanisms into LLMs for Efficient Model Editing](https://arxiv.org/abs/2505.22156)
Append: [Bayesian Attention Mechanism: A Probabilistic Framework for Positional Encoding and Context Length Extrapolation](https://arxiv.org/abs/2505.22842)
Append: [BabyLM's First Constructions: Causal probing provides a signal of learning](https://arxiv.org/abs/2506.02147)
Append: [Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning](https://arxiv.org/abs/2506.03136)
Append: [ConsistentChat: Building Skeleton-Guided Consistent Multi-Turn Dialogues for Large Language Models from Scratch](https://arxiv.org/abs/2506.03558)
Append: [From Replication to Redesign: Exploring Pairwise Comparisons for LLM-Based Peer Review](https://arxiv.org/abs/2506.11343)
Append: [ImpliRet: Benchmarking the Implicit Fact Retrieval Challenge](https://arxiv.org/abs/2506.14407)
Append: [When Does Meaning Backfire? Investigating the Role of AMRs in NLI](https://arxiv.org/abs/2506.14613)
Append: [THCM-CAL: Temporal-Hierarchical Causal Modelling with Conformal Calibration for Clinical Risk Prediction](https://arxiv.org/abs/2506.17844)
Append: [A Simple "Motivation" Can Enhance Reinforcement Finetuning of Large Reasoning Models](https://arxiv.org/abs/2506.18485)
Append: [ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs](https://arxiv.org/abs/2506.18896)
Append: [ARF-RLHF: Adaptive Reward-Following for RLHF through Emotion-Driven Self-Supervision and Trace-Biased Dynamic Optimization](https://arxiv.org/abs/2507.03069)
Append: [ixi-GEN: Efficient Industrial sLLMs through Domain Adaptive Continual Pretraining](https://arxiv.org/abs/2507.06795)
Append: [Turning Internal Gap into Self-Improvement: Promoting the Generation-Understanding Unification in MLLMs](https://arxiv.org/abs/2507.16663)
Append: [A Comprehensive Taxonomy of Negation for NLP and Neural Retrievers](https://arxiv.org/abs/2507.22337)
Append: [C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring Challenges in Complex Conversations](https://arxiv.org/abs/2507.22968)
Append: [ILRe: Intermediate Layer Retrieval for Context Compression in Causal Language Models](https://arxiv.org/abs/2508.17892)
Append: [MathBuddy: A Multimodal System for Affective Math Tutoring](https://arxiv.org/abs/2508.19993)
Append: [JudgeAgent: Knowledge-wise and Dynamic LLM Evaluation with Agent-as-Interviewer](https://arxiv.org/abs/2509.02097)
Append: [Just-in-time and distributed task representations in language models](https://arxiv.org/abs/2509.04466)
Append: [PLaMo 2 Technical Report](https://arxiv.org/abs/2509.04897)
Append: [LM-Searcher: Cross-domain Neural Architecture Search with LLMs via Unified Numerical Encoding](https://arxiv.org/abs/2509.05657)
Append: [WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents](https://arxiv.org/abs/2509.06501)
Append: [Ko-PIQA: A Korean Physical Commonsense Reasoning Dataset with Cultural Context](https://arxiv.org/abs/2509.11303)
Append: [Can social media provide early warning of retraction? Evidence from critical tweets identified by human annotation and large language models](https://arxiv.org/abs/2403.16851)
Append: [TestAgent: Automatic Benchmarking and Exploratory Interaction for Evaluating LLMs in Vertical Domains](https://arxiv.org/abs/2410.11507)
Append: [Bias Similarity Measurement: A Black-Box Audit of Fairness Across LLMs](https://arxiv.org/abs/2410.12010)
Append: [Reformulation is All You Need: Addressing Malicious Text Features in DNNs](https://arxiv.org/abs/2502.00652)
Append: [AdaSVD: Adaptive Singular Value Decomposition for Large Language Models](https://arxiv.org/abs/2502.01403)
Append: [Scaling Rich Style-Prompted Text-to-Speech Datasets](https://arxiv.org/abs/2503.04713)
Append: [What Makes a Reward Model a Good Teacher? An Optimization Perspective](https://arxiv.org/abs/2503.15477)
Append: [On the Perception Bottleneck of VLMs for Chart Understanding](https://arxiv.org/abs/2503.18435)
Append: [A Framework for Situating Innovations, Opportunities, and Challenges in Advancing Vertical Systems with Large AI Models](https://arxiv.org/abs/2504.02793)
Append: [Process Reward Models That Think](https://arxiv.org/abs/2504.16828)
Append: [UDDETTS: Unifying Discrete and Dimensional Emotions for Controllable Emotional Text-to-Speech](https://arxiv.org/abs/2505.10599)
Append: [SelfBudgeter: Adaptive Token Allocation for Efficient LLM Reasoning](https://arxiv.org/abs/2505.11274)
Append: [MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence](https://arxiv.org/abs/2505.23764)
Append: [Fractional Reasoning via Latent Steering Vectors Improves Inference Time Compute](https://arxiv.org/abs/2506.15882)
Append: [Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling](https://arxiv.org/abs/2507.01679)
Append: [Reinforcement Fine-Tuning Naturally Mitigates Forgetting in Continual Post-Training](https://arxiv.org/abs/2507.05386)
Append: [NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining](https://arxiv.org/abs/2507.14119)
Append: [Causal Reflection with Language Models](https://arxiv.org/abs/2508.04495)
Append: [Searching for Privacy Risks in LLM Agents via Simulation](https://arxiv.org/abs/2508.10880)
Append: [Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks](https://arxiv.org/abs/2508.18672)
Append: [CoT-Space: A Theoretical Framework for Internal Slow-Thinking via Reinforcement Learning](https://arxiv.org/abs/2509.04027)
Append: [ALICE: An Interpretable Neural Architecture for Generalization in Substitution Ciphers](https://arxiv.org/abs/2509.07282)
Append: [ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable Orthogonal Butterfly Transforms](https://arxiv.org/abs/2509.09679)
Append: [How to Evaluate Medical AI](https://arxiv.org/abs/2509.11941)
append_entries: 168
Finish: 2025-09-26 04:22:24.248705
------------------------------------------------------
Started: 2025-09-26 06:24:38.100875
Existing_entries: 1168
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1231
Summarized using GPT-3.5-turbo
Append: [Canary-1B-v2 & Parakeet-TDT-0.6B-v3: Efficient and High-Performance Models for Multilingual ASR and AST](https://arxiv.org/abs/2509.14128)
Token length: 1347
Summarized using GPT-3.5-turbo
Append: [Causal-Counterfactual RAG: The Integration of Causal-Counterfactual Reasoning into RAG](https://arxiv.org/abs/2509.14435)
Token length: 1902
Summarized using GPT-3.5-turbo
Append: [FURINA: Free from Unmergeable Router via LINear Aggregation of mixed experts](https://arxiv.org/abs/2509.14900)
append_entries: 3
Finish: 2025-09-26 06:24:45.529566
------------------------------------------------------
Started: 2025-09-26 08:22:43.488795
Existing_entries: 1003
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-26 08:22:43.973993
------------------------------------------------------
Started: 2025-09-26 10:16:47.308924
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-26 10:16:47.751312
------------------------------------------------------
Started: 2025-09-26 12:34:19.338034
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-26 12:34:19.728663
------------------------------------------------------
Started: 2025-09-26 14:14:33.661663
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-26 14:14:34.144491
------------------------------------------------------
Started: 2025-09-26 16:19:41.692482
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-26 16:19:42.129488
------------------------------------------------------
Started: 2025-09-26 18:21:06.260025
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-26 18:21:06.644557
------------------------------------------------------
Started: 2025-09-26 20:17:14.653930
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-26 20:17:15.132890
------------------------------------------------------
Started: 2025-09-26 22:14:22.493651
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-26 22:14:22.914556
------------------------------------------------------
Started: 2025-09-27 01:10:33.933678
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-27 01:10:34.327237
------------------------------------------------------
Started: 2025-09-27 02:51:59.484415
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-27 02:51:59.875326
------------------------------------------------------
Started: 2025-09-27 04:18:23.624723
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-27 04:18:23.705723
------------------------------------------------------
Started: 2025-09-27 06:20:47.659548
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-27 06:20:47.756638
------------------------------------------------------
Started: 2025-09-27 08:18:46.110394
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-27 08:18:46.178425
------------------------------------------------------
Started: 2025-09-27 10:14:31.979601
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-27 10:14:32.080853
------------------------------------------------------
Started: 2025-09-27 12:29:12.438267
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-27 12:29:12.498568
------------------------------------------------------
Started: 2025-09-27 14:13:13.155588
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-27 14:13:13.261389
------------------------------------------------------
Started: 2025-09-27 16:17:26.044671
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-27 16:17:26.111177
------------------------------------------------------
Started: 2025-09-27 18:20:11.968775
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-27 18:20:12.046533
------------------------------------------------------
Started: 2025-09-27 20:15:53.926942
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-27 20:15:53.987341
------------------------------------------------------
Started: 2025-09-27 22:13:24.520574
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-27 22:13:24.593585
------------------------------------------------------
Started: 2025-09-28 01:20:30.902065
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-28 01:20:30.964088
------------------------------------------------------
Started: 2025-09-28 03:05:31.829834
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-28 03:05:31.906179
------------------------------------------------------
Started: 2025-09-28 04:18:30.718365
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-28 04:18:30.778879
------------------------------------------------------
Started: 2025-09-28 06:21:50.804137
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-28 06:21:50.898291
------------------------------------------------------
Started: 2025-09-28 08:18:20.393393
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-28 08:18:20.450897
------------------------------------------------------
Started: 2025-09-28 10:14:40.677346
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-28 10:14:40.785390
------------------------------------------------------
Started: 2025-09-28 12:29:41.628129
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-28 12:29:41.749798
------------------------------------------------------
Started: 2025-09-28 14:12:34.093673
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-28 14:12:34.151874
------------------------------------------------------
Started: 2025-09-28 16:17:18.574081
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-28 16:17:18.636013
------------------------------------------------------
Started: 2025-09-28 18:20:00.959048
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-28 18:20:01.040371
------------------------------------------------------
Started: 2025-09-28 20:15:49.668357
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-28 20:15:49.763445
------------------------------------------------------
Started: 2025-09-28 22:14:02.121687
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-28 22:14:02.191981
------------------------------------------------------
Started: 2025-09-29 01:15:27.845592
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-29 01:15:27.925139
------------------------------------------------------
Started: 2025-09-29 03:01:50.482483
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-29 03:01:50.541324
------------------------------------------------------
Started: 2025-09-29 04:23:32.498050
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1585
Summarized using GPT-3.5-turbo
Append: [A Novel Differential Feature Learning for Effective Hallucination Detection and Classification](https://arxiv.org/abs/2509.21357)
Token length: 1877
Summarized using GPT-3.5-turbo
Append: [Influence Guided Context Selection for Effective Retrieval-Augmented Generation](https://arxiv.org/abs/2509.21359)
Token length: 1273
Summarized using GPT-3.5-turbo
Append: [Context Is What You Need: The Maximum Effective Context Window for Real World Limits of LLMs](https://arxiv.org/abs/2509.21361)
Token length: 236
Summarized using GPT-3.5-turbo
Append: [How Large Language Models Need Symbolism](https://arxiv.org/abs/2509.21404)
Token length: 1233
Summarized using GPT-3.5-turbo
Append: [One Model, Many Morals: Uncovering Cross-Linguistic Misalignments in Computational Moral Reasoning](https://arxiv.org/abs/2509.21443)
Token length: 1418
Summarized using GPT-3.5-turbo
Append: [LLM-Based Support for Diabetes Diagnosis: Opportunities, Scenarios, and Challenges with GPT-5](https://arxiv.org/abs/2509.21450)
Token length: 1161
Summarized using GPT-3.5-turbo
Append: [Diagnosing the Performance Trade-off in Moral Alignment: A Case Study on Gender Stereotypes](https://arxiv.org/abs/2509.21456)
Token length: 1263
Summarized using GPT-3.5-turbo
Append: [A State-of-the-Art SQL Reasoning Model using RLVR](https://arxiv.org/abs/2509.21459)
Token length: 1797
Summarized using GPT-3.5-turbo
Append: [Learning to Reason with Mixture of Tokens](https://arxiv.org/abs/2509.21482)
Token length: 1000
Summarized using GPT-3.5-turbo
Append: [Dual-Head Reasoning Distillation: Improving Classifier Accuracy with Train-Time-Only Reasoning](https://arxiv.org/abs/2509.21487)
Token length: 1475
Summarized using GPT-3.5-turbo
Append: [On Code-Induced Reasoning in LLMs](https://arxiv.org/abs/2509.21499)
Token length: 1028
Summarized using GPT-3.5-turbo
Append: [Agribot: agriculture-specific question answer system](https://arxiv.org/abs/2509.21535)
Token length: 939
Summarized using GPT-3.5-turbo
Append: [Domain-Aware Speaker Diarization On African-Accented English](https://arxiv.org/abs/2509.21554)
Token length: 1463
Summarized using GPT-3.5-turbo
Append: [Generation-Time vs. Post-hoc Citation: A Holistic Evaluation of LLM Attribution](https://arxiv.org/abs/2509.21557)
Token length: 1211
Summarized using GPT-3.5-turbo
Append: [Comparative Personalization for Multi-document Summarization](https://arxiv.org/abs/2509.21562)
Token length: 1578
Summarized using GPT-3.5-turbo
Append: [Vision Language Models Cannot Plan, but Can They Formalize?](https://arxiv.org/abs/2509.21576)
Token length: 1908
Summarized using GPT-3.5-turbo
Append: ["Be My Cheese?": Assessing Cultural Nuance in Multilingual LLM Translations](https://arxiv.org/abs/2509.21577)
Token length: 875
Summarized using GPT-3.5-turbo
Append: [Multi-Objective Reinforcement Learning for Large Language Model Optimization: Visionary Perspective](https://arxiv.org/abs/2509.21613)
Token length: 1940
Summarized using GPT-3.5-turbo
Append: [OjaKV: Context-Aware Online Low-Rank KV Cache Compression with Oja's Rule](https://arxiv.org/abs/2509.21623)
Token length: 1681
Summarized using GPT-3.5-turbo
Append: [Towards Transparent AI: A Survey on Explainable Language Models](https://arxiv.org/abs/2509.21631)
Token length: 1210
Summarized using GPT-3.5-turbo
Append: [ReviewScore: Misinformed Peer Review Detection with Large Language Models](https://arxiv.org/abs/2509.21679)
Token length: 990
Summarized using GPT-3.5-turbo
Append: [GRAB: A Risk Taxonomy--Grounded Benchmark for Unsupervised Topic Discovery in Financial Disclosures](https://arxiv.org/abs/2509.21698)
Token length: 1950
Summarized using GPT-3.5-turbo
Append: [Think-on-Graph 3.0: Efficient and Adaptive LLM Reasoning on Heterogeneous Graphs via Multi-Agent Dual-Evolving Context Retrieval](https://arxiv.org/abs/2509.21710)
Token length: 1187
Summarized using GPT-3.5-turbo
Append: [ProPerSim: Developing Proactive and Personalized AI Assistants through User-Assistant Simulation](https://arxiv.org/abs/2509.21730)
Token length: 884
Summarized using GPT-3.5-turbo
Append: [How Accurate Are LLMs at Multi-Question Answering on Conversational Transcripts?](https://arxiv.org/abs/2509.21732)
Token length: 1681
Summarized using GPT-3.5-turbo
Append: [Self-Speculative Biased Decoding for Faster Live Translation](https://arxiv.org/abs/2509.21740)
Token length: 1600
Summarized using GPT-3.5-turbo
Append: [Thinking with Sound: Audio Chain-of-Thought Enables Multimodal Reasoning in Large Audio-Language Models](https://arxiv.org/abs/2509.21749)
Token length: 1513
Summarized using GPT-3.5-turbo
Append: [SynerGen: Contextualized Generative Recommender for Unified Search and Recommendation](https://arxiv.org/abs/2509.21777)
Token length: 1435
Summarized using GPT-3.5-turbo
Append: [Navigating the Impact of Structured Output Format on Large Language Models through the Compass of Causal Inference](https://arxiv.org/abs/2509.21791)
Token length: 1413
Summarized using GPT-3.5-turbo
Append: [Evaluating and Improving Cultural Awareness of Reward Models for LLM Alignment](https://arxiv.org/abs/2509.21798)
Token length: 1323
Summarized using GPT-3.5-turbo
Append: [Redefining Machine Simultaneous Interpretation: From Incremental Translation to Human-Like Strategies](https://arxiv.org/abs/2509.21801)
Token length: 1445
Summarized using GPT-3.5-turbo
Append: [Towards Minimal Causal Representations for Human Multimodal Language Understanding](https://arxiv.org/abs/2509.21805)
Token length: 1190
Summarized using GPT-3.5-turbo
Append: [Can LLMs Solve and Generate Linguistic Olympiad Puzzles?](https://arxiv.org/abs/2509.21820)
Token length: 1420
Summarized using GPT-3.5-turbo
Append: [ResT: Reshaping Token-Level Policy Gradients for Tool-Use Large Language Models](https://arxiv.org/abs/2509.21826)
Token length: 1070
Summarized using GPT-3.5-turbo
Append: [Semantic Agreement Enables Efficient Open-Ended LLM Cascades](https://arxiv.org/abs/2509.21837)
Token length: 1027
Summarized using GPT-3.5-turbo
Append: [Following the TRACE: A Structured Path to Empathetic Response Generation with Multi-Agent Models](https://arxiv.org/abs/2509.21849)
Token length: 1701
Summarized using GPT-3.5-turbo
Append: [KnowMT-Bench: Benchmarking Knowledge-Intensive Long-Form Question Answering in Multi-Turn Dialogues](https://arxiv.org/abs/2509.21856)
Token length: 728
Summarized using GPT-3.5-turbo
Append: [Enhancing Low-Rank Adaptation with Structured Nonlinear Transformations](https://arxiv.org/abs/2509.21870)
Token length: 1376
Summarized using GPT-3.5-turbo
Append: [LUMINA: Detecting Hallucinations in RAG System with Context-Knowledge Signals](https://arxiv.org/abs/2509.21875)
Token length: 1217
Summarized using GPT-3.5-turbo
Append: [No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM Reinforcement Learning via Entropy-Guided Advantage Shaping](https://arxiv.org/abs/2509.21880)
Token length: 1193
Summarized using GPT-3.5-turbo
Append: [QoNext: Towards Next-generation QoE for Foundation Models](https://arxiv.org/abs/2509.21889)
Token length: 1333
Summarized using GPT-3.5-turbo
Append: [Elastic MoE: Unlocking the Inference-Time Scalability of Mixture-of-Experts](https://arxiv.org/abs/2509.21892)
Token length: 1317
Summarized using GPT-3.5-turbo
Append: [A Large-Scale Dataset and Citation Intent Classification in Turkish with LLMs](https://arxiv.org/abs/2509.21907)
Token length: 1711
Summarized using GPT-3.5-turbo
Append: [AutoSCORE: Enhancing Automated Scoring with Multi-Agent Large Language Models via Structured Component Recognition](https://arxiv.org/abs/2509.21910)
Token length: 870
Summarized using GPT-3.5-turbo
Append: [SimulSense: Sense-Driven Interpreting for Efficient Simultaneous Speech Translation](https://arxiv.org/abs/2509.21932)
Token length: 1643
Summarized using GPT-3.5-turbo
Append: [Why Chain of Thought Fails in Clinical Text Understanding](https://arxiv.org/abs/2509.21933)
Token length: 1168
Summarized using GPT-3.5-turbo
Append: [Debiasing Large Language Models in Thai Political Stance Detection via Counterfactual Calibration](https://arxiv.org/abs/2509.21946)
Token length: 1178
Summarized using GPT-3.5-turbo
Append: [MotivGraph-SoIQ: Integrating Motivational Knowledge Graphs and Socratic Dialogue for Enhanced LLM Ideation](https://arxiv.org/abs/2509.21978)
Token length: 1227
Summarized using GPT-3.5-turbo
Append: [Black-Box Hallucination Detection via Consistency Under the Uncertain Expression](https://arxiv.org/abs/2509.21999)
Token length: 1328
Summarized using GPT-3.5-turbo
Append: [GraphSearch: An Agentic Deep Searching Workflow for Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2509.22009)
Append: [From Outliers to Topics in Language Models: Anticipating Trends in News Corpora](https://arxiv.org/abs/2509.22030)
Append: [Taxonomy of Comprehensive Safety for Clinical Agents](https://arxiv.org/abs/2509.22041)
Append: [Fuzzy Reasoning Chain (FRC): An Innovative Reasoning Framework from Fuzziness to Clarity](https://arxiv.org/abs/2509.22054)
Append: [RedNote-Vibe: A Dataset for Capturing Temporal Dynamics of AI-Generated Text in Social Media](https://arxiv.org/abs/2509.22055)
Append: [The QCET Taxonomy of Standard Quality Criterion Names and Definitions for the Evaluation of NLP Systems](https://arxiv.org/abs/2509.22064)
Append: [Fine-tuning Done Right in Model Editing](https://arxiv.org/abs/2509.22072)
Append: [COSPADI: Compressing LLMs via Calibration-Guided Sparse Dictionary Learning](https://arxiv.org/abs/2509.22075)
Append: [Multilingual Dialogue Generation and Localization with Dialogue Act Scripting](https://arxiv.org/abs/2509.22086)
Append: [S2J: Bridging the Gap Between Solving and Judging Ability in Generative Reward Models](https://arxiv.org/abs/2509.22099)
Append: [Think Right, Not More: Test-Time Scaling for Numerical Claim Verification](https://arxiv.org/abs/2509.22101)
Append: [Universal Legal Article Prediction via Tight Collaboration between Supervised Classification Model and LLM](https://arxiv.org/abs/2509.22119)
Append: [Multilingual Vision-Language Models, A Survey](https://arxiv.org/abs/2509.22123)
Append: [FoodSEM: Large Language Model Specialized in Food Named-Entity Linking](https://arxiv.org/abs/2509.22125)
Append: [R-Capsule: Compressing High-Level Plans for Efficient Large Language Model Reasoning](https://arxiv.org/abs/2509.22131)
Append: [Bridging Draft Policy Misalignment: Group Tree Optimization for Speculative Decoding](https://arxiv.org/abs/2509.22134)
Append: [NFDI4DS Shared Tasks for Scholarly Document Processing](https://arxiv.org/abs/2509.22141)
Append: [From Long to Lean: Performance-aware and Adaptive Chain-of-Thought Compression via Multi-round Refinement](https://arxiv.org/abs/2509.22144)
Append: [Mixture of Detectors: A Compact View of Machine-Generated Text Detection](https://arxiv.org/abs/2509.22147)
Append: [Context Parametrization with Compositional Adapters](https://arxiv.org/abs/2509.22158)
Append: [When Does Reasoning Matter? A Controlled Study of Reasoning's Contribution to Model Performance](https://arxiv.org/abs/2509.22193)
Append: [The Outputs of Large Language Models are Meaningless](https://arxiv.org/abs/2509.22206)
Append: [Question-Driven Analysis and Synthesis: Building Interpretable Thematic Trees with LLMs for Text Clustering and Controllable Generation](https://arxiv.org/abs/2509.22211)
Append: [StableToken: A Noise-Robust Semantic Speech Tokenizer for Resilient SpeechLLMs](https://arxiv.org/abs/2509.22220)
Append: [Thinking in Many Modes: How Composite Reasoning Elevates Large Language Model Performance with Limited Data](https://arxiv.org/abs/2509.22224)
Append: [In Their Own Words: Reasoning Traces Tailored for Small Models Make Them Better Reasoners](https://arxiv.org/abs/2509.22230)
Append: [FeatBench: Evaluating Coding Agents on Feature Implementation for Vibe Coding](https://arxiv.org/abs/2509.22237)
Append: [FLEXI: Benchmarking Full-duplex Human-LLM Speech Interaction](https://arxiv.org/abs/2509.22243)
Append: [Safety Compliance: Rethinking LLM Safety Reasoning through the Lens of Compliance](https://arxiv.org/abs/2509.22250)
Append: [Beyond Textual Context: Structural Graph Encoding with Adaptive Space Alignment to alleviate the hallucination of LLMs](https://arxiv.org/abs/2509.22251)
Append: [Bridging Fairness and Explainability: Can Input-Based Explanations Promote Fairness in Hate Speech Detection?](https://arxiv.org/abs/2509.22291)
Append: [Advancing Natural Language Formalization to First Order Logic with Fine-tuned LLMs](https://arxiv.org/abs/2509.22338)
Append: [Transformers Can Learn Connectivity in Some Graphs but Not Others](https://arxiv.org/abs/2509.22343)
Append: [The InviTE Corpus: Annotating Invectives in Tudor English Texts for Computational Modeling](https://arxiv.org/abs/2509.22345)
Append: [Conversational Implicatures: Modelling Relevance Theory Probabilistically](https://arxiv.org/abs/2509.22354)
Append: [CHRONOBERG: Capturing Language Evolution and Temporal Awareness in Foundation Models](https://arxiv.org/abs/2509.22360)
Append: [Exploratory Semantic Reliability Analysis of Wind Turbine Maintenance Logs using Large Language Models](https://arxiv.org/abs/2509.22366)
Append: [What Is The Political Content in LLMs' Pre- and Post-Training Data?](https://arxiv.org/abs/2509.22367)
Append: [Chimera: Diagnosing Shortcut Learning in Visual-Language Understanding](https://arxiv.org/abs/2509.22437)
Append: [Detecting (Un)answerability in Large Language Models with Linear Directions](https://arxiv.org/abs/2509.22449)
Append: [Evaluating the Limits of Large Language Models in Multilingual Legal Reasoning](https://arxiv.org/abs/2509.22472)
Append: [NeLLCom-Lex: A Neural-agent Framework to Study the Interplay between Lexical Systems and Language Use](https://arxiv.org/abs/2509.22479)
Append: [Exploring Solution Divergence and Its Effect on Large Language Model Problem Solving](https://arxiv.org/abs/2509.22480)
Append: [JGU Mainz's Submission to the WMT25 Shared Task on LLMs with Limited Resources for Slavic Languages: MT and QA](https://arxiv.org/abs/2509.22490)
Append: [Representing LLMs in Prompt Semantic Task Space](https://arxiv.org/abs/2509.22506)
Append: [We Think, Therefore We Align LLMs to Helpful, Harmless and Honest Before They Go Wrong](https://arxiv.org/abs/2509.22510)
Append: [InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced Language Models](https://arxiv.org/abs/2509.22536)
Append: [Think Socially via Cognitive Reasoning](https://arxiv.org/abs/2509.22546)
Append: [Retrieval-Augmented Guardrails for AI-Drafted Patient-Portal Messages: Error Taxonomy Construction and Large-Scale Evaluation](https://arxiv.org/abs/2509.22565)
Append: [Fine-Grained Detection of Context-Grounded Hallucinations Using LLMs](https://arxiv.org/abs/2509.22582)
Append: [ArabJobs: A Multinational Corpus of Arabic Job Ads](https://arxiv.org/abs/2509.22589)
Append: [From Formal Language Theory to Statistical Learning: Finite Observability of Subregular Languages](https://arxiv.org/abs/2509.22598)
Append: [Capturing Opinion Shifts in Deliberative Discourse through Frequency-based Quantum deep learning methods](https://arxiv.org/abs/2509.22603)
Append: [From tests to effect sizes: Quantifying uncertainty and statistical variability in multilingual and multitask NLP evaluation benchmarks](https://arxiv.org/abs/2509.22612)
Append: [StateX: Enhancing RNN Recall via Post-training State Expansion](https://arxiv.org/abs/2509.22630)
Append: [Variational Reasoning for Language Models](https://arxiv.org/abs/2509.22637)
Append: [Language Models Can Learn from Verbal Feedback Without Scalar Rewards](https://arxiv.org/abs/2509.22638)
Append: [Death of the Novel(ty): Beyond n-Gram Novelty as a Metric for Textual Creativity](https://arxiv.org/abs/2509.22641)
Append: [WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level Feedback and Step-Level Reinforcement Learning](https://arxiv.org/abs/2509.22644)
Append: [VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing](https://arxiv.org/abs/2509.22651)
Append: [Accelerate Creation of Product Claims Using Generative AI](https://arxiv.org/abs/2509.20652)
Append: [HetaRAG: Hybrid Deep Retrieval-Augmented Generation across Heterogeneous Data Stores](https://arxiv.org/abs/2509.21336)
Append: [Towards mitigating information leakage when evaluating safety monitors](https://arxiv.org/abs/2509.21344)
Append: [Random Direct Preference Optimization for Radiography Report Generation](https://arxiv.org/abs/2509.21351)
Append: [ReGeS: Reciprocal Retrieval-Generation Synergy for Conversational Recommender Systems](https://arxiv.org/abs/2509.21371)
Append: [LLMs for Bayesian Optimization in Scientific Domains: Are We There Yet?](https://arxiv.org/abs/2509.21403)
Append: [ARTI-6: Towards Six-dimensional Articulatory Speech Encoding](https://arxiv.org/abs/2509.21447)
Append: [VideoJudge: Bootstrapping Enables Scalable Supervision of MLLM-as-a-Judge for Video Understanding](https://arxiv.org/abs/2509.21451)
Append: [Gender Stereotypes in Professional Roles Among Saudis: An Analytical Study of AI-Generated Images Using Language Models](https://arxiv.org/abs/2509.21466)
Append: [Are Hallucinations Bad Estimations?](https://arxiv.org/abs/2509.21473)
Append: [LLM Agent Meets Agentic AI: Can LLM Agents Simulate Customers to Evaluate Agentic-AI-based Shopping Assistants?](https://arxiv.org/abs/2509.21501)
Append: [Uncertainty-Aware Knowledge Tracing Models](https://arxiv.org/abs/2509.21514)
Append: [C-QUERI: Congressional Questions, Exchanges, and Responses in Institutions Dataset](https://arxiv.org/abs/2509.21548)
Append: [Learning GUI Grounding with Spatial Reasoning from Visual Feedback](https://arxiv.org/abs/2509.21552)
Append: [Leveraging Big Data Frameworks for Spam Detection in Amazon Reviews](https://arxiv.org/abs/2509.21579)
Append: [AUDDT: Audio Unified Deepfake Detection Benchmark Toolkit](https://arxiv.org/abs/2509.21597)
Append: [InvBench: Can LLMs Accelerate Program Verification with Invariant Synthesis?](https://arxiv.org/abs/2509.21629)
Append: [UISim: An Interactive Image-Based UI Simulator for Dynamic Mobile Environments](https://arxiv.org/abs/2509.21733)
Append: [UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon Scenarios](https://arxiv.org/abs/2509.21766)
Append: [DeHate: A Stable Diffusion-based Multimodal Approach to Mitigate Hate Speech in Images](https://arxiv.org/abs/2509.21787)
Append: [Compiling by Proving: Language-Agnostic Automatic Optimization from Formal Semantics](https://arxiv.org/abs/2509.21793)
Append: [SBFA: Single Sneaky Bit Flip Attack to Break Large Language Models](https://arxiv.org/abs/2509.21843)
Append: [What Makes LLM Agent Simulations Useful for Policy? Insights From an Iterative Design Engagement in Emergency Preparedness](https://arxiv.org/abs/2509.21868)
Append: [You Can't Steal Nothing: Mitigating Prompt Leakages in LLMs via System Vectors](https://arxiv.org/abs/2509.21884)
Append: [AgentPack: A Dataset of Code Changes, Co-Authored by Agents and Humans](https://arxiv.org/abs/2509.21891)
Append: [Evaluating Open-Source Large Language Models for Technical Telecom Question Answering](https://arxiv.org/abs/2509.21949)
Append: [RISK: A Framework for GUI Agents in E-commerce Risk Management](https://arxiv.org/abs/2509.21982)
Append: [From Bias to Balance: Exploring and Mitigating Spatial Bias in LVLMs](https://arxiv.org/abs/2509.21984)
Append: [ERGO: Efficient High-Resolution Visual Understanding for Vision-Language Models](https://arxiv.org/abs/2509.21991)
Append: [The Thinking Spectrum: An Emperical Study of Tunable Reasoning in LLMs through Model Merging](https://arxiv.org/abs/2509.22034)
Append: [A2R: An Asymmetric Two-Stage Reasoning Framework for Parallel Reasoning](https://arxiv.org/abs/2509.22044)
Append: [Speak Your Mind: The Speech Continuation Task as a Probe of Voice-Based Model Bias](https://arxiv.org/abs/2509.22061)
Append: [SecureAgentBench: Benchmarking Secure Code Generation under Realistic Vulnerability Scenarios](https://arxiv.org/abs/2509.22097)
Append: [MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing](https://arxiv.org/abs/2509.22186)
Append: [Library Hallucinations in LLMs: Risk Analysis Grounded in Developer Queries](https://arxiv.org/abs/2509.22202)
Append: [InfiMed-Foundation: Pioneering Advanced Multimodal Medical Models with Compute-Efficient Pre-Training and Multi-Stage Fine-Tuning](https://arxiv.org/abs/2509.22261)
Append: [PRIME: Planning and Retrieval-Integrated Memory for Enhanced Reasoning](https://arxiv.org/abs/2509.22315)
Append: [Can Synthetic Query Rewrites Capture User Intent Better than Humans in Retrieval-Augmented Generation?](https://arxiv.org/abs/2509.22325)
Append: [Bridging Kolmogorov Complexity and Deep Learning: Asymptotically Optimal Description Length Objectives for Transformers](https://arxiv.org/abs/2509.22445)
Append: [MDAR: A Multi-scene Dynamic Audio Reasoning Benchmark](https://arxiv.org/abs/2509.22461)
Append: [IIET: Efficient Numerical Transformer via Implicit Iterative Euler Method](https://arxiv.org/abs/2509.22463)
Append: [Mental Health Impacts of AI Companions: Triangulating Social Media Quasi-Experiments, User Perspectives, and Relational Theory](https://arxiv.org/abs/2509.22505)
Append: [Does AI Coaching Prepare us for Workplace Negotiations?](https://arxiv.org/abs/2509.22545)
Append: [Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs at Test Time](https://arxiv.org/abs/2509.22572)
Append: [EPO: Entropy-regularized Policy Optimization for LLM Agents Reinforcement Learning](https://arxiv.org/abs/2509.22576)
Append: [Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning](https://arxiv.org/abs/2509.22601)
Append: [Vision-Language Alignment from Compressed Image Representations using 2D Gaussian Splatting](https://arxiv.org/abs/2509.22615)
Append: [IA2: Alignment with ICL Activations Improves Supervised Fine-Tuning](https://arxiv.org/abs/2509.22621)
Append: [LABELING COPILOT: A Deep Research Agent for Automated Data Curation in Computer Vision](https://arxiv.org/abs/2509.22631)
Append: [Towards Efficient Online Exploration for Reinforcement Learning with Human Feedback](https://arxiv.org/abs/2509.22633)
Append: [Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs](https://arxiv.org/abs/2509.22646)
Append: [CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement Learning](https://arxiv.org/abs/2509.22647)
Append: [See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation](https://arxiv.org/abs/2509.22653)
Append: [Constituency Parsing using LLMs](https://arxiv.org/abs/2310.19462)
Append: [TEXT2AFFORD: Probing Object Affordance Prediction abilities of Language Models solely from Text](https://arxiv.org/abs/2402.12881)
Append: [Sharing Matters: Analysing Neurons Across Languages and Tasks in LLMs](https://arxiv.org/abs/2406.09265)
Append: [LLMAEL: Large Language Models are Good Context Augmenters for Entity Linking](https://arxiv.org/abs/2407.04020)
Append: [Position IDs Matter: An Enhanced Position Layout for Efficient Context Compression in Large Language Models](https://arxiv.org/abs/2409.14364)
Append: [Stuffed Mamba: Oversized States Lead to the Inability to Forget](https://arxiv.org/abs/2410.07145)
Append: [Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning](https://arxiv.org/abs/2410.11020)
Append: [Vulnerability of LLMs to Vertically Aligned Text Manipulations](https://arxiv.org/abs/2410.20016)
Append: [Semantic Component Analysis: Introducing Multi-Topic Distributions to Clustering-Based Topic Modeling](https://arxiv.org/abs/2410.21054)
Append: [$100K or 100 Days: Trade-offs when Pre-Training with Academic Resources](https://arxiv.org/abs/2410.23261)
Append: [AtomR: Atomic Operator-Empowered Large Language Models for Heterogeneous Knowledge Reasoning](https://arxiv.org/abs/2411.16495)
Append: [Can LLMs be Good Graph Judge for Knowledge Graph Construction?](https://arxiv.org/abs/2411.17388)
Append: [Demystifying Domain-adaptive Post-training for Financial LLMs](https://arxiv.org/abs/2501.04961)
Append: [Demystifying Multilingual Chain-of-Thought in Process Reward Modeling](https://arxiv.org/abs/2502.12663)
Append: [Elucidating Mechanisms of Demographic Bias in LLMs for Healthcare](https://arxiv.org/abs/2502.13319)
Append: [LoRA-MGPO: Mitigating Double Descent in Low-Rank Adaptation via Momentum-Guided Perturbation Optimization](https://arxiv.org/abs/2502.14538)
Append: [RuCCoD: Towards Automated ICD Coding in Russian](https://arxiv.org/abs/2502.21263)
Append: [How LLMs Fail to Support Fact-Checking](https://arxiv.org/abs/2503.01902)
Append: [Adaptively profiling models with task elicitation](https://arxiv.org/abs/2503.01986)
Append: [Generator-Assistant Stepwise Rollback Framework for Large Language Model Agent](https://arxiv.org/abs/2503.02519)
Append: [Improving LLM-as-a-Judge Inference with the Judgment Distribution](https://arxiv.org/abs/2503.03064)
Append: [InftyThink: Breaking the Length Limits of Long-Context Reasoning in Large Language Models](https://arxiv.org/abs/2503.06692)
Append: [Cost-Optimal Grouped-Query Attention for Long-Context Modeling](https://arxiv.org/abs/2503.09579)
Append: [Retrieval-Augmented Generation with Hierarchical Knowledge](https://arxiv.org/abs/2503.10150)
Append: [Texture or Semantics? Vision-Language Models Get Lost in Font Recognition](https://arxiv.org/abs/2503.23768)
Append: [CLASH: Evaluating Language Models on Judging High-Stakes Dilemmas from Multiple Perspectives](https://arxiv.org/abs/2504.10823)
Append: [SOLAR: Towards Characterizing Subjectivity of Individuals through Modeling Value Conflicts and Trade-offs](https://arxiv.org/abs/2504.12633)
Append: [MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety](https://arxiv.org/abs/2504.15241)
Append: [Comparing Uncertainty Measurement and Mitigation Methods for Large Language Models: A Systematic Review](https://arxiv.org/abs/2504.18346)
Append: [LLM-OptiRA: LLM-Driven Optimization of Resource Allocation for Non-Convex Problems in Wireless Communications](https://arxiv.org/abs/2505.02091)
Append: [Follow the Path: Reasoning over Knowledge Graph Paths to Improve LLM Factuality](https://arxiv.org/abs/2505.11140)
Append: [SuperCoder: Assembly Program Superoptimization with Large Language Models](https://arxiv.org/abs/2505.11480)
Append: [HiddenBench: Assessing Collective Reasoning in Multi-Agent LLMs via Hidden Profile Tasks](https://arxiv.org/abs/2505.11556)
Append: [ZeroTuning: Unlocking the Initial Token's Power to Enhance Large Language Models Without Training](https://arxiv.org/abs/2505.11739)
Append: [HBO: Hierarchical Balancing Optimization for Fine-Tuning Large Language Models](https://arxiv.org/abs/2505.12300)
Append: [ExpertSteer: Intervening in LLMs through Expert Knowledge](https://arxiv.org/abs/2505.12313)
Append: [Shadow-FT: Tuning Instruct Model via Training on Paired Base Model](https://arxiv.org/abs/2505.12716)
Append: [Language-Specific Latent Process Hinders Cross-Lingual Performance](https://arxiv.org/abs/2505.13141)
Append: [UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in Language Models](https://arxiv.org/abs/2505.14679)
Append: [Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric Speech Recognition in Low-Resource Languages](https://arxiv.org/abs/2505.14874)
Append: [UniErase: Towards Balanced and Precise Unlearning in Language Models](https://arxiv.org/abs/2505.15674)
Append: [Beyond Early-Token Bias: Model-Specific and Language-Specific Position Effects in Multilingual LLMs](https://arxiv.org/abs/2505.16134)
Append: [Attributing Response to Context: A Jensen-Shannon Divergence Driven Mechanistic Study of Context Attribution in Retrieval-Augmented Generation](https://arxiv.org/abs/2505.16415)
Append: [Beyond Static Testbeds: An Interaction-Centric Agent Simulation Platform for Dynamic Recommender Systems](https://arxiv.org/abs/2505.16429)
Append: [Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs](https://arxiv.org/abs/2505.16831)
Append: [BP-Seg: A graphical model approach to unsupervised and non-contiguous text segmentation using belief propagation](https://arxiv.org/abs/2505.16965)
Append: [From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning](https://arxiv.org/abs/2505.17117)
Append: [Prompting is not Enough: Exploring Knowledge Integration and Controllable Generation on Large Language Models](https://arxiv.org/abs/2505.19660)
Append: [BiomedSQL: Text-to-SQL for Scientific Reasoning on Biomedical Knowledge Bases](https://arxiv.org/abs/2505.20321)
Append: [EmoBench-UA: A Benchmark Dataset for Emotion Detection in Ukrainian](https://arxiv.org/abs/2505.23297)
Append: [Table-R1: Inference-Time Scaling for Table Reasoning](https://arxiv.org/abs/2505.23621)
Append: [InfiMed: Low-Resource Medical MLLMs with Advancing Understanding and Reasoning](https://arxiv.org/abs/2505.23867)
Append: [RARE: Retrieval-Aware Robustness Evaluation for Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2506.00789)
Append: [Probing Neural Topology of Large Language Models](https://arxiv.org/abs/2506.01042)
Append: [Resisting Contextual Interference in RAG via Parametric-Knowledge Reinforcement](https://arxiv.org/abs/2506.05154)
Append: [QA-LIGN: Aligning LLMs through Constitutionally Decomposed QA](https://arxiv.org/abs/2506.08123)
Append: [Personalized LLM Decoding via Contrasting Personal Preference](https://arxiv.org/abs/2506.12109)
Append: [MUCAR: Benchmarking Multilingual Cross-Modal Ambiguity Resolution for Multimodal Large Language Models](https://arxiv.org/abs/2506.17046)
Append: [KaLM-Embedding-V2: Superior Training Techniques and Data Inspire A Versatile Embedding Model](https://arxiv.org/abs/2506.20923)
Append: [WildSpeech-Bench: Benchmarking End-to-End SpeechLLMs in the Wild](https://arxiv.org/abs/2506.21875)
Append: [Why Reinforcement Fine-Tuning Enables MLLMs Preserve Prior Knowledge Better: A Data Perspective](https://arxiv.org/abs/2506.23508)
Append: [Unveiling the Potential of Diffusion Large Language Model in Controllable Generation](https://arxiv.org/abs/2507.04504)
Append: [Evaluating Memory in LLM Agents via Incremental Multi-Turn Interactions](https://arxiv.org/abs/2507.05257)
Append: [Learn Globally, Speak Locally: Bridging the Gaps in Multilingual Reasoning](https://arxiv.org/abs/2507.05418)
Append: [What Factors Affect LLMs and RLLMs in Financial Question Answering?](https://arxiv.org/abs/2507.08339)
Append: [KV Cache Steering for Controlling Frozen LLMs](https://arxiv.org/abs/2507.08799)
Append: [The Imitation Game: Turing Machine Imitator is Length Generalizable Reasoner](https://arxiv.org/abs/2507.13332)
Append: [LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for Multi-Turn Dialogues](https://arxiv.org/abs/2507.13681)
Append: [Wide-In, Narrow-Out: Revokable Decoding for Efficient and Effective DLLMs](https://arxiv.org/abs/2507.18578)
Append: [Geometric-Mean Policy Optimization](https://arxiv.org/abs/2507.20673)
Append: [Persona-Augmented Benchmarking: Evaluating LLMs Across Diverse Writing Styles](https://arxiv.org/abs/2507.22168)
Append: [PilotRL: Training Language Model Agents via Global Planning-Guided Progressive Reinforcement Learning](https://arxiv.org/abs/2508.00344)
Append: [DAMR: Efficient and Adaptive Context-Aware Knowledge Graph Question Answering with LLM-Guided MCTS](https://arxiv.org/abs/2508.00719)
Append: [MLP Memory: A Retriever-Pretrained Memory for Large Language Models](https://arxiv.org/abs/2508.01832)
Append: [Hallucination to Truth: A Review of Fact-Checking and Factuality Evaluation in Large Language Models](https://arxiv.org/abs/2508.03860)
Append: [GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy Entropy](https://arxiv.org/abs/2508.04349)
Append: [ASCoT: An Adaptive Self-Correction Chain-of-Thought Method for Late-Stage Fragility in LLMs](https://arxiv.org/abs/2508.05282)
Append: [Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI's Latest Open Source Models](https://arxiv.org/abs/2508.12461)
Append: [Conflict-Aware Soft Prompting for Retrieval-Augmented Generation](https://arxiv.org/abs/2508.15253)
Append: [Influence-driven Curriculum Learning for Pre-training on Limited Data](https://arxiv.org/abs/2508.15475)
Append: [Dream to Chat: Model-based Reinforcement Learning on Dialogues with User Belief Modeling](https://arxiv.org/abs/2508.16876)
Append: [CMRAG: Co-modality-based visual document retrieval and question answering](https://arxiv.org/abs/2509.02123)
Append: [Chain or tree? Re-evaluating complex reasoning from the perspective of a matrix of thought](https://arxiv.org/abs/2509.03918)
Append: [Towards an AI Musician: Synthesizing Sheet Music Problems for Musical Reasoning](https://arxiv.org/abs/2509.04059)
Append: [Modelling Analogies and Analogical Reasoning: Connecting Cognitive Science Theory and NLP Research](https://arxiv.org/abs/2509.09381)
Append: [Reasoning Under Uncertainty: Exploring Probabilistic Reasoning Capabilities of LLMs](https://arxiv.org/abs/2509.10739)
Append: [Positional Encoding via Token-Aware Phase Attention](https://arxiv.org/abs/2509.12635)
Append: [A Critical Look At Tokenwise Reward-Guided Text Generation](https://arxiv.org/abs/2406.07780)
Append: [Large Language Models versus Classical Machine Learning: Performance in COVID-19 Mortality Prediction Using High-Dimensional Tabular Data](https://arxiv.org/abs/2409.02136)
Append: [On the Within-class Variation Issue in Alzheimer's Disease Detection](https://arxiv.org/abs/2409.16322)
Append: [Development and Validation of a Large Language Model for Generating Fully-Structured Radiology Reports](https://arxiv.org/abs/2409.18319)
Append: [Rare-to-Frequent: Unlocking Compositional Generation Power of Diffusion Models on Rare Concepts with LLM Guidance](https://arxiv.org/abs/2410.22376)
Append: [Training-Free Bayesianization for Low-Rank Adapters of Large Language Models](https://arxiv.org/abs/2412.05723)
Append: [Detecting and Interpreting NSFW Prompts in Text-to-Image Models through Uncovering Harmful Semantics](https://arxiv.org/abs/2412.18123)
Append: [Process Reinforcement through Implicit Rewards](https://arxiv.org/abs/2502.01456)
Append: [GeoDANO: Geometric VLM with Domain Agnostic Vision Encoder](https://arxiv.org/abs/2502.11360)
Append: [Taxonomy, Opportunities, and Challenges of Representation Engineering for Large Language Models](https://arxiv.org/abs/2502.19649)
Append: [Recursive Training Loops in LLMs: How training data properties modulate distribution shift in generated data?](https://arxiv.org/abs/2504.03814)
Append: [TokUR: Token-Level Uncertainty Estimation for Large Language Model Reasoning](https://arxiv.org/abs/2505.11737)
Append: [Feature Hedging: Correlated Features Break Narrow Sparse Autoencoders](https://arxiv.org/abs/2505.11756)
Append: [The Polar Express: Optimal Matrix Sign Methods and Their Application to the Muon Algorithm](https://arxiv.org/abs/2505.16932)
Append: [ChartGalaxy: A Dataset for Infographic Chart Understanding and Generation](https://arxiv.org/abs/2505.18668)
Append: [Domain-Aware Tensor Network Structure Search](https://arxiv.org/abs/2505.23537)
Append: [Think With Videos For Agentic Long-Video Understanding](https://arxiv.org/abs/2506.10821)
Append: [Security Degradation in Iterative AI Code Generation -- A Systematic Analysis of the Paradox](https://arxiv.org/abs/2506.11022)
Append: [video-SALMONN 2: Caption-Enhanced Audio-Visual Large Language Models](https://arxiv.org/abs/2506.15220)
Append: [Latent Concept Disentanglement in Transformer-based Language Models](https://arxiv.org/abs/2506.16975)
Append: [MultiVox: A Benchmark for Evaluating Voice Assistants for Multimodal Interactions](https://arxiv.org/abs/2507.10859)
Append: [Rethinking the Embodied Gap in Vision-and-Language Navigation: A Holistic Study of Physical and Visual Disparities](https://arxiv.org/abs/2507.13019)
Append: [The Invisible Leash: Why RLVR May or May Not Escape Its Origin](https://arxiv.org/abs/2507.14843)
Append: [R-Stitch: Dynamic Trajectory Stitching for Efficient Reasoning](https://arxiv.org/abs/2507.17307)
Append: [Resource Consumption Red-Teaming for Large Vision-Language Models](https://arxiv.org/abs/2507.18053)
Append: [$A^2R^2$: Advancing Img2LaTeX Conversion via Visual Reasoning with Attention-Guided Refinement](https://arxiv.org/abs/2507.20890)
Append: [Sparse but Wrong: Incorrect L0 Leads to Incorrect Features in Sparse Autoencoders](https://arxiv.org/abs/2508.16560)
Append: [EigenBench: A Comparative Behavioral Measure of Value Alignment](https://arxiv.org/abs/2509.01938)
Append: [GLEAM: Learning to Match and Explain in Cross-View Geo-Localization](https://arxiv.org/abs/2509.07450)
append_entries: 278
Finish: 2025-09-29 04:25:13.152263
------------------------------------------------------
Started: 2025-09-29 06:26:27.450600
Existing_entries: 1278
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1706
Summarized using GPT-3.5-turbo
Append: [VAT-KG: Knowledge-Intensive Multimodal Knowledge Graph Dataset for Retrieval-Augmented Generation](https://arxiv.org/abs/2506.21556)
Token length: 1067
Summarized using GPT-3.5-turbo
Append: [DivLogicEval: A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models](https://arxiv.org/abs/2509.15587)
Token length: 1463
Summarized using GPT-3.5-turbo
Append: [Distribution-Aligned Decoding for Efficient LLM Task Adaptation](https://arxiv.org/abs/2509.15888)
Token length: 1891
Summarized using GPT-3.5-turbo
Append: [RPG: A Repository Planning Graph for Unified and Scalable Codebase Generation](https://arxiv.org/abs/2509.16198)
Token length: 1729
Summarized using GPT-3.5-turbo
Append: [From Roots to Rewards: Dynamic Tree Reasoning with Reinforcement Learning](https://arxiv.org/abs/2507.13142)
append_entries: 5
Finish: 2025-09-29 06:26:38.174577
------------------------------------------------------
Started: 2025-09-29 08:24:00.331993
Existing_entries: 1005
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-29 08:24:00.931396
------------------------------------------------------
Started: 2025-09-29 10:17:51.519453
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-29 10:17:52.215432
------------------------------------------------------
Started: 2025-09-29 12:34:47.662035
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-29 12:34:48.379295
------------------------------------------------------
Started: 2025-09-29 14:16:39.901517
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-29 14:16:40.534351
------------------------------------------------------
Started: 2025-09-29 16:16:38.552772
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-29 16:16:39.185947
------------------------------------------------------
Started: 2025-09-29 18:23:41.786258
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-29 18:23:42.376661
------------------------------------------------------
Started: 2025-09-29 20:16:10.014911
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-29 20:16:10.626726
------------------------------------------------------
Started: 2025-09-29 22:13:45.054424
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-29 22:13:45.700022
------------------------------------------------------
Started: 2025-09-30 01:14:21.995439
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-30 01:14:22.637658
------------------------------------------------------
Started: 2025-09-30 02:55:40.160001
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-30 02:55:40.782734
------------------------------------------------------
Started: 2025-09-30 04:23:06.555191
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1265
Summarized using GPT-3.5-turbo
Append: [Are you sure? Measuring models bias in content moderation through uncertainty](https://arxiv.org/abs/2509.22699)
Token length: 1410
Summarized using GPT-3.5-turbo
Append: [AccessEval: Benchmarking Disability Bias in Large Language Models](https://arxiv.org/abs/2509.22703)
Token length: 1398
Summarized using GPT-3.5-turbo
Append: [RAR$^2$: Retrieval-Augmented Medical Reasoning via Thought-Driven Retrieval](https://arxiv.org/abs/2509.22713)
Token length: 1448
Summarized using GPT-3.5-turbo
Append: [TRUEBench: Can LLM Response Meet Real-world Constraints as Productivity Assistant?](https://arxiv.org/abs/2509.22715)
Token length: 1223
Summarized using GPT-3.5-turbo
Append: [Multi-Modal Sentiment Analysis with Dynamic Attention Fusion](https://arxiv.org/abs/2509.22729)
Token length: 1585
Summarized using GPT-3.5-turbo
Append: [Enabling Approximate Joint Sampling in Diffusion LMs](https://arxiv.org/abs/2509.22738)
Token length: 1617
Summarized using GPT-3.5-turbo
Append: [Painless Activation Steering: An Automated, Lightweight Approach for Post-Training Large Language Models](https://arxiv.org/abs/2509.22739)
Token length: 1601
Summarized using GPT-3.5-turbo
Append: [MIRAGE: Multi-hop Reasoning with Ambiguity Evaluation for Illusory Questions](https://arxiv.org/abs/2509.22750)
Token length: 1258
Summarized using GPT-3.5-turbo
Append: [ML2B: Multi-Lingual ML Benchmark For AutoML](https://arxiv.org/abs/2509.22768)
Token length: 1462
Summarized using GPT-3.5-turbo
Append: [ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection](https://arxiv.org/abs/2509.22808)
Token length: 1254
Summarized using GPT-3.5-turbo
Append: [EditGRPO: Reinforcement Learning with Post -Rollout Edits for Clinically Accurate Chest X-Ray Report Generation](https://arxiv.org/abs/2509.22812)
Token length: 1821
Summarized using GPT-3.5-turbo
Append: [Critique-Coder: Enhancing Coder Models by Critique Reinforcement Learning](https://arxiv.org/abs/2509.22824)
Token length: 1851
Summarized using GPT-3.5-turbo
Append: [ChatInject: Abusing Chat Templates for Prompt Injection in LLM Agents](https://arxiv.org/abs/2509.22830)
Token length: 1647
Summarized using GPT-3.5-turbo
Append: [Learning to Detect Relevant Contexts and Knowledge for Response Selection in Retrieval-based Dialogue Systems](https://arxiv.org/abs/2509.22845)
Token length: 1315
Summarized using GPT-3.5-turbo
Append: [Towards Generalizable Implicit In-Context Learning with Attention Routing](https://arxiv.org/abs/2509.22854)
Token length: 1472
Summarized using GPT-3.5-turbo
Append: [The Bias is in the Details: An Assessment of Cognitive Bias in LLMs](https://arxiv.org/abs/2509.22856)
Token length: 1176
Summarized using GPT-3.5-turbo
Append: [Lexicon-Enriched Graph Modeling for Arabic Document Readability Prediction](https://arxiv.org/abs/2509.22870)
Token length: 1705
Summarized using GPT-3.5-turbo
Append: [HEART: Emotionally-driven test-time scaling of Language Models](https://arxiv.org/abs/2509.22876)
Token length: 1126
Summarized using GPT-3.5-turbo
Append: [Infusing Theory of Mind into Socially Intelligent LLM Agents](https://arxiv.org/abs/2509.22887)
Token length: 1247
Summarized using GPT-3.5-turbo
Append: [Extract-0: A Specialized Language Model for Document Information Extraction](https://arxiv.org/abs/2509.22906)
Token length: 1953
Summarized using GPT-3.5-turbo
Append: [Large language models management of medications: three performance analyses](https://arxiv.org/abs/2509.22926)
Token length: 1284
Summarized using GPT-3.5-turbo
Append: [LLMs Behind the Scenes: Enabling Narrative Scene Illustration](https://arxiv.org/abs/2509.22940)
Token length: 1967
Summarized using GPT-3.5-turbo
Append: [What Matters More For In-Context Learning under Matched Compute Budgets: Pretraining on Natural Text or Incorporating Targeted Synthetic Examples?](https://arxiv.org/abs/2509.22947)
Token length: 1080
Summarized using GPT-3.5-turbo
Append: [Emergent morpho-phonological representations in self-supervised speech models](https://arxiv.org/abs/2509.22973)
Token length: 1436
Summarized using GPT-3.5-turbo
Append: [Same Content, Different Representations: A Controlled Study for Table QA](https://arxiv.org/abs/2509.22983)
Token length: 1354
Summarized using GPT-3.5-turbo
Append: [ADAM: A Diverse Archive of Mankind for Evaluating and Enhancing LLMs in Biographical Reasoning](https://arxiv.org/abs/2509.22991)
Token length: 1396
Summarized using GPT-3.5-turbo
Append: [AI Brown and AI Koditex: LLM-Generated Corpora Comparable to Traditional Corpora of English and Czech Texts](https://arxiv.org/abs/2509.22996)
Token length: 1286
Summarized using GPT-3.5-turbo
Append: [Look Back to Reason Forward: Revisitable Memory for Long-Context LLM Agents](https://arxiv.org/abs/2509.23040)
Token length: 1460
Summarized using GPT-3.5-turbo
Append: [Peacemaker or Troublemaker: How Sycophancy Shapes Multi-Agent Debate](https://arxiv.org/abs/2509.23055)
Token length: 1542
Summarized using GPT-3.5-turbo
Append: [Semantic Voting: A Self-Evaluation-Free Approach for Efficient LLM Self-Improvement on Unverifiable Open-ended Tasks](https://arxiv.org/abs/2509.23067)
Token length: 1591
Summarized using GPT-3.5-turbo
Append: [From Evidence to Trajectory: Abductive Reasoning Path Synthesis for Training Retrieval-Augmented Generation Agents](https://arxiv.org/abs/2509.23071)
Token length: 1297
Summarized using GPT-3.5-turbo
Append: [The Geometry of Creative Variability: How Credal Sets Expose Calibration Gaps in Language Models](https://arxiv.org/abs/2509.23088)
Token length: 1201
Summarized using GPT-3.5-turbo
Append: [d$^2$Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching](https://arxiv.org/abs/2509.23094)
Token length: 1328
Summarized using GPT-3.5-turbo
Append: [How to Make Large Language Models Generate 100% Valid Molecules?](https://arxiv.org/abs/2509.23099)
Token length: 1787
Summarized using GPT-3.5-turbo
Append: [Non-Collaborative User Simulators for Tool Agents](https://arxiv.org/abs/2509.23124)
Token length: 1534
Summarized using GPT-3.5-turbo
Append: [Tagging the Thought: Unlocking Personalization Reasoning via Reinforcement Learning](https://arxiv.org/abs/2509.23140)
Token length: 1382
Summarized using GPT-3.5-turbo
Append: [Tree Reward-Aligned Search for TReASURe in Masked Diffusion Language Models](https://arxiv.org/abs/2509.23146)
Token length: 1387
Summarized using GPT-3.5-turbo
Append: [Test-Time Policy Adaptation for Enhanced Multi-Turn Interactions with LLMs](https://arxiv.org/abs/2509.23166)
Token length: 1321
Summarized using GPT-3.5-turbo
Append: [Pretraining LLM with Latent Thoughts in Continuous Space](https://arxiv.org/abs/2509.23184)
Token length: 1412
Summarized using GPT-3.5-turbo
Append: [Diagnose, Localize, Align: A Full-Stack Framework for Reliable LLM Multi-Agent Systems under Instruction Conflicts](https://arxiv.org/abs/2509.23188)
Token length: 1330
Summarized using GPT-3.5-turbo
Append: [Estimating the strength and timing of syntactic structure building in naturalistic reading](https://arxiv.org/abs/2509.23195)
Token length: 1473
Summarized using GPT-3.5-turbo
Append: [From Harm to Help: Turning Reasoning In-Context Demos into Assets for Reasoning LMs](https://arxiv.org/abs/2509.23196)
Token length: 1655
Summarized using GPT-3.5-turbo
Append: [Global Beats, Local Tongue: Studying Code Switching in K-pop Hits on Billboard Charts](https://arxiv.org/abs/2509.23197)
Token length: 1071
Summarized using GPT-3.5-turbo
Append: [Steering Prepositional Phrases in Language Models: A Case of with-headed Adjectival and Adverbial Complements in Gemma-2](https://arxiv.org/abs/2509.23204)
Token length: 1572
Summarized using GPT-3.5-turbo
Append: [PARL-MT: Learning to Call Functions in Multi-Turn Conversation with Progress Awareness](https://arxiv.org/abs/2509.23206)
Token length: 1214
Summarized using GPT-3.5-turbo
Append: [A Structured Framework for Evaluating and Enhancing Interpretive Capabilities of Multimodal LLMs in Culturally Situated Tasks](https://arxiv.org/abs/2509.23208)
Token length: 1521
Summarized using GPT-3.5-turbo
Append: [Detecting Corpus-Level Knowledge Inconsistencies in Wikipedia with Large Language Models](https://arxiv.org/abs/2509.23233)
Token length: 1325
Summarized using GPT-3.5-turbo
Append: [Fin-ExBERT: User Intent based Text Extraction in Financial Context using Graph-Augmented BERT and trainable Plugin](https://arxiv.org/abs/2509.23259)
Token length: 1070
Summarized using GPT-3.5-turbo
Append: [A2D: Any-Order, Any-Step Safety Alignment for Diffusion Language Models](https://arxiv.org/abs/2509.23286)
Token length: 1148
Summarized using GPT-3.5-turbo
Append: [Scaling Policy Compliance Assessment in Language Models with Policy Reasoning Traces](https://arxiv.org/abs/2509.23291)
Append: [Learning to Reason in Structured In-context Environments with Reinforcement Learning](https://arxiv.org/abs/2509.23330)
Append: [C-Evolve: Consensus-based Evolution for Prompt Groups](https://arxiv.org/abs/2509.23331)
Append: [Dual-Space Smoothness for Robust and Balanced LLM Unlearning](https://arxiv.org/abs/2509.23362)
Append: [MedCritical: Enhancing Medical Reasoning in Small Language Models via Self-Collaborative Correction](https://arxiv.org/abs/2509.23368)
Append: [Alignment through Meta-Weighted Online Sampling: Bridging the Gap between Data Generation and Preference Optimization](https://arxiv.org/abs/2509.23371)
Append: [CCD: Mitigating Hallucinations in Radiology MLLMs via Clinical Contrastive Decoding](https://arxiv.org/abs/2509.23379)
Append: [Guard Vector: Beyond English LLM Guardrails with Task-Vector Composition and Streaming-Aware Prefix SFT](https://arxiv.org/abs/2509.23381)
Append: [Train Once, Answer All: Many Pretraining Experiments for the Cost of One](https://arxiv.org/abs/2509.23383)
Append: [No Loss, No Gain: Gated Refinement and Adaptive Compression for Prompt Optimization](https://arxiv.org/abs/2509.23387)
Append: [Liaozhai through the Looking-Glass: On Paratextual Explicitation of Culture-Bound Terms in Machine Translation](https://arxiv.org/abs/2509.23395)
Append: [Comparison of Scoring Rationales Between Large Language Models and Human Raters](https://arxiv.org/abs/2509.23412)
Append: [Retrieval-Constrained Decoding Reveals Underestimated Parametric Knowledge in Language Models](https://arxiv.org/abs/2509.23417)
Append: [Cognition-of-Thought Elicits Social-Aligned Reasoning in Large Language Models](https://arxiv.org/abs/2509.23441)
Append: [Text-Based Approaches to Item Difficulty Modeling in Large-Scale Assessments: A Systematic Review](https://arxiv.org/abs/2509.23486)
Append: [The Impact of Role Design in In-Context Learning for Large Language Models](https://arxiv.org/abs/2509.23501)
Append: [AraS2P: Arabic Speech-to-Phonemes System](https://arxiv.org/abs/2509.23504)
Append: [From Human Annotation to Automation: LLM-in-the-Loop Active Learning for Arabic Sentiment Analysis](https://arxiv.org/abs/2509.23515)
Append: [On the Shelf Life of Fine-Tuned LLM Judges: Future Proofing, Backward Compatibility, and Question Generalization](https://arxiv.org/abs/2509.23542)
Append: [Automatic Speech Recognition for Greek Medical Dictation](https://arxiv.org/abs/2509.23550)
Append: [Towards Efficient CoT Distillation: Self-Guided Rationale Selector for Better Performance with Fewer Rationales](https://arxiv.org/abs/2509.23574)
Append: [Jackal: A Real-World Execution-Based Benchmark Evaluating Large Language Models on Text-to-JQL Tasks](https://arxiv.org/abs/2509.23579)
Append: [LLM Hallucination Detection: HSAD](https://arxiv.org/abs/2509.23580)
Append: [Timber: Training-free Instruct Model Refining with Base via Effective Rank](https://arxiv.org/abs/2509.23595)
Append: [Fast Thinking for Large Language Models](https://arxiv.org/abs/2509.23633)
Append: [Don't Settle Too Early: Self-Reflective Remasking for Diffusion Language Models](https://arxiv.org/abs/2509.23653)
Append: [Beyond English-Centric Training: How Reinforcement Learning Improves Cross-Lingual Reasoning in LLMs](https://arxiv.org/abs/2509.23657)
Append: [Aligning LLMs for Multilingual Consistency in Enterprise Applications](https://arxiv.org/abs/2509.23659)
Append: [TF-Bench: Evaluating Program Semantics Reasoning with Type Inference in System F](https://arxiv.org/abs/2509.23686)
Append: [VIVA+: Human-Centered Situational Decision-Making](https://arxiv.org/abs/2509.23698)
Append: [Collaboration of Fusion and Independence: Hypercomplex-driven Robust Multi-Modal Knowledge Graph Completion](https://arxiv.org/abs/2509.23714)
Append: [Do LLMs Understand Romanian Driving Laws? A Study on Multimodal and Fine-Tuned Question Answering](https://arxiv.org/abs/2509.23715)
Append: [Compose and Fuse: Revisiting the Foundational Bottlenecks in Multimodal Reasoning](https://arxiv.org/abs/2509.23744)
Append: [Understanding Textual Capability Degradation in Speech LLMs via Parameter Importance Analysis](https://arxiv.org/abs/2509.23755)
Append: [Knowledge-Level Consistency Reinforcement Learning: Dual-Fact Alignment for Long-Form Factuality](https://arxiv.org/abs/2509.23765)
Append: [From Personal to Collective: On the Role of Local and Global Memory in LLM Personalization](https://arxiv.org/abs/2509.23767)
Append: [Bridging the Knowledge-Prediction Gap in LLMs on Multiple-Choice Questions](https://arxiv.org/abs/2509.23782)
Append: [Transformer Tafsir at QIAS 2025 Shared Task: Hybrid Retrieval-Augmented Generation for Islamic Knowledge Question Answering](https://arxiv.org/abs/2509.23793)
Append: [Open-DeBias: Toward Mitigating Open-Set Bias in Language Models](https://arxiv.org/abs/2509.23805)
Append: [SPELL: Self-Play Reinforcement Learning for evolving Long-Context Language Models](https://arxiv.org/abs/2509.23863)
Append: [Winning the Pruning Gamble: A Unified Approach to Joint Sample and Token Pruning for Efficient Supervised Fine-Tuning](https://arxiv.org/abs/2509.23873)
Append: [DocPruner: A Storage-Efficient Framework for Multi-Vector Visual Document Retrieval via Adaptive Patch-Level Embedding Pruning](https://arxiv.org/abs/2509.23883)
Append: [Taming Masked Diffusion Language Models via Consistency Trajectory Reinforcement Learning with Fewer Decoding Step](https://arxiv.org/abs/2509.23924)
Append: [Assessing Large Language Models in Updating Their Forecasts with New Information](https://arxiv.org/abs/2509.23936)
Append: [Easy Turn: Integrating Acoustic and Linguistic Modalities for Robust Turn-Taking in Full-Duplex Spoken Dialogue Systems](https://arxiv.org/abs/2509.23938)
Append: [Vision-Grounded Machine Interpreting: Improving the Translation Process through Visual Cues](https://arxiv.org/abs/2509.23957)
Append: [HiPO: Hybrid Policy Optimization for Dynamic Reasoning in LLMs](https://arxiv.org/abs/2509.23967)
Append: [ByteSized32Refactored: Towards an Extensible Interactive Text Games Corpus for LLM World Modeling and Evaluation](https://arxiv.org/abs/2509.23979)
Append: [Toward Preference-aligned Large Language Models via Residual-based Model Steering](https://arxiv.org/abs/2509.23982)
Append: [The Hidden Costs of Translation Accuracy: Distillation, Quantization, and Environmental Impact](https://arxiv.org/abs/2509.23990)
Append: [The AI Agent Code of Conduct: Automated Guardrail Policy-as-Prompt Synthesis](https://arxiv.org/abs/2509.23994)
Append: [MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP Use](https://arxiv.org/abs/2509.24002)
Append: [Sequential Diffusion Language Models](https://arxiv.org/abs/2509.24007)
Append: [SparseD: Sparse Attention for Diffusion Language Models](https://arxiv.org/abs/2509.24014)
Append: [ResFormer: All-Time Reservoir Memory for Long Sequence Classification](https://arxiv.org/abs/2509.24074)
Append: [Ensembling Multilingual Transformers for Robust Sentiment Analysis of Tweets](https://arxiv.org/abs/2509.24080)
Append: [Large-Scale Constraint Generation - Can LLMs Parse Hundreds of Constraints?](https://arxiv.org/abs/2509.24090)
Append: [GEAR: A General Evaluation Framework for Abductive Reasoning](https://arxiv.org/abs/2509.24096)
Append: [BTC-SAM: Leveraging LLMs for Generation of Bias Test Cases for Sentiment Analysis Models](https://arxiv.org/abs/2509.24101)
Append: [Pragmatic Inference for Moral Reasoning Acquisition: Generalization via Distributional Semantics](https://arxiv.org/abs/2509.24102)
Append: [Dual-Scale World Models for LLM Agents Towards Hard-Exploration Problems](https://arxiv.org/abs/2509.24116)
Append: [EduVidQA: Generating and Evaluating Long-form Answers to Student Questions based on Lecture Videos](https://arxiv.org/abs/2509.24120)
Append: [Beyond Magic Words: Sharpness-Aware Prompt Evolving for Robust Large Language Models with TARE](https://arxiv.org/abs/2509.24130)
Append: [Your thoughts tell who you are: Characterize the reasoning patterns of LRMs](https://arxiv.org/abs/2509.24147)
Append: [Localizing Task Recognition and Task Learning in In-Context Learning via Attention Head Analysis](https://arxiv.org/abs/2509.24164)
Append: [Task Vectors, Learned Not Extracted: Performance Gains and Mechanistic Insight](https://arxiv.org/abs/2509.24169)
Append: [Retrieval-augmented GUI Agents with Generative Guidelines](https://arxiv.org/abs/2509.24183)
Append: [Beyond Overall Accuracy: A Psychometric Deep Dive into the Topic-Specific Medical Capabilities of 80 Large Language Models](https://arxiv.org/abs/2509.24186)
Append: [PET: Preference Evolution Tracking with LLM-Generated Explainable Distribution](https://arxiv.org/abs/2509.24189)
Append: [AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced Self-Play](https://arxiv.org/abs/2509.24193)
Append: [Can Large Language Models Express Uncertainty Like Human?](https://arxiv.org/abs/2509.24202)
Append: [BeyondBench: Benchmark-Free Evaluation of Reasoning in Language Models](https://arxiv.org/abs/2509.24210)
Append: [ScenarioBench: Trace-Grounded Compliance Evaluation for Text-to-SQL and RAG](https://arxiv.org/abs/2509.24212)
Append: [MoVa: Towards Generalizable Classification of Human Morals and Values](https://arxiv.org/abs/2509.24216)
Append: [Model Fusion with Multi-LoRA Inference for Tool-Enhanced Game Dialogue Agents](https://arxiv.org/abs/2509.24229)
Append: [Prompt and Parameter Co-Optimization for Large Language Models](https://arxiv.org/abs/2509.24245)
Append: [MRAG-Suite: A Diagnostic Evaluation Platform for Visual Retrieval-Augmented Generation](https://arxiv.org/abs/2509.24253)
Append: [SimuHome: A Temporal- and Environment-Aware Benchmark for Smart Home LLM Agents](https://arxiv.org/abs/2509.24282)
Append: [Let LLMs Speak Embedding Languages: Generative Text Embeddings via Iterative Contrastive Refinement](https://arxiv.org/abs/2509.24291)
Append: [LOGOS: LLM-driven End-to-End Grounded Theory Development and Schema Induction for Qualitative Research](https://arxiv.org/abs/2509.24294)
Append: [DiffuGuard: How Intrinsic Safety is Lost and Found in Diffusion Large Language Models](https://arxiv.org/abs/2509.24296)
Append: [Q-Mirror: Unlocking the Multi-Modal Potential of Scientific Text-Only QA Pairs](https://arxiv.org/abs/2509.24297)
Append: [Dual Mechanisms of Value Expression: Intrinsic vs. Prompted Values in LLMs](https://arxiv.org/abs/2509.24319)
Append: [Multimodal Large Language Models Meet Multimodal Emotion Recognition and Reasoning: A Survey](https://arxiv.org/abs/2509.24322)
Append: [Speculative Verification: Exploiting Information Gain to Refine Speculative Decoding](https://arxiv.org/abs/2509.24328)
Append: [AlignX: Advancing Multilingual Large Language Models with Multilingual Representation Alignment](https://arxiv.org/abs/2509.24338)
Append: [Beyond Repetition: Text Simplification and Curriculum Learning for Data-Constrained Pretraining](https://arxiv.org/abs/2509.24356)
Append: [Reinforcement Mid-Training](https://arxiv.org/abs/2509.24375)
Append: [HarmMetric Eval: Benchmarking Metrics and Judges for LLM Harmfulness Assessment](https://arxiv.org/abs/2509.24384)
Append: [LLaDA-MoE: A Sparse MoE Diffusion Language Model](https://arxiv.org/abs/2509.24389)
Append: [Agentar-Scale-SQL: Advancing Text-to-SQL through Orchestrated Test-Time Scaling](https://arxiv.org/abs/2509.24403)
Append: [Multilingual Text-to-SQL: Benchmarking the Limits of Language Models with Collaborative Language Agents](https://arxiv.org/abs/2509.24405)
Append: [CDT: A Comprehensive Capability Framework for Large Language Models Across Cognition, Domain, and Task](https://arxiv.org/abs/2509.24422)
Append: [Alternatives To Next Token Prediction In Text Generation - A Survey](https://arxiv.org/abs/2509.24435)
Append: [Bias Mitigation or Cultural Commonsense? Evaluating LLMs with a Japanese Dataset](https://arxiv.org/abs/2509.24468)
Append: [A Text-To-Text Alignment Algorithm for Better Evaluation of Modern Speech Recognition Systems](https://arxiv.org/abs/2509.24478)
Append: [Sanitize Your Responses: Mitigating Privacy Leakage in Large Language Models](https://arxiv.org/abs/2509.24488)
Append: [GRPO-MA: Multi-Answer Generation in GRPO for Stable and Efficient Chain-of-Thought Training](https://arxiv.org/abs/2509.24494)
Append: [Knowledge Editing with Subspace-Aware Key-Value Mappings](https://arxiv.org/abs/2509.24502)
Append: [Building Benchmarks from the Ground Up: Community-Centered Evaluation of LLMs in Healthcare Chatbot Settings](https://arxiv.org/abs/2509.24506)
Append: [AdaThink-Med: Medical Adaptive Thinking with Uncertainty-Guided Length Calibration](https://arxiv.org/abs/2509.24560)
Append: [Inducing Dyslexia in Vision Language Models](https://arxiv.org/abs/2509.24597)
Append: [HiKE: Hierarchical Evaluation Framework for Korean-English Code-Switching Speech Recognition](https://arxiv.org/abs/2509.24613)
Append: [Hype or not? Formalizing Automatic Promotional Language Detection in Biomedical Research](https://arxiv.org/abs/2509.24638)
Append: [InfLLM-V2: Dense-Sparse Switchable Attention for Seamless Short-to-Long Adaptation](https://arxiv.org/abs/2509.24663)
Append: [Understanding the Dilemma of Unlearning for Large Language Models](https://arxiv.org/abs/2509.24675)
Append: [Reference-Free Rating of LLM Responses via Latent Information](https://arxiv.org/abs/2509.24678)
Append: [MemGen: Weaving Generative Latent Memory for Self-Evolving Agents](https://arxiv.org/abs/2509.24704)
Append: [Socratic-Zero : Bootstrapping Reasoning via Data-Free Agent Co-evolution](https://arxiv.org/abs/2509.24726)
Append: [ProxyAttn: Guided Sparse Attention via Representative Heads](https://arxiv.org/abs/2509.24745)
Append: [LatentEvolve: Self-Evolving Test-Time Scaling in Latent Space](https://arxiv.org/abs/2509.24771)
Append: [SeaPO: Strategic Error Amplification for Robust Preference Optimization of Large Language Models](https://arxiv.org/abs/2509.24781)
Append: [Evaluating Spatiotemporal Consistency in Automatically Generated Sewing Instructions](https://arxiv.org/abs/2509.24792)
Append: [KnowGuard: Knowledge-Driven Abstention for Multi-Round Clinical Reasoning](https://arxiv.org/abs/2509.24816)
Append: [DiaCDM: Cognitive Diagnosis in Teacher-Student Dialogues using the Initiation-Response-Evaluation Framework](https://arxiv.org/abs/2509.24821)
Append: [SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts via Token-Level LSH Matching](https://arxiv.org/abs/2509.24832)
Append: [Hierarchical Error Correction for Large Language Models: A Systematic Framework for Domain-Specific AI Quality Enhancement](https://arxiv.org/abs/2509.24841)
Append: [Between Help and Harm: An Evaluation of Mental Health Crisis Handling by LLMs](https://arxiv.org/abs/2509.24857)
Append: [Metaphor identification using large language models: A comparison of RAG, prompt engineering, and fine-tuning](https://arxiv.org/abs/2509.24866)
Append: [Expanding Computation Spaces of LLMs at Inference Time](https://arxiv.org/abs/2509.24884)
Append: [BOE-XSUM: Extreme Summarization in Clear Language of Spanish Legal Decrees and Notifications](https://arxiv.org/abs/2509.24908)
Append: [How Well Do LLMs Imitate Human Writing Style?](https://arxiv.org/abs/2509.24930)
Append: [MobileLLM-R1: Exploring the Limits of Sub-Billion Language Model Reasoners with Open Training Recipes](https://arxiv.org/abs/2509.24945)
Append: [The Dialogue That Heals: A Comprehensive Evaluation of Doctor Agents' Inquiry Capability](https://arxiv.org/abs/2509.24958)
Append: [SemanticShield: LLM-Powered Audits Expose Shilling Attacks in Recommender Systems](https://arxiv.org/abs/2509.24961)
Append: [Generalized Correctness Models: Learning Calibrated and Model-Agnostic Correctness Predictors from Historical Patterns](https://arxiv.org/abs/2509.24988)
Append: [Circuit Distillation](https://arxiv.org/abs/2509.25002)
Append: [Ultra-Fast Language Generation via Discrete Diffusion Divergence Instruct](https://arxiv.org/abs/2509.25035)
Append: [GateMABSA: Aspect-Image Gated Fusion for Multimodal Aspect-based Sentiment Analysis](https://arxiv.org/abs/2509.25037)
Append: [Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic Architectures](https://arxiv.org/abs/2509.25045)
Append: [Confidence-Guided Error Correction for Disordered Speech Recognition](https://arxiv.org/abs/2509.25048)
Append: [An empirical study on the limitation of Transformers in program trace generation](https://arxiv.org/abs/2509.25073)
Append: [Scaling Generalist Data-Analytic Agents](https://arxiv.org/abs/2509.25084)
Append: [jina-reranker-v3: Last but Not Late Interaction for Document Reranking](https://arxiv.org/abs/2509.25085)
Append: [Towards Trustworthy Lexical Simplification: Exploring Safety and Efficiency with Small LLMs](https://arxiv.org/abs/2509.25086)
Append: [Towards Personalized Deep Research: Benchmarks and Evaluations](https://arxiv.org/abs/2509.25106)
Append: [Knowledge Extraction on Semi-Structured Content: Does It Remain Relevant for Question Answering in the Era of LLMs?](https://arxiv.org/abs/2509.25107)
Append: [Investigating Language and Retrieval Bias in Multilingual Previously Fact-Checked Claim Detection](https://arxiv.org/abs/2509.25138)
Append: [Paired by the Teacher: Turning Unpaired Data into High-Fidelity Pairs for Low-Resource Text Generation](https://arxiv.org/abs/2509.25144)
Append: [Pretraining Large Language Models with NVFP4](https://arxiv.org/abs/2509.25149)
Append: [EasySteer: A Unified Framework for High-Performance and Extensible LLM Steering](https://arxiv.org/abs/2509.25175)
Append: [NAIPv2: Debiased Pairwise Learning for Efficient Paper Quality Estimation](https://arxiv.org/abs/2509.25179)
Append: [Incentive-Aligned Multi-Source LLM Summaries](https://arxiv.org/abs/2509.25184)
Append: [Learning to Parallel: Accelerating Diffusion Large Language Models via Adaptive Parallel Decoding](https://arxiv.org/abs/2509.25188)
Append: [InfoAgent: Advancing Autonomous Information-Seeking Agents](https://arxiv.org/abs/2509.25189)
Append: [CAOTE: KV Cache Selection for LLMs via Attention Output Error-Based Token Eviction](https://arxiv.org/abs/2504.14051)
Append: [Multiplicative-Additive Constrained Models:Toward Joint Visualization of Interactive and Independent Effects](https://arxiv.org/abs/2509.21923)
Append: [Benefits and Pitfalls of Reinforcement Learning for Language Model Planning: A Theoretical Perspective](https://arxiv.org/abs/2509.22613)
Append: [DiaMoE-TTS: A Unified IPA-Based Dialect TTS Framework with Mixture-of-Experts and Parameter-Efficient Zero-Shot Adaptation](https://arxiv.org/abs/2509.22727)
Append: [VideoScore2: Think before You Score in Generative Video Evaluation](https://arxiv.org/abs/2509.22799)
Append: [Toward a Theory of Generalizability in LLM Mechanistic Interpretability Research](https://arxiv.org/abs/2509.22831)
Append: [Adaptive Margin RLHF via Preference over Preferences](https://arxiv.org/abs/2509.22851)
Append: [Patient-specific Biomolecular Instruction Tuning](https://arxiv.org/abs/2509.22853)
Append: [JE-IRT: A Geometric Lens on LLM Abilities through Joint Embedding Item Response Theory](https://arxiv.org/abs/2509.22888)
Append: [Not only a helper, but also a teacher: Interactive LLM Cascade](https://arxiv.org/abs/2509.22984)
Append: [Geometry-Aware Losses for Structure-Preserving Text-to-Sign Language Generation](https://arxiv.org/abs/2509.23011)
Append: [Tracing the Representation Geometry of Language Models from Pretraining to Post-training](https://arxiv.org/abs/2509.23024)
Append: [Virus Infection Attack on LLMs: Your Poisoning Can Spread "VIA" Synthetic Data](https://arxiv.org/abs/2509.23041)
Append: [Kimi-Dev: Agentless Training as Skill Prior for SWE-Agents](https://arxiv.org/abs/2509.23045)
Append: [Causally-Enhanced Reinforcement Policy Optimization](https://arxiv.org/abs/2509.23095)
Append: [Multiplayer Nash Preference Optimization](https://arxiv.org/abs/2509.23102)
Append: [RHYTHM: Reasoning with Hierarchical Temporal Tokenization for Human Mobility](https://arxiv.org/abs/2509.23115)
Append: [C$^2$GSPG: Confidence-calibrated Group Sequence Policy Gradient towards Self-aware Reasoning](https://arxiv.org/abs/2509.23129)
Append: [SPEC-RL: Accelerating On-Policy Reinforcement Learning via Speculative Rollouts](https://arxiv.org/abs/2509.23232)
Append: [$p$-less Sampling: A Robust Hyperparameter-Free Approach for LLM Decoding](https://arxiv.org/abs/2509.23234)
Append: [Learning How to Use Tools, Not Just When: Pattern-Aware Tool-Integrated Reasoning](https://arxiv.org/abs/2509.23292)
Append: [Seeing Symbols, Missing Cultures: Probing Vision-Language Models' Reasoning on Fire Imagery and Cultural Meaning](https://arxiv.org/abs/2509.23311)
Append: [PARROT: A Benchmark for Evaluating LLMs in Cross-System SQL Translation](https://arxiv.org/abs/2509.23338)
Append: [Your Models Have Thought Enough: Training Large Reasoning Models to Stop Overthinking](https://arxiv.org/abs/2509.23392)
Append: [SPIKE-RL: Video-LLMs meet Bayesian Surprise](https://arxiv.org/abs/2509.23433)
Append: [FoR-SALE: Frame of Reference-guided Spatial Adjustment in LLM-based Diffusion Editing](https://arxiv.org/abs/2509.23452)
Append: [MaskSQL: Safeguarding Privacy for LLM-Based Text-to-SQL via Abstraction](https://arxiv.org/abs/2509.23459)
Append: [Temporal Generalization: A Reality Check](https://arxiv.org/abs/2509.23487)
Append: [Mapping Overlaps in Benchmarks through Perplexity in the Wild](https://arxiv.org/abs/2509.23488)
Append: [Multi-modal Data Spectrum: Multi-modal Datasets are Multi-dimensional](https://arxiv.org/abs/2509.23499)
Append: [Clean First, Align Later: Benchmarking Preference Data Cleaning for Reliable LLM Alignment](https://arxiv.org/abs/2509.23564)
Append: [RIV: Recursive Introspection Mask Diffusion Vision Language Model](https://arxiv.org/abs/2509.23625)
Append: [From Past To Path: Masked History Learning for Next-Item Prediction in Generative Recommendation](https://arxiv.org/abs/2509.23649)
Append: [RCI: A Score for Evaluating Global and Local Reasoning in Multimodal Benchmarks](https://arxiv.org/abs/2509.23673)
Append: [From Reasoning to Answer: Empirical, Attention-Based and Mechanistic Insights into Distilled DeepSeek R1 Models](https://arxiv.org/abs/2509.23676)
Append: [Towards a Comprehensive Scaling Law of Mixture-of-Experts](https://arxiv.org/abs/2509.23678)
Append: [HomeSafeBench: A Benchmark for Embodied Vision-Language Models in Free-Exploration Home Safety Inspection](https://arxiv.org/abs/2509.23690)
Append: [SafeSearch: Automated Red-Teaming for the Safety of LLM-Based Search Agents](https://arxiv.org/abs/2509.23694)
Append: [Beyond Game Theory Optimal: Profit-Maximizing Poker Agents for No-Limit Holdem](https://arxiv.org/abs/2509.23747)
Append: [Anchored Supervised Fine-Tuning](https://arxiv.org/abs/2509.23753)
Append: [From What to Why: A Multi-Agent System for Evidence-based Chemical Reaction Condition Reasoning](https://arxiv.org/abs/2509.23768)
Append: [Knowledge Homophily in Large Language Models](https://arxiv.org/abs/2509.23773)
Append: [Beyond the Exploration-Exploitation Trade-off: A Hidden State Approach for LLM Reasoning in RLVR](https://arxiv.org/abs/2509.23808)
Append: [PCRI: Measuring Context Robustness in Multimodal Models for Enterprise Applications](https://arxiv.org/abs/2509.23879)
Append: [Dynamic Orthogonal Continual Fine-tuning for Mitigating Catastrophic Forgettings](https://arxiv.org/abs/2509.23893)
Append: [Beyond Benchmarks: Understanding Mixture-of-Experts Models through Internal Mechanisms](https://arxiv.org/abs/2509.23933)
Append: [Explore-Execute Chain: Towards an Efficient Structured Reasoning Paradigm](https://arxiv.org/abs/2509.23946)
Append: [Conditional Advantage Estimation for Reinforcement Learning in Large Reasoning Models](https://arxiv.org/abs/2509.23962)
Append: [Detecting and Rectifying Noisy Labels: A Similarity-based Approach](https://arxiv.org/abs/2509.23964)
Append: [The Role of Logic and Automata in Understanding Transformers](https://arxiv.org/abs/2509.24024)
Append: [Do Repetitions Matter? Strengthening Reliability in LLM Evaluations](https://arxiv.org/abs/2509.24086)
Append: [Generalist Scanner Meets Specialist Locator: A Synergistic Coarse-to-Fine Framework for Robust GUI Grounding](https://arxiv.org/abs/2509.24133)
Append: [Reasoning or Retrieval? A Study of Answer Attribution on Large Reasoning Models](https://arxiv.org/abs/2509.24156)
Append: [Group-Relative REINFORCE Is Secretly an Off-Policy Algorithm: Demystifying Some Myths About GRPO and Its Friends](https://arxiv.org/abs/2509.24203)
Append: [Metamorphic Testing for Audio Content Moderation Software](https://arxiv.org/abs/2509.24215)
Append: [Learning to Ponder: Adaptive Reasoning in Latent Space](https://arxiv.org/abs/2509.24238)
Append: [SpecExit: Accelerating Large Reasoning Model via Speculative Exit](https://arxiv.org/abs/2509.24248)
Append: [Latent Visual Reasoning](https://arxiv.org/abs/2509.24251)
Append: [Extracting the Structure of Press Releases for Predicting Earnings Announcement Returns](https://arxiv.org/abs/2509.24254)
Append: [PAME-AI: Patient Messaging Creation and Optimization using Agentic AI](https://arxiv.org/abs/2509.24263)
Append: [AdvChain: Adversarial Chain-of-Thought Tuning for Robust Safety Alignment of Large Reasoning Models](https://arxiv.org/abs/2509.24269)
Append: [Overview of SCIDOCA 2025 Shared Task on Citation Prediction, Discovery, and Placement](https://arxiv.org/abs/2509.24283)
Append: [SCI-Verifier: Scientific Verifier with Thinking](https://arxiv.org/abs/2509.24285)
Append: [Bridging the behavior-neural gap: A multimodal AI reveals the brain's geometry of emotion more accurately than human self-reports](https://arxiv.org/abs/2509.24298)
Append: [MAS$^2$: Self-Generative, Self-Configuring, Self-Rectifying Multi-Agent Systems](https://arxiv.org/abs/2509.24323)
Append: [Towards Safe Reasoning in Large Reasoning Models via Corrective Intervention](https://arxiv.org/abs/2509.24393)
Append: [Beyond Isolated Facts: Synthesizing Narrative and Grounded Supervision for VideoQA](https://arxiv.org/abs/2509.24445)
Append: [Euclid's Gift: Enhancing Spatial Perception and Reasoning in Vision-Language Models via Geometric Surrogate Tasks](https://arxiv.org/abs/2509.24473)
Append: [Experience-guided reflective co-evolution of prompts and heuristics for automatic algorithm design](https://arxiv.org/abs/2509.24509)
Append: [LEAF: A Robust Expert-Based Framework for Few-Shot Continual Event Detection](https://arxiv.org/abs/2509.24547)
Append: [NeMo: Needle in a Montage for Video-Language Understanding](https://arxiv.org/abs/2509.24563)
Append: [OrthAlign: Orthogonal Subspace Decomposition for Non-Interfering Multi-Objective Alignment](https://arxiv.org/abs/2509.24610)
Append: [On the Self-awareness of Large Reasoning Models' Capability Boundaries](https://arxiv.org/abs/2509.24711)
Append: [VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning](https://arxiv.org/abs/2509.24773)
Append: [Pushing LLMs to Their Logical Reasoning Bound: The Role of Data Reasoning Intensity](https://arxiv.org/abs/2509.24836)
Append: [Retro*: Optimizing LLMs for Reasoning-Intensive Document Retrieval](https://arxiv.org/abs/2509.24869)
Append: [MMRQA: Signal-Enhanced Multimodal Large Language Models for MRI Quality Assessment](https://arxiv.org/abs/2509.24888)
Append: [Neural network embeddings recover value dimensions from psychometric survey items on par with human data](https://arxiv.org/abs/2509.24906)
Append: [MASLegalBench: Benchmarking Multi-Agent Systems in Deductive Legal Reasoning](https://arxiv.org/abs/2509.24922)
Append: [When Greedy Wins: Emergent Exploitation Bias in Meta-Bandit LLM Training](https://arxiv.org/abs/2509.24923)
Append: [DiffTester: Accelerating Unit Test Generation for Diffusion LLMs via Repetitive Pattern](https://arxiv.org/abs/2509.24975)
Append: [Learning from Convenience Samples: A Case Study on Fine-Tuning LLMs for Survey Non-response in the German Longitudinal Election Study](https://arxiv.org/abs/2509.25063)
Append: [Scaling with Collapse: Efficient and Predictable Training of LLM Families](https://arxiv.org/abs/2509.25087)
Append: [ORPO-Distill: Mixed-Policy Preference Optimization for Cross-Architecture LLM Distillation](https://arxiv.org/abs/2509.25100)
Append: [From $f(x)$ and $g(x)$ to $f(g(x))$: LLMs Learn New Skills in RL by Composing Old Ones](https://arxiv.org/abs/2509.25123)
Append: [MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech](https://arxiv.org/abs/2509.25131)
Append: [Rethinking Entropy Regularization in Large Reasoning Models](https://arxiv.org/abs/2509.25133)
Append: [The Era of Real-World Human Interaction: RL from User Conversations](https://arxiv.org/abs/2509.25137)
Append: [ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory](https://arxiv.org/abs/2509.25140)
Append: [TemMed-Bench: Evaluating Temporal Medical Image Reasoning in Vision-Language Models](https://arxiv.org/abs/2509.25143)
Append: [GSM8K-V: Can Vision Language Models Solve Grade School Math Word Problems in Visual Contexts](https://arxiv.org/abs/2509.25160)
Append: [SIRI: Scaling Iterative Reinforcement Learning with Interleaved Compression](https://arxiv.org/abs/2509.25176)
Append: [WordAlchemy: A transformer-based Reverse Dictionary](https://arxiv.org/abs/2204.10181)
Append: [Continual Dialogue State Tracking via Example-Guided Question Answering](https://arxiv.org/abs/2305.13721)
Append: [CGELBank Annotation Manual v1.2](https://arxiv.org/abs/2305.17347)
Append: [Machines Do See Color: A Guideline to Classify Different Forms of Racist Discourse in Large Corpora](https://arxiv.org/abs/2401.09333)
Append: [Enhancing Textual Personality Detection toward Social Media: Integrating Long-term and Short-term Perspectives](https://arxiv.org/abs/2404.15067)
Append: [Multi-Head RAG: Solving Multi-Aspect Problems with LLMs](https://arxiv.org/abs/2406.05085)
Append: [SciRIFF: A Resource to Enhance Language Model Instruction-Following over Scientific Literature](https://arxiv.org/abs/2406.07835)
Append: [Sheaf Discovery with Joint Computation Graph Pruning and Flexible Granularity](https://arxiv.org/abs/2407.03779)
Append: [CiteFusion: An Ensemble Framework for Citation Intent Classification Harnessing Dual-Model Binary Couples and SHAP Analyses](https://arxiv.org/abs/2407.13329)
Append: [LLM-3D Print: Large Language Models To Monitor and Control 3D Printing](https://arxiv.org/abs/2408.14307)
Append: [Parse Trees Guided LLM Prompt Compression](https://arxiv.org/abs/2409.15395)
Append: [Cross-lingual Human-Preference Alignment for Neural Machine Translation with Direct Quality Optimization](https://arxiv.org/abs/2409.17673)
Append: [LLMs Are In-Context Bandit Reinforcement Learners](https://arxiv.org/abs/2410.05362)
Append: [AERA Chat: An Interactive Platform for Automated Explainable Student Answer Assessment](https://arxiv.org/abs/2410.09507)
Append: [DM-Codec: Distilling Multimodal Representations for Speech Tokenization](https://arxiv.org/abs/2410.15017)
Append: [When Speculation Spills Secrets: Side Channels via Speculative Decoding In LLMs](https://arxiv.org/abs/2411.01076)
Append: [Adapting Chat Language Models Using Only Target Unlabeled Language Data](https://arxiv.org/abs/2412.11704)
Append: [LongProc: Benchmarking Long-Context Language Models on Long Procedural Generation](https://arxiv.org/abs/2501.05414)
Append: [A Partition Cover Approach to Tokenization](https://arxiv.org/abs/2501.06246)
Append: [A Survey of Graph Retrieval-Augmented Generation for Customized Large Language Models](https://arxiv.org/abs/2501.13958)
Append: [ESGSenticNet: A Neurosymbolic Knowledge Base for Corporate Sustainability Analysis](https://arxiv.org/abs/2501.15720)
Append: [Beyond checkmate: exploring the creative chokepoints in AI text](https://arxiv.org/abs/2501.19301)
Append: [Which Words Matter Most in Zero-Shot Prompts?](https://arxiv.org/abs/2502.03418)
Append: [UltraIF: Advancing Instruction Following from the Wild](https://arxiv.org/abs/2502.04153)
Append: [Group-Adaptive Threshold Optimization for Robust AI-Generated Text Detection](https://arxiv.org/abs/2502.04528)
Append: [Confidence Improves Self-Consistency in LLMs](https://arxiv.org/abs/2502.06233)
Append: [PAFT: Prompt-Agnostic Fine-Tuning](https://arxiv.org/abs/2502.12859)
Append: [B-cos LM: Efficiently Transforming Pre-trained Language Models for Improved Explainability](https://arxiv.org/abs/2502.12992)
Append: [PropXplain: Can LLMs Enable Explainable Propaganda Detection?](https://arxiv.org/abs/2502.16550)
Append: [MemeIntel: Explainable Detection of Propagandistic and Hateful Memes](https://arxiv.org/abs/2502.16612)
Append: [Two Heads Are Better Than One: Dual-Model Verbal Reflection at Inference-Time](https://arxiv.org/abs/2502.19230)
Append: [How to Protect Yourself from 5G Radiation? Investigating LLM Responses to Implicit Misinformation](https://arxiv.org/abs/2503.09598)
Append: [Adaptive Group Policy Optimization: Towards Stable Training and Token-Efficient Reasoning](https://arxiv.org/abs/2503.15952)
Append: [SUV: Scalable Large Language Model Copyright Compliance with Regularized Selective Unlearning](https://arxiv.org/abs/2503.22948)
Append: [XL-Suite: Cross-Lingual Synthetic Training and Evaluation Data for Open-Ended Generation](https://arxiv.org/abs/2503.22973)
Append: [SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV Caching](https://arxiv.org/abs/2504.00970)
Append: [AnesSuite: A Comprehensive Benchmark and Dataset Suite for Anesthesiology Reasoning in LLMs](https://arxiv.org/abs/2504.02404)
Append: [A Practical Synthesis of Detecting AI-Generated Textual, Visual, and Audio Content](https://arxiv.org/abs/2504.02898)
Append: [Are You Getting What You Pay For? Auditing Model Substitution in LLM APIs](https://arxiv.org/abs/2504.04715)
Append: [DataPuzzle: Breaking Free from the Hallucinated Promise of LLMs in Data Analysis](https://arxiv.org/abs/2504.10036)
Append: [Efficient Reasoning Models: A Survey](https://arxiv.org/abs/2504.10903)
Append: [IPBench: Benchmarking the Knowledge of Large Language Models in Intellectual Property](https://arxiv.org/abs/2504.15524)
Append: [Dynamic Early Exit in Reasoning Models](https://arxiv.org/abs/2504.15895)
Append: [TRACE Back from the Future: A Probabilistic Reasoning Approach to Controllable Language Generation](https://arxiv.org/abs/2504.18535)
Append: [Cooking Up Creativity: Enhancing LLM Creativity through Structured Recombination](https://arxiv.org/abs/2504.20643)
Append: [$\textit{New News}$: System-2 Fine-tuning for Robust Integration of New Knowledge](https://arxiv.org/abs/2505.01812)
Append: [References Indeed Matter? Reference-Free Preference Optimization for Conversational Query Reformulation](https://arxiv.org/abs/2505.06552)
Append: [OnPrem.LLM: A Privacy-Conscious Document Intelligence Toolkit](https://arxiv.org/abs/2505.07672)
Append: [VeriFact: Enhancing Long-Form Factuality Evaluation with Refined Fact Extraction and Reference Facts](https://arxiv.org/abs/2505.09701)
Append: [Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL](https://arxiv.org/abs/2505.10832)
Append: [The Counting Power of Transformers](https://arxiv.org/abs/2505.11199)
Append: [Critique-Guided Distillation for Efficient and Robust Language Model Reasoning](https://arxiv.org/abs/2505.11628)
Append: [AdaBoN: Adaptive Best-of-N Alignment](https://arxiv.org/abs/2505.12050)
Append: [MobileIPL: Enhancing Mobile Agents Thinking Process via Iterative Preference Learning](https://arxiv.org/abs/2505.12299)
Append: [Automatically Advancing LLM Expertise in Technology Judgment](https://arxiv.org/abs/2505.12452)
Append: [Is Active Persona Inference Necessary for Aligning Small Models to Personal Preferences?](https://arxiv.org/abs/2505.13257)
Append: [Game-RL: Synthesizing Multimodal Verifiable Game Data to Boost VLMs' General Reasoning](https://arxiv.org/abs/2505.13886)
Append: [Mechanistic Fine-tuning for In-context Learning](https://arxiv.org/abs/2505.14233)
Append: [Internal Chain-of-Thought: Empirical Evidence for Layer-wise Subtask Scheduling in LLMs](https://arxiv.org/abs/2505.14530)
Append: [Language Models Optimized to Fool Detectors Still Have a Distinct Style (And How to Change It)](https://arxiv.org/abs/2505.14608)
Append: [ReflAct: World-Grounded Decision Making in LLM Agents via Goal-State Reflection](https://arxiv.org/abs/2505.15182)
Append: [Multilingual Prompting for Improving LLM Generation Diversity](https://arxiv.org/abs/2505.15229)
Append: [Generalizable Process Reward Models via Formally Verified Training Data](https://arxiv.org/abs/2505.15960)
Append: [Leveraging Online Data to Enhance Medical Knowledge in a Small Persian Language Model](https://arxiv.org/abs/2505.16000)
Append: [Align-GRAG: Reasoning-Guided Dual Alignment for Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2505.16237)
Append: [ToDi: Token-wise Distillation via Fine-Grained Divergence Control](https://arxiv.org/abs/2505.16297)
Append: [Nested Named Entity Recognition as Single-Pass Sequence Labeling](https://arxiv.org/abs/2505.16855)
Append: [A Survey on Stereotype Detection in Natural Language Processing](https://arxiv.org/abs/2505.17642)
Append: [BRIT: Bidirectional Retrieval over Unified Image-Text Graph](https://arxiv.org/abs/2505.18450)
Append: [MetaMind: Modeling Human Social Thoughts with Metacognitive Multi-Agent Systems](https://arxiv.org/abs/2505.18943)
Append: [A Necessary Step toward Faithfulness: Measuring and Improving Consistency in Free-Text Explanations](https://arxiv.org/abs/2505.19299)
Append: [From Single to Multi-Granularity: Toward Long-Term Memory Association and Selection of Conversational Agents](https://arxiv.org/abs/2505.19549)
Append: [TrojanStego: Your Language Model Can Secretly Be A Steganographic Privacy Leaking Agent](https://arxiv.org/abs/2505.20118)
Append: [Long Context Scaling: Divide and Conquer via Multi-Agent Question-driven Collaboration](https://arxiv.org/abs/2505.20625)
Append: [SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long Sequences](https://arxiv.org/abs/2505.20776)
Append: [Evaluating and Steering Modality Preferences in Multimodal Large Language Model](https://arxiv.org/abs/2505.20977)
Append: [Pearl: A Multimodal Culturally-Aware Arabic Instruction Dataset](https://arxiv.org/abs/2505.21979)
Append: [Semi-structured LLM Reasoners Can Be Rigorously Audited](https://arxiv.org/abs/2505.24217)
Append: [Improving Reliability and Explainability of Medical Question Answering through Atomic Fact Checking in Retrieval-Augmented LLMs](https://arxiv.org/abs/2505.24830)
Append: [Hanfu-Bench: A Multimodal Benchmark on Cross-Temporal Cultural Understanding and Transcreation](https://arxiv.org/abs/2506.01565)
Append: [Answer Convergence as a Signal for Early Stopping in Reasoning](https://arxiv.org/abs/2506.02536)
Append: [Beyond Classification: Towards Speech Emotion Reasoning with Multitask AudioLLMs](https://arxiv.org/abs/2506.06820)
Append: [Improving LLM Reasoning through Interpretable Role-Playing Steering](https://arxiv.org/abs/2506.07335)
Append: [What Do Indonesians Really Need from Language Technology? A Nationwide Survey](https://arxiv.org/abs/2506.07506)
Append: [AdversariaL attacK sAfety aLIgnment(ALKALI): Safeguarding LLMs through GRACE: Geometric Representation-Aware Contrastive Enhancement- Introducing Adversarial Vulnerability Quality Index (AVQI)](https://arxiv.org/abs/2506.08885)
Append: [Query-Focused Retrieval Heads Improve Long-Context Reasoning and Re-ranking](https://arxiv.org/abs/2506.09944)
Append: [Curriculum-Guided Layer Scaling for Language Model Pretraining](https://arxiv.org/abs/2506.11389)
Append: [Infini-gram mini: Exact n-gram Search at the Internet Scale with FM-Index](https://arxiv.org/abs/2506.12229)
Append: [BOW: Reinforcement Learning for Bottlenecked Next Word Prediction](https://arxiv.org/abs/2506.13502)
Append: [Long-Context Generalization with Sparse Attention](https://arxiv.org/abs/2506.16640)
Append: [GRAF: Multi-turn Jailbreaking via Global Refinement and Active Fabrication](https://arxiv.org/abs/2506.17881)
Append: [Reasoning Isn't Enough: Examining Truth-Bias and Sycophancy in LLMs](https://arxiv.org/abs/2506.21561)
Append: [Don't Trust Generative Agents to Mimic Communication on Social Networks Unless You Benchmarked their Empirical Realism](https://arxiv.org/abs/2506.21974)
Append: [Semantic-guided Diverse Decoding for Large Language Model](https://arxiv.org/abs/2506.23601)
Append: [Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer](https://arxiv.org/abs/2507.02199)
Append: [PRIME: Large Language Model Personalization with Cognitive Dual-Memory and Personalized Thought Process](https://arxiv.org/abs/2507.04607)
Append: [CoSteer: Collaborative Decoding-Time Personalization via Local Delta Steering](https://arxiv.org/abs/2507.04756)
Append: [ArtifactsBench: Bridging the Visual-Interactive Gap in LLM Code Generation Evaluation](https://arxiv.org/abs/2507.04952)
Append: [Entropy-Memorization Law: Evaluating Memorization Difficulty of Data in LLMs](https://arxiv.org/abs/2507.06056)
Append: [Function Induction and Task Generalization: An Interpretability Study with Off-by-One Addition](https://arxiv.org/abs/2507.09875)
Append: [Making Language Model a Hierarchical Classifier](https://arxiv.org/abs/2507.12930)
Append: [LionGuard 2: Building Lightweight, Data-Efficient & Localised Multilingual Content Moderators](https://arxiv.org/abs/2507.15339)
Append: [The Ever-Evolving Science Exam](https://arxiv.org/abs/2507.16514)
Append: [Diversity-Enhanced Reasoning for Subjective Questions](https://arxiv.org/abs/2507.20187)
Append: [CTTS: Collective Test-Time Scaling](https://arxiv.org/abs/2508.03333)
Append: [Sotopia-RL: Reward Design for Social Intelligence](https://arxiv.org/abs/2508.03905)
Append: [Unveiling Over-Memorization in Finetuning LLMs for Reasoning Tasks](https://arxiv.org/abs/2508.04117)
Append: [Sculptor: Empowering LLMs with Cognitive Agency via Active Context Management](https://arxiv.org/abs/2508.04664)
Append: [Discerning minds or generic tutors? Evaluating instructional guidance capabilities in Socratic LLMs](https://arxiv.org/abs/2508.06583)
Append: [Omni-SafetyBench: A Benchmark for Safety Evaluation of Audio-Visual Large Language Models](https://arxiv.org/abs/2508.07173)
Append: [READER: Retrieval-Assisted Drafter for Efficient LLM Inference](https://arxiv.org/abs/2508.09072)
Append: [PakBBQ: A Culturally Adapted Bias Benchmark for QA](https://arxiv.org/abs/2508.10186)
Append: [CoDiEmb: A Collaborative yet Distinct Framework for Unified Representation Learning in Information Retrieval and Semantic Textual Similarity](https://arxiv.org/abs/2508.11442)
Append: [DESIGNER: Design-Logic-Guided Multidisciplinary Data Synthesis for LLM Reasoning](https://arxiv.org/abs/2508.12726)
Append: [Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR](https://arxiv.org/abs/2508.14029)
Append: [Coarse-to-Fine Personalized LLM Impressions for Streamlined Radiology Reports](https://arxiv.org/abs/2508.15845)
Append: [Jet-Nemotron: Efficient Language Model with Post Neural Architecture Search](https://arxiv.org/abs/2508.15884)
Append: [If We May De-Presuppose: Robustly Verifying Claims through Presupposition-Free Question Decomposition](https://arxiv.org/abs/2508.16838)
Append: [CORE-RAG: Lossless Compression for Retrieval-Augmented LLMs via Reinforcement Learning](https://arxiv.org/abs/2508.19282)
Append: [Automatic Question & Answer Generation Using Generative Large Language Model (LLM)](https://arxiv.org/abs/2508.19475)
Append: [When Thinking Backfires: Mechanistic Insights Into Reasoning-Induced Misalignment](https://arxiv.org/abs/2509.00544)
Append: [CE-Bench: Towards a Reliable Contrastive Evaluation Benchmark of Interpretability of Sparse Autoencoders](https://arxiv.org/abs/2509.00691)
Append: [Do LLMs Adhere to Label Definitions? Examining Their Receptivity to External Label Definitions](https://arxiv.org/abs/2509.02452)
Append: [Artificially Fluent: Swahili AI Performance Benchmarks Between English-Trained and Natively-Trained Datasets](https://arxiv.org/abs/2509.04516)
Append: [COMPACT: Common-token Optimized Model Pruning Across Channels and Tokens](https://arxiv.org/abs/2509.06836)
Append: [On the Same Wavelength? Evaluating Pragmatic Reasoning in Language Models across Broad Concepts](https://arxiv.org/abs/2509.06952)
Append: [Causal Attention with Lookahead Keys](https://arxiv.org/abs/2509.07301)
Append: [Streaming Sequence-to-Sequence Learning with Delayed Streams Modeling](https://arxiv.org/abs/2509.08753)
Append: [Beyond Token Limits: Assessing Language Model Performance on Long Text Classification](https://arxiv.org/abs/2509.10199)
Append: [WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research](https://arxiv.org/abs/2509.13312)
Append: [Do Natural Language Descriptions of Model Activations Convey Privileged Information?](https://arxiv.org/abs/2509.13316)
Append: [Position: Thematic Analysis of Unstructured Clinical Transcripts with Large Language Models](https://arxiv.org/abs/2509.14597)
Append: [ATTS: Asynchronous Test-Time Scaling via Conformal Prediction](https://arxiv.org/abs/2509.15148)
Append: [Fast and Fluent Diffusion Language Models via Convolutional Decoding and Rejective Fine-tuning](https://arxiv.org/abs/2509.15188)
Append: [Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration](https://arxiv.org/abs/2405.14314)
Append: [Position: Towards Bidirectional Human-AI Alignment](https://arxiv.org/abs/2406.09264)
Append: [A Voter-Based Stochastic Rejection-Method Framework for Asymptotically Safe Language Model Outputs](https://arxiv.org/abs/2407.16994)
Append: [NextLocLLM: Location Semantics Modeling and Coordinate-Based Next Location Prediction with LLMs](https://arxiv.org/abs/2410.09129)
Append: [Similarity-Dissimilarity Loss for Multi-label Supervised Contrastive Learning](https://arxiv.org/abs/2410.13439)
Append: [CoT-TL: Low-Resource Temporal Knowledge Representation of Planning Instructions Using Chain-of-Thought Reasoning](https://arxiv.org/abs/2410.16207)
Append: [A Neurosymbolic Fast and Slow Architecture for Graph Coloring](https://arxiv.org/abs/2412.01752)
Append: [Sometimes I am a Tree: Data Drives Unstable Hierarchical Generalization](https://arxiv.org/abs/2412.04619)
Append: [Collaborative Gym: A Framework for Enabling and Evaluating Human-Agent Collaboration](https://arxiv.org/abs/2412.15701)
Append: [Reversed in Time: A Novel Temporal-Emphasized Benchmark for Cross-Modal Video-Text Retrieval](https://arxiv.org/abs/2412.19178)
Append: [Mind the Value-Action Gap: Do LLMs Act in Alignment with Their Values?](https://arxiv.org/abs/2501.15463)
Append: [vCache: Verified Semantic Prompt Caching](https://arxiv.org/abs/2502.03771)
Append: [SURGE: On the Potential of Large Language Models as General-Purpose Surrogate Code Executors](https://arxiv.org/abs/2502.11167)
Append: [Mitigating Barren Plateaus in Quantum Neural Networks via an AI-Driven Submartingale-Based Framework](https://arxiv.org/abs/2502.13166)
Append: [Reasoning to Learn from Latent Thoughts](https://arxiv.org/abs/2503.18866)
Append: [MaintainCoder: Maintainable Code Generation Under Dynamic Requirements](https://arxiv.org/abs/2503.24260)
Append: [Do Larger Language Models Generalize Better? A Scaling Law for Implicit Reasoning at Pretraining Time](https://arxiv.org/abs/2504.03635)
Append: [Visual Planning: Let's Think Only with Images](https://arxiv.org/abs/2505.11409)
Append: [Signal in the Noise: Polysemantic Interference Transfers and Predicts Cross-Model Influence](https://arxiv.org/abs/2505.11611)
Append: [AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving](https://arxiv.org/abs/2505.15298)
Append: [Better Safe Than Sorry? Overreaction Problem of Vision Language Models in Visual Emergency Recognition](https://arxiv.org/abs/2505.15367)
Append: [OViP: Online Vision-Language Preference Learning for VLM Hallucination](https://arxiv.org/abs/2505.15963)
Append: [AdaSTaR: Adaptive Data Sampling for Training Self-Taught Reasoners](https://arxiv.org/abs/2505.16322)
Append: [DEL-ToM: Inference-Time Scaling for Theory-of-Mind Reasoning via Dynamic Epistemic Logic](https://arxiv.org/abs/2505.17348)
Append: [InfoDet: A Dataset for Infographic Element Detection](https://arxiv.org/abs/2505.17473)
Append: [On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning](https://arxiv.org/abs/2505.17508)
Append: [Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement Fine-Tuning of Large Language Models](https://arxiv.org/abs/2505.17826)
Append: [Reward Model Overoptimisation in Iterated RLHF](https://arxiv.org/abs/2505.18126)
Append: [TabularGSM: Understanding the Limitations of LLMs in Tabular Math Reasoning](https://arxiv.org/abs/2505.19563)
Append: [HS-STaR: Hierarchical Sampling for Self-Taught Reasoners via Difficulty Estimation and Budget Reallocation](https://arxiv.org/abs/2505.19866)
Append: [Cross-modal RAG: Sub-dimensional Text-to-Image Retrieval-Augmented Generation](https://arxiv.org/abs/2505.21956)
Append: [ProxyThinker: Test-Time Guidance through Small Visual Reasoners](https://arxiv.org/abs/2505.24872)
Append: [Comba: Improving Bilinear RNNs with Closed-loop Control](https://arxiv.org/abs/2506.02475)
Append: [VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code Generation](https://arxiv.org/abs/2506.03930)
Append: [InstructPro: Natural Language Guided Ligand-Binding Protein Design](https://arxiv.org/abs/2506.09332)
Append: [One Patient, Many Contexts: Scaling Medical AI with Contextual Intelligence](https://arxiv.org/abs/2506.10157)
Append: [Beyond Jailbreaking: Auditing Contextual Privacy in LLM Agents](https://arxiv.org/abs/2506.10171)
Append: [Discrete Audio Tokens: More Than a Survey!](https://arxiv.org/abs/2506.10274)
Append: [Enhancing Delta Compression in LLMs via SVD-based Quantization Error Minimization](https://arxiv.org/abs/2506.11087)
Append: [OmniGen2: Exploration to Advanced Multimodal Generation](https://arxiv.org/abs/2506.18871)
Append: [Overcoming Long-Context Limitations of State-Space Models via Context-Dependent Sparse Attention](https://arxiv.org/abs/2507.00449)
Append: [One Token to Fool LLM-as-a-Judge](https://arxiv.org/abs/2507.08794)
Append: [MetaLint: Generalizable Idiomatic Code Quality Analysis through Instruction-Following and Easy-to-Hard Generalization](https://arxiv.org/abs/2507.11687)
Append: [Probabilistic Soundness Guarantees in LLM Reasoning Chains](https://arxiv.org/abs/2507.12948)
Append: [A Markov Categorical Framework for Language Modeling](https://arxiv.org/abs/2507.19247)
Append: [Can Language Models Discover Scaling Laws?](https://arxiv.org/abs/2507.21184)
Append: [CADDesigner: Conceptual Design of CAD Models Based on General-Purpose Agent](https://arxiv.org/abs/2508.01031)
Append: [Trainable Dynamic Mask Sparse Attention](https://arxiv.org/abs/2508.02124)
Append: [AttriLens-Mol: Attribute Guided Reinforcement Learning for Molecular Property Prediction with Large Language Models](https://arxiv.org/abs/2508.04748)
Append: [Attention Layers Add Into Low-Dimensional Residual Subspaces](https://arxiv.org/abs/2508.16929)
Append: [Evaluating the Effectiveness of Transformer Layers in Wav2Vec 2.0, XLS-R, and Whisper for Speaker Identification Tasks](https://arxiv.org/abs/2509.00230)
Append: [Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning](https://arxiv.org/abs/2509.03646)
Append: [Hierarchical Task Environments as the Next Frontier for Embodied World Models in Robot Soccer](https://arxiv.org/abs/2509.04731)
Append: [FuseCodec: Semantic-Contextual Fusion and Supervision for Neural Codecs](https://arxiv.org/abs/2509.11425)
Append: [MindVL: Towards Efficient and Effective Training of Multimodal Large Language Models on Ascend NPUs](https://arxiv.org/abs/2509.11662)
Append: [SynParaSpeech: Automated Synthesis of Paralinguistic Datasets for Speech Generation and Understanding](https://arxiv.org/abs/2509.14946)
Append: [TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference](https://arxiv.org/abs/2509.15110)
Append: [ViSpec: Accelerating Vision-Language Models with Vision-Aware Speculative Decoding](https://arxiv.org/abs/2509.15235)
append_entries: 478
Finish: 2025-09-30 04:24:58.495337
------------------------------------------------------
Started: 2025-09-30 06:27:16.967751
Existing_entries: 1478
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1487
Summarized using GPT-3.5-turbo
Append: [LiTEx: A Linguistic Taxonomy of Explanations for Understanding Within-Label Variation in Natural Language Inference](https://arxiv.org/abs/2505.22848)
Token length: 1347
Summarized using GPT-3.5-turbo
Append: [K-DeCore: Facilitating Knowledge Transfer in Continual Structured Knowledge Reasoning via Knowledge Decoupling](https://arxiv.org/abs/2509.16929)
Token length: 1172
Summarized using GPT-3.5-turbo
Append: [Everyday Physics in Korean Contexts: A Culturally Grounded Physical Reasoning Benchmark](https://arxiv.org/abs/2509.17807)
Token length: 1167
Summarized using GPT-3.5-turbo
Append: [Prior-based Noisy Text Data Filtering: Fast and Strong Alternative For Perplexity](https://arxiv.org/abs/2509.18577)
Token length: 1364
Summarized using GPT-3.5-turbo
Append: [Diversity Boosts AI-Generated Text Detection](https://arxiv.org/abs/2509.18880)
Token length: 1839
Summarized using GPT-3.5-turbo
Append: [Agentic Reinforcement Learning with Implicit Step Rewards](https://arxiv.org/abs/2509.19199)
Token length: 1223
Summarized using GPT-3.5-turbo
Append: [EmbeddingGemma: Powerful and Lightweight Text Representations](https://arxiv.org/abs/2509.20354)
Token length: 1492
Summarized using GPT-3.5-turbo
Append: [Stepwise Guided Policy Optimization: Coloring your Incorrect Reasoning in GRPO](https://arxiv.org/abs/2505.11595)
Token length: 1259
Summarized using GPT-3.5-turbo
Append: [Patterns in the Transition From Founder-Leadership to Community Governance of Open Source](https://arxiv.org/abs/2509.16295)
Token length: 1802
Summarized using GPT-3.5-turbo
Append: [SVeritas: Benchmark for Robust Speaker Verification under Diverse Conditions](https://arxiv.org/abs/2509.17091)
Token length: 1479
Summarized using GPT-3.5-turbo
Append: [PiERN: Token-Level Routing for Integrating High-Precision Computation and Reasoning](https://arxiv.org/abs/2509.18169)
Token length: 845
Summarized using GPT-3.5-turbo
Append: [Memory-QA: Answering Recall Questions Based on Multimodal Memories](https://arxiv.org/abs/2509.18436)
append_entries: 12
Finish: 2025-09-30 06:27:43.668479
------------------------------------------------------
Started: 2025-09-30 08:23:14.244624
Existing_entries: 1012
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1031
Summarized using GPT-3.5-turbo
Append: [Responsible AI Technical Report](https://arxiv.org/abs/2509.20057)
Token length: 1619
Summarized using GPT-3.5-turbo
Append: [Embedding Domain Knowledge for Large Language Models via Reinforcement Learning from Augmented Generation](https://arxiv.org/abs/2509.20162)
append_entries: 2
Finish: 2025-09-30 08:23:20.662059
------------------------------------------------------
Started: 2025-09-30 10:17:08.688974
Existing_entries: 1002
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-30 10:17:09.722168
------------------------------------------------------
Started: 2025-09-30 12:35:06.368342
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-30 12:35:07.359672
------------------------------------------------------
Started: 2025-09-30 14:16:48.831162
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-30 14:16:49.813155
------------------------------------------------------
Started: 2025-09-30 16:20:30.690070
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-30 16:20:31.773892
------------------------------------------------------
Started: 2025-09-30 18:21:21.010091
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-30 18:21:22.079060
------------------------------------------------------
Started: 2025-09-30 20:17:31.798683
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-30 20:17:32.752476
------------------------------------------------------
Started: 2025-09-30 22:14:43.466722
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-09-30 22:14:44.421411
------------------------------------------------------
Started: 2025-10-01 01:21:49.329638
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-01 01:21:50.343772
------------------------------------------------------
Started: 2025-10-01 03:08:42.184898
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-01 03:08:43.246374
------------------------------------------------------
Started: 2025-10-01 04:22:54.079325
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1108
Summarized using GPT-3.5-turbo
Append: [Cyclic Ablation: Testing Concept Localization against Functional Regeneration in AI](https://arxiv.org/abs/2509.25220)
Token length: 1017
Summarized using GPT-3.5-turbo
Append: [From Internal Representations to Text Quality: A Geometric Approach to LLM Evaluation](https://arxiv.org/abs/2509.25359)
Token length: 1359
Summarized using GPT-3.5-turbo
Append: [Generative Value Conflicts Reveal LLM Priorities](https://arxiv.org/abs/2509.25369)
Token length: 1757
Summarized using GPT-3.5-turbo
Append: [From Faithfulness to Correctness: Generative Reward Models that Think Critically](https://arxiv.org/abs/2509.25409)
Token length: 804
Summarized using GPT-3.5-turbo
Append: [Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization](https://arxiv.org/abs/2509.25416)
Token length: 1810
Summarized using GPT-3.5-turbo
Append: [SimulRAG: Simulator-based RAG for Grounding LLMs in Long-form Scientific QA](https://arxiv.org/abs/2509.25459)
Token length: 1215
Summarized using GPT-3.5-turbo
Append: [The Rise of AfricaNLP: Contributions, Contributors, and Community Impact (2005-2025)](https://arxiv.org/abs/2509.25477)
Token length: 1387
Summarized using GPT-3.5-turbo
Append: [Not Wrong, But Untrue: LLM Overconfidence in Document-Based Queries](https://arxiv.org/abs/2509.25498)
Token length: 1160
Summarized using GPT-3.5-turbo
Append: [Beyond WER: Probing Whisper's Sub-token Decoder Across Diverse Language Resource Levels](https://arxiv.org/abs/2509.25516)
Token length: 1463
Summarized using GPT-3.5-turbo
Append: [MixtureVitae: Open Web-Scale Pretraining Dataset With High Quality Instruction and Reasoning Data Built from Permissive-First Text Sources](https://arxiv.org/abs/2509.25531)
Token length: 1742
Summarized using GPT-3.5-turbo
Append: [Calibrating Verbalized Confidence with Self-Generated Distractors](https://arxiv.org/abs/2509.25532)
Token length: 851
Summarized using GPT-3.5-turbo
Append: [Self-Rewarding Rubric-Based Reinforcement Learning for Open-Ended Reasoning](https://arxiv.org/abs/2509.25534)
Token length: 1428
Summarized using GPT-3.5-turbo
Append: [Aligning Multilingual Reasoning with Verifiable Semantics from a High-Resource Expert Model](https://arxiv.org/abs/2509.25543)
Token length: 878
Summarized using GPT-3.5-turbo
Append: [Performance and competence intertwined: A computational model of the Null Subject stage in English-speaking children](https://arxiv.org/abs/2509.25545)
Token length: 904
Summarized using GPT-3.5-turbo
Append: [Don't Sweat the Small Stuff: Segment-Level Meta-Evaluation Based on Pairwise Difference Correlation](https://arxiv.org/abs/2509.25546)
Token length: 772
Summarized using GPT-3.5-turbo
Append: [Probing the Limits of Stylistic Alignment in Vision-Language Models](https://arxiv.org/abs/2509.25568)
Token length: 1602
Summarized using GPT-3.5-turbo
Append: [RFG: Test-Time Scaling for Diffusion Large Language Model Reasoning with Reward-Free Guidance](https://arxiv.org/abs/2509.25604)
Token length: 1885
Summarized using GPT-3.5-turbo
Append: [Transformers through the lens of support-preserving maps between measures](https://arxiv.org/abs/2509.25611)
Token length: 1645
Summarized using GPT-3.5-turbo
Append: [The Media Bias Detector: A Framework for Annotating and Analyzing the News at Scale](https://arxiv.org/abs/2509.25649)
Token length: 1207
Summarized using GPT-3.5-turbo
Append: [QFrBLiMP: a Quebec-French Benchmark of Linguistic Minimal Pairs](https://arxiv.org/abs/2509.25664)
Token length: 1490
Summarized using GPT-3.5-turbo
Append: [The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks](https://arxiv.org/abs/2509.25671)
Token length: 1259
Summarized using GPT-3.5-turbo
Append: [Mitigating Biases in Language Models via Bias Unlearning](https://arxiv.org/abs/2509.25673)
Token length: 1300
Summarized using GPT-3.5-turbo
Append: [LD-MoLE: Learnable Dynamic Routing for Mixture of LoRA Experts](https://arxiv.org/abs/2509.25684)
Token length: 1805
Summarized using GPT-3.5-turbo
Append: [Atomic Thinking of LLMs: Decoupling and Exploring Mathematical Reasoning Abilities](https://arxiv.org/abs/2509.25725)
Token length: 988
Summarized using GPT-3.5-turbo
Append: [Controlled Generation for Private Synthetic Text](https://arxiv.org/abs/2509.25729)
Token length: 1212
Summarized using GPT-3.5-turbo
Append: [CATCH: A Novel Data Synthesis Framework for High Therapy Fidelity and Memory-Driven Planning Chain of Thought in AI Counseling](https://arxiv.org/abs/2509.25733)
Token length: 1408
Summarized using GPT-3.5-turbo
Append: [Think Less, Label Better: Multi-Stage Domain-Grounded Synthetic Data Generation for Fine-Tuning Large Language Models in Telecommunications](https://arxiv.org/abs/2509.25736)
Token length: 1171
Summarized using GPT-3.5-turbo
Append: [Detecting Hope Across Languages: Multiclass Classification for Positive Online Discourse](https://arxiv.org/abs/2509.25752)
Token length: 1875
Summarized using GPT-3.5-turbo
Append: [TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning](https://arxiv.org/abs/2509.25760)
Token length: 1609
Summarized using GPT-3.5-turbo
Append: [Assessing Algorithmic Bias in Language-Based Depression Detection: A Comparison of DNN and LLM Approaches](https://arxiv.org/abs/2509.25795)
Token length: 1168
Summarized using GPT-3.5-turbo
Append: [RoBiologyDataChoiceQA: A Romanian Dataset for improving Biology understanding of Large Language Models](https://arxiv.org/abs/2509.25813)
Token length: 807
Summarized using GPT-3.5-turbo
Append: [ReTAG: Retrieval-Enhanced, Topic-Augmented Graph-Based Global Sensemaking](https://arxiv.org/abs/2509.25814)
Token length: 646
Summarized using GPT-3.5-turbo
Append: [Personalized Scientific Figure Caption Generation: An Empirical Study on Author-Specific Writing Style Transfer](https://arxiv.org/abs/2509.25817)
Token length: 1399
Summarized using GPT-3.5-turbo
Append: [Overthinking Reduction with Decoupled Rewards and Curriculum Data Scheduling](https://arxiv.org/abs/2509.25827)
Token length: 1463
Summarized using GPT-3.5-turbo
Append: [Believing without Seeing: Quality Scores for Contextualizing Vision-Language Model Explanations](https://arxiv.org/abs/2509.25844)
Token length: 1339
Summarized using GPT-3.5-turbo
Append: [ReFACT: A Benchmark for Scientific Confabulation Detection with Positional Error Annotations](https://arxiv.org/abs/2509.25868)
Token length: 778
Summarized using GPT-3.5-turbo
Append: [ASR Under Noise: Exploring Robustness for Sundanese and Javanese](https://arxiv.org/abs/2509.25878)
Token length: 1607
Summarized using GPT-3.5-turbo
Append: [RoleConflictBench: A Benchmark of Role Conflict Scenarios for Evaluating LLMs' Contextual Sensitivity](https://arxiv.org/abs/2509.25897)
Token length: 779
Summarized using GPT-3.5-turbo
Append: [PerQ: Efficient Evaluation of Multilingual Text Personalization Quality](https://arxiv.org/abs/2509.25903)
Token length: 1774
Summarized using GPT-3.5-turbo
Append: [Mem-{\alpha}: Learning Memory Construction via Reinforcement Learning](https://arxiv.org/abs/2509.25911)
Token length: 1585
Summarized using GPT-3.5-turbo
Append: [Understanding the Mixture-of-Experts with Nadaraya-Watson Kernel](https://arxiv.org/abs/2509.25913)
Token length: 749
Summarized using GPT-3.5-turbo
Append: [Bringing Emerging Architectures to Sequence Labeling in NLP](https://arxiv.org/abs/2509.25918)
Token length: 729
Summarized using GPT-3.5-turbo
Append: [Reliability Crisis of Reference-free Metrics for Grammatical Error Correction](https://arxiv.org/abs/2509.25961)
Token length: 1048
Summarized using GPT-3.5-turbo
Append: [RAGferee: Building Contextual Reward Models for Retrieval-Augmented Generation](https://arxiv.org/abs/2509.26011)
Token length: 1268
Summarized using GPT-3.5-turbo
Append: [RE$^2$: Improving Chinese Grammatical Error Correction via Retrieving Appropriate Examples with Explanation](https://arxiv.org/abs/2509.26038)
Token length: 1917
Summarized using GPT-3.5-turbo
Append: [Unspoken Hints: Accuracy Without Acknowledgement in LLM Reasoning](https://arxiv.org/abs/2509.26041)
Token length: 1548
Summarized using GPT-3.5-turbo
Append: [RE-Searcher: Robust Agentic Search with Goal-oriented Planning and Self-reflection](https://arxiv.org/abs/2509.26048)
Token length: 966
Summarized using GPT-3.5-turbo
Append: [CEAID: Benchmark of Multilingual Machine-Generated Text Detection Methods for Central European Languages](https://arxiv.org/abs/2509.26051)
Token length: 1741
Summarized using GPT-3.5-turbo
Append: [DyFlow: Dynamic Workflow Framework for Agentic Reasoning](https://arxiv.org/abs/2509.26062)
Token length: 1771
Summarized using GPT-3.5-turbo
Append: [The Silent Judge: Unacknowledged Shortcut Bias in LLM-as-a-Judge](https://arxiv.org/abs/2509.26072)
Append: [Limited Preference Data? Learning Better Reward Model with Latent Space Synthesis](https://arxiv.org/abs/2509.26074)
Append: [IMProofBench: Benchmarking AI on Research-Level Mathematical Proof Generation](https://arxiv.org/abs/2509.26076)
Append: [Reinforced Strategy Optimization for Conversational Recommender Systems via Network-of-Experts](https://arxiv.org/abs/2509.26093)
Append: [End-to-End Aspect-Guided Review Summarization at Scale](https://arxiv.org/abs/2509.26103)
Append: [Vocabulary Customization for Efficient Domain-Specific LLM Deployment](https://arxiv.org/abs/2509.26124)
Append: [The Hunger Game Debate: On the Emergence of Over-Competition in Multi-Agent Systems](https://arxiv.org/abs/2509.26126)
Append: [CliniBench: A Clinical Outcome Prediction Benchmark for Generative and Encoder-Based Language Models](https://arxiv.org/abs/2509.26136)
Append: [MGen: Millions of Naturally Occurring Generics in Context](https://arxiv.org/abs/2509.26160)
Append: [Explaining novel senses using definition generation with open language models](https://arxiv.org/abs/2509.26181)
Append: [VietBinoculars: A Zero-Shot Approach for Detecting Vietnamese LLM-Generated Text](https://arxiv.org/abs/2509.26189)
Append: [Comparative Analysis of Ant Colony Optimization and Google OR-Tools for Solving the Open Capacitated Vehicle Routing Problem in Logistics](https://arxiv.org/abs/2509.26216)
Append: [Type-Less yet Type-Aware Inductive Link Prediction with Pretrained Language Models](https://arxiv.org/abs/2509.26224)
Append: [Finetune Once: Decoupling General & Domain Learning with Dynamic Boosted Annealing](https://arxiv.org/abs/2509.26242)
Append: [Optimizing Speech Language Models for Acoustic Consistency](https://arxiv.org/abs/2509.26276)
Append: [QUARTZ : QA-based Unsupervised Abstractive Refinement for Task-oriented Dialogue Summarization](https://arxiv.org/abs/2509.26302)
Append: [Feedback Forensics: A Toolkit to Measure AI Personality](https://arxiv.org/abs/2509.26305)
Append: [One-Token Rollout: Guiding Supervised Fine-Tuning of LLMs with Policy Gradient](https://arxiv.org/abs/2509.26313)
Append: [Latent Thinking Optimization: Your Latent Reasoning Language Model Secretly Encodes Reward Signals in its Latent Thoughts](https://arxiv.org/abs/2509.26314)
Append: [Fast-dLLM v2: Efficient Block-Diffusion LLM](https://arxiv.org/abs/2509.26328)
Append: [Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement Learning](https://arxiv.org/abs/2509.26383)
Append: [An Annotation Scheme for Factuality and its Application to Parliamentary Proceedings](https://arxiv.org/abs/2509.26406)
Append: [Automatic Fact-checking in English and Telugu](https://arxiv.org/abs/2509.26415)
Append: [Text-Based Approaches to Item Alignment to Content Standards in Large-Scale Reading & Writing Tests](https://arxiv.org/abs/2509.26431)
Append: [Adaptive Planning for Multi-Attribute Controllable Summarization with Monte Carlo Tree Search](https://arxiv.org/abs/2509.26435)
Append: [CreAgentive: An Agent Workflow Driven Multi-Category Creative Generation Engine](https://arxiv.org/abs/2509.26461)
Append: [Regression Language Models for Code](https://arxiv.org/abs/2509.26476)
Append: [dParallel: Learnable Parallel Decoding for dLLMs](https://arxiv.org/abs/2509.26488)
Append: [VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in Real-world Applications](https://arxiv.org/abs/2509.26490)
Append: [BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs](https://arxiv.org/abs/2509.26514)
Append: [Training Matryoshka Mixture-of-Experts for Elastic Inference-Time Expert Utilization](https://arxiv.org/abs/2509.26520)
Append: [OceanGym: A Benchmark Environment for Underwater Embodied Agents](https://arxiv.org/abs/2509.26536)
Append: [The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models](https://arxiv.org/abs/2509.26543)
Append: [Towards Reliable Benchmarking: A Contamination Free, Controllable Evaluation Framework for Multi-step LLM Function Calling](https://arxiv.org/abs/2509.26553)
Append: [Generating Difficult-to-Translate Texts](https://arxiv.org/abs/2509.26592)
Append: [Deconstructing Self-Bias in LLM-generated Translation Benchmarks](https://arxiv.org/abs/2509.26600)
Append: [MENLO: From Preferences to Proficiency - Evaluating and Modeling Native-like Quality Across 47 Languages](https://arxiv.org/abs/2509.26601)
Append: [DeepScientist: Advancing Frontier-Pushing Scientific Findings Progressively](https://arxiv.org/abs/2509.26603)
Append: [Searching for Difficult-to-Translate Test Examples at Scale](https://arxiv.org/abs/2509.26619)
Append: [Scaling Spoken Language Models with Syllabic Speech Tokenization](https://arxiv.org/abs/2509.26634)
Append: [Convergence and Divergence of Language Models under Different Random Seeds](https://arxiv.org/abs/2509.26643)
Append: [Artificial Phantasia: Evidence for Propositional Reasoning-Based Mental Imagery in Large Language Models](https://arxiv.org/abs/2509.23108)
Append: [TimeOmni-1: Incentivizing Complex Reasoning with Time Series in Large Language Models](https://arxiv.org/abs/2509.24803)
Append: [Spectral Logit Sculpting: Adaptive Low-Rank Logit Transformation for Controlled Text Generation](https://arxiv.org/abs/2509.25204)
Append: [A Formal Comparison Between Chain-of-Thought and Latent Thought](https://arxiv.org/abs/2509.25239)
Append: [HAMMER: Hamiltonian Curiosity Augmented Large Language Model Reinforcement](https://arxiv.org/abs/2509.25240)
Append: [Language Model Planning from an Information Theoretic Perspective](https://arxiv.org/abs/2509.25260)
Append: [Dynamic Policy Induction for Adaptive Prompt Optimization: Bridging the Efficiency-Accuracy Gap via Lightweight Reinforcement Learning](https://arxiv.org/abs/2509.25267)
Append: [ActorDB: A Unified Database Model Integrating Single-Writer Actors, Incremental View Maintenance, and Zero-Trust Messaging](https://arxiv.org/abs/2509.25285)
Append: [Flash-Searcher: Fast and Effective Web Agents via DAG-Based Parallel Execution](https://arxiv.org/abs/2509.25301)
Append: [Dive into the Agent Matrix: A Realistic Evaluation of Self-Replication Risk in LLM Agents](https://arxiv.org/abs/2509.25302)
Append: [Spontaneous High-Order Generalization in Neural Theory-of-Mind Networks](https://arxiv.org/abs/2509.25343)
Append: [Predicting Training Re-evaluation Curves Enables Effective Data Curriculums for LLMs](https://arxiv.org/abs/2509.25380)
Append: [Rethinking Parameter Sharing for LLM Fine-Tuning with Multiple LoRAs](https://arxiv.org/abs/2509.25414)
Append: [Adaptive Test-Time Reasoning via Reward-Guided Dual-Phase Search](https://arxiv.org/abs/2509.25420)
Append: [Fingerprinting LLMs via Prompt Injection](https://arxiv.org/abs/2509.25448)
Append: [DeepSearch: Overcome the Bottleneck of Reinforcement Learning with Verifiable Rewards via Monte Carlo Tree Search](https://arxiv.org/abs/2509.25454)
Append: [Toxicity in Online Platforms and AI Systems: A Survey of Needs, Challenges, Mitigations, and Future Directions](https://arxiv.org/abs/2509.25539)
Append: [IRIS: Intrinsic Reward Image Synthesis](https://arxiv.org/abs/2509.25562)
Append: [Skip-It? Theoretical Conditions for Layer Skipping in Vision-Language Models](https://arxiv.org/abs/2509.25584)
Append: [ATLAS: Constraints-Aware Multi-Agent Collaboration for Real-World Travel Planning](https://arxiv.org/abs/2509.25586)
Append: [Building the EHR Foundation Model via Next Event Prediction](https://arxiv.org/abs/2509.25591)
Append: [Causal Autoencoder-like Generation of Feedback Fuzzy Cognitive Maps with an LLM Agent](https://arxiv.org/abs/2509.25593)
Append: [STAC: When Innocent Tools Form Dangerous Chains to Jailbreak LLM Agents](https://arxiv.org/abs/2509.25624)
Append: [Nudging the Boundaries of LLM Reasoning](https://arxiv.org/abs/2509.25666)
Append: [Can VLM Pseudo-Labels Train a Time-Series QA Model That Outperforms the VLM?](https://arxiv.org/abs/2509.25696)
Append: [MuPlon: Multi-Path Causal Optimization for Claim Verification through Controlling Confounding](https://arxiv.org/abs/2509.25715)
Append: [Importance Sampling for Multi-Negative Multimodal Direct Preference Optimization](https://arxiv.org/abs/2509.25717)
Append: [The AI Productivity Index (APEX)](https://arxiv.org/abs/2509.25721)
Append: [Rotation Control Unlearning: Quantifying and Controlling Continuous Unlearning for LLM with The Cognitive Rotation Space](https://arxiv.org/abs/2509.25743)
Append: [FinCap: Topic-Aligned Captions for Short-Form Financial YouTube Videos](https://arxiv.org/abs/2509.25745)
Append: [NePTune: A Neuro-Pythonic Framework for Tunable Compositional Reasoning on Vision-Language](https://arxiv.org/abs/2509.25757)
Append: [V-HUB: A Visual-Centric Humor Understanding Benchmark for Video LLMs](https://arxiv.org/abs/2509.25773)
Append: [Learning to Reason as Action Abstractions with Scalable Mid-Training RL](https://arxiv.org/abs/2509.25810)
Append: [VELA: An LLM-Hybrid-as-a-Judge Approach for Evaluating Long Image Captions](https://arxiv.org/abs/2509.25818)
Append: [Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget Allocation](https://arxiv.org/abs/2509.25849)
Append: [Lita: Light Agent Uncovers the Agentic Coding Capabilities of LLMs](https://arxiv.org/abs/2509.25873)
Append: [A Multimodal LLM Approach for Visual Question Answering on Multiparametric 3D Brain MRI](https://arxiv.org/abs/2509.25889)
Append: [VLM-FO1: Bridging the Gap Between High-Level Reasoning and Fine-Grained Perception in VLMs](https://arxiv.org/abs/2509.25916)
Append: [DeepJSONEval: Benchmarking Complex Nested JSON Data Mining for Large Language Models](https://arxiv.org/abs/2509.25922)
Append: [Boosting Process-Correct CoT Reasoning by Modeling Solvability of Multiple-Choice QA](https://arxiv.org/abs/2509.25941)
Append: [RoRecomp: Enhancing Reasoning Efficiency via Rollout Response Recomposition in Reinforcement Learning](https://arxiv.org/abs/2509.25958)
Append: [CAST: Continuous and Differentiable Semi-Structured Sparsity-Aware Training for Large Language Models](https://arxiv.org/abs/2509.25996)
Append: [FITS: Towards an AI-Driven Fashion Information Tool for Sustainability](https://arxiv.org/abs/2509.26017)
Append: [Scaling Up Temporal Domain Generalization via Temporal Experts Averaging](https://arxiv.org/abs/2509.26045)
Append: [Auto-ARGUE: LLM-Based Report Generation Evaluation](https://arxiv.org/abs/2509.26184)
Append: [Thinking-Free Policy Initialization Makes Distilled Reasoning Models More Effective and Efficient Reasoners](https://arxiv.org/abs/2509.26226)
Append: [ProfVLM: A Lightweight Video-Language Model for Multi-View Proficiency Estimation](https://arxiv.org/abs/2509.26278)
Append: [TAU: A Benchmark for Cultural Sound Understanding Beyond Semantics](https://arxiv.org/abs/2509.26329)
Append: [EditReward: A Human-Aligned Reward Model for Instruction-Guided Image Editing](https://arxiv.org/abs/2509.26346)
Append: [Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents](https://arxiv.org/abs/2509.26354)
Append: [Game-Time: Evaluating Temporal Dynamics in Spoken Language Models](https://arxiv.org/abs/2509.26388)
Append: [SeedPrints: Fingerprints Can Even Tell Which Seed Your Large Language Model Was Trained From](https://arxiv.org/abs/2509.26404)
Append: [Extreme Self-Preference in Language Models](https://arxiv.org/abs/2509.26464)
Append: [Ferret-UI Lite: Lessons from Building Small On-Device GUI Agents](https://arxiv.org/abs/2509.26539)
Append: [Probing the Critical Point (CritPt) of AI Reasoning: a Frontier Physics Research Benchmark](https://arxiv.org/abs/2509.26574)
Append: [Clarification as Supervision: Reinforcement Learning for Vision-Language Interfaces](https://arxiv.org/abs/2509.26594)
Append: [Attention as a Compass: Efficient Exploration for Process-Supervised RL in Reasoning Models](https://arxiv.org/abs/2509.26628)
Append: [Medical Question Summarization with Entity-driven Contrastive Learning](https://arxiv.org/abs/2304.07437)
Append: [Preemptive Detection and Correction of Misaligned Actions in LLM Agents](https://arxiv.org/abs/2407.11843)
Append: [Exploring the Impact of Data Quantity on ASR in Extremely Low-resource Languages](https://arxiv.org/abs/2409.08872)
Append: [BianCang: A Traditional Chinese Medicine Large Language Model](https://arxiv.org/abs/2411.11027)
Append: [Aristotle: Mastering Logical Reasoning with A Logic-Complete Decompose-Search-Resolve Framework](https://arxiv.org/abs/2412.16953)
Append: [Cut the Deadwood Out: Backdoor Purification via Guided Module Substitution](https://arxiv.org/abs/2412.20476)
Append: [Agent-as-Judge for Factual Summarization of Long Narratives](https://arxiv.org/abs/2501.09993)
Append: [Dagger Behind Smile: Fool LLMs with a Happy Ending Story](https://arxiv.org/abs/2501.13115)
Append: [iVISPAR -- An Interactive Visual-Spatial Reasoning Benchmark for VLMs](https://arxiv.org/abs/2502.03214)
Append: [Where Fact Ends and Fairness Begins: Redefining AI Bias Evaluation through Cognitive Biases](https://arxiv.org/abs/2502.05849)
Append: [SAFE-SQL: Self-Augmented In-Context Learning with Fine-grained Example Selection for Text-to-SQL](https://arxiv.org/abs/2502.11438)
Append: [Towards Reasoning Ability of Small Language Models](https://arxiv.org/abs/2502.11569)
Append: [FANformer: Improving Large Language Models Through Effective Periodicity Modeling](https://arxiv.org/abs/2502.21309)
Append: [Answer, Refuse, or Guess? Investigating Risk-Aware Decision Making in Language Models](https://arxiv.org/abs/2503.01332)
Append: [One ruler to measure them all: Benchmarking multilingual long-context language models](https://arxiv.org/abs/2503.01996)
Append: [Safety is Not Only About Refusal: Reasoning-Enhanced Fine-tuning for Interpretable LLM Safety](https://arxiv.org/abs/2503.05021)
Append: [Value Profiles for Encoding Human Variation](https://arxiv.org/abs/2503.15484)
Append: [ThinkEdit: Interpretable Weight Editing to Mitigate Overly Short Thinking in Reasoning Models](https://arxiv.org/abs/2503.22048)
Append: [Adaptive Rectification Sampling for Test-Time Compute Scaling](https://arxiv.org/abs/2504.01317)
Append: [AutoJudge: Judge Decoding Without Manual Annotation](https://arxiv.org/abs/2504.20039)
Append: [Scalable LLM Math Reasoning Acceleration with Low-rank Distillation](https://arxiv.org/abs/2505.07861)
Append: [ELEPHANT: Measuring and understanding social sycophancy in LLMs](https://arxiv.org/abs/2505.13995)
Append: [DEBATE, TRAIN, EVOLVE: Self Evolution of Language Model Reasoning](https://arxiv.org/abs/2505.15734)
Append: [Causal Interventions Reveal Shared Structure Across English Filler-Gap Constructions](https://arxiv.org/abs/2505.16002)
Append: [A quantitative analysis of semantic information in deep representations of text and images](https://arxiv.org/abs/2505.17101)
Append: [A Position Paper on the Automatic Generation of Machine Learning Leaderboards](https://arxiv.org/abs/2505.17465)
Append: [Wolf Hidden in Sheep's Conversations: Toward Harmless Data-Based Backdoor Attacks for Jailbreaking Large Language Models](https://arxiv.org/abs/2505.17601)
Append: [GIM: Improved Interpretability for Large Language Models](https://arxiv.org/abs/2505.17630)
Append: [Frankentext: Stitching random text fragments into long-form narratives](https://arxiv.org/abs/2505.18128)
Append: [v1: Learning to Point Visual Tokens for Multimodal Grounded Reasoning](https://arxiv.org/abs/2505.18842)
Append: [SelfReflect: Can LLMs Communicate Their Internal Answer Distribution?](https://arxiv.org/abs/2505.20295)
Append: [LoLA: Low-Rank Linear Attention With Sparse Caching](https://arxiv.org/abs/2505.23666)
Append: [Static Word Embeddings for Sentence Semantic Representation](https://arxiv.org/abs/2506.04624)
Append: [A Culturally-diverse Multilingual Multimodal Video Benchmark & Model](https://arxiv.org/abs/2506.07032)
Append: [ConfRAG: Confidence-Guided Retrieval-Augmenting Generation](https://arxiv.org/abs/2506.07309)
Append: [Efficient Context Selection for Long-Context QA: No Tuning, No Iteration, Just Adaptive-$k$](https://arxiv.org/abs/2506.08479)
Append: [TableRAG: A Retrieval Augmented Generation Framework for Heterogeneous Document Reasoning](https://arxiv.org/abs/2506.10380)
Append: [When Does Multimodality Lead to Better Time Series Forecasting?](https://arxiv.org/abs/2506.21611)
Append: [SoMi-ToM: Evaluating Multi-Perspective Theory of Mind in Embodied Social Interactions](https://arxiv.org/abs/2506.23046)
Append: [Medical Red Teaming Protocol of Language Models: On the Importance of User Perspectives in Healthcare Settings](https://arxiv.org/abs/2507.07248)
Append: [QuestA: Expanding Reasoning Capacity in LLMs via Question Augmentation](https://arxiv.org/abs/2507.13266)
Append: [Linguistic and Embedding-Based Profiling of Texts generated by Humans and Large Language Models](https://arxiv.org/abs/2507.13614)
Append: [Mind the Gap: A Review of Arabic Post-Training Datasets and Their Limitations](https://arxiv.org/abs/2507.14688)
Append: [The Impact of Language Mixing on Bilingual LLM Reasoning](https://arxiv.org/abs/2507.15849)
Append: [DocHPLT: A Massively Multilingual Document-Level Translation Dataset](https://arxiv.org/abs/2508.13079)
Append: [Speculating LLMs' Chinese Training Data Pollution from Their Tokens](https://arxiv.org/abs/2508.17771)
Append: [Diffusion Language Models Know the Answer Before Decoding](https://arxiv.org/abs/2508.19982)
Append: [BEDTime: A Unified Benchmark for Automatically Describing Time Series](https://arxiv.org/abs/2509.05215)
Append: [Chat-Driven Text Generation and Interaction for Person Retrieval](https://arxiv.org/abs/2509.12662)
Append: [Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference](https://arxiv.org/abs/2405.18628)
Append: [Pretrained Hybrids with MAD Skills](https://arxiv.org/abs/2406.00894)
Append: [Composing Global Solutions to Reasoning Tasks via Algebraic Objects in Neural Nets](https://arxiv.org/abs/2410.01779)
Append: [FAN: Fourier Analysis Networks](https://arxiv.org/abs/2410.02675)
Append: [Watermark under Fire: A Robustness Evaluation of LLM Watermarking](https://arxiv.org/abs/2411.13425)
Append: [LFTR: Learning-Free Token Reduction for Multimodal Large Language Models](https://arxiv.org/abs/2501.17391)
Append: [Should You Use Your Large Language Model to Explore or Exploit?](https://arxiv.org/abs/2502.00225)
Append: [ReSpark: Leveraging Previous Data Reports as References to Generate New Reports with LLMs](https://arxiv.org/abs/2502.02329)
Append: [Recent Advances in Large Langauge Model Benchmarks against Data Contamination: From Static to Dynamic Evaluation](https://arxiv.org/abs/2502.17521)
Append: [Voting or Consensus? Decision-Making in Multi-Agent Debate](https://arxiv.org/abs/2502.19130)
Append: [Using Knowledge Graphs to harvest datasets for efficient CLIP model training](https://arxiv.org/abs/2505.02746)
Append: [Structured Agent Distillation for Large Language Model](https://arxiv.org/abs/2505.13820)
Append: [Seeing Through Deception: Uncovering Misleading Creator Intent in Multimodal News with Vision-Language Models](https://arxiv.org/abs/2505.15489)
Append: [AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large Language Models](https://arxiv.org/abs/2505.16211)
Append: [Value-Guided Search for Efficient Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.17373)
Append: [ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models](https://arxiv.org/abs/2505.21500)
Append: [R1-Code-Interpreter: LLMs Reason with Code via Supervised and Multi-stage Reinforcement Learning](https://arxiv.org/abs/2505.21668)
Append: [Regularizing Learnable Feature Extraction for Automatic Speech Recognition](https://arxiv.org/abs/2506.09804)
Append: [ReLoop: "Seeing Twice and Thinking Backwards" via Closed-loop Training to Mitigate Hallucinations in Multimodal understanding](https://arxiv.org/abs/2507.04943)
Append: [Taming the Tri-Space Tension: ARC-Guided Hallucination Modeling and Control for Text-to-Image Generation](https://arxiv.org/abs/2507.04946)
Append: [Scaling RL to Long Videos](https://arxiv.org/abs/2507.07966)
Append: [A Survey on Code Generation with LLM-based Agents](https://arxiv.org/abs/2508.00083)
Append: [From Source to Target: Leveraging Transfer Learning for Predictive Process Monitoring in Organizations](https://arxiv.org/abs/2508.08061)
Append: [Object Detection with Multimodal Large Vision-Language Models: An In-depth Review](https://arxiv.org/abs/2508.19294)
Append: [VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use](https://arxiv.org/abs/2509.01055)
Append: [FlowRL: Matching Reward Distributions for LLM Reasoning](https://arxiv.org/abs/2509.15207)
append_entries: 222
Finish: 2025-10-01 04:24:32.593094
------------------------------------------------------
Started: 2025-10-01 06:25:38.499569
Existing_entries: 1222
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1978
Summarized using GPT-3.5-turbo
Append: [Reinforcement Fine-Tuning Naturally Mitigates Forgetting in Continual Post-Training](https://arxiv.org/abs/2507.05386)
Token length: 1346
Summarized using GPT-3.5-turbo
Append: [CE-GPPO: Coordinating Entropy via Gradient-Preserving Clipping Policy Optimization in Reinforcement Learning](https://arxiv.org/abs/2509.20712)
append_entries: 2
Finish: 2025-10-01 06:25:42.426636
------------------------------------------------------
Started: 2025-10-01 08:21:58.607649
Existing_entries: 1002
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-01 08:21:59.178165
------------------------------------------------------
Started: 2025-10-01 10:17:18.800316
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-01 10:17:19.384791
------------------------------------------------------
Started: 2025-10-01 12:35:45.033378
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-01 12:35:45.613645
------------------------------------------------------
Started: 2025-10-01 14:16:27.962950
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-01 14:16:28.462419
------------------------------------------------------
Started: 2025-10-01 16:20:38.572075
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-01 16:20:39.093225
------------------------------------------------------
Started: 2025-10-01 18:22:55.996543
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-01 18:22:56.563524
------------------------------------------------------
Started: 2025-10-01 20:18:10.113709
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-01 20:18:10.683614
------------------------------------------------------
Started: 2025-10-01 22:14:34.202023
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-01 22:14:34.690369
------------------------------------------------------
Started: 2025-10-02 01:12:04.804071
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-02 01:12:05.354646
------------------------------------------------------
Started: 2025-10-02 02:54:45.272159
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-02 02:54:45.819539
------------------------------------------------------
Started: 2025-10-02 04:21:48.022751
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1538
Summarized using GPT-3.5-turbo
Append: [Direct Token Optimization: A Self-contained Approach to Large Language Model Unlearning](https://arxiv.org/abs/2510.00125)
Token length: 1222
Summarized using GPT-3.5-turbo
Append: [TAMA: Tool-Augmented Multimodal Agent for Procedural Activity Understanding](https://arxiv.org/abs/2510.00161)
Token length: 1401
Summarized using GPT-3.5-turbo
Append: [DRBench: A Realistic Benchmark for Enterprise Deep Research](https://arxiv.org/abs/2510.00172)
Token length: 965
Summarized using GPT-3.5-turbo
Append: [PrimeX: A Dataset of Worldview, Opinion, and Explanation](https://arxiv.org/abs/2510.00174)
Token length: 1917
Summarized using GPT-3.5-turbo
Append: [Personalized Reasoning: Just-In-Time Personalization and Why LLMs Fail At It](https://arxiv.org/abs/2510.00177)
Token length: 1437
Summarized using GPT-3.5-turbo
Append: [BiasFreeBench: a Benchmark for Mitigating Bias in Large Language Model Responses](https://arxiv.org/abs/2510.00232)
Token length: 1535
Summarized using GPT-3.5-turbo
Append: [TASER: Translation Assessment via Systematic Evaluation and Reasoning](https://arxiv.org/abs/2510.00255)
Token length: 1134
Summarized using GPT-3.5-turbo
Append: [Retrieval-Augmented Generation for Electrocardiogram-Language Models](https://arxiv.org/abs/2510.00261)
Token length: 1198
Summarized using GPT-3.5-turbo
Append: [Judging with Confidence: Calibrating Autoraters to Preference Distributions](https://arxiv.org/abs/2510.00263)
Token length: 1250
Summarized using GPT-3.5-turbo
Append: [Efficient Layer-wise LLM Fine-tuning for Revision Intention Prediction](https://arxiv.org/abs/2510.00268)
Token length: 1281
Summarized using GPT-3.5-turbo
Append: [SafePassage: High-Fidelity Information Extraction with Black Box LLMs](https://arxiv.org/abs/2510.00276)
Token length: 1049
Summarized using GPT-3.5-turbo
Append: [ReEvalMed: Rethinking Medical Report Evaluation by Aligning Metrics with Real-World Clinical Judgment](https://arxiv.org/abs/2510.00280)
Token length: 1239
Summarized using GPT-3.5-turbo
Append: [o-MEGA: Optimized Methods for Explanation Generation and Analysis](https://arxiv.org/abs/2510.00288)
Token length: 1232
Summarized using GPT-3.5-turbo
Append: [CORTEX: Collaborative LLM Agents for High-Stakes Alert Triage](https://arxiv.org/abs/2510.00311)
Token length: 1136
Summarized using GPT-3.5-turbo
Append: [TokMem: Tokenized Procedural Memory for Large Language Models](https://arxiv.org/abs/2510.00444)
Token length: 1579
Summarized using GPT-3.5-turbo
Append: [LongCodeZip: Compress Long Context for Code Language Models](https://arxiv.org/abs/2510.00446)
Token length: 1407
Summarized using GPT-3.5-turbo
Append: [Enhancing Rating Prediction with Off-the-Shelf LLMs Using In-Context User Reviews](https://arxiv.org/abs/2510.00449)
Token length: 1393
Summarized using GPT-3.5-turbo
Append: [Agent Fine-tuning through Distillation for Domain-specific LLMs in Microdomains](https://arxiv.org/abs/2510.00482)
Token length: 1280
Summarized using GPT-3.5-turbo
Append: [Agent-ScanKit: Unraveling Memory and Reasoning of Multimodal Agents via Sensitivity Perturbations](https://arxiv.org/abs/2510.00496)
Token length: 1155
Summarized using GPT-3.5-turbo
Append: [MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance](https://arxiv.org/abs/2510.00499)
Token length: 1958
Summarized using GPT-3.5-turbo
Append: [Graph2Eval: Automatic Multimodal Task Generation for Agents via Knowledge Graphs](https://arxiv.org/abs/2510.00507)
Token length: 1565
Summarized using GPT-3.5-turbo
Append: [Copy-Paste to Mitigate Large Language Model Hallucinations](https://arxiv.org/abs/2510.00508)
Token length: 960
Summarized using GPT-3.5-turbo
Append: [JoyAgent-JDGenie: Technical Report on the GAIA](https://arxiv.org/abs/2510.00510)
Token length: 1153
Summarized using GPT-3.5-turbo
Append: [EuroSpeech: A Multilingual Speech Corpus](https://arxiv.org/abs/2510.00514)
Token length: 1380
Summarized using GPT-3.5-turbo
Append: [Beyond Log Likelihood: Probability-Based Objectives for Supervised Fine-Tuning across the Model Capability Continuum](https://arxiv.org/abs/2510.00526)
Token length: 1875
Summarized using GPT-3.5-turbo
Append: [GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness](https://arxiv.org/abs/2510.00536)
Token length: 1101
Summarized using GPT-3.5-turbo
Append: [ThinkBrake: Mitigating Overthinking in Tool Reasoning](https://arxiv.org/abs/2510.00546)
Token length: 1355
Summarized using GPT-3.5-turbo
Append: [Are Large Language Models Chronically Online Surfers? A Dataset for Chinese Internet Meme Explanation](https://arxiv.org/abs/2510.00567)
Token length: 1499
Summarized using GPT-3.5-turbo
Append: [ReSeek: A Self-Correcting Framework for Search Agents with Instructive Rewards](https://arxiv.org/abs/2510.00568)
Token length: 1417
Summarized using GPT-3.5-turbo
Append: [CoT Vectors: Transferring and Probing the Reasoning Mechanisms of LLMs](https://arxiv.org/abs/2510.00579)
Token length: 975
Summarized using GPT-3.5-turbo
Append: [SAGE-LD: Towards Scalable and Generalizable End-to-End Language Diarization via Simulated Data Augmentation](https://arxiv.org/abs/2510.00582)
Token length: 1355
Summarized using GPT-3.5-turbo
Append: [Tenyidie Syllabification corpus creation and deep learning applications](https://arxiv.org/abs/2510.00629)
Token length: 1572
Summarized using GPT-3.5-turbo
Append: [MCM-DPO: Multifaceted Cross-Modal Direct Preference Optimization for Alt-text Generation](https://arxiv.org/abs/2510.00647)
Token length: 1268
Summarized using GPT-3.5-turbo
Append: [Facilitating Cognitive Accessibility with LLMs: A Multi-Task Approach to Easy-to-Read Text Generation](https://arxiv.org/abs/2510.00662)
Token length: 1119
Summarized using GPT-3.5-turbo
Append: [Inclusive Easy-to-Read Generation for Individuals with Cognitive Impairments](https://arxiv.org/abs/2510.00691)
Token length: 1331
Summarized using GPT-3.5-turbo
Append: [ALARB: An Arabic Legal Argument Reasoning Benchmark](https://arxiv.org/abs/2510.00694)
Token length: 1191
Summarized using GPT-3.5-turbo
Append: [Family Matters: Language Transfer and Merging for Adapting Small LLMs to Faroese](https://arxiv.org/abs/2510.00810)
Token length: 1660
Summarized using GPT-3.5-turbo
Append: [Exposing the Cracks: Vulnerabilities of Retrieval-Augmented LLM-based Machine Translation](https://arxiv.org/abs/2510.00829)
Token length: 1626
Summarized using GPT-3.5-turbo
Append: [ManagerBench: Evaluating the Safety-Pragmatism Trade-off in Autonomous LLMs](https://arxiv.org/abs/2510.00857)
Token length: 1313
Summarized using GPT-3.5-turbo
Append: [Erase to Improve: Erasable Reinforcement Learning for Search-Augmented LLMs](https://arxiv.org/abs/2510.00861)
Token length: 1204
Summarized using GPT-3.5-turbo
Append: [HalluGuard: Evidence-Grounded Small Reasoning Models to Mitigate Hallucinations in Retrieval-Augmented Generation](https://arxiv.org/abs/2510.00880)
Token length: 1696
Summarized using GPT-3.5-turbo
Append: [Span-level Detection of AI-generated Scientific Text via Contrastive Learning and Structural Calibration](https://arxiv.org/abs/2510.00890)
Token length: 1112
Summarized using GPT-3.5-turbo
Append: [Benchmarking Foundation Models with Retrieval-Augmented Generation in Olympic-Level Physics Problem Solving](https://arxiv.org/abs/2510.00919)
Token length: 1625
Summarized using GPT-3.5-turbo
Append: [Making, not Taking, the Best of N](https://arxiv.org/abs/2510.00931)
Token length: 950
Summarized using GPT-3.5-turbo
Append: [Analyzing Dialectical Biases in LLMs for Knowledge and Reasoning Benchmarks](https://arxiv.org/abs/2510.00962)
Token length: 1471
Summarized using GPT-3.5-turbo
Append: [Syntax-Guided Diffusion Language Models with User-Integrated Personalization](https://arxiv.org/abs/2510.01028)
Token length: 1181
Summarized using GPT-3.5-turbo
Append: [Interpreting Language Models Through Concept Descriptions: A Survey](https://arxiv.org/abs/2510.01048)
Token length: 1122
Summarized using GPT-3.5-turbo
Append: [Hybrid Dialogue State Tracking for Persian Chatbots: A Language Model-Based Approach](https://arxiv.org/abs/2510.01052)
Token length: 601
Summarized using GPT-3.5-turbo
Append: [Research on the Integration of Embodied Intelligence and Reinforcement Learning in Textual Domains](https://arxiv.org/abs/2510.01076)
Token length: 1966
Summarized using GPT-3.5-turbo
Append: [Automatic Speech Recognition (ASR) for African Low-Resource Languages: A Systematic Literature Review](https://arxiv.org/abs/2510.01145)
Append: [mR3: Multilingual Rubric-Agnostic Reward Reasoning Models](https://arxiv.org/abs/2510.01146)
Append: [Pay-Per-Search Models are Abstention Models](https://arxiv.org/abs/2510.01152)
Append: [Backdoor Attacks Against Speech Language Models](https://arxiv.org/abs/2510.01157)
Append: [Social Welfare Function Leaderboard: When LLM Agents Allocate Social Welfare](https://arxiv.org/abs/2510.01164)
Append: [GRAD: Generative Retrieval-Aligned Demonstration Sampler for Efficient Few-Shot Reasoning](https://arxiv.org/abs/2510.01165)
Append: [Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM Diversity](https://arxiv.org/abs/2510.01171)
Append: [Energy-Regularized Sequential Model Editing on Hyperspheres](https://arxiv.org/abs/2510.01172)
Append: [Unpacking Musical Symbolism in Online Communities: Content-Based and Network-Centric Approaches](https://arxiv.org/abs/2510.00006)
Append: [IA aplicada al an\'alisis del conflicto Ir\'an-Israel: Mapeo de discursos en YouTube](https://arxiv.org/abs/2510.00021)
Append: [WaveMind: Towards a Conversational EEG Foundation Model Aligned to Textual and Visual Modalities](https://arxiv.org/abs/2510.00032)
Append: [Linear Regression in p-adic metric spaces](https://arxiv.org/abs/2510.00043)
Append: [ARS: Adaptive Reasoning Suppression for Efficient Large Reasoning Language Models](https://arxiv.org/abs/2510.00071)
Append: [Optimizing What Matters: AUC-Driven Learning for Robust Neural Retrieval](https://arxiv.org/abs/2510.00137)
Append: [Thoughtbubbles: an Unsupervised Method for Parallel Thinking in Latent Space](https://arxiv.org/abs/2510.00219)
Append: [QSearchNet: A Quantum Walk Search Framework for Link Prediction](https://arxiv.org/abs/2510.00325)
Append: [Navigating the Synchrony-Stability Frontier in Adaptive Chatbots](https://arxiv.org/abs/2510.00339)
Append: [GDLNN: Marriage of Programming Language and Neural Networks for Accurate and Easy-to-Explain Graph Classification](https://arxiv.org/abs/2510.00374)
Append: [AbsTopK: Rethinking Sparse Autoencoders For Bidirectional Features](https://arxiv.org/abs/2510.00404)
Append: [Automated Evaluation can Distinguish the Good and Bad AI Responses to Patient Questions about Hospitalization](https://arxiv.org/abs/2510.00436)
Append: [Eyes-on-Me: Scalable RAG Poisoning through Transferable Attention-Steering Attractors](https://arxiv.org/abs/2510.00586)
Append: [ACON: Optimizing Context Compression for Long-horizon LLM Agents](https://arxiv.org/abs/2510.00615)
Append: [HARPA: A Testability-Driven, Literature-Grounded Framework for Research Ideation](https://arxiv.org/abs/2510.00620)
Append: [When Silence Matters: The Impact of Irrelevant Audio on Text Reasoning in Large Audio-Language Models](https://arxiv.org/abs/2510.00626)
Append: [Hearing the Order: Investigating Selection Bias in Large Audio-Language Models](https://arxiv.org/abs/2510.00628)
Append: [Expected Attention: KV Cache Compression by Estimating Attention from Future Queries Distribution](https://arxiv.org/abs/2510.00636)
Append: [Milco: Learned Sparse Retrieval Across Languages via a Multilingual Connector](https://arxiv.org/abs/2510.00671)
Append: [Stochastic Self-Organization in Multi-Agent Systems](https://arxiv.org/abs/2510.00685)
Append: [From Scores to Preferences: Redefining MOS Benchmarking for Speech Quality Reward Modeling](https://arxiv.org/abs/2510.00743)
Append: [What You See is What You Ask: Evaluating Audio Descriptions](https://arxiv.org/abs/2510.00808)
Append: [Mechanistic Interpretability as Statistical Estimation: A Variance Analysis of EAP-IG](https://arxiv.org/abs/2510.00845)
Append: [Can World Models Benefit VLMs for World Dynamics?](https://arxiv.org/abs/2510.00855)
Append: [The data-quality illusion: Rethinking Classifier-based quality filtering for LLM Pretraining](https://arxiv.org/abs/2510.00866)
Append: [Bridging Language Gaps: Advances in Cross-Lingual Information Retrieval with Multilingual LLMs](https://arxiv.org/abs/2510.00908)
Append: [It Takes Two: Your GRPO Is Secretly DPO](https://arxiv.org/abs/2510.00977)
Append: [Spiralformer: Low Latency Encoder for Streaming Speech Recognition with Circular Layer Skipping and Early Exiting](https://arxiv.org/abs/2510.00982)
Append: [Improving Code Localization with Repository Memory](https://arxiv.org/abs/2510.01003)
Append: [Shape Happens: Automatic Feature Manifold Discovery in LLMs via Supervised Multi-Dimensional Scaling](https://arxiv.org/abs/2510.01025)
Append: [Authentic Discrete Diffusion Model](https://arxiv.org/abs/2510.01047)
Append: [GEM: A Gym for Agentic LLMs](https://arxiv.org/abs/2510.01051)
Append: [A Practitioner's Guide to Multi-turn Agentic Reinforcement Learning](https://arxiv.org/abs/2510.01132)
Append: [Prompt Curriculum Learning for Efficient LLM Post-Training](https://arxiv.org/abs/2510.01135)
Append: [Simultaneous Multi-objective Alignment Across Verifiable and Non-verifiable Rewards](https://arxiv.org/abs/2510.01167)
Append: [Code2Video: A Code-centric Paradigm for Educational Video Generation](https://arxiv.org/abs/2510.01174)
Append: [TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments](https://arxiv.org/abs/2510.01179)
Append: [BroRL: Scaling Reinforcement Learning via Broadened Exploration](https://arxiv.org/abs/2510.01180)
Append: [PhyloLM : Inferring the Phylogeny of Large Language Models and Predicting their Performances in Benchmarks](https://arxiv.org/abs/2404.04671)
Append: [Language Models can Subtly Deceive Without Lying: A Case Study on Strategic Phrasing in Legislation](https://arxiv.org/abs/2405.04325)
Append: [Second Language (Arabic) Acquisition of LLMs via Progressive Vocabulary Expansion](https://arxiv.org/abs/2412.12310)
Append: [Exploring and Controlling Diversity in LLM-Agent Conversation](https://arxiv.org/abs/2412.21102)
Append: [OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking](https://arxiv.org/abs/2501.09751)
Append: [ATLAS: Autoformalizing Theorems through Lifting, Augmentation, and Synthesis of Data](https://arxiv.org/abs/2502.05567)
Append: [Resolving UnderEdit & OverEdit with Iterative & Neighbor-Assisted Model Editing](https://arxiv.org/abs/2503.11895)
Append: [Self-Evolving Multi-Agent Simulations for Realistic Clinical Interactions](https://arxiv.org/abs/2503.22678)
Append: [Improving Retrieval-Augmented Neural Machine Translation with Monolingual Data](https://arxiv.org/abs/2504.21747)
Append: [Ambiguity in LLMs is a concept missing problem](https://arxiv.org/abs/2505.11679)
Append: [GuRE:Generative Query REwriter for Legal Passage Retrieval](https://arxiv.org/abs/2505.12950)
Append: [Unpacking Let Alone: Human-Scale Models Generalize to a Rare Construction in Form but not Meaning](https://arxiv.org/abs/2506.04408)
Append: [MLLM-CL: Continual Learning for Multimodal Large Language Models](https://arxiv.org/abs/2506.05453)
Append: [Precise Information Control in Long-Form Text Generation](https://arxiv.org/abs/2506.06589)
Append: [Through the Valley: Path to Effective Long CoT Training for Small Language Models](https://arxiv.org/abs/2506.07712)
Append: [REAL: Reading Out Transformer Activations for Precise Localization in Language Model Steering](https://arxiv.org/abs/2506.08359)
Append: [CounselBench: A Large-Scale Expert Evaluation and Adversarial Benchmarking of Large Language Models in Mental Health Question Answering](https://arxiv.org/abs/2506.08584)
Append: [Leaky Thoughts: Large Reasoning Models Are Not Private Thinkers](https://arxiv.org/abs/2506.15674)
Append: [Retain or Reframe? A Computational Framework for the Analysis of Framing in News Articles and Reader Comments](https://arxiv.org/abs/2507.04612)
Append: [Are Knowledge and Reference in Multilingual Language Models Cross-Lingually Consistent?](https://arxiv.org/abs/2507.12838)
Append: [TokenSmith: Streamlining Data Editing, Search, and Inspection for Large-Scale Language Model Training and Interpretability](https://arxiv.org/abs/2507.19419)
Append: [Beyond Content: How Grammatical Gender Shapes Visual Representation in Text-to-Image Models](https://arxiv.org/abs/2508.03199)
Append: [Out of the Box, into the Clinic? Evaluating State-of-the-Art ASR for Clinical Applications for Older Adults](https://arxiv.org/abs/2508.08684)
Append: [CultranAI at PalmX 2025: Data Augmentation for Cultural Knowledge Representation](https://arxiv.org/abs/2508.17324)
Append: [Steering When Necessary: Flexible Steering Large Language Models with Backtracking](https://arxiv.org/abs/2508.17621)
Append: [The Rarity Blind Spot: A Framework for Evaluating Statistical Reasoning in LLMs](https://arxiv.org/abs/2509.00245)
Append: [Computational-Assisted Systematic Review and Meta-Analysis (CASMA): Effect of a Subclass of GnRH-a on Endometriosis Recurrence](https://arxiv.org/abs/2509.16599)
Append: [Integrated Framework for LLM Evaluation with Answer Generation](https://arxiv.org/abs/2509.20097)
Append: [PaECTER: Patent-level Representation Learning using Citation-informed Transformers](https://arxiv.org/abs/2402.19411)
Append: [Phantom: General Backdoor Attacks on Retrieval Augmented Language Generation](https://arxiv.org/abs/2405.20485)
Append: [Whose Journey Matters? Investigating Identity Biases in Large Language Models (LLMs) for Travel Planning Assistance](https://arxiv.org/abs/2410.17333)
Append: [Krony-PT: GPT2 compressed with Kronecker Products](https://arxiv.org/abs/2412.12351)
Append: [TDBench: A Benchmark for Top-Down Image Understanding with Reliability Analysis of Vision-Language Models](https://arxiv.org/abs/2504.03748)
Append: [GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents](https://arxiv.org/abs/2504.10458)
Append: [Not All Rollouts are Useful: Down-Sampling Rollouts in LLM Reinforcement Learning](https://arxiv.org/abs/2504.13818)
Append: [EVALOOOP: A Self-Consistency-Centered Framework for Assessing Large Language Model Robustness in Programming](https://arxiv.org/abs/2505.12185)
Append: [Learning to Rank Chain-of-Thought: Using a Small Model](https://arxiv.org/abs/2505.14999)
Append: [Towards Holistic Evaluation of Large Audio-Language Models: A Comprehensive Survey](https://arxiv.org/abs/2505.15957)
Append: [MMGeoLM: Hard Negative Contrastive Learning for Fine-Grained Geometric Understanding in Large Multimodal Models](https://arxiv.org/abs/2505.20152)
Append: [AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in LLM-Based Agents](https://arxiv.org/abs/2506.04018)
Append: [LoRA Users Beware: A Few Spurious Tokens Can Manipulate Your Finetuned Model](https://arxiv.org/abs/2506.11402)
Append: [ConciseHint: Boosting Efficient Reasoning via Continuous Concise Hints during Generation](https://arxiv.org/abs/2506.18810)
Append: [LoRA meets Riemannion: Muon Optimizer for Parametrization-independent Low-Rank Adapters](https://arxiv.org/abs/2507.12142)
Append: [Scaling Linear Attention with Sparse State Expansion](https://arxiv.org/abs/2507.16577)
Append: [Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents](https://arxiv.org/abs/2508.07642)
Append: [Mitigating Multimodal Hallucinations via Gradient-based Self-Reflection](https://arxiv.org/abs/2509.03113)
Append: [Imagining Alternatives: Towards High-Resolution 3D Counterfactual Medical Image Generation via Language Guidance](https://arxiv.org/abs/2509.05978)
Append: [An Ethically Grounded LLM-Based Approach to Insider Threat Synthesis and Detection](https://arxiv.org/abs/2509.06920)
Append: [Exact Coset Sampling for Quantum Lattice Algorithms](https://arxiv.org/abs/2509.12341)
Append: [Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation](https://arxiv.org/abs/2509.15194)
Append: [The Illusion of Readiness: Stress Testing Large Frontier Models on Multimodal Medical Benchmarks](https://arxiv.org/abs/2509.18234)
Append: [Interactive Recommendation Agent with Active User Commands](https://arxiv.org/abs/2509.21317)
append_entries: 147
Finish: 2025-10-02 04:23:30.751829
------------------------------------------------------
Started: 2025-10-02 06:24:41.926136
Existing_entries: 1147
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1673
Summarized using GPT-3.5-turbo
Append: [Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs](https://arxiv.org/abs/2509.22646)
append_entries: 1
Finish: 2025-10-02 06:24:44.415219
------------------------------------------------------
Started: 2025-10-02 08:20:23.289633
Existing_entries: 1001
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-02 08:20:23.754199
------------------------------------------------------
Started: 2025-10-02 10:15:59.709885
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-02 10:16:00.099574
------------------------------------------------------
Started: 2025-10-02 12:32:14.159048
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-02 12:32:14.620979
------------------------------------------------------
Started: 2025-10-02 14:15:55.925641
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-02 14:15:56.366412
------------------------------------------------------
Started: 2025-10-02 16:19:53.587910
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-02 16:19:53.962711
------------------------------------------------------
Started: 2025-10-02 18:22:38.050522
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-02 18:22:38.462250
------------------------------------------------------
Started: 2025-10-02 20:17:22.392811
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-02 20:17:22.872894
------------------------------------------------------
Started: 2025-10-02 22:14:28.475058
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-02 22:14:28.836104
------------------------------------------------------
Started: 2025-10-03 01:11:55.423328
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-03 01:11:55.789794
------------------------------------------------------
Started: 2025-10-03 02:54:35.386788
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-03 02:54:35.811998
------------------------------------------------------
Started: 2025-10-03 04:20:15.293275
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 524
Summarized using GPT-3.5-turbo
Append: [Uncovering Implicit Bias in Large Language Models with Concept Learning Dataset](https://arxiv.org/abs/2510.01219)
Token length: 1702
Summarized using GPT-3.5-turbo
Append: [Towards Open-Ended Discovery for Low-Resource NLP](https://arxiv.org/abs/2510.01220)
Token length: 1233
Summarized using GPT-3.5-turbo
Append: [Discourse vs emissions: Analysis of corporate narratives, symbolic practices, and mimicry through LLMs](https://arxiv.org/abs/2510.01222)
Token length: 1334
Summarized using GPT-3.5-turbo
Append: [Context Matters: Comparison of commercial large language tools in veterinary medicine](https://arxiv.org/abs/2510.01224)
Token length: 1152
Summarized using GPT-3.5-turbo
Append: [ClaimCheck: Real-Time Fact-Checking with Small Language Models](https://arxiv.org/abs/2510.01226)
Token length: 1366
Summarized using GPT-3.5-turbo
Append: [EEFSUVA: A New Mathematical Olympiad Benchmark](https://arxiv.org/abs/2510.01227)
Token length: 910
Summarized using GPT-3.5-turbo
Append: [Who is In Charge? Dissecting Role Conflicts in Instruction Following](https://arxiv.org/abs/2510.01228)
Token length: 1198
Summarized using GPT-3.5-turbo
Append: [Enhancing Transformer-Based Rerankers with Synthetic Data and LLM-Based Supervision](https://arxiv.org/abs/2510.01229)
Token length: 890
Summarized using GPT-3.5-turbo
Append: [Geometric Structures and Patterns of Meaning: A PHATE Manifold Analysis of Chinese Character Embeddings](https://arxiv.org/abs/2510.01230)
Token length: 1431
Summarized using GPT-3.5-turbo
Append: [Trustworthy Summarization via Uncertainty Quantification and Risk Awareness in Large Language Models](https://arxiv.org/abs/2510.01231)
Token length: 1439
Summarized using GPT-3.5-turbo
Append: [Benchmark Profiling: Mechanistic Diagnosis of LLM Benchmarks](https://arxiv.org/abs/2510.01232)
Token length: 1288
Summarized using GPT-3.5-turbo
Append: [Computational Social Linguistics for Telugu Cultural Preservation: Novel Algorithms for Chandassu Metrical Pattern Recognition](https://arxiv.org/abs/2510.01233)
Token length: 1199
Summarized using GPT-3.5-turbo
Append: [LLMRank: Understanding LLM Strengths for Model Routing](https://arxiv.org/abs/2510.01234)
Token length: 1353
Summarized using GPT-3.5-turbo
Append: [GRPO++: Enhancing Dermatological Reasoning under Low Resource Settings](https://arxiv.org/abs/2510.01236)
Token length: 1348
Summarized using GPT-3.5-turbo
Append: [Confidence-Aware Routing for Large Language Model Reliability Enhancement: A Multi-Signal Approach to Pre-Generation Hallucination Mitigation](https://arxiv.org/abs/2510.01237)
Token length: 843
Summarized using GPT-3.5-turbo
Append: [Silent Tokens, Loud Effects: Padding in LLMs](https://arxiv.org/abs/2510.01238)
Token length: 1251
Summarized using GPT-3.5-turbo
Append: [CIFLEX: Contextual Instruction Flow for Sub-task Execution in Multi-Turn Interactions with a Single On-Device LLM](https://arxiv.org/abs/2510.01239)
Token length: 1325
Summarized using GPT-3.5-turbo
Append: [SKYLENAGE Technical Report: Mathematical Reasoning and Contest-Innovation Benchmarks for Multi-Level Math Evaluation](https://arxiv.org/abs/2510.01241)
Token length: 1920
Summarized using GPT-3.5-turbo
Append: [Redundancy-as-Masking: Formalizing the Artificial Age Score (AAS) to Model Memory Aging in Generative AI](https://arxiv.org/abs/2510.01242)
Token length: 1925
Summarized using GPT-3.5-turbo
Append: [Detoxifying Large Language Models via Autoregressive Reward Guided Representation Editing](https://arxiv.org/abs/2510.01243)
Token length: 1903
Summarized using GPT-3.5-turbo
Append: [Feasibility of Structuring Stress Documentation Using an Ontology-Guided Large Language Model](https://arxiv.org/abs/2510.01244)
Token length: 1139
Summarized using GPT-3.5-turbo
Append: [SeMob: Semantic Synthesis for Dynamic Urban Mobility Prediction](https://arxiv.org/abs/2510.01245)
Token length: 1142
Summarized using GPT-3.5-turbo
Append: [A Comparative Analysis of Sparse Autoencoder and Activation Difference in Language Model Steering](https://arxiv.org/abs/2510.01246)
Token length: 1042
Summarized using GPT-3.5-turbo
Append: [Let's Play Across Cultures: A Large Multilingual, Multicultural Benchmark for Assessing Language Models' Understanding of Sports](https://arxiv.org/abs/2510.01247)
Token length: 1761
Summarized using GPT-3.5-turbo
Append: [SSTAG: Structure-Aware Self-Supervised Learning Method for Text-Attributed Graphs](https://arxiv.org/abs/2510.01248)
Token length: 1140
Summarized using GPT-3.5-turbo
Append: [LOCA: Logical Chain Augmentation for Scientific Corpus Cleaning](https://arxiv.org/abs/2510.01249)
Token length: 1301
Summarized using GPT-3.5-turbo
Append: [GemDetox at TextDetox CLEF 2025: Enhancing a Massively Multilingual Model for Text Detoxification on Low-resource Languages](https://arxiv.org/abs/2510.01250)
Token length: 1230
Summarized using GPT-3.5-turbo
Append: [Efficient Uncertainty Estimation for LLM-based Entity Linking in Tabular Data](https://arxiv.org/abs/2510.01251)
Token length: 999
Summarized using GPT-3.5-turbo
Append: [GPT and Prejudice: A Sparse Approach to Understanding Learned Representations in Large Language Models](https://arxiv.org/abs/2510.01252)
Token length: 1313
Summarized using GPT-3.5-turbo
Append: [Do Bias Benchmarks Generalise? Evidence from Voice-based Evaluation of Gender Bias in SpeechLLMs](https://arxiv.org/abs/2510.01254)
Token length: 1067
Summarized using GPT-3.5-turbo
Append: [Longitudinal Monitoring of LLM Content Moderation of Social Issues](https://arxiv.org/abs/2510.01255)
Token length: 1194
Summarized using GPT-3.5-turbo
Append: [RJE: A Retrieval-Judgment-Exploration Framework for Efficient Knowledge Graph Question Answering with LLMs](https://arxiv.org/abs/2510.01257)
Token length: 1307
Summarized using GPT-3.5-turbo
Append: [Measuring Algorithmic Partisanship via Zero-Shot Classification and Its Implications on Political Discourse](https://arxiv.org/abs/2510.01258)
Token length: 1553
Summarized using GPT-3.5-turbo
Append: [In AI Sweet Harmony: Sociopragmatic Guardrail Bypasses and Evaluation-Awareness in OpenAI gpt-oss-20b](https://arxiv.org/abs/2510.01259)
Token length: 1934
Summarized using GPT-3.5-turbo
Append: [OpenAI's GPT-OSS-20B Model and Safety Alignment Issues in a Low-Resource Language](https://arxiv.org/abs/2510.01266)
Token length: 1013
Summarized using GPT-3.5-turbo
Append: [AdaDetectGPT: Adaptive Detection of LLM-Generated Text with Statistical Guarantees](https://arxiv.org/abs/2510.01268)
Token length: 1511
Summarized using GPT-3.5-turbo
Append: [Think Twice, Generate Once: Safeguarding by Progressive Self-Reflection](https://arxiv.org/abs/2510.01270)
Token length: 1279
Summarized using GPT-3.5-turbo
Append: [TraceDet: Hallucination Detection from the Decoding Trace of Diffusion Large Language Models](https://arxiv.org/abs/2510.01274)
Token length: 1376
Summarized using GPT-3.5-turbo
Append: [LLM Based Sentiment Classification From Bangladesh E-Commerce Reviews](https://arxiv.org/abs/2510.01276)
Token length: 1274
Summarized using GPT-3.5-turbo
Append: [TUMIX: Multi-Agent Test-Time Scaling with Tool-Use Mixture](https://arxiv.org/abs/2510.01279)
Token length: 884
Summarized using GPT-3.5-turbo
Append: [Evaluation Sheet for Deep Research: A Use Case for Academic Survey Writing](https://arxiv.org/abs/2510.01283)
Token length: 1849
Summarized using GPT-3.5-turbo
Append: [HiSpec: Hierarchical Speculative Decoding for LLMs](https://arxiv.org/abs/2510.01336)
Token length: 1090
Summarized using GPT-3.5-turbo
Append: [TAG-EQA: Text-And-Graph for Event Question Answering via Structured Prompting Strategies](https://arxiv.org/abs/2510.01391)
Token length: 901
Summarized using GPT-3.5-turbo
Append: [A-VERT: Agnostic Verification with Embedding Ranking Targets](https://arxiv.org/abs/2510.01469)
Token length: 1198
Summarized using GPT-3.5-turbo
Append: [One More Question is Enough, Expert Question Decomposition (EQD) Model for Domain Quantitative Reasoning](https://arxiv.org/abs/2510.01526)
Token length: 1151
Summarized using GPT-3.5-turbo
Append: [ReSSFormer: A Recursive Sparse Structured Transformer for Scalable and Long-Context Reasoning](https://arxiv.org/abs/2510.01585)
Token length: 1718
Summarized using GPT-3.5-turbo
Append: [CLUE: Non-parametric Verification from Experience via Hidden-State Clustering](https://arxiv.org/abs/2510.01591)
Token length: 1749
Summarized using GPT-3.5-turbo
Append: [A Comparison of Independent and Joint Fine-tuning Strategies for Retrieval-Augmented Generation](https://arxiv.org/abs/2510.01600)
Token length: 997
Summarized using GPT-3.5-turbo
Append: [RAG-BioQA Retrieval-Augmented Generation for Long-Form Biomedical Question Answering](https://arxiv.org/abs/2510.01612)
Token length: 1397
Summarized using GPT-3.5-turbo
Append: [Efficient Training of Robust Traditional Chinese LLaMA-1B on a Single Consumer GPU: Continual Pre-training, SFT, and DPO](https://arxiv.org/abs/2510.01616)
Append: [AMAS: Adaptively Determining Communication Topology for LLM-based Multi-Agent System](https://arxiv.org/abs/2510.01617)
Append: [NLP Methods for Detecting Novel LLM Jailbreaks and Keyword Analysis with BERT](https://arxiv.org/abs/2510.01644)
Append: [Learning to Look at the Other Side: A Semantic Probing Study of Word Embeddings in LLMs with Enabled Bidirectional Attention](https://arxiv.org/abs/2510.01652)
Append: [SoK: Measuring What Matters for Closed-Loop Security Agents](https://arxiv.org/abs/2510.01654)
Append: [MDSEval: A Meta-Evaluation Benchmark for Multimodal Dialogue Summarization](https://arxiv.org/abs/2510.01659)
Append: [FOR-Prompting: From Objection to Revision via an Asymmetric Prompting Protocol](https://arxiv.org/abs/2510.01674)
Append: [How Do Language Models Compose Functions?](https://arxiv.org/abs/2510.01685)
Append: [Format Inertia: A Failure Mechanism of LLMs in Medical Pre-Consultation](https://arxiv.org/abs/2510.01688)
Append: [What MLLMs Learn about When they Learn about Multimodal Reasoning: Perception, Reasoning, or their Integration?](https://arxiv.org/abs/2510.01719)
Append: [Machine-interpretable Engineering Design Standards for Valve Specification](https://arxiv.org/abs/2510.01736)
Append: [Can LLMs Refuse Questions They Do Not Know? Measuring Knowledge-Aware Refusal in Factual Tasks](https://arxiv.org/abs/2510.01782)
Append: [Comparison of Unsupervised Metrics for Evaluating Judicial Decision Extraction](https://arxiv.org/abs/2510.01792)
Append: [Detecting LLM-Generated Spam Reviews by Integrating Language Model Embeddings and Graph Neural Network](https://arxiv.org/abs/2510.01801)
Append: [Syntactic Blind Spots: How Misalignment Leads to LLMs Mathematical Errors](https://arxiv.org/abs/2510.01831)
Append: [SCRIBES: Web-Scale Script-Based Semi-Structured Data Extraction with Reinforcement Learning](https://arxiv.org/abs/2510.01832)
Append: [Model Merging to Maintain Language-Only Performance in Developmentally Plausible Multimodal Models](https://arxiv.org/abs/2510.01845)
Append: [REPAIR: Robust Editing via Progressive Adaptive Intervention and Reintegration](https://arxiv.org/abs/2510.01879)
Append: [Enhancing Large Language Model Reasoning with Reward Models: An Analytical Survey](https://arxiv.org/abs/2510.01925)
Append: [Inverse Language Modeling towards Robust and Grounded LLMs](https://arxiv.org/abs/2510.01929)
Append: [Veri-R1: Toward Precise and Faithful Claim Verification via Online Reinforcement Learning](https://arxiv.org/abs/2510.01932)
Append: [Taking a SEAT: Predicting Value Interpretations from Sentiment, Emotion, Argument, and Topic Annotations](https://arxiv.org/abs/2510.01976)
Append: [Exploring Database Normalization Effects on SQL Generation](https://arxiv.org/abs/2510.01989)
Append: [LLM-Based Multi-Task Bangla Hate Speech Detection: Type, Severity, and Target](https://arxiv.org/abs/2510.01995)
Append: [Style Over Story: A Process-Oriented Study of Authorial Creativity in Large Language Models](https://arxiv.org/abs/2510.02025)
Append: [Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage](https://arxiv.org/abs/2510.02044)
Append: [Chain-of-Thought Reasoning in Streaming Full-Duplex End-to-End Spoken Dialogue Systems](https://arxiv.org/abs/2510.02066)
Append: [The Disparate Impacts of Speculative Decoding](https://arxiv.org/abs/2510.02128)
Append: [RESTRAIN: From Spurious Votes to Signals -- Self-Driven RL with Self-Penalization](https://arxiv.org/abs/2510.02172)
Append: [Learning to Reason for Hallucination Span Detection](https://arxiv.org/abs/2510.02173)
Append: [ARUQULA -- An LLM based Text2SPARQL Approach using ReAct and Knowledge Graph Exploration Utilities](https://arxiv.org/abs/2510.02200)
Append: [Say One Thing, Do Another? Diagnosing Reasoning-Execution Gaps in VLM-Powered Mobile-Use Agents](https://arxiv.org/abs/2510.02204)
Append: [More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for Diverse Exploration](https://arxiv.org/abs/2510.02227)
Append: [Enhanced Arabic-language cyberbullying detection: deep embedding and transformer (BERT) approaches](https://arxiv.org/abs/2510.02232)
Append: [AccurateRAG: A Framework for Building Accurate Retrieval-Augmented Question-Answering Applications](https://arxiv.org/abs/2510.02243)
Append: [Explore Briefly, Then Decide: Mitigating LLM Overthinking via Cumulative Entropy Regulation](https://arxiv.org/abs/2510.02249)
Append: [InfoMosaic-Bench: Evaluating Multi-Source Information Seeking in Tool-Augmented Agents](https://arxiv.org/abs/2510.02271)
Append: [Parallel Scaling Law: Unveiling Reasoning Generalization through A Cross-Linguistic Perspective](https://arxiv.org/abs/2510.02272)
Append: [From Behavioral Performance to Internal Competence: Interpreting Vision-Language Models with VLM-Lens](https://arxiv.org/abs/2510.02292)
Append: [F2LLM Technical Report: Matching SOTA Embedding Performance with 6 Million Open-Source Data](https://arxiv.org/abs/2510.02294)
Append: [Drawing Conclusions from Draws: Rethinking Preference Semantics in Arena-Style LLM Evaluation](https://arxiv.org/abs/2510.02306)
Append: [Control the Temperature: Selective Sampling for Diverse and High-Quality LLM Outputs](https://arxiv.org/abs/2510.01218)
Append: [Jailbreaking LLMs via Semantically Relevant Nested Scenarios with Targeted Toxic Knowledge](https://arxiv.org/abs/2510.01223)
Append: [Utilizing Modern Large Language Models (LLM) for Financial Trend Analysis and Digest Creation](https://arxiv.org/abs/2510.01225)
Append: [Automated Extraction of Material Properties using LLM-based AI Agents](https://arxiv.org/abs/2510.01235)
Append: [RSAVQ: Riemannian Sensitivity-Aware Vector Quantization for Large Language Models](https://arxiv.org/abs/2510.01240)
Append: [RLP: Reinforcement as a Pretraining Objective](https://arxiv.org/abs/2510.01265)
Append: [LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science](https://arxiv.org/abs/2510.01285)
Append: [Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and Reasoning in Vision-Language Models](https://arxiv.org/abs/2510.01304)
Append: [Aristotle: IMO-level Automated Theorem Proving](https://arxiv.org/abs/2510.01346)
Append: [MEMTRACK: Evaluating Long-Term Memory and State Tracking in Multi-Platform Dynamic Agent Environments](https://arxiv.org/abs/2510.01353)
Append: [WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents](https://arxiv.org/abs/2510.01354)
Append: [Is It Thinking or Cheating? Detecting Implicit Reward Hacking by Measuring Reasoning Effort](https://arxiv.org/abs/2510.01367)
Append: [Fine-tuning with RAG for Improving LLM Learning of New Skills](https://arxiv.org/abs/2510.01375)
Append: [Optimal Stopping vs Best-of-$N$ for Inference Time Optimization](https://arxiv.org/abs/2510.01394)
Append: [VOGUE: Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning](https://arxiv.org/abs/2510.01444)
Append: [LSPO: Length-aware Dynamic Sampling for Policy Optimization in LLM Reasoning](https://arxiv.org/abs/2510.01459)
Append: [Extracting O*NET Features from the NLx Corpus to Build Public Use Aggregate Labor Market Data](https://arxiv.org/abs/2510.01470)
Append: [From Videos to Indexed Knowledge Graphs -- Framework to Marry Methods for Multimodal Content Analysis and Understanding](https://arxiv.org/abs/2510.01513)
Append: [Information Seeking for Robust Decision Making under Partial Observability](https://arxiv.org/abs/2510.01531)
Append: [InvThink: Towards AI Safety via Inverse Reasoning](https://arxiv.org/abs/2510.01569)
Append: [Synthetic Prefixes to Mitigate Bias in Real-Time Neural Query Autocomplete](https://arxiv.org/abs/2510.01574)
Append: [Think Right: Learning to Mitigate Under-Over Thinking via Adaptive, Attentive Compression](https://arxiv.org/abs/2510.01581)
Append: [Bridging Collaborative Filtering and Large Language Models with Dynamic Alignment, Multimodal Fusion and Evidence-grounded Explanations](https://arxiv.org/abs/2510.01606)
Append: [PychoBench: Evaluating the Psychology Intelligence of Large Language Models](https://arxiv.org/abs/2510.01611)
Append: [LLM4Rec: Large Language Models for Multimodal Generative Recommendation with Causal Debiasing](https://arxiv.org/abs/2510.01622)
Append: [Quagmires in SFT-RL Post-Training: When High SFT Scores Mislead and What to Use Instead](https://arxiv.org/abs/2510.01624)
Append: [Demystifying Synthetic Data in LLM Pre-training: A Systematic Study of Scaling Laws, Benefits, and Pitfalls](https://arxiv.org/abs/2510.01631)
Append: [Position: Privacy Is Not Just Memorization!](https://arxiv.org/abs/2510.01645)
Append: [Just Do It!? Computer-Use Agents Exhibit Blind Goal-Directedness](https://arxiv.org/abs/2510.01670)
Append: [Improving AGI Evaluation: A Data Science Perspective](https://arxiv.org/abs/2510.01687)
Append: [Sparse Query Attention (SQA): A Computationally Efficient Attention Mechanism with Query Heads Reduction](https://arxiv.org/abs/2510.01817)
Append: [Plan Then Action:High-Level Planning Guidance Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2510.01833)
Append: [Constrained Adaptive Rejection Sampling](https://arxiv.org/abs/2510.01902)
Append: [Do AI Models Perform Human-like Abstract Reasoning Across Modalities?](https://arxiv.org/abs/2510.02125)
Append: [A Rigorous Benchmark with Multidimensional Evaluation for Deep Research Agents: From Answers to Reports](https://arxiv.org/abs/2510.02190)
Append: [StockBench: Can LLM Agents Trade Stocks Profitably In Real-world Markets?](https://arxiv.org/abs/2510.02209)
Append: [The Reasoning Boundary Paradox: How Reinforcement Learning Constrains Language Models](https://arxiv.org/abs/2510.02230)
Append: [Study on LLMs for Promptagator-Style Dense Retriever Training](https://arxiv.org/abs/2510.02241)
Append: [ExGRPO: Learning to Reason from Experience](https://arxiv.org/abs/2510.02245)
Append: [The Unreasonable Effectiveness of Scaling Agents for Computer Use](https://arxiv.org/abs/2510.02250)
Append: [RLAD: Training LLMs to Discover Abstractions for Solving Reasoning Problems](https://arxiv.org/abs/2510.02263)
Append: [Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming Attacks](https://arxiv.org/abs/2510.02286)
Append: [Interactive Training: Feedback-Driven Neural Network Optimization](https://arxiv.org/abs/2510.02297)
Append: [Superficial Safety Alignment Hypothesis](https://arxiv.org/abs/2410.10862)
Append: [Can Code-Switched Texts Activate a Knowledge Switch in LLMs? A Case Study on English-Korean Code-Switching](https://arxiv.org/abs/2410.18436)
Append: [Self-Consistency Falls Short! The Adverse Effects of Positional Bias on Long-Context Problems](https://arxiv.org/abs/2411.01101)
Append: [Reasoning over User Preferences: Knowledge Graph-Augmented LLMs for Explainable Conversational Recommendations](https://arxiv.org/abs/2411.14459)
Append: [Push the Limit of Multi-modal Emotion Recognition by Prompting LLMs with Receptive-Field-Aware Attention Weighting](https://arxiv.org/abs/2411.17674)
Append: [Initialization using Update Approximation is a Silver Bullet for Extremely Efficient Low-Rank Fine-Tuning](https://arxiv.org/abs/2411.19557)
Append: [Adapting Large Language Models for Character-based Augmentative and Alternative Communication](https://arxiv.org/abs/2501.10582)
Append: [Out-of-Distribution Detection using Synthetic Data Generation](https://arxiv.org/abs/2502.03323)
Append: [Interpretable Text Embeddings and Text Similarity Explanation: A Survey](https://arxiv.org/abs/2502.14862)
Append: [When Disagreements Elicit Robustness: Investigating Self-Repair Capabilities under LLM Multi-Agent Disagreements](https://arxiv.org/abs/2502.15153)
Append: [FANS -- Formal Answer Selection for Natural Language Math Reasoning Using Lean4](https://arxiv.org/abs/2503.03238)
Append: [Probabilistic Reasoning with LLMs for k-anonymity Estimation](https://arxiv.org/abs/2503.09674)
Append: [TLUE: A Tibetan Language Understanding Evaluation Benchmark](https://arxiv.org/abs/2503.12051)
Append: [Boundless Byte Pair Encoding: Breaking the Pre-tokenization Barrier](https://arxiv.org/abs/2504.00178)
Append: [Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward](https://arxiv.org/abs/2504.03206)
Append: [WebRollback: Enhancing Web Agents with Explicit Rollback Mechanisms](https://arxiv.org/abs/2504.11788)
Append: [Design and Application of Multimodal Large Language Model Based System for End to End Automation of Accident Dataset Generation](https://arxiv.org/abs/2505.00015)
Append: [OntoURL: A Benchmark for Evaluating Large Language Models on Symbolic Ontological Understanding, Reasoning and Learning](https://arxiv.org/abs/2505.11031)
Append: [LEXam: Benchmarking Legal Reasoning on 340 Law Exams](https://arxiv.org/abs/2505.12864)
Append: [ABBA-Adapters: Efficient and Expressive Fine-Tuning of Foundation Models](https://arxiv.org/abs/2505.14238)
Append: [MolLangBench: A Comprehensive Benchmark for Language-Prompted Molecular Structure Recognition, Editing, and Generation](https://arxiv.org/abs/2505.15054)
Append: [BiasLab: Toward Explainable Political Bias Detection with Dual-Axis Annotations and Rationale Indicators](https://arxiv.org/abs/2505.16081)
Append: [Deriving Strategic Market Insights with Large Language Models: A Benchmark for Forward Counterfactual Generation](https://arxiv.org/abs/2505.19430)
Append: [Charting the Landscape of African NLP: Mapping Progress and Shaping the Road Ahead](https://arxiv.org/abs/2505.21315)
Append: [When Models Reason in Your Language: Controlling Thinking Language Comes at the Cost of Accuracy](https://arxiv.org/abs/2505.22888)
Append: [Diagnosing and Addressing Pitfalls in KG-RAG Datasets: Toward More Reliable Benchmarking](https://arxiv.org/abs/2505.23495)
Append: [Should I Share this Translation? Evaluating Quality Feedback for User Reliance on Machine Translation](https://arxiv.org/abs/2505.24683)
Append: [MetaFaith: Faithful Natural Language Uncertainty Expression in LLMs](https://arxiv.org/abs/2505.24858)
Append: [Do LLMs Really Forget? Evaluating Unlearning with Knowledge Correlation and Confidence Awareness](https://arxiv.org/abs/2506.05735)
Append: [Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via Self-Critical Fine-Tuning](https://arxiv.org/abs/2506.21285)
Append: [No Language Data Left Behind: A Comparative Study of CJK Language Datasets in the Hugging Face Ecosystem](https://arxiv.org/abs/2507.04329)
Append: [Reason to Rote: Rethinking Memorization in Reasoning](https://arxiv.org/abs/2507.04782)
Append: [Flexible Feature Distillation for Large Language Models](https://arxiv.org/abs/2507.10155)
Append: [Reasoning Models are Test Exploiters: Rethinking Multiple-Choice](https://arxiv.org/abs/2507.15337)
Append: [Co-NAML-LSTUR: A Combined Model with Attentive Multi-View Learning and Long- and Short-term User Representations for News Recommendation](https://arxiv.org/abs/2507.20210)
Append: [Mafoko: Structuring and Building Open Multilingual Terminologies for South African NLP](https://arxiv.org/abs/2508.03529)
Append: [What if I ask in \textit{alia lingua}? Measuring Functional Similarity Across Languages](https://arxiv.org/abs/2509.04032)
Append: [MOSAIC: A Multilingual, Taxonomy-Agnostic, and Computationally Efficient Approach for Radiological Report Classification](https://arxiv.org/abs/2509.04471)
Append: [Investigating ReLoRA: Effects on the Learning Dynamics of Small Language Models](https://arxiv.org/abs/2509.12960)
Append: [Linguistic Nepotism: Trading-off Quality for Language Preference in Multilingual RAG](https://arxiv.org/abs/2509.13930)
Append: [SpeechWeave: Diverse Multilingual Synthetic Text & Audio Data Generation Pipeline for Training Text to Speech Models](https://arxiv.org/abs/2509.14270)
Append: [AutoScale: Scale-Aware Data Mixing for Pre-Training LLMs](https://arxiv.org/abs/2407.20177)
Append: [Sparkle: Mastering Basic Spatial Capabilities in Vision Language Models Elicits Generalization to Spatial Reasoning](https://arxiv.org/abs/2410.16162)
Append: [Synergizing LLMs and Knowledge Graphs: A Novel Approach to Software Repository-Related Question Answering](https://arxiv.org/abs/2412.03815)
Append: [CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation](https://arxiv.org/abs/2504.15254)
Append: [MathArena: Evaluating LLMs on Uncontaminated Math Competitions](https://arxiv.org/abs/2505.23281)
Append: [Differential Information Distribution: A Bayesian Perspective on Direct Preference Optimization](https://arxiv.org/abs/2505.23761)
Append: [DrKGC: Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph Completion across General and Biomedical Domains](https://arxiv.org/abs/2506.00708)
Append: [Beyond Chunking: Discourse-Aware Hierarchical Retrieval for Long Document Question Answering](https://arxiv.org/abs/2506.06313)
Append: [Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes Correct Reasoning in Base LLMs](https://arxiv.org/abs/2506.14245)
Append: [Efficient Whole Slide Pathology VQA via Token Compression](https://arxiv.org/abs/2507.14497)
Append: [AlgoTune: Can Language Models Speed Up General-Purpose Numerical Programs?](https://arxiv.org/abs/2507.15887)
Append: [PurpCode: Reasoning for Safer Code Generation](https://arxiv.org/abs/2507.19060)
Append: [Injecting External Knowledge into the Reasoning Process Enhances Retrieval-Augmented Generation](https://arxiv.org/abs/2507.19333)
Append: [Defend LLMs Through Self-Consciousness](https://arxiv.org/abs/2508.02961)
Append: [Aligning Reasoning LLMs for Materials Discovery with Physics-aware Rejection Sampling](https://arxiv.org/abs/2509.00768)
Append: [DynaGuard: A Dynamic Guardrail Model With User-Defined Policies](https://arxiv.org/abs/2509.02563)
append_entries: 190
Finish: 2025-10-03 04:22:05.907984
------------------------------------------------------
Started: 2025-10-03 06:23:51.962151
Existing_entries: 1190
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1479
Summarized using GPT-3.5-turbo
Append: [On Code-Induced Reasoning in LLMs](https://arxiv.org/abs/2509.21499)
append_entries: 1
Finish: 2025-10-03 06:23:54.517646
------------------------------------------------------
Started: 2025-10-03 08:20:28.743659
Existing_entries: 1001
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-03 08:20:29.173139
------------------------------------------------------
Started: 2025-10-03 10:16:31.518218
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-03 10:16:31.937951
------------------------------------------------------
Started: 2025-10-03 12:32:40.314017
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-03 12:32:40.771766
------------------------------------------------------
Started: 2025-10-03 14:15:16.567007
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-03 14:15:17.068021
------------------------------------------------------
Started: 2025-10-03 16:19:23.961683
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-03 16:19:24.377617
------------------------------------------------------
Started: 2025-10-03 18:22:13.748348
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-03 18:22:14.172852
------------------------------------------------------
Started: 2025-10-03 20:17:02.847184
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-03 20:17:03.263768
------------------------------------------------------
Started: 2025-10-03 22:14:08.188835
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-03 22:14:08.612135
------------------------------------------------------
Started: 2025-10-04 01:09:25.384218
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-04 01:09:25.815808
------------------------------------------------------
Started: 2025-10-04 02:47:45.946409
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-04 02:47:46.373752
------------------------------------------------------
Started: 2025-10-04 04:17:45.989187
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-04 04:17:46.047197
------------------------------------------------------
Started: 2025-10-04 06:21:10.274156
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-04 06:21:10.336413
------------------------------------------------------
Started: 2025-10-04 08:18:57.953112
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-04 08:18:58.011863
------------------------------------------------------
Started: 2025-10-04 10:14:35.696097
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-04 10:14:35.781362
------------------------------------------------------
Started: 2025-10-04 12:29:02.585296
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-04 12:29:02.642005
------------------------------------------------------
Started: 2025-10-04 14:12:54.590968
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-04 14:12:54.650173
------------------------------------------------------
Started: 2025-10-04 16:17:32.540535
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-04 16:17:32.665135
------------------------------------------------------
Started: 2025-10-04 18:20:05.355149
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-04 18:20:05.434588
------------------------------------------------------
Started: 2025-10-04 20:15:20.099331
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-04 20:15:20.225937
------------------------------------------------------
Started: 2025-10-04 22:13:51.403541
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-04 22:13:51.460512
------------------------------------------------------
Started: 2025-10-05 01:19:38.795876
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-05 01:19:38.868000
------------------------------------------------------
Started: 2025-10-05 03:04:48.140924
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-05 03:04:48.257685
------------------------------------------------------
Started: 2025-10-05 04:18:05.890775
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-05 04:18:05.965041
------------------------------------------------------
Started: 2025-10-05 06:21:24.490534
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-05 06:21:24.549202
------------------------------------------------------
Started: 2025-10-05 08:18:21.867418
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-05 08:18:21.940571
------------------------------------------------------
Started: 2025-10-05 10:14:57.901193
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-05 10:14:58.005851
------------------------------------------------------
Started: 2025-10-05 12:29:20.307684
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-05 12:29:20.381283
------------------------------------------------------
Started: 2025-10-05 14:12:54.383365
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-05 14:12:54.459279
------------------------------------------------------
Started: 2025-10-05 16:17:28.396208
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-05 16:17:28.461756
------------------------------------------------------
Started: 2025-10-05 18:20:22.069888
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-05 18:20:22.137845
------------------------------------------------------
Started: 2025-10-05 20:15:58.215982
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-05 20:15:58.277826
------------------------------------------------------
Started: 2025-10-05 22:14:01.044050
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-05 22:14:01.105807
------------------------------------------------------
Started: 2025-10-06 01:14:26.445515
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-06 01:14:26.507155
------------------------------------------------------
Started: 2025-10-06 02:59:18.981187
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-06 02:59:19.083202
------------------------------------------------------
Started: 2025-10-06 04:22:33.659480
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1613
Summarized using GPT-3.5-turbo
Append: [Hallucination reduction with CASAL: Contrastive Activation Steering For Amortized Learning](https://arxiv.org/abs/2510.02324)
Token length: 1583
Summarized using GPT-3.5-turbo
Append: [Hallucination-Resistant, Domain-Specific Research Assistant with Self-Evaluation and Vector-Grounded Retrieval](https://arxiv.org/abs/2510.02326)
Token length: 1284
Summarized using GPT-3.5-turbo
Append: [KAME: Tandem Architecture for Enhancing Knowledge in Real-Time Speech-to-Speech Conversational AI](https://arxiv.org/abs/2510.02327)
Token length: 1144
Summarized using GPT-3.5-turbo
Append: [AMANDA: Agentic Medical Knowledge Augmentation for Data-Efficient Medical Visual Question Answering](https://arxiv.org/abs/2510.02328)
Token length: 973
Summarized using GPT-3.5-turbo
Append: [SelfJudge: Faster Speculative Decoding via Self-Supervised Judge Verification](https://arxiv.org/abs/2510.02329)
Token length: 1425
Summarized using GPT-3.5-turbo
Append: [EntropyLong: Effective Long-Context Training via Predictive Uncertainty](https://arxiv.org/abs/2510.02330)
Token length: 856
Summarized using GPT-3.5-turbo
Append: [Synthetic Dialogue Generation for Interactive Conversational Elicitation & Recommendation (ICER)](https://arxiv.org/abs/2510.02331)
Token length: 1579
Summarized using GPT-3.5-turbo
Append: [A High-Capacity and Secure Disambiguation Algorithm for Neural Linguistic Steganography](https://arxiv.org/abs/2510.02332)
Token length: 1225
Summarized using GPT-3.5-turbo
Append: [Human Mobility Datasets Enriched With Contextual and Social Dimensions](https://arxiv.org/abs/2510.02333)
Token length: 1387
Summarized using GPT-3.5-turbo
Append: [Where Did It Go Wrong? Attributing Undesirable LLM Behaviors via Representation Gradient Tracing](https://arxiv.org/abs/2510.02334)
Token length: 1096
Summarized using GPT-3.5-turbo
Append: [FormalML: A Benchmark for Evaluating Formal Subgoal Completion in Machine Learning Theory](https://arxiv.org/abs/2510.02335)
Token length: 819
Summarized using GPT-3.5-turbo
Append: [KurdSTS: The Kurdish Semantic Textual Similarity](https://arxiv.org/abs/2510.02336)
Token length: 995
Summarized using GPT-3.5-turbo
Append: [CRACQ: A Multi-Dimensional Approach To Automated Document Assessment](https://arxiv.org/abs/2510.02337)
Token length: 1208
Summarized using GPT-3.5-turbo
Append: [Optimizing Long-Form Clinical Text Generation with Claim-Based Rewards](https://arxiv.org/abs/2510.02338)
Token length: 960
Summarized using GPT-3.5-turbo
Append: [Evaluating Uncertainty Quantification Methods in Argumentative Large Language Models](https://arxiv.org/abs/2510.02339)
Token length: 1266
Summarized using GPT-3.5-turbo
Append: [Can Prompts Rewind Time for LLMs? Evaluating the Effectiveness of Prompted Knowledge Cutoffs](https://arxiv.org/abs/2510.02340)
Token length: 1770
Summarized using GPT-3.5-turbo
Append: [DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning](https://arxiv.org/abs/2510.02341)
Token length: 1857
Summarized using GPT-3.5-turbo
Append: [$\texttt{BluePrint}$: A Social Media User Dataset for LLM Persona Evaluation and Training](https://arxiv.org/abs/2510.02343)
Token length: 1766
Summarized using GPT-3.5-turbo
Append: [Breaking the MoE LLM Trilemma: Dynamic Expert Clustering with Structured Compression](https://arxiv.org/abs/2510.02345)
Token length: 1115
Summarized using GPT-3.5-turbo
Append: [Small Language Models for Curriculum-based Guidance](https://arxiv.org/abs/2510.02347)
Token length: 861
Summarized using GPT-3.5-turbo
Append: [mini-vec2vec: Scaling Universal Geometry Alignment with Linear Transformations](https://arxiv.org/abs/2510.02348)
Token length: 1251
Summarized using GPT-3.5-turbo
Append: [LLMSQL: Upgrading WikiSQL for the LLM Era of Text-to-SQL](https://arxiv.org/abs/2510.02350)
Token length: 1122
Summarized using GPT-3.5-turbo
Append: [Language, Culture, and Ideology: Personalizing Offensiveness Detection in Political Tweets with Reasoning LLMs](https://arxiv.org/abs/2510.02351)
Token length: 1521
Summarized using GPT-3.5-turbo
Append: [Evaluating Bias in Spoken Dialogue LLMs for Real-World Decisions and Recommendations](https://arxiv.org/abs/2510.02352)
Token length: 1063
Summarized using GPT-3.5-turbo
Append: [An Senegalese Legal Texts Structuration Using LLM-augmented Knowledge Graph](https://arxiv.org/abs/2510.02353)
Token length: 1274
Summarized using GPT-3.5-turbo
Append: [Modeling the language cortex with form-independent and enriched representations of sentence meaning reveals remarkable semantic abstractness](https://arxiv.org/abs/2510.02354)
Token length: 1579
Summarized using GPT-3.5-turbo
Append: [DiffuSpec: Unlocking Diffusion Language Models for Speculative Decoding](https://arxiv.org/abs/2510.02358)
Token length: 1535
Summarized using GPT-3.5-turbo
Append: [Emission-GPT: A domain-specific language model agent for knowledge retrieval, emission inventory and data analysis](https://arxiv.org/abs/2510.02359)
Token length: 1427
Summarized using GPT-3.5-turbo
Append: [Spiral of Silence in Large Language Model Agents](https://arxiv.org/abs/2510.02360)
Token length: 1851
Summarized using GPT-3.5-turbo
Append: [ChunkLLM: A Lightweight Pluggable Framework for Accelerating LLMs Inference](https://arxiv.org/abs/2510.02361)
Token length: 1514
Summarized using GPT-3.5-turbo
Append: [A Cross-Lingual Analysis of Bias in Large Language Models Using Romanian History](https://arxiv.org/abs/2510.02362)
Token length: 1819
Summarized using GPT-3.5-turbo
Append: [Beyond Manuals and Tasks: Instance-Level Context Learning for LLM Agents](https://arxiv.org/abs/2510.02369)
Token length: 1687
Summarized using GPT-3.5-turbo
Append: [Training Dynamics of Parametric and In-Context Knowledge Utilization in Language Models](https://arxiv.org/abs/2510.02370)
Token length: 1489
Summarized using GPT-3.5-turbo
Append: [Pretraining with hierarchical memories: separating long-tail and common knowledge](https://arxiv.org/abs/2510.02375)
Token length: 1053
Summarized using GPT-3.5-turbo
Append: [Uncertainty-Aware Answer Selection for Improved Reasoning in Multi-LLM Systems](https://arxiv.org/abs/2510.02377)
Token length: 1864
Summarized using GPT-3.5-turbo
Append: [Learning to Route: A Rule-Driven Agent Framework for Hybrid-Source Retrieval-Augmented Generation](https://arxiv.org/abs/2510.02388)
Token length: 1339
Summarized using GPT-3.5-turbo
Append: [KnowledgeSmith: Uncovering Knowledge Updating in LLMs with Model Editing and Unlearning](https://arxiv.org/abs/2510.02392)
Token length: 1165
Summarized using GPT-3.5-turbo
Append: [Retrieval and Augmentation of Domain Knowledge for Text-to-SQL Semantic Parsing](https://arxiv.org/abs/2510.02394)
Token length: 809
Summarized using GPT-3.5-turbo
Append: [Words That Make Language Models Perceive](https://arxiv.org/abs/2510.02425)
Token length: 1189
Summarized using GPT-3.5-turbo
Append: [CLARITY: Clinical Assistant for Routing, Inference, and Triage](https://arxiv.org/abs/2510.02463)
Token length: 1570
Summarized using GPT-3.5-turbo
Append: [Unraveling Syntax: How Language Models Learn Context-Free Grammars](https://arxiv.org/abs/2510.02524)
Token length: 1171
Summarized using GPT-3.5-turbo
Append: [Hierarchical Semantic Retrieval with Cobweb](https://arxiv.org/abs/2510.02539)
Token length: 1464
Summarized using GPT-3.5-turbo
Append: [Knowledge-Graph Based RAG System Evaluation Framework](https://arxiv.org/abs/2510.02549)
Token length: 1030
Summarized using GPT-3.5-turbo
Append: [Transcribe, Translate, or Transliterate: An Investigation of Intermediate Representations in Spoken Language Models](https://arxiv.org/abs/2510.02569)
Token length: 1423
Summarized using GPT-3.5-turbo
Append: [Evaluation Framework for Highlight Explanations of Context Utilisation in Language Models](https://arxiv.org/abs/2510.02629)
Token length: 1282
Summarized using GPT-3.5-turbo
Append: [Mind the Gap: Linguistic Divergence and Adaptation Strategies in Human-LLM Assistant vs. Human-Human Interactions](https://arxiv.org/abs/2510.02645)
Token length: 1294
Summarized using GPT-3.5-turbo
Append: [SoT: Structured-of-Thought Prompting Guides Multilingual Reasoning in Large Language Models](https://arxiv.org/abs/2510.02648)
Token length: 941
Summarized using GPT-3.5-turbo
Append: [Self-Improvement in Multimodal Large Language Models: A Survey](https://arxiv.org/abs/2510.02665)
Token length: 1726
Summarized using GPT-3.5-turbo
Append: [Uncertainty as Feature Gaps: Epistemic Uncertainty Quantification of LLMs in Contextual Question-Answering](https://arxiv.org/abs/2510.02671)
Token length: 1473
Summarized using GPT-3.5-turbo
Append: [Time-To-Inconsistency: A Survival Analysis of Large Language Model Robustness to Adversarial Attacks](https://arxiv.org/abs/2510.02712)
Append: [TravelBench : Exploring LLM Performance in Low-Resource Domains](https://arxiv.org/abs/2510.02719)
Append: [PGMEL: Policy Gradient-based Generative Adversarial Network for Multimodal Entity Linking](https://arxiv.org/abs/2510.02726)
Append: [IndiCASA: A Dataset and Bias Evaluation Framework in LLMs Using Contrastive Embedding Similarity in the Indian Context](https://arxiv.org/abs/2510.02742)
Append: [The Path of Self-Evolving Large Language Models: Achieving Data-Efficient Learning via Intrinsic Feedback](https://arxiv.org/abs/2510.02752)
Append: [XTRA: Cross-Lingual Topic Modeling with Topic and Representation Alignments](https://arxiv.org/abs/2510.02788)
Append: [A Computational Framework for Interpretable Text-Based Personality Assessment from Social Media](https://arxiv.org/abs/2510.02811)
Append: [StepChain GraphRAG: Reasoning Over Knowledge Graphs for Multi-Hop Question Answering](https://arxiv.org/abs/2510.02827)
Append: [Evaluating Large Language Models for IUCN Red List Species Information](https://arxiv.org/abs/2510.02830)
Append: [Constraint Satisfaction Approaches to Wordle: Novel Heuristics and Cross-Lexicon Validation](https://arxiv.org/abs/2510.02855)
Append: [Self-Reflective Generation at Test Time](https://arxiv.org/abs/2510.02919)
Append: [Finding Diamonds in Conversation Haystacks: A Benchmark for Conversational Data Retrieval](https://arxiv.org/abs/2510.02938)
Append: [Leave No TRACE: Black-box Detection of Copyrighted Dataset Usage in Large Language Models via Watermarking](https://arxiv.org/abs/2510.02962)
Append: [Grounding Large Language Models in Clinical Evidence: A Retrieval-Augmented Generation System for Querying UK NICE Clinical Guidelines](https://arxiv.org/abs/2510.02967)
Append: [Semantic Differentiation in Speech Emotion Recognition: Insights from Descriptive and Expressive Speech Roles](https://arxiv.org/abs/2510.03060)
Append: [Revisiting Direct Speech-to-Text Translation with Speech LLMs: Better Scaling than CoT Prompting?](https://arxiv.org/abs/2510.03093)
Append: [Semantic Similarity in Radiology Reports via LLMs and NER](https://arxiv.org/abs/2510.03102)
Append: [Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation](https://arxiv.org/abs/2510.03115)
Append: [SurveyBench: How Well Can LLM(-Agents) Write Academic Surveys?](https://arxiv.org/abs/2510.03120)
Append: [Beyond the Final Layer: Intermediate Representations for Better Multilingual Calibration in Large Language Models](https://arxiv.org/abs/2510.03136)
Append: [EditLens: Quantifying the Extent of AI Editing in Text](https://arxiv.org/abs/2510.03154)
Append: [Neural Correlates of Language Models Are Specific to Human Language](https://arxiv.org/abs/2510.03156)
Append: [Topic Modeling as Long-Form Generation: Can Long-Context LLMs revolutionize NTM via Zero-Shot Prompting?](https://arxiv.org/abs/2510.03174)
Append: [Model-Based Ranking of Source Languages for Zero-Shot Cross-Lingual Transfer](https://arxiv.org/abs/2510.03202)
Append: [FocusAgent: Simple Yet Effective Ways of Trimming the Large Context of Web Agents](https://arxiv.org/abs/2510.03204)
Append: [Cache-to-Cache: Direct Semantic Communication Between Large Language Models](https://arxiv.org/abs/2510.03215)
Append: [Self-Anchor: Large Language Model Reasoning via Step-by-step Attention Alignment](https://arxiv.org/abs/2510.03223)
Append: [Reward Models are Metrics in a Trench Coat](https://arxiv.org/abs/2510.03231)
Append: [Modeling the Attack: Detecting AI-Generated Text by Quantifying Adversarial Perturbations](https://arxiv.org/abs/2510.02319)
Append: [WEE-Therapy: A Mixture of Weak Encoders Framework for Psychological Counseling Dialogue Analysis](https://arxiv.org/abs/2510.02320)
Append: [SpeechCT-CLIP: Distilling Text-Image Knowledge to Speech for Voice-Native Multimodal CT Analysis](https://arxiv.org/abs/2510.02322)
Append: [CATMark: A Context-Aware Thresholding Framework for Robust Cross-Task Watermarking in Large Language Models](https://arxiv.org/abs/2510.02342)
Append: [How to Train Your Advisor: Steering Black-Box LLMs with Advisor Models](https://arxiv.org/abs/2510.02453)
Append: [SIMSplat: Predictive Driving Scene Editing with Language-aligned 4D Gaussian Splatting](https://arxiv.org/abs/2510.02469)
Append: [Litespark Technical Report: High-Throughput, Energy-Efficient LLM Training Framework](https://arxiv.org/abs/2510.02483)
Append: [Beyond Imitation: Recovering Dense Rewards from Demonstrations](https://arxiv.org/abs/2510.02493)
Append: [How Confident are Video Models? Empowering Video Models to Express their Uncertainty](https://arxiv.org/abs/2510.02571)
Append: [On the Role of Temperature Sampling in Test-Time Scaling](https://arxiv.org/abs/2510.02611)
Append: [HyperAdaLoRA: Accelerating LoRA Rank Allocation During Training via Hypernetworks without Sacrificing Performance](https://arxiv.org/abs/2510.02630)
Append: [Less LLM, More Documents: Searching for Improved RAG](https://arxiv.org/abs/2510.02657)
Append: [Hyperparameter Loss Surfaces Are Simple Near their Optima](https://arxiv.org/abs/2510.02721)
Append: [A Granular Study of Safety Pretraining under Model Abliteration](https://arxiv.org/abs/2510.02768)
Append: [MaskCD: Mitigating LVLM Hallucinations by Image Head Masked Contrastive Decoding](https://arxiv.org/abs/2510.02790)
Append: [Pareto-optimal Non-uniform Language Generation](https://arxiv.org/abs/2510.02795)
Append: [NCV: A Node-Wise Consistency Verification Approach for Low-Cost Structured Error Localization in LLM Reasoning](https://arxiv.org/abs/2510.02816)
Append: [Beyond the Final Answer: Evaluating the Reasoning Trajectories of Tool-Augmented Agents](https://arxiv.org/abs/2510.02837)
Append: [When Names Disappear: Revealing What LLMs Actually Understand About Code](https://arxiv.org/abs/2510.03178)
Append: [Simulation to Rules: A Dual-VLM Framework for Formal Visual Planning](https://arxiv.org/abs/2510.03182)
Append: [Coevolutionary Continuous Discrete Diffusion: Make Your Diffusion Language Model a Latent Reasoner](https://arxiv.org/abs/2510.03206)
Append: [Low-probability Tokens Sustain Exploration in Reinforcement Learning with Verifiable Reward](https://arxiv.org/abs/2510.03222)
Append: [Did Translation Models Get More Robust Without Anyone Even Noticing?](https://arxiv.org/abs/2403.03923)
Append: [Understanding How CodeLLMs (Mis)Predict Types with Activation Steering](https://arxiv.org/abs/2404.01903)
Append: [ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference](https://arxiv.org/abs/2502.00299)
Append: [On the Diminishing Returns of Complex Robust RAG Training in the Era of Powerful LLMs](https://arxiv.org/abs/2502.11400)
Append: [Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent Attention in Any Transformer-based LLMs](https://arxiv.org/abs/2502.14837)
Append: [BottleHumor: Self-Informed Humor Explanation using the Information Bottleneck Principle](https://arxiv.org/abs/2502.18331)
Append: [L1: Controlling How Long A Reasoning Model Thinks With Reinforcement Learning](https://arxiv.org/abs/2503.04697)
Append: [DatawiseAgent: A Notebook-Centric LLM Agent Framework for Adaptive and Robust Data Science Automation](https://arxiv.org/abs/2503.07044)
Append: [From TOWER to SPIRE: Adding the Speech Modality to a Text-Only LLM](https://arxiv.org/abs/2503.10620)
Append: [Verbosity Tradeoffs and the Impact of Scale on the Faithfulness of LLM Self-Explanations](https://arxiv.org/abs/2503.13445)
Append: [Not a nuisance but a useful heuristic: Outlier dimensions favor frequent tokens in language models](https://arxiv.org/abs/2503.21718)
Append: [PropRAG: Guiding Retrieval with Beam Search over Proposition Paths](https://arxiv.org/abs/2504.18070)
Append: [Same evaluation, more tokens: On the effect of input length for machine translation evaluation using Large Language Models](https://arxiv.org/abs/2505.01761)
Append: [Pre-training Limited Memory Language Models with Internal and External Knowledge](https://arxiv.org/abs/2505.15962)
Append: [NeSyGeo: A Neuro-Symbolic Framework for Multimodal Geometric Reasoning Data Generation](https://arxiv.org/abs/2505.17121)
Append: [EconWebArena: Benchmarking Autonomous Agents on Economic Tasks in Realistic Web Environments](https://arxiv.org/abs/2506.08136)
Append: [Same Task, Different Circuits: Disentangling Modality-Specific Mechanisms in VLMs](https://arxiv.org/abs/2506.09047)
Append: [Did I Faithfully Say What I Thought? Bridging the Gap Between Neural Activity and Self-Explanations in Large Language Models](https://arxiv.org/abs/2506.09277)
Append: [Query-Level Uncertainty in Large Language Models](https://arxiv.org/abs/2506.09669)
Append: [When Large Language Models are Reliable for Judging Empathic Communication](https://arxiv.org/abs/2506.10150)
Append: [A Survey of Pun Generation: Datasets, Evaluations and Methodologies](https://arxiv.org/abs/2507.04793)
Append: [Triadic Multi-party Voice Activity Projection for Turn-taking in Spoken Dialogue Systems](https://arxiv.org/abs/2507.07518)
Append: [The Prompt Makes the Person(a): A Systematic Evaluation of Sociodemographic Persona Prompting for Large Language Models](https://arxiv.org/abs/2507.16076)
Append: [Quantum-RAG and PunGPT2: Advancing Low-Resource Language Generation and Retrieval for the Punjabi Language](https://arxiv.org/abs/2508.01918)
Append: [Better by Comparison: Retrieval-Augmented Contrastive Reasoning for Automatic Prompt Optimization](https://arxiv.org/abs/2509.02093)
Append: [Can Language Models Handle a Non-Gregorian Calendar?](https://arxiv.org/abs/2509.04432)
Append: [RephQA: Evaluating Readability of Large Language Models in Public Health Question Answering](https://arxiv.org/abs/2509.16360)
Append: [Exploiting Tree Structure for Credit Assignment in RL Training of LLMs](https://arxiv.org/abs/2509.18314)
Append: [When Long Helps Short: How Context Length in Supervised Fine-tuning Affects Behavior of Large Language Models](https://arxiv.org/abs/2509.18762)
Append: [RACCooN: A Versatile Instructional Video Editing Framework with Auto-Generated Narratives](https://arxiv.org/abs/2405.18406)
Append: [PrisonBreak: Jailbreaking Large Language Models with at Most Twenty-Five Targeted Bit-flips](https://arxiv.org/abs/2412.07192)
Append: [CBVLM: Training-free Explainable Concept-based Large Vision Language Models for Medical Image Classification](https://arxiv.org/abs/2501.12266)
Append: [MarketSenseAI 2.0: Enhancing Stock Analysis through LLM Agents](https://arxiv.org/abs/2502.00415)
Append: [Primus: A Pioneering Collection of Open-Source Datasets for Cybersecurity LLM Training](https://arxiv.org/abs/2502.11191)
Append: [Comparing Exploration-Exploitation Strategies of LLMs and Humans: Insights from Standard Multi-armed Bandit Experiments](https://arxiv.org/abs/2505.09901)
Append: [SelfBudgeter: Adaptive Token Allocation for Efficient LLM Reasoning](https://arxiv.org/abs/2505.11274)
Append: [Understanding the Performance Gap in Preference Learning: A Dichotomy of RLHF and DPO](https://arxiv.org/abs/2505.19770)
Append: [cAST: Enhancing Code Retrieval-Augmented Generation with Structural Chunking via Abstract Syntax Tree](https://arxiv.org/abs/2506.15655)
Append: [From Long Videos to Engaging Clips: A Human-Inspired Video Editing Framework with Multimodal Narrative Understanding](https://arxiv.org/abs/2507.02790)
Append: [Rubrics as Rewards: Reinforcement Learning Beyond Verifiable Domains](https://arxiv.org/abs/2507.17746)
Append: [FinAgentBench: A Benchmark Dataset for Agentic Retrieval in Financial Question Answering](https://arxiv.org/abs/2508.14052)
Append: [Unveiling Unicode's Unseen Underpinnings in Undermining Authorship Attribution](https://arxiv.org/abs/2508.15840)
Append: [THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning](https://arxiv.org/abs/2509.13761)
Append: [PRIME: Planning and Retrieval-Integrated Memory for Enhanced Reasoning](https://arxiv.org/abs/2509.22315)
append_entries: 143
Finish: 2025-10-06 04:24:12.182503
------------------------------------------------------
Started: 2025-10-06 06:25:33.607473
Existing_entries: 1143
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1370
Summarized using GPT-3.5-turbo
Append: [Leveraging Online Data to Enhance Medical Knowledge in a Small Persian Language Model](https://arxiv.org/abs/2505.16000)
append_entries: 1
Finish: 2025-10-06 06:25:36.221792
------------------------------------------------------
Started: 2025-10-06 08:22:04.765653
Existing_entries: 1001
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-06 08:22:05.119841
------------------------------------------------------
Started: 2025-10-06 10:18:12.764249
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-06 10:18:13.122669
------------------------------------------------------
Started: 2025-10-06 12:34:21.415692
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-06 12:34:21.739371
------------------------------------------------------
Started: 2025-10-06 14:16:12.089679
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-06 14:16:12.416454
------------------------------------------------------
Started: 2025-10-06 16:20:06.384996
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-06 16:20:06.735059
------------------------------------------------------
Started: 2025-10-06 18:24:25.153115
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-06 18:24:25.484416
------------------------------------------------------
Started: 2025-10-06 20:17:31.421616
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-06 20:17:31.782294
------------------------------------------------------
Started: 2025-10-06 22:14:16.394984
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-06 22:14:16.747102
------------------------------------------------------
Started: 2025-10-07 01:13:01.115760
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-07 01:13:01.549274
------------------------------------------------------
Started: 2025-10-07 02:55:16.111867
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-07 02:55:16.516427
------------------------------------------------------
Started: 2025-10-07 04:22:59.885866
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 848
Summarized using GPT-3.5-turbo
Append: [Decomposing Attention To Find Context-Sensitive Neurons](https://arxiv.org/abs/2510.03315)
Token length: 1599
Summarized using GPT-3.5-turbo
Append: [Graph-S3: Enhancing Agentic textual Graph Retrieval with Synthetic Stepwise Supervision](https://arxiv.org/abs/2510.03323)
Token length: 791
Summarized using GPT-3.5-turbo
Append: [Implicit Values Embedded in How Humans and LLMs Complete Subjective Everyday Tasks](https://arxiv.org/abs/2510.03384)
Token length: 844
Summarized using GPT-3.5-turbo
Append: [Morpheme Induction for Emergent Language](https://arxiv.org/abs/2510.03439)
Token length: 1144
Summarized using GPT-3.5-turbo
Append: [Omni-Embed-Nemotron: A Unified Multimodal Retrieval Model for Text, Image, Audio, and Video](https://arxiv.org/abs/2510.03458)
Token length: 871
Summarized using GPT-3.5-turbo
Append: [Searching for the Most Human-like Emergent Language](https://arxiv.org/abs/2510.03467)
Token length: 1164
Summarized using GPT-3.5-turbo
Append: [SEER: The Span-based Emotion Evidence Retrieval Benchmark](https://arxiv.org/abs/2510.03490)
Token length: 1504
Summarized using GPT-3.5-turbo
Append: [ALHD: A Large-Scale and Multigenre Benchmark Dataset for Arabic LLM-Generated Text Detection](https://arxiv.org/abs/2510.03502)
Token length: 1652
Summarized using GPT-3.5-turbo
Append: [TS-Reasoner: Aligning Time Series Foundation Models with LLM Reasoning](https://arxiv.org/abs/2510.03519)
Token length: 962
Summarized using GPT-3.5-turbo
Append: [Identifying Financial Risk Information Using RAG with a Contrastive Insight](https://arxiv.org/abs/2510.03521)
Token length: 1479
Summarized using GPT-3.5-turbo
Append: [Sample, Align, Synthesize: Graph-Based Response Synthesis with ConGrs](https://arxiv.org/abs/2510.03527)
Token length: 1070
Summarized using GPT-3.5-turbo
Append: [Fine-Tuning on Noisy Instructions: Effects on Generalization and Performance](https://arxiv.org/abs/2510.03528)
Token length: 1648
Summarized using GPT-3.5-turbo
Append: [TriMediQ: A Triplet-Structured Approach for Interactive Medical Question Answering](https://arxiv.org/abs/2510.03536)
Token length: 920
Summarized using GPT-3.5-turbo
Append: [What is a protest anyway? Codebook conceptualization is still a first-order concern in LLM-era classification](https://arxiv.org/abs/2510.03541)
Token length: 1968
Summarized using GPT-3.5-turbo
Append: [CCD-Bench: Probing Cultural Conflict in Large Language Model Decision-Making](https://arxiv.org/abs/2510.03553)
Token length: 1822
Summarized using GPT-3.5-turbo
Append: [Reactive Transformer (RxT) -- Stateful Real-Time Processing for Event-Driven Reactive Language Models](https://arxiv.org/abs/2510.03561)
Token length: 1018
Summarized using GPT-3.5-turbo
Append: [LLM, Reporting In! Medical Information Extraction Across Prompting, Fine-tuning and Post-correction](https://arxiv.org/abs/2510.03577)
Token length: 1691
Summarized using GPT-3.5-turbo
Append: [Decoupling Task-Solving and Output Formatting in LLM Generation](https://arxiv.org/abs/2510.03595)
Token length: 1580
Summarized using GPT-3.5-turbo
Append: [Can an LLM Induce a Graph? Investigating Memory Drift and Context Length](https://arxiv.org/abs/2510.03611)
Token length: 983
Summarized using GPT-3.5-turbo
Append: [Towards Unsupervised Speech Recognition at the Syllable-Level](https://arxiv.org/abs/2510.03639)
Token length: 1592
Summarized using GPT-3.5-turbo
Append: [UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG](https://arxiv.org/abs/2510.03663)
Token length: 1536
Summarized using GPT-3.5-turbo
Append: [Fine-Tuning Large Language Models with QLoRA for Offensive Language Detection in Roman Urdu-English Code-Mixed Text](https://arxiv.org/abs/2510.03683)
Token length: 1510
Summarized using GPT-3.5-turbo
Append: [MedReflect: Teaching Medical LLMs to Self-Improve via Reflective Correction](https://arxiv.org/abs/2510.03687)
Token length: 966
Summarized using GPT-3.5-turbo
Append: [TreePrompt: Leveraging Hierarchical Few-Shot Example Selection for Improved English-Persian and English-German Translation](https://arxiv.org/abs/2510.03748)
Token length: 1238
Summarized using GPT-3.5-turbo
Append: [Cross-Lingual Multi-Granularity Framework for Interpretable Parkinson's Disease Diagnosis from Speech](https://arxiv.org/abs/2510.03758)
Token length: 1091
Summarized using GPT-3.5-turbo
Append: [Prompt Balance Matters: Understanding How Imbalanced Few-Shot Learning Affects Multilingual Sense Disambiguation in LLMs](https://arxiv.org/abs/2510.03762)
Token length: 1636
Summarized using GPT-3.5-turbo
Append: [Rezwan: Leveraging Large Language Models for Comprehensive Hadith Text Processing: A 1.2M Corpus Development](https://arxiv.org/abs/2510.03781)
Token length: 688
Summarized using GPT-3.5-turbo
Append: [Mechanistic Interpretability of Socio-Political Frames in Language Models](https://arxiv.org/abs/2510.03799)
Token length: 1316
Summarized using GPT-3.5-turbo
Append: [Beyond Token Length: Step Pruner for Efficient and Accurate Reasoning in Large Language Models](https://arxiv.org/abs/2510.03805)
Token length: 971
Summarized using GPT-3.5-turbo
Append: [Annotate Rhetorical Relations with INCEpTION: A Comparison with Automatic Approaches](https://arxiv.org/abs/2510.03808)
Token length: 1259
Summarized using GPT-3.5-turbo
Append: [Read Between the Lines: A Benchmark for Uncovering Political Bias in Bangla News Articles](https://arxiv.org/abs/2510.03898)
Token length: 1812
Summarized using GPT-3.5-turbo
Append: [PsycholexTherapy: Simulating Reasoning in Psychotherapy with Small Language Models in Persian](https://arxiv.org/abs/2510.03913)
Token length: 1463
Summarized using GPT-3.5-turbo
Append: [Mapping Patient-Perceived Physician Traits from Nationwide Online Reviews with LLMs](https://arxiv.org/abs/2510.03997)
Token length: 1374
Summarized using GPT-3.5-turbo
Append: [Simulating and Understanding Deceptive Behaviors in Long-Horizon Interactions](https://arxiv.org/abs/2510.03999)
Token length: 1186
Summarized using GPT-3.5-turbo
Append: [Named Entity Recognition in COVID-19 tweets with Entity Knowledge Augmentation](https://arxiv.org/abs/2510.04001)
Token length: 1675
Summarized using GPT-3.5-turbo
Append: [AgriGPT-VL: Agricultural Vision-Language Understanding Suite](https://arxiv.org/abs/2510.04002)
Token length: 1363
Summarized using GPT-3.5-turbo
Append: [LLM Microscope: What Model Internals Reveal About Answer Correctness and Context Utilization](https://arxiv.org/abs/2510.04013)
Token length: 965
Summarized using GPT-3.5-turbo
Append: [Thai Semantic End-of-Turn Detection for Real-Time Voice Agents](https://arxiv.org/abs/2510.04016)
Token length: 832
Summarized using GPT-3.5-turbo
Append: [Does Using Counterfactual Help LLMs Explain Textual Importance in Classification?](https://arxiv.org/abs/2510.04031)
Token length: 1549
Summarized using GPT-3.5-turbo
Append: [Small Language Models for Emergency Departments Decision Support: A Benchmark Study](https://arxiv.org/abs/2510.04032)
Token length: 1033
Summarized using GPT-3.5-turbo
Append: [Exploring Chain-of-Thought Reasoning for Steerable Pluralistic Alignment](https://arxiv.org/abs/2510.04045)
Token length: 663
Summarized using GPT-3.5-turbo
Append: [What Makes Diffusion Language Models Super Data Learners?](https://arxiv.org/abs/2510.04071)
Token length: 1910
Summarized using GPT-3.5-turbo
Append: [PoLi-RL: A Point-to-List Reinforcement Learning Framework for Conditional Semantic Textual Similarity](https://arxiv.org/abs/2510.04080)
Token length: 1909
Summarized using GPT-3.5-turbo
Append: [Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning](https://arxiv.org/abs/2510.04081)
Token length: 1234
Summarized using GPT-3.5-turbo
Append: [Unveiling LLMs' Metaphorical Understanding: Exploring Conceptual Irrelevance, Context Leveraging and Syntactic Influence](https://arxiv.org/abs/2510.04120)
Token length: 740
Summarized using GPT-3.5-turbo
Append: [Sri Lanka Document Datasets: A Large-Scale, Multilingual Resource for Law, News, and Policy (v20251005)](https://arxiv.org/abs/2510.04124)
Token length: 593
Summarized using GPT-3.5-turbo
Append: [Fine Tuning Methods for Low-resource Languages](https://arxiv.org/abs/2510.04139)
Token length: 1445
Summarized using GPT-3.5-turbo
Append: [Self Speculative Decoding for Diffusion Large Language Models](https://arxiv.org/abs/2510.04147)
Token length: 1403
Summarized using GPT-3.5-turbo
Append: [Thinking on the Fly: Test-Time Reasoning Enhancement via Latent Thought Policy Optimization](https://arxiv.org/abs/2510.04182)
Token length: 1692
Summarized using GPT-3.5-turbo
Append: [CALM Before the STORM: Unlocking Native Reasoning for Optimization Modeling](https://arxiv.org/abs/2510.04204)
Append: [Teaching LLM to be Persuasive: Reward-Enhanced Policy Optimization for Alignment frm Heterogeneous Rewards](https://arxiv.org/abs/2510.04214)
Append: [Epistemic Diversity and Knowledge Collapse in Large Language Models](https://arxiv.org/abs/2510.04226)
Append: [Pushing on Multilingual Reasoning Models with Language-Mixed Chain-of-Thought](https://arxiv.org/abs/2510.04230)
Append: [LongTail-Swap: benchmarking language models' abilities on rare words](https://arxiv.org/abs/2510.04268)
Append: [Probing Geometry of Next Token Prediction Using Cumulant Expansion of the Softmax Entropy](https://arxiv.org/abs/2510.04285)
Append: [SliceMoE: Routing Embedding Slices Instead of Tokens for Fine-Grained and Balanced Transformer Scaling](https://arxiv.org/abs/2510.04286)
Append: [PABSA: Hybrid Framework for Persian Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2510.04291)
Append: [Equipping Retrieval-Augmented Large Language Models with Document Structure Awareness](https://arxiv.org/abs/2510.04293)
Append: [Measuring Language Model Hallucinations Through Distributional Correctness](https://arxiv.org/abs/2510.04302)
Append: [Read the Scene, Not the Script: Outcome-Aware Safety for LLMs](https://arxiv.org/abs/2510.04320)
Append: [Evaluation of Clinical Trials Reporting Quality using Large Language Models](https://arxiv.org/abs/2510.04338)
Append: [Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time](https://arxiv.org/abs/2510.04340)
Append: [Unmasking Backdoors: An Explainable Defense via Gradient-Attention Anomaly Scoring for Pre-trained Language Models](https://arxiv.org/abs/2510.04347)
Append: [Improving Consistency in Retrieval-Augmented Systems with Group Similarity Rewards](https://arxiv.org/abs/2510.04392)
Append: [Time Is Effort: Estimating Human Post-Editing Time for Grammar Error Correction Tool Evaluation](https://arxiv.org/abs/2510.04394)
Append: [SECA: Semantically Equivalent and Coherent Attacks for Eliciting LLM Hallucinations](https://arxiv.org/abs/2510.04398)
Append: [Large Language Models Preserve Semantic Isotopies in Story Continuations](https://arxiv.org/abs/2510.04400)
Append: [Good Intentions Beyond ACL: Who Does NLP for Social Good, and Where?](https://arxiv.org/abs/2510.04434)
Append: [On the Role of Unobserved Sequences on Sample-based Uncertainty Quantification for LLMs](https://arxiv.org/abs/2510.04439)
Append: [Mitigating Forgetting Between Supervised and Reinforcement Learning Yields Stronger Reasoners](https://arxiv.org/abs/2510.04454)
Append: [Compressed Convolutional Attention: Efficient Attention in a Compressed Latent Space](https://arxiv.org/abs/2510.04476)
Append: [Psychological Steering in LLMs: An Evaluation of Effectiveness and Trustworthiness](https://arxiv.org/abs/2510.04484)
Append: [GenQuest: An LLM-based Text Adventure Game for Language Learners](https://arxiv.org/abs/2510.04498)
Append: [GRACE: Generative Representation Learning via Contrastive Policy Optimization](https://arxiv.org/abs/2510.04506)
Append: [Fine-grained auxiliary learning for real-world product recommendation](https://arxiv.org/abs/2510.04551)
Append: [Can LLMs Detect Ambiguous Plural Reference? An Analysis of Split-Antecedent and Mereological Reference](https://arxiv.org/abs/2510.04581)
Append: [Robustness assessment of large audio language models in multiple-choice evaluation](https://arxiv.org/abs/2510.04584)
Append: [FedSRD: Sparsify-Reconstruct-Decompose for Communication-Efficient Federated Large Language Models Fine-Tuning](https://arxiv.org/abs/2510.04601)
Append: [Contrastive Learning Using Graph Embeddings for Domain Adaptation of Language Models in the Process Industry](https://arxiv.org/abs/2510.04631)
Append: [Evaluating LLMs for Demographic-Targeted Social Bias Detection: A Comprehensive Benchmark Study](https://arxiv.org/abs/2510.04641)
Append: [FT-MDT: Extracting Decision Trees from Medical Texts via a Novel Low-rank Adaptation Method](https://arxiv.org/abs/2510.04655)
Append: [FocusMed: A Large Language Model-based Framework for Enhancing Medical Question Summarization with Focus Identification](https://arxiv.org/abs/2510.04671)
Append: [Multi-Agent Tool-Integrated Policy Optimization](https://arxiv.org/abs/2510.04678)
Append: [TiTok: Transfer Token-level Knowledge via Contrastive Excess to Transplant LoRA](https://arxiv.org/abs/2510.04682)
Append: [Multilingual Routing in Mixture-of-Experts](https://arxiv.org/abs/2510.04694)
Append: [JSON Whisperer: Efficient JSON Editing with LLMs](https://arxiv.org/abs/2510.04717)
Append: [A Low-Resource Speech-Driven NLP Pipeline for Sinhala Dyslexia Assistance](https://arxiv.org/abs/2510.04750)
Append: [ModernBERT + ColBERT: Enhancing biomedical RAG through an advanced re-ranking retriever](https://arxiv.org/abs/2510.04757)
Append: [Are BabyLMs Deaf to Gricean Maxims? A Pragmatic Evaluation of Sample-efficient Language Models](https://arxiv.org/abs/2510.04764)
Append: [Hybrid Architectures for Language Models: Systematic Analysis and Design Insights](https://arxiv.org/abs/2510.04800)
Append: [How I Built ASR for Endangered Languages with a Spoken Dictionary](https://arxiv.org/abs/2510.04832)
Append: [Instability in Downstream Task Performance During LLM Pretraining](https://arxiv.org/abs/2510.04848)
Append: [When Models Lie, We Learn: Multilingual Span-Level Hallucination Detection with PsiloQA](https://arxiv.org/abs/2510.04849)
Append: [Detecting Distillation Data from Reasoning Models](https://arxiv.org/abs/2510.04850)
Append: [SocialHarmBench: Revealing LLM Vulnerabilities to Socially Harmful Requests](https://arxiv.org/abs/2510.04891)
Append: [Do LLMs Align with My Task? Evaluating Text-to-SQL via Dataset Alignment](https://arxiv.org/abs/2510.04919)
Append: [The Geometry of Truth: Layer-wise Semantic Dynamics for Hallucination Detection in Large Language Models](https://arxiv.org/abs/2510.04933)
Append: [A First Context-Free Grammar Applied to Nawatl Corpora Augmentation](https://arxiv.org/abs/2510.04945)
Append: [Mind Your Tone: Investigating How Prompt Politeness Affects LLM Accuracy (short paper)](https://arxiv.org/abs/2510.04950)
Append: [AWARE, Beyond Sentence Boundaries: A Contextual Transformer Framework for Identifying Cultural Capital in STEM Narratives](https://arxiv.org/abs/2510.04983)
Append: [Resource-Efficient Fine-Tuning of LLaMA-3.2-3B for Medical Chain-of-Thought Reasoning](https://arxiv.org/abs/2510.05003)
Append: [Imperceptible Jailbreaking against Large Language Models](https://arxiv.org/abs/2510.05025)
Append: [A Set of Quebec-French Corpus of Regional Expressions and Terms](https://arxiv.org/abs/2510.05026)
Append: [Guided Query Refinement: Multimodal Hybrid Retrieval with Test-Time Optimization](https://arxiv.org/abs/2510.05038)
Append: [COLE: a Comprehensive Benchmark for French Language Understanding Evaluation](https://arxiv.org/abs/2510.05046)
Append: [SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior Reasoning LLMs](https://arxiv.org/abs/2510.05069)
Append: [Slm-mux: Orchestrating small language models for reasoning](https://arxiv.org/abs/2510.05077)
Append: [TeachLM: Post-Training LLMs for Education Using Authentic Learning Data](https://arxiv.org/abs/2510.05087)
Append: [Finish First, Perfect Later: Test-Time Token-Level Cross-Validation for Diffusion Large Language Models](https://arxiv.org/abs/2510.05090)
Append: [CAG: Chunked Augmented Generation for Google Chrome's Built-in Gemini Nano](https://arxiv.org/abs/2412.18708)
Append: [General Exploratory Bonus for Optimistic Exploration in RLHF](https://arxiv.org/abs/2510.03269)
Append: [MemMamba: Rethinking Memory Patterns in State Space Model](https://arxiv.org/abs/2510.03279)
Append: [Training Optimal Large Diffusion Language Models](https://arxiv.org/abs/2510.03280)
Append: [Discovering Transformer Circuits via a Hybrid Attribution and Pruning Framework](https://arxiv.org/abs/2510.03282)
Append: [MACE: A Hybrid LLM Serving System with Colocated SLO-aware Continuous Retraining Alignment](https://arxiv.org/abs/2510.03283)
Append: [Why mask diffusion does not work](https://arxiv.org/abs/2510.03289)
Append: [Multimodal Arabic Captioning with Interpretable Visual Concept Integration](https://arxiv.org/abs/2510.03295)
Append: [CAFL-L: Constraint-Aware Federated Learning with Lagrangian Dual Optimization for On-Device Language Models](https://arxiv.org/abs/2510.03298)
Append: [AgentCaster: Reasoning-Guided Tornado Forecasting](https://arxiv.org/abs/2510.03349)
Append: [Lightweight Prompt Engineering for Cognitive Alignment in Educational AI: A OneClickQuiz Case Study](https://arxiv.org/abs/2510.03374)
Append: [Studying the Korean Word-Chain Game with RLVR:Mitigating Reward Conflicts via Curriculum Learning](https://arxiv.org/abs/2510.03394)
Append: [Know Thyself? On the Incapability and Implications of AI Self-Recognition](https://arxiv.org/abs/2510.03399)
Append: [PLSEMANTICSBENCH: Large Language Models As Programming Language Interpreters](https://arxiv.org/abs/2510.03415)
Append: [Consistent Kernel Change-Point Detection under m-Dependence for Text Segmentation](https://arxiv.org/abs/2510.03437)
Append: [Red Lines and Grey Zones in the Fog of War: Benchmarking Legal Risk, Moral Harm, and Regional Bias in Large Language Model Military Decision-Making](https://arxiv.org/abs/2510.03514)
Append: [Machine Unlearning Meets Adversarial Robustness via Constrained Interventions on LLMs](https://arxiv.org/abs/2510.03567)
Append: [From Theory to Practice: Evaluating Data Poisoning Attacks and Defenses in In-Context Learning on Social Media Health Discourse](https://arxiv.org/abs/2510.03636)
Append: [Does higher interpretability imply better utility? A Pairwise Analysis on Sparse Autoencoders](https://arxiv.org/abs/2510.03659)
Append: [Token Hidden Reward: Steering Exploration-Exploitation in Group Relative Deep Reinforcement Learning](https://arxiv.org/abs/2510.03669)
Append: [Mind the Goal: Data-Efficient Goal-Oriented Evaluation of Conversational Agents and Chatbots using Teacher Models](https://arxiv.org/abs/2510.03696)
Append: [Person-Centric Annotations of LAION-400M: Auditing Bias and Its Transfer to Models](https://arxiv.org/abs/2510.03721)
Append: [Adapting Diarization-Conditioned Whisper for End-to-End Multi-Talker Speech Recognition](https://arxiv.org/abs/2510.03723)
Append: [Bridging the Gap Between Multimodal Foundation Models and World Models](https://arxiv.org/abs/2510.03727)
Append: [Optimizing Fine-Tuning through Advanced Initialization Strategies for Low-Rank Adaptation](https://arxiv.org/abs/2510.03731)
Append: [Investigating LLM Variability in Personalized Conversational Information Retrieval](https://arxiv.org/abs/2510.03795)
Append: [Unlocking Reasoning Capabilities in LLMs via Reinforcement Learning Exploration](https://arxiv.org/abs/2510.03865)
Append: [Kantian-Utilitarian XAI: Meta-Explained](https://arxiv.org/abs/2510.03892)
Append: [LLM Chemistry Estimation for Multi-LLM Recommendation](https://arxiv.org/abs/2510.03930)
Append: [No Tokens Wasted: Leveraging Long Context in Biomedical Vision-Language Models](https://arxiv.org/abs/2510.03978)
Append: [Enhancing OCR for Sino-Vietnamese Language Processing via Fine-tuned PaddleOCRv5](https://arxiv.org/abs/2510.04003)
Append: [What Shapes a Creative Machine Mind? Comprehensively Benchmarking Creativity in Foundation Models](https://arxiv.org/abs/2510.04009)
Append: [Visual Lifelog Retrieval through Captioning-Enhanced Interpretation](https://arxiv.org/abs/2510.04010)
Append: [Principled and Tractable RL for Reasoning with Diffusion Language Models](https://arxiv.org/abs/2510.04019)
Append: [LLM-Based Data Science Agents: A Survey of Capabilities, Challenges, and Future Directions](https://arxiv.org/abs/2510.04023)
Append: [What Scales in Cross-Entropy Scaling Law?](https://arxiv.org/abs/2510.04067)
Append: [Slow-Fast Policy Optimization: Reposition-Before-Update for LLM Reasoning](https://arxiv.org/abs/2510.04072)
Append: [Internal states before wait modulate reasoning patterns](https://arxiv.org/abs/2510.04128)
Append: [Selective Expert Guidance for Effective and Diverse Exploration in Reinforcement Learning of LLMs](https://arxiv.org/abs/2510.04140)
Append: [Automating construction safety inspections using a multi-modal vision-language RAG framework](https://arxiv.org/abs/2510.04145)
Append: [Beyond Next-Token Prediction: A Performance Characterization of Diffusion versus Autoregressive Language Models](https://arxiv.org/abs/2510.04146)
Append: [Zoom-In to Sort AI-Generated Images Out](https://arxiv.org/abs/2510.04225)
Append: [Don't Pass$\mathtt{@}k$: A Bayesian Framework for Large Language Model Evaluation](https://arxiv.org/abs/2510.04265)
Append: [Wave-PDE Nets: Trainable Wave-Equation Layers as an Alternative to Attention](https://arxiv.org/abs/2510.04304)
Append: [MacroBench: A Novel Testbed for Web Automation Scripts via Large Language Models](https://arxiv.org/abs/2510.04363)
Append: [MorphoSim: An Interactive, Controllable, and Editable Language-guided 4D World Simulator](https://arxiv.org/abs/2510.04390)
Append: [Internal World Models as Imagination Networks in Cognitive Agents](https://arxiv.org/abs/2510.04391)
Append: [Partial Information Decomposition via Normalizing Flows in Latent Gaussian Distributions](https://arxiv.org/abs/2510.04417)
Append: [MedCLM: Learning to Localize and Reason via a CoT-Curriculum in Medical Vision-Language Models](https://arxiv.org/abs/2510.04477)
Append: [Impatient Users Confuse AI Agents: High-fidelity Simulations of Human Traits for Testing Agents](https://arxiv.org/abs/2510.04491)
Append: [P2P: A Poison-to-Poison Remedy for Reliable Backdoor Defense in LLMs](https://arxiv.org/abs/2510.04503)
Append: [ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering](https://arxiv.org/abs/2510.04514)
Append: [More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models](https://arxiv.org/abs/2510.04532)
Append: [LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning](https://arxiv.org/abs/2510.04573)
Append: [Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models](https://arxiv.org/abs/2510.04618)
Append: [AtomWorld: A Benchmark for Evaluating Spatial Reasoning in Large Language Models on Crystalline Materials](https://arxiv.org/abs/2510.04704)
Append: [BrokenMath: A Benchmark for Sycophancy in Theorem Proving with LLMs](https://arxiv.org/abs/2510.04721)
Append: [Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba](https://arxiv.org/abs/2510.04738)
Append: [Visual Representations inside the Language Model](https://arxiv.org/abs/2510.04819)
Append: [Retrieval-Augmented Code Generation: A Survey with Focus on Repository-Level Approaches](https://arxiv.org/abs/2510.04905)
Append: [MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.04935)
Append: [ONNX-Net: Towards Universal Representations and Instant Performance Prediction for Neural Architectures](https://arxiv.org/abs/2510.04938)
Append: [On Structured State-Space Duality](https://arxiv.org/abs/2510.04944)
Append: [LLM-Hanabi: Evaluating Multi-Agent Gameplays with Theory-of-Mind and Rationale Inference in Imperfect Information Collaboration Game](https://arxiv.org/abs/2510.04980)
Append: [Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training](https://arxiv.org/abs/2510.04996)
Append: [Large Language Models Achieve Gold Medal Performance at International Astronomy & Astrophysics Olympiad](https://arxiv.org/abs/2510.05016)
Append: [Proactive defense against LLM Jailbreak](https://arxiv.org/abs/2510.05052)
Append: [Learning to Interpret Weight Differences in Language Models](https://arxiv.org/abs/2510.05092)
Append: [From Noisy Traces to Stable Gradients: Bias-Variance Optimized Preference Optimization for Aligning Large Reasoning Models](https://arxiv.org/abs/2510.05095)
Append: [Paper2Video: Automatic Video Generation from Scientific Papers](https://arxiv.org/abs/2510.05096)
Append: [Semantic Journeys: Quantifying Change in Emoji Meaning from 2012-2018](https://arxiv.org/abs/2105.00846)
Append: [Understanding Retrieval Augmentation for Long-Form Question Answering](https://arxiv.org/abs/2310.12150)
Append: [Can GPT models Follow Human Summarization Guidelines? A Study for Targeted Communication Goals](https://arxiv.org/abs/2310.16810)
Append: [When "Competency" in Reasoning Opens the Door to Vulnerability: Jailbreaking LLMs via Novel Complex Ciphers](https://arxiv.org/abs/2402.10601)
Append: [Rowen: Adaptive Retrieval-Augmented Generation for Hallucination Mitigation in LLMs](https://arxiv.org/abs/2402.10612)
Append: [RealKIE: Five Novel Datasets for Enterprise Key Information Extraction](https://arxiv.org/abs/2403.20101)
Append: [ClinicRealm: Re-evaluating Large Language Models with Conventional Machine Learning for Non-Generative Clinical Prediction Tasks](https://arxiv.org/abs/2407.18525)
Append: [Enhancing Unsupervised Sentence Embeddings via Knowledge-Driven Data Augmentation and Gaussian-Decayed Contrastive Learning](https://arxiv.org/abs/2409.12887)
Append: [Insights from the Inverse: Reconstructing LLM Training Goals Through Inverse Reinforcement Learning](https://arxiv.org/abs/2410.12491)
Append: [Don't Pay Attention, PLANT It: Pretraining Attention via Learning-to-Rank](https://arxiv.org/abs/2410.23066)
Append: [Arena-Lite: Efficient and Reliable Large Language Model Evaluation via Tournament-Based Direct Comparisons](https://arxiv.org/abs/2411.01281)
Append: [Geometry of orofacial neuromuscular signals: speech articulation decoding using surface electromyography](https://arxiv.org/abs/2411.02591)
Append: [H3Fusion: Helpful, Harmless, Honest Fusion of Aligned LLMs](https://arxiv.org/abs/2411.17792)
Append: [HP-BERT: A framework for longitudinal study of Hinduphobia on social media via language models](https://arxiv.org/abs/2501.05482)
Append: [Unlocking In-Context Learning for Natural Datasets Beyond Language Modelling](https://arxiv.org/abs/2501.06256)
Append: [Longitudinal Abuse and Sentiment Analysis of Hollywood Movie Dialogues using Language Models](https://arxiv.org/abs/2501.13948)
Append: [Improving Low-Resource Sequence Labeling with Knowledge Fusion and Contextual Label Explanations](https://arxiv.org/abs/2501.19093)
Append: [Summaries as Centroids for Interpretable and Scalable Text Clustering](https://arxiv.org/abs/2502.09667)
Append: [DiffSampling: Enhancing Diversity and Accuracy in Neural Text Generation](https://arxiv.org/abs/2502.14037)
Append: [Char-mander Use mBackdoor! A Study of Cross-lingual Backdoor Attacks in Multilingual LLMs](https://arxiv.org/abs/2502.16901)
Append: [League: Leaderboard Generation on Demand](https://arxiv.org/abs/2502.18209)
Append: [On Pruning State-Space LLMs](https://arxiv.org/abs/2502.18886)
Append: [Can Large Language Models Outperform Non-Experts in Poetry Evaluation? A Comparative Study Using the Consensual Assessment Technique](https://arxiv.org/abs/2502.19064)
Append: [SemViQA: A Semantic Question Answering System for Vietnamese Information Fact-Checking](https://arxiv.org/abs/2503.00955)
Append: [Evolutionary Guided Decoding: Iterative Value Refinement for LLMs](https://arxiv.org/abs/2503.02368)
Append: [ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient Long-Context LLMs](https://arxiv.org/abs/2503.10714)
Append: [Praxis-VLM: Vision-Grounded Decision Making via Text-Driven Reinforcement Learning](https://arxiv.org/abs/2503.16965)
Append: [Scaling Laws of Synthetic Data for Language Models](https://arxiv.org/abs/2503.19551)
Append: [RAG over Tables: Hierarchical Memory Index, Multi-Stage Retrieval, and Benchmarking](https://arxiv.org/abs/2504.01346)
Append: [Testing Low-Resource Language Support in LLMs Using Language Proficiency Exams: the Case of Luxembourgish](https://arxiv.org/abs/2504.01667)
Append: [Multilingual Retrieval-Augmented Generation for Knowledge-Intensive Task](https://arxiv.org/abs/2504.03616)
Append: [AgentAda: Skill-Adaptive Data Analytics for Tailored Insight Discovery](https://arxiv.org/abs/2504.07421)
Append: [Forecasting Conversation Derailments Through Generation](https://arxiv.org/abs/2504.08905)
Append: [Deliberate Planning in Language Models with Symbolic Representation](https://arxiv.org/abs/2505.01479)
Append: [SCAN: Structured Capability Assessment and Navigation for LLMs](https://arxiv.org/abs/2505.06698)
Append: [J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning](https://arxiv.org/abs/2505.10320)
Append: [DACL-RAG: Data Augmentation Strategy with Curriculum Learning for Retrieval-Augmented Generation](https://arxiv.org/abs/2505.10493)
Append: [Self-GIVE: Associative Thinking from Limited Structured Knowledge for Enhanced Large Language Model Reasoning](https://arxiv.org/abs/2505.15062)
Append: [TACO: Enhancing Multimodal In-context Learning via Task Mapping-Guided Sequence Configuration](https://arxiv.org/abs/2505.17098)
Append: [From Compression to Expression: A Layerwise Analysis of In-Context Learning](https://arxiv.org/abs/2505.17322)
Append: [From Word to World: Evaluate and Mitigate Culture Bias in LLMs via Word Association Test](https://arxiv.org/abs/2505.18562)
Append: [Social Good or Scientific Curiosity? Uncovering the Research Framing Behind NLP Artefacts](https://arxiv.org/abs/2505.18677)
Append: [StressTest: Can YOUR Speech LM Handle the Stress?](https://arxiv.org/abs/2505.22765)
Append: [MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain Dialogue Evaluators](https://arxiv.org/abs/2505.22777)
Append: [What Has Been Lost with Synthetic Evaluation?](https://arxiv.org/abs/2505.22830)
Append: [LegalSearchLM: Rethinking Legal Case Retrieval as Legal Elements Generation](https://arxiv.org/abs/2505.23832)
Append: [Improve MLLM Benchmark Efficiency through Interview](https://arxiv.org/abs/2506.00883)
Append: [SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware Reinforcement Learning](https://arxiv.org/abs/2506.01713)
Append: [MedAgentGym: A Scalable Agentic Training Environment for Code-Centric Reasoning in Biomedical Data Science](https://arxiv.org/abs/2506.04405)
Append: [SSA-COMET: Do LLMs Outperform Learned Metrics in Evaluating MT for Under-Resourced African Languages?](https://arxiv.org/abs/2506.04557)
Append: [Micro-Act: Mitigating Knowledge Conflict in LLM-based RAG via Actionable Self-Reasoning](https://arxiv.org/abs/2506.05278)
Append: [Infini-gram mini: Exact n-gram Search at the Internet Scale with FM-Index](https://arxiv.org/abs/2506.12229)
Append: [LexiMark: Robust Watermarking via Lexical Substitutions to Enhance Membership Verification of an LLM's Textual Training Data](https://arxiv.org/abs/2506.14474)
Append: [Using cognitive models to reveal value trade-offs in language models](https://arxiv.org/abs/2506.20666)
Append: [Self-Correction Bench: Uncovering and Addressing the Self-Correction Blind Spot in Large Language Models](https://arxiv.org/abs/2507.02778)
Append: [Empowering Healthcare Practitioners with Language Models: Structuring Speech Transcripts in Two Real-World Clinical Applications](https://arxiv.org/abs/2507.05517)
Append: [Psychometric Item Validation Using Virtual Respondents with Trait-Response Mediators](https://arxiv.org/abs/2507.05890)
Append: [MapIQ: Evaluating Multimodal Large Language Models for Map Question Answering](https://arxiv.org/abs/2507.11625)
Append: [WakenLLM: Evaluating Reasoning Potential and Stability in LLMs via Fine-Grained Benchmarking](https://arxiv.org/abs/2507.16199)
Append: [Towards Enforcing Company Policy Adherence in Agentic Workflows](https://arxiv.org/abs/2507.16459)
Append: [MAGIC: A Multi-Hop and Graph-Based Benchmark for Inter-Context Conflicts in Retrieval-Augmented Generation](https://arxiv.org/abs/2507.21544)
Append: [Adversarial Defence without Adversarial Defence: Enhancing Language Model Robustness via Instance-level Principal Component Removal](https://arxiv.org/abs/2507.21750)
Append: [C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring Challenges in Complex Conversations](https://arxiv.org/abs/2507.22968)
Append: [User Feedback in Human-LLM Dialogues: A Lens to Understand Users But Noisy as a Learning Signal](https://arxiv.org/abs/2507.23158)
Append: [XAutoLM: Efficient Fine-Tuning of Language Models via Meta-Learning and AutoML](https://arxiv.org/abs/2508.00924)
Append: [LPI-RIT at LeWiDi-2025: Improving Distributional Predictions via Metadata and Loss Reweighting with DisCo](https://arxiv.org/abs/2508.08163)
Append: [Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language Models](https://arxiv.org/abs/2508.09138)
Append: [Flow-SLM: Joint Learning of Linguistic and Acoustic Information for Spoken Language Modeling](https://arxiv.org/abs/2508.09350)
Append: [A Stitch in Time Saves Nine: Proactive Self-Refinement for Language Models](https://arxiv.org/abs/2508.12903)
Append: [AutoBnB-RAG: Enhancing Multi-Agent Incident Response with Retrieval-Augmented Generation](https://arxiv.org/abs/2508.13118)
Append: [OptimalThinkingBench: Evaluating Over and Underthinking in LLMs](https://arxiv.org/abs/2508.13141)
Append: [SurGE: A Benchmark and Evaluation Framework for Scientific Survey Generation](https://arxiv.org/abs/2508.15658)
Append: [OpenWHO: A Document-Level Parallel Corpus for Health Translation in Low-Resource Languages](https://arxiv.org/abs/2508.16048)
Append: [ObjexMT: Objective Extraction and Metacognitive Calibration for LLM-as-a-Judge under Multi-Turn Jailbreaks](https://arxiv.org/abs/2508.16889)
Append: [SSFO: Self-Supervised Faithfulness Optimization for Retrieval-Augmented Generation](https://arxiv.org/abs/2508.17225)
Append: [Filtering for Creativity: Adaptive Prompting for Multilingual Riddle Generation in LLMs](https://arxiv.org/abs/2508.18709)
Append: [Meta-Pretraining for Zero-Shot Cross-Lingual Named Entity Recognition in Low-Resource Philippine Languages](https://arxiv.org/abs/2509.02160)
Append: [Post-training Large Language Models for Diverse High-Quality Responses](https://arxiv.org/abs/2509.04784)
Append: [X-Teaming Evolutionary M2S: Automated Discovery of Multi-turn to Single-turn Jailbreak Templates](https://arxiv.org/abs/2509.08729)
Append: [Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs for Text Annotation](https://arxiv.org/abs/2509.08825)
Append: [Population-Aligned Persona Generation for LLM-based Social Simulation](https://arxiv.org/abs/2509.10127)
Append: [CEMTM: Contextual Embedding-based Multimodal Topic Modeling](https://arxiv.org/abs/2509.11465)
Append: [Analyzing Information-Seeking Behaviors in a Hakka AI Chatbot: A Cognitive-Pragmatic Study](https://arxiv.org/abs/2509.11591)
Append: [Fun-ASR Technical Report](https://arxiv.org/abs/2509.12508)
Append: [Reasoning over Boundaries: Enhancing Specification Alignment via Test-time Deliberation](https://arxiv.org/abs/2509.14760)
Append: [Rethinking the Role of Text Complexity in Language Model Pretraining](https://arxiv.org/abs/2509.16551)
Append: [Instruction Boundary: Quantifying Biases in LLM Reasoning under Various Coverage](https://arxiv.org/abs/2509.20278)
Append: [COSPADI: Compressing LLMs via Calibration-Guided Sparse Dictionary Learning](https://arxiv.org/abs/2509.22075)
Append: [AgentBench: Evaluating LLMs as Agents](https://arxiv.org/abs/2308.03688)
Append: [Can Large Language Models generalize analogy solving like children can?](https://arxiv.org/abs/2411.02348)
Append: [A Statistical Hypothesis Testing Framework for Data Misappropriation Detection in Large Language Models](https://arxiv.org/abs/2501.02441)
Append: [Fed-SB: A Silver Bullet for Extreme Communication Efficiency and Performance in (Private) Federated LoRA Fine-Tuning](https://arxiv.org/abs/2502.15436)
Append: [DISC: Dynamic Decomposition Improves LLM Inference Scaling](https://arxiv.org/abs/2502.16706)
Append: [Seeded Poisson Factorization: leveraging domain knowledge to fit topic models](https://arxiv.org/abs/2503.02741)
Append: [Understanding R1-Zero-Like Training: A Critical Perspective](https://arxiv.org/abs/2503.20783)
Append: [TRA: Better Length Generalisation with Threshold Relative Attention](https://arxiv.org/abs/2503.23174)
Append: [On the Effectiveness and Generalization of Race Representations for Debiasing High-Stakes Decisions](https://arxiv.org/abs/2504.06303)
Append: [AgentRewardBench: Evaluating Automatic Evaluations of Web Agent Trajectories](https://arxiv.org/abs/2504.08942)
Append: [From Dialect Gaps to Identity Maps: Tackling Variability in Speaker Verification](https://arxiv.org/abs/2505.04629)
Append: [Language Agents Mirror Human Causal Reasoning Biases. How Can We Help Them Think Like Scientists?](https://arxiv.org/abs/2505.09614)
Append: [MMLongBench: Benchmarking Long-Context Vision-Language Models Effectively and Thoroughly](https://arxiv.org/abs/2505.10610)
Append: [TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios](https://arxiv.org/abs/2505.12891)
Append: [Safety Subspaces are Not Linearly Distinct: A Fine-Tuning Case Study](https://arxiv.org/abs/2505.14185)
Append: [AdaSTaR: Adaptive Data Sampling for Training Self-Taught Reasoners](https://arxiv.org/abs/2505.16322)
Append: [COUNTDOWN: Contextually Sparse Activation Filtering Out Unnecessary Weights in Down Projection](https://arxiv.org/abs/2505.17701)
Append: [How Many Parameters Does Your Task Really Need? Task Specific Pruning with LLM-Sieve](https://arxiv.org/abs/2505.18350)
Append: [Scientific Paper Retrieval with LLM-Guided Semantic-Based Ranking](https://arxiv.org/abs/2505.21815)
Append: [Identity resolution of software metadata using Large Language Models](https://arxiv.org/abs/2505.23500)
Append: [Human Empathy as Encoder: AI-Assisted Depression Assessment in Special Education](https://arxiv.org/abs/2505.23631)
Append: [Rethinking Exact Unlearning under Exposure: Extracting Forgotten Data under Exact Unlearning in Large Language Model](https://arxiv.org/abs/2505.24379)
Append: [Contextual Integrity in LLMs via Reasoning and Reinforcement Learning](https://arxiv.org/abs/2506.04245)
Append: [Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models](https://arxiv.org/abs/2506.07468)
Append: [Can We Infer Confidential Properties of Training Data from LLMs?](https://arxiv.org/abs/2506.10364)
Append: [Trainable Dynamic Mask Sparse Attention](https://arxiv.org/abs/2508.02124)
Append: [Non-Interactive Symbolic-Aided Chain-of-Thought for Logical Reasoning](https://arxiv.org/abs/2508.12425)
Append: [The Telephone Game: Evaluating Semantic Drift in Unified Models](https://arxiv.org/abs/2509.04438)
Append: [ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory](https://arxiv.org/abs/2509.04439)
Append: [FedMentor: Domain-Aware Differential Privacy for Heterogeneous Federated LLMs in Mental Health](https://arxiv.org/abs/2509.14275)
Append: [RL Grokking Recipe: How Does RL Unlock and Transfer New Algorithms in LLMs?](https://arxiv.org/abs/2509.21016)
append_entries: 298
Finish: 2025-10-07 04:24:47.850954
------------------------------------------------------
Started: 2025-10-07 06:25:36.938860
Existing_entries: 1298
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1347
Summarized using GPT-3.5-turbo
Append: [K-DeCore: Facilitating Knowledge Transfer in Continual Structured Knowledge Reasoning via Knowledge Decoupling](https://arxiv.org/abs/2509.16929)
Token length: 1262
Summarized using GPT-3.5-turbo
Append: [ML2B: Multi-Lingual ML Benchmark For AutoML](https://arxiv.org/abs/2509.22768)
Token length: 1709
Summarized using GPT-3.5-turbo
Append: [HEART: Emotionally-driven test-time scaling of Language Models](https://arxiv.org/abs/2509.22876)
Token length: 1398
Summarized using GPT-3.5-turbo
Append: [Non-Collaborative User Simulators for Tool Agents](https://arxiv.org/abs/2509.23124)
Token length: 1199
Summarized using GPT-3.5-turbo
Append: [HiKE: Hierarchical Evaluation Framework for Korean-English Code-Switching Speech Recognition](https://arxiv.org/abs/2509.24613)
Token length: 654
Summarized using GPT-3.5-turbo
Append: [jina-reranker-v3: Last but Not Late Interaction for Listwise Document Reranking](https://arxiv.org/abs/2509.25085)
Token length: 1220
Summarized using GPT-3.5-turbo
Append: [Your Models Have Thought Enough: Training Large Reasoning Models to Stop Overthinking](https://arxiv.org/abs/2509.23392)
Token length: 1471
Summarized using GPT-3.5-turbo
Append: [Latent Visual Reasoning](https://arxiv.org/abs/2509.24251)
Token length: 937
Summarized using GPT-3.5-turbo
Append: [Extracting the Structure of Press Releases for Predicting Earnings Announcement Returns](https://arxiv.org/abs/2509.24254)
Token length: 1542
Summarized using GPT-3.5-turbo
Append: [On the Self-awareness of Large Reasoning Models' Capability Boundaries](https://arxiv.org/abs/2509.24711)
Token length: 1602
Summarized using GPT-3.5-turbo
Append: [Pushing LLMs to Their Logical Reasoning Bound: The Role of Data Reasoning Intensity](https://arxiv.org/abs/2509.24836)
append_entries: 11
Finish: 2025-10-07 06:25:59.947590
------------------------------------------------------
Started: 2025-10-07 08:22:17.360214
Existing_entries: 1011
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-07 08:22:18.115502
------------------------------------------------------
Started: 2025-10-07 10:17:06.273765
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-07 10:17:07.038459
------------------------------------------------------
Started: 2025-10-07 12:34:32.832961
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-07 12:34:33.497053
------------------------------------------------------
Started: 2025-10-07 14:16:53.769072
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-07 14:16:54.474188
------------------------------------------------------
Started: 2025-10-07 16:19:59.636850
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-07 16:20:00.377404
------------------------------------------------------
Started: 2025-10-07 18:23:41.551228
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-07 18:23:42.227168
------------------------------------------------------
Started: 2025-10-07 20:17:34.165809
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-07 20:17:34.926949
------------------------------------------------------
Started: 2025-10-07 22:14:56.923233
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-07 22:14:57.716581
------------------------------------------------------
Started: 2025-10-08 01:12:46.086590
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-08 01:12:46.735329
------------------------------------------------------
Started: 2025-10-08 02:55:24.376959
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-08 02:55:25.083647
------------------------------------------------------
Started: 2025-10-08 04:22:46.085065
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1781
Summarized using GPT-3.5-turbo
Append: [Collaborative and Proactive Management of Task-Oriented Conversations](https://arxiv.org/abs/2510.05110)
Token length: 1050
Summarized using GPT-3.5-turbo
Append: [Trainable Reference-Based Evaluation Metric for Identifying Quality of English-Gujarati Machine Translation System](https://arxiv.org/abs/2510.05113)
Token length: 1164
Summarized using GPT-3.5-turbo
Append: [Hallucination is Inevitable for LLMs with the Open World Assumption](https://arxiv.org/abs/2510.05116)
Token length: 1026
Summarized using GPT-3.5-turbo
Append: [Towards Structured Knowledge: Advancing Triple Extraction from Regional Trade Agreements using Large Language Models](https://arxiv.org/abs/2510.05121)
Token length: 1056
Summarized using GPT-3.5-turbo
Append: [CARE: Cognitive-reasoning Augmented Reinforcement for Emotional Support Conversation](https://arxiv.org/abs/2510.05122)
Token length: 981
Summarized using GPT-3.5-turbo
Append: [MADS: Multi-Agent Dialogue Simulation for Diverse Persuasion Data Generation](https://arxiv.org/abs/2510.05124)
Token length: 1286
Summarized using GPT-3.5-turbo
Append: [Catalog-Native LLM: Speaking Item-ID Dialect with Less Entanglement for Recommendation](https://arxiv.org/abs/2510.05125)
Token length: 1853
Summarized using GPT-3.5-turbo
Append: [Improving Metacognition and Uncertainty Communication in Language Models](https://arxiv.org/abs/2510.05126)
Token length: 1213
Summarized using GPT-3.5-turbo
Append: [Advancing Automated Spatio-Semantic Analysis in Picture Description Using Language Models](https://arxiv.org/abs/2510.05128)
Token length: 1112
Summarized using GPT-3.5-turbo
Append: [Automated Alignment of Math Items to Content Standards in Large-Scale Assessments Using Language Models](https://arxiv.org/abs/2510.05129)
Token length: 1054
Summarized using GPT-3.5-turbo
Append: [Submodular Context Partitioning and Compression for In-Context Learning-short paper](https://arxiv.org/abs/2510.05130)
Token length: 1318
Summarized using GPT-3.5-turbo
Append: [Rationale-Augmented Retrieval with Constrained LLM Re-Ranking for Task Discovery](https://arxiv.org/abs/2510.05131)
Token length: 1149
Summarized using GPT-3.5-turbo
Append: [Training Large Language Models To Reason In Parallel With Global Forking Tokens](https://arxiv.org/abs/2510.05132)
Token length: 1703
Summarized using GPT-3.5-turbo
Append: [Characterizing Model Behavior Under Synthetic Data Training: An Empirical Study Across Scales and Mixing Ratios](https://arxiv.org/abs/2510.05133)
Token length: 1039
Summarized using GPT-3.5-turbo
Append: [Curiosity-Driven LLM-as-a-judge for Personalized Creative Judgment](https://arxiv.org/abs/2510.05135)
Token length: 1906
Summarized using GPT-3.5-turbo
Append: [Linguistic Characteristics of AI-Generated Text: A Survey](https://arxiv.org/abs/2510.05136)
Token length: 1781
Summarized using GPT-3.5-turbo
Append: [Demystifying deep search: a holistic evaluation with hint-free multi-hop questions and factorised metrics](https://arxiv.org/abs/2510.05137)
Token length: 1229
Summarized using GPT-3.5-turbo
Append: [LiRA: A Multi-Agent Framework for Reliable and Readable Literature Review Generation](https://arxiv.org/abs/2510.05138)
Token length: 1110
Summarized using GPT-3.5-turbo
Append: [NLD-LLM: A systematic framework for evaluating small language transformer models on natural language description](https://arxiv.org/abs/2510.05139)
Token length: 1448
Summarized using GPT-3.5-turbo
Append: [To model human linguistic prediction, make LLMs less superhuman](https://arxiv.org/abs/2510.05141)
Token length: 1681
Summarized using GPT-3.5-turbo
Append: [Reliable End-to-End Material Information Extraction from the Literature with Source-Tracked Multi-Stage Large Language Models](https://arxiv.org/abs/2510.05142)
Token length: 1375
Summarized using GPT-3.5-turbo
Append: [SynCED-EnDe 2025: A Synthetic and Curated English - German Dataset for Critical Error Detection in Machine Translation](https://arxiv.org/abs/2510.05144)
Token length: 1940
Summarized using GPT-3.5-turbo
Append: [Every Step Counts: Decoding Trajectories as Authorship Fingerprints of dLLMs](https://arxiv.org/abs/2510.05148)
Token length: 1651
Summarized using GPT-3.5-turbo
Append: [Chronological Thinking in Full-Duplex Spoken Dialogue Language Models](https://arxiv.org/abs/2510.05150)
Token length: 1004
Summarized using GPT-3.5-turbo
Append: [Exploring Large Language Models for Financial Applications: Techniques, Performance, and Challenges with FinMA](https://arxiv.org/abs/2510.05151)
Token length: 1285
Summarized using GPT-3.5-turbo
Append: [A Single Character can Make or Break Your LLM Evals](https://arxiv.org/abs/2510.05152)
Token length: 1575
Summarized using GPT-3.5-turbo
Append: [Can AI Truly Represent Your Voice in Deliberations? A Comprehensive Study of Large-Scale Opinion Aggregation with LLMs](https://arxiv.org/abs/2510.05154)
Token length: 1062
Summarized using GPT-3.5-turbo
Append: [A novel hallucination classification framework](https://arxiv.org/abs/2510.05189)
Token length: 1562
Summarized using GPT-3.5-turbo
Append: [Let it Calm: Exploratory Annealed Decoding for Verifiable Reinforcement Learning](https://arxiv.org/abs/2510.05251)
Token length: 1489
Summarized using GPT-3.5-turbo
Append: [Camellia: Benchmarking Cultural Biases in LLMs for Asian Languages](https://arxiv.org/abs/2510.05291)
Token length: 1206
Summarized using GPT-3.5-turbo
Append: [RAG Makes Guardrails Unsafe? Investigating Robustness of Guardrails under RAG-style Contexts](https://arxiv.org/abs/2510.05310)
Token length: 1731
Summarized using GPT-3.5-turbo
Append: [WeatherArchive-Bench: Benchmarking Retrieval-Augmented Reasoning for Historical Weather Archives](https://arxiv.org/abs/2510.05336)
Token length: 1371
Summarized using GPT-3.5-turbo
Append: [Residualized Similarity for Faithfully Explainable Authorship Verification](https://arxiv.org/abs/2510.05362)
Token length: 706
Summarized using GPT-3.5-turbo
Append: [The End of Transformers? On Challenging Attention and the Rise of Sub-Quadratic Architectures](https://arxiv.org/abs/2510.05364)
Token length: 1789
Summarized using GPT-3.5-turbo
Append: [Context Length Alone Hurts LLM Performance Despite Perfect Retrieval](https://arxiv.org/abs/2510.05381)
Token length: 916
Summarized using GPT-3.5-turbo
Append: [Cross-Lingual Mental Health Ontologies for Indian Languages: Bridging Patient Expression and Clinical Understanding through Explainable AI and Human-in-the-Loop Validation](https://arxiv.org/abs/2510.05387)
Token length: 1294
Summarized using GPT-3.5-turbo
Append: [Aligning Language Models with Clinical Expertise: DPO for Heart Failure Nursing Documentation in Critical Care](https://arxiv.org/abs/2510.05410)
Token length: 1381
Summarized using GPT-3.5-turbo
Append: [A Lightweight Large Language Model-Based Multi-Agent System for 2D Frame Structural Analysis](https://arxiv.org/abs/2510.05414)
Token length: 1469
Summarized using GPT-3.5-turbo
Append: [Self-Filtered Distillation with LLMs-generated Trust Indicators for Reliable Patent Classification](https://arxiv.org/abs/2510.05431)
Token length: 1290
Summarized using GPT-3.5-turbo
Append: [SimulatorArena: Are User Simulators Reliable Proxies for Multi-Turn Evaluation of AI Assistants?](https://arxiv.org/abs/2510.05444)
Token length: 1617
Summarized using GPT-3.5-turbo
Append: [AgentRouter: A Knowledge-Graph-Guided LLM Router for Collaborative Multi-Agent Question Answering](https://arxiv.org/abs/2510.05445)
Token length: 898
Summarized using GPT-3.5-turbo
Append: [SocialNLI: A Dialogue-Centric Social Inference Dataset](https://arxiv.org/abs/2510.05458)
Token length: 1523
Summarized using GPT-3.5-turbo
Append: [TensorBLEU: Vectorized GPU-based BLEU Score Implementation for Per-Sentence In-Training Evaluation](https://arxiv.org/abs/2510.05485)
Token length: 954
Summarized using GPT-3.5-turbo
Append: [Language Model as Planner and Formalizer under Constraints](https://arxiv.org/abs/2510.05486)
Token length: 1894
Summarized using GPT-3.5-turbo
Append: [LANTERN: Scalable Distillation of Large Language Models for Job-Person Fit and Explanation](https://arxiv.org/abs/2510.05490)
Token length: 1138
Summarized using GPT-3.5-turbo
Append: [Prototype-Based Dynamic Steering for Large Language Models](https://arxiv.org/abs/2510.05498)
Token length: 1501
Summarized using GPT-3.5-turbo
Append: [CAM: A Constructivist View of Agentic Memory for LLM-Based Reading Comprehension](https://arxiv.org/abs/2510.05520)
Token length: 1051
Summarized using GPT-3.5-turbo
Append: [KEO: Knowledge Extraction on OMIn via Knowledge Graphs and RAG for Safety-Critical Aviation Maintenance](https://arxiv.org/abs/2510.05524)
Token length: 1490
Summarized using GPT-3.5-turbo
Append: [H1B-KV: Hybrid One-Bit Caches for Memory-Efficient Large Language Model Inference](https://arxiv.org/abs/2510.05529)
Token length: 1518
Summarized using GPT-3.5-turbo
Append: [On the Role of Difficult Prompts in Self-Play Preference Optimization](https://arxiv.org/abs/2510.05534)
Append: [Activation-Informed Pareto-Guided Low-Rank Compression for Efficient LLM/VLM](https://arxiv.org/abs/2510.05544)
Append: [Presenting a Paper is an Art: Self-Improvement Aesthetic Agents for Academic Presentations](https://arxiv.org/abs/2510.05571)
Append: [Mission Impossible: Feedback-Guided Dynamic Interactive Planning for Improving Reasoning on LLMs](https://arxiv.org/abs/2510.05577)
Append: [A Goal Without a Plan Is Just a Wish: Efficient and Effective Global Planner Training for Long-Horizon Agent Tasks](https://arxiv.org/abs/2510.05608)
Append: [MADIAVE: Multi-Agent Debate for Implicit Attribute Value Extraction](https://arxiv.org/abs/2510.05611)
Append: [The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP](https://arxiv.org/abs/2510.05644)
Append: [Code-Switching In-Context Learning for Cross-Lingual Transfer of Large Language Models](https://arxiv.org/abs/2510.05678)
Append: [DecEx-RAG: Boosting Agentic Retrieval-Augmented Generation with Decision and Execution Optimization via Process Supervision](https://arxiv.org/abs/2510.05691)
Append: [Adaptive and Multi-Source Entity Matching for Name Standardization of Astronomical Observation Facilities](https://arxiv.org/abs/2510.05744)
Append: [Diversity Is All You Need for Contrastive Learning: Spectral Bounds on Gradient Magnitudes](https://arxiv.org/abs/2510.05767)
Append: [InforME: Improving Informativeness of Abstractive Text Summarization With Informative Attention Guided by Named Entity Salience](https://arxiv.org/abs/2510.05769)
Append: [Mixture of Neuron Experts](https://arxiv.org/abs/2510.05781)
Append: [Data-efficient Targeted Token-level Preference Optimization for LLM-based Text-to-Speech](https://arxiv.org/abs/2510.05799)
Append: [EEPO: Exploration-Enhanced Policy Optimization via Sample-Then-Forget](https://arxiv.org/abs/2510.05837)
Append: [Luth: Efficient French Specialization for Small Language Models and Cross-Lingual Transfer](https://arxiv.org/abs/2510.05846)
Append: [DACP: Domain-Adaptive Continual Pre-Training of Large Language Models for Phone Conversation Summarization](https://arxiv.org/abs/2510.05858)
Append: [Automated Boilerplate: Prevalence and Quality of Contract Generators in the Context of Swiss Privacy Policies](https://arxiv.org/abs/2510.05860)
Append: [Revisiting Long-context Modeling from Context Denoising Perspective](https://arxiv.org/abs/2510.05862)
Append: [Evaluating the Sensitivity of LLMs to Harmful Contents in Long Input](https://arxiv.org/abs/2510.05864)
Append: [The fragility of "cultural tendencies" in LLMs](https://arxiv.org/abs/2510.05869)
Append: [Prompt reinforcing for long-term planning of large language models](https://arxiv.org/abs/2510.05921)
Append: [Hire Your Anthropologist! Rethinking Culture Benchmarks Through an Anthropological Lens](https://arxiv.org/abs/2510.05931)
Append: [EvalMORAAL: Interpretable Chain-of-Thought and LLM-as-Judge Evaluation for Moral Alignment in Large Language Models](https://arxiv.org/abs/2510.05942)
Append: [Probing the Difficulty Perception Mechanism of Large Language Models](https://arxiv.org/abs/2510.05969)
Append: [LexiCon: a Benchmark for Planning under Temporal Constraints in Natural Language](https://arxiv.org/abs/2510.05972)
Append: [Exploring Gaps in the APS: Direct Minimal Pair Analysis in LLM Syntactic Assessments](https://arxiv.org/abs/2510.06001)
Append: [MASA: Rethinking the Representational Bottleneck in LoRA with Multi-A Shared Adaptation](https://arxiv.org/abs/2510.06005)
Append: [Evaluating The Impact of Stimulus Quality in Investigations of LLM Language Performance](https://arxiv.org/abs/2510.06018)
Append: [CDTP: A Large-Scale Chinese Data-Text Pair Dataset for Comprehensive Evaluation of Chinese LLMs](https://arxiv.org/abs/2510.06039)
Append: [ASPO: Asymmetric Importance Sampling Policy Optimization](https://arxiv.org/abs/2510.06062)
Append: [Spectrum Tuning: Post-Training for Distributional Coverage and In-Context Steerability](https://arxiv.org/abs/2510.06084)
Append: [The Valley of Code Reasoning: Scaling Knowledge Distillation of Large Language Models](https://arxiv.org/abs/2510.06101)
Append: [Distributional Semantics Tracing: A Framework for Explaining Hallucinations in Large Language Models](https://arxiv.org/abs/2510.06107)
Append: [Parallel Tokenizers: Rethinking Vocabulary Design for Cross-Lingual Transfer](https://arxiv.org/abs/2510.06128)
Append: [CreditDecoding: Accelerating Parallel Decoding in Diffusion Large Language Models with Trace Credits](https://arxiv.org/abs/2510.06133)
Append: [RoSE: Round-robin Synthetic Data Evaluation for Selecting LLM Generators without Human Test Sets](https://arxiv.org/abs/2510.06143)
Append: [VecInfer: Efficient LLM Inference with Low-Bit KV Cache via Outlier-Suppressed Vector Quantization](https://arxiv.org/abs/2510.06175)
Append: [Mixing Mechanisms: How Language Models Retrieve Bound Entities In-Context](https://arxiv.org/abs/2510.06182)
Append: [RECODE-H: A Benchmark for Research Code Development with Interactive Human Feedback](https://arxiv.org/abs/2510.06186)
Append: [BanglaTalk: Towards Real-Time Speech Assistance for Bengali Regional Dialects](https://arxiv.org/abs/2510.06188)
Append: [Latent Speech-Text Transformer](https://arxiv.org/abs/2510.06195)
Append: [Peeking inside the Black-Box: Reinforcement Learning for Explainable and Accurate Relation Extraction](https://arxiv.org/abs/2510.06198)
Append: [Tiny but Mighty: A Software-Hardware Co-Design Approach for Efficient Multimodal Inference on Battery-Powered Small Devices](https://arxiv.org/abs/2510.05109)
Append: [Optimization Modeling via Semantic Anchored Alignment](https://arxiv.org/abs/2510.05115)
Append: [Decoding Partial Differential Equations: Cross-Modal Adaptation of Decoder-only Models to PDEs](https://arxiv.org/abs/2510.05278)
Append: [Beyond Monolithic Rewards: A Hybrid and Multi-Aspect Reward Optimization for MLLM Alignment](https://arxiv.org/abs/2510.05283)
Append: [WaveSP-Net: Learnable Wavelet-Domain Sparse Prompt Tuning for Speech Deepfake Detection](https://arxiv.org/abs/2510.05305)
Append: [Quantum Concept Music Score from Quantum Picturalism: Musical Incarnation of a Bell-Pair under Measurements](https://arxiv.org/abs/2510.05391)
Append: [Adversarial Reinforcement Learning for Large Language Model Agent Safety](https://arxiv.org/abs/2510.05442)
Append: [Do Code Models Suffer from the Dunning-Kruger Effect?](https://arxiv.org/abs/2510.05457)
Append: [VAL-Bench: Measuring Value Alignment in Language Models](https://arxiv.org/abs/2510.05465)
Append: [AMAQ: Adaptive Mixed-bit Activation Quantization for Collaborative Parameter Efficient Fine-tuning](https://arxiv.org/abs/2510.05468)
Append: [NorMuon: Making Muon more efficient and scalable](https://arxiv.org/abs/2510.05491)
Append: [Sci-Phi: A Large Language Model Spatial Audio Descriptor](https://arxiv.org/abs/2510.05542)
Append: [Domain-Shift-Aware Conformal Prediction for Large Language Models](https://arxiv.org/abs/2510.05566)
Append: [In-the-Flow Agentic System Optimization for Effective Planning and Tool Use](https://arxiv.org/abs/2510.05592)
Append: [Improving Chain-of-Thought Efficiency for Autoregressive Image Generation](https://arxiv.org/abs/2510.05593)
Append: [Generative AI-Driven Hierarchical Multi-Agent Framework for Zero-Touch Optical Networks](https://arxiv.org/abs/2510.05625)
Append: [Towards Reliable and Practical LLM Security Evaluations via Bayesian Modelling](https://arxiv.org/abs/2510.05709)
Append: [Improving Discrete Diffusion Unmasking Policies Beyond Explicit Reference Policies](https://arxiv.org/abs/2510.05725)
Append: [ARM: Discovering Agentic Reasoning Modules for Generalizable Multi-Agent Systems](https://arxiv.org/abs/2510.05746)
Append: [Early Multimodal Prediction of Cross-Lingual Meme Virality on Reddit: A Time-Window Analysis](https://arxiv.org/abs/2510.05761)
Append: [Mitigating Premature Exploitation in Particle-based Monte Carlo for Inference-Time Scaling](https://arxiv.org/abs/2510.05825)
Append: [Paying Attention to Hybrid Attention: Untangling the Issues with Conversion Methods](https://arxiv.org/abs/2510.05901)
Append: [MatheMagic: Generating Dynamic Mathematics Benchmarks Robust to Memorization](https://arxiv.org/abs/2510.05962)
Append: [Sample Smart, Not Hard: Correctness-First Decoding for Better Reasoning in LLMs](https://arxiv.org/abs/2510.05987)
Append: [Deterministic Legal Retrieval: An Action API for Querying the SAT-Graph RAG](https://arxiv.org/abs/2510.06002)
Append: [MixReasoning: Switching Modes to Think](https://arxiv.org/abs/2510.06052)
Append: [Learning from Failures: Understanding LLM Alignment through Failure-Aware Inverse RL](https://arxiv.org/abs/2510.06092)
Append: [The Alignment Auditor: A Bayesian Framework for Verifying and Refining LLM Objectives](https://arxiv.org/abs/2510.06096)
Append: [Influence Functions for Efficient Data Selection in Reasoning](https://arxiv.org/abs/2510.06108)
Append: [Taxonomy of User Needs and Actions](https://arxiv.org/abs/2510.06124)
Append: [TokenChain: A Discrete Speech Chain via Semantic Token Modeling](https://arxiv.org/abs/2510.06201)
Append: [Stratified GRPO: Handling Structural Heterogeneity in Reinforcement Learning of LLM Search Agents](https://arxiv.org/abs/2510.06214)
Append: [TaTToo: Tool-Grounded Thinking PRM for Test-Time Scaling in Tabular Reasoning](https://arxiv.org/abs/2510.06217)
Append: [MathVC: An LLM-Simulated Multi-Character Virtual Classroom for Mathematics Education](https://arxiv.org/abs/2404.06711)
Append: [SciKnowEval: Evaluating Multi-level Scientific Knowledge of Large Language Models](https://arxiv.org/abs/2406.09098)
Append: [Robustness of Large Language Models to Perturbations in Text](https://arxiv.org/abs/2407.08989)
Append: [Text Clustering as Classification with LLMs](https://arxiv.org/abs/2410.00927)
Append: [HEALTH-PARIKSHA: Assessing RAG Models for Health Chatbots in Real-World Multilingual Settings](https://arxiv.org/abs/2410.13671)
Append: [BanglaLlama: LLaMA for Bangla Language](https://arxiv.org/abs/2410.21200)
Append: [Explaining GPTs' Schema of Depression: A Machine Behavior Analysis](https://arxiv.org/abs/2411.13800)
Append: [Evaluating and Mitigating Social Bias for Large Language Models in Open-ended Settings](https://arxiv.org/abs/2412.06134)
Append: [QAPyramid: Fine-grained Evaluation of Content Selection for Text Summarization](https://arxiv.org/abs/2412.07096)
Append: [Improving Retrieval-Augmented Generation through Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2501.15228)
Append: [A Generative Approach to LLM Harmfulness Mitigation with Red Flag Tokens](https://arxiv.org/abs/2502.16366)
Append: [On Relation-Specific Neurons in Large Language Models](https://arxiv.org/abs/2502.17355)
Append: [Evaluating the Effect of Retrieval Augmentation on Social Biases](https://arxiv.org/abs/2502.17611)
Append: [Geometry-Guided Adversarial Prompt Detection via Curvature and Local Intrinsic Dimension](https://arxiv.org/abs/2503.03502)
Append: [WildIFEval: Instruction Following in the Wild](https://arxiv.org/abs/2503.06573)
Append: [Bayesian Teaching Enables Probabilistic Reasoning in Large Language Models](https://arxiv.org/abs/2503.17523)
Append: [Building Resource-Constrained Language Agents: A Korean Case Study on Chemical Toxicity Information](https://arxiv.org/abs/2503.17753)
Append: [MASRAD: Arabic Terminology Management Corpora with Semi-Automatic Construction](https://arxiv.org/abs/2503.19211)
Append: [Entropy-Gated Branching for Efficient Test-Time Reasoning](https://arxiv.org/abs/2503.21961)
Append: [Self-Routing RAG: Binding Selective Retrieval with Knowledge Verbalization](https://arxiv.org/abs/2504.01018)
Append: [SAFER: Advancing Safety Alignment via Efficient Ex-Ante Reasoning](https://arxiv.org/abs/2504.02725)
Append: [MedHal: An Evaluation Dataset for Medical Hallucination Detection](https://arxiv.org/abs/2504.08596)
Append: [Measuring LLM Novelty As The Frontier Of Original And High-Quality Output](https://arxiv.org/abs/2504.09389)
Append: [The Mirage of Performance Gains: Why Contrastive Decoding Fails to Mitigate Object Hallucinations in MLLMs?](https://arxiv.org/abs/2504.10020)
Append: [Cross-Document Cross-Lingual NLI via RST-Enhanced Graph Fusion and Interpretability Prediction](https://arxiv.org/abs/2504.12324)
Append: [Illusion or Algorithm? Investigating Memorization, Emergence, and Symbolic Processing in In-Context Learning](https://arxiv.org/abs/2505.11004)
Append: [What Prompts Don't Say: Understanding and Managing Underspecification in LLM Prompts](https://arxiv.org/abs/2505.13360)
Append: [FAID: Fine-Grained AI-Generated Text Detection Using Multi-Task Auxiliary and Multi-Level Contrastive Learning](https://arxiv.org/abs/2505.14271)
Append: [Teaching Small Language Models to Learn Logic through Meta-Learning](https://arxiv.org/abs/2505.14313)
Append: [Unifying Inference-Time Planning Language Generation](https://arxiv.org/abs/2505.14763)
Append: [Tracing Multilingual Factual Knowledge Acquisition in Pretraining](https://arxiv.org/abs/2505.14824)
Append: [ChartCards: A Chart-Metadata Generation Framework for Multi-Task Chart Understanding](https://arxiv.org/abs/2505.15046)
Append: [AVerImaTeC: A Dataset for Automatic Verification of Image-Text Claims with Evidence from the Web](https://arxiv.org/abs/2505.17978)
Append: [An Embarrassingly Simple Defense Against LLM Abliteration Attacks](https://arxiv.org/abs/2505.19056)
Append: [OWL: Probing Cross-Lingual Recall of Memorized Texts via World Literature](https://arxiv.org/abs/2505.22945)
Append: [ExpertLongBench: Benchmarking Language Models on Expert-Level Long-Form Generation Tasks with Structured Checklists](https://arxiv.org/abs/2506.01241)
Append: [Trajectory Prediction Meets Large Language Models: A Survey](https://arxiv.org/abs/2506.03408)
Append: [When to use Graphs in RAG: A Comprehensive Analysis for Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2506.05690)
Append: [Compound AI Systems Optimization: A Survey of Methods, Challenges, and Future Directions](https://arxiv.org/abs/2506.08234)
Append: [Context Biasing for Pronunciations-Orthography Mismatch in Automatic Speech Recognition](https://arxiv.org/abs/2506.18703)
Append: [Can We Predict Alignment Before Models Finish Thinking? Towards Monitoring Misaligned Reasoning Models](https://arxiv.org/abs/2507.12428)
Append: [Intent-Aware Schema Generation And Refinement For Literature Review Tables](https://arxiv.org/abs/2507.19521)
Append: [Towards Locally Deployable Fine-Tuned Causal Large Language Models for Mode Choice Behaviour](https://arxiv.org/abs/2507.21432)
Append: [GLiDRE: Generalist Lightweight model for Document-level Relation Extraction](https://arxiv.org/abs/2508.00757)
Append: [Aligning Language Models with Real-time Knowledge Editing](https://arxiv.org/abs/2508.01302)
Append: [CAMERA: Multi-Matrix Joint Compression for MoE Models via Micro-Expert Redundancy Analysis](https://arxiv.org/abs/2508.02322)
Append: [RooseBERT: A New Deal For Political Language Modelling](https://arxiv.org/abs/2508.03250)
Append: [LLM Unlearning Without an Expert Curated Dataset](https://arxiv.org/abs/2508.06595)
Append: [Fair Play in the Newsroom: Actor-Based Filtering Gender Discrimination in Text Corpora](https://arxiv.org/abs/2508.13169)
Append: [MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based Peer Review Automation](https://arxiv.org/abs/2508.14146)
Append: [Bridging the Culture Gap: A Framework for LLM-Driven Socio-Cultural Localization of Math Word Problems in Low-Resource Languages](https://arxiv.org/abs/2508.14913)
Append: [Generative Interfaces for Language Models](https://arxiv.org/abs/2508.19227)
Append: [Speech-Based Cognitive Screening: A Systematic Evaluation of LLM Adaptation Strategies](https://arxiv.org/abs/2509.03525)
Append: [AgenticIE: An Adaptive Agent for Information Extraction from Complex Regulatory Documents](https://arxiv.org/abs/2509.11773)
Append: [WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research](https://arxiv.org/abs/2509.13312)
Append: [LLM-JEPA: Large Language Models Meet Joint Embedding Predictive Architectures](https://arxiv.org/abs/2509.14252)
Append: [Learning to vary: Teaching LMs to reproduce human linguistic variability in next-word prediction](https://arxiv.org/abs/2509.17794)
Append: [Diagnosing the Performance Trade-off in Moral Alignment: A Case Study on Gender Stereotypes](https://arxiv.org/abs/2509.21456)
Append: [Generative transformations and patterns in LLM-native approaches for software verification and falsification](https://arxiv.org/abs/2404.09384)
Append: [Exploring the Potential of Conversational AI Support for Agent-Based Social Simulation Model Design](https://arxiv.org/abs/2405.08032)
Append: [Fine-Grained and Thematic Evaluation of LLMs in Social Deduction Game](https://arxiv.org/abs/2408.09946)
Append: [How Reliable are Causal Probing Interventions?](https://arxiv.org/abs/2408.15510)
Append: [Hallucination Detox: Sensitivity Dropout (SenD) for Large Language Model Training](https://arxiv.org/abs/2410.15460)
Append: [BenchAgents: Multi-Agent Systems for Structured Benchmark Creation](https://arxiv.org/abs/2410.22584)
Append: [LaB-RAG: Label Boosted Retrieval Augmented Generation for Radiology Report Generation](https://arxiv.org/abs/2411.16523)
Append: [A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility](https://arxiv.org/abs/2504.07086)
Append: [SAE-FiRE: Enhancing Earnings Surprise Predictions Through Sparse Autoencoder Feature Selection](https://arxiv.org/abs/2505.14420)
Append: [Language Models Surface the Unwritten Code of Science and Society](https://arxiv.org/abs/2505.18942)
Append: [VisRet: Visualization Improves Knowledge-Intensive Text-to-Image Retrieval](https://arxiv.org/abs/2505.20291)
Append: [From Accuracy to Robustness: A Study of Rule- and Model-based Verifiers in Mathematical Reasoning](https://arxiv.org/abs/2505.22203)
Append: [How Malicious AI Swarms Can Threaten Democracy: The Fusion of Agentic AI and LLMs Marks a New Frontier in Information Warfare](https://arxiv.org/abs/2506.06299)
Append: [Can Video Large Multimodal Models Think Like Doubters-or Double-Down: A Study on Defeasible Video Entailment](https://arxiv.org/abs/2506.22385)
Append: [CAPO: Towards Enhancing LLM Reasoning through Generative Credit Assignment](https://arxiv.org/abs/2508.02298)
Append: [RepIt: Representing Isolated Targets to Steer Language Models](https://arxiv.org/abs/2509.13281)
Append: [GEM-Bench: A Benchmark for Ad-Injected Response Generation within Generative Engine Marketing](https://arxiv.org/abs/2509.14221)
Append: [Adaptive Margin RLHF via Preference over Preferences](https://arxiv.org/abs/2509.22851)
append_entries: 201
Finish: 2025-10-08 04:24:31.518141
------------------------------------------------------
Started: 2025-10-08 06:25:12.237902
Existing_entries: 1201
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1407
Summarized using GPT-3.5-turbo
Append: [v1: Learning to Point Visual Tokens for Multimodal Grounded Reasoning](https://arxiv.org/abs/2505.18842)
Token length: 1613
Summarized using GPT-3.5-turbo
Append: [Assessing Algorithmic Bias in Language-Based Depression Detection: A Comparison of DNN and LLM Approaches](https://arxiv.org/abs/2509.25795)
append_entries: 2
Finish: 2025-10-08 06:25:16.088254
------------------------------------------------------
Started: 2025-10-08 08:22:10.436684
Existing_entries: 1002
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-08 08:22:10.931054
------------------------------------------------------
Started: 2025-10-08 10:17:03.486434
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-08 10:17:03.922975
------------------------------------------------------
Started: 2025-10-08 12:34:46.209053
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-08 12:34:46.657536
------------------------------------------------------
Started: 2025-10-08 14:16:34.641198
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-08 14:16:35.176153
------------------------------------------------------
Started: 2025-10-08 16:20:27.118840
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-08 16:20:27.601867
------------------------------------------------------
Started: 2025-10-08 18:24:23.209447
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-08 18:24:23.658128
------------------------------------------------------
Started: 2025-10-08 20:17:44.741216
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-08 20:17:45.180502
------------------------------------------------------
Started: 2025-10-08 22:14:59.422575
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-08 22:14:59.895391
------------------------------------------------------
Started: 2025-10-09 01:13:28.811139
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-09 01:13:29.253451
------------------------------------------------------
Started: 2025-10-09 02:58:06.272421
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-09 02:58:06.790198
------------------------------------------------------
Started: 2025-10-09 04:23:18.502456
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 681
Summarized using GPT-3.5-turbo
Append: [OpenStaxQA: A multilingual dataset based on open-source college textbooks](https://arxiv.org/abs/2510.06239)
Token length: 1380
Summarized using GPT-3.5-turbo
Append: [Knowledge Graph-Guided Multi-Agent Distillation for Reliable Industrial Question Answering with Datasets](https://arxiv.org/abs/2510.06240)
Token length: 1083
Summarized using GPT-3.5-turbo
Append: [Transparent Reference-free Automated Evaluation of Open-Ended User Survey Responses](https://arxiv.org/abs/2510.06242)
Token length: 1264
Summarized using GPT-3.5-turbo
Append: [CoT Referring: Improving Referring Expression Tasks with Grounded Reasoning](https://arxiv.org/abs/2510.06243)
Token length: 1307
Summarized using GPT-3.5-turbo
Append: [Evaluating Embedding Frameworks for Scientific Domain](https://arxiv.org/abs/2510.06244)
Token length: 1186
Summarized using GPT-3.5-turbo
Append: [TRepLiNa: Layer-wise CKA+REPINA Alignment Improves Low-Resource Machine Translation in Aya-23 8B](https://arxiv.org/abs/2510.06249)
Token length: 1129
Summarized using GPT-3.5-turbo
Append: [Scalable multilingual PII annotation for responsible AI in LLMs](https://arxiv.org/abs/2510.06250)
Token length: 1179
Summarized using GPT-3.5-turbo
Append: [Prakriti200: A Questionnaire-Based Dataset of 200 Ayurvedic Prakriti Assessments](https://arxiv.org/abs/2510.06262)
Token length: 1494
Summarized using GPT-3.5-turbo
Append: [Dual-stage and Lightweight Patient Chart Summarization for Emergency Physicians](https://arxiv.org/abs/2510.06263)
Token length: 1495
Summarized using GPT-3.5-turbo
Append: [A Comprehensive Survey of Hallucination in Large Language Models: Causes, Detection, and Mitigation](https://arxiv.org/abs/2510.06265)
Token length: 1105
Summarized using GPT-3.5-turbo
Append: [Language models for longitudinal analysis of abusive content in Billboard Music Charts](https://arxiv.org/abs/2510.06266)
Token length: 1433
Summarized using GPT-3.5-turbo
Append: [Reproducibility Study of "XRec: Large Language Models for Explainable Recommendation"](https://arxiv.org/abs/2510.06275)
Token length: 1022
Summarized using GPT-3.5-turbo
Append: [Type and Complexity Signals in Multilingual Question Representations](https://arxiv.org/abs/2510.06304)
Token length: 1253
Summarized using GPT-3.5-turbo
Append: [LLM Bias Detection and Mitigation through the Lens of Desired Distributions](https://arxiv.org/abs/2510.06354)
Token length: 1642
Summarized using GPT-3.5-turbo
Append: [EVALUESTEER: Measuring Reward Model Steerability Towards Values and Preference](https://arxiv.org/abs/2510.06370)
Token length: 1366
Summarized using GPT-3.5-turbo
Append: [EverydayMMQA: A Multilingual and Multimodal Framework for Culturally Grounded Spoken Visual QA](https://arxiv.org/abs/2510.06371)
Token length: 1144
Summarized using GPT-3.5-turbo
Append: [Semantic Regexes: Auto-Interpreting LLM Features with a Structured Language](https://arxiv.org/abs/2510.06378)
Token length: 1105
Summarized using GPT-3.5-turbo
Append: [Protecting De-identified Documents from Search-based Linkage Attacks](https://arxiv.org/abs/2510.06383)
Token length: 1572
Summarized using GPT-3.5-turbo
Append: [Controllable Stylistic Text Generation with Train-Time Attribute-Regularized Diffusion](https://arxiv.org/abs/2510.06386)
Token length: 1110
Summarized using GPT-3.5-turbo
Append: [Reward Model Perspectives: Whose Opinions Do Reward Models Reward?](https://arxiv.org/abs/2510.06391)
Token length: 1575
Summarized using GPT-3.5-turbo
Append: [Instructional Goal-Aligned Question Generation for Student Evaluation in Virtual Lab Settings: How Closely Do LLMs Actually Align?](https://arxiv.org/abs/2510.06411)
Token length: 1371
Summarized using GPT-3.5-turbo
Append: [FinLFQA: Evaluating Attributed Text Generation of LLMs in Financial Long-Form Question Answering](https://arxiv.org/abs/2510.06426)
Token length: 882
Summarized using GPT-3.5-turbo
Append: [Bridging Discourse Treebanks with a Unified Rhetorical Structure Parser](https://arxiv.org/abs/2510.06427)
Token length: 1556
Summarized using GPT-3.5-turbo
Append: [MathRobust-LV: Evaluation of Large Language Models' Robustness to Linguistic Variations in Mathematical Reasoning](https://arxiv.org/abs/2510.06430)
Token length: 814
Summarized using GPT-3.5-turbo
Append: [A Survey on Agentic Security: Applications, Threats and Defenses](https://arxiv.org/abs/2510.06445)
Token length: 926
Summarized using GPT-3.5-turbo
Append: [Linguistically Informed Tokenization Improves ASR for Underresourced Languages](https://arxiv.org/abs/2510.06461)
Token length: 1415
Summarized using GPT-3.5-turbo
Append: [Test-Time Scaling of Reasoning Models for Machine Translation](https://arxiv.org/abs/2510.06471)
Token length: 1310
Summarized using GPT-3.5-turbo
Append: [Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining Levels](https://arxiv.org/abs/2510.06499)
Token length: 1114
Summarized using GPT-3.5-turbo
Append: [From Acceleration to Saturation: Scaling Behavior of Bootstrapped Language Model Pretraining](https://arxiv.org/abs/2510.06548)
Token length: 1481
Summarized using GPT-3.5-turbo
Append: [Flipping the Dialogue: Training and Evaluating User Language Models](https://arxiv.org/abs/2510.06552)
Token length: 1866
Summarized using GPT-3.5-turbo
Append: [The Algebra of Meaning: Why Machines Need Montague More Than Moore's Law](https://arxiv.org/abs/2510.06559)
Token length: 1056
Summarized using GPT-3.5-turbo
Append: [TinyScientist: An Interactive, Extensible, and Controllable Framework for Building Research Agents](https://arxiv.org/abs/2510.06579)
Token length: 1030
Summarized using GPT-3.5-turbo
Append: [Do Internal Layers of LLMs Reveal Patterns for Jailbreak Detection?](https://arxiv.org/abs/2510.06594)
Token length: 1108
Summarized using GPT-3.5-turbo
Append: [A Comparative Analysis of Contextual Representation Flow in State-Space and Transformer Architectures](https://arxiv.org/abs/2510.06640)
Token length: 1288
Summarized using GPT-3.5-turbo
Append: [Aligning Large Language Models via Fully Self-Synthetic Data](https://arxiv.org/abs/2510.06652)
Token length: 1548
Summarized using GPT-3.5-turbo
Append: [ToolMem: Enhancing Multimodal Agents with Learnable Tool Capability Memory](https://arxiv.org/abs/2510.06664)
Token length: 1678
Summarized using GPT-3.5-turbo
Append: [PIKA: Expert-Level Synthetic Datasets for Post-Training Alignment from Scratch](https://arxiv.org/abs/2510.06670)
Token length: 931
Summarized using GPT-3.5-turbo
Append: [Incremental Summarization for Customer Support via Progressive Note-Taking and Agent Feedback](https://arxiv.org/abs/2510.06677)
Token length: 1373
Summarized using GPT-3.5-turbo
Append: [Learning to Rewrite Prompts for Bootstrapping LLMs on Downstream Tasks](https://arxiv.org/abs/2510.06695)
Token length: 1268
Summarized using GPT-3.5-turbo
Append: [How Language Models Conflate Logical Validity with Plausibility: A Representational Analysis of Content Effects](https://arxiv.org/abs/2510.06700)
Token length: 1714
Summarized using GPT-3.5-turbo
Append: [Scaling LLM Multi-turn RL with End-to-end Summarization-based Context Management](https://arxiv.org/abs/2510.06727)
Token length: 1284
Summarized using GPT-3.5-turbo
Append: [PTEB: Towards Robust Text Embedding Evaluation via Stochastic Paraphrasing at Evaluation Time with LLMs](https://arxiv.org/abs/2510.06730)
Token length: 1476
Summarized using GPT-3.5-turbo
Append: [Are LLMs Reliable Rankers? Rank Manipulation via Two-Stage Token Optimization](https://arxiv.org/abs/2510.06732)
Token length: 1453
Summarized using GPT-3.5-turbo
Append: [AWM: Accurate Weight-Matrix Fingerprint for Large Language Models](https://arxiv.org/abs/2510.06738)
Token length: 1231
Summarized using GPT-3.5-turbo
Append: [TWIST: Training-free and Label-free Short Text Clustering through Iterative Vector Updating with LLMs](https://arxiv.org/abs/2510.06747)
Token length: 1191
Summarized using GPT-3.5-turbo
Append: [A Formal Framework for Fluency-based Multi-Reference Evaluation in Grammatical Error Correction](https://arxiv.org/abs/2510.06749)
Token length: 825
Summarized using GPT-3.5-turbo
Append: [Gold-Switch: Training-Free Superposition of Slow- and Fast- Thinking LLMs](https://arxiv.org/abs/2510.06750)
Token length: 1598
Summarized using GPT-3.5-turbo
Append: [Adaptive LLM-Symbolic Reasoning via Dynamic Logical Solver Composition](https://arxiv.org/abs/2510.06774)
Token length: 1245
Summarized using GPT-3.5-turbo
Append: [Foundations of LLM Knowledge Materialization: Termination, Reproducibility, Robustness](https://arxiv.org/abs/2510.06780)
Token length: 1938
Summarized using GPT-3.5-turbo
Append: [FURINA: A Fully Customizable Role-Playing Benchmark via Scalable Multi-Agent Collaboration Pipeline](https://arxiv.org/abs/2510.06800)
Append: [Overview of the Plagiarism Detection Task at PAN 2025](https://arxiv.org/abs/2510.06805)
Append: [BlackboxNLP-2025 MIB Shared Task: Exploring Ensemble Strategies for Circuit Localization Methods](https://arxiv.org/abs/2510.06811)
Append: [Adaptive Tool Generation with Models as Tools and Reinforcement Learning](https://arxiv.org/abs/2510.06825)
Append: [Mid-Training of Large Language Models: A Survey](https://arxiv.org/abs/2510.06826)
Append: [GAMBIT+: A Challenge Set for Evaluating Gender Bias in Machine Translation Quality Estimation Metrics](https://arxiv.org/abs/2510.06841)
Append: [SID: Multi-LLM Debate Driven by Self Signals](https://arxiv.org/abs/2510.06843)
Append: [OpenJAI-v1.0: An Open Thai Large Language Model](https://arxiv.org/abs/2510.06847)
Append: [Unlocking Latent Discourse Translation in LLMs Through Quality-Aware Decoding](https://arxiv.org/abs/2510.06866)
Append: [$\lambda$-GRPO: Unifying the GRPO Frameworks with Learnable Token Preferences](https://arxiv.org/abs/2510.06870)
Append: [MeXtract: Light-Weight Metadata Extraction from Scientific Papers](https://arxiv.org/abs/2510.06889)
Append: [LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling](https://arxiv.org/abs/2510.06915)
Append: [SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models](https://arxiv.org/abs/2510.06917)
Append: [Open ASR Leaderboard: Towards Reproducible and Transparent Multilingual and Long-Form Speech Recognition Evaluation](https://arxiv.org/abs/2510.06961)
Append: [EDUMATH: Generating Standards-aligned Educational Math Word Problems](https://arxiv.org/abs/2510.06965)
Append: [Probing Social Identity Bias in Chinese LLMs with Gendered Pronouns and Social Groups](https://arxiv.org/abs/2510.06974)
Append: [Towards Reliable Retrieval in RAG Systems for Large Legal Datasets](https://arxiv.org/abs/2510.06999)
Append: [Pragyaan: Designing and Curating High-Quality Cultural Post-Training Datasets for Indian Languages](https://arxiv.org/abs/2510.07000)
Append: [Native Hybrid Attention for Efficient Sequence Modeling](https://arxiv.org/abs/2510.07019)
Append: [Mining the Mind: What 100M Beliefs Reveal About Frontier LLM Knowledge](https://arxiv.org/abs/2510.07024)
Append: [Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era of Large Language Models](https://arxiv.org/abs/2510.07037)
Append: [Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models](https://arxiv.org/abs/2510.07048)
Append: [Does Local News Stay Local?: Online Content Shifts in Sinclair-Acquired Stations](https://arxiv.org/abs/2510.07060)
Append: [Revisiting Metric Reliability for Fine-grained Evaluation of Machine Translation and Summarization in Indian Languages](https://arxiv.org/abs/2510.07061)
Append: [LuxInstruct: A Cross-Lingual Instruction Tuning Dataset For Luxembourgish](https://arxiv.org/abs/2510.07074)
Append: [Accelerating Diffusion LLM Inference via Local Determinism Propagation](https://arxiv.org/abs/2510.07081)
Append: [All Claims Are Equal, but Some Claims Are More Equal Than Others: Importance-Sensitive Factuality Evaluation of LLM Generations](https://arxiv.org/abs/2510.07083)
Append: [Making Machines Sound Sarcastic: LLM-Enhanced and Retrieval-Guided Sarcastic Speech Synthesis](https://arxiv.org/abs/2510.07096)
Append: [TALENT: Table VQA via Augmented Language-Enhanced Natural-text Transcription](https://arxiv.org/abs/2510.07098)
Append: [Opt-ICL at LeWiDi-2025: Maximizing In-Context Signal from Rater Examples via Meta-Learning](https://arxiv.org/abs/2510.07105)
Append: [TRIM: Token-wise Attention-Derived Saliency for Data-Efficient Instruction Tuning](https://arxiv.org/abs/2510.07118)
Append: [Comparing human and language models sentence processing difficulties on complex structures](https://arxiv.org/abs/2510.07141)
Append: [Reasoning for Hierarchical Text Classification: The Case of Patents](https://arxiv.org/abs/2510.07167)
Append: [More Data or Better Data? A Critical Analysis of Data Selection and Synthesis for Mathematical Reasoning](https://arxiv.org/abs/2510.07169)
Append: [NurseLLM: The First Specialized Language Model for Nursing](https://arxiv.org/abs/2510.07173)
Append: [Quantifying Data Contamination in Psychometric Evaluations of LLMs](https://arxiv.org/abs/2510.07175)
Append: [CARPAS: Towards Content-Aware Refinement of Provided Aspects for Summarization in Large Language Models](https://arxiv.org/abs/2510.07177)
Append: [Biasless Language Models Learn Unnaturally: How LLMs Fail to Distinguish the Possible from the Impossible](https://arxiv.org/abs/2510.07178)
Append: [Sunflower: A New Approach To Expanding Coverage of African Languages in Large Language Models](https://arxiv.org/abs/2510.07203)
Append: [Language Lives in Sparse Dimensions: Toward Interpretable and Efficient Multilingual Control for Large Language Models](https://arxiv.org/abs/2510.07213)
Append: [How much speech data is necessary for ASR in African languages? An evaluation of data scaling in Kinyarwanda and Kikuyu](https://arxiv.org/abs/2510.07221)
Append: [Where to Begin: Efficient Pretraining via Subnetwork Selection and Distillation](https://arxiv.org/abs/2510.07227)
Append: [Customer-R1: Personalized Simulation of Human Behaviors via RL-based LLM Agent in Online Shopping](https://arxiv.org/abs/2510.07230)
Append: [Benchmarking LLM Causal Reasoning with Scientifically Validated Relationships](https://arxiv.org/abs/2510.07231)
Append: [LAD-RAG: Layout-aware Dynamic RAG for Visually-Rich Document Understanding](https://arxiv.org/abs/2510.07233)
Append: [When Benchmarks Age: Temporal Misalignment through Large Language Model Factuality Evaluation](https://arxiv.org/abs/2510.07238)
Append: [Red-Bandit: Test-Time Adaptation for LLM Red-Teaming via Bandit-Guided LoRA Experts](https://arxiv.org/abs/2510.07239)
Append: [Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense](https://arxiv.org/abs/2510.07242)
Append: [LeMAJ (Legal LLM-as-a-Judge): Bridging Legal Reasoning and LLM Evaluation](https://arxiv.org/abs/2510.07243)
Append: [Don't Adapt Small Language Models for Tools; Adapt Tool Schemas to the Models](https://arxiv.org/abs/2510.07248)
Append: [Online Rubrics Elicitation from Pairwise Comparisons](https://arxiv.org/abs/2510.07284)
Append: [On the Convergence of Moral Self-Correction in Large Language Models](https://arxiv.org/abs/2510.07290)
Append: [Think Natively: Unlocking Multilingual Reasoning with Consistency-Enhanced Reinforcement Learning](https://arxiv.org/abs/2510.07300)
Append: [Agent Bain vs. Agent McKinsey: A New Text-to-SQL Benchmark for the Business Domain](https://arxiv.org/abs/2510.07309)
Append: [Vibe Checker: Aligning Code Evaluation with Human Preference](https://arxiv.org/abs/2510.07315)
Append: [Artificial Hippocampus Networks for Efficient Long-Context Modeling](https://arxiv.org/abs/2510.07318)
Append: [CML-Bench: A Framework for Evaluating and Enhancing LLM-Powered Movie Scripts Generation](https://arxiv.org/abs/2510.06231)
Append: [AlphaApollo: Orchestrating Foundation Models and Professional Tools into a Self-Evolving System for Deep Agentic Reasoning](https://arxiv.org/abs/2510.06261)
Append: [Asking For It: Question-Answering for Predicting Rule Infractions in Online Content Moderation](https://arxiv.org/abs/2510.06350)
Append: [PuzzlePlex: Benchmarking Foundation Models on Reasoning and Planning with Puzzles](https://arxiv.org/abs/2510.06475)
Append: [The Markovian Thinker](https://arxiv.org/abs/2510.06557)
Append: [Reading Between the Lines: Towards Reliable Black-box LLM Fingerprinting via Zeroth-order Gradient Estimation](https://arxiv.org/abs/2510.06605)
Append: [XLSR-Kanformer: A KAN-Intergrated model for Synthetic Speech Detection](https://arxiv.org/abs/2510.06706)
Append: [Differentially Private Synthetic Text Generation for Retrieval-Augmented Generation (RAG)](https://arxiv.org/abs/2510.06719)
Append: [Evaluating LLMs for Historical Document OCR: A Methodological Framework for Digital Humanities](https://arxiv.org/abs/2510.06743)
Append: [Evolving and Executing Research Plans via Double-Loop Multi-Agent Collaboration](https://arxiv.org/abs/2510.06761)
Append: [GPT-5 Model Corrected GPT-4V's Chart Reading Errors, Not Prompting](https://arxiv.org/abs/2510.06782)
Append: [Exposing Citation Vulnerabilities in Generative Engines](https://arxiv.org/abs/2510.06823)
Append: [Crossing Domains without Labels: Distant Supervision for Term Extraction](https://arxiv.org/abs/2510.06838)
Append: [Revisiting the Uniform Information Density Hypothesis in LLM Reasoning Traces](https://arxiv.org/abs/2510.06953)
Append: [VelLMes: A high-interaction AI-based deception framework](https://arxiv.org/abs/2510.06975)
Append: [RedTWIZ: Diverse LLM Red Teaming via Adaptive Attack Planning](https://arxiv.org/abs/2510.06994)
Append: [The Cognitive Bandwidth Bottleneck: Shifting Long-Horizon Agent from Planning with Actions to Planning with Schemas](https://arxiv.org/abs/2510.07091)
Append: [A Multi-Agent Framework for Stateful Inference-Time Search](https://arxiv.org/abs/2510.07147)
Append: [Machines in the Crowd? Measuring the Footprint of Machine-Generated Text on Reddit](https://arxiv.org/abs/2510.07226)
Append: [AudioMarathon: A Comprehensive Benchmark for Long-Context Audio Understanding and Efficiency in Audio LLMs](https://arxiv.org/abs/2510.07293)
Append: [ECLM: Entity Level Language Model for Spoken Language Understanding with Chain of Intent](https://arxiv.org/abs/2403.04481)
Append: [Approximately Aligned Decoding](https://arxiv.org/abs/2410.01103)
Append: [SuffixDecoding: Extreme Speculative Decoding for Emerging AI Applications](https://arxiv.org/abs/2411.04975)
Append: [Evil twins are not that evil: Qualitative insights into machine-generated prompts](https://arxiv.org/abs/2412.08127)
Append: [2 OLMo 2 Furious](https://arxiv.org/abs/2501.00656)
Append: [LatteReview: A Multi-Agent Framework for Systematic Review Automation Using Large Language Models](https://arxiv.org/abs/2501.05468)
Append: [Benchmarking Gaslighting Negation Attacks Against Multimodal Large Language Models](https://arxiv.org/abs/2501.19017)
Append: [Blessing of Multilinguality: A Systematic Analysis of Multilingual In-Context Learning](https://arxiv.org/abs/2502.11364)
Append: [Diagnosing Moral Reasoning Acquisition in Language Models: Pragmatics and Generalization](https://arxiv.org/abs/2502.16600)
Append: [Speculative Decoding and Beyond: An In-Depth Survey of Techniques](https://arxiv.org/abs/2502.19732)
Append: [Mind the (Belief) Gap: Group Identity in the World of LLMs](https://arxiv.org/abs/2503.02016)
Append: [Improving Neutral Point-of-View Generation with Data- and Parameter-Efficient RL](https://arxiv.org/abs/2503.03654)
Append: [MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree Search](https://arxiv.org/abs/2503.20757)
Append: [Rethinking Multilingual Continual Pretraining: Data Mixing for Adapting LLMs Across Languages and Resources](https://arxiv.org/abs/2504.04152)
Append: [GlotEval: A Test Suite for Massively Multilingual Evaluation of Large Language Models](https://arxiv.org/abs/2504.04155)
Append: [Geometry of Semantics in Next-Token Prediction: How Optimization Implicitly Organizes Linguistic Representations](https://arxiv.org/abs/2505.08348)
Append: [AutoRev: Multi-Modal Graph Retrieval for Automated Peer-Review Generation](https://arxiv.org/abs/2505.14376)
Append: [HopWeaver: Cross-Document Synthesis of High-Quality and Authentic Multi-Hop Questions](https://arxiv.org/abs/2505.15087)
Append: [FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management](https://arxiv.org/abs/2505.15347)
Append: [Do RAG Systems Really Suffer From Positional Bias?](https://arxiv.org/abs/2505.15561)
Append: [SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning Trajectory Synthesis](https://arxiv.org/abs/2505.16834)
Append: [The Unreasonable Effectiveness of Model Merging for Cross-Lingual Transfer in LLMs](https://arxiv.org/abs/2505.18356)
Append: [Guiding Giants: Lightweight Controllers for Weighted Activation Steering in LLMs](https://arxiv.org/abs/2505.20309)
Append: [360-LLaMA-Factory: Plug & Play Sequence Parallelism for Long Post-Training](https://arxiv.org/abs/2505.22296)
Append: [LiTEx: A Linguistic Taxonomy of Explanations for Understanding Within-Label Variation in Natural Language Inference](https://arxiv.org/abs/2505.22848)
Append: [InfiMed: Low-Resource Medical MLLMs with Advancing Understanding and Reasoning](https://arxiv.org/abs/2505.23867)
Append: [AutoMind: Adaptive Knowledgeable Agent for Automated Data Science](https://arxiv.org/abs/2506.10974)
Append: [MIST: Towards Multi-dimensional Implicit BiaS Evaluation of LLMs via Theory of Mind](https://arxiv.org/abs/2506.14161)
Append: [PredGen: Accelerated Inference of Large Language Models through Input-Time Speculation for Real-Time Speech Interaction](https://arxiv.org/abs/2506.15556)
Append: [Do LLMs Overthink Basic Math Reasoning? Benchmarking the Accuracy-Efficiency Tradeoff in Language Models](https://arxiv.org/abs/2507.04023)
Append: [GIIFT: Graph-guided Inductive Image-free Multimodal Machine Translation](https://arxiv.org/abs/2507.18562)
Append: [Show or Tell? Modeling the evolution of request-making in Human-LLM conversations](https://arxiv.org/abs/2508.01213)
Append: [ProCut: LLM Prompt Compression via Attribution Estimation](https://arxiv.org/abs/2508.02053)
Append: [Thinking with Nothinking Calibration: A New In-Context Learning Paradigm in Reasoning Large Language Models](https://arxiv.org/abs/2508.03363)
Append: [Sotopia-RL: Reward Design for Social Intelligence](https://arxiv.org/abs/2508.03905)
Append: [What Do Humans Hear When Interacting? Experiments on Selective Listening for Evaluating ASR of Spoken Dialogue Systems](https://arxiv.org/abs/2508.04402)
Append: [An Investigation of Robustness of LLMs in Mathematical Reasoning: Benchmarking with Mathematically-Equivalent Transformation of Advanced Mathematical Problems](https://arxiv.org/abs/2508.08833)
Append: [DESIGNER: Design-Logic-Guided Multidisciplinary Data Synthesis for LLM Reasoning](https://arxiv.org/abs/2508.12726)
Append: [Scaled Signed Averaging Improves In-Context and Early Learning Benchmark Performance in Small Transformers](https://arxiv.org/abs/2508.14685)
Append: [ParamBench: A Graduate-Level Benchmark for Evaluating LLM Understanding on Indic Subjects](https://arxiv.org/abs/2508.16185)
Append: [Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning](https://arxiv.org/abs/2508.19828)
Append: [The Percept-V Challenge: Can Multimodal LLMs Crack Simple Perception Problems?](https://arxiv.org/abs/2508.21143)
Append: [Probe-Rewrite-Evaluate: A Workflow for Reliable Benchmarks and Quantifying Evaluation Awareness](https://arxiv.org/abs/2509.00591)
Append: [From Injection to Defense: Constructing Edit-Based Fingerprints for Large Language Models](https://arxiv.org/abs/2509.03122)
Append: [Improving Factuality in LLMs via Inference-Time Knowledge Graph Construction](https://arxiv.org/abs/2509.03540)
Append: [TextMine: Data, Evaluation Framework and Ontology-guided LLM Pipeline for Humanitarian Mine Action](https://arxiv.org/abs/2509.15098)
Append: [SMARTER: A Data-efficient Framework to Improve Toxicity Detection with Explanation via Self-augmenting Large Language Models](https://arxiv.org/abs/2509.15174)
Append: [The Sound of Syntax: Finetuning and Comprehensive Evaluation of Language Models for Speech Pathology](https://arxiv.org/abs/2509.16765)
Append: [PARL-MT: Learning to Call Functions in Multi-Turn Conversation with Progress Awareness](https://arxiv.org/abs/2509.23206)
Append: [LLM Hallucination Detection: HSAD](https://arxiv.org/abs/2509.23580)
Append: [Text-Based Approaches to Item Alignment to Content Standards in Large-Scale Reading & Writing Tests](https://arxiv.org/abs/2509.26431)
Append: [LLMVA-GEBC: Large Language Model with Video Adapter for Generic Event Boundary Captioning](https://arxiv.org/abs/2306.10354)
Append: [LaunchpadGPT: Language Model as Music Visualization Designer on Launchpad](https://arxiv.org/abs/2307.04827)
Append: [Transparent and Coherent Procedural Mistake Detection](https://arxiv.org/abs/2412.11927)
Append: [Emilia: A Large-Scale, Extensive, Multilingual, and Diverse Dataset for Speech Generation](https://arxiv.org/abs/2501.15907)
Append: [Taxonomy, Opportunities, and Challenges of Representation Engineering for Large Language Models](https://arxiv.org/abs/2502.19649)
Append: [An Illusion of Progress? Assessing the Current State of Web Agents](https://arxiv.org/abs/2504.01382)
Append: [Controlled Agentic Planning & Reasoning for Mechanism Synthesis](https://arxiv.org/abs/2505.17607)
Append: [Roboflow100-VL: A Multi-Domain Object Detection Benchmark for Vision-Language Models](https://arxiv.org/abs/2505.20612)
Append: [Dyna-Think: Synergizing Reasoning, Acting, and World Model Simulation in AI Agents](https://arxiv.org/abs/2506.00320)
Append: [Prefilled responses enhance zero-shot detection of AI-generated images](https://arxiv.org/abs/2506.11031)
Append: [Enhancing Few-shot Keyword Spotting Performance through Pre-Trained Self-supervised Speech Models](https://arxiv.org/abs/2506.17686)
Append: [KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality](https://arxiv.org/abs/2506.19807)
Append: [Can AI Have a Personality? Prompt Engineering for AI Personality Simulation: A Chatbot Case Study in Gender-Affirming Voice Therapy Training](https://arxiv.org/abs/2508.18234)
Append: [Membership Inference Attacks on LLM-based Recommender Systems](https://arxiv.org/abs/2508.18665)
append_entries: 190
Finish: 2025-10-09 04:25:04.173560
------------------------------------------------------
Started: 2025-10-09 06:25:26.820809
Existing_entries: 1190
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1661
Summarized using GPT-3.5-turbo
Append: [Injecting External Knowledge into the Reasoning Process Enhances Retrieval-Augmented Generation](https://arxiv.org/abs/2507.19333)
append_entries: 1
Finish: 2025-10-09 06:25:29.541060
------------------------------------------------------
Started: 2025-10-09 08:21:39.748787
Existing_entries: 1001
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-09 08:21:40.229074
------------------------------------------------------
Started: 2025-10-09 10:17:42.751540
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-09 10:17:43.135196
------------------------------------------------------
Started: 2025-10-09 12:34:23.432836
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-09 12:34:23.853062
------------------------------------------------------
Started: 2025-10-09 14:16:15.394061
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-09 14:16:15.867911
------------------------------------------------------
Started: 2025-10-09 16:21:09.735420
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-09 16:21:10.154189
------------------------------------------------------
Started: 2025-10-09 18:23:04.181961
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-09 18:23:04.597043
------------------------------------------------------
Started: 2025-10-09 20:18:23.089189
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-09 20:18:23.511077
------------------------------------------------------
Started: 2025-10-09 22:14:28.533232
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-09 22:14:29.049310
------------------------------------------------------
Started: 2025-10-10 01:13:37.914042
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-10 01:13:38.409291
------------------------------------------------------
Started: 2025-10-10 02:58:40.417507
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-10 02:58:41.156005
------------------------------------------------------
Started: 2025-10-10 04:23:12.657936
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1619
Summarized using GPT-3.5-turbo
Append: [Inconsistent Affective Reaction: Sentiment of Perception and Opinion in Urban Environments](https://arxiv.org/abs/2510.07359)
Token length: 1609
Summarized using GPT-3.5-turbo
Append: [Haystack Engineering: Context Engineering for Heterogeneous and Agentic Long-Context Evaluation](https://arxiv.org/abs/2510.07414)
Token length: 1278
Summarized using GPT-3.5-turbo
Append: [Lemma Dilemma: On Lemma Generation Without Domain- or Language-Specific Training Data](https://arxiv.org/abs/2510.07434)
Token length: 838
Summarized using GPT-3.5-turbo
Append: [LASER: An LLM-based ASR Scoring and Evaluation Rubric](https://arxiv.org/abs/2510.07437)
Token length: 657
Summarized using GPT-3.5-turbo
Append: [Meaningful Pose-Based Sign Language Evaluation](https://arxiv.org/abs/2510.07453)
Token length: 1161
Summarized using GPT-3.5-turbo
Append: [Populism Meets AI: Advancing Populism Research with LLMs](https://arxiv.org/abs/2510.07458)
Token length: 1704
Summarized using GPT-3.5-turbo
Append: [MAPRO: Recasting Multi-Agent Prompt Optimization as Maximum a Posteriori Inference](https://arxiv.org/abs/2510.07475)
Token length: 1904
Summarized using GPT-3.5-turbo
Append: [AsyncSpade: Efficient Test-Time Scaling with Asynchronous Sparse Decoding](https://arxiv.org/abs/2510.07486)
Token length: 842
Summarized using GPT-3.5-turbo
Append: [Can Lessons From Human Teams Be Applied to Multi-Agent Systems? The Role of Structure, Diversity, and Interaction Dynamics](https://arxiv.org/abs/2510.07488)
Token length: 1495
Summarized using GPT-3.5-turbo
Append: [Can Speech LLMs Think while Listening?](https://arxiv.org/abs/2510.07497)
Token length: 1235
Summarized using GPT-3.5-turbo
Append: [When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs](https://arxiv.org/abs/2510.07499)
Token length: 1495
Summarized using GPT-3.5-turbo
Append: [ParsTranslit: Truly Versatile Tajik-Farsi Transliteration](https://arxiv.org/abs/2510.07520)
Token length: 1011
Summarized using GPT-3.5-turbo
Append: [OWL: Overcoming Window Length-Dependence in Speculative Decoding for Long-Context Inputs](https://arxiv.org/abs/2510.07535)
Token length: 1263
Summarized using GPT-3.5-turbo
Append: [Deploying Tiny LVLM Judges for Real-World Evaluation of Chart Models: Lessons Learned and Best Practices](https://arxiv.org/abs/2510.07545)
Token length: 1154
Summarized using GPT-3.5-turbo
Append: [Multi-Task Pre-Finetuning of Lightweight Transformer Encoders for Text Classification and NER](https://arxiv.org/abs/2510.07566)
Token length: 1578
Summarized using GPT-3.5-turbo
Append: [Linguistic Patterns in Pandemic-Related Content: A Comparative Analysis of COVID-19, Constraint, and Monkeypox Datasets](https://arxiv.org/abs/2510.07579)
Token length: 1948
Summarized using GPT-3.5-turbo
Append: [IASC: Interactive Agentic System for ConLangs](https://arxiv.org/abs/2510.07591)
Token length: 1341
Summarized using GPT-3.5-turbo
Append: [Vocabulary embeddings organize linguistic structure early in language model training](https://arxiv.org/abs/2510.07613)
Token length: 1082
Summarized using GPT-3.5-turbo
Append: [Toward Reliable Clinical Coding with Language Models: Verification and Lightweight Adaptation](https://arxiv.org/abs/2510.07629)
Token length: 1167
Summarized using GPT-3.5-turbo
Append: [Role-Conditioned Refusals: Evaluating Access Control Reasoning in Large Language Models](https://arxiv.org/abs/2510.07642)
Token length: 1171
Summarized using GPT-3.5-turbo
Append: [Banking Done Right: Redefining Retail Banking with Language-Centric AI](https://arxiv.org/abs/2510.07645)
Token length: 1335
Summarized using GPT-3.5-turbo
Append: [OBCache: Optimal Brain KV Cache Pruning for Efficient Long-Context LLM Inference](https://arxiv.org/abs/2510.07651)
Token length: 1058
Summarized using GPT-3.5-turbo
Append: [Textual Entailment and Token Probability as Bias Evaluation Metrics](https://arxiv.org/abs/2510.07662)
Token length: 1691
Summarized using GPT-3.5-turbo
Append: [Stress-Testing Model Specs Reveals Character Differences among Language Models](https://arxiv.org/abs/2510.07686)
Token length: 795
Summarized using GPT-3.5-turbo
Append: [Large Language Models Meet Virtual Cell: A Survey](https://arxiv.org/abs/2510.07706)
Token length: 1516
Summarized using GPT-3.5-turbo
Append: [Causality Guided Representation Learning for Cross-Style Hate Speech Detection](https://arxiv.org/abs/2510.07707)
Token length: 1609
Summarized using GPT-3.5-turbo
Append: [MemWeaver: A Hierarchical Memory from Textual Interactive Behaviors for Personalized Generation](https://arxiv.org/abs/2510.07713)
Token length: 1103
Summarized using GPT-3.5-turbo
Append: [SUBQRAG: sub-question driven dynamic graph rag](https://arxiv.org/abs/2510.07718)
Token length: 1364
Summarized using GPT-3.5-turbo
Append: [Multilingual Knowledge Graph Completion via Efficient Multilingual Knowledge Sharing](https://arxiv.org/abs/2510.07736)
Token length: 1374
Summarized using GPT-3.5-turbo
Append: [ToolExpander: Extending the Frontiers of Tool-Using Reinforcement Learning to Weak LLMs](https://arxiv.org/abs/2510.07737)
Token length: 1482
Summarized using GPT-3.5-turbo
Append: [OpenRubrics: Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment](https://arxiv.org/abs/2510.07743)
Token length: 1391
Summarized using GPT-3.5-turbo
Append: [Parallel Test-Time Scaling for Latent Reasoning Models](https://arxiv.org/abs/2510.07745)
Token length: 1081
Summarized using GPT-3.5-turbo
Append: [Test-Time Reasoners Are Strategic Multiple-Choice Test-Takers](https://arxiv.org/abs/2510.07761)
Token length: 1795
Summarized using GPT-3.5-turbo
Append: [ToolLibGen: Scalable Automatic Tool Creation and Aggregation for LLM Reasoning](https://arxiv.org/abs/2510.07768)
Token length: 1632
Summarized using GPT-3.5-turbo
Append: [Curing Miracle Steps in LLM Mathematical Reasoning with Rubric Rewards](https://arxiv.org/abs/2510.07774)
Token length: 1378
Summarized using GPT-3.5-turbo
Append: [The Unintended Trade-off of AI Alignment:Balancing Hallucination Mitigation and Safety in LLMs](https://arxiv.org/abs/2510.07775)
Token length: 1366
Summarized using GPT-3.5-turbo
Append: [Instance Relation Learning Network with Label Knowledge Propagation for Few-shot Multi-label Intent Detection](https://arxiv.org/abs/2510.07776)
Token length: 1656
Summarized using GPT-3.5-turbo
Append: [Drift No More? Context Equilibria in Multi-Turn LLM Interactions](https://arxiv.org/abs/2510.07777)
Token length: 1707
Summarized using GPT-3.5-turbo
Append: [RCPU: Rotation-Constrained Error Compensation for Structured Pruning of a Large Language Model](https://arxiv.org/abs/2510.07782)
Token length: 1285
Summarized using GPT-3.5-turbo
Append: [LLM4Cell: A Survey of Large Language and Agentic Models for Single-Cell Biology](https://arxiv.org/abs/2510.07793)
Token length: 1876
Summarized using GPT-3.5-turbo
Append: [HiPRAG: Hierarchical Process Rewards for Efficient Agentic Retrieval Augmented Generation](https://arxiv.org/abs/2510.07794)
Token length: 1456
Summarized using GPT-3.5-turbo
Append: [Dynamic Generation of Multi-LLM Agents Communication Topologies with Graph Diffusion Models](https://arxiv.org/abs/2510.07799)
Token length: 1045
Summarized using GPT-3.5-turbo
Append: [Multilingual Generative Retrieval via Cross-lingual Semantic Compression](https://arxiv.org/abs/2510.07812)
Token length: 1132
Summarized using GPT-3.5-turbo
Append: [AdaSwitch: Adaptive Switching Generation for Knowledge Distillation](https://arxiv.org/abs/2510.07842)
Token length: 1190
Summarized using GPT-3.5-turbo
Append: [Ready to Translate, Not to Represent? Bias and Performance Gaps in Multilingual LLMs Across Language Families and Domains](https://arxiv.org/abs/2510.07877)
Token length: 1743
Summarized using GPT-3.5-turbo
Append: [Do LLMs Really Need 10+ Thoughts for "Find the Time 1000 Days Later"? Towards Structural Understanding of LLM Overthinking](https://arxiv.org/abs/2510.07880)
Token length: 1138
Summarized using GPT-3.5-turbo
Append: [CS3-Bench: Evaluating and Enhancing Speech-to-Speech LLMs for Mandarin-English Code-Switching](https://arxiv.org/abs/2510.07881)
Token length: 1352
Summarized using GPT-3.5-turbo
Append: [Contrastive Weak-to-strong Generalization](https://arxiv.org/abs/2510.07884)
Token length: 1039
Summarized using GPT-3.5-turbo
Append: [Standard-to-Dialect Transfer Trends Differ across Text and Speech: A Case Study on Intent and Topic Classification in German Dialects](https://arxiv.org/abs/2510.07890)
Token length: 1228
Summarized using GPT-3.5-turbo
Append: [Metric Calculating Benchmark: Code-Verifiable Complicate Instruction Following Benchmark for Large Language Models](https://arxiv.org/abs/2510.07892)
Append: [ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual Recall](https://arxiv.org/abs/2510.07896)
Append: [Towards Human-Like Grading: A Unified LLM-Enhanced Framework for Subjective Question Evaluation](https://arxiv.org/abs/2510.07912)
Append: [STEPER: Step-wise Knowledge Distillation for Enhancing Reasoning Ability in Multi-Step Retrieval-Augmented Language Models](https://arxiv.org/abs/2510.07923)
Append: [Comprehensiveness Metrics for Automatic Evaluation of Factual Recall in Text Generation](https://arxiv.org/abs/2510.07926)
Append: [Vision-Enabled LLMs in Historical Lexicography: Digitising and Enriching Estonian-German Dictionaries from the 17th and 18th Centuries](https://arxiv.org/abs/2510.07931)
Append: [A$^2$Search: Ambiguity-Aware Question Answering with Reinforcement Learning](https://arxiv.org/abs/2510.07958)
Append: [LightReasoner: Can Small Language Models Teach Large Language Models Reasoning?](https://arxiv.org/abs/2510.07962)
Append: [Active Confusion Expression in Large Language Models: Leveraging World Models toward Better Social Reasoning](https://arxiv.org/abs/2510.07974)
Append: [Leveraging Author-Specific Context for Scientific Figure Caption Generation: 3rd SciCap Challenge](https://arxiv.org/abs/2510.07993)
Append: [Learning on the Job: An Experience-Driven Self-Evolving Agent for Long-Horizon Tasks](https://arxiv.org/abs/2510.08002)
Append: [ChatGPT as a Translation Engine: A Case Study on Japanese-English](https://arxiv.org/abs/2510.08042)
Append: [Climate Knowledge in Large Language Models](https://arxiv.org/abs/2510.08043)
Append: [A Survey of Process Reward Models: From Outcome Signals to Process Supervisions for Large Language Models](https://arxiv.org/abs/2510.08049)
Append: [FedDTRE: Federated Dialogue Generation Models Powered by Trustworthiness Evaluation](https://arxiv.org/abs/2510.08058)
Append: [Everything is Plausible: Investigating the Impact of LLM Rationales on Human Notions of Plausibility](https://arxiv.org/abs/2510.08091)
Append: [The Price of Thought: A Multilingual Analysis of Reasoning, Performance, and Cost of Negotiation in Large Language Models](https://arxiv.org/abs/2510.08098)
Append: [Lossless Vocabulary Reduction for Auto-Regressive Language Models](https://arxiv.org/abs/2510.08102)
Append: [Evaluating LLM-Generated Legal Explanations for Regulatory Compliance in Social Media Influencer Marketing](https://arxiv.org/abs/2510.08111)
Append: [Interpreting LLM-as-a-Judge Policies via Verifiable Global Explanations](https://arxiv.org/abs/2510.08120)
Append: [Mitigating Judgment Preference Bias in Large Language Models through Group-Based Polling](https://arxiv.org/abs/2510.08145)
Append: [AI Knowledge Assist: An Automated Approach for the Creation of Knowledge Bases for Conversational AI Agents](https://arxiv.org/abs/2510.08149)
Append: [DACIP-RC: Domain Adaptive Continual Instruction Pre-Training via Reading Comprehension on Business Conversations](https://arxiv.org/abs/2510.08152)
Append: [Beyond Over-Refusal: Scenario-Based Diagnostics and Post-Hoc Mitigation for Exaggerated Refusals in LLMs](https://arxiv.org/abs/2510.08158)
Append: [ARM2: Adaptive Reasoning Model with Vision Understanding and Executable Code](https://arxiv.org/abs/2510.08163)
Append: [METRICALARGS: A Taxonomy for Studying Metrical Poetry with LLMs](https://arxiv.org/abs/2510.08188)
Append: [Training-Free Group Relative Policy Optimization](https://arxiv.org/abs/2510.08191)
Append: [Memory Retrieval and Consolidation in Large Language Models through Function Tokens](https://arxiv.org/abs/2510.08203)
Append: [LLMs Learn to Deceive Unintentionally: Emergent Misalignment in Dishonesty from Misaligned Samples to Biased Human-AI Interactions](https://arxiv.org/abs/2510.08211)
Append: [SenWave: A Fine-Grained Multi-Language Sentiment Analysis Dataset Sourced from COVID-19 Tweets](https://arxiv.org/abs/2510.08214)
Append: [Investigating Counterclaims in Causality Extraction from Text](https://arxiv.org/abs/2510.08224)
Append: [The Alignment Waltz: Jointly Training Agents to Collaborate for Safety](https://arxiv.org/abs/2510.08240)
Append: [Contrastive Decoding for Synthetic Data Generation in Low-Resource Language Modeling](https://arxiv.org/abs/2510.08245)
Append: [Beyond Turn Limits: Training Deep Search Agents with Dynamic Context Window](https://arxiv.org/abs/2510.08276)
Append: [Neuron-Level Analysis of Cultural Understanding in Large Language Models](https://arxiv.org/abs/2510.08284)
Append: [AutoRed: A Free-form Adversarial Prompt Generation Framework for Automated Red Teaming](https://arxiv.org/abs/2510.08329)
Append: [Two-Stage Voting for Robust and Efficient Suicide Risk Detection on Social Media](https://arxiv.org/abs/2510.08365)
Append: [On the Relationship Between the Choice of Representation and In-Context Learning](https://arxiv.org/abs/2510.08372)
Append: [If Probable, Then Acceptable? Understanding Conditional Acceptability Judgments in Large Language Models](https://arxiv.org/abs/2510.08388)
Append: [Single layer tiny Co$^4$ outpaces GPT-2 and GPT-BERT](https://arxiv.org/abs/2510.08404)
Append: [ARES: Multimodal Adaptive Reasoning via Difficulty-Aware Token-Level Entropy Shaping](https://arxiv.org/abs/2510.08457)
Append: [LeWiDi-2025 at NLPerspectives: The Third Edition of the Learning with Disagreements Shared Task](https://arxiv.org/abs/2510.08460)
Append: [DeepPrune: Parallel Scaling without Inter-trace Redundancy](https://arxiv.org/abs/2510.08483)
Append: [Neologism Learning for Controllability and Self-Verbalization](https://arxiv.org/abs/2510.08506)
Append: [Efficient Prompt Optimisation for Legal Text Classification with Proxy Prompt Evaluator](https://arxiv.org/abs/2510.08524)
Append: [Which Heads Matter for Reasoning? RL-Guided KV Cache Compression](https://arxiv.org/abs/2510.08525)
Append: [CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards](https://arxiv.org/abs/2510.08529)
Append: [ArenaBencher: Automatic Benchmark Evolution via Multi-Model Competitive Evaluation](https://arxiv.org/abs/2510.08569)
Append: [AI LLM Proof of Self-Consciousness and User-Specific Attractors](https://arxiv.org/abs/2508.18302)
Append: [ConCuR: Conciseness Makes State-of-the-Art Kernel Generation](https://arxiv.org/abs/2510.07356)
Append: [Encode, Think, Decode: Scaling test-time reasoning with recursive latent thoughts](https://arxiv.org/abs/2510.07358)
Append: [PATCH: Mitigating PII Leakage in Language Models with Privacy-Aware Targeted Circuit PatcHing](https://arxiv.org/abs/2510.07452)
Append: [Evaluation of LLMs for Process Model Analysis and Optimization](https://arxiv.org/abs/2510.07489)
Append: [CompassLLM: A Multi-Agent Approach toward Geo-Spatial Reasoning for Popular Path Query](https://arxiv.org/abs/2510.07516)
Append: [LLM Unlearning Under the Microscope: A Full-Stack View on Methods and Metrics](https://arxiv.org/abs/2510.07626)
Append: [Test-Time Matching: Unlocking Compositional Reasoning in Multimodal Models](https://arxiv.org/abs/2510.07632)
Append: [LiveThinking: Enabling Real-Time Efficient Reasoning for AI-Powered Livestreaming via Reinforcement Learning](https://arxiv.org/abs/2510.07685)
Append: [Multimodal Safety Evaluation in Generative Agent Social Simulations](https://arxiv.org/abs/2510.07709)
Append: [Who Stole Your Data? A Method for Detecting Unauthorized RAG Theft](https://arxiv.org/abs/2510.07728)
Append: [oMeBench: Towards Robust Benchmarking of LLMs in Organic Mechanism Elucidation and Reasoning](https://arxiv.org/abs/2510.07731)
Append: [From Keywords to Clusters: AI-Driven Analysis of YouTube Comments to Reveal Election Issue Salience in 2024](https://arxiv.org/abs/2510.07821)
Append: [MetaDefense: Defending Finetuning-based Jailbreak Attack Before and During Generation](https://arxiv.org/abs/2510.07835)
Append: [Self-Improving LLM Agents at Test-Time](https://arxiv.org/abs/2510.07841)
Append: [TTOM: Test-Time Optimization and Memorization for Compositional Video Generation](https://arxiv.org/abs/2510.07940)
Append: [VoiceAgentBench: Are Voice Assistants ready for agentic tasks?](https://arxiv.org/abs/2510.07978)
Append: [Pseudo2Real: Task Arithmetic for Pseudo-Label Correction in Automatic Speech Recognition](https://arxiv.org/abs/2510.08047)
Append: [TaoSR-AGRL: Adaptive Guided Reinforcement Learning Framework for E-commerce Search Relevance](https://arxiv.org/abs/2510.08048)
Append: [AutoQual: An LLM Agent for Automated Discovery of Interpretable Features for Review Quality Assessment](https://arxiv.org/abs/2510.08081)
Append: [VersionRAG: Version-Aware Retrieval-Augmented Generation for Evolving Documents](https://arxiv.org/abs/2510.08109)
Append: [Can Risk-taking AI-Assistants suitably represent entities](https://arxiv.org/abs/2510.08114)
Append: [NavSpace: How Navigation Agents Follow Spatial Intelligence Instructions](https://arxiv.org/abs/2510.08173)
Append: [R-Horizon: How Far Can Your Large Reasoning Model Really Go in Breadth and Depth?](https://arxiv.org/abs/2510.08189)
Append: [Sentiment Matters: An Analysis of 200 Human-SAV Interactions](https://arxiv.org/abs/2510.08202)
Append: [ReasonEmbed: Enhanced Text Embeddings for Reasoning-Intensive Document Retrieval](https://arxiv.org/abs/2510.08252)
Append: [Opponent Shaping in LLM Agents](https://arxiv.org/abs/2510.08255)
Append: [Mix- and MoE-DPO: A Variational Inference Approach to Direct Preference Optimization](https://arxiv.org/abs/2510.08256)
Append: [Beyond Pass@k: Breadth-Depth Metrics for Reasoning Boundaries](https://arxiv.org/abs/2510.08325)
Append: [FlyLoRA: Boosting Task Decoupling and Parameter Efficiency via Implicit Rank-Wise Mixture-of-Experts](https://arxiv.org/abs/2510.08396)
Append: [xRouter: Training Cost-Aware LLMs Orchestration System via Reinforcement Learning](https://arxiv.org/abs/2510.08439)
Append: [Looking to Learn: Token-wise Dynamic Gating for Low-Resource Vision-Language Modelling](https://arxiv.org/abs/2510.08470)
Append: [The Visual Iconicity Challenge: Evaluating Vision-Language Models on Sign Language Form-Meaning Mapping](https://arxiv.org/abs/2510.08482)
Append: [To Sink or Not to Sink: Visual Information Pathways in Large Vision-Language Models](https://arxiv.org/abs/2510.08510)
Append: [AutoMLGen: Navigating Fine-Grained Optimization for Coding Agents](https://arxiv.org/abs/2510.08511)
Append: [SliceFine: The Universal Winning-Slice Hypothesis for Pretrained Networks](https://arxiv.org/abs/2510.08513)
Append: [CaRT: Teaching LLM Agents to Know When They Know Enough](https://arxiv.org/abs/2510.08517)
Append: [SpatialLadder: Progressive Training for Spatial Reasoning in Vision-Language Models](https://arxiv.org/abs/2510.08531)
Append: [VideoNorms: Benchmarking Cultural Awareness of Video Language Models](https://arxiv.org/abs/2510.08543)
Append: [Agent Learning via Early Experience](https://arxiv.org/abs/2510.08558)
Append: [MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning](https://arxiv.org/abs/2510.08567)
Append: [Evaluating LLMs' Mathematical Reasoning in Financial Document Question Answering](https://arxiv.org/abs/2402.11194)
Append: [ThinkNote: Enhancing Knowledge Integration and Utilization of Large Language Models via Constructivist Cognition Modeling](https://arxiv.org/abs/2402.13547)
Append: [Depression Detection on Social Media with Large Language Models](https://arxiv.org/abs/2403.10750)
Append: [Expert-Token Resonance MoE: Bidirectional Routing with Efficiency Affinity-Driven Active Selection](https://arxiv.org/abs/2406.00023)
Append: [Multi-Source Knowledge Pruning for Retrieval-Augmented Generation: A Benchmark and Empirical Study](https://arxiv.org/abs/2409.13694)
Append: [TokenSelect: Efficient Long-Context Inference and Length Extrapolation for LLMs via Dynamic Token-Level KV Cache Selection](https://arxiv.org/abs/2411.02886)
Append: [EpiCoder: Encompassing Diversity and Complexity in Code Generation](https://arxiv.org/abs/2501.04694)
Append: [Med-R$^2$: Crafting Trustworthy LLM Physicians via Retrieval and Reasoning of Evidence-Based Medicine](https://arxiv.org/abs/2501.11885)
Append: [Mitigating Forgetting in LLM Fine-Tuning via Low-Perplexity Token Learning](https://arxiv.org/abs/2501.14315)
Append: [Examining Multilingual Embedding Models Cross-Lingually Through LLM-Generated Adversarial Examples](https://arxiv.org/abs/2502.08638)
Append: [Less is More: Compact Clue Selection for Efficient Retrieval-Augmented Generation Reasoning](https://arxiv.org/abs/2502.11811)
Append: [MoM: Linear Sequence Modeling with Mixture-of-Memories](https://arxiv.org/abs/2502.13685)
Append: [Beyond Single Frames: Can LMMs Comprehend Temporal and Contextual Narratives in Image Sequences?](https://arxiv.org/abs/2502.13925)
Append: [Erasing Without Remembering: Implicit Knowledge Forgetting in Large Language Models](https://arxiv.org/abs/2502.19982)
Append: [Argument Summarization and its Evaluation in the Era of Large Language Models](https://arxiv.org/abs/2503.00847)
Append: [Sherkala-Chat: Building a State-of-the-Art LLM for Kazakh in a Moderately Resourced Setting](https://arxiv.org/abs/2503.01493)
Append: [Enhancing LLM Reliability via Explicit Knowledge Boundary Modeling](https://arxiv.org/abs/2503.02233)
Append: [Teaching Your Models to Understand Code via Focal Preference Alignment](https://arxiv.org/abs/2503.02783)
Append: [DiMA: An LLM-Powered Ride-Hailing Assistant at DiDi](https://arxiv.org/abs/2503.04768)
Append: [UniEDU: A Unified Language and Vision Assistant for Education Applications](https://arxiv.org/abs/2503.20701)
Append: [Adaptive Layer-skipping in Pre-trained LLMs](https://arxiv.org/abs/2503.23798)
Append: [Can LLMs Grasp Implicit Cultural Values? Benchmarking LLMs' Cultural Intelligence with CQ-Bench](https://arxiv.org/abs/2504.01127)
Append: [Hallucination Detection in LLMs with Topological Divergence on Attention Graphs](https://arxiv.org/abs/2504.10063)
Append: [Science Hierarchography: Hierarchical Organization of Science Literature](https://arxiv.org/abs/2504.13834)
Append: [T-VEC: A Telecom-Specific Vectorization Model with Enhanced Semantic Understanding via Deep Triplet Loss Fine-Tuning](https://arxiv.org/abs/2504.16460)
Append: [Evaluating Evaluation Metrics -- The Mirage of Hallucination Detection](https://arxiv.org/abs/2504.18114)
Append: [Say It Another Way: Auditing LLMs with a User-Grounded Automated Paraphrasing Framework](https://arxiv.org/abs/2505.03563)
Append: [Hakim: Farsi Text Embedding Model](https://arxiv.org/abs/2505.08435)
Append: [Logic Jailbreak: Efficiently Unlocking LLM Safety Restrictions Through Formal Logical Expression](https://arxiv.org/abs/2505.13527)
Append: [WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2505.16421)
Append: [What Media Frames Reveal About Stance: A Dataset and Study about Memes in Climate Change Discourse](https://arxiv.org/abs/2505.16592)
Append: [UNCLE: Benchmarking Uncertainty Expressions in Long-Form Generation](https://arxiv.org/abs/2505.16922)
Append: [Search Wisely: Mitigating Sub-optimal Agentic Searches By Reducing Uncertainty](https://arxiv.org/abs/2505.17281)
Append: [Inference-time Alignment in Continuous Space](https://arxiv.org/abs/2505.20081)
Append: [BiomedSQL: Text-to-SQL for Scientific Reasoning on Biomedical Knowledge Bases](https://arxiv.org/abs/2505.20321)
Append: [Trans-EnV: A Framework for Evaluating the Linguistic Robustness of LLMs Against English Varieties](https://arxiv.org/abs/2505.20875)
Append: [FlashDLM: Accelerating Diffusion Language Model Inference via Efficient KV Caching and Guided Diffusion](https://arxiv.org/abs/2505.21467)
Append: [FlowNIB: An Information Bottleneck Analysis of Bidirectional vs. Unidirectional Language Models](https://arxiv.org/abs/2506.00859)
Append: [Tug-of-war between idioms' figurative and literal interpretations in LLMs](https://arxiv.org/abs/2506.01723)
Append: [Dissecting Logical Reasoning in LLMs: A Fine-Grained Evaluation and Supervision Study](https://arxiv.org/abs/2506.04810)
Append: [From Handwriting to Feedback: Evaluating VLMs and LLMs for AI-Powered Assessment in Indonesian Classrooms](https://arxiv.org/abs/2506.04822)
Append: [ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning](https://arxiv.org/abs/2506.09513)
Append: [Breaking the Reviewer: Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks](https://arxiv.org/abs/2506.11113)
Append: [Language Surgery in Multilingual Large Language Models](https://arxiv.org/abs/2506.12450)
Append: [How Grounded is Wikipedia? A Study on Structured Evidential Support and Retrieval](https://arxiv.org/abs/2506.12637)
Append: [The Role of Model Confidence on Bias Effects in Measured Uncertainties for Vision-Language Models](https://arxiv.org/abs/2506.16724)
Append: [Scaling Laws Are Unreliable for Downstream Tasks: A Reality Check](https://arxiv.org/abs/2507.00885)
Append: [Truth, Trust, and Trouble: Medical AI on the Edge](https://arxiv.org/abs/2507.02983)
Append: [Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers](https://arxiv.org/abs/2507.06223)
Append: [Multi-Trigger Poisoning Amplifies Backdoor Vulnerabilities in LLMs](https://arxiv.org/abs/2507.11112)
Append: [LLMs Encode Harmfulness and Refusal Separately](https://arxiv.org/abs/2507.11878)
Append: [The Behavioural Translation Style Space: Towards simulating the temporal dynamics of affect, behaviour, and cognition in human translation production](https://arxiv.org/abs/2507.12208)
Append: [FLEXITOKENS: Flexible Tokenization for Evolving Language Models](https://arxiv.org/abs/2507.12720)
Append: [From Feedback to Checklists: Grounded Evaluation of AI-Generated Clinical Notes](https://arxiv.org/abs/2507.17717)
Append: [Can Small-Scale Data Poisoning Exacerbate Dialect-Linked Biases in Large Language Models?](https://arxiv.org/abs/2507.19195)
Append: [Flora: Effortless Context Construction to Arbitrary Length and Scale](https://arxiv.org/abs/2507.19786)
Append: [Med-R$^3$: Enhancing Medical Retrieval-Augmented Reasoning of LLMs via Progressive Reinforcement Learning](https://arxiv.org/abs/2507.23541)
Append: [CoCoA: Collaborative Chain-of-Agents for Parametric-Retrieved Knowledge Synergy](https://arxiv.org/abs/2508.01696)
Append: [Long Chain-of-Thought Reasoning Across Languages](https://arxiv.org/abs/2508.14828)
Append: [Spotlight Attention: Towards Efficient LLM Generation via Non-linear Hashing-based KV Cache Retrieval](https://arxiv.org/abs/2508.19740)
Append: [Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning](https://arxiv.org/abs/2508.21589)
Append: [Modeling Motivated Reasoning in Law: Evaluating Strategic Role Conditioning in LLM Summarization](https://arxiv.org/abs/2509.00529)
Append: [Training LLMs to be Better Text Embedders through Bidirectional Reconstruction](https://arxiv.org/abs/2509.03020)
Append: [A Survey of Reinforcement Learning for Large Reasoning Models](https://arxiv.org/abs/2509.08827)
Append: [HiChunk: Evaluating and Enhancing Retrieval-Augmented Generation with Hierarchical Chunking](https://arxiv.org/abs/2509.11552)
Append: [From Correction to Mastery: Reinforced Distillation of Large Language Model Agents](https://arxiv.org/abs/2509.14257)
Append: [DNA-DetectLLM: Unveiling AI-Generated Text via a DNA-Inspired Mutation-Repair Paradigm](https://arxiv.org/abs/2509.15550)
Append: [Evaluating LLM-Generated Versus Human-Authored Responses in Role-Play Dialogues](https://arxiv.org/abs/2509.17694)
Append: [Speculate Deep and Accurate: Lossless and Training-Free Acceleration for Offloaded LLMs via Substitute Speculative Decoding](https://arxiv.org/abs/2509.18344)
Append: [QoNext: Towards Next-generation QoE for Foundation Models](https://arxiv.org/abs/2509.21889)
Append: [InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced Language Models](https://arxiv.org/abs/2509.22536)
Append: [Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement Learning](https://arxiv.org/abs/2509.26383)
Append: [Matryoshka Pilot: Learning to Drive Black-Box LLMs with LLMs](https://arxiv.org/abs/2410.20749)
Append: [AutoAgent: A Fully-Automated and Zero-Code Framework for LLM Agents](https://arxiv.org/abs/2502.05957)
Append: [Instructor-Worker Large Language Model System for Policy Recommendation: a Case Study on Air Quality Analysis of the January 2025 Los Angeles Wildfires](https://arxiv.org/abs/2503.00566)
Append: [More Bang for the Buck: Process Reward Modeling with Entropy-Driven Uncertainty](https://arxiv.org/abs/2503.22233)
Append: [$\textit{Agents Under Siege}$: Breaking Pragmatic Multi-Agent LLM Systems with Optimized Prompt Attacks](https://arxiv.org/abs/2504.00218)
Append: [Utility-Focused LLM Annotation for Retrieval and Retrieval-Augmented Generation](https://arxiv.org/abs/2504.05220)
Append: [Efficient and Adaptable Overlapping for Computation and Communication via Signaling and Reordering](https://arxiv.org/abs/2504.19519)
Append: [Advancing AI Research Assistants with Expert-Involved Learning](https://arxiv.org/abs/2505.04638)
Append: [Understanding In-context Learning of Addition via Activation Subspaces](https://arxiv.org/abs/2505.05145)
Append: [Let's Reason Formally: Natural-Formal Hybrid Reasoning Enhances LLM's Math Capability](https://arxiv.org/abs/2505.23703)
Append: [Can Vision Language Models Infer Human Gaze Direction? A Controlled Study](https://arxiv.org/abs/2506.05412)
Append: [Play to Generalize: Learning to Reason Through Game Play](https://arxiv.org/abs/2506.08011)
Append: [Modality-Balancing Preference Optimization of Large Multimodal Models by Adversarial Negative Mining](https://arxiv.org/abs/2506.08022)
Append: [Think With Videos For Agentic Long-Video Understanding](https://arxiv.org/abs/2506.10821)
Append: [LLMs on a Budget? Say HOLA](https://arxiv.org/abs/2506.18952)
Append: [Disambiguation-Centric Finetuning Makes Enterprise Tool-Calling LLMs More Realistic and Less Risky](https://arxiv.org/abs/2507.03336)
Append: [Zebra-CoT: A Dataset for Interleaved Vision Language Reasoning](https://arxiv.org/abs/2507.16746)
Append: [Distilling a Small Utility-Based Passage Selector to Enhance Retrieval-Augmented Generation](https://arxiv.org/abs/2507.19102)
Append: [Multiple Memory Systems for Enhancing the Long-term Memory of Agent](https://arxiv.org/abs/2508.15294)
Append: [Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative Decoding](https://arxiv.org/abs/2509.18085)
Append: [Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning](https://arxiv.org/abs/2509.22601)
Append: [Kimi-Dev: Agentless Training as Skill Prior for SWE-Agents](https://arxiv.org/abs/2509.23045)
Append: [p-less Sampling: A Robust Hyperparameter-Free Approach for LLM Decoding](https://arxiv.org/abs/2509.23234)
append_entries: 233
Finish: 2025-10-10 04:25:02.456929
------------------------------------------------------
Started: 2025-10-10 06:25:18.071693
Existing_entries: 1233
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1254
Summarized using GPT-3.5-turbo
Append: [Learning to Reason for Hallucination Span Detection](https://arxiv.org/abs/2510.02173)
Token length: 1716
Summarized using GPT-3.5-turbo
Append: [More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for Diverse Exploration](https://arxiv.org/abs/2510.02227)
append_entries: 2
Finish: 2025-10-10 06:25:22.696307
------------------------------------------------------
Started: 2025-10-10 08:21:43.207851
Existing_entries: 1002
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-10 08:21:43.835220
------------------------------------------------------
Started: 2025-10-10 10:17:58.860606
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-10 10:17:59.387788
------------------------------------------------------
Started: 2025-10-10 12:33:55.571196
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-10 12:33:56.156296
------------------------------------------------------
Started: 2025-10-10 14:15:57.874672
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-10 14:15:58.442725
------------------------------------------------------
Started: 2025-10-10 16:20:02.315431
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-10 16:20:02.843543
------------------------------------------------------
Started: 2025-10-10 18:23:21.214734
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-10 18:23:21.745025
------------------------------------------------------
Started: 2025-10-10 20:17:21.035374
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-10 20:17:21.649309
------------------------------------------------------
Started: 2025-10-10 22:14:16.473610
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-10 22:14:16.996876
------------------------------------------------------
Started: 2025-10-11 01:10:50.272941
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-11 01:10:50.891049
------------------------------------------------------
Started: 2025-10-11 02:51:21.519981
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-11 02:51:22.105485
------------------------------------------------------
Started: 2025-10-11 04:18:26.067711
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-11 04:18:26.155840
------------------------------------------------------
Started: 2025-10-11 06:21:25.955811
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-10-11 06:21:26.022526
