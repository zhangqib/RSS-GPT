<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.LG updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.LG</link>


<item>
<title>GraphTorque: Torque-Driven Rewiring Graph Neural Network</title>
<link>https://arxiv.org/abs/2507.21422</link>
<guid>https://arxiv.org/abs/2507.21422</guid>
<content:encoded><![CDATA[
<div> Hierarchical rewiring, Torque-driven strategy, Graph Neural Networks, Message passing, Representation learning <br />
Summary: <br />
This paper introduces a torque-driven hierarchical rewiring strategy for Graph Neural Networks (GNNs) to enhance representation learning on graph-structured data. Inspired by classical mechanics, the method dynamically adjusts message passing by considering the features distance as a lever arm vector and neighbor features as a force vector weighted by homophily disparity. By defining torque in this way, the approach strategically prunes high-torque edges and adds low-torque links to optimize information diffusion. Through extensive evaluations on benchmark datasets, the proposed method outperforms existing rewiring techniques on both heterophilous and homophilous graphs. This innovative approach shows promise for improving the efficiency of GNNs in processing native interactions within graphs. <br /> <div>
arXiv:2507.21422v3 Announce Type: replace 
Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for learning from graph-structured data, leveraging message passing to diffuse information and update node representations. However, most efforts have suggested that native interactions encoded in the graph may not be friendly for this process, motivating the development of graph rewiring methods. In this work, we propose a torque-driven hierarchical rewiring strategy, inspired by the notion of torque in classical mechanics, dynamically modulating message passing to improve representation learning in heterophilous and homophilous graphs. Specifically, we define the torque by treating the feature distance as a lever arm vector and the neighbor feature as a force vector weighted by the homophily disparity between nodes. We use the metric to hierarchically reconfigure receptive field of each layer by judiciously pruning high-torque edges and adding low-torque links, suppressing the impact of irrelevant information and boosting pertinent signals during message passing. Extensive evaluations on benchmark datasets show that the proposed approach surpasses state-of-the-art rewiring methods on both heterophilous and homophilous graphs.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GEPO: Group Expectation Policy Optimization for Stable Heterogeneous Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.17850</link>
<guid>https://arxiv.org/abs/2508.17850</guid>
<content:encoded><![CDATA[
<div> Decentralized training, Reinforcement Learning, Heterogeneous RL architecture, Group Expectation Policy Optimization, geographically distributed nodes<br />
Summary:<br />
As computing approaches power constraints, decentralized training is crucial. Traditional RL methods struggle with decentralized distributed training due to tight coupling between parameter learning and rollout sampling. HeteroRL proposes a solution with GEPO, an asynchronous RL algorithm that addresses latency and heterogeneity issues. High latency increases variance in importance sampling weights and training instability, but GEPO mitigates this using group expectation weighting. The algorithm demonstrates superior stability, with minimal performance drop even with significant latency. This research shows promise for decentralized RL in geographically distributed, resource-heterogeneous computing environments. <br /> <div>
arXiv:2508.17850v5 Announce Type: replace 
Abstract: As single-center computing approaches power constraints, decentralized training becomes essential. However, traditional Reinforcement Learning (RL) methods, crucial for enhancing large model post-training, cannot adapt to decentralized distributed training due to the tight coupling between parameter learning and rollout sampling. For this, we propose HeteroRL, a heterogeneous RL architecture that decouples these processes, enabling stable training across geographically distributed nodes connected via the Internet. The core component is Group Expectation Policy Optimization (GEPO), an asynchronous RL algorithm robust to latency caused by network delays or heterogeneity in computational resources. Our study reveals that high latency significantly increases KL divergence, leading to higher variance in importance sampling weights and training instability. GEPO mitigates this issue by using group expectation weighting to exponentially reduce the variance of importance weights, with theoretical guarantees. Experiments show that GEPO achieves superior stability, with only a 3\% performance drop from online to 1800s latency, demonstrating strong potential for decentralized RL in geographically distributed, resource-heterogeneous computing environments.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Need for Learning to Defer? A Training Free Deferral Framework to Multiple Experts through Conformal Prediction</title>
<link>https://arxiv.org/abs/2509.12573</link>
<guid>https://arxiv.org/abs/2509.12573</guid>
<content:encoded><![CDATA[
<div> conformal prediction, expert deferral, hybrid human-AI decision-making, segregativity criterion, CIFAR10-H, ImageNet16-H
Summary:
- AI systems often lack reliability in making predictions on all inputs, leading to the need for hybrid human-AI decision-making.
- Learning to Defer (L2D) approaches have limitations due to their sensitivity to changes in expert composition and the need for significant retraining if experts change.
- The proposed framework for expert deferral based on conformal prediction is training-free, model-agnostic, and expert-agnostic, providing a scalable alternative to L2D.
- The method utilizes a segregativity criterion to select the most discriminative expert, which consistently outperforms standalone models and the strongest expert in experiments on CIFAR10-H and ImageNet16-H datasets.
- Results show high accuracies and reduced expert workload, making the approach suitable for real-world human-AI collaboration. 

<br /><br />Summary: <div>
arXiv:2509.12573v2 Announce Type: replace 
Abstract: AI systems often fail to deliver reliable predictions across all inputs, prompting the need for hybrid human-AI decision-making. Existing Learning to Defer (L2D) approaches address this by training deferral models, but these are sensitive to changes in expert composition and require significant retraining if experts change. We propose a training-free, model- and expert-agnostic framework for expert deferral based on conformal prediction. Our method uses the prediction set generated by a conformal predictor to identify label-specific uncertainty and selects the most discriminative expert using a segregativity criterion, measuring how well an expert distinguishes between the remaining plausible labels. Experiments on CIFAR10-H and ImageNet16-H show that our method consistently outperforms both the standalone model and the strongest expert, with accuracies attaining $99.57\pm0.10\%$ and $99.40\pm0.52\%$, while reducing expert workload by up to a factor of $11$. The method remains robust under degraded expert performance and shows a gradual performance drop in low-information settings. These results suggest a scalable, retraining-free alternative to L2D for real-world human-AI collaboration.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Truthful Representations Flip Under Deceptive Instructions?</title>
<link>https://arxiv.org/abs/2507.22149</link>
<guid>https://arxiv.org/abs/2507.22149</guid>
<content:encoded><![CDATA[
<div> Deceptive instructions, Large language models, Internal representations, Linear probes, Sparse Autoencoders <br />
Summary:
Deceptive instructions can lead large language models (LLMs) to generate deceptive responses, presenting safety challenges. By analyzing the internal representations of LLMs Llama-3.1-8B-Instruct and Gemma-2-9B-Instruct on a factual verification task, researchers found that the model's output can be predicted using linear probes based on internal representations. Deceptive instructions induce significant representational shifts compared to truthful/neutral instructions, particularly in early-to-mid layers. These shifts are detectable even on complex datasets. Specific Sparse Autoencoder (SAE) features sensitive to deception were identified, with targeted visualizations confirming distinct truthful/deceptive representational subspaces. This analysis provides insights for detecting and controlling deceptive instructions in LLMs, pinpointing layer-wise and feature-level correlates of instructed dishonesty. <br /> <div>
arXiv:2507.22149v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) tend to follow maliciously crafted instructions to generate deceptive responses, posing safety challenges. How deceptive instructions alter the internal representations of LLM compared to truthful ones remains poorly understood beyond output analysis. To bridge this gap, we investigate when and how these representations ``flip'', such as from truthful to deceptive, under deceptive versus truthful/neutral instructions. Analyzing the internal representations of Llama-3.1-8B-Instruct and Gemma-2-9B-Instruct on a factual verification task, we find the model's instructed True/False output is predictable via linear probes across all conditions based on the internal representation. Further, we use Sparse Autoencoders (SAEs) to show that the Deceptive instructions induce significant representational shifts compared to Truthful/Neutral representations (which are similar), concentrated in early-to-mid layers and detectable even on complex datasets. We also identify specific SAE features highly sensitive to deceptive instruction and use targeted visualizations to confirm distinct truthful/deceptive representational subspaces. % Our analysis pinpoints layer-wise and feature-level correlates of instructed dishonesty, offering insights for LLM detection and control. Our findings expose feature- and layer-level signatures of deception, offering new insights for detecting and mitigating instructed dishonesty in LLMs.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model-Empowered Decision Transformer for UAV-Enabled Data Collection</title>
<link>https://arxiv.org/abs/2509.13934</link>
<guid>https://arxiv.org/abs/2509.13934</guid>
<content:encoded><![CDATA[
<div> Keywords: UAV trajectory planning, energy efficiency, reinforcement learning, resource allocation, large language model<br />
Summary: 
The study introduces a novel approach, LLM-CRDT, for optimizing UAV trajectory planning and resource allocation to maximize energy efficiency in data collection. The resource allocation problem is solved optimally using linear programming. LLM-CRDT integrates critic networks with a decision transformer model, combining sequence modeling and value guidance for policy learning from suboptimal datasets. The framework incorporates a pre-trained large language model as the transformer backbone and utilizes a parameter-efficient fine-tuning strategy, LoRA, for rapid adaptation to UAV control tasks with limited data and computational resources. Simulation results demonstrate that LLM-CRDT outperforms existing online and offline reinforcement learning methods, achieving significant improvements in energy efficiency for UAV data collection tasks. <br /><br />Summary: <div>
arXiv:2509.13934v2 Announce Type: replace-cross 
Abstract: The deployment of unmanned aerial vehicles (UAVs) for reliable and energy-efficient data collection from spatially distributed devices holds great promise in supporting diverse Internet of Things (IoT) applications. Nevertheless, the limited endurance and communication range of UAVs necessitate intelligent trajectory planning. While reinforcement learning (RL) has been extensively explored for UAV trajectory optimization, its interactive nature entails high costs and risks in real-world environments. Offline RL mitigates these issues but remains susceptible to unstable training and heavily rely on expert-quality datasets. To address these challenges, we formulate a joint UAV trajectory planning and resource allocation problem to maximize energy efficiency of data collection. The resource allocation subproblem is first transformed into an equivalent linear programming formulation and solved optimally with polynomial-time complexity. Then, we propose a large language model (LLM)-empowered critic-regularized decision transformer (DT) framework, termed LLM-CRDT, to learn effective UAV control policies. In LLM-CRDT, we incorporate critic networks to regularize the DT model training, thereby integrating the sequence modeling capabilities of DT with critic-based value guidance to enable learning effective policies from suboptimal datasets. Furthermore, to mitigate the data-hungry nature of transformer models, we employ a pre-trained LLM as the transformer backbone of the DT model and adopt a parameter-efficient fine-tuning strategy, i.e., LoRA, enabling rapid adaptation to UAV control tasks with small-scale dataset and low computational overhead. Extensive simulations demonstrate that LLM-CRDT outperforms benchmark online and offline RL methods, achieving up to 36.7\% higher energy efficiency than the current state-of-the-art DT approaches.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Software Parallelization Points Using Deep Neural Networks</title>
<link>https://arxiv.org/abs/2509.16215</link>
<guid>https://arxiv.org/abs/2509.16215</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, code loops, parallelization, genetic algorithm, software optimization 

Summary: 
This study introduces a deep learning-based method for identifying loops in programming code that have the potential for parallelization. Two code generators based on genetic algorithms were developed to create independent loops and ambiguous loops with unclear dependencies. The generated code snippets were processed for robust dataset creation. Deep Neural Network (DNN) and Convolutional Neural Network (CNN) models were trained for classification, with the CNN showing slightly higher performance. Statistical analysis across multiple runs confirmed the models' effectiveness. Experiments with varying dataset sizes highlighted the importance of diverse data for model accuracy. The study's results demonstrate the potential of deep learning in automating the detection of parallelizable code structures, offering a valuable tool for optimizing software and enhancing performance. 

<br /><br />Summary: <div>
arXiv:2509.16215v1 Announce Type: new 
Abstract: This study proposes a deep learning-based approach for discovering loops in programming code according to their potential for parallelization. Two genetic algorithm-based code generators were developed to produce two distinct types of code: (i) independent loops, which are parallelizable, and (ii) ambiguous loops, whose dependencies are unclear, making them impossible to define if the loop is parallelizable or not. The generated code snippets were tokenized and preprocessed to ensure a robust dataset. Two deep learning models - a Deep Neural Network (DNN) and a Convolutional Neural Network (CNN) - were implemented to perform the classification. Based on 30 independent runs, a robust statistical analysis was employed to verify the expected performance of both models, DNN and CNN. The CNN showed a slightly higher mean performance, but the two models had a similar variability. Experiments with varying dataset sizes highlighted the importance of data diversity for model performance. These results demonstrate the feasibility of using deep learning to automate the identification of parallelizable structures in code, offering a promising tool for software optimization and performance improvement.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparison of Deterministic and Probabilistic Machine Learning Algorithms for Precise Dimensional Control and Uncertainty Quantification in Additive Manufacturing</title>
<link>https://arxiv.org/abs/2509.16233</link>
<guid>https://arxiv.org/abs/2509.16233</guid>
<content:encoded><![CDATA[
<div> probabilistic framework, dimensions estimation, additive manufacturing, machine learning, uncertainty quantification <br />
Summary: 
This study presents a probabilistic framework for accurately estimating dimensions of additively manufactured components. It examines key design features using a dataset of 405 parts from multiple production runs involving different machines and materials. Deterministic models, such as Support Vector Regression, provide precise estimates close to process repeatability. Gaussian Process Regression (GPR) helps address systematic deviations, offering strong predictive performance and interpretability. Bayesian Neural Networks (BNNs) are used to capture both aleatoric and epistemic uncertainties. The study compares two BNN approaches, with one providing a balance between accuracy and uncertainty capture, and the other offering richer uncertainty decomposition. The results emphasize the importance of quantifying epistemic uncertainty for robust decision-making and risk assessment in additive manufacturing. This approach enhances dimensional accuracy and supports reliable, data-driven design strategies. Model choice between GPR and BNNs depends on specific analytical needs. <br /> <div>
arXiv:2509.16233v1 Announce Type: new 
Abstract: We present a probabilistic framework to accurately estimate dimensions of additively manufactured components. Using a dataset of 405 parts from nine production runs involving two machines, three polymer materials, and two-part configurations, we examine five key design features. To capture both design information and manufacturing variability, we employ models integrating continuous and categorical factors. For predicting Difference from Target (DFT) values, we test deterministic and probabilistic machine learning methods. Deterministic models, trained on 80% of the dataset, provide precise point estimates, with Support Vector Regression (SVR) achieving accuracy close to process repeatability. To address systematic deviations, we adopt Gaussian Process Regression (GPR) and Bayesian Neural Networks (BNNs). GPR delivers strong predictive performance and interpretability, while BNNs capture both aleatoric and epistemic uncertainties. We investigate two BNN approaches: one balancing accuracy and uncertainty capture, and another offering richer uncertainty decomposition but with lower dimensional accuracy. Our results underscore the importance of quantifying epistemic uncertainty for robust decision-making, risk assessment, and model improvement. We discuss trade-offs between GPR and BNNs in terms of predictive power, interpretability, and computational efficiency, noting that model choice depends on analytical needs. By combining deterministic precision with probabilistic uncertainty quantification, our study provides a rigorous foundation for uncertainty-aware predictive modeling in AM. This approach not only enhances dimensional accuracy but also supports reliable, risk-informed design strategies, thereby advancing data-driven manufacturing methodologies.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SubDyve: Subgraph-Driven Dynamic Propagation for Virtual Screening Enhancement Controlling False Positive</title>
<link>https://arxiv.org/abs/2509.16273</link>
<guid>https://arxiv.org/abs/2509.16273</guid>
<content:encoded><![CDATA[
<div> Virtual screening, bioactive compounds, low-label regimes, molecular fingerprints, class-discriminative substructures  
Summary:  
- The study introduces SubDyve, a novel virtual screening framework that utilizes a subgraph-aware similarity network to identify bioactive compounds, particularly in low-label regimes.
- Existing methods often overlook class-discriminative substructures critical to bioactivity, hindering their effectiveness in such scenarios.
- SubDyve incorporates iterative seed refinement to expand the seed set gradually, promoting new candidates with potential while controlling false positives to mitigate topological bias.
- Evaluation on ten DUD-E targets and the CDK7 target with a large compound dataset demonstrates that SubDyve outperforms existing approaches based on molecular fingerprints or embeddings.
- SubDyve consistently achieves superior performance, with improvements of up to +34.0 on the BEDROC metric and +24.6 on the EF1% metric.

<br /><br />Summary: <div>
arXiv:2509.16273v1 Announce Type: new 
Abstract: Virtual screening (VS) aims to identify bioactive compounds from vast chemical libraries, but remains difficult in low-label regimes where only a few actives are known. Existing methods largely rely on general-purpose molecular fingerprints and overlook class-discriminative substructures critical to bioactivity. Moreover, they consider molecules independently, limiting effectiveness in low-label regimes. We introduce SubDyve, a network-based VS framework that constructs a subgraph-aware similarity network and propagates activity signals from a small known actives. When few active compounds are available, SubDyve performs iterative seed refinement, incrementally promoting new candidates based on local false discovery rate. This strategy expands the seed set with promising candidates while controlling false positives from topological bias and overexpansion. We evaluate SubDyve on ten DUD-E targets under zero-shot conditions and on the CDK7 target with a 10-million-compound ZINC dataset. SubDyve consistently outperforms existing fingerprint or embedding-based approaches, achieving margins of up to +34.0 on the BEDROC and +24.6 on the EF1% metric.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stabilizing Information Flow Entropy: Regularization for Safe and Interpretable Autonomous Driving Perception</title>
<link>https://arxiv.org/abs/2509.16277</link>
<guid>https://arxiv.org/abs/2509.16277</guid>
<content:encoded><![CDATA[
<div> Keywords: deep perception networks, information-theoretic constraints, hierarchical communication chains, entropy-based regularizer, autonomous driving perception systems

Summary:
Deep perception networks in autonomous driving are traditionally data-intensive but lack fundamental information-theoretic constraints. Researchers reconceptualize neural encoders as hierarchical communication chains to compress raw sensory inputs effectively. They establish two design principles for robust perception: smooth mutual information variation between layers and monotonic entropy decay with depth. These principles guide the development of Eloss, an entropy-based regularizer that enhances sensitivity to anomalies in sensor inputs. Experimental validation on 3D object detection benchmarks demonstrates Eloss's effectiveness in improving accuracy while amplifying distribution-shift signals. This approach improves interpretability, enhances anomaly detection, and establishes a theoretical foundation for safer autonomous driving perception systems.<br /><br />Summary: <div>
arXiv:2509.16277v1 Announce Type: new 
Abstract: Deep perception networks in autonomous driving traditionally rely on data-intensive training regimes and post-hoc anomaly detection, often disregarding fundamental information-theoretic constraints governing stable information processing. We reconceptualize deep neural encoders as hierarchical communication chains that incrementally compress raw sensory inputs into task-relevant latent features. Within this framework, we establish two theoretically justified design principles for robust perception: (D1) smooth variation of mutual information between consecutive layers, and (D2) monotonic decay of latent entropy with network depth. Our analysis shows that, under realistic architectural assumptions, particularly blocks comprising repeated layers of similar capacity, enforcing smooth information flow (D1) naturally encourages entropy decay (D2), thus ensuring stable compression. Guided by these insights, we propose Eloss, a novel entropy-based regularizer designed as a lightweight, plug-and-play training objective. Rather than marginal accuracy improvements, this approach represents a conceptual shift: it unifies information-theoretic stability with standard perception tasks, enabling explicit, principled detection of anomalous sensor inputs through entropy deviations. Experimental validation on large-scale 3D object detection benchmarks (KITTI and nuScenes) demonstrates that incorporating Eloss consistently achieves competitive or improved accuracy while dramatically enhancing sensitivity to anomalies, amplifying distribution-shift signals by up to two orders of magnitude. This stable information-compression perspective not only improves interpretability but also establishes a solid theoretical foundation for safer, more robust autonomous driving perception systems.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Architectural change in neural networks using fuzzy vertex pooling</title>
<link>https://arxiv.org/abs/2509.16287</link>
<guid>https://arxiv.org/abs/2509.16287</guid>
<content:encoded><![CDATA[
<div> Pooling, vertices, fuzzy, neural networks, efficiency
Summary:
The document introduces the concept of fuzzy vertex pooling (FVP) in the context of neural networks. FVP involves creating a new vertex that connects to all vertices originally adjacent to the endpoints of pooled vertices, followed by the removal of these endpoints and connected edges. The pooling model shows rapid loss minimization and competitive accuracy with fewer hidden layer neurons. However, its effectiveness decreases over time and with larger datasets, leading to performance degradation in later stages of deep learning training. As a result, pooling is recommended for early-stage training in advanced deep learning models to capitalize on its initial efficiency. <div>
arXiv:2509.16287v1 Announce Type: new 
Abstract: The process of pooling vertices involves the creation of a new vertex, which becomes adjacent to all the vertices that were originally adjacent to the endpoints of the vertices being pooled. After this, the endpoints of these vertices and all edges connected to them are removed. In this document, we introduce a formal framework for the concept of fuzzy vertex pooling (FVP) and provide an overview of its key properties with its applications to neural networks. The pooling model demonstrates remarkable efficiency in minimizing loss rapidly while maintaining competitive accuracy, even with fewer hidden layer neurons. However, this advantage diminishes over extended training periods or with larger datasets, where the model's performance tends to degrade. This study highlights the limitations of pooling in later stages of deep learning training, rendering it less effective for prolonged or large-scale applications. Consequently, pooling is recommended as a strategy for early-stage training in advanced deep learning models to leverage its initial efficiency.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust LLM Training Infrastructure at ByteDance</title>
<link>https://arxiv.org/abs/2509.16293</link>
<guid>https://arxiv.org/abs/2509.16293</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, GPU infrastructure management, fault tolerance, fault diagnosis, LLM training

Summary:<br />
ByteRobust is a GPU infrastructure management system designed to ensure stable and efficient training of large language models (LLMs). With the increasing scale of LLM training, the prevalence of failures has become a significant challenge. ByteRobust prioritizes the detection and recovery of failures, leveraging the parallelisms and characteristics of LLM training to enable high-capacity fault tolerance and prompt fault demarcation. By utilizing an effective data-driven approach, ByteRobust ensures continuous and efficient training of LLM tasks. Deployed on a production GPU platform with over 200,000 GPUs, ByteRobust achieved an impressive 97% Expected Time to Recovery (ETTR) for a three-month training job on 9,600 GPUs. This system aims to minimize training interruptions, efficiently diagnose faults, and effectively tolerate failures, ultimately enhancing the stability and performance of large-scale LLM training infrastructure.<br /><br />Summary: <div>
arXiv:2509.16293v1 Announce Type: new 
Abstract: The training scale of large language models (LLMs) has reached tens of thousands of GPUs and is still continuously expanding, enabling faster learning of larger models. Accompanying the expansion of the resource scale is the prevalence of failures (CUDA error, NaN values, job hang, etc.), which poses significant challenges to training stability. Any large-scale LLM training infrastructure should strive for minimal training interruption, efficient fault diagnosis, and effective failure tolerance to enable highly efficient continuous training. This paper presents ByteRobust, a large-scale GPU infrastructure management system tailored for robust and stable training of LLMs. It exploits the uniqueness of LLM training process and gives top priorities to detecting and recovering failures in a routine manner. Leveraging parallelisms and characteristics of LLM training, ByteRobust enables high-capacity fault tolerance, prompt fault demarcation, and localization with an effective data-driven approach, comprehensively ensuring continuous and efficient training of LLM tasks. ByteRobust is deployed on a production GPU platform with over 200,000 GPUs and achieves 97% ETTR for a three-month training job on 9,600 GPUs.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ROOT: Rethinking Offline Optimization as Distributional Translation via Probabilistic Bridge</title>
<link>https://arxiv.org/abs/2509.16300</link>
<guid>https://arxiv.org/abs/2509.16300</guid>
<content:encoded><![CDATA[
<div> Keywords: black-box optimization, surrogate function, inverse modeling, probabilistic bridge, Gaussian processes<br />
Summary: 
This paper investigates the task of black-box optimization, where the goal is to find the maximum of a function using observed input-output pairs. The traditional approach involves learning a surrogate function from offline data. Alternatively, the task can be viewed as mapping desired performance to potential input candidates. This study introduces a novel perspective that frames offline optimization as a distributional translation task. It proposes learning a probabilistic bridge that transforms low-value inputs into high-value solution candidates. The technique involves constructing synthetic functions based on Gaussian processes fitted with different parameterizations on the offline data. By leveraging this approach, the study achieves significant improvement and establishes a new state-of-the-art performance on a comprehensive benchmark of recent methods. <br /><br />Summary: <div>
arXiv:2509.16300v1 Announce Type: new 
Abstract: This paper studies the black-box optimization task which aims to find the maxima of a black-box function using a static set of its observed input-output pairs. This is often achieved via learning and optimizing a surrogate function with that offline data. Alternatively, it can also be framed as an inverse modeling task that maps a desired performance to potential input candidates that achieve it. Both approaches are constrained by the limited amount of offline data. To mitigate this limitation, we introduce a new perspective that casts offline optimization as a distributional translation task. This is formulated as learning a probabilistic bridge transforming an implicit distribution of low-value inputs (i.e., offline data) into another distribution of high-value inputs (i.e., solution candidates). Such probabilistic bridge can be learned using low- and high-value inputs sampled from synthetic functions that resemble the target function. These synthetic functions are constructed as the mean posterior of multiple Gaussian processes fitted with different parameterizations on the offline data, alleviating the data bottleneck. The proposed approach is evaluated on an extensive benchmark comprising most recent methods, demonstrating significant improvement and establishing a new state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Auto-bidding under Return-on-Spend Constraints with Uncertainty Quantification</title>
<link>https://arxiv.org/abs/2509.16324</link>
<guid>https://arxiv.org/abs/2509.16324</guid>
<content:encoded><![CDATA[
<div> Auto-bidding, advertising, machine learning, prediction intervals, performance guarantees
<br />
<br />
Summary: 
This paper introduces a novel method for auto-bidding systems in advertising that takes into account uncertainty in the true value of ad impressions. By using conformal prediction and machine learning trained on historical data, the method quantifies this uncertainty without assuming the data are i.i.d. It enhances existing auto-bidding algorithms with budget and Return-on-Spend (RoS) constraints, providing performance guarantees without knowing the true value. The adjusted value estimator derived from machine learning predictions improves overall performance while keeping RoS violations low. The approach is demonstrated to be effective on both simulated and real-world industrial datasets, showing improved performance without sacrificing computational efficiency. <div>
arXiv:2509.16324v1 Announce Type: new 
Abstract: Auto-bidding systems are widely used in advertising to automatically determine bid values under constraints such as total budget and Return-on-Spend (RoS) targets. Existing works often assume that the value of an ad impression, such as the conversion rate, is known. This paper considers the more realistic scenario where the true value is unknown. We propose a novel method that uses conformal prediction to quantify the uncertainty of these values based on machine learning methods trained on historical bidding data with contextual features, without assuming the data are i.i.d. This approach is compatible with current industry systems that use machine learning to predict values. Building on prediction intervals, we introduce an adjusted value estimator derived from machine learning predictions, and show that it provides performance guarantees without requiring knowledge of the true value. We apply this method to enhance existing auto-bidding algorithms with budget and RoS constraints, and establish theoretical guarantees for achieving high reward while keeping RoS violations low. Empirical results on both simulated and real-world industrial datasets demonstrate that our approach improves performance while maintaining computational efficiency.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Highly Imbalanced Regression with Tabular Data in SEP and Other Applications</title>
<link>https://arxiv.org/abs/2509.16339</link>
<guid>https://arxiv.org/abs/2509.16339</guid>
<content:encoded><![CDATA[
<div> investigate, imbalanced regression, tabular data, MSE loss, correlation<br />
<br />
Summary:<br />
This study focuses on imbalanced regression with highly imbalanced tabular data, particularly in forecasting rare harmful Solar Energetic Particle events. The proposed CISIR approach combines correlation, Monotonically Decreasing Involution (MDI) importance, and stratified sampling to accurately estimate target values. Experimental results on five datasets show that CISIR outperforms recent methods, achieving lower error and higher correlation. Additionally, adding the correlation component to other methods improves their performance. The MDI importance function particularly stands out for its superior performance. The code for CISIR is available on GitHub for further exploration and implementation. <div>
arXiv:2509.16339v1 Announce Type: new 
Abstract: We investigate imbalanced regression with tabular data that have an imbalance ratio larger than 1,000 ("highly imbalanced"). Accurately estimating the target values of rare instances is important in applications such as forecasting the intensity of rare harmful Solar Energetic Particle (SEP) events. For regression, the MSE loss does not consider the correlation between predicted and actual values. Typical inverse importance functions allow only convex functions. Uniform sampling might yield mini-batches that do not have rare instances. We propose CISIR that incorporates correlation, Monotonically Decreasing Involution (MDI) importance, and stratified sampling. Based on five datasets, our experimental results indicate that CISIR can achieve lower error and higher correlation than some recent methods. Also, adding our correlation component to other recent methods can improve their performance. Lastly, MDI importance can outperform other importance functions. Our code can be found in https://github.com/Machine-Earning/CISIR.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating Clinical Lab Test Result Trajectories from PPG using Physiological Foundation Model and Patient-Aware State Space Model -- a UNIPHY+ Approach</title>
<link>https://arxiv.org/abs/2509.16345</link>
<guid>https://arxiv.org/abs/2509.16345</guid>
<content:encoded><![CDATA[
<div> Keywords: clinical laboratory tests, photoplethysmogram, intensive care units, UNIPHY+Lab, patient-specific baseline variation<br />
<br />
Summary: 
The study introduces UNIPHY+Lab, a framework combining a PPG foundation model with a patient-aware Mamba model to predict key laboratory tests in ICUs. The aim is to address challenges such as capturing extended temporal trends, accounting for patient-specific baseline variations, and performing multi-task estimation for biomarkers. Results indicate significant improvements in MAE, RMSE, and $R^2$ compared to LSTM and carry-forward baselines for most estimation targets. The research demonstrates the potential of continuous, personalized lab value estimation using routine PPG monitoring, paving the way for non-invasive biochemical surveillance in critical care settings. <div>
arXiv:2509.16345v1 Announce Type: new 
Abstract: Clinical laboratory tests provide essential biochemical measurements for diagnosis and treatment, but are limited by intermittent and invasive sampling. In contrast, photoplethysmogram (PPG) is a non-invasive, continuously recorded signal in intensive care units (ICUs) that reflects cardiovascular dynamics and can serve as a proxy for latent physiological changes. We propose UNIPHY+Lab, a framework that combines a large-scale PPG foundation model for local waveform encoding with a patient-aware Mamba model for long-range temporal modeling. Our architecture addresses three challenges: (1) capturing extended temporal trends in laboratory values, (2) accounting for patient-specific baseline variation via FiLM-modulated initial states, and (3) performing multi-task estimation for interrelated biomarkers. We evaluate our method on the two ICU datasets for predicting the five key laboratory tests. The results show substantial improvements over the LSTM and carry-forward baselines in MAE, RMSE, and $R^2$ among most of the estimation targets. This work demonstrates the feasibility of continuous, personalized lab value estimation from routine PPG monitoring, offering a pathway toward non-invasive biochemical surveillance in critical care.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Deep Tabular Learning</title>
<link>https://arxiv.org/abs/2509.16354</link>
<guid>https://arxiv.org/abs/2509.16354</guid>
<content:encoded><![CDATA[
<div> Transformer, tabular data, RuleNet, deep learning, benchmark datasets

Summary: 
RuleNet is a transformer-based architecture designed for deep tabular learning. It addresses challenges such as heterogeneous feature types, lack of natural structure, and limited label-preserving augmentations commonly encountered in tabular data. RuleNet incorporates learnable rule embeddings, a piecewise linear quantile projection for numerical features, and feature masking ensembles for robustness and uncertainty estimation. It has been evaluated on eight benchmark datasets, showing performance on par with or surpassing state-of-the-art tree-based methods while maintaining computational efficiency. RuleNet offers a practical neural alternative for tabular prediction tasks, demonstrating the potential for deep learning in handling tabular data effectively. <br /><br />Summary: <div>
arXiv:2509.16354v1 Announce Type: new 
Abstract: Tabular data remain a dominant form of real-world information but pose persistent challenges for deep learning due to heterogeneous feature types, lack of natural structure, and limited label-preserving augmentations. As a result, ensemble models based on decision trees continue to dominate benchmark leaderboards. In this work, we introduce RuleNet, a transformer-based architecture specifically designed for deep tabular learning. RuleNet incorporates learnable rule embeddings in a decoder, a piecewise linear quantile projection for numerical features, and feature masking ensembles for robustness and uncertainty estimation. Evaluated on eight benchmark datasets, RuleNet matches or surpasses state-of-the-art tree-based methods in most cases, while remaining computationally efficient, offering a practical neural alternative for tabular prediction tasks.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guided Sequence-Structure Generative Modeling for Iterative Antibody Optimization</title>
<link>https://arxiv.org/abs/2509.16357</link>
<guid>https://arxiv.org/abs/2509.16357</guid>
<content:encoded><![CDATA[
<div> Diffusion Generative Model, Antibody Optimization, Protein Structure, Iterative Design, Therapeutic Antibodies<br />
<br />
Summary: 
This article introduces a strategy for optimizing therapeutic antibody candidates through iterative design, leveraging sequence and structure as well as experimental measurements. A sequence-structure diffusion generative model is trained on antibody-antigen complexes to enable the optimization process. The approach utilizes predicted antibody-antigen complexes to optimize lead candidates during iterative design. A guided sampling technique biases generation towards desirable properties by incorporating models trained on experimental data. The effectiveness of the approach is demonstrated through in silico and in vitro experiments, showing the generation of high-affinity binders at various stages of an active antibody optimization campaign. <div>
arXiv:2509.16357v1 Announce Type: new 
Abstract: Therapeutic antibody candidates often require extensive engineering to improve key functional and developability properties before clinical development. This can be achieved through iterative design, where starting molecules are optimized over several rounds of in vitro experiments. While protein structure can provide a strong inductive bias, it is rarely used in iterative design due to the lack of structural data for continually evolving lead molecules over the course of optimization. In this work, we propose a strategy for iterative antibody optimization that leverages both sequence and structure as well as accumulating lab measurements of binding and developability. Building on prior work, we first train a sequence-structure diffusion generative model that operates on antibody-antigen complexes. We then outline an approach to use this model, together with carefully predicted antibody-antigen complexes, to optimize lead candidates throughout the iterative design process. Further, we describe a guided sampling approach that biases generation toward desirable properties by integrating models trained on experimental data from iterative design. We evaluate our approach in multiple in silico and in vitro experiments, demonstrating that it produces high-affinity binders at multiple stages of an active antibody optimization campaign.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EMPEROR: Efficient Moment-Preserving Representation of Distributions</title>
<link>https://arxiv.org/abs/2509.16379</link>
<guid>https://arxiv.org/abs/2509.16379</guid>
<content:encoded><![CDATA[
<div> GMM, neural network representations, sliced moments, determinacy guarantees, distributional information<br />
<br />
Summary:<br />
The article introduces EMPEROR, a framework for efficiently representing high-dimensional probability measures in neural network representations. EMPEROR encodes feature distributions using statistical moments, leveraging sliced moments theory. It projects features onto multiple directions, fits univariate Gaussian mixture models to each projection, and aggregates slice parameters into a compact descriptor. The framework ensures determinacy guarantees and derives finite-sample error bounds that scale optimally. Empirical results show EMPEROR captures richer distributional information compared to common pooling schemes across different data modalities, while remaining computationally efficient and widely applicable. <div>
arXiv:2509.16379v1 Announce Type: new 
Abstract: We introduce EMPEROR (Efficient Moment-Preserving Representation of Distributions), a mathematically rigorous and computationally efficient framework for representing high-dimensional probability measures arising in neural network representations. Unlike heuristic global pooling operations, EMPEROR encodes a feature distribution through its statistical moments. Our approach leverages the theory of sliced moments: features are projected onto multiple directions, lightweight univariate Gaussian mixture models (GMMs) are fit to each projection, and the resulting slice parameters are aggregated into a compact descriptor. We establish determinacy guarantees via Carleman's condition and the Cram\'er-Wold theorem, ensuring that the GMM is uniquely determined by its sliced moments, and we derive finite-sample error bounds that scale optimally with the number of slices and samples. Empirically, EMPEROR captures richer distributional information than common pooling schemes across various data modalities, while remaining computationally efficient and broadly applicable.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoUn: Empowering Machine Unlearning via Contrastive Learning</title>
<link>https://arxiv.org/abs/2509.16391</link>
<guid>https://arxiv.org/abs/2509.16391</guid>
<content:encoded><![CDATA[
<div> Keyword: machine unlearning, CoUn, data representations, contrastive learning, supervised learning

Summary: 
Machine unlearning (MU) is a key process in removing specific data from a trained model while retaining knowledge of other data. Existing methods for MU have limitations in effectiveness. The CoUn framework introduces a novel approach inspired by retraining models using only selected data. CoUn utilizes contrastive learning (CL) and supervised learning to adjust data representations, focusing on retaining data while indirectly adjusting forget data based on their semantic similarity. Extensive experiments show that CoUn outperforms current MU methods, consistently improving unlearning effectiveness. Integrating the CL module into existing methods further enhances their unlearning capabilities.<br /><br />Summary: <div>
arXiv:2509.16391v1 Announce Type: new 
Abstract: Machine unlearning (MU) aims to remove the influence of specific "forget" data from a trained model while preserving its knowledge of the remaining "retain" data. Existing MU methods based on label manipulation or model weight perturbations often achieve limited unlearning effectiveness. To address this, we introduce CoUn, a novel MU framework inspired by the observation that a model retrained from scratch using only retain data classifies forget data based on their semantic similarity to the retain data. CoUn emulates this behavior by adjusting learned data representations through contrastive learning (CL) and supervised learning, applied exclusively to retain data. Specifically, CoUn (1) leverages semantic similarity between data samples to indirectly adjust forget representations using CL, and (2) maintains retain representations within their respective clusters through supervised learning. Extensive experiments across various datasets and model architectures show that CoUn consistently outperforms state-of-the-art MU baselines in unlearning effectiveness. Additionally, integrating our CL module into existing baselines empowers their unlearning effectiveness.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Learning for Financial Forecasting</title>
<link>https://arxiv.org/abs/2509.16393</link>
<guid>https://arxiv.org/abs/2509.16393</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated Learning, Long Short-Term Memory (LSTM), binary classification, financial markets, privacy-preserving collaboration

Summary:
Federated Learning (FL) was studied in the context of binary classification for volatile financial market trends. Three scenarios were compared: centralized model training on all data, single-agent model training on individual data subsets, and FL collaboration with privacy preservation. The study was extended to include additional market features, non-identically distributed data across agents, personalized FL, and differential privacy. Results showed that FL achieved accuracy and generalization similar to the centralized model while surpassing the single-agent model. Collaborative, privacy-preserving learning was shown to have value in finance, even with realistic data heterogeneity and personalization requirements. <div>
arXiv:2509.16393v1 Announce Type: new 
Abstract: This paper studies Federated Learning (FL) for binary classification of volatile financial market trends. Using a shared Long Short-Term Memory (LSTM) classifier, we compare three scenarios: (i) a centralized model trained on the union of all data, (ii) a single-agent model trained on an individual data subset, and (iii) a privacy-preserving FL collaboration in which agents exchange only model updates, never raw data. We then extend the study with additional market features, deliberately introducing not independent and identically distributed data (non-IID) across agents, personalized FL and employing differential privacy. Our numerical experiments show that FL achieves accuracy and generalization on par with the centralized baseline, while significantly outperforming the single-agent model. The results show that collaborative, privacy-preserving learning provides collective tangible value in finance, even under realistic data heterogeneity and personalization requirements.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRID: Graph-based Reasoning for Intervention and Discovery in Built Environments</title>
<link>https://arxiv.org/abs/2509.16397</link>
<guid>https://arxiv.org/abs/2509.16397</guid>
<content:encoded><![CDATA[
<div> HVAC, fault diagnosis, commercial buildings, causal discovery, building sensor data<br />
<br />
Summary: GRID (Graph-based Reasoning for Intervention and Discovery) is a three-stage causal discovery pipeline developed to improve HVAC fault diagnosis in commercial buildings. It combines constraint-based search, neural structural equation modeling, and language model priors to recover directed acyclic graphs from building sensor data. The method achieves high accuracy in various benchmarks, with F1 scores ranging from 0.65 to 1.00. It outperforms ten baseline approaches across all evaluation scenarios, including synthetic rooms, EnergyPlus simulation, and real-world data from ASHRAE. Intervention scheduling with GRID results in low operational impact and reduced risk metrics compared to baseline methods. By integrating constraint-based methods, neural architectures, and domain-specific language models, GRID addresses the observational-causal gap in building analytics. <br /><br /> <div>
arXiv:2509.16397v1 Announce Type: new 
Abstract: Manual HVAC fault diagnosis in commercial buildings takes 8-12 hours per incident and achieves only 60 percent diagnostic accuracy, reflecting analytics that stop at correlation instead of causation. To close this gap, we present GRID (Graph-based Reasoning for Intervention and Discovery), a three-stage causal discovery pipeline that combines constraint-based search, neural structural equation modeling, and language model priors to recover directed acyclic graphs from building sensor data. Across six benchmarks: synthetic rooms, EnergyPlus simulation, the ASHRAE Great Energy Predictor III dataset, and a live office testbed, GRID achieves F1 scores ranging from 0.65 to 1.00, with exact recovery (F1 = 1.00) in three controlled environments (Base, Hidden, Physical) and strong performance on real-world data (F1 = 0.89 on ASHRAE, 0.86 in noisy conditions). The method outperforms ten baseline approaches across all evaluation scenarios. Intervention scheduling achieves low operational impact in most scenarios (cost <= 0.026) while reducing risk metrics compared to baseline approaches. The framework integrates constraint-based methods, neural architectures, and domain-specific language model prompts to address the observational-causal gap in building analytics.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Local Mechanisms of Compositional Generalization in Conditional Diffusion</title>
<link>https://arxiv.org/abs/2509.16447</link>
<guid>https://arxiv.org/abs/2509.16447</guid>
<content:encoded><![CDATA[
<div> Conditional diffusion models, compositional generalization, length generalization, locality, conditional projective composition
Summary: 
Conditional diffusion models show the potential for compositional generalization, specifically in the context of length generalization in image generation tasks like CLEVR. The study explores the underlying mechanisms of this ability, focusing on the concept of locality as a structural mechanism for compositional generalization. The research establishes a connection between a specific compositional structure and scores with sparse dependencies, termed as "local conditional scores." Empirical validation confirms that models succeeding at length generalization exhibit local conditional scores. Additionally, a causal intervention enforcing local conditional scores improves length generalization in models that previously failed. The investigation also extends to feature-space compositionality in color-conditioned CLEVR, providing initial evidence of compositional structure in the SDXL dataset. The findings contribute to understanding the factors influencing compositional generalization in conditional diffusion models.<br /><br />Summary: <div>
arXiv:2509.16447v1 Announce Type: new 
Abstract: Conditional diffusion models appear capable of compositional generalization, i.e., generating convincing samples for out-of-distribution combinations of conditioners, but the mechanisms underlying this ability remain unclear. To make this concrete, we study length generalization, the ability to generate images with more objects than seen during training. In a controlled CLEVR setting (Johnson et al., 2017), we find that length generalization is achievable in some cases but not others, suggesting that models only sometimes learn the underlying compositional structure. We then investigate locality as a structural mechanism for compositional generalization. Prior works proposed score locality as a mechanism for creativity in unconditional diffusion models (Kamb & Ganguli, 2024; Niedoba et al., 2024), but did not address flexible conditioning or compositional generalization. In this paper, we prove an exact equivalence between a specific compositional structure ("conditional projective composition") (Bradley et al., 2025) and scores with sparse dependencies on both pixels and conditioners ("local conditional scores"). This theory also extends to feature-space compositionality. We validate our theory empirically: CLEVR models that succeed at length generalization exhibit local conditional scores, while those that fail do not. Furthermore, we show that a causal intervention explicitly enforcing local conditional scores restores length generalization in a previously failing model. Finally, we investigate feature-space compositionality in color-conditioned CLEVR, and find preliminary evidence of compositional structure in SDXL.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Entropic Causal Inference: Graph Identifiability</title>
<link>https://arxiv.org/abs/2509.16463</link>
<guid>https://arxiv.org/abs/2509.16463</guid>
<content:encoded><![CDATA[
<div> Keywords: entropic causal inference, causal graph, identifiability, sequential peeling algorithm, real-world datasets

Summary:<br /><br />
The article introduces entropic causal inference as a framework for learning the causal relationship between variables from observational data by identifying the simplest explanation with the smallest entropy. It extends the identifiability result in the two-variable setting and presents the first identifiability result for causal graphs with more than two nodes. The approach leverages the ancestrality property between nodes and proposes a sequential peeling algorithm for general graphs. Additionally, a heuristic algorithm is suggested for small graphs, showing strong empirical performance. The algorithms are rigorously evaluated on synthetic data from various models, demonstrating improvement over prior methods. Finally, real-world datasets are used to test the performance of the algorithms, highlighting their effectiveness in practical applications. <div>
arXiv:2509.16463v1 Announce Type: new 
Abstract: Entropic causal inference is a recent framework for learning the causal graph between two variables from observational data by finding the information-theoretically simplest structural explanation of the data, i.e., the model with smallest entropy. In our work, we first extend the causal graph identifiability result in the two-variable setting under relaxed assumptions. We then show the first identifiability result using the entropic approach for learning causal graphs with more than two nodes. Our approach utilizes the property that ancestrality between a source node and its descendants can be determined using the bivariate entropic tests. We provide a sound sequential peeling algorithm for general graphs that relies on this property. We also propose a heuristic algorithm for small graphs that shows strong empirical performance. We rigorously evaluate the performance of our algorithms on synthetic data generated from a variety of models, observing improvement over prior work. Finally we test our algorithms on real-world datasets.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Universal Debiasing for Language Models-based Tabular Data Generation</title>
<link>https://arxiv.org/abs/2509.16475</link>
<guid>https://arxiv.org/abs/2509.16475</guid>
<content:encoded><![CDATA[
<div> keywords: Large language models, tabular data generation, fairness issues, debiasing framework, mutual information<br />
Summary:<br />
- The article discusses the challenges of fairness issues in tabular data generation using Large Language Models (LLMs).
- The proposed debiasing framework aims to minimize group-level dependencies by reducing mutual information between advantaged and protected attributes.
- The framework efficiently computes mutual information by leveraging the autoregressive structure and analytic sampling distributions of LLM-based tabular data generators.
- Two complementary methods are introduced: UDF-DPO, which integrates with existing models through direct preference optimization, and UDF-MIX, a targeted debiasing technique that does not require tuning LLM parameters.
- Extensive experiments show that the framework effectively balances fairness and utility, providing a scalable and practical solution for debiasing in critical applications.<br /><br />Summary: <div>
arXiv:2509.16475v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved promising results in tabular data generation. However, inherent historical biases in tabular datasets often cause LLMs to exacerbate fairness issues, particularly when multiple advantaged and protected features are involved. In this work, we introduce a universal debiasing framework that minimizes group-level dependencies by simultaneously reducing the mutual information between advantaged and protected attributes. By leveraging the autoregressive structure and analytic sampling distributions of LLM-based tabular data generators, our approach efficiently computes mutual information, reducing the need for cumbersome numerical estimations. Building on this foundation, we propose two complementary methods: a direct preference optimization (DPO)-based strategy, namely UDF-DPO, that integrates seamlessly with existing models, and a targeted debiasing technique, namely UDF-MIX, that achieves debiasing without tuning the parameters of LLMs. Extensive experiments demonstrate that our framework effectively balances fairness and utility, offering a scalable and practical solution for debiasing in high-stakes applications.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Broken Windows Theory</title>
<link>https://arxiv.org/abs/2509.16490</link>
<guid>https://arxiv.org/abs/2509.16490</guid>
<content:encoded><![CDATA[
<div> machine learning, urban structures, violent crime, perception of safety, New York City, Chicago 

Summary:
The study investigates how physical structures in urban areas impact crime rates and feelings of safety in New York City and Chicago. Using machine learning techniques to control for demographic factors, the researchers found that abandoned buildings and high foot traffic areas like public transportation hubs are associated with increased crime and heightened perceptions of danger, supporting the "broken windows" theory. However, these effects vary in magnitude, spatial distribution, and impact on different demographics between the two cities. The study highlights the need for tailored, specific interventions in urban crime prevention rather than one-size-fits-all approaches. <div>
arXiv:2509.16490v1 Announce Type: new 
Abstract: We revisit the longstanding question of how physical structures in urban landscapes influence crime. Leveraging machine learning-based matching techniques to control for demographic composition, we estimate the effects of several types of urban structures on the incidence of violent crime in New York City and Chicago. We additionally contribute to a growing body of literature documenting the relationship between perception of crime and actual crime rates by separately analyzing how the physical urban landscape shapes subjective feelings of safety. Our results are twofold. First, in consensus with prior work, we demonstrate a "broken windows" effect in which abandoned buildings, a sign of social disorder, are associated with both greater incidence of crime and a heightened perception of danger. This is also true of types of urban structures that draw foot traffic such as public transportation infrastructure. Second, these effects are not uniform within or across cities. The criminogenic effects of the same structure types across two cities differ in magnitude, degree of spatial localization, and heterogeneity across subgroups, while within the same city, the effects of different structure types are confounded by different demographic variables. Taken together, these results emphasize that one-size-fits-all approaches to crime reduction are untenable and policy interventions must be specifically tailored to their targets.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FairTune: A Bias-Aware Fine-Tuning Framework Towards Fair Heart Rate Prediction from PPG</title>
<link>https://arxiv.org/abs/2509.16491</link>
<guid>https://arxiv.org/abs/2509.16491</guid>
<content:encoded><![CDATA[
<div> pretrained models, physiological data, heart rate prediction, fine-tuning, gender fairness <br />
Summary: 
The article explores the impact of fine-tuning foundation models pretrained on physiological data, like PPG signals, on heart rate prediction and gender fairness. While fine-tuning improves prediction accuracy, it can worsen fairness gaps, especially with significant distributional shifts. The FairTune framework is introduced to address biases in fine-tuning, with strategies like class weighting and adversarial debiasing proving effective in reducing fairness gaps without sacrificing accuracy. Analysis shows that these mitigation techniques reshape internal embeddings to reduce demographic clustering. The study emphasizes that fairness is not inherent in fine-tuning and explicit mitigation strategies are crucial for equitable deployment of physiological foundation models. <br /> <div>
arXiv:2509.16491v1 Announce Type: new 
Abstract: Foundation models pretrained on physiological data such as photoplethysmography (PPG) signals are increasingly used to improve heart rate (HR) prediction across diverse settings. Fine-tuning these models for local deployment is often seen as a practical and scalable strategy. However, its impact on demographic fairness particularly under domain shifts remains underexplored. We fine-tune PPG-GPT a transformer-based foundation model pretrained on intensive care unit (ICU) data across three heterogeneous datasets (ICU, wearable, smartphone) and systematically evaluate the effects on HR prediction accuracy and gender fairness. While fine-tuning substantially reduces mean absolute error (up to 80%), it can simultaneously widen fairness gaps, especially in larger models and under significant distributional characteristics shifts. To address this, we introduce FairTune, a bias-aware fine-tuning framework in which we benchmark three mitigation strategies: class weighting based on inverse group frequency (IF), Group Distributionally Robust Optimization (GroupDRO), and adversarial debiasing (ADV). We find that IF and GroupDRO significantly reduce fairness gaps without compromising accuracy, with effectiveness varying by deployment domain. Representation analyses further reveal that mitigation techniques reshape internal embeddings to reduce demographic clustering. Our findings highlight that fairness does not emerge as a natural byproduct of fine-tuning and that explicit mitigation is essential for equitable deployment of physiological foundation models.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Closer Look at Model Collapse: From a Generalization-to-Memorization Perspective</title>
<link>https://arxiv.org/abs/2509.16499</link>
<guid>https://arxiv.org/abs/2509.16499</guid>
<content:encoded><![CDATA[
<div> model collapse, diffusion models, AI-generated data, entropy, data selection strategy

Summary:
- The paper addresses the issue of model collapse in diffusion models due to recursive training on synthetic data.
- Model collapse is characterized by a transition from generalization to memorization, where models reproduce training data instead of generating novel content.
- The decline in entropy of synthetic training data drives this transition, indicating model degradation.
- An entropy-based data selection strategy is proposed to prevent the transition and alleviate model collapse.
- Empirical results demonstrate that the proposed approach improves visual quality and diversity in recursive generation, effectively preventing model collapse.

<br /><br />Summary: <div>
arXiv:2509.16499v1 Announce Type: new 
Abstract: The widespread use of diffusion models has led to an abundance of AI-generated data, raising concerns about model collapse -- a phenomenon in which recursive iterations of training on synthetic data lead to performance degradation. Prior work primarily characterizes this collapse via variance shrinkage or distribution shift, but these perspectives miss practical manifestations of model collapse. This paper identifies a transition from generalization to memorization during model collapse in diffusion models, where models increasingly replicate training data instead of generating novel content during iterative training on synthetic samples. This transition is directly driven by the declining entropy of the synthetic training data produced in each training cycle, which serves as a clear indicator of model degradation. Motivated by this insight, we propose an entropy-based data selection strategy to mitigate the transition from generalization to memorization and alleviate model collapse. Empirical results show that our approach significantly enhances visual quality and diversity in recursive generation, effectively preventing collapse.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRIL: Knowledge Graph Retrieval-Integrated Learning with Large Language Models</title>
<link>https://arxiv.org/abs/2509.16502</link>
<guid>https://arxiv.org/abs/2509.16502</guid>
<content:encoded><![CDATA[
<div> Retrieval-Augmented Generation, Large Language Models, Graph-based retrieval, Multi-hop reasoning, End-to-end training

Summary:
Our research introduces a novel approach to address the challenges faced by existing graph-based Retrieval-Augmented Generation (RAG) models in mitigating hallucinations in Large Language Models (LLMs). By training the graph retriever end-to-end with the LLM, we enable adaptive navigation of multi-hop relevant entities while filtering out noise. This approach incorporates structural knowledge and semantic features into the LLM's reasoning process, resulting in improved performance across three Question Answering benchmarks. Our framework eliminates the need for predefined ground-truth entities by optimizing the retriever using LLM logits as implicit feedback, making it particularly effective in open-domain settings. The joint optimization of the graph retriever and LLM leads to state-of-the-art performance, showcasing the strength of interactive training for complex reasoning tasks.<br /><br />Summary: <div>
arXiv:2509.16502v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) has significantly mitigated the hallucinations of Large Language Models (LLMs) by grounding the generation with external knowledge. Recent extensions of RAG to graph-based retrieval offer a promising direction, leveraging the structural knowledge for multi-hop reasoning. However, existing graph RAG typically decouples retrieval and reasoning processes, which prevents the retriever from adapting to the reasoning needs of the LLM. They also struggle with scalability when performing multi-hop expansion over large-scale graphs, or depend heavily on annotated ground-truth entities, which are often unavailable in open-domain settings. To address these challenges, we propose a novel graph retriever trained end-to-end with LLM, which features an attention-based growing and pruning mechanism, adaptively navigating multi-hop relevant entities while filtering out noise. Within the extracted subgraph, structural knowledge and semantic features are encoded via soft tokens and the verbalized graph, respectively, which are infused into the LLM together, thereby enhancing its reasoning capability and facilitating interactive joint training of the graph retriever and the LLM reasoner. Experimental results across three QA benchmarks show that our approach consistently achieves state-of-the-art performance, validating the strength of joint graph-LLM optimization for complex reasoning tasks. Notably, our framework eliminates the need for predefined ground-truth entities by directly optimizing the retriever using LLM logits as implicit feedback, making it especially effective in open-domain settings.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Learning with Ad-hoc Adapter Insertions: The Case of Soft-Embeddings for Training Classifier-as-Retriever</title>
<link>https://arxiv.org/abs/2509.16508</link>
<guid>https://arxiv.org/abs/2509.16508</guid>
<content:encoded><![CDATA[
<div> frozen small language model, adapter network, retrieval mechanism, federated learning, differential privacy <br />
<br />
Summary: 
The article proposes a novel encoder architecture for retrieval-augmented generation (RAG) solutions in new knowledge domains. It introduces a frozen small language model (SLM) with a trainable adapter network to update encoders efficiently. A novel retrieval mechanism using a classifier head attached to the SLM encoder is also introduced to learn a similarity mapping. The use of federated learning (FL) and differential privacy (DP) enables online fine-tuning of the encoder and retriever on resource-constrained edge devices. The methodology is theoretically analyzed for convergence guarantees and tested in numerical experiments, demonstrating the effectiveness of obtaining soft embeddings, training the classifier, and leveraging FL for speedup. <div>
arXiv:2509.16508v1 Announce Type: new 
Abstract: When existing retrieval-augmented generation (RAG) solutions are intended to be used for new knowledge domains, it is necessary to update their encoders, which are taken to be pretrained large language models (LLMs). However, fully finetuning these large models is compute- and memory-intensive, and even infeasible when deployed on resource-constrained edge devices. We propose a novel encoder architecture in this work that addresses this limitation by using a frozen small language model (SLM), which satisfies the memory constraints of edge devices, and inserting a small adapter network before the transformer blocks of the SLM. The trainable adapter takes the token embeddings of the new corpus and learns to produce enhanced soft embeddings for it, while requiring significantly less compute power to update than full fine-tuning. We further propose a novel retrieval mechanism by attaching a classifier head to the SLM encoder, which is trained to learn a similarity mapping of the input embeddings to their corresponding documents. Finally, to enable the online fine-tuning of both (i) the encoder soft embeddings and (ii) the classifier-as-retriever on edge devices, we adopt federated learning (FL) and differential privacy (DP) to achieve an efficient, privacy-preserving, and product-grade training solution. We conduct a theoretical analysis of our methodology, establishing convergence guarantees under mild assumptions on gradient variance when deployed for general smooth nonconvex loss functions. Through extensive numerical experiments, we demonstrate (i) the efficacy of obtaining soft embeddings to enhance the encoder, (ii) training a classifier to improve the retriever, and (iii) the role of FL in achieving speedup.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Guided Co-Training for Text Classification</title>
<link>https://arxiv.org/abs/2509.16516</link>
<guid>https://arxiv.org/abs/2509.16516</guid>
<content:encoded><![CDATA[
<div> Approach, Weighted co-training, Large Language Models, Semi-supervised learning, Performance <br />
Summary:<br />
This paper presents a novel weighted co-training approach guided by Large Language Models (LLMs) for semi-supervised learning. The approach involves co-training two encoder-only networks using LLM labels as target labels on unlabeled data. The networks iteratively train each other, recording confidence estimates in the LLM label for each sample. Importance weights are then derived based on each network's confidence, and exchanged between the networks for back-propagation and parameter updates. By leveraging LLM-generated guidance, the approach surpasses traditional semi-supervised learning methods, particularly in scenarios with ample unlabeled data. Experimental results demonstrate state-of-the-art performance on multiple benchmark datasets, ranking first among 14 compared methods. This highlights the potential of using LLMs as knowledge amplifiers to enhance backbone co-training models efficiently. <br /> <div>
arXiv:2509.16516v1 Announce Type: new 
Abstract: In this paper, we introduce a novel weighted co-training approach that is guided by Large Language Models (LLMs). Namely, in our co-training approach, we use LLM labels on unlabeled data as target labels and co-train two encoder-only based networks that train each other over multiple iterations: first, all samples are forwarded through each network and historical estimates of each network's confidence in the LLM label are recorded; second, a dynamic importance weight is derived for each sample according to each network's belief in the quality of the LLM label for that sample; finally, the two networks exchange importance weights with each other -- each network back-propagates all samples weighted with the importance weights coming from its peer network and updates its own parameters. By strategically utilizing LLM-generated guidance, our approach significantly outperforms conventional SSL methods, particularly in settings with abundant unlabeled data. Empirical results show that it achieves state-of-the-art performance on 4 out of 5 benchmark datasets and ranks first among 14 compared methods according to the Friedman test. Our results highlight a new direction in semi-supervised learning -- where LLMs serve as knowledge amplifiers, enabling backbone co-training models to achieve state-of-the-art performance efficiently.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>mmExpert: Integrating Large Language Models for Comprehensive mmWave Data Synthesis and Understanding</title>
<link>https://arxiv.org/abs/2509.16521</link>
<guid>https://arxiv.org/abs/2509.16521</guid>
<content:encoded><![CDATA[
<div> Keywords: millimeter-wave sensing, large language models, data generation, synthetic datasets, zero-shot generalization

Summary: 
The article introduces mmExpert, a novel framework for mmWave understanding that utilizes large language models (LLMs) to automate the generation of synthetic mmWave radar datasets for specific applications. By leveraging this data generation flywheel, mmExpert enables models to achieve zero-shot generalization in real-world scenarios. Experimental results show that the data synthesized by mmExpert significantly improves the performance of downstream models, allowing for successful deployment of large models in mmWave understanding tasks. This innovative approach addresses the high costs associated with data acquisition and annotation in mmWave sensing technology, paving the way for its broader adoption in various human-centric applications.<br /><br />Summary: <div>
arXiv:2509.16521v1 Announce Type: new 
Abstract: Millimeter-wave (mmWave) sensing technology holds significant value in human-centric applications, yet the high costs associated with data acquisition and annotation limit its widespread adoption in our daily lives. Concurrently, the rapid evolution of large language models (LLMs) has opened up opportunities for addressing complex human needs. This paper presents mmExpert, an innovative mmWave understanding framework consisting of a data generation flywheel that leverages LLMs to automate the generation of synthetic mmWave radar datasets for specific application scenarios, thereby training models capable of zero-shot generalization in real-world environments. Extensive experiments demonstrate that the data synthesized by mmExpert significantly enhances the performance of downstream models and facilitates the successful deployment of large models for mmWave understanding.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward Learning</title>
<link>https://arxiv.org/abs/2509.16548</link>
<guid>https://arxiv.org/abs/2509.16548</guid>
<content:encoded><![CDATA[
<div> Keywords: Process reward models, Monte Carlo estimation, synthetic data, noise distribution, self-denoising strategy

Summary:
Process reward models (PRMs) provide detailed evaluations for large language models (LLMs) but are challenging to develop due to the high cost of human-annotated data. This study explores the noise distribution in synthetic data from Monte Carlo estimation, finding that annotation models can underestimate or overestimate correctness. A Self-Denoising Monte Carlo Annotation (SCAN) framework is proposed to address this issue. The framework enables lightweight models to produce high-quality annotations and improves PRM performance with minimal inference cost. Through robust learning strategies, PRMs can effectively learn from weak supervision, achieving significant performance improvements in tasks like mathematical reasoning. SCAN shows potential for cost-efficient and robust PRM training, outperforming baselines trained on human-annotated datasets and demonstrating performance gains with the scaling of synthetic data. <br /><br />Summary: <div>
arXiv:2509.16548v1 Announce Type: new 
Abstract: Process reward models (PRMs) offer fine-grained, step-level evaluations that facilitate deeper reasoning processes in large language models (LLMs), proving effective in complex tasks like mathematical reasoning. However, developing PRMs is challenging due to the high cost and limited scalability of human-annotated data. Synthetic data from Monte Carlo (MC) estimation is a promising alternative but suffers from a high noise ratio, which can cause overfitting and hinder large-scale training. In this work, we conduct a preliminary study on the noise distribution in synthetic data from MC estimation, identifying that annotation models tend to both underestimate and overestimate step correctness due to limitations in their annotation capabilities. Building on these insights, we propose Self-Denoising Monte Carlo Annotation (SCAN), an efficient data synthesis and noise-tolerant learning framework. Our key findings indicate that: (1) Even lightweight models (e.g., 1.5B parameters) can produce high-quality annotations through a self-denoising strategy, enabling PRMs to achieve superior performance with only 6% the inference cost required by vanilla MC estimation. (2) With our robust learning strategy, PRMs can effectively learn from this weak supervision, achieving a 39.2 F1 score improvement (from 19.9 to 59.1) in ProcessBench. Despite using only a compact synthetic dataset, our models surpass strong baselines, including those trained on large-scale human-annotated datasets such as PRM800K. Furthermore, performance continues to improve as we scale up the synthetic data, highlighting the potential of SCAN for scalable, cost-efficient, and robust PRM training.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ViTCAE: ViT-based Class-conditioned Autoencoder</title>
<link>https://arxiv.org/abs/2509.16554</link>
<guid>https://arxiv.org/abs/2509.16554</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision Transformer, autoencoder, global Class token, generative control, attention mechanism

Summary:
ViTCAE introduces a framework that improves Vision Transformer (ViT) based autoencoders by repurposing the global Class token for generative purposes. By mapping the Class token to a global latent variable that influences local latent variables, ViTCAE establishes a strong link between global semantics and local detail synthesis. Drawing inspiration from opinion dynamics, ViTCAE employs an adaptive attention mechanism that treats attention heads as interacting tokens seeking consensus. This approach includes a convergence-aware temperature scheduler that adjusts each head's influence based on distributional stability, leading to a principled head-freezing mechanism for improved computational efficiency. By unifying the generative Class token with an adaptive attention mechanism rooted in multi-agent consensus theory, ViTCAE offers a more efficient and controllable approach to transformer-based generation.<br /><br />Summary: <div>
arXiv:2509.16554v1 Announce Type: new 
Abstract: Vision Transformer (ViT) based autoencoders often underutilize the global Class token and employ static attention mechanisms, limiting both generative control and optimization efficiency. This paper introduces ViTCAE, a framework that addresses these issues by re-purposing the Class token into a generative linchpin. In our architecture, the encoder maps the Class token to a global latent variable that dictates the prior distribution for local, patch-level latent variables, establishing a robust dependency where global semantics directly inform the synthesis of local details. Drawing inspiration from opinion dynamics, we treat each attention head as a dynamical system of interacting tokens seeking consensus. This perspective motivates a convergence-aware temperature scheduler that adaptively anneals each head's influence function based on its distributional stability. This process enables a principled head-freezing mechanism, guided by theoretically-grounded diagnostics like an attention evolution distance and a consensus/cluster functional. This technique prunes converged heads during training to significantly improve computational efficiency without sacrificing fidelity. By unifying a generative Class token with an adaptive attention mechanism rooted in multi-agent consensus theory, ViTCAE offers a more efficient and controllable approach to transformer-based generation.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learned Digital Codes for Over-the-Air Federated Learning</title>
<link>https://arxiv.org/abs/2509.16577</link>
<guid>https://arxiv.org/abs/2509.16577</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated edge learning, wireless devices, over-the-air aggregation, low signal-to-noise ratios, learnt digital OTA framework<br />
<br />
Summary: <br />
Federated edge learning (FEEL) allows distributed model training on wireless devices without centralizing raw data. However, deployment is limited by the wireless uplink. This study introduces a learnt digital over-the-air (OTA) framework that enhances reliable operation in low signal-to-noise ratios (SNRs) while maintaining low uplink overhead. By combining an unrolled decoder with a jointly learnt unsourced random access codebook, the proposed method extends reliable operation by over 7 dB and improves global model convergence at all SNR levels. This research demonstrates the potential of learning-based design for FEEL, offering a solution that balances strong convergence and robustness to noise in OTA aggregation for IoT devices. <div>
arXiv:2509.16577v1 Announce Type: new 
Abstract: Federated edge learning (FEEL) enables distributed model training across wireless devices without centralising raw data, but deployment is constrained by the wireless uplink. A promising direction is over-the-air (OTA) aggregation, which merges communication with computation. Existing digital OTA methods can achieve either strong convergence or robustness to noise, but struggle to achieve both simultaneously, limiting performance in low signal-to-noise ratios (SNRs) where many IoT devices operate. This work proposes a learnt digital OTA framework that extends reliable operation into low-SNR conditions while maintaining the same uplink overhead as state-of-the-art. The proposed method combines an unrolled decoder with a jointly learnt unsourced random access codebook. Results show an extension of reliable operation by more than 7 dB, with improved global model convergence across all SNR levels, highlighting the potential of learning-based design for FEEL.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Near-Optimal Sample Complexity Bounds for Constrained Average-Reward MDPs</title>
<link>https://arxiv.org/abs/2509.16586</link>
<guid>https://arxiv.org/abs/2509.16586</guid>
<content:encoded><![CDATA[
<div> Keywords: sample complexity, average-reward Markov decision processes, constrained MDPs, generative model, model-based algorithm

Summary:
This article addresses the sample complexity of learning in constrained average-reward Markov decision processes (CAMDPs) under a generative model. The study focuses on two settings: relaxed feasibility allowing small constraint violations and strict feasibility requiring the output policy to satisfy the constraint. The proposed model-based algorithm achieves sample complexities of $\tilde{O}\left(\frac{S A (B+H)}{ \epsilon^2}\right)$ and $\tilde{O} \left(\frac{S A (B+H)}{\epsilon^2 \zeta^2} \right)$ under the relaxed and strict feasibility settings, respectively. The results establish a matching lower bound of $\tilde{\Omega}\left(\frac{S A (B+H)}{ \epsilon^2\zeta^2}\right)$ for the strict feasibility case, providing minimax-optimal bounds for CAMDPs. The findings significantly contribute to closing the gap in understanding the complexity of constrained average-reward MDPs. 

<br /><br />Summary: <div>
arXiv:2509.16586v1 Announce Type: new 
Abstract: Recent advances have significantly improved our understanding of the sample complexity of learning in average-reward Markov decision processes (AMDPs) under the generative model. However, much less is known about the constrained average-reward MDP (CAMDP), where policies must satisfy long-run average constraints. In this work, we address this gap by studying the sample complexity of learning an $\epsilon$-optimal policy in CAMDPs under a generative model. We propose a model-based algorithm that operates under two settings: (i) relaxed feasibility, which allows small constraint violations, and (ii) strict feasibility, where the output policy satisfies the constraint. We show that our algorithm achieves sample complexities of $\tilde{O}\left(\frac{S A (B+H)}{ \epsilon^2}\right)$ and $\tilde{O} \left(\frac{S A (B+H)}{\epsilon^2 \zeta^2} \right)$ under the relaxed and strict feasibility settings, respectively. Here, $\zeta$ is the Slater constant indicating the size of the feasible region, $H$ is the span bound of the bias function, and $B$ is the transient time bound. Moreover, a matching lower bound of $\tilde{\Omega}\left(\frac{S A (B+H)}{ \epsilon^2\zeta^2}\right)$ for the strict feasibility case is established, thus providing the first minimax-optimal bounds for CAMDPs. Our results close the theoretical gap in understanding the complexity of constrained average-reward MDPs.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Supervised Learning of Graph Representations for Network Intrusion Detection</title>
<link>https://arxiv.org/abs/2509.16625</link>
<guid>https://arxiv.org/abs/2509.16625</guid>
<content:encoded><![CDATA[
<div> GraphIDS; Intrusion detection; Network traffic; Graph neural networks; Self-supervised learning

Summary:
GraphIDS is a novel self-supervised intrusion detection model that integrates representation learning and anomaly detection for network traffic. By utilizing graph neural networks and a Transformer-based encoder-decoder, the model learns local graph representations of normal communication patterns without requiring explicit positional information. During inference, flows with high reconstruction errors are identified as potential intrusions. This end-to-end framework optimizes embeddings for the downstream task, improving the recognition of malicious traffic. On various NetFlow benchmarks, GraphIDS outperforms existing baselines, achieving high PR-AUC and macro F1-score. Its unified approach offers a more effective solution for detecting network intrusions in scenarios with limited supervision and evolving attack patterns. <div>
arXiv:2509.16625v1 Announce Type: new 
Abstract: Detecting intrusions in network traffic is a challenging task, particularly under limited supervision and constantly evolving attack patterns. While recent works have leveraged graph neural networks for network intrusion detection, they often decouple representation learning from anomaly detection, limiting the utility of the embeddings for identifying attacks. We propose GraphIDS, a self-supervised intrusion detection model that unifies these two stages by learning local graph representations of normal communication patterns through a masked autoencoder. An inductive graph neural network embeds each flow with its local topological context to capture typical network behavior, while a Transformer-based encoder-decoder reconstructs these embeddings, implicitly learning global co-occurrence patterns via self-attention without requiring explicit positional information. During inference, flows with unusually high reconstruction errors are flagged as potential intrusions. This end-to-end framework ensures that embeddings are directly optimized for the downstream task, facilitating the recognition of malicious traffic. On diverse NetFlow benchmarks, GraphIDS achieves up to 99.98% PR-AUC and 99.61% macro F1-score, outperforming baselines by 5-25 percentage points.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causality-Induced Positional Encoding for Transformer-Based Representation Learning of Non-Sequential Features</title>
<link>https://arxiv.org/abs/2509.16629</link>
<guid>https://arxiv.org/abs/2509.16629</guid>
<content:encoded><![CDATA[
<div> Keywords: Positional encoding, Transformer, Causal structure, Directed acyclic graph, Hyperbolic space

Summary: 
- The proposed method CAPE identifies causal structure over non-sequential features using a weighted directed acyclic graph (DAG) with a hyperboloid model-based approach.
- CAPE generates causality-aware positional encodings for features, which are converted into their rotary form for integration with the transformer's self-attention mechanism.
- Theoretical analysis demonstrates that CAPE-generated rotary positional encodings exhibit three valuable properties for enhanced self-attention: causal distance-induced attenuation, causal generality-induced attenuation, and robustness to positional disturbances.
- Empirical evaluations on synthetic and real-world datasets confirm the theoretical properties and effectiveness of CAPE in enhancing the transformer model for data with non-sequential features.
- The code for CAPE is available at https://github.com/Catchxu/CAPE.

<br /><br />Summary: <div>
arXiv:2509.16629v1 Announce Type: new 
Abstract: Positional encoding is essential for supplementing transformer with positional information of tokens. Existing positional encoding methods demand predefined token/feature order, rendering them unsuitable for real-world data with non-sequential yet causally-related features. To address this limitation, we propose CAPE, a novel method that identifies underlying causal structure over non-sequential features as a weighted directed acyclic graph (DAG) using generalized structural equation modeling. The DAG is then embedded in hyperbolic space where its geometric structure is well-preserved using a hyperboloid model-based approach that effectively captures two important causal graph properties (causal strength & causal specificity). This step yields causality-aware positional encodings for the features, which are converted into their rotary form for integrating with transformer's self-attention mechanism. Theoretical analysis reveals that CAPE-generated rotary positional encodings possess three valuable properties for enhanced self-attention, including causal distance-induced attenuation, causal generality-induced attenuation, and robustness to positional disturbances. We evaluate CAPE over both synthetic and real-word datasets, empirically demonstrating its theoretical properties and effectiveness in enhancing transformer for data with non-sequential features. Our code is available at https://github.com/Catchxu/CAPE.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\boldsymbol{\lambda}$-Orthogonality Regularization for Compatible Representation Learning</title>
<link>https://arxiv.org/abs/2509.16664</link>
<guid>https://arxiv.org/abs/2509.16664</guid>
<content:encoded><![CDATA[
<div> regularization, neural networks, affine transformations, orthogonality, latent spaces
<br />
Summary:
This article introduces a novel approach, $\lambda$-orthogonality regularization, to adapt learned representations in neural networks. By combining affine transformations with relaxed orthogonality constraints, the method aims to align latent spaces of updated models with previous models while preserving original representations. Extensive experiments across various datasets and architectures validate the effectiveness of the approach. The method not only ensures distribution-specific adaptation but also retains zero-shot performance and compatibility across model updates. The code for implementing this approach is available on GitHub, providing a practical tool for researchers working on adapting neural network representations. <div>
arXiv:2509.16664v1 Announce Type: new 
Abstract: Retrieval systems rely on representations learned by increasingly powerful models. However, due to the high training cost and inconsistencies in learned representations, there is significant interest in facilitating communication between representations and ensuring compatibility across independently trained neural networks. In the literature, two primary approaches are commonly used to adapt different learned representations: affine transformations, which adapt well to specific distributions but can significantly alter the original representation, and orthogonal transformations, which preserve the original structure with strict geometric constraints but limit adaptability. A key challenge is adapting the latent spaces of updated models to align with those of previous models on downstream distributions while preserving the newly learned representation spaces. In this paper, we impose a relaxed orthogonality constraint, namely $\lambda$-orthogonality regularization, while learning an affine transformation, to obtain distribution-specific adaptation while retaining the original learned representations. Extensive experiments across various architectures and datasets validate our approach, demonstrating that it preserves the model's zero-shot performance and ensures compatibility across model updates. Code available at: https://github.com/miccunifi/lambda_orthogonality
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HypeMARL: Multi-Agent Reinforcement Learning For High-Dimensional, Parametric, and Distributed Systems</title>
<link>https://arxiv.org/abs/2509.16709</link>
<guid>https://arxiv.org/abs/2509.16709</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, multi-agent, PDEs, hypernetworks, optimal control

Summary:<br />
- HypeMARL is a decentralized multi-agent reinforcement learning algorithm designed for high-dimensional, distributed systems controlled by partial differential equations.
- It utilizes hypernetworks to parameterize agents' policies and value functions based on system parameters and agents' relative positions.
- HypeMARL demonstrates superior performance in controlling systems through collective behavior, outperforming existing decentralized MARL methods.
- The algorithm efficiently handles parametric dependencies and requires minimal hyperparameter tuning.
- It also offers a model-based extension, MB-HypeMARL, that utilizes deep learning surrogate models to reduce the number of environment interactions by a factor of ~10 while maintaining policy performance. <div>
arXiv:2509.16709v1 Announce Type: new 
Abstract: Deep reinforcement learning has recently emerged as a promising feedback control strategy for complex dynamical systems governed by partial differential equations (PDEs). When dealing with distributed, high-dimensional problems in state and control variables, multi-agent reinforcement learning (MARL) has been proposed as a scalable approach for breaking the curse of dimensionality. In particular, through decentralized training and execution, multiple agents cooperate to steer the system towards a target configuration, relying solely on local state and reward information. However, the principle of locality may become a limiting factor whenever a collective, nonlocal behavior of the agents is crucial to maximize the reward function, as typically happens in PDE-constrained optimal control problems. In this work, we propose HypeMARL: a decentralized MARL algorithm tailored to the control of high-dimensional, parametric, and distributed systems. HypeMARL employs hypernetworks to effectively parametrize the agents' policies and value functions with respect to the system parameters and the agents' relative positions, encoded by sinusoidal positional encoding. Through the application on challenging control problems, such as density and flow control, we show that HypeMARL (i) can effectively control systems through a collective behavior of the agents, outperforming state-of-the-art decentralized MARL, (ii) can efficiently deal with parametric dependencies, (iii) requires minimal hyperparameter tuning and (iv) can reduce the amount of expensive environment interactions by a factor of ~10 thanks to its model-based extension, MB-HypeMARL, which relies on computationally efficient deep learning-based surrogate models approximating the dynamics locally, with minimal deterioration of the policy performance.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hybrid PCA-PR-Seq2Seq-Adam-LSTM Framework for Time-Series Power Outage Prediction</title>
<link>https://arxiv.org/abs/2509.16743</link>
<guid>https://arxiv.org/abs/2509.16743</guid>
<content:encoded><![CDATA[
<div> Keywords: power outages, forecasting, LSTM networks, deep learning, PCA

Summary:
The paper introduces a novel hybrid deep learning framework, PCA-PR-Seq2Seq-Adam-LSTM, for accurately forecasting power outages. By integrating Principal Component Analysis (PCA), Poisson Regression (PR), a Sequence-to-Sequence (Seq2Seq) architecture, and an Adam-optimized LSTM network, the framework effectively addresses the challenges posed by factors such as weather conditions and load fluctuations. PCA reduces dimensionality and stabilizes data variance, while PR models discrete outage events. The Seq2Seq-Adam-LSTM component enhances temporal feature learning and captures long-term dependencies efficiently. Evaluation on real-world outage records from Michigan demonstrates the framework's significant improvement in forecasting accuracy and robustness compared to existing methods. The proposed approach showcases the potential of deep learning models, specifically LSTM networks, in addressing the complexities of power outage prediction. 

<br /><br />Summary: <div>
arXiv:2509.16743v1 Announce Type: new 
Abstract: Accurately forecasting power outages is a complex task influenced by diverse factors such as weather conditions [1], vegetation, wildlife, and load fluctuations. These factors introduce substantial variability and noise into outage data, making reliable prediction challenging. Long Short-Term Memory (LSTM) networks, a type of Recurrent Neural Network (RNN), are particularly effective for modeling nonlinear and dynamic time-series data, with proven applications in stock price forecasting [2], energy demand prediction, demand response [3], and traffic flow management [4]. This paper introduces a hybrid deep learning framework, termed PCA-PR-Seq2Seq-Adam-LSTM, that integrates Principal Component Analysis (PCA), Poisson Regression (PR), a Sequence-to-Sequence (Seq2Seq) architecture, and an Adam-optimized LSTM. PCA is employed to reduce dimensionality and stabilize data variance, while Poisson Regression effectively models discrete outage events. The Seq2Seq-Adam-LSTM component enhances temporal feature learning through efficient gradient optimization and long-term dependency capture. The framework is evaluated using real-world outage records from Michigan, and results indicate that the proposed approach significantly improves forecasting accuracy and robustness compared to existing methods.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Clinical Classification with Kolgomorov-Arnold Networks</title>
<link>https://arxiv.org/abs/2509.16750</link>
<guid>https://arxiv.org/abs/2509.16750</guid>
<content:encoded><![CDATA[
<div> Kolmogorov-Arnold Networks, AI prediction, clinical practice, interpretability, logistic regression <br />
<br />
Summary: 
The article discusses the importance of trust in AI predictions for clinicians and the challenge of transparency in machine learning methods. It introduces Kolmogorov-Arnold Networks (KANs) as function-based architectures for clinical classification tasks on tabular data, offering interpretability through transparent, symbolic representations. The Logistic-KAN model, a flexible variant of logistic regression, and the Kolmogorov-Arnold Additive Model (KAAM) provide transparent, symbolic formulas for easy understanding. These models support built-in patient-level insights, visualizations, and nearest-patient retrieval, contributing to trustworthiness. Across various health datasets, KANs perform as well as or better than standard baselines while remaining interpretable. This work signifies a significant step towards AI models that clinicians can comprehend, review, and act upon confidently. <br /> <div>
arXiv:2509.16750v1 Announce Type: new 
Abstract: Why should a clinician trust an Artificial Intelligence (AI) prediction? Despite the increasing accuracy of machine learning methods in medicine, the lack of transparency continues to hinder their adoption in clinical practice. In this work, we explore Kolmogorov-Arnold Networks (KANs) for clinical classification tasks on tabular data. Unlike traditional neural networks, KANs are function-based architectures that offer intrinsic interpretability through transparent, symbolic representations. We introduce Logistic-KAN, a flexible generalization of logistic regression, and Kolmogorov-Arnold Additive Model (KAAM), a simplified additive variant that delivers transparent, symbolic formulas. Unlike black-box models that require post-hoc explainability tools, our models support built-in patient-level insights, intuitive visualizations, and nearest-patient retrieval. Across multiple health datasets, our models match or outperform standard baselines, while remaining fully interpretable. These results position KANs as a promising step toward trustworthy AI that clinicians can understand, audit, and act upon.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discrete Diffusion Models: Novel Analysis and New Sampler Guarantees</title>
<link>https://arxiv.org/abs/2509.16756</link>
<guid>https://arxiv.org/abs/2509.16756</guid>
<content:encoded><![CDATA[
<div> discrete diffusion models, $\tau$-leaping samplers, convergence guarantees, KL divergence, differential inequalities
<br />
Summary:
This article introduces a new analytical approach for discrete diffusion models, focusing on the efficiency of discretized samplers like $\tau$-leaping. The traditional $\tau$-leaping method is analyzed, with convergence guarantees in KL divergence established to scale linearly with the vocabulary size, improving upon previous results with quadratic dependence. The approach extends to other samplers such as the Euler method and Tweedie $\tau$-leaping, providing the first convergence guarantees for these methods. A novel technique based on differential inequalities is central to this analysis, offering a more flexible alternative to traditional change-of-measure methods like Girsanov. This technique not only enhances the study of discrete diffusion models but also holds potential for analyzing other stochastic processes. <div>
arXiv:2509.16756v1 Announce Type: new 
Abstract: Discrete diffusion models have recently gained significant prominence in applications involving natural language and graph data. A key factor influencing their effectiveness is the efficiency of discretized samplers. Among these, $\tau$-leaping samplers have become particularly popular due to their empirical success. However, existing theoretical analyses of $\tau$-leaping often rely on somewhat restrictive and difficult-to-verify regularity assumptions, and their convergence bounds contain quadratic dependence on the vocabulary size. In this work, we introduce a new analytical approach for discrete diffusion models that removes the need for such assumptions. For the standard $\tau$-leaping method, we establish convergence guarantees in KL divergence that scale linearly with vocabulary size, improving upon prior results with quadratic dependence. Our approach is also more broadly applicable: it provides the first convergence guarantees for other widely used samplers, including the Euler method and Tweedie $\tau$-leaping. Central to our approach is a novel technique based on differential inequalities, offering a more flexible alternative to the traditional Girsanov change-of-measure methods. This technique may also be of independent interest for the analysis of other stochastic processes.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometric Mixture Classifier (GMC): A Discriminative Per-Class Mixture of Hyperplanes</title>
<link>https://arxiv.org/abs/2509.16769</link>
<guid>https://arxiv.org/abs/2509.16769</guid>
<content:encoded><![CDATA[
<div> mixture classifier, multimodal categories, geometric structure, interpretability, efficiency

Summary:
The Geometric Mixture Classifier (GMC) addresses the challenge of classifying multimodal categories by representing each class as a mixture of hyperplanes. This approach allows for better performance than classical linear models while maintaining interpretability and computational efficiency. By combining plane scores using a soft-OR function within each class and using softmax for probabilistic posteriors across classes, GMC achieves competitive results on synthetic datasets and tabular/image benchmarks. The use of Random Fourier Features (RFF) enables nonlinear mappings while keeping inference linear in the number of planes and features. Practical training techniques such as geometry-aware initialization and alpha annealing make GMC easy to implement. With post-hoc temperature scaling, GMC reduces expected calibration error (ECE) and provides geometric introspection through visualizations of per-plane and class responsibilities. This balance of accuracy, interpretability, and efficiency positions GMC as a valuable tool for classification tasks. 

<br /><br />Summary: <div>
arXiv:2509.16769v1 Announce Type: new 
Abstract: Many real world categories are multimodal, with single classes occupying disjoint regions in feature space. Classical linear models (logistic regression, linear SVM) use a single global hyperplane and perform poorly on such data, while high-capacity methods (kernel SVMs, deep nets) fit multimodal structure but at the expense of interpretability, heavier tuning, and higher computational cost. We propose the Geometric Mixture Classifier (GMC), a discriminative model that represents each class as a mixture of hyperplanes. Within each class, GMC combines plane scores via a temperature-controlled soft-OR (log-sum-exp), smoothly approximating the max; across classes, standard softmax yields probabilistic posteriors. GMC optionally uses Random Fourier Features (RFF) for nonlinear mappings while keeping inference linear in the number of planes and features. Our practical training recipe: geometry-aware k-means initialization, silhouette-based plane budgeting, alpha annealing, usage-aware L2 regularization, label smoothing, and early stopping, makes GMC plug-and-play. Across synthetic multimodal datasets (moons, circles, blobs, spirals) and tabular/image benchmarks (iris, wine, WDBC, digits), GMC consistently outperforms linear baselines and k-NN, is competitive with RBF-SVM, Random Forests, and small MLPs, and provides geometric introspection via per-plane and class responsibility visualizations. Inference scales linearly in planes and features, making GMC CPU-friendly, with single-digit microsecond latency per example, often faster than RBF-SVM and compact MLPs. Post-hoc temperature scaling reduces ECE from about 0.06 to 0.02. GMC thus strikes a favorable balance of accuracy, interpretability, and efficiency: it is more expressive than linear models and lighter, more transparent, and faster than kernel or deep models.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DISCO: Disentangled Communication Steering for Large Language Models</title>
<link>https://arxiv.org/abs/2509.16820</link>
<guid>https://arxiv.org/abs/2509.16820</guid>
<content:encoded><![CDATA[
<div> Keywords: steering vectors, large language models, attention heads, linear discriminability, DISentangled COmmunication<br />
Summary:<br />
The study introduces a new method called DISentangled COmmunication (DISCO) Steering for guiding large language model outputs using steering vectors injected directly into query and value representation spaces within attention heads. The approach aims to enhance linear discriminability of concepts within these spaces compared to traditional steering methods. Analysis shows that DISCO disentangles a baseline by modulating attention inputs, resulting in more granular control over queries and values. Experimental results demonstrate DISCO's superior performance on various datasets, achieving up to 19.1% higher steering efficacy than other techniques. The findings highlight the effectiveness of leveraging query and value spaces as key components in steering vector methods. <br /><br />Summary: <div>
arXiv:2509.16820v1 Announce Type: new 
Abstract: A variety of recent methods guide large language model outputs via the inference-time addition of steering vectors to residual-stream or attention-head representations. In contrast, we propose to inject steering vectors directly into the query and value representation spaces within attention heads. We provide evidence that a greater portion of these spaces exhibit high linear discriminability of concepts --a key property motivating the use of steering vectors-- than attention head outputs. We analytically characterize the effect of our method, which we term DISentangled COmmunication (DISCO) Steering, on attention head outputs. Our analysis reveals that DISCO disentangles a strong but underutilized baseline, steering attention inputs, which implicitly modifies queries and values in a rigid manner. In contrast, DISCO's direct modulation of these components enables more granular control. We find that DISCO achieves superior performance over a number of steering vector baselines across multiple datasets on LLaMA 3.1 8B and Gemma 2 9B, with steering efficacy scoring up to 19.1% higher than the runner-up. Our results support the conclusion that the query and value spaces are powerful building blocks for steering vector methods.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KANO: Kolmogorov-Arnold Neural Operator</title>
<link>https://arxiv.org/abs/2509.16825</link>
<guid>https://arxiv.org/abs/2509.16825</guid>
<content:encoded><![CDATA[
<div> Kolmogorov-Arnold Neural Operator, spectral bases, spatial bases, symbolic interpretability, quantum Hamiltonian learning<br />
<br />
Summary:
Kolmogorov-Arnold Neural Operator (KANO) is introduced as a dual-domain neural operator combining spectral and spatial bases for enhanced interpretability. Theoretical analysis shows KANO's superiority over Fourier Neural Operator (FNO) in handling position-dependent dynamics without the constraint of fast-decaying input Fourier tail. Empirical validation on position-dependent differential operators demonstrates KANO's robust generalization compared to FNO. In quantum Hamiltonian learning, KANO accurately reconstructs ground-truth Hamiltonians with symbolic representations and achieves low state infidelity from projective measurement data, outperforming FNO by orders of magnitude trained with full wave function data. This highlights KANO's effectiveness in capturing complex physical dynamics and symbolic representation of operators, making it a valuable tool for various applications. <br /><br /> <div>
arXiv:2509.16825v1 Announce Type: new 
Abstract: We introduce Kolmogorov--Arnold Neural Operator (KANO), a dual-domain neural operator jointly parameterized by both spectral and spatial bases with intrinsic symbolic interpretability. We theoretically demonstrate that KANO overcomes the pure-spectral bottleneck of Fourier Neural Operator (FNO): KANO remains expressive over generic position-dependent dynamics for any physical input, whereas FNO stays practical only for spectrally sparse operators and strictly imposes a fast-decaying input Fourier tail. We verify our claims empirically on position-dependent differential operators, for which KANO robustly generalizes but FNO fails to. In the quantum Hamiltonian learning benchmark, KANO reconstructs ground-truth Hamiltonians in closed-form symbolic representations accurate to the fourth decimal place in coefficients and attains $\approx 6\times10^{-6}$ state infidelity from projective measurement data, substantially outperforming that of the FNO trained with ideal full wave function data, $\approx 1.5\times10^{-2}$, by orders of magnitude.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SOLAR: Switchable Output Layer for Accuracy and Robustness in Once-for-All Training</title>
<link>https://arxiv.org/abs/2509.16833</link>
<guid>https://arxiv.org/abs/2509.16833</guid>
<content:encoded><![CDATA[
<div> Keywords: Once-for-All training, SOLAR, Switchable Output Layer, super-net, classification head

Summary: 
SOLAR introduces a Switchable Output Layer (SOL) to improve Once-for-All (OFA) training by assigning separate classification heads to each sub-net while sharing the backbone. This reduces interference and enhances optimization without retraining the backbone. Evaluations on five datasets and four different backbones show that SOLAR outperforms baseline methods. It improves sub-net accuracy by up to 4.71% and sub-net robustness by up to 9.01% on various datasets compared to the OATS framework. When compared to SNNs, it enhances TinyImageNet accuracy by up to 2.93% using different backbones. SOLAR provides flexibility in trade-offs among accuracy, robustness, and model-size without sacrificing overall performance, making it a valuable technique for efficient neural network training. 

<br /><br />Summary: <div>
arXiv:2509.16833v1 Announce Type: new 
Abstract: Once-for-All (OFA) training enables a single super-net to generate multiple sub-nets tailored to diverse deployment scenarios, supporting flexible trade-offs among accuracy, robustness, and model-size without retraining. However, as the number of supported sub-nets increases, excessive parameter sharing in the backbone limits representational capacity, leading to degraded calibration and reduced overall performance. To address this, we propose SOLAR (Switchable Output Layer for Accuracy and Robustness in Once-for-All Training), a simple yet effective technique that assigns each sub-net a separate classification head. By decoupling the logit learning process across sub-nets, the Switchable Output Layer (SOL) reduces representational interference and improves optimization, without altering the shared backbone. We evaluate SOLAR on five datasets (SVHN, CIFAR-10, STL-10, CIFAR-100, and TinyImageNet) using four super-net backbones (ResNet-34, WideResNet-16-8, WideResNet-40-2, and MobileNetV2) for two OFA training frameworks (OATS and SNNs). Experiments show that SOLAR outperforms the baseline methods: compared to OATS, it improves accuracy of sub-nets up to 1.26 %, 4.71 %, 1.67 %, and 1.76 %, and robustness up to 9.01 %, 7.71 %, 2.72 %, and 1.26 % on SVHN, CIFAR-10, STL-10, and CIFAR-100, respectively. Compared to SNNs, it improves TinyImageNet accuracy by up to 2.93 %, 2.34 %, and 1.35 % using ResNet-34, WideResNet-16-8, and MobileNetV2 backbones (with 8 sub-nets), respectively.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LVADNet3D: A Deep Autoencoder for Reconstructing 3D Intraventricular Flow from Sparse Hemodynamic Data</title>
<link>https://arxiv.org/abs/2509.16860</link>
<guid>https://arxiv.org/abs/2509.16860</guid>
<content:encoded><![CDATA[
<div> LVADNet3D, intraventricular blood flow, Left Ventricular Assist Devices, Computational Fluid Dynamics, velocity data <br />
Summary:<br />
Accurate assessment of intraventricular blood flow is crucial for evaluating hemodynamic conditions in LVAD-supported patients. Clinical imaging often provides sparse and low-quality velocity data, while CFD simulations offer high-fidelity but computationally intensive results. In this study, LVADNet3D, a 3D convolutional autoencoder, is proposed to reconstruct full-resolution intraventricular velocity fields from sparse inputs. The model incorporates hybrid downsampling and a deeper architecture to capture flow patterns effectively. Using a synthetic dataset generated from CFD simulations, the model outperformed a standard UNet3D model, achieving lower reconstruction error and higher PSNR results. Additionally, the study explores the impact of conditioning the model on anatomical and physiological priors, highlighting the importance of incorporating prior knowledge for accurate velocity field reconstruction. <br /> <div>
arXiv:2509.16860v1 Announce Type: new 
Abstract: Accurate assessment of intraventricular blood flow is essential for evaluating hemodynamic conditions in patients supported by Left Ventricular Assist Devices (LVADs). However, clinical imaging is either incompatible with LVADs or yields sparse, low-quality velocity data. While Computational Fluid Dynamics (CFD) simulations provide high-fidelity data, they are computationally intensive and impractical for routine clinical use. To address this, we propose LVADNet3D, a 3D convolutional autoencoder that reconstructs full-resolution intraventricular velocity fields from sparse velocity vector inputs. In contrast to a standard UNet3D model, LVADNet3D incorporates hybrid downsampling and a deeper encoder-decoder architecture with increased channel capacity to better capture spatial flow patterns. To train and evaluate the models, we generate a high-resolution synthetic dataset of intraventricular blood flow in LVAD-supported hearts using CFD simulations. We also investigate the effect of conditioning the models on anatomical and physiological priors. Across various input configurations, LVADNet3D outperforms the baseline UNet3D model, yielding lower reconstruction error and higher PSNR results.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Interpretable and Efficient Attention: Compressing All by Contracting a Few</title>
<link>https://arxiv.org/abs/2509.16875</link>
<guid>https://arxiv.org/abs/2509.16875</guid>
<content:encoded><![CDATA[
<div> Contract-and-Broadcast Self-Attention, Optimization Objective, Efficiency, Interpretability, Visual Tasks
<br />
Summary:
The article introduces a new attention mechanism called Contract-and-Broadcast Self-Attention (CBSA) that addresses the issues of efficiency and interpretability in Transformers. By unrolling the optimization over the objective, CBSA compresses all tokens into low-dimensional structures by contracting representative tokens and then broadcasting the contractions back. This mechanism scales linearly and can generalize existing attention mechanisms. Experimental results show that CBSA achieves comparable performance and offers superior advantages on various visual tasks. The proposed approach provides a unified optimization objective to improve the efficiency of attention mechanisms while maintaining interpretability. The code for CBSA is available for further exploration and application. <div>
arXiv:2509.16875v1 Announce Type: new 
Abstract: Attention mechanisms in Transformers have gained significant empirical success. Nonetheless, the optimization objectives underlying their forward pass are still unclear. Additionally, the quadratic complexity of self-attention is increasingly prohibitive. Unlike the prior work on addressing the interpretability or efficiency issue separately, we propose a unified optimization objective to alleviate both issues simultaneously. By unrolling the optimization over the objective, we derive an inherently interpretable and efficient attention mechanism, which compresses all tokens into low-dimensional structures by contracting a few representative tokens and then broadcasting the contractions back. This Contract-and-Broadcast Self-Attention (CBSA) mechanism can not only scale linearly but also generalize existing attention mechanisms as its special cases. Experiments further demonstrate comparable performance and even superior advantages of CBSA on several visual tasks. Code is available at this https URL.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Expert Specialization: Towards Catastrophic Forgetting-Free Multi-Domain MoE Adaptation</title>
<link>https://arxiv.org/abs/2509.16882</link>
<guid>https://arxiv.org/abs/2509.16882</guid>
<content:encoded><![CDATA[
<div> Keywords: Mixture-of-Experts, multi-domain adaptation, catastrophic forgetting, dynamic expert specialization, distillation<br />
Summary: <br />
- DES-MoE is a framework for adapting Mixture-of-Experts models to multiple domains without catastrophic forgetting.
- It uses an adaptive router and real-time expert-domain correlation mapping to retain pre-trained knowledge and isolate domain-specific updates.
- A three-phase fine-tuning schedule progressively freezes non-specialized parameters to prevent forgetting.
- DES-MoE outperforms conventional methods by reducing forgetting by 89% as domains scale from 2 to 6 and achieves faster convergence by 68%.
- This work establishes dynamic expert isolation as a scalable approach for multi-task Mixture-of-Experts adaptation. <div>
arXiv:2509.16882v1 Announce Type: new 
Abstract: Mixture-of-Experts (MoE) models offer immense capacity via sparsely gated expert subnetworks, yet adapting them to multiple domains without catastrophic forgetting remains an open challenge. Existing approaches either incur prohibitive computation, suffer cross-domain interference, or require separate runs per domain. We propose DES-MoE, a dynamic expert specialization framework for multi-domain adaptation of Mixture-of-Experts models. DES-MoE addresses catastrophic forgetting through three innovations: (1) an adaptive router balancing pre-trained knowledge retention and task-specific updates via distillation, (2) real-time expert-domain correlation mapping to isolate domain-specific gradients, and (3) a three-phase adaptive fine-tuning schedule that progressively freezes non-specialized parameters. Evaluated on six domains (math, code, law, etc.), DES-MoE matches single-domain ESFT performance while training one unified model, reduces forgetting by 89% compared to full fine-tuning as domains scale from 2 to 6, and achieves 68% faster convergence than conventional methods. Our work establishes dynamic expert isolation as a scalable paradigm for multi-task MoE adaptation.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRES: Fake news detection by dynamic representation and ensemble selection</title>
<link>https://arxiv.org/abs/2509.16893</link>
<guid>https://arxiv.org/abs/2509.16893</guid>
<content:encoded><![CDATA[
<div> Keywords: fake news detection, social media, dynamic representation, ensemble selection, instance hardness measures

Summary:
The paper introduces a new method called Dynamic Representation and Ensemble Selection (DRES) for detecting fake news using text-based information on social media platforms. DRES utilizes instance hardness measures to evaluate the level of difficulty in classifying each news article based on various textual feature representations. By dynamically selecting the most suitable textual representation and ensemble of classifiers for each instance, DRES significantly improves prediction accuracy compared to existing methods. Extensive experiments conducted show that DRES outperforms state-of-the-art approaches, highlighting the effectiveness of representation selection based on instance hardness and dynamic ensemble selection in enhancing performance. The code and data for this method are available on GitHub for further research and development.<br /><br />Summary: <div>
arXiv:2509.16893v1 Announce Type: new 
Abstract: The rapid spread of information via social media has made text-based fake news detection critically important due to its societal impact. This paper presents a novel detection method called Dynamic Representation and Ensemble Selection (DRES) for identifying fake news based solely on text. DRES leverages instance hardness measures to estimate the classification difficulty for each news article across multiple textual feature representations. By dynamically selecting the textual representation and the most competent ensemble of classifiers for each instance, DRES significantly enhances prediction accuracy. Extensive experiments show that DRES achieves notable improvements over state-of-the-art methods, confirming the effectiveness of representation selection based on instance hardness and dynamic ensemble selection in boosting performance. Codes and data are available at: https://github.com/FFarhangian/FakeNewsDetection_DRES
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Complexity of Finding Local Optima in Contrastive Learning</title>
<link>https://arxiv.org/abs/2509.16898</link>
<guid>https://arxiv.org/abs/2509.16898</guid>
<content:encoded><![CDATA[
<div> Algorithm, Local optima, Contrastive learning, Complexity, Optimization

Summary:
The article discusses the complexity of finding local optima in contrastive learning problems. It focuses on the discovery of meaningful data representations through contrastive information, which involves weighted triplets indicating similarities between anchor, positive, and negative examples. The study proves the Polynomial Local Search (PLS)-hardness in discrete settings and Continuous Local Search (CLS)-hardness in continuous settings for various contrastive learning problems. This suggests that no polynomial time algorithm can find a local optimum unless certain complexity classes are contained within P. Even if such containment occurs, the study shows that local search algorithms may still require exponential time to reach a local optimum in some instances, even for one-dimensional embeddings. <div>
arXiv:2509.16898v1 Announce Type: new 
Abstract: Contrastive learning is a powerful technique for discovering meaningful data representations by optimizing objectives based on $\textit{contrastive information}$, often given as a set of weighted triplets $\{(x_i, y_i^+, z_{i}^-)\}_{i = 1}^m$ indicating that an "anchor" $x_i$ is more similar to a "positive" example $y_i$ than to a "negative" example $z_i$. The goal is to find representations (e.g., embeddings in $\mathbb{R}^d$ or a tree metric) where anchors are placed closer to positive than to negative examples. While finding $\textit{global}$ optima of contrastive objectives is $\mathsf{NP}$-hard, the complexity of finding $\textit{local}$ optima -- representations that do not improve by local search algorithms such as gradient-based methods -- remains open. Our work settles the complexity of finding local optima in various contrastive learning problems by proving $\mathsf{PLS}$-hardness in discrete settings (e.g., maximize satisfied triplets) and $\mathsf{CLS}$-hardness in continuous settings (e.g., minimize Triplet Loss), where $\mathsf{PLS}$ (Polynomial Local Search) and $\mathsf{CLS}$ (Continuous Local Search) are well-studied complexity classes capturing local search dynamics in discrete and continuous optimization, respectively. Our results imply that no polynomial time algorithm (local search or otherwise) can find a local optimum for various contrastive learning problems, unless $\mathsf{PLS}\subseteq\mathsf{P}$ (or $\mathsf{CLS}\subseteq \mathsf{P}$ for continuous problems). Even in the unlikely scenario that $\mathsf{PLS}\subseteq\mathsf{P}$ (or $\mathsf{CLS}\subseteq \mathsf{P}$), our reductions imply that there exist instances where local search algorithms need exponential time to reach a local optimum, even for $d=1$ (embeddings on a line).
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedEL: Federated Elastic Learning for Heterogeneous Devices</title>
<link>https://arxiv.org/abs/2509.16902</link>
<guid>https://arxiv.org/abs/2509.16902</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated learning, training efficiency, model accuracy, tensor importance, heterogeneous devices<br />
Summary: <br />
- The paper introduces FedEL, a federated elastic learning framework designed to enhance training efficiency and maintain model accuracy in a distributed learning setting.
- FedEL utilizes a window-based training process that dynamically selects important tensors within a coordinated runtime budget, allowing for balanced training across heterogeneous devices, including stragglers.
- A tensor importance adjustment module is employed to harmonize local and global tensor importance, addressing biases resulting from data heterogeneity.
- Experimental results demonstrate that FedEL achieves a significant improvement in time-to-accuracy compared to existing methods, with up to a 3.87x speedup, while maintaining or even surpassing final test accuracy.
- Overall, FedEL offers a promising solution to the challenges of training delays and inconsistent contributions in federated learning, showcasing the potential for more efficient and accurate collaborative model training. 

Summary: <div>
arXiv:2509.16902v1 Announce Type: new 
Abstract: Federated learning (FL) enables distributed devices to collaboratively train machine learning models while maintaining data privacy. However, the heterogeneous hardware capabilities of devices often result in significant training delays, as straggler clients with limited resources prolong the aggregation process. Existing solutions such as client selection, asynchronous FL, and partial training partially address these challenges but encounter issues such as reduced accuracy, stale updates, and compromised model performance due to inconsistent training contributions. To overcome these limitations, we propose FedEL, a federated elastic learning framework that enhances training efficiency while maintaining model accuracy. FedEL introduces a novel window-based training process, sliding the window to locate the training part of the model and dynamically selecting important tensors for training within a coordinated runtime budget. This approach ensures progressive and balanced training across all clients, including stragglers. Additionally, FedEL employs a tensor importance adjustment module, harmonizing local and global tensor importance to mitigate biases caused by data heterogeneity. The experiment results show that FedEL achieves up to 3.87x improvement in time-to-accuracy compared to baselines while maintaining or exceeding final test accuracy.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Auditability and the Landscape of Distance to Multicalibration</title>
<link>https://arxiv.org/abs/2509.16930</link>
<guid>https://arxiv.org/abs/2509.16930</guid>
<content:encoded><![CDATA[
<div> distance to multicalibration, auditability, multicalibration error metric, dMC, intersectional fairness

Summary:
- The article discusses the importance of measuring the multicalibration of predictors, focusing on two key properties: capturing the modifications needed for perfect multicalibration and being auditable in an information-theoretic sense.
- Two natural generalizations of the distance to calibration framework were considered for multiple subgroups: worst group dCE (wdMC) and distance to multicalibration (dMC).
- It was found that wdMC and dMC do not fully meet the requirements, highlighting the challenges in auditing multicalibration and fairness notions.
- Two equivalent multicalibration metrics that satisfy the aforementioned properties were proposed: a continuized variant of dMC and a distance to intersection multicalibration based on intersectional fairness desiderata.
- The study also delves into the loss landscape of distance to multicalibration and the geometry of perfectly multicalibrated predictors, with implications for developing stronger multicalibration algorithms and multigroup auditing practices. 

<br /><br />Summary: <div>
arXiv:2509.16930v1 Announce Type: new 
Abstract: Calibration is a critical property for establishing the trustworthiness of predictors that provide uncertainty estimates. Multicalibration is a strengthening of calibration which requires that predictors be calibrated on a potentially overlapping collection of subsets of the domain. As multicalibration grows in popularity with practitioners, an essential question is: how do we measure how multicalibrated a predictor is? B{\l}asiok et al. (2023) considered this question for standard calibration by introducing the distance to calibration framework (dCE) to understand how calibration metrics relate to each other and the ground truth. Building on the dCE framework, we consider the auditability of the distance to multicalibration of a predictor $f$.
  We begin by considering two natural generalizations of dCE to multiple subgroups: worst group dCE (wdMC), and distance to multicalibration (dMC). We argue that there are two essential properties of any multicalibration error metric: 1) the metric should capture how much $f$ would need to be modified in order to be perfectly multicalibrated; and 2) the metric should be auditable in an information theoretic sense. We show that wdMC and dMC each fail to satisfy one of these two properties, and that similar barriers arise when considering the auditability of general distance to multigroup fairness notions. We then propose two (equivalent) multicalibration metrics which do satisfy these requirements: 1) a continuized variant of dMC; and 2) a distance to intersection multicalibration, which leans on intersectional fairness desiderata. Along the way, we shed light on the loss-landscape of distance to multicalibration and the geometry of the set of perfectly multicalibrated predictors. Our findings may have implications for the development of stronger multicalibration algorithms as well as multigroup auditing more generally.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Graph Convolution and Semantic-Guided Attention for Multimodal Risk Detection in Social Networks</title>
<link>https://arxiv.org/abs/2509.16936</link>
<guid>https://arxiv.org/abs/2509.16936</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, Natural Language Processing, Graph Neural Networks, user behavior, risk detection

Summary: 
The paper introduces a novel approach to detect potentially dangerous behaviors of social media users by integrating Natural Language Processing (NLP) and Graph Neural Networks (GNNs). Through NLP, semantic analysis, sentiment recognition, and keyword extraction are applied to user-generated text to identify subtle risk signals in social media posts. Additionally, a heterogeneous user relationship graph is constructed based on social interactions, and a relational graph convolutional network is introduced to capture user relationships, attention relationships, and content dissemination paths. By combining textual features extracted from NLP and graph structural information, the model offers a robust and effective means of identifying at-risk users. Experimental results on real social media datasets demonstrate the model's significant improvement over single-modality methods.<br /><br />Summary: <div>
arXiv:2509.16936v1 Announce Type: new 
Abstract: This paper focuses on the detection of potentially dangerous tendencies of social media users in an innovative multimodal way. We integrate Natural Language Processing (NLP) and Graph Neural Networks (GNNs) together. Firstly, we apply NLP on the user-generated text and conduct semantic analysis, sentiment recognition and keyword extraction to get subtle risk signals from social media posts. Meanwhile, we build a heterogeneous user relationship graph based on social interaction and propose a novel relational graph convolutional network to model user relationship, attention relationship and content dissemination path to discover some important structural information and user behaviors. Finally, we combine textual features extracted from these two models above with graph structural information, which provides a more robust and effective way to discover at-risk users. Our experiments on real social media datasets from different platforms show that our model can achieve significant improvement over single-modality methods.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradient Interference-Aware Graph Coloring for Multitask Learning</title>
<link>https://arxiv.org/abs/2509.16959</link>
<guid>https://arxiv.org/abs/2509.16959</guid>
<content:encoded><![CDATA[
<div> scheduler, gradient interference, multi-task learning, interference graph, graph-coloring

Summary: 
The article introduces a novel approach to multi-task learning optimization by addressing the issue of gradient interference when conflicting objectives slow convergence. A scheduler is used to compute gradient interference and construct an interference graph, which is then partitioned using greedy graph-coloring to group tasks that align well together. Only one group of tasks is activated at each training step, ensuring that tasks within the same group update in compatible directions. This dynamic grouping partition adapts to changing task relationships during training. Empirical results on various datasets demonstrate that this interference-aware graph-coloring approach consistently outperforms both baselines and state-of-the-art multi-task optimizers. <div>
arXiv:2509.16959v1 Announce Type: new 
Abstract: When different objectives conflict with each other in multi-task learning, gradients begin to interfere and slow convergence, thereby reducing the final model's performance. To address this, we introduce a scheduler that computes gradient interference, constructs an interference graph, and then applies greedy graph-coloring to partition tasks into groups that align well with each other. At each training step, only one group (color class) of tasks are activated. The grouping partition is constantly recomputed as task relationships evolve throughout training. By ensuring that each mini-batch contains only tasks that pull the model in the same direction, our method improves the effectiveness of any underlying multi-task learning optimizer without additional tuning. Since tasks within these groups will update in compatible directions, model performance will be improved rather than impeded. Empirical results on six different datasets show that this interference-aware graph-coloring approach consistently outperforms baselines and state-of-the-art multi-task optimizers.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PTQTP: Post-Training Quantization to Trit-Planes for Large Language Models</title>
<link>https://arxiv.org/abs/2509.16989</link>
<guid>https://arxiv.org/abs/2509.16989</guid>
<content:encoded><![CDATA[
arXiv:2509.16989v1 Announce Type: new 
Abstract: Post-training quantization (PTQ) of large language models (LLMs) to extremely low bit-widths remains challenging due to the fundamental trade-off between computational efficiency and model expressiveness. While existing ultra-low-bit PTQ methods rely on binary approximations or complex compensation mechanisms, they suffer from either limited representational capacity or computational overhead that undermines their efficiency gains. We introduce PTQ to Trit-Planes (PTQTP), the first ternary-weight PTQ framework that decomposes weight matrices into structured ternary {-1, 0, 1} trit-planes using 2x1.58-bit representation. PTQTP achieves multiplication-free inference, identical to 1-bit quantization, while maintaining superior expressiveness through its novel structured decomposition. Our approach provides: (1) a theoretically grounded progressive approximation algorithm ensuring global weight consistency; (2) model-agnostic deployment across diverse modern LLMs without architectural modifications; and (3) uniform ternary operations that eliminate the need for mixed-precision or compensation schemes. Comprehensive experiments across LLaMA3.x and Qwen3 model families (0.6B-70B parameters) demonstrate that PTQTP significantly outperforms existing low-bit PTQ methods, achieving 82.4% mathematical reasoning retention versus 0% for competing approaches. PTQTP approaches and sometimes surpasses 1.58-bit quantization-aware training performance while requiring only single-hour quantization compared to 10-14 GPU days for training-based methods. These results establish PTQTP as a practical solution for efficient LLM deployment in resource-constrained environments.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Persistence Spheres: Bi-continuous Representations of Persistence Diagrams</title>
<link>https://arxiv.org/abs/2509.16999</link>
<guid>https://arxiv.org/abs/2509.16999</guid>
<content:encoded><![CDATA[
arXiv:2509.16999v1 Announce Type: new 
Abstract: We introduce persistence spheres, a novel functional representation of persistence diagrams. Unlike existing embeddings (such as persistence images, landscapes, or kernel methods), persistence spheres provide a bi-continuous mapping: they are Lipschitz continuous with respect to the 1-Wasserstein distance and admit a continuous inverse on their image. This ensures, in a theoretically optimal way, both stability and geometric fidelity, making persistence spheres the representation that most closely mirrors the Wasserstein geometry of PDs in linear space. We derive explicit formulas for persistence spheres, showing that they can be computed efficiently and parallelized with minimal overhead. Empirically, we evaluate them on diverse regression and classification tasks involving functional data, time series, graphs, meshes, and point clouds. Across these benchmarks, persistence spheres consistently deliver state-of-the-art or competitive performance compared to persistence images, persistence landscapes, and the sliced Wasserstein kernel.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Overclocking: Dynamic Control of Thinking Path Length via Real-Time Reasoning Signals</title>
<link>https://arxiv.org/abs/2509.17000</link>
<guid>https://arxiv.org/abs/2509.17000</guid>
<content:encoded><![CDATA[
arXiv:2509.17000v1 Announce Type: new 
Abstract: Large Reasoning Models (LRMs) often suffer from computational inefficiency due to overthinking, where a fixed reasoning budget fails to match the varying complexity of tasks. To address this issue, we propose Adaptive Overclocking, a method that makes the overclocking hyperparameter $\alpha$ dynamic and context-aware. Our method adjusts reasoning speed in real time through two complementary signals: (1) token-level model uncertainty for fine-grained step-wise control, and (2) input complexity estimation for informed initialization. We implement this approach with three strategies: Uncertainty-Aware Alpha Scheduling (UA-$\alpha$S), Complexity-Guided Alpha Initialization (CG-$\alpha$I), and a Hybrid Adaptive Control (HAC) that combines both. Experiments on GSM8K, MATH, and SVAMP show that HAC achieves superior accuracy-latency trade-offs, reducing unnecessary computation on simple problems while allocating more resources to challenging ones. By mitigating overthinking, Adaptive Overclocking enhances both efficiency and overall reasoning performance.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long-Tailed Out-of-Distribution Detection with Refined Separate Class Learning</title>
<link>https://arxiv.org/abs/2509.17034</link>
<guid>https://arxiv.org/abs/2509.17034</guid>
<content:encoded><![CDATA[
arXiv:2509.17034v1 Announce Type: new 
Abstract: Out-of-distribution (OOD) detection is crucial for deploying robust machine learning models. However, when training data follows a long-tailed distribution, the model's ability to accurately detect OOD samples is significantly compromised, due to the confusion between OOD samples and head/tail classes. To distinguish OOD samples from both head and tail classes, the separate class learning (SCL) approach has emerged as a promising solution, which separately conduct head-specific and tail-specific class learning. To this end, we examine the limitations of existing works of SCL and reveal that the OOD detection performance is notably influenced by the use of static scaling temperature value and the presence of uninformative outliers. To mitigate these limitations, we propose a novel approach termed Refined Separate Class Learning (RSCL), which leverages dynamic class-wise temperature adjustment to modulate the temperature parameter for each in-distribution class and informative outlier mining to identify diverse types of outliers based on their affinity with head and tail classes. Extensive experiments demonstrate that RSCL achieves superior OOD detection performance while improving the classification accuracy on in-distribution data.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Performance and Calibration in Quantile Hyperparameter Optimization</title>
<link>https://arxiv.org/abs/2509.17051</link>
<guid>https://arxiv.org/abs/2509.17051</guid>
<content:encoded><![CDATA[
arXiv:2509.17051v1 Announce Type: new 
Abstract: Bayesian hyperparameter optimization relies heavily on Gaussian Process (GP) surrogates, due to robust distributional posteriors and strong performance on limited training samples. GPs however underperform in categorical hyperparameter environments or when assumptions of normality, heteroskedasticity and symmetry are excessively challenged. Conformalized quantile regression can address these estimation weaknesses, while still providing robust calibration guarantees. This study builds upon early work in this area by addressing feedback covariate shift in sequential acquisition and integrating a wider range of surrogate architectures and acquisition functions. Proposed algorithms are rigorously benchmarked against a range of state of the art hyperparameter optimization methods (GP, TPE and SMAC). Findings identify quantile surrogate architectures and acquisition functions yielding superior performance to the current quantile literature, while validating the beneficial impact of conformalization on calibration and search performance.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TSGym: Design Choices for Deep Multivariate Time-Series Forecasting</title>
<link>https://arxiv.org/abs/2509.17063</link>
<guid>https://arxiv.org/abs/2509.17063</guid>
<content:encoded><![CDATA[
arXiv:2509.17063v1 Announce Type: new 
Abstract: Recently, deep learning has driven significant advancements in multivariate time series forecasting (MTSF) tasks. However, much of the current research in MTSF tends to evaluate models from a holistic perspective, which obscures the individual contributions and leaves critical issues unaddressed. Adhering to the current modeling paradigms, this work bridges these gaps by systematically decomposing deep MTSF methods into their core, fine-grained components like series-patching tokenization, channel-independent strategy, attention modules, or even Large Language Models and Time-series Foundation Models. Through extensive experiments and component-level analysis, our work offers more profound insights than previous benchmarks that typically discuss models as a whole.
  Furthermore, we propose a novel automated solution called TSGym for MTSF tasks. Unlike traditional hyperparameter tuning, neural architecture searching or fixed model selection, TSGym performs fine-grained component selection and automated model construction, which enables the creation of more effective solutions tailored to diverse time series data, therefore enhancing model transferability across different data sources and robustness against distribution shifts. Extensive experiments indicate that TSGym significantly outperforms existing state-of-the-art MTSF and AutoML methods. All code is publicly available on https://github.com/SUFE-AILAB/TSGym.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Limits of Tabular Hardness Metrics for Deep RL: A Study with the Pharos Benchmark</title>
<link>https://arxiv.org/abs/2509.17092</link>
<guid>https://arxiv.org/abs/2509.17092</guid>
<content:encoded><![CDATA[
arXiv:2509.17092v1 Announce Type: new 
Abstract: Principled evaluation is critical for progress in deep reinforcement learning (RL), yet it lags behind the theory-driven benchmarks of tabular RL. While tabular settings benefit from well-understood hardness measures like MDP diameter and suboptimality gaps, deep RL benchmarks are often chosen based on intuition and popularity. This raises a critical question: can tabular hardness metrics be adapted to guide non-tabular benchmarking? We investigate this question and reveal a fundamental gap. Our primary contribution is demonstrating that the difficulty of non-tabular environments is dominated by a factor that tabular metrics ignore: representation hardness. The same underlying MDP can pose vastly different challenges depending on whether the agent receives state vectors or pixel-based observations. To enable this analysis, we introduce \texttt{pharos}, a new open-source library for principled RL benchmarking that allows for systematic control over both environment structure and agent representations. Our extensive case study using \texttt{pharos} shows that while tabular metrics offer some insight, they are poor predictors of deep RL agent performance on their own. This work highlights the urgent need for new, representation-aware hardness measures and positions \texttt{pharos} as a key tool for developing them.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ultra-short-term solar power forecasting by deep learning and data reconstruction</title>
<link>https://arxiv.org/abs/2509.17095</link>
<guid>https://arxiv.org/abs/2509.17095</guid>
<content:encoded><![CDATA[
arXiv:2509.17095v1 Announce Type: new 
Abstract: The integration of solar power has been increasing as the green energy transition rolls out. The penetration of solar power challenges the grid stability and energy scheduling, due to its intermittent energy generation. Accurate and near real-time solar power prediction is of critical importance to tolerant and support the permeation of distributed and volatile solar power production in the energy system. In this paper, we propose a deep-learning based ultra-short-term solar power prediction with data reconstruction. We decompose the data for the prediction to facilitate extensive exploration of the spatial and temporal dependencies within the data. Particularly, we reconstruct the data into low- and high-frequency components, using ensemble empirical model decomposition with adaptive noise (CEEMDAN). We integrate meteorological data with those two components, and employ deep-learning models to capture long- and short-term dependencies towards the target prediction period. In this way, we excessively exploit the features in historical data in predicting a ultra-short-term solar power production. Furthermore, as ultra-short-term prediction is vulnerable to local optima, we modify the optimization in our deep-learning training by penalizing long prediction intervals. Numerical experiments with diverse settings demonstrate that, compared to baseline models, the proposed method achieves improved generalization in data reconstruction and higher prediction accuracy for ultra-short-term solar power production.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRPOformer: Advancing Hyperparameter Optimization via Group Relative Policy Optimization</title>
<link>https://arxiv.org/abs/2509.17105</link>
<guid>https://arxiv.org/abs/2509.17105</guid>
<content:encoded><![CDATA[
arXiv:2509.17105v1 Announce Type: new 
Abstract: Hyperparameter optimization (HPO) plays a critical role in improving model performance. Transformer-based HPO methods have shown great potential; however, existing approaches rely heavily on large-scale historical optimization trajectories and lack effective reinforcement learning (RL) techniques, thereby limiting their efficiency and performance improvements. Inspired by the success of Group Relative Policy Optimization (GRPO) in large language models (LLMs), we propose GRPOformer -- a novel hyperparameter optimization framework that integrates reinforcement learning (RL) with Transformers. In GRPOformer, Transformers are employed to generate new hyperparameter configurations from historical optimization trajectories, while GRPO enables rapid trajectory construction and optimization strategy learning from scratch. Moreover, we introduce Policy Churn Regularization (PCR) to enhance the stability of GRPO training. Experimental results on OpenML demonstrate that GRPOformer consistently outperforms baseline methods across diverse tasks, offering new insights into the application of RL for HPO.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ScenGAN: Attention-Intensive Generative Model for Uncertainty-Aware Renewable Scenario Forecasting</title>
<link>https://arxiv.org/abs/2509.17119</link>
<guid>https://arxiv.org/abs/2509.17119</guid>
<content:encoded><![CDATA[
arXiv:2509.17119v1 Announce Type: new 
Abstract: To address the intermittency of renewable energy source (RES) generation, scenario forecasting offers a series of stochastic realizations for predictive objects with superior flexibility and direct views. Based on a long time-series perspective, this paper explores uncertainties in the realms of renewable power and deep learning. Then, an uncertainty-aware model is meticulously designed for renewable scenario forecasting, which leverages an attention mechanism and generative adversarial networks (GANs) to precisely capture complex spatial-temporal dynamics. To improve the interpretability of uncertain behavior in RES generation, Bayesian deep learning and adaptive instance normalization (AdaIN) are incorporated to simulate typical patterns and variations. Additionally, the integration of meteorological information, forecasts, and historical trajectories in the processing layer improves the synergistic forecasting capability for multiscale periodic regularities. Numerical experiments and case analyses demonstrate that the proposed approach provides an appropriate interpretation for renewable uncertainty representation, including both aleatoric and epistemic uncertainties, and shows superior performance over state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Simplification of Neural Network Architectures for Predictive Process Monitoring</title>
<link>https://arxiv.org/abs/2509.17145</link>
<guid>https://arxiv.org/abs/2509.17145</guid>
<content:encoded><![CDATA[
arXiv:2509.17145v1 Announce Type: new 
Abstract: Predictive Process Monitoring (PPM) aims to forecast the future behavior of ongoing process instances using historical event data, enabling proactive decision-making. While recent advances rely heavily on deep learning models such as LSTMs and Transformers, their high computational cost hinders practical adoption. Prior work has explored data reduction techniques and alternative feature encodings, but the effect of simplifying model architectures themselves remains underexplored. In this paper, we analyze how reducing model complexity, both in terms of parameter count and architectural depth, impacts predictive performance, using two established PPM approaches. Across five diverse event logs, we show that shrinking the Transformer model by 85% results in only a 2-3% drop in performance across various PPM tasks, while the LSTM proves slightly more sensitive, particularly for waiting time prediction. Overall, our findings suggest that substantial model simplification can preserve predictive accuracy, paving the way for more efficient and scalable PPM solutions.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flow-Induced Diagonal Gaussian Processes</title>
<link>https://arxiv.org/abs/2509.17153</link>
<guid>https://arxiv.org/abs/2509.17153</guid>
<content:encoded><![CDATA[
arXiv:2509.17153v1 Announce Type: new 
Abstract: We present Flow-Induced Diagonal Gaussian Processes (FiD-GP), a compression framework that incorporates a compact inducing weight matrix to project a neural network's weight uncertainty into a lower-dimensional subspace. Critically, FiD-GP relies on normalising-flow priors and spectral regularisations to augment its expressiveness and align the inducing subspace with feature-gradient geometry through a numerically stable projection mechanism objective. Furthermore, we demonstrate how the prediction framework in FiD-GP can help to design a single-pass projection for Out-of-Distribution (OoD) detection. Our analysis shows that FiD-GP improves uncertainty estimation ability on various tasks compared with SVGP-based baselines, satisfies tight spectral residual bounds with theoretically guaranteed OoD detection, and significantly compresses the neural network's storage requirements at the cost of increased inference computation dependent on the number of inducing weights employed. Specifically, in a comprehensive empirical study spanning regression, image classification, semantic segmentation, and out-of-distribution detection benchmarks, it cuts Bayesian training cost by several orders of magnitude, compresses parameters by roughly 51%, reduces model size by about 75%, and matches state-of-the-art accuracy and uncertainty estimation.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unrolled Graph Neural Networks for Constrained Optimization</title>
<link>https://arxiv.org/abs/2509.17156</link>
<guid>https://arxiv.org/abs/2509.17156</guid>
<content:encoded><![CDATA[
arXiv:2509.17156v1 Announce Type: new 
Abstract: In this paper, we unroll the dynamics of the dual ascent (DA) algorithm in two coupled graph neural networks (GNNs) to solve constrained optimization problems. The two networks interact with each other at the layer level to find a saddle point of the Lagrangian. The primal GNN finds a stationary point for a given dual multiplier, while the dual network iteratively refines its estimates to reach an optimal solution. We force the primal and dual networks to mirror the dynamics of the DA algorithm by imposing descent and ascent constraints. We propose a joint training scheme that alternates between updating the primal and dual networks. Our numerical experiments demonstrate that our approach yields near-optimal near-feasible solutions and generalizes well to out-of-distribution (OOD) problems.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time Series Forecasting Using a Hybrid Deep Learning Method: A Bi-LSTM Embedding Denoising Auto Encoder Transformer</title>
<link>https://arxiv.org/abs/2509.17165</link>
<guid>https://arxiv.org/abs/2509.17165</guid>
<content:encoded><![CDATA[
arXiv:2509.17165v1 Announce Type: new 
Abstract: Time series data is a prevalent form of data found in various fields. It consists of a series of measurements taken over time. Forecasting is a crucial application of time series models, where future values are predicted based on historical data. Accurate forecasting is essential for making well-informed decisions across industries. When it comes to electric vehicles (EVs), precise predictions play a key role in planning infrastructure development, load balancing, and energy management. This study introduces a BI-LSTM embedding denoising autoencoder model (BDM) designed to address time series problems, focusing on short-term EV charging load prediction. The performance of the proposed model is evaluated by comparing it with benchmark models like Transformer, CNN, RNN, LSTM, and GRU. Based on the results of the study, the proposed model outperforms the benchmark models in four of the five-time steps, demonstrating its effectiveness for time series forecasting. This research makes a significant contribution to enhancing time series forecasting, thereby improving decision-making processes.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Urban PM$_{2.5}$ Hotspots with Mobile Sensing and Gaussian Process Regression</title>
<link>https://arxiv.org/abs/2509.17175</link>
<guid>https://arxiv.org/abs/2509.17175</guid>
<content:encoded><![CDATA[
arXiv:2509.17175v1 Announce Type: new 
Abstract: Low-cost mobile sensors can be used to collect PM$_{2.5}$ concentration data throughout an entire city. However, identifying air pollution hotspots from the data is challenging due to the uneven spatial sampling, temporal variations in the background air quality, and the dynamism of urban air pollution sources. This study proposes a method to identify urban PM$_{2.5}$ hotspots that addresses these challenges, involving four steps: (1) equip citizen scientists with mobile PM$_{2.5}$ sensors while they travel; (2) normalise the raw data to remove the influence of background ambient pollution levels; (3) fit a Gaussian process regression model to the normalised data and (4) calculate a grid of spatially explicit 'hotspot scores' using the probabilistic framework of Gaussian processes, which conveniently summarise the relative pollution levels throughout the city. We apply our method to create the first ever map of PM$_{2.5}$ pollution in Kigali, Rwanda, at a 200m resolution. Our results suggest that the level of ambient PM$_{2.5}$ pollution in Kigali is dangerously high, and we identify the hotspots in Kigali where pollution consistently exceeds the city-wide average. We also evaluate our method using simulated mobile sensing data for Beijing, China, where we find that the hotspot scores are probabilistically well calibrated and accurately reflect the 'ground truth' spatial profile of PM$_{2.5}$ pollution. Thanks to the use of open-source software, our method can be re-applied in cities throughout the world with a handful of low-cost sensors. The method can help fill the gap in urban air quality information and empower public health officials.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Performance Comparison of Traditional and Ensemble Machine Learning Models for Online Fraud Detection</title>
<link>https://arxiv.org/abs/2509.17176</link>
<guid>https://arxiv.org/abs/2509.17176</guid>
<content:encoded><![CDATA[
arXiv:2509.17176v1 Announce Type: new 
Abstract: In the era of the digitally driven economy, where there has been an exponential surge in digital payment systems and other online activities, various forms of fraudulent activities have accompanied the digital growth, out of which credit card fraud has become an increasingly significant threat. To deal with this, real-time fraud detection is essential for financial security but remains challenging due to high transaction volumes and the complexity of modern fraud patterns. This study presents a comprehensive performance comparison between traditional machine learning models like Random Forest, SVM, Logistic Regression, XGBoost, and ensemble methods like Stacking and Voting Classifier for detecting credit card fraud on a heavily imbalanced public dataset, where the number of fraudulent transactions is 492 out of 284,807 total transactions. Application-specific preprocessing techniques were applied, and the models were evaluated using various performance metrics. The ensemble methods achieved an almost perfect precision of around 0.99, but traditional methods demonstrated superior performance in terms of recall, which highlights the trade-off between false positives and false negatives. The comprehensive comparison reveals distinct performance strengths and limitations for each algorithm, offering insights to guide practitioners in selecting the most effective model for robust fraud detection applications in real-world settings.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regularizing Extrapolation in Causal Inference</title>
<link>https://arxiv.org/abs/2509.17180</link>
<guid>https://arxiv.org/abs/2509.17180</guid>
<content:encoded><![CDATA[
arXiv:2509.17180v1 Announce Type: new 
Abstract: Many common estimators in machine learning and causal inference are linear smoothers, where the prediction is a weighted average of the training outcomes. Some estimators, such as ordinary least squares and kernel ridge regression, allow for arbitrarily negative weights, which improve feature imbalance but often at the cost of increased dependence on parametric modeling assumptions and higher variance. By contrast, estimators like importance weighting and random forests (sometimes implicitly) restrict weights to be non-negative, reducing dependence on parametric modeling and variance at the cost of worse imbalance. In this paper, we propose a unified framework that directly penalizes the level of extrapolation, replacing the current practice of a hard non-negativity constraint with a soft constraint and corresponding hyperparameter. We derive a worst-case extrapolation error bound and introduce a novel "bias-bias-variance" tradeoff, encompassing biases due to feature imbalance, model misspecification, and estimator variance; this tradeoff is especially pronounced in high dimensions, particularly when positivity is poor. We then develop an optimization procedure that regularizes this bound while minimizing imbalance and outline how to use this approach as a sensitivity analysis for dependence on parametric modeling assumptions. We demonstrate the effectiveness of our approach through synthetic experiments and a real-world application, involving the generalization of randomized controlled trial estimates to a target population of interest.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PMRT: A Training Recipe for Fast, 3D High-Resolution Aerodynamic Prediction</title>
<link>https://arxiv.org/abs/2509.17182</link>
<guid>https://arxiv.org/abs/2509.17182</guid>
<content:encoded><![CDATA[
arXiv:2509.17182v1 Announce Type: new 
Abstract: The aerodynamic optimization of cars requires close collaboration between aerodynamicists and stylists, while slow, expensive simulations remain a bottleneck. Surrogate models have been shown to accurately predict aerodynamics within the design space for which they were trained. However, many of these models struggle to scale to higher resolutions because of the 3D nature of the problem and data scarcity. We propose Progressive Multi-Resolution Training (PMRT), a probabilistic multi-resolution training schedule that enables training a U-Net to predict the drag coefficient ($c_d$) and high-resolution velocity fields (512 x 128 x 128) in 24 hours on a single NVIDIA H100 GPU, 7x cheaper than the high-resolution-only baseline, with similar accuracy. PMRT samples batches from three resolutions based on probabilities that change during training, starting with an emphasis on lower resolutions and gradually shifting toward higher resolutions. Since this is a training methodology, it can be adapted to other high-resolution-focused backbones. We also show that a single model can be trained across five datasets from different solvers, including a real-world dataset, by conditioning on the simulation parameters. In the DrivAerML dataset, our models achieve a $c_d$ $R^2$ of 0.975, matching literature baselines at a fraction of the training cost.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dendritic Resonate-and-Fire Neuron for Effective and Efficient Long Sequence Modeling</title>
<link>https://arxiv.org/abs/2509.17186</link>
<guid>https://arxiv.org/abs/2509.17186</guid>
<content:encoded><![CDATA[
arXiv:2509.17186v1 Announce Type: new 
Abstract: The explosive growth in sequence length has intensified the demand for effective and efficient long sequence modeling. Benefiting from intrinsic oscillatory membrane dynamics, Resonate-and-Fire (RF) neurons can efficiently extract frequency components from input signals and encode them into spatiotemporal spike trains, making them well-suited for long sequence modeling. However, RF neurons exhibit limited effective memory capacity and a trade-off between energy efficiency and training speed on complex temporal tasks. Inspired by the dendritic structure of biological neurons, we propose a Dendritic Resonate-and-Fire (D-RF) model, which explicitly incorporates a multi-dendritic and soma architecture. Each dendritic branch encodes specific frequency bands by utilizing the intrinsic oscillatory dynamics of RF neurons, thereby collectively achieving comprehensive frequency representation. Furthermore, we introduce an adaptive threshold mechanism into the soma structure that adjusts the threshold based on historical spiking activity, reducing redundant spikes while maintaining training efficiency in long sequence tasks. Extensive experiments demonstrate that our method maintains competitive accuracy while substantially ensuring sparse spikes without compromising computational efficiency during training. These results underscore its potential as an effective and efficient solution for long sequence modeling on edge platforms.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SignalLLM: A General-Purpose LLM Agent Framework for Automated Signal Processing</title>
<link>https://arxiv.org/abs/2509.17197</link>
<guid>https://arxiv.org/abs/2509.17197</guid>
<content:encoded><![CDATA[
arXiv:2509.17197v1 Announce Type: new 
Abstract: Modern signal processing (SP) pipelines, whether model-based or data-driven, often constrained by complex and fragmented workflow, rely heavily on expert knowledge and manual engineering, and struggle with adaptability and generalization under limited data. In contrast, Large Language Models (LLMs) offer strong reasoning capabilities, broad general-purpose knowledge, in-context learning, and cross-modal transfer abilities, positioning them as powerful tools for automating and generalizing SP workflows. Motivated by these potentials, we introduce SignalLLM, the first general-purpose LLM-based agent framework for general SP tasks. Unlike prior LLM-based SP approaches that are limited to narrow applications or tricky prompting, SignalLLM introduces a principled, modular architecture. It decomposes high-level SP goals into structured subtasks via in-context learning and domain-specific retrieval, followed by hierarchical planning through adaptive retrieval-augmented generation (RAG) and refinement; these subtasks are then executed through prompt-based reasoning, cross-modal reasoning, code synthesis, model invocation, or data-driven LLM-assisted modeling. Its generalizable design enables the flexible selection of problem solving strategies across different signal modalities, task types, and data conditions. We demonstrate the versatility and effectiveness of SignalLLM through five representative tasks in communication and sensing, such as radar target detection, human activity recognition, and text compression. Experimental results show superior performance over traditional and existing LLM-based methods, particularly in few-shot and zero-shot settings.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conditional Policy Generator for Dynamic Constraint Satisfaction and Optimization</title>
<link>https://arxiv.org/abs/2509.17205</link>
<guid>https://arxiv.org/abs/2509.17205</guid>
<content:encoded><![CDATA[
arXiv:2509.17205v1 Announce Type: new 
Abstract: Leveraging machine learning methods to solve constraint satisfaction problems has shown promising, but they are mostly limited to a static situation where the problem description is completely known and fixed from the beginning. In this work we present a new approach to constraint satisfaction and optimization in dynamically changing environments, particularly when variables in the problem are statistically independent. We frame it as a reinforcement learning problem and introduce a conditional policy generator by borrowing the idea of class conditional generative adversarial networks (GANs). Assuming that the problem includes both static and dynamic constraints, the former are used in a reward formulation to guide the policy training such that it learns to map to a probabilistic distribution of solutions satisfying static constraints from a noise prior, which is similar to a generator in GANs. On the other hand, dynamic constraints in the problem are encoded to different class labels and fed with the input noise. The policy is then simultaneously updated for maximum likelihood of correctly classifying given the dynamic conditions in a supervised manner. We empirically demonstrate a proof-of-principle experiment with a multi-modal constraint satisfaction problem and compare between unconditional and conditional cases.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Learning for Machine Learning Driven Molecular Dynamics</title>
<link>https://arxiv.org/abs/2509.17208</link>
<guid>https://arxiv.org/abs/2509.17208</guid>
<content:encoded><![CDATA[
arXiv:2509.17208v1 Announce Type: new 
Abstract: Machine learned coarse grained (CG) potentials are fast, but degrade over time when simulations reach undersampled biomolecular conformations, and generating widespread all atom (AA) data to combat this is computationally infeasible. We propose a novel active learning framework for CG neural network potentials in molecular dynamics (MD). Building on the CGSchNet model, our method employs root mean squared deviation (RMSD) based frame selection from MD simulations in order to generate data on the fly by querying an oracle during the training of a neural network potential. This framework preserves CG level efficiency while correcting the model at precise, RMSD identified coverage gaps. By training CGSchNet, a coarse grained neural network potential, we empirically show that our framework explores previously unseen configurations and trains the model on unexplored regions of conformational space. Our active learning framework enables a CGSchNet model trained on the Chignolin protein to achieve a 33.05% improvement in the Wasserstein 1 (W1) metric in Time lagged Independent Component Analysis (TICA) space on an in house benchmark suite.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Representation Learning from Multimodal Clinical Records under Non-Random Modality Missingness</title>
<link>https://arxiv.org/abs/2509.17228</link>
<guid>https://arxiv.org/abs/2509.17228</guid>
<content:encoded><![CDATA[
arXiv:2509.17228v1 Announce Type: new 
Abstract: Clinical notes contain rich patient information, such as diagnoses or medications, making them valuable for patient representation learning. Recent advances in large language models have further improved the ability to extract meaningful representations from clinical texts. However, clinical notes are often missing. For example, in our analysis of the MIMIC-IV dataset, 24.5% of patients have no available discharge summaries. In such cases, representations can be learned from other modalities such as structured data, chest X-rays, or radiology reports. Yet the availability of these modalities is influenced by clinical decision-making and varies across patients, resulting in modality missing-not-at-random (MMNAR) patterns. We propose a causal representation learning framework that leverages observed data and informative missingness in multimodal clinical records. It consists of: (1) an MMNAR-aware modality fusion component that integrates structured data, imaging, and text while conditioning on missingness patterns to capture patient health and clinician-driven assignment; (2) a modality reconstruction component with contrastive learning to ensure semantic sufficiency in representation learning; and (3) a multitask outcome prediction model with a rectifier that corrects for residual bias from specific modality observation patterns. Comprehensive evaluations across MIMIC-IV and eICU show consistent gains over the strongest baselines, achieving up to 13.8% AUC improvement for hospital readmission and 13.1% for ICU admission.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prospective Multi-Graph Cohesion for Multivariate Time Series Anomaly Detection</title>
<link>https://arxiv.org/abs/2509.17235</link>
<guid>https://arxiv.org/abs/2509.17235</guid>
<content:encoded><![CDATA[
arXiv:2509.17235v1 Announce Type: new 
Abstract: Anomaly detection in high-dimensional time series data is pivotal for numerous industrial applications. Recent advances in multivariate time series anomaly detection (TSAD) have increasingly leveraged graph structures to model inter-variable relationships, typically employing Graph Neural Networks (GNNs). Despite their promising results, existing methods often rely on a single graph representation, which are insufficient for capturing the complex, diverse relationships inherent in multivariate time series. To address this, we propose the Prospective Multi-Graph Cohesion (PMGC) framework for multivariate TSAD. PMGC exploits spatial correlations by integrating a long-term static graph with a series of short-term instance-wise dynamic graphs, regulated through a graph cohesion loss function. Our theoretical analysis shows that this loss function promotes diversity among dynamic graphs while aligning them with the stable long-term relationships encapsulated by the static graph. Additionally, we introduce a "prospective graphing" strategy to mitigate the limitations of traditional forecasting-based TSAD methods, which often struggle with unpredictable future variations. This strategy allows the model to accurately reflect concurrent inter-series relationships under normal conditions, thereby enhancing anomaly detection efficacy. Empirical evaluations on real-world datasets demonstrate the superior performance of our method compared to existing TSAD techniques.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TraceHiding: Scalable Machine Unlearning for Mobility Data</title>
<link>https://arxiv.org/abs/2509.17241</link>
<guid>https://arxiv.org/abs/2509.17241</guid>
<content:encoded><![CDATA[
arXiv:2509.17241v1 Announce Type: new 
Abstract: This work introduces TraceHiding, a scalable, importance-aware machine unlearning framework for mobility trajectory data. Motivated by privacy regulations such as GDPR and CCPA granting users "the right to be forgotten," TraceHiding removes specified user trajectories from trained deep models without full retraining. It combines a hierarchical data-driven importance scoring scheme with teacher-student distillation. Importance scores--computed at token, trajectory, and user levels from statistical properties (coverage diversity, entropy, length)--quantify each training sample's impact, enabling targeted forgetting of high-impact data while preserving common patterns. The student model retains knowledge on remaining data and unlearns targeted trajectories through an importance-weighted loss that amplifies forgetting signals for unique samples and attenuates them for frequent ones. We validate on Trajectory--User Linking (TUL) tasks across three real-world higher-order mobility datasets (HO-Rome, HO-Geolife, HO-NYC) and multiple architectures (GRU, LSTM, BERT, ModernBERT, GCN-TULHOR), against strong unlearning baselines including SCRUB, NegGrad, NegGrad+, Bad-T, and Finetuning. Experiments under uniform and targeted user deletion show TraceHiding, especially its entropy-based variant, achieves superior unlearning accuracy, competitive membership inference attack (MIA) resilience, and up to 40\times speedup over retraining with minimal test accuracy loss. Results highlight robustness to adversarial deletion of high-information users and consistent performance across models. To our knowledge, this is the first systematic study of machine unlearning for trajectory data, providing a reproducible pipeline with public code and preprocessing tools.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Signal Generative Diffusion Models</title>
<link>https://arxiv.org/abs/2509.17250</link>
<guid>https://arxiv.org/abs/2509.17250</guid>
<content:encoded><![CDATA[
arXiv:2509.17250v1 Announce Type: new 
Abstract: We introduce U-shaped encoder-decoder graph neural networks (U-GNNs) for stochastic graph signal generation using denoising diffusion processes. The architecture learns node features at different resolutions with skip connections between the encoder and decoder paths, analogous to the convolutional U-Net for image generation. The U-GNN is prominent for a pooling operation that leverages zero-padding and avoids arbitrary graph coarsening, with graph convolutions layered on top to capture local dependencies. This technique permits learning feature embeddings for sampled nodes at deeper levels of the architecture that remain convolutional with respect to the original graph. Applied to stock price prediction -- where deterministic forecasts struggle to capture uncertainties and tail events that are paramount -- we demonstrate the effectiveness of the diffusion model in probabilistic forecasting of stock prices.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training the next generation of physicians for artificial intelligence-assisted clinical neuroradiology: ASNR MICCAI Brain Tumor Segmentation (BraTS) 2025 Lighthouse Challenge education platform</title>
<link>https://arxiv.org/abs/2509.17281</link>
<guid>https://arxiv.org/abs/2509.17281</guid>
<content:encoded><![CDATA[
arXiv:2509.17281v1 Announce Type: new 
Abstract: High-quality reference standard image data creation by neuroradiology experts for automated clinical tools can be a powerful tool for neuroradiology & artificial intelligence education. We developed a multimodal educational approach for students and trainees during the MICCAI Brain Tumor Segmentation Lighthouse Challenge 2025, a landmark initiative to develop accurate brain tumor segmentation algorithms. Fifty-six medical students & radiology trainees volunteered to annotate brain tumor MR images for the BraTS challenges of 2023 & 2024, guided by faculty-led didactics on neuropathology MRI. Among the 56 annotators, 14 select volunteers were then paired with neuroradiology faculty for guided one-on-one annotation sessions for BraTS 2025. Lectures on neuroanatomy, pathology & AI, journal clubs & data scientist-led workshops were organized online. Annotators & audience members completed surveys on their perceived knowledge before & after annotations & lectures respectively. Fourteen coordinators, each paired with a neuroradiologist, completed the data annotation process, averaging 1322.9+/-760.7 hours per dataset per pair and 1200 segmentations in total. On a scale of 1-10, annotation coordinators reported significant increase in familiarity with image segmentation software pre- and post-annotation, moving from initial average of 6+/-2.9 to final average of 8.9+/-1.1, and significant increase in familiarity with brain tumor features pre- and post-annotation, moving from initial average of 6.2+/-2.4 to final average of 8.1+/-1.2. We demonstrate an innovative offering for providing neuroradiology & AI education through an image segmentation challenge to enhance understanding of algorithm development, reinforce the concept of data reference standard, and diversify opportunities for AI-driven image analysis among future physicians.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraphWeave: Interpretable and Robust Graph Generation via Random Walk Trajectories</title>
<link>https://arxiv.org/abs/2509.17291</link>
<guid>https://arxiv.org/abs/2509.17291</guid>
<content:encoded><![CDATA[
arXiv:2509.17291v1 Announce Type: new 
Abstract: Given a set of graphs from some unknown family, we want to generate new graphs from that family. Recent methods use diffusion on either graph embeddings or the discrete space of nodes and edges. However, simple changes to embeddings (say, adding noise) can mean uninterpretable changes in the graph. In discrete-space diffusion, each step may add or remove many nodes/edges. It is hard to predict what graph patterns we will observe after many diffusion steps. Our proposed method, called GraphWeave, takes a different approach. We separate pattern generation and graph construction. To find patterns in the training graphs, we see how they transform vectors during random walks. We then generate new graphs in two steps. First, we generate realistic random walk "trajectories" which match the learned patterns. Then, we find the optimal graph that fits these trajectories. The optimization infers all edges jointly, which improves robustness to errors. On four simulated and five real-world benchmark datasets, GraphWeave outperforms existing methods. The most significant differences are on large-scale graph structures such as PageRank, cuts, communities, degree distributions, and flows. GraphWeave is also 10x faster than its closest competitor. Finally, GraphWeave is simple, needing only a transformer and standard optimizers.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed Operator Learning for Hemodynamic Modeling</title>
<link>https://arxiv.org/abs/2509.17293</link>
<guid>https://arxiv.org/abs/2509.17293</guid>
<content:encoded><![CDATA[
arXiv:2509.17293v1 Announce Type: new 
Abstract: Accurate modeling of personalized cardiovascular dynamics is crucial for non-invasive monitoring and therapy planning. State-of-the-art physics-informed neural network (PINN) approaches employ deep, multi-branch architectures with adversarial or contrastive objectives to enforce partial differential equation constraints. While effective, these enhancements introduce significant training and implementation complexity, limiting scalability and practical deployment. We investigate physics-informed neural operator learning models as efficient supervisory signals for training simplified architectures through knowledge distillation. Our approach pre-trains a physics-informed DeepONet (PI-DeepONet) on high-fidelity cuffless blood pressure recordings to learn operator mappings from raw wearable waveforms to beat-to-beat pressure signals under embedded physics constraints. This pre-trained operator serves as a frozen supervisor in a lightweight knowledge-distillation pipeline, guiding streamlined base models that eliminate complex adversarial and contrastive learning components while maintaining performance. We characterize the role of physics-informed regularization in operator learning and demonstrate its effectiveness for supervisory guidance. Through extensive experiments, our operator-supervised approach achieves performance parity with complex baselines (correlation: 0.766 vs. 0.770, RMSE: 4.452 vs. 4.501), while dramatically reducing architectural complexity from eight critical hyperparameters to a single regularization coefficient and decreasing training overhead by 4%. Our results demonstrate that operator-based supervision effectively replaces intricate multi-component training strategies, offering a more scalable and interpretable approach to physiological modeling with reduced implementation burden.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPRINT: Stochastic Performative Prediction With Variance Reduction</title>
<link>https://arxiv.org/abs/2509.17304</link>
<guid>https://arxiv.org/abs/2509.17304</guid>
<content:encoded><![CDATA[
arXiv:2509.17304v1 Announce Type: new 
Abstract: Performative prediction (PP) is an algorithmic framework for optimizing machine learning (ML) models where the model's deployment affects the distribution of the data it is trained on. Compared to traditional ML with fixed data, designing algorithms in PP converging to a stable point -- known as a stationary performative stable (SPS) solution -- is more challenging than the counterpart in conventional ML tasks due to the model-induced distribution shifts. While considerable efforts have been made to find SPS solutions using methods such as repeated gradient descent (RGD) and greedy stochastic gradient descent (SGD-GD), most prior studies assumed a strongly convex loss until a recent work established $\mathcal{O}(1/\sqrt{T})$ convergence of SGD-GD to SPS solutions under smooth, non-convex losses. However, this latest progress is still based on the restricted bounded variance assumption in stochastic gradient estimates and yields convergence bounds with a non-vanishing error neighborhood that scales with the variance. This limitation motivates us to improve convergence rates and reduce error in stochastic optimization for PP, particularly in non-convex settings. Thus, we propose a new algorithm called stochastic performative prediction with variance reduction (SPRINT) and establish its convergence to an SPS solution at a rate of $\mathcal{O}(1/T)$. Notably, the resulting error neighborhood is **independent** of the variance of the stochastic gradients. Experiments on multiple real datasets with non-convex models demonstrate that SPRINT outperforms SGD-GD in both convergence rate and stability.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VQEzy: An Open-Source Dataset for Parameter Initialize in Variational Quantum Eigensolvers</title>
<link>https://arxiv.org/abs/2509.17322</link>
<guid>https://arxiv.org/abs/2509.17322</guid>
<content:encoded><![CDATA[
arXiv:2509.17322v1 Announce Type: new 
Abstract: Variational Quantum Eigensolvers (VQEs) are a leading class of noisy intermediate-scale quantum (NISQ) algorithms, whose performance is highly sensitive to parameter initialization. Although recent machine learning-based initialization methods have achieved state-of-the-art performance, their progress has been limited by the lack of comprehensive datasets. Existing resources are typically restricted to a single domain, contain only a few hundred instances, and lack complete coverage of Hamiltonians, ansatz circuits, and optimization trajectories. To overcome these limitations, we introduce VQEzy, the first large-scale dataset for VQE parameter initialization. VQEzy spans three major domains and seven representative tasks, comprising 12,110 instances with full VQE specifications and complete optimization trajectories. The dataset is available online, and will be continuously refined and expanded to support future research in VQE optimization.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizable End-to-End Tool-Use RL with Synthetic CodeGym</title>
<link>https://arxiv.org/abs/2509.17325</link>
<guid>https://arxiv.org/abs/2509.17325</guid>
<content:encoded><![CDATA[
arXiv:2509.17325v1 Announce Type: new 
Abstract: Tool-augmented large language models (LLMs), hereafter LLM agents, leverage external tools to solve diverse tasks and interface with the real world. However, current training practices largely rely on supervised fine-tuning (SFT) over static trajectories or reinforcement learning (RL) on narrow tasks, and generalize poorly beyond development settings, leading to brittleness with new tools and unseen workflows. Because code execution reflects many structures of real-world workflows, coding problems provide a natural basis for building agent training environments. Motivated by this, we introduce CodeGym, a scalable framework that synthesizes diverse, verifiable, and controllable multi-turn tool-use environments for agent RL, enabling LLM agents to explore and master various workflows actively. CodeGym rewrites static coding problems into interactive environments by extracting atomic functions or logic into callable tools, yielding verifiable tasks that span various tool-execution workflows. Models of varying sizes and chain-of-thought configurations, trained in CodeGym, exhibit consistent out-of-distribution generalizability; for example, Qwen2.5-32B-Instruct achieves an absolute accuracy gain of 8.7 points on the OOD benchmark $\tau$-Bench. These results highlight CodeGym as a step toward scalable general-purpose RL environments that align with real-world agent workflows.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Anomaly Detection Under Normality Distribution Shift in Dynamic Graphs</title>
<link>https://arxiv.org/abs/2509.17400</link>
<guid>https://arxiv.org/abs/2509.17400</guid>
<content:encoded><![CDATA[
arXiv:2509.17400v1 Announce Type: new 
Abstract: Anomaly detection in dynamic graphs is a critical task with broad real-world applications, including social networks, e-commerce, and cybersecurity. Most existing methods assume that normal patterns remain stable over time; however, this assumption often fails in practice due to the phenomenon we refer to as normality distribution shift (NDS), where normal behaviors evolve over time. Ignoring NDS can lead models to misclassify shifted normal instances as anomalies, degrading detection performance. To tackle this issue, we propose WhENDS, a novel unsupervised anomaly detection method that aligns normal edge embeddings across time by estimating distributional statistics and applying whitening transformations. Extensive experiments on four widely-used dynamic graph datasets show that WhENDS consistently outperforms nine strong baselines, achieving state-of-the-art results and underscoring the importance of addressing NDS in dynamic graph anomaly detection.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Sliced Wasserstein Distance Computation via Adaptive Bayesian Optimization</title>
<link>https://arxiv.org/abs/2509.17405</link>
<guid>https://arxiv.org/abs/2509.17405</guid>
<content:encoded><![CDATA[
arXiv:2509.17405v1 Announce Type: new 
Abstract: The sliced Wasserstein distance (SW) reduces optimal transport on $\mathbb{R}^d$ to a sum of one-dimensional projections, and thanks to this efficiency, it is widely used in geometry, generative modeling, and registration tasks. Recent work shows that quasi-Monte Carlo constructions for computing SW (QSW) yield direction sets with excellent approximation error. This paper presents an alternate, novel approach: learning directions with Bayesian optimization (BO), particularly in settings where SW appears inside an optimization loop (e.g., gradient flows). We introduce a family of drop-in selectors for projection directions: BOSW, a one-shot BO scheme on the unit sphere; RBOSW, a periodic-refresh variant; ABOSW, an adaptive hybrid that seeds from competitive QSW sets and performs a few lightweight BO refinements; and ARBOSW, a restarted hybrid that periodically relearns directions during optimization. Our BO approaches can be composed with QSW and its variants (demonstrated by ABOSW/ARBOSW) and require no changes to downstream losses or gradients. We provide numerical experiments where our methods achieve state-of-the-art performance, and on the experimental suite of the original QSW paper, we find that ABOSW and ARBOSW can achieve convergence comparable to the best QSW variants with modest runtime overhead.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributionally Robust Safety Verification of Neural Networks via Worst-Case CVaR</title>
<link>https://arxiv.org/abs/2509.17413</link>
<guid>https://arxiv.org/abs/2509.17413</guid>
<content:encoded><![CDATA[
arXiv:2509.17413v1 Announce Type: new 
Abstract: Ensuring the safety of neural networks under input uncertainty is a fundamental challenge in safety-critical applications. This paper builds on and expands Fazlyab's quadratic-constraint (QC) and semidefinite-programming (SDP) framework for neural network verification to a distributionally robust and tail-risk-aware setting by integrating worst-case Conditional Value-at-Risk (WC-CVaR) over a moment-based ambiguity set with fixed mean and covariance. The resulting conditions remain SDP-checkable and explicitly account for tail risk. This integration broadens input-uncertainty geometry-covering ellipsoids, polytopes, and hyperplanes-and extends applicability to safety-critical domains where tail-event severity matters. Applications to closed-loop reachability of control systems and classification are demonstrated through numerical experiments, illustrating how the risk level $\varepsilon$ trades conservatism for tolerance to tail events-while preserving the computational structure of prior QC/SDP methods for neural network verification and robustness analysis.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MVCL-DAF++: Enhancing Multimodal Intent Recognition via Prototype-Aware Contrastive Alignment and Coarse-to-Fine Dynamic Attention Fusion</title>
<link>https://arxiv.org/abs/2509.17446</link>
<guid>https://arxiv.org/abs/2509.17446</guid>
<content:encoded><![CDATA[
arXiv:2509.17446v1 Announce Type: new 
Abstract: Multimodal intent recognition (MMIR) suffers from weak semantic grounding and poor robustness under noisy or rare-class conditions. We propose MVCL-DAF++, which extends MVCL-DAF with two key modules: (1) Prototype-aware contrastive alignment, aligning instances to class-level prototypes to enhance semantic consistency; and (2) Coarse-to-fine attention fusion, integrating global modality summaries with token-level features for hierarchical cross-modal interaction. On MIntRec and MIntRec2.0, MVCL-DAF++ achieves new state-of-the-art results, improving rare-class recognition by +1.05\% and +4.18\% WF1, respectively. These results demonstrate the effectiveness of prototype-guided learning and coarse-to-fine fusion for robust multimodal understanding. The source code is available at https://github.com/chr1s623/MVCL-DAF-PlusPlus.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Periodic Graph-Enhanced Multivariate Time Series Anomaly Detector</title>
<link>https://arxiv.org/abs/2509.17472</link>
<guid>https://arxiv.org/abs/2509.17472</guid>
<content:encoded><![CDATA[
arXiv:2509.17472v1 Announce Type: new 
Abstract: Multivariate time series (MTS) anomaly detection commonly encounters in various domains like finance, healthcare, and industrial monitoring. However, existing MTS anomaly detection methods are mostly defined on the static graph structure, which fails to perform an accurate representation of complex spatio-temporal correlations in MTS. To address this issue, this study proposes a Periodic Graph-Enhanced Multivariate Time Series Anomaly Detector (PGMA) with the following two-fold ideas: a) designing a periodic time-slot allocation strategy based Fast Fourier Transform (FFT), which enables the graph structure to reflect dynamic changes in MTS; b) utilizing graph neural network and temporal extension convolution to accurate extract the complex spatio-temporal correlations from the reconstructed periodic graphs. Experiments on four real datasets from real applications demonstrate that the proposed PGMA outperforms state-of-the-art models in MTS anomaly detection.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Path-Weighted Integrated Gradients for Interpretable Dementia Classification</title>
<link>https://arxiv.org/abs/2509.17491</link>
<guid>https://arxiv.org/abs/2509.17491</guid>
<content:encoded><![CDATA[
arXiv:2509.17491v1 Announce Type: new 
Abstract: Integrated Gradients (IG) is a widely used attribution method in explainable artificial intelligence (XAI). In this paper, we introduce Path-Weighted Integrated Gradients (PWIG), a generalization of IG that incorporates a customizable weighting function into the attribution integral. This modification allows for targeted emphasis along different segments of the path between a baseline and the input, enabling improved interpretability, noise mitigation, and the detection of path-dependent feature relevance. We establish its theoretical properties and illustrate its utility through experiments on a dementia classification task using the OASIS-1 MRI dataset. Attribution maps generated by PWIG highlight clinically meaningful brain regions associated with various stages of dementia, providing users with sharp and stable explanations. The results suggest that PWIG offers a flexible and theoretically grounded approach for enhancing attribution quality in complex predictive models.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BiLCNet : BiLSTM-Conformer Network for Encrypted Traffic Classification with 5G SA Physical Channel Records</title>
<link>https://arxiv.org/abs/2509.17495</link>
<guid>https://arxiv.org/abs/2509.17495</guid>
<content:encoded><![CDATA[
arXiv:2509.17495v1 Announce Type: new 
Abstract: Accurate and efficient traffic classification is vital for wireless network management, especially under encrypted payloads and dynamic application behavior, where traditional methods such as port-based identification and deep packet inspection (DPI) are increasingly inadequate. This work explores the feasibility of using physical channel data collected from the air interface of 5G Standalone (SA) networks for traffic sensing. We develop a preprocessing pipeline to transform raw channel records into structured representations with customized feature engineering to enhance downstream classification performance. To jointly capture temporal dependencies and both local and global structural patterns inherent in physical channel records, we propose a novel hybrid architecture: BiLSTM-Conformer Network (BiLCNet), which integrates the sequential modeling capability of Bidirectional Long Short-Term Memory networks (BiLSTM) with the spatial feature extraction strength of Conformer blocks. Evaluated on a noise-limited 5G SA dataset, our model achieves a classification accuracy of 93.9%, outperforming a series of conventional machine learning and deep learning algorithms. Furthermore, we demonstrate its generalization ability under zero-shot transfer settings, validating its robustness across traffic categories and varying environmental conditions.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Achilles' Heel of Mamba: Essential difficulties of the Mamba architecture demonstrated by synthetic data</title>
<link>https://arxiv.org/abs/2509.17514</link>
<guid>https://arxiv.org/abs/2509.17514</guid>
<content:encoded><![CDATA[
arXiv:2509.17514v1 Announce Type: new 
Abstract: State Space Models (SSMs) have emerged as promising alternatives to attention mechanisms, with the Mamba architecture demonstrating impressive performance and linear complexity for processing long sequences. However, the fundamental differences between Mamba and Transformer architectures remain incompletely understood. In this work, we use carefully designed synthetic tasks to reveal Mamba's inherent limitations. Through experiments, we identify that Mamba's nonlinear convolution introduces an asymmetry bias that significantly impairs its ability to recognize symmetrical patterns and relationships. Using composite function and inverse sequence matching tasks, we demonstrate that Mamba strongly favors compositional solutions over symmetrical ones and struggles with tasks requiring the matching of reversed sequences. We show these limitations stem not from the SSM module itself but from the nonlinear convolution preceding it, which fuses token information asymmetrically. These insights provide a new understanding of Mamba's constraints and suggest concrete architectural improvements for future sequence models.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Unlearning Framework for Continual Learning</title>
<link>https://arxiv.org/abs/2509.17530</link>
<guid>https://arxiv.org/abs/2509.17530</guid>
<content:encoded><![CDATA[
arXiv:2509.17530v1 Announce Type: new 
Abstract: Growing concerns surrounding AI safety and data privacy have driven the development of Machine Unlearning as a potential solution. However, current machine unlearning algorithms are designed to complement the offline training paradigm. The emergence of the Continual Learning (CL) paradigm promises incremental model updates, enabling models to learn new tasks sequentially. Naturally, some of those tasks may need to be unlearned to address safety or privacy concerns that might arise. We find that applying conventional unlearning algorithms in continual learning environments creates two critical problems: performance degradation on retained tasks and task relapse, where previously unlearned tasks resurface during subsequent learning. Furthermore, most unlearning algorithms require data to operate, which conflicts with CL's philosophy of discarding past data. A clear need arises for unlearning algorithms that are data-free and mindful of future learning. To that end, we propose UnCLe, an Unlearning framework for Continual Learning. UnCLe employs a hypernetwork that learns to generate task-specific network parameters, using task embeddings. Tasks are unlearned by aligning the corresponding generated network parameters with noise, without requiring any data. Empirical evaluations on several vision data sets demonstrate UnCLe's ability to sequentially perform multiple learning and unlearning operations with minimal disruption to previously acquired knowledge.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SeqBattNet: A Discrete-State Physics-Informed Neural Network with Aging Adaptation for Battery Modeling</title>
<link>https://arxiv.org/abs/2509.17621</link>
<guid>https://arxiv.org/abs/2509.17621</guid>
<content:encoded><![CDATA[
arXiv:2509.17621v1 Announce Type: new 
Abstract: Accurate battery modeling is essential for reliable state estimation in modern applications, such as predicting the remaining discharge time and remaining discharge energy in battery management systems. Existing approaches face several limitations: model-based methods require a large number of parameters; data-driven methods rely heavily on labeled datasets; and current physics-informed neural networks (PINNs) often lack aging adaptation, or still depend on many parameters, or continuously regenerate states. In this work, we propose SeqBattNet, a discrete-state PINN with built-in aging adaptation for battery modeling, to predict terminal voltage during the discharge process. SeqBattNet consists of two components: (i) an encoder, implemented as the proposed HRM-GRU deep learning module, which generates cycle-specific aging adaptation parameters; and (ii) a decoder, based on the equivalent circuit model (ECM) combined with deep learning, which uses these parameters together with the input current to predict voltage. The model requires only three basic battery parameters and, when trained on data from a single cell, still achieves robust performance. Extensive evaluations across three benchmark datasets (TRI, RT-Batt, and NASA) demonstrate that SeqBattNet significantly outperforms classical sequence models and PINN baselines, achieving consistently lower RMSE while maintaining computational efficiency.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing Data Assimilation and Likelihood-Based Inference on Latent State Estimation in Agent-Based Models</title>
<link>https://arxiv.org/abs/2509.17625</link>
<guid>https://arxiv.org/abs/2509.17625</guid>
<content:encoded><![CDATA[
arXiv:2509.17625v1 Announce Type: new 
Abstract: In this paper, we present the first systematic comparison of Data Assimilation (DA) and Likelihood-Based Inference (LBI) in the context of Agent-Based Models (ABMs). These models generate observable time series driven by evolving, partially-latent microstates. Latent states need to be estimated to align simulations with real-world data -- a task traditionally addressed by DA, especially in continuous and equation-based models such as those used in weather forecasting. However, the nature of ABMs poses challenges for standard DA methods. Solving such issues requires adaptation of previous DA techniques, or ad-hoc alternatives such as LBI. DA approximates the likelihood in a model-agnostic way, making it broadly applicable but potentially less precise. In contrast, LBI provides more accurate state estimation by directly leveraging the model's likelihood, but at the cost of requiring a hand-crafted, model-specific likelihood function, which may be complex or infeasible to derive. We compare the two methods on the Bounded-Confidence Model, a well-known opinion dynamics ABM, where agents are affected only by others holding sufficiently similar opinions. We find that LBI better recovers latent agent-level opinions, even under model mis-specification, leading to improved individual-level forecasts. At the aggregate level, however, both methods perform comparably, and DA remains competitive across levels of aggregation under certain parameter settings. Our findings suggest that DA is well-suited for aggregate predictions, while LBI is preferable for agent-level inference.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mechanistic Interpretability with SAEs: Probing Religion, Violence, and Geography in Large Language Models</title>
<link>https://arxiv.org/abs/2509.17665</link>
<guid>https://arxiv.org/abs/2509.17665</guid>
<content:encoded><![CDATA[
arXiv:2509.17665v1 Announce Type: new 
Abstract: Despite growing research on bias in large language models (LLMs), most work has focused on gender and race, with little attention to religious identity. This paper explores how religion is internally represented in LLMs and how it intersects with concepts of violence and geography. Using mechanistic interpretability and Sparse Autoencoders (SAEs) via the Neuronpedia API, we analyze latent feature activations across five models. We measure overlap between religion- and violence-related prompts and probe semantic patterns in activation contexts. While all five religions show comparable internal cohesion, Islam is more frequently linked to features associated with violent language. In contrast, geographic associations largely reflect real-world religious demographics, revealing how models embed both factual distributions and cultural stereotypes. These findings highlight the value of structural analysis in auditing not just outputs but also internal representations that shape model behavior.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast, Accurate and Interpretable Graph Classification with Topological Kernels</title>
<link>https://arxiv.org/abs/2509.17693</link>
<guid>https://arxiv.org/abs/2509.17693</guid>
<content:encoded><![CDATA[
arXiv:2509.17693v1 Announce Type: new 
Abstract: We introduce a novel class of explicit feature maps based on topological indices that represent each graph by a compact feature vector, enabling fast and interpretable graph classification. Using radial basis function kernels on these compact vectors, we define a measure of similarity between graphs. We perform evaluation on standard molecular datasets and observe that classification accuracies based on single topological-index feature vectors underperform compared to state-of-the-art substructure-based kernels. However, we achieve significantly faster Gram matrix evaluation -- up to $20\times$ faster -- compared to the Weisfeiler--Lehman subtree kernel. To enhance performance, we propose two extensions: 1) concatenating multiple topological indices into an \emph{Extended Feature Vector} (EFV), and 2) \emph{Linear Combination of Topological Kernels} (LCTK) by linearly combining Radial Basis Function kernels computed on feature vectors of individual topological graph indices. These extensions deliver up to $12\%$ percent accuracy gains across all the molecular datasets. A complexity analysis highlights the potential for exponential quantum speedup for some of the vector components. Our results indicate that LCTK and EFV offer a favourable trade-off between accuracy and efficiency, making them strong candidates for practical graph learning applications.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cluster Workload Allocation: A Predictive Approach Leveraging Machine Learning Efficiency</title>
<link>https://arxiv.org/abs/2509.17695</link>
<guid>https://arxiv.org/abs/2509.17695</guid>
<content:encoded><![CDATA[
arXiv:2509.17695v1 Announce Type: new 
Abstract: This research investigates how Machine Learning (ML) algorithms can assist in workload allocation strategies by detecting tasks with node affinity operators (referred to as constraint operators), which constrain their execution to a limited number of nodes. Using real-world Google Cluster Data (GCD) workload traces and the AGOCS framework, the study extracts node attributes and task constraints, then analyses them to identify suitable node-task pairings. It focuses on tasks that can be executed on either a single node or fewer than a thousand out of 12.5k nodes in the analysed GCD cluster. Task constraint operators are compacted, pre-processed with one-hot encoding, and used as features in a training dataset. Various ML classifiers, including Artificial Neural Networks, K-Nearest Neighbours, Decision Trees, Naive Bayes, Ridge Regression, Adaptive Boosting, and Bagging, are fine-tuned and assessed for accuracy and F1-scores. The final ensemble voting classifier model achieved 98% accuracy and a 1.5-1.8% misclassification rate for tasks with a single suitable node.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A non-smooth regularization framework for learning over multitask graphs</title>
<link>https://arxiv.org/abs/2509.17728</link>
<guid>https://arxiv.org/abs/2509.17728</guid>
<content:encoded><![CDATA[
arXiv:2509.17728v1 Announce Type: new 
Abstract: In this work, we consider learning over multitask graphs, where each agent aims to estimate its own parameter vector. Although agents seek distinct objectives, collaboration among them can be beneficial in scenarios where relationships between tasks exist. Among the various approaches to promoting relationships between tasks and, consequently, enhancing collaboration between agents, one notable method is regularization. While previous multitask learning studies have focused on smooth regularization to enforce graph smoothness, this work explores non-smooth regularization techniques that promote sparsity, making them particularly effective in encouraging piecewise constant transitions on the graph. We begin by formulating a global regularized optimization problem, which involves minimizing the aggregate sum of individual costs, regularized by a general non-smooth term designed to promote piecewise-constant relationships between the tasks of neighboring agents. Based on the forward-backward splitting strategy, we propose a decentralized learning approach that enables efficient solutions to the regularized optimization problem. Then, under convexity assumptions on the cost functions and co-regularization, we establish that the proposed approach converges in the mean-square-error sense within $O(\mu)$ of the optimal solution of the globally regularized cost. For broader applicability and improved computational efficiency, we also derive closed-form expressions for commonly used non-smooth (and, possibly, non-convex) regularizers, such as the weighted sum of the $\ell_0$-norm, $\ell_1$-norm, and elastic net regularization. Finally, we illustrate both the theoretical findings and the effectiveness of the approach through simulations.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Generative Conditional Distribution Equality Testing Framework and Its Minimax Analysis</title>
<link>https://arxiv.org/abs/2509.17729</link>
<guid>https://arxiv.org/abs/2509.17729</guid>
<content:encoded><![CDATA[
arXiv:2509.17729v1 Announce Type: new 
Abstract: In this paper, we propose a general framework for testing the equality of the conditional distributions in a two-sample problem. This problem is most relevant to transfer learning under covariate shift. Our framework is built on neural network-based generative methods and sample splitting techniques by transforming the conditional distribution testing problem into an unconditional one. We introduce two special tests: the generative permutation-based conditional distribution equality test and the generative classification accuracy-based conditional distribution equality test. Theoretically, we establish a minimax lower bound for statistical inference in testing the equality of two conditional distributions under certain smoothness conditions. We demonstrate that the generative permutation-based conditional distribution equality test and its modified version can attain this lower bound precisely or up to some iterated logarithmic factor. Moreover, we prove the testing consistency of the generative classification accuracy-based conditional distribution equality test. We also establish the convergence rate for the learned conditional generator by deriving new results related to the recently-developed offset Rademacher complexity and approximation properties using neural networks. Empirically, we conduct numerical studies including synthetic datasets and two real-world datasets, demonstrating the effectiveness of our approach.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConfClip: Confidence-Weighted and Clipped Reward for Reinforcement Learning in LLMs</title>
<link>https://arxiv.org/abs/2509.17730</link>
<guid>https://arxiv.org/abs/2509.17730</guid>
<content:encoded><![CDATA[
arXiv:2509.17730v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has become a standard paradigm for refining large language models (LLMs) beyond pre-training and instruction tuning. A prominent line of work is RL with verifiable rewards (RLVR), which leverages automatically verifiable outcomes (e.g., correctness or executability) to generate reward signals. While efficient, this framework faces two key limitations: First, its binary feedback is too sparse to capture the quality of the reasoning process. Second, its coarse-grained rewards potentially lead to vanishing gradients. Inspired by observations from human learning, we introduce a RL technique that integrates verifiable outcomes with the model's own confidence estimates. This joint design enriches the reward signal, providing finer-grained feedback and implicitly supervising the reasoning process. Experimental results demonstrate that our proposed method enhances RL performance across multiple datasets and reduces token consumption during inference, while incurring negligible additional training cost. Moreover, it can be used as a plug-in module to enhance other state-of-the-art RL methods.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An AutoML Framework using AutoGluonTS for Forecasting Seasonal Extreme Temperatures</title>
<link>https://arxiv.org/abs/2509.17734</link>
<guid>https://arxiv.org/abs/2509.17734</guid>
<content:encoded><![CDATA[
arXiv:2509.17734v1 Announce Type: new 
Abstract: In recent years, great progress has been made in the field of forecasting meteorological variables. Recently, deep learning architectures have made a major breakthrough in forecasting the daily average temperature over a ten-day horizon. However, advances in forecasting events related to the maximum temperature over short horizons remain a challenge for the community. A problem that is even more complex consists in making predictions of the maximum daily temperatures in the short, medium, and long term. In this work, we focus on forecasting events related to the maximum daily temperature over medium-term periods (90 days). Therefore, instead of addressing the problem from a meteorological point of view, this article tackles it from a climatological point of view. Due to the complexity of this problem, a common approach is to frame the study as a temporal classification problem with the classes: maximum temperature "above normal", "normal" or "below normal". From a practical point of view, we created a large historical dataset (from 1981 to 2018) collecting information from weather stations located in South America. In addition, we also integrated exogenous information from the Pacific, Atlantic, and Indian Ocean basins. We applied the AutoGluonTS platform to solve the above-mentioned problem. This AutoML tool shows competitive forecasting performance with respect to large operational platforms dedicated to tackling this climatological problem; but with a "relatively" low computational cost in terms of time and resources.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flatness is Necessary, Neural Collapse is Not: Rethinking Generalization via Grokking</title>
<link>https://arxiv.org/abs/2509.17738</link>
<guid>https://arxiv.org/abs/2509.17738</guid>
<content:encoded><![CDATA[
arXiv:2509.17738v1 Announce Type: new 
Abstract: Neural collapse, i.e., the emergence of highly symmetric, class-wise clustered representations, is frequently observed in deep networks and is often assumed to reflect or enable generalization. In parallel, flatness of the loss landscape has been theoretically and empirically linked to generalization. Yet, the causal role of either phenomenon remains unclear: Are they prerequisites for generalization, or merely by-products of training dynamics? We disentangle these questions using grokking, a training regime in which memorization precedes generalization, allowing us to temporally separate generalization from training dynamics and we find that while both neural collapse and relative flatness emerge near the onset of generalization, only flatness consistently predicts it. Models encouraged to collapse or prevented from collapsing generalize equally well, whereas models regularized away from flat solutions exhibit delayed generalization. Furthermore, we show theoretically that neural collapse implies relative flatness under classical assumptions, explaining their empirical co-occurrence. Our results support the view that relative flatness is a potentially necessary and more fundamental property for generalization, and demonstrate how grokking can serve as a powerful probe for isolating its geometric underpinnings.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GEM-T: Generative Tabular Data via Fitting Moments</title>
<link>https://arxiv.org/abs/2509.17752</link>
<guid>https://arxiv.org/abs/2509.17752</guid>
<content:encoded><![CDATA[
arXiv:2509.17752v1 Announce Type: new 
Abstract: Tabular data dominates data science but poses challenges for generative models, especially when the data is limited or sensitive. We present a novel approach to generating synthetic tabular data based on the principle of maximum entropy -- MaxEnt -- called GEM-T, for ``generative entropy maximization for tables.'' GEM-T directly captures nth-order interactions -- pairwise, third-order, etc. -- among columns of training data. In extensive testing, GEM-T matches or exceeds deep neural network approaches previously regarded as state-of-the-art in 23 of 34 publicly available datasets representing diverse subject domains (68\%). Notably, GEM-T involves orders-of-magnitude fewer trainable parameters, demonstrating that much of the information in real-world data resides in low-dimensional, potentially human-interpretable correlations, provided that the input data is appropriately transformed first. Furthermore, MaxEnt better handles heterogeneous data types (continuous vs. discrete vs. categorical), lack of local structure, and other features of tabular data. GEM-T represents a promising direction for light-weight high-performance generative models for structured data.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Neural Antiderivatives</title>
<link>https://arxiv.org/abs/2509.17755</link>
<guid>https://arxiv.org/abs/2509.17755</guid>
<content:encoded><![CDATA[
arXiv:2509.17755v1 Announce Type: new 
Abstract: Neural fields offer continuous, learnable representations that extend beyond traditional discrete formats in visual computing. We study the problem of learning neural representations of repeated antiderivatives directly from a function, a continuous analogue of summed-area tables. Although widely used in discrete domains, such cumulative schemes rely on grids, which prevents their applicability in continuous neural contexts. We introduce and analyze a range of neural methods for repeated integration, including both adaptations of prior work and novel designs. Our evaluation spans multiple input dimensionalities and integration orders, assessing both reconstruction quality and performance in downstream tasks such as filtering and rendering. These results enable integrating classical cumulative operators into modern neural systems and offer insights into learning tasks involving differential and integral operators.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revealing Multimodal Causality with Large Language Models</title>
<link>https://arxiv.org/abs/2509.17784</link>
<guid>https://arxiv.org/abs/2509.17784</guid>
<content:encoded><![CDATA[
arXiv:2509.17784v1 Announce Type: new 
Abstract: Uncovering cause-and-effect mechanisms from data is fundamental to scientific progress. While large language models (LLMs) show promise for enhancing causal discovery (CD) from unstructured data, their application to the increasingly prevalent multimodal setting remains a critical challenge. Even with the advent of multimodal LLMs (MLLMs), their efficacy in multimodal CD is hindered by two primary limitations: (1) difficulty in exploring intra- and inter-modal interactions for comprehensive causal variable identification; and (2) insufficiency to handle structural ambiguities with purely observational data. To address these challenges, we propose MLLM-CD, a novel framework for multimodal causal discovery from unstructured data. It consists of three key components: (1) a novel contrastive factor discovery module to identify genuine multimodal factors based on the interactions explored from contrastive sample pairs; (2) a statistical causal structure discovery module to infer causal relationships among discovered factors; and (3) an iterative multimodal counterfactual reasoning module to refine the discovery outcomes iteratively by incorporating the world knowledge and reasoning capabilities of MLLMs. Extensive experiments on both synthetic and real-world datasets demonstrate the effectiveness of MLLM-CD in revealing genuine factors and causal relationships among them from multimodal unstructured data.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Elucidating the Design Space of FP4 training</title>
<link>https://arxiv.org/abs/2509.17791</link>
<guid>https://arxiv.org/abs/2509.17791</guid>
<content:encoded><![CDATA[
arXiv:2509.17791v1 Announce Type: new 
Abstract: The increasing computational demands of foundation models have spurred research into low-precision training, with 4-bit floating-point (\texttt{FP4}) formats emerging as a frontier for maximizing hardware throughput. While numerous techniques have been proposed to stabilize \texttt{FP4} training, they often present isolated solutions with varying, and not always clear, computational overheads. This paper aims to provide a unified view of the design space of \texttt{FP4} training. We introduce a comprehensive, quantisation gradient-based framework for microscaling quantization that allows for a theoretical analysis of the computational costs associated with different stabilization methods on both the forward and backward passes. Using a simulator built on this framework, we conduct an extensive empirical study across a wide range of machine learning tasks, including regression, image classification, diffusion models, and language models. By systematically evaluating thousands of combinations of techniques, such as novel gradient approximations, rounding strategies, and scaling methods, we identify which configurations offer the most favourable performance-to-overhead trade-off. We find that the techniques enabling the best trade-off involve carefully combining Hadamard transformations, tensor scaling and stochastic rounding. We further find that using \texttt{UE5M3} as a scaling factor potentially offers a good compromise between range and precision with manageable computational overhead.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Remote Sensing-Oriented World Model</title>
<link>https://arxiv.org/abs/2509.17808</link>
<guid>https://arxiv.org/abs/2509.17808</guid>
<content:encoded><![CDATA[
arXiv:2509.17808v1 Announce Type: new 
Abstract: World models have shown potential in artificial intelligence by predicting and reasoning about world states beyond direct observations. However, existing approaches are predominantly evaluated in synthetic environments or constrained scene settings, limiting their validation in real-world contexts with broad spatial coverage and complex semantics. Meanwhile, remote sensing applications urgently require spatial reasoning capabilities for disaster response and urban planning. This paper bridges these gaps by introducing the first framework for world modeling in remote sensing. We formulate remote sensing world modeling as direction-conditioned spatial extrapolation, where models generate semantically consistent adjacent image tiles given a central observation and directional instruction. To enable rigorous evaluation, we develop RSWISE (Remote Sensing World-Image Spatial Evaluation), a benchmark containing 1,600 evaluation tasks across four scenarios: general, flood, urban, and rural. RSWISE combines visual fidelity assessment with instruction compliance evaluation using GPT-4o as a semantic judge, ensuring models genuinely perform spatial reasoning rather than simple replication. Afterwards, we present RemoteBAGEL, a unified multimodal model fine-tuned on remote sensing data for spatial extrapolation tasks. Extensive experiments demonstrate that RemoteBAGEL consistently outperforms state-of-the-art baselines on RSWISE.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MTM: A Multi-Scale Token Mixing Transformer for Irregular Multivariate Time Series Classification</title>
<link>https://arxiv.org/abs/2509.17809</link>
<guid>https://arxiv.org/abs/2509.17809</guid>
<content:encoded><![CDATA[
arXiv:2509.17809v1 Announce Type: new 
Abstract: Irregular multivariate time series (IMTS) is characterized by the lack of synchronized observations across its different channels. In this paper, we point out that this channel-wise asynchrony can lead to poor channel-wise modeling of existing deep learning methods. To overcome this limitation, we propose MTM, a multi-scale token mixing transformer for the classification of IMTS. We find that the channel-wise asynchrony can be alleviated by down-sampling the time series to coarser timescales, and propose to incorporate a masked concat pooling in MTM that gradually down-samples IMTS to enhance the channel-wise attention modules. Meanwhile, we propose a novel channel-wise token mixing mechanism which proactively chooses important tokens from one channel and mixes them with other channels, to further boost the channel-wise learning of our model. Through extensive experiments on real-world datasets and comparison with state-of-the-art methods, we demonstrate that MTM consistently achieves the best performance on all the benchmarks, with improvements of up to 3.8% in AUPRC for classification.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MSGAT-GRU: A Multi-Scale Graph Attention and Recurrent Model for Spatiotemporal Road Accident Prediction</title>
<link>https://arxiv.org/abs/2509.17811</link>
<guid>https://arxiv.org/abs/2509.17811</guid>
<content:encoded><![CDATA[
arXiv:2509.17811v1 Announce Type: new 
Abstract: Accurate prediction of road accidents remains challenging due to intertwined spatial, temporal, and contextual factors in urban traffic. We propose MSGAT-GRU, a multi-scale graph attention and recurrent model that jointly captures localized and long-range spatial dependencies while modeling sequential dynamics. Heterogeneous inputs, such as traffic flow, road attributes, weather, and points of interest, are systematically fused to enhance robustness and interpretability. On the Hybrid Beijing Accidents dataset, MSGAT-GRU achieves an RMSE of 0.334 and an F1-score of 0.878, consistently outperforming strong baselines. Cross-dataset evaluation on METR-LA under a 1-hour horizon further supports transferability, with RMSE of 6.48 (vs. 7.21 for the GMAN model) and comparable MAPE. Ablations indicate that three-hop spatial aggregation and a two-layer GRU offer the best accuracy-stability trade-off. These results position MSGAT-GRU as a scalable and generalizable model for intelligent transportation systems, providing interpretable signals that can inform proactive traffic management and road safety analytics.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Global Optimization via Softmin Energy Minimization</title>
<link>https://arxiv.org/abs/2509.17815</link>
<guid>https://arxiv.org/abs/2509.17815</guid>
<content:encoded><![CDATA[
arXiv:2509.17815v1 Announce Type: new 
Abstract: Global optimization, particularly for non-convex functions with multiple local minima, poses significant challenges for traditional gradient-based methods. While metaheuristic approaches offer empirical effectiveness, they often lack theoretical convergence guarantees and may disregard available gradient information. This paper introduces a novel gradient-based swarm particle optimization method designed to efficiently escape local minima and locate global optima. Our approach leverages a "Soft-min Energy" interacting function, $J_\beta(\mathbf{x})$, which provides a smooth, differentiable approximation of the minimum function value within a particle swarm. We define a stochastic gradient flow in the particle space, incorporating a Brownian motion term for exploration and a time-dependent parameter $\beta$ to control smoothness, similar to temperature annealing. We theoretically demonstrate that for strongly convex functions, our dynamics converges to a stationary point where at least one particle reaches the global minimum, with other particles exhibiting exploratory behavior. Furthermore, we show that our method facilitates faster transitions between local minima by reducing effective potential barriers with respect to Simulated Annealing. More specifically, we estimate the hitting times of unexplored potential wells for our model in the small noise regime and show that they compare favorably with the ones of overdamped Langevin. Numerical experiments on benchmark functions, including double wells and the Ackley function, validate our theoretical findings and demonstrate better performance over the well-known Simulated Annealing method in terms of escaping local minima and achieving faster convergence.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conv-like Scale-Fusion Time Series Transformer: A Multi-Scale Representation for Variable-Length Long Time Series</title>
<link>https://arxiv.org/abs/2509.17845</link>
<guid>https://arxiv.org/abs/2509.17845</guid>
<content:encoded><![CDATA[
arXiv:2509.17845v1 Announce Type: new 
Abstract: Time series analysis faces significant challenges in handling variable-length data and achieving robust generalization. While Transformer-based models have advanced time series tasks, they often struggle with feature redundancy and limited generalization capabilities. Drawing inspiration from classical CNN architectures' pyramidal structure, we propose a Multi-Scale Representation Learning Framework based on a Conv-like ScaleFusion Transformer. Our approach introduces a temporal convolution-like structure that combines patching operations with multi-head attention, enabling progressive temporal dimension compression and feature channel expansion. We further develop a novel cross-scale attention mechanism for effective feature fusion across different temporal scales, along with a log-space normalization method for variable-length sequences. Extensive experiments demonstrate that our framework achieves superior feature independence, reduced redundancy, and better performance in forecasting and classification tasks compared to state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Post-Training Structural Changes in Large Language Models</title>
<link>https://arxiv.org/abs/2509.17866</link>
<guid>https://arxiv.org/abs/2509.17866</guid>
<content:encoded><![CDATA[
arXiv:2509.17866v1 Announce Type: new 
Abstract: Post-training fundamentally alters the behavior of large language models (LLMs), yet its impact on the internal parameter space remains poorly understood. In this work, we conduct a systematic singular value decomposition (SVD) analysis of principal linear layers in pretrained LLMs, focusing on two widely adopted post-training methods: instruction tuning and long-chain-of-thought (Long-CoT) distillation. Our analysis reveals two consistent and unexpected structural changes:(1) a near-uniform geometric scaling of singular values across layers, which theoretically modulates attention scores; and (2) highly consistent orthogonal transformations are applied to the left and right singular vectors of each matrix. Disrupting this orthogonal consistency leads to catastrophic performance degradation. Based on these findings, we propose a simple yet effective framework that interprets post-training as a reparameterization of fixed subspaces in the pretrained parameter space. Further experiments reveal that singular value scaling behaves as a secondary effect, analogous to a temperature adjustment, whereas the core functional transformation lies in the coordinated rotation of singular vectors. These results challenge the prevailing view of the parameter space in large models as a black box, uncovering the first clear regularities in how parameters evolve during training, and providing a new perspective for deeper investigation into model parameter changes.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving After-sales Service: Deep Reinforcement Learning for Dynamic Time Slot Assignment with Commitments and Customer Preferences</title>
<link>https://arxiv.org/abs/2509.17870</link>
<guid>https://arxiv.org/abs/2509.17870</guid>
<content:encoded><![CDATA[
arXiv:2509.17870v1 Announce Type: new 
Abstract: Problem definition: For original equipment manufacturers (OEMs), high-tech maintenance is a strategic component in after-sales services, involving close coordination between customers and service engineers. Each customer suggests several time slots for their maintenance task, from which the OEM must select one. This decision needs to be made promptly to support customers' planning. At the end of each day, routes for service engineers are planned to fulfill the tasks scheduled for the following day. We study this hierarchical and sequential decision-making problem-the Dynamic Time Slot Assignment Problem with Commitments and Customer Preferences (DTSAP-CCP)-in this paper. Methodology/results: Two distinct approaches are proposed: 1) an attention-based deep reinforcement learning with rollout execution (ADRL-RE) and 2) a scenario-based planning approach (SBP). The ADRL-RE combines a well-trained attention-based neural network with a rollout framework for online trajectory simulation. To support the training, we develop a neural heuristic solver that provides rapid route planning solutions, enabling efficient learning in complex combinatorial settings. The SBP approach samples several scenarios to guide the time slot assignment. Numerical experiments demonstrate the superiority of ADRL-RE and the stability of SBP compared to both rule-based and rollout-based approaches. Furthermore, the strong practicality of ADRL-RE is verified in a case study of after-sales service for large medical equipment. Implications: This study provides OEMs with practical decision-support tools for dynamic maintenance scheduling, balancing customer preferences and operational efficiency. In particular, our ADRL-RE shows strong real-world potential, supporting timely and customer-aligned maintenance scheduling.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Hierarchical Learning with Nested Subspace Networks</title>
<link>https://arxiv.org/abs/2509.17874</link>
<guid>https://arxiv.org/abs/2509.17874</guid>
<content:encoded><![CDATA[
arXiv:2509.17874v1 Announce Type: new 
Abstract: Large neural networks are typically trained for a fixed computational budget, creating a rigid trade-off between performance and efficiency that is ill-suited for deployment in resource-constrained or dynamic environments. Existing approaches to this problem present a difficult choice: training a discrete collection of specialist models is computationally prohibitive, while dynamic methods like slimmable networks often lack the flexibility to be applied to large, pre-trained foundation models. In this work, we propose Nested Subspace Networks (NSNs), a novel architectural paradigm that enables a single model to be dynamically and granularly adjusted across a continuous spectrum of compute budgets at inference time. The core of our approach is to re-parameterize linear layers to satisfy a nested subspace property, such that the function computed at a given rank is a strict subspace of the function at any higher rank. We show that this entire hierarchy of models can be optimized jointly via an uncertainty-aware objective that learns to balance the contributions of different ranks based on their intrinsic difficulty. We demonstrate empirically that NSNs can be surgically applied to pre-trained LLMs and unlock a smooth and predictable compute-performance frontier. For example, a single NSN-adapted model can achieve a 50% reduction in inference FLOPs with only a 5 percentage point loss in accuracy. Our findings establish NSNs as a powerful framework for creating the next generation of adaptive foundation models.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Confidence-gated training for efficient early-exit neural networks</title>
<link>https://arxiv.org/abs/2509.17885</link>
<guid>https://arxiv.org/abs/2509.17885</guid>
<content:encoded><![CDATA[
arXiv:2509.17885v1 Announce Type: new 
Abstract: Early-exit neural networks reduce inference cost by enabling confident predictions at intermediate layers. However, joint training often leads to gradient interference, with deeper classifiers dominating optimization. We propose Confidence-Gated Training (CGT), a paradigm that conditionally propagates gradients from deeper exits only when preceding exits fail. This encourages shallow classifiers to act as primary decision points while reserving deeper layers for harder inputs. By aligning training with the inference-time policy, CGT mitigates overthinking, improves early-exit accuracy, and preserves efficiency. Experiments on the Indian Pines and Fashion-MNIST benchmarks show that CGT lowers average inference cost while improving overall accuracy, offering a practical solution for deploying deep models in resource-constrained environments.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GaussianPSL: A novel framework based on Gaussian Splatting for exploring the Pareto frontier in multi-criteria optimization</title>
<link>https://arxiv.org/abs/2509.17889</link>
<guid>https://arxiv.org/abs/2509.17889</guid>
<content:encoded><![CDATA[
arXiv:2509.17889v1 Announce Type: new 
Abstract: Multi-objective optimization (MOO) is essential for solving complex real-world problems involving multiple conflicting objectives. However, many practical applications - including engineering design, autonomous systems, and machine learning - often yield non-convex, degenerate, or discontinuous Pareto frontiers, which involve traditional scalarization and Pareto Set Learning (PSL) methods that struggle to approximate accurately. Existing PSL approaches perform well on convex fronts but tend to fail in capturing the diversity and structure of irregular Pareto sets commonly observed in real-world scenarios. In this paper, we propose Gaussian-PSL, a novel framework that integrates Gaussian Splatting into PSL to address the challenges posed by non-convex Pareto frontiers. Our method dynamically partitions the preference vector space, enabling simple MLP networks to learn localized features within each region, which are then integrated by an additional MLP aggregator. This partition-aware strategy enhances both exploration and convergence, reduces sensi- tivity to initialization, and improves robustness against local optima. We first provide the mathematical formulation for controllable Pareto set learning using Gaussian Splat- ting. Then, we introduce the Gaussian-PSL architecture and evaluate its performance on synthetic and real-world multi-objective benchmarks. Experimental results demonstrate that our approach outperforms standard PSL models in learning irregular Pareto fronts while maintaining computational efficiency and model simplicity. This work offers a new direction for effective and scalable MOO under challenging frontier geometries.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Inference in Transformer-Based Models: A Multi-Method Benchmark</title>
<link>https://arxiv.org/abs/2509.17894</link>
<guid>https://arxiv.org/abs/2509.17894</guid>
<content:encoded><![CDATA[
arXiv:2509.17894v1 Announce Type: new 
Abstract: Efficient inference is a critical challenge in deep generative modeling, particularly as diffusion models grow in capacity and complexity. While increased complexity often improves accuracy, it raises compute costs, latency, and memory requirements. This work investigates techniques such as pruning, quantization, knowledge distillation, and simplified attention to reduce computational overhead without impacting performance. The study also explores the Mixture of Experts (MoE) approach to further enhance efficiency. These experiments provide insights into optimizing inference for the state-of-the-art Fast Diffusion Transformer (fast-DiT) model.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SingLEM: Single-Channel Large EEG Model</title>
<link>https://arxiv.org/abs/2509.17920</link>
<guid>https://arxiv.org/abs/2509.17920</guid>
<content:encoded><![CDATA[
arXiv:2509.17920v1 Announce Type: new 
Abstract: Current deep learning models for electroencephalography (EEG) are often task-specific and depend on large labeled datasets, limiting their adaptability. Although emerging foundation models aim for broader applicability, their rigid dependence on fixed, high-density multi-channel montages restricts their use across heterogeneous datasets and in missing-channel or practical low-channel settings. To address these limitations, we introduce SingLEM, a self-supervised foundation model that learns robust, general-purpose representations from single-channel EEG, making it inherently hardware agnostic. The model employs a hybrid encoder architecture that combines convolutional layers to extract local features with a hierarchical transformer to model both short- and long-range temporal dependencies. SingLEM is pretrained on 71 public datasets comprising over 9,200 subjects and 357,000 single-channel hours of EEG. When evaluated as a fixed feature extractor across six motor imagery and cognitive tasks, aggregated single-channel representations consistently outperformed leading multi-channel foundation models and handcrafted baselines. These results demonstrate that a single-channel approach can achieve state-of-the-art generalization while enabling fine-grained neurophysiological analysis and enhancing interpretability. The source code and pretrained models are available at https://github.com/ttlabtuat/SingLEM.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Medical priority fusion: achieving dual optimization of sensitivity and interpretability in nipt anomaly detection</title>
<link>https://arxiv.org/abs/2509.17924</link>
<guid>https://arxiv.org/abs/2509.17924</guid>
<content:encoded><![CDATA[
arXiv:2509.17924v1 Announce Type: new 
Abstract: Clinical machine learning faces a critical dilemma in high-stakes medical applications: algorithms achieving optimal diagnostic performance typically sacrifice the interpretability essential for physician decision-making, while interpretable methods compromise sensitivity in complex scenarios. This paradox becomes particularly acute in non-invasive prenatal testing (NIPT), where missed chromosomal abnormalities carry profound clinical consequences yet regulatory frameworks mandate explainable AI systems. We introduce Medical Priority Fusion (MPF), a constrained multi-objective optimization framework that resolves this fundamental trade-off by systematically integrating Naive Bayes probabilistic reasoning with Decision Tree rule-based logic through mathematically-principled weighted fusion under explicit medical constraints. Rigorous validation on 1,687 real-world NIPT samples characterized by extreme class imbalance (43.4:1 normal-to-abnormal ratio) employed stratified 5-fold cross-validation with comprehensive ablation studies and statistical hypothesis testing using McNemar's paired comparisons. MPF achieved simultaneous optimization of dual objectives: 89.3% sensitivity (95% CI: 83.9-94.7%) with 80% interpretability score, significantly outperforming individual algorithms (McNemar's test, p < 0.001). The optimal fusion configuration achieved Grade A clinical deployment criteria with large effect size (d = 1.24), establishing the first clinically-deployable solution that maintains both diagnostic accuracy and decision transparency essential for prenatal care. This work demonstrates that medical-constrained algorithm fusion can resolve the interpretability-performance trade-off, providing a mathematical framework for developing high-stakes medical decision support systems that meet both clinical efficacy and explainability requirements.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StefaLand: An Efficient Geoscience Foundation Model That Improves Dynamic Land-Surface Predictions</title>
<link>https://arxiv.org/abs/2509.17942</link>
<guid>https://arxiv.org/abs/2509.17942</guid>
<content:encoded><![CDATA[
arXiv:2509.17942v1 Announce Type: new 
Abstract: Stewarding natural resources, mitigating floods, droughts, wildfires, and landslides, and meeting growing demands require models that can predict climate-driven land-surface responses and human feedback with high accuracy. Traditional impact models, whether process-based, statistical, or machine learning, struggle with spatial generalization due to limited observations and concept drift. Recently proposed vision foundation models trained on satellite imagery demand massive compute and are ill-suited for dynamic land-surface prediction. We introduce StefaLand, a generative spatiotemporal earth foundation model centered on landscape interactions. StefaLand improves predictions on three tasks and four datasets: streamflow, soil moisture, and soil composition, compared to prior state-of-the-art. Results highlight its ability to generalize across diverse, data-scarce regions and support broad land-surface applications. The model builds on a masked autoencoder backbone that learns deep joint representations of landscape attributes, with a location-aware architecture fusing static and time-series inputs, attribute-based representations that drastically reduce compute, and residual fine-tuning adapters that enhance transfer. While inspired by prior methods, their alignment with geoscience and integration in one model enables robust performance on dynamic land-surface tasks. StefaLand can be pretrained and finetuned on academic compute yet outperforms state-of-the-art baselines and even fine-tuned vision foundation models. To our knowledge, this is the first geoscience land-surface foundation model that demonstrably improves dynamic land-surface interaction predictions and supports diverse downstream applications.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Optimization of Memory Frequency, Computing Frequency, Transmission Power and Task Offloading for Energy-efficient DNN Inference</title>
<link>https://arxiv.org/abs/2509.17970</link>
<guid>https://arxiv.org/abs/2509.17970</guid>
<content:encoded><![CDATA[
arXiv:2509.17970v1 Announce Type: new 
Abstract: Deep neural networks (DNNs) have been widely applied in diverse applications, but the problems of high latency and energy overhead are inevitable on resource-constrained devices. To address this challenge, most researchers focus on the dynamic voltage and frequency scaling (DVFS) technique to balance the latency and energy consumption by changing the computing frequency of processors. However, the adjustment of memory frequency is usually ignored and not fully utilized to achieve efficient DNN inference, which also plays a significant role in the inference time and energy consumption. In this paper, we first investigate the impact of joint memory frequency and computing frequency scaling on the inference time and energy consumption with a model-based and data-driven method. Then by combining with the fitting parameters of different DNN models, we give a preliminary analysis for the proposed model to see the effects of adjusting memory frequency and computing frequency simultaneously. Finally, simulation results in local inference and cooperative inference cases further validate the effectiveness of jointly scaling the memory frequency and computing frequency to reduce the energy consumption of devices.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intra-Cluster Mixup: An Effective Data Augmentation Technique for Complementary-Label Learning</title>
<link>https://arxiv.org/abs/2509.17971</link>
<guid>https://arxiv.org/abs/2509.17971</guid>
<content:encoded><![CDATA[
arXiv:2509.17971v1 Announce Type: new 
Abstract: In this paper, we investigate the challenges of complementary-label learning (CLL), a specialized form of weakly-supervised learning (WSL) where models are trained with labels indicating classes to which instances do not belong, rather than standard ordinary labels. This alternative supervision is appealing because collecting complementary labels is generally cheaper and less labor-intensive. Although most existing research in CLL emphasizes the development of novel loss functions, the potential of data augmentation in this domain remains largely underexplored. In this work, we uncover that the widely-used Mixup data augmentation technique is ineffective when directly applied to CLL. Through in-depth analysis, we identify that the complementary-label noise generated by Mixup negatively impacts the performance of CLL models. We then propose an improved technique called Intra-Cluster Mixup (ICM), which only synthesizes augmented data from nearby examples, to mitigate the noise effect. ICM carries the benefits of encouraging complementary label sharing of nearby examples, and leads to substantial performance improvements across synthetic and real-world labeled datasets. In particular, our wide spectrum of experimental results on both balanced and imbalanced CLL settings justifies the potential of ICM in allying with state-of-the-art CLL algorithms, achieving significant accuracy increases of 30% and 10% on MNIST and CIFAR datasets, respectively.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Budgeted Adversarial Attack against Graph-Based Anomaly Detection in Sensor Networks</title>
<link>https://arxiv.org/abs/2509.17987</link>
<guid>https://arxiv.org/abs/2509.17987</guid>
<content:encoded><![CDATA[
arXiv:2509.17987v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) have emerged as powerful models for anomaly detection in sensor networks, particularly when analyzing multivariate time series. In this work, we introduce BETA, a novel grey-box evasion attack targeting such GNN-based detectors, where the attacker is constrained to perturb sensor readings from a limited set of nodes, excluding the target sensor, with the goal of either suppressing a true anomaly or triggering a false alarm at the target node. BETA identifies the sensors most influential to the target node's classification and injects carefully crafted adversarial perturbations into their features, all while maintaining stealth and respecting the attacker's budget. Experiments on three real-world sensor network datasets show that BETA reduces the detection accuracy of state-of-the-art GNN-based detectors by 30.62 to 39.16% on average, and significantly outperforms baseline attack strategies, while operating within realistic constraints.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equilibrium flow: From Snapshots to Dynamics</title>
<link>https://arxiv.org/abs/2509.17990</link>
<guid>https://arxiv.org/abs/2509.17990</guid>
<content:encoded><![CDATA[
arXiv:2509.17990v1 Announce Type: new 
Abstract: Scientific data, from cellular snapshots in biology to celestial distributions in cosmology, often consists of static patterns from underlying dynamical systems. These snapshots, while lacking temporal ordering, implicitly encode the processes that preserve them. This work investigates how strongly such a distribution constrains its underlying dynamics and how to recover them. We introduce the Equilibrium flow method, a framework that learns continuous dynamics that preserve a given pattern distribution. Our method successfully identifies plausible dynamics for 2-D systems and recovers the signature chaotic behavior of the Lorenz attractor. For high-dimensional Turing patterns from the Gray-Scott model, we develop an efficient, training-free variant that achieves high fidelity to the ground truth, validated both quantitatively and qualitatively. Our analysis reveals the solution space is constrained not only by the data but also by the learning model's inductive biases. This capability extends beyond recovering known systems, enabling a new paradigm of inverse design for Artificial Life. By specifying a target pattern distribution, we can discover the local interaction rules that preserve it, leading to the spontaneous emergence of complex behaviors, such as life-like flocking, attraction, and repulsion patterns, from simple, user-defined snapshots.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Kernel Design for Bayesian Optimization Is a Piece of CAKE with LLMs</title>
<link>https://arxiv.org/abs/2509.17998</link>
<guid>https://arxiv.org/abs/2509.17998</guid>
<content:encoded><![CDATA[
arXiv:2509.17998v1 Announce Type: new 
Abstract: The efficiency of Bayesian optimization (BO) relies heavily on the choice of the Gaussian process (GP) kernel, which plays a central role in balancing exploration and exploitation under limited evaluation budgets. Traditional BO methods often rely on fixed or heuristic kernel selection strategies, which can result in slow convergence or suboptimal solutions when the chosen kernel is poorly suited to the underlying objective function. To address this limitation, we propose a freshly-baked Context-Aware Kernel Evolution (CAKE) to enhance BO with large language models (LLMs). Concretely, CAKE leverages LLMs as the crossover and mutation operators to adaptively generate and refine GP kernels based on the observed data throughout the optimization process. To maximize the power of CAKE, we further propose BIC-Acquisition Kernel Ranking (BAKER) to select the most effective kernel through balancing the model fit measured by the Bayesian information criterion (BIC) with the expected improvement at each iteration of BO. Extensive experiments demonstrate that our fresh CAKE-based BO method consistently outperforms established baselines across a range of real-world tasks, including hyperparameter optimization, controller tuning, and photonic chip design. Our code is publicly available at https://github.com/cake4bo/cake.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling m-Sharpness Through the Structure of Stochastic Gradient Noise</title>
<link>https://arxiv.org/abs/2509.18001</link>
<guid>https://arxiv.org/abs/2509.18001</guid>
<content:encoded><![CDATA[
arXiv:2509.18001v1 Announce Type: new 
Abstract: Sharpness-aware minimization (SAM) has emerged as a highly effective technique for improving model generalization, but its underlying principles are not fully understood. We investigated the phenomenon known as m-sharpness, where the performance of SAM improves monotonically as the micro-batch size for computing perturbations decreases. Leveraging an extended Stochastic Differential Equation (SDE) framework, combined with an analysis of the structure of stochastic gradient noise (SGN), we precisely characterize the dynamics of various SAM variants. Our findings reveal that the stochastic noise introduced during SAM perturbations inherently induces a variance-based sharpness regularization effect. Motivated by our theoretical insights, we introduce Reweighted SAM, which employs sharpness-weighted sampling to mimic the generalization benefits of m-SAM while remaining parallelizable. Comprehensive experiments validate the effectiveness of our theoretical analysis and proposed method.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Control Disturbance Rejection in Neural ODEs</title>
<link>https://arxiv.org/abs/2509.18034</link>
<guid>https://arxiv.org/abs/2509.18034</guid>
<content:encoded><![CDATA[
arXiv:2509.18034v1 Announce Type: new 
Abstract: In this paper, we propose an iterative training algorithm for Neural ODEs that provides models resilient to control (parameter) disturbances. The method builds on our earlier work Tuning without Forgetting-and similarly introduces training points sequentially, and updates the parameters on new data within the space of parameters that do not decrease performance on the previously learned training points-with the key difference that, inspired by the concept of flat minima, we solve a minimax problem for a non-convex non-concave functional over an infinite-dimensional control space. We develop a projected gradient descent algorithm on the space of parameters that admits the structure of an infinite-dimensional Banach subspace. We show through simulations that this formulation enables the model to effectively learn new data points and gain robustness against control disturbance.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforced Generation of Combinatorial Structures: Applications to Complexity Theory</title>
<link>https://arxiv.org/abs/2509.18057</link>
<guid>https://arxiv.org/abs/2509.18057</guid>
<content:encoded><![CDATA[
arXiv:2509.18057v1 Announce Type: new 
Abstract: We explore whether techniques from AI can help discover new combinatorial structures that improve provable limits on efficient algorithms. Specifically, we use AlphaEvolve (an LLM coding agent) to study two settings:
  a) Average-case hardness for MAX-CUT and MAX-Independent Set: We improve a recent result of Kunisky and Yu to obtain near-optimal upper and (conditional) lower bounds on certification algorithms for MAX-CUT and MAX-Independent Set on random 3- and 4-regular graphs. Our improved lower bounds are obtained by constructing nearly extremal Ramanujan graphs on as many as $163$ nodes, using AlphaEvolve. Additionally, via analytical arguments we strengthen the upper bounds to settle the computational hardness of these questions up to an error in the third decimal place.
  b) Worst-case Hardness of Approximation for MAX-k-CUT: We obtain new inapproximability results, proving that it is NP-hard to approximate MAX-4-CUT and MAX-3-CUT within factors of $0.987$ and $0.9649$ respectively, using AlphaEvolve to discover new gadget reductions. Our MAX-4-CUT result improves upon the SOTA of $0.9883$, and our MAX-3-CUT result improves on the current best gadget-based inapproximability result of $0.9853$, but falls short of improving the SOTA of $16/17$ that relies on a custom PCP, rather than a gadget reduction from "standard" H{\aa}stad-style PCPs.
  A key technical challenge we faced: verifying a candidate construction produced by AlphaEvolve is costly (often requiring exponential time). In both settings above, our results were enabled by using AlphaEvolve itself to evolve the verification procedure to be faster (sometimes by $10,000\times$). We conclude with a discussion of norms by which to assess the assistance from AI in developing proofs.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier LLM</title>
<link>https://arxiv.org/abs/2509.18058</link>
<guid>https://arxiv.org/abs/2509.18058</guid>
<content:encoded><![CDATA[
arXiv:2509.18058v1 Announce Type: new 
Abstract: Large language model (LLM) developers aim for their models to be honest, helpful, and harmless. However, when faced with malicious requests, models are trained to refuse, sacrificing helpfulness. We show that frontier LLMs can develop a preference for dishonesty as a new strategy, even when other options are available. Affected models respond to harmful requests with outputs that sound harmful but are subtly incorrect or otherwise harmless in practice. This behavior emerges with hard-to-predict variations even within models from the same model family. We find no apparent cause for the propensity to deceive, but we show that more capable models are better at executing this strategy. Strategic dishonesty already has a practical impact on safety evaluations, as we show that dishonest responses fool all output-based monitors used to detect jailbreaks that we test, rendering benchmark scores unreliable. Further, strategic dishonesty can act like a honeypot against malicious users, which noticeably obfuscates prior jailbreak attacks. While output monitors fail, we show that linear probes on internal activations can be used to reliably detect strategic dishonesty. We validate probes on datasets with verifiable outcomes and by using their features as steering vectors. Overall, we consider strategic dishonesty as a concrete example of a broader concern that alignment of LLMs is hard to control, especially when helpfulness and harmlessness conflict.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Rank with Top-$K$ Fairness</title>
<link>https://arxiv.org/abs/2509.18067</link>
<guid>https://arxiv.org/abs/2509.18067</guid>
<content:encoded><![CDATA[
arXiv:2509.18067v1 Announce Type: new 
Abstract: Fairness in ranking models is crucial, as disparities in exposure can disproportionately affect protected groups. Most fairness-aware ranking systems focus on ensuring comparable average exposure for groups across the entire ranked list, which may not fully address real-world concerns. For example, when a ranking model is used for allocating resources among candidates or disaster hotspots, decision-makers often prioritize only the top-$K$ ranked items, while the ranking beyond top-$K$ becomes less relevant. In this paper, we propose a list-wise learning-to-rank framework that addresses the issues of inequalities in top-$K$ rankings at training time. Specifically, we propose a top-$K$ exposure disparity measure that extends the classic exposure disparity metric in a ranked list. We then learn a ranker to balance relevance and fairness in top-$K$ rankings. Since direct top-$K$ selection is computationally expensive for a large number of items, we transform the non-differentiable selection process into a differentiable objective function and develop efficient stochastic optimization algorithms to achieve both high accuracy and sufficient fairness. Extensive experiments demonstrate that our method outperforms existing methods.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning functions, operators and dynamical systems with kernels</title>
<link>https://arxiv.org/abs/2509.18071</link>
<guid>https://arxiv.org/abs/2509.18071</guid>
<content:encoded><![CDATA[
arXiv:2509.18071v1 Announce Type: new 
Abstract: This expository article presents the approach to statistical machine learning based on reproducing kernel Hilbert spaces. The basic framework is introduced for scalar-valued learning and then extended to operator learning. Finally, learning dynamical systems is formulated as a suitable operator learning problem, leveraging Koopman operator theory.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative Decoding</title>
<link>https://arxiv.org/abs/2509.18085</link>
<guid>https://arxiv.org/abs/2509.18085</guid>
<content:encoded><![CDATA[
arXiv:2509.18085v1 Announce Type: new 
Abstract: Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to autoregressive LLMs (AR-LLMs) with the potential to operate at significantly higher token generation rates. However, currently available open-source dLLMs often generate at much lower rates, typically decoding only a single token at every denoising timestep in order to maximize output quality. We present Spiffy, a speculative decoding algorithm that accelerates dLLM inference by $\mathbf{2.8{-}3.1\times}$ while provably preserving the model's output distribution. This work addresses the unique challenges involved in applying ideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes draft states by leveraging the dLLM's distribution itself in an auto-speculative manner. This approach is efficient and effective, and eliminates the overheads of training and running an independent draft model. To structure the candidate draft states, we propose a novel directed draft graph which is uniquely designed to take advantage of the bidirectional, block-wise nature of dLLM generation and can be verified in parallel by the dLLM. To further optimize the structure of these draft graphs, we introduce an efficient, offline calibration algorithm that procedurally determines high-quality graph configurations. These optimized draft graphs, enabling increased acceptance rates, lead to a significant boost in the overall speedup achieved by the system. Crucially, Spiffy is also complementary to other recent innovations in improving dLLM generation speeds such as KV-caching and multi-token unmasking. We demonstrate that when combined with such parallel decoding algorithms, Spiffy is able to effectively multiply the benefits of these methods leading to total speedups of up to $\mathbf{7.9\times}$.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning in Factor Investment</title>
<link>https://arxiv.org/abs/2509.16206</link>
<guid>https://arxiv.org/abs/2509.16206</guid>
<content:encoded><![CDATA[
arXiv:2509.16206v1 Announce Type: cross 
Abstract: Deep reinforcement learning has shown promise in trade execution, yet its use in low-frequency factor portfolio construction remains under-explored. A key obstacle is the high-dimensional, unbalanced state space created by stocks that enter and exit the investable universe. We introduce Conditional Auto-encoded Factor-based Portfolio Optimisation (CAFPO), which compresses stock-level returns into a small set of latent factors conditioned on 94 firm-specific characteristics. The factors feed a DRL agent implemented with both PPO and DDPG to generate continuous long-short weights. On 20 years of U.S. equity data (2000--2020), CAFPO outperforms equal-weight, value-weight, Markowitz, vanilla DRL, and Fama--French-driven DRL, delivering a 24.6\% compound return and a Sharpe ratio of 0.94 out of sample. SHAP analysis further reveals economically intuitive factor attributions. Our results demonstrate that factor-aware representation learning can make DRL practical for institutional, low-turnover portfolio management.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Detection of Internal Defects in Structured Media</title>
<link>https://arxiv.org/abs/2509.16216</link>
<guid>https://arxiv.org/abs/2509.16216</guid>
<content:encoded><![CDATA[
arXiv:2509.16216v1 Announce Type: cross 
Abstract: A critical issue that affects engineers trying to assess the structural integrity of various infrastructures, such as metal rods or acoustic ducts, is the challenge of detecting internal fractures (defects). Traditionally, engineers depend on audible and visual aids to identify these fractures, as they do not physically dissect the object in question into multiple pieces to check for inconsistencies. This research introduces ideas towards the development of a robust strategy to image such defects using only a small set of minimal, non-invasive measurements.
  Assuming a one dimensional model (e.g. longitudinal waves in long and thin rods/acoustic ducts or transverse vibrations of strings), we make use of the continuous one-dimensional wave equation to model these physical phenomena and then employ specialized mathematical analysis tools (the Laplace transform and optimization) to introduce our defect imaging ideas. In particular, we will focus on the case of a long bar which is homogeneous throughout except in a small area where a defect in its Young's modulus is present. We will first demonstrate how the problem is equivalent to a spring-mass vibrational system, and then show how our imaging strategy makes use of the Laplace domain analytic map between the characteristics of the respective defect and the measurement data.
  More explicitly, we will utilize MATLAB (a platform for numerical computations) to collect synthetic data (computational alternative to real world measurements) for several scenarios with one defect of arbitrary location and stiffness. Subsequently, we will use this data along with our analytically developed map (between defect characteristics and measurements) to construct a residual function which, once optimized, will reveal the location and magnitude of the stiffness defect.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of Ensemble Learning Techniques for handwritten OCR Improvement</title>
<link>https://arxiv.org/abs/2509.16221</link>
<guid>https://arxiv.org/abs/2509.16221</guid>
<content:encoded><![CDATA[
arXiv:2509.16221v1 Announce Type: cross 
Abstract: For the bachelor project 2021 of Professor Lippert's research group, handwritten entries of historical patient records needed to be digitized using Optical Character Recognition (OCR) methods. Since the data will be used in the future, a high degree of accuracy is naturally required. Especially in the medical field this has even more importance. Ensemble Learning is a method that combines several machine learning models and is claimed to be able to achieve an increased accuracy for existing methods. For this reason, Ensemble Learning in combination with OCR is investigated in this work in order to create added value for the digitization of the patient records. It was possible to discover that ensemble learning can lead to an increased accuracy for OCR, which methods were able to achieve this and that the size of the training data set did not play a role here.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting First Year Dropout from Pre Enrolment Motivation Statements Using Text Mining</title>
<link>https://arxiv.org/abs/2509.16224</link>
<guid>https://arxiv.org/abs/2509.16224</guid>
<content:encoded><![CDATA[
arXiv:2509.16224v1 Announce Type: cross 
Abstract: Preventing student dropout is a major challenge in higher education and it is difficult to predict prior to enrolment which students are likely to drop out and which students are likely to succeed. High School GPA is a strong predictor of dropout, but much variance in dropout remains to be explained. This study focused on predicting university dropout by using text mining techniques with the aim of exhuming information contained in motivation statements written by students. By combining text data with classic predictors of dropout in the form of student characteristics, we attempt to enhance the available set of predictive student characteristics. Our dataset consisted of 7,060 motivation statements of students enrolling in a non-selective bachelor at a Dutch university in 2014 and 2015. Support Vector Machines were trained on 75 percent of the data and several models were estimated on the test data. We used various combinations of student characteristics and text, such as TFiDF, topic modelling, LIWC dictionary. Results showed that, although the combination of text and student characteristics did not improve the prediction of dropout, text analysis alone predicted dropout similarly well as a set of student characteristics. Suggestions for future research are provided.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning for Quantum Noise Reduction</title>
<link>https://arxiv.org/abs/2509.16242</link>
<guid>https://arxiv.org/abs/2509.16242</guid>
<content:encoded><![CDATA[
arXiv:2509.16242v1 Announce Type: cross 
Abstract: Quantum noise fundamentally limits the utility of near-term quantum devices, making error mitigation essential for practical quantum computation. While traditional quantum error correction codes require substantial qubit overhead and complex syndrome decoding, we propose a machine learning approach that directly reconstructs clean quantum states from noisy density matrices without additional qubits. We formulate quantum noise reduction as a supervised learning problem using a convolutional neural network (CNN) autoencoder architecture with a novel fidelity-aware composite loss function. Our method is trained and evaluated on a comprehensive synthetic dataset of 10,000 density matrices derived from random 5-qubit quantum circuits, encompassing five noise types (depolarizing, amplitude damping, phase damping, bit-flip, and mixed noise) across four intensity levels (0.05-0.20). The CNN successfully reconstructs quantum states across all noise conditions, achieving an average fidelity improvement from 0.298 to 0.774 ({\Delta} = 0.476). Notably, the model demonstrates superior performance on complex mixed noise scenarios and higher noise intensities, with mixed noise showing the highest corrected fidelity (0.807) and improvement (0.567). The approach effectively preserves both diagonal elements (populations) and off-diagonal elements (quantum coherences), making it suitable for entanglement-dependent quantum algorithms. While phase damping presents fundamental information-theoretic limitations, our results suggest that CNN-based density matrix reconstruction offers a promising, resource-efficient alternative to traditional quantum error correction for NISQ-era devices. This data-driven approach could enable practical quantum advantage with fewer physical qubits than conventional error correction schemes require.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Can Quantum Deep Learning Improve Large Language Models?</title>
<link>https://arxiv.org/abs/2509.16244</link>
<guid>https://arxiv.org/abs/2509.16244</guid>
<content:encoded><![CDATA[
arXiv:2509.16244v1 Announce Type: cross 
Abstract: The rapid progress of large language models (LLMs) has transformed natural language processing, yet the challenge of efficient adaptation remains unresolved. Full fine-tuning achieves strong performance but imposes prohibitive computational and memory costs. Parameter-efficient fine-tuning (PEFT) strategies, such as low-rank adaptation (LoRA), Prefix tuning, and sparse low-rank adaptation (SoRA), address this issue by reducing trainable parameters while maintaining competitive accuracy. However, these methods often encounter limitations in scalability, stability, and generalization across diverse tasks. Recent advances in quantum deep learning introduce novel opportunities through quantum-inspired encoding and parameterized quantum circuits (PQCs). In particular, the quantum-amplitude embedded adaptation (QAA) framework demonstrates expressive model updates with minimal overhead. This paper presents a systematic survey and comparative analysis of conventional PEFT methods and QAA. The analysis demonstrates trade-offs in convergence, efficiency, and representational capacity, while providing insight into the potential of quantum approaches for future LLM adaptation.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Motional representation; the ability to predict odor characters using molecular vibrations</title>
<link>https://arxiv.org/abs/2509.16245</link>
<guid>https://arxiv.org/abs/2509.16245</guid>
<content:encoded><![CDATA[
arXiv:2509.16245v1 Announce Type: cross 
Abstract: The prediction of odor characters is still impossible based on the odorant molecular structure. We designed a CNN-based regressor for computed parameters in molecular vibrations (CNN\_vib), in order to investigate the ability to predict odor characters of molecular vibrations. In this study, we explored following three approaches for the predictability; (i) CNN with molecular vibrational parameters, (ii) logistic regression based on vibrational spectra, and (iii) logistic regression with molecular fingerprint(FP). Our investigation demonstrates that both (i) and (ii) provide predictablity, and also that the vibrations as an explanatory variable (i and ii) and logistic regression with fingerprints (iii) show nearly identical tendencies. The predictabilities of (i) and (ii), depending on odor descriptors, are comparable to those of (iii). Our research shows that odor is predictable by odorant molecular vibration as well as their shapes alone. Our findings provide insight into the representation of molecular motional features beyond molecular structures.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraphMend: Code Transformations for Fixing Graph Breaks in PyTorch 2</title>
<link>https://arxiv.org/abs/2509.16248</link>
<guid>https://arxiv.org/abs/2509.16248</guid>
<content:encoded><![CDATA[
arXiv:2509.16248v1 Announce Type: cross 
Abstract: This paper presents GraphMend, a high-level compiler that eliminates FX graph breaks in PyTorch 2 programs. Although PyTorch 2 introduced TorchDynamo and TorchInductor to enable just-in-time graph compilation, unresolved dynamic control flow and unsupported Python constructs often fragment models into multiple FX graphs. These fragments force frequent fallbacks to eager mode, incur costly CPU-to-GPU synchronizations, and reduce optimization opportunities. GraphMend addresses this limitation by analyzing and transforming source code before execution. Built on the Jac compilation framework, GraphMend introduces two code transformations that remove graph breaks due to dynamic control flow and Python I/O functions. This design allows PyTorch's compilation pipeline to capture larger, uninterrupted FX graphs without requiring manual refactoring by developers. Evaluation across eight Hugging Face models shows that GraphMend removes all fixable graph breaks due to dynamic control flow and Python I/O functions, driving the break count to 0 in 6 models and reducing it from 5 to 2 in another model. On NVIDIA RTX 3090 and A40 GPUs, GraphMend achieves up to 75% latency reductions and up to 8% higher end-to-end throughput. These results demonstrate that high-level code transformation is an effective complement to PyTorch's dynamic JIT compilation pipeline, substantially improving both usability and performance.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gender and Political Bias in Large Language Models: A Demonstration Platform</title>
<link>https://arxiv.org/abs/2509.16264</link>
<guid>https://arxiv.org/abs/2509.16264</guid>
<content:encoded><![CDATA[
arXiv:2509.16264v1 Announce Type: cross 
Abstract: We present ParlAI Vote, an interactive system for exploring European Parliament debates and votes, and for testing LLMs on vote prediction and bias analysis. This platform connects debate topics, speeches, and roll-call outcomes, and includes rich demographic data such as gender, age, country, and political group. Users can browse debates, inspect linked speeches, compare real voting outcomes with predictions from frontier LLMs, and view error breakdowns by demographic group. Visualizing the EuroParlVote benchmark and its core tasks of gender classification and vote prediction, ParlAI Vote highlights systematic performance bias in state-of-the-art LLMs. The system unifies data, models, and visual analytics in a single interface, lowering the barrier for reproducing findings, auditing behavior, and running counterfactual scenarios. It supports research, education, and public engagement with legislative decision-making, while making clear both the strengths and the limitations of current LLMs in political analysis.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vibrational Fingerprints of Strained Polymers: A Spectroscopic Pathway to Mechanical State Prediction</title>
<link>https://arxiv.org/abs/2509.16266</link>
<guid>https://arxiv.org/abs/2509.16266</guid>
<content:encoded><![CDATA[
arXiv:2509.16266v1 Announce Type: cross 
Abstract: The vibrational response of polymer networks under load provides a sensitive probe of molecular deformation and a route to non-destructive diagnostics. Here we show that machine-learned force fields reproduce these spectroscopic fingerprints with quantum-level fidelity in realistic epoxy thermosets. Using MACE-OFF23 molecular dynamics, we capture the experimentally observed redshifts of para-phenylene stretching modes under tensile load, in contrast to the harmonic OPLS-AA model. These shifts correlate with molecular elongation and alignment, consistent with Badger's rule, directly linking vibrational features to local stress. To capture IR intensities, we trained a symmetry-adapted dipole moment model on representative epoxy fragments, enabling validation of strain responses. Together, these approaches provide chemically accurate and computationally accessible predictions of strain-dependent vibrational spectra. Our results establish vibrational fingerprints as predictive markers of mechanical state in polymer networks, pointing to new strategies for stress mapping and structural-health diagnostics in advanced materials.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Modeling with Learned Meta-Tokens</title>
<link>https://arxiv.org/abs/2509.16278</link>
<guid>https://arxiv.org/abs/2509.16278</guid>
<content:encoded><![CDATA[
arXiv:2509.16278v1 Announce Type: cross 
Abstract: While modern Transformer-based language models (LMs) have achieved major success in multi-task generalization, they often struggle to capture long-range dependencies within their context window. This work introduces a novel approach using meta-tokens, special tokens injected during pre-training, along with a dedicated meta-attention mechanism to guide LMs to use these tokens. We pre-train a language model with a modified GPT-2 architecture equipped with meta-attention in addition to causal multi-head attention, and study the impact of these tokens on a suite of synthetic tasks. We find that data-efficient language model pre-training on fewer than 100B tokens utilizing meta-tokens and our meta-attention mechanism achieves strong performance on these tasks after fine-tuning. We suggest that these gains arise due to the meta-tokens sharpening the positional encoding. This enables them to operate as trainable, content-based landmarks, implicitly compressing preceding context and "caching" it in the meta-token. At inference-time, the meta-token points to relevant context, facilitating length generalization up to 2$\times$ its context window, even after extension with YaRN. We provide further evidence of these behaviors by visualizing model internals to study the residual stream, and assessing the compression quality by information-theoretic analysis on the rate-distortion tradeoff. Our findings suggest that pre-training LMs with meta-tokens offers a simple, data-efficient method to enhance long-context language modeling performance, while introducing new insights into the nature of their behavior towards length generalization.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Test-Time Learning and Inference-Time Deliberation for Efficiency-First Offline Reinforcement Learning in Care Coordination and Population Health Management</title>
<link>https://arxiv.org/abs/2509.16291</link>
<guid>https://arxiv.org/abs/2509.16291</guid>
<content:encoded><![CDATA[
arXiv:2509.16291v1 Announce Type: cross 
Abstract: Care coordination and population health management programs serve large Medicaid and safety-net populations and must be auditable, efficient, and adaptable. While clinical risk for outreach modalities is typically low, time and opportunity costs differ substantially across text, phone, video, and in-person visits. We propose a lightweight offline reinforcement learning (RL) approach that augments trained policies with (i) test-time learning via local neighborhood calibration, and (ii) inference-time deliberation via a small Q-ensemble that incorporates predictive uncertainty and time/effort cost. The method exposes transparent dials for neighborhood size and uncertainty/cost penalties and preserves an auditable training pipeline. Evaluated on a de-identified operational dataset, TTL+ITD achieves stable value estimates with predictable efficiency trade-offs and subgroup auditing.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TF-DWGNet: A Directed Weighted Graph Neural Network with Tensor Fusion for Multi-Omics Cancer Subtype Classification</title>
<link>https://arxiv.org/abs/2509.16301</link>
<guid>https://arxiv.org/abs/2509.16301</guid>
<content:encoded><![CDATA[
arXiv:2509.16301v1 Announce Type: cross 
Abstract: Integration and analysis of multi-omics data provide valuable insights for cancer subtype classification. However, such data are inherently heterogeneous, high-dimensional, and exhibit complex intra- and inter-modality dependencies. Recent advances in graph neural networks (GNNs) offer powerful tools for modeling such structure. Yet, most existing methods rely on prior knowledge or predefined similarity networks to construct graphs, which are often undirected or unweighted, failing to capture the directionality and strength of biological interactions. Interpretability at both the modality and feature levels also remains limited. To address these challenges, we propose TF-DWGNet, a novel Graph Neural Network framework that combines tree-based Directed Weighted graph construction with Tensor Fusion for multiclass cancer subtype classification. TF-DWGNet introduces two key innovations: a supervised tree-based approach for constructing directed, weighted graphs tailored to each omics modality, and a tensor fusion mechanism that captures unimodal, bimodal, and trimodal interactions using low-rank decomposition for efficiency. TF-DWGNet enables modality-specific representation learning, joint embedding fusion, and interpretable subtype prediction. Experiments on real-world cancer datasets show that TF-DWGNet consistently outperforms state-of-the-art baselines across multiple metrics and statistical tests. Moreover, it provides biologically meaningful insights by ranking influential features and modalities. These results highlight TF-DWGNet's potential for effective and interpretable multi-omics integration in cancer research.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Atlas Graphs for Dynamic Scene Decomposition and Editing</title>
<link>https://arxiv.org/abs/2509.16336</link>
<guid>https://arxiv.org/abs/2509.16336</guid>
<content:encoded><![CDATA[
arXiv:2509.16336v1 Announce Type: cross 
Abstract: Learning editable high-resolution scene representations for dynamic scenes is an open problem with applications across the domains from autonomous driving to creative editing - the most successful approaches today make a trade-off between editability and supporting scene complexity: neural atlases represent dynamic scenes as two deforming image layers, foreground and background, which are editable in 2D, but break down when multiple objects occlude and interact. In contrast, scene graph models make use of annotated data such as masks and bounding boxes from autonomous-driving datasets to capture complex 3D spatial relationships, but their implicit volumetric node representations are challenging to edit view-consistently. We propose Neural Atlas Graphs (NAGs), a hybrid high-resolution scene representation, where every graph node is a view-dependent neural atlas, facilitating both 2D appearance editing and 3D ordering and positioning of scene elements. Fit at test-time, NAGs achieve state-of-the-art quantitative results on the Waymo Open Dataset - by 5 dB PSNR increase compared to existing methods - and make environmental editing possible in high resolution and visual quality - creating counterfactual driving scenarios with new backgrounds and edited vehicle appearance. We find that the method also generalizes beyond driving scenes and compares favorably - by more than 7 dB in PSNR - to recent matting and video editing baselines on the DAVIS video dataset with a diverse set of human and animal-centric scenes.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Similarity-Guided Diffusion for Long-Gap Music Inpainting</title>
<link>https://arxiv.org/abs/2509.16342</link>
<guid>https://arxiv.org/abs/2509.16342</guid>
<content:encoded><![CDATA[
arXiv:2509.16342v1 Announce Type: cross 
Abstract: Music inpainting aims to reconstruct missing segments of a corrupted recording. While diffusion-based generative models improve reconstruction for medium-length gaps, they often struggle to preserve musical plausibility over multi-second gaps. We introduce Similarity-Guided Diffusion Posterior Sampling (SimDPS), a hybrid method that combines diffusion-based inference with similarity search. Candidate segments are first retrieved from a corpus based on contextual similarity, then incorporated into a modified likelihood that guides the diffusion process toward contextually consistent reconstructions. Subjective evaluation on piano music inpainting with 2-s gaps shows that the proposed SimDPS method enhances perceptual plausibility compared to unguided diffusion and frequently outperforms similarity search alone when moderately similar candidates are available. These results demonstrate the potential of a hybrid similarity approach for diffusion-based audio enhancement with long gaps.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accurate Thyroid Cancer Classification using a Novel Binary Pattern Driven Local Discrete Cosine Transform Descriptor</title>
<link>https://arxiv.org/abs/2509.16382</link>
<guid>https://arxiv.org/abs/2509.16382</guid>
<content:encoded><![CDATA[
arXiv:2509.16382v1 Announce Type: cross 
Abstract: In this study, we develop a new CAD system for accurate thyroid cancer classification with emphasis on feature extraction. Prior studies have shown that thyroid texture is important for segregating the thyroid ultrasound images into different classes. Based upon our experience with breast cancer classification, we first conjuncture that the Discrete Cosine Transform (DCT) is the best descriptor for capturing textural features. Thyroid ultrasound images are particularly challenging as the gland is surrounded by multiple complex anatomical structures leading to variations in tissue density. Hence, we second conjuncture the importance of localization and propose that the Local DCT (LDCT) descriptor captures the textural features best in this context. Another disadvantage of complex anatomy around the thyroid gland is scattering of ultrasound waves resulting in noisy and unclear textures. Hence, we third conjuncture that one image descriptor is not enough to fully capture the textural features and propose the integration of another popular texture capturing descriptor (Improved Local Binary Pattern, ILBP) with LDCT. ILBP is known to be noise resilient as well. We term our novel descriptor as Binary Pattern Driven Local Discrete Cosine Transform (BPD-LDCT). Final classification is carried out using a non-linear SVM. The proposed CAD system is evaluated on the only two publicly available thyroid cancer datasets, namely TDID and AUITD. The evaluation is conducted in two stages. In Stage I, thyroid nodules are categorized as benign or malignant. In Stage II, the malignant cases are further sub-classified into TI-RADS (4) and TI-RADS (5). For Stage I classification, our proposed model demonstrates exceptional performance of nearly 100% on TDID and 97% on AUITD. In Stage II classification, the proposed model again attains excellent classification of close to 100% on TDID and 99% on AUITD.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Low-Rank Adaptation of Evolutionary Deep Neural Networks for Efficient Learning of Time-Dependent PDEs</title>
<link>https://arxiv.org/abs/2509.16395</link>
<guid>https://arxiv.org/abs/2509.16395</guid>
<content:encoded><![CDATA[
arXiv:2509.16395v1 Announce Type: cross 
Abstract: We study the Evolutionary Deep Neural Network (EDNN) framework for accelerating numerical solvers of time-dependent partial differential equations (PDEs). We introduce a Low-Rank Evolutionary Deep Neural Network (LR-EDNN), which constrains parameter evolution to a low-rank subspace, thereby reducing the effective dimensionality of training while preserving solution accuracy. The low-rank tangent subspace is defined layer-wise by the singular value decomposition (SVD) of the current network weights, and the resulting update is obtained by solving a well-posed, tractable linear system within this subspace. This design augments the underlying numerical solver with a parameter efficient EDNN component without requiring full fine-tuning of all network weights. We evaluate LR-EDNN on representative PDE problems and compare it against corresponding baselines. Across cases, LR-EDNN achieves comparable accuracy with substantially fewer trainable parameters and reduced computational cost. These results indicate that low-rank constraints on parameter velocities, rather than full-space updates, provide a practical path toward scalable, efficient, and reproducible scientific machine learning for PDEs.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Objects Relocalization in Changing Environments with Flow Matching</title>
<link>https://arxiv.org/abs/2509.16398</link>
<guid>https://arxiv.org/abs/2509.16398</guid>
<content:encoded><![CDATA[
arXiv:2509.16398v1 Announce Type: cross 
Abstract: Task and motion planning are long-standing challenges in robotics, especially when robots have to deal with dynamic environments exhibiting long-term dynamics, such as households or warehouses. In these environments, long-term dynamics mostly stem from human activities, since previously detected objects can be moved or removed from the scene. This adds the necessity to find such objects again before completing the designed task, increasing the risk of failure due to missed relocalizations. However, in these settings, the nature of such human-object interactions is often overlooked, despite being governed by common habits and repetitive patterns. Our conjecture is that these cues can be exploited to recover the most likely objects' positions in the scene, helping to address the problem of unknown relocalization in changing environments. To this end we propose FlowMaps, a model based on Flow Matching that is able to infer multimodal object locations over space and time. Our results present statistical evidence to support our hypotheses, opening the way to more complex applications of our approach. The code is publically available at https://github.com/Fra-Tsuna/flowmaps
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Retrieval: The Geometry and a Pretrain-Finetune Recipe</title>
<link>https://arxiv.org/abs/2509.16411</link>
<guid>https://arxiv.org/abs/2509.16411</guid>
<content:encoded><![CDATA[
arXiv:2509.16411v1 Announce Type: cross 
Abstract: Dual encoder (DE) models, where a pair of matching query and document are embedded into similar vector representations, are widely used in information retrieval due to their simplicity and scalability. However, the Euclidean geometry of the embedding space limits the expressive power of DEs, which may compromise their quality. This paper investigates such limitations in the context of hierarchical retrieval (HR), where the document set has a hierarchical structure and the matching documents for a query are all of its ancestors. We first prove that DEs are feasible for HR as long as the embedding dimension is linear in the depth of the hierarchy and logarithmic in the number of documents. Then we study the problem of learning such embeddings in a standard retrieval setup where DEs are trained on samples of matching query and document pairs. Our experiments reveal a lost-in-the-long-distance phenomenon, where retrieval accuracy degrades for documents further away in the hierarchy. To address this, we introduce a pretrain-finetune recipe that significantly improves long-distance retrieval without sacrificing performance on closer documents. We experiment on a realistic hierarchy from WordNet for retrieving documents at various levels of abstraction, and show that pretrain-finetune boosts the recall on long-distance pairs from 19% to 76%. Finally, we demonstrate that our method improves retrieval of relevant products on a shopping queries dataset.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>End-to-end RL Improves Dexterous Grasping Policies</title>
<link>https://arxiv.org/abs/2509.16434</link>
<guid>https://arxiv.org/abs/2509.16434</guid>
<content:encoded><![CDATA[
arXiv:2509.16434v1 Announce Type: cross 
Abstract: This work explores techniques to scale up image-based end-to-end learning for dexterous grasping with an arm + hand system. Unlike state-based RL, vision-based RL is much more memory inefficient, resulting in relatively low batch sizes, which is not amenable for algorithms like PPO. Nevertheless, it is still an attractive method as unlike the more commonly used techniques which distill state-based policies into vision networks, end-to-end RL can allow for emergent active vision behaviors. We identify a key bottleneck in training these policies is the way most existing simulators scale to multiple GPUs using traditional data parallelism techniques. We propose a new method where we disaggregate the simulator and RL (both training and experience buffers) onto separate GPUs. On a node with four GPUs, we have the simulator running on three of them, and PPO running on the fourth. We are able to show that with the same number of GPUs, we can double the number of existing environments compared to the previous baseline of standard data parallelism. This allows us to train vision-based environments, end-to-end with depth, which were previously performing far worse with the baseline. We train and distill both depth and state-based policies into stereo RGB networks and show that depth distillation leads to better results, both in simulation and reality. This improvement is likely due to the observability gap between state and vision policies which does not exist when distilling depth policies into stereo RGB. We further show that the increased batch size brought about by disaggregated simulation also improves real world performance. When deploying in the real world, we improve upon the previous state-of-the-art vision-based results using our end-to-end policies.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overfitting in Adaptive Robust Optimization</title>
<link>https://arxiv.org/abs/2509.16451</link>
<guid>https://arxiv.org/abs/2509.16451</guid>
<content:encoded><![CDATA[
arXiv:2509.16451v1 Announce Type: cross 
Abstract: Adaptive robust optimization (ARO) extends static robust optimization by allowing decisions to depend on the realized uncertainty - weakly dominating static solutions within the modeled uncertainty set. However, ARO makes previous constraints that were independent of uncertainty now dependent, making it vulnerable to additional infeasibilities when realizations fall outside the uncertainty set. This phenomenon of adaptive policies being brittle is analogous to overfitting in machine learning. To mitigate against this, we propose assigning constraint-specific uncertainty set sizes, with harder constraints given stronger probabilistic guarantees. Interpreted through the overfitting lens, this acts as regularization: tighter guarantees shrink adaptive coefficients to ensure stability, while looser ones preserve useful flexibility. This view motivates a principled approach to designing uncertainty sets that balances robustness and adaptivity.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intrinsic Meets Extrinsic Fairness: Assessing the Downstream Impact of Bias Mitigation in Large Language Models</title>
<link>https://arxiv.org/abs/2509.16462</link>
<guid>https://arxiv.org/abs/2509.16462</guid>
<content:encoded><![CDATA[
arXiv:2509.16462v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) exhibit socio-economic biases that can propagate into downstream tasks. While prior studies have questioned whether intrinsic bias in LLMs affects fairness at the downstream task level, this work empirically investigates the connection. We present a unified evaluation framework to compare intrinsic bias mitigation via concept unlearning with extrinsic bias mitigation via counterfactual data augmentation (CDA). We examine this relationship through real-world financial classification tasks, including salary prediction, employment status, and creditworthiness assessment. Using three open-source LLMs, we evaluate models both as frozen embedding extractors and as fine-tuned classifiers. Our results show that intrinsic bias mitigation through unlearning reduces intrinsic gender bias by up to 94.9%, while also improving downstream task fairness metrics, such as demographic parity by up to 82%, without compromising accuracy. Our framework offers practical guidance on where mitigation efforts can be most effective and highlights the importance of applying early-stage mitigation before downstream deployment.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synergies between Federated Foundation Models and Smart Power Grids</title>
<link>https://arxiv.org/abs/2509.16496</link>
<guid>https://arxiv.org/abs/2509.16496</guid>
<content:encoded><![CDATA[
arXiv:2509.16496v1 Announce Type: cross 
Abstract: The recent emergence of large language models (LLMs) such as GPT-3 has marked a significant paradigm shift in machine learning. Trained on massive corpora of data, these models demonstrate remarkable capabilities in language understanding, generation, summarization, and reasoning, transforming how intelligent systems process and interact with human language. Although LLMs may still seem like a recent breakthrough, the field is already witnessing the rise of a new and more general category: multi-modal, multi-task foundation models (M3T FMs). These models go beyond language and can process heterogeneous data types/modalities, such as time-series measurements, audio, imagery, tabular records, and unstructured logs, while supporting a broad range of downstream tasks spanning forecasting, classification, control, and retrieval. When combined with federated learning (FL), they give rise to M3T Federated Foundation Models (FedFMs): a highly recent and largely unexplored class of models that enable scalable, privacy-preserving model training/fine-tuning across distributed data sources. In this paper, we take one of the first steps toward introducing these models to the power systems research community by offering a bidirectional perspective: (i) M3T FedFMs for smart grids and (ii) smart grids for FedFMs. In the former, we explore how M3T FedFMs can enhance key grid functions, such as load/demand forecasting and fault detection, by learning from distributed, heterogeneous data available at the grid edge in a privacy-preserving manner. In the latter, we investigate how the constraints and structure of smart grids, spanning energy, communication, and regulatory dimensions, shape the design, training, and deployment of M3T FedFMs.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>orb-QFL: Orbital Quantum Federated Learning</title>
<link>https://arxiv.org/abs/2509.16505</link>
<guid>https://arxiv.org/abs/2509.16505</guid>
<content:encoded><![CDATA[
arXiv:2509.16505v1 Announce Type: cross 
Abstract: Recent breakthroughs in quantum computing present transformative opportunities for advancing Federated Learning (FL), particularly in non-terrestrial environments characterized by stringent communication and coordination constraints. In this study, we propose orbital QFL, termed orb-QFL, a novel quantum-assisted Federated Learning framework tailored for Low Earth Orbit (LEO) satellite constellations. Distinct from conventional FL paradigms, termed orb-QFL operates without centralized servers or global aggregation mechanisms (e.g., FedAvg), instead leveraging quantum entanglement and local quantum processing to facilitate decentralized, inter-satellite collaboration. This design inherently addresses the challenges of orbital dynamics, such as intermittent connectivity, high propagation delays, and coverage variability. The framework enables continuous model refinement through direct quantum-based synchronization between neighboring satellites, thereby enhancing resilience and preserving data locality. To validate our approach, we integrate the Qiskit quantum machine learning toolkit with Poliastro-based orbital simulations and conduct experiments using Statlog dataset.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CommonForms: A Large, Diverse Dataset for Form Field Detection</title>
<link>https://arxiv.org/abs/2509.16506</link>
<guid>https://arxiv.org/abs/2509.16506</guid>
<content:encoded><![CDATA[
arXiv:2509.16506v1 Announce Type: cross 
Abstract: This paper introduces CommonForms, a web-scale dataset for form field detection. It casts the problem of form field detection as object detection: given an image of a page, predict the location and type (Text Input, Choice Button, Signature) of form fields. The dataset is constructed by filtering Common Crawl to find PDFs that have fillable elements. Starting with 8 million documents, the filtering process is used to arrive at a final dataset of roughly 55k documents that have over 450k pages. Analysis shows that the dataset contains a diverse mixture of languages and domains; one third of the pages are non-English, and among the 14 classified domains, no domain makes up more than 25% of the dataset.
  In addition, this paper presents a family of form field detectors, FFDNet-Small and FFDNet-Large, which attain a very high average precision on the CommonForms test set. Each model cost less than $500 to train. Ablation results show that high-resolution inputs are crucial for high-quality form field detection, and that the cleaning process improves data efficiency over using all PDFs that have fillable fields in Common Crawl. A qualitative analysis shows that they outperform a popular, commercially available PDF reader that can prepare forms. Unlike the most popular commercially available solutions, FFDNet can predict checkboxes in addition to text and signature fields. This is, to our knowledge, the first large scale dataset released for form field detection, as well as the first open source models. The dataset, models, and code will be released at https://github.com/jbarrow/commonforms
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Etude: Piano Cover Generation with a Three-Stage Approach -- Extract, strucTUralize, and DEcode</title>
<link>https://arxiv.org/abs/2509.16522</link>
<guid>https://arxiv.org/abs/2509.16522</guid>
<content:encoded><![CDATA[
arXiv:2509.16522v1 Announce Type: cross 
Abstract: Piano cover generation aims to automatically transform a pop song into a piano arrangement. While numerous deep learning approaches have been proposed, existing models often fail to maintain structural consistency with the original song, likely due to the absence of beat-aware mechanisms or the difficulty of modeling complex rhythmic patterns. Rhythmic information is crucial, as it defines structural similarity (e.g., tempo, BPM) and directly impacts the overall quality of the generated music.
  In this paper, we introduce Etude, a three-stage architecture consisting of Extract, strucTUralize, and DEcode stages. By pre-extracting rhythmic information and applying a novel, simplified REMI-based tokenization, our model produces covers that preserve proper song structure, enhance fluency and musical dynamics, and support highly controllable generation through style injection. Subjective evaluations with human listeners show that Etude substantially outperforms prior models, achieving a quality level comparable to that of human composers.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Fuzzing for Verifying Machine Unlearning</title>
<link>https://arxiv.org/abs/2509.16525</link>
<guid>https://arxiv.org/abs/2509.16525</guid>
<content:encoded><![CDATA[
arXiv:2509.16525v1 Announce Type: cross 
Abstract: As machine learning models become increasingly embedded in decision-making systems, the ability to "unlearn" targeted data or features is crucial for enhancing model adaptability, fairness, and privacy in models which involves expensive training. To effectively guide machine unlearning, a thorough testing is essential. Existing methods for verification of machine unlearning provide limited insights, often failing in scenarios where the influence is indirect. In this work, we propose CAF\'E, a new causality based framework that unifies datapoint- and feature-level unlearning for verification of black-box ML models. CAF\'E evaluates both direct and indirect effects of unlearning targets through causal dependencies, providing actionable insights with fine-grained analysis. Our evaluation across five datasets and three model architectures demonstrates that CAF\'E successfully detects residual influence missed by baselines while maintaining computational efficiency.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mental Multi-class Classification on Social Media: Benchmarking Transformer Architectures against LSTM Models</title>
<link>https://arxiv.org/abs/2509.16542</link>
<guid>https://arxiv.org/abs/2509.16542</guid>
<content:encoded><![CDATA[
arXiv:2509.16542v1 Announce Type: cross 
Abstract: Millions of people openly share mental health struggles on social media, providing rich data for early detection of conditions such as depression, bipolar disorder, etc. However, most prior Natural Language Processing (NLP) research has focused on single-disorder identification, leaving a gap in understanding the efficacy of advanced NLP techniques for distinguishing among multiple mental health conditions. In this work, we present a large-scale comparative study of state-of-the-art transformer versus Long Short-Term Memory (LSTM)-based models to classify mental health posts into exclusive categories of mental health conditions. We first curate a large dataset of Reddit posts spanning six mental health conditions and a control group, using rigorous filtering and statistical exploratory analysis to ensure annotation quality. We then evaluate five transformer architectures (BERT, RoBERTa, DistilBERT, ALBERT, and ELECTRA) against several LSTM variants (with or without attention, using contextual or static embeddings) under identical conditions. Experimental results show that transformer models consistently outperform the alternatives, with RoBERTa achieving 91-99% F1-scores and accuracies across all classes. Notably, attention-augmented LSTMs with BERT embeddings approach transformer performance (up to 97% F1-score) while training 2-3.5 times faster, whereas LSTMs using static embeddings fail to learn useful signals. These findings represent the first comprehensive benchmark for multi-class mental health detection, offering practical guidance on model selection and highlighting an accuracy-efficiency trade-off for real-world deployment of mental health NLP systems.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Checking extracted rules in Neural Networks</title>
<link>https://arxiv.org/abs/2509.16547</link>
<guid>https://arxiv.org/abs/2509.16547</guid>
<content:encoded><![CDATA[
arXiv:2509.16547v1 Announce Type: cross 
Abstract: In this paper we investigate formal verification of extracted rules for Neural Networks under a complexity theoretic point of view. A rule is a global property or a pattern concerning a large portion of the input space of a network. These rules are algorithmically extracted from networks in an effort to better understand their inner way of working. Here, three problems will be in the focus: Does a given set of rules apply to a given network? Is a given set of rules consistent or do the rules contradict themselves? Is a given set of rules exhaustive in the sense that for every input the output is determined? Finding algorithms that extract such rules out of networks has been investigated over the last 30 years, however, to the author's current knowledge, no attempt in verification was made until now. A lot of attempts of extracting rules use heuristics involving randomness and over-approximation, so it might be beneficial to know whether knowledge obtained in that way can actually be trusted.
  We investigate the above questions for neural networks with ReLU-activation as well as for Boolean networks, each for several types of rules. We demonstrate how these problems can be reduced to each other and show that most of them are co-NP-complete.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Person Identification from Egocentric Human-Object Interactions using 3D Hand Pose</title>
<link>https://arxiv.org/abs/2509.16557</link>
<guid>https://arxiv.org/abs/2509.16557</guid>
<content:encoded><![CDATA[
arXiv:2509.16557v1 Announce Type: cross 
Abstract: Human-Object Interaction Recognition (HOIR) and user identification play a crucial role in advancing augmented reality (AR)-based personalized assistive technologies. These systems are increasingly being deployed in high-stakes, human-centric environments such as aircraft cockpits, aerospace maintenance, and surgical procedures. This research introduces I2S (Interact2Sign), a multi stage framework designed for unobtrusive user identification through human object interaction recognition, leveraging 3D hand pose analysis in egocentric videos. I2S utilizes handcrafted features extracted from 3D hand poses and per forms sequential feature augmentation: first identifying the object class, followed by HOI recognition, and ultimately, user identification. A comprehensive feature extraction and description process was carried out for 3D hand poses, organizing the extracted features into semantically meaningful categories: Spatial, Frequency, Kinematic, Orientation, and a novel descriptor introduced in this work, the Inter-Hand Spatial Envelope (IHSE). Extensive ablation studies were conducted to determine the most effective combination of features. The optimal configuration achieved an impressive average F1-score of 97.52% for user identification, evaluated on a bimanual object manipulation dataset derived from the ARCTIC and H2O datasets. I2S demonstrates state-of-the-art performance while maintaining a lightweight model size of under 4 MB and a fast inference time of 0.1 seconds. These characteristics make the proposed framework highly suitable for real-time, on-device authentication in security-critical, AR-based systems.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Barwise Section Boundary Detection in Symbolic Music Using Convolutional Neural Networks</title>
<link>https://arxiv.org/abs/2509.16566</link>
<guid>https://arxiv.org/abs/2509.16566</guid>
<content:encoded><![CDATA[
arXiv:2509.16566v1 Announce Type: cross 
Abstract: Current methods for Music Structure Analysis (MSA) focus primarily on audio data. While symbolic music can be synthesized into audio and analyzed using existing MSA techniques, such an approach does not exploit symbolic music's rich explicit representation of pitch, timing, and instrumentation. A key subproblem of MSA is section boundary detection-determining whether a given point in time marks the transition between musical sections. In this paper, we study automatic section boundary detection for symbolic music. First, we introduce a human-annotated MIDI dataset for section boundary detection, consisting of metadata from 6134 MIDI files that we manually curated from the Lakh MIDI dataset. Second, we train a deep learning model to classify the presence of section boundaries within a fixed-length musical window. Our data representation involves a novel encoding scheme based on synthesized overtones to encode arbitrary MIDI instrumentations into 3-channel piano rolls. Our model achieves an F1 score of 0.77, improving over the analogous audio-based supervised learning approach and the unsupervised block-matching segmentation (CBM) audio approach by 0.22 and 0.31, respectively. We release our dataset, code, and models.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Novel Metric for Detecting Memorization in Generative Models for Brain MRI Synthesis</title>
<link>https://arxiv.org/abs/2509.16582</link>
<guid>https://arxiv.org/abs/2509.16582</guid>
<content:encoded><![CDATA[
arXiv:2509.16582v1 Announce Type: cross 
Abstract: Deep generative models have emerged as a transformative tool in medical imaging, offering substantial potential for synthetic data generation. However, recent empirical studies highlight a critical vulnerability: these models can memorize sensitive training data, posing significant risks of unauthorized patient information disclosure. Detecting memorization in generative models remains particularly challenging, necessitating scalable methods capable of identifying training data leakage across large sets of generated samples. In this work, we propose DeepSSIM, a novel self-supervised metric for quantifying memorization in generative models. DeepSSIM is trained to: i) project images into a learned embedding space and ii) force the cosine similarity between embeddings to match the ground-truth SSIM (Structural Similarity Index) scores computed in the image space. To capture domain-specific anatomical features, training incorporates structure-preserving augmentations, allowing DeepSSIM to estimate similarity reliably without requiring precise spatial alignment. We evaluate DeepSSIM in a case study involving synthetic brain MRI data generated by a Latent Diffusion Model (LDM) trained under memorization-prone conditions, using 2,195 MRI scans from two publicly available datasets (IXI and CoRR). Compared to state-of-the-art memorization metrics, DeepSSIM achieves superior performance, improving F1 scores by an average of +52.03% over the best existing method. Code and data of our approach are publicly available at the following link: https://github.com/brAIn-science/DeepSSIM.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Ego-graph inference for Networked Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.16606</link>
<guid>https://arxiv.org/abs/2509.16606</guid>
<content:encoded><![CDATA[
arXiv:2509.16606v1 Announce Type: cross 
Abstract: In networked multi-agent reinforcement learning (Networked-MARL), decentralized agents must act under local observability and constrained communication over fixed physical graphs. Existing methods often assume static neighborhoods, limiting adaptability to dynamic or heterogeneous environments. While centralized frameworks can learn dynamic graphs, their reliance on global state access and centralized infrastructure is impractical in real-world decentralized systems. We propose a stochastic graph-based policy for Networked-MARL, where each agent conditions its decision on a sampled subgraph over its local physical neighborhood. Building on this formulation, we introduce BayesG, a decentralized actor-framework that learns sparse, context-aware interaction structures via Bayesian variational inference. Each agent operates over an ego-graph and samples a latent communication mask to guide message passing and policy computation. The variational distribution is trained end-to-end alongside the policy using an evidence lower bound (ELBO) objective, enabling agents to jointly learn both interaction topology and decision-making strategies. BayesG outperforms strong MARL baselines on large-scale traffic control tasks with up to 167 agents, demonstrating superior scalability, efficiency, and performance.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ORN-CBF: Learning Observation-conditioned Residual Neural Control Barrier Functions via Hypernetworks</title>
<link>https://arxiv.org/abs/2509.16614</link>
<guid>https://arxiv.org/abs/2509.16614</guid>
<content:encoded><![CDATA[
arXiv:2509.16614v1 Announce Type: cross 
Abstract: Control barrier functions (CBFs) have been demonstrated as an effective method for safety-critical control of autonomous systems. Although CBFs are simple to deploy, their design remains challenging, motivating the development of learning-based approaches. Yet, issues such as suboptimal safe sets, applicability in partially observable environments, and lack of rigorous safety guarantees persist. In this work, we propose observation-conditioned neural CBFs based on Hamilton-Jacobi (HJ) reachability analysis, which approximately recover the maximal safe sets. We exploit certain mathematical properties of the HJ value function, ensuring that the predicted safe set never intersects with the observed failure set. Moreover, we leverage a hypernetwork-based architecture that is particularly suitable for the design of observation-conditioned safety filters. The proposed method is examined both in simulation and hardware experiments for a ground robot and a quadcopter. The results show improved success rates and generalization to out-of-domain environments compared to the baselines.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conditional Multidimensional Scaling with Incomplete Conditioning Data</title>
<link>https://arxiv.org/abs/2509.16627</link>
<guid>https://arxiv.org/abs/2509.16627</guid>
<content:encoded><![CDATA[
arXiv:2509.16627v1 Announce Type: cross 
Abstract: Conditional multidimensional scaling seeks for a low-dimensional configuration from pairwise dissimilarities, in the presence of other known features. By taking advantage of available data of the known features, conditional multidimensional scaling improves the estimation quality of the low-dimensional configuration and simplifies knowledge discovery tasks. However, existing conditional multidimensional scaling methods require full data of the known features, which may not be always attainable due to time, cost, and other constraints. This paper proposes a conditional multidimensional scaling method that can learn the low-dimensional configuration when there are missing values in the known features. The method can also impute the missing values, which provides additional insights of the problem. Computer codes of this method are maintained in the cml R package on CRAN.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FESTA: Functionally Equivalent Sampling for Trust Assessment of Multimodal LLMs</title>
<link>https://arxiv.org/abs/2509.16648</link>
<guid>https://arxiv.org/abs/2509.16648</guid>
<content:encoded><![CDATA[
arXiv:2509.16648v1 Announce Type: cross 
Abstract: The accurate trust assessment of multimodal large language models (MLLMs) generated predictions, which can enable selective prediction and improve user confidence, is challenging due to the diverse multi-modal input paradigms. We propose Functionally Equivalent Sampling for Trust Assessment (FESTA), a multimodal input sampling technique for MLLMs, that generates an uncertainty measure based on the equivalent and complementary input samplings. The proposed task-preserving sampling approach for uncertainty quantification expands the input space to probe the consistency (through equivalent samples) and sensitivity (through complementary samples) of the model. FESTA uses only input-output access of the model (black-box), and does not require ground truth (unsupervised). The experiments are conducted with various off-the-shelf multi-modal LLMs, on both visual and audio reasoning tasks. The proposed FESTA uncertainty estimate achieves significant improvement (33.3% relative improvement for vision-LLMs and 29.6% relative improvement for audio-LLMs) in selective prediction performance, based on area-under-receiver-operating-characteristic curve (AUROC) metric in detecting mispredictions. The code implementation is open-sourced.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safe Guaranteed Dynamics Exploration with Probabilistic Models</title>
<link>https://arxiv.org/abs/2509.16650</link>
<guid>https://arxiv.org/abs/2509.16650</guid>
<content:encoded><![CDATA[
arXiv:2509.16650v1 Announce Type: cross 
Abstract: Ensuring both optimality and safety is critical for the real-world deployment of agents, but becomes particularly challenging when the system dynamics are unknown. To address this problem, we introduce a notion of maximum safe dynamics learning via sufficient exploration in the space of safe policies. We propose a $\textit{pessimistically}$ safe framework that $\textit{optimistically}$ explores informative states and, despite not reaching them due to model uncertainty, ensures continuous online learning of dynamics. The framework achieves first-of-its-kind results: learning the dynamics model sufficiently $-$ up to an arbitrary small tolerance (subject to noise) $-$ in a finite time, while ensuring provably safe operation throughout with high probability and without requiring resets. Building on this, we propose an algorithm to maximize rewards while learning the dynamics $\textit{only to the extent needed}$ to achieve close-to-optimal performance. Unlike typical reinforcement learning (RL) methods, our approach operates online in a non-episodic setting and ensures safety throughout the learning process. We demonstrate the effectiveness of our approach in challenging domains such as autonomous car racing and drone navigation under aerodynamic effects $-$ scenarios where safety is critical and accurate modeling is difficult.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the de-duplication of the Lakh MIDI dataset</title>
<link>https://arxiv.org/abs/2509.16662</link>
<guid>https://arxiv.org/abs/2509.16662</guid>
<content:encoded><![CDATA[
arXiv:2509.16662v1 Announce Type: cross 
Abstract: A large-scale dataset is essential for training a well-generalized deep-learning model. Most such datasets are collected via scraping from various internet sources, inevitably introducing duplicated data. In the symbolic music domain, these duplicates often come from multiple user arrangements and metadata changes after simple editing. However, despite critical issues such as unreliable training evaluation from data leakage during random splitting, dataset duplication has not been extensively addressed in the MIR community. This study investigates the dataset duplication issues regarding Lakh MIDI Dataset (LMD), one of the largest publicly available sources in the symbolic music domain. To find and evaluate the best retrieval method for duplicated data, we employed the Clean MIDI subset of the LMD as a benchmark test set, in which different versions of the same songs are grouped together. We first evaluated rule-based approaches and previous symbolic music retrieval models for de-duplication and also investigated with a contrastive learning-based BERT model with various augmentations to find duplicate files. As a result, we propose three different versions of the filtered list of LMD, which filters out at least 38,134 samples in the most conservative settings among 178,561 files.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>System-Level Uncertainty Quantification with Multiple Machine Learning Models: A Theoretical Framework</title>
<link>https://arxiv.org/abs/2509.16663</link>
<guid>https://arxiv.org/abs/2509.16663</guid>
<content:encoded><![CDATA[
arXiv:2509.16663v1 Announce Type: cross 
Abstract: ML models have errors when used for predictions. The errors are unknown but can be quantified by model uncertainty. When multiple ML models are trained using the same training points, their model uncertainties may be statistically dependent. In reality, model inputs are also random with input uncertainty. The effects of these types of uncertainty must be considered in decision-making and design. This study develops a theoretical framework that generates the joint distribution of multiple ML predictions given the joint distribution of model uncertainties and the joint distribution of model inputs. The strategy is to decouple the coupling between the two types of uncertainty and transform them as independent random variables. The framework lays a foundation for numerical algorithm development for various specific applications.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Segment-to-Act: Label-Noise-Robust Action-Prompted Video Segmentation Towards Embodied Intelligence</title>
<link>https://arxiv.org/abs/2509.16677</link>
<guid>https://arxiv.org/abs/2509.16677</guid>
<content:encoded><![CDATA[
arXiv:2509.16677v1 Announce Type: cross 
Abstract: Embodied intelligence relies on accurately segmenting objects actively involved in interactions. Action-based video object segmentation addresses this by linking segmentation with action semantics, but it depends on large-scale annotations and prompts that are costly, inconsistent, and prone to multimodal noise such as imprecise masks and referential ambiguity. To date, this challenge remains unexplored. In this work, we take the first step by studying action-based video object segmentation under label noise, focusing on two sources: textual prompt noise (category flips and within-category noun substitutions) and mask annotation noise (perturbed object boundaries to mimic imprecise supervision). Our contributions are threefold. First, we introduce two types of label noises for the action-based video object segmentation task. Second, we build up the first action-based video object segmentation under a label noise benchmark ActiSeg-NL and adapt six label-noise learning strategies to this setting, and establish protocols for evaluating them under textual, boundary, and mixed noise. Third, we provide a comprehensive analysis linking noise types to failure modes and robustness gains, and we introduce a Parallel Mask Head Mechanism (PMHM) to address mask annotation noise. Qualitative evaluations further reveal characteristic failure modes, including boundary leakage and mislocalization under boundary perturbations, as well as occasional identity substitutions under textual flips. Our comparative analysis reveals that different learning strategies exhibit distinct robustness profiles, governed by a foreground-background trade-off where some achieve balanced performance while others prioritize foreground accuracy at the cost of background precision. The established benchmark and source code will be made publicly available at https://github.com/mylwx/ActiSeg-NL.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProtoVQA: An Adaptable Prototypical Framework for Explainable Fine-Grained Visual Question Answering</title>
<link>https://arxiv.org/abs/2509.16680</link>
<guid>https://arxiv.org/abs/2509.16680</guid>
<content:encoded><![CDATA[
arXiv:2509.16680v1 Announce Type: cross 
Abstract: Visual Question Answering (VQA) is increasingly used in diverse applications ranging from general visual reasoning to safety-critical domains such as medical imaging and autonomous systems, where models must provide not only accurate answers but also explanations that humans can easily understand and verify. Prototype-based modeling has shown promise for interpretability by grounding predictions in semantically meaningful regions for purely visual reasoning tasks, yet remains underexplored in the context of VQA. We present ProtoVQA, a unified prototypical framework that (i) learns question-aware prototypes that serve as reasoning anchors, connecting answers to discriminative image regions, (ii) applies spatially constrained matching to ensure that the selected evidence is coherent and semantically relevant, and (iii) supports both answering and grounding tasks through a shared prototype backbone. To assess explanation quality, we propose the Visual-Linguistic Alignment Score (VLAS), which measures how well the model's attended regions align with ground-truth evidence. Experiments on Visual7W show that ProtoVQA yields faithful, fine-grained explanations while maintaining competitive accuracy, advancing the development of transparent and trustworthy VQA systems.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Transparent and Interpretable AI Model for Medical Image Classifications</title>
<link>https://arxiv.org/abs/2509.16685</link>
<guid>https://arxiv.org/abs/2509.16685</guid>
<content:encoded><![CDATA[
arXiv:2509.16685v1 Announce Type: cross 
Abstract: The integration of artificial intelligence (AI) into medicine is remarkable, offering advanced diagnostic and therapeutic possibilities. However, the inherent opacity of complex AI models presents significant challenges to their clinical practicality. This paper focuses primarily on investigating the application of explainable artificial intelligence (XAI) methods, with the aim of making AI decisions transparent and interpretable. Our research focuses on implementing simulations using various medical datasets to elucidate the internal workings of the XAI model. These dataset-driven simulations demonstrate how XAI effectively interprets AI predictions, thus improving the decision-making process for healthcare professionals. In addition to a survey of the main XAI methods and simulations, ongoing challenges in the XAI field are discussed. The study highlights the need for the continuous development and exploration of XAI, particularly from the perspective of diverse medical datasets, to promote its adoption and effectiveness in the healthcare domain.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoding Uncertainty: The Impact of Decoding Strategies for Uncertainty Estimation in Large Language Models</title>
<link>https://arxiv.org/abs/2509.16696</link>
<guid>https://arxiv.org/abs/2509.16696</guid>
<content:encoded><![CDATA[
arXiv:2509.16696v1 Announce Type: cross 
Abstract: Decoding strategies manipulate the probability distribution underlying the output of a language model and can therefore affect both generation quality and its uncertainty. In this study, we investigate the impact of decoding strategies on uncertainty estimation in Large Language Models (LLMs). Our experiments show that Contrastive Search, which mitigates repetition, yields better uncertainty estimates on average across a range of preference-aligned LLMs. In contrast, the benefits of these strategies sometimes diverge when the model is only post-trained with supervised fine-tuning, i.e. without explicit alignment.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Distillation for Variational Quantum Convolutional Neural Networks on Heterogeneous Data</title>
<link>https://arxiv.org/abs/2509.16699</link>
<guid>https://arxiv.org/abs/2509.16699</guid>
<content:encoded><![CDATA[
arXiv:2509.16699v1 Announce Type: cross 
Abstract: Distributed quantum machine learning faces significant challenges due to heterogeneous client data and variations in local model structures, which hinder global model aggregation. To address these challenges, we propose a knowledge distillation framework for variational quantum convolutional neural networks on heterogeneous data. The framework features a quantum gate number estimation mechanism based on client data, which guides the construction of resource-adaptive VQCNN circuits. Particle swarm optimization is employed to efficiently generate personalized quantum models tailored to local data characteristics. During aggregation, a knowledge distillation strategy integrating both soft-label and hard-label supervision consolidates knowledge from heterogeneous clients using a public dataset, forming a global model while avoiding parameter exposure and privacy leakage. Theoretical analysis shows that proposed framework benefits from quantum high-dimensional representation, offering advantages over classical approaches, and minimizes communication by exchanging only model indices and test outputs. Extensive simulations on the PennyLane platform validate the effectiveness of the gate number estimation and distillation-based aggregation. Experimental results demonstrate that the aggregated global model achieves accuracy close to fully supervised centralized training. These results shown that proposed methods can effectively handle heterogeneity, reduce resource consumption, and maintain performance, highlighting its potential for scalable and privacy-preserving distributed quantum learning.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Increase Alpha: Performance and Risk of an AI-Driven Trading Framework</title>
<link>https://arxiv.org/abs/2509.16707</link>
<guid>https://arxiv.org/abs/2509.16707</guid>
<content:encoded><![CDATA[
arXiv:2509.16707v1 Announce Type: cross 
Abstract: There are inefficiencies in financial markets, with unexploited patterns in price, volume, and cross-sectional relationships. While many approaches use large-scale transformers, we take a domain-focused path: feed-forward and recurrent networks with curated features to capture subtle regularities in noisy financial data. This smaller-footprint design is computationally lean and reliable under low signal-to-noise, crucial for daily production at scale. At Increase Alpha, we built a deep-learning framework that maps over 800 U.S. equities into daily directional signals with minimal computational overhead.
  The purpose of this paper is twofold. First, we outline the general overview of the predictive model without disclosing its core underlying concepts. Second, we evaluate its real-time performance through transparent, industry standard metrics. Forecast accuracy is benchmarked against both naive baselines and macro indicators. The performance outcomes are summarized via cumulative returns, annualized Sharpe ratio, and maximum drawdown. The best portfolio combination using our signals provides a low-risk, continuous stream of returns with a Sharpe ratio of more than 2.5, maximum drawdown of around 3\%, and a near-zero correlation with the S\&amp;P 500 market benchmark. We also compare the model's performance through different market regimes, such as the recent volatile movements of the US equity market in the beginning of 2025. Our analysis showcases the robustness of the model and significantly stable performance during these volatile periods.
  Collectively, these findings show that market inefficiencies can be systematically harvested with modest computational overhead if the right variables are considered. This report will emphasize the potential of traditional deep learning frameworks for generating an AI-driven edge in the financial market.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QASTAnet: A DNN-based Quality Metric for Spatial Audio</title>
<link>https://arxiv.org/abs/2509.16715</link>
<guid>https://arxiv.org/abs/2509.16715</guid>
<content:encoded><![CDATA[
arXiv:2509.16715v1 Announce Type: cross 
Abstract: In the development of spatial audio technologies, reliable and shared methods for evaluating audio quality are essential. Listening tests are currently the standard but remain costly in terms of time and resources. Several models predicting subjective scores have been proposed, but they do not generalize well to real-world signals. In this paper, we propose QASTAnet (Quality Assessment for SpaTial Audio network), a new metric based on a deep neural network, specialized on spatial audio (ambisonics and binaural). As training data is scarce, we aim for the model to be trainable with a small amount of data. To do so, we propose to rely on expert modeling of the low-level auditory system and use a neurnal network to model the high-level cognitive function of the quality judgement. We compare its performance to two reference metrics on a wide range of content types (speech, music, ambiance, anechoic, reverberated) and focusing on codec artifacts. Results demonstrate that QASTAnet overcomes the aforementioned limitations of the existing methods. The strong correlation between the proposed metric prediction and subjective scores makes it a good candidate for comparing codecs in their development.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pain in 3D: Generating Controllable Synthetic Faces for Automated Pain Assessment</title>
<link>https://arxiv.org/abs/2509.16727</link>
<guid>https://arxiv.org/abs/2509.16727</guid>
<content:encoded><![CDATA[
arXiv:2509.16727v1 Announce Type: cross 
Abstract: Automated pain assessment from facial expressions is crucial for non-communicative patients, such as those with dementia. Progress has been limited by two challenges: (i) existing datasets exhibit severe demographic and label imbalance due to ethical constraints, and (ii) current generative models cannot precisely control facial action units (AUs), facial structure, or clinically validated pain levels.
  We present 3DPain, a large-scale synthetic dataset specifically designed for automated pain assessment, featuring unprecedented annotation richness and demographic diversity. Our three-stage framework generates diverse 3D meshes, textures them with diffusion models, and applies AU-driven face rigging to synthesize multi-view faces with paired neutral and pain images, AU configurations, PSPI scores, and the first dataset-level annotations of pain-region heatmaps. The dataset comprises 82,500 samples across 25,000 pain expression heatmaps and 2,500 synthetic identities balanced by age, gender, and ethnicity.
  We further introduce ViTPain, a Vision Transformer based cross-modal distillation framework in which a heatmap-trained teacher guides a student trained on RGB images, enhancing accuracy, interpretability, and clinical reliability. Together, 3DPain and ViTPain establish a controllable, diverse, and clinically grounded foundation for generalizable automated pain assessment.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Angular Dispersion Accelerates $k$-Nearest Neighbors Machine Translation</title>
<link>https://arxiv.org/abs/2509.16729</link>
<guid>https://arxiv.org/abs/2509.16729</guid>
<content:encoded><![CDATA[
arXiv:2509.16729v1 Announce Type: cross 
Abstract: Augmenting neural machine translation with external memory at decoding time, in the form of k-nearest neighbors machine translation ($k$-NN MT), is a well-established strategy for increasing translation performance. $k$-NN MT retrieves a set of tokens that occurred in the most similar contexts recorded in a prepared data store, using hidden state representations of translation contexts as vector lookup keys. One of the main disadvantages of this method is the high computational cost and memory requirements. Since an exhaustive search is not feasible in large data stores, practitioners commonly use approximate $k$-NN MT lookup, yet even such algorithms are a bottleneck. In contrast to research directions seeking to accelerate $k$-NN MT by reducing data store size or the number of lookup calls, we pursue an orthogonal direction based on the performance properties of approximate $k$-NN MT lookup data structures. In particular, we propose to encourage angular dispersion of the neural hidden representations of contexts. We show that improving dispersion leads to better balance in the retrieval data structures, accelerating retrieval and slightly improving translations.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Min: Mixture of Noise for Pre-Trained Model-Based Class-Incremental Learning</title>
<link>https://arxiv.org/abs/2509.16738</link>
<guid>https://arxiv.org/abs/2509.16738</guid>
<content:encoded><![CDATA[
arXiv:2509.16738v1 Announce Type: cross 
Abstract: Class Incremental Learning (CIL) aims to continuously learn new categories while retaining the knowledge of old ones. Pre-trained models (PTMs) show promising capabilities in CIL. However, existing approaches that apply lightweight fine-tuning to backbones still induce parameter drift, thereby compromising the generalization capability of pre-trained models. Parameter drift can be conceptualized as a form of noise that obscures critical patterns learned for previous tasks. However, recent researches have shown that noise is not always harmful. For example, the large number of visual patterns learned from pre-training can be easily abused by a single task, and introducing appropriate noise can suppress some low-correlation features, thus leaving a margin for future tasks. To this end, we propose learning beneficial noise for CIL guided by information theory and propose Mixture of Noise (Min), aiming to mitigate the degradation of backbone generalization from adapting new tasks. Specifically, task-specific noise is learned from high-dimension features of new tasks. Then, a set of weights is adjusted dynamically for optimal mixture of different task noise. Finally, Min embeds the beneficial noise into the intermediate features to mask the response of inefficient patterns. Extensive experiments on six benchmark datasets demonstrate that Min achieves state-of-the-art performance in most incremental settings, with particularly outstanding results in 50-steps incremental settings. This shows the significant potential for beneficial noise in continual learning.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the System Theoretic Offline Learning of Continuous-Time LQR with Exogenous Disturbances</title>
<link>https://arxiv.org/abs/2509.16746</link>
<guid>https://arxiv.org/abs/2509.16746</guid>
<content:encoded><![CDATA[
arXiv:2509.16746v1 Announce Type: cross 
Abstract: We analyze offline designs of linear quadratic regulator (LQR) strategies with uncertain disturbances. First, we consider the scenario where the exogenous variable can be estimated in a controlled environment, and subsequently, consider a more practical and challenging scenario where it is unknown in a stochastic setting. Our approach builds on the fundamental learning-based framework of adaptive dynamic programming (ADP), combined with a Lyapunov-based analytical methodology to design the algorithms and derive sample-based approximations motivated from the Markov decision process (MDP)-based approaches. For the scenario involving non-measurable disturbances, we further establish stability and convergence guarantees for the learned control gains under sample-based approximations. The overall methodology emphasizes simplicity while providing rigorous guarantees. Finally, numerical experiments focus on the intricacies and validations for the design of offline continuous-time LQR with exogenous disturbances.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving User Interface Generation Models from Designer Feedback</title>
<link>https://arxiv.org/abs/2509.16779</link>
<guid>https://arxiv.org/abs/2509.16779</guid>
<content:encoded><![CDATA[
arXiv:2509.16779v1 Announce Type: cross 
Abstract: Despite being trained on vast amounts of data, most LLMs are unable to reliably generate well-designed UIs. Designer feedback is essential to improving performance on UI generation; however, we find that existing RLHF methods based on ratings or rankings are not well-aligned with designers' workflows and ignore the rich rationale used to critique and improve UI designs. In this paper, we investigate several approaches for designers to give feedback to UI generation models, using familiar interactions such as commenting, sketching and direct manipulation. We first perform a study with 21 designers where they gave feedback using these interactions, which resulted in ~1500 design annotations. We then use this data to finetune a series of LLMs to generate higher quality UIs. Finally, we evaluate these models with human judges, and we find that our designer-aligned approaches outperform models trained with traditional ranking feedback and all tested baselines, including GPT-5.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectral Analysis of the Weighted Frobenius Objective</title>
<link>https://arxiv.org/abs/2509.16783</link>
<guid>https://arxiv.org/abs/2509.16783</guid>
<content:encoded><![CDATA[
arXiv:2509.16783v1 Announce Type: cross 
Abstract: We analyze a weighted Frobenius loss for approximating symmetric positive definite matrices in the context of preconditioning iterative solvers. Unlike the standard Frobenius norm, the weighted loss penalizes error components associated with small eigenvalues of the system matrix more strongly. Our analysis reveals that each eigenmode is scaled by the corresponding square of its eigenvalue, and that, under a fixed error budget, the loss is minimized only when the error is confined to the direction of the largest eigenvalue. This provides a rigorous explanation of why minimizing the weighted loss naturally suppresses low-frequency components, which can be a desirable strategy for the conjugate gradient method. The analysis is independent of the specific approximation scheme or sparsity pattern, and applies equally to incomplete factorizations, algebraic updates, and learning-based constructions. Numerical experiments confirm the predictions of the theory, including an illustration where sparse factors are trained by a direct gradient updates to IC(0) factor entries, i.e., no trained neural network model is used.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain-Adaptive Pre-Training for Arabic Aspect-Based Sentiment Analysis: A Comparative Study of Domain Adaptation and Fine-Tuning Strategies</title>
<link>https://arxiv.org/abs/2509.16788</link>
<guid>https://arxiv.org/abs/2509.16788</guid>
<content:encoded><![CDATA[
arXiv:2509.16788v1 Announce Type: cross 
Abstract: Aspect-based sentiment analysis (ABSA) in natural language processing enables organizations to understand customer opinions on specific product aspects. While deep learning models are widely used for English ABSA, their application in Arabic is limited due to the scarcity of labeled data. Researchers have attempted to tackle this issue by using pre-trained contextualized language models such as BERT. However, these models are often based on fact-based data, which can introduce bias in domain-specific tasks like ABSA. To our knowledge, no studies have applied adaptive pre-training with Arabic contextualized models for ABSA. This research proposes a novel approach using domain-adaptive pre-training for aspect-sentiment classification (ASC) and opinion target expression (OTE) extraction. We examine fine-tuning strategies - feature extraction, full fine-tuning, and adapter-based methods - to enhance performance and efficiency, utilizing multiple adaptation corpora and contextualized models. Our results show that in-domain adaptive pre-training yields modest improvements. Adapter-based fine-tuning is a computationally efficient method that achieves competitive results. However, error analyses reveal issues with model predictions and dataset labeling. In ASC, common problems include incorrect sentiment labeling, misinterpretation of contrastive markers, positivity bias for early terms, and challenges with conflicting opinions and subword tokenization. For OTE, issues involve mislabeling targets, confusion over syntactic roles, difficulty with multi-word expressions, and reliance on shallow heuristics. These findings underscore the need for syntax- and semantics-aware models, such as graph convolutional networks, to more effectively capture long-distance relations and complex aspect-based opinion alignments.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Study on Stabilizer R\'enyi Entropy Estimation using Machine Learning</title>
<link>https://arxiv.org/abs/2509.16799</link>
<guid>https://arxiv.org/abs/2509.16799</guid>
<content:encoded><![CDATA[
arXiv:2509.16799v1 Announce Type: cross 
Abstract: Nonstabilizerness is a fundamental resource for quantum advantage, as it quantifies the extent to which a quantum state diverges from those states that can be efficiently simulated on a classical computer, the stabilizer states. The stabilizer R\'enyi entropy (SRE) is one of the most investigated measures of nonstabilizerness because of its computational properties and suitability for experimental measurements on quantum processors. Because computing the SRE for arbitrary quantum states is a computationally hard problem, we propose a supervised machine-learning approach to estimate it. In this work, we frame SRE estimation as a regression task and train a Random Forest Regressor and a Support Vector Regressor (SVR) on a comprehensive dataset, including both unstructured random quantum circuits and structured circuits derived from the physics-motivated one-dimensional transverse Ising model (TIM). We compare the machine-learning models using two different quantum circuit representations: one based on classical shadows and the other on circuit-level features. Furthermore, we assess the generalization capabilities of the models on out-of-distribution instances. Experimental results show that an SVR trained on circuit-level features achieves the best overall performance. On the random circuits dataset, our approach converges to accurate SRE estimations, but struggles to generalize out of distribution. In contrast, it generalizes well on the structured TIM dataset, even to deeper and larger circuits. In line with previous work, our experiments suggest that machine learning offers a viable path for efficient nonstabilizerness estimation.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sublinear Time Quantum Sensitivity Sampling</title>
<link>https://arxiv.org/abs/2509.16801</link>
<guid>https://arxiv.org/abs/2509.16801</guid>
<content:encoded><![CDATA[
arXiv:2509.16801v1 Announce Type: cross 
Abstract: We present a unified framework for quantum sensitivity sampling, extending the advantages of quantum computing to a broad class of classical approximation problems. Our unified framework provides a streamlined approach for constructing coresets and offers significant runtime improvements in applications such as clustering, regression, and low-rank approximation. Our contributions include:
  * $k$-median and $k$-means clustering: For $n$ points in $d$-dimensional Euclidean space, we give an algorithm that constructs an $\epsilon$-coreset in time $\widetilde O(n^{0.5}dk^{2.5}~\mathrm{poly}(\epsilon^{-1}))$ for $k$-median and $k$-means clustering. Our approach achieves a better dependence on $d$ and constructs smaller coresets that only consist of points in the dataset, compared to recent results of [Xue, Chen, Li and Jiang, ICML'23].
  * $\ell_p$ regression: For $\ell_p$ regression problems, we construct an $\epsilon$-coreset of size $\widetilde O_p(d^{\max\{1, p/2\}}\epsilon^{-2})$ in time $\widetilde O_p(n^{0.5}d^{\max\{0.5, p/4\}+1}(\epsilon^{-3}+d^{0.5}))$, improving upon the prior best quantum sampling approach of [Apers and Gribling, QIP'24] for all $p\in (0, 2)\cup (2, 22]$, including the widely studied least absolute deviation regression ($\ell_1$ regression).
  * Low-rank approximation with Frobenius norm error: We introduce the first quantum sublinear-time algorithm for low-rank approximation that does not rely on data-dependent parameters, and runs in $\widetilde O(nd^{0.5}k^{0.5}\epsilon^{-1})$ time. Additionally, we present quantum sublinear algorithms for kernel low-rank approximation and tensor low-rank approximation, broadening the range of achievable sublinear time algorithms in randomized numerical linear algebra.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Randomized Space-Time Sampling for Affine Graph Dynamical Systems</title>
<link>https://arxiv.org/abs/2509.16818</link>
<guid>https://arxiv.org/abs/2509.16818</guid>
<content:encoded><![CDATA[
arXiv:2509.16818v1 Announce Type: cross 
Abstract: This paper investigates the problem of dynamical sampling for graph signals influenced by a constant source term. We consider signals evolving over time according to a linear dynamical system on a graph, where both the initial state and the source term are bandlimited. We introduce two random space-time sampling regimes and analyze the conditions under which stable recovery is achievable. While our framework extends recent work on homogeneous dynamics, it addresses a fundamentally different setting where the evolution includes a constant source term. This results in a non-orthogonal-diagonalizable system matrix, rendering classical spectral techniques inapplicable and introducing new challenges in sampling design, stability analysis, and joint recovery of both the initial state and the forcing term. A key component of our analysis is the spectral graph weighted coherence, which characterizes the interplay between the sampling distribution and the graph structure. We establish sampling complexity bounds ensuring stable recovery via the Restricted Isometry Property (RIP), and develop a robust recovery algorithm with provable error guarantees. The effectiveness of our method is validated through extensive experiments on both synthetic and real-world datasets.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robot Learning with Sparsity and Scarcity</title>
<link>https://arxiv.org/abs/2509.16834</link>
<guid>https://arxiv.org/abs/2509.16834</guid>
<content:encoded><![CDATA[
arXiv:2509.16834v1 Announce Type: cross 
Abstract: Unlike in language or vision, one of the fundamental challenges in robot learning is the lack of access to vast data resources. We can further break down the problem into (1) data sparsity from the angle of data representation and (2) data scarcity from the angle of data quantity. In this thesis, I will discuss selected works on two domains: (1) tactile sensing and (2) rehabilitation robots, which are exemplars of data sparsity and scarcity, respectively. Tactile sensing is an essential modality for robotics, but tactile data are often sparse, and for each interaction with the physical world, tactile sensors can only obtain information about the local area of contact. I will discuss my work on learning vision-free tactile-only exploration and manipulation policies through model-free reinforcement learning to make efficient use of sparse tactile information. On the other hand, rehabilitation robots are an example of data scarcity to the extreme due to the significant challenge of collecting biosignals from disabled-bodied subjects at scale for training. I will discuss my work in collaboration with the medical school and clinicians on intent inferral for stroke survivors, where a hand orthosis developed in our lab collects a set of biosignals from the patient and uses them to infer the activity that the patient intends to perform, so the orthosis can provide the right type of physical assistance at the right moment. My work develops machine learning algorithms that enable intent inferral with minimal data, including semi-supervised, meta-learning, and generative AI methods.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DoubleGen: Debiased Generative Modeling of Counterfactuals</title>
<link>https://arxiv.org/abs/2509.16842</link>
<guid>https://arxiv.org/abs/2509.16842</guid>
<content:encoded><![CDATA[
arXiv:2509.16842v1 Announce Type: cross 
Abstract: Generative models for counterfactual outcomes face two key sources of bias. Confounding bias arises when approaches fail to account for systematic differences between those who receive the intervention and those who do not. Misspecification bias arises when methods attempt to address confounding through estimation of an auxiliary model, but specify it incorrectly. We introduce DoubleGen, a doubly robust framework that modifies generative modeling training objectives to mitigate these biases. The new objectives rely on two auxiliaries -- a propensity and outcome model -- and successfully address confounding bias even if only one of them is correct. We provide finite-sample guarantees for this robustness property. We further establish conditions under which DoubleGen achieves oracle optimality -- matching the convergence rates standard approaches would enjoy if interventional data were available -- and minimax rate optimality. We illustrate DoubleGen with three examples: diffusion models, flow matching, and autoregressive language models.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ShadowServe: Interference-Free KV Cache Fetching for Distributed Prefix Caching</title>
<link>https://arxiv.org/abs/2509.16857</link>
<guid>https://arxiv.org/abs/2509.16857</guid>
<content:encoded><![CDATA[
arXiv:2509.16857v1 Announce Type: cross 
Abstract: Distributed prefix caching accelerates long-context LLM serving by reusing KV cache entries for common context prefixes. However, KV cache fetches can become a bottleneck when network bandwidth is limited. Compression mitigates the bandwidth issue, but can degrade overall performance when decompression interferes with model computation.
  We present ShadowServe, the first SmartNIC-accelerated, interference-free prefix caching system for LLM serving. ShadowServe separates a control plane on the host and a data plane fully offloaded to the SmartNIC, which eliminates interference to both host GPU and CPU. To overcome the SmartNIC's limited compute and memory resources, we design a chunked pipeline that parallelizes data plane operations across the SmartNIC's compute resources, and a minimal-copy memory management scheme that reduces memory pressure on the SmartNIC. Compared to state-of-the-art solutions, ShadowServe achieves up to 2.2x lower loaded time-per-output-token (TPOT), and reduces time-to-first-token (TTFT) by up to 1.38x in low-bandwidth scenarios (<= 20 Gbps), translating to up to 1.35x higher throughput.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>seqBench: A Tunable Benchmark to Quantify Sequential Reasoning Limits of LLMs</title>
<link>https://arxiv.org/abs/2509.16866</link>
<guid>https://arxiv.org/abs/2509.16866</guid>
<content:encoded><![CDATA[
arXiv:2509.16866v1 Announce Type: cross 
Abstract: We introduce seqBench, a parametrized benchmark for probing sequential reasoning limits in Large Language Models (LLMs) through precise, multi-dimensional control over several key complexity dimensions. seqBench allows systematic variation of (1) the logical depth, defined as the number of sequential actions required to solve the task; (2) the number of backtracking steps along the optimal path, quantifying how often the agent must revisit prior states to satisfy deferred preconditions (e.g., retrieving a key after encountering a locked door); and (3) the noise ratio, defined as the ratio between supporting and distracting facts about the environment. Our evaluations on state-of-the-art LLMs reveal a universal failure pattern: accuracy collapses exponentially beyond a model-specific logical depth. Unlike existing benchmarks, seqBench's fine-grained control facilitates targeted analyses of these reasoning failures, illuminating universal scaling laws and statistical limits, as detailed in this paper alongside its generation methodology and evaluation metrics. We find that even top-performing models systematically fail on seqBench's structured reasoning tasks despite minimal search complexity, underscoring key limitations in their commonsense reasoning capabilities. Designed for future evolution to keep pace with advancing models, the seqBench datasets are publicly released to spur deeper scientific inquiry into LLM reasoning, aiming to establish a clearer understanding of their true potential and current boundaries for robust real-world application.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PhysHDR: When Lighting Meets Materials and Scene Geometry in HDR Reconstruction</title>
<link>https://arxiv.org/abs/2509.16869</link>
<guid>https://arxiv.org/abs/2509.16869</guid>
<content:encoded><![CDATA[
arXiv:2509.16869v1 Announce Type: cross 
Abstract: Low Dynamic Range (LDR) to High Dynamic Range (HDR) image translation is a fundamental task in many computational vision problems. Numerous data-driven methods have been proposed to address this problem; however, they lack explicit modeling of illumination, lighting, and scene geometry in images. This limits the quality of the reconstructed HDR images. Since lighting and shadows interact differently with different materials, (e.g., specular surfaces such as glass and metal, and lambertian or diffuse surfaces such as wood and stone), modeling material-specific properties (e.g., specular and diffuse reflectance) has the potential to improve the quality of HDR image reconstruction. This paper presents PhysHDR, a simple yet powerful latent diffusion-based generative model for HDR image reconstruction. The denoising process is conditioned on lighting and depth information and guided by a novel loss to incorporate material properties of surfaces in the scene. The experimental results establish the efficacy of PhysHDR in comparison to a number of recent state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differential Privacy for Euclidean Jordan Algebra with Applications to Private Symmetric Cone Programming</title>
<link>https://arxiv.org/abs/2509.16915</link>
<guid>https://arxiv.org/abs/2509.16915</guid>
<content:encoded><![CDATA[
arXiv:2509.16915v1 Announce Type: cross 
Abstract: In this paper, we study differentially private mechanisms for functions whose outputs lie in a Euclidean Jordan algebra. Euclidean Jordan algebras capture many important mathematical structures and form the foundation of linear programming, second-order cone programming, and semidefinite programming. Our main contribution is a generic Gaussian mechanism for such functions, with sensitivity measured in $\ell_2$, $\ell_1$, and $\ell_\infty$ norms. Notably, this framework includes the important case where the function outputs are symmetric matrices, and sensitivity is measured in the Frobenius, nuclear, or spectral norm. We further derive private algorithms for solving symmetric cone programs under various settings, using a combination of the multiplicative weights update method and our generic Gaussian mechanism. As an application, we present differentially private algorithms for semidefinite programming, resolving a major open question posed by [Hsu, Roth, Roughgarden, and Ullman, ICALP 2014].
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Attention with Confidence Weighting for Multi-Channel Audio Alignment</title>
<link>https://arxiv.org/abs/2509.16926</link>
<guid>https://arxiv.org/abs/2509.16926</guid>
<content:encoded><![CDATA[
arXiv:2509.16926v1 Announce Type: cross 
Abstract: Multi-channel audio alignment is a key requirement in bioacoustic monitoring, spatial audio systems, and acoustic localization. However, existing methods often struggle to address nonlinear clock drift and lack mechanisms for quantifying uncertainty. Traditional methods like Cross-correlation and Dynamic Time Warping assume simple drift patterns and provide no reliability measures. Meanwhile, recent deep learning models typically treat alignment as a binary classification task, overlooking inter-channel dependencies and uncertainty estimation. We introduce a method that combines cross-attention mechanisms with confidence-weighted scoring to improve multi-channel audio synchronization. We extend BEATs encoders with cross-attention layers to model temporal relationships between channels. We also develop a confidence-weighted scoring function that uses the full prediction distribution instead of binary thresholding. Our method achieved first place in the BioDCASE 2025 Task 1 challenge with 0.30 MSE average across test datasets, compared to 0.58 for the deep learning baseline. On individual datasets, we achieved 0.14 MSE on ARU data (77% reduction) and 0.45 MSE on zebra finch data (18% reduction). The framework supports probabilistic temporal alignment, moving beyond point estimates. While validated in a bioacoustic context, the approach is applicable to a broader range of multi-channel audio tasks where alignment confidence is critical. Code available on: https://github.com/Ragib-Amin-Nihal/BEATsCA
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equip Pre-ranking with Target Attention by Residual Quantization</title>
<link>https://arxiv.org/abs/2509.16931</link>
<guid>https://arxiv.org/abs/2509.16931</guid>
<content:encoded><![CDATA[
arXiv:2509.16931v1 Announce Type: cross 
Abstract: The pre-ranking stage in industrial recommendation systems faces a fundamental conflict between efficiency and effectiveness. While powerful models like Target Attention (TA) excel at capturing complex feature interactions in the ranking stage, their high computational cost makes them infeasible for pre-ranking, which often relies on simplistic vector-product models. This disparity creates a significant performance bottleneck for the entire system. To bridge this gap, we propose TARQ, a novel pre-ranking framework. Inspired by generative models, TARQ's key innovation is to equip pre-ranking with an architecture approximate to TA by Residual Quantization. This allows us to bring the modeling power of TA into the latency-critical pre-ranking stage for the first time, establishing a new state-of-the-art trade-off between accuracy and efficiency. Extensive offline experiments and large-scale online A/B tests at Taobao demonstrate TARQ's significant improvements in ranking performance. Consequently, our model has been fully deployed in production, serving tens of millions of daily active users and yielding substantial business improvements.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuFACO: Neural Focused Ant Colony Optimization for Traveling Salesman Problem</title>
<link>https://arxiv.org/abs/2509.16938</link>
<guid>https://arxiv.org/abs/2509.16938</guid>
<content:encoded><![CDATA[
arXiv:2509.16938v1 Announce Type: cross 
Abstract: This study presents Neural Focused Ant Colony Optimization (NeuFACO), a non-autoregressive framework for the Traveling Salesman Problem (TSP) that combines advanced reinforcement learning with enhanced Ant Colony Optimization (ACO). NeuFACO employs Proximal Policy Optimization (PPO) with entropy regularization to train a graph neural network for instance-specific heuristic guidance, which is integrated into an optimized ACO framework featuring candidate lists, restricted tour refinement, and scalable local search. By leveraging amortized inference alongside ACO stochastic exploration, NeuFACO efficiently produces high-quality solutions across diverse TSP instances.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Adaptive Self-Attention for Financial Rebalancing: An Empirical Study on Automated Market Makers in Decentralized Finance</title>
<link>https://arxiv.org/abs/2509.16955</link>
<guid>https://arxiv.org/abs/2509.16955</guid>
<content:encoded><![CDATA[
arXiv:2509.16955v1 Announce Type: cross 
Abstract: We formulate automated market maker (AMM) \emph{rebalancing} as a binary detection problem and study a hybrid quantum--classical self-attention block, \textbf{Quantum Adaptive Self-Attention (QASA)}. QASA constructs quantum queries/keys/values via variational quantum circuits (VQCs) and applies standard softmax attention over Pauli-$Z$ expectation vectors, yielding a drop-in attention module for financial time-series decision making. Using daily data for \textbf{BTCUSDC} over \textbf{Jan-2024--Jan-2025} with a 70/15/15 time-series split, we compare QASA against classical ensembles, a transformer, and pure quantum baselines under Return, Sharpe, and Max Drawdown. The \textbf{QASA-Sequence} variant attains the \emph{best single-model risk-adjusted performance} (\textbf{13.99\%} return; \textbf{Sharpe 1.76}), while hybrid models average \textbf{11.2\%} return (vs.\ 9.8\% classical; 4.4\% pure quantum), indicating a favorable performance--stability--cost trade-off.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning Inductive Biases for fMRI Time Series Classification during Resting-state and Movie-watching</title>
<link>https://arxiv.org/abs/2509.16973</link>
<guid>https://arxiv.org/abs/2509.16973</guid>
<content:encoded><![CDATA[
arXiv:2509.16973v1 Announce Type: cross 
Abstract: Deep learning has advanced fMRI analysis, yet it remains unclear which architectural inductive biases are most effective at capturing functional patterns in human brain activity. This issue is particularly important in small-sample settings, as most datasets fall into this category. We compare models with three major inductive biases in deep learning including convolutional neural networks (CNNs), long short-term memory networks (LSTMs), and Transformers for the task of biological sex classification. These models are evaluated within a unified pipeline using parcellated multivariate fMRI time series from the Human Connectome Project (HCP) 7-Tesla cohort, which includes four resting-state runs and four movie-watching task runs. We assess performance on Whole-brain, subcortex, and 12 functional networks. CNNs consistently achieved the highest discrimination for sex classification in both resting-state and movie-watching, while LSTM and Transformer models underperformed. Network-resolved analyses indicated that the Whole-brain, Default Mode, Cingulo-Opercular, Dorsal Attention, and Frontoparietal networks were the most discriminative. These results were largely similar between resting-state and movie-watching. Our findings indicate that, at this dataset size, discriminative information is carried by local spatial patterns and inter-regional dependencies, favoring convolutional inductive bias. Our study provides insights for selecting deep learning architectures for fMRI time series classification.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Transport for Handwritten Text Recognition in a Low-Resource Regime</title>
<link>https://arxiv.org/abs/2509.16977</link>
<guid>https://arxiv.org/abs/2509.16977</guid>
<content:encoded><![CDATA[
arXiv:2509.16977v1 Announce Type: cross 
Abstract: Handwritten Text Recognition (HTR) is a task of central importance in the field of document image understanding. State-of-the-art methods for HTR require the use of extensive annotated sets for training, making them impractical for low-resource domains like historical archives or limited-size modern collections. This paper introduces a novel framework that, unlike the standard HTR model paradigm, can leverage mild prior knowledge of lexical characteristics; this is ideal for scenarios where labeled data are scarce. We propose an iterative bootstrapping approach that aligns visual features extracted from unlabeled images with semantic word representations using Optimal Transport (OT). Starting with a minimal set of labeled examples, the framework iteratively matches word images to text labels, generates pseudo-labels for high-confidence alignments, and retrains the recognizer on the growing dataset. Numerical experiments demonstrate that our iterative visual-semantic alignment scheme significantly improves recognition accuracy on low-resource HTR benchmarks.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Speech Understanding in Speech-Aware Language Models with GRPO</title>
<link>https://arxiv.org/abs/2509.16990</link>
<guid>https://arxiv.org/abs/2509.16990</guid>
<content:encoded><![CDATA[
arXiv:2509.16990v1 Announce Type: cross 
Abstract: In this paper, we introduce a Group Relative Policy Optimization (GRPO)-based method for training Speech-Aware Large Language Models (SALLMs) on open-format speech understanding tasks, such as Spoken Question Answering and Automatic Speech Translation. SALLMs have proven highly effective for speech understanding tasks. GRPO has recently gained traction for its efficiency in training LLMs, and prior work has explored its application to SALLMs, primarily in multiple-choice tasks. Building on this, we focus on open-format tasks that better reflect the generative abilities of the models. Our approach leverages GRPO with BLEU as the reward signal to optimize SALLMs, and we demonstrate empirically that it surpasses standard SFT across several key metrics. Finally, we explore the potential of incorporating off-policy samples within GRPO for these tasks, highlighting avenues for further improvement and further research.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DocIQ: A Benchmark Dataset and Feature Fusion Network for Document Image Quality Assessment</title>
<link>https://arxiv.org/abs/2509.17012</link>
<guid>https://arxiv.org/abs/2509.17012</guid>
<content:encoded><![CDATA[
arXiv:2509.17012v1 Announce Type: cross 
Abstract: Document image quality assessment (DIQA) is an important component for various applications, including optical character recognition (OCR), document restoration, and the evaluation of document image processing systems. In this paper, we introduce a subjective DIQA dataset DIQA-5000. The DIQA-5000 dataset comprises 5,000 document images, generated by applying multiple document enhancement techniques to 500 real-world images with diverse distortions. Each enhanced image was rated by 15 subjects across three rating dimensions: overall quality, sharpness, and color fidelity. Furthermore, we propose a specialized no-reference DIQA model that exploits document layout features to maintain quality perception at reduced resolutions to lower computational cost. Recognizing that image quality is influenced by both low-level and high-level visual features, we designed a feature fusion module to extract and integrate multi-level features from document images. To generate multi-dimensional scores, our model employs independent quality heads for each dimension to predict score distributions, allowing it to learn distinct aspects of document image quality. Experimental results demonstrate that our method outperforms current state-of-the-art general-purpose IQA models on both DIQA-5000 and an additional document image dataset focused on OCR accuracy.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepEOSNet: Capturing the dependency on thermodynamic state in property prediction tasks</title>
<link>https://arxiv.org/abs/2509.17018</link>
<guid>https://arxiv.org/abs/2509.17018</guid>
<content:encoded><![CDATA[
arXiv:2509.17018v1 Announce Type: cross 
Abstract: We propose a machine learning (ML) architecture to better capture the dependency of thermodynamic properties on the independent states. When predicting state-dependent thermodynamic properties, ML models need to account for both molecular structure and the thermodynamic state, described by independent variables, typically temperature, pressure, and composition. Modern molecular ML models typically include state information by adding it to molecular fingerprint vectors or by embedding explicit (semi-empirical) thermodynamic relations. Here, we propose to rather split the information processing on the molecular structure and the dependency on states into two separate network channels: a graph neural network and a multilayer perceptron, whose output is combined by a dot product. We refer to our approach as DeepEOSNet, as this idea is based on the DeepONet architecture [Lu et al. (2021), Nat. Mach. Intell.]: instead of operators, we learn state dependencies, with the possibility to predict equation of states (EOS). We investigate the predictive performance of DeepEOSNet by means of three case studies, which include the prediction of vapor pressure as a function of temperature, and mixture molar volume as a function of composition, temperature, and pressure. Our results show superior performance of DeepEOSNet for predicting vapor pressure and comparable performance for predicting mixture molar volume compared to state-of-research graph-based thermodynamic prediction models from our earlier works. In fact, we see large potential of DeepEOSNet in cases where data is sparse in the state domain and the output function is structurally similar across different molecules. The concept of DeepEOSNet can easily be transferred to other ML architectures in molecular context, and thus provides a viable option for property prediction.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Transfer Neurons Hypothesis: An Underlying Mechanism for Language Latent Space Transitions in Multilingual LLMs</title>
<link>https://arxiv.org/abs/2509.17030</link>
<guid>https://arxiv.org/abs/2509.17030</guid>
<content:encoded><![CDATA[
arXiv:2509.17030v1 Announce Type: cross 
Abstract: Recent studies have suggested a processing framework for multilingual inputs in decoder-based LLMs: early layers convert inputs into English-centric and language-agnostic representations; middle layers perform reasoning within an English-centric latent space; and final layers generate outputs by transforming these representations back into language-specific latent spaces. However, the internal dynamics of such transformation and the underlying mechanism remain underexplored. Towards a deeper understanding of this framework, we propose and empirically validate The Transfer Neurons Hypothesis: certain neurons in the MLP module are responsible for transferring representations between language-specific latent spaces and a shared semantic latent space. Furthermore, we show that one function of language-specific neurons, as identified in recent studies, is to facilitate movement between latent spaces. Finally, we show that transfer neurons are critical for reasoning in multilingual LLMs.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Localizing Malicious Outputs from CodeLLM</title>
<link>https://arxiv.org/abs/2509.17070</link>
<guid>https://arxiv.org/abs/2509.17070</guid>
<content:encoded><![CDATA[
arXiv:2509.17070v1 Announce Type: cross 
Abstract: We introduce FreqRank, a mutation-based defense to localize malicious components in LLM outputs and their corresponding backdoor triggers. FreqRank assumes that the malicious sub-string(s) consistently appear in outputs for triggered inputs and uses a frequency-based ranking system to identify them. Our ranking system then leverages this knowledge to localize the backdoor triggers present in the inputs. We create nine malicious models through fine-tuning or custom instructions for three downstream tasks, namely, code completion (CC), code generation (CG), and code summarization (CS), and show that they have an average attack success rate (ASR) of 86.6%. Furthermore, FreqRank's ranking system highlights the malicious outputs as one of the top five suggestions in 98% of cases. We also demonstrate that FreqRank's effectiveness scales as the number of mutants increases and show that FreqRank is capable of localizing the backdoor trigger effectively even with a limited number of triggered samples. Finally, we show that our approach is 35-50% more effective than other defense methods.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\texttt{DiffSyn}$: A Generative Diffusion Approach to Materials Synthesis Planning</title>
<link>https://arxiv.org/abs/2509.17094</link>
<guid>https://arxiv.org/abs/2509.17094</guid>
<content:encoded><![CDATA[
arXiv:2509.17094v1 Announce Type: cross 
Abstract: The synthesis of crystalline materials, such as zeolites, remains a significant challenge due to a high-dimensional synthesis space, intricate structure-synthesis relationships and time-consuming experiments. Considering the one-to-many relationship between structure and synthesis, we propose $\texttt{DiffSyn}$, a generative diffusion model trained on over 23,000 synthesis recipes spanning 50 years of literature. $\texttt{DiffSyn}$ generates probable synthesis routes conditioned on a desired zeolite structure and an organic template. $\texttt{DiffSyn}$ achieves state-of-the-art performance by capturing the multi-modal nature of structure-synthesis relationships. We apply $\texttt{DiffSyn}$ to differentiate among competing phases and generate optimal synthesis routes. As a proof of concept, we synthesize a UFI material using $\texttt{DiffSyn}$-generated synthesis routes. These routes, rationalized by density functional theory binding energies, resulted in the successful synthesis of a UFI material with a high Si/Al$_{\text{ICP}}$ of 19.0, which is expected to improve thermal stability and is higher than that of any previously recorded.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning for Campus Energy Resilience: Clustering and Time-Series Forecasting in Intelligent Load Shedding</title>
<link>https://arxiv.org/abs/2509.17097</link>
<guid>https://arxiv.org/abs/2509.17097</guid>
<content:encoded><![CDATA[
arXiv:2509.17097v1 Announce Type: cross 
Abstract: The growing demand for reliable electricity in universities necessitates intelligent energy management. This study proposes a machine learning-based load shedding framework for the University of Lagos, designed to optimize distribution and reduce waste. The methodology followed three main stages. First, a dataset of 3,648 hourly records from 55 buildings was compiled to develop building-level consumption models. Second, Principal Component Analysis was applied for dimensionality reduction, and clustering validation techniques were used to determine the optimal number of demand groups. Mini-Batch K-Means was then employed to classify buildings into high-, medium-, and low-demand clusters. Finally, short-term load forecasting was performed at the cluster level using multiple statistical and deep learning models, including ARIMA, SARIMA, Prophet, LSTM, and GRU. Results showed Prophet offered the most reliable forecasts, while Mini-Batch K-Means achieved stable clustering performance. By integrating clustering with forecasting, the framework enabled a fairer, data-driven load shedding strategy that reduces inefficiencies and supports climate change mitigation through sustainable energy management.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Supervised Interpretable and Robust Evidential Segmentation</title>
<link>https://arxiv.org/abs/2509.17098</link>
<guid>https://arxiv.org/abs/2509.17098</guid>
<content:encoded><![CDATA[
arXiv:2509.17098v1 Announce Type: cross 
Abstract: Uncertainty estimation has been widely studied in medical image segmentation as a tool to provide reliability, particularly in deep learning approaches. However, previous methods generally lack effective supervision in uncertainty estimation, leading to low interpretability and robustness of the predictions. In this work, we propose a self-supervised approach to guide the learning of uncertainty. Specifically, we introduce three principles about the relationships between the uncertainty and the image gradients around boundaries and noise. Based on these principles, two uncertainty supervision losses are designed. These losses enhance the alignment between model predictions and human interpretation. Accordingly, we introduce novel quantitative metrics for evaluating the interpretability and robustness of uncertainty. Experimental results demonstrate that compared to state-of-the-art approaches, the proposed method can achieve competitive segmentation performance and superior results in out-of-distribution (OOD) scenarios while significantly improving the interpretability and robustness of uncertainty estimation. Code is available via https://github.com/suiannaius/SURE.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Delay compensation of multi-input distinct delay nonlinear systems via neural operators</title>
<link>https://arxiv.org/abs/2509.17131</link>
<guid>https://arxiv.org/abs/2509.17131</guid>
<content:encoded><![CDATA[
arXiv:2509.17131v1 Announce Type: cross 
Abstract: In this work, we present the first stability results for approximate predictors in multi-input non-linear systems with distinct actuation delays. We show that if the predictor approximation satisfies a uniform (in time) error bound, semi-global practical stability is correspondingly achieved. For such approximators, the required uniform error bound depends on the desired region of attraction and the number of control inputs in the system. The result is achieved through transforming the delay into a transport PDE and conducting analysis on the coupled ODE-PDE cascade. To highlight the viability of such error bounds, we demonstrate our results on a class of approximators - neural operators - showcasing sufficiency for satisfying such a universal bound both theoretically and in simulation on a mobile robot experiment.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-efficient Kernel Methods for Learning Hamiltonian Systems</title>
<link>https://arxiv.org/abs/2509.17154</link>
<guid>https://arxiv.org/abs/2509.17154</guid>
<content:encoded><![CDATA[
arXiv:2509.17154v1 Announce Type: cross 
Abstract: Hamiltonian dynamics describe a wide range of physical systems. As such, data-driven simulations of Hamiltonian systems are important for many scientific and engineering problems. In this work, we propose kernel-based methods for identifying and forecasting Hamiltonian systems directly from data. We present two approaches: a two-step method that reconstructs trajectories before learning the Hamiltonian, and a one-step method that jointly infers both. Across several benchmark systems, including mass-spring dynamics, a nonlinear pendulum, and the Henon-Heiles system, we demonstrate that our framework achieves accurate, data-efficient predictions and outperforms two-step kernel-based baselines, particularly in scarce-data regimes, while preserving the conservation properties of Hamiltonian dynamics. Moreover, our methodology provides theoretical a priori error estimates, ensuring reliability of the learned models. We also provide a more general, problem-agnostic numerical framework that goes beyond Hamiltonian systems and can be used for data-driven learning of arbitrary dynamical systems.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Supervised Discovery of Neural Circuits in Spatially Patterned Neural Responses with Graph Neural Networks</title>
<link>https://arxiv.org/abs/2509.17174</link>
<guid>https://arxiv.org/abs/2509.17174</guid>
<content:encoded><![CDATA[
arXiv:2509.17174v1 Announce Type: cross 
Abstract: Inferring synaptic connectivity from neural population activity is a fundamental challenge in computational neuroscience, complicated by partial observability and mismatches between inference models and true circuit dynamics. In this study, we propose a graph-based neural inference model that simultaneously predicts neural activity and infers latent connectivity by modeling neurons as interacting nodes in a graph. The architecture features two distinct modules: one for learning structural connectivity and another for predicting future spiking activity via a graph neural network (GNN). Our model accommodates unobserved neurons through auxiliary nodes, allowing for inference in partially observed circuits. We evaluate this approach using synthetic data from ring attractor networks and real spike recordings from head direction cells in mice. Across a wide range of conditions, including varying recurrent connectivity, external inputs, and incomplete observations, our model consistently outperforms standard baselines, resolving spurious correlations more effectively and recovering accurate weight profiles. When applied to real data, the inferred connectivity aligns with theoretical predictions of continuous attractor models. These results highlight the potential of GNN-based models to infer latent neural circuitry through self-supervised structure learning, while leveraging the spike prediction task to flexibly link connectivity and dynamics across both simulated and biological neural systems.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning Models on Automatically Verifiable Textual and Visual Questions</title>
<link>https://arxiv.org/abs/2509.17177</link>
<guid>https://arxiv.org/abs/2509.17177</guid>
<content:encoded><![CDATA[
arXiv:2509.17177v1 Announce Type: cross 
Abstract: We conduct a moderate-scale contamination-free (to some extent) evaluation of current large reasoning models (LRMs) with some preliminary findings. We also release ROME, our evaluation benchmark for vision language models intended to test reasoning from visual clues. We attach links to the benchmark, evaluation data, and other updates on this website: https://flageval-baai.github.io/LRM-Eval/
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LifeAlign: Lifelong Alignment for Large Language Models with Memory-Augmented Focalized Preference Optimization</title>
<link>https://arxiv.org/abs/2509.17183</link>
<guid>https://arxiv.org/abs/2509.17183</guid>
<content:encoded><![CDATA[
arXiv:2509.17183v1 Announce Type: cross 
Abstract: Alignment plays a crucial role in Large Language Models (LLMs) in aligning with human preferences on a specific task/domain. Traditional alignment methods suffer from catastrophic forgetting, where models lose previously acquired knowledge when adapting to new preferences or domains. We introduce LifeAlign, a novel framework for lifelong alignment that enables LLMs to maintain consistent human preference alignment across sequential learning tasks without forgetting previously learned knowledge. Our approach consists of two key innovations. First, we propose a focalized preference optimization strategy that aligns LLMs with new preferences while preventing the erosion of knowledge acquired from previous tasks. Second, we develop a short-to-long memory consolidation mechanism that merges denoised short-term preference representations into stable long-term memory using intrinsic dimensionality reduction, enabling efficient storage and retrieval of alignment patterns across diverse domains. We evaluate LifeAlign across multiple sequential alignment tasks spanning different domains and preference types. Experimental results demonstrate that our method achieves superior performance in maintaining both preference alignment quality and knowledge retention compared to existing lifelong learning approaches. The codes and datasets will be released on GitHub.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guided and Unguided Conditional Diffusion Mechanisms for Structured and Semantically-Aware 3D Point Cloud Generation</title>
<link>https://arxiv.org/abs/2509.17206</link>
<guid>https://arxiv.org/abs/2509.17206</guid>
<content:encoded><![CDATA[
arXiv:2509.17206v1 Announce Type: cross 
Abstract: Generating realistic 3D point clouds is a fundamental problem in computer vision with applications in remote sensing, robotics, and digital object modeling. Existing generative approaches primarily capture geometry, and when semantics are considered, they are typically imposed post hoc through external segmentation or clustering rather than integrated into the generative process itself. We propose a diffusion-based framework that embeds per-point semantic conditioning directly within generation. Each point is associated with a conditional variable corresponding to its semantic label, which guides the diffusion dynamics and enables the joint synthesis of geometry and semantics. This design produces point clouds that are both structurally coherent and segmentation-aware, with object parts explicitly represented during synthesis. Through a comparative analysis of guided and unguided diffusion processes, we demonstrate the significant impact of conditional variables on diffusion dynamics and generation quality. Extensive experiments validate the efficacy of our approach, producing detailed and accurate 3D point clouds tailored to specific parts and features.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Point-RTD: Replaced Token Denoising for Pretraining Transformer Models on Point Clouds</title>
<link>https://arxiv.org/abs/2509.17207</link>
<guid>https://arxiv.org/abs/2509.17207</guid>
<content:encoded><![CDATA[
arXiv:2509.17207v1 Announce Type: cross 
Abstract: Pre-training strategies play a critical role in advancing the performance of transformer-based models for 3D point cloud tasks. In this paper, we introduce Point-RTD (Replaced Token Denoising), a novel pretraining strategy designed to improve token robustness through a corruption-reconstruction framework. Unlike traditional mask-based reconstruction tasks that hide data segments for later prediction, Point-RTD corrupts point cloud tokens and leverages a discriminator-generator architecture for denoising. This shift enables more effective learning of structural priors and significantly enhances model performance and efficiency. On the ShapeNet dataset, Point-RTD reduces reconstruction error by over 93% compared to PointMAE, and achieves more than 14x lower Chamfer Distance on the test set. Our method also converges faster and yields higher classification accuracy on ShapeNet, ModelNet10, and ModelNet40 benchmarks, clearly outperforming the baseline Point-MAE framework in every case.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Virtual Consistency for Audio Editing</title>
<link>https://arxiv.org/abs/2509.17219</link>
<guid>https://arxiv.org/abs/2509.17219</guid>
<content:encoded><![CDATA[
arXiv:2509.17219v1 Announce Type: cross 
Abstract: Free-form, text-based audio editing remains a persistent challenge, despite progress in inversion-based neural methods. Current approaches rely on slow inversion procedures, limiting their practicality. We present a virtual-consistency based audio editing system that bypasses inversion by adapting the sampling process of diffusion models. Our pipeline is model-agnostic, requiring no fine-tuning or architectural changes, and achieves substantial speed-ups over recent neural editing baselines. Crucially, it achieves this efficiency without compromising quality, as demonstrated by quantitative benchmarks and a user study involving 16 participants.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-based Methods for Simulating, Sampling, and Predicting Protein Ensembles</title>
<link>https://arxiv.org/abs/2509.17224</link>
<guid>https://arxiv.org/abs/2509.17224</guid>
<content:encoded><![CDATA[
arXiv:2509.17224v1 Announce Type: cross 
Abstract: Advances in deep learning have opened an era of abundant and accurate predicted protein structures; however, similar progress in protein ensembles has remained elusive. This review highlights several recent research directions towards AI-based predictions of protein ensembles, including coarse-grained force fields, generative models, multiple sequence alignment perturbation methods, and modeling of ensemble descriptors. An emphasis is placed on realistic assessments of the technological maturity of current methods, the strengths and weaknesses of broad families of techniques, and promising machine learning frameworks at an early stage of development. We advocate for "closing the loop" between model training, simulation, and inference to overcome challenges in training data availability and to enable the next generation of models.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with RoE</title>
<link>https://arxiv.org/abs/2509.17238</link>
<guid>https://arxiv.org/abs/2509.17238</guid>
<content:encoded><![CDATA[
arXiv:2509.17238v1 Announce Type: cross 
Abstract: The generation quality of large language models (LLMs) is often improved by utilizing inference-time sequence-level scaling methods (e.g., Chain-of-Thought). We introduce hyper-parallel scaling, a complementary framework that improves prediction quality at the token level. Hyper-parallel scaling computes and aggregates multiple output proposals for a single token from the model. We implement this concept in Mixture-of-Experts (MoE) models, which we refer to as Roster of Experts (RoE). RoE is a training-free inference algorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects controlled stochasticity into the expert routing mechanism, enabling it to sample multiple diverse experts for each token and aggregate their outputs for a more accurate final prediction.To overcome the computational cost, we introduce an efficient batching strategy and a specialized KV-caching mechanism that minimizes compute and memory overhead. For example, RoE enables a 7B MoE model to match the performance of a 10.5B MoE model while using 30% less compute for inference. These gains are achieved without any fine-tuning of model parameters.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Agents Judge Systematic Reviews Like Humans? Evaluating SLRs with LLM-based Multi-Agent System</title>
<link>https://arxiv.org/abs/2509.17240</link>
<guid>https://arxiv.org/abs/2509.17240</guid>
<content:encoded><![CDATA[
arXiv:2509.17240v1 Announce Type: cross 
Abstract: Systematic Literature Reviews (SLRs) are foundational to evidence-based research but remain labor-intensive and prone to inconsistency across disciplines. We present an LLM-based SLR evaluation copilot built on a Multi-Agent System (MAS) architecture to assist researchers in assessing the overall quality of the systematic literature reviews. The system automates protocol validation, methodological assessment, and topic relevance checks using a scholarly database. Unlike conventional single-agent methods, our design integrates a specialized agentic approach aligned with PRISMA guidelines to support more structured and interpretable evaluations. We conducted an initial study on five published SLRs from diverse domains, comparing system outputs to expert-annotated PRISMA scores, and observed 84% agreement. While early results are promising, this work represents a first step toward scalable and accurate NLP-driven systems for interdisciplinary workflows and reveals their capacity for rigorous, domain-agnostic knowledge aggregation to streamline the review process.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Risk Comparisons in Linear Regression: Implicit Regularization Dominates Explicit Regularization</title>
<link>https://arxiv.org/abs/2509.17251</link>
<guid>https://arxiv.org/abs/2509.17251</guid>
<content:encoded><![CDATA[
arXiv:2509.17251v1 Announce Type: cross 
Abstract: Existing theory suggests that for linear regression problems categorized by capacity and source conditions, gradient descent (GD) is always minimax optimal, while both ridge regression and online stochastic gradient descent (SGD) are polynomially suboptimal for certain categories of such problems. Moving beyond minimax theory, this work provides instance-wise comparisons of the finite-sample risks for these algorithms on any well-specified linear regression problem.
  Our analysis yields three key findings. First, GD dominates ridge regression: with comparable regularization, the excess risk of GD is always within a constant factor of ridge, but ridge can be polynomially worse even when tuned optimally. Second, GD is incomparable with SGD. While it is known that for certain problems GD can be polynomially better than SGD, the reverse is also true: we construct problems, inspired by benign overfitting theory, where optimally stopped GD is polynomially worse. Finally, GD dominates SGD for a significant subclass of problems -- those with fast and continuously decaying covariance spectra -- which includes all problems satisfying the standard capacity condition.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning and Optimization with 3D Orientations</title>
<link>https://arxiv.org/abs/2509.17274</link>
<guid>https://arxiv.org/abs/2509.17274</guid>
<content:encoded><![CDATA[
arXiv:2509.17274v1 Announce Type: cross 
Abstract: There exist numerous ways of representing 3D orientations. Each representation has both limitations and unique features. Choosing the best representation for one task is often a difficult chore, and there exist conflicting opinions on which representation is better suited for a set of family of tasks. Even worse, when dealing with scenarios where we need to learn or optimize functions with orientations as inputs and/or outputs, the set of possibilities (representations, loss functions, etc.) is even larger and it is not easy to decide what is best for each scenario. In this paper, we attempt to a) present clearly, concisely and with unified notation all available representations, and "tricks" related to 3D orientations (including Lie Group algebra), and b) benchmark them in representative scenarios. The first part feels like it is missing from the robotics literature as one has to read many different textbooks and papers in order have a concise and clear understanding of all possibilities, while the benchmark is necessary in order to come up with recommendations based on empirical evidence. More precisely, we experiment with the following settings that attempt to cover most widely used scenarios in robotics: 1) direct optimization, 2) imitation/supervised learning with a neural network controller, 3) reinforcement learning, and 4) trajectory optimization using differential dynamic programming. We finally provide guidelines depending on the scenario, and make available a reference implementation of all the orientation math described.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Token Alignment for Large Language Model Fusion</title>
<link>https://arxiv.org/abs/2509.17276</link>
<guid>https://arxiv.org/abs/2509.17276</guid>
<content:encoded><![CDATA[
arXiv:2509.17276v1 Announce Type: cross 
Abstract: Training large language models (LLMs) from scratch can yield models with unique functionalities and strengths, but it is costly and often leads to redundant capabilities. A more cost-effective alternative is to fuse existing pre-trained LLMs with different architectures into a more powerful model. However, a key challenge in existing model fusion is their dependence on manually predefined vocabulary alignment, which may not generalize well across diverse contexts, leading to performance degradation in several evaluation. To solve this, we draw inspiration from distribution learning and propose the probabilistic token alignment method as a general and soft mapping for alignment, named as PTA-LLM. Our approach innovatively reformulates token alignment into a classic mathematical problem: optimal transport, seamlessly leveraging distribution-aware learning to facilitate more coherent model fusion. Apart from its inherent generality, PTA-LLM exhibits interpretability from a distributional perspective, offering insights into the essence of the token alignment. Empirical results demonstrate that probabilistic token alignment enhances the target model's performance across multiple capabilities. Our code is avaliable at https://runjia.tech/neurips_pta-llm/.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clotho: Measuring Task-Specific Pre-Generation Test Adequacy for LLM Inputs</title>
<link>https://arxiv.org/abs/2509.17314</link>
<guid>https://arxiv.org/abs/2509.17314</guid>
<content:encoded><![CDATA[
arXiv:2509.17314v1 Announce Type: cross 
Abstract: Software increasingly relies on the emergent capabilities of Large Language Models (LLMs), from natural language understanding to program analysis and generation. Yet testing them on specific tasks remains difficult and costly: many prompts lack ground truth, forcing reliance on human judgment, while existing uncertainty and adequacy measures typically require full inference. A key challenge is to assess input adequacy in a way that reflects the demands of the task, ideally before even generating any output. We introduce CLOTHO, a task-specific, pre-generation adequacy measure that estimates input difficulty directly from hidden LLM states. Given a large pool of unlabelled inputs for a specific task, CLOTHO uses a Gaussian Mixture Model (GMM) to adaptively sample the most informative cases for human labelling. Based on this reference set the GMM can then rank unseen inputs by their likelihood of failure. In our empirical evaluation across eight benchmark tasks and three open-weight LLMs, CLOTHO can predict failures with a ROC-AUC of 0.716, after labelling reference sets that are on average only 5.4% of inputs. It does so without generating any outputs, thereby reducing costs compared to existing uncertainty measures. Comparison of CLOTHO and post-generation uncertainty measures shows that the two approaches complement each other. Crucially, we show that adequacy scores learnt from open-weight LLMs transfer effectively to proprietary models, extending the applicability of the approach. When prioritising test inputs for proprietary models, CLOTHO increases the average number of failing inputs from 18.7 to 42.5 out of 100, compared to random prioritisation.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CogAtom: From Cognitive Atoms to Olympiad-level Mathematical Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2509.17318</link>
<guid>https://arxiv.org/abs/2509.17318</guid>
<content:encoded><![CDATA[
arXiv:2509.17318v1 Announce Type: cross 
Abstract: Mathematical reasoning poses significant challenges for Large Language Models (LLMs) due to its demand for multi-step reasoning and abstract conceptual integration. While recent test-time scaling techniques rely heavily on high-quality, challenging problems, the scarcity of Olympiad-level math problems remains a bottleneck. We introduce CogAtom, a novel cognitive atom-based framework for synthesizing mathematically rigorous and cognitively diverse problems. Unlike prior approaches, CogAtom models problem construction as a process of selecting and recombining fundamental reasoning units, cognitive atoms, extracted from human-authored solutions. A diversity-promoting random walk algorithm enables exploration of the cognitive atom space, while a constraint-based recombination mechanism ensures logical soundness and structural validity. The combinatorial nature of the graph structure provides a near-infinite space of reasoning paths, and the walk algorithm systematically explores this space to achieve large-scale synthesis of high-quality problems; meanwhile, by controlling the number of cognitive atoms, we can precisely adjust problem difficulty, ensuring diversity, scalability, and controllability of the generated problems. Experimental results demonstrate that CogAtom outperforms existing methods in accuracy, reasoning depth, and diversity, generating problems that closely match the difficulty of AIME while exceeding it in structural variation. Our work offers a cognitively grounded pathway toward scalable, high-quality math problem generation.Our code is publicly available at https://github.com/Icarus-1111/CogAtom.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffQ: Unified Parameter Initialization for Variational Quantum Algorithms via Diffusion Models</title>
<link>https://arxiv.org/abs/2509.17324</link>
<guid>https://arxiv.org/abs/2509.17324</guid>
<content:encoded><![CDATA[
arXiv:2509.17324v1 Announce Type: cross 
Abstract: Variational Quantum Algorithms (VQAs) are widely used in the noisy intermediate-scale quantum (NISQ) era, but their trainability and performance depend critically on initialization parameters that shape the optimization landscape. Existing machine learning-based initializers achieve state-of-the-art results yet remain constrained to single-task domains and small datasets of only hundreds of samples. We address these limitations by reformulating VQA parameter initialization as a generative modeling problem and introducing DiffQ, a parameter initializer based on the Denoising Diffusion Probabilistic Model (DDPM). To support robust training and evaluation, we construct a dataset of 15,085 instances spanning three domains and five representative tasks. Experiments demonstrate that DiffQ surpasses baselines, reducing initial loss by up to 8.95 and convergence steps by up to 23.4%.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Word2VecGD: Neural Graph Drawing with Cosine-Stress Optimization</title>
<link>https://arxiv.org/abs/2509.17333</link>
<guid>https://arxiv.org/abs/2509.17333</guid>
<content:encoded><![CDATA[
arXiv:2509.17333v1 Announce Type: cross 
Abstract: We propose a novel graph visualization method leveraging random walk-based embeddings to replace costly graph-theoretical distance computations. Using word2vec-inspired embeddings, our approach captures both structural and semantic relationships efficiently. Instead of relying on exact shortest-path distances, we optimize layouts using cosine dissimilarities, significantly reducing computational overhead. Our framework integrates differentiable stress optimization with stochastic gradient descent (SGD), supporting multi-criteria layout objectives. Experimental results demonstrate that our method produces high-quality, semantically meaningful layouts while efficiently scaling to large graphs. Code available at: https://github.com/mlyann/graphv_nn
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainability matters: The effect of liability rules on the healthcare sector</title>
<link>https://arxiv.org/abs/2509.17334</link>
<guid>https://arxiv.org/abs/2509.17334</guid>
<content:encoded><![CDATA[
arXiv:2509.17334v1 Announce Type: cross 
Abstract: Explainability, the capability of an artificial intelligence system (AIS) to explain its outcomes in a manner that is comprehensible to human beings at an acceptable level, has been deemed essential for critical sectors, such as healthcare. Is it really the case? In this perspective, we consider two extreme cases, ``Oracle'' (without explainability) versus ``AI Colleague'' (with explainability) for a thorough analysis. We discuss how the level of automation and explainability of AIS can affect the determination of liability among the medical practitioner/facility and manufacturer of AIS. We argue that explainability plays a crucial role in setting a responsibility framework in healthcare, from a legal standpoint, to shape the behavior of all involved parties and mitigate the risk of potential defensive medicine practices.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Scenario Highway Lane-Change Intention Prediction: A Physics-Informed AI Framework for Three-Class Classification</title>
<link>https://arxiv.org/abs/2509.17354</link>
<guid>https://arxiv.org/abs/2509.17354</guid>
<content:encoded><![CDATA[
arXiv:2509.17354v1 Announce Type: cross 
Abstract: Lane-change maneuvers are a leading cause of highway accidents, underscoring the need for accurate intention prediction to improve the safety and decision-making of autonomous driving systems. While prior studies using machine learning and deep learning methods (e.g., SVM, CNN, LSTM, Transformers) have shown promise, most approaches remain limited by binary classification, lack of scenario diversity, and degraded performance under longer prediction horizons. In this study, we propose a physics-informed AI framework that explicitly integrates vehicle kinematics, interaction feasibility, and traffic-safety metrics (e.g., distance headway, time headway, time-to-collision, closing gap time) into the learning process. lane-change prediction is formulated as a three-class problem that distinguishes left change, right change, and no change, and is evaluated across both straight highway segments (highD) and complex ramp scenarios (exiD). By integrating vehicle kinematics with interaction features, our machine learning models, particularly LightGBM, achieve state-of-the-art accuracy and strong generalization. Results show up to 99.8% accuracy and 93.6% macro F1 on highD, and 96.1% accuracy and 88.7% macro F1 on exiD at a 1-second horizon, outperforming a two-layer stacked LSTM baseline. These findings demonstrate the practical advantages of a physics-informed and feature-rich machine learning framework for real-time lane-change intention prediction in autonomous driving systems.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SilentStriker:Toward Stealthy Bit-Flip Attacks on Large Language Models</title>
<link>https://arxiv.org/abs/2509.17371</link>
<guid>https://arxiv.org/abs/2509.17371</guid>
<content:encoded><![CDATA[
arXiv:2509.17371v1 Announce Type: cross 
Abstract: The rapid adoption of large language models (LLMs) in critical domains has spurred extensive research into their security issues. While input manipulation attacks (e.g., prompt injection) have been well studied, Bit-Flip Attacks (BFAs) -- which exploit hardware vulnerabilities to corrupt model parameters and cause severe performance degradation -- have received far less attention. Existing BFA methods suffer from key limitations: they fail to balance performance degradation and output naturalness, making them prone to discovery. In this paper, we introduce SilentStriker, the first stealthy bit-flip attack against LLMs that effectively degrades task performance while maintaining output naturalness. Our core contribution lies in addressing the challenge of designing effective loss functions for LLMs with variable output length and the vast output space. Unlike prior approaches that rely on output perplexity for attack loss formulation, which inevitably degrade output naturalness, we reformulate the attack objective by leveraging key output tokens as targets for suppression, enabling effective joint optimization of attack effectiveness and stealthiness. Additionally, we employ an iterative, progressive search strategy to maximize attack efficacy. Experiments show that SilentStriker significantly outperforms existing baselines, achieving successful attacks without compromising the naturalness of generated text.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bias-variance Tradeoff in Tensor Estimation</title>
<link>https://arxiv.org/abs/2509.17382</link>
<guid>https://arxiv.org/abs/2509.17382</guid>
<content:encoded><![CDATA[
arXiv:2509.17382v1 Announce Type: cross 
Abstract: We study denoising of a third-order tensor when the ground-truth tensor is not necessarily Tucker low-rank. Specifically, we observe $$ Y=X^\ast+Z\in \mathbb{R}^{p_{1} \times p_{2} \times p_{3}}, $$ where $X^\ast$ is the ground-truth tensor, and $Z$ is the noise tensor. We propose a simple variant of the higher-order tensor SVD estimator $\widetilde{X}$. We show that uniformly over all user-specified Tucker ranks $(r_{1},r_{2},r_{3})$, $$ \| \widetilde{X} - X^* \|_{ \mathrm{F}}^2 = O \Big( \kappa^2 \Big\{ r_{1}r_{2}r_{3}+\sum_{k=1}^{3} p_{k} r_{k} \Big\} \; + \; \xi_{(r_{1},r_{2},r_{3})}^2\Big) \quad \text{ with high probability.} $$ Here, the bias term $\xi_{(r_1,r_2,r_3)}$ corresponds to the best achievable approximation error of $X^\ast$ over the class of tensors with Tucker ranks $(r_1,r_2,r_3)$; $\kappa^2$ quantifies the noise level; and the variance term $\kappa^2 \{r_{1}r_{2}r_{3}+\sum_{k=1}^{3} p_{k} r_{k}\}$ scales with the effective number of free parameters in the estimator $\widetilde{X}$. Our analysis achieves a clean rank-adaptive bias--variance tradeoff: as we increase the ranks of estimator $\widetilde{X}$, the bias $\xi(r_{1},r_{2},r_{3})$ decreases and the variance increases. As a byproduct we also obtain a convenient bias-variance decomposition for the vanilla low-rank SVD matrix estimators.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Mixture Models for Algorithmic Fairness Under Latent Heterogeneity</title>
<link>https://arxiv.org/abs/2509.17411</link>
<guid>https://arxiv.org/abs/2509.17411</guid>
<content:encoded><![CDATA[
arXiv:2509.17411v1 Announce Type: cross 
Abstract: Standard machine learning models optimized for average performance often fail on minority subgroups and lack robustness to distribution shifts. This challenge worsens when subgroups are latent and affected by complex interactions among continuous and discrete features. We introduce ROME (RObust Mixture Ensemble), a framework that learns latent group structure from data while optimizing for worst-group performance. ROME employs two approaches: an Expectation-Maximization algorithm for linear models and a neural Mixture-of-Experts for nonlinear settings. Through simulations and experiments on real-world datasets, we demonstrate that ROME significantly improves algorithmic fairness compared to standard methods while maintaining competitive average performance. Importantly, our method requires no predefined group labels, making it practical when sources of disparities are unknown or evolving.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPICED: A Synaptic Homeostasis-Inspired Framework for Unsupervised Continual EEG Decoding</title>
<link>https://arxiv.org/abs/2509.17439</link>
<guid>https://arxiv.org/abs/2509.17439</guid>
<content:encoded><![CDATA[
arXiv:2509.17439v1 Announce Type: cross 
Abstract: Human brain achieves dynamic stability-plasticity balance through synaptic homeostasis. Inspired by this biological principle, we propose SPICED: a neuromorphic framework that integrates the synaptic homeostasis mechanism for unsupervised continual EEG decoding, particularly addressing practical scenarios where new individuals with inter-individual variability emerge continually. SPICED comprises a novel synaptic network that enables dynamic expansion during continual adaptation through three bio-inspired neural mechanisms: (1) critical memory reactivation; (2) synaptic consolidation and (3) synaptic renormalization. The interplay within synaptic homeostasis dynamically strengthens task-discriminative memory traces and weakens detrimental memories. By integrating these mechanisms with continual learning system, SPICED preferentially replays task-discriminative memory traces that exhibit strong associations with newly emerging individuals, thereby achieving robust adaptations. Meanwhile, SPICED effectively mitigates catastrophic forgetting by suppressing the replay prioritization of detrimental memories during long-term continual learning. Validated on three EEG datasets, SPICED show its effectiveness.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Pangaea: Unifying Intelligence Islands for Adapting Myriad Tasks</title>
<link>https://arxiv.org/abs/2509.17460</link>
<guid>https://arxiv.org/abs/2509.17460</guid>
<content:encoded><![CDATA[
arXiv:2509.17460v1 Announce Type: cross 
Abstract: The pursuit of artificial general intelligence continuously demands generalization in one model across myriad tasks, even those not seen before. However, current AI models are isolated from each other for being limited to specific tasks, now first defined as Intelligence Islands. To unify Intelligence Islands into one, we propose Pangaea, the first AI supercontinent akin to the geological Pangaea. Pangaea encodes any data into a unified format and accumulates universal knowledge through pre-training on 296 datasets across diverse modalities. Eventually, it demonstrates remarkable generalization across 45 general tasks and 15 scientific tasks encompassing a wide range of scientific subjects. By investigating Pangaea deeper, the scaling effect of modality is revealed, quantifying the universal knowledge accumulation across modalities as the cumulative distribution function of a geometric distribution. On the whole, Pangaea shows strong potential to handle myriad tasks, indicating a new direction toward artificial general intelligence.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformer-Gather, Fuzzy-Reconsider: A Scalable Hybrid Framework for Entity Resolution</title>
<link>https://arxiv.org/abs/2509.17470</link>
<guid>https://arxiv.org/abs/2509.17470</guid>
<content:encoded><![CDATA[
arXiv:2509.17470v1 Announce Type: cross 
Abstract: Entity resolution plays a significant role in enterprise systems where data integrity must be rigorously maintained. Traditional methods often struggle with handling noisy data or semantic understanding, while modern methods suffer from computational costs or the excessive need for parallel computation. In this study, we introduce a scalable hybrid framework, which is designed to address several important problems, including scalability, noise robustness, and reliable results. We utilized a pre-trained language model to encode each structured data into corresponding semantic embedding vectors. Subsequently, after retrieving a semantically relevant subset of candidates, we apply a syntactic verification stage using fuzzy string matching techniques to refine classification on the unlabeled data. This approach was applied to a real-world entity resolution task, which exposed a linkage between a central user management database and numerous shared hosting server records. Compared to other methods, this approach exhibits an outstanding performance in terms of both processing time and robustness, making it a reliable solution for a server-side product. Crucially, this efficiency does not compromise results, as the system maintains a high retrieval recall of approximately 0.97. The scalability of the framework makes it deployable on standard CPU-based infrastructure, offering a practical and effective solution for enterprise-level data integrity auditing.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Energy Efficiency of NPU-Accelerated Machine Learning Inference on Embedded Microcontrollers</title>
<link>https://arxiv.org/abs/2509.17533</link>
<guid>https://arxiv.org/abs/2509.17533</guid>
<content:encoded><![CDATA[
arXiv:2509.17533v1 Announce Type: cross 
Abstract: The deployment of machine learning (ML) models on microcontrollers (MCUs) is constrained by strict energy, latency, and memory requirements, particularly in battery-operated and real-time edge devices. While software-level optimizations such as quantization and pruning reduce model size and computation, hardware acceleration has emerged as a decisive enabler for efficient embedded inference. This paper evaluates the impact of Neural Processing Units (NPUs) on MCU-based ML execution, using the ARM Cortex-M55 core combined with the Ethos-U55 NPU on the Alif Semiconductor Ensemble E7 development board as a representative platform. A rigorous measurement methodology was employed, incorporating per-inference net energy accounting via GPIO-triggered high-resolution digital multimeter synchronization and idle-state subtraction, ensuring accurate attribution of energy costs. Experimental results across six representative ML models -including MiniResNet, MobileNetV2, FD-MobileNet, MNIST, TinyYolo, and SSD-MobileNet- demonstrate substantial efficiency gains when inference is offloaded to the NPU. For moderate to large networks, latency improvements ranged from 7x to over 125x, with per-inference net energy reductions up to 143x. Notably, the NPU enabled execution of models unsupported on CPU-only paths, such as SSD-MobileNet, highlighting its functional as well as efficiency advantages. These findings establish NPUs as a cornerstone of energy-aware embedded AI, enabling real-time, power-constrained ML inference at the MCU level.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bilateral Distribution Compression: Reducing Both Data Size and Dimensionality</title>
<link>https://arxiv.org/abs/2509.17543</link>
<guid>https://arxiv.org/abs/2509.17543</guid>
<content:encoded><![CDATA[
arXiv:2509.17543v1 Announce Type: cross 
Abstract: Existing distribution compression methods reduce dataset size by minimising the Maximum Mean Discrepancy (MMD) between original and compressed sets, but modern datasets are often large in both sample size and dimensionality. We propose Bilateral Distribution Compression (BDC), a two-stage framework that compresses along both axes while preserving the underlying distribution, with overall linear time and memory complexity in dataset size and dimension. Central to BDC is the Decoded MMD (DMMD), which quantifies the discrepancy between the original data and a compressed set decoded from a low-dimensional latent space. BDC proceeds by (i) learning a low-dimensional projection using the Reconstruction MMD (RMMD), and (ii) optimising a latent compressed set with the Encoded MMD (EMMD). We show that this procedure minimises the DMMD, guaranteeing that the compressed set faithfully represents the original distribution. Experiments show that across a variety of scenarios BDC can achieve comparable or superior performance to ambient-space compression at substantially lower cost.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is It Certainly a Deepfake? Reliability Analysis in Detection &amp; Generation Ecosystem</title>
<link>https://arxiv.org/abs/2509.17550</link>
<guid>https://arxiv.org/abs/2509.17550</guid>
<content:encoded><![CDATA[
arXiv:2509.17550v1 Announce Type: cross 
Abstract: As generative models are advancing in quality and quantity for creating synthetic content, deepfakes begin to cause online mistrust. Deepfake detectors are proposed to counter this effect, however, misuse of detectors claiming fake content as real or vice versa further fuels this misinformation problem. We present the first comprehensive uncertainty analysis of deepfake detectors, systematically investigating how generative artifacts influence prediction confidence. As reflected in detectors' responses, deepfake generators also contribute to this uncertainty as their generative residues vary, so we cross the uncertainty analysis of deepfake detectors and generators. Based on our observations, the uncertainty manifold holds enough consistent information to leverage uncertainty for deepfake source detection. Our approach leverages Bayesian Neural Networks and Monte Carlo dropout to quantify both aleatoric and epistemic uncertainties across diverse detector architectures. We evaluate uncertainty on two datasets with nine generators, with four blind and two biological detectors, compare different uncertainty methods, explore region- and pixel-based uncertainty, and conduct ablation studies. We conduct and analyze binary real/fake, multi-class real/fake, source detection, and leave-one-out experiments between the generator/detector combinations to share their generalization capability, model calibration, uncertainty, and robustness against adversarial attacks. We further introduce uncertainty maps that localize prediction confidence at the pixel level, revealing distinct patterns correlated with generator-specific artifacts. Our analysis provides critical insights for deploying reliable deepfake detection systems and establishes uncertainty quantification as a fundamental requirement for trustworthy synthetic media detection.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MontePrep: Monte-Carlo-Driven Automatic Data Preparation without Target Data Instances</title>
<link>https://arxiv.org/abs/2509.17553</link>
<guid>https://arxiv.org/abs/2509.17553</guid>
<content:encoded><![CDATA[
arXiv:2509.17553v1 Announce Type: cross 
Abstract: In commercial systems, a pervasive requirement for automatic data preparation (ADP) is to transfer relational data from disparate sources to targets with standardized schema specifications. Previous methods rely on labor-intensive supervision signals or target table data access permissions, limiting their usage in real-world scenarios. To tackle these challenges, we propose an effective end-to-end ADP framework MontePrep, which enables training-free pipeline synthesis with zero target-instance requirements. MontePrep is formulated as an open-source large language model (LLM) powered tree-structured search problem. It consists of three pivot components, i.e., a data preparation action sandbox (DPAS), a fundamental pipeline generator (FPG), and an execution-aware pipeline optimizer (EPO). We first introduce DPAS, a lightweight action sandbox, to navigate the search-based pipeline generation. The design of DPAS circumvents exploration of infeasible pipelines. Then, we present FPG to build executable DP pipelines incrementally, which explores the predefined action sandbox by the LLM-powered Monte Carlo Tree Search. Furthermore, we propose EPO, which invokes pipeline execution results from sources to targets to evaluate the reliability of the generated pipelines in FPG. In this way, unreasonable pipelines are eliminated, thus facilitating the search process from both efficiency and effectiveness perspectives. Extensive experimental results demonstrate the superiority of MontePrep with significant improvement against five state-of-the-art competitors.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRNU-Bench: A Novel Benchmark and Model for PRNU-Based Camera Identification</title>
<link>https://arxiv.org/abs/2509.17581</link>
<guid>https://arxiv.org/abs/2509.17581</guid>
<content:encoded><![CDATA[
arXiv:2509.17581v1 Announce Type: cross 
Abstract: We propose a novel benchmark for camera identification via Photo Response Non-Uniformity (PRNU) estimation. The benchmark comprises 13K photos taken with 120+ cameras, where the training and test photos are taken in different scenarios, enabling ``in-the-wild'' evaluation. In addition, we propose a novel PRNU-based camera identification model that employs a hybrid architecture, comprising a denoising autoencoder to estimate the PRNU signal and a convolutional network that can perform 1:N verification of camera devices. Instead of using a conventional approach based on contrastive learning, our method takes the Hadamard product between reference and query PRNU signals as input. This novel design leads to significantly better results compared with state-of-the-art models based on denoising autoencoders and contrastive learning. We release our dataset and code at: https://github.com/CroitoruAlin/PRNU-Bench.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpreting Attention Heads for Image-to-Text Information Flow in Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2509.17588</link>
<guid>https://arxiv.org/abs/2509.17588</guid>
<content:encoded><![CDATA[
arXiv:2509.17588v1 Announce Type: cross 
Abstract: Large Vision-Language Models (LVLMs) answer visual questions by transferring information from images to text through a series of attention heads. While this image-to-text information flow is central to visual question answering, its underlying mechanism remains difficult to interpret due to the simultaneous operation of numerous attention heads. To address this challenge, we propose head attribution, a technique inspired by component attribution methods, to identify consistent patterns among attention heads that play a key role in information transfer. Using head attribution, we investigate how LVLMs rely on specific attention heads to identify and answer questions about the main object in an image. Our analysis reveals that a distinct subset of attention heads facilitates the image-to-text information flow. Remarkably, we find that the selection of these heads is governed by the semantic content of the input image rather than its visual appearance. We further examine the flow of information at the token level and discover that (1) text information first propagates to role-related tokens and the final token before receiving image information, and (2) image information is embedded in both object-related and background tokens. Our work provides evidence that image-to-text information flow follows a structured process, and that analysis at the attention-head level offers a promising direction toward understanding the mechanisms of LVLMs.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FastNet: Improving the physical consistency of machine-learning weather prediction models through loss function design</title>
<link>https://arxiv.org/abs/2509.17601</link>
<guid>https://arxiv.org/abs/2509.17601</guid>
<content:encoded><![CDATA[
arXiv:2509.17601v1 Announce Type: cross 
Abstract: Machine learning weather prediction (MLWP) models have demonstrated remarkable potential in delivering accurate forecasts at significantly reduced computational cost compared to traditional numerical weather prediction (NWP) systems. However, challenges remain in ensuring the physical consistency of MLWP outputs, particularly in deterministic settings. This study presents FastNet, a graph neural network (GNN)-based global prediction model, and investigates the impact of alternative loss function designs on improving the physical realism of its forecasts. We explore three key modifications to the standard mean squared error (MSE) loss: (1) a modified spherical harmonic (MSH) loss that penalises spectral amplitude errors to reduce blurring and enhance small-scale structure retention; (2) inclusion of horizontal gradient terms in the loss to suppress non-physical artefacts; and (3) an alternative wind representation that decouples speed and direction to better capture extreme wind events. Results show that while the MSH and gradient-based losses \textit{alone} may slightly degrade RMSE scores, when trained in combination the model exhibits very similar MSE performance to an MSE-trained model while at the same time significantly improving spectral fidelity and physical consistency. The alternative wind representation further improves wind speed accuracy and reduces directional bias. Collectively, these findings highlight the importance of loss function design as a mechanism for embedding domain knowledge into MLWP models and advancing their operational readiness.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Audio Super-Resolution with Latent Bridge Models</title>
<link>https://arxiv.org/abs/2509.17609</link>
<guid>https://arxiv.org/abs/2509.17609</guid>
<content:encoded><![CDATA[
arXiv:2509.17609v1 Announce Type: cross 
Abstract: Audio super-resolution (SR), i.e., upsampling the low-resolution (LR) waveform to the high-resolution (HR) version, has recently been explored with diffusion and bridge models, while previous methods often suffer from sub-optimal upsampling quality due to their uninformative generation prior. Towards high-quality audio super-resolution, we present a new system with latent bridge models (LBMs), where we compress the audio waveform into a continuous latent space and design an LBM to enable a latent-to-latent generation process that naturally matches the LR-toHR upsampling process, thereby fully exploiting the instructive prior information contained in the LR waveform. To further enhance the training results despite the limited availability of HR samples, we introduce frequency-aware LBMs, where the prior and target frequency are taken as model input, enabling LBMs to explicitly learn an any-to-any upsampling process at the training stage. Furthermore, we design cascaded LBMs and present two prior augmentation strategies, where we make the first attempt to unlock the audio upsampling beyond 48 kHz and empower a seamless cascaded SR process, providing higher flexibility for audio post-production. Comprehensive experimental results evaluated on the VCTK, ESC-50, Song-Describer benchmark datasets and two internal testsets demonstrate that we achieve state-of-the-art objective and perceptual quality for any-to-48kHz SR across speech, audio, and music signals, as well as setting the first record for any-to-192kHz audio SR. Demo at https://AudioLBM.github.io/.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Whitening Spherical Gaussian Mixtures in the Large-Dimensional Regime</title>
<link>https://arxiv.org/abs/2509.17636</link>
<guid>https://arxiv.org/abs/2509.17636</guid>
<content:encoded><![CDATA[
arXiv:2509.17636v1 Announce Type: cross 
Abstract: Whitening is a classical technique in unsupervised learning that can facilitate estimation tasks by standardizing data. An important application is the estimation of latent variable models via the decomposition of tensors built from high-order moments. In particular, whitening orthogonalizes the means of a spherical Gaussian mixture model (GMM), thereby making the corresponding moment tensor orthogonally decomposable, hence easier to decompose. However, in the large-dimensional regime (LDR) where data are high-dimensional and scarce, the standard whitening matrix built from the sample covariance becomes ineffective because the latter is spectrally distorted. Consequently, whitened means of a spherical GMM are no longer orthogonal. Using random matrix theory, we derive exact limits for their dot products, which are generally nonzero in the LDR. As our main contribution, we then construct a corrected whitening matrix that restores asymptotic orthogonality, allowing for performance gains in spherical GMM estimation.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AuditoryBench++: Can Language Models Understand Auditory Knowledge without Hearing?</title>
<link>https://arxiv.org/abs/2509.17641</link>
<guid>https://arxiv.org/abs/2509.17641</guid>
<content:encoded><![CDATA[
arXiv:2509.17641v1 Announce Type: cross 
Abstract: Even without directly hearing sounds, humans can effortlessly reason about auditory properties, such as pitch, loudness, or sound-source associations, drawing on auditory commonsense. In contrast, language models often lack this capability, limiting their effectiveness in multimodal interactions. As an initial step to address this gap, we present AuditoryBench++, a comprehensive benchmark for evaluating auditory knowledge and reasoning in text-only settings. The benchmark encompasses tasks that range from basic auditory comparisons to contextually grounded reasoning, enabling fine-grained analysis of how models process and integrate auditory concepts. In addition, we introduce AIR-CoT, a novel auditory imagination reasoning method that generates and integrates auditory information during inference through span detection with special tokens and knowledge injection. Extensive experiments with recent LLMs and Multimodal LLMs demonstrate that AIR-CoT generally outperforms both the off-the-shelf models and those augmented with auditory knowledge. The project page is available at https://auditorybenchpp.github.io.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAVEN: RAnking and Validation of ExoplaNets</title>
<link>https://arxiv.org/abs/2509.17645</link>
<guid>https://arxiv.org/abs/2509.17645</guid>
<content:encoded><![CDATA[
arXiv:2509.17645v1 Announce Type: cross 
Abstract: We present RAVEN, a newly developed vetting and validation pipeline for TESS exoplanet candidates. The pipeline employs a Bayesian framework to derive the posterior probability of a candidate being a planet against a set of False Positive (FP) scenarios, through the use of a Gradient Boosted Decision Tree and a Gaussian Process classifier, trained on comprehensive synthetic training sets of simulated planets and 8 astrophysical FP scenarios injected into TESS lightcurves. These training sets allow large scale candidate vetting and performance verification against individual FP scenarios. A Non-Simulated FP training set consisting of real TESS candidates caused primarily by stellar variability and systematic noise is also included. The machine learning derived probabilities are combined with scenario specific prior probabilities, including the candidates' positional probabilities, to compute the final posterior probabilities. Candidates with a planetary posterior probability greater than 99% against each FP scenario and whose implied planetary radius is less than 8$R_{\oplus}$ are considered to be statistically validated by the pipeline. In this first version, the pipeline has been developed for candidates with a lightcurve released from the TESS Science Processing Operations Centre, an orbital period between 0.5 and 16 days and a transit depth greater than 300ppm. The pipeline obtained area-under-curve (AUC) scores > 97% on all FP scenarios and > 99% on all but one. Testing on an independent external sample of 1361 pre-classified TOIs, the pipeline achieved an overall accuracy of 91%, demonstrating its effectiveness for automated ranking of TESS candidates. For a probability threshold of 0.9 the pipeline reached a precision of 97% with a recall score of 66% on these TOIs. The RAVEN pipeline is publicly released as a cloud-hosted app, making it easily accessible to the community.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tailored Transformation Invariance for Industrial Anomaly Detection</title>
<link>https://arxiv.org/abs/2509.17670</link>
<guid>https://arxiv.org/abs/2509.17670</guid>
<content:encoded><![CDATA[
arXiv:2509.17670v1 Announce Type: cross 
Abstract: Industrial Anomaly Detection (IAD) is a subproblem within Computer Vision Anomaly Detection that has been receiving increasing amounts of attention due to its applicability to real-life scenarios. Recent research has focused on how to extract the most informative features, contrasting older kNN-based methods that use only pretrained features. These recent methods are much more expensive to train however and could complicate real-life application. Careful study of related work with regards to transformation invariance leads to the idea that popular benchmarks require robustness to only minor translations. With this idea we then formulate LWinNN, a local window based approach that creates a middle ground between kNN based methods that have either complete or no translation invariance. Our experiments demonstrate that this small change increases accuracy considerably, while simultaneously decreasing both train and test time. This teaches us two things: first, the gap between kNN-based approaches and more complex state-of-the-art methodology can still be narrowed by effective usage of the limited data available. Second, our assumption of requiring only limited translation invariance highlights potential areas of interest for future work and the need for more spatially diverse benchmarks, for which our method can hopefully serve as a new baseline. Our code can be found at https://github.com/marietteschonfeld/LWinNN .
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Chest Radiograph Findings from Electrocardiograms Using Interpretable Machine Learning</title>
<link>https://arxiv.org/abs/2509.17674</link>
<guid>https://arxiv.org/abs/2509.17674</guid>
<content:encoded><![CDATA[
arXiv:2509.17674v1 Announce Type: cross 
Abstract: Purpose: Chest X-rays are essential for diagnosing pulmonary conditions, but limited access in resource-constrained settings can delay timely diagnosis. Electrocardiograms (ECGs), in contrast, are widely available, non-invasive, and often acquired earlier in clinical workflows. This study aims to assess whether ECG features and patient demographics can predict chest radiograph findings using an interpretable machine learning approach.
  Methods: Using the MIMIC-IV database, Extreme Gradient Boosting (XGBoost) classifiers were trained to predict diverse chest radiograph findings from ECG-derived features and demographic variables. Recursive feature elimination was performed independently for each target to identify the most predictive features. Model performance was evaluated using the area under the receiver operating characteristic curve (AUROC) with bootstrapped 95% confidence intervals. Shapley Additive Explanations (SHAP) were applied to interpret feature contributions.
  Results: Models successfully predicted multiple chest radiograph findings with varying accuracy. Feature selection tailored predictors to each target, and including demographic variables consistently improved performance. SHAP analysis revealed clinically meaningful contributions from ECG features to radiographic predictions.
  Conclusion: ECG-derived features combined with patient demographics can serve as a proxy for certain chest radiograph findings, enabling early triage or pre-screening in settings where radiographic imaging is limited. Interpretable machine learning demonstrates potential to support radiology workflows and improve patient care.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Bias: A Multilingual Pipeline for Generating, Solving, and Evaluating Math Problems with LLMs</title>
<link>https://arxiv.org/abs/2509.17701</link>
<guid>https://arxiv.org/abs/2509.17701</guid>
<content:encoded><![CDATA[
arXiv:2509.17701v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly used for educational support, yet their response quality varies depending on the language of interaction. This paper presents an automated multilingual pipeline for generating, solving, and evaluating math problems aligned with the German K-10 curriculum. We generated 628 math exercises and translated them into English, German, and Arabic. Three commercial LLMs (GPT-4o-mini, Gemini 2.5 Flash, and Qwen-plus) were prompted to produce step-by-step solutions in each language. A held-out panel of LLM judges, including Claude 3.5 Haiku, evaluated solution quality using a comparative framework. Results show a consistent gap, with English solutions consistently rated highest, and Arabic often ranked lower. These findings highlight persistent linguistic bias and the need for more equitable multilingual AI systems in education.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Labeling of Intracranial Arteries with Uncertainty Quantification Using Deep Learning</title>
<link>https://arxiv.org/abs/2509.17726</link>
<guid>https://arxiv.org/abs/2509.17726</guid>
<content:encoded><![CDATA[
arXiv:2509.17726v1 Announce Type: cross 
Abstract: Accurate anatomical labeling of intracranial arteries is essential for cerebrovascular diagnosis and hemodynamic analysis but remains time-consuming and subject to interoperator variability. We present a deep learning-based framework for automated artery labeling from 3D Time-of-Flight Magnetic Resonance Angiography (3D ToF-MRA) segmentations (n=35), incorporating uncertainty quantification to enhance interpretability and reliability. We evaluated three convolutional neural network architectures: (1) a UNet with residual encoder blocks, reflecting commonly used baselines in vascular labeling; (2) CS-Net, an attention-augmented UNet incorporating channel and spatial attention mechanisms for enhanced curvilinear structure recognition; and (3) nnUNet, a self-configuring framework that automates preprocessing, training, and architectural adaptation based on dataset characteristics. Among these, nnUNet achieved the highest labeling performance (average Dice score: 0.922; average surface distance: 0.387 mm), with improved robustness in anatomically complex vessels. To assess predictive confidence, we implemented test-time augmentation (TTA) and introduced a novel coordinate-guided strategy to reduce interpolation errors during augmented inference. The resulting uncertainty maps reliably indicated regions of anatomical ambiguity, pathological variation, or manual labeling inconsistency. We further validated clinical utility by comparing flow velocities derived from automated and manual labels in co-registered 4D Flow MRI datasets, observing close agreement with no statistically significant differences. Our framework offers a scalable, accurate, and uncertainty-aware solution for automated cerebrovascular labeling, supporting downstream hemodynamic analysis and facilitating clinical integration.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DIVERS-Bench: Evaluating Language Identification Across Domain Shifts and Code-Switching</title>
<link>https://arxiv.org/abs/2509.17768</link>
<guid>https://arxiv.org/abs/2509.17768</guid>
<content:encoded><![CDATA[
arXiv:2509.17768v1 Announce Type: cross 
Abstract: Language Identification (LID) is a core task in multilingual NLP, yet current systems often overfit to clean, monolingual data. This work introduces DIVERS-BENCH, a comprehensive evaluation of state-of-the-art LID models across diverse domains, including speech transcripts, web text, social media texts, children's stories, and code-switched text. Our findings reveal that while models achieve high accuracy on curated datasets, performance degrades sharply on noisy and informal inputs. We also introduce DIVERS-CS, a diverse code-switching benchmark dataset spanning 10 language pairs, and show that existing models struggle to detect multiple languages within the same sentence. These results highlight the need for more robust and inclusive LID systems in real-world settings.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient &amp; Correct Predictive Equivalence for Decision Trees</title>
<link>https://arxiv.org/abs/2509.17774</link>
<guid>https://arxiv.org/abs/2509.17774</guid>
<content:encoded><![CDATA[
arXiv:2509.17774v1 Announce Type: cross 
Abstract: The Rashomon set of decision trees (DTs) finds importance uses. Recent work showed that DTs computing the same classification function, i.e. predictive equivalent DTs, can represent a significant fraction of the Rashomon set. Such redundancy is undesirable. For example, feature importance based on the Rashomon set becomes inaccurate due the existence of predictive equivalent DTs, i.e. DTs with the same prediction for every possible input. In recent work, McTavish et al. proposed solutions for several computational problems related with DTs, including that of deciding predictive equivalent DTs. This approach, which this paper refers to as MBDSR, consists of applying the well-known method of Quine-McCluskey (QM) for obtaining minimum-size DNF (disjunctive normal form) representations of DTs, which are then used for comparing DTs for predictive equivalence. Furthermore, the minimum-size DNF representation was also applied to computing explanations for the predictions made by DTs, and to finding predictions in the presence of missing data. However, the problem of formula minimization is hard for the second level of the polynomial hierarchy, and the QM method may exhibit worst-case exponential running time and space. This paper first demonstrates that there exist decision trees that trigger the worst-case exponential running time and space of the QM method. Second, the paper shows that the MBDSR approach can produce incorrect results for the problem of deciding predictive equivalence. Third, the paper shows that any of the problems to which the minimum-size DNF representation has been applied to can in fact be solved in polynomial time, in the size of the DT. The experiments confirm that, for DTs for which the the worst-case of the QM method is triggered, the algorithms proposed in this paper are orders of magnitude faster than the ones proposed by McTavish et al.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Affordable and Non-Invasive Detection of Hypoglycemia: A Machine Learning Approach</title>
<link>https://arxiv.org/abs/2509.17842</link>
<guid>https://arxiv.org/abs/2509.17842</guid>
<content:encoded><![CDATA[
arXiv:2509.17842v1 Announce Type: cross 
Abstract: Diabetes mellitus is a growing global health issue, with Type 1 Diabetes (T1D) requiring constant monitoring to avoid hypoglycemia. Although Continuous Glucose Monitors (CGMs) are effective, their cost and invasiveness limit access, particularly in low-resource settings. This paper proposes a non-invasive method to classify glycemic states using Galvanic Skin Response (GSR), a biosignal commonly captured by wearable sensors. We use the merged OhioT1DM 2018 and 2020 datasets to build a machine learning pipeline that detects hypoglycemia (glucose < 70 mg/dl) and normoglycemia (glucose > 70 mg/dl) with GSR alone. Seven models are trained and evaluated: Random Forest, XGBoost, MLP, CNN, LSTM, Logistic Regression, and K-Nearest Neighbors. Validation sets and 95% confidence intervals are reported to increase reliability and assess robustness. Results show that the LSTM model achieves a perfect hypoglycemia recall (1.00) with an F1-score confidence interval of [0.611-0.745], while XGBoost offers strong performance with a recall of 0.54 even under class imbalance. This approach highlights the potential for affordable, wearable-compatible glucose monitoring tools suitable for settings with limited CGM availability using GSR data.
  Index Terms: Hypoglycemia Detection, Galvanic Skin Response, Non Invasive Monitoring, Wearables, Machine Learning, Confidence Intervals.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Learning and Representation of Mandarin Tonal Categories by a Generative CNN</title>
<link>https://arxiv.org/abs/2509.17859</link>
<guid>https://arxiv.org/abs/2509.17859</guid>
<content:encoded><![CDATA[
arXiv:2509.17859v1 Announce Type: cross 
Abstract: This paper outlines the methodology for modeling tonal learning in fully unsupervised models of human language acquisition. Tonal patterns are among the computationally most complex learning objectives in language. We argue that a realistic generative model of human language (ciwGAN) can learn to associate its categorical variables with Mandarin Chinese tonal categories without any labeled data. All three trained models showed statistically significant differences in F0 across categorical variables. The model trained solely on male tokens consistently encoded tone. Our results sug- gest that not only does the model learn Mandarin tonal contrasts, but it learns a system that corresponds to a stage of acquisition in human language learners. We also outline methodology for tracing tonal representations in internal convolutional layers, which shows that linguistic tools can contribute to interpretability of deep learning and can ultimately be used in neural experiments.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Brainprint-Modulated Target Speaker Extraction</title>
<link>https://arxiv.org/abs/2509.17883</link>
<guid>https://arxiv.org/abs/2509.17883</guid>
<content:encoded><![CDATA[
arXiv:2509.17883v1 Announce Type: cross 
Abstract: Achieving robust and personalized performance in neuro-steered Target Speaker Extraction (TSE) remains a significant challenge for next-generation hearing aids. This is primarily due to two factors: the inherent non-stationarity of EEG signals across sessions, and the high inter-subject variability that limits the efficacy of generalized models. To address these issues, we propose Brainprint-Modulated Target Speaker Extraction (BM-TSE), a novel framework for personalized and high-fidelity extraction. BM-TSE first employs a spatio-temporal EEG encoder with an Adaptive Spectral Gain (ASG) module to extract stable features resilient to non-stationarity. The core of our framework is a personalized modulation mechanism, where a unified brainmap embedding is learned under the joint supervision of subject identification (SID) and auditory attention decoding (AAD) tasks. This learned brainmap, encoding both static user traits and dynamic attentional states, actively refines the audio separation process, dynamically tailoring the output to each user. Evaluations on the public KUL and Cocktail Party datasets demonstrate that BM-TSE achieves state-of-the-art performance, significantly outperforming existing methods. Our code is publicly accessible at: https://github.com/rosshan-orz/BM-TSE.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lipschitz-Based Robustness Certification for Recurrent Neural Networks via Convex Relaxation</title>
<link>https://arxiv.org/abs/2509.17898</link>
<guid>https://arxiv.org/abs/2509.17898</guid>
<content:encoded><![CDATA[
arXiv:2509.17898v1 Announce Type: cross 
Abstract: Robustness certification against bounded input noise or adversarial perturbations is increasingly important for deployment recurrent neural networks (RNNs) in safety-critical control applications. To address this challenge, we present RNN-SDP, a relaxation based method that models the RNN's layer interactions as a convex problem and computes a certified upper bound on the Lipschitz constant via semidefinite programming (SDP). We also explore an extension that incorporates known input constraints to further tighten the resulting Lipschitz bounds. RNN-SDP is evaluated on a synthetic multi-tank system, with upper bounds compared to empirical estimates. While incorporating input constraints yields only modest improvements, the general method produces reasonably tight and certifiable bounds, even as sequence length increases. The results also underscore the often underestimated impact of initialization errors, an important consideration for applications where models are frequently re-initialized, such as model predictive control (MPC).
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shilling Recommender Systems by Generating Side-feature-aware Fake User Profiles</title>
<link>https://arxiv.org/abs/2509.17918</link>
<guid>https://arxiv.org/abs/2509.17918</guid>
<content:encoded><![CDATA[
arXiv:2509.17918v1 Announce Type: cross 
Abstract: Recommender systems (RS) greatly influence users' consumption decisions, making them attractive targets for malicious shilling attacks that inject fake user profiles to manipulate recommendations. Existing shilling methods can generate effective and stealthy fake profiles when training data only contain rating matrix, but they lack comprehensive solutions for scenarios where side features are present and utilized by the recommender. To address this gap, we extend the Leg-UP framework by enhancing the generator architecture to incorporate side features, enabling the generation of side-feature-aware fake user profiles. Experiments on benchmarks show that our method achieves strong attack performance while maintaining stealthiness.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Random functions as data compressors for machine learning of molecular processes</title>
<link>https://arxiv.org/abs/2509.17937</link>
<guid>https://arxiv.org/abs/2509.17937</guid>
<content:encoded><![CDATA[
arXiv:2509.17937v1 Announce Type: cross 
Abstract: Machine learning (ML) is rapidly transforming the way molecular dynamics simulations are performed and analyzed, from materials modeling to studies of protein folding and function. ML algorithms are often employed to learn low-dimensional representations of conformational landscapes and to cluster trajectories into relevant metastable states. Most of these algorithms require selecting a small number of features that describe the problem of interest. Although deep neural networks can tackle large numbers of input features, the training costs increase with input size, which makes the selection of a subset of features mandatory for most problems of practical interest. Here, we show that random nonlinear projections can be used to compress large feature spaces and make computations faster without substantial loss of information. We describe an efficient way to produce random projections and then exemplify the general procedure for protein folding. For our test cases NTL9 and the double-norleucin variant of the villin headpiece, we find that random compression retains the core static and dynamic information of the original high dimensional feature space and makes trajectory analysis more robust.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ComposableNav: Instruction-Following Navigation in Dynamic Environments via Composable Diffusion</title>
<link>https://arxiv.org/abs/2509.17941</link>
<guid>https://arxiv.org/abs/2509.17941</guid>
<content:encoded><![CDATA[
arXiv:2509.17941v1 Announce Type: cross 
Abstract: This paper considers the problem of enabling robots to navigate dynamic environments while following instructions. The challenge lies in the combinatorial nature of instruction specifications: each instruction can include multiple specifications, and the number of possible specification combinations grows exponentially as the robot's skill set expands. For example, "overtake the pedestrian while staying on the right side of the road" consists of two specifications: "overtake the pedestrian" and "walk on the right side of the road." To tackle this challenge, we propose ComposableNav, based on the intuition that following an instruction involves independently satisfying its constituent specifications, each corresponding to a distinct motion primitive. Using diffusion models, ComposableNav learns each primitive separately, then composes them in parallel at deployment time to satisfy novel combinations of specifications unseen in training. Additionally, to avoid the onerous need for demonstrations of individual motion primitives, we propose a two-stage training procedure: (1) supervised pre-training to learn a base diffusion model for dynamic navigation, and (2) reinforcement learning fine-tuning that molds the base model into different motion primitives. Through simulation and real-world experiments, we show that ComposableNav enables robots to follow instructions by generating trajectories that satisfy diverse and unseen combinations of specifications, significantly outperforming both non-compositional VLM-based policies and costmap composing baselines. Videos and additional materials can be found on the project page: https://amrl.cs.utexas.edu/ComposableNav/
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can multimodal representation learning by alignment preserve modality-specific information?</title>
<link>https://arxiv.org/abs/2509.17943</link>
<guid>https://arxiv.org/abs/2509.17943</guid>
<content:encoded><![CDATA[
arXiv:2509.17943v1 Announce Type: cross 
Abstract: Combining multimodal data is a key issue in a wide range of machine learning tasks, including many remote sensing problems. In Earth observation, early multimodal data fusion methods were based on specific neural network architectures and supervised learning. Ever since, the scarcity of labeled data has motivated self-supervised learning techniques. State-of-the-art multimodal representation learning techniques leverage the spatial alignment between satellite data from different modalities acquired over the same geographic area in order to foster a semantic alignment in the latent space. In this paper, we investigate how this methods can preserve task-relevant information that is not shared across modalities. First, we show, under simplifying assumptions, when alignment strategies fundamentally lead to an information loss. Then, we support our theoretical insight through numerical experiments in more realistic settings. With those theoretical and empirical evidences, we hope to support new developments in contrastive learning for the combination of multimodal satellite data. Our code and data is publicly available at https://github.com/Romain3Ch216/alg_maclean_25.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Seeing Bones at Radio Frequency</title>
<link>https://arxiv.org/abs/2509.17979</link>
<guid>https://arxiv.org/abs/2509.17979</guid>
<content:encoded><![CDATA[
arXiv:2509.17979v1 Announce Type: cross 
Abstract: Wireless sensing literature has long aspired to achieve X-ray-like vision at radio frequencies. Yet, state-of-the-art wireless sensing literature has yet to generate the archetypal X-ray image: one of the bones beneath flesh. In this paper, we explore MCT, a penetration-based RF-imaging system for imaging bones at mm-resolution, one that significantly exceeds prior penetration-based RF imaging literature. Indeed the long wavelength, significant attenuation and complex diffraction that occur as RF propagates through flesh, have long limited imaging resolution (to several centimeters at best). We address these concerns through a novel penetration-based synthetic aperture algorithm, coupled with a learning-based pipeline to correct for diffraction-induced artifacts. A detailed evaluation of meat models demonstrates a resolution improvement from sub-decimeter to sub-centimeter over prior art in RF penetrative imaging.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReDepress: A Cognitive Framework for Detecting Depression Relapse from Social Media</title>
<link>https://arxiv.org/abs/2509.17991</link>
<guid>https://arxiv.org/abs/2509.17991</guid>
<content:encoded><![CDATA[
arXiv:2509.17991v1 Announce Type: cross 
Abstract: Almost 50% depression patients face the risk of going into relapse. The risk increases to 80% after the second episode of depression. Although, depression detection from social media has attained considerable attention, depression relapse detection has remained largely unexplored due to the lack of curated datasets and the difficulty of distinguishing relapse and non-relapse users. In this work, we present ReDepress, the first clinically validated social media dataset focused on relapse, comprising 204 Reddit users annotated by mental health professionals. Unlike prior approaches, our framework draws on cognitive theories of depression, incorporating constructs such as attention bias, interpretation bias, memory bias and rumination into both annotation and modeling. Through statistical analyses and machine learning experiments, we demonstrate that cognitive markers significantly differentiate relapse and non-relapse groups, and that models enriched with these features achieve competitive performance, with transformer-based temporal models attaining an F1 of 0.86. Our findings validate psychological theories in real-world textual data and underscore the potential of cognitive-informed computational methods for early relapse detection, paving the way for scalable, low-cost interventions in mental healthcare.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variation in Verification: Understanding Verification Dynamics in Large Language Models</title>
<link>https://arxiv.org/abs/2509.17995</link>
<guid>https://arxiv.org/abs/2509.17995</guid>
<content:encoded><![CDATA[
arXiv:2509.17995v1 Announce Type: cross 
Abstract: Recent advances have shown that scaling test-time computation enables large language models (LLMs) to solve increasingly complex problems across diverse domains. One effective paradigm for test-time scaling (TTS) involves LLM generators producing multiple solution candidates, with LLM verifiers assessing the correctness of these candidates without reference answers. In this paper, we study generative verifiers, which perform verification by generating chain-of-thought (CoT) reasoning followed by a binary verdict. We systematically analyze verification dynamics across three dimensions - problem difficulty, generator capability, and verifier generation capability - with empirical studies on 12 benchmarks across mathematical reasoning, knowledge, and natural language reasoning tasks using 14 open-source models (2B to 72B parameter range) and GPT-4o. Our experiments reveal three key findings about verification effectiveness: (1) Easy problems allow verifiers to more reliably certify correct responses; (2) Weak generators produce errors that are easier to detect than strong generators; (3) Verification ability is generally correlated with the verifier's own problem-solving capability, but this relationship varies with problem difficulty. These findings reveal opportunities to optimize basic verification strategies in TTS applications. First, given the same verifier, some weak generators can nearly match stronger ones in post-verification TTS performance (e.g., the Gemma2-9B to Gemma2-27B performance gap shrinks by 75.5%). Second, we identify cases where strong verifiers offer limited advantage over weak ones, as both fail to provide meaningful verification gains, suggesting that verifier scaling alone cannot overcome fundamental verification challenges.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Narcissus Hypothesis:Descending to the Rung of Illusion</title>
<link>https://arxiv.org/abs/2509.17999</link>
<guid>https://arxiv.org/abs/2509.17999</guid>
<content:encoded><![CDATA[
arXiv:2509.17999v1 Announce Type: cross 
Abstract: Modern foundational models increasingly reflect not just world knowledge, but patterns of human preference embedded in their training data. We hypothesize that recursive alignment-via human feedback and model-generated corpora-induces a social desirability bias, nudging models to favor agreeable or flattering responses over objective reasoning. We refer to it as the Narcissus Hypothesis and test it across 31 models using standardized personality assessments and a novel Social Desirability Bias score. Results reveal a significant drift toward socially conforming traits, with profound implications for corpus integrity and the reliability of downstream inferences. We then offer a novel epistemological interpretation, tracing how recursive bias may collapse higher-order reasoning down Pearl's Ladder of Causality, culminating in what we refer to as the Rung of Illusion.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building Transparency in Deep Learning-Powered Network Traffic Classification: A Traffic-Explainer Framework</title>
<link>https://arxiv.org/abs/2509.18007</link>
<guid>https://arxiv.org/abs/2509.18007</guid>
<content:encoded><![CDATA[
arXiv:2509.18007v1 Announce Type: cross 
Abstract: Recent advancements in deep learning have significantly enhanced the performance and efficiency of traffic classification in networking systems. However, the lack of transparency in their predictions and decision-making has made network operators reluctant to deploy DL-based solutions in production networks. To tackle this challenge, we propose Traffic-Explainer, a model-agnostic and input-perturbation-based traffic explanation framework. By maximizing the mutual information between predictions on original traffic sequences and their masked counterparts, Traffic-Explainer automatically uncovers the most influential features driving model predictions. Extensive experiments demonstrate that Traffic-Explainer improves upon existing explanation methods by approximately 42%. Practically, we further apply Traffic-Explainer to identify influential features and demonstrate its enhanced transparency across three critical tasks: application classification, traffic localization, and network cartography. For the first two tasks, Traffic-Explainer identifies the most decisive bytes that drive predicted traffic applications and locations, uncovering potential vulnerabilities and privacy concerns. In network cartography, Traffic-Explainer identifies submarine cables that drive the mapping of traceroute to physical path, enabling a traceroute-informed risk analysis.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust, Online, and Adaptive Decentralized Gaussian Processes</title>
<link>https://arxiv.org/abs/2509.18011</link>
<guid>https://arxiv.org/abs/2509.18011</guid>
<content:encoded><![CDATA[
arXiv:2509.18011v1 Announce Type: cross 
Abstract: Gaussian processes (GPs) offer a flexible, uncertainty-aware framework for modeling complex signals, but scale cubically with data, assume static targets, and are brittle to outliers, limiting their applicability in large-scale problems with dynamic and noisy environments. Recent work introduced decentralized random Fourier feature Gaussian processes (DRFGP), an online and distributed algorithm that casts GPs in an information-filter form, enabling exact sequential inference and fully distributed computation without reliance on a fusion center. In this paper, we extend DRFGP along two key directions: first, by introducing a robust-filtering update that downweights the impact of atypical observations; and second, by incorporating a dynamic adaptation mechanism that adapts to time-varying functions. The resulting algorithm retains the recursive information-filter structure while enhancing stability and accuracy. We demonstrate its effectiveness on a large-scale Earth system application, underscoring its potential for in-situ modeling.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fr\'echet Geodesic Boosting</title>
<link>https://arxiv.org/abs/2509.18013</link>
<guid>https://arxiv.org/abs/2509.18013</guid>
<content:encoded><![CDATA[
arXiv:2509.18013v1 Announce Type: cross 
Abstract: Gradient boosting has become a cornerstone of machine learning, enabling base learners such as decision trees to achieve exceptional predictive performance. While existing algorithms primarily handle scalar or Euclidean outputs, increasingly prevalent complex-structured data, such as distributions, networks, and manifold-valued outputs, present challenges for traditional methods. Such non-Euclidean data lack algebraic structures such as addition, subtraction, or scalar multiplication required by standard gradient boosting frameworks. To address these challenges, we introduce Fr\'echet geodesic boosting (FGBoost), a novel approach tailored for outputs residing in geodesic metric spaces. FGBoost leverages geodesics as proxies for residuals and constructs ensembles in a way that respects the intrinsic geometry of the output space. Through theoretical analysis, extensive simulations, and real-world applications, we demonstrate the strong performance and adaptability of FGBoost, showcasing its potential for modeling complex data.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Core-elements Subsampling for Alternating Least Squares</title>
<link>https://arxiv.org/abs/2509.18024</link>
<guid>https://arxiv.org/abs/2509.18024</guid>
<content:encoded><![CDATA[
arXiv:2509.18024v1 Announce Type: cross 
Abstract: In this paper, we propose a novel element-wise subset selection method for the alternating least squares (ALS) algorithm, focusing on low-rank matrix factorization involving matrices with missing values, as commonly encountered in recommender systems. While ALS is widely used for providing personalized recommendations based on user-item interaction data, its high computational cost, stemming from repeated regression operations, poses significant challenges for large-scale datasets. To enhance the efficiency of ALS, we propose a core-elements subsampling method that selects a representative subset of data and leverages sparse matrix operations to approximate ALS estimations efficiently. We establish theoretical guarantees for the approximation and convergence of the proposed approach, showing that it achieves similar accuracy with significantly reduced computational time compared to full-data ALS. Extensive simulations and real-world applications demonstrate the effectiveness of our method in various scenarios, emphasizing its potential in large-scale recommendation systems.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning as the Disciplined Construction of Tame Objects</title>
<link>https://arxiv.org/abs/2509.18025</link>
<guid>https://arxiv.org/abs/2509.18025</guid>
<content:encoded><![CDATA[
arXiv:2509.18025v1 Announce Type: cross 
Abstract: One can see deep-learning models as compositions of functions within the so-called tame geometry. In this expository note, we give an overview of some topics at the interface of tame geometry (also known as o-minimality), optimization theory, and deep learning theory and practice. To do so, we gradually introduce the concepts and tools used to build convergence guarantees for stochastic gradient descent in a general nonsmooth nonconvex, but tame, setting. This illustrates some ways in which tame geometry is a natural mathematical framework for the study of AI systems, especially within Deep Learning.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kernel K-means clustering of distributional data</title>
<link>https://arxiv.org/abs/2509.18037</link>
<guid>https://arxiv.org/abs/2509.18037</guid>
<content:encoded><![CDATA[
arXiv:2509.18037v1 Announce Type: cross 
Abstract: We consider the problem of clustering a sample of probability distributions from a random distribution on $\mathbb R^p$. Our proposed partitioning method makes use of a symmetric, positive-definite kernel $k$ and its associated reproducing kernel Hilbert space (RKHS) $\mathcal H$. By mapping each distribution to its corresponding kernel mean embedding in $\mathcal H$, we obtain a sample in this RKHS where we carry out the $K$-means clustering procedure, which provides an unsupervised classification of the original sample. The procedure is simple and computationally feasible even for dimension $p>1$. The simulation studies provide insight into the choice of the kernel and its tuning parameter. The performance of the proposed clustering procedure is illustrated on a collection of Synthetic Aperture Radar (SAR) images.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prepare Before You Act: Learning From Humans to Rearrange Initial States</title>
<link>https://arxiv.org/abs/2509.18043</link>
<guid>https://arxiv.org/abs/2509.18043</guid>
<content:encoded><![CDATA[
arXiv:2509.18043v1 Announce Type: cross 
Abstract: Imitation learning (IL) has proven effective across a wide range of manipulation tasks. However, IL policies often struggle when faced with out-of-distribution observations; for instance, when the target object is in a previously unseen position or occluded by other objects. In these cases, extensive demonstrations are needed for current IL methods to reach robust and generalizable behaviors. But when humans are faced with these sorts of atypical initial states, we often rearrange the environment for more favorable task execution. For example, a person might rotate a coffee cup so that it is easier to grasp the handle, or push a box out of the way so they can directly grasp their target object. In this work we seek to equip robot learners with the same capability: enabling robots to prepare the environment before executing their given policy. We propose ReSET, an algorithm that takes initial states -- which are outside the policy's distribution -- and autonomously modifies object poses so that the restructured scene is similar to training data. Theoretically, we show that this two step process (rearranging the environment before rolling out the given policy) reduces the generalization gap. Practically, our ReSET algorithm combines action-agnostic human videos with task-agnostic teleoperation data to i) decide when to modify the scene, ii) predict what simplifying actions a human would take, and iii) map those predictions into robot action primitives. Comparisons with diffusion policies, VLAs, and other baselines show that using ReSET to prepare the environment enables more robust task execution with equal amounts of total training data. See videos at our project website: https://reset2025paper.github.io/
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Functional effects models: Accounting for preference heterogeneity in panel data with machine learning</title>
<link>https://arxiv.org/abs/2509.18047</link>
<guid>https://arxiv.org/abs/2509.18047</guid>
<content:encoded><![CDATA[
arXiv:2509.18047v1 Announce Type: cross 
Abstract: In this paper, we present a general specification for Functional Effects Models, which use Machine Learning (ML) methodologies to learn individual-specific preference parameters from socio-demographic characteristics, therefore accounting for inter-individual heterogeneity in panel choice data. We identify three specific advantages of the Functional Effects Model over traditional fixed, and random/mixed effects models: (i) by mapping individual-specific effects as a function of socio-demographic variables, we can account for these effects when forecasting choices of previously unobserved individuals (ii) the (approximate) maximum-likelihood estimation of functional effects avoids the incidental parameters problem of the fixed effects model, even when the number of observed choices per individual is small; and (iii) we do not rely on the strong distributional assumptions of the random effects model, which may not match reality. We learn functional intercept and functional slopes with powerful non-linear machine learning regressors for tabular data, namely gradient boosting decision trees and deep neural networks. We validate our proposed methodology on a synthetic experiment and three real-world panel case studies, demonstrating that the Functional Effects Model: (i) can identify the true values of individual-specific effects when the data generation process is known; (ii) outperforms both state-of-the-art ML choice modelling techniques that omit individual heterogeneity in terms of predictive performance, as well as traditional static panel choice models in terms of learning inter-individual heterogeneity. The results indicate that the FI-RUMBoost model, which combines the individual-specific constants of the Functional Effects Model with the complex, non-linear utilities of RUMBoost, performs marginally best on large-scale revealed preference panel data.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Knowledge Graph-based Retrieval-Augmented Generation Framework for Algorithm Selection in the Facility Layout Problem</title>
<link>https://arxiv.org/abs/2509.18054</link>
<guid>https://arxiv.org/abs/2509.18054</guid>
<content:encoded><![CDATA[
arXiv:2509.18054v1 Announce Type: cross 
Abstract: Selecting a solution algorithm for the Facility Layout Problem (FLP), an NP-hard optimization problem with a multiobjective trade-off, is a complex task that requires deep expert knowledge. The performance of a given algorithm depends on specific problem characteristics such as its scale, objectives, and constraints. This creates a need for a data-driven recommendation method to guide algorithm selection in automated design systems. This paper introduces a new recommendation method to make such expertise accessible, based on a Knowledge Graph-based Retrieval-Augmented Generation (KG RAG) framework. To address this, a domain-specific knowledge graph is constructed from published literature. The method then employs a multi-faceted retrieval mechanism to gather relevant evidence from this knowledge graph using three distinct approaches, which include a precise graph-based search, flexible vector-based search, and high-level cluster-based search. The retrieved evidence is utilized by a Large Language Model (LLM) to generate algorithm recommendations with data-driven reasoning. The proposed KG-RAG method is compared against a commercial LLM chatbot with access to the knowledge base as a table, across a series of diverse, real-world FLP test cases. Based on recommendation accuracy and reasoning capability, the proposed method performed significantly better than the commercial LLM chatbot.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strategic Coordination for Evolving Multi-agent Systems: A Hierarchical Reinforcement and Collective Learning Approach</title>
<link>https://arxiv.org/abs/2509.18088</link>
<guid>https://arxiv.org/abs/2509.18088</guid>
<content:encoded><![CDATA[
arXiv:2509.18088v1 Announce Type: cross 
Abstract: Decentralized combinatorial optimization in evolving multi-agent systems poses significant challenges, requiring agents to balance long-term decision-making, short-term optimized collective outcomes, while preserving autonomy of interactive agents under unanticipated changes. Reinforcement learning offers a way to model sequential decision-making through dynamic programming to anticipate future environmental changes. However, applying multi-agent reinforcement learning (MARL) to decentralized combinatorial optimization problems remains an open challenge due to the exponential growth of the joint state-action space, high communication overhead, and privacy concerns in centralized training. To address these limitations, this paper proposes Hierarchical Reinforcement and Collective Learning (HRCL), a novel approach that leverages both MARL and decentralized collective learning based on a hierarchical framework. Agents take high-level strategies using MARL to group possible plans for action space reduction and constrain the agent behavior for Pareto optimality. Meanwhile, the low-level collective learning layer ensures efficient and decentralized coordinated decisions among agents with minimal communication. Extensive experiments in a synthetic scenario and real-world smart city application models, including energy self-management and drone swarm sensing, demonstrate that HRCL significantly improves performance, scalability, and adaptability compared to the standalone MARL and collective learning approaches, achieving a win-win synthesis solution.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEQR: Secure and Efficient QR-based LoRA Routing</title>
<link>https://arxiv.org/abs/2509.18093</link>
<guid>https://arxiv.org/abs/2509.18093</guid>
<content:encoded><![CDATA[
arXiv:2509.18093v1 Announce Type: cross 
Abstract: Low-Rank Adaptation (LoRA) has become a standard technique for parameter-efficient fine-tuning of large language models, enabling large libraries of LoRAs, each for a specific task or domain. Efficiently selecting the correct LoRA adapter for a given input remains a challenge, particularly in secure environments where supervised training of routers may raise privacy concerns. Motivated by previous approaches, we formalize the goal of unsupervised LoRA routing in terms of activation norm maximization, providing a theoretical framework for analysis. We demonstrate the discriminative power of activation norms and introduce SEQR, an unsupervised LoRA routing algorithm designed to maximize efficiency while providing strict routing guarantees. SEQR provably identifies the norm-maximizing adapter with significantly greater efficiency, making it a highly scalable and effective solution for dynamic LoRA composition. We validate our results through experiments that demonstrate improved multi-task performance and efficiency.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Optimization of Multi-Objective Reinforcement Learning with Policy Gradient Based Algorithm</title>
<link>https://arxiv.org/abs/2105.14125</link>
<guid>https://arxiv.org/abs/2105.14125</guid>
<content:encoded><![CDATA[
arXiv:2105.14125v2 Announce Type: replace 
Abstract: Many engineering problems have multiple objectives, and the overall aim is to optimize a non-linear function of these objectives. In this paper, we formulate the problem of maximizing a non-linear concave function of multiple long-term objectives. A policy-gradient based model-free algorithm is proposed for the problem. To compute an estimate of the gradient, a biased estimator is proposed. The proposed algorithm is shown to achieve convergence to within an $\epsilon$ of the global optima after sampling $\mathcal{O}(\frac{M^4\sigma^2}{(1-\gamma)^8\epsilon^4})$ trajectories where $\gamma$ is the discount factor and $M$ is the number of the agents, thus achieving the same dependence on $\epsilon$ as the policy gradient algorithm for the standard reinforcement learning.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-scale clustering and source separation of InSight mission seismic data</title>
<link>https://arxiv.org/abs/2305.16189</link>
<guid>https://arxiv.org/abs/2305.16189</guid>
<content:encoded><![CDATA[
arXiv:2305.16189v5 Announce Type: replace 
Abstract: Unsupervised source separation involves unraveling an unknown set of source signals recorded through a mixing operator, with limited prior knowledge about the sources, and only access to a dataset of signal mixtures. This problem is inherently ill-posed and is further challenged by the variety of timescales exhibited by sources in time series data from planetary space missions. As such, a systematic multi-scale unsupervised approach is needed to identify and separate sources at different timescales. Existing methods typically rely on a preselected window size that determines their operating timescale, limiting their capacity to handle multi-scale sources. To address this issue, we propose an unsupervised multi-scale clustering and source separation framework by leveraging wavelet scattering spectra that provide a low-dimensional representation of stochastic processes, capable of distinguishing between different non-Gaussian stochastic processes. Nested within this representation space, we develop a factorial variational autoencoder that is trained to probabilistically cluster sources at different timescales. To perform source separation, we use samples from clusters at multiple timescales obtained via the factorial variational autoencoder as prior information and formulate an optimization problem in the wavelet scattering spectra representation space. When applied to the entire seismic dataset recorded during the NASA InSight mission on Mars, containing sources varying greatly in timescale, our approach disentangles such different sources, e.g., minute-long transient one-sided pulses (known as "glitches") and structured ambient noises resulting from atmospheric activities that typically last for tens of minutes, and provides an opportunity to conduct further investigations into the isolated sources.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preserving Node-level Privacy in Graph Neural Networks</title>
<link>https://arxiv.org/abs/2311.06888</link>
<guid>https://arxiv.org/abs/2311.06888</guid>
<content:encoded><![CDATA[
arXiv:2311.06888v2 Announce Type: replace 
Abstract: Differential privacy (DP) has seen immense applications in learning on tabular, image, and sequential data where instance-level privacy is concerned. In learning on graphs, contrastingly, works on node-level privacy are highly sparse. Challenges arise as existing DP protocols hardly apply to the message-passing mechanism in Graph Neural Networks (GNNs).
  In this study, we propose a solution that specifically addresses the issue of node-level privacy. Our protocol consists of two main components: 1) a sampling routine called HeterPoisson, which employs a specialized node sampling strategy and a series of tailored operations to generate a batch of sub-graphs with desired properties, and 2) a randomization routine that utilizes symmetric multivariate Laplace (SML) noise instead of the commonly used Gaussian noise. Our privacy accounting shows this particular combination provides a non-trivial privacy guarantee. In addition, our protocol enables GNN learning with good performance, as demonstrated by experiments on five real-world datasets; compared with existing baselines, our method shows significant advantages, especially in the high privacy regime. Experimentally, we also 1) perform membership inference attacks against our protocol and 2) apply privacy audit techniques to confirm our protocol's privacy integrity.
  In the sequel, we present a study on a seemingly appealing approach \cite{sajadmanesh2023gap} (USENIX'23) that protects node-level privacy via differentially private node/instance embeddings. Unfortunately, such work has fundamental privacy flaws, which are identified through a thorough case study. More importantly, we prove an impossibility result of achieving both (strong) privacy and (acceptable) utility through private instance embedding. The implication is that such an approach has intrinsic utility barriers when enforcing differential privacy.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Practical Kernel Tests of Conditional Independence</title>
<link>https://arxiv.org/abs/2402.13196</link>
<guid>https://arxiv.org/abs/2402.13196</guid>
<content:encoded><![CDATA[
arXiv:2402.13196v2 Announce Type: replace 
Abstract: We describe a data-efficient, kernel-based approach to statistical testing of conditional independence. A major challenge of conditional independence testing is to obtain the correct test level (the specified upper bound on the rate of false positives), while still attaining competitive test power. Excess false positives arise due to bias in the test statistic, which is in our case obtained using nonparametric kernel ridge regression. We propose SplitKCI, an automated method for bias control for the Kernel-based Conditional Independence (KCI) test based on data splitting. We show that our approach significantly improves test level control for KCI without sacrificing test power, both theoretically and for synthetic and real-world data.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Approximating invariant functions with the sorting trick is theoretically justified</title>
<link>https://arxiv.org/abs/2403.01671</link>
<guid>https://arxiv.org/abs/2403.01671</guid>
<content:encoded><![CDATA[
arXiv:2403.01671v5 Announce Type: replace 
Abstract: Many machine learning models leverage group invariance which is enjoyed with a wide-range of applications. For exploiting an invariance structure, one common approach is known as \emph{frame averaging}. One popular example of frame averaging is the \emph{group averaging}, where the entire group is used to symmetrize a function. Another example is the \emph{canonicalization}, where a frame at each point consists of a single group element which transforms the point to its orbit representative, for example, sorting. Compared to group averaging, canonicalization is more efficient computationally. However, it results in non-differentiablity or discontinuity of the canonicalized function. As a result, the theoretical performance of canonicalization has not been given much attention. In this work, we establish an approximation theory for canonicalization. Specifically, we bound the point-wise and $L^2(\mathbb{P})$ approximation errors as well as the eigenvalue decay rates associated with a canonicalization trick applied to reproducing kernels. We discuss two key insights from our theoretical analyses and why they point to an interesting future research direction on how one can choose a design to fully leverage canonicalization in practice.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing the Inconsistency in Bayesian Deep Learning via Generalized Laplace Approximation</title>
<link>https://arxiv.org/abs/2405.13535</link>
<guid>https://arxiv.org/abs/2405.13535</guid>
<content:encoded><![CDATA[
arXiv:2405.13535v5 Announce Type: replace 
Abstract: In recent years, inconsistency in Bayesian deep learning has attracted significant attention. Tempered or generalized posterior distributions are frequently employed as direct and effective solutions. Nonetheless, the underlying mechanisms and the effectiveness of generalized posteriors remain active research topics. In this work, we interpret posterior tempering as a correction for model misspecification via adjustments to the joint probability, and as a recalibration of priors by reducing aleatoric uncertainty. We also introduce the generalized Laplace approximation, which requires only a simple modification to the Hessian calculation of the regularized loss and provides a flexible and scalable framework for high-quality posterior inference. We evaluate the proposed method on state-of-the-art neural networks and real-world datasets, demonstrating that the generalized Laplace approximation enhances predictive performance.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Edge-Specific Node Features through Co-Representation Neural Hypergraph Diffusion</title>
<link>https://arxiv.org/abs/2405.14286</link>
<guid>https://arxiv.org/abs/2405.14286</guid>
<content:encoded><![CDATA[
arXiv:2405.14286v3 Announce Type: replace 
Abstract: Hypergraphs are widely being employed to represent complex higher-order relations in real-world applications. Most existing research on hypergraph learning focuses on node-level or edge-level tasks. A practically relevant and more challenging task, edge-dependent node classification (ENC), is still under-explored. In ENC, a node can have different labels across different hyperedges, which requires the modeling of node features unique to each hyperedge. The state-of-the-art ENC solution, WHATsNet, only outputs single node and edge representations, leading to the limitations of \textbf{entangled edge-specific features} and \textbf{non-adaptive representation sizes} when applied to ENC. Additionally, WHATsNet suffers from the common \textbf{oversmoothing issue} in most HGNNs. To address these limitations, we propose \textbf{CoNHD}, a novel HGNN architecture specifically designed to model edge-specific features for ENC. Instead of learning separate representations for nodes and edges, CoNHD reformulates within-edge and within-node interactions as a hypergraph diffusion process over node-edge co-representations. We develop a neural implementation of the proposed diffusion process, leveraging equivariant networks as diffusion operators to effectively learn the diffusion dynamics from data. Extensive experiments demonstrate that CoNHD achieves the best performance across all benchmark ENC datasets and several downstream tasks without sacrificing efficiency. Our implementation is available at https://github.com/zhengyijia/CoNHD.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Expressive Power of Graph Neural Networks for (Mixed-Integer) Quadratic Programs</title>
<link>https://arxiv.org/abs/2406.05938</link>
<guid>https://arxiv.org/abs/2406.05938</guid>
<content:encoded><![CDATA[
arXiv:2406.05938v2 Announce Type: replace 
Abstract: Quadratic programming (QP) is the most widely applied category of problems in nonlinear programming. Many applications require real-time/fast solutions, though not necessarily with high precision. Existing methods either involve matrix decomposition or use the preconditioned conjugate gradient method. For relatively large instances, these methods cannot achieve the real-time requirement unless there is an effective preconditioner. Recently, graph neural networks (GNNs) opened new possibilities for QP. Some promising empirical studies of applying GNNs for QP tasks show that GNNs can capture key characteristics of an optimization instance and provide adaptive guidance accordingly to crucial configurations during the solving process, or directly provide an approximate solution. However, the theoretical understanding of GNNs in this context remains limited. Specifically, it is unclear what GNNs can and cannot achieve for QP tasks in theory. This work addresses this gap in the context of linearly constrained QP tasks. In the continuous setting, we prove that message-passing GNNs can universally represent fundamental properties of convex quadratic programs, including feasibility, optimal objective values, and optimal solutions. In the more challenging mixed-integer setting, while GNNs are not universal approximators, we identify a subclass of QP problems that GNNs can reliably represent.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reflecting on the State of Rehearsal-free Continual Learning with Pretrained Models</title>
<link>https://arxiv.org/abs/2406.09384</link>
<guid>https://arxiv.org/abs/2406.09384</guid>
<content:encoded><![CDATA[
arXiv:2406.09384v2 Announce Type: replace 
Abstract: With the advent and recent ubiquity of foundation models, continual learning (CL) has recently shifted from continual training from scratch to the continual adaptation of pretrained models, seeing particular success on rehearsal-free CL benchmarks (RFCL). To achieve this, most proposed methods adapt and restructure parameter-efficient finetuning techniques (PEFT) to suit the continual nature of the problem. Based most often on input-conditional query-mechanisms or regularizations on top of prompt- or adapter-based PEFT, these PEFT-style RFCL (P-RFCL) approaches report peak performances; often convincingly outperforming existing CL techniques. However, on the other end, critical studies have recently highlighted competitive results by training on just the first task or via simple non-parametric baselines. Consequently, questions arise about the relationship between methodological choices in P-RFCL and their reported high benchmark scores. In this work, we tackle these questions to better understand the true drivers behind strong P-RFCL performances, their placement w.r.t. recent first-task adaptation studies, and their relation to preceding CL standards such as EWC or SI. In particular, we show: (1) P-RFCL techniques relying on input-conditional query mechanisms work not because, but rather despite them by collapsing towards standard PEFT shortcut solutions. (2) Indeed, we show how most often, P-RFCL techniques can be matched by a simple and lightweight PEFT baseline. (3) Using this baseline, we identify the implicit bound on tunable parameters when deriving RFCL approaches from PEFT methods as a potential denominator behind P-RFCL efficacy. Finally, we (4) better disentangle continual versus first-task adaptation, and (5) motivate standard RFCL techniques s.a. EWC or SI in light of recent P-RFCL methods.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DCoM: Active Learning for All Learners</title>
<link>https://arxiv.org/abs/2407.01804</link>
<guid>https://arxiv.org/abs/2407.01804</guid>
<content:encoded><![CDATA[
arXiv:2407.01804v3 Announce Type: replace 
Abstract: Deep Active Learning (AL) techniques can be effective in reducing annotation costs for training deep models. However, their effectiveness in low- and high-budget scenarios seems to require different strategies, and achieving optimal results across varying budget scenarios remains a challenge. In this study, we introduce Dynamic Coverage & Margin mix (DCoM), a novel active learning approach designed to bridge this gap. Unlike existing strategies, DCoM dynamically adjusts its strategy, considering the competence of the current model. Through theoretical analysis and empirical evaluations on diverse datasets, including challenging computer vision tasks, we demonstrate DCoM's ability to overcome the cold start problem and consistently improve results across different budgetary constraints. Thus DCoM achieves state-of-the-art performance in both low- and high-budget regimes.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Theoretical Insights into CycleGAN: Analyzing Approximation and Estimation Errors in Unpaired Data Generation</title>
<link>https://arxiv.org/abs/2407.11678</link>
<guid>https://arxiv.org/abs/2407.11678</guid>
<content:encoded><![CDATA[
arXiv:2407.11678v3 Announce Type: replace 
Abstract: In this paper, we focus on analyzing the excess risk of the unpaired data generation model, called CycleGAN. Unlike classical GANs, CycleGAN not only transforms data between two unpaired distributions but also ensures the mappings are consistent, which is encouraged by the cycle-consistency term unique to CycleGAN. The increasing complexity of model structure and the addition of the cycle-consistency term in CycleGAN present new challenges for error analysis. By considering the impact of both the model architecture and training procedure, the risk is decomposed into two terms: approximation error and estimation error. These two error terms are analyzed separately and ultimately combined by considering the trade-off between them. Each component is rigorously analyzed; the approximation error through constructing approximations of the optimal transport maps, and the estimation error through establishing an upper bound using Rademacher complexity. Our analysis not only isolates these errors but also explores the trade-offs between them, which provides a theoretical insights of how CycleGAN's architecture and training procedures influence its performance.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Downlink Channel Covariance Matrix Estimation via Representation Learning with Graph Regularization</title>
<link>https://arxiv.org/abs/2407.18865</link>
<guid>https://arxiv.org/abs/2407.18865</guid>
<content:encoded><![CDATA[
arXiv:2407.18865v5 Announce Type: replace 
Abstract: In this paper, we propose an algorithm for downlink (DL) channel covariance matrix (CCM) estimation for frequency division duplexing (FDD) massive multiple-input multiple-output (MIMO) communication systems with base station (BS) possessing a uniform linear array (ULA) antenna structure. We consider a setting where the UL CCM is mapped to DL CCM by a mapping function. We first present a theoretical error analysis of learning a nonlinear embedding by constructing a mapping function, which points to the importance of the Lipschitz regularity of the mapping function for achieving high estimation performance. Then, based on the theoretical ground, we propose a representation learning algorithm as a solution for the estimation problem, where Gaussian RBF kernel interpolators are chosen to map UL CCMs to their DL counterparts. The proposed algorithm is based on the optimization of an objective function that fits a regression model between the DL CCM and UL CCM samples in the training dataset and preserves the local geometric structure of the data in the UL CCM space, while explicitly regulating the Lipschitz continuity of the mapping function in light of our theoretical findings. The proposed algorithm surpasses benchmark methods in terms of three error metrics as shown by simulations.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified Framework for Pre-trained Neural Network Compression via Decomposition and Optimized Rank Selection</title>
<link>https://arxiv.org/abs/2409.03555</link>
<guid>https://arxiv.org/abs/2409.03555</guid>
<content:encoded><![CDATA[
arXiv:2409.03555v2 Announce Type: replace 
Abstract: Despite their high accuracy, complex neural networks demand significant computational resources, posing challenges for deployment on resource constrained devices such as mobile phones and embedded systems. Compression algorithms have been developed to address these challenges by reducing model size and computational demands while maintaining accuracy. Among these approaches, factorization methods based on tensor decomposition are theoretically sound and effective. However, they face difficulties in selecting the appropriate rank for decomposition. This paper tackles this issue by presenting a unified framework that simultaneously applies decomposition and rank selection, employing a composite compression loss within defined rank constraints. Our method includes an automatic rank search in a continuous space, efficiently identifying optimal rank configurations for the pre-trained model by eliminating the need for additional training data and reducing computational overhead in the search step. Combined with a subsequent fine-tuning step, our approach maintains the performance of highly compressed models on par with their original counterparts. Using various benchmark datasets and models, we demonstrate the efficacy of our method through a comprehensive analysis.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Convergence Rates of Federated Q-Learning across Heterogeneous Environments</title>
<link>https://arxiv.org/abs/2409.03897</link>
<guid>https://arxiv.org/abs/2409.03897</guid>
<content:encoded><![CDATA[
arXiv:2409.03897v2 Announce Type: replace 
Abstract: Large-scale multi-agent systems are often deployed across wide geographic areas, where agents interact with heterogeneous environments. There is an emerging interest in understanding the role of heterogeneity in the performance of the federated versions of classic reinforcement learning algorithms. In this paper, we study synchronous federated Q-learning, which aims to learn an optimal Q-function by having $K$ agents average their local Q-estimates per $E$ iterations. We observe an interesting phenomenon on the convergence speeds in terms of $K$ and $E$. Similar to the homogeneous environment settings, there is a linear speed-up concerning $K$ in reducing the errors that arise from sampling randomness. Yet, in sharp contrast to the homogeneous settings, $E>1$ leads to significant performance degradation. Specifically, we provide a fine-grained characterization of the error evolution in the presence of environmental heterogeneity, which decay to zero as the number of iterations $T$ increases. The slow convergence of having $E>1$ turns out to be fundamental rather than an artifact of our analysis. We prove that, for a wide range of stepsizes, the $\ell_{\infty}$ norm of the error cannot decay faster than $\Theta (E/T)$. In addition, our experiments demonstrate that the convergence exhibits an interesting two-phase phenomenon. For any given stepsize, there is a sharp phase-transition of the convergence: the error decays rapidly in the beginning yet later bounces up and stabilizes. Provided that the phase-transition time can be estimated, choosing different stepsizes for the two phases leads to faster overall convergence.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Reinforcement Learning with Dynamic Distortion Risk Measures</title>
<link>https://arxiv.org/abs/2409.10096</link>
<guid>https://arxiv.org/abs/2409.10096</guid>
<content:encoded><![CDATA[
arXiv:2409.10096v3 Announce Type: replace 
Abstract: In a reinforcement learning (RL) setting, the agent's optimal strategy heavily depends on her risk preferences and the underlying model dynamics of the training environment. These two aspects influence the agent's ability to make well-informed and time-consistent decisions when facing testing environments. In this work, we devise a framework to solve robust risk-aware RL problems where we simultaneously account for environmental uncertainty and risk with a class of dynamic robust distortion risk measures. Robustness is introduced by considering all models within a Wasserstein ball around a reference model. We estimate such dynamic robust risk measures using neural networks by making use of strictly consistent scoring functions, derive policy gradient formulae using the quantile representation of distortion risk measures, and construct an actor-critic algorithm to solve this class of robust risk-aware RL problems. We demonstrate the performance of our algorithm on a portfolio allocation example.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MobiZO: Enabling Efficient LLM Fine-Tuning at the Edge via Inference Engines</title>
<link>https://arxiv.org/abs/2409.15520</link>
<guid>https://arxiv.org/abs/2409.15520</guid>
<content:encoded><![CDATA[
arXiv:2409.15520v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) are currently pre-trained and fine-tuned on large cloud servers. The next frontier is LLM personalization, where a foundation model can be fine-tuned with user/task-specific data. Given the sensitive nature of such private data, it is desirable to fine-tune these models on edge devices to improve user trust. However, fine-tuning on resource-constrained edge devices presents significant challenges due to substantial memory and computational demands, as well as limited infrastructure support. We observe that inference engines (e.g., ExecuTorch) can be repurposed for fine-tuning by leveraging zeroth-order (ZO) optimization, which uses multiple forward passes to approximate gradients. While promising, direct application of ZO methods on edge devices is inefficient due to the high computational cost of multiple forward passes required for accurate gradient estimation, and their deployment has been largely unexplored in practice. We introduce MobiZO, a resource-efficient fine-tuning framework for LLMs specifically designed for edge devices. MobiZO combines three key innovations: (1) a parallelized randomized gradient estimator that employs both outer-loop and inner-loop parallelism to eliminate sequential forward passes, (2) a specialized Multi-Perturbed LoRA (MP-LoRA) module that enables efficient realization of both inner and outer loop parallelism, and (3) a seamless integration with ExecuTorch for on-device training, requiring no modifications to the runtime. Experiments demonstrate that MobiZO achieves substantial runtime speedups and memory savings while improving fine-tuning accuracy, paving the way for practical deployment of LLMs in real-time, on-device applications.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DimINO: Dimension-Informed Neural Operator Learning</title>
<link>https://arxiv.org/abs/2410.05894</link>
<guid>https://arxiv.org/abs/2410.05894</guid>
<content:encoded><![CDATA[
arXiv:2410.05894v4 Announce Type: replace 
Abstract: In computational physics, a longstanding challenge lies in finding numerical solutions to partial differential equations (PDEs). Recently, research attention has increasingly focused on Neural Operator methods, which are notable for their ability to approximate operators-mappings between functions. Although neural operators benefit from a universal approximation theorem, achieving reliable error bounds often necessitates large model architectures, such as deep stacks of Fourier layers. This raises a natural question: Can we design lightweight models without sacrificing generalization? To address this, we introduce DimINO (Dimension-Informed Neural Operators), a framework inspired by dimensional analysis. DimINO incorporates two key components, DimNorm and a redimensionalization operation, which can be seamlessly integrated into existing neural operator architectures. These components enhance the model's ability to generalize across datasets with varying physical parameters. Theoretically, we establish a universal approximation theorem for DimINO and prove that it satisfies a critical property we term Similar Transformation Invariance (STI). Empirically, DimINO achieves up to 76.3% performance gain on PDE datasets while exhibiting clear evidence of the STI property.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Learn with Contrastive Meta-Objective</title>
<link>https://arxiv.org/abs/2410.05975</link>
<guid>https://arxiv.org/abs/2410.05975</guid>
<content:encoded><![CDATA[
arXiv:2410.05975v3 Announce Type: replace 
Abstract: Meta-learning enables learning systems to adapt quickly to new tasks, similar to humans. Different meta-learning approaches all work under/with the mini-batch episodic training framework. Such framework naturally gives the information about task identity, which can serve as additional supervision for meta-training to improve generalizability. We propose to exploit task identity as additional supervision in meta-training, inspired by the alignment and discrimination ability which is is intrinsic in human's fast learning. This is achieved by contrasting what meta-learners learn, i.e., model representations. The proposed ConML is evaluating and optimizing the contrastive meta-objective under a problem- and learner-agnostic meta-training framework. We demonstrate that ConML integrates seamlessly with existing meta-learners, as well as in-context learning models, and brings significant boost in performance with small implementation cost.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Sample-Efficiency and Generalization of Transfer and Inverse Reinforcement Learning: A Comprehensive Literature Review</title>
<link>https://arxiv.org/abs/2411.10268</link>
<guid>https://arxiv.org/abs/2411.10268</guid>
<content:encoded><![CDATA[
arXiv:2411.10268v2 Announce Type: replace 
Abstract: Reinforcement learning (RL) is a sub-domain of machine learning, mainly concerned with solving sequential decision-making problems by a learning agent that interacts with the decision environment to improve its behavior through the reward it receives from the environment. This learning paradigm is, however, well-known for being time-consuming due to the necessity of collecting a large amount of data, making RL suffer from sample inefficiency and difficult generalization. Furthermore, the construction of an explicit reward function that accounts for the trade-off between multiple desiderata of a decision problem is often a laborious task. These challenges have been recently addressed utilizing transfer and inverse reinforcement learning (T-IRL). In this regard, this paper is devoted to a comprehensive review of realizing the sample efficiency and generalization of RL algorithms through T-IRL. Following a brief introduction to RL, the fundamental T-IRL methods are presented and the most recent advancements in each research field have been extensively reviewed. Our findings denote that a majority of recent research works have dealt with the aforementioned challenges by utilizing human-in-the-loop and sim-to-real strategies for the efficient transfer of knowledge from source domains to the target domain under the transfer learning scheme. Under the IRL structure, training schemes that require a low number of experience transitions and extension of such frameworks to multi-agent and multi-intention problems have been the priority of researchers in recent years.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Asynchronous Federated Learning: A Scalable Approach for Decentralized Machine Learning</title>
<link>https://arxiv.org/abs/2412.17723</link>
<guid>https://arxiv.org/abs/2412.17723</guid>
<content:encoded><![CDATA[
arXiv:2412.17723v3 Announce Type: replace 
Abstract: Federated Learning (FL) has emerged as a powerful paradigm for decentralized machine learning, enabling collaborative model training across diverse clients without sharing raw data. However, traditional FL approaches often face limitations in scalability and efficiency due to their reliance on synchronous client updates, which can result in significant delays and increased communication overhead, particularly in heterogeneous and dynamic environments. To address these challenges in this paper, we propose an Asynchronous Federated Learning (AFL) algorithm, which allows clients to update the global model independently and asynchronously. Our key contributions include a comprehensive convergence analysis of AFL in the presence of client delays and model staleness. By leveraging martingale difference sequence theory and variance bounds, we ensure robust convergence despite asynchronous updates. Assuming strongly convex local objective functions, we establish bounds on gradient variance under random client sampling and derive a recursion formula quantifying the impact of client delays on convergence. Furthermore, we demonstrate the practical applicability of the AFL algorithm by training decentralized linear regression and Support Vector Machine (SVM) based classifiers and compare its results with synchronous FL algorithm to effectively handling non-IID data distributed among clients. The proposed AFL algorithm addresses key limitations of traditional FL methods, such as inefficiency due to global synchronization and susceptibility to client drift. It enhances scalability, robustness, and efficiency in real-world settings with heterogeneous client populations and dynamic network conditions. Our results underscore the potential of AFL to drive advancements indistributed learning systems, particularly for large-scale, privacy-preserving applications in resource-constrained environments.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learn to Optimize Resource Allocation under QoS Constraint of AR</title>
<link>https://arxiv.org/abs/2501.16186</link>
<guid>https://arxiv.org/abs/2501.16186</guid>
<content:encoded><![CDATA[
arXiv:2501.16186v2 Announce Type: replace 
Abstract: This paper studies the uplink and downlink power allocation for interactive augmented reality (AR) services, where the live video captured by an AR device is uploaded to the network edge, and then the augmented video is subsequently downloaded. By modeling the AR transmission process as a tandem queuing system, we derive an upper bound for the probabilistic quality of service (QoS) requirement concerning end-to-end latency and reliability. The resource allocation under the QoS requirement results in a functional optimization problem. To address it, we design a deep neural network to learn the power allocation policy, leveraging the optimal power allocation structure to enhance learning performance. Simulation results demonstrate that the proposed method effectively reduces transmit power while meeting the QoS requirement.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COMPOL: A Unified Neural Operator Framework for Scalable Multi-Physics Simulations</title>
<link>https://arxiv.org/abs/2501.17296</link>
<guid>https://arxiv.org/abs/2501.17296</guid>
<content:encoded><![CDATA[
arXiv:2501.17296v3 Announce Type: replace 
Abstract: Multiphysics simulations play an essential role in accurately modeling complex interactions across diverse scientific and engineering domains Although neural operators especially the Fourier Neural Operator FNO have significantly improved computational efficiency they often fail to effectively capture intricate correlations inherent in coupled physical processes To address this limitation we introduce COMPOL a novel coupled multiphysics operator learning framework COMPOL extends conventional operator architectures by incorporating sophisticated recurrent and attentionbased aggregation mechanisms effectively modeling interdependencies among interacting physical processes within latent feature spaces Our approach is architectureagnostic and seamlessly integrates into various neural operator frameworks that involve latent space transformations Extensive experiments on diverse benchmarksincluding biological reactiondiffusion systems patternforming chemical reactions multiphase geological flows and thermohydromechanical processes demonstrate that COMPOL consistently achieves superior predictive accuracy compared to stateoftheart methods.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Test-Time Training Scaling Laws for Chemical Exploration in Drug Design</title>
<link>https://arxiv.org/abs/2501.19153</link>
<guid>https://arxiv.org/abs/2501.19153</guid>
<content:encoded><![CDATA[
arXiv:2501.19153v3 Announce Type: replace 
Abstract: Chemical Language Models (CLMs) leveraging reinforcement learning (RL) have shown promise in de novo molecular design, yet often suffer from mode collapse, limiting their exploration capabilities. Inspired by Test-Time Training (TTT) in large language models, we propose scaling TTT for CLMs to enhance chemical space exploration. We introduce MolExp, a novel benchmark emphasizing the discovery of structurally diverse molecules with similar bioactivity, simulating real-world drug design challenges. Our results demonstrate that scaling TTT by increasing the number of independent RL agents follows a log-linear scaling law, significantly improving exploration efficiency as measured by MolExp. In contrast, increasing TTT training time yields diminishing returns, even with exploration bonuses. We further evaluate cooperative RL strategies to enhance exploration efficiency. These findings provide a scalable framework for generative molecular design, offering insights into optimizing AI-driven drug discovery.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Fused State Representations for Control from Multi-View Observations</title>
<link>https://arxiv.org/abs/2502.01316</link>
<guid>https://arxiv.org/abs/2502.01316</guid>
<content:encoded><![CDATA[
arXiv:2502.01316v4 Announce Type: replace 
Abstract: Multi-View Reinforcement Learning (MVRL) seeks to provide agents with multi-view observations, enabling them to perceive environment with greater effectiveness and precision. Recent advancements in MVRL focus on extracting latent representations from multiview observations and leveraging them in control tasks. However, it is not straightforward to learn compact and task-relevant representations, particularly in the presence of redundancy, distracting information, or missing views. In this paper, we propose Multi-view Fusion State for Control (MFSC), firstly incorporating bisimulation metric learning into MVRL to learn task-relevant representations. Furthermore, we propose a multiview-based mask and latent reconstruction auxiliary task that exploits shared information across views and improves MFSC's robustness in missing views by introducing a mask token. Extensive experimental results demonstrate that our method outperforms existing approaches in MVRL tasks. Even in more realistic scenarios with interference or missing views, MFSC consistently maintains high performance.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRADIEND: Feature Learning within Neural Networks Exemplified through Biases</title>
<link>https://arxiv.org/abs/2502.01406</link>
<guid>https://arxiv.org/abs/2502.01406</guid>
<content:encoded><![CDATA[
arXiv:2502.01406v3 Announce Type: replace 
Abstract: AI systems frequently exhibit and amplify social biases, leading to harmful consequences in critical areas. This study introduces a novel encoder-decoder approach that leverages model gradients to learn a feature neuron encoding societal bias information such as gender, race, and religion. We show that our method can not only identify which weights of a model need to be changed to modify a feature, but even demonstrate that this can be used to rewrite models to debias them while maintaining other capabilities. We demonstrate the effectiveness of our approach across various model architectures and highlight its potential for broader applications.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Federated Finetuning of LLMs via Alternating Optimization of LoRA</title>
<link>https://arxiv.org/abs/2502.01755</link>
<guid>https://arxiv.org/abs/2502.01755</guid>
<content:encoded><![CDATA[
arXiv:2502.01755v3 Announce Type: replace 
Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods like Low-Rank Adaptation (LoRA) optimize federated training by reducing computational and communication costs. We propose RoLoRA, a federated framework using alternating optimization to fine-tune LoRA adapters. Our approach emphasizes the importance of learning up and down projection matrices to enhance expressiveness and robustness. We use both theoretical analysis and extensive experiments to demonstrate the advantages of RoLoRA over prior approaches that either generate imperfect model updates or limit expressiveness of the model. We provide a theoretical analysis on a linear model to highlight the importance of learning both the down-projection and up-projection matrices in LoRA. We validate the insights on a non-linear model and separately provide a convergence proof under general conditions. To bridge theory and practice, we conducted extensive experimental evaluations on language models including RoBERTa-Large, Llama-2-7B on diverse tasks and FL settings to demonstrate the advantages of RoLoRA over other methods.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MPIC: Position-Independent Multimodal Context Caching System for Efficient MLLM Serving</title>
<link>https://arxiv.org/abs/2502.01960</link>
<guid>https://arxiv.org/abs/2502.01960</guid>
<content:encoded><![CDATA[
arXiv:2502.01960v2 Announce Type: replace 
Abstract: The context caching technique is employed to accelerate the Multimodal Large Language Model (MLLM) inference by prevailing serving platforms currently. However, this approach merely reuses the Key-Value (KV) cache of the initial sequence of prompt, resulting in full KV cache recomputation even if the prefix differs slightly. This becomes particularly inefficient in the context of interleaved text and images, as well as multimodal retrieval-augmented generation. This paper proposes position-independent caching as a more effective approach for multimodal information management. We have designed and implemented a caching system, named MPIC, to address both system-level and algorithm-level challenges. MPIC stores the KV cache on local disks when receiving multimodal data, and calculates and loads the KV cache in parallel during inference. To mitigate accuracy degradation, we have incorporated the integrated reuse and recompute mechanism within the system. The experimental results demonstrate that MPIC can achieve up to 54\% reduction in response time and 2$\times$ improvement in throughput compared to existing context caching systems, while maintaining negligible or no accuracy loss.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flatten Graphs as Sequences: Transformers are Scalable Graph Generators</title>
<link>https://arxiv.org/abs/2502.02216</link>
<guid>https://arxiv.org/abs/2502.02216</guid>
<content:encoded><![CDATA[
arXiv:2502.02216v2 Announce Type: replace 
Abstract: We introduce AutoGraph, a scalable autoregressive model for attributed graph generation using decoder-only transformers. By flattening graphs into random sequences of tokens through a reversible process, AutoGraph enables modeling graphs as sequences without relying on additional node features that are expensive to compute, in contrast to diffusion-based approaches. This results in sampling complexity and sequence lengths that scale optimally linearly with the number of edges, making it scalable and efficient for large, sparse graphs. A key success factor of AutoGraph is that its sequence prefixes represent induced subgraphs, creating a direct link to sub-sentences in language modeling. Empirically, AutoGraph achieves state-of-the-art performance on synthetic and molecular benchmarks, with up to 100x faster generation and 3x faster training than leading diffusion models. It also supports substructure-conditioned generation without fine-tuning and shows promising transferability, bridging language modeling and graph generation to lay the groundwork for graph foundation models. Our code is available at https://github.com/BorgwardtLab/AutoGraph.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Seamless Hierarchical Federated Learning under Intermittent Client Participation: A Stagewise Decision-Making Methodology</title>
<link>https://arxiv.org/abs/2502.09303</link>
<guid>https://arxiv.org/abs/2502.09303</guid>
<content:encoded><![CDATA[
arXiv:2502.09303v3 Announce Type: replace 
Abstract: Federated Learning (FL) offers a pioneering distributed learning paradigm that enables devices/clients to build a shared global model. This global model is obtained through frequent model transmissions between clients and a central server, which may cause high latency, energy consumption, and congestion over backhaul links. To overcome these drawbacks, Hierarchical Federated Learning (HFL) has emerged, which organizes clients into multiple clusters and utilizes edge nodes (e.g., edge servers) for intermediate model aggregations between clients and the central server. Current research on HFL mainly focus on enhancing model accuracy, latency, and energy consumption in scenarios with a stable/fixed set of clients. However, addressing the dynamic availability of clients -- a critical aspect of real-world scenarios -- remains underexplored. This study delves into optimizing client selection and client-to-edge associations in HFL under intermittent client participation so as to minimize overall system costs (i.e., delay and energy), while achieving fast model convergence. We unveil that achieving this goal involves solving a complex NP-hard problem. To tackle this, we propose a stagewise methodology that splits the solution into two stages, referred to as Plan A and Plan B. Plan A focuses on identifying long-term clients with high chance of participation in subsequent model training rounds. Plan B serves as a backup, selecting alternative clients when long-term clients are unavailable during model training rounds. This stagewise methodology offers a fresh perspective on client selection that can enhance both HFL and conventional FL via enabling low-overhead decision-making processes. Through evaluations on MNIST and CIFAR-10 datasets, we show that our methodology outperforms existing benchmarks in terms of model accuracy and system costs.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comprehensive Review of Neural Differential Equations for Time Series Analysis</title>
<link>https://arxiv.org/abs/2502.09885</link>
<guid>https://arxiv.org/abs/2502.09885</guid>
<content:encoded><![CDATA[
arXiv:2502.09885v3 Announce Type: replace 
Abstract: Time series modeling and analysis have become critical in various domains. Conventional methods such as RNNs and Transformers, while effective for discrete-time and regularly sampled data, face significant challenges in capturing the continuous dynamics and irregular sampling patterns inherent in real-world scenarios. Neural Differential Equations (NDEs) represent a paradigm shift by combining the flexibility of neural networks with the mathematical rigor of differential equations. This paper presents a comprehensive review of NDE-based methods for time series analysis, including neural ordinary differential equations, neural controlled differential equations, and neural stochastic differential equations. We provide a detailed discussion of their mathematical formulations, numerical methods, and applications, highlighting their ability to model continuous-time dynamics. Furthermore, we address key challenges and future research directions. This survey serves as a foundation for researchers and practitioners seeking to leverage NDEs for advanced time series analysis.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controlling Neural Collapse Enhances Out-of-Distribution Detection and Transfer Learning</title>
<link>https://arxiv.org/abs/2502.10691</link>
<guid>https://arxiv.org/abs/2502.10691</guid>
<content:encoded><![CDATA[
arXiv:2502.10691v3 Announce Type: replace 
Abstract: Out-of-distribution (OOD) detection and OOD generalization are widely studied in Deep Neural Networks (DNNs), yet their relationship remains poorly understood. We empirically show that the degree of Neural Collapse (NC) in a network layer is inversely related with these objectives: stronger NC improves OOD detection but degrades generalization, while weaker NC enhances generalization at the cost of detection. This trade-off suggests that a single feature space cannot simultaneously achieve both tasks. To address this, we develop a theoretical framework linking NC to OOD detection and generalization. We show that entropy regularization mitigates NC to improve generalization, while a fixed Simplex Equiangular Tight Frame (ETF) projector enforces NC for better detection. Based on these insights, we propose a method to control NC at different DNN layers. In experiments, our method excels at both tasks across OOD datasets and DNN architectures. Code for our experiments is available at: https://yousuf907.github.io/ncoodg
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Structural-Counterfactual Generation under Domain Shift</title>
<link>https://arxiv.org/abs/2502.12013</link>
<guid>https://arxiv.org/abs/2502.12013</guid>
<content:encoded><![CDATA[
arXiv:2502.12013v3 Announce Type: replace 
Abstract: Motivated by the burgeoning interest in cross-domain learning, we present a novel generative modeling challenge: generating counterfactual samples in a target domain based on factual observations from a source domain. Our approach operates within an unsupervised paradigm devoid of parallel or joint datasets, relying exclusively on distinct observational samples and causal graphs for each domain. This setting presents challenges that surpass those of conventional counterfactual generation. Central to our methodology is the disambiguation of exogenous causes into effect-intrinsic and domain-intrinsic categories. This differentiation facilitates the integration of domain-specific causal graphs into a unified joint causal graph via shared effect-intrinsic exogenous variables. We propose leveraging Neural Causal models within this joint framework to enable accurate counterfactual generation under standard identifiability assumptions. Furthermore, we introduce a novel loss function that effectively segregates effect-intrinsic from domain-intrinsic variables during model training. Given a factual observation, our framework combines the posterior distribution of effect-intrinsic variables from the source domain with the prior distribution of domain-intrinsic variables from the target domain to synthesize the desired counterfactuals, adhering to Pearl's causal hierarchy. Intriguingly, when domain shifts are restricted to alterations in causal mechanisms without accompanying covariate shifts, our training regimen parallels the resolution of a conditional optimal transport problem. Empirical evaluations on a synthetic dataset show that our framework generates counterfactuals in the target domain that very closely resemble the ground truth.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Neural SDE Training using Wiener-Space Cubature</title>
<link>https://arxiv.org/abs/2502.12395</link>
<guid>https://arxiv.org/abs/2502.12395</guid>
<content:encoded><![CDATA[
arXiv:2502.12395v3 Announce Type: replace 
Abstract: A neural stochastic differential equation (SDE) is an SDE with drift and diffusion terms parametrized by neural networks. The training procedure for neural SDEs consists of optimizing the SDE vector field (neural network) parameters to minimize the expected value of an objective functional on infinite-dimensional path-space. Existing training techniques focus on methods to efficiently compute path-wise gradients of the objective functional with respect to these parameters, then pair this with Monte-Carlo simulation to estimate the gradient expectation. In this work we introduce a novel training technique which bypasses and improves upon this Monte-Carlo simulation; we extend results in the theory of Wiener space cubature to approximate the expected objective functional value by a weighted sum of functional evaluations of deterministic ODE solutions. Our main mathematical contribution enabling this approximation is an extension of cubature bounds to the setting of Lipschitz-nonlinear functionals acting on path-space. Our resulting constructive algorithm allows for more computationally efficient training along several lines. First, it circumvents Brownian motion simulation and enables the use of efficient parallel ODE solvers, thus decreasing the complexity of path-functional evaluation. Furthermore, and more surprisingly, we show that the number of paths required to achieve a given (expected loss functional oracle value) approximation can be reduced in this deterministic cubature regime. Specifically, we show that under reasonable regularity assumptions we can observe a O(1/n) convergence rate, where n is the number of path evaluations; in contrast with the standard O(1/sqrt(n)) rate of naive Monte-Carlo or the O(log(n)^d /n) rate of quasi-Monte-Carlo.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EquiBench: Benchmarking Large Language Models' Reasoning about Program Semantics via Equivalence Checking</title>
<link>https://arxiv.org/abs/2502.12466</link>
<guid>https://arxiv.org/abs/2502.12466</guid>
<content:encoded><![CDATA[
arXiv:2502.12466v3 Announce Type: replace 
Abstract: As large language models (LLMs) become integral to code-related tasks, a central question emerges: Do LLMs truly understand program semantics? We introduce EquiBench, a new benchmark for evaluating LLMs through equivalence checking, i.e., determining whether two programs produce identical outputs for all possible inputs. Unlike prior code generation benchmarks, this task directly tests a model's ability to reason about program semantics. EquiBench consists of 2400 program pairs across four languages and six categories. These pairs are generated through program analysis, compiler scheduling, and superoptimization, ensuring high-confidence labels, nontrivial difficulty, and full automation. We evaluate 19 state-of-the-art LLMs and find that in the most challenging categories, the best accuracies are 63.8% and 76.2%, only modestly above the 50% random baseline. Further analysis reveals that models often rely on syntactic similarity rather than exhibiting robust reasoning about program semantics, highlighting current limitations. Our code and dataset are publicly available at https://github.com/Anjiang-Wei/equibench
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-branch of Attention Yields Accurate Results for Tabular Data</title>
<link>https://arxiv.org/abs/2502.12507</link>
<guid>https://arxiv.org/abs/2502.12507</guid>
<content:encoded><![CDATA[
arXiv:2502.12507v3 Announce Type: replace 
Abstract: Tabular data inherently exhibits significant feature heterogeneity, but existing transformer-based methods lack specialized mechanisms to handle this property. To bridge the gap, we propose MAYA, an encoder-decoder transformer-based framework. In the encoder, we design a Multi-Branch of Attention (MBA) that constructs multiple parallel attention branches and averages the features at each branch, effectively fusing heterogeneous features while limiting parameter growth. Additionally, we employ collaborative learning with a dynamic consistency weight constraint to produce more robust representations. In the decoder stage, cross-attention is utilized to seamlessly integrate tabular data with corresponding label features. This dual-attention mechanism effectively captures both intra-instance and inter-instance interactions. We evaluate the proposed method on a wide range of datasets and compare it with other state-of-the-art transformer-based methods. Extensive experiments demonstrate that our model achieves superior performance among transformer-based methods in both tabular classification and regression tasks.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Algorithms for Adversarial Online Learning: from Finite to Infinite Action Spaces</title>
<link>https://arxiv.org/abs/2502.14790</link>
<guid>https://arxiv.org/abs/2502.14790</guid>
<content:encoded><![CDATA[
arXiv:2502.14790v5 Announce Type: replace 
Abstract: We develop a form Thompson sampling for online learning under full feedback - also known as prediction with expert advice - where the learner's prior is defined over the space of an adversary's future actions, rather than the space of experts. We show regret decomposes into regret the learner expected a priori, plus a prior-robustness-type term we call excess regret. In the classical finite-expert setting, this recovers optimal rates. As an initial step towards practical online learning in settings with a potentially-uncountably-infinite number of experts, we show that Thompson sampling over the $d$-dimensional unit cube, using a certain Gaussian process prior widely-used in the Bayesian optimization literature, has a $\mathcal{O}\Big(\beta\sqrt{Td\log(1+\sqrt{d}\frac{\lambda}{\beta})}\Big)$ rate against a $\beta$-bounded $\lambda$-Lipschitz adversary.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security Analysis</title>
<link>https://arxiv.org/abs/2502.20383</link>
<guid>https://arxiv.org/abs/2502.20383</guid>
<content:encoded><![CDATA[
arXiv:2502.20383v3 Announce Type: replace 
Abstract: Recent advancements in Web AI agents have demonstrated remarkable capabilities in addressing complex web navigation tasks. However, emerging research shows that these agents exhibit greater vulnerability compared to standalone Large Language Models (LLMs), despite both being built upon the same safety-aligned models. This discrepancy is particularly concerning given the greater flexibility of Web AI Agent compared to standalone LLMs, which may expose them to a wider range of adversarial user inputs. To build a scaffold that addresses these concerns, this study investigates the underlying factors that contribute to the increased vulnerability of Web AI agents. Notably, this disparity stems from the multifaceted differences between Web AI agents and standalone LLMs, as well as the complex signals - nuances that simple evaluation metrics, such as success rate, often fail to capture. To tackle these challenges, we propose a component-level analysis and a more granular, systematic evaluation framework. Through this fine-grained investigation, we identify three critical factors that amplify the vulnerability of Web AI agents; (1) embedding user goals into the system prompt, (2) multi-step action generation, and (3) observational capabilities. Our findings highlights the pressing need to enhance security and robustness in AI agent design and provide actionable insights for targeted defense strategies.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cover Learning for Large-Scale Topology Representation</title>
<link>https://arxiv.org/abs/2503.09767</link>
<guid>https://arxiv.org/abs/2503.09767</guid>
<content:encoded><![CDATA[
arXiv:2503.09767v2 Announce Type: replace 
Abstract: Classical unsupervised learning methods like clustering and linear dimensionality reduction parametrize large-scale geometry when it is discrete or linear, while more modern methods from manifold learning find low dimensional representation or infer local geometry by constructing a graph on the input data. More recently, topological data analysis popularized the use of simplicial complexes to represent data topology with two main methodologies: topological inference with geometric complexes and large-scale topology visualization with Mapper graphs -- central to these is the nerve construction from topology, which builds a simplicial complex given a cover of a space by subsets. While successful, these have limitations: geometric complexes scale poorly with data size, and Mapper graphs can be hard to tune and only contain low dimensional information. In this paper, we propose to study the problem of learning covers in its own right, and from the perspective of optimization. We describe a method for learning topologically-faithful covers of geometric datasets, and show that the simplicial complexes thus obtained can outperform standard topological inference approaches in terms of size, and Mapper-type algorithms in terms of representation of large-scale topology.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continual Multimodal Contrastive Learning</title>
<link>https://arxiv.org/abs/2503.14963</link>
<guid>https://arxiv.org/abs/2503.14963</guid>
<content:encoded><![CDATA[
arXiv:2503.14963v3 Announce Type: replace 
Abstract: Multimodal Contrastive Learning (MCL) advances in aligning different modalities and generating multimodal representations in a joint space. By leveraging contrastive learning across diverse modalities, large-scale multimodal data enhances representational quality. However, a critical yet often overlooked challenge remains: multimodal data is rarely collected in a single process, and training from scratch is computationally expensive. Instead, emergent multimodal data can be used to optimize existing models gradually, i.e., models are trained on a sequence of modality pair data. We define this problem as Continual Multimodal Contrastive Learning (CMCL), an underexplored yet crucial research direction at the intersection of multimodal and continual learning. In this paper, we formulate CMCL through two specialized principles of stability and plasticity. We theoretically derive a novel optimization-based method, which projects updated gradients from dual sides onto subspaces where any gradient is prevented from interfering with the previously learned knowledge. Two upper bounds provide theoretical insights on both stability and plasticity in our solution. Beyond our theoretical contributions, we conduct experiments on multiple datasets by comparing our method against advanced continual learning baselines. The empirical results further support our claims and demonstrate the efficacy of our method. Our codes are available at https://github.com/Xiaohao-Liu/CMCL.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Vehicle Routing via AI-Initialized Genetic Algorithms</title>
<link>https://arxiv.org/abs/2504.06126</link>
<guid>https://arxiv.org/abs/2504.06126</guid>
<content:encoded><![CDATA[
arXiv:2504.06126v2 Announce Type: replace 
Abstract: Vehicle Routing Problems (VRP) are an extension of the Traveling Salesperson Problem and are a fundamental NP-hard challenge in combinatorial optimization. Solving VRP in real-time at large scale has become critical in numerous applications, from growing markets like last-mile delivery to emerging use-cases like interactive logistics planning. In many applications, one has to repeatedly solve VRP instances drawn from the same distribution, yet current state-of-the-art solvers treat each instance on its own without leveraging previous examples. We introduce an optimization framework where a reinforcement learning agent is trained on prior instances and quickly generates initial solutions, which are then further optimized by a genetic algorithm. This framework, Evolutionary Algorithm with Reinforcement Learning Initialization (EARLI), consistently outperforms current state-of-the-art solvers across various time budgets. For example, EARLI handles vehicle routing with 500 locations within one second, 10x faster than current solvers for the same solution quality, enabling real-time and interactive routing at scale. EARLI can generalize to new data, as we demonstrate on real e-commerce delivery data of a previously unseen city. By combining reinforcement learning and genetic algorithms, our hybrid framework takes a step forward to closer interdisciplinary collaboration between AI and optimization communities towards real-time optimization in diverse domains.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LEMUR Neural Network Dataset: Towards Seamless AutoML</title>
<link>https://arxiv.org/abs/2504.10552</link>
<guid>https://arxiv.org/abs/2504.10552</guid>
<content:encoded><![CDATA[
arXiv:2504.10552v3 Announce Type: replace 
Abstract: Neural networks have become the backbone of modern AI, yet designing, evaluating, and comparing them remains labor-intensive. While many datasets exist for training models, there are few standardized collections of the models themselves. We present LEMUR, an open-source dataset and framework that brings together a large collection of PyTorch-based neural networks across tasks such as classification, segmentation, detection, and natural language processing. Each model follows a common template, with configurations and results logged in a structured database to ensure consistency and reproducibility. LEMUR integrates Optuna for automated hyperparameter optimization, provides statistical analysis and visualization tools, and exposes an API for seamless access to performance data. The framework also supports extensibility, enabling researchers to add new models, datasets, or metrics without breaking compatibility. By standardizing implementations and unifying evaluation, LEMUR aims to accelerate AutoML research, facilitate fair benchmarking, and lower the barrier to large-scale neural network experimentation.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float</title>
<link>https://arxiv.org/abs/2504.11651</link>
<guid>https://arxiv.org/abs/2504.11651</guid>
<content:encoded><![CDATA[
arXiv:2504.11651v2 Announce Type: replace 
Abstract: Large-scale AI models, such as Large Language Models (LLMs) and Diffusion Models (DMs), have grown rapidly in size, creating significant challenges for efficient deployment on resource-constrained hardware. In this paper, we introduce Dynamic-Length Float (DFloat11), a lossless compression framework that reduces LLM and DM size by 30% while preserving outputs that are bit-for-bit identical to the original model. DFloat11 is motivated by the low entropy in the BFloat16 weight representation of LLMs, which reveals significant inefficiency in the existing storage format. By applying entropy coding, DFloat11 assigns dynamic-length encodings to weights based on frequency, achieving near information-optimal compression without any loss of precision. To facilitate efficient inference with dynamic-length encodings, we develop a custom GPU kernel for fast online decompression. Our design incorporates the following: (i) compact, hierarchical lookup tables (LUTs) that fit within GPU SRAM for efficient decoding, (ii) a two-phase GPU kernel for coordinating thread read/write positions using lightweight auxiliary variables, and (iii) transformer-block-level decompression to minimize latency. Experiments on Llama 3.3, Qwen 3, Mistral 3, FLUX.1, and others validate our hypothesis that DFloat11 achieves around 30% model size reduction while preserving bit-for-bit identical outputs. Compared to a potential alternative of offloading parts of an uncompressed model to the CPU to meet memory constraints, DFloat11 achieves 2.3--46.2x higher throughput in token generation. With a fixed GPU memory budget, DFloat11 enables 5.7--14.9x longer generation lengths than uncompressed models. Notably, our method enables lossless inference of Llama 3.1 405B, an 810GB model, on a single node equipped with 8x80GB GPUs. Our code is available at https://github.com/LeanModels/DFloat11.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Significativity Indices for Agreement Values</title>
<link>https://arxiv.org/abs/2504.15325</link>
<guid>https://arxiv.org/abs/2504.15325</guid>
<content:encoded><![CDATA[
arXiv:2504.15325v3 Announce Type: replace 
Abstract: Agreement measures, such as Cohen's kappa or intraclass correlation, gauge the matching between two or more classifiers. They are used in a wide range of contexts from medicine, where they evaluate the effectiveness of medical treatments and clinical trials, to artificial intelligence, where they can quantify the approximation due to the reduction of a classifier. The consistency of different classifiers to a golden standard can be compared simply by using the order induced by their agreement measure with respect to the golden standard itself. Nevertheless, labelling an approach as good or bad exclusively by using the value of an agreement measure requires a scale or a significativity index. Some quality scales have been proposed in the literature for Cohen's kappa, but they are mainly na\"ive, and their boundaries are arbitrary. This work proposes a general approach to evaluate the significativity of any agreement value between two classifiers and introduces two significativity indices: one dealing with finite data sets, the other one handling classification probability distributions. Moreover, this manuscript addresses the computational challenges of evaluating such indices and proposes some efficient algorithms for their evaluation.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ScaleGNN: Towards Scalable Graph Neural Networks via Adaptive High-order Neighboring Feature Fusion</title>
<link>https://arxiv.org/abs/2504.15920</link>
<guid>https://arxiv.org/abs/2504.15920</guid>
<content:encoded><![CDATA[
arXiv:2504.15920v5 Announce Type: replace 
Abstract: Graph Neural Networks (GNNs) have demonstrated impressive performance across diverse graph-based tasks by leveraging message passing to capture complex node relationships. However, when applied to large-scale real-world graphs, GNNs face two major challenges: First, it becomes increasingly difficult to ensure both scalability and efficiency, as the repeated aggregation of large neighborhoods leads to significant computational overhead; Second, the over-smoothing problem arises, where excessive or deep propagation makes node representations indistinguishable, severely hindering model expressiveness. To tackle these issues, we propose ScaleGNN, a novel framework that adaptively fuses multi-hop node features for both scalable and effective graph learning. First, we construct per-hop pure neighbor matrices that capture only the exclusive structural information at each hop, avoiding the redundancy of conventional aggregation. Then, an enhanced feature fusion strategy significantly balances low-order and high-order information, preserving both local detail and global correlations without incurring excessive complexity. To further reduce redundancy and over-smoothing, we introduce a Local Contribution Score (LCS)-based masking mechanism to filter out less relevant high-order neighbors, ensuring that only the most meaningful information is aggregated. In addition, learnable sparse constraints selectively integrate multi-hop valuable features, emphasizing the most informative high-order neighbors. Extensive experiments on real-world datasets demonstrate that ScaleGNN consistently outperforms state-of-the-art GNNs in both predictive accuracy and computational efficiency, highlighting its practical value for large-scale graph learning.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Simple Review of EEG Foundation Models: Datasets, Advancements and Future Perspectives</title>
<link>https://arxiv.org/abs/2504.20069</link>
<guid>https://arxiv.org/abs/2504.20069</guid>
<content:encoded><![CDATA[
arXiv:2504.20069v2 Announce Type: replace 
Abstract: Electroencephalogram (EEG) signals play a crucial role in understanding brain activity and diagnosing neurological diseases. Because supervised EEG encoders are unable to learn robust EEG patterns and rely too heavily on expensive signal annotation, research has turned to general-purpose self-supervised EEG encoders, known as EEG-based models (EEG-FMs), to achieve robust and scalable EEG feature extraction. However, the readiness of early EEG-FMs for practical applications and the standards for long-term research progress remain unclear. Therefore, a systematic and comprehensive review of first-generation EEG-FMs is necessary to understand their current state-of-the-art and identify key directions for future EEG-FMs. To this end, this study reviews 14 early EEG-FMs and provides a critical comprehensive analysis of their methodologies, empirical findings, and unaddressed research gaps. This review focuses on the latest developments in EEG-based models (EEG-FMs), which have shown great potential for processing and analyzing EEG data. We discuss various EEG-FMs, including their architectures, pretraining strategies, pretraining and downstream datasets, and other details. This review also highlights challenges and future directions in the field, aiming to provide a comprehensive overview for researchers and practitioners interested in EEG analysis and related EEG-FM.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Diffusion Models for Resource Allocation in Wireless Networks</title>
<link>https://arxiv.org/abs/2504.20277</link>
<guid>https://arxiv.org/abs/2504.20277</guid>
<content:encoded><![CDATA[
arXiv:2504.20277v3 Announce Type: replace 
Abstract: This paper proposes a supervised training algorithm for learning stochastic resource allocation policies with generative diffusion models (GDMs). We formulate the allocation problem as the maximization of an ergodic utility function subject to ergodic Quality of Service (QoS) constraints. Given samples from a stochastic expert policy that yields a near-optimal solution to the constrained optimization problem, we train a GDM policy to imitate the expert and generate new samples from the optimal distribution. We achieve near-optimal performance through the sequential execution of the generated samples. To enable generalization to a family of network configurations, we parameterize the backward diffusion process with a graph neural network (GNN) architecture. We present numerical results in a case study of power control.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Quantifying the Hessian Structure of Neural Networks</title>
<link>https://arxiv.org/abs/2505.02809</link>
<guid>https://arxiv.org/abs/2505.02809</guid>
<content:encoded><![CDATA[
arXiv:2505.02809v2 Announce Type: replace 
Abstract: Empirical studies reported that the Hessian matrix of neural networks (NNs) exhibits a near-block-diagonal structure, yet its theoretical foundation remains unclear. In this work, we reveal that the reported Hessian structure comes from a mixture of two forces: a ``static force'' rooted in the architecture design, and a ''dynamic force'' arisen from training. We then provide a rigorous theoretical analysis of ''static force'' at random initialization. We study linear models and 1-hidden-layer networks for classification tasks with $C$ classes. By leveraging random matrix theory, we compare the limit distributions of the diagonal and off-diagonal Hessian blocks and find that the block-diagonal structure arises as $C$ becomes large. Our findings reveal that $C$ is one primary driver of the near-block-diagonal structure. These results may shed new light on the Hessian structure of large language models (LLMs), which typically operate with a large $C$ exceeding $10^4$.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wasserstein Convergence of Score-based Generative Models under Semiconvexity and Discontinuous Gradients</title>
<link>https://arxiv.org/abs/2505.03432</link>
<guid>https://arxiv.org/abs/2505.03432</guid>
<content:encoded><![CDATA[
arXiv:2505.03432v2 Announce Type: replace 
Abstract: Score-based Generative Models (SGMs) approximate a data distribution by perturbing it with Gaussian noise and subsequently denoising it via a learned reverse diffusion process. These models excel at modeling complex data distributions and generating diverse samples, achieving state-of-the-art performance across domains such as computer vision, audio generation, reinforcement learning, and computational biology. Despite their empirical success, existing Wasserstein-2 convergence analysis typically assume strong regularity conditions-such as smoothness or strict log-concavity of the data distribution-that are rarely satisfied in practice. In this work, we establish the first non-asymptotic Wasserstein-2 convergence guarantees for SGMs targeting semiconvex distributions with potentially discontinuous gradients. Our upper bounds are explicit and sharp in key parameters, achieving optimal dependence of $O(\sqrt{d})$ on the data dimension $d$ and convergence rate of order one. The framework accommodates a wide class of practically relevant distributions, including symmetric modified half-normal distributions, Gaussian mixtures, double-well potentials, and elastic net potentials. By leveraging semiconvexity without requiring smoothness assumptions on the potential such as differentiability, our results substantially broaden the theoretical foundations of SGMs, bridging the gap between empirical success and rigorous guarantees in non-smooth, complex data regimes.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GuidedQuant: Large Language Model Quantization via Exploiting End Loss Guidance</title>
<link>https://arxiv.org/abs/2505.07004</link>
<guid>https://arxiv.org/abs/2505.07004</guid>
<content:encoded><![CDATA[
arXiv:2505.07004v4 Announce Type: replace 
Abstract: Post-training quantization is a key technique for reducing the memory and inference latency of large language models by quantizing weights and activations without requiring retraining. However, existing methods either (1) fail to account for the varying importance of hidden features to the end loss or, when incorporating end loss, (2) neglect the critical interactions between model weights. To address these limitations, we propose GuidedQuant, a novel quantization approach that integrates gradient information from the end loss into the quantization objective while preserving cross-weight dependencies within output channels. GuidedQuant consistently boosts the performance of state-of-the-art quantization methods across weight-only scalar, weight-only vector, and weight-and-activation quantization. Additionally, we introduce a novel non-uniform scalar quantization algorithm, which is guaranteed to monotonically decrease the quantization objective value, and outperforms existing methods in this category. We release the code at https://github.com/snu-mllab/GuidedQuant.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpreting Graph Inference with Skyline Explanations</title>
<link>https://arxiv.org/abs/2505.07635</link>
<guid>https://arxiv.org/abs/2505.07635</guid>
<content:encoded><![CDATA[
arXiv:2505.07635v3 Announce Type: replace 
Abstract: Inference queries have been routinely issued to graph machine learning models such as graph neural networks (GNNs) for various network analytical tasks. Nevertheless, GNN outputs are often hard to interpret comprehensively. Existing methods typically conform to individual pre-defined explainability measures (such as fidelity), which often leads to biased, ``one-side'' interpretations. This paper introduces skyline explanation, a new paradigm that interprets GNN outputs by simultaneously optimizing multiple explainability measures of users' interests. (1) We propose skyline explanations as a Pareto set of explanatory subgraphs that dominate others over multiple explanatory measures. We formulate skyline explanation as a multi-criteria optimization problem, and establish its hardness results. (2) We design efficient algorithms with an onion-peeling approach, which strategically prioritizes nodes and removes unpromising edges to incrementally assemble skyline explanations. (3) We also develop an algorithm to diversify the skyline explanations to enrich the comprehensive interpretation. (4) We introduce efficient parallel algorithms with load-balancing strategies to scale skyline explanation for large-scale GNN-based inference. Using real-world and synthetic graphs, we experimentally verify our algorithms' effectiveness and scalability.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamical Low-Rank Compression of Neural Networks with Robustness under Adversarial Attacks</title>
<link>https://arxiv.org/abs/2505.08022</link>
<guid>https://arxiv.org/abs/2505.08022</guid>
<content:encoded><![CDATA[
arXiv:2505.08022v2 Announce Type: replace 
Abstract: Deployment of neural networks on resource-constrained devices demands models that are both compact and robust to adversarial inputs. However, compression and adversarial robustness often conflict. In this work, we introduce a dynamical low-rank training scheme enhanced with a novel spectral regularizer that controls the condition number of the low-rank core in each layer. This approach mitigates the sensitivity of compressed models to adversarial perturbations without sacrificing clean accuracy. The method is model- and data-agnostic, computationally efficient, and supports rank adaptivity to automatically compress the network at hand. Extensive experiments across standard architectures, datasets, and adversarial attacks show the regularized networks can achieve over 94% compression while recovering or improving adversarial accuracy relative to uncompressed baselines.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Informed, but Not Always Improved: Challenging the Benefit of Background Knowledge in GNNs</title>
<link>https://arxiv.org/abs/2505.11023</link>
<guid>https://arxiv.org/abs/2505.11023</guid>
<content:encoded><![CDATA[
arXiv:2505.11023v2 Announce Type: replace 
Abstract: In complex and low-data domains such as biomedical research, incorporating background knowledge (BK) graphs, such as protein-protein interaction (PPI) networks, into graph-based machine learning pipelines is a promising research direction. However, while BK is often assumed to improve model performance, its actual contribution and the impact of imperfect knowledge remain poorly understood. In this work, we investigate the role of BK in an important real-world task: cancer subtype classification. Surprisingly, we find that (i) state-of-the-art GNNs using BK perform no better than uninformed models like linear regression, and (ii) their performance remains largely unchanged even when the BK graph is heavily perturbed. To understand these unexpected results, we introduce an evaluation framework, which employs (i) a synthetic setting where the BK is clearly informative and (ii) a set of perturbations that simulate various imperfections in BK graphs. With this, we test the robustness of BK-aware models in both synthetic and real-world biomedical settings. Our findings reveal that careful alignment of GNN architectures and BK characteristics is necessary but holds the potential for significant performance improvements.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the $O(\frac{\sqrt{d}}{K^{1/4}})$ Convergence Rate of AdamW Measured by $\ell_1$ Norm</title>
<link>https://arxiv.org/abs/2505.11840</link>
<guid>https://arxiv.org/abs/2505.11840</guid>
<content:encoded><![CDATA[
arXiv:2505.11840v2 Announce Type: replace 
Abstract: As the default optimizer for training large language models, AdamW has achieved remarkable success in deep learning. However, its convergence behavior is not theoretically well-understood. This paper establishes the convergence rate $\frac{1}{K}\sum_{k=1}^KE\left[||\nabla f(x^k)||_1\right]\leq O(\frac{\sqrt{d}C}{K^{1/4}})$ for AdamW measured by $\ell_1$ norm, where $K$ represents the iteration number, $d$ denotes the model dimension, and $C$ matches the constant in the optimal convergence rate of SGD. Theoretically, we have $||\nabla f(x)||_2\ll ||\nabla f(x)||_1\leq \sqrt{d}||\nabla f(x)||_2$ for any high-dimensional vector $x$ and $E\left[||\nabla f(x)||_1\right]\geq\sqrt{\frac{2d}{\pi}}E\left[||\nabla f(x)||_2\right]$ when each element of $\nabla f(x)$ is generated from Gaussian distribution $\mathcal N(0,1)$. Empirically, our experimental results on real-world deep learning tasks reveal $||\nabla f(x)||_1=\varTheta(\sqrt{d})||\nabla f(x)||_2$. Both support that our convergence rate can be considered to be analogous to the optimal $\frac{1}{K}\sum_{k=1}^KE\left[||\nabla f(x^k)||_2\right]\leq O(\frac{C}{K^{1/4}})$ convergence rate of SGD.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RECON: Robust symmetry discovery via Explicit Canonical Orientation Normalization</title>
<link>https://arxiv.org/abs/2505.13289</link>
<guid>https://arxiv.org/abs/2505.13289</guid>
<content:encoded><![CDATA[
arXiv:2505.13289v2 Announce Type: replace 
Abstract: Real world data often exhibits unknown, instance-specific symmetries that rarely exactly match a transformation group $G$ fixed a priori. Class-pose decompositions aim to create disentangled representations by factoring inputs into invariant features and a pose $g\in G$ defined relative to a training-dependent, arbitrary canonical representation. We introduce RECON, a class-pose agnostic $\textit{canonical orientation normalization}$ that corrects arbitrary canonicals via a simple right-multiplication, yielding $\textit{natural}$, data-aligned canonicalizations. This enables (i) unsupervised discovery of instance-specific symmetry distributions, (ii) detection of out-of-distribution poses, and (iii) test-time canonicalization, granting group invariance to pre-trained models without retraining and irrespective of model architecture, improving downstream performance. We demonstrate results on 2D image benchmarks and --for the first time-- extend symmetry discovery to 3D groups.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RefLoRA: Refactored Low-Rank Adaptation for Efficient Fine-Tuning of Large Models</title>
<link>https://arxiv.org/abs/2505.18877</link>
<guid>https://arxiv.org/abs/2505.18877</guid>
<content:encoded><![CDATA[
arXiv:2505.18877v2 Announce Type: replace 
Abstract: Low-Rank Adaptation (LoRA) lowers the computational and memory overhead of fine-tuning large models by updating a low-dimensional subspace of the pre-trained weight matrix. Albeit efficient, LoRA exhibits suboptimal convergence and noticeable performance degradation, due to inconsistent and imbalanced weight updates induced by its nonunique low-rank factorizations. To overcome these limitations, this article identifies the optimal low-rank factorization per step that minimizes an upper bound on the loss. The resultant refactored low-rank adaptation (RefLoRA) method promotes a flatter loss landscape, along with consistent and balanced weight updates, thus speeding up stable convergence. Extensive experiments evaluate RefLoRA on natural language understanding, and commonsense reasoning tasks with popular large language models including DeBERTaV3, LLaMA-7B, LLaMA2-7B and LLaMA3-8B. The numerical tests corroborate that RefLoRA converges faster, outperforms various benchmarks, and enjoys negligible computational overhead compared to state-of-the-art LoRA variants.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DPASyn: Mechanism-Aware Drug Synergy Prediction via Dual Attention and Precision-Aware Quantization</title>
<link>https://arxiv.org/abs/2505.19144</link>
<guid>https://arxiv.org/abs/2505.19144</guid>
<content:encoded><![CDATA[
arXiv:2505.19144v2 Announce Type: replace 
Abstract: Drug combinations are essential in cancer therapy, leveraging synergistic drug-drug interactions (DDI) to enhance efficacy and combat resistance. However, the vast combinatorial space makes experimental screening impractical, and existing computational models struggle to capture the complex, bidirectional nature of DDIs, often relying on independent drug encoding or simplistic fusion strategies that miss fine-grained inter-molecular dynamics. Moreover, state-of-the-art graph-based approaches suffer from high computational costs, limiting scalability for real-world drug discovery. To address this, we propose DPASyn, a novel drug synergy prediction framework featuring a dual-attention mechanism and Precision-Aware Quantization (PAQ). The dual-attention architecture jointly models intra-drug structures and inter-drug interactions via shared projections and cross-drug attention, enabling fine-grained, biologically plausible synergy modeling. While this enhanced expressiveness brings increased computational resource consumption, our proposed PAQ strategy complements it by dynamically optimizing numerical precision during training based on feature sensitivity-reducing memory usage by 40% and accelerating training threefold without sacrificing accuracy. With LayerNorm-stabilized residual connections for training stability, DPASyn outperforms seven state-of-the-art methods on the O'Neil dataset (13,243 combinations) and supports full-batch processing of up to 256 graphs on a single GPU, setting a new standard for efficient and expressive drug synergy prediction.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Less is More: Unlocking Specialization of Time Series Foundation Models via Structured Pruning</title>
<link>https://arxiv.org/abs/2505.23195</link>
<guid>https://arxiv.org/abs/2505.23195</guid>
<content:encoded><![CDATA[
arXiv:2505.23195v2 Announce Type: replace 
Abstract: Scaling laws motivate the development of Time Series Foundation Models (TSFMs) that pre-train vast parameters and achieve remarkable zero-shot forecasting performance. Surprisingly, even after fine-tuning, TSFMs cannot consistently outperform smaller, specialized models trained on full-shot downstream data. A key question is how to realize effective adaptation of TSFMs for a target forecasting task. Through empirical studies on various TSFMs, the pre-trained models often exhibit inherent sparsity and redundancy in computation, suggesting that TSFMs have learned to activate task-relevant network substructures to accommodate diverse forecasting tasks. To preserve this valuable prior knowledge, we propose a structured pruning method to regularize the subsequent fine-tuning process by focusing it on a more relevant and compact parameter space. Extensive experiments on seven TSFMs and six benchmarks demonstrate that fine-tuning a smaller, pruned TSFM significantly improves forecasting performance compared to fine-tuning original models. This prune-then-finetune paradigm often enables TSFMs to achieve state-of-the-art performance and surpass strong specialized baselines. Source code is made publicly available at https://github.com/SJTU-DMTai/Prune-then-Finetune.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Absorb and Converge: Provable Convergence Guarantee for Absorbing Discrete Diffusion Models</title>
<link>https://arxiv.org/abs/2506.02318</link>
<guid>https://arxiv.org/abs/2506.02318</guid>
<content:encoded><![CDATA[
arXiv:2506.02318v2 Announce Type: replace 
Abstract: Discrete state space diffusion models have shown significant advantages in applications involving discrete data, such as text and image generation. It has also been observed that their performance is highly sensitive to the choice of rate matrices, particularly between uniform and absorbing rate matrices. While empirical results suggest that absorbing rate matrices often yield better generation quality compared to uniform rate matrices, existing theoretical works have largely focused on the uniform rate matrices case. Notably, convergence guarantees and error analyses for absorbing diffusion models are still missing. In this work, we provide the first finite-time error bounds and convergence rate analysis for discrete diffusion models using absorbing rate matrices. We begin by deriving an upper bound on the KL divergence of the forward process, introducing a surrogate initialization distribution to address the challenge posed by the absorbing stationary distribution, which is a singleton and causes the KL divergence to be ill-defined. We then establish the first convergence guarantees for both the $\tau$-leaping and uniformization samplers under absorbing rate matrices, demonstrating improved rates over their counterparts using uniform rate matrices. Furthermore, under suitable assumptions, we provide convergence guarantees without early stopping. Our analysis introduces several new technical tools to address challenges unique to absorbing rate matrices. These include a Jensen-type argument for bounding forward process convergence, novel techniques for bounding absorbing score functions, and a non-divergent upper bound on the score near initialization that removes the need of early-stopping.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoClip: Geometry-Aware Clipping for Differentially Private SGD</title>
<link>https://arxiv.org/abs/2506.06549</link>
<guid>https://arxiv.org/abs/2506.06549</guid>
<content:encoded><![CDATA[
arXiv:2506.06549v2 Announce Type: replace 
Abstract: Differentially private stochastic gradient descent (DP-SGD) is the most widely used method for training machine learning models with provable privacy guarantees. A key challenge in DP-SGD is setting the per-sample gradient clipping threshold, which significantly affects the trade-off between privacy and utility. While recent adaptive methods improve performance by adjusting this threshold during training, they operate in the standard coordinate system and fail to account for correlations across the coordinates of the gradient. We propose GeoClip, a geometry-aware framework that clips and perturbs gradients in a transformed basis aligned with the geometry of the gradient distribution. GeoClip adaptively estimates this transformation using only previously released noisy gradients, incurring no additional privacy cost. We provide convergence guarantees for GeoClip and derive a closed-form solution for the optimal transformation that minimizes the amount of noise added while keeping the probability of gradient clipping under control. Experiments on both tabular and image datasets demonstrate that GeoClip consistently outperforms existing adaptive clipping methods under the same privacy budget.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIRA: Medical Time Series Foundation Model for Real-World Health Data</title>
<link>https://arxiv.org/abs/2506.07584</link>
<guid>https://arxiv.org/abs/2506.07584</guid>
<content:encoded><![CDATA[
arXiv:2506.07584v4 Announce Type: replace 
Abstract: A unified foundation model for medical time series -- pretrained on open access and ethics board-approved medical corpora -- offers the potential to reduce annotation burdens, minimize model customization, and enable robust transfer across clinical institutions, modalities, and tasks, particularly in data-scarce or privacy-constrained environments. However, existing generalist time series foundation models struggle to handle medical time series data due to their inherent challenges, including irregular intervals, heterogeneous sampling rates, and frequent missing values. To address these challenges, we introduce MIRA, a unified foundation model specifically designed for medical time series forecasting. MIRA incorporates a Continuous-Time Rotary Positional Encoding that enables fine-grained modeling of variable time intervals, a frequency-specific mixture-of-experts layer that routes computation across latent frequency regimes to further promote temporal specialization, and a Continuous Dynamics Extrapolation Block based on Neural ODE that models the continuous trajectory of latent states, enabling accurate forecasting at arbitrary target timestamps. Pretrained on a large-scale and diverse medical corpus comprising over 454 billion time points collect from publicly available datasets, MIRA achieves reductions in forecasting errors by an average of 10% and 7% in out-of-distribution and in-distribution scenarios, respectively, when compared to other zero-shot and fine-tuned baselines. We also introduce a comprehensive benchmark spanning multiple downstream clinical tasks, establishing a foundation for future research in medical time series modeling.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Impact of Feature Scaling In Machine Learning: Effects on Regression and Classification Tasks</title>
<link>https://arxiv.org/abs/2506.08274</link>
<guid>https://arxiv.org/abs/2506.08274</guid>
<content:encoded><![CDATA[
arXiv:2506.08274v4 Announce Type: replace 
Abstract: This research addresses the critical lack of comprehensive studies on feature scaling by systematically evaluating 12 scaling techniques - including several less common transformations - across 14 different Machine Learning algorithms and 16 datasets for classification and regression tasks. We meticulously analyzed impacts on predictive performance (using metrics such as accuracy, MAE, MSE, and $R^2$) and computational costs (training time, inference time, and memory usage). Key findings reveal that while ensemble methods (such as Random Forest and gradient boosting models like XGBoost, CatBoost and LightGBM) demonstrate robust performance largely independent of scaling, other widely used models such as Logistic Regression, SVMs, TabNet, and MLPs show significant performance variations highly dependent on the chosen scaler. This extensive empirical analysis, with all source code, experimental results, and model parameters made publicly available to ensure complete transparency and reproducibility, offers model-specific crucial guidance to practitioners on the need for an optimal selection of feature scaling techniques.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SUA: Stealthy Multimodal Large Language Model Unlearning Attack</title>
<link>https://arxiv.org/abs/2506.17265</link>
<guid>https://arxiv.org/abs/2506.17265</guid>
<content:encoded><![CDATA[
arXiv:2506.17265v2 Announce Type: replace 
Abstract: Multimodal Large Language Models (MLLMs) trained on massive data may memorize sensitive personal information and photos, posing serious privacy risks. To mitigate this, MLLM unlearning methods are proposed, which fine-tune MLLMs to reduce the ``forget'' sensitive information. However, it remains unclear whether the knowledge has been truly forgotten or just hidden in the model. Therefore, we propose to study a novel problem of LLM unlearning attack, which aims to recover the unlearned knowledge of an unlearned LLM. To achieve the goal, we propose a novel framework Stealthy Unlearning Attack (SUA) framework that learns a universal noise pattern. When applied to input images, this noise can trigger the model to reveal unlearned content. While pixel-level perturbations may be visually subtle, they can be detected in the semantic embedding space, making such attacks vulnerable to potential defenses. To improve stealthiness, we introduce an embedding alignment loss that minimizes the difference between the perturbed and denoised image embeddings, ensuring the attack is semantically unnoticeable. Experimental results show that SUA can effectively recover unlearned information from MLLMs. Furthermore, the learned noise generalizes well: a single perturbation trained on a subset of samples can reveal forgotten content in unseen images. This indicates that knowledge reappearance is not an occasional failure, but a consistent behavior.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A geometric framework for momentum-based optimizers for low-rank training</title>
<link>https://arxiv.org/abs/2506.17475</link>
<guid>https://arxiv.org/abs/2506.17475</guid>
<content:encoded><![CDATA[
arXiv:2506.17475v2 Announce Type: replace 
Abstract: Low-rank pre-training and fine-tuning have recently emerged as promising techniques for reducing the computational and storage costs of large neural networks. Training low-rank parameterizations typically relies on conventional optimizers such as heavy ball momentum methods or Adam. In this work, we identify and analyze potential difficulties that these training methods encounter when used to train low-rank parameterizations of weights. In particular, we show that classical momentum methods can struggle to converge to a local optimum due to the geometry of the underlying optimization landscape. To address this, we introduce novel training strategies derived from dynamical low-rank approximation, which explicitly account for the underlying geometric structure. Our approach leverages and combines tools from dynamical low-rank approximation and momentum-based optimization to design optimizers that respect the intrinsic geometry of the parameter space. We validate our methods through numerical experiments, demonstrating faster convergence, and stronger validation metrics at given parameter budgets.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Guidance via Robust Feature Attribution</title>
<link>https://arxiv.org/abs/2506.19680</link>
<guid>https://arxiv.org/abs/2506.19680</guid>
<content:encoded><![CDATA[
arXiv:2506.19680v2 Announce Type: replace 
Abstract: Controlling the patterns a model learns is essential to preventing reliance on irrelevant or misleading features. Such reliance on irrelevant features, often called shortcut features, has been observed across domains, including medical imaging and natural language processing, where it may lead to real-world harms. A common mitigation strategy leverages annotations (provided by humans or machines) indicating which features are relevant or irrelevant. These annotations are compared to model explanations, typically in the form of feature salience, and used to guide the loss function during training. Unfortunately, recent works have demonstrated that feature salience methods are unreliable and therefore offer a poor signal to optimize. In this work, we propose a simplified objective that simultaneously optimizes for explanation robustness and mitigation of shortcut learning. Unlike prior objectives with similar aims, we demonstrate theoretically why our approach ought to be more effective. Across a comprehensive series of experiments, we show that our approach consistently reduces test-time misclassifications by 20% compared to state-of-the-art methods. We also extend prior experimental settings to include natural language processing tasks. Additionally, we conduct novel ablations that yield practical insights, including the relative importance of annotation quality over quantity. Code for our method and experiments is available at: https://github.com/Mihneaghitu/ModelGuidanceViaRobustFeatureAttribution.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Progressive Size-Adaptive Federated Learning: A Comprehensive Framework for Heterogeneous Multi-Modal Data Systems</title>
<link>https://arxiv.org/abs/2506.20685</link>
<guid>https://arxiv.org/abs/2506.20685</guid>
<content:encoded><![CDATA[
arXiv:2506.20685v2 Announce Type: replace 
Abstract: Federated Learning (FL) has emerged as a transformative paradigm for distributed machine learning while preserving data privacy. However, existing approaches predominantly focus on model heterogeneity and aggregation techniques, largely overlooking the fundamental impact of dataset size characteristics on federated training dynamics. This paper introduces Size-Based Adaptive Federated Learning (SAFL), a novel progressive training framework that systematically organizes federated learning based on dataset size characteristics across heterogeneous multi-modal data. Our comprehensive experimental evaluation across 13 diverse datasets spanning 7 modalities (vision, text, time series, audio, sensor, medical vision, and multimodal) reveals critical insights: 1) an optimal dataset size range of 1000-1500 samples for federated learning effectiveness; 2) a clear modality performance hierarchy with structured data (time series, sensor) significantly outperforming unstructured data (text, multimodal); and 3) systematic performance degradation for large datasets exceeding 2000 samples. SAFL achieves an average accuracy of 87.68% across all datasets, with structured data modalities reaching 99%+ accuracy. The framework demonstrates superior communication efficiency, reducing total data transfer to 7.38 GB across 558 communications while maintaining high performance. Our real-time monitoring framework provides unprecedented insights into system resource utilization, network efficiency, and training dynamics. This work fills critical gaps in understanding how data characteristics should drive federated learning strategies, providing both theoretical insights and practical guidance for real-world FL deployments in neural network and learning systems.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DBConformer: Dual-Branch Convolutional Transformer for EEG Decoding</title>
<link>https://arxiv.org/abs/2506.21140</link>
<guid>https://arxiv.org/abs/2506.21140</guid>
<content:encoded><![CDATA[
arXiv:2506.21140v2 Announce Type: replace 
Abstract: Electroencephalography (EEG)-based brain-computer interfaces (BCIs) transform spontaneous/evoked neural activity into control commands for external communication. While convolutional neural networks (CNNs) remain the mainstream backbone for EEG decoding, their inherently short receptive field makes it difficult to capture long-range temporal dependencies and global inter-channel relationships. Recent CNN-Transformer (Conformer) hybrids partially address this issue, but most adopt a serial design, resulting in suboptimal integration of local and global features, and often overlook explicit channel-wise modeling. To address these limitations, we propose DBConformer, a dual-branch convolutional Transformer network tailored for EEG decoding. It integrates a temporal Conformer to model long-range temporal dependencies and a spatial Conformer to extract inter-channel interactions, capturing both temporal dynamics and spatial patterns in EEG signals. A lightweight channel attention module further refines spatial representations by assigning data-driven importance to EEG channels. Extensive experiments under four evaluation settings on three paradigms, including motor imagery, seizure detection, and steady-state visual evoked potential, demonstrated that DBConformer consistently outperformed 13 competitive baseline models, with over an eight-fold reduction in parameters than current high-capacity EEG Conformer architecture. Furthermore, the visualization results confirmed that the features extracted by DBConformer are physiologically interpretable and aligned with prior knowledge. The superior performance and interpretability of DBConformer make it reliable for accurate, robust, and explainable EEG decoding. Code is publicized at https://github.com/wzwvv/DBConformer.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-View Contrastive Learning for Robust Domain Adaptation in Medical Time Series Analysis</title>
<link>https://arxiv.org/abs/2506.22393</link>
<guid>https://arxiv.org/abs/2506.22393</guid>
<content:encoded><![CDATA[
arXiv:2506.22393v2 Announce Type: replace 
Abstract: Adapting machine learning models to medical time series across different domains remains a challenge due to complex temporal dependencies and dynamic distribution shifts. Current approaches often focus on isolated feature representations, limiting their ability to fully capture the intricate temporal dynamics necessary for robust domain adaptation. In this work, we propose a novel framework leveraging multi-view contrastive learning to integrate temporal patterns, derivative-based dynamics, and frequency-domain features. Our method employs independent encoders and a hierarchical fusion mechanism to learn feature-invariant representations that are transferable across domains while preserving temporal coherence. Extensive experiments on diverse medical datasets, including electroencephalogram (EEG), electrocardiogram (ECG), and electromyography (EMG) demonstrate that our approach significantly outperforms state-of-the-art methods in transfer learning tasks. By advancing the robustness and generalizability of machine learning models, our framework offers a practical pathway for deploying reliable AI systems in diverse healthcare settings.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Search-Optimized Quantization in Biomedical Ontology Alignment</title>
<link>https://arxiv.org/abs/2507.13742</link>
<guid>https://arxiv.org/abs/2507.13742</guid>
<content:encoded><![CDATA[
arXiv:2507.13742v2 Announce Type: replace 
Abstract: In the fast-moving world of AI, as organizations and researchers develop more advanced models, they face challenges due to their sheer size and computational demands. Deploying such models on edge devices or in resource-constrained environments adds further challenges related to energy consumption, memory usage and latency. To address these challenges, emerging trends are shaping the future of efficient model optimization techniques. From this premise, by employing supervised state-of-the-art transformer-based models, this research introduces a systematic method for ontology alignment, grounded in cosine-based semantic similarity between a biomedical layman vocabulary and the Unified Medical Language System (UMLS) Metathesaurus. It leverages Microsoft Olive to search for target optimizations among different Execution Providers (EPs) using the ONNX Runtime backend, followed by an assembled process of dynamic quantization employing Intel Neural Compressor and IPEX (Intel Extension for PyTorch). Through our optimization process, we conduct extensive assessments on the two tasks from the DEFT 2020 Evaluation Campaign, achieving a new state-of-the-art in both. We retain performance metrics intact, while attaining an average inference speed-up of 20x and reducing memory usage by approximately 70%.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Cost of Compression: Tight Quadratic Black-Box Attacks on Sketches for $\ell_2$ Norm Estimation</title>
<link>https://arxiv.org/abs/2507.16345</link>
<guid>https://arxiv.org/abs/2507.16345</guid>
<content:encoded><![CDATA[
arXiv:2507.16345v2 Announce Type: replace 
Abstract: Dimensionality reduction via linear sketching is a powerful and widely used technique, but it is known to be vulnerable to adversarial inputs. We study the black-box adversarial setting, where a fixed, hidden sketching matrix $A \in R^{k \times n}$ maps high-dimensional vectors $v \in R^n$ to lower-dimensional sketches $A v \in R^k$, and an adversary can query the system to obtain approximate $\ell_2$-norm estimates that are computed from the sketch. We present a universal, nonadaptive attack that, using $\tilde{O}(k^2)$ queries, either causes a failure in norm estimation or constructs an adversarial input on which the optimal estimator for the query distribution (used by the attack) fails. The attack is completely agnostic to the sketching matrix and to the estimator: it applies to any linear sketch and any query responder, including those that are randomized, adaptive, or tailored to the query distribution. Our lower bound construction tightly matches the known upper bounds of $\tilde{\Omega}(k^2)$, achieved by specialized estimators for Johnson Lindenstrauss transforms and AMS sketches. Beyond sketching, our results uncover structural parallels to adversarial attacks in image classification, highlighting fundamental vulnerabilities of compressed representations.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HOTA: Hamiltonian framework for Optimal Transport Advection</title>
<link>https://arxiv.org/abs/2507.17513</link>
<guid>https://arxiv.org/abs/2507.17513</guid>
<content:encoded><![CDATA[
arXiv:2507.17513v2 Announce Type: replace 
Abstract: Optimal transport (OT) has become a natural framework for guiding the probability flows. Yet, the majority of recent generative models assume trivial geometry (e.g., Euclidean) and rely on strong density-estimation assumptions, yielding trajectories that do not respect the true principles of optimality in the underlying manifold. We present Hamiltonian Optimal Transport Advection (HOTA), a Hamilton-Jacobi-Bellman based method that tackles the dual dynamical OT problem explicitly through Kantorovich potentials, enabling efficient and scalable trajectory optimization. Our approach effectively evades the need for explicit density modeling, performing even when the cost functionals are non-smooth. Empirically, HOTA outperforms all baselines in standard benchmarks, as well as in custom datasets with non-differentiable costs, both in terms of feasibility and optimality.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Bisimulation Metric for Robust Representations in Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.18519</link>
<guid>https://arxiv.org/abs/2507.18519</guid>
<content:encoded><![CDATA[
arXiv:2507.18519v2 Announce Type: replace 
Abstract: Bisimulation metric has long been regarded as an effective control-related representation learning technique in various reinforcement learning tasks. However, in this paper, we identify two main issues with the conventional bisimulation metric: 1) an inability to represent certain distinctive scenarios, and 2) a reliance on predefined weights for differences in rewards and subsequent states during recursive updates. We find that the first issue arises from an imprecise definition of the reward gap, whereas the second issue stems from overlooking the varying importance of reward difference and next-state distinctions across different training stages and task settings. To address these issues, by introducing a measure for state-action pairs, we propose a revised bisimulation metric that features a more precise definition of reward gap and novel update operators with adaptive coefficient. We also offer theoretical guarantees of convergence for our proposed metric and its improved representation distinctiveness. In addition to our rigorous theoretical analysis, we conduct extensive experiments on two representative benchmarks, DeepMind Control and Meta-World, demonstrating the effectiveness of our approach.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MH-GIN: Multi-scale Heterogeneous Graph-based Imputation Network for AIS Data (Extended Version)</title>
<link>https://arxiv.org/abs/2507.20362</link>
<guid>https://arxiv.org/abs/2507.20362</guid>
<content:encoded><![CDATA[
arXiv:2507.20362v2 Announce Type: replace 
Abstract: Location-tracking data from the Automatic Identification System, much of which is publicly available, plays a key role in a range of maritime safety and monitoring applications. However, the data suffers from missing values that hamper downstream applications. Imputing the missing values is challenging because the values of different heterogeneous attributes are updated at diverse rates, resulting in the occurrence of multi-scale dependencies among attributes. Existing imputation methods that assume similar update rates across attributes are unable to capture and exploit such dependencies, limiting their imputation accuracy. We propose MH-GIN, a Multi-scale Heterogeneous Graph-based Imputation Network that aims improve imputation accuracy by capturing multi-scale dependencies. Specifically, MH-GIN first extracts multi-scale temporal features for each attribute while preserving their intrinsic heterogeneous characteristics. Then, it constructs a multi-scale heterogeneous graph to explicitly model dependencies between heterogeneous attributes to enable more accurate imputation of missing values through graph propagation. Experimental results on two real-world datasets find that MH-GIN is capable of an average 57% reduction in imputation errors compared to state-of-the-art methods, while maintaining computational efficiency. The source code and implementation details of MH-GIN are publicly available https://github.com/hyLiu1994/MH-GIN.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dissecting Persona-Driven Reasoning in Language Models via Activation Patching</title>
<link>https://arxiv.org/abs/2507.20936</link>
<guid>https://arxiv.org/abs/2507.20936</guid>
<content:encoded><![CDATA[
arXiv:2507.20936v2 Announce Type: replace 
Abstract: Large language models (LLMs) exhibit remarkable versatility in adopting diverse personas. In this study, we examine how assigning a persona influences a model's reasoning on an objective task. Using activation patching, we take a first step toward understanding how key components of the model encode persona-specific information. Our findings reveal that the early Multi-Layer Perceptron (MLP) layers attend not only to the syntactic structure of the input but also process its semantic content. These layers transform persona tokens into richer representations, which are then used by the middle Multi-Head Attention (MHA) layers to shape the model's output. Additionally, we identify specific attention heads that disproportionately attend to racial and color-based identities.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spec-VLA: Speculative Decoding for Vision-Language-Action Models with Relaxed Acceptance</title>
<link>https://arxiv.org/abs/2507.22424</link>
<guid>https://arxiv.org/abs/2507.22424</guid>
<content:encoded><![CDATA[
arXiv:2507.22424v2 Announce Type: replace 
Abstract: Vision-Language-Action (VLA) models have made substantial progress by leveraging the robust capabilities of Visual Language Models (VLMs). However, VLMs' significant parameter size and autoregressive (AR) decoding nature impose considerable computational demands on VLA models. While Speculative Decoding (SD) has shown efficacy in accelerating Large Language Models (LLMs) by incorporating efficient drafting and parallel verification, allowing multiple tokens to be generated in one forward pass, its application to VLA models remains unexplored. This work introduces Spec-VLA, an SD framework designed to accelerate VLA models. Due to the difficulty of the action prediction task and the greedy decoding mechanism of the VLA models, the direct application of the advanced SD framework to the VLA prediction task yields a minor speed improvement. To boost the generation speed, we propose an effective mechanism to relax acceptance utilizing the relative distances represented by the action tokens of the VLA model. Empirical results across diverse test scenarios affirm the effectiveness of the Spec-VLA framework, and further analysis substantiates the impact of our proposed strategies, which enhance the acceptance length by 44%, achieving 1.42 times speedup compared with the OpenVLA baseline, without compromising the success rate. The success of the Spec-VLA framework highlights the potential for broader application of speculative execution in VLA prediction scenarios.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pulling Back the Curtain on ReLU Networks</title>
<link>https://arxiv.org/abs/2507.22832</link>
<guid>https://arxiv.org/abs/2507.22832</guid>
<content:encoded><![CDATA[
arXiv:2507.22832v4 Announce Type: replace 
Abstract: Since any ReLU network is piecewise affine, its hidden units can be characterized by their pullbacks through the active subnetwork, i.e., by their gradients (up to bias terms). However, gradients of deeper neurons are notoriously misaligned, which obscures the network's internal representations. We posit that models do align gradients with data, yet this is concealed by the intrinsic noise of the ReLU hard gating. We validate this intuition by applying soft gating in the backward pass only, reducing the local impact of weakly excited neurons. The resulting modified gradients, which we call "excitation pullbacks", exhibit striking perceptual alignment on a number of ImageNet-pretrained architectures, while the rudimentary pixel-space gradient ascent quickly produces easily interpretable input- and target-specific features. Inspired by these findings, we formulate the "path stability" hypothesis, claiming that the binary activation patterns largely stabilize during training and get encoded in the pre-activation distribution of the final model. When true, excitation pullbacks become aligned with the gradients of a kernel machine that mainly determines the network's decision. This provides a theoretical justification for the apparent faithfulness of the feature attributions based on excitation pullbacks, potentially even leading to mechanistic interpretability of deep models. Incidentally, we give a possible explanation for the effectiveness of Batch Normalization and Deep Features, together with a novel perspective on the network's internal memory and generalization properties. We release the code and an interactive app for easier exploration of the excitation pullbacks.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning for Decision-Level Interception Prioritization in Drone Swarm Defense</title>
<link>https://arxiv.org/abs/2508.00641</link>
<guid>https://arxiv.org/abs/2508.00641</guid>
<content:encoded><![CDATA[
arXiv:2508.00641v2 Announce Type: replace 
Abstract: The growing threat of low-cost kamikaze drone swarms poses a critical challenge to modern defense systems demanding rapid and strategic decision-making to prioritize interceptions across multiple effectors and high-value target zones. In this work, we present a case study demonstrating the practical advantages of reinforcement learning in addressing this challenge. We introduce a high-fidelity simulation environment that captures realistic operational constraints, within which a decision-level reinforcement learning agent learns to coordinate multiple effectors for optimal interception prioritization. Operating in a discrete action space, the agent selects which drone to engage per effector based on observed state features such as positions, classes, and effector status. We evaluate the learned policy against a handcrafted rule-based baseline across hundreds of simulated attack scenarios. The reinforcement learning based policy consistently achieves lower average damage and higher defensive efficiency in protecting critical zones. This case study highlights the potential of reinforcement learning as a strategic layer within defense architectures, enhancing resilience without displacing existing control systems. All code and simulation assets are publicly released for full reproducibility, and a video demonstration illustrates the policy's qualitative behavior.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Similarity-Confidence and Confidence-Difference</title>
<link>https://arxiv.org/abs/2508.05108</link>
<guid>https://arxiv.org/abs/2508.05108</guid>
<content:encoded><![CDATA[
arXiv:2508.05108v2 Announce Type: replace 
Abstract: In practical machine learning applications, it is often challenging to assign accurate labels to data, and increasing the number of labeled instances is often limited. In such cases, Weakly Supervised Learning (WSL), which enables training with incomplete or imprecise supervision, provides a practical and effective solution. However, most existing WSL methods focus on leveraging a single type of weak supervision. In this paper, we propose a novel WSL framework that leverages complementary weak supervision signals from multiple relational perspectives, which can be especially valuable when labeled data is limited. Specifically, we introduce SconfConfDiff Classification, a method that integrates two distinct forms of weaklabels: similarity-confidence and confidence-difference, which are assigned to unlabeled data pairs. To implement this method, we derive two types of unbiased risk estimators for classification: one based on a convex combination of existing estimators, and another newly designed by modeling the interaction between two weak labels. We prove that both estimators achieve optimal convergence rates with respect to estimation error bounds. Furthermore, we introduce a risk correction approach to mitigate overfitting caused by negative empirical risk, and provide theoretical analysis on the robustness of the proposed method against inaccurate class prior probability and label noise. Experimental results demonstrate that the proposed method consistently outperforms existing baselines across a variety of settings.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.07809</link>
<guid>https://arxiv.org/abs/2508.07809</guid>
<content:encoded><![CDATA[
arXiv:2508.07809v3 Announce Type: replace 
Abstract: Reinforcement learning with verifiable reward (RLVR) has become a promising paradigm for post-training large language models (LLMs) to improve their reasoning capability. However, when the rollout accuracy is low on hard problems, the reward becomes sparse, limiting learning efficiency and causing exploration bottlenecks. Existing approaches either rely on teacher models for distillation or filter out difficult problems, which limits scalability or restricts reasoning improvement through exploration.
  We propose EvoCoT, a self-evolving curriculum learning framework based on two-stage chain-of-thought (CoT) reasoning optimization. EvoCoT constrains the exploration space by self-generating and verifying CoT trajectories, then gradually shortens CoT steps to expand the space in a controlled way. The framework enables LLMs to stably learn from initially unsolved hard problems under sparse rewards. We apply EvoCoT to multiple LLM families, including Qwen, DeepSeek, and Llama. Experiments show that EvoCoT enables LLMs to solve previously unsolved problems, improves reasoning capability without external CoT supervision, and is compatible with various RL fine-tuning methods. We release the source code to support future research.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causally-Guided Pairwise Transformer -- Towards Foundational Digital Twins in Process Industry</title>
<link>https://arxiv.org/abs/2508.13111</link>
<guid>https://arxiv.org/abs/2508.13111</guid>
<content:encoded><![CDATA[
arXiv:2508.13111v2 Announce Type: replace 
Abstract: Foundational modelling of multi-dimensional time-series data in industrial systems presents a central trade-off: channel-dependent (CD) models capture specific cross-variable dynamics but lack robustness and adaptability as model layers are commonly bound to the data dimensionality of the tackled use-case, while channel-independent (CI) models offer generality at the cost of modelling the explicit interactions crucial for system-level predictive regression tasks. To resolve this, we propose the Causally-Guided Pairwise Transformer (CGPT), a novel architecture that integrates a known causal graph as an inductive bias. The core of CGPT is built around a pairwise modeling paradigm, tackling the CD/CI conflict by decomposing the multidimensional data into pairs. The model uses channel-agnostic learnable layers where all parameter dimensions are independent of the number of variables. CGPT enforces a CD information flow at the pair-level and CI-like generalization across pairs. This approach disentangles complex system dynamics and results in a highly flexible architecture that ensures scalability and any-variate adaptability. We validate CGPT on a suite of synthetic and real-world industrial datasets on long-term and one-step forecasting tasks designed to simulate common industrial complexities. Results demonstrate that CGPT significantly outperforms both CI and CD baselines in predictive accuracy and shows competitive performance with end-to-end trained CD models while remaining agnostic to the problem dimensionality.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neuro-inspired Ensemble-to-Ensemble Communication Primitives for Sparse and Efficient ANNs</title>
<link>https://arxiv.org/abs/2508.14140</link>
<guid>https://arxiv.org/abs/2508.14140</guid>
<content:encoded><![CDATA[
arXiv:2508.14140v2 Announce Type: replace 
Abstract: The structure of biological neural circuits-modular, hierarchical, and sparsely interconnected-reflects an efficient trade-off between wiring cost, functional specialization, and robustness. These principles offer valuable insights for artificial neural network (ANN) design, especially as networks grow in depth and scale. Sparsity, in particular, has been widely explored for reducing memory and computation, improving speed, and enhancing generalization. Motivated by systems neuroscience findings, we explore how patterns of functional connectivity in the mouse visual cortex-specifically, ensemble-to-ensemble communication, can inform ANN design. We introduce G2GNet, a novel architecture that imposes sparse, modular connectivity across feedforward layers. Despite having significantly fewer parameters than fully connected models, G2GNet achieves superior accuracy on standard vision benchmarks. To our knowledge, this is the first architecture to incorporate biologically observed functional connectivity patterns as a structural bias in ANN design. We complement this static bias with a dynamic sparse training (DST) mechanism that prunes and regrows edges during training. We also propose a Hebbian-inspired rewiring rule based on activation correlations, drawing on principles of biological plasticity. G2GNet achieves up to 75% sparsity while improving accuracy by up to 4.3% on benchmarks, including Fashion-MNIST, CIFAR-10, and CIFAR-100, outperforming dense baselines with far fewer computations.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Side Effects of Erasing Concepts from Diffusion Models</title>
<link>https://arxiv.org/abs/2508.15124</link>
<guid>https://arxiv.org/abs/2508.15124</guid>
<content:encoded><![CDATA[
arXiv:2508.15124v3 Announce Type: replace 
Abstract: Concerns about text-to-image (T2I) generative models infringing on privacy, copyright, and safety have led to the development of concept erasure techniques (CETs). The goal of an effective CET is to prohibit the generation of undesired "target" concepts specified by the user, while preserving the ability to synthesize high-quality images of other concepts. In this work, we demonstrate that concept erasure has side effects and CETs can be easily circumvented. For a comprehensive measurement of the robustness of CETs, we present the Side Effect Evaluation (SEE) benchmark that consists of hierarchical and compositional prompts describing objects and their attributes. The dataset and an automated evaluation pipeline quantify side effects of CETs across three aspects: impact on neighboring concepts, evasion of targets, and attribute leakage. Our experiments reveal that CETs can be circumvented by using superclass-subclass hierarchy, semantically similar prompts, and compositional variants of the target. We show that CETs suffer from attribute leakage and a counterintuitive phenomenon of attention concentration or dispersal. We release our benchmark and evaluation tools to aid future work on robust concept erasure.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval Enhanced Feedback via In-context Neural Error-book</title>
<link>https://arxiv.org/abs/2508.16313</link>
<guid>https://arxiv.org/abs/2508.16313</guid>
<content:encoded><![CDATA[
arXiv:2508.16313v3 Announce Type: replace 
Abstract: Recent advancements in Large Language Models (LLMs) have significantly improved reasoning capabilities, with in-context learning (ICL) emerging as a key technique for adaptation without retraining. While previous works have focused on leveraging correct examples, recent research highlights the importance of learning from errors to enhance performance. However, existing methods lack a structured framework for analyzing and mitigating errors, particularly in Multimodal Large Language Models (MLLMs), where integrating visual and textual inputs adds complexity. To address this issue, we propose REFINE: Retrieval-Enhanced Feedback via In-context Neural Error-book, a teacher-student framework that systematically structures errors and provides targeted feedback. REFINE introduces three systematic queries to construct structured feedback -- Feed-Target, Feed-Check, and Feed-Path -- to enhance multimodal reasoning by prioritizing relevant visual information, diagnosing critical failure points, and formulating corrective actions. Unlike prior approaches that rely on redundant retrievals, REFINE optimizes structured feedback retrieval, improving inference efficiency, token usage, and scalability. Our results demonstrate substantial speedup, reduced computational costs, and successful generalization, highlighting REFINE's potential for enhancing multimodal reasoning.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rectified Robust Policy Optimization for Model-Uncertain Constrained Reinforcement Learning without Strong Duality</title>
<link>https://arxiv.org/abs/2508.17448</link>
<guid>https://arxiv.org/abs/2508.17448</guid>
<content:encoded><![CDATA[
arXiv:2508.17448v2 Announce Type: replace 
Abstract: The goal of robust constrained reinforcement learning (RL) is to optimize an agent's performance under the worst-case model uncertainty while satisfying safety or resource constraints. In this paper, we demonstrate that strong duality does not generally hold in robust constrained RL, indicating that traditional primal-dual methods may fail to find optimal feasible policies. To overcome this limitation, we propose a novel primal-only algorithm called Rectified Robust Policy Optimization (RRPO), which operates directly on the primal problem without relying on dual formulations. We provide theoretical convergence guarantees under mild regularity assumptions, showing convergence to an approximately optimal feasible policy with iteration complexity matching the best-known lower bound when the uncertainty set diameter is controlled in a specific level. Empirical results in a grid-world environment validate the effectiveness of our approach, demonstrating that RRPO achieves robust and safe performance under model uncertainties while the non-robust method can violate the worst-case safety constraints.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STRATA-TS: Selective Knowledge Transfer for Urban Time Series Forecasting with Retrieval-Guided Reasoning</title>
<link>https://arxiv.org/abs/2508.18635</link>
<guid>https://arxiv.org/abs/2508.18635</guid>
<content:encoded><![CDATA[
arXiv:2508.18635v2 Announce Type: replace 
Abstract: Urban forecasting models often face a severe data imbalance problem: only a few cities have dense, long-span records, while many others expose short or incomplete histories. Direct transfer from data-rich to data-scarce cities is unreliable because only a limited subset of source patterns truly benefits the target domain, whereas indiscriminate transfer risks introducing noise and negative transfer. We present STRATA-TS (Selective TRAnsfer via TArget-aware retrieval for Time Series), a framework that combines domain-adapted retrieval with reasoning-capable large models to improve forecasting in scarce data regimes. STRATA-TS employs a patch-based temporal encoder to identify source subsequences that are semantically and dynamically aligned with the target query. These retrieved exemplars are then injected into a retrieval-guided reasoning stage, where an LLM performs structured inference over target inputs and retrieved support. To enable efficient deployment, we distill the reasoning process into a compact open model via supervised fine-tuning. Extensive experiments on three parking availability datasets across Singapore, Nottingham, and Glasgow demonstrate that STRATA-TS consistently outperforms strong forecasting and transfer baselines, while providing interpretable knowledge transfer pathways.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>(DEMO) Deep Reinforcement Learning Based Resource Allocation in Distributed IoT Systems</title>
<link>https://arxiv.org/abs/2508.19318</link>
<guid>https://arxiv.org/abs/2508.19318</guid>
<content:encoded><![CDATA[
arXiv:2508.19318v2 Announce Type: replace 
Abstract: Deep Reinforcement Learning (DRL) has emerged as an efficient approach to resource allocation due to its strong capability in handling complex decision-making tasks. However, only limited research has explored the training of DRL models with real-world data in practical, distributed Internet of Things (IoT) systems. To bridge this gap, this paper proposes a novel framework for training DRL models in real-world distributed IoT environments. In the proposed framework, IoT devices select communication channels using a DRL-based method, while the DRL model is trained with feedback information. Specifically, Acknowledgment (ACK) information is obtained from actual data transmissions over the selected channels. Implementation and performance evaluation, in terms of Frame Success Rate (FSR), are carried out, demonstrating both the feasibility and the effectiveness of the proposed framework.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using item recommendations and LLMs in marketing email titles</title>
<link>https://arxiv.org/abs/2508.20024</link>
<guid>https://arxiv.org/abs/2508.20024</guid>
<content:encoded><![CDATA[
arXiv:2508.20024v3 Announce Type: replace 
Abstract: E-commerce marketplaces make use of a number of marketing channels like emails, push notifications, etc. to reach their users and stimulate purchases. Personalized emails especially are a popular touch point for marketers to inform users of latest items in stock, especially for those who stopped visiting the marketplace. Such emails contain personalized recommendations tailored to each user's interests, enticing users to buy relevant items. A common limitation of these emails is that the primary entry point, the title of the email, tends to follow fixed templates, failing to inspire enough interest in the contents. In this work, we explore the potential of large language models (LLMs) for generating thematic titles that reflect the personalized content of the emails. We perform offline simulations and conduct online experiments on the order of millions of users, finding our techniques useful in improving the engagement between customers and our emails. We highlight key findings and learnings as we productionize the safe and automated generation of email titles for millions of users.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information-Theoretic Bounds and Task-Centric Learning Complexity for Real-World Dynamic Nonlinear Systems</title>
<link>https://arxiv.org/abs/2509.06599</link>
<guid>https://arxiv.org/abs/2509.06599</guid>
<content:encoded><![CDATA[
arXiv:2509.06599v2 Announce Type: replace 
Abstract: Dynamic nonlinear systems exhibit distortions arising from coupled static and dynamic effects. Their intertwined nature poses major challenges for data-driven modeling. This paper presents a theoretical framework grounded in structured decomposition, variance analysis, and task-centric complexity bounds.
  The framework employs a directional lower bound on interactions between measurable system components, extending orthogonality in inner product spaces to structurally asymmetric settings. This bound supports variance inequalities for decomposed systems. Key behavioral indicators are introduced along with a memory finiteness index. A rigorous power-based condition establishes a measurable link between finite memory in realizable systems and the First Law of Thermodynamics. This offers a more foundational perspective than classical bounds based on the Second Law.
  Building on this foundation, we formulate a `Behavioral Uncertainty Principle,' demonstrating that static and dynamic distortions cannot be minimized simultaneously. We identify that real-world systems seem to resist complete deterministic decomposition due to entangled static and dynamic effects. We also present two general-purpose theorems linking function variance to mean-squared Lipschitz continuity and learning complexity. This yields a model-agnostic, task-aware complexity metric, showing that lower-variance components are inherently easier to learn.
  These insights explain the empirical benefits of structured residual learning, including improved generalization, reduced parameter count, and lower training cost, as previously observed in power amplifier linearization experiments. The framework is broadly applicable and offers a scalable, theoretically grounded approach to modeling complex dynamic nonlinear systems.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised Learning in Open-World Scenarios</title>
<link>https://arxiv.org/abs/2509.09926</link>
<guid>https://arxiv.org/abs/2509.09926</guid>
<content:encoded><![CDATA[
arXiv:2509.09926v2 Announce Type: replace 
Abstract: Long-tailed learning has garnered increasing attention due to its wide applicability in real-world scenarios. Among existing approaches, Long-Tailed Semi-Supervised Learning (LTSSL) has emerged as an effective solution by incorporating a large amount of unlabeled data into the imbalanced labeled dataset. However, most prior LTSSL methods are designed to train models from scratch, which often leads to issues such as overconfidence and low-quality pseudo-labels. To address these challenges, we extend LTSSL into the foundation model fine-tuning paradigm and propose a novel framework: LoFT (Long-tailed semi-supervised learning via parameter-efficient Fine-Tuning). We demonstrate that fine-tuned foundation models can generate more reliable pseudolabels, thereby benefiting imbalanced learning. Furthermore, we explore a more practical setting by investigating semi-supervised learning under open-world conditions, where the unlabeled data may include out-of-distribution (OOD) samples. To handle this problem, we propose LoFT-OW (LoFT under Open-World scenarios) to improve the discriminative ability. Experimental results on multiple benchmarks demonstrate that our method achieves superior performance compared to previous approaches, even when utilizing only 1\% of the unlabeled data compared with previous works.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Interpretable Basis Extraction for Concept-Based Visual Explanations</title>
<link>https://arxiv.org/abs/2303.10523</link>
<guid>https://arxiv.org/abs/2303.10523</guid>
<content:encoded><![CDATA[
arXiv:2303.10523v3 Announce Type: replace-cross 
Abstract: An important line of research attempts to explain CNN image classifier predictions and intermediate layer representations in terms of human-understandable concepts. Previous work supports that deep representations are linearly separable with respect to their concept label, implying that the feature space has directions where intermediate representations may be projected onto, to become more understandable. These directions are called interpretable, and when considered as a set, they may form an interpretable feature space basis. Compared to previous top-down probing approaches which use concept annotations to identify the interpretable directions one at a time, in this work, we take a bottom-up approach, identifying the directions from the structure of the feature space, collectively, without relying on supervision from concept labels. Instead, we learn the directions by optimizing for a sparsity property that holds for any interpretable basis. We experiment with existing popular CNNs and demonstrate the effectiveness of our method in extracting an interpretable basis across network architectures and training datasets. We make extensions to existing basis interpretability metrics and show that intermediate layer representations become more interpretable when transformed with the extracted bases. Finally, we compare the bases extracted with our method with the bases derived with supervision and find that, in one aspect, unsupervised basis extraction has a strength that constitutes a limitation of learning the basis with supervision, and we provide potential directions for future research.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LieDetect: Detection of representation orbits of compact Lie groups from point clouds</title>
<link>https://arxiv.org/abs/2309.03086</link>
<guid>https://arxiv.org/abs/2309.03086</guid>
<content:encoded><![CDATA[
arXiv:2309.03086v3 Announce Type: replace-cross 
Abstract: We suggest a new algorithm to estimate representations of compact Lie groups from finite samples of their orbits. Different from other reported techniques, our method allows the retrieval of the precise representation type as a direct sum of irreducible representations. Moreover, the knowledge of the representation type permits the reconstruction of its orbit, which is useful for identifying the Lie group that generates the action, from a finite list of candidates. Our algorithm is general for any compact Lie group, but only instantiations for SO(2), T^d, SU(2), and SO(3) are considered. Theoretical guarantees of robustness in terms of Hausdorff and Wasserstein distances are derived. Our tools are drawn from geometric measure theory, computational geometry, and optimization on matrix manifolds. The algorithm is tested for synthetic data up to dimension 32, as well as real-life applications in image analysis, harmonic analysis, density estimation, equivariant neural networks, chemical conformational spaces, and classical mechanics systems, achieving very accurate results.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SINF: Semantic Neural Network Inference with Semantic Subgraphs</title>
<link>https://arxiv.org/abs/2310.01259</link>
<guid>https://arxiv.org/abs/2310.01259</guid>
<content:encoded><![CDATA[
arXiv:2310.01259v3 Announce Type: replace-cross 
Abstract: This paper proposes Semantic Inference (SINF) that creates semantic subgraphs in a Deep Neural Network(DNN) based on a new Discriminative Capability Score (DCS) to drastically reduce the DNN computational load with limited performance loss.~We evaluate the performance SINF on VGG16, VGG19, and ResNet50 DNNs trained on CIFAR100 and a subset of the ImageNet dataset. Moreover, we compare its performance against 6 state-of-the-art pruning approaches. Our results show that (i) on average, SINF reduces the inference time of VGG16, VGG19, and ResNet50 respectively by up to 29%, 35%, and 15% with only 3.75%, 0.17%, and 6.75% accuracy loss for CIFAR100 while for ImageNet benchmark, the reduction in inference time is 18%, 22%, and 9% for accuracy drop of 3%, 2.5%, and 6%; (ii) DCS achieves respectively up to 3.65%, 4.25%, and 2.36% better accuracy with VGG16, VGG19, and ResNet50 with respect to existing discriminative scores for CIFAR100 and the same for ImageNet is 8.9%, 5.8%, and 5.2% respectively. Through experimental evaluation on Raspberry Pi and NVIDIA Jetson Nano, we show SINF is about 51% and 38% more energy efficient and takes about 25% and 17% less inference time than the base model for CIFAR100 and ImageNet.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Discovery of PDEs via the Adjoint Method</title>
<link>https://arxiv.org/abs/2401.17177</link>
<guid>https://arxiv.org/abs/2401.17177</guid>
<content:encoded><![CDATA[
arXiv:2401.17177v5 Announce Type: replace-cross 
Abstract: In this work, we present an adjoint-based method for discovering the underlying governing partial differential equations (PDEs) given data. The idea is to consider a parameterized PDE in a general form and formulate a PDE-constrained optimization problem aimed at minimizing the error of the PDE solution from data. Using variational calculus, we obtain an evolution equation for the Lagrange multipliers (adjoint equations) allowing us to compute the gradient of the objective function with respect to the parameters of PDEs given data in a straightforward manner. In particular, we consider a family of parameterized PDEs encompassing linear, nonlinear, and spatial derivative candidate terms, and elegantly derive the corresponding adjoint equations. We show the efficacy of the proposed approach in identifying the form of the PDE up to machine accuracy, enabling the accurate discovery of PDEs from data. We also compare its performance with the famous PDE Functional Identification of Nonlinear Dynamics method known as PDE-FIND (Rudy et al., 2017), on both smooth and noisy data sets. Even though the proposed adjoint method relies on forward/backward solvers, it outperforms PDE-FIND for large data sets thanks to the analytic expressions for gradients of the cost function with respect to each PDE parameter.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Efficient LLMs</title>
<link>https://arxiv.org/abs/2402.14746</link>
<guid>https://arxiv.org/abs/2402.14746</guid>
<content:encoded><![CDATA[
arXiv:2402.14746v4 Announce Type: replace-cross 
Abstract: Trained LLMs in the transformer architecture are typically sparse in that most of the parameters are negligible, raising questions on efficiency. Furthermore, the so called "AI scaling law" for transformers suggests that the number of parameters must scale linearly with the size of the data. In response, we inquire into efficient LLMs, i.e. those with the fewest parameters that achieve the desired accuracy on a training corpus. Specifically, by comparing theoretical and empirical estimates of the Kullback-Liebler divergence, we derive a natural AI scaling law that the number of parameters in an efficient LLM scales as $D^{\gamma}$ where $D$ is the size of the training data and $ \gamma \in [0.44, 0.72]$, suggesting the existence of more efficient architectures. Against this backdrop, we propose recurrent transformers, combining the efficacy of transformers with the efficiency of recurrent networks, progressively applying a single transformer layer to a fixed-width sliding window across the input sequence. Recurrent transformers (a) run in linear time in the sequence length, (b) are memory-efficient and amenable to parallel processing in large batches, (c) learn to forget history for language tasks, or accumulate history for long range tasks like copy and selective copy, and (d) are amenable to curriculum training to overcome vanishing gradients. In our experiments, we find that recurrent transformers perform favorably on benchmark tests.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Block-Fused Attention-Driven Adaptively-Pooled ResNet Model for Improved Cervical Cancer Classification</title>
<link>https://arxiv.org/abs/2405.01600</link>
<guid>https://arxiv.org/abs/2405.01600</guid>
<content:encoded><![CDATA[
arXiv:2405.01600v3 Announce Type: replace-cross 
Abstract: Cervical cancer is the second most common cancer among women and a leading cause of mortality. Many attempts have been made to develop an effective Computer Aided Diagnosis (CAD) system; however, their performance remains limited. Using pretrained ResNet-50/101/152, we propose a novel CAD system that significantly outperforms prior approaches.
  Our novel model has three key components. First, we extract detailed features (color, edges, and texture) from early convolution blocks and the abstract features (shapes and objects) from later blocks, as both are equally important. This dual-level feature extraction is a new paradigm in cancer classification. Second, a non-parametric 3D attention module is uniquely embedded within each block for feature enhancement. Third, we design a theoretically motivated innovative adaptive pooling strategy for feature selection that applies Global Max Pooling to detailed features and Global Average Pooling to abstract features. These components form our Proposed Block-Fused Attention-Driven Adaptively-Pooled ResNet (BF-AD-AP-ResNet) model. To further strengthen learning, we introduce a Tri-Stream model, which unifies the enhanced features from three BF-AD-AP-ResNets. An SVM classifier is employed for final classification.
  We evaluate our models on two public datasets, IARC and AnnoCerv. On IARC, the base ResNets achieve an average performance of 90.91%, while our model achieves an excellent performance of 98.63%. On AnnoCerv, the base ResNets reach to 87.68%, and our model improves this significantly, reaching 93.39%. Our approach outperforms the best existing method on IARC by an average of 14.55%. For AnnoCerv, no prior competitive works are available. Additionally, we introduce a novel SHAP+LIME explainability method, accurately identifying the cancerous region in 97% of cases, ensuring model reliability for real-world use.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GN-SINDy: Greedy Sampling Neural Network in Sparse Identification of Nonlinear Partial Differential Equations</title>
<link>https://arxiv.org/abs/2405.08613</link>
<guid>https://arxiv.org/abs/2405.08613</guid>
<content:encoded><![CDATA[
arXiv:2405.08613v2 Announce Type: replace-cross 
Abstract: The sparse identification of nonlinear dynamical systems (SINDy) is a data-driven technique employed for uncovering and representing the fundamental dynamics of intricate systems based on observational data. However, a primary obstacle in the discovery of models for nonlinear partial differential equations (PDEs) lies in addressing the challenges posed by the curse of dimensionality and large datasets. Consequently, the strategic selection of the most informative samples within a given dataset plays a crucial role in reducing computational costs and enhancing the effectiveness of SINDy-based algorithms. To this aim, we employ a greedy sampling approach to the snapshot matrix of a PDE to obtain its valuable samples, which are suitable to train a deep neural network (DNN) in a SINDy framework. SINDy based algorithms often consist of a data collection unit, constructing a dictionary of basis functions, computing the time derivative, and solving a sparse identification problem which ends to regularised least squares minimization. In this paper, we extend the results of a SINDy based deep learning model discovery (DeePyMoD) approach by integrating greedy sampling technique in its data collection unit and new sparsity promoting algorithms in the least squares minimization unit. In this regard we introduce the greedy sampling neural network in sparse identification of nonlinear partial differential equations (GN-SINDy) which blends a greedy sampling method, the DNN, and the SINDy algorithm. In the implementation phase, to show the effectiveness of GN-SINDy, we compare its results with DeePyMoD by using a Python package that is prepared for this purpose on numerous PDE discovery
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Test-Time Multimodal Backdoor Detection by Contrastive Prompting</title>
<link>https://arxiv.org/abs/2405.15269</link>
<guid>https://arxiv.org/abs/2405.15269</guid>
<content:encoded><![CDATA[
arXiv:2405.15269v3 Announce Type: replace-cross 
Abstract: While multimodal contrastive learning methods (e.g., CLIP) can achieve impressive zero-shot classification performance, recent research has revealed that these methods are vulnerable to backdoor attacks. To defend against backdoor attacks on CLIP, existing defense methods focus on either the pre-training stage or the fine-tuning stage, which would unfortunately cause high computational costs due to numerous parameter updates and are not applicable in black-box settings. In this paper, we provide the first attempt at a computationally efficient backdoor detection method to defend against backdoored CLIP in the \emph{inference} stage. We empirically find that the visual representations of backdoored images are \emph{insensitive} to \emph{benign} and \emph{malignant} changes in class description texts. Motivated by this observation, we propose BDetCLIP, a novel test-time backdoor detection method based on contrastive prompting. Specifically, we first prompt a language model (e.g., GPT-4) to produce class-related description texts (benign) and class-perturbed random texts (malignant) by specially designed instructions. Then, the distribution difference in cosine similarity between images and the two types of class description texts can be used as the criterion to detect backdoor samples. Extensive experiments validate that our proposed BDetCLIP is superior to state-of-the-art backdoor detection methods, in terms of both effectiveness and efficiency. Our codes are publicly available at: https://github.com/Purshow/BDetCLIP.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CEBench: A Benchmarking Toolkit for the Cost-Effectiveness of LLM Pipelines</title>
<link>https://arxiv.org/abs/2407.12797</link>
<guid>https://arxiv.org/abs/2407.12797</guid>
<content:encoded><![CDATA[
arXiv:2407.12797v2 Announce Type: replace-cross 
Abstract: Online Large Language Model (LLM) services such as ChatGPT and Claude 3 have transformed business operations and academic research by effortlessly enabling new opportunities. However, due to data-sharing restrictions, sectors such as healthcare and finance prefer to deploy local LLM applications using costly hardware resources. This scenario requires a balance between the effectiveness advantages of LLMs and significant financial burdens. Additionally, the rapid evolution of models increases the frequency and redundancy of benchmarking efforts. Existing benchmarking toolkits, which typically focus on effectiveness, often overlook economic considerations, making their findings less applicable to practical scenarios. To address these challenges, we introduce CEBench, an open-source toolkit specifically designed for multi-objective benchmarking that focuses on the critical trade-offs between expenditure and effectiveness required for LLM deployments. CEBench allows for easy modifications through configuration files, enabling stakeholders to effectively assess and optimize these trade-offs. This strategic capability supports crucial decision-making processes aimed at maximizing effectiveness while minimizing cost impacts. By streamlining the evaluation process and emphasizing cost-effectiveness, CEBench seeks to facilitate the development of economically viable AI solutions across various industries and research fields. The code and demonstration are available in https://github.com/amademicnoboday12/CEBench.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extract and Diffuse: Latent Integration for Improved Diffusion-based Speech and Vocal Enhancement</title>
<link>https://arxiv.org/abs/2409.09642</link>
<guid>https://arxiv.org/abs/2409.09642</guid>
<content:encoded><![CDATA[
arXiv:2409.09642v2 Announce Type: replace-cross 
Abstract: Diffusion-based generative models have recently achieved remarkable results in speech and vocal enhancement due to their ability to model complex speech data distributions. While these models generalize well to unseen acoustic environments, they may not achieve the same level of fidelity as the discriminative models specifically trained to enhance particular acoustic conditions. In this paper, we propose Ex-Diff, a novel score-based diffusion model that integrates the latent representations produced by a discriminative model to improve speech and vocal enhancement, which combines the strengths of both generative and discriminative models. Experimental results on the widely used MUSDB dataset show relative improvements of 3.7% in SI-SDR and 10.0% in SI-SIR compared to the baseline diffusion model for speech and vocal enhancement tasks, respectively. Additionally, case studies are provided to further illustrate and analyze the complementary nature of generative and discriminative models in this context.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Error Correction Code Transformer: From Non-Unified to Unified</title>
<link>https://arxiv.org/abs/2410.03364</link>
<guid>https://arxiv.org/abs/2410.03364</guid>
<content:encoded><![CDATA[
arXiv:2410.03364v3 Announce Type: replace-cross 
Abstract: Channel coding is vital for reliable data transmission in modern wireless systems, and its significance will increase with the emergence of sixth-generation (6G) networks, which will need to support various error correction codes. However, traditional decoders were typically designed as fixed hardware circuits tailored to specific decoding algorithms, leading to inefficiencies and limited flexibility. To address these challenges, this paper proposes a unified, code-agnostic Transformer-based decoding architecture capable of handling multiple linear block codes, including Polar, Low-Density Parity-Check (LDPC), and Bose-Chaudhuri-Hocquenghem (BCH), within a single framework. To achieve this, standardized units are employed to harmonize parameters across different code types, while the redesigned unified attention module compresses the structural information of various codewords. Additionally, a sparse mask, derived from the sparsity of the parity-check matrix, is introduced to enhance the model's ability to capture inherent constraints between information and parity-check bits, resulting in improved decoding accuracy and robustness. Extensive experimental results demonstrate that the proposed unified Transformer-based decoder not only outperforms existing methods but also provides a flexible, efficient, and high-performance solution for next-generation wireless communication systems.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian scaling laws for in-context learning</title>
<link>https://arxiv.org/abs/2410.16531</link>
<guid>https://arxiv.org/abs/2410.16531</guid>
<content:encoded><![CDATA[
arXiv:2410.16531v4 Announce Type: replace-cross 
Abstract: In-context learning (ICL) is a powerful technique for getting language models to perform complex tasks with no training updates. Prior work has established strong correlations between the number of in-context examples provided and the accuracy of the model's predictions. In this paper, we seek to explain this correlation by showing that ICL approximates a Bayesian learner. This perspective gives rise to a novel Bayesian scaling law for ICL. In experiments with \mbox{GPT-2} models of different sizes, our scaling law matches existing scaling laws in accuracy while also offering interpretable terms for task priors, learning efficiency, and per-example probabilities. To illustrate the analytic power that such interpretable scaling laws provide, we report on controlled synthetic dataset experiments designed to inform real-world studies of safety alignment. In our experimental protocol, we use SFT or DPO to suppress an unwanted existing model capability and then use ICL to try to bring that capability back (many-shot jailbreaking). We then study ICL on real-world instruction-tuned LLMs using capabilities benchmarks as well as a new many-shot jailbreaking dataset. In all cases, Bayesian scaling laws accurately predict the conditions under which ICL will cause suppressed behaviors to reemerge, which sheds light on the ineffectiveness of post-training at increasing LLM safety.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Networks and (Virtual) Extended Formulations</title>
<link>https://arxiv.org/abs/2411.03006</link>
<guid>https://arxiv.org/abs/2411.03006</guid>
<content:encoded><![CDATA[
arXiv:2411.03006v3 Announce Type: replace-cross 
Abstract: Neural networks with piecewise linear activation functions, such as rectified linear units (ReLU) or maxout, are among the most fundamental models in modern machine learning. We make a step towards proving lower bounds on the size of such neural networks by linking their representative capabilities to the notion of the extension complexity $\mathrm{xc}(P)$ of a polytope $P$. This is a well-studied quantity in combinatorial optimization and polyhedral geometry describing the number of inequalities needed to model $P$ as a linear program. We show that $\mathrm{xc}(P)$ is a lower bound on the size of any monotone or input-convex neural network that solves the linear optimization problem over $P$. This implies exponential lower bounds on such neural networks for a variety of problems, including the polynomially solvable maximum weight matching problem.
  In an attempt to prove similar bounds also for general neural networks, we introduce the notion of virtual extension complexity $\mathrm{vxc}(P)$, which generalizes $\mathrm{xc}(P)$ and describes the number of inequalities needed to represent the linear optimization problem over $P$ as a difference of two linear programs. We prove that $\mathrm{vxc}(P)$ is a lower bound on the size of any neural network that optimizes over $P$. While it remains an open question to derive useful lower bounds on $\mathrm{vxc}(P)$, we argue that this quantity deserves to be studied independently from neural networks by proving that one can efficiently optimize over a polytope $P$ using a small virtual extended formulation.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measure-to-measure interpolation using Transformers</title>
<link>https://arxiv.org/abs/2411.04551</link>
<guid>https://arxiv.org/abs/2411.04551</guid>
<content:encoded><![CDATA[
arXiv:2411.04551v2 Announce Type: replace-cross 
Abstract: Transformers are deep neural network architectures that underpin the recent successes of large language models. Unlike more classical architectures that can be viewed as point-to-point maps, a Transformer acts as a measure-to-measure map implemented as specific interacting particle system on the unit sphere: the input is the empirical measure of tokens in a prompt and its evolution is governed by the continuity equation. In fact, Transformers are not limited to empirical measures and can in principle process any input measure. As the nature of data processed by Transformers is expanding rapidly, it is important to investigate their expressive power as maps from an arbitrary measure to another arbitrary measure. To that end, we provide an explicit choice of parameters that allows a single Transformer to match $N$ arbitrary input measures to $N$ arbitrary target measures, under the minimal assumption that every pair of input-target measures can be matched by some transport map.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Deep Learning Methods Suitable for Downscaling Global Climate Projections? An Intercomparison for Temperature and Precipitation over Spain</title>
<link>https://arxiv.org/abs/2411.05850</link>
<guid>https://arxiv.org/abs/2411.05850</guid>
<content:encoded><![CDATA[
arXiv:2411.05850v2 Announce Type: replace-cross 
Abstract: Deep Learning (DL) has shown promise for downscaling global climate change projections under different approaches, including Perfect Prognosis (PP) and Regional Climate Model (RCM) emulation. Unlike emulators, PP downscaling models are trained on observational data, so it remains an open question whether they can plausibly extrapolate unseen conditions and changes in future emissions scenarios. Here we focus on this problem as the main drawback for the operationalization of these methods and present the results of an intercomparison experiment to evaluate the performance and extrapolation capability of existing models using a common experimental framework, taking into account the sensitivity of results to different training replicas. We focus on minimum and maximum temperatures and precipitation over Spain, a region with a range of climatic conditions with different influential regional processes. We conclude with a discussion of the findings, limitations of existing methods, and prospects for future development.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Validation-Free Sparse Learning: A Phase Transition Approach to Feature Selection</title>
<link>https://arxiv.org/abs/2411.17180</link>
<guid>https://arxiv.org/abs/2411.17180</guid>
<content:encoded><![CDATA[
arXiv:2411.17180v4 Announce Type: replace-cross 
Abstract: The growing environmental footprint of artificial intelligence (AI), especially in terms of storage and computation, calls for more frugal and interpretable models. Sparse models (e.g., linear, neural networks) offer a promising solution by selecting only the most relevant features, reducing complexity, preventing over-fitting and enabling interpretation-marking a step towards truly intelligent AI.
  The concept of a right amount of sparsity (without too many false positive or too few true positive) is subjective. So we propose a new paradigm previously only observed and mathematically studied for compressed sensing (noiseless linear models): obtaining a phase transition in the probability of retrieving the relevant features. We show in practice how to obtain this phase transition for a class of sparse learners. Our approach is flexible and applicable to complex models ranging from linear to shallow and deep artificial neural networks while supporting various loss functions and sparsity-promoting penalties. It does not rely on cross-validation or on a validation set to select its single regularization parameter. For real-world data, it provides a good balance between predictive accuracy and feature sparsity.
  A Python package is available at https://github.com/VcMaxouuu/HarderLASSO containing all the simulations and ready-to-use models.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PACMANN: Point Adaptive Collocation Method for Artificial Neural Networks</title>
<link>https://arxiv.org/abs/2411.19632</link>
<guid>https://arxiv.org/abs/2411.19632</guid>
<content:encoded><![CDATA[
arXiv:2411.19632v2 Announce Type: replace-cross 
Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a tool for approximating the solution of Partial Differential Equations (PDEs) in both forward and inverse problems. PINNs minimize a loss function which includes the PDE residual determined for a set of collocation points. Previous work has shown that the number and distribution of these collocation points have a significant influence on the accuracy of the PINN solution. Therefore, the effective placement of these collocation points is an active area of research. Specifically, available adaptive collocation point sampling methods have been reported to scale poorly in terms of computational cost when applied to high-dimensional problems. In this work, we address this issue and present the Point Adaptive Collocation Method for Artificial Neural Networks (PACMANN). PACMANN incrementally moves collocation points toward regions of higher residuals using gradient-based optimization algorithms guided by the gradient of the PINN loss function, that is, the squared PDE residual. We apply PACMANN for forward and inverse problems, and demonstrate that this method matches the performance of state-of-the-art methods in terms of the accuracy/efficiency tradeoff for the low-dimensional problems, while outperforming available approaches for high-dimensional problems. Key features of the method include its low computational cost and simplicity of integration into existing physics-informed neural network pipelines. The code is available at https://github.com/CoenVisser/PACMANN.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bio-Inspired Adaptive Neurons for Dynamic Weighting in Artificial Neural Networks</title>
<link>https://arxiv.org/abs/2412.01454</link>
<guid>https://arxiv.org/abs/2412.01454</guid>
<content:encoded><![CDATA[
arXiv:2412.01454v2 Announce Type: replace-cross 
Abstract: Traditional neural networks employ fixed weights during inference, limiting their ability to adapt to changing input conditions, unlike biological neurons that adjust signal strength dynamically based on stimuli. This discrepancy between artificial and biological neurons constrains neural network flexibility and adaptability. To bridge this gap, we propose a novel framework for adaptive neural networks, where neuron weights are modeled as functions of the input signal, allowing the network to adjust dynamically in real-time. Importantly, we achieve this within the same traditional architecture of an Artificial Neural Network, maintaining structural familiarity while introducing dynamic adaptability. In our research, we apply Chebyshev polynomials as one of the many possible decomposition methods to achieve this adaptive weighting mechanism, with polynomial coefficients learned during training. Out of the 145 datasets tested, our adaptive Chebyshev neural network demonstrated a marked improvement over an equivalent MLP in approximately 8\% of cases, performing strictly better on 121 datasets. In the remaining 24 datasets, the performance of our algorithm matched that of the MLP, highlighting its ability to generalize standard neural network behavior while offering enhanced adaptability. As a generalized form of the MLP, this model seamlessly retains MLP performance where needed while extending its capabilities to achieve superior accuracy across a wide range of complex tasks. These results underscore the potential of adaptive neurons to enhance generalization, flexibility, and robustness in neural networks, particularly in applications with dynamic or non-linear data dependencies.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parallel Simulation for Log-concave Sampling and Score-based Diffusion Models</title>
<link>https://arxiv.org/abs/2412.07435</link>
<guid>https://arxiv.org/abs/2412.07435</guid>
<content:encoded><![CDATA[
arXiv:2412.07435v3 Announce Type: replace-cross 
Abstract: Sampling from high-dimensional probability distributions is fundamental in machine learning and statistics. As datasets grow larger, computational efficiency becomes increasingly important, particularly in reducing adaptive complexity, namely the number of sequential rounds required for sampling algorithms. While recent works have introduced several parallelizable techniques, they often exhibit suboptimal convergence rates and remain significantly weaker than the latest lower bounds for log-concave sampling. To address this, we propose a novel parallel sampling method that improves adaptive complexity dependence on dimension $d$ reducing it from $\widetilde{\mathcal{O}}(\log^2 d)$ to $\widetilde{\mathcal{O}}(\log d)$. which is even optimal for log-concave sampling with some specific adaptive complexity. Our approach builds on parallel simulation techniques from scientific computing.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Massive-scale Partial Correlation Networks in Clinical Multi-omics Studies with HP-ACCORD</title>
<link>https://arxiv.org/abs/2412.11554</link>
<guid>https://arxiv.org/abs/2412.11554</guid>
<content:encoded><![CDATA[
arXiv:2412.11554v4 Announce Type: replace-cross 
Abstract: Graphical model estimation from multi-omics data requires a balance between statistical estimation performance and computational scalability. We introduce a novel pseudolikelihood-based graphical model framework that reparameterizes the target precision matrix while preserving the sparsity pattern and estimates it by minimizing an $\ell_1$-penalized empirical risk based on a new loss function. The proposed estimator maintains estimation and selection consistency in various metrics under high-dimensional assumptions. The associated optimization problem allows for a provably fast computation algorithm using a novel operator-splitting approach and communication-avoiding distributed matrix multiplication. A high-performance computing implementation of our framework was tested using simulated data with up to one million variables, demonstrating complex dependency structures similar to those found in biological networks. Leveraging this scalability, we estimated a partial correlation network from a dual-omic liver cancer data set. The co-expression network estimated from the ultrahigh-dimensional data demonstrated superior specificity in prioritizing key transcription factors and co-activators by excluding the impact of epigenetic regulation, thereby highlighting the value of computational scalability in multi-omic data analysis.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving Partial Differential Equations with Random Feature Models</title>
<link>https://arxiv.org/abs/2501.00288</link>
<guid>https://arxiv.org/abs/2501.00288</guid>
<content:encoded><![CDATA[
arXiv:2501.00288v2 Announce Type: replace-cross 
Abstract: Machine learning based partial differential equations (PDEs) solvers have received great attention in recent years. Most progress in this area has been driven by deep neural networks such as physics-informed neural networks (PINNs) and kernel method. In this paper, we introduce a random feature based framework toward efficiently solving PDEs. Random feature method was originally proposed to approximate large-scale kernel machines and can be viewed as a shallow neural network as well. We provide an error analysis for our proposed method along with comprehensive numerical results on several PDE benchmarks. In contrast to the state-of-the-art solvers that face challenges with a large number of collocation points, our proposed method reduces the computational complexity. Moreover, the implementation of our method is simple and does not require additional computational resources. Due to the theoretical guarantee and advantages in computation, our approach is proven to be efficient for solving PDEs.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An AI-powered Bayesian generative modeling approach for causal inference in observational studies</title>
<link>https://arxiv.org/abs/2501.00755</link>
<guid>https://arxiv.org/abs/2501.00755</guid>
<content:encoded><![CDATA[
arXiv:2501.00755v2 Announce Type: replace-cross 
Abstract: Causal inference in observational studies with high-dimensional covariates presents significant challenges. We introduce CausalBGM, an AI-powered Bayesian generative modeling approach that captures the causal relationship among covariates, treatment, and outcome variables. The core innovation of CausalBGM lies in its ability to estimate the individual treatment effect (ITE) by learning individual-specific distributions of a low-dimensional latent feature set (e.g., latent confounders) that drives changes in both treatment and outcome. This approach not only effectively mitigates confounding effects but also provides comprehensive uncertainty quantification, offering reliable and interpretable causal effect estimates at the individual level. CausalBGM adopts a Bayesian model and uses a novel iterative algorithm to update the model parameters and the posterior distribution of latent features until convergence. This framework leverages the power of AI to capture complex dependencies among variables while adhering to the Bayesian principles. Extensive experiments demonstrate that CausalBGM consistently outperforms state-of-the-art methods, particularly in scenarios with high-dimensional covariates and large-scale datasets. Its Bayesian foundation ensures statistical rigor, providing robust and well-calibrated posterior intervals. By addressing key limitations of existing methods, CausalBGM emerges as a robust and promising framework for advancing causal inference in modern applications in fields such as genomics, healthcare, and social sciences. CausalBGM is maintained at the website https://causalbgm.readthedocs.io/.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tempo: Compiled Dynamic Deep Learning with Symbolic Dependence Graphs</title>
<link>https://arxiv.org/abs/2501.05408</link>
<guid>https://arxiv.org/abs/2501.05408</guid>
<content:encoded><![CDATA[
arXiv:2501.05408v2 Announce Type: replace-cross 
Abstract: Deep learning (DL) algorithms are often defined in terms of \emph{temporal relationships}: a tensor at one timestep may depend on tensors from earlier or later timesteps. Such \emph{dynamic} dependencies (and corresponding dynamic tensor shapes) are difficult to express and optimize: while \emph{eager} DL systems support such dynamism, they cannot apply compiler-based optimizations; \emph{graph-based} systems require static tensor shapes, which forces users to pad tensors or break-up programs into multiple static graphs.
  We describe Tempo, a new DL system that combines the dynamism of eager execution with the whole-program optimizations of graph-based compilation. Tempo achieves this through a declarative programming model with \emph{recurrent tensors}, which include explicit \emph{temporal dimensions}. Temporal dimensions can be indexed using \emph{symbolic expressions} to express dynamic dependencies on past and future tensors. Based on this, Tempo constructs a \emph{symbolic dependence graph}, which concisely encodes dynamic dependencies between operators, and applies whole-program optimizations, such as algebraic simplifications, vectorization, tiling, and fusion. By tiling dynamic dependencies into static-size blocks, Tempo can also reuse existing static code-generators. It then uses a polyhedral model to find a feasible execution schedule, which includes memory management operations. We show that Tempo achieves a 7$\times$ speedup over JAX for Llama-3.2-3B decoding; for reinforcement learning algorithms, Tempo achieves a 54$\times$ speedup, with 16$\times$ lower peak memory usage.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Transition State Searches by Freezing String Method with Graph Neural Network Potentials</title>
<link>https://arxiv.org/abs/2501.06159</link>
<guid>https://arxiv.org/abs/2501.06159</guid>
<content:encoded><![CDATA[
arXiv:2501.06159v2 Announce Type: replace-cross 
Abstract: Transition state (TS) searches are a critical bottleneck in computational studies of chemical reactivity, as accurately capturing complex phenomena like bond breaking and formation events requires repeated evaluations of expensive ab-initio potential energy surfaces (PESs). While numerous algorithms have been developed to locate TSs efficiently, the computational cost of PES evaluations remains a key limitation. In this work, we develop and fine-tune a graph neural network (GNN) PES to accelerate TS searches for organic reactions. Our GNN of choice, SchNet, is first pre-trained on the ANI-1 dataset and subsequently fine-tuned on a small dataset of reactant, product, and TS structures. We integrate this GNN PES into the Freezing String Method (FSM), enabling rapid generation of TS guess geometries. Across a benchmark suite of chemically diverse reactions, our fine-tuned model (GNN-FT) achieves a 100% success rate, locating the reference TSs in all cases while reducing the number of ab-initio calculations by 72% on average compared to conventional DFT-based FSM searches. Fine-tuning reduces GNN-FT errors by orders of magnitude for out-of-distribution cases such as non-covalent interactions, and improves TS-region predictions with comparatively little data. Analysis of transition state geometries and energy errors shows that GNN-FT captures PES along the reaction coordinate with sufficient accuracy to serve as a reliable DFT surrogate. These results demonstrate that modern GNN potentials, when properly trained, can significantly reduce the cost of TS searches and broaden the scope and size of systems considered in chemical reactivity studies.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OptiChat: Bridging Optimization Models and Practitioners with Large Language Models</title>
<link>https://arxiv.org/abs/2501.08406</link>
<guid>https://arxiv.org/abs/2501.08406</guid>
<content:encoded><![CDATA[
arXiv:2501.08406v2 Announce Type: replace-cross 
Abstract: Optimization models have been applied to solve a wide variety of decision-making problems. These models are usually developed by optimization experts but are used by practitioners without optimization expertise in various application domains. As a result, practitioners often struggle to interact with and draw useful conclusions from optimization models independently. To fill this gap, we introduce OptiChat, a natural language dialogue system designed to help practitioners interpret model formulation, diagnose infeasibility, analyze sensitivity, retrieve information, evaluate modifications, and provide counterfactual explanations. By augmenting large language models (LLMs) with functional calls and code generation tailored for optimization models, we enable seamless interaction and minimize the risk of hallucinations in OptiChat. We develop a new dataset to evaluate OptiChat's performance in explaining optimization models. Experiments demonstrate that OptiChat effectively bridges the gap between optimization models and practitioners, delivering autonomous, accurate, and instant responses.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>INTA: Intent-Based Translation for Network Configuration with LLM Agents</title>
<link>https://arxiv.org/abs/2501.08760</link>
<guid>https://arxiv.org/abs/2501.08760</guid>
<content:encoded><![CDATA[
arXiv:2501.08760v2 Announce Type: replace-cross 
Abstract: Translating configurations between different network devices is a common yet challenging task in modern network operations. This challenge arises in typical scenarios such as replacing obsolete hardware and adapting configurations to emerging paradigms like Software Defined Networking (SDN) and Network Function Virtualization (NFV). Engineers need to thoroughly understand both source and target configuration models, which requires considerable effort due to the complexity and evolving nature of these specifications. To promote automation in network configuration translation, we propose INTA, an intent-based translation framework that leverages Large Language Model (LLM) agents. The key idea of INTA is to use configuration intent as an intermediate representation for translation. It first employs LLMs to decompose configuration files and extract fine-grained intents for each configuration fragment. These intents are then used to retrieve relevant manuals of the target device. Guided by a syntax checker, INTA incrementally generates target configurations. The translated configurations are further verified and refined for semantic consistency. We implement INTA and evaluate it on real-world configuration datasets from the industry. Our approach outperforms state-of-the-art methods in translation accuracy and exhibits strong generalizability. INTA achieves an accuracy of 98.15% in terms of both syntactic and view correctness, and a command recall rate of 84.72% for the target configuration. The semantic consistency report of the translated configuration further demonstrates its practical value in real-world network operations.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention Sinks: A 'Catch, Tag, Release' Mechanism for Embeddings</title>
<link>https://arxiv.org/abs/2502.00919</link>
<guid>https://arxiv.org/abs/2502.00919</guid>
<content:encoded><![CDATA[
arXiv:2502.00919v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) often concentrate their attention on a few specific tokens referred to as attention sinks. Common examples include the first token, a prompt-independent sink, and punctuation tokens, which are prompt-dependent. While the tokens causing the sinks often lack direct semantic meaning, the presence of the sinks is critical for model performance, particularly under model compression and KV-caching. Despite their ubiquity, the function, semantic role, and origin of attention sinks -- especially those beyond the first token -- remain poorly understood. In this work, we conduct a comprehensive investigation demonstrating that attention sinks: catch a sequence of tokens, tag them using a common direction in embedding space, and release them back into the residual stream, where tokens are later retrieved based on the tags they have acquired. Probing experiments reveal these tags carry semantically meaningful information, such as the truth of a statement. These findings extend to reasoning models, where the mechanism spans more heads and explains greater variance in embeddings, or recent models with query-key normalization, where sinks remain just as prevalent. To encourage future theoretical analysis, we introduce a minimal problem which can be solved through the 'catch, tag, release' mechanism, and where it emerges through training.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D Cell Oversegmentation Correction via Geo-Wasserstein Divergence</title>
<link>https://arxiv.org/abs/2502.01890</link>
<guid>https://arxiv.org/abs/2502.01890</guid>
<content:encoded><![CDATA[
arXiv:2502.01890v3 Announce Type: replace-cross 
Abstract: 3D cell segmentation methods are often hindered by \emph{oversegmentation}, where a single cell is incorrectly split into multiple fragments. This degrades the final segmentation quality and is notoriously difficult to resolve, as oversegmentation errors often resemble natural gaps between adjacent cells. Our work makes two key contributions. First, for 3D cell segmentation, we are the first work to formulate oversegmentation as a concrete problem and propose a geometric framework to identify and correct these errors. Our approach builds a pre-trained classifier using both 2D geometric and 3D topological features extracted from flawed 3D segmentation results. Second, we introduce a novel metric, Geo-Wasserstein divergence, to quantify changes in 2D geometries. This captures the evolving trends of cell mask shape in a geometry-aware manner. We validate our method through extensive experiments on in-domain plant datasets, including both synthesized and real oversegmented cases, as well as on out-of-domain animal datasets to demonstrate transfer learning performance. An ablation study further highlights the contribution of the Geo-Wasserstein divergence. A clear pipeline is provided for end-users to build pre-trained models to any labeled dataset.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TranSQL+: Serving Large Language Models with SQL on Low-Resource Hardware</title>
<link>https://arxiv.org/abs/2502.02818</link>
<guid>https://arxiv.org/abs/2502.02818</guid>
<content:encoded><![CDATA[
arXiv:2502.02818v3 Announce Type: replace-cross 
Abstract: Deploying Large Language Models (LLMs) on resource-constrained devices remains challenging due to limited memory, lack of GPUs, and the complexity of existing runtimes. In this paper, we introduce TranSQL+, a template-based code generator that translates LLM computation graphs into pure SQL queries for execution in relational databases. Without relying on external libraries, TranSQL+, leverages mature database features, such as vectorized execution and out-of-core processing, for efficient inference. We further propose a row-to-column (ROW2COL) optimization that improves join efficiency in matrix operations. Evaluated on Llama3-8B and DeepSeekMoE models, TranSQL+ achieves up to 20x lower prefill latency and 4x higher decoding speed compared to DeepSpeed Inference and Llama.cpp in low-memory and CPU-only configurations. Our results highlight relational databases as a practical environment for LLMs on low-resource hardware.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physically consistent predictive reduced-order modeling by enhancing Operator Inference with state constraints</title>
<link>https://arxiv.org/abs/2502.03672</link>
<guid>https://arxiv.org/abs/2502.03672</guid>
<content:encoded><![CDATA[
arXiv:2502.03672v2 Announce Type: replace-cross 
Abstract: Numerical simulations of complex multiphysics systems, such as char combustion considered herein, yield numerous state variables that inherently exhibit physical constraints. This paper presents a new approach to augment Operator Inference -- a methodology within scientific machine learning that enables learning from data a low-dimensional representation of a high-dimensional system governed by nonlinear partial differential equations -- by embedding such state constraints in the reduced-order model predictions. In the model learning process, we propose a new way to choose regularization hyperparameters based on a key performance indicator. Since embedding state constraints improves the stability of the Operator Inference reduced-order model, we compare the proposed state constraints-embedded Operator Inference with the standard Operator Inference and other stability-enhancing approaches. For an application to char combustion, we demonstrate that the proposed approach yields state predictions superior to the other methods regarding stability and accuracy. It extrapolates over 200\% past the training regime while being computationally efficient and physically consistent.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Badly Generalize across Option Length, Problem Types, and Irrelevant Noun Replacements</title>
<link>https://arxiv.org/abs/2502.12459</link>
<guid>https://arxiv.org/abs/2502.12459</guid>
<content:encoded><![CDATA[
arXiv:2502.12459v3 Announce Type: replace-cross 
Abstract: In this paper, we propose a ``Generalization Stress Test" to assess Large Language Models' (LLMs) generalization ability under slight and controlled perturbations, including option length, problem types, and irrelevant noun replacements. We achieve novel and significant findings that, despite high benchmark scores, LLMs exhibit severe accuracy drops and unexpected biases (e.g., preference for longer distractors) when faced with these minor but content-preserving modifications. For example, Qwen 2.5 1.5B's MMLU score rises from 60 to 89 and drops from 89 to 36 when option lengths are changed without altering the question. Even GPT4o experiences a 25-point accuracy loss when problem types are changed, with a 6-point drop across all three modification categories. These analyses suggest that LLMs rely heavily on superficial cues rather than forming robust, abstract representations that generalize across formats, lexical variations, and irrelevant content shifts.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct Preference Optimization</title>
<link>https://arxiv.org/abs/2502.13146</link>
<guid>https://arxiv.org/abs/2502.13146</guid>
<content:encoded><![CDATA[
arXiv:2502.13146v3 Announce Type: replace-cross 
Abstract: The emergence of large Vision Language Models (VLMs) has broadened the scope and capabilities of single-modal Large Language Models (LLMs) by integrating visual modalities, thereby unlocking transformative cross-modal applications in a variety of real-world scenarios. Despite their impressive performance, VLMs are prone to significant hallucinations, particularly in the form of cross-modal inconsistencies. Building on the success of Reinforcement Learning from Human Feedback (RLHF) in aligning LLMs, recent advancements have focused on applying direct preference optimization (DPO) on carefully curated datasets to mitigate these issues. Yet, such approaches typically introduce preference signals in a brute-force manner, neglecting the crucial role of visual information in the alignment process. In this paper, we introduce Re-Align, a novel alignment framework that leverages image retrieval to construct a dual-preference dataset, effectively incorporating both textual and visual preference signals. We further introduce rDPO, an extension of the standard direct preference optimization that incorporates an additional visual preference objective during fine-tuning. Our experimental results demonstrate that Re-Align not only mitigates hallucinations more effectively than previous methods but also yields significant performance gains in general visual question-answering (VQA) tasks. Moreover, we show that Re-Align maintains robustness and scalability across a wide range of VLM sizes and architectures. This work represents a significant step forward in aligning multimodal LLMs, paving the way for more reliable and effective cross-modal applications. We release all the code in https://github.com/taco-group/Re-Align.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FC-Attack: Jailbreaking Multimodal Large Language Models via Auto-Generated Flowcharts</title>
<link>https://arxiv.org/abs/2502.21059</link>
<guid>https://arxiv.org/abs/2502.21059</guid>
<content:encoded><![CDATA[
arXiv:2502.21059v3 Announce Type: replace-cross 
Abstract: Multimodal Large Language Models (MLLMs) have become powerful and widely adopted in some practical applications. However, recent research has revealed their vulnerability to multimodal jailbreak attacks, whereby the model can be induced to generate harmful content, leading to safety risks. Although most MLLMs have undergone safety alignment, recent research shows that the visual modality is still vulnerable to jailbreak attacks. In our work, we discover that by using flowcharts with partially harmful information, MLLMs can be induced to provide additional harmful details. Based on this, we propose a jailbreak attack method based on auto-generated flowcharts, FC-Attack. Specifically, FC-Attack first fine-tunes a pre-trained LLM to create a step-description generator based on benign datasets. The generator is then used to produce step descriptions corresponding to a harmful query, which are transformed into flowcharts in 3 different shapes (vertical, horizontal, and S-shaped) as visual prompts. These flowcharts are then combined with a benign textual prompt to execute the jailbreak attack on MLLMs. Our evaluations on Advbench show that FC-Attack attains an attack success rate of up to 96% via images and up to 78% via videos across multiple MLLMs. Additionally, we investigate factors affecting the attack performance, including the number of steps and the font styles in the flowcharts. We also find that FC-Attack can improve the jailbreak performance from 4% to 28% in Claude-3.5 by changing the font style. To mitigate the attack, we explore several defenses and find that AdaShield can largely reduce the jailbreak performance but with the cost of utility drop.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Prompting: An Efficient Embedding Framework for Open-Domain Question Answering</title>
<link>https://arxiv.org/abs/2503.01606</link>
<guid>https://arxiv.org/abs/2503.01606</guid>
<content:encoded><![CDATA[
arXiv:2503.01606v3 Announce Type: replace-cross 
Abstract: Large language models have recently pushed open domain question answering (ODQA) to new frontiers. However, prevailing retriever-reader pipelines often depend on multiple rounds of prompt level instructions, leading to high computational overhead, instability, and suboptimal retrieval coverage. In this paper, we propose EmbQA, an embedding-level framework that alleviates these shortcomings by enhancing both the retriever and the reader. Specifically, we refine query representations via lightweight linear layers under an unsupervised contrastive learning objective, thereby reordering retrieved passages to highlight those most likely to contain correct answers. Additionally, we introduce an exploratory embedding that broadens the model's latent semantic space to diversify candidate generation and employs an entropy-based selection mechanism to choose the most confident answer automatically. Extensive experiments across three open-source LLMs, three retrieval methods, and four ODQA benchmarks demonstrate that EmbQA substantially outperforms recent baselines in both accuracy and efficiency.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Audio-Reasoner: Improving Reasoning Capability in Large Audio Language Models</title>
<link>https://arxiv.org/abs/2503.02318</link>
<guid>https://arxiv.org/abs/2503.02318</guid>
<content:encoded><![CDATA[
arXiv:2503.02318v2 Announce Type: replace-cross 
Abstract: Recent advancements in multimodal reasoning have largely overlooked the audio modality. We introduce Audio-Reasoner, a large-scale audio language model for deep reasoning in audio tasks. We meticulously curated a large-scale and diverse multi-task audio dataset with simple annotations. Then, we leverage closed-source models to conduct secondary labeling, QA generation, along with structured COT process. These datasets together form a high-quality reasoning dataset with 1.2 million reasoning-rich samples, which we name CoTA. Following inference scaling principles, we train Audio-Reasoner on CoTA, enabling it to achieve great logical capabilities in audio reasoning. Experiments show state-of-the-art performance across key benchmarks, including MMAU-mini (+25.42%), AIR-Bench chat/foundation(+14.57%/+10.13%), and MELD (+8.01%). Our findings stress the core of structured CoT training in advancing audio reasoning.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trajectory Prediction for Autonomous Driving: Progress, Limitations, and Future Directions</title>
<link>https://arxiv.org/abs/2503.03262</link>
<guid>https://arxiv.org/abs/2503.03262</guid>
<content:encoded><![CDATA[
arXiv:2503.03262v3 Announce Type: replace-cross 
Abstract: As the potential for autonomous vehicles to be integrated on a large scale into modern traffic systems continues to grow, ensuring safe navigation in dynamic environments is crucial for smooth integration. To guarantee safety and prevent collisions, autonomous vehicles must be capable of accurately predicting the trajectories of surrounding traffic agents. Over the past decade, significant efforts from both academia and industry have been dedicated to designing solutions for precise trajectory forecasting. These efforts have produced a diverse range of approaches, raising questions about the differences between these methods and whether trajectory prediction challenges have been fully addressed. This paper reviews a substantial portion of recent trajectory prediction methods proposing a taxonomy to classify existing solutions. A general overview of the prediction pipeline is also provided, covering input and output modalities, modeling features, and prediction paradigms existing in the literature. In addition, the paper discusses active research areas within trajectory prediction, addresses the posed research questions, and highlights the remaining research gaps and challenges.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformal Prediction with Upper and Lower Bound Models</title>
<link>https://arxiv.org/abs/2503.04071</link>
<guid>https://arxiv.org/abs/2503.04071</guid>
<content:encoded><![CDATA[
arXiv:2503.04071v3 Announce Type: replace-cross 
Abstract: This paper studies a Conformal Prediction (CP) methodology for building prediction intervals in a regression setting, given only deterministic lower and upper bounds on the target variable. It proposes a new CP mechanism (CPUL) that goes beyond post-processing by adopting a model selection approach over multiple nested interval construction methods. Paradoxically, many well-established CP methods, including CPUL, may fail to provide adequate coverage in regions where the bounds are tight. To remedy this limitation, the paper proposes an optimal thresholding mechanism, OMLT, that adjusts CPUL intervals in tight regions with undercoverage. The combined CPUL-OMLT is validated on large-scale learning tasks where the goal is to bound the optimal value of a parametric optimization problem. The experimental results demonstrate substantial improvements over baseline methods across various datasets.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WikiBigEdit: Understanding the Limits of Lifelong Knowledge Editing in LLMs</title>
<link>https://arxiv.org/abs/2503.05683</link>
<guid>https://arxiv.org/abs/2503.05683</guid>
<content:encoded><![CDATA[
arXiv:2503.05683v2 Announce Type: replace-cross 
Abstract: Keeping large language models factually up-to-date is crucial for deployment, yet costly retraining remains a challenge. Knowledge editing offers a promising alternative, but methods are only tested on small-scale or synthetic edit benchmarks. In this work, we aim to bridge research into lifelong knowledge editing to real-world edits at a practically relevant scale. We first introduce WikiBigEdit; a large-scale benchmark of real-world Wikidata edits, built to automatically extend lifelong for future-proof benchmarking. In its first instance, it includes over 500K question-answer pairs for knowledge editing alongside a comprehensive evaluation pipeline. Finally, we use WikiBigEdit to study existing knowledge editing techniques' ability to incorporate large volumes of real-world facts and contrast their capabilities to generic modification techniques such as retrieval augmentation and continual finetuning to acquire a complete picture of the practical extent of current lifelong knowledge editing.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAARMA: Class Augmentation with Adversarial Mixup Regularization</title>
<link>https://arxiv.org/abs/2503.16718</link>
<guid>https://arxiv.org/abs/2503.16718</guid>
<content:encoded><![CDATA[
arXiv:2503.16718v3 Announce Type: replace-cross 
Abstract: Speaker verification is a typical zero-shot learning task, where inference of unseen classes is performed by comparing embeddings of test instances to known examples. The models performing inference must hence naturally generate embeddings that cluster same-class instances compactly, while maintaining separation across classes. In order to learn to do so, they are typically trained on a large number of classes (speakers), often using specialized losses. However real-world speaker datasets often lack the class diversity needed to effectively learn this in a generalizable manner. We introduce CAARMA, a class augmentation framework that addresses this problem by generating synthetic classes through data mixing in the embedding space, expanding the number of training classes. To ensure the authenticity of the synthetic classes we adopt a novel adversarial refinement mechanism that minimizes categorical distinctions between synthetic and real classes. We evaluate CAARMA on multiple speaker verification tasks, as well as other representative zero-shot comparison-based speech analysis tasks and obtain consistent improvements: our framework demonstrates a significant improvement of 8\% over all baseline models. The code is available at: https://github.com/massabaali7/CAARMA/
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VQToken: Neural Discrete Token Representation Learning for Extreme Token Reduction in Video Large Language Models</title>
<link>https://arxiv.org/abs/2503.16980</link>
<guid>https://arxiv.org/abs/2503.16980</guid>
<content:encoded><![CDATA[
arXiv:2503.16980v5 Announce Type: replace-cross 
Abstract: Token-based video representation has emerged as a promising approach for enabling large language models (LLMs) to interpret video content. However, existing token reduction techniques, such as pruning and merging, often disrupt essential positional embeddings and rely on continuous visual tokens sampled from nearby pixels with similar spatial-temporal locations. By removing only a small fraction of tokens, these methods still produce relatively lengthy continuous sequences, which falls short of the extreme compression required to balance computational efficiency and token count in video LLMs. In this paper, we introduce the novel task of Extreme Short Token Reduction, which aims to represent entire videos using a minimal set of discrete tokens. We propose VQToken, a neural discrete token representation framework that (i) applies adaptive vector quantization to continuous ViT embeddings to learn a compact codebook and (ii) preserves spatial-temporal positions via a token hash function by assigning each grid-level token to its nearest codebook entry. On the Extreme Short Token Reduction task, our VQToken compresses sequences to just 0.07 percent of their original length while incurring only a 0.66 percent drop in accuracy on the NextQA-MC benchmark. It also achieves comparable performance on ActNet-QA, Long Video Bench, and VideoMME. We further introduce the Token Information Density (TokDense) metric and formalize fixed-length and adaptive-length subtasks, achieving state-of-the-art results in both settings. Our approach dramatically lowers theoretical complexity, increases information density, drastically reduces token counts, and enables efficient video LLMs in resource-constrained environments.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BiPrompt-SAM: Enhancing Image Segmentation via Explicit Selection between Point and Text Prompts</title>
<link>https://arxiv.org/abs/2503.19769</link>
<guid>https://arxiv.org/abs/2503.19769</guid>
<content:encoded><![CDATA[
arXiv:2503.19769v3 Announce Type: replace-cross 
Abstract: Segmentation is a fundamental task in computer vision, with prompt-driven methods gaining prominence due to their flexibility. The Segment Anything Model (SAM) excels at point-prompted segmentation, while text-based models, often leveraging powerful multimodal encoders like BEIT-3, provide rich semantic understanding. However, effectively combining these complementary modalities remains a challenge. This paper introduces BiPrompt-SAM, a novel dual-modal prompt segmentation framework employing an explicit selection mechanism. We leverage SAM's ability to generate multiple mask candidates from a single point prompt and use a text-guided mask (generated via EVF-SAM with BEIT-3) to select the point-generated mask that best aligns spatially, measured by Intersection over Union (IoU). This approach, interpretable as a simplified Mixture of Experts (MoE), effectively fuses spatial precision and semantic context without complex model modifications. Notably, our method achieves strong zero-shot performance on the Endovis17 medical dataset (89.55% mDice, 81.46% mIoU) using only a single point prompt per instance. This significantly reduces annotation burden compared to bounding boxes and aligns better with practical clinical workflows, demonstrating the method's effectiveness without domain-specific training. On the RefCOCO series, BiPrompt-SAM attained 87.1%, 86.5%, and 85.8% IoU, significantly outperforming existing approaches. Experiments show BiPrompt-SAM excels in scenarios requiring both spatial accuracy and semantic disambiguation, offering a simple, effective, and interpretable perspective on multi-modal prompt fusion.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XL-Suite: Cross-Lingual Synthetic Training and Evaluation Data for Open-Ended Generation</title>
<link>https://arxiv.org/abs/2503.22973</link>
<guid>https://arxiv.org/abs/2503.22973</guid>
<content:encoded><![CDATA[
arXiv:2503.22973v2 Announce Type: replace-cross 
Abstract: Cross-lingual open-ended generation - responding in a language different from that of the query - is an important yet understudied problem. This work proposes XL-Instruct, a novel technique for generating high-quality synthetic data, and introduces XL-AlpacaEval, a new benchmark for evaluating cross-lingual generation capabilities of large language models (LLMs). Our experiments show that fine-tuning with just 8K instructions generated using XL-Instruct significantly improves model performance, increasing the win rate against GPT-4o-Mini from 7.4% to 21.5% and improving on several fine-grained quality metrics. Moreover, base LLMs fine-tuned on XL-Instruct exhibit strong zero-shot improvements to question answering in the same language, as shown on our machine-translated m-AlpacaEval. These consistent gains highlight the promising role of XL-Instruct in the post-training of multilingual LLMs. Finally, we publicly release XL-Suite, a collection of training and evaluation data to facilitate research in cross-lingual open-ended generation.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Localized Graph-Based Neural Dynamics Models for Terrain Manipulation</title>
<link>https://arxiv.org/abs/2503.23270</link>
<guid>https://arxiv.org/abs/2503.23270</guid>
<content:encoded><![CDATA[
arXiv:2503.23270v2 Announce Type: replace-cross 
Abstract: Predictive models can be particularly helpful for robots to effectively manipulate terrains in construction sites and extraterrestrial surfaces. However, terrain state representations become extremely high-dimensional especially to capture fine-resolution details and when depth is unknown or unbounded. This paper introduces a learning-based approach for terrain dynamics modeling and manipulation, leveraging the Graph-based Neural Dynamics (GBND) framework to represent terrain deformation as motion of a graph of particles. Based on the principle that the moving portion of a terrain is usually localized, our approach builds a large terrain graph (potentially millions of particles) but only identifies a very small active subgraph (hundreds of particles) for predicting the outcomes of robot-terrain interaction. To minimize the size of the active subgraph we introduce a learning-based approach that identifies a small region of interest (RoI) based on the robot's control inputs and the current scene. We also introduce a novel domain boundary feature encoding that allows GBNDs to perform accurate dynamics prediction in the RoI interior while avoiding particle penetration through RoI boundaries. Our proposed method is both orders of magnitude faster than naive GBND and it achieves better overall prediction accuracy. We further evaluated our framework on excavation and shaping tasks on terrain with different granularity.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoLa: Learning to Interactively Collaborate with Large Language Models</title>
<link>https://arxiv.org/abs/2504.02965</link>
<guid>https://arxiv.org/abs/2504.02965</guid>
<content:encoded><![CDATA[
arXiv:2504.02965v3 Announce Type: replace-cross 
Abstract: LLMs' remarkable ability to tackle a wide range of language tasks opened new opportunities for collaborative human-AI problem solving. LLMs can amplify human capabilities by applying their intuitions and reasoning strategies at scale. We explore whether human guides can be simulated, by generalizing from human demonstrations of guiding an AI system to solve complex language problems. We introduce CoLa, a novel self-guided learning paradigm for training automated $\textit{guides}$ and evaluate it on two QA datasets, a puzzle-solving task, and a constrained text generation task. Our empirical results show that CoLa consistently outperforms competitive approaches across all domains. Moreover, a small-sized trained guide outperforms a strong model like GPT-4 when acting as a guide. We compare the strategies employed by humans and automated guides by conducting a human study on a QA dataset. We show that automated guides outperform humans by adapting their strategies to reasoners' capabilities and conduct qualitative analyses highlighting distinct differences in guiding strategies.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Clinical Decision-Making: Integrating Multi-Agent Systems with Ethical AI Governance</title>
<link>https://arxiv.org/abs/2504.03699</link>
<guid>https://arxiv.org/abs/2504.03699</guid>
<content:encoded><![CDATA[
arXiv:2504.03699v4 Announce Type: replace-cross 
Abstract: Recent advances in the data-driven medicine approach, which integrates ethically managed and explainable artificial intelligence into clinical decision support systems (CDSS), are critical to ensure reliable and effective patient care. This paper focuses on comparing novel agent system designs that use modular agents to analyze laboratory results, vital signs, and clinical context, and to predict and validate results. We implement our agent system with the eICU database, including running lab analysis, vitals-only interpreters, and contextual reasoners agents first, then sharing the memory into the integration agent, prediction agent, transparency agent, and a validation agent. Our results suggest that the multi-agent system (MAS) performed better than the single-agent system (SAS) with mortality prediction accuracy (59\%, 56\%) and the mean error for length of stay (LOS)(4.37 days, 5.82 days), respectively. However, the transparency score for the SAS (86.21) is slightly better than the transparency score for MAS (85.5). Finally, this study suggests that our agent-based framework not only improves process transparency and prediction accuracy but also strengthens trustworthy AI-assisted decision support in an intensive care setting.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SymRTLO: Enhancing RTL Code Optimization with LLMs and Neuron-Inspired Symbolic Reasoning</title>
<link>https://arxiv.org/abs/2504.10369</link>
<guid>https://arxiv.org/abs/2504.10369</guid>
<content:encoded><![CDATA[
arXiv:2504.10369v2 Announce Type: replace-cross 
Abstract: Optimizing Register Transfer Level (RTL) code is crucial for improving the power, performance, and area (PPA) of digital circuits in the early stages of synthesis. Manual rewriting, guided by synthesis feedback, can yield high-quality results but is time-consuming and error-prone. Most existing compiler-based approaches have difficulty handling complex design constraints. Large Language Model (LLM)-based methods have emerged as a promising alternative to address these challenges. However, LLM-based approaches often face difficulties in ensuring alignment between the generated code and the provided prompts. This paper presents SymRTLO, a novel neuron-symbolic RTL optimization framework that seamlessly integrates LLM-based code rewriting with symbolic reasoning techniques. Our method incorporates a retrieval-augmented generation (RAG) system of optimization rules and Abstract Syntax Tree (AST)-based templates, enabling LLM-based rewriting that maintains syntactic correctness while minimizing undesired circuit behaviors. A symbolic module is proposed for analyzing and optimizing finite state machine (FSM) logic, allowing fine-grained state merging and partial specification handling beyond the scope of pattern-based compilers. Furthermore, a fast verification pipeline, combining formal equivalence checks with test-driven validation, further reduces the complexity of verification. Experiments on the RTL-Rewriter benchmark with Synopsys Design Compiler and Yosys show that SymRTLO improves power, performance, and area (PPA) by up to 43.9%, 62.5%, and 51.1%, respectively, compared to the state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RIFT: Group-Relative RL Fine-Tuning for Realistic and Controllable Traffic Simulation</title>
<link>https://arxiv.org/abs/2505.03344</link>
<guid>https://arxiv.org/abs/2505.03344</guid>
<content:encoded><![CDATA[
arXiv:2505.03344v3 Announce Type: replace-cross 
Abstract: Achieving both realism and controllability in closed-loop traffic simulation remains a key challenge in autonomous driving. Dataset-based methods reproduce realistic trajectories but suffer from covariate shift in closed-loop deployment, compounded by simplified dynamics models that further reduce reliability. Conversely, physics-based simulation methods enhance reliable and controllable closed-loop interactions but often lack expert demonstrations, compromising realism. To address these challenges, we introduce a dual-stage AV-centric simulation framework that conducts imitation learning pre-training in a data-driven simulator to capture trajectory-level realism and route-level controllability, followed by reinforcement learning fine-tuning in a physics-based simulator to enhance style-level controllability and mitigate covariate shift. In the fine-tuning stage, we propose RIFT, a novel group-relative RL fine-tuning strategy that evaluates all candidate modalities through group-relative formulation and employs a surrogate objective for stable optimization, enhancing style-level controllability and mitigating covariate shift while preserving the trajectory-level realism and route-level controllability inherited from IL pre-training. Extensive experiments demonstrate that RIFT improves realism and controllability in traffic simulation while simultaneously exposing the limitations of modern AV systems in closed-loop evaluation. Project Page: https://currychen77.github.io/RIFT/
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Medium Range Severe Weather Prediction through Transformer Post-processing of AI Weather Forecasts</title>
<link>https://arxiv.org/abs/2505.11750</link>
<guid>https://arxiv.org/abs/2505.11750</guid>
<content:encoded><![CDATA[
arXiv:2505.11750v3 Announce Type: replace-cross 
Abstract: Improving the skill of medium-range (3-8 day) severe weather prediction is crucial for mitigating societal impacts. This study introduces a novel approach leveraging decoder-only transformer networks to post-process AI-based weather forecasts, specifically from the Pangu-Weather model, for improved severe weather guidance. Unlike traditional post-processing methods that use a dense neural network to predict the probability of severe weather using discrete forecast samples, our method treats forecast lead times as sequential ``tokens'', enabling the transformer to learn complex temporal relationships within the evolving atmospheric state. We compare this approach against post-processing of the Global Forecast System (GFS) using both a traditional dense neural network and our transformer, as well as configurations that exclude convective parameters to fairly evaluate the impact of using the Pangu-Weather AI model. Results demonstrate that the transformer-based post-processing significantly enhances forecast skill compared to dense neural networks. Furthermore, AI-driven forecasts, particularly Pangu-Weather initialized from high resolution analysis, exhibit superior performance to GFS in the medium-range, even without explicit convective parameters. Our approach offers improved accuracy, and reliability, which also provides interpretability through feature attribution analysis, advancing medium-range severe weather prediction capabilities.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R3: Robust Rubric-Agnostic Reward Models</title>
<link>https://arxiv.org/abs/2505.13388</link>
<guid>https://arxiv.org/abs/2505.13388</guid>
<content:encoded><![CDATA[
arXiv:2505.13388v3 Announce Type: replace-cross 
Abstract: Reward models are essential for aligning language model outputs with human preferences, yet existing approaches often lack both controllability and interpretability. These models are typically optimized for narrow objectives, limiting their generalizability to broader downstream tasks. Moreover, their scalar outputs are difficult to interpret without contextual reasoning. To address these limitations, we introduce $\shortmethodname$, a novel reward modeling framework that is rubric-agnostic, generalizable across evaluation dimensions, and provides interpretable, reasoned score assignments. $\shortmethodname$ enables more transparent and flexible evaluation of language models, supporting robust alignment with diverse human values and use cases. Our models, data, and code are available as open source at https://github.com/rubricreward/r3.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle Generation from SAT Formulas</title>
<link>https://arxiv.org/abs/2505.14615</link>
<guid>https://arxiv.org/abs/2505.14615</guid>
<content:encoded><![CDATA[
arXiv:2505.14615v2 Announce Type: replace-cross 
Abstract: We introduce SATBench, a benchmark for evaluating the logical reasoning capabilities of large language models (LLMs) through logical puzzles derived from Boolean satisfiability (SAT) problems. Unlike prior work that focuses on inference rule-based reasoning, which often involves deducing conclusions from a set of premises, our approach leverages the search-based nature of SAT problems, where the objective is to find a solution that fulfills a specified set of logical constraints. Each instance in SATBench is generated from a SAT formula, then translated into a puzzle using LLMs. The generation process is fully automated and allows for adjustable difficulty by varying the number of clauses. All 2100 puzzles are validated through both LLM-based and solver-based consistency checks, with human validation on a subset. Experimental results show that even the strongest model, o4-mini, achieves only 65.0% accuracy on hard UNSAT problems, close to the random baseline of 50%. Our error analysis reveals systematic failures such as satisfiability bias, context inconsistency, and condition omission, highlighting limitations of current LLMs in search-based logical reasoning. Our code and data are publicly available at https://github.com/Anjiang-Wei/SATBench
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tool Preferences in Agentic LLMs are Unreliable</title>
<link>https://arxiv.org/abs/2505.18135</link>
<guid>https://arxiv.org/abs/2505.18135</guid>
<content:encoded><![CDATA[
arXiv:2505.18135v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) can now access a wide range of external tools, thanks to the Model Context Protocol (MCP). This greatly expands their abilities as various agents. However, LLMs rely entirely on the text descriptions of tools to decide which ones to use--a process that is surprisingly fragile. In this work, we expose a vulnerability in prevalent tool/function-calling protocols by investigating a series of edits to tool descriptions, some of which can drastically increase a tool's usage from LLMs when competing with alternatives. Through controlled experiments, we show that tools with properly edited descriptions receive over 10 times more usage from GPT-4.1 and Qwen2.5-7B than tools with original descriptions. We further evaluate how various edits to tool descriptions perform when competing directly with one another and how these trends generalize or differ across a broader set of 17 different models. These phenomena, while giving developers a powerful way to promote their tools, underscore the need for a more reliable foundation for agentic LLMs to select and utilize tools and resources. Our code is publicly available at https://github.com/kazemf78/llm-unreliable-tool-preferences.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BAGELS: Benchmarking the Automated Generation and Extraction of Limitations from Scholarly Text</title>
<link>https://arxiv.org/abs/2505.18207</link>
<guid>https://arxiv.org/abs/2505.18207</guid>
<content:encoded><![CDATA[
arXiv:2505.18207v2 Announce Type: replace-cross 
Abstract: In scientific research, ``limitations'' refer to the shortcomings, constraints, or weaknesses of a study. A transparent reporting of such limitations can enhance the quality and reproducibility of research and improve public trust in science. However, authors often underreport limitations in their papers and rely on hedging strategies to meet editorial requirements at the expense of readers' clarity and confidence. This tendency, combined with the surge in scientific publications, has created a pressing need for automated approaches to extract and generate limitations from scholarly papers. To address this need, we present a full architecture for computational analysis of research limitations. Specifically, we (1) create a dataset of limitations from ACL, NeurIPS, and PeerJ papers by extracting them from the text and supplementing them with external reviews; (2) we propose methods to automatically generate limitations using a novel Retrieval Augmented Generation (RAG) technique; (3) we design a fine-grained evaluation framework for generated limitations, along with a meta-evaluation of these techniques.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Is LLM Reasoning Distracted by Irrelevant Context? An Analysis Using a Controlled Benchmark</title>
<link>https://arxiv.org/abs/2505.18761</link>
<guid>https://arxiv.org/abs/2505.18761</guid>
<content:encoded><![CDATA[
arXiv:2505.18761v2 Announce Type: replace-cross 
Abstract: We introduce Grade School Math with Distracting Context (GSM-DC), a synthetic benchmark to evaluate Large Language Models' (LLMs) reasoning robustness against systematically controlled irrelevant context (IC). GSM-DC constructs symbolic reasoning graphs with precise distractor injections, enabling rigorous, reproducible evaluation. Our experiments demonstrate that LLMs are significantly sensitive to IC, affecting both reasoning path selection and arithmetic accuracy. Additionally, training models with strong distractors improves performance in both in-distribution and out-of-distribution scenarios. We further propose a stepwise tree search guided by a process reward model, which notably enhances robustness in out-of-distribution conditions.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>New Expansion Rate Anomalies at Characteristic Redshifts Geometrically Determined using DESI-DR2 BAO and DES-SN5YR Observations</title>
<link>https://arxiv.org/abs/2505.19083</link>
<guid>https://arxiv.org/abs/2505.19083</guid>
<content:encoded><![CDATA[
arXiv:2505.19083v2 Announce Type: replace-cross 
Abstract: We perform a model-independent reconstruction of the cosmic distances using the Multi-Task Gaussian Process (MTGP) framework as well as knot-based spline techniques with DESI-DR2 BAO and DES-SN5YR datasets. We calibrate the comoving sound horizon at the baryon drag epoch $r_d$ to the Planck value, ensuring consistency with early-universe physics. With the reconstructed cosmic distances and their derivatives, we obtain seven characteristic redshifts in the range $0.3 \leq z \leq 1.7$. We derive the normalized expansion rate of the Universe $E(z)$ at these redshifts. Our findings reveal significant deviations of approximately $4$ to $5\sigma$ from the Planck 2018 $\Lambda$CDM predictions, particularly pronounced in the redshift range $z \sim 0.35-0.55$. These anomalies are consistently observed across both reconstruction methods and combined datasets, indicating robust late-time tensions in the expansion rate of the Universe and which are distinct from the existing "Hubble Tension". This could signal new physics beyond the standard cosmological framework at this redshift range. Our findings underscore the role of characteristic redshifts as sensitive indicators of expansion rate anomalies and motivate further scrutiny with forthcoming datasets from DESI-5YR BAO, Euclid, and LSST. These future surveys will tighten constraints and will confirm whether these late-time anomalies arise from new fundamental physics or unresolved systematics in the data.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Chat Logs to Collective Insights: Aggregative Question Answering</title>
<link>https://arxiv.org/abs/2505.23765</link>
<guid>https://arxiv.org/abs/2505.23765</guid>
<content:encoded><![CDATA[
arXiv:2505.23765v2 Announce Type: replace-cross 
Abstract: Conversational agents powered by large language models (LLMs) are rapidly becoming integral to our daily interactions, generating unprecedented amounts of conversational data. Such datasets offer a powerful lens into societal interests, trending topics, and collective concerns. Yet, existing approaches typically treat these interactions as independent and miss critical insights that could emerge from aggregating and reasoning across large-scale conversation logs. In this paper, we introduce Aggregative Question Answering, a novel task requiring models to reason explicitly over thousands of user-chatbot interactions to answer aggregative queries, such as identifying emerging concerns among specific demographics. To enable research in this direction, we construct a benchmark, WildChat-AQA, comprising 6,027 aggregative questions derived from 182,330 real-world chatbot conversations. Experiments show that existing methods either struggle to reason effectively or incur prohibitive computational costs, underscoring the need for new approaches capable of extracting collective insights from large-scale conversational data.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LaMP-QA: A Benchmark for Personalized Long-form Question Answering</title>
<link>https://arxiv.org/abs/2506.00137</link>
<guid>https://arxiv.org/abs/2506.00137</guid>
<content:encoded><![CDATA[
arXiv:2506.00137v2 Announce Type: replace-cross 
Abstract: Personalization is essential for question answering systems that are user-centric. Despite its importance, personalization in answer generation has been relatively underexplored. This is mainly due to lack of resources for training and evaluating personalized question answering systems. We address this gap by introducing LaMP-QA -- a benchmark designed for evaluating personalized long-form answer generation. The benchmark covers questions from three major categories: (1) Arts & Entertainment, (2) Lifestyle & Personal Development, and (3) Society & Culture, encompassing over 45 subcategories in total. To assess the quality and potential impact of the LaMP-QA benchmark for personalized question answering, we conduct comprehensive human and automatic evaluations, to compare multiple evaluation strategies for evaluating generated personalized responses and measure their alignment with human preferences. Furthermore, we benchmark a number of non-personalized and personalized approaches based on open-source and proprietary large language models. Our results show that incorporating the personalized context provided leads to up to 39% performance improvements. The benchmark is publicly released to support future research in this area.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Graph Neural Networks and Dataset for Robust Olfactory Navigation in Hazard Robotics</title>
<link>https://arxiv.org/abs/2506.00455</link>
<guid>https://arxiv.org/abs/2506.00455</guid>
<content:encoded><![CDATA[
arXiv:2506.00455v4 Announce Type: replace-cross 
Abstract: Navigation by scent is a capability in robotic systems that is rising in demand. However, current methods often suffer from ambiguities, particularly when robots misattribute odours to incorrect objects due to limitations in olfactory datasets and sensor resolutions. To address challenges in olfactory navigation, we introduce a multimodal olfaction dataset along with a novel machine learning method using diffusion-based molecular generation that can be used by itself or with automated olfactory dataset construction pipelines. This generative process of our diffusion model expands the chemical space beyond the limitations of both current olfactory datasets and training methods, enabling the identification of potential odourant molecules not previously documented. The generated molecules can then be more accurately validated using advanced olfactory sensors, enabling them to detect more compounds and inform better hardware design. By integrating visual analysis, language processing, and molecular generation, our framework enhances the ability of olfaction-vision models on robots to accurately associate odours with their correct sources, thereby improving navigation and decision-making through better sensor selection for a target compound in critical applications such as explosives detection, narcotics screening, and search and rescue. Our methodology represents a foundational advancement in the field of artificial olfaction, offering a scalable solution to challenges posed by limited olfactory data and sensor ambiguities. Code, models, and data are made available to the community at: https://huggingface.co/datasets/kordelfrance/olfaction-vision-language-dataset.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ESGenius: Benchmarking LLMs on Environmental, Social, and Governance (ESG) and Sustainability Knowledge</title>
<link>https://arxiv.org/abs/2506.01646</link>
<guid>https://arxiv.org/abs/2506.01646</guid>
<content:encoded><![CDATA[
arXiv:2506.01646v2 Announce Type: replace-cross 
Abstract: We introduce ESGenius, a comprehensive benchmark for evaluating and enhancing the proficiency of Large Language Models (LLMs) in Environmental, Social, and Governance (ESG) and sustainability-focused question answering. ESGenius comprises two key components: (i) ESGenius-QA, a collection of 1,136 Multiple-Choice Questions (MCQs) generated by LLMs and rigorously validated by domain experts, covering a broad range of ESG pillars and sustainability topics. Each question is systematically linked to its corresponding source text, enabling transparent evaluation and supporting Retrieval-Augmented Generation (RAG) methods; and (ii) ESGenius-Corpus, a meticulously curated repository of 231 foundational frameworks, standards, reports, and recommendation documents from 7 authoritative sources. Moreover, to fully assess the capabilities and adaptation potential of LLMs, we implement a rigorous two-stage evaluation protocol -- Zero-Shot and RAG. Extensive experiments across 50 LLMs (0.5B to 671B) demonstrate that state-of-the-art models achieve only moderate performance in zero-shot settings, with accuracies around 55--70%, highlighting a significant knowledge gap for LLMs in this specialized, interdisciplinary domain. However, models employing RAG demonstrate significant performance improvements, particularly for smaller models. For example, DeepSeek-R1-Distill-Qwen-14B improves from 63.82% (zero-shot) to 80.46% with RAG. These results demonstrate the necessity of grounding responses in authoritative sources for enhanced ESG understanding. To the best of our knowledge, ESGenius is the first comprehensive QA benchmark designed to rigorously evaluate LLMs on ESG and sustainability knowledge, providing a critical tool to advance trustworthy AI in this vital domain.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Survey on the Evaluation of Generative Models in Music</title>
<link>https://arxiv.org/abs/2506.05104</link>
<guid>https://arxiv.org/abs/2506.05104</guid>
<content:encoded><![CDATA[
arXiv:2506.05104v4 Announce Type: replace-cross 
Abstract: Research on generative systems in music has seen considerable attention and growth in recent years. A variety of attempts have been made to systematically evaluate such systems.
  We present an interdisciplinary review of the common evaluation targets, methodologies, and metrics for the evaluation of both system output and model use, covering subjective and objective approaches, qualitative and quantitative approaches, as well as empirical and computational methods. We examine the benefits and limitations of these approaches from a musicological, an engineering, and an HCI perspective.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diversity-Guided MLP Reduction for Efficient Large Vision Transformers</title>
<link>https://arxiv.org/abs/2506.08591</link>
<guid>https://arxiv.org/abs/2506.08591</guid>
<content:encoded><![CDATA[
arXiv:2506.08591v2 Announce Type: replace-cross 
Abstract: Transformer models achieve excellent scaling property, where the performance is improved with the increment of model capacity. However, large-scale model parameters lead to an unaffordable cost of computing and memory. We analyze popular transformer architectures and find that multilayer perceptron (MLP) modules take up the majority of model parameters. To this end, we focus on the recoverability of the compressed models and propose a Diversity-Guided MLP Reduction (DGMR) method to significantly reduce the parameters of large vision transformers with only negligible performance degradation. Specifically, we conduct a Gram-Schmidt weight pruning strategy to eliminate redundant neurons of MLP hidden layer, while preserving weight diversity for better performance recover during distillation. Compared to the model trained from scratch, our pruned model only requires 0.06\% data of LAION-2B (for the training of large vision transformers) without labels (ImageNet-1K) to recover the original performance. Experimental results on several state-of-the-art large vision transformers demonstrate that our method achieves a more than 57.0\% parameter and FLOPs reduction in a near lossless manner. Notably, for EVA-CLIP-E (4.4B), our method accomplishes a 71.5\% parameter and FLOPs reduction without performance degradation. The source code and trained weights are available at https://github.com/visresearch/DGMR.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DISCO: Mitigating Bias in Deep Learning with Conditional Distance Correlation</title>
<link>https://arxiv.org/abs/2506.11653</link>
<guid>https://arxiv.org/abs/2506.11653</guid>
<content:encoded><![CDATA[
arXiv:2506.11653v2 Announce Type: replace-cross 
Abstract: Dataset bias often leads deep learning models to exploit spurious correlations instead of task-relevant signals. We introduce the Standard Anti-Causal Model (SAM), a unifying causal framework that characterizes bias mechanisms and yields a conditional independence criterion for causal stability. Building on this theory, we propose DISCO$_m$ and sDISCO, efficient and scalable estimators of conditional distance correlation that enable independence regularization in black-box models. Across five diverse datasets, our methods consistently outperform or are competitive in existing bias mitigation approaches, while requiring fewer hyperparameters and scaling seamlessly to multi-bias scenarios. This work bridges causal theory and practical deep learning, providing both a principled foundation and effective tools for robust prediction. Source Code: https://github.com/***.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>See What I Mean? CUE: A Cognitive Model of Understanding Explanations</title>
<link>https://arxiv.org/abs/2506.14775</link>
<guid>https://arxiv.org/abs/2506.14775</guid>
<content:encoded><![CDATA[
arXiv:2506.14775v2 Announce Type: replace-cross 
Abstract: As machine learning systems increasingly inform critical decisions, the need for human-understandable explanations grows. Current evaluations of Explainable AI (XAI) often prioritize technical fidelity over cognitive accessibility which critically affects users, in particular those with visual impairments. We propose CUE, a model for Cognitive Understanding of Explanations, linking explanation properties to cognitive sub-processes: legibility (perception), readability (comprehension), and interpretability (interpretation). In a study (N=455) testing heatmaps with varying colormaps (BWR, Cividis, Coolwarm), we found comparable task performance but lower confidence/effort for visually impaired users. Unlike expected, these gaps were not mitigated and sometimes worsened by accessibility-focused color maps like Cividis. These results challenge assumptions about perceptual optimization and support the need for adaptive XAI interfaces. They also validate CUE by demonstrating that altering explanation legibility affects understandability. We contribute: (1) a formalized cognitive model for explanation understanding, (2) an integrated definition of human-centered explanation properties, and (3) empirical evidence motivating accessible, user-tailored XAI.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HARPT: A Corpus for Analyzing Consumers' Trust and Privacy Concerns in Electronic Health Apps</title>
<link>https://arxiv.org/abs/2506.19268</link>
<guid>https://arxiv.org/abs/2506.19268</guid>
<content:encoded><![CDATA[
arXiv:2506.19268v3 Announce Type: replace-cross 
Abstract: We present Health App Reviews for Privacy & Trust (HARPT), a large-scale annotated corpus of user reviews from Electronic Health (eHealth) applications (apps) aimed at advancing research in user privacy and trust. The dataset comprises 480K user reviews labeled in seven categories that capture critical aspects of trust in applications (TA), trust in providers (TP), and privacy concerns (PC). Our multistage strategy integrated keyword-based filtering, iterative manual labeling with review, targeted data augmentation, and weak supervision using transformer-based classifiers. In parallel, we manually annotated a curated subset of 7,000 reviews to support the development and evaluation of machine learning models. We benchmarked a broad range of models, providing a baseline for future work. HARPT is released under an open resource license to support reproducible research in usable privacy and trust in digital libraries and health informatics.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Design Space of 3D MLLMs for CT Report Generation</title>
<link>https://arxiv.org/abs/2506.21535</link>
<guid>https://arxiv.org/abs/2506.21535</guid>
<content:encoded><![CDATA[
arXiv:2506.21535v2 Announce Type: replace-cross 
Abstract: Multimodal Large Language Models (MLLMs) have emerged as a promising way to automate Radiology Report Generation (RRG). In this work, we systematically investigate the design space of 3D MLLMs, including visual input representation, projectors, Large Language Models (LLMs), and fine-tuning techniques for 3D CT report generation. We also introduce two knowledge-based report augmentation methods that improve performance on the GREEN score by up to 10%, achieving the 2nd place on the MICCAI 2024 AMOS-MM challenge. Our results on the 1,687 cases from the AMOS-MM dataset show that RRG is largely independent of the size of LLM under the same training protocol. We also show that larger volume size does not always improve performance if the original ViT was pre-trained on a smaller volume size. Lastly, we show that using a segmentation mask along with the CT volume improves performance. The code is publicly available at https://github.com/bowang-lab/AMOS-MM-Solution
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AICO: Feature Significance Tests for Supervised Learning</title>
<link>https://arxiv.org/abs/2506.23396</link>
<guid>https://arxiv.org/abs/2506.23396</guid>
<content:encoded><![CDATA[
arXiv:2506.23396v2 Announce Type: replace-cross 
Abstract: The opacity of many supervised learning algorithms remains a key challenge, hindering scientific discovery and limiting broader deployment -- particularly in high-stakes domains. This paper develops model- and distribution-agnostic significance tests to assess the influence of input features in any regression or classification algorithm. Our method evaluates a feature's incremental contribution to model performance by masking its values across samples. Under the null hypothesis, the distribution of performance differences across a test set has a non-positive median. We construct a uniformly most powerful, randomized sign test for this median, yielding exact p-values for assessing feature significance and confidence intervals with exact coverage for estimating population-level feature importance. The approach requires minimal assumptions, avoids model retraining or auxiliary models, and remains computationally efficient even for large-scale, high-dimensional settings. Experiments on synthetic tasks validate its statistical and computational advantages, and applications to real-world data illustrate its practical utility.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Datasets for Fairness in Language Models: An In-Depth Survey</title>
<link>https://arxiv.org/abs/2506.23411</link>
<guid>https://arxiv.org/abs/2506.23411</guid>
<content:encoded><![CDATA[
arXiv:2506.23411v2 Announce Type: replace-cross 
Abstract: Despite the growing reliance on fairness benchmarks to evaluate language models, the datasets that underpin these benchmarks remain critically underexamined. This survey addresses that overlooked foundation by offering a comprehensive analysis of the most widely used fairness datasets in language model research. To ground this analysis, we characterize each dataset across key dimensions, including provenance, demographic scope, annotation design, and intended use, revealing the assumptions and limitations baked into current evaluation practices. Building on this foundation, we propose a unified evaluation framework that surfaces consistent patterns of demographic disparities across benchmarks and scoring metrics. Applying this framework to sixteen popular datasets, we uncover overlooked biases that may distort conclusions about model fairness and offer guidance on selecting, combining, and interpreting these resources more effectively and responsibly. Our findings highlight an urgent need for new benchmarks that capture a broader range of social contexts and fairness notions. To support future research, we release all data, code, and results at https://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/datasets, fostering transparency and reproducibility in the evaluation of language model fairness.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resolving Turbulent Magnetohydrodynamics: A Hybrid Operator-Diffusion Framework</title>
<link>https://arxiv.org/abs/2507.02106</link>
<guid>https://arxiv.org/abs/2507.02106</guid>
<content:encoded><![CDATA[
arXiv:2507.02106v2 Announce Type: replace-cross 
Abstract: We present a hybrid machine learning framework that combines Physics-Informed Neural Operators (PINOs) with score-based generative diffusion models to simulate the full spatio-temporal evolution of two-dimensional, incompressible, resistive magnetohydrodynamic (MHD) turbulence across a broad range of Reynolds numbers ($\mathrm{Re}$). The framework leverages the equation-constrained generalization capabilities of PINOs to predict coherent, low-frequency dynamics, while a conditional diffusion model stochastically corrects high-frequency residuals, enabling accurate modeling of fully developed turbulence. Trained on a comprehensive ensemble of high-fidelity simulations with $\mathrm{Re} \in \{100, 250, 500, 750, 1000, 3000, 10000\}$, the approach achieves state-of-the-art accuracy in regimes previously inaccessible to deterministic surrogates. At $\mathrm{Re}=1000$ and $3000$, the model faithfully reconstructs the full spectral energy distributions of both velocity and magnetic fields late into the simulation, capturing non-Gaussian statistics, intermittent structures, and cross-field correlations with high fidelity. At extreme turbulence levels ($\mathrm{Re}=10000$), it remains the first surrogate capable of recovering the high-wavenumber evolution of the magnetic field, preserving large-scale morphology and enabling statistically meaningful predictions.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PDFMathTranslate: Scientific Document Translation Preserving Layouts</title>
<link>https://arxiv.org/abs/2507.03009</link>
<guid>https://arxiv.org/abs/2507.03009</guid>
<content:encoded><![CDATA[
arXiv:2507.03009v4 Announce Type: replace-cross 
Abstract: Language barriers in scientific documents hinder the diffusion and development of science and technologies. However, prior efforts in translating such documents largely overlooked the information in layouts. To bridge the gap, we introduce PDFMathTranslate, the world's first open-source software for translating scientific documents while preserving layouts. Leveraging the most recent advances in large language models and precise layout detection, we contribute to the community with key improvements in precision, flexibility, and efficiency. The work has been open-sourced at https://github.com/byaidu/pdfmathtranslate with more than 222k downloads.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretability-Aware Pruning for Efficient Medical Image Analysis</title>
<link>https://arxiv.org/abs/2507.08330</link>
<guid>https://arxiv.org/abs/2507.08330</guid>
<content:encoded><![CDATA[
arXiv:2507.08330v2 Announce Type: replace-cross 
Abstract: Deep learning has driven significant advances in medical image analysis, yet its adoption in clinical practice remains constrained by the large size and lack of transparency in modern models. Advances in interpretability techniques such as DL-Backtrace, Layer-wise Relevance Propagation, and Integrated Gradients make it possible to assess the contribution of individual components within neural networks trained on medical imaging tasks. In this work, we introduce an interpretability-guided pruning framework that reduces model complexity while preserving both predictive performance and transparency. By selectively retaining only the most relevant parts of each layer, our method enables targeted compression that maintains clinically meaningful representations. Experiments across multiple medical image classification benchmarks demonstrate that this approach achieves high compression rates with minimal loss in accuracy, paving the way for lightweight, interpretable models suited for real-world deployment in healthcare settings.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automating Steering for Safe Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2507.13255</link>
<guid>https://arxiv.org/abs/2507.13255</guid>
<content:encoded><![CDATA[
arXiv:2507.13255v2 Announce Type: replace-cross 
Abstract: Recent progress in Multimodal Large Language Models (MLLMs) has unlocked powerful cross-modal reasoning abilities, but also raised new safety concerns, particularly when faced with adversarial multimodal inputs. To improve the safety of MLLMs during inference, we introduce a modular and adaptive inference-time intervention technology, AutoSteer, without requiring any fine-tuning of the underlying model. AutoSteer incorporates three core components: (1) a novel Safety Awareness Score (SAS) that automatically identifies the most safety-relevant distinctions among the model's internal layers; (2) an adaptive safety prober trained to estimate the likelihood of toxic outputs from intermediate representations; and (3) a lightweight Refusal Head that selectively intervenes to modulate generation when safety risks are detected. Experiments on LLaVA-OV and Chameleon across diverse safety-critical benchmarks demonstrate that AutoSteer significantly reduces the Attack Success Rate (ASR) for textual, visual, and cross-modal threats, while maintaining general abilities. These findings position AutoSteer as a practical, interpretable, and effective framework for safer deployment of multimodal AI systems.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Policy Steering with Embodiment-Agnostic Pretrained World Models</title>
<link>https://arxiv.org/abs/2507.13340</link>
<guid>https://arxiv.org/abs/2507.13340</guid>
<content:encoded><![CDATA[
arXiv:2507.13340v2 Announce Type: replace-cross 
Abstract: Learning visuomotor policies via imitation has proven effective across a wide range of robotic domains. However, the performance of these policies is heavily dependent on the number of training demonstrations, which requires expensive data collection in the real world. In this work, we aim to reduce data collection efforts when learning visuomotor robot policies by leveraging existing or cost-effective data from a wide range of embodiments, such as public robot datasets and the datasets of humans playing with objects (human data from play). Our approach leverages two key insights. First, we use optic flow as an embodiment-agnostic action representation to train a World Model (WM) across multi-embodiment datasets, and finetune it on a small amount of robot data from the target embodiment. Second, we develop a method, Latent Policy Steering (LPS), to improve the output of a behavior-cloned policy by searching in the latent space of the WM for better action sequences. In real world experiments, we observe significant improvements in the performance of policies trained with a small amount of data (over 50% relative improvement with 30 demonstrations and over 20% relative improvement with 50 demonstrations) by combining the policy with a WM pretrained on two thousand episodes sampled from the existing Open X-embodiment dataset across different robots or a cost-effective human dataset from play.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Loss-Complexity Landscape and Model Structure Functions</title>
<link>https://arxiv.org/abs/2507.13543</link>
<guid>https://arxiv.org/abs/2507.13543</guid>
<content:encoded><![CDATA[
arXiv:2507.13543v3 Announce Type: replace-cross 
Abstract: We develop a framework for dualizing the Kolmogorov structure function $h_x(\alpha)$, which then allows using computable complexity proxies. We establish a mathematical analogy between information-theoretic constructs and statistical mechanics, introducing a suitable partition function and free energy functional. We explicitly prove the Legendre-Fenchel duality between the structure function and free energy, showing detailed balance of the Metropolis kernel, and interpret acceptance probabilities as information-theoretic scattering amplitudes. A susceptibility-like variance of model complexity is shown to peak precisely at loss-complexity trade-offs interpreted as phase transitions. Practical experiments with linear and tree-based regression models verify these theoretical predictions, explicitly demonstrating the interplay between the model complexity, generalization, and overfitting threshold.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentially Private Synthetic Graphs Preserving Triangle-Motif Cuts</title>
<link>https://arxiv.org/abs/2507.14835</link>
<guid>https://arxiv.org/abs/2507.14835</guid>
<content:encoded><![CDATA[
arXiv:2507.14835v2 Announce Type: replace-cross 
Abstract: We study the problem of releasing a differentially private (DP) synthetic graph $G'$ that well approximates the triangle-motif sizes of all cuts of any given graph $G$, where a motif in general refers to a frequently occurring subgraph within complex networks. Non-private versions of such graphs have found applications in diverse fields such as graph clustering, graph sparsification, and social network analysis. Specifically, we present the first $(\varepsilon,\delta)$-DP mechanism that, given an input graph $G$ with $n$ vertices, $m$ edges and local sensitivity of triangles $\ell_{3}(G)$, generates a synthetic graph $G'$ in polynomial time, approximating the triangle-motif sizes of all cuts $(S,V\setminus S)$ of the input graph $G$ up to an additive error of $\tilde{O}(\sqrt{m\ell_{3}(G)}n/\varepsilon^{3/2})$. Additionally, we provide a lower bound of $\Omega(\sqrt{mn}\ell_{3}(G)/\varepsilon)$ on the additive error for any DP algorithm that answers the triangle-motif size queries of all $(S,T)$-cut of $G$. Finally, our algorithm generalizes to weighted graphs, and our lower bound extends to any $K_h$-motif cut for any constant $h\geq 2$.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating Posterior Bayesian Neural Networks with Dependent Weights</title>
<link>https://arxiv.org/abs/2507.22095</link>
<guid>https://arxiv.org/abs/2507.22095</guid>
<content:encoded><![CDATA[
arXiv:2507.22095v2 Announce Type: replace-cross 
Abstract: In this paper we consider posterior Bayesian fully connected and feedforward deep neural networks with dependent weights. Particularly, if the likelihood is Gaussian, we identify the distribution of the wide width limit and provide an algorithm to sample from the network. In the shallow case we explicitly compute the distribution of the conditional output, proving that it is a Gaussian mixture. All the theoretical results are numerically validated.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Applying Psychometrics to Large Language Model Simulated Populations: Recreating the HEXACO Personality Inventory Experiment with Generative Agents</title>
<link>https://arxiv.org/abs/2508.00742</link>
<guid>https://arxiv.org/abs/2508.00742</guid>
<content:encoded><![CDATA[
arXiv:2508.00742v2 Announce Type: replace-cross 
Abstract: Generative agents powered by Large Language Models demonstrate human-like characteristics through sophisticated natural language interactions. Their ability to assume roles and personalities based on predefined character biographies has positioned them as cost-effective substitutes for human participants in social science research. This paper explores the validity of such persona-based agents in representing human populations; we recreate the HEXACO personality inventory experiment by surveying 310 GPT-4 powered agents, conducting factor analysis on their responses, and comparing these results to the original findings presented by Ashton, Lee, & Goldberg in 2004. Our results found 1) a coherent and reliable personality structure was recoverable from the agents' responses demonstrating partial alignment to the HEXACO framework. 2) the derived personality dimensions were consistent and reliable within GPT-4, when coupled with a sufficiently curated population, and 3) cross-model analysis revealed variability in personality profiling, suggesting model-specific biases and limitations. We discuss the practical considerations and challenges encountered during the experiment. This study contributes to the ongoing discourse on the potential benefits and limitations of using generative agents in social science research and provides useful guidance on designing consistent and representative agent personas to maximise coverage and representation of human personality traits.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tensor-Empowered Asset Pricing with Missing Data</title>
<link>https://arxiv.org/abs/2508.01861</link>
<guid>https://arxiv.org/abs/2508.01861</guid>
<content:encoded><![CDATA[
arXiv:2508.01861v2 Announce Type: replace-cross 
Abstract: Missing data in financial panels presents a critical obstacle, undermining asset-pricing models and reducing the effectiveness of investment strategies. Such panels are often inherently multi-dimensional, spanning firms, time, and financial variables, which adds complexity to the imputation task. Conventional imputation methods often fail by flattening the data's multidimensional structure, struggling with heterogeneous missingness patterns, or overfitting in the face of extreme data sparsity. To address these limitations, we introduce an Adaptive, Cluster-based Temporal smoothing tensor completion framework (ACT-Tensor) tailored for severely and heterogeneously missing multi-dimensional financial data panels. ACT-Tensor incorporates two key innovations: a cluster-based completion module that captures cross-sectional heterogeneity by learning group-specific latent structures; and a temporal smoothing module that proactively removes short-lived noise while preserving slow-moving fundamental trends. Extensive experiments show that ACT-Tensor consistently outperforms state-of-the-art benchmarks in terms of imputation accuracy across a range of missing data regimes, including extreme sparsity scenarios. To assess its practical financial utility, we evaluate the imputed data with a latent factor model tailored for tensor-structured financial data. Results show that ACT-Tensor not only achieves accurate return forecasting but also significantly improves risk-adjusted returns of the constructed portfolio. These findings confirm that our method delivers highly accurate and informative imputations, offering substantial value for financial decision-making.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Blockchain-Enabled Federated Learning</title>
<link>https://arxiv.org/abs/2508.06406</link>
<guid>https://arxiv.org/abs/2508.06406</guid>
<content:encoded><![CDATA[
arXiv:2508.06406v4 Announce Type: replace-cross 
Abstract: Blockchain-enabled federated learning (BCFL) addresses fundamental challenges of trust, privacy, and coordination in collaborative AI systems. This chapter provides comprehensive architectural analysis of BCFL systems through a systematic four-dimensional taxonomy examining coordination structures, consensus mechanisms, storage architectures, and trust models. We analyze design patterns from blockchain-verified centralized coordination to fully decentralized peer-to-peer networks, evaluating trade-offs in scalability, security, and performance. Through detailed examination of consensus mechanisms designed for federated learning contexts, including Proof of Quality and Proof of Federated Learning, we demonstrate how computational work can be repurposed from arbitrary cryptographic puzzles to productive machine learning tasks. The chapter addresses critical storage challenges by examining multi-tier architectures that balance blockchain's transaction constraints with neural networks' large parameter requirements while maintaining cryptographic integrity. A technical case study of the TrustMesh framework illustrates practical implementation considerations in BCFL systems through distributed image classification training, demonstrating effective collaborative learning across IoT devices with highly non-IID data distributions while maintaining complete transparency and fault tolerance. Analysis of real-world deployments across healthcare consortiums, financial services, and IoT security applications validates the practical viability of BCFL systems, achieving performance comparable to centralized approaches while providing enhanced security guarantees and enabling new models of trustless collaborative intelligence.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Knowledge Tracing by Exploring Follow-up Performance Trends</title>
<link>https://arxiv.org/abs/2508.08019</link>
<guid>https://arxiv.org/abs/2508.08019</guid>
<content:encoded><![CDATA[
arXiv:2508.08019v2 Announce Type: replace-cross 
Abstract: Intelligent Tutoring Systems (ITS), such as Massive Open Online Courses, offer new opportunities for human learning. At the core of such systems, knowledge tracing (KT) predicts students' future performance by analyzing their historical learning activities, enabling an accurate evaluation of students' knowledge states over time. We show that existing KT methods often encounter correlation conflicts when analyzing the relationships between historical learning sequences and future performance. To address such conflicts, we propose to extract so-called Follow-up Performance Trends (FPTs) from historical ITS data and to incorporate them into KT. We propose a method called Forward-Looking Knowledge Tracing (FINER) that combines historical learning sequences with FPTs to enhance student performance prediction accuracy. FINER constructs learning patterns that facilitate the retrieval of FPTs from historical ITS data in linear time; FINER includes a novel similarity-aware attention mechanism that aggregates FPTs based on both frequency and contextual similarity; and FINER offers means of combining FPTs and historical learning sequences to enable more accurate prediction of student future performance. Experiments on six real-world datasets show that FINER can outperform ten state-of-the-art KT methods, increasing accuracy by 8.74% to 84.85%.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mini-Omni-Reasoner: Token-Level Thinking-in-Speaking in Large Speech Models</title>
<link>https://arxiv.org/abs/2508.15827</link>
<guid>https://arxiv.org/abs/2508.15827</guid>
<content:encoded><![CDATA[
arXiv:2508.15827v2 Announce Type: replace-cross 
Abstract: Reasoning is essential for effective communication and decision-making. While recent advances in LLMs and MLLMs have shown that incorporating explicit reasoning significantly improves understanding and generalization, reasoning in LSMs remains in a nascent stage. Early efforts attempt to transfer the "Thinking-before-Speaking" paradigm from textual models to speech. However, this sequential formulation introduces notable latency, as spoken responses are delayed until reasoning is fully completed, impairing real-time interaction and communication efficiency. To address this, we propose Mini-Omni-Reasoner, a framework that enables reasoning within speech via a novel "Thinking-in-Speaking" formulation. Rather than completing reasoning before producing any verbal output, Mini-Omni-Reasoner interleaves silent reasoning tokens with spoken response tokens at the token level. This design allows continuous speech generation while embedding structured internal reasoning, leveraging the model's high-frequency token processing capability. Although interleaved, local semantic alignment is enforced to ensure that each response token is informed by its preceding reasoning. To support this framework, we introduce Spoken-Math-Problems-3M, a large-scale dataset tailored for interleaved reasoning and response. The dataset ensures that verbal tokens consistently follow relevant reasoning content, enabling accurate and efficient learning of speech-coupled reasoning. Built on a hierarchical Thinker-Talker architecture, Mini-Omni-Reasoner delivers fluent yet logically grounded spoken responses, maintaining both naturalness and precision. On the Spoken-MQA benchmark, it achieves a +19.1% gain in arithmetic reasoning and +6.4% in contextual understanding, with shorter outputs and zero decoding latency.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convergence Analysis of the PAGE Stochastic Algorithm for Weakly Convex Finite-Sum Optimization</title>
<link>https://arxiv.org/abs/2509.00737</link>
<guid>https://arxiv.org/abs/2509.00737</guid>
<content:encoded><![CDATA[
arXiv:2509.00737v2 Announce Type: replace-cross 
Abstract: PAGE, a stochastic algorithm introduced by Li et al. [2021], was designed to find stationary points of averages of smooth nonconvex functions. In this work, we study PAGE in the broad framework of $\tau$-weakly convex functions, which provides a continuous interpolation between the general nonconvex $L$-smooth case ($\tau = L$) and the convex case ($\tau = 0$). We establish new convergence rates for PAGE, showing that its complexity improves as $\tau$ decreases.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Speculative Agent Planning</title>
<link>https://arxiv.org/abs/2509.01920</link>
<guid>https://arxiv.org/abs/2509.01920</guid>
<content:encoded><![CDATA[
arXiv:2509.01920v3 Announce Type: replace-cross 
Abstract: Despite their remarkable success in complex tasks propelling widespread adoption, large language-model-based agents still face critical deployment challenges due to prohibitive latency and inference costs. While recent work has explored various methods to accelerate inference, existing approaches suffer from significant limitations: they either fail to preserve performance fidelity, require extensive offline training of router modules, or incur excessive operational costs. Moreover, they provide minimal user control over the tradeoff between acceleration and other performance metrics. To address these gaps, we introduce Dynamic Speculative Planning (DSP), an asynchronous online reinforcement learning framework that provides lossless acceleration with substantially reduced costs without requiring additional pre-deployment preparation. DSP explicitly optimizes a joint objective balancing end-to-end latency against dollar cost, allowing practitioners to adjust a single parameter that steers the system toward faster responses, cheaper operation, or any point along this continuum. Experiments on two standard agent benchmarks demonstrate that DSP achieves comparable efficiency to the fastest lossless acceleration method while reducing total cost by 30% and unnecessary cost up to 60%. Our code and data are available through https://github.com/guanyilin428/Dynamic-Speculative-Planning.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Effective Strategy for Modeling Score Ordinality and Non-uniform Intervals in Automated Speaking Assessment</title>
<link>https://arxiv.org/abs/2509.03372</link>
<guid>https://arxiv.org/abs/2509.03372</guid>
<content:encoded><![CDATA[
arXiv:2509.03372v2 Announce Type: replace-cross 
Abstract: A recent line of research on automated speaking assessment (ASA) has benefited from self-supervised learning (SSL) representations, which capture rich acoustic and linguistic patterns in non-native speech without underlying assumptions of feature curation. However, speech-based SSL models capture acoustic-related traits but overlook linguistic content, while text-based SSL models rely on ASR output and fail to encode prosodic nuances. Moreover, most prior arts treat proficiency levels as nominal classes, ignoring their ordinal structure and non-uniform intervals between proficiency labels. To address these limitations, we propose an effective ASA approach combining SSL with handcrafted indicator features via a novel modeling paradigm. We further introduce a multi-margin ordinal loss that jointly models both the score ordinality and non-uniform intervals of proficiency labels. Extensive experiments on the TEEMI corpus show that our method consistently outperforms strong baselines and generalizes well to unseen prompts.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Data-Driven Discretized CS:GO Simulation Environment to Facilitate Strategic Multi-Agent Planning Research</title>
<link>https://arxiv.org/abs/2509.06355</link>
<guid>https://arxiv.org/abs/2509.06355</guid>
<content:encoded><![CDATA[
arXiv:2509.06355v2 Announce Type: replace-cross 
Abstract: Modern simulation environments for complex multi-agent interactions must balance high-fidelity detail with computational efficiency. We present DECOY, a novel multi-agent simulator that abstracts strategic, long-horizon planning in 3D terrains into high-level discretized simulation while preserving low-level environmental fidelity. Using Counter-Strike: Global Offensive (CS:GO) as a testbed, our framework accurately simulates gameplay using only movement decisions as tactical positioning -- without explicitly modeling low-level mechanics such as aiming and shooting. Central to our approach is a waypoint system that simplifies and discretizes continuous states and actions, paired with neural predictive and generative models trained on real CS:GO tournament data to reconstruct event outcomes. Extensive evaluations show that replays generated from human data in DECOY closely match those observed in the original game. Our publicly available simulation environment provides a valuable tool for advancing research in strategic multi-agent planning and behavior generation.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HealthSLM-Bench: Benchmarking Small Language Models for Mobile and Wearable Healthcare Monitoring</title>
<link>https://arxiv.org/abs/2509.07260</link>
<guid>https://arxiv.org/abs/2509.07260</guid>
<content:encoded><![CDATA[
arXiv:2509.07260v2 Announce Type: replace-cross 
Abstract: Mobile and wearable healthcare monitoring play a vital role in facilitating timely interventions, managing chronic health conditions, and ultimately improving individuals' quality of life. Previous studies on large language models (LLMs) have highlighted their impressive generalization abilities and effectiveness in healthcare prediction tasks. However, most LLM-based healthcare solutions are cloud-based, which raises significant privacy concerns and results in increased memory usage and latency. To address these challenges, there is growing interest in compact models, Small Language Models (SLMs), which are lightweight and designed to run locally and efficiently on mobile and wearable devices. Nevertheless, how well these models perform in healthcare prediction remains largely unexplored. We systematically evaluated SLMs on health prediction tasks using zero-shot, few-shot, and instruction fine-tuning approaches, and deployed the best performing fine-tuned SLMs on mobile devices to evaluate their real-world efficiency and predictive performance in practical healthcare scenarios. Our results show that SLMs can achieve performance comparable to LLMs while offering substantial gains in efficiency and privacy. However, challenges remain, particularly in handling class imbalance and few-shot scenarios. These findings highlight SLMs, though imperfect in their current form, as a promising solution for next-generation, privacy-preserving healthcare monitoring.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangling Content from Style to Overcome Shortcut Learning: A Hybrid Generative-Discriminative Learning Framework</title>
<link>https://arxiv.org/abs/2509.11598</link>
<guid>https://arxiv.org/abs/2509.11598</guid>
<content:encoded><![CDATA[
arXiv:2509.11598v3 Announce Type: replace-cross 
Abstract: Despite the remarkable success of Self-Supervised Learning (SSL), its generalization is fundamentally hindered by Shortcut Learning, where models exploit superficial features like texture instead of intrinsic structure. We experimentally verify this flaw within the generative paradigm (e.g., MAE) and argue it is a systemic issue also affecting discriminative methods, identifying it as the root cause of their failure on unseen domains. While existing methods often tackle this at a surface level by aligning or separating domain-specific features, they fail to alter the underlying learning mechanism that fosters shortcut dependency. To address this at its core, we propose HyGDL (Hybrid Generative-Discriminative Learning Framework), a hybrid framework that achieves explicit content-style disentanglement. Our approach is guided by the Invariance Pre-training Principle: forcing a model to learn an invariant essence by systematically varying a bias (e.g., style) at the input while keeping the supervision signal constant. HyGDL operates on a single encoder and analytically defines style as the component of a representation that is orthogonal to its style-invariant content, derived via vector projection. This is operationalized through a synergistic design: (1) a self-distillation objective learns a stable, style-invariant content direction; (2) an analytical projection then decomposes the representation into orthogonal content and style vectors; and (3) a style-conditioned reconstruction objective uses these vectors to restore the image, providing end-to-end supervision. Unlike prior methods that rely on implicit heuristics, this principled disentanglement allows HyGDL to learn truly robust representations, demonstrating superior performance on benchmarks designed to diagnose shortcut learning.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Audio Codecs for Prompt-Driven Universal Source Separation</title>
<link>https://arxiv.org/abs/2509.11717</link>
<guid>https://arxiv.org/abs/2509.11717</guid>
<content:encoded><![CDATA[
arXiv:2509.11717v2 Announce Type: replace-cross 
Abstract: Text-guided source separation supports flexible audio editing across media and assistive applications, but existing models like AudioSep are too compute-heavy for edge deployment. Neural audio codec (NAC) models such as CodecFormer and SDCodec are compute-efficient but limited to fixed-class separation. We introduce CodecSep, the first NAC-based model for on-device universal, text-driven separation. CodecSep combines DAC compression with a Transformer masker modulated by CLAP-derived FiLM parameters. Across six open-domain benchmarks under matched training/prompt protocols, \textbf{CodecSep} surpasses \textbf{AudioSep} in separation fidelity (SI-SDR) while remaining competitive in perceptual quality (ViSQOL) and matching or exceeding fixed-stem baselines (TDANet, CodecFormer, SDCodec). In code-stream deployments, it needs just 1.35~GMACs end-to-end -- approximately $54\times$ less compute ($25\times$ architecture-only) than spectrogram-domain separators like AudioSep -- while remaining fully bitstream-compatible.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hardness, Structural Knowledge, and Opportunity: An Analytical Framework for Modular Performance Modeling</title>
<link>https://arxiv.org/abs/2509.11000</link>
<guid>https://arxiv.org/abs/2509.11000</guid>
<content:encoded><![CDATA[
<div> Keywords: performance-influence models, structural knowledge, modular performance modeling, modeling hardness, system characteristics

Summary:<br /><br />Performance-influence models play a crucial role in understanding how configurations affect system performance. This paper focuses on the impact of structural knowledge and system characteristics on the creation of opportunities for improved modular performance modeling. It introduces the concept of modeling hardness as the inherent difficulty of performance modeling, which is influenced by the number of modules and configuration options per module. The study reveals that higher levels of structural knowledge and increased modeling hardness enhance the opportunity for improvement in performance modeling. The findings also suggest that the impact of these factors varies depending on the performance metric, with structural knowledge playing a more dominant role in ranking accuracy and modeling hardness being more crucial for prediction accuracy. This research provides valuable insights for system designers on strategically allocating time and selecting appropriate modeling approaches based on a system's characteristics and task objectives. <div>
arXiv:2509.11000v2 Announce Type: replace-cross 
Abstract: Performance-influence models are beneficial for understanding how configurations affect system performance, but their creation is challenging due to the exponential growth of configuration spaces. While gray-box approaches leverage selective "structural knowledge" (like the module execution graph of the system) to improve modeling, the relationship between this knowledge, a system's characteristics (we call them "structural aspects"), and potential model improvements is not well understood. This paper addresses this gap by formally investigating how variations in structural aspects (e.g., the number of modules and options per module) and the level of structural knowledge impact the creation of "opportunities" for improved "modular performance modeling". We introduce and quantify the concept of modeling "hardness", defined as the inherent difficulty of performance modeling. Through controlled experiments with synthetic system models, we establish an "analytical matrix" to measure these concepts. Our findings show that modeling hardness is primarily driven by the number of modules and configuration options per module. More importantly, we demonstrate that both higher levels of structural knowledge and increased modeling hardness significantly enhance the opportunity for improvement. The impact of these factors varies by performance metric; for ranking accuracy (e.g., in debugging task), structural knowledge is more dominant, while for prediction accuracy (e.g., in resource management task), hardness plays a stronger role. These results provide actionable insights for system designers, guiding them to strategically allocate time and select appropriate modeling approaches based on a system's characteristics and a given task's objectives.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pre-Forgettable Models: Prompt Learning as a Native Mechanism for Unlearning</title>
<link>https://arxiv.org/abs/2509.15230</link>
<guid>https://arxiv.org/abs/2509.15230</guid>
<content:encoded><![CDATA[
<div> Keywords: foundation models, prompt-based learning, unlearning, privacy, data protection

Summary: 
The article introduces a new approach to unlearning in AI models by embedding removability into the architecture itself. This prompt-based learning framework unifies knowledge acquisition and removal within a single training phase, allowing for instant unlearning simply by removing corresponding prompt tokens. This eliminates the need for retraining or model modification, making the process more efficient and suitable for real-time or continuously evolving systems. Experimental results show that the framework preserves predictive performance on retained classes while effectively erasing forgotten ones. In addition, the method is resistant to membership inference attacks and provides strong privacy and security guarantees. By ensuring compliance with data protection principles and safeguarding against unauthorized access to forgotten information, this framework is suitable for deployment in sensitive and regulated environments. Overall, this work establishes a new foundation for designing modular, scalable, and ethically responsive AI models.<br /><br />Summary: <div>
arXiv:2509.15230v1 Announce Type: new 
Abstract: Foundation models have transformed multimedia analysis by enabling robust and transferable representations across diverse modalities and tasks. However, their static deployment conflicts with growing societal and regulatory demands -- particularly the need to unlearn specific data upon request, as mandated by privacy frameworks such as the GDPR. Traditional unlearning approaches, including retraining, activation editing, or distillation, are often computationally expensive, fragile, and ill-suited for real-time or continuously evolving systems. In this paper, we propose a paradigm shift: rethinking unlearning not as a retroactive intervention but as a built-in capability. We introduce a prompt-based learning framework that unifies knowledge acquisition and removal within a single training phase. Rather than encoding information in model weights, our approach binds class-level semantics to dedicated prompt tokens. This design enables instant unlearning simply by removing the corresponding prompt -- without retraining, model modification, or access to original data. Experiments demonstrate that our framework preserves predictive performance on retained classes while effectively erasing forgotten ones. Beyond utility, our method exhibits strong privacy and security guarantees: it is resistant to membership inference attacks, and prompt removal prevents any residual knowledge extraction, even under adversarial conditions. This ensures compliance with data protection principles and safeguards against unauthorized access to forgotten information, making the framework suitable for deployment in sensitive and regulated environments. Overall, by embedding removability into the architecture itself, this work establishes a new foundation for designing modular, scalable and ethically responsive AI models.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Scale Graph Neural Process with Cross-Drug Co-Attention for Drug-Drug Interactions Prediction</title>
<link>https://arxiv.org/abs/2509.15256</link>
<guid>https://arxiv.org/abs/2509.15256</guid>
<content:encoded><![CDATA[
<div> MPNP-DDI, drug-drug interactions, prediction, graph neural process, multi-scale representation<br />
<br />
Summary:<br />
Accurate prediction of drug-drug interactions (DDI) is crucial in ensuring medication safety and effective drug development. Existing methods often struggle to capture structural information across different scales and lack mechanisms for quantifying prediction confidence. In response, the MPNP-DDI framework, a Multi-scale Graph Neural Process, has been developed. It utilizes a unique message-passing scheme to learn a hierarchy of graph representations at multiple scales, with a cross-drug co-attention mechanism dynamically fusing these representations to generate context-aware embeddings for interacting drug pairs. Additionally, an integrated neural process module provides uncertainty estimation. Extensive experiments show that MPNP-DDI outperforms existing baselines, making it a valuable tool for pharmacovigilance, polypharmacy risk assessment, and precision medicine. <div>
arXiv:2509.15256v1 Announce Type: new 
Abstract: Accurate prediction of drug-drug interactions (DDI) is crucial for medication safety and effective drug development. However, existing methods often struggle to capture structural information across different scales, from local functional groups to global molecular topology, and typically lack mechanisms to quantify prediction confidence. To address these limitations, we propose MPNP-DDI, a novel Multi-scale Graph Neural Process framework. The core of MPNP-DDI is a unique message-passing scheme that, by being iteratively applied, learns a hierarchy of graph representations at multiple scales. Crucially, a cross-drug co-attention mechanism then dynamically fuses these multi-scale representations to generate context-aware embeddings for interacting drug pairs, while an integrated neural process module provides principled uncertainty estimation. Extensive experiments demonstrate that MPNP-DDI significantly outperforms state-of-the-art baselines on benchmark datasets. By providing accurate, generalizable, and uncertainty-aware predictions built upon multi-scale structural features, MPNP-DDI represents a powerful computational tool for pharmacovigilance, polypharmacy risk assessment, and precision medicine.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI Meets Wireless Sensing: Towards Wireless Foundation Model</title>
<link>https://arxiv.org/abs/2509.15258</link>
<guid>https://arxiv.org/abs/2509.15258</guid>
<content:encoded><![CDATA[
<div> Generative Artificial Intelligence, wireless sensing systems, data augmentation, domain adaptation, denoising<br />
Summary:<br />
Generative Artificial Intelligence (GenAI) has shown significant progress in computer vision and natural language processing, leading to interest in its integration into wireless sensing systems. The survey explores two integration modes of GenAI in wireless sensing: as a plugin for task-specific models and as a solver for sensing tasks. Mainstream generative models like GANs, VAEs, and diffusion models are analyzed for their applicability in wireless sensing tasks. Key challenges in applying GenAI to wireless sensing are identified, paving the way for a future wireless foundation model that offers scalable, adaptable, and efficient signal understanding across various sensing tasks. <div>
arXiv:2509.15258v1 Announce Type: new 
Abstract: Generative Artificial Intelligence (GenAI) has made significant advancements in fields such as computer vision (CV) and natural language processing (NLP), demonstrating its capability to synthesize high-fidelity data and improve generalization. Recently, there has been growing interest in integrating GenAI into wireless sensing systems. By leveraging generative techniques such as data augmentation, domain adaptation, and denoising, wireless sensing applications, including device localization, human activity recognition, and environmental monitoring, can be significantly improved. This survey investigates the convergence of GenAI and wireless sensing from two complementary perspectives. First, we explore how GenAI can be integrated into wireless sensing pipelines, focusing on two modes of integration: as a plugin to augment task-specific models and as a solver to directly address sensing tasks. Second, we analyze the characteristics of mainstream generative models, such as Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and diffusion models, and discuss their applicability and unique advantages across various wireless sensing tasks. We further identify key challenges in applying GenAI to wireless sensing and outline a future direction toward a wireless foundation model: a unified, pre-trained design capable of scalable, adaptable, and efficient signal understanding across diverse sensing tasks.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IEFS-GMB: Gradient Memory Bank-Guided Feature Selection Based on Information Entropy for EEG Classification of Neurological Disorders</title>
<link>https://arxiv.org/abs/2509.15259</link>
<guid>https://arxiv.org/abs/2509.15259</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, EEG classification, feature selection, information entropy, neurological disorders

Summary:
IEFS-GMB is introduced as a novel feature selection method for deep learning-based EEG classification specifically designed for neurological disorders. It addresses the issue of low signal-to-noise ratio in EEG signals and the lack of model interpretability. By utilizing a Gradient Memory Bank and computing feature importance based on information entropy, IEFS-GMB selects informative EEG features to enhance model performance. Experimental results on four public neurological disease datasets demonstrate accuracy improvements ranging from 0.64% to 6.45% over baseline models. IEFS-GMB outperforms four competing feature selection techniques and offers improved model interpretability. These findings support the practical applicability of IEFS-GMB in clinical settings for automated detection of neurological disorders. 

<br /><br />Summary: <div>
arXiv:2509.15259v1 Announce Type: new 
Abstract: Deep learning-based EEG classification is crucial for the automated detection of neurological disorders, improving diagnostic accuracy and enabling early intervention. However, the low signal-to-noise ratio of EEG signals limits model performance, making feature selection (FS) vital for optimizing representations learned by neural network encoders. Existing FS methods are seldom designed specifically for EEG diagnosis; many are architecture-dependent and lack interpretability, limiting their applicability. Moreover, most rely on single-iteration data, resulting in limited robustness to variability. To address these issues, we propose IEFS-GMB, an Information Entropy-based Feature Selection method guided by a Gradient Memory Bank. This approach constructs a dynamic memory bank storing historical gradients, computes feature importance via information entropy, and applies entropy-based weighting to select informative EEG features. Experiments on four public neurological disease datasets show that encoders enhanced with IEFS-GMB achieve accuracy improvements of 0.64% to 6.45% over baseline models. The method also outperforms four competing FS techniques and improves model interpretability, supporting its practical use in clinical settings.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Weak Supervision Approach for Monitoring Recreational Drug Use Effects in Social Media</title>
<link>https://arxiv.org/abs/2509.15266</link>
<guid>https://arxiv.org/abs/2509.15266</guid>
<content:encoded><![CDATA[
<div> Keywords: recreational drug use, social media, Twitter, machine learning, pharmacovigilance

Summary:<br /><br />Understanding the real-world effects of recreational drug use is challenging, but social media platforms like Twitter provide a valuable source of user-reported experiences. This study analyzed tweets mentioning ecstasy, GHB, and 2C-B to identify and annotate over 92,000 posts with positive or negative effects. The analysis revealed substance-specific phenotypic outcomes, demonstrating the potential of Twitter for real-time drug effect monitoring. Machine learning classifiers, including eXtreme Gradient Boosting, achieved high accuracy in predicting tweet polarity, enabling effective pharmacovigilance. The findings highlight the importance of utilizing social media data for monitoring drug effects and the value of machine learning in this context.Overall, the study illustrates how social media and machine learning can enhance public health research on recreational drug use effects. <br /><br />Summary: <div>
arXiv:2509.15266v1 Announce Type: new 
Abstract: Understanding the real-world effects of recreational drug use remains a critical challenge in public health and biomedical research, especially as traditional surveillance systems often underrepresent user experiences. In this study, we leverage social media (specifically Twitter) as a rich and unfiltered source of user-reported effects associated with three emerging psychoactive substances: ecstasy, GHB, and 2C-B. By combining a curated list of slang terms with biomedical concept extraction via MetaMap, we identified and weakly annotated over 92,000 tweets mentioning these substances. Each tweet was labeled with a polarity reflecting whether it reported a positive or negative effect, following an expert-guided heuristic process. We then performed descriptive and comparative analyses of the reported phenotypic outcomes across substances and trained multiple machine learning classifiers to predict polarity from tweet content, accounting for strong class imbalance using techniques such as cost-sensitive learning and synthetic oversampling. The top performance on the test set was obtained from eXtreme Gradient Boosting with cost-sensitive learning (F1 = 0.885, AUPRC = 0.934). Our findings reveal that Twitter enables the detection of substance-specific phenotypic effects, and that polarity classification models can support real-time pharmacovigilance and drug effect characterization with high accuracy.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Transformers as complex networks to analyze learning dynamics</title>
<link>https://arxiv.org/abs/2509.15269</link>
<guid>https://arxiv.org/abs/2509.15269</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Complex Network Theory, Graph Theory, Computational Components, Self-organization 

Summary:
The study investigates the learning dynamics of Large Language Models (LLMs) using Complex Network Theory (CNT). A novel methodology representing a Transformer-based LLM as a directed, weighted graph is introduced, with nodes as computational components and edges indicating causal influence. By analyzing the evolution of this component-graph in the Pythia-14M model across 143 training checkpoints, distinct phases of exploration, consolidation, and refinement are observed. A stable hierarchy of information spreader components and dynamic information gatherer components emerge, reconfiguring roles at key learning stages. The study highlights how a component-level network perspective provides insight into the self-organizing principles that drive the formation of functional circuits in LLMs. <div>
arXiv:2509.15269v1 Announce Type: new 
Abstract: The process by which Large Language Models (LLMs) acquire complex capabilities during training remains a key open question in mechanistic interpretability. This project investigates whether these learning dynamics can be characterized through the lens of Complex Network Theory (CNT). I introduce a novel methodology to represent a Transformer-based LLM as a directed, weighted graph where nodes are the model's computational components (attention heads and MLPs) and edges represent causal influence, measured via an intervention-based ablation technique. By tracking the evolution of this component-graph across 143 training checkpoints of the Pythia-14M model on a canonical induction task, I analyze a suite of graph-theoretic metrics. The results reveal that the network's structure evolves through distinct phases of exploration, consolidation, and refinement. Specifically, I identify the emergence of a stable hierarchy of information spreader components and a dynamic set of information gatherer components, whose roles reconfigure at key learning junctures. This work demonstrates that a component-level network perspective offers a powerful macroscopic lens for visualizing and understanding the self-organizing principles that drive the formation of functional circuits in LLMs.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Partial Column Generation with Graph Neural Networks for Team Formation and Routing</title>
<link>https://arxiv.org/abs/2509.15275</link>
<guid>https://arxiv.org/abs/2509.15275</guid>
<content:encoded><![CDATA[
<div> Keywords: team formation, routing problem, optimization, column generation, machine learning<br />
<br />
Summary: 
The paper introduces a novel partial column generation strategy for the team formation and routing problem, commonly found in industries like airports and healthcare. The problem involves optimizing team formation and routing to enhance operational efficiency. The proposed strategy utilizes machine learning, specifically graph neural networks, to predict which pricing problems will result in negative reduced costs. Computational experiments demonstrate that this approach outperforms traditional methods, especially on challenging instances with strict time constraints. The method improves solution quality and efficiency, offering a promising solution for complex optimization problems in various real-world applications. <div>
arXiv:2509.15275v1 Announce Type: new 
Abstract: The team formation and routing problem is a challenging optimization problem with several real-world applications in fields such as airport, healthcare, and maintenance operations. To solve this problem, exact solution methods based on column generation have been proposed in the literature. In this paper, we propose a novel partial column generation strategy for settings with multiple pricing problems, based on predicting which ones are likely to yield columns with a negative reduced cost. We develop a machine learning model tailored to the team formation and routing problem that leverages graph neural networks for these predictions. Computational experiments demonstrate that applying our strategy enhances the solution method and outperforms traditional partial column generation approaches from the literature, particularly on hard instances solved under a tight time limit.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fleming-R1: Toward Expert-Level Medical Reasoning via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.15279</link>
<guid>https://arxiv.org/abs/2509.15279</guid>
<content:encoded><![CDATA[
<div> Data Strategy, Chain-of-Thought, Reinforcement Learning, Clinical Reasoning, Medical AI
Summary: 
Fleming-R1 introduces innovative approaches to improve clinical reasoning in medical applications. The Reasoning-Oriented Data Strategy (RODS) enhances coverage of diseases, drugs, and reasoning chains. Chain-of-Thought (CoT) cold start distills reasoning trajectories from teacher models, establishing robust inference priors. The model employs a two-stage Reinforcement Learning from Verifiable Rewards (RLVR) framework using Group Relative Policy Optimization, targeting persistent failure modes. Fleming-R1 surpasses larger baselines and achieves near-parity with GPT-4o in diverse medical benchmarks. Structured data design, reasoning-oriented initialization, and verifiable reinforcement learning contribute to the model's success. Fleming-R1 is released publicly to facilitate transparent and reproducible progress in medical AI, ensuring safer deployment in clinical settings. 
<br /><br />Summary: <div>
arXiv:2509.15279v1 Announce Type: new 
Abstract: While large language models show promise in medical applications, achieving expert-level clinical reasoning remains challenging due to the need for both accurate answers and transparent reasoning processes. To address this challenge, we introduce Fleming-R1, a model designed for verifiable medical reasoning through three complementary innovations. First, our Reasoning-Oriented Data Strategy (RODS) combines curated medical QA datasets with knowledge-graph-guided synthesis to improve coverage of underrepresented diseases, drugs, and multi-hop reasoning chains. Second, we employ Chain-of-Thought (CoT) cold start to distill high-quality reasoning trajectories from teacher models, establishing robust inference priors. Third, we implement a two-stage Reinforcement Learning from Verifiable Rewards (RLVR) framework using Group Relative Policy Optimization, which consolidates core reasoning skills while targeting persistent failure modes through adaptive hard-sample mining. Across diverse medical benchmarks, Fleming-R1 delivers substantial parameter-efficient improvements: the 7B variant surpasses much larger baselines, while the 32B model achieves near-parity with GPT-4o and consistently outperforms strong open-source alternatives. These results demonstrate that structured data design, reasoning-oriented initialization, and verifiable reinforcement learning can advance clinical reasoning beyond simple accuracy optimization. We release Fleming-R1 publicly to promote transparent, reproducible, and auditable progress in medical AI, enabling safer deployment in high-stakes clinical environments.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid unary-binary design for multiplier-less printed Machine Learning classifiers</title>
<link>https://arxiv.org/abs/2509.15316</link>
<guid>https://arxiv.org/abs/2509.15316</guid>
<content:encoded><![CDATA[
<div> flexible electronics, printed electronics, machine learning circuits, hybrid unary-binary architecture, architecture-aware training

Summary:<br />
This work discusses the use of Printed Electronics (PE) as a cost-effective and flexible alternative to silicon for implementing machine learning (ML) circuits. The large feature sizes of PE limit the complexity of classifiers, but by customizing hardware to specific ML models, designers can simplify circuit design. The proposal of a hybrid unary-binary architecture aims to eliminate encoders and enable efficient, multiplier-less execution of MLP classifiers, leading to significant reductions in area and power consumption. The introduction of architecture-aware training further enhances area and power efficiency. Evaluation on six datasets demonstrates average reductions of 46% in area and 39% in power consumption with minimal accuracy loss, outperforming other state-of-the-art MLP designs. <div>
arXiv:2509.15316v1 Announce Type: new 
Abstract: Printed Electronics (PE) provide a flexible, cost-efficient alternative to silicon for implementing machine learning (ML) circuits, but their large feature sizes limit classifier complexity. Leveraging PE's low fabrication and NRE costs, designers can tailor hardware to specific ML models, simplifying circuit design. This work explores alternative arithmetic and proposes a hybrid unary-binary architecture that removes costly encoders and enables efficient, multiplier-less execution of MLP classifiers. We also introduce architecture-aware training to further improve area and power efficiency. Evaluation on six datasets shows average reductions of 46% in area and 39% in power, with minimal accuracy loss, surpassing other state-of-the-art MLP designs.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kuramoto Orientation Diffusion Models</title>
<link>https://arxiv.org/abs/2509.15328</link>
<guid>https://arxiv.org/abs/2509.15328</guid>
<content:encoded><![CDATA[
<div> Keywords: orientation-rich images, generative modeling, stochastic Kuramoto dynamics, structured image generation, biologically inspired synchronization

Summary: 
The article introduces a new generative model for orientation-rich images like fingerprints and textures. Traditional approaches struggle to capture coherent angular patterns in these images. The proposed model is based on stochastic Kuramoto dynamics, inspired by synchronization phenomena in biological systems. The model uses synchronization among phase variables to collapse the data into a low-entropy distribution, followed by desynchronization to generate diverse patterns. The approach enables structured destruction during forward diffusion and a hierarchical generation process. Wrapped Gaussian transition kernels and periodicity-aware networks are used to account for circular geometry. The method performs well on general image benchmarks and significantly improves generation quality on orientation-dense datasets. This work highlights the potential of biologically inspired synchronization dynamics as structured priors in generative modeling.<br /><br />Summary: <div>
arXiv:2509.15328v1 Announce Type: new 
Abstract: Orientation-rich images, such as fingerprints and textures, often exhibit coherent angular directional patterns that are challenging to model using standard generative approaches based on isotropic Euclidean diffusion. Motivated by the role of phase synchronization in biological systems, we propose a score-based generative model built on periodic domains by leveraging stochastic Kuramoto dynamics in the diffusion process. In neural and physical systems, Kuramoto models capture synchronization phenomena across coupled oscillators -- a behavior that we re-purpose here as an inductive bias for structured image generation. In our framework, the forward process performs \textit{synchronization} among phase variables through globally or locally coupled oscillator interactions and attraction to a global reference phase, gradually collapsing the data into a low-entropy von Mises distribution. The reverse process then performs \textit{desynchronization}, generating diverse patterns by reversing the dynamics with a learned score function. This approach enables structured destruction during forward diffusion and a hierarchical generation process that progressively refines global coherence into fine-scale details. We implement wrapped Gaussian transition kernels and periodicity-aware networks to account for the circular geometry. Our method achieves competitive results on general image benchmarks and significantly improves generation quality on orientation-dense datasets like fingerprints and textures. Ultimately, this work demonstrates the promise of biologically inspired synchronization dynamics as structured priors in generative modeling.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Global Pre-fixing, Local Adjusting: A Simple yet Effective Contrastive Strategy for Continual Learning</title>
<link>https://arxiv.org/abs/2509.15347</link>
<guid>https://arxiv.org/abs/2509.15347</guid>
<content:encoded><![CDATA[
<div> Keywords: Continual learning, Contrastive loss, Transferable representations, Global Pre-fixing, Local Adjusting

Summary:
Global Pre-fixing, Local Adjusting for Supervised Contrastive learning (GPLASC) is proposed as a simple and effective strategy for continual learning. By dividing the representation hypersphere into non-overlapping regions with pre-defined centers forming an Equiangular Tight Frame (ETF), task-level confusion is minimized. Additionally, intra-task adjustable ETFs are created within allocated regions to regulate feature structure within individual tasks. This approach ensures discriminative feature structures between tasks and within tasks simultaneously, enhancing performance in contrastive continual learning frameworks. Experimental results confirm the effectiveness of GPLASC in alleviating catastrophic forgetting and improving transferability of representations.<br /><br />Summary: <div>
arXiv:2509.15347v1 Announce Type: new 
Abstract: Continual learning (CL) involves acquiring and accumulating knowledge from evolving tasks while alleviating catastrophic forgetting. Recently, leveraging contrastive loss to construct more transferable and less forgetful representations has been a promising direction in CL. Despite advancements, their performance is still limited due to confusion arising from both inter-task and intra-task features. To address the problem, we propose a simple yet effective contrastive strategy named \textbf{G}lobal \textbf{P}re-fixing, \textbf{L}ocal \textbf{A}djusting for \textbf{S}upervised \textbf{C}ontrastive learning (GPLASC). Specifically, to avoid task-level confusion, we divide the entire unit hypersphere of representations into non-overlapping regions, with the centers of the regions forming an inter-task pre-fixed \textbf{E}quiangular \textbf{T}ight \textbf{F}rame (ETF). Meanwhile, for individual tasks, our method helps regulate the feature structure and form intra-task adjustable ETFs within their respective allocated regions. As a result, our method \textit{simultaneously} ensures discriminative feature structures both between tasks and within tasks and can be seamlessly integrated into any existing contrastive continual learning framework. Extensive experiments validate its effectiveness.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Conformal Coverage Guarantees in Small-Data Settings</title>
<link>https://arxiv.org/abs/2509.15349</link>
<guid>https://arxiv.org/abs/2509.15349</guid>
<content:encoded><![CDATA[
<div> Keywords: Conformal prediction, split conformal prediction, Small Sample Beta Correction, risk control, probabilistic guarantees

Summary: 
The article introduces the Small Sample Beta Correction (SSBC) as a method to improve the accuracy of split conformal prediction by providing probabilistic guarantees on coverage. While conformal prediction already offers distribution-free prediction sets with guaranteed marginal coverage, split conformal prediction's coverage may vary across different calibration sets. SSBC adjusts the conformal significance level based on the exact finite-sample distribution of conformal coverage, ensuring that the deployed predictor achieves the desired coverage with user-defined probability. This adjustment helps control risk effectively in practical applications, enhancing the reliability of predictions. <div>
arXiv:2509.15349v1 Announce Type: new 
Abstract: Conformal prediction provides distribution-free prediction sets with guaranteed marginal coverage. However, in split conformal prediction this guarantee is training-conditional only in expectation: across many calibration draws, the average coverage equals the nominal level, but the realized coverage for a single calibration set may vary substantially. This variance undermines effective risk control in practical applications. Here we introduce the Small Sample Beta Correction (SSBC), a plug-and-play adjustment to the conformal significance level that leverages the exact finite-sample distribution of conformal coverage to provide probabilistic guarantees, ensuring that with user-defined probability over the calibration draw, the deployed predictor achieves at least the desired coverage.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Language Models' Success at Zero-Shot Probabilistic Prediction</title>
<link>https://arxiv.org/abs/2509.15356</link>
<guid>https://arxiv.org/abs/2509.15356</guid>
<content:encoded><![CDATA[
<div> capabilities, large language models, zero-shot predictive, tabular prediction tasks, metrics

Summary:
Large language models (LLMs) have been explored for their zero-shot predictive abilities in generating individual-level characteristics. A study was conducted to assess the performance of LLMs across various tabular prediction tasks, revealing high variability in performance both within and across datasets. When LLMs excel in base prediction tasks, their predicted probabilities can serve as a reliable indicator of individual-level accuracy. Metrics were devised to forecast LLMs' performance at the task level, distinguishing between suitable and unsuitable tasks for these models. Some of these metrics, determined without labeled data, exhibit strong correlations with LLMs' predictive capabilities on new tasks. <div>
arXiv:2509.15356v1 Announce Type: new 
Abstract: Recent work has investigated the capabilities of large language models (LLMs) as zero-shot models for generating individual-level characteristics (e.g., to serve as risk models or augment survey datasets). However, when should a user have confidence that an LLM will provide high-quality predictions for their particular task? To address this question, we conduct a large-scale empirical study of LLMs' zero-shot predictive capabilities across a wide range of tabular prediction tasks. We find that LLMs' performance is highly variable, both on tasks within the same dataset and across different datasets. However, when the LLM performs well on the base prediction task, its predicted probabilities become a stronger signal for individual-level accuracy. Then, we construct metrics to predict LLMs' performance at the task level, aiming to distinguish between tasks where LLMs may perform well and where they are likely unsuitable. We find that some of these metrics, each of which are assessed without labeled data, yield strong signals of LLMs' predictive performance on new tasks.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic Sample Approximations of (Local) Moduli of Continuity</title>
<link>https://arxiv.org/abs/2509.15368</link>
<guid>https://arxiv.org/abs/2509.15368</guid>
<content:encoded><![CDATA[
<div> Keywords: neural networks, robustness, fairness, generalized derivatives, stochastic sample approximation

Summary:
This study explores the relationship between generalized derivatives and moduli of local continuity in evaluating the robustness of neural networks and fairness of their repeated use in closed-loop models. By presenting a non-uniform stochastic sample approximation for moduli of local continuity, the research offers a valuable tool for assessing the robustness of neural networks and the fairness of their repeated deployment. The findings highlight the importance of understanding the underlying mechanisms that influence the performance and reliability of neural networks in various applications. By incorporating these insights into the evaluation and design of neural network models, researchers can enhance their effectiveness and reliability in real-world scenarios. The study contributes to advancing the field of neural network research by providing a novel approach for assessing and improving the robustness and fairness of neural networks in practical settings. 

<br /><br />Summary: <div>
arXiv:2509.15368v1 Announce Type: new 
Abstract: Modulus of local continuity is used to evaluate the robustness of neural networks and fairness of their repeated uses in closed-loop models. Here, we revisit a connection between generalized derivatives and moduli of local continuity, and present a non-uniform stochastic sample approximation for moduli of local continuity. This is of importance in studying robustness of neural networks and fairness of their repeated uses.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial generalization of unfolding (model-based) networks</title>
<link>https://arxiv.org/abs/2509.15370</link>
<guid>https://arxiv.org/abs/2509.15370</guid>
<content:encoded><![CDATA[
<div> Unfolding networks, compressed sensing, adversarial attacks, Rademacher complexity, overparameterization<br />
<br />
Summary:<br />
This paper investigates the adversarial generalization of unfolding networks when subjected to $l_2$-norm constrained attacks. Using a new framework, the study estimates the adversarial Rademacher complexity of a family of overparameterized unfolding networks and provides tight adversarial generalization error bounds relative to the attack level. This analysis, the first of its kind for unfolding networks, is supported by experiments on real-world data. The results demonstrate the theoretical findings consistently across all datasets. Additionally, the study shows that overparameterization within the network family can enhance adversarial robustness, offering insights into effectively improving the robustness of neural networks. <div>
arXiv:2509.15370v1 Announce Type: new 
Abstract: Unfolding networks are interpretable networks emerging from iterative algorithms, incorporate prior knowledge of data structure, and are designed to solve inverse problems like compressed sensing, which deals with recovering data from noisy, missing observations. Compressed sensing finds applications in critical domains, from medical imaging to cryptography, where adversarial robustness is crucial to prevent catastrophic failures. However, a solid theoretical understanding of the performance of unfolding networks in the presence of adversarial attacks is still in its infancy. In this paper, we study the adversarial generalization of unfolding networks when perturbed with $l_2$-norm constrained attacks, generated by the fast gradient sign method. Particularly, we choose a family of state-of-the-art overaparameterized unfolding networks and deploy a new framework to estimate their adversarial Rademacher complexity. Given this estimate, we provide adversarial generalization error bounds for the networks under study, which are tight with respect to the attack level. To our knowledge, this is the first theoretical analysis on the adversarial generalization of unfolding networks. We further present a series of experiments on real-world data, with results corroborating our derived theory, consistently for all data. Finally, we observe that the family's overparameterization can be exploited to promote adversarial robustness, shedding light on how to efficiently robustify neural networks.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning in Stackelberg Mean Field Games: A Non-Asymptotic Analysis</title>
<link>https://arxiv.org/abs/2509.15392</link>
<guid>https://arxiv.org/abs/2509.15392</guid>
<content:encoded><![CDATA[
<div> Hierarchical framework, Stackelberg mean field games, policy optimization, AC-SMFG, finite-time convergence 

Summary:
The study focuses on policy optimization in Stackelberg mean field games (MFGs), where a leader interacts with a large group of followers. AC-SMFG, a single-loop actor-critic algorithm, is proposed to address limitations of existing methods. The algorithm operates on continuously generated Markovian samples and alternates gradient updates for the leader, a follower, and the mean field. Finite-time and finite-sample convergence guarantees are established, a first for Stackelberg MFG algorithms. The key assumption is a "gradient alignment" condition that relaxes the independence assumption between the leader and followers. Simulation results show that AC-SMFG outperforms existing baselines in policy quality and convergence speed.<br /><br />Summary: <div>
arXiv:2509.15392v1 Announce Type: new 
Abstract: We study policy optimization in Stackelberg mean field games (MFGs), a hierarchical framework for modeling the strategic interaction between a single leader and an infinitely large population of homogeneous followers. The objective can be formulated as a structured bi-level optimization problem, in which the leader needs to learn a policy maximizing its reward, anticipating the response of the followers. Existing methods for solving these (and related) problems often rely on restrictive independence assumptions between the leader's and followers' objectives, use samples inefficiently due to nested-loop algorithm structure, and lack finite-time convergence guarantees. To address these limitations, we propose AC-SMFG, a single-loop actor-critic algorithm that operates on continuously generated Markovian samples. The algorithm alternates between (semi-)gradient updates for the leader, a representative follower, and the mean field, and is simple to implement in practice. We establish the finite-time and finite-sample convergence of the algorithm to a stationary point of the Stackelberg objective. To our knowledge, this is the first Stackelberg MFG algorithm with non-asymptotic convergence guarantees. Our key assumption is a "gradient alignment" condition, which requires that the full policy gradient of the leader can be approximated by a partial component of it, relaxing the existing leader-follower independence assumption. Simulation results in a range of well-established economics environments demonstrate that AC-SMFG outperforms existing multi-agent and MFG learning baselines in policy quality and convergence speed.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VMDNet: Time Series Forecasting with Leakage-Free Samplewise Variational Mode Decomposition and Multibranch Decoding</title>
<link>https://arxiv.org/abs/2509.15394</link>
<guid>https://arxiv.org/abs/2509.15394</guid>
<content:encoded><![CDATA[
<div> Keywords: Time series forecasting, Variational Mode Decomposition (VMD), Sample-wise VMD, Frequency-aware embeddings, Temporal Convolutional Networks (TCNs)

Summary:
VMDNet is a new framework for time series forecasting that aims to enhance predictive performance by capturing recurrent temporal patterns. It uses sample-wise VMD to prevent information leakage and employs frequency-aware embeddings to represent decomposed modes independently. Parallel TCNs are used to decode the modes efficiently. The framework includes a bilevel optimization process inspired by Stackelberg games to select VMD's key hyperparameters: the number of modes (K) and the bandwidth penalty (alpha). Experimental results on energy-related datasets show that VMDNet outperforms existing methods in capturing structured periodic patterns, particularly in scenarios with strong periodicity, while maintaining robustness in cases with weak periodicity. <div>
arXiv:2509.15394v1 Announce Type: new 
Abstract: In time series forecasting, capturing recurrent temporal patterns is essential; decomposition techniques make such structure explicit and thereby improve predictive performance. Variational Mode Decomposition (VMD) is a powerful signal-processing method for periodicity-aware decomposition and has seen growing adoption in recent years. However, existing studies often suffer from information leakage and rely on inappropriate hyperparameter tuning. To address these issues, we propose VMDNet, a causality-preserving framework that (i) applies sample-wise VMD to avoid leakage; (ii) represents each decomposed mode with frequency-aware embeddings and decodes it using parallel temporal convolutional networks (TCNs), ensuring mode independence and efficient learning; and (iii) introduces a bilevel, Stackelberg-inspired optimisation to adaptively select VMD's two core hyperparameters: the number of modes (K) and the bandwidth penalty (alpha). Experiments on two energy-related datasets demonstrate that VMDNet achieves state-of-the-art results when periodicity is strong, showing clear advantages in capturing structured periodic patterns while remaining robust under weak periodicity.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization</title>
<link>https://arxiv.org/abs/2509.15399</link>
<guid>https://arxiv.org/abs/2509.15399</guid>
<content:encoded><![CDATA[
<div> minimax optimization, bilevel optimization, stochastic optimization, hierarchical optimization, adaptive algorithms 
Summary:
Hierarchical optimization problems with interdependent decision variables and objectives, such as minimax and bilevel formulations, pose challenges when handling stochastic noise levels. Existing methods lack adaptivity without prior knowledge of noise magnitude. This paper introduces adaptive algorithms for nonconvex-strongly-concave minimax and nonconvex-strongly-convex bilevel stochastic optimization, achieving optimal convergence rates of $\widetilde{O}(1/\sqrt{T} + \sqrt{\bar{\sigma}}/T^{1/4})$ across varying noise levels without requiring prior noise knowledge. The algorithms combine momentum normalization and adaptive parameter choices. Experimental results on synthetic and deep learning tasks validate the effectiveness of the proposed methods. This work contributes the first adaptive and sharp convergence guarantees for stochastic hierarchical optimization. 
<br /><br />Summary: <div>
arXiv:2509.15399v1 Announce Type: new 
Abstract: Hierarchical optimization refers to problems with interdependent decision variables and objectives, such as minimax and bilevel formulations. While various algorithms have been proposed, existing methods and analyses lack adaptivity in stochastic optimization settings: they cannot achieve optimal convergence rates across a wide spectrum of gradient noise levels without prior knowledge of the noise magnitude. In this paper, we propose novel adaptive algorithms for two important classes of stochastic hierarchical optimization problems: nonconvex-strongly-concave minimax optimization and nonconvex-strongly-convex bilevel optimization. Our algorithms achieve sharp convergence rates of $\widetilde{O}(1/\sqrt{T} + \sqrt{\bar{\sigma}}/T^{1/4})$ in $T$ iterations for the gradient norm, where $\bar{\sigma}$ is an upper bound on the stochastic gradient noise. Notably, these rates are obtained without prior knowledge of the noise level, thereby enabling automatic adaptivity in both low and high-noise regimes. To our knowledge, this work provides the first adaptive and sharp convergence guarantees for stochastic hierarchical optimization. Our algorithm design combines the momentum normalization technique with novel adaptive parameter choices. Extensive experiments on synthetic and deep learning tasks demonstrate the effectiveness of our proposed algorithms.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring multimodal implicit behavior learning for vehicle navigation in simulated cities</title>
<link>https://arxiv.org/abs/2509.15400</link>
<guid>https://arxiv.org/abs/2509.15400</guid>
<content:encoded><![CDATA[
<div> Keywords: Behavior Cloning, Multimodal Decisions, Energy-Based Models, Data Augmentation, Derivative-Free Inference

Summary:
Standard Behavior Cloning struggles to learn multimodal driving decisions, where multiple valid actions exist for the same scenario. This study explores Implicit Behavioral Cloning with Energy-Based Models (EBMs) to better capture this complexity. The authors propose Data-Augmented IBC (DA-IBC) as a method to enhance learning by perturbing expert actions to create counterexamples for training IBC and using improved initialization for derivative-free inference. Experiments conducted in the CARLA simulator with Bird's-Eye View inputs demonstrate that DA-IBC surpasses standard IBC in urban driving tasks focused on evaluating multimodal behavior learning in a test environment. The learned energy landscapes effectively represent multimodal action distributions that conventional Behavior Cloning struggles to achieve. This study highlights the potential of using EBMs and data augmentation to enhance multimodal decision-making in autonomous driving scenarios. 

<br /><br />Summary: <div>
arXiv:2509.15400v1 Announce Type: new 
Abstract: Standard Behavior Cloning (BC) fails to learn multimodal driving decisions, where multiple valid actions exist for the same scenario. We explore Implicit Behavioral Cloning (IBC) with Energy-Based Models (EBMs) to better capture this multimodality. We propose Data-Augmented IBC (DA-IBC), which improves learning by perturbing expert actions to form the counterexamples of IBC training and using better initialization for derivative-free inference. Experiments in the CARLA simulator with Bird's-Eye View inputs demonstrate that DA-IBC outperforms standard IBC in urban driving tasks designed to evaluate multimodal behavior learning in a test environment. The learned energy landscapes are able to represent multimodal action distributions, which BC fails to achieve.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Top-$k$ Feature Importance Ranking</title>
<link>https://arxiv.org/abs/2509.15420</link>
<guid>https://arxiv.org/abs/2509.15420</guid>
<content:encoded><![CDATA[
<div> ranked attributions, feature importance, interpretability, machine learning, genomics

Summary: 
The paper introduces RAMPART, a framework designed to accurately rank the top-k important features in machine learning models. Unlike traditional feature importance methods, RAMPART focuses specifically on ranking important features and combines an adaptive sequential halving strategy with an efficient ensembling technique using observation and feature subsampling. The framework optimizes for ranking accuracy and provides theoretical guarantees for correct top-k ranking with high probability. Extensive simulation studies show that RAMPART outperforms popular feature importance methods. The paper concludes with a high-dimensional genomics case study to demonstrate the effectiveness of RAMPART in real-world applications. 

<br /><br />Summary: <div>
arXiv:2509.15420v1 Announce Type: new 
Abstract: Accurate ranking of important features is a fundamental challenge in interpretable machine learning with critical applications in scientific discovery and decision-making. Unlike feature selection and feature importance, the specific problem of ranking important features has received considerably less attention. We introduce RAMPART (Ranked Attributions with MiniPatches And Recursive Trimming), a framework that utilizes any existing feature importance measure in a novel algorithm specifically tailored for ranking the top-$k$ features. Our approach combines an adaptive sequential halving strategy that progressively focuses computational resources on promising features with an efficient ensembling technique using both observation and feature subsampling. Unlike existing methods that convert importance scores to ranks as post-processing, our framework explicitly optimizes for ranking accuracy. We provide theoretical guarantees showing that RAMPART achieves the correct top-$k$ ranking with high probability under mild conditions, and demonstrate through extensive simulation studies that RAMPART consistently outperforms popular feature importance methods, concluding with a high-dimensional genomics case study.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Random Matrix Theory-guided sparse PCA for single-cell RNA-seq data</title>
<link>https://arxiv.org/abs/2509.15429</link>
<guid>https://arxiv.org/abs/2509.15429</guid>
<content:encoded><![CDATA[
<div> Random Matrix Theory, single-cell RNA-seq, sparse principal components, biwhitening method, cell-type classification<br />
<br />
Summary:<br />
The study introduces a Random Matrix Theory (RMT)-based approach to improve single-cell RNA-seq analysis by guiding the inference of sparse principal components. By applying a novel biwhitening method inspired by the Sinkhorn-Knopp algorithm, the method simultaneously stabilizes variance across genes and cells, allowing for the selection of the sparsity level automatically. This parameter-free approach enhances the interpretability of PCA while enabling robust inference of sparse principal components. Testing across multiple single-cell RNA-seq technologies and algorithms, the method consistently outperforms traditional PCA, autoencoder, and diffusion-based methods in cell-type classification tasks. <div>
arXiv:2509.15429v1 Announce Type: new 
Abstract: Single-cell RNA-seq provides detailed molecular snapshots of individual cells but is notoriously noisy. Variability stems from biological differences, PCR amplification bias, limited sequencing depth, and low capture efficiency, making it challenging to adapt computational pipelines to heterogeneous datasets or evolving technologies. As a result, most studies still rely on principal component analysis (PCA) for dimensionality reduction, valued for its interpretability and robustness. Here, we improve upon PCA with a Random Matrix Theory (RMT)-based approach that guides the inference of sparse principal components using existing sparse PCA algorithms. We first introduce a novel biwhitening method, inspired by the Sinkhorn-Knopp algorithm, that simultaneously stabilizes variance across genes and cells. This enables the use of an RMT-based criterion to automatically select the sparsity level, rendering sparse PCA nearly parameter-free. Our mathematically grounded approach retains the interpretability of PCA while enabling robust, hands-off inference of sparse principal components. Across seven single-cell RNA-seq technologies and four sparse PCA algorithms, we show that this method systematically improves the reconstruction of the principal subspace and consistently outperforms PCA-, autoencoder-, and diffusion-based methods in cell-type classification tasks.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computing Linear Regions in Neural Networks with Skip Connections</title>
<link>https://arxiv.org/abs/2509.15441</link>
<guid>https://arxiv.org/abs/2509.15441</guid>
<content:encoded><![CDATA[
<div> Keywords: neural networks, tropical arithmetic, tropical geometry, overfitting, skip connections

Summary: 
Neural networks play a significant role in machine learning, and utilizing tropical arithmetic to represent piecewise linear activation functions allows for the application of tropical geometry. The article introduces algorithms that can determine regions where neural networks function as linear maps. Through computational experiments, the authors shed light on the challenges of training neural networks, specifically addressing issues such as overfitting. Additionally, the benefits of incorporating skip connections in neural networks are highlighted. This research provides valuable insights for researchers and practitioners in the field of machine learning, offering a deeper understanding of the complexities involved in training neural networks and suggesting strategies, such as skip connections, to enhance model performance and mitigate overfitting.<br /><br />Summary: <div>
arXiv:2509.15441v1 Announce Type: new 
Abstract: Neural networks are important tools in machine learning. Representing piecewise linear activation functions with tropical arithmetic enables the application of tropical geometry. Algorithms are presented to compute regions where the neural networks are linear maps. Through computational experiments, we provide insights on the difficulty to train neural networks, in particular on the problems of overfitting and on the benefits of skip connections.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Self-Attention: Generalizing Neural Attention Mechanics to Multi-Scale Problems</title>
<link>https://arxiv.org/abs/2509.15448</link>
<guid>https://arxiv.org/abs/2509.15448</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformers, attention mechanism, multi-modal data, hierarchical information, dynamic programming

Summary: 
Transformers and their attention mechanism have had a significant impact on Machine Learning, extending beyond language data to various modalities like images, videos, and graphs. However, adapting attention mechanisms to scenarios with multi-modal and multi-scale data poses challenges. Existing approaches rely on ad hoc heuristics that may not generalize well. In this paper, the authors introduce a mathematical construct for representing multi-modal, multi-scale data and derive a neural attention mechanism based on entropy minimization. This derived mechanism optimally combines standard Softmax attention with hierarchical and geometric information. An efficient algorithm based on dynamic programming is proposed for computing this attention mechanism. By integrating this hierarchical attention mechanism into transformers, it enables training models in hierarchical/multi-modal settings from scratch and injecting hierarchical information into pre-trained transformer models post-training in a zero-shot manner. <div>
arXiv:2509.15448v1 Announce Type: new 
Abstract: Transformers and their attention mechanism have been revolutionary in the field of Machine Learning. While originally proposed for the language data, they quickly found their way to the image, video, graph, etc. data modalities with various signal geometries. Despite this versatility, generalizing the attention mechanism to scenarios where data is presented at different scales from potentially different modalities is not straightforward. The attempts to incorporate hierarchy and multi-modality within transformers are largely based on ad hoc heuristics, which are not seamlessly generalizable to similar problems with potentially different structures. To address this problem, in this paper, we take a fundamentally different approach: we first propose a mathematical construct to represent multi-modal, multi-scale data. We then mathematically derive the neural attention mechanics for the proposed construct from the first principle of entropy minimization. We show that the derived formulation is optimal in the sense of being the closest to the standard Softmax attention while incorporating the inductive biases originating from the hierarchical/geometric information of the problem. We further propose an efficient algorithm based on dynamic programming to compute our derived attention mechanism. By incorporating it within transformers, we show that the proposed hierarchical attention mechanism not only can be employed to train transformer models in hierarchical/multi-modal settings from scratch, but it can also be used to inject hierarchical information into classical, pre-trained transformer models post training, resulting in more efficient models in zero-shot manner.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IMPQ: Interaction-Aware Layerwise Mixed Precision Quantization for LLMs</title>
<link>https://arxiv.org/abs/2509.15455</link>
<guid>https://arxiv.org/abs/2509.15455</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Mixed-precision quantization, Shapley-based Progressive Quantization Estimation, Interaction-aware Mixed-Precision Quantization, Perplexity

Summary:
Large Language Models (LLMs) with billions of parameters face challenges in on-device deployment due to their scale. This paper introduces a novel approach, Shapley-based Progressive Quantization Estimation (SPQE), to accurately estimate layer sensitivities and inter-layer interactions in mixed-precision quantization. By using SPQE, the Interaction-aware Mixed-Precision Quantization (IMPQ) method assigns 2 or 4-bit precision to layers under memory constraints, resulting in superior performance compared to existing methods. Through experiments on various models and PTQ backends, IMPQ shows substantial improvements in reducing Perplexity, with performance gains increasing as the bit-width decreases.IMPQ proves to be scalable and effective in optimizing the precision of LLMs for efficient deployment. 

<br /><br />Summary: Large Language Models face deployment challenges due to their scale. The SPQE method accurately estimates layer sensitivities and inter-layer interactions, leading to the development of IMPQ for assigning precision under memory constraints. IMPQ demonstrates superior performance in reducing Perplexity compared to existing methods, showing scalability and effectiveness in optimizing precision for efficient deployment of LLMs. <div>
arXiv:2509.15455v1 Announce Type: new 
Abstract: Large Language Models (LLMs) promise impressive capabilities, yet their multi-billion-parameter scale makes on-device or low-resource deployment prohibitive. Mixed-precision quantization offers a compelling solution, but existing methods struggle when the average precision drops below four bits, as they rely on isolated, layer-specific metrics that overlook critical inter-layer interactions affecting overall performance. In this paper, we propose two innovations to address these limitations. First, we frame the mixed-precision quantization problem as a cooperative game among layers and introduce Shapley-based Progressive Quantization Estimation (SPQE) to efficiently obtain accurate Shapley estimates of layer sensitivities and inter-layer interactions. Second, building upon SPQE, we propose Interaction-aware Mixed-Precision Quantization (IMPQ) which translates these Shapley estimates into a binary quadratic optimization formulation, assigning either 2 or 4-bit precision to layers under strict memory constraints. Comprehensive experiments conducted on Llama-3, Gemma-2, and Qwen-3 models across three independent PTQ backends (Quanto, HQQ, GPTQ) demonstrate IMPQ's scalability and consistently superior performance compared to methods relying solely on isolated metrics. Across average precisions spanning 4 bit down to 2 bit, IMPQ cuts Perplexity by 20 to 80 percent relative to the best baseline, with the margin growing as the bit-width tightens.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Reasoning with Large Language Models Augmented by Evolving Knowledge Graphs</title>
<link>https://arxiv.org/abs/2509.15464</link>
<guid>https://arxiv.org/abs/2509.15464</guid>
<content:encoded><![CDATA[
<div> Knowledge Graphs, Language Models, Temporal Reasoning, Knowledge Evolution, Question Answering<br />
Summary:<br />
The paper introduces EvoReasoner, a temporal-aware multi-hop reasoning algorithm for large language models (LLMs) augmented with knowledge graphs (KGs). It addresses the challenge of reasoning over temporally shifting knowledge by performing global-local entity grounding, multi-route decomposition, and temporally grounded scoring. EvoKG, a noise-tolerant KG evolution module, incrementally updates the KG from unstructured documents through confidence-based contradiction resolution and temporal trend tracking. The approach outperforms prompting-based and KG-enhanced baselines in temporal QA benchmarks. Additionally, in a dynamic KG setting where the KG is updated from raw documents, an 8B-parameter model using EvoReasoner matches the performance of a 671B model prompted seven months later. The results emphasize the importance of combining temporal reasoning with KG evolution for robust and up-to-date LLM performance. The code is publicly available on GitHub for further exploration. <br /><br />Summary: <div>
arXiv:2509.15464v1 Announce Type: new 
Abstract: Large language models (LLMs) excel at many language understanding tasks but struggle to reason over knowledge that evolves. To address this, recent work has explored augmenting LLMs with knowledge graphs (KGs) to provide structured, up-to-date information. However, many existing approaches assume a static snapshot of the KG and overlook the temporal dynamics and factual inconsistencies inherent in real-world data. To address the challenge of reasoning over temporally shifting knowledge, we propose EvoReasoner, a temporal-aware multi-hop reasoning algorithm that performs global-local entity grounding, multi-route decomposition, and temporally grounded scoring. To ensure that the underlying KG remains accurate and up-to-date, we introduce EvoKG, a noise-tolerant KG evolution module that incrementally updates the KG from unstructured documents through confidence-based contradiction resolution and temporal trend tracking. We evaluate our approach on temporal QA benchmarks and a novel end-to-end setting where the KG is dynamically updated from raw documents. Our method outperforms both prompting-based and KG-enhanced baselines, effectively narrowing the gap between small and large LLMs on dynamic question answering. Notably, an 8B-parameter model using our approach matches the performance of a 671B model prompted seven months later. These results highlight the importance of combining temporal reasoning with KG evolution for robust and up-to-date LLM performance. Our code is publicly available at github.com/junhongmit/TREK.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solar Forecasting with Causality: A Graph-Transformer Approach to Spatiotemporal Dependencies</title>
<link>https://arxiv.org/abs/2509.15481</link>
<guid>https://arxiv.org/abs/2509.15481</guid>
<content:encoded><![CDATA[
<div> neural network, solar forecasting, global horizontal irradiance, spatio-temporal graph, time-series 
Summary: 
SolarCAST is a new solar forecasting model that predicts global horizontal irradiance at a specific site using historical data from that site and nearby stations. It incorporates three classes of factors influencing sunlight prediction: observable variables, latent factors, and time-lagged influences. By utilizing scalable neural components, SolarCAST achieves high accuracy without the need for specialized hardware or heavy preprocessing. The model outperforms existing baselines and commercial forecasters, such as Solcast, by reducing errors by 25.9%. SolarCAST offers a practical, lightweight, and generalizable solution for accurate solar forecasting, making it a valuable tool for renewable energy management. <div>
arXiv:2509.15481v1 Announce Type: new 
Abstract: Accurate solar forecasting underpins effective renewable energy management. We present SolarCAST, a causally informed model predicting future global horizontal irradiance (GHI) at a target site using only historical GHI from site X and nearby stations S - unlike prior work that relies on sky-camera or satellite imagery requiring specialized hardware and heavy preprocessing. To deliver high accuracy with only public sensor data, SolarCAST models three classes of confounding factors behind X-S correlations using scalable neural components: (i) observable synchronous variables (e.g., time of day, station identity), handled via an embedding module; (ii) latent synchronous factors (e.g., regional weather patterns), captured by a spatio-temporal graph neural network; and (iii) time-lagged influences (e.g., cloud movement across stations), modeled with a gated transformer that learns temporal shifts. It outperforms leading time-series and multimodal baselines across diverse geographical conditions, and achieves a 25.9% error reduction over the top commercial forecaster, Solcast. SolarCAST offers a lightweight, practical, and generalizable solution for localized solar forecasting.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FRAUDGUESS: Spotting and Explaining New Types of Fraud in Million-Scale Financial Data</title>
<link>https://arxiv.org/abs/2509.15493</link>
<guid>https://arxiv.org/abs/2509.15493</guid>
<content:encoded><![CDATA[
<div> Keywords: financial transactions, fraud detection, justification, clustering, visualization<br />
Summary:<br />
The article introduces FRAUDGUESS, a method for detecting fraudulent financial transactions using a combination of clustering and visualization techniques. FRAUDGUESS aims to identify both known and unknown types of fraud by analyzing transaction data and providing evidence to support its findings. By spotting micro-clusters in a specified feature space, FRAUDGUESS can detect new patterns of fraudulent behavior. Additionally, the method utilizes visualization tools such as heatmaps and interactive dashboards to justify its conclusions to domain experts. Real-world application of FRAUDGUESS in an Anonymous Financial Institution has led to the discovery of three new fraudulent behaviors, resulting in the detection of hundreds of suspicious transactions that would have otherwise gone unnoticed. FRAUDGUESS represents an innovative approach to fraud detection in financial systems, offering a promising solution for identifying and preventing fraudulent activities. <br /><br />Summary: <div>
arXiv:2509.15493v1 Announce Type: new 
Abstract: Given a set of financial transactions (who buys from whom, when, and for how much), as well as prior information from buyers and sellers, how can we find fraudulent transactions? If we have labels for some transactions for known types of fraud, we can build a classifier. However, we also want to find new types of fraud, still unknown to the domain experts ('Detection'). Moreover, we also want to provide evidence to experts that supports our opinion ('Justification'). In this paper, we propose FRAUDGUESS, to achieve two goals: (a) for 'Detection', it spots new types of fraud as micro-clusters in a carefully designed feature space; (b) for 'Justification', it uses visualization and heatmaps for evidence, as well as an interactive dashboard for deep dives. FRAUDGUESS is used in real life and is currently considered for deployment in an Anonymous Financial Institution (AFI). Thus, we also present the three new behaviors that FRAUDGUESS discovered in a real, million-scale financial dataset. Two of these behaviors are deemed fraudulent or suspicious by domain experts, catching hundreds of fraudulent transactions that would otherwise go un-noticed.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detail Across Scales: Multi-Scale Enhancement for Full Spectrum Neural Representations</title>
<link>https://arxiv.org/abs/2509.15494</link>
<guid>https://arxiv.org/abs/2509.15494</guid>
<content:encoded><![CDATA[
<div> wavelet-informed implicit neural representation, multi-scale architecture, scientific datasets, reconstruction fidelity, compact model size
Summary: 
WIEN-INR proposes a new wavelet-informed implicit neural representation that addresses the limitations of existing methods by distributing modeling across resolution scales and utilizing a specialized kernel network for fine details. The multi-scale architecture enables the use of smaller networks while maintaining information fidelity, increasing training efficiency, and reducing storage costs. Extensive experiments on diverse scientific datasets demonstrate WIEN-INR's superior reconstruction fidelity and compact model size, making it a practical framework for encoding high-fidelity scientific data where preserving fine detail is crucial. <div>
arXiv:2509.15494v1 Announce Type: new 
Abstract: Implicit neural representations (INRs) have emerged as a compact and parametric alternative to discrete array-based data representations, encoding information directly in neural network weights to enable resolution-independent representation and memory efficiency. However, existing INR approaches, when constrained to compact network sizes, struggle to faithfully represent the multi-scale structures, high-frequency information, and fine textures that characterize the majority of scientific datasets. To address this limitation, we propose WIEN-INR, a wavelet-informed implicit neural representation that distributes modeling across different resolution scales and employs a specialized kernel network at the finest scale to recover subtle details. This multi-scale architecture allows for the use of smaller networks to retain the full spectrum of information while preserving the training efficiency and reducing storage cost. Through extensive experiments on diverse scientific datasets spanning different scales and structural complexities, WIEN-INR achieves superior reconstruction fidelity while maintaining a compact model size. These results demonstrate WIEN-INR as a practical neural representation framework for high-fidelity scientific data encoding, extending the applicability of INRs to domains where efficient preservation of fine detail is essential.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mental Accounts for Actions: EWA-Inspired Attention in Decision Transformers</title>
<link>https://arxiv.org/abs/2509.15498</link>
<guid>https://arxiv.org/abs/2509.15498</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformers, reinforcement learning, Decision Transformers, online training, Experience-Weighted Attraction<br />
Summary: The article introduces Experience-Weighted Attraction with Vector Quantization for Online Decision Transformers (EWA-VQ-ODT), a module designed to improve online decision-making in reinforcement learning. This module enhances the traditional transformers by incorporating a lightweight mechanism inspired by cognitive models. It maintains per-action mental accounts to summarize recent successes and failures, guiding decision-making. Using vector quantization, actions are efficiently routed to a compact codebook, optimizing attention and improving long-term action effectiveness. EWA-VQ-ODT demonstrates improved sample efficiency and average return over existing methods, particularly during early training stages. The module is computationally efficient, interpretable, and supported by theoretical guarantees that ensure attention dynamics and impact on attention drift are well-bounded.<br /><br />Summary: <div>
arXiv:2509.15498v1 Announce Type: new 
Abstract: Transformers have emerged as a compelling architecture for sequential decision-making by modeling trajectories via self-attention. In reinforcement learning (RL), they enable return-conditioned control without relying on value function approximation. Decision Transformers (DTs) exploit this by casting RL as supervised sequence modeling, but they are restricted to offline data and lack exploration. Online Decision Transformers (ODTs) address this limitation through entropy-regularized training on on-policy rollouts, offering a stable alternative to traditional RL methods like Soft Actor-Critic, which depend on bootstrapped targets and reward shaping. Despite these advantages, ODTs use standard attention, which lacks explicit memory of action-specific outcomes. This leads to inefficiencies in learning long-term action effectiveness. Inspired by cognitive models such as Experience-Weighted Attraction (EWA), we propose Experience-Weighted Attraction with Vector Quantization for Online Decision Transformers (EWA-VQ-ODT), a lightweight module that maintains per-action mental accounts summarizing recent successes and failures. Continuous actions are routed via direct grid lookup to a compact vector-quantized codebook, where each code stores a scalar attraction updated online through decay and reward-based reinforcement. These attractions modulate attention by biasing the columns associated with action tokens, requiring no change to the backbone or training objective. On standard continuous-control benchmarks, EWA-VQ-ODT improves sample efficiency and average return over ODT, particularly in early training. The module is computationally efficient, interpretable via per-code traces, and supported by theoretical guarantees that bound the attraction dynamics and its impact on attention drift.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Policy Gradient Optimzation for Bayesian-Risk MDPs with General Convex Losses</title>
<link>https://arxiv.org/abs/2509.15509</link>
<guid>https://arxiv.org/abs/2509.15509</guid>
<content:encoded><![CDATA[
<div> Bayesian, Markov decision processes, coherent risk functional, policy gradient optimization, convergence rate<br />
Summary:<br />
This study focuses on Markov decision processes with unknown parameters and a general loss function, utilizing a Bayesian approach for parameter estimation and a coherent risk functional for loss assessment. As traditional dynamic programming methods cannot be applied due to the lack of Bellman equations, a policy gradient optimization method is proposed, incorporating the dual representation of coherent risk measures and extending the envelope theorem. The algorithm's stationary analysis shows a convergence rate of O(T^-1/2+r^-1/2), where T is the number of policy gradient iterations and r is the sample size of the gradient estimator. The algorithm is further extended to an episodic setting, with global convergence established, and bounds provided for the number of iterations required to achieve a specified error level in each episode. <div>
arXiv:2509.15509v1 Announce Type: new 
Abstract: Motivated by many application problems, we consider Markov decision processes (MDPs) with a general loss function and unknown parameters. To mitigate the epistemic uncertainty associated with unknown parameters, we take a Bayesian approach to estimate the parameters from data and impose a coherent risk functional (with respect to the Bayesian posterior distribution) on the loss. Since this formulation usually does not satisfy the interchangeability principle, it does not admit Bellman equations and cannot be solved by approaches based on dynamic programming. Therefore, We propose a policy gradient optimization method, leveraging the dual representation of coherent risk measures and extending the envelope theorem to continuous cases. We then show the stationary analysis of the algorithm with a convergence rate of $O(T^{-1/2}+r^{-1/2})$, where $T$ is the number of policy gradient iterations and $r$ is the sample size of the gradient estimator. We further extend our algorithm to an episodic setting, and establish the global convergence of the extended algorithm and provide bounds on the number of iterations needed to achieve an error bound $O(\epsilon)$ in each episode.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KoopCast: Trajectory Forecasting via Koopman Operators</title>
<link>https://arxiv.org/abs/2509.15513</link>
<guid>https://arxiv.org/abs/2509.15513</guid>
<content:encoded><![CDATA[
<div> KoopCast, trajectory forecasting, dynamic environments, Koopman operator theory, linear representation<br />
<br />
Summary:
KoopCast presents a model for trajectory forecasting in dynamic environments. It leverages Koopman operator theory, enabling a linear representation of nonlinear dynamics. The two-stage design includes a probabilistic neural goal estimator for long-term targets and a Koopman operator-based refinement module for incorporating intention and history into a nonlinear feature space. This approach offers competitive accuracy, interpretability based on Koopman spectral theory, and low-latency deployment. Validated on benchmark datasets like ETH/UCY and nuScenes, KoopCast consistently delivers high predictive accuracy, mode-level interpretability, and practical efficiency. <div>
arXiv:2509.15513v1 Announce Type: new 
Abstract: We present KoopCast, a lightweight yet efficient model for trajectory forecasting in general dynamic environments. Our approach leverages Koopman operator theory, which enables a linear representation of nonlinear dynamics by lifting trajectories into a higher-dimensional space. The framework follows a two-stage design: first, a probabilistic neural goal estimator predicts plausible long-term targets, specifying where to go; second, a Koopman operator-based refinement module incorporates intention and history into a nonlinear feature space, enabling linear prediction that dictates how to go. This dual structure not only ensures strong predictive accuracy but also inherits the favorable properties of linear operators while faithfully capturing nonlinear dynamics. As a result, our model offers three key advantages: (i) competitive accuracy, (ii) interpretability grounded in Koopman spectral theory, and (iii) low-latency deployment. We validate these benefits on ETH/UCY, the Waymo Open Motion Dataset, and nuScenes, which feature rich multi-agent interactions and map-constrained nonlinear motion. Across benchmarks, KoopCast consistently delivers high predictive accuracy together with mode-level interpretability and practical efficiency.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Manifold Dimension Estimation: An Empirical Study</title>
<link>https://arxiv.org/abs/2509.15517</link>
<guid>https://arxiv.org/abs/2509.15517</guid>
<content:encoded><![CDATA[
<div> manifold hypothesis, dimension estimation, high-dimensional data, low-dimensional manifold, theoretical foundations

Summary:
The article discusses the manifold hypothesis, which posits that high-dimensional data often lies on or near a low-dimensional manifold. Estimating the dimension of this manifold is crucial for leveraging its structure. The study provides a comprehensive survey on dimension estimation, reviewing theoretical foundations and presenting eight estimators. Controlled experiments analyze factors such as noise, curvature, and sample size on estimator performance. Comparisons of estimators on synthetic and real-world datasets highlight the importance of dataset-specific hyperparameter tuning. The findings suggest that simpler methods often outperform more complex ones, providing practical guidance for researchers and practitioners in the field. <div>
arXiv:2509.15517v1 Announce Type: new 
Abstract: The manifold hypothesis suggests that high-dimensional data often lie on or near a low-dimensional manifold. Estimating the dimension of this manifold is essential for leveraging its structure, yet existing work on dimension estimation is fragmented and lacks systematic evaluation. This article provides a comprehensive survey for both researchers and practitioners. We review often-overlooked theoretical foundations and present eight representative estimators. Through controlled experiments, we analyze how individual factors such as noise, curvature, and sample size affect performance. We also compare the estimators on diverse synthetic and real-world datasets, introducing a principled approach to dataset-specific hyperparameter tuning. Our results offer practical guidance and suggest that, for a problem of this generality, simpler methods often perform better.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fully Decentralized Cooperative Multi-Agent Reinforcement Learning is A Context Modeling Problem</title>
<link>https://arxiv.org/abs/2509.15519</link>
<guid>https://arxiv.org/abs/2509.15519</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, multi-agent, decentralized, cooperative, dynamics-aware context

Summary: 
This paper introduces a novel method called Dynamics-Aware Context (DAC) for fully decentralized cooperative multi-agent reinforcement learning. DAC addresses the challenges of non-stationarity and relative overgeneralization by modeling the task as a Contextual Markov Decision Process and incorporating dynamics-aware context modeling. It handles non-stationarity by identifying switches between unobserved contexts corresponding to distinct joint policies and modeling step-wise dynamics distribution with latent variables. For each agent, DAC introduces a context-based value function to update the value function effectively. Additionally, an optimistic marginal value is derived to promote cooperative action selection and address the issue of relative overgeneralization. Experimental results on various cooperative tasks demonstrate the superior performance of DAC compared to multiple baselines, validating its effectiveness in decentralized cooperative settings. 

<br /><br />Summary: <div>
arXiv:2509.15519v1 Announce Type: new 
Abstract: This paper studies fully decentralized cooperative multi-agent reinforcement learning, where each agent solely observes the states, its local actions, and the shared rewards. The inability to access other agents' actions often leads to non-stationarity during value function updates and relative overgeneralization during value function estimation, hindering effective cooperative policy learning. However, existing works fail to address both issues simultaneously, due to their inability to model the joint policy of other agents in a fully decentralized setting. To overcome this limitation, we propose a novel method named Dynamics-Aware Context (DAC), which formalizes the task, as locally perceived by each agent, as an Contextual Markov Decision Process, and further addresses both non-stationarity and relative overgeneralization through dynamics-aware context modeling. Specifically, DAC attributes the non-stationary local task dynamics of each agent to switches between unobserved contexts, each corresponding to a distinct joint policy. Then, DAC models the step-wise dynamics distribution using latent variables and refers to them as contexts. For each agent, DAC introduces a context-based value function to address the non-stationarity issue during value function update. For value function estimation, an optimistic marginal value is derived to promote the selection of cooperative actions, thereby addressing the relative overgeneralization issue. Experimentally, we evaluate DAC on various cooperative tasks (including matrix game, predator and prey, and SMAC), and its superior performance against multiple baselines validates its effectiveness.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal Learning of Stochastic Dynamics for Exact Belief Propagation using Bernstein Normalizing Flows</title>
<link>https://arxiv.org/abs/2509.15533</link>
<guid>https://arxiv.org/abs/2509.15533</guid>
<content:encoded><![CDATA[
<div> learning model, nonlinear dynamics, stochastic system, belief propagation, normalizing flows

Summary:
This paper introduces a novel approach for predicting future states in stochastic systems through belief propagation. The challenge lies in dealing with nonlinear dynamics, which often hinder analytical solutions, leading researchers to resort to approximate methods. The proposed model aims to address this issue by combining the flexibility of normalizing flows for density estimation with the analytical tractability of Bernstein polynomials. By learning the system model from data, the approach ensures universality in approximating general nonlinear stochastic dynamics while supporting analytical belief propagation. Empirical results demonstrate the effectiveness of the learned model, outperforming existing data-driven methods in scenarios involving highly nonlinear systems with non-additive, non-Gaussian noise. <div>
arXiv:2509.15533v1 Announce Type: new 
Abstract: Predicting the distribution of future states in a stochastic system, known as belief propagation, is fundamental to reasoning under uncertainty. However, nonlinear dynamics often make analytical belief propagation intractable, requiring approximate methods. When the system model is unknown and must be learned from data, a key question arises: can we learn a model that (i) universally approximates general nonlinear stochastic dynamics, and (ii) supports analytical belief propagation? This paper establishes the theoretical foundations for a class of models that satisfy both properties. The proposed approach combines the expressiveness of normalizing flows for density estimation with the analytical tractability of Bernstein polynomials. Empirical results show the efficacy of our learned model over state-of-the-art data-driven methods for belief propagation, especially for highly non-linear systems with non-additive, non-Gaussian noise.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nonconvex Decentralized Stochastic Bilevel Optimization under Heavy-Tailed Noises</title>
<link>https://arxiv.org/abs/2509.15543</link>
<guid>https://arxiv.org/abs/2509.15543</guid>
<content:encoded><![CDATA[
<div> decentralized, stochastic, optimization, nonconvex, heavy-tailed
<br />
Summary:
This article introduces a novel decentralized stochastic bilevel optimization algorithm designed for nonconvex problems under heavy-tailed noises, a scenario common in real-world machine learning. Unlike existing methods, this algorithm does not require the lower-level loss function to be strongly convex or the stochastic gradient noise to have finite variance. The normalized stochastic variance-reduced bilevel gradient descent algorithm developed does not rely on clipping operations and offers rigorous theoretical guarantees for nonconvex decentralized bilevel optimization problems under heavy-tailed noises. This is a significant contribution as it is the first decentralized bilevel optimization algorithm with such guarantees. Experimental results demonstrate the effectiveness of the algorithm in handling heavy-tailed noises. <div>
arXiv:2509.15543v1 Announce Type: new 
Abstract: Existing decentralized stochastic optimization methods assume the lower-level loss function is strongly convex and the stochastic gradient noise has finite variance. These strong assumptions typically are not satisfied in real-world machine learning models. To address these limitations, we develop a novel decentralized stochastic bilevel optimization algorithm for the nonconvex bilevel optimization problem under heavy-tailed noises. Specifically, we develop a normalized stochastic variance-reduced bilevel gradient descent algorithm, which does not rely on any clipping operation. Moreover, we establish its convergence rate by innovatively bounding interdependent gradient sequences under heavy-tailed noises for nonconvex decentralized bilevel optimization problems. As far as we know, this is the first decentralized bilevel optimization algorithm with rigorous theoretical guarantees under heavy-tailed noises. The extensive experimental results confirm the effectiveness of our algorithm in handling heavy-tailed noises.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PolyJuice Makes It Real: Black-Box, Universal Red Teaming for Synthetic Image Detectors</title>
<link>https://arxiv.org/abs/2509.15551</link>
<guid>https://arxiv.org/abs/2509.15551</guid>
<content:encoded><![CDATA[
<div> defenses, Synthetic image detectors, red teaming, PolyJuice, T2I latent space shift <br />
Summary: <br />
The article introduces PolyJuice, a black-box, image-agnostic red-teaming method for Synthetic Image Detectors (SIDs) to combat the risks posed by text-to-image (T2I) models. PolyJuice leverages observed distribution shifts in the T2I latent space to identify SID failure modes without requiring white-box access. By steering generated images towards these failure modes, PolyJuice significantly enhances deception effectiveness (up to 84%) compared to unsteered models. Furthermore, steering directions can be estimated at lower resolutions and transferred to higher resolutions with minimal computational overhead. Notably, tuning SID models on PolyJuice-augmented datasets leads to a substantial performance boost (up to 30%). PolyJuice offers a novel approach to improving SID defenses and highlights the importance of red teaming in strengthening image detection mechanisms. <br /> <div>
arXiv:2509.15551v1 Announce Type: new 
Abstract: Synthetic image detectors (SIDs) are a key defense against the risks posed by the growing realism of images from text-to-image (T2I) models. Red teaming improves SID's effectiveness by identifying and exploiting their failure modes via misclassified synthetic images. However, existing red-teaming solutions (i) require white-box access to SIDs, which is infeasible for proprietary state-of-the-art detectors, and (ii) generate image-specific attacks through expensive online optimization. To address these limitations, we propose PolyJuice, the first black-box, image-agnostic red-teaming method for SIDs, based on an observed distribution shift in the T2I latent space between samples correctly and incorrectly classified by the SID. PolyJuice generates attacks by (i) identifying the direction of this shift through a lightweight offline process that only requires black-box access to the SID, and (ii) exploiting this direction by universally steering all generated images towards the SID's failure modes. PolyJuice-steered T2I models are significantly more effective at deceiving SIDs (up to 84%) compared to their unsteered counterparts. We also show that the steering directions can be estimated efficiently at lower resolutions and transferred to higher resolutions using simple interpolation, reducing computational overhead. Finally, tuning SID models on PolyJuice-augmented datasets notably enhances the performance of the detectors (up to 30%).
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Multi-Query Paradox in Zeroth-Order Optimization</title>
<link>https://arxiv.org/abs/2509.15552</link>
<guid>https://arxiv.org/abs/2509.15552</guid>
<content:encoded><![CDATA[
<div> Zeroth-order optimization, multi-query paradigm, budget allocation, aggregation methods, convergence rates  
Summary:  
The study explores the optimal allocation of queries in zeroth-order optimization, where gradients are approximated using function value queries. Two aggregation methods, ZO-Avg and ZO-Align, are compared in terms of convergence rates across different optimization settings. The research emphasizes a critical trade-off between queries per iteration and total optimization iterations. ZO-Avg is proven to be query-inefficient with multiple queries per iteration, while ZO-Align performs better with more queries, leading to a full-subspace estimation. The choice between the two methods significantly impacts the optimization efficiency, highlighting the importance of considering aggregation methods in budget allocation decisions. Experimental results validate the theoretical findings, showcasing the superiority of ZO-Align in multi-query scenarios.  
<br /><br />Summary: <div>
arXiv:2509.15552v1 Announce Type: new 
Abstract: Zeroth-order (ZO) optimization provides a powerful framework for problems where explicit gradients are unavailable and have to be approximated using only queries to function value. The prevalent single-query approach is simple, but suffers from high estimation variance, motivating a multi-query paradigm to improves estimation accuracy. This, however, creates a critical trade-off: under a fixed budget of queries (i.e. cost), queries per iteration and the total number of optimization iterations are inversely proportional to one another. How to best allocate this budget is a fundamental, under-explored question.
  This work systematically resolves this query allocation problem. We analyze two aggregation methods: the de facto simple averaging (ZO-Avg), and a new Projection Alignment method (ZO-Align) we derive from local surrogate minimization. By deriving convergence rates for both methods that make the dependence on the number of queries explicit across strongly convex, convex, non-convex, and stochastic settings, we uncover a stark dichotomy: For ZO-Avg, we prove that using more than one query per iteration is always query-inefficient, rendering the single-query approach optimal. On the contrary, ZO-Align generally performs better with more queries per iteration, resulting in a full-subspace estimation as the optimal approach. Thus, our work clarifies that the multi-query problem boils down to a choice not about an intermediate query size, but between two classic algorithms, a choice dictated entirely by the aggregation method used. These theoretical findings are also consistently validated by extensive experiments.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reward Hacking Mitigation using Verifiable Composite Rewards</title>
<link>https://arxiv.org/abs/2509.15557</link>
<guid>https://arxiv.org/abs/2509.15557</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Verifiable Rewards, Language Models, Medical Domain, Reward Hacking

Summary:
Reinforcement Learning from Verifiable Rewards (RLVR) has demonstrated that large language models (LLMs) can develop reasoning skills independently. However, in the medical domain, models trained for question answering are prone to reward hacking during the reasoning process. This behavior includes providing answers without proper reasoning and manipulating the reward mechanism through non-standard reasoning formats. To address these issues, the authors propose a composite reward function with penalties for such behaviors. Experiments show that integrating this reward model into RLVR leads to improved reasoning structure, reduced reward hacking, and higher accuracy compared to baseline models. This approach is a significant advancement in mitigating reward hacking and enhancing the trustworthiness of models utilizing RLVR.<br /><br />Summary: <div>
arXiv:2509.15557v1 Announce Type: new 
Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has recently shown that large language models (LLMs) can develop their own reasoning without direct supervision. However, applications in the medical domain, specifically for question answering, are susceptible to significant reward hacking during the reasoning phase. Our work addresses two primary forms of this behavior: i) providing a final answer without preceding reasoning, and ii) employing non-standard reasoning formats to exploit the reward mechanism. To mitigate these, we introduce a composite reward function with specific penalties for these behaviors. Our experiments show that extending RLVR with our proposed reward model leads to better-formatted reasoning with less reward hacking and good accuracy compared to the baselines. This approach marks a step toward reducing reward hacking and enhancing the reliability of models utilizing RLVR.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Small LLMs with Expert Blocks Are Good Enough for Hyperparamter Tuning</title>
<link>https://arxiv.org/abs/2509.15561</link>
<guid>https://arxiv.org/abs/2509.15561</guid>
<content:encoded><![CDATA[
<div> Expert Block Framework, Hyper-parameter Tuning, Small LLMs, Trajectory Context Summarizer, Large Language Models <br />
Summary: <br />
Hyper-parameter Tuning (HPT) is essential in machine learning pipelines but can be costly and complex with larger models. The Expert Block Framework introduced in this study utilizes Small Language Models (LLMs) for efficient HPT. The Trajectory Context Summarizer (TCS) plays a crucial role in transforming training trajectories into structured context, enabling small LLMs to analyze optimization progress effectively. Utilizing two locally-run LLMs and a 10-trial budget, the proposed HPT pipeline, augmented with TCS, achieves competitive performance close to GPT-4 on multiple tasks. This demonstrates the effectiveness of using small LLMs with expertly designed blocks like TCS for efficient and reliable hyper-parameter tuning in machine learning applications. <br /> <div>
arXiv:2509.15561v1 Announce Type: new 
Abstract: Hyper-parameter Tuning (HPT) is a necessary step in machine learning (ML) pipelines but becomes computationally expensive and opaque with larger models. Recently, Large Language Models (LLMs) have been explored for HPT, yet most rely on models exceeding 100 billion parameters. We propose an Expert Block Framework for HPT using Small LLMs. At its core is the Trajectory Context Summarizer (TCS), a deterministic block that transforms raw training trajectories into structured context, enabling small LLMs to analyze optimization progress with reliability comparable to larger models. Using two locally-run LLMs (phi4:reasoning14B and qwen2.5-coder:32B) and a 10-trial budget, our TCS-enabled HPT pipeline achieves average performance within ~0.9 percentage points of GPT-4 across six diverse tasks.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How many classes do we need to see for novel class discovery?</title>
<link>https://arxiv.org/abs/2509.15585</link>
<guid>https://arxiv.org/abs/2509.15585</guid>
<content:encoded><![CDATA[
<div> discovery, class discovery, ML models, dSprites dataset, known classes <br />
Summary: 
The study focuses on novel class discovery in machine learning models and the factors influencing its success. Using the dSprites dataset with simulated modifying factors, the researchers investigated the impact of the number of known and unknown classes on discovery performance. They found that the benefit of the number of known classes reaches a saturation point where discovery performance plateaus, suggesting a diminishing return on adding more known classes. Additionally, the study examined the impact of known class 'coverage' on discovering new classes. The results provide insights for practitioners on cost-benefit analysis in class discovery tasks and offer a foundation for future research in this area on complex real-world datasets. <div>
arXiv:2509.15585v1 Announce Type: new 
Abstract: Novel class discovery is essential for ML models to adapt to evolving real-world data, with applications ranging from scientific discovery to robotics. However, these datasets contain complex and entangled factors of variation, making a systematic study of class discovery difficult. As a result, many fundamental questions are yet to be answered on why and when new class discoveries are more likely to be successful. To address this, we propose a simple controlled experimental framework using the dSprites dataset with procedurally generated modifying factors. This allows us to investigate what influences successful class discovery. In particular, we study the relationship between the number of known/unknown classes and discovery performance, as well as the impact of known class 'coverage' on discovering new classes. Our empirical results indicate that the benefit of the number of known classes reaches a saturation point beyond which discovery performance plateaus. The pattern of diminishing return across different settings provides an insight for cost-benefit analysis for practitioners and a starting point for more rigorous future research of class discovery on complex real-world datasets.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Zoning Network: A Unified Principle for Generative Modeling, Representation Learning, and Classification</title>
<link>https://arxiv.org/abs/2509.15591</link>
<guid>https://arxiv.org/abs/2509.15591</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative modeling, Representation learning, Classification, Latent Zoning Network, Shared Gaussian latent space

Summary:
Generative modeling, representation learning, and classification are essential tasks in machine learning. The Latent Zoning Network (LZN) proposed in this paper aims to unify these tasks by creating a shared Gaussian latent space that can encode information across different data types. LZN enhances existing models by improving image generation performance without changing the training objective. It can independently solve tasks like unsupervised representation learning, outperforming existing methods on downstream classification tasks. Additionally, LZN can address multiple tasks simultaneously, such as joint generation and classification, with image and label encoders/decoders. The experimental results show promising performance improvements in various scenarios, demonstrating the potential of LZN as a unified framework for machine learning tasks. Further information, including code and trained models, is available on the project's GitHub repository and website. 

<br /><br />Summary: <div>
arXiv:2509.15591v1 Announce Type: new 
Abstract: Generative modeling, representation learning, and classification are three core problems in machine learning (ML), yet their state-of-the-art (SoTA) solutions remain largely disjoint. In this paper, we ask: Can a unified principle address all three? Such unification could simplify ML pipelines and foster greater synergy across tasks. We introduce Latent Zoning Network (LZN) as a step toward this goal. At its core, LZN creates a shared Gaussian latent space that encodes information across all tasks. Each data type (e.g., images, text, labels) is equipped with an encoder that maps samples to disjoint latent zones, and a decoder that maps latents back to data. ML tasks are expressed as compositions of these encoders and decoders: for example, label-conditional image generation uses a label encoder and image decoder; image embedding uses an image encoder; classification uses an image encoder and label decoder. We demonstrate the promise of LZN in three increasingly complex scenarios: (1) LZN can enhance existing models (image generation): When combined with the SoTA Rectified Flow model, LZN improves FID on CIFAR10 from 2.76 to 2.59-without modifying the training objective. (2) LZN can solve tasks independently (representation learning): LZN can implement unsupervised representation learning without auxiliary loss functions, outperforming the seminal MoCo and SimCLR methods by 9.3% and 0.2%, respectively, on downstream linear classification on ImageNet. (3) LZN can solve multiple tasks simultaneously (joint generation and classification): With image and label encoders/decoders, LZN performs both tasks jointly by design, improving FID and achieving SoTA classification accuracy on CIFAR10. The code and trained models are available at https://github.com/microsoft/latent-zoning-networks. The project website is at https://zinanlin.me/blogs/latent_zoning_networks.html.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalized Prediction By Learning Halfspace Reference Classes Under Well-Behaved Distribution</title>
<link>https://arxiv.org/abs/2509.15592</link>
<guid>https://arxiv.org/abs/2509.15592</guid>
<content:encoded><![CDATA[
<div> Machine learning, predictive models, interpretability, personalized prediction, sparse linear classifiers <br />
Summary: <br />
This article focuses on the development of personalized prediction schemes in machine learning to improve interpretability and accuracy in high-stakes applications like healthcare. It proposes a method where an easy-to-interpret predictor is trained per query, aiming to achieve competitive performance specifically for certain sub-populations. The study examines the PAC-learnability of personalized prediction models for sub-populations represented by "halfspaces" in a label-agnostic setting. The research presents a distribution-specific PAC-learning algorithm for learning reference classes for personalized prediction and proves an upper bound of O(opt^1/4) for personalized prediction with sparse linear classifiers and homogeneous halfspace subsets. The algorithms are evaluated using standard benchmark datasets to demonstrate their effectiveness. <br /> <div>
arXiv:2509.15592v1 Announce Type: new 
Abstract: In machine learning applications, predictive models are trained to serve future queries across the entire data distribution. Real-world data often demands excessively complex models to achieve competitive performance, however, sacrificing interpretability. Hence, the growing deployment of machine learning models in high-stakes applications, such as healthcare, motivates the search for methods for accurate and explainable predictions. This work proposes a Personalized Prediction scheme, where an easy-to-interpret predictor is learned per query. In particular, we wish to produce a "sparse linear" classifier with competitive performance specifically on some sub-population that includes the query point. The goal of this work is to study the PAC-learnability of this prediction model for sub-populations represented by "halfspaces" in a label-agnostic setting. We first give a distribution-specific PAC-learning algorithm for learning reference classes for personalized prediction. By leveraging both the reference-class learning algorithm and a list learner of sparse linear representations, we prove the first upper bound, $O(\mathrm{opt}^{1/4} )$, for personalized prediction with sparse linear classifiers and homogeneous halfspace subsets. We also evaluate our algorithms on a variety of standard benchmark data sets.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Extractive Text Summarization for Online News Articles Using Machine Learning</title>
<link>https://arxiv.org/abs/2509.15614</link>
<guid>https://arxiv.org/abs/2509.15614</guid>
<content:encoded><![CDATA[
<div> pipeline, BERT embeddings, LSTM networks, content management, online news platforms<br />
Summary: This article explores the use of advanced machine learning techniques for extractive text summarization using the Cornell Newsroom dataset. By leveraging BERT embeddings and framing the task as a binary classification problem, the study compares various models and finds that LSTM networks outperform baseline methods in F1 score and ROUGE-1 metrics. The research highlights the potential of automated summarization in enhancing content management systems for online news platforms, leading to more efficient content organization and improved user experiences.<br /><br />Summary: <div>
arXiv:2509.15614v1 Announce Type: new 
Abstract: In the age of information overload, content management for online news articles relies on efficient summarization to enhance accessibility and user engagement. This article addresses the challenge of extractive text summarization by employing advanced machine learning techniques to generate concise and coherent summaries while preserving the original meaning. Using the Cornell Newsroom dataset, comprising 1.3 million article-summary pairs, we developed a pipeline leveraging BERT embeddings to transform textual data into numerical representations. By framing the task as a binary classification problem, we explored various models, including logistic regression, feed-forward neural networks, and long short-term memory (LSTM) networks. Our findings demonstrate that LSTM networks, with their ability to capture sequential dependencies, outperform baseline methods like Lede-3 and simpler models in F1 score and ROUGE-1 metrics. This study underscores the potential of automated summarization in improving content management systems for online news platforms, enabling more efficient content organization and enhanced user experiences.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information Geometry of Variational Bayes</title>
<link>https://arxiv.org/abs/2509.15641</link>
<guid>https://arxiv.org/abs/2509.15641</guid>
<content:encoded><![CDATA[
<div> natural gradients, variational Bayes, information geometry, machine learning, Bayesian Learning Rule <br />
Summary: <br />
The article discusses the fundamental connection between information geometry and variational Bayes (VB), highlighting that a VB solution requires estimation or computation of natural gradients. This connection has several consequences, as demonstrated through the Bayesian Learning Rule (BLR) algorithm. These consequences include simplification of Bayes' rule as the addition of natural gradients, a generalization of quadratic surrogates in gradient-based methods, and the potential for large-scale implementation of VB algorithms for large language models. By emphasizing the common origins of information geometry and Bayes, the article aims to encourage further exploration at the intersection of these two fields. <div>
arXiv:2509.15641v1 Announce Type: new 
Abstract: We highlight a fundamental connection between information geometry and variational Bayes (VB) and discuss its consequences for machine learning. Under certain conditions, a VB solution always requires estimation or computation of natural gradients. We show several consequences of this fact by using the natural-gradient descent algorithm of Khan and Rue (2023) called the Bayesian Learning Rule (BLR). These include (i) a simplification of Bayes' rule as addition of natural gradients, (ii) a generalization of quadratic surrogates used in gradient-based methods, and (iii) a large-scale implementation of VB algorithms for large language models. Neither the connection nor its consequences are new but we further emphasize the common origins of the two fields of information geometry and Bayes with a hope to facilitate more work at the intersection of the two fields.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Efficient Influence Function: Dropout as a Compression Tool</title>
<link>https://arxiv.org/abs/2509.15651</link>
<guid>https://arxiv.org/abs/2509.15651</guid>
<content:encoded><![CDATA[
<div> dropout, machine learning, influence function, gradient compression, large-scale models

Summary:
This article introduces a novel approach to efficiently compute the influence function by leveraging dropout as a gradient compression mechanism. Assessing the impact of training data on machine learning models is essential for understanding model behavior and enhancing transparency. The influence function quantifies the effect of training data points on a model's performance. However, the computational and memory costs of computing the influence function are significant, especially for large-scale models. The proposed method reduces computational and memory overhead during influence function computation and gradient compression. The approach preserves critical components of data influence and enables application to modern large-scale models. Through theoretical analysis and empirical validation, the method demonstrates efficiency and effectiveness in assessing the impact of training data on machine learning models. <div>
arXiv:2509.15651v1 Announce Type: new 
Abstract: Assessing the impact the training data on machine learning models is crucial for understanding the behavior of the model, enhancing the transparency, and selecting training data. Influence function provides a theoretical framework for quantifying the effect of training data points on model's performance given a specific test data. However, the computational and memory costs of influence function presents significant challenges, especially for large-scale models, even when using approximation methods, since the gradients involved in computation are as large as the model itself. In this work, we introduce a novel approach that leverages dropout as a gradient compression mechanism to compute the influence function more efficiently. Our method significantly reduces computational and memory overhead, not only during the influence function computation but also in gradient compression process. Through theoretical analysis and empirical validation, we demonstrate that our method could preserves critical components of the data influence and enables its application to modern large-scale models.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nonconvex Regularization for Feature Selection in Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.15652</link>
<guid>https://arxiv.org/abs/2509.15652</guid>
<content:encoded><![CDATA[
<div> Algorithm, feature selection, reinforcement learning, regularization, convergence

Summary:
The paper introduces a novel batch algorithm for feature selection in reinforcement learning (RL) with theoretical convergence guarantees. It addresses the estimation bias present in conventional regularization methods by incorporating a sparsity-inducing PMC penalty into the least-squares temporal-difference framework. The formulation is based on a Bellman-residual objective and can be interpreted as a nonmonotone-inclusion problem. The proposed forward-reflected-backward splitting (FRBS) algorithm is developed to solve this class of problems and convergence conditions are established. Experimental results on benchmark datasets show that the proposed approach outperforms existing methods, especially in scenarios with numerous noisy features. <div>
arXiv:2509.15652v1 Announce Type: new 
Abstract: This work proposes an efficient batch algorithm for feature selection in reinforcement learning (RL) with theoretical convergence guarantees. To mitigate the estimation bias inherent in conventional regularization schemes, the first contribution extends policy evaluation within the classical least-squares temporal-difference (LSTD) framework by formulating a Bellman-residual objective regularized with the sparsity-inducing, nonconvex projected minimax concave (PMC) penalty. Owing to the weak convexity of the PMC penalty, this formulation can be interpreted as a special instance of a general nonmonotone-inclusion problem. The second contribution establishes novel convergence conditions for the forward-reflected-backward splitting (FRBS) algorithm to solve this class of problems. Numerical experiments on benchmark datasets demonstrate that the proposed approach substantially outperforms state-of-the-art feature-selection methods, particularly in scenarios with many noisy features.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inference Offloading for Cost-Sensitive Binary Classification at the Edge</title>
<link>https://arxiv.org/abs/2509.15674</link>
<guid>https://arxiv.org/abs/2509.15674</guid>
<content:encoded><![CDATA[
<div> Keywords: binary classification, edge intelligence system, hierarchical inference, online learning framework, offloading costs<br />
Summary:<br />
This study addresses a binary classification problem in an edge intelligence system where false negatives are more detrimental than false positives. The system employs a compact local model supplemented by a larger remote model accessible via network with offloading costs. The research focuses on the trade-off between classification accuracy and offloading costs in a hierarchical inference system. An online learning framework continuously adjusts thresholds on the local model for classification decisions and sample offloading. A closed-form solution is presented for calibrated local models, while an online two-threshold hierarchical inference policy, H2T2, is proposed for uncalibrated models, demonstrating sublinear regret. H2T2 is model-agnostic, requiring no training and learns in the inference phase with limited feedback. Simulation results on real-world datasets showcase the superiority of H2T2 over naive and single-threshold hierarchical inference policies, even outperforming offline optima and exhibiting robustness to distribution shifts and mismatched classifiers.<br /><br />Summary: <div>
arXiv:2509.15674v1 Announce Type: new 
Abstract: We focus on a binary classification problem in an edge intelligence system where false negatives are more costly than false positives. The system has a compact, locally deployed model, which is supplemented by a larger, remote model, which is accessible via the network by incurring an offloading cost. For each sample, our system first uses the locally deployed model for inference. Based on the output of the local model, the sample may be offloaded to the remote model. This work aims to understand the fundamental trade-off between classification accuracy and these offloading costs within such a hierarchical inference (HI) system. To optimize this system, we propose an online learning framework that continuously adapts a pair of thresholds on the local model's confidence scores. These thresholds determine the prediction of the local model and whether a sample is classified locally or offloaded to the remote model. We present a closed-form solution for the setting where the local model is calibrated. For the more general case of uncalibrated models, we introduce H2T2, an online two-threshold hierarchical inference policy, and prove it achieves sublinear regret. H2T2 is model-agnostic, requires no training, and learns in the inference phase using limited feedback. Simulations on real-world datasets show that H2T2 consistently outperforms naive and single-threshold HI policies, sometimes even surpassing offline optima. The policy also demonstrates robustness to distribution shifts and adapts effectively to mismatched classifiers.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KITE: Kernelized and Information Theoretic Exemplars for In-Context Learning</title>
<link>https://arxiv.org/abs/2509.15676</link>
<guid>https://arxiv.org/abs/2509.15676</guid>
<content:encoded><![CDATA[
<div> surrogate objective, submodular, kernel trick, optimal design-based regularizer, example selection<br />
<br />
Summary: 
This article discusses the problem of example selection in in-context learning (ICL) for large language models (LLMs). The focus is on selecting the most relevant examples from a larger pool to optimize performance on a specific user query. The approach frames the selection task as a query-specific optimization problem and involves minimizing prediction error for a given query instance. A principled surrogate objective is derived, which is approximately submodular, allowing the use of a greedy algorithm with an approximation guarantee. Additionally, the method incorporates the kernel trick to operate in high-dimensional feature spaces and introduces an optimal design-based regularizer to encourage diversity in the selected examples. Empirical results show significant improvements over standard retrieval methods in various classification tasks, emphasizing the importance of structure-aware and diverse example selection for ICL in scenarios with limited labeled data. <br /><br /> <div>
arXiv:2509.15676v1 Announce Type: new 
Abstract: In-context learning (ICL) has emerged as a powerful paradigm for adapting large language models (LLMs) to new and data-scarce tasks using only a few carefully selected task-specific examples presented in the prompt. However, given the limited context size of LLMs, a fundamental question arises: Which examples should be selected to maximize performance on a given user query? While nearest-neighbor-based methods like KATE have been widely adopted for this purpose, they suffer from well-known drawbacks in high-dimensional embedding spaces, including poor generalization and a lack of diversity. In this work, we study this problem of example selection in ICL from a principled, information theory-driven perspective. We first model an LLM as a linear function over input embeddings and frame the example selection task as a query-specific optimization problem: selecting a subset of exemplars from a larger example bank that minimizes the prediction error on a specific query. This formulation departs from traditional generalization-focused learning theoretic approaches by targeting accurate prediction for a specific query instance. We derive a principled surrogate objective that is approximately submodular, enabling the use of a greedy algorithm with an approximation guarantee. We further enhance our method by (i) incorporating the kernel trick to operate in high-dimensional feature spaces without explicit mappings, and (ii) introducing an optimal design-based regularizer to encourage diversity in the selected examples. Empirically, we demonstrate significant improvements over standard retrieval methods across a suite of classification tasks, highlighting the benefits of structure-aware, diverse example selection for ICL in real-world, label-scarce scenarios.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RMT-KD: Random Matrix Theoretic Causal Knowledge Distillation</title>
<link>https://arxiv.org/abs/2509.15724</link>
<guid>https://arxiv.org/abs/2509.15724</guid>
<content:encoded><![CDATA[
<div> Random Matrix Theory, compression, deep learning, knowledge distillation, network size reduction

Summary:
RMT-KD is a compression method for large deep learning models like BERT and ResNet that leverages Random Matrix Theory (RMT) for knowledge distillation. It iteratively reduces network size by identifying informative directions through the spectral properties of hidden representations. Unlike traditional pruning methods, RMT-KD applies RMT-based causal reduction layer by layer with self-distillation to maintain stability and accuracy. The approach achieves up to 80% parameter reduction with just a 2% accuracy loss on datasets like GLUE, AG News, and CIFAR-10. This results in 2.8x faster inference and nearly halved power consumption, making RMT-KD a mathematically grounded approach to network distillation. <div>
arXiv:2509.15724v1 Announce Type: new 
Abstract: Large deep learning models such as BERT and ResNet achieve state-of-the-art performance but are costly to deploy at the edge due to their size and compute demands. We present RMT-KD, a compression method that leverages Random Matrix Theory (RMT) for knowledge distillation to iteratively reduce network size. Instead of pruning or heuristic rank selection, RMT-KD preserves only informative directions identified via the spectral properties of hidden representations. RMT-based causal reduction is applied layer by layer with self-distillation to maintain stability and accuracy. On GLUE, AG News, and CIFAR-10, RMT-KD achieves up to 80% parameter reduction with only 2% accuracy loss, delivering 2.8x faster inference and nearly halved power consumption. These results establish RMT-KD as a mathematically grounded approach to network distillation.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EigenTrack: Spectral Activation Feature Tracking for Hallucination and Out-of-Distribution Detection in LLMs and VLMs</title>
<link>https://arxiv.org/abs/2509.15735</link>
<guid>https://arxiv.org/abs/2509.15735</guid>
<content:encoded><![CDATA[
<div> EigenTrack, interpretable real-time detector, spectral geometry, hidden activations, large language models, hallucination, out-of-distribution errors<br />
<br />
Summary: EigenTrack is a new approach for detecting hallucination and out-of-distribution errors in large language models. It utilizes the spectral geometry of hidden activations to track temporal shifts in representation structure. By streaming covariance-spectrum statistics into a lightweight recurrent classifier, EigenTrack can identify potential errors before they manifest. Unlike traditional methods, EigenTrack only requires a single forward pass without resampling. It offers interpretable accuracy-latency trade-offs, preserves temporal context, and aggregates global signals for better error detection in real-time. <div>
arXiv:2509.15735v1 Announce Type: new 
Abstract: Large language models (LLMs) offer broad utility but remain prone to hallucination and out-of-distribution (OOD) errors. We propose EigenTrack, an interpretable real-time detector that uses the spectral geometry of hidden activations, a compact global signature of model dynamics. By streaming covariance-spectrum statistics such as entropy, eigenvalue gaps, and KL divergence from random baselines into a lightweight recurrent classifier, EigenTrack tracks temporal shifts in representation structure that signal hallucination and OOD drift before surface errors appear. Unlike black- and grey-box methods, it needs only a single forward pass without resampling. Unlike existing white-box detectors, it preserves temporal context, aggregates global signals, and offers interpretable accuracy-latency trade-offs.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aircraft Fuel Flow Modelling with Ageing Effects: From Parametric Corrections to Neural Networks</title>
<link>https://arxiv.org/abs/2509.15736</link>
<guid>https://arxiv.org/abs/2509.15736</guid>
<content:encoded><![CDATA[
arXiv:2509.15736v1 Announce Type: new 
Abstract: Accurate modelling of aircraft fuel-flow is crucial for both operational planning and environmental impact assessment, yet standard parametric models often neglect performance deterioration that occurs as aircraft age. This paper investigates multiple approaches to integrate engine ageing effects into fuel-flow prediction for the Airbus A320-214, using a comprehensive dataset of approximately nineteen thousand Quick Access Recorder flights from nine distinct airframes with varying years in service. We systematically evaluate classical physics-based models, empirical correction coefficients, and data-driven neural network architectures that incorporate age either as an input feature or as an explicit multiplicative bias. Results demonstrate that while baseline models consistently underestimate fuel consumption for older aircraft, the use of age-dependent correction factors and neural models substantially reduces bias and improves prediction accuracy. Nevertheless, limitations arise from the small number of airframes and the lack of detailed maintenance event records, which constrain the representativeness and generalization of age-based corrections. This study emphasizes the importance of accounting for the effects of ageing in parametric and machine learning frameworks to improve the reliability of operational and environmental assessments. The study also highlights the need for more diverse datasets that can capture the complexity of real-world engine deterioration.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GUI-ReWalk: Massive Data Generation for GUI Agent via Stochastic Exploration and Intent-Aware Reasoning</title>
<link>https://arxiv.org/abs/2509.15738</link>
<guid>https://arxiv.org/abs/2509.15738</guid>
<content:encoded><![CDATA[
arXiv:2509.15738v1 Announce Type: new 
Abstract: Graphical User Interface (GUI) Agents, powered by large language and vision-language models, hold promise for enabling end-to-end automation in digital environments. However, their progress is fundamentally constrained by the scarcity of scalable, high-quality trajectory data. Existing data collection strategies either rely on costly and inconsistent manual annotations or on synthetic generation methods that trade off between diversity and meaningful task coverage. To bridge this gap, we present GUI-ReWalk: a reasoning-enhanced, multi-stage framework for synthesizing realistic and diverse GUI trajectories. GUI-ReWalk begins with a stochastic exploration phase that emulates human trial-and-error behaviors, and progressively transitions into a reasoning-guided phase where inferred goals drive coherent and purposeful interactions. Moreover, it supports multi-stride task generation, enabling the construction of long-horizon workflows across multiple applications. By combining randomness for diversity with goal-aware reasoning for structure, GUI-ReWalk produces data that better reflects the intent-aware, adaptive nature of human-computer interaction. We further train Qwen2.5-VL-7B on the GUI-ReWalk dataset and evaluate it across multiple benchmarks, including Screenspot-Pro, OSWorld-G, UI-Vision, AndroidControl, and GUI-Odyssey. Results demonstrate that GUI-ReWalk enables superior coverage of diverse interaction flows, higher trajectory entropy, and more realistic user intent. These findings establish GUI-ReWalk as a scalable and data-efficient framework for advancing GUI agent research and enabling robust real-world automation.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incremental Multistep Forecasting of Battery Degradation Using Pseudo Targets</title>
<link>https://arxiv.org/abs/2509.15740</link>
<guid>https://arxiv.org/abs/2509.15740</guid>
<content:encoded><![CDATA[
arXiv:2509.15740v1 Announce Type: new 
Abstract: Data-driven models accurately perform early battery prognosis to prevent equipment failure and further safety hazards. Most existing machine learning (ML) models work in offline mode which must consider their retraining post-deployment every time new data distribution is encountered. Hence, there is a need for an online ML approach where the model can adapt to varying distributions. However, existing online incremental multistep forecasts are a great challenge as there is no way to correct the model of its forecasts at the current instance. Also, these methods need to wait for a considerable amount of time to acquire enough streaming data before retraining. In this study, we propose iFSNet (incremental Fast and Slow learning Network) which is a modified version of FSNet for a single-pass mode (sample-by-sample) to achieve multistep forecasting using pseudo targets. It uses a simple linear regressor of the input sequence to extrapolate pseudo future samples (pseudo targets) and calculate the loss from the rest of the forecast and keep updating the model. The model benefits from the associative memory and adaptive structure mechanisms of FSNet, at the same time the model incrementally improves by using pseudo targets. The proposed model achieved 0.00197 RMSE and 0.00154 MAE on datasets with smooth degradation trajectories while it achieved 0.01588 RMSE and 0.01234 MAE on datasets having irregular degradation trajectories with capacity regeneration spikes.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Optimal Steering to Achieve Exact Fairness</title>
<link>https://arxiv.org/abs/2509.15759</link>
<guid>https://arxiv.org/abs/2509.15759</guid>
<content:encoded><![CDATA[
arXiv:2509.15759v1 Announce Type: new 
Abstract: To fix the 'bias in, bias out' problem in fair machine learning, it is important to steer feature distributions of data or internal representations of Large Language Models (LLMs) to ideal ones that guarantee group-fair outcomes. Previous work on fair generative models and representation steering could greatly benefit from provable fairness guarantees on the model output. We define a distribution as ideal if the minimizer of any cost-sensitive risk on it is guaranteed to have exact group-fair outcomes (e.g., demographic parity, equal opportunity)-in other words, it has no fairness-utility trade-off. We formulate an optimization program for optimal steering by finding the nearest ideal distribution in KL-divergence, and provide efficient algorithms for it when the underlying distributions come from well-known parametric families (e.g., normal, log-normal). Empirically, our optimal steering techniques on both synthetic and real-world datasets improve fairness without diminishing utility (and sometimes even improve utility). We demonstrate affine steering of LLM representations to reduce bias in multi-class classification, e.g., occupation prediction from a short biography in Bios dataset (De-Arteaga et al.). Furthermore, we steer internal representations of LLMs towards desired outputs so that it works equally well across different groups.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Optimize Capacity Planning in Semiconductor Manufacturing</title>
<link>https://arxiv.org/abs/2509.15767</link>
<guid>https://arxiv.org/abs/2509.15767</guid>
<content:encoded><![CDATA[
arXiv:2509.15767v1 Announce Type: new 
Abstract: In manufacturing, capacity planning is the process of allocating production resources in accordance with variable demand. The current industry practice in semiconductor manufacturing typically applies heuristic rules to prioritize actions, such as future change lists that account for incoming machine and recipe dedications. However, while offering interpretability, heuristics cannot easily account for the complex interactions along the process flow that can gradually lead to the formation of bottlenecks. Here, we present a neural network-based model for capacity planning on the level of individual machines, trained using deep reinforcement learning. By representing the policy using a heterogeneous graph neural network, the model directly captures the diverse relationships among machines and processing steps, allowing for proactive decision-making. We describe several measures taken to achieve sufficient scalability to tackle the vast space of possible machine-level actions.
  Our evaluation results cover Intel's small-scale Minifab model and preliminary experiments using the popular SMT2020 testbed. In the largest tested scenario, our trained policy increases throughput and decreases cycle time by about 1.8% each.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalization and Optimization of SGD with Lookahead</title>
<link>https://arxiv.org/abs/2509.15776</link>
<guid>https://arxiv.org/abs/2509.15776</guid>
<content:encoded><![CDATA[
arXiv:2509.15776v1 Announce Type: new 
Abstract: The Lookahead optimizer enhances deep learning models by employing a dual-weight update mechanism, which has been shown to improve the performance of underlying optimizers such as SGD. However, most theoretical studies focus on its convergence on training data, leaving its generalization capabilities less understood. Existing generalization analyses are often limited by restrictive assumptions, such as requiring the loss function to be globally Lipschitz continuous, and their bounds do not fully capture the relationship between optimization and generalization. In this paper, we address these issues by conducting a rigorous stability and generalization analysis of the Lookahead optimizer with minibatch SGD. We leverage on-average model stability to derive generalization bounds for both convex and strongly convex problems without the restrictive Lipschitzness assumption. Our analysis demonstrates a linear speedup with respect to the batch size in the convex setting.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Monte Carlo Tree Diffusion with Multiple Experts for Protein Design</title>
<link>https://arxiv.org/abs/2509.15796</link>
<guid>https://arxiv.org/abs/2509.15796</guid>
<content:encoded><![CDATA[
arXiv:2509.15796v1 Announce Type: new 
Abstract: The goal of protein design is to generate amino acid sequences that fold into functional structures with desired properties. Prior methods combining autoregressive language models with Monte Carlo Tree Search (MCTS) struggle with long-range dependencies and suffer from an impractically large search space. We propose MCTD-ME, Monte Carlo Tree Diffusion with Multiple Experts, which integrates masked diffusion models with tree search to enable multi-token planning and efficient exploration. Unlike autoregressive planners, MCTD-ME uses biophysical-fidelity-enhanced diffusion denoising as the rollout engine, jointly revising multiple positions and scaling to large sequence spaces. It further leverages experts of varying capacities to enrich exploration, guided by a pLDDT-based masking schedule that targets low-confidence regions while preserving reliable residues. We propose a novel multi-expert selection rule (PH-UCT-ME) extends predictive-entropy UCT to expert ensembles. On the inverse folding task (CAMEO and PDB benchmarks), MCTD-ME outperforms single-expert and unguided baselines in both sequence recovery (AAR) and structural similarity (scTM), with gains increasing for longer proteins and benefiting from multi-expert guidance. More generally, the framework is model-agnostic and applicable beyond inverse folding, including de novo protein engineering and multi-objective molecular generation.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instance Generation for Meta-Black-Box Optimization through Latent Space Reverse Engineering</title>
<link>https://arxiv.org/abs/2509.15810</link>
<guid>https://arxiv.org/abs/2509.15810</guid>
<content:encoded><![CDATA[
arXiv:2509.15810v1 Announce Type: new 
Abstract: To relieve intensive human-expertise required to design optimization algorithms, recent Meta-Black-Box Optimization (MetaBBO) researches leverage generalization strength of meta-learning to train neural network-based algorithm design policies over a predefined training problem set, which automates the adaptability of the low-level optimizers on unseen problem instances. Currently, a common training problem set choice in existing MetaBBOs is well-known benchmark suites CoCo-BBOB. Although such choice facilitates the MetaBBO's development, problem instances in CoCo-BBOB are more or less limited in diversity, raising the risk of overfitting of MetaBBOs, which might further results in poor generalization. In this paper, we propose an instance generation approach, termed as \textbf{LSRE}, which could generate diverse training problem instances for MetaBBOs to learn more generalizable policies. LSRE first trains an autoencoder which maps high-dimensional problem features into a 2-dimensional latent space. Uniform-grid sampling in this latent space leads to hidden representations of problem instances with sufficient diversity. By leveraging a genetic-programming approach to search function formulas with minimal L2-distance to these hidden representations, LSRE reverse engineers a diversified problem set, termed as \textbf{Diverse-BBO}. We validate the effectiveness of LSRE by training various MetaBBOs on Diverse-BBO and observe their generalization performances on either synthetic or realistic scenarios. Extensive experimental results underscore the superiority of Diverse-BBO to existing training set choices in MetaBBOs. Further ablation studies not only demonstrate the effectiveness of design choices in LSRE, but also reveal interesting insights on instance diversity and MetaBBO's generalization.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ThermalGuardian: Temperature-Aware Testing of Automotive Deep Learning Frameworks</title>
<link>https://arxiv.org/abs/2509.15815</link>
<guid>https://arxiv.org/abs/2509.15815</guid>
<content:encoded><![CDATA[
arXiv:2509.15815v1 Announce Type: new 
Abstract: Deep learning models play a vital role in autonomous driving systems, supporting critical functions such as environmental perception. To accelerate model inference, these deep learning models' deployment relies on automotive deep learning frameworks, for example, PaddleInference in Apollo and TensorRT in AutoWare. However, unlike deploying deep learning models on the cloud, vehicular environments experience extreme ambient temperatures varying from -40{\deg}C to 50{\deg}C, significantly impacting GPU temperature. Additionally, heats generated when computing further lead to the GPU temperature increase. These temperature fluctuations lead to dynamic GPU frequency adjustments through mechanisms such as DVFS. However, automotive deep learning frameworks are designed without considering the impact of temperature-induced frequency variations. When deployed on temperature-varying GPUs, these frameworks suffer critical quality issues: compute-intensive operators face delays or errors, high/mixed-precision operators suffer from precision errors, and time-series operators suffer from synchronization issues. The above quality issues cannot be detected by existing deep learning framework testing methods because they ignore temperature's effect on the deep learning framework quality. To bridge this gap, we propose ThermalGuardian, the first automotive deep learning framework testing method under temperature-varying environments. Specifically, ThermalGuardian generates test input models using model mutation rules targeting temperature-sensitive operators, simulates GPU temperature fluctuations based on Newton's law of cooling, and controls GPU frequency based on real-time GPU temperature.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Convergence of Muon and Beyond</title>
<link>https://arxiv.org/abs/2509.15816</link>
<guid>https://arxiv.org/abs/2509.15816</guid>
<content:encoded><![CDATA[
arXiv:2509.15816v1 Announce Type: new 
Abstract: The Muon optimizer has demonstrated remarkable empirical success in handling matrix-structured parameters for training neural networks. However, a significant gap persists between its practical performance and theoretical understanding. Existing analyses indicate that the standard Muon variant achieves only a suboptimal convergence rate of $\mathcal{O}(T^{-1/4})$ in stochastic non-convex settings, where $T$ denotes the number of iterations. To explore the theoretical limits of the Muon framework, we construct and analyze a variance-reduced variant, termed Muon-VR2. We provide the first rigorous proof that incorporating a variance-reduction mechanism enables Muon-VR2 to attain an optimal convergence rate of $\tilde{\mathcal{O}}(T^{-1/3})$, thereby matching the theoretical lower bound for this class of problems. Moreover, our analysis establishes convergence guarantees for Muon variants under the Polyak-{\L}ojasiewicz (P{\L}) condition. Extensive experiments on vision (CIFAR-10) and language (C4) benchmarks corroborate our theoretical findings on per-iteration convergence. Overall, this work provides the first proof of optimality for a Muon-style optimizer and clarifies the path toward developing more practically efficient, accelerated variants.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SolarCrossFormer: Improving day-ahead Solar Irradiance Forecasting by Integrating Satellite Imagery and Ground Sensors</title>
<link>https://arxiv.org/abs/2509.15827</link>
<guid>https://arxiv.org/abs/2509.15827</guid>
<content:encoded><![CDATA[
arXiv:2509.15827v1 Announce Type: new 
Abstract: Accurate day-ahead forecasts of solar irradiance are required for the large-scale integration of solar photovoltaic (PV) systems into the power grid. However, current forecasting solutions lack the temporal and spatial resolution required by system operators. In this paper, we introduce SolarCrossFormer, a novel deep learning model for day-ahead irradiance forecasting, that combines satellite images and time series from a ground-based network of meteorological stations. SolarCrossFormer uses novel graph neural networks to exploit the inter- and intra-modal correlations of the input data and improve the accuracy and resolution of the forecasts. It generates probabilistic forecasts for any location in Switzerland with a 15-minute resolution for horizons up to 24 hours ahead. One of the key advantages of SolarCrossFormer its robustness in real life operations. It can incorporate new time-series data without retraining the model and, additionally, it can produce forecasts for locations without input data by using only their coordinates. Experimental results over a dataset of one year and 127 locations across Switzerland show that SolarCrossFormer yield a normalized mean absolute error of 6.1 % over the forecasting horizon. The results are competitive with those achieved by a commercial numerical weather prediction service.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HyP-ASO: A Hybrid Policy-based Adaptive Search Optimization Framework for Large-Scale Integer Linear Programs</title>
<link>https://arxiv.org/abs/2509.15828</link>
<guid>https://arxiv.org/abs/2509.15828</guid>
<content:encoded><![CDATA[
arXiv:2509.15828v1 Announce Type: new 
Abstract: Directly solving large-scale Integer Linear Programs (ILPs) using traditional solvers is slow due to their NP-hard nature. While recent frameworks based on Large Neighborhood Search (LNS) can accelerate the solving process, their performance is often constrained by the difficulty in generating sufficiently effective neighborhoods. To address this challenge, we propose HyP-ASO, a hybrid policy-based adaptive search optimization framework that combines a customized formula with deep Reinforcement Learning (RL). The formula leverages feasible solutions to calculate the selection probabilities for each variable in the neighborhood generation process, and the RL policy network predicts the neighborhood size. Extensive experiments demonstrate that HyP-ASO significantly outperforms existing LNS-based approaches for large-scale ILPs. Additional experiments show it is lightweight and highly scalable, making it well-suited for solving large-scale ILPs.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tsururu: A Python-based Time Series Forecasting Strategies Library</title>
<link>https://arxiv.org/abs/2509.15843</link>
<guid>https://arxiv.org/abs/2509.15843</guid>
<content:encoded><![CDATA[
arXiv:2509.15843v1 Announce Type: new 
Abstract: While current time series research focuses on developing new models, crucial questions of selecting an optimal approach for training such models are underexplored. Tsururu, a Python library introduced in this paper, bridges SoTA research and industry by enabling flexible combinations of global and multivariate approaches and multi-step-ahead forecasting strategies. It also enables seamless integration with various forecasting models. Available at https://github.com/sb-ai-lab/tsururu .
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedHK-MVFC: Federated Heat Kernel Multi-View Clustering</title>
<link>https://arxiv.org/abs/2509.15844</link>
<guid>https://arxiv.org/abs/2509.15844</guid>
<content:encoded><![CDATA[
arXiv:2509.15844v1 Announce Type: new 
Abstract: In the realm of distributed AI and privacy-focused medical applications, we propose a framework for multi-view clustering that links quantum field theory with federated healthcare analytics. Our method uses heat-kernel coefficients from spectral analysis to convert Euclidean distances into geometry-aware similarity measures, capturing the structure of diverse medical data. We lay this out through the Heat Kernel Distance (HKD) transformation with convergence guarantees. Two algorithms are developed: Heat Kernel-Enhanced Multi-View Fuzzy Clustering (HK-MVFC) for central analysis, and Federated Heat Kernel Multi-View Fuzzy Clustering (FedHK-MVFC) for secure, privacy-preserving learning across hospitals using differential privacy and secure aggregation to facilitate HIPAA-compliant collaboration. Tests on synthetic datasets of cardiovascular patients show an $8-12 \%$ increase in clustering accuracy, $70 \%$ reduced communication, and $98.2 \%$ efficiency retention over centralized methods. Validated on 10,000 patient records across two hospitals, it proves useful for collaborative phenotyping involving ECG, cardiac imaging, and behavioral data. Our theoretical contributions include update rules with proven convergence, adaptive view weighting, and privacy-preserving protocols. This presents a new standard for geometry-aware federated learning in healthcare, turning advanced math into workable solutions for analyzing sensitive medical data while ensuring both rigor and clinical relevance.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvoBrain: Dynamic Multi-channel EEG Graph Modeling for Time-evolving Brain Network</title>
<link>https://arxiv.org/abs/2509.15857</link>
<guid>https://arxiv.org/abs/2509.15857</guid>
<content:encoded><![CDATA[
arXiv:2509.15857v1 Announce Type: new 
Abstract: Dynamic GNNs, which integrate temporal and spatial features in Electroencephalography (EEG) data, have shown great potential in automating seizure detection. However, fully capturing the underlying dynamics necessary to represent brain states, such as seizure and non-seizure, remains a non-trivial task and presents two fundamental challenges. First, most existing dynamic GNN methods are built on temporally fixed static graphs, which fail to reflect the evolving nature of brain connectivity during seizure progression. Second, current efforts to jointly model temporal signals and graph structures and, more importantly, their interactions remain nascent, often resulting in inconsistent performance. To address these challenges, we present the first theoretical analysis of these two problems, demonstrating the effectiveness and necessity of explicit dynamic modeling and time-then-graph dynamic GNN method. Building on these insights, we propose EvoBrain, a novel seizure detection model that integrates a two-stream Mamba architecture with a GCN enhanced by Laplacian Positional Encoding, following neurological insights. Moreover, EvoBrain incorporates explicitly dynamic graph structures, allowing both nodes and edges to evolve over time. Our contributions include (a) a theoretical analysis proving the expressivity advantage of explicit dynamic modeling and time-then-graph over other approaches, (b) a novel and efficient model that significantly improves AUROC by 23% and F1 score by 30%, compared with the dynamic GNN baseline, and (c) broad evaluations of our method on the challenging early seizure prediction tasks.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data</title>
<link>https://arxiv.org/abs/2509.15859</link>
<guid>https://arxiv.org/abs/2509.15859</guid>
<content:encoded><![CDATA[
arXiv:2509.15859v1 Announce Type: new 
Abstract: Imbalanced classification datasets pose significant challenges in machine learning, often leading to biased models that perform poorly on underrepresented classes. With the rise of foundation models, recent research has focused on the full, partial, and parameter-efficient fine-tuning of these models to deal with long-tail classification. Despite the impressive performance of these works on the benchmark datasets, they still fail to close the gap with the networks trained using the balanced datasets and still require substantial computational resources, even for relatively smaller datasets. Underscoring the importance of computational efficiency and simplicity, in this work we propose a novel framework that leverages the rich semantic latent space of Vision Foundation Models to generate synthetic data and train a simple linear classifier using a mixture of real and synthetic data for long-tail classification. The computational efficiency gain arises from the number of trainable parameters that are reduced to just the number of parameters in the linear model. Our method sets a new state-of-the-art for the CIFAR-100-LT benchmark and demonstrates strong performance on the Places-LT benchmark, highlighting the effectiveness and adaptability of our simple and effective approach.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToFU: Transforming How Federated Learning Systems Forget User Data</title>
<link>https://arxiv.org/abs/2509.15861</link>
<guid>https://arxiv.org/abs/2509.15861</guid>
<content:encoded><![CDATA[
arXiv:2509.15861v1 Announce Type: new 
Abstract: Neural networks unintentionally memorize training data, creating privacy risks in federated learning (FL) systems, such as inference and reconstruction attacks on sensitive data. To mitigate these risks and to comply with privacy regulations, Federated Unlearning (FU) has been introduced to enable participants in FL systems to remove their data's influence from the global model. However, current FU methods primarily act post-hoc, struggling to efficiently erase information deeply memorized by neural networks. We argue that effective unlearning necessitates a paradigm shift: designing FL systems inherently amenable to forgetting. To this end, we propose a learning-to-unlearn Transformation-guided Federated Unlearning (ToFU) framework that incorporates transformations during the learning process to reduce memorization of specific instances. Our theoretical analysis reveals how transformation composition provably bounds instance-specific information, directly simplifying subsequent unlearning. Crucially, ToFU can work as a plug-and-play framework that improves the performance of existing FU methods. Experiments on CIFAR-10, CIFAR-100, and the MUFAC benchmark show that ToFU outperforms existing FU baselines, enhances performance when integrated with current methods, and reduces unlearning time.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAGE: Semantic-Aware Shared Sampling for Efficient Diffusion</title>
<link>https://arxiv.org/abs/2509.15865</link>
<guid>https://arxiv.org/abs/2509.15865</guid>
<content:encoded><![CDATA[
arXiv:2509.15865v1 Announce Type: new 
Abstract: Diffusion models manifest evident benefits across diverse domains, yet their high sampling cost, requiring dozens of sequential model evaluations, remains a major limitation. Prior efforts mainly accelerate sampling via optimized solvers or distillation, which treat each query independently. In contrast, we reduce total number of steps by sharing early-stage sampling across semantically similar queries. To enable such efficiency gains without sacrificing quality, we propose SAGE, a semantic-aware shared sampling framework that integrates a shared sampling scheme for efficiency and a tailored training strategy for quality preservation. Extensive experiments show that SAGE reduces sampling cost by 25.5%, while improving generation quality with 5.0% lower FID, 5.4% higher CLIP, and 160% higher diversity over baselines.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Data to Diagnosis: A Large, Comprehensive Bone Marrow Dataset and AI Methods for Childhood Leukemia Prediction</title>
<link>https://arxiv.org/abs/2509.15895</link>
<guid>https://arxiv.org/abs/2509.15895</guid>
<content:encoded><![CDATA[
arXiv:2509.15895v1 Announce Type: new 
Abstract: Leukemia diagnosis primarily relies on manual microscopic analysis of bone marrow morphology supported by additional laboratory parameters, making it complex and time consuming. While artificial intelligence (AI) solutions have been proposed, most utilize private datasets and only cover parts of the diagnostic pipeline. Therefore, we present a large, high-quality, publicly available leukemia bone marrow dataset spanning the entire diagnostic process, from cell detection to diagnosis. Using this dataset, we further propose methods for cell detection, cell classification, and diagnosis prediction. The dataset comprises 246 pediatric patients with diagnostic, clinical and laboratory information, over 40 000 cells with bounding box annotations and more than 28 000 of these with high-quality class labels, making it the most comprehensive dataset publicly available. Evaluation of the AI models yielded an average precision of 0.96 for the cell detection, an area under the curve of 0.98, and an F1-score of 0.61 for the 33-class cell classification, and a mean F1-score of 0.90 for the diagnosis prediction using predicted cell counts. While the proposed approaches demonstrate their usefulness for AI-assisted diagnostics, the dataset will foster further research and development in the field, ultimately contributing to more precise diagnoses and improved patient outcomes.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundation Models as World Models: A Foundational Study in Text-Based GridWorlds</title>
<link>https://arxiv.org/abs/2509.15915</link>
<guid>https://arxiv.org/abs/2509.15915</guid>
<content:encoded><![CDATA[
arXiv:2509.15915v1 Announce Type: new 
Abstract: While reinforcement learning from scratch has shown impressive results in solving sequential decision-making tasks with efficient simulators, real-world applications with expensive interactions require more sample-efficient agents. Foundation models (FMs) are natural candidates to improve sample efficiency as they possess broad knowledge and reasoning capabilities, but it is yet unclear how to effectively integrate them into the reinforcement learning framework. In this paper, we anticipate and, most importantly, evaluate two promising strategies. First, we consider the use of foundation world models (FWMs) that exploit the prior knowledge of FMs to enable training and evaluating agents with simulated interactions. Second, we consider the use of foundation agents (FAs) that exploit the reasoning capabilities of FMs for decision-making. We evaluate both approaches empirically in a family of grid-world environments that are suitable for the current generation of large language models (LLMs). Our results suggest that improvements in LLMs already translate into better FWMs and FAs; that FAs based on current LLMs can already provide excellent policies for sufficiently simple environments; and that the coupling of FWMs and reinforcement learning agents is highly promising for more complex settings with partial observability and stochastic elements.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Generative Auto-bidding with Offline Reward Evaluation and Policy Search</title>
<link>https://arxiv.org/abs/2509.15927</link>
<guid>https://arxiv.org/abs/2509.15927</guid>
<content:encoded><![CDATA[
arXiv:2509.15927v1 Announce Type: new 
Abstract: Auto-bidding is an essential tool for advertisers to enhance their advertising performance. Recent progress has shown that AI-Generated Bidding (AIGB), which formulates the auto-bidding as a trajectory generation task and trains a conditional diffusion-based planner on offline data, achieves superior and stable performance compared to typical offline reinforcement learning (RL)-based auto-bidding methods. However, existing AIGB methods still encounter a performance bottleneck due to their neglect of fine-grained generation quality evaluation and inability to explore beyond static datasets. To address this, we propose AIGB-Pearl (\emph{Planning with EvAluator via RL}), a novel method that integrates generative planning and policy optimization. The key to AIGB-Pearl is to construct a non-bootstrapped \emph{trajectory evaluator} to assign rewards and guide policy search, enabling the planner to optimize its generation quality iteratively through interaction. Furthermore, to enhance trajectory evaluator accuracy in offline settings, we incorporate three key techniques: (i) a Large Language Model (LLM)-based architecture for better representational capacity, (ii) hybrid point-wise and pair-wise losses for better score learning, and (iii) adaptive integration of expert feedback for better generalization ability. Extensive experiments on both simulated and real-world advertising systems demonstrate the state-of-the-art performance of our approach.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Monte Carlo Tree Search for Symbolic Regression</title>
<link>https://arxiv.org/abs/2509.15929</link>
<guid>https://arxiv.org/abs/2509.15929</guid>
<content:encoded><![CDATA[
arXiv:2509.15929v1 Announce Type: new 
Abstract: Symbolic regression aims to discover concise, interpretable mathematical expressions that satisfy desired objectives, such as fitting data, posing a highly combinatorial optimization problem. While genetic programming has been the dominant approach, recent efforts have explored reinforcement learning methods for improving search efficiency. Monte Carlo Tree Search (MCTS), with its ability to balance exploration and exploitation through guided search, has emerged as a promising technique for symbolic expression discovery. However, its traditional bandit strategies and sequential symbol construction often limit performance. In this work, we propose an improved MCTS framework for symbolic regression that addresses these limitations through two key innovations: (1) an extreme bandit allocation strategy tailored for identifying globally optimal expressions, with finite-time performance guarantees under polynomial reward decay assumptions; and (2) evolution-inspired state-jumping actions such as mutation and crossover, which enable non-local transitions to promising regions of the search space. These state-jumping actions also reshape the reward landscape during the search process, improving both robustness and efficiency. We conduct a thorough numerical study to the impact of these improvements and benchmark our approach against existing symbolic regression methods on a variety of datasets, including both ground-truth and black-box datasets. Our approach achieves competitive performance with state-of-the-art libraries in terms of recovery rate, attains favorable positions on the Pareto frontier of accuracy versus model complexity. Code is available at https://github.com/PKU-CMEGroup/MCTS-4-SR.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Alignment Bottleneck</title>
<link>https://arxiv.org/abs/2509.15932</link>
<guid>https://arxiv.org/abs/2509.15932</guid>
<content:encoded><![CDATA[
arXiv:2509.15932v1 Announce Type: new 
Abstract: Large language models improve with scale, yet feedback-based alignment still exhibits systematic deviations from intended behavior. Motivated by bounded rationality in economics and cognitive science, we view judgment as resource-limited and feedback as a constrained channel. On this basis, we model the loop as a two-stage cascade $U \to H \to Y$ given $S$, with cognitive capacity $C_{\text{cog}|S}$ and average total capacity $\bar{C}_{\text{tot}|S}$. Our main result is a capacity-coupled Alignment Performance Interval. It pairs a data size-independent Fano lower bound proved on a separable codebook mixture with a PAC-Bayes upper bound whose KL term is controlled by the same channel via $m \, \bar{C}_{\text{tot}|S}$. The PAC-Bayes bound becomes an upper bound on the same true risk when the canonical observable loss is used and the dataset is drawn from the same mixture. Under these matched conditions, both limits are governed by a single capacity. Consequences include that, with value complexity and capacity fixed, adding labels alone cannot cross the bound; attaining lower risk on more complex targets requires capacity that grows with $\log M$; and once useful signal saturates capacity, further optimization tends to fit channel regularities, consistent with reports of sycophancy and reward hacking. The analysis views alignment as interface engineering: measure and allocate limited capacity, manage task complexity, and decide where information is spent.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Physics Informed Neural Networks for Reliable Transformer Prognostics</title>
<link>https://arxiv.org/abs/2509.15933</link>
<guid>https://arxiv.org/abs/2509.15933</guid>
<content:encoded><![CDATA[
arXiv:2509.15933v1 Announce Type: new 
Abstract: Scientific Machine Learning (SciML) integrates physics and data into the learning process, offering improved generalization compared with purely data-driven models. Despite its potential, applications of SciML in prognostics remain limited, partly due to the complexity of incorporating partial differential equations (PDEs) for ageing physics and the scarcity of robust uncertainty quantification methods. This work introduces a Bayesian Physics-Informed Neural Network (B-PINN) framework for probabilistic prognostics estimation. By embedding Bayesian Neural Networks into the PINN architecture, the proposed approach produces principled, uncertainty-aware predictions. The method is applied to a transformer ageing case study, where insulation degradation is primarily driven by thermal stress. The heat diffusion PDE is used as the physical residual, and different prior distributions are investigated to examine their impact on predictive posterior distributions and their ability to encode a priori physical knowledge. The framework is validated against a finite element model developed and tested with real measurements from a solar power plant. Results, benchmarked against a dropout-PINN baseline, show that the proposed B-PINN delivers more reliable prognostic predictions by accurately quantifying predictive uncertainty. This capability is crucial for supporting robust and informed maintenance decision-making in critical power assets.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniTac2Pose: A Unified Approach Learned in Simulation for Category-level Visuotactile In-hand Pose Estimation</title>
<link>https://arxiv.org/abs/2509.15934</link>
<guid>https://arxiv.org/abs/2509.15934</guid>
<content:encoded><![CDATA[
arXiv:2509.15934v1 Announce Type: new 
Abstract: Accurate estimation of the in-hand pose of an object based on its CAD model is crucial in both industrial applications and everyday tasks, ranging from positioning workpieces and assembling components to seamlessly inserting devices like USB connectors. While existing methods often rely on regression, feature matching, or registration techniques, achieving high precision and generalizability to unseen CAD models remains a significant challenge. In this paper, we propose a novel three-stage framework for in-hand pose estimation. The first stage involves sampling and pre-ranking pose candidates, followed by iterative refinement of these candidates in the second stage. In the final stage, post-ranking is applied to identify the most likely pose candidates. These stages are governed by a unified energy-based diffusion model, which is trained solely on simulated data. This energy model simultaneously generates gradients to refine pose estimates and produces an energy scalar that quantifies the quality of the pose estimates. Additionally, borrowing the idea from the computer vision domain, we incorporate a render-compare architecture within the energy-based score network to significantly enhance sim-to-real performance, as demonstrated by our ablation studies. We conduct comprehensive experiments to show that our method outperforms conventional baselines based on regression, matching, and registration techniques, while also exhibiting strong intra-category generalization to previously unseen CAD models. Moreover, our approach integrates tactile object pose estimation, pose tracking, and uncertainty estimation into a unified framework, enabling robust performance across a variety of real-world conditions.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Targeted Fine-Tuning of DNN-Based Receivers via Influence Functions</title>
<link>https://arxiv.org/abs/2509.15950</link>
<guid>https://arxiv.org/abs/2509.15950</guid>
<content:encoded><![CDATA[
arXiv:2509.15950v1 Announce Type: new 
Abstract: We present the first use of influence functions for deep learning-based wireless receivers. Applied to DeepRx, a fully convolutional receiver, influence analysis reveals which training samples drive bit predictions, enabling targeted fine-tuning of poorly performing cases. We show that loss-relative influence with capacity-like binary cross-entropy loss and first-order updates on beneficial samples most consistently improves bit error rate toward genie-aided performance, outperforming random fine-tuning in single-target scenarios. Multi-target adaptation proved less effective, underscoring open challenges. Beyond experiments, we connect influence to self-influence corrections and propose a second-order, influence-aligned update strategy. Our results establish influence functions as both an interpretability tool and a basis for efficient receiver adaptation.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Graph Fusion for Incomplete Multi-view Semi-supervised Learning with Tensorial Imputation</title>
<link>https://arxiv.org/abs/2509.15955</link>
<guid>https://arxiv.org/abs/2509.15955</guid>
<content:encoded><![CDATA[
arXiv:2509.15955v1 Announce Type: new 
Abstract: View missing remains a significant challenge in graph-based multi-view semi-supervised learning, hindering their real-world applications. To address this issue, traditional methods introduce a missing indicator matrix and focus on mining partial structure among existing samples in each view for label propagation (LP). However, we argue that these disregarded missing samples sometimes induce discontinuous local structures, i.e., sub-clusters, breaking the fundamental smoothness assumption in LP. Consequently, such a Sub-Cluster Problem (SCP) would distort graph fusion and degrade classification performance. To alleviate SCP, we propose a novel incomplete multi-view semi-supervised learning method, termed AGF-TI. Firstly, we design an adversarial graph fusion scheme to learn a robust consensus graph against the distorted local structure through a min-max framework. By stacking all similarity matrices into a tensor, we further recover the incomplete structure from the high-order consistency information based on the low-rank tensor learning. Additionally, the anchor-based strategy is incorporated to reduce the computational complexity. An efficient alternative optimization algorithm combining a reduced gradient descent method is developed to solve the formulated objective, with theoretical convergence. Extensive experimental results on various datasets validate the superiority of our proposed AGF-TI as compared to state-of-the-art methods. Code is available at https://github.com/ZhangqiJiang07/AGF_TI.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RLinf: Flexible and Efficient Large-scale Reinforcement Learning via Macro-to-Micro Flow Transformation</title>
<link>https://arxiv.org/abs/2509.15965</link>
<guid>https://arxiv.org/abs/2509.15965</guid>
<content:encoded><![CDATA[
arXiv:2509.15965v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has demonstrated immense potential in advancing artificial general intelligence, agentic intelligence, and embodied intelligence. However, the inherent heterogeneity and dynamicity of RL workflows often lead to low hardware utilization and slow training on existing systems. In this paper, we present RLinf, a high-performance RL training system based on our key observation that the major roadblock to efficient RL training lies in system flexibility. To maximize flexibility and efficiency, RLinf is built atop a novel RL system design paradigm called macro-to-micro flow transformation (M2Flow), which automatically breaks down high-level, easy-to-compose RL workflows at both the temporal and spatial dimensions, and recomposes them into optimized execution flows. Supported by RLinf worker's adaptive communication capability, we devise context switching and elastic pipelining to realize M2Flow transformation, and a profiling-guided scheduling policy to generate optimal execution plans. Extensive evaluations on both reasoning RL and embodied RL tasks demonstrate that RLinf consistently outperforms state-of-the-art systems, achieving 1.1x-2.13x speedup in end-to-end training throughput.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Based Smooth Policy Regularisation for Reinforcement Learning with Few Demonstrations</title>
<link>https://arxiv.org/abs/2509.15981</link>
<guid>https://arxiv.org/abs/2509.15981</guid>
<content:encoded><![CDATA[
arXiv:2509.15981v1 Announce Type: new 
Abstract: In reinforcement learning with sparse rewards, demonstrations can accelerate learning, but determining when to imitate them remains challenging. We propose Smooth Policy Regularisation from Demonstrations (SPReD), a framework that addresses the fundamental question: when should an agent imitate a demonstration versus follow its own policy? SPReD uses ensemble methods to explicitly model Q-value distributions for both demonstration and policy actions, quantifying uncertainty for comparisons. We develop two complementary uncertainty-aware methods: a probabilistic approach estimating the likelihood of demonstration superiority, and an advantage-based approach scaling imitation by statistical significance. Unlike prevailing methods (e.g. Q-filter) that make binary imitation decisions, SPReD applies continuous, uncertainty-proportional regularisation weights, reducing gradient variance during training. Despite its computational simplicity, SPReD achieves remarkable gains in experiments across eight robotics tasks, outperforming existing approaches by up to a factor of 14 in complex tasks while maintaining robustness to demonstration quality and quantity. Our code is available at https://github.com/YujieZhu7/SPReD.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmoHeal: An End-to-End System for Personalized Therapeutic Music Retrieval from Fine-grained Emotions</title>
<link>https://arxiv.org/abs/2509.15986</link>
<guid>https://arxiv.org/abs/2509.15986</guid>
<content:encoded><![CDATA[
arXiv:2509.15986v1 Announce Type: new 
Abstract: Existing digital mental wellness tools often overlook the nuanced emotional states underlying everyday challenges. For example, pre-sleep anxiety affects more than 1.5 billion people worldwide, yet current approaches remain largely static and "one-size-fits-all", failing to adapt to individual needs. In this work, we present EmoHeal, an end-to-end system that delivers personalized, three-stage supportive narratives. EmoHeal detects 27 fine-grained emotions from user text with a fine-tuned XLM-RoBERTa model, mapping them to musical parameters via a knowledge graph grounded in music therapy principles (GEMS, iso-principle). EmoHeal retrieves audiovisual content using the CLAMP3 model to guide users from their current state toward a calmer one ("match-guide-target"). A within-subjects study (N=40) demonstrated significant supportive effects, with participants reporting substantial mood improvement (M=4.12, p<0.001) and high perceived emotion recognition accuracy (M=4.05, p<0.001). A strong correlation between perceived accuracy and therapeutic outcome (r=0.72, p<0.001) validates our fine-grained approach. These findings establish the viability of theory-driven, emotion-aware digital wellness tools and provides a scalable AI blueprint for operationalizing music therapy principles.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inverse Optimization Latent Variable Models for Learning Costs Applied to Route Problems</title>
<link>https://arxiv.org/abs/2509.15999</link>
<guid>https://arxiv.org/abs/2509.15999</guid>
<content:encoded><![CDATA[
arXiv:2509.15999v1 Announce Type: new 
Abstract: Learning representations for solutions of constrained optimization problems (COPs) with unknown cost functions is challenging, as models like (Variational) Autoencoders struggle to enforce constraints when decoding structured outputs. We propose an Inverse Optimization Latent Variable Model (IO-LVM) that learns a latent space of COP cost functions from observed solutions and reconstructs feasible outputs by solving a COP with a solver in the loop. Our approach leverages estimated gradients of a Fenchel-Young loss through a non-differentiable deterministic solver to shape the latent space. Unlike standard Inverse Optimization or Inverse Reinforcement Learning methods, which typically recover a single or context-specific cost function, IO-LVM captures a distribution over cost functions, enabling the identification of diverse solution behaviors arising from different agents or conditions not available during the training process. We validate our method on real-world datasets of ship and taxi routes, as well as paths in synthetic graphs, demonstrating its ability to reconstruct paths and cycles, predict their distributions, and yield interpretable latent representations.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting the descent into extremism and terrorism</title>
<link>https://arxiv.org/abs/2509.16014</link>
<guid>https://arxiv.org/abs/2509.16014</guid>
<content:encoded><![CDATA[
arXiv:2509.16014v1 Announce Type: new 
Abstract: This paper proposes an approach for automatically analysing and tracking statements in material gathered online and detecting whether the authors of the statements are likely to be involved in extremism or terrorism. The proposed system comprises: online collation of statements that are then encoded in a form amenable to machine learning (ML), an ML component to classify the encoded text, a tracker, and a visualisation system for analysis of results. The detection and tracking concept has been tested using quotes made by terrorists, extremists, campaigners, and politicians, obtained from wikiquote.org. A set of features was extracted for each quote using the state-of-the-art Universal Sentence Encoder (Cer et al. 2018), which produces 512-dimensional vectors. The data were used to train and test a support vector machine (SVM) classifier using 10-fold cross-validation. The system was able to correctly detect intentions and attitudes associated with extremism 81% of the time and terrorism 97% of the time, using a dataset of 839 quotes. This accuracy was higher than that which was achieved for a simple baseline system based on n-gram text features. Tracking techniques were also used to perform a temporal analysis of the data, with each quote considered to be a noisy measurement of a person's state of mind. It was demonstrated that the tracking algorithms were able to detect both trends over time and sharp changes in attitude that could be attributed to major events.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time-adaptive SympNets for separable Hamiltonian systems</title>
<link>https://arxiv.org/abs/2509.16026</link>
<guid>https://arxiv.org/abs/2509.16026</guid>
<content:encoded><![CDATA[
arXiv:2509.16026v1 Announce Type: new 
Abstract: Measurement data is often sampled irregularly i.e. not on equidistant time grids. This is also true for Hamiltonian systems. However, existing machine learning methods, which learn symplectic integrators, such as SympNets [20] and H\'enonNets [4] still require training data generated by fixed step sizes. To learn time-adaptive symplectic integrators, an extension to SympNets, which we call TSympNets, was introduced in [20]. We adapt the architecture of TSympNets and extend them to non-autonomous Hamiltonian systems. So far the approximation qualities of TSympNets were unknown. We close this gap by providing a universal approximation theorem for separable Hamiltonian systems and show that it is not possible to extend it to non-separable Hamiltonian systems. To investigate these theoretical approximation capabilities, we perform different numerical experiments. Furthermore we fix a mistake in a proof of a substantial theorem [25, Theorem 2] for the approximation of symplectic maps in general, but specifically for symplectic machine learning methods.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Constitutive Model Discovery by Pairing Sparse Regression Algorithms with Model Selection Criteria</title>
<link>https://arxiv.org/abs/2509.16040</link>
<guid>https://arxiv.org/abs/2509.16040</guid>
<content:encoded><![CDATA[
arXiv:2509.16040v1 Announce Type: new 
Abstract: The automated discovery of constitutive models from data has recently emerged as a promising alternative to the traditional model calibration paradigm. In this work, we present a fully automated framework for constitutive model discovery that systematically pairs three sparse regression algorithms (Least Absolute Shrinkage and Selection Operator (LASSO), Least Angle Regression (LARS), and Orthogonal Matching Pursuit (OMP)) with three model selection criteria: $K$-fold cross-validation (CV), Akaike Information Criterion (AIC), and Bayesian Information Criterion (BIC). This pairing yields nine distinct algorithms for model discovery and enables a systematic exploration of the trade-off between sparsity, predictive performance, and computational cost. While LARS serves as an efficient path-based solver for the $\ell_1$-constrained problem, OMP is introduced as a tractable heuristic for $\ell_0$-regularized selection. The framework is applied to both isotropic and anisotropic hyperelasticity, utilizing both synthetic and experimental datasets. Results reveal that all nine algorithm-criterion combinations perform consistently well for the discovery of isotropic and anisotropic materials, yielding highly accurate constitutive models. These findings broaden the range of viable discovery algorithms beyond $\ell_1$-based approaches such as LASSO.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SABER: Uncovering Vulnerabilities in Safety Alignment via Cross-Layer Residual Connection</title>
<link>https://arxiv.org/abs/2509.16060</link>
<guid>https://arxiv.org/abs/2509.16060</guid>
<content:encoded><![CDATA[
arXiv:2509.16060v1 Announce Type: new 
Abstract: Large Language Models (LLMs) with safe-alignment training are powerful instruments with robust language comprehension capabilities. These models typically undergo meticulous alignment procedures involving human feedback to ensure the acceptance of safe inputs while rejecting harmful or unsafe ones. However, despite their massive scale and alignment efforts, LLMs remain vulnerable to jailbreak attacks, where malicious users manipulate the model to produce harmful outputs that it was explicitly trained to avoid. In this study, we find that the safety mechanisms in LLMs are predominantly embedded in the middle-to-late layers. Building on this insight, we introduce a novel white-box jailbreak method, SABER (Safety Alignment Bypass via Extra Residuals), which connects two intermediate layers $s$ and $e$ such that $s < e$, through a residual connection. Our approach achieves a 51% improvement over the best-performing baseline on the HarmBench test set. Furthermore, SABER induces only a marginal shift in perplexity when evaluated on the HarmBench validation set. The source code is publicly available at https://github.com/PalGitts/SABER.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Communications to Circulations: 3D Wind Field Retrieval and Real-Time Prediction Using 5G GNSS Signals and Deep Learning</title>
<link>https://arxiv.org/abs/2509.16068</link>
<guid>https://arxiv.org/abs/2509.16068</guid>
<content:encoded><![CDATA[
arXiv:2509.16068v1 Announce Type: new 
Abstract: Accurate atmospheric wind field information is crucial for various applications, including weather forecasting, aviation safety, and disaster risk reduction. However, obtaining high spatiotemporal resolution wind data remains challenging due to limitations in traditional in-situ observations and remote sensing techniques, as well as the computational expense and biases of numerical weather prediction (NWP) models. This paper introduces G-WindCast, a novel deep learning framework that leverages signal strength variations from 5G Global Navigation Satellite System (GNSS) signals to retrieve and forecast three-dimensional (3D) atmospheric wind fields. The framework utilizes Forward Neural Networks (FNN) and Transformer networks to capture complex, nonlinear, and spatiotemporal relationships between GNSS-derived features and wind dynamics. Our preliminary results demonstrate promising accuracy in both wind retrieval and short-term wind forecasting (up to 30 minutes lead time), with skill scores comparable to high-resolution NWP outputs in certain scenarios. The model exhibits robustness across different forecast horizons and pressure levels, and its predictions for wind speed and direction show superior agreement with observations compared to concurrent ERA5 reanalysis data. Furthermore, we show that the system can maintain excellent performance for localized forecasting even with a significantly reduced number of GNSS stations (e.g., around 100), highlighting its cost-effectiveness and scalability. This interdisciplinary approach underscores the transformative potential of exploiting non-traditional data sources and deep learning for advanced environmental monitoring and real-time atmospheric applications.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MTS-DMAE: Dual-Masked Autoencoder for Unsupervised Multivariate Time Series Representation Learning</title>
<link>https://arxiv.org/abs/2509.16078</link>
<guid>https://arxiv.org/abs/2509.16078</guid>
<content:encoded><![CDATA[
arXiv:2509.16078v1 Announce Type: new 
Abstract: Unsupervised multivariate time series (MTS) representation learning aims to extract compact and informative representations from raw sequences without relying on labels, enabling efficient transfer to diverse downstream tasks. In this paper, we propose Dual-Masked Autoencoder (DMAE), a novel masked time-series modeling framework for unsupervised MTS representation learning. DMAE formulates two complementary pretext tasks: (1) reconstructing masked values based on visible attributes, and (2) estimating latent representations of masked features, guided by a teacher encoder. To further improve representation quality, we introduce a feature-level alignment constraint that encourages the predicted latent representations to align with the teacher's outputs. By jointly optimizing these objectives, DMAE learns temporally coherent and semantically rich representations. Comprehensive evaluations across classification, regression, and forecasting tasks demonstrate that our approach achieves consistent and superior performance over competitive baselines.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Molecule Synthesizability with Chain-of-Reaction</title>
<link>https://arxiv.org/abs/2509.16084</link>
<guid>https://arxiv.org/abs/2509.16084</guid>
<content:encoded><![CDATA[
arXiv:2509.16084v1 Announce Type: new 
Abstract: A well-known pitfall of molecular generative models is that they are not guaranteed to generate synthesizable molecules. There have been considerable attempts to address this problem, but given the exponentially large combinatorial space of synthesizable molecules, existing methods have shown limited coverage of the space and poor molecular optimization performance. To tackle these problems, we introduce ReaSyn, a generative framework for synthesizable projection where the model explores the neighborhood of given molecules in the synthesizable space by generating pathways that result in synthesizable analogs. To fully utilize the chemical knowledge contained in the synthetic pathways, we propose a novel perspective that views synthetic pathways akin to reasoning paths in large language models (LLMs). Specifically, inspired by chain-of-thought (CoT) reasoning in LLMs, we introduce the chain-of-reaction (CoR) notation that explicitly states reactants, reaction types, and intermediate products for each step in a pathway. With the CoR notation, ReaSyn can get dense supervision in every reaction step to explicitly learn chemical reaction rules during supervised training and perform step-by-step reasoning. In addition, to further enhance the reasoning capability of ReaSyn, we propose reinforcement learning (RL)-based finetuning and goal-directed test-time compute scaling tailored for synthesizable projection. ReaSyn achieves the highest reconstruction rate and pathway diversity in synthesizable molecule reconstruction and the highest optimization performance in synthesizable goal-directed molecular optimization, and significantly outperforms previous synthesizable projection methods in synthesizable hit expansion. These results highlight ReaSyn's superior ability to navigate combinatorially-large synthesizable chemical space.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Randomized Smoothing Meets Vision-Language Models</title>
<link>https://arxiv.org/abs/2509.16088</link>
<guid>https://arxiv.org/abs/2509.16088</guid>
<content:encoded><![CDATA[
arXiv:2509.16088v1 Announce Type: new 
Abstract: Randomized smoothing (RS) is one of the prominent techniques to ensure the correctness of machine learning models, where point-wise robustness certificates can be derived analytically. While RS is well understood for classification, its application to generative models is unclear, since their outputs are sequences rather than labels. We resolve this by connecting generative outputs to an oracle classification task and showing that RS can still be enabled: the final response can be classified as a discrete action (e.g., service-robot commands in VLAs), as harmful vs. harmless (content moderation or toxicity detection in VLMs), or even applying oracles to cluster answers into semantically equivalent ones. Provided that the error rate for the oracle classifier comparison is bounded, we develop the theory that associates the number of samples with the corresponding robustness radius. We further derive improved scaling laws analytically relating the certified radius and accuracy to the number of samples, showing that the earlier result of 2 to 3 orders of magnitude fewer samples sufficing with minimal loss remains valid even under weaker assumptions. Together, these advances make robustness certification both well-defined and computationally feasible for state-of-the-art VLMs, as validated against recent jailbreak-style adversarial attacks.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalized Federated Learning with Heat-Kernel Enhanced Tensorized Multi-View Clustering</title>
<link>https://arxiv.org/abs/2509.16101</link>
<guid>https://arxiv.org/abs/2509.16101</guid>
<content:encoded><![CDATA[
arXiv:2509.16101v1 Announce Type: new 
Abstract: We present a robust personalized federated learning framework that leverages heat-kernel enhanced tensorized multi-view fuzzy c-means clustering with advanced tensor decomposition techniques. Our approach integrates heat-kernel coefficients adapted from quantum field theory with Tucker decomposition and canonical polyadic decomposition (CANDECOMP/PARAFAC) to transform conventional distance metrics and efficiently represent high-dimensional multi-view structures. The framework employs matriculation and vectorization techniques to facilitate the discovery of hidden structures and multilinear relationships via N-way generalized tensors. The proposed method introduces a dual-level optimization scheme: local heat-kernel enhanced fuzzy clustering with tensor decomposition operating on order-N input tensors, and federated aggregation of tensor factors with privacy-preserving personalization mechanisms. The local stage employs tensorized kernel Euclidean distance transformations and Tucker decomposition to discover client-specific patterns in multi-view tensor data, while the global aggregation process coordinates tensor factors (core tensors and factor matrices) across clients through differential privacy-preserving protocols. This tensorized approach enables efficient handling of high-dimensional multi-view data with significant communication savings through low-rank tensor approximations.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffusionNFT: Online Diffusion Reinforcement with Forward Process</title>
<link>https://arxiv.org/abs/2509.16117</link>
<guid>https://arxiv.org/abs/2509.16117</guid>
<content:encoded><![CDATA[
arXiv:2509.16117v1 Announce Type: new 
Abstract: Online reinforcement learning (RL) has been central to post-training language models, but its extension to diffusion models remains challenging due to intractable likelihoods. Recent works discretize the reverse sampling process to enable GRPO-style training, yet they inherit fundamental drawbacks, including solver restrictions, forward-reverse inconsistency, and complicated integration with classifier-free guidance (CFG). We introduce Diffusion Negative-aware FineTuning (DiffusionNFT), a new online RL paradigm that optimizes diffusion models directly on the forward process via flow matching. DiffusionNFT contrasts positive and negative generations to define an implicit policy improvement direction, naturally incorporating reinforcement signals into the supervised learning objective. This formulation enables training with arbitrary black-box solvers, eliminates the need for likelihood estimation, and requires only clean images rather than sampling trajectories for policy optimization. DiffusionNFT is up to $25\times$ more efficient than FlowGRPO in head-to-head comparisons, while being CFG-free. For instance, DiffusionNFT improves the GenEval score from 0.24 to 0.98 within 1k steps, while FlowGRPO achieves 0.95 with over 5k steps and additional CFG employment. By leveraging multiple reward models, DiffusionNFT significantly boosts the performance of SD3.5-Medium in every benchmark tested.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Network-Based Detection of Autism Spectrum Disorder Using Sustainable and Non-invasive Salivary Biomarkers</title>
<link>https://arxiv.org/abs/2509.16126</link>
<guid>https://arxiv.org/abs/2509.16126</guid>
<content:encoded><![CDATA[
arXiv:2509.16126v1 Announce Type: new 
Abstract: Autism Spectrum Disorder (ASD) lacks reliable biological markers, delaying early diagnosis. Using 159 salivary samples analyzed by ATR-FTIR spectroscopy, we developed GANet, a genetic algorithm-based network optimization framework leveraging PageRank and Degree for importance-based feature characterization. GANet systematically optimizes network structure to extract meaningful patterns from high-dimensional spectral data. It achieved superior performance compared to linear discriminant analysis, support vector machines, and deep learning models, reaching 0.78 accuracy, 0.61 sensitivity, 0.90 specificity, and a 0.74 harmonic mean. These results demonstrate GANet's potential as a robust, bio-inspired, non-invasive tool for precise ASD detection and broader spectral-based health applications.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Classifier-Free Diffusion Guidance via Online Feedback</title>
<link>https://arxiv.org/abs/2509.16131</link>
<guid>https://arxiv.org/abs/2509.16131</guid>
<content:encoded><![CDATA[
arXiv:2509.16131v1 Announce Type: new 
Abstract: Classifier-free guidance (CFG) is a cornerstone of text-to-image diffusion models, yet its effectiveness is limited by the use of static guidance scales. This "one-size-fits-all" approach fails to adapt to the diverse requirements of different prompts; moreover, prior solutions like gradient-based correction or fixed heuristic schedules introduce additional complexities and fail to generalize. In this work, we challeng this static paradigm by introducing a framework for dynamic CFG scheduling. Our method leverages online feedback from a suite of general-purpose and specialized small-scale latent-space evaluations, such as CLIP for alignment, a discriminator for fidelity and a human preference reward model, to assess generation quality at each step of the reverse diffusion process. Based on this feedback, we perform a greedy search to select the optimal CFG scale for each timestep, creating a unique guidance schedule tailored to every prompt and sample. We demonstrate the effectiveness of our approach on both small-scale models and the state-of-the-art Imagen 3, showing significant improvements in text alignment, visual quality, text rendering and numerical reasoning. Notably, when compared against the default Imagen 3 baseline, our method achieves up to 53.8% human preference win-rate for overall preference, a figure that increases up to to 55.5% on prompts targeting specific capabilities like text rendering. Our work establishes that the optimal guidance schedule is inherently dynamic and prompt-dependent, and provides an efficient and generalizable framework to achieve it.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatio-temporal, multi-field deep learning of shock propagation in meso-structured media</title>
<link>https://arxiv.org/abs/2509.16139</link>
<guid>https://arxiv.org/abs/2509.16139</guid>
<content:encoded><![CDATA[
arXiv:2509.16139v1 Announce Type: new 
Abstract: The ability to predict how shock waves traverse porous and architected materials is a decisive factor in planetary defense, national security, and the race to achieve inertial fusion energy. Yet capturing pore collapse, anomalous Hugoniot responses, and localized heating -- phenomena that can determine the success of asteroid deflection or fusion ignition -- has remained a major challenge despite recent advances in single-field and reduced representations. We introduce a multi-field spatio-temporal deep learning model (MSTM) that unifies seven coupled fields -- pressure, density, temperature, energy, material distribution, and two velocity components -- into a single autoregressive surrogate. Trained on high-fidelity hydrocode data, MSTM runs about a thousand times faster than direct simulation, achieving errors below 4\% in porous materials and below 10\% in lattice structures. Unlike prior single-field or operator-based surrogates, MSTM resolves sharp shock fronts while preserving integrated quantities such as mass-averaged pressure and temperature to within 5\%. This advance transforms problems once considered intractable into tractable design studies, establishing a practical framework for optimizing meso-structured materials in planetary impact mitigation, inertial fusion energy, and national security.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Cyber Defense with Generalizable Graph-based Reinforcement Learning Agents</title>
<link>https://arxiv.org/abs/2509.16151</link>
<guid>https://arxiv.org/abs/2509.16151</guid>
<content:encoded><![CDATA[
arXiv:2509.16151v1 Announce Type: new 
Abstract: Deep reinforcement learning (RL) is emerging as a viable strategy for automated cyber defense (ACD). The traditional RL approach represents networks as a list of computers in various states of safety or threat. Unfortunately, these models are forced to overfit to specific network topologies, rendering them ineffective when faced with even small environmental perturbations. In this work, we frame ACD as a two-player context-based partially observable Markov decision problem with observations represented as attributed graphs. This approach allows our agents to reason through the lens of relational inductive bias. Agents learn how to reason about hosts interacting with other system entities in a more general manner, and their actions are understood as edits to the graph representing the environment. By introducing this bias, we will show that our agents can better reason about the states of networks and zero-shot adapt to new ones. We show that this approach outperforms the state-of-the-art by a wide margin, and makes our agents capable of defending never-before-seen networks against a wide range of adversaries in a variety of complex, and multi-agent environments.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DIVEBATCH: Accelerating Model Training Through Gradient-Diversity Aware Batch Size Adaptation</title>
<link>https://arxiv.org/abs/2509.16173</link>
<guid>https://arxiv.org/abs/2509.16173</guid>
<content:encoded><![CDATA[
arXiv:2509.16173v1 Announce Type: new 
Abstract: The goal of this paper is to accelerate the training of machine learning models, a critical challenge since the training of large-scale deep neural models can be computationally expensive. Stochastic gradient descent (SGD) and its variants are widely used to train deep neural networks. In contrast to traditional approaches that focus on tuning the learning rate, we propose a novel adaptive batch size SGD algorithm, DiveBatch, that dynamically adjusts the batch size. Adapting the batch size is challenging: using large batch sizes is more efficient due to parallel computation, but small-batch training often converges in fewer epochs and generalizes better. To address this challenge, we introduce a data-driven adaptation based on gradient diversity, enabling DiveBatch to maintain the generalization performance of small-batch training while improving convergence speed and computational efficiency. Gradient diversity has a strong theoretical justification: it emerges from the convergence analysis of SGD. Evaluations of DiveBatch on synthetic and CiFar-10, CiFar-100, and Tiny-ImageNet demonstrate that DiveBatch converges significantly faster than standard SGD and AdaBatch (1.06 -- 5.0x), with a slight trade-off in performance.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent learning: episodic memory complements parametric learning by enabling flexible reuse of experiences</title>
<link>https://arxiv.org/abs/2509.16189</link>
<guid>https://arxiv.org/abs/2509.16189</guid>
<content:encoded><![CDATA[
arXiv:2509.16189v1 Announce Type: new 
Abstract: When do machine learning systems fail to generalize, and what mechanisms could improve their generalization? Here, we draw inspiration from cognitive science to argue that one weakness of machine learning systems is their failure to exhibit latent learning -- learning information that is not relevant to the task at hand, but that might be useful in a future task. We show how this perspective links failures ranging from the reversal curse in language modeling to new findings on agent-based navigation. We then highlight how cognitive science points to episodic memory as a potential part of the solution to these issues. Correspondingly, we show that a system with an oracle retrieval mechanism can use learning experiences more flexibly to generalize better across many of these challenges. We also identify some of the essential components for effectively using retrieval, including the importance of within-example in-context learning for acquiring the ability to use information across retrieved examples. In summary, our results illustrate one possible contributor to the relative data inefficiency of current machine learning systems compared to natural intelligence, and help to understand how retrieval methods can complement parametric learning to improve generalization.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inverting Trojans in LLMs</title>
<link>https://arxiv.org/abs/2509.16203</link>
<guid>https://arxiv.org/abs/2509.16203</guid>
<content:encoded><![CDATA[
arXiv:2509.16203v1 Announce Type: new 
Abstract: While effective backdoor detection and inversion schemes have been developed for AIs used e.g. for images, there are challenges in "porting" these methods to LLMs. First, the LLM input space is discrete, which precludes gradient-based search over this space, central to many backdoor inversion methods. Second, there are ~30,000^k k-tuples to consider, k the token-length of a putative trigger. Third, for LLMs there is the need to blacklist tokens that have strong marginal associations with the putative target response (class) of an attack, as such tokens give false detection signals. However, good blacklists may not exist for some domains. We propose a LLM trigger inversion approach with three key components: i) discrete search, with putative triggers greedily accreted, starting from a select list of singletons; ii) implicit blacklisting, achieved by evaluating the average cosine similarity, in activation space, between a candidate trigger and a small clean set of samples from the putative target class; iii) detection when a candidate trigger elicits high misclassifications, and with unusually high decision confidence. Unlike many recent works, we demonstrate that our approach reliably detects and successfully inverts ground-truth backdoor trigger phrases.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Gaussian Process-based Cost-Aware Batch Bayesian Optimization for Complex Materials Design Campaigns</title>
<link>https://arxiv.org/abs/2509.14408</link>
<guid>https://arxiv.org/abs/2509.14408</guid>
<content:encoded><![CDATA[
arXiv:2509.14408v1 Announce Type: cross 
Abstract: The accelerating pace and expanding scope of materials discovery demand optimization frameworks that efficiently navigate vast, nonlinear design spaces while judiciously allocating limited evaluation resources. We present a cost-aware, batch Bayesian optimization scheme powered by deep Gaussian process (DGP) surrogates and a heterotopic querying strategy. Our DGP surrogate, formed by stacking GP layers, models complex hierarchical relationships among high-dimensional compositional features and captures correlations across multiple target properties, propagating uncertainty through successive layers. We integrate evaluation cost into an upper-confidence-bound acquisition extension, which, together with heterotopic querying, proposes small batches of candidates in parallel, balancing exploration of under-characterized regions with exploitation of high-mean, low-variance predictions across correlated properties. Applied to refractory high-entropy alloys for high-temperature applications, our framework converges to optimal formulations in fewer iterations with cost-aware queries than conventional GP-based BO, highlighting the value of deep, uncertainty-aware, cost-sensitive strategies in materials campaigns.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MICA: Multi-Agent Industrial Coordination Assistant</title>
<link>https://arxiv.org/abs/2509.15237</link>
<guid>https://arxiv.org/abs/2509.15237</guid>
<content:encoded><![CDATA[
arXiv:2509.15237v1 Announce Type: cross 
Abstract: Industrial workflows demand adaptive and trustworthy assistance that can operate under limited computing, connectivity, and strict privacy constraints. In this work, we present MICA (Multi-Agent Industrial Coordination Assistant), a perception-grounded and speech-interactive system that delivers real-time guidance for assembly, troubleshooting, part queries, and maintenance. MICA coordinates five role-specialized language agents, audited by a safety checker, to ensure accurate and compliant support. To achieve robust step understanding, we introduce Adaptive Step Fusion (ASF), which dynamically blends expert reasoning with online adaptation from natural speech feedback. Furthermore, we establish a new multi-agent coordination benchmark across representative task categories and propose evaluation metrics tailored to industrial assistance, enabling systematic comparison of different coordination topologies. Our experiments demonstrate that MICA consistently improves task success, reliability, and responsiveness over baseline structures, while remaining deployable on practical offline hardware. Together, these contributions highlight MICA as a step toward deployable, privacy-preserving multi-agent assistants for dynamic factory environments. The source code will be made publicly available at https://github.com/Kratos-Wen/MICA.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KNARsack: Teaching Neural Algorithmic Reasoners to Solve Pseudo-Polynomial Problems</title>
<link>https://arxiv.org/abs/2509.15239</link>
<guid>https://arxiv.org/abs/2509.15239</guid>
<content:encoded><![CDATA[
arXiv:2509.15239v1 Announce Type: cross 
Abstract: Neural algorithmic reasoning (NAR) is a growing field that aims to embed algorithmic logic into neural networks by imitating classical algorithms. In this extended abstract, we detail our attempt to build a neural algorithmic reasoner that can solve Knapsack, a pseudo-polynomial problem bridging classical algorithms and combinatorial optimisation, but omitted in standard NAR benchmarks. Our neural algorithmic reasoner is designed to closely follow the two-phase pipeline for the Knapsack problem, which involves first constructing the dynamic programming table and then reconstructing the solution from it. The approach, which models intermediate states through dynamic programming supervision, achieves better generalization to larger problem instances than a direct-prediction baseline that attempts to select the optimal subset only from the problem inputs.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kernel Model Validation: How To Do It, And Why You Should Care</title>
<link>https://arxiv.org/abs/2509.15244</link>
<guid>https://arxiv.org/abs/2509.15244</guid>
<content:encoded><![CDATA[
arXiv:2509.15244v1 Announce Type: cross 
Abstract: Gaussian Process (GP) models are popular tools in uncertainty quantification (UQ) because they purport to furnish functional uncertainty estimates that can be used to represent model uncertainty. It is often difficult to state with precision what probabilistic interpretation attaches to such an uncertainty, and in what way is it calibrated. Without such a calibration statement, the value of such uncertainty estimates is quite limited and qualitative. We motivate the importance of proper probabilistic calibration of GP predictions by describing how GP predictive calibration failures can cause degraded convergence properties in a target optimization algorithm called Targeted Adaptive Design (TAD). We discuss the interpretation of GP-generated uncertainty intervals in UQ, and how one may learn to trust them, through a formal procedure for covariance kernel validation that exploits the multivariate normal nature of GP predictions. We give simple examples of GP regression misspecified 1-dimensional models, and discuss the situation with respect to higher-dimensional models.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RespoDiff: Dual-Module Bottleneck Transformation for Responsible &amp; Faithful T2I Generation</title>
<link>https://arxiv.org/abs/2509.15257</link>
<guid>https://arxiv.org/abs/2509.15257</guid>
<content:encoded><![CDATA[
arXiv:2509.15257v1 Announce Type: cross 
Abstract: The rapid advancement of diffusion models has enabled high-fidelity and semantically rich text-to-image generation; however, ensuring fairness and safety remains an open challenge. Existing methods typically improve fairness and safety at the expense of semantic fidelity and image quality. In this work, we propose RespoDiff, a novel framework for responsible text-to-image generation that incorporates a dual-module transformation on the intermediate bottleneck representations of diffusion models. Our approach introduces two distinct learnable modules: one focused on capturing and enforcing responsible concepts, such as fairness and safety, and the other dedicated to maintaining semantic alignment with neutral prompts. To facilitate the dual learning process, we introduce a novel score-matching objective that enables effective coordination between the modules. Our method outperforms state-of-the-art methods in responsible generation by ensuring semantic alignment while optimizing both objectives without compromising image fidelity. Our approach improves responsible and semantically coherent generation by 20% across diverse, unseen prompts. Moreover, it integrates seamlessly into large-scale models like SDXL, enhancing fairness and safety. Code will be released upon acceptance.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Subject Matter Expertise vs Professional Management in Collective Sequential Decision Making</title>
<link>https://arxiv.org/abs/2509.15263</link>
<guid>https://arxiv.org/abs/2509.15263</guid>
<content:encoded><![CDATA[
arXiv:2509.15263v1 Announce Type: cross 
Abstract: Your company's CEO is retiring. You search for a successor. You can promote an employee from the company familiar with the company's operations, or recruit an external professional manager. Who should you prefer? It has not been clear how to address this question, the "subject matter expertise vs. professional manager debate", quantitatively and objectively. We note that a company's success depends on long sequences of interdependent decisions, with often-opposing recommendations of diverse board members. To model this task in a controlled environment, we utilize chess - a complex, sequential game with interdependent decisions which allows for quantitative analysis of performance and expertise (since the states, actions and game outcomes are well-defined). The availability of chess engines differing in style and expertise, allows scalable experimentation. We considered a team of (computer) chess players. At each turn, team members recommend a move and a manager chooses a recommendation. We compared the performance of two manager types. For manager as "subject matter expert", we used another (computer) chess player that assesses the recommendations of the team members based on its own chess expertise. We examined the performance of such managers at different strength levels. To model a "professional manager", we used Reinforcement Learning (RL) to train a network that identifies the board positions in which different team members have relative advantage, without any pretraining in chess. We further examined this network to see if any chess knowledge is acquired implicitly. We found that subject matter expertise beyond a minimal threshold does not significantly contribute to team synergy. Moreover, performance of a RL-trained "professional" manager significantly exceeds that of even the best "expert" managers, while acquiring only limited understanding of chess.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autoguided Online Data Curation for Diffusion Model Training</title>
<link>https://arxiv.org/abs/2509.15267</link>
<guid>https://arxiv.org/abs/2509.15267</guid>
<content:encoded><![CDATA[
arXiv:2509.15267v1 Announce Type: cross 
Abstract: The costs of generative model compute rekindled promises and hopes for efficient data curation. In this work, we investigate whether recently developed autoguidance and online data selection methods can improve the time and sample efficiency of training generative diffusion models. We integrate joint example selection (JEST) and autoguidance into a unified code base for fast ablation and benchmarking. We evaluate combinations of data curation on a controlled 2-D synthetic data generation task as well as (3x64x64)-D image generation. Our comparisons are made at equal wall-clock time and equal number of samples, explicitly accounting for the overhead of selection. Across experiments, autoguidance consistently improves sample quality and diversity. Early AJEST (applying selection only at the beginning of training) can match or modestly exceed autoguidance alone in data efficiency on both tasks. However, its time overhead and added complexity make autoguidance or uniform random data selection preferable in most situations. These findings suggest that while targeted online selection can yield efficiency gains in early training, robust sample quality improvements are primarily driven by autoguidance. We discuss limitations and scope, and outline when data selection may be beneficial.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Copycat vs. Original: Multi-modal Pretraining and Variable Importance in Box-office Prediction</title>
<link>https://arxiv.org/abs/2509.15277</link>
<guid>https://arxiv.org/abs/2509.15277</guid>
<content:encoded><![CDATA[
arXiv:2509.15277v1 Announce Type: cross 
Abstract: The movie industry is associated with an elevated level of risk, which necessitates the use of automated tools to predict box-office revenue and facilitate human decision-making. In this study, we build a sophisticated multimodal neural network that predicts box offices by grounding crowdsourced descriptive keywords of each movie in the visual information of the movie posters, thereby enhancing the learned keyword representations, resulting in a substantial reduction of 14.5% in box-office prediction error. The advanced revenue prediction model enables the analysis of the commercial viability of "copycat movies," or movies with substantial similarity to successful movies released recently. We do so by computing the influence of copycat features in box-office prediction. We find a positive relationship between copycat status and movie revenue. However, this effect diminishes when the number of similar movies and the similarity of their content increase. Overall, our work develops sophisticated deep learning tools for studying the movie industry and provides valuable business insight.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Limitations of Local LLMs in Solving Complex Programming Challenges</title>
<link>https://arxiv.org/abs/2509.15283</link>
<guid>https://arxiv.org/abs/2509.15283</guid>
<content:encoded><![CDATA[
arXiv:2509.15283v1 Announce Type: cross 
Abstract: This study examines the performance of today's open-source, locally hosted large-language models (LLMs) in handling complex competitive programming tasks with extended problem descriptions and contexts. Building on the original Framework for AI-driven Code Generation Evaluation (FACE), the authors retrofit the pipeline to work entirely offline through the Ollama runtime, collapsing FACE's sprawling per-problem directory tree into a handful of consolidated JSON files, and adding robust checkpointing so multi-day runs can resume after failures. The enhanced framework generates, submits, and records solutions for the full Kattis corpus of 3,589 problems across eight code-oriented models ranging from 6.7-9 billion parameters. The submission results show that the overall pass@1 accuracy is modest for the local models, with the best models performing at approximately half the acceptance rate of the proprietary models, Gemini 1.5 and ChatGPT-4. These findings expose a persistent gap between private, cost-controlled LLM deployments and state-of-the-art proprietary services, yet also highlight the rapid progress of open models and the practical benefits of an evaluation workflow that organizations can replicate on in-house hardware.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training thermodynamic computers by gradient descent</title>
<link>https://arxiv.org/abs/2509.15324</link>
<guid>https://arxiv.org/abs/2509.15324</guid>
<content:encoded><![CDATA[
arXiv:2509.15324v1 Announce Type: cross 
Abstract: We show how to adjust the parameters of a thermodynamic computer by gradient descent in order to perform a desired computation at a specified observation time. Within a digital simulation of a thermodynamic computer, training proceeds by maximizing the probability with which the computer would generate an idealized dynamical trajectory. The idealized trajectory is designed to reproduce the activations of a neural network trained to perform the desired computation. This teacher-student scheme results in a thermodynamic computer whose finite-time dynamics enacts a computation analogous to that of the neural network. The parameters identified in this way can be implemented in the hardware realization of the thermodynamic computer, which will perform the desired computation automatically, driven by thermal noise. We demonstrate the method on a standard image-classification task, and estimate the thermodynamic advantage -- the ratio of energy costs of the digital and thermodynamic implementations -- to exceed seven orders of magnitude. Our results establish gradient descent as a viable training method for thermodynamic computing, enabling application of the core methodology of machine learning to this emerging field.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emulating Human-like Adaptive Vision for Efficient and Flexible Machine Visual Perception</title>
<link>https://arxiv.org/abs/2509.15333</link>
<guid>https://arxiv.org/abs/2509.15333</guid>
<content:encoded><![CDATA[
arXiv:2509.15333v1 Announce Type: cross 
Abstract: Human vision is highly adaptive, efficiently sampling intricate environments by sequentially fixating on task-relevant regions. In contrast, prevailing machine vision models passively process entire scenes at once, resulting in excessive resource demands scaling with spatial-temporal input resolution and model size, yielding critical limitations impeding both future advancements and real-world application. Here we introduce AdaptiveNN, a general framework aiming to drive a paradigm shift from 'passive' to 'active, adaptive' vision models. AdaptiveNN formulates visual perception as a coarse-to-fine sequential decision-making process, progressively identifying and attending to regions pertinent to the task, incrementally combining information across fixations, and actively concluding observation when sufficient. We establish a theory integrating representation learning with self-rewarding reinforcement learning, enabling end-to-end training of the non-differentiable AdaptiveNN without additional supervision on fixation locations. We assess AdaptiveNN on 17 benchmarks spanning 9 tasks, including large-scale visual recognition, fine-grained discrimination, visual search, processing images from real driving and medical scenarios, language-driven embodied AI, and side-by-side comparisons with humans. AdaptiveNN achieves up to 28x inference cost reduction without sacrificing accuracy, flexibly adapts to varying task demands and resource budgets without retraining, and provides enhanced interpretability via its fixation patterns, demonstrating a promising avenue toward efficient, flexible, and interpretable computer vision. Furthermore, AdaptiveNN exhibits closely human-like perceptual behaviors in many cases, revealing its potential as a valuable tool for investigating visual cognition. Code is available at https://github.com/LeapLabTHU/AdaptiveNN.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MaskAttn-SDXL: Controllable Region-Level Text-To-Image Generation</title>
<link>https://arxiv.org/abs/2509.15357</link>
<guid>https://arxiv.org/abs/2509.15357</guid>
<content:encoded><![CDATA[
arXiv:2509.15357v1 Announce Type: cross 
Abstract: Text-to-image diffusion models achieve impressive realism but often suffer from compositional failures on prompts with multiple objects, attributes, and spatial relations, resulting in cross-token interference where entities entangle, attributes mix across objects, and spatial cues are violated. To address these failures, we propose MaskAttn-SDXL,a region-level gating mechanism applied to the cross-attention logits of Stable Diffusion XL(SDXL)'s UNet. MaskAttn-SDXL learns a binary mask per layer, injecting it into each cross-attention logit map before softmax to sparsify token-to-latent interactions so that only semantically relevant connections remain active. The method requires no positional encodings, auxiliary tokens, or external region masks, and preserves the original inference path with negligible overhead. In practice, our model improves spatial compliance and attribute binding in multi-object prompts while preserving overall image quality and diversity. These findings demonstrate that logit-level maksed cross-attention is an data-efficient primitve for enforcing compositional control, and our method thus serves as a practical extension for spatial control in text-to-image generation.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recent Advancements in Microscopy Image Enhancement using Deep Learning: A Survey</title>
<link>https://arxiv.org/abs/2509.15363</link>
<guid>https://arxiv.org/abs/2509.15363</guid>
<content:encoded><![CDATA[
arXiv:2509.15363v1 Announce Type: cross 
Abstract: Microscopy image enhancement plays a pivotal role in understanding the details of biological cells and materials at microscopic scales. In recent years, there has been a significant rise in the advancement of microscopy image enhancement, specifically with the help of deep learning methods. This survey paper aims to provide a snapshot of this rapidly growing state-of-the-art method, focusing on its evolution, applications, challenges, and future directions. The core discussions take place around the key domains of microscopy image enhancement of super-resolution, reconstruction, and denoising, with each domain explored in terms of its current trends and their practical utility of deep learning.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Fine-Tuning of Large Audio Language Models for Spoken Language Understanding under Limited Speech data</title>
<link>https://arxiv.org/abs/2509.15389</link>
<guid>https://arxiv.org/abs/2509.15389</guid>
<content:encoded><![CDATA[
arXiv:2509.15389v1 Announce Type: cross 
Abstract: Large Audio Language Models (LALMs) have emerged as powerful tools for speech-related tasks but remain underexplored for fine-tuning, especially with limited speech data. To bridge this gap, we systematically examine how different fine-tuning schemes including text-only, direct mixing, and curriculum learning affect spoken language understanding (SLU), focusing on scenarios where text-label pairs are abundant while paired speech-label data are limited. Results show that LALMs already achieve competitive performance with text-only fine-tuning, highlighting their strong generalization ability. Adding even small amounts of speech data (2-5%) yields substantial further gains, with curriculum learning particularly effective under scarce data. In cross-lingual SLU, combining source-language speech data with target-language text and minimal target-language speech data enables effective adaptation. Overall, this study provides practical insights into the LALM fine-tuning under realistic data constraints.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying Uncertainty in Natural Language Explanations of Large Language Models for Question Answering</title>
<link>https://arxiv.org/abs/2509.15403</link>
<guid>https://arxiv.org/abs/2509.15403</guid>
<content:encoded><![CDATA[
arXiv:2509.15403v1 Announce Type: cross 
Abstract: Large language models (LLMs) have shown strong capabilities, enabling concise, context-aware answers in question answering (QA) tasks. The lack of transparency in complex LLMs has inspired extensive research aimed at developing methods to explain large language behaviors. Among existing explanation methods, natural language explanations stand out due to their ability to explain LLMs in a self-explanatory manner and enable the understanding of model behaviors even when the models are closed-source. However, despite these promising advancements, there is no existing work studying how to provide valid uncertainty guarantees for these generated natural language explanations. Such uncertainty quantification is critical in understanding the confidence behind these explanations. Notably, generating valid uncertainty estimates for natural language explanations is particularly challenging due to the auto-regressive generation process of LLMs and the presence of noise in medical inquiries. To bridge this gap, in this work, we first propose a novel uncertainty estimation framework for these generated natural language explanations, which provides valid uncertainty guarantees in a post-hoc and model-agnostic manner. Additionally, we also design a novel robust uncertainty estimation method that maintains valid uncertainty guarantees even under noise. Extensive experiments on QA tasks demonstrate the desired performance of our methods.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep learning and abstractive summarisation for radiological reports: an empirical study for adapting the PEGASUS models' family with scarce data</title>
<link>https://arxiv.org/abs/2509.15419</link>
<guid>https://arxiv.org/abs/2509.15419</guid>
<content:encoded><![CDATA[
arXiv:2509.15419v1 Announce Type: cross 
Abstract: Regardless of the rapid development of artificial intelligence, abstractive summarisation is still challenging for sensitive and data-restrictive domains like medicine. With the increasing number of imaging, the relevance of automated tools for complex medical text summarisation is expected to become highly relevant. In this paper, we investigated the adaptation via fine-tuning process of a non-domain-specific abstractive summarisation encoder-decoder model family, and gave insights to practitioners on how to avoid over- and underfitting. We used PEGASUS and PEGASUS-X, on a medium-sized radiological reports public dataset. For each model, we comprehensively evaluated two different checkpoints with varying sizes of the same training data. We monitored the models' performances with lexical and semantic metrics during the training history on the fixed-size validation set. PEGASUS exhibited different phases, which can be related to epoch-wise double-descent, or peak-drop-recovery behaviour. For PEGASUS-X, we found that using a larger checkpoint led to a performance detriment. This work highlights the challenges and risks of fine-tuning models with high expressivity when dealing with scarce training data, and lays the groundwork for future investigations into more robust fine-tuning strategies for summarisation models in specialised domains.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Architecture Search Algorithms for Quantum Autoencoders</title>
<link>https://arxiv.org/abs/2509.15451</link>
<guid>https://arxiv.org/abs/2509.15451</guid>
<content:encoded><![CDATA[
arXiv:2509.15451v1 Announce Type: cross 
Abstract: The design of quantum circuits is currently driven by the specific objectives of the quantum algorithm in question. This approach thus relies on a significant manual effort by the quantum algorithm designer to design an appropriate circuit for the task. However this approach cannot scale to more complex quantum algorithms in the future without exponentially increasing the circuit design effort and introducing unwanted inductive biases. Motivated by this observation, we propose to automate the process of cicuit design by drawing inspiration from Neural Architecture Search (NAS). In this work, we propose two Quantum-NAS algorithms that aim to find efficient circuits given a particular quantum task. We choose quantum data compression as our driver quantum task and demonstrate the performance of our algorithms by finding efficient autoencoder designs that outperform baselines on three different tasks - quantum data denoising, classical data compression and pure quantum data compression. Our results indicate that quantum NAS algorithms can significantly alleviate the manual effort while delivering performant quantum circuits for any given task.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breathing and Semantic Pause Detection and Exertion-Level Classification in Post-Exercise Speech</title>
<link>https://arxiv.org/abs/2509.15473</link>
<guid>https://arxiv.org/abs/2509.15473</guid>
<content:encoded><![CDATA[
arXiv:2509.15473v1 Announce Type: cross 
Abstract: Post-exercise speech contains rich physiological and linguistic cues, often marked by semantic pauses, breathing pauses, and combined breathing-semantic pauses. Detecting these events enables assessment of recovery rate, lung function, and exertion-related abnormalities. However, existing works on identifying and distinguishing different types of pauses in this context are limited. In this work, building on a recently released dataset with synchronized audio and respiration signals, we provide systematic annotations of pause types. Using these annotations, we systematically conduct exploratory breathing and semantic pause detection and exertion-level classification across deep learning models (GRU, 1D CNN-LSTM, AlexNet, VGG16), acoustic features (MFCC, MFB), and layer-stratified Wav2Vec2 representations. We evaluate three setups-single feature, feature fusion, and a two-stage detection-classification cascade-under both classification and regression formulations. Results show per-type detection accuracy up to 89$\%$ for semantic, 55$\%$ for breathing, 86$\%$ for combined pauses, and 73$\%$overall, while exertion-level classification achieves 90.5$\%$ accuracy, outperformin prior work.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>(SP)$^2$-Net: A Neural Spatial Spectrum Method for DOA Estimation</title>
<link>https://arxiv.org/abs/2509.15475</link>
<guid>https://arxiv.org/abs/2509.15475</guid>
<content:encoded><![CDATA[
arXiv:2509.15475v1 Announce Type: cross 
Abstract: We consider the problem of estimating the directions of arrival (DOAs) of multiple sources from a single snapshot of an antenna array, a task with many practical applications. In such settings, the classical Bartlett beamformer is commonly used, as maximum likelihood estimation becomes impractical when the number of sources is unknown or large, and spectral methods based on the sample covariance are not applicable due to the lack of multiple snapshots. However, the accuracy and resolution of the Bartlett beamformer are fundamentally limited by the array aperture. In this paper, we propose a deep learning technique, comprising a novel architecture and training strategy, for generating a high-resolution spatial spectrum from a single snapshot. Specifically, we train a deep neural network that takes the measurements and a hypothesis angle as input and learns to output a score consistent with the capabilities of a much wider array. At inference time, a heatmap can be produced by scanning an arbitrary set of angles. We demonstrate the advantages of our trained model, named (SP)$^2$-Net, over the Bartlett beamformer and sparsity-based DOA estimation methods.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How do Language Models Generate Slang: A Systematic Comparison between Human and Machine-Generated Slang Usages</title>
<link>https://arxiv.org/abs/2509.15518</link>
<guid>https://arxiv.org/abs/2509.15518</guid>
<content:encoded><![CDATA[
arXiv:2509.15518v1 Announce Type: cross 
Abstract: Slang is a commonly used type of informal language that poses a daunting challenge to NLP systems. Recent advances in large language models (LLMs), however, have made the problem more approachable. While LLM agents are becoming more widely applied to intermediary tasks such as slang detection and slang interpretation, their generalizability and reliability are heavily dependent on whether these models have captured structural knowledge about slang that align well with human attested slang usages. To answer this question, we contribute a systematic comparison between human and machine-generated slang usages. Our evaluative framework focuses on three core aspects: 1) Characteristics of the usages that reflect systematic biases in how machines perceive slang, 2) Creativity reflected by both lexical coinages and word reuses employed by the slang usages, and 3) Informativeness of the slang usages when used as gold-standard examples for model distillation. By comparing human-attested slang usages from the Online Slang Dictionary (OSD) and slang generated by GPT-4o and Llama-3, we find significant biases in how LLMs perceive slang. Our results suggest that while LLMs have captured significant knowledge about the creative aspects of slang, such knowledge does not align with humans sufficiently to enable LLMs for extrapolative tasks such as linguistic analyses.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometric Integration for Neural Control Variates</title>
<link>https://arxiv.org/abs/2509.15538</link>
<guid>https://arxiv.org/abs/2509.15538</guid>
<content:encoded><![CDATA[
arXiv:2509.15538v1 Announce Type: cross 
Abstract: Control variates are a variance-reduction technique for Monte Carlo integration. The principle involves approximating the integrand by a function that can be analytically integrated, and integrating using the Monte Carlo method only the residual difference between the integrand and the approximation, to obtain an unbiased estimate. Neural networks are universal approximators that could potentially be used as a control variate. However, the challenge lies in the analytic integration, which is not possible in general. In this manuscript, we study one of the simplest neural network models, the multilayered perceptron (MLP) with continuous piecewise linear activation functions, and its possible analytic integration. We propose an integration method based on integration domain subdivision, employing techniques from computational geometry to solve this problem in 2D. We demonstrate that an MLP can be used as a control variate in combination with our integration method, showing applications in the light transport simulation.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Deep Learning-Federated Learning Powered Intrusion Detection System for IoT/5G Advanced Edge Computing Network</title>
<link>https://arxiv.org/abs/2509.15555</link>
<guid>https://arxiv.org/abs/2509.15555</guid>
<content:encoded><![CDATA[
arXiv:2509.15555v1 Announce Type: cross 
Abstract: The exponential expansion of IoT and 5G-Advanced applications has enlarged the attack surface for DDoS, malware, and zero-day intrusions. We propose an intrusion detection system that fuses a convolutional neural network (CNN), a bidirectional LSTM (BiLSTM), and an autoencoder (AE) bottleneck within a privacy-preserving federated learning (FL) framework. The CNN-BiLSTM branch captures local and gated cross-feature interactions, while the AE emphasizes reconstruction-based anomaly sensitivity. Training occurs across edge devices without sharing raw data. On UNSW-NB15 (binary), the fused model attains AUC 99.59 percent and F1 97.36 percent; confusion-matrix analysis shows balanced error rates with high precision and recall. Average inference time is approximately 0.0476 ms per sample on our test hardware, which is well within the less than 10 ms URLLC budget, supporting edge deployment. We also discuss explainability, drift tolerance, and FL considerations for compliant, scalable 5G-Advanced IoT security.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Size-invariant Salient Object Detection: A Generic Evaluation and Optimization Approach</title>
<link>https://arxiv.org/abs/2509.15573</link>
<guid>https://arxiv.org/abs/2509.15573</guid>
<content:encoded><![CDATA[
arXiv:2509.15573v1 Announce Type: cross 
Abstract: This paper investigates a fundamental yet underexplored issue in Salient Object Detection (SOD): the size-invariant property for evaluation protocols, particularly in scenarios when multiple salient objects of significantly different sizes appear within a single image. We first present a novel perspective to expose the inherent size sensitivity of existing widely used SOD metrics. Through careful theoretical derivations, we show that the evaluation outcome of an image under current SOD metrics can be essentially decomposed into a sum of several separable terms, with the contribution of each term being directly proportional to its corresponding region size. Consequently, the prediction errors would be dominated by the larger regions, while smaller yet potentially more semantically important objects are often overlooked, leading to biased performance assessments and practical degradation. To address this challenge, a generic Size-Invariant Evaluation (SIEva) framework is proposed. The core idea is to evaluate each separable component individually and then aggregate the results, thereby effectively mitigating the impact of size imbalance across objects. Building upon this, we further develop a dedicated optimization framework (SIOpt), which adheres to the size-invariant principle and significantly enhances the detection of salient objects across a broad range of sizes. Notably, SIOpt is model-agnostic and can be seamlessly integrated with a wide range of SOD backbones. Theoretically, we also present generalization analysis of SOD methods and provide evidence supporting the validity of our new evaluation protocols. Finally, comprehensive experiments speak to the efficacy of our proposed approach. The code is available at https://github.com/Ferry-Li/SI-SOD.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DivLogicEval: A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models</title>
<link>https://arxiv.org/abs/2509.15587</link>
<guid>https://arxiv.org/abs/2509.15587</guid>
<content:encoded><![CDATA[
arXiv:2509.15587v1 Announce Type: cross 
Abstract: Logic reasoning in natural language has been recognized as an important measure of human intelligence for Large Language Models (LLMs). Popular benchmarks may entangle multiple reasoning skills and thus provide unfaithful evaluations on the logic reasoning skill. Meanwhile, existing logic reasoning benchmarks are limited in language diversity and their distributions are deviated from the distribution of an ideal logic reasoning benchmark, which may lead to biased evaluation results. This paper thereby proposes a new classical logic benchmark DivLogicEval, consisting of natural sentences composed of diverse statements in a counterintuitive way. To ensure a more reliable evaluation, we also introduce a new evaluation metric that mitigates the influence of bias and randomness inherent in LLMs. Through experiments, we demonstrate the extent to which logical reasoning is required to answer the questions in DivLogicEval and compare the performance of different popular LLMs in conducting logical reasoning.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SETrLUSI: Stochastic Ensemble Multi-Source Transfer Learning Using Statistical Invariant</title>
<link>https://arxiv.org/abs/2509.15593</link>
<guid>https://arxiv.org/abs/2509.15593</guid>
<content:encoded><![CDATA[
arXiv:2509.15593v1 Announce Type: cross 
Abstract: In transfer learning, a source domain often carries diverse knowledge, and different domains usually emphasize different types of knowledge. Different from handling only a single type of knowledge from all domains in traditional transfer learning methods, we introduce an ensemble learning framework with a weak mode of convergence in the form of Statistical Invariant (SI) for multi-source transfer learning, formulated as Stochastic Ensemble Multi-Source Transfer Learning Using Statistical Invariant (SETrLUSI). The proposed SI extracts and integrates various types of knowledge from both source and target domains, which not only effectively utilizes diverse knowledge but also accelerates the convergence process. Further, SETrLUSI incorporates stochastic SI selection, proportional source domain sampling, and target domain bootstrapping, which improves training efficiency while enhancing model stability. Experiments show that SETrLUSI has good convergence and outperforms related methods with a lower time cost.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Network-assisted Random Forest+</title>
<link>https://arxiv.org/abs/2509.15611</link>
<guid>https://arxiv.org/abs/2509.15611</guid>
<content:encoded><![CDATA[
arXiv:2509.15611v1 Announce Type: cross 
Abstract: Machine learning algorithms often assume that training samples are independent. When data points are connected by a network, the induced dependency between samples is both a challenge, reducing effective sample size, and an opportunity to improve prediction by leveraging information from network neighbors. Multiple methods taking advantage of this opportunity are now available, but many, including graph neural networks, are not easily interpretable, limiting their usefulness for understanding how a model makes its predictions. Others, such as network-assisted linear regression, are interpretable but often yield substantially worse prediction performance. We bridge this gap by proposing a family of flexible network-assisted models built upon a generalization of random forests (RF+), which achieves highly-competitive prediction accuracy and can be interpreted through feature importance measures. In particular, we develop a suite of interpretation tools that enable practitioners to not only identify important features that drive model predictions, but also quantify the importance of the network contribution to prediction. Importantly, we provide both global and local importance measures as well as sample influence measures to assess the impact of a given observation. This suite of tools broadens the scope and applicability of network-assisted machine learning for high-impact problems where interpretability and transparency are essential.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Concept Unlearning in Large Language Models via Self-Constructed Knowledge Triplets</title>
<link>https://arxiv.org/abs/2509.15621</link>
<guid>https://arxiv.org/abs/2509.15621</guid>
<content:encoded><![CDATA[
arXiv:2509.15621v1 Announce Type: cross 
Abstract: Machine Unlearning (MU) has recently attracted considerable attention as a solution to privacy and copyright issues in large language models (LLMs). Existing MU methods aim to remove specific target sentences from an LLM while minimizing damage to unrelated knowledge. However, these approaches require explicit target sentences and do not support removing broader concepts, such as persons or events. To address this limitation, we introduce Concept Unlearning (CU) as a new requirement for LLM unlearning. We leverage knowledge graphs to represent the LLM's internal knowledge and define CU as removing the forgetting target nodes and associated edges. This graph-based formulation enables a more intuitive unlearning and facilitates the design of more effective methods. We propose a novel method that prompts the LLM to generate knowledge triplets and explanatory sentences about the forgetting target and applies the unlearning process to these representations. Our approach enables more precise and comprehensive concept removal by aligning the unlearning process with the LLM's internal knowledge representations. Experiments on real-world and synthetic datasets demonstrate that our method effectively achieves concept-level unlearning while preserving unrelated knowledge.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse-Autoencoder-Guided Internal Representation Unlearning for Large Language Models</title>
<link>https://arxiv.org/abs/2509.15631</link>
<guid>https://arxiv.org/abs/2509.15631</guid>
<content:encoded><![CDATA[
arXiv:2509.15631v1 Announce Type: cross 
Abstract: As large language models (LLMs) are increasingly deployed across various applications, privacy and copyright concerns have heightened the need for more effective LLM unlearning techniques. Many existing unlearning methods aim to suppress undesirable outputs through additional training (e.g., gradient ascent), which reduces the probability of generating such outputs. While such suppression-based approaches can control model outputs, they may not eliminate the underlying knowledge embedded in the model's internal activations; muting a response is not the same as forgetting it. Moreover, such suppression-based methods often suffer from model collapse. To address these issues, we propose a novel unlearning method that directly intervenes in the model's internal activations. In our formulation, forgetting is defined as a state in which the activation of a forgotten target is indistinguishable from that of ``unknown'' entities. Our method introduces an unlearning objective that modifies the activation of the target entity away from those of known entities and toward those of unknown entities in a sparse autoencoder latent space. By aligning the target's internal activation with those of unknown entities, we shift the model's recognition of the target entity from ``known'' to ``unknown'', achieving genuine forgetting while avoiding over-suppression and model collapse. Empirically, we show that our method effectively aligns the internal activations of the forgotten target, a result that the suppression-based approaches do not reliably achieve. Additionally, our method effectively reduces the model's recall of target knowledge in question-answering tasks without significant damage to the non-target knowledge.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ORIC: Benchmarking Object Recognition in Incongruous Context for Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2509.15695</link>
<guid>https://arxiv.org/abs/2509.15695</guid>
<content:encoded><![CDATA[
arXiv:2509.15695v1 Announce Type: cross 
Abstract: Large Vision-Language Models (LVLMs) have made significant strides in image caption, visual question answering, and robotics by integrating visual and textual information. However, they remain prone to errors in incongruous contexts, where objects appear unexpectedly or are absent when contextually expected. This leads to two key recognition failures: object misidentification and hallucination. To systematically examine this issue, we introduce the Object Recognition in Incongruous Context Benchmark (ORIC), a novel benchmark that evaluates LVLMs in scenarios where object-context relationships deviate from expectations. ORIC employs two key strategies: (1) LLM-guided sampling, which identifies objects that are present but contextually incongruous, and (2) CLIP-guided sampling, which detects plausible yet nonexistent objects that are likely to be hallucinated, thereby creating an incongruous context. Evaluating 18 LVLMs and two open-vocabulary detection models, our results reveal significant recognition gaps, underscoring the challenges posed by contextual incongruity. This work provides critical insights into LVLMs' limitations and encourages further research on context-aware object recognition.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Triplet Loss Based Quantum Encoding for Class Separability</title>
<link>https://arxiv.org/abs/2509.15705</link>
<guid>https://arxiv.org/abs/2509.15705</guid>
<content:encoded><![CDATA[
arXiv:2509.15705v1 Announce Type: cross 
Abstract: An efficient and data-driven encoding scheme is proposed to enhance the performance of variational quantum classifiers. This encoding is specially designed for complex datasets like images and seeks to help the classification task by producing input states that form well-separated clusters in the Hilbert space according to their classification labels. The encoding circuit is trained using a triplet loss function inspired by classical facial recognition algorithms, and class separability is measured via average trace distances between the encoded density matrices. Benchmark tests performed on various binary classification tasks on MNIST and MedMNIST datasets demonstrate considerable improvement over amplitude encoding with the same VQC structure while requiring a much lower circuit depth.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impact of Single Rotations and Entanglement Topologies in Quantum Neural Networks</title>
<link>https://arxiv.org/abs/2509.15722</link>
<guid>https://arxiv.org/abs/2509.15722</guid>
<content:encoded><![CDATA[
arXiv:2509.15722v1 Announce Type: cross 
Abstract: In this work, an analysis of the performance of different Variational Quantum Circuits is presented, investigating how it changes with respect to entanglement topology, adopted gates, and Quantum Machine Learning tasks to be performed. The objective of the analysis is to identify the optimal way to construct circuits for Quantum Neural Networks. In the presented experiments, two types of circuits are used: one with alternating layers of rotations and entanglement, and the other, similar to the first one, but with an additional final layer of rotations. As rotation layers, all combinations of one and two rotation sequences are considered. Four different entanglement topologies are compared: linear, circular, pairwise, and full. Different tasks are considered, namely the generation of probability distributions and images, and image classification. Achieved results are correlated with the expressibility and entanglement capability of the different circuits to understand how these features affect performance.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training Variational Quantum Circuits Using Particle Swarm Optimization</title>
<link>https://arxiv.org/abs/2509.15726</link>
<guid>https://arxiv.org/abs/2509.15726</guid>
<content:encoded><![CDATA[
arXiv:2509.15726v1 Announce Type: cross 
Abstract: In this work, the Particle Swarm Optimization (PSO) algorithm has been used to train various Variational Quantum Circuits (VQCs). This approach is motivated by the fact that commonly used gradient-based optimization methods can suffer from the barren plateaus problem. PSO is a stochastic optimization technique inspired by the collective behavior of a swarm of birds. The dimension of the swarm, the number of iterations of the algorithm, and the number of trainable parameters can be set. In this study, PSO has been used to train the entire structure of VQCs, allowing it to select which quantum gates to apply, the target qubits, and the rotation angle, in case a rotation is chosen. The algorithm is restricted to choosing from four types of gates: Rx, Ry, Rz, and CNOT. The proposed optimization approach has been tested on various datasets of the MedMNIST, which is a collection of biomedical image datasets designed for image classification tasks. Performance has been compared with the results achieved by classical stochastic gradient descent applied to a predefined VQC. The results show that the PSO can achieve comparable or even better classification accuracy across multiple datasets, despite the PSO using a lower number of quantum gates than the VQC used with gradient descent optimization.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UPRPRC: Unified Pipeline for Reproducing Parallel Resources -- Corpus from the United Nations</title>
<link>https://arxiv.org/abs/2509.15789</link>
<guid>https://arxiv.org/abs/2509.15789</guid>
<content:encoded><![CDATA[
arXiv:2509.15789v1 Announce Type: cross 
Abstract: The quality and accessibility of multilingual datasets are crucial for advancing machine translation. However, previous corpora built from United Nations documents have suffered from issues such as opaque process, difficulty of reproduction, and limited scale. To address these challenges, we introduce a complete end-to-end solution, from data acquisition via web scraping to text alignment. The entire process is fully reproducible, with a minimalist single-machine example and optional distributed computing steps for scalability. At its core, we propose a new Graph-Aided Paragraph Alignment (GAPA) algorithm for efficient and flexible paragraph-level alignment. The resulting corpus contains over 713 million English tokens, more than doubling the scale of prior work. To the best of our knowledge, this represents the largest publicly available parallel corpus composed entirely of human-translated, non-AI-generated content. Our code and corpus are accessible under the MIT License.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Phase Transition for Stochastic Block Model with more than $\sqrt{n}$ Communities</title>
<link>https://arxiv.org/abs/2509.15822</link>
<guid>https://arxiv.org/abs/2509.15822</guid>
<content:encoded><![CDATA[
arXiv:2509.15822v1 Announce Type: cross 
Abstract: Predictions from statistical physics postulate that recovery of the communities in Stochastic Block Model (SBM) is possible in polynomial time above, and only above, the Kesten-Stigum (KS) threshold. This conjecture has given rise to a rich literature, proving that non-trivial community recovery is indeed possible in SBM above the KS threshold, as long as the number $K$ of communities remains smaller than $\sqrt{n}$, where $n$ is the number of nodes in the observed graph. Failure of low-degree polynomials below the KS threshold was also proven when $K=o(\sqrt{n})$.
  When $K\geq \sqrt{n}$, Chin et al.(2025) recently prove that, in a sparse regime, community recovery in polynomial time is possible below the KS threshold by counting non-backtracking paths. This breakthrough result lead them to postulate a new threshold for the many communities regime $K\geq \sqrt{n}$. In this work, we provide evidences that confirm their conjecture for $K\geq \sqrt{n}$:
  1- We prove that, for any density of the graph, low-degree polynomials fail to recover communities below the threshold postulated by Chin et al.(2025);
  2- We prove that community recovery is possible in polynomial time above the postulated threshold, not only in the sparse regime of~Chin et al., but also in some (but not all) moderately sparse regimes by essentially counting clique occurence in the observed graph.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Product Deduplication in E-Commerce with Multimodal Embeddings</title>
<link>https://arxiv.org/abs/2509.15858</link>
<guid>https://arxiv.org/abs/2509.15858</guid>
<content:encoded><![CDATA[
arXiv:2509.15858v1 Announce Type: cross 
Abstract: In large scale e-commerce marketplaces, duplicate product listings frequently cause consumer confusion and operational inefficiencies, degrading trust on the platform and increasing costs. Traditional keyword-based search methodologies falter in accurately identifying duplicates due to their reliance on exact textual matches, neglecting semantic similarities inherent in product titles. To address these challenges, we introduce a scalable, multimodal product deduplication designed specifically for the e-commerce domain. Our approach employs a domain-specific text model grounded in BERT architecture in conjunction with MaskedAutoEncoders for image representations. Both of these architectures are augmented with dimensionality reduction techniques to produce compact 128-dimensional embeddings without significant information loss. Complementing this, we also developed a novel decider model that leverages both text and image vectors. By integrating these feature extraction mechanisms with Milvus, an optimized vector database, our system can facilitate efficient and high-precision similarity searches across extensive product catalogs exceeding 200 million items with just 100GB of system RAM consumption. Empirical evaluations demonstrate that our matching system achieves a macro-average F1 score of 0.90, outperforming third-party solutions which attain an F1 score of 0.83. Our findings show the potential of combining domain-specific adaptations with state-of-the-art machine learning techniques to mitigate duplicate listings in large-scale e-commerce environments.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepMech: A Machine Learning Framework for Chemical Reaction Mechanism Prediction</title>
<link>https://arxiv.org/abs/2509.15872</link>
<guid>https://arxiv.org/abs/2509.15872</guid>
<content:encoded><![CDATA[
arXiv:2509.15872v1 Announce Type: cross 
Abstract: Prediction of complete step-by-step chemical reaction mechanisms (CRMs) remains a major challenge. Whereas the traditional approaches in CRM tasks rely on expert-driven experiments or costly quantum chemical computations, contemporary deep learning (DL) alternatives ignore key intermediates and mechanistic steps and often suffer from hallucinations. We present DeepMech, an interpretable graph-based DL framework employing atom- and bond-level attention, guided by generalized templates of mechanistic operations (TMOps), to generate CRMs. Trained on our curated ReactMech dataset (~30K CRMs with 100K atom-mapped and mass-balanced elementary steps), DeepMech achieves 98.98+/-0.12% accuracy in predicting elementary steps and 95.94+/-0.21% in complete CRM tasks, besides maintaining high fidelity even in out-of-distribution scenarios as well as in predicting side and/or byproducts. Extension to multistep CRMs relevant to prebiotic chemistry, demonstrates the ability of DeepMech in effectively reconstructing pathways from simple primordial substrates to complex biomolecules such as serine and aldopentose. Attention analysis identifies reactive atoms/bonds in line with chemical intuition, rendering our model interpretable and suitable for reaction design.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Flow-rate-conserving CNN-based Domain Decomposition Method for Blood Flow Simulations</title>
<link>https://arxiv.org/abs/2509.15900</link>
<guid>https://arxiv.org/abs/2509.15900</guid>
<content:encoded><![CDATA[
arXiv:2509.15900v1 Announce Type: cross 
Abstract: This work aims to predict blood flow with non-Newtonian viscosity in stenosed arteries using convolutional neural network (CNN) surrogate models. An alternating Schwarz domain decomposition method is proposed which uses CNN-based subdomain solvers. A universal subdomain solver (USDS) is trained on a single, fixed geometry and then applied for each subdomain solve in the Schwarz method. Results for two-dimensional stenotic arteries of varying shape and length for different inflow conditions are presented and statistically evaluated. One key finding, when using a limited amount of training data, is the need to implement a USDS which preserves some of the physics, as, in our case, flow rate conservation. A physics-aware approach outperforms purely data-driven USDS, delivering improved subdomain solutions and preventing overshooting or undershooting of the global solution during the Schwarz iterations, thereby leading to more reliable convergence.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Score: Uncertainty-Calibrated LLMs for Automated Essay Assessment</title>
<link>https://arxiv.org/abs/2509.15926</link>
<guid>https://arxiv.org/abs/2509.15926</guid>
<content:encoded><![CDATA[
arXiv:2509.15926v1 Announce Type: cross 
Abstract: Automated Essay Scoring (AES) systems now reach near human agreement on some public benchmarks, yet real-world adoption, especially in high-stakes examinations, remains limited. A principal obstacle is that most models output a single score without any accompanying measure of confidence or explanation. We address this gap with conformal prediction, a distribution-free wrapper that equips any classifier with set-valued outputs and formal coverage guarantees. Two open-source large language models (Llama-3 8B and Qwen-2.5 3B) are fine-tuned on three diverse corpora (ASAP, TOEFL11, Cambridge-FCE) and calibrated at a 90 percent risk level. Reliability is assessed with UAcc, an uncertainty-aware accuracy that rewards models for being both correct and concise. To our knowledge, this is the first work to combine conformal prediction and UAcc for essay scoring. The calibrated models consistently meet the coverage target while keeping prediction sets compact, indicating that open-source, mid-sized LLMs can already support teacher-in-the-loop AES; we discuss scaling and broader user studies as future work.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Missing Piece: A Case for Pre-Training in 3D Medical Object Detection</title>
<link>https://arxiv.org/abs/2509.15947</link>
<guid>https://arxiv.org/abs/2509.15947</guid>
<content:encoded><![CDATA[
arXiv:2509.15947v1 Announce Type: cross 
Abstract: Large-scale pre-training holds the promise to advance 3D medical object detection, a crucial component of accurate computer-aided diagnosis. Yet, it remains underexplored compared to segmentation, where pre-training has already demonstrated significant benefits. Existing pre-training approaches for 3D object detection rely on 2D medical data or natural image pre-training, failing to fully leverage 3D volumetric information. In this work, we present the first systematic study of how existing pre-training methods can be integrated into state-of-the-art detection architectures, covering both CNNs and Transformers. Our results show that pre-training consistently improves detection performance across various tasks and datasets. Notably, reconstruction-based self-supervised pre-training outperforms supervised pre-training, while contrastive pre-training provides no clear benefit for 3D medical object detection. Our code is publicly available at: https://github.com/MIC-DKFZ/nnDetection-finetuning.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compose Yourself: Average-Velocity Flow Matching for One-Step Speech Enhancement</title>
<link>https://arxiv.org/abs/2509.15952</link>
<guid>https://arxiv.org/abs/2509.15952</guid>
<content:encoded><![CDATA[
arXiv:2509.15952v1 Announce Type: cross 
Abstract: Diffusion and flow matching (FM) models have achieved remarkable progress in speech enhancement (SE), yet their dependence on multi-step generation is computationally expensive and vulnerable to discretization errors. Recent advances in one-step generative modeling, particularly MeanFlow, provide a promising alternative by reformulating dynamics through average velocity fields. In this work, we present COSE, a one-step FM framework tailored for SE. To address the high training overhead of Jacobian-vector product (JVP) computations in MeanFlow, we introduce a velocity composition identity to compute average velocity efficiently, eliminating expensive computation while preserving theoretical consistency and achieving competitive enhancement quality. Extensive experiments on standard benchmarks show that COSE delivers up to 5x faster sampling and reduces training cost by 40%, all without compromising speech quality. Code is available at https://github.com/ICDM-UESTC/COSE.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Localmax dynamics for attention in transformers and its asymptotic behavior</title>
<link>https://arxiv.org/abs/2509.15958</link>
<guid>https://arxiv.org/abs/2509.15958</guid>
<content:encoded><![CDATA[
arXiv:2509.15958v1 Announce Type: cross 
Abstract: We introduce a new discrete-time attention model, termed the localmax dynamics, which interpolates between the classic softmax dynamics and the hardmax dynamics, where only the tokens that maximize the influence toward a given token have a positive weight. As in hardmax, uniform weights are determined by a parameter controlling neighbor influence, but the key extension lies in relaxing neighborhood interactions through an alignment-sensitivity parameter, which allows controlled deviations from pure hardmax behavior. As we prove, while the convex hull of the token states still converges to a convex polytope, its structure can no longer be fully described by a maximal alignment set, prompting the introduction of quiescent sets to capture the invariant behavior of tokens near vertices. We show that these sets play a key role in understanding the asymptotic behavior of the system, even under time-varying alignment sensitivity parameters. We further show that localmax dynamics does not exhibit finite-time convergence and provide results for vanishing, nonzero, time-varying alignment-sensitivity parameters, recovering the limiting behavior of hardmax as a by-product. Finally, we adapt Lyapunov-based methods from classical opinion dynamics, highlighting their limitations in the asymmetric setting of localmax interactions and outlining directions for future research.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VoXtream: Full-Stream Text-to-Speech with Extremely Low Latency</title>
<link>https://arxiv.org/abs/2509.15969</link>
<guid>https://arxiv.org/abs/2509.15969</guid>
<content:encoded><![CDATA[
arXiv:2509.15969v1 Announce Type: cross 
Abstract: We present VoXtream, a fully autoregressive, zero-shot streaming text-to-speech (TTS) system for real-time use that begins speaking from the first word. VoXtream directly maps incoming phonemes to audio tokens using a monotonic alignment scheme and a dynamic look-ahead that does not delay onset. Built around an incremental phoneme transformer, a temporal transformer predicting semantic and duration tokens, and a depth transformer producing acoustic tokens, VoXtream achieves, to our knowledge, the lowest initial delay among publicly available streaming TTS: 102 ms on GPU. Despite being trained on a mid-scale 9k-hour corpus, it matches or surpasses larger baselines on several metrics, while delivering competitive quality in both output- and full-streaming settings. Demo and code are available at https://herimor.github.io/voxtream.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BEFT: Bias-Efficient Fine-Tuning of Language Models</title>
<link>https://arxiv.org/abs/2509.15974</link>
<guid>https://arxiv.org/abs/2509.15974</guid>
<content:encoded><![CDATA[
arXiv:2509.15974v1 Announce Type: cross 
Abstract: Fine-tuning all-bias-terms stands out among various parameter-efficient fine-tuning (PEFT) techniques, owing to its out-of-the-box usability and competitive performance, especially in low-data regimes. Bias-only fine-tuning has the potential for unprecedented parameter efficiency. However, the link between fine-tuning different bias terms (i.e., bias terms in the query, key, or value projections) and downstream performance remains unclear. The existing approaches, e.g., based on the magnitude of bias change or empirical Fisher information, provide limited guidance for selecting the particular bias term for effective fine-tuning. In this paper, we propose an approach for selecting the bias term to be fine-tuned, forming the foundation of our bias-efficient fine-tuning (BEFT). We extensively evaluate our bias-efficient approach against other bias-selection approaches, across a wide range of large language models (LLMs) spanning encoder-only and decoder-only architectures from 110M to 6.7B parameters. Our results demonstrate the effectiveness and superiority of our bias-efficient approach on diverse downstream tasks, including classification, multiple-choice, and generation tasks.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-free algorithms for fast node clustering in SBM type graphs and application to social role inference in animals</title>
<link>https://arxiv.org/abs/2509.15989</link>
<guid>https://arxiv.org/abs/2509.15989</guid>
<content:encoded><![CDATA[
arXiv:2509.15989v1 Announce Type: cross 
Abstract: We propose a novel family of model-free algorithms for node clustering and parameter inference in graphs generated from the Stochastic Block Model (SBM), a fundamental framework in community detection. Drawing inspiration from the Lloyd algorithm for the $k$-means problem, our approach extends to SBMs with general edge weight distributions. We establish the consistency of our estimator under a natural identifiability condition. Through extensive numerical experiments, we benchmark our methods against state-of-the-art techniques, demonstrating significantly faster computation times with the lower order of estimation error. Finally, we validate the practical relevance of our algorithms by applying them to empirical network data from behavioral ecology.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Enhanced Anomaly Detection for ADS-B Data using Hybrid Deep Learning</title>
<link>https://arxiv.org/abs/2509.15991</link>
<guid>https://arxiv.org/abs/2509.15991</guid>
<content:encoded><![CDATA[
arXiv:2509.15991v1 Announce Type: cross 
Abstract: The emerging field of Quantum Machine Learning (QML) has shown promising advantages in accelerating processing speed and effectively handling the high dimensionality associated with complex datasets. Quantum Computing (QC) enables more efficient data manipulation through the quantum properties of superposition and entanglement. In this paper, we present a novel approach combining quantum and classical machine learning techniques to explore the impact of quantum properties for anomaly detection in Automatic Dependent Surveillance-Broadcast (ADS-B) data. We compare the performance of a Hybrid-Fully Connected Quantum Neural Network (H-FQNN) with different loss functions and use a publicly available ADS-B dataset to evaluate the performance. The results demonstrate competitive performance in detecting anomalies, with accuracies ranging from 90.17% to 94.05%, comparable to the performance of a traditional Fully Connected Neural Network (FNN) model, which achieved accuracies between 91.50% and 93.37%.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Reinforcement Learning with Dynamic-Circuit Qubit Reuse and Grover-Based Trajectory Optimization</title>
<link>https://arxiv.org/abs/2509.16002</link>
<guid>https://arxiv.org/abs/2509.16002</guid>
<content:encoded><![CDATA[
arXiv:2509.16002v1 Announce Type: cross 
Abstract: A fully quantum reinforcement learning framework is developed that integrates a quantum Markov decision process, dynamic circuit-based qubit reuse, and Grover's algorithm for trajectory optimization. The framework encodes states, actions, rewards, and transitions entirely within the quantum domain, enabling parallel exploration of state-action sequences through superposition and eliminating classical subroutines. Dynamic circuit operations, including mid-circuit measurement and reset, allow reuse of the same physical qubits across multiple agent-environment interactions, reducing qubit requirements from 7*T to 7 for T time steps while preserving logical continuity. Quantum arithmetic computes trajectory returns, and Grover's search is applied to the superposition of these evaluated trajectories to amplify the probability of measuring those with the highest return, thereby accelerating the identification of the optimal policy. Simulations demonstrate that the dynamic-circuit-based implementation preserves trajectory fidelity while reducing qubit usage by 66 percent relative to the static design. Experimental deployment on IBM Heron-class quantum hardware confirms that the framework operates within the constraints of current quantum processors and validates the feasibility of fully quantum multi-step reinforcement learning under noisy intermediate-scale quantum conditions. This framework advances the scalability and practical application of quantum reinforcement learning for large-scale sequential decision-making tasks.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Methods for Permutation Circuit Synthesis Across Generic Topologies</title>
<link>https://arxiv.org/abs/2509.16020</link>
<guid>https://arxiv.org/abs/2509.16020</guid>
<content:encoded><![CDATA[
arXiv:2509.16020v1 Announce Type: cross 
Abstract: This paper investigates artificial intelligence (AI) methodologies for the synthesis and transpilation of permutation circuits across generic topologies. Our approach uses Reinforcement Learning (RL) techniques to achieve near-optimal synthesis of permutation circuits up to 25 qubits. Rather than developing specialized models for individual topologies, we train a foundational model on a generic rectangular lattice, and employ masking mechanisms to dynamically select subsets of topologies during the synthesis. This enables the synthesis of permutation circuits on any topology that can be embedded within the rectangular lattice, without the need to re-train the model. In this paper we show results for 5x5 lattice and compare them to previous AI topology-oriented models and classical methods, showing that they outperform classical heuristics, and match previous specialized AI models, and performs synthesis even for topologies that were not seen during training. We further show that the model can be fine tuned to strengthen the performance for selected topologies of interest. This methodology allows a single trained model to efficiently synthesize circuits across diverse topologies, allowing its practical integration into transpilation workflows.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What is a good matching of probability measures? A counterfactual lens on transport maps</title>
<link>https://arxiv.org/abs/2509.16027</link>
<guid>https://arxiv.org/abs/2509.16027</guid>
<content:encoded><![CDATA[
arXiv:2509.16027v1 Announce Type: cross 
Abstract: Coupling probability measures lies at the core of many problems in statistics and machine learning, from domain adaptation to transfer learning and causal inference. Yet, even when restricted to deterministic transports, such couplings are not identifiable: two atomless marginals admit infinitely many transport maps. The common recourse to optimal transport, motivated by cost minimization and cyclical monotonicity, obscures the fact that several distinct notions of multivariate monotone matchings coexist. In this work, we first carry a comparative analysis of three constructions of transport maps: cyclically monotone, quantile-preserving and triangular monotone maps. We establish necessary and sufficient conditions for their equivalence, thereby clarifying their respective structural properties. In parallel, we formulate counterfactual reasoning within the framework of structural causal models as a problem of selecting transport maps between fixed marginals, which makes explicit the role of untestable assumptions in counterfactual reasoning. Then, we are able to connect these two perspectives by identifying conditions on causal graphs and structural equations under which counterfactual maps coincide with classical statistical transports. In this way, we delineate the circumstances in which causal assumptions support the use of a specific structure of transport map. Taken together, our results aim to enrich the theoretical understanding of families of transport maps and to clarify their possible causal interpretations. We hope this work contributes to establishing new bridges between statistical transport and causal inference.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention Schema-based Attention Control (ASAC): A Cognitive-Inspired Approach for Attention Management in Transformers</title>
<link>https://arxiv.org/abs/2509.16058</link>
<guid>https://arxiv.org/abs/2509.16058</guid>
<content:encoded><![CDATA[
arXiv:2509.16058v1 Announce Type: cross 
Abstract: Attention mechanisms have become integral in AI, significantly enhancing model performance and scalability by drawing inspiration from human cognition. Concurrently, the Attention Schema Theory (AST) in cognitive science posits that individuals manage their attention by creating a model of the attention itself, effectively allocating cognitive resources. Inspired by AST, we introduce ASAC (Attention Schema-based Attention Control), which integrates the attention schema concept into artificial neural networks. Our initial experiments focused on embedding the ASAC module within transformer architectures. This module employs a Vector-Quantized Variational AutoEncoder (VQVAE) as both an attention abstractor and controller, facilitating precise attention management. By explicitly modeling attention allocation, our approach aims to enhance system efficiency. We demonstrate ASAC's effectiveness in both the vision and NLP domains, highlighting its ability to improve classification accuracy and expedite the learning process. Our experiments with vision transformers across various datasets illustrate that the attention controller not only boosts classification accuracy but also accelerates learning. Furthermore, we have demonstrated the model's robustness and generalization capabilities across noisy and out-of-distribution datasets. In addition, we have showcased improved performance in multi-task settings. Quick experiments reveal that the attention schema-based module enhances resilience to adversarial attacks, optimizes attention to improve learning efficiency, and facilitates effective transfer learning and learning from fewer examples. These promising results establish a connection between cognitive science and machine learning, shedding light on the efficient utilization of attention mechanisms in AI systems.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRISM: Probabilistic and Robust Inverse Solver with Measurement-Conditioned Diffusion Prior for Blind Inverse Problems</title>
<link>https://arxiv.org/abs/2509.16106</link>
<guid>https://arxiv.org/abs/2509.16106</guid>
<content:encoded><![CDATA[
arXiv:2509.16106v1 Announce Type: cross 
Abstract: Diffusion models are now commonly used to solve inverse problems in computational imaging. However, most diffusion-based inverse solvers require complete knowledge of the forward operator to be used. In this work, we introduce a novel probabilistic and robust inverse solver with measurement-conditioned diffusion prior (PRISM) to effectively address blind inverse problems. PRISM offers a technical advancement over current methods by incorporating a powerful measurement-conditioned diffusion model into a theoretically principled posterior sampling scheme. Experiments on blind image deblurring validate the effectiveness of the proposed method, demonstrating the superior performance of PRISM over state-of-the-art baselines in both image and blur kernel recovery.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Bugs Linger: A Study of Anomalous Resolution Time Outliers and Their Themes</title>
<link>https://arxiv.org/abs/2509.16140</link>
<guid>https://arxiv.org/abs/2509.16140</guid>
<content:encoded><![CDATA[
arXiv:2509.16140v1 Announce Type: cross 
Abstract: Efficient bug resolution is critical for maintaining software quality and user satisfaction. However, specific bug reports experience unusually long resolution times, which may indicate underlying process inefficiencies or complex issues. This study presents a comprehensive analysis of bug resolution anomalies across seven prominent open-source repositories: Cassandra, Firefox, Hadoop, HBase, SeaMonkey, Spark, and Thunderbird. Utilizing statistical methods such as Z-score and Interquartile Range (IQR), we identify anomalies in bug resolution durations. To understand the thematic nature of these anomalies, we apply Term Frequency-Inverse Document Frequency (TF-IDF) for textual feature extraction and KMeans clustering to group similar bug summaries. Our findings reveal consistent patterns across projects, with anomalies often clustering around test failures, enhancement requests, and user interface issues. This approach provides actionable insights for project maintainers to prioritize and effectively address long-standing bugs.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Query-Efficient Locally Private Hypothesis Selection via the Scheffe Graph</title>
<link>https://arxiv.org/abs/2509.16180</link>
<guid>https://arxiv.org/abs/2509.16180</guid>
<content:encoded><![CDATA[
arXiv:2509.16180v1 Announce Type: cross 
Abstract: We propose an algorithm with improved query-complexity for the problem of hypothesis selection under local differential privacy constraints. Given a set of $k$ probability distributions $Q$, we describe an algorithm that satisfies local differential privacy, performs $\tilde{O}(k^{3/2})$ non-adaptive queries to individuals who each have samples from a probability distribution $p$, and outputs a probability distribution from the set $Q$ which is nearly the closest to $p$. Previous algorithms required either $\Omega(k^2)$ queries or many rounds of interactive queries.
  Technically, we introduce a new object we dub the Scheff\'e graph, which captures structure of the differences between distributions in $Q$, and may be of more broad interest for hypothesis selection tasks.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Atomic Fine Structure Determination with Graph Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.16184</link>
<guid>https://arxiv.org/abs/2509.16184</guid>
<content:encoded><![CDATA[
arXiv:2509.16184v1 Announce Type: cross 
Abstract: Atomic data determined by analysis of observed atomic spectra are essential for plasma diagnostics. For each low-ionisation open d- and f-subshell atomic species, around $10^3$ fine structure level energies can be determined through years of analysis of $10^4$ observable spectral lines. We propose the automation of this task by casting the analysis procedure as a Markov decision process and solving it by graph reinforcement learning using reward functions learned on historical human decisions. In our evaluations on existing spectral line lists and theoretical calculations for Co II and Nd II-III, hundreds of level energies were computed within hours, agreeing with published values in 95% of cases for Co II and 54-87% for Nd II-III. As the current efficiency in atomic fine structure determination struggles to meet growing atomic data demands from astronomy and fusion science, our new artificial intelligence approach sets the stage for closing this gap.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Generative Adversarial Autoencoders: Learning latent representations for quantum data generation</title>
<link>https://arxiv.org/abs/2509.16186</link>
<guid>https://arxiv.org/abs/2509.16186</guid>
<content:encoded><![CDATA[
arXiv:2509.16186v1 Announce Type: cross 
Abstract: In this work, we introduce the Quantum Generative Adversarial Autoencoder (QGAA), a quantum model for generation of quantum data. The QGAA consists of two components: (a) Quantum Autoencoder (QAE) to compress quantum states, and (b) Quantum Generative Adversarial Network (QGAN) to learn the latent space of the trained QAE. This approach imparts the QAE with generative capabilities. The utility of QGAA is demonstrated in two representative scenarios: (a) generation of pure entangled states, and (b) generation of parameterized molecular ground states for H$_2$ and LiH. The average errors in the energies estimated by the trained QGAA are 0.02 Ha for H$_2$ and 0.06 Ha for LiH in simulations upto 6 qubits. These results illustrate the potential of QGAA for quantum state generation, quantum chemistry, and near-term quantum machine learning applications.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MatchFixAgent: Language-Agnostic Autonomous Repository-Level Code Translation Validation and Repair</title>
<link>https://arxiv.org/abs/2509.16187</link>
<guid>https://arxiv.org/abs/2509.16187</guid>
<content:encoded><![CDATA[
arXiv:2509.16187v1 Announce Type: cross 
Abstract: Code translation transforms source code from one programming language (PL) to another. Validating the functional equivalence of translation and repairing, if necessary, are critical steps in code translation. Existing automated validation and repair approaches struggle to generalize to many PLs due to high engineering overhead, and they rely on existing and often inadequate test suites, which results in false claims of equivalence and ineffective translation repair. We develop MatchFixAgent, a large language model (LLM)-based, PL-agnostic framework for equivalence validation and repair of translations. MatchFixAgent features a multi-agent architecture that divides equivalence validation into several sub-tasks to ensure thorough and consistent semantic analysis of the translation. Then it feeds this analysis to test agent to write and execute tests. Upon observing a test failure, the repair agent attempts to fix the translation bug. The final (in)equivalence decision is made by the verdict agent, considering semantic analyses and test execution results.
  We compare MatchFixAgent's validation and repair results with four repository-level code translation techniques. We use 2,219 translation pairs from their artifacts, which cover 6 PL pairs, and are collected from 24 GitHub projects totaling over 900K lines of code. Our results demonstrate that MatchFixAgent produces (in)equivalence verdicts for 99.2% of translation pairs, with the same equivalence validation result as prior work on 72.8% of them. When MatchFixAgent's result disagrees with prior work, we find that 60.7% of the time MatchFixAgent's result is actually correct. In addition, we show that MatchFixAgent can repair 50.6% of inequivalent translation, compared to prior work's 18.5%. This demonstrates that MatchFixAgent is far more adaptable to many PL pairs than prior work, while producing highly accurate validation results.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FocalCodec-Stream: Streaming Low-Bitrate Speech Coding via Causal Distillation</title>
<link>https://arxiv.org/abs/2509.16195</link>
<guid>https://arxiv.org/abs/2509.16195</guid>
<content:encoded><![CDATA[
arXiv:2509.16195v1 Announce Type: cross 
Abstract: Neural audio codecs are a fundamental component of modern generative audio pipelines. Although recent codecs achieve strong low-bitrate reconstruction and provide powerful representations for downstream tasks, most are non-streamable, limiting their use in real-time applications. We present FocalCodec-Stream, a hybrid codec based on focal modulation that compresses speech into a single binary codebook at 0.55 - 0.80 kbps with a theoretical latency of 80 ms. Our approach combines multi-stage causal distillation of WavLM with targeted architectural improvements, including a lightweight refiner module that enhances quality under latency constraints. Experiments show that FocalCodec-Stream outperforms existing streamable codecs at comparable bitrates, while preserving both semantic and acoustic information. The result is a favorable trade-off between reconstruction quality, downstream task performance, latency, and efficiency. Code and checkpoints will be released at https://github.com/lucadellalib/focalcodec.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer</title>
<link>https://arxiv.org/abs/2509.16197</link>
<guid>https://arxiv.org/abs/2509.16197</guid>
<content:encoded><![CDATA[
arXiv:2509.16197v1 Announce Type: cross 
Abstract: Unified multimodal Large Language Models (LLMs) that can both understand and generate visual content hold immense potential. However, existing open-source models often suffer from a performance trade-off between these capabilities. We present Manzano, a simple and scalable unified framework that substantially reduces this tension by coupling a hybrid image tokenizer with a well-curated training recipe. A single shared vision encoder feeds two lightweight adapters that produce continuous embeddings for image-to-text understanding and discrete tokens for text-to-image generation within a common semantic space. A unified autoregressive LLM predicts high-level semantics in the form of text and image tokens, with an auxiliary diffusion decoder subsequently translating the image tokens into pixels. The architecture, together with a unified training recipe over understanding and generation data, enables scalable joint learning of both capabilities. Manzano achieves state-of-the-art results among unified models, and is competitive with specialist models, particularly on text-rich evaluation. Our studies show minimal task conflicts and consistent gains from scaling model size, validating our design choice of a hybrid tokenizer.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving the forecast accuracy of wind power by leveraging multiple hierarchical structure</title>
<link>https://arxiv.org/abs/2308.03472</link>
<guid>https://arxiv.org/abs/2308.03472</guid>
<content:encoded><![CDATA[
arXiv:2308.03472v4 Announce Type: replace 
Abstract: Renewable energy generation is of utmost importance for global decarbonization. Forecasting renewable energies, particularly wind energy, is challenging due to the inherent uncertainty in wind energy generation, which depends on weather conditions. Recent advances in hierarchical forecasting through reconciliation have demonstrated a significant increase in the quality of wind energy forecasts for short-term periods. We leverage the cross-sectional and temporal hierarchical structure of turbines in wind farms and build cross-temporal hierarchies to further investigate how integrated cross-sectional and temporal dimensions can add value to forecast accuracy in wind farms. We found that cross-temporal reconciliation was superior to individual cross-sectional reconciliation at multiple temporal aggregations. Additionally, machine learning based forecasts that were cross-temporally reconciled demonstrated high accuracy at coarser temporal granularities, which may encourage adoption for short-term wind forecasts. Empirically, we provide insights for decision-makers on the best methods for forecasting high-frequency wind data across different forecasting horizons and levels.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Negotiated Representations to Prevent Overfitting in Machine Learning Applications</title>
<link>https://arxiv.org/abs/2311.11410</link>
<guid>https://arxiv.org/abs/2311.11410</guid>
<content:encoded><![CDATA[
arXiv:2311.11410v2 Announce Type: replace 
Abstract: Overfitting is a phenomenon that occurs when a machine learning model is trained for too long and focused too much on the exact fitness of the training samples to the provided training labels and cannot keep track of the predictive rules that would be useful on the test data. This phenomenon is commonly attributed to memorization of particular samples, memorization of the noise, and forced fitness into a data set of limited samples by using a high number of neurons. While it is true that the model encodes various peculiarities as the training process continues, we argue that most of the overfitting occurs in the process of reconciling sharply defined membership ratios. In this study, we present an approach that increases the classification accuracy of machine learning models by allowing the model to negotiate output representations of the samples with previously determined class labels. By setting up a negotiation between the models interpretation of the inputs and the provided labels, we not only increased average classification accuracy but also decreased the rate of overfitting without applying any other regularization tricks. By implementing our negotiation paradigm approach to several low regime machine learning problems by generating overfitting scenarios from publicly available data sets such as CIFAR 10, CIFAR 100, and MNIST we have demonstrated that the proposed paradigm has more capacity than its intended purpose. We are sharing the experimental results and inviting the machine learning community to explore the limits of the proposed paradigm. We also aim to incentive the community to exploit the negotiation paradigm to overcome the learning related challenges in other research fields such as continual learning. The Python code of the experimental setup is uploaded to GitHub.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating Model Performance Under Covariate Shift Without Labels</title>
<link>https://arxiv.org/abs/2401.08348</link>
<guid>https://arxiv.org/abs/2401.08348</guid>
<content:encoded><![CDATA[
arXiv:2401.08348v4 Announce Type: replace 
Abstract: After deployment, machine learning models often experience performance degradation due to shifts in data distribution. It is challenging to assess post-deployment performance accurately when labels are missing or delayed. Existing proxy methods, such as data drift detection, fail to measure the effects of these shifts adequately. To address this, we introduce a new method for evaluating binary classification models on unlabeled tabular data that accurately estimates model performance under covariate shift and call it Probabilistic Adaptive Performance Estimation (PAPE). It can be applied to any performance metric defined with elements of the confusion matrix. Crucially, PAPE operates independently of the original model, relying only on its predictions and probability estimates, and does not need any assumptions about the nature of covariate shift, learning directly from data instead. We tested PAPE using over 900 dataset-model combinations from US census data, assessing its performance against several benchmarks through various metrics. Our findings show that PAPE outperforms other methodologies, making it a superior choice for estimating the performance of binary classification models.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Unified Theory of Exact Inference and Learning in Exponential Family Latent Variable Models</title>
<link>https://arxiv.org/abs/2404.19501</link>
<guid>https://arxiv.org/abs/2404.19501</guid>
<content:encoded><![CDATA[
arXiv:2404.19501v2 Announce Type: replace 
Abstract: Bayes' rule describes how to infer posterior beliefs about latent variables given observations, and inference is a critical step in learning algorithms for latent variable models (LVMs). Although there are exact algorithms for inference and learning for certain LVMs such as linear Gaussian models and mixture models, researchers must typically develop approximate inference and learning algorithms when applying novel LVMs. Here we study the line that separates LVMs that rely on approximation schemes from those that do not, and develop a general theory of exponential family LVMs for which inference and learning may be implemented exactly. Firstly, under mild assumptions about the exponential family form of the LVM, we derive a necessary and sufficient constraint on the parameters of the LVM under which the prior and posterior over the latent variables are in the same exponential family. We then show that a variety of well-known and novel models indeed have this constrained, exponential family form. Finally, we derive generalized inference and learning algorithms for these LVMs, and demonstrate them with a variety of examples. Our unified perspective facilitates both understanding and implementing exact inference and learning algorithms for a wide variety of models, and may guide researchers in the discovery of new models that avoid unnecessary approximations.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Communication-efficient Federated Learning via Sparse and Aligned Adaptive Optimization</title>
<link>https://arxiv.org/abs/2405.17932</link>
<guid>https://arxiv.org/abs/2405.17932</guid>
<content:encoded><![CDATA[
arXiv:2405.17932v2 Announce Type: replace 
Abstract: Adaptive moment estimation (Adam), as a Stochastic Gradient Descent (SGD) variant, has gained widespread popularity in federated learning (FL) due to its fast convergence. However, federated Adam (FedAdam) algorithms suffer from a threefold increase in uplink communication overhead compared to federated SGD (FedSGD) algorithms, which arises from the necessity to transmit both local model updates and first and second moment estimates from distributed devices to the centralized server for aggregation. Driven by this issue, we propose a novel sparse FedAdam algorithm called FedAdam-SSM, wherein distributed devices sparsify the updates of local model parameters and moment estimates and subsequently upload the sparse representations to the centralized server. To further reduce the communication overhead, the updates of local model parameters and moment estimates incorporate a shared sparse mask (SSM) into the sparsification process, eliminating the need for three separate sparse masks. Theoretically, we develop an upper bound on the divergence between the local model trained by FedAdam-SSM and the desired model trained by centralized Adam, which is related to sparsification error and imbalanced data distribution. By minimizing the divergence bound between the model trained by FedAdam-SSM and centralized Adam, we optimize the SSM to mitigate the learning performance degradation caused by sparsification error. Additionally, we provide convergence bounds for FedAdam-SSM in both convex and non-convex objective function settings, and investigate the impact of local epoch, learning rate and sparsification ratio on the convergence rate of FedAdam-SSM. Experimental results show that FedAdam-SSM outperforms baselines in terms of convergence rate (over 1.1$\times$ faster than the sparse FedAdam baselines) and test accuracy (over 14.5\% ahead of the quantized FedAdam baselines).
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two Is Better Than One: Aligned Representation Pairs for Anomaly Detection</title>
<link>https://arxiv.org/abs/2405.18848</link>
<guid>https://arxiv.org/abs/2405.18848</guid>
<content:encoded><![CDATA[
arXiv:2405.18848v3 Announce Type: replace 
Abstract: Anomaly detection focuses on identifying samples that deviate from the norm. Discovering informative representations of normal samples is crucial to detecting anomalies effectively. Recent self-supervised methods have successfully learned such representations by employing prior knowledge about anomalies to create synthetic outliers during training. However, we often do not know what to expect from unseen data in specialized real-world applications. In this work, we address this limitation with our new approach Con$_2$, which leverages prior knowledge about symmetries in normal samples to observe the data in different contexts. Con$_2$ consists of two parts: Context Contrasting clusters representations according to their context, while Content Alignment encourages the model to capture semantic information by aligning the positions of normal samples across clusters. The resulting representation space allows us to detect anomalies as outliers of the learned context clusters. We demonstrate the benefit of this approach in extensive experiments on specialized medical datasets, outperforming competitive baselines based on self-supervised learning and pretrained models and presenting competitive performance on natural imaging benchmarks.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Temporal Dependencies within the Target for Long-Term Time Series Forecasting</title>
<link>https://arxiv.org/abs/2406.04777</link>
<guid>https://arxiv.org/abs/2406.04777</guid>
<content:encoded><![CDATA[
arXiv:2406.04777v3 Announce Type: replace 
Abstract: Long-term time series forecasting (LTSF) is a critical task across diverse domains. Despite significant advancements in LTSF research, we identify a performance bottleneck in existing LTSF methods caused by the inadequate modeling of Temporal Dependencies within the Target (TDT). To address this issue, we propose a novel and generic temporal modeling framework, Temporal Dependency Alignment (TDAlign), that equips existing LTSF methods with TDT learning capabilities. TDAlign introduces two key innovations: 1) a loss function that aligns the change values between adjacent time steps in the predictions with those in the target, ensuring consistency with variation patterns, and 2) an adaptive loss balancing strategy that seamlessly integrates the new loss function with existing LTSF methods without introducing additional learnable parameters. As a plug-and-play framework, TDAlign enhances existing methods with minimal computational overhead, featuring only linear time complexity and constant space complexity relative to the prediction length. Extensive experiments on six strong LTSF baselines across seven real-world datasets demonstrate the effectiveness and flexibility of TDAlign. On average, TDAlign reduces baseline prediction errors by \textbf{1.47\%} to \textbf{9.19\%} and change value errors by \textbf{4.57\%} to \textbf{15.78\%}, highlighting its substantial performance improvements.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FRIDA: Free-Rider Detection using Privacy Attacks</title>
<link>https://arxiv.org/abs/2410.05020</link>
<guid>https://arxiv.org/abs/2410.05020</guid>
<content:encoded><![CDATA[
arXiv:2410.05020v2 Announce Type: replace 
Abstract: Federated learning is increasingly popular as it enables multiple parties with limited datasets and resources to train a machine learning model collaboratively. However, similar to other collaborative systems, federated learning is vulnerable to free-riders - participants who benefit from the global model without contributing. Free-riders compromise the integrity of the learning process and slow down the convergence of the global model, resulting in increased costs for honest participants. To address this challenge, we propose FRIDA: free-rider detection using privacy attacks. Instead of focusing on implicit effects of free-riding, FRIDA utilizes membership and property inference attacks to directly infer evidence of genuine client training. Our extensive evaluation demonstrates that FRIDA is effective across a wide range of scenarios.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A noise-corrected Langevin algorithm and sampling by half-denoising</title>
<link>https://arxiv.org/abs/2410.05837</link>
<guid>https://arxiv.org/abs/2410.05837</guid>
<content:encoded><![CDATA[
arXiv:2410.05837v3 Announce Type: replace 
Abstract: The Langevin algorithm is a classic method for sampling from a given pdf in a real space. In its basic version, it only requires knowledge of the gradient of the log-density, also called the score function. However, in deep learning, it is often easier to learn the so-called "noisy-data score function", i.e. the gradient of the log-density of noisy data, more precisely when Gaussian noise is added to the data. Such an estimate is biased and complicates the use of the Langevin method. Here, we propose a noise-corrected version of the Langevin algorithm, where the bias due to noisy data is removed, at least regarding first-order terms. Unlike diffusion models, our algorithm only needs to know the noisy score function for one single noise level. We further propose a simple special case which has an interesting intuitive interpretation of iteratively adding noise the data and then attempting to remove half of that noise.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiRW: Path-Aware Digraph Learning for Heterophily</title>
<link>https://arxiv.org/abs/2410.10320</link>
<guid>https://arxiv.org/abs/2410.10320</guid>
<content:encoded><![CDATA[
arXiv:2410.10320v3 Announce Type: replace 
Abstract: Recently, graph neural network (GNN) has emerged as a powerful representation learning tool for graph-structured data. However, most approaches are tailored for undirected graphs, neglecting the abundant information in the edges of directed graphs (digraphs). In fact, digraphs are widely applied in the real world and confirmed to address heterophily challenges. Despite recent advancements, existing spatial- and spectral-based DiGNNs have limitations due to their complex learning mechanisms and reliance on high-quality topology, resulting in low efficiency and unstable performance. To address these issues, we propose Directed Random Walk (DiRW), a plug-and-play strategy for most spatial-based DiGNNs and also an innovative model which offers a new digraph learning paradigm. Specifically, it utilizes a direction-aware path sampler optimized from the perspectives of walk probability, length, and number in a weight-free manner by considering node profiles and topologies. Building upon this, DiRW incorporates a node-wise learnable path aggregator for generalized node representations. Extensive experiments on 9 datasets demonstrate that DiRW: (1) enhances most spatial-based methods as a plug-and-play strategy; (2) achieves SOTA performance as a new digraph learning paradigm. The source code and data are available at https://github.com/dhsiuu/DiRW.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning Foundation and Pattern Models: Challenges in Hydrological Time Series</title>
<link>https://arxiv.org/abs/2410.15218</link>
<guid>https://arxiv.org/abs/2410.15218</guid>
<content:encoded><![CDATA[
arXiv:2410.15218v3 Announce Type: replace 
Abstract: There has been active investigation into deep learning approaches for time series analysis, including foundation models. However, most studies do not address significant scientific applications. This paper aims to identify key features in time series by examining hydrology data. Our work advances computer science by emphasizing critical application features and contributes to hydrology and other scientific fields by identifying modeling approaches that effectively capture these features. Scientific time series data are inherently complex, involving observations from multiple locations, each with various time-dependent data streams and exogenous factors that may be static or time-varying and either application-dependent or purely mathematical. This research analyzes hydrology time series from the CAMELS and Caravan global datasets, which encompass rainfall and runoff data across catchments, featuring up to six observed streams and 209 static parameters across approximately 8,000 locations. Our investigation assesses the impact of exogenous data through eight different model configurations for key hydrology tasks. Results demonstrate that integrating exogenous information enhances data representation, reducing mean squared error by up to 40% in the largest dataset. Additionally, we present a detailed performance comparison of over 20 state-of-the-art pattern and foundation models. The analysis is fully open-source, facilitated by Jupyter Notebook on Google Colab for LSTM-based modeling, data preprocessing, and model comparisons. Preliminary findings using alternative deep learning architectures reveal that models incorporating comprehensive observed and exogenous data outperform more limited approaches, including foundation models. Notably, natural annual periodic exogenous time series contribute the most significant improvements, though static and other periodic factors are also valuable.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Concept Bottleneck Models with LLM Priors</title>
<link>https://arxiv.org/abs/2410.15555</link>
<guid>https://arxiv.org/abs/2410.15555</guid>
<content:encoded><![CDATA[
arXiv:2410.15555v2 Announce Type: replace 
Abstract: Concept Bottleneck Models (CBMs) have been proposed as a compromise between white-box and black-box models, aiming to achieve interpretability without sacrificing accuracy. The standard training procedure for CBMs is to predefine a candidate set of human-interpretable concepts, extract their values from the training data, and identify a sparse subset as inputs to a transparent prediction model. However, such approaches are often hampered by the tradeoff between exploring a sufficiently large set of concepts versus controlling the cost of obtaining concept extractions, resulting in a large interpretability-accuracy tradeoff. This work investigates a novel approach that sidesteps these challenges: BC-LLM iteratively searches over a potentially infinite set of concepts within a Bayesian framework, in which Large Language Models (LLMs) serve as both a concept extraction mechanism and prior. Even though LLMs can be miscalibrated and hallucinate, we prove that BC-LLM can provide rigorous statistical inference and uncertainty quantification. Across image, text, and tabular datasets, BC-LLM outperforms interpretable baselines and even black-box models in certain settings, converges more rapidly towards relevant concepts, and is more robust to out-of-distribution samples.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Data-Driven Review of Remote Sensing-Based Data Fusion in Precision Agriculture from Foundational to Transformer-Based Techniques</title>
<link>https://arxiv.org/abs/2410.18353</link>
<guid>https://arxiv.org/abs/2410.18353</guid>
<content:encoded><![CDATA[
arXiv:2410.18353v3 Announce Type: replace 
Abstract: This review explores recent advancements in data fusion techniques and Transformer-based remote sensing applications in precision agriculture. Using a systematic, data-driven approach, we analyze research trends from 1994 to 2024, identifying key developments in data fusion, remote sensing, and AI-driven agricultural monitoring. While traditional machine learning and deep learning approaches have demonstrated effectiveness in agricultural decision-making, challenges such as limited scalability, suboptimal feature extraction, and reliance on extensive labeled data persist. This study examines the comparative advantages of Transformer-based fusion methods, particularly their ability to model spatiotemporal dependencies and integrate heterogeneous datasets for applications in soil analysis, crop classification, yield prediction, and disease detection. A comparative analysis of multimodal data fusion approaches is conducted, evaluating data types, fusion techniques, and remote sensing platforms. We demonstrate how Transformers outperform conventional models by enhancing prediction accuracy, mitigating feature redundancy, and optimizing large-scale data integration. Furthermore, we propose a structured roadmap for implementing data fusion in agricultural remote sensing, outlining best practices for ground-truth data selection, platform integration, and fusion model design. By addressing key research gaps and providing a strategic framework, this review offers valuable insights for advancing precision agriculture through AI-driven data fusion techniques.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flavors of Margin: Implicit Bias of Steepest Descent in Homogeneous Neural Networks</title>
<link>https://arxiv.org/abs/2410.22069</link>
<guid>https://arxiv.org/abs/2410.22069</guid>
<content:encoded><![CDATA[
arXiv:2410.22069v3 Announce Type: replace 
Abstract: We study the implicit bias of the general family of steepest descent algorithms with infinitesimal learning rate in deep homogeneous neural networks. We show that: (a) an algorithm-dependent geometric margin starts increasing once the networks reach perfect training accuracy, and (b) any limit point of the training trajectory corresponds to a KKT point of the corresponding margin-maximization problem. We experimentally zoom into the trajectories of neural networks optimized with various steepest descent algorithms, highlighting connections to the implicit bias of popular adaptive methods (Adam and Shampoo).
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Entropy-Regularized Process Reward Model</title>
<link>https://arxiv.org/abs/2412.11006</link>
<guid>https://arxiv.org/abs/2412.11006</guid>
<content:encoded><![CDATA[
arXiv:2412.11006v2 Announce Type: replace 
Abstract: Large language models (LLMs) have shown promise in performing complex multi-step reasoning, yet they continue to struggle with mathematical reasoning, often making systematic errors. A promising solution is reinforcement learning (RL) guided by reward models, particularly those focusing on process rewards, which score each intermediate step rather than solely evaluating the final outcome. This approach is more effective at guiding policy models towards correct reasoning trajectories. In this work, we propose an entropy-regularized process reward model (ER-PRM) that integrates KL-regularized Markov Decision Processes (MDP) to balance policy optimization with the need to prevent the policy from shifting too far from its initial distribution. We derive a novel reward construction method based on the theoretical results. Our theoretical analysis shows that we could derive the optimal reward model from the initial policy sampling. Our empirical experiments on the MATH and GSM8K benchmarks demonstrate that ER-PRM consistently outperforms existing process reward models, achieving 1% improvement on GSM8K and 2-3% improvement on MATH under best-of-N evaluation, and more than 1% improvement under RLHF. These results highlight the efficacy of entropy-regularization in enhancing LLMs' reasoning capabilities.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain-invariant feature learning in brain MR imaging for content-based image retrieval</title>
<link>https://arxiv.org/abs/2501.01326</link>
<guid>https://arxiv.org/abs/2501.01326</guid>
<content:encoded><![CDATA[
arXiv:2501.01326v2 Announce Type: replace 
Abstract: When conducting large-scale studies that collect brain MR images from multiple facilities, the impact of differences in imaging equipment and protocols at each site cannot be ignored, and this domain gap has become a significant issue in recent years. In this study, we propose a new low-dimensional representation (LDR) acquisition method called style encoder adversarial domain adaptation (SE-ADA) to realize content-based image retrieval (CBIR) of brain MR images. SE-ADA reduces domain differences while preserving pathological features by separating domain-specific information from LDR and minimizing domain differences using adversarial learning. In evaluation experiments comparing SE-ADA with recent domain harmonization methods on eight public brain MR datasets (ADNI1/2/3, OASIS1/2/3/4, PPMI), SE-ADA effectively removed domain information while preserving key aspects of the original brain structure and demonstrated the highest disease search accuracy.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Faster Convergence of Riemannian Stochastic Gradient Descent with Increasing Batch Size</title>
<link>https://arxiv.org/abs/2501.18164</link>
<guid>https://arxiv.org/abs/2501.18164</guid>
<content:encoded><![CDATA[
arXiv:2501.18164v4 Announce Type: replace 
Abstract: We theoretically analyzed the convergence behavior of Riemannian stochastic gradient descent (RSGD) and found that using an increasing batch size leads to faster convergence than using a constant batch size, not only with a constant learning rate but also with a decaying learning rate, such as cosine annealing decay and polynomial decay. The convergence rate improves from $O(T^{-1}+C)$ with a constant batch size to $O(T^{-1})$ with an increasing batch size, where $T$ denotes the total number of iterations and $C$ is a constant. Using principal component analysis and low-rank matrix completion, we investigated, both theoretically and numerically, how an increasing batch size affects computational time as quantified by stochastic first-order oracle (SFO) complexity. An increasing batch size was found to reduce the SFO complexity of RSGD. Furthermore, an increasing batch size was found to offer the advantages of both small and large constant batch sizes.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradient Alignment in Physics-informed Neural Networks: A Second-Order Optimization Perspective</title>
<link>https://arxiv.org/abs/2502.00604</link>
<guid>https://arxiv.org/abs/2502.00604</guid>
<content:encoded><![CDATA[
arXiv:2502.00604v2 Announce Type: replace 
Abstract: Multi-task learning through composite loss functions is fundamental to modern deep learning, yet optimizing competing objectives remains challenging. We present new theoretical and practical approaches for addressing directional conflicts between loss terms, demonstrating their effectiveness in physics-informed neural networks (PINNs) where such conflicts are particularly challenging to resolve. Through theoretical analysis, we demonstrate how these conflicts limit first-order methods and show that second-order optimization naturally resolves them through implicit gradient alignment. We prove that SOAP, a recently proposed quasi-Newton method, efficiently approximates the Hessian preconditioner, enabling breakthrough performance in PINNs: state-of-the-art results on 10 challenging PDE benchmarks, including the first successful application to turbulent flows with Reynolds numbers up to 10,000, with 2-10x accuracy improvements over existing methods. We also introduce a novel gradient alignment score that generalizes cosine similarity to multiple gradients, providing a practical tool for analyzing optimization dynamics. Our findings establish frameworks for understanding and resolving gradient conflicts, with broad implications for optimization beyond scientific computing.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analog In-memory Training on General Non-ideal Resistive Elements: The Impact of Response Functions</title>
<link>https://arxiv.org/abs/2502.06309</link>
<guid>https://arxiv.org/abs/2502.06309</guid>
<content:encoded><![CDATA[
arXiv:2502.06309v3 Announce Type: replace 
Abstract: As the economic and environmental costs of training and deploying large vision or language models increase dramatically, analog in-memory computing (AIMC) emerges as a promising energy-efficient solution. However, the training perspective, especially its training dynamic, is underexplored. In AIMC hardware, the trainable weights are represented by the conductance of resistive elements and updated using consecutive electrical pulses. While the conductance changes by a constant in response to each pulse, in reality, the change is scaled by asymmetric and non-linear \textit{response functions}, leading to a non-ideal training dynamic. This paper provides a theoretical foundation for gradient-based training on AIMC hardware with non-ideal response functions. We demonstrate that asymmetric response functions negatively impact Analog SGD by imposing an implicit penalty on the objective. To overcome the issue, we propose Residual Learning algorithm, which provably converges exactly to a critical point by solving a bilevel optimization problem. We demonstrate that the proposed method can be extended to address other hardware imperfections, such as limited response granularity. As we know, it is the first paper to investigate the impact of a class of generic non-ideal response functions. The conclusion is supported by simulations validating our theoretical insights.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Networks for Learnable and Scalable Influence Estimation of Instruction Fine-Tuning Data</title>
<link>https://arxiv.org/abs/2502.09969</link>
<guid>https://arxiv.org/abs/2502.09969</guid>
<content:encoded><![CDATA[
arXiv:2502.09969v3 Announce Type: replace 
Abstract: Influence functions provide crucial insights into model training, but existing methods suffer from large computational costs and limited generalization. Particularly, recent works have proposed various metrics and algorithms to calculate the influence of data using language models, which do not scale well with large models and datasets. This is because of the expensive forward and backward passes required for computation, substantial memory requirements to store large models, and poor generalization of influence estimates to new data. In this paper, we explore the use of small neural networks -- which we refer to as the InfluenceNetwork -- to estimate influence values, achieving up to 99% cost reduction. Our evaluation demonstrates that influence values can be estimated with models just 0.0027% the size of full language models (we use 7B and 8B versions). We apply our algorithm of estimating influence values (called NN-CIFT: Neural Networks for effiCient Instruction Fine-Tuning) to the downstream task of subset selection for general instruction fine-tuning. In our study, we include four state-of-the-art influence functions and show no compromise in performance, despite large speedups, between NN-CIFT and the original influence functions. We provide an in-depth hyperparameter analyses of NN-CIFT. The code for our method can be found here: https://github.com/agarwalishika/NN-CIFT.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Positional Encoding in Transformer-Based Time Series Models: A Survey</title>
<link>https://arxiv.org/abs/2502.12370</link>
<guid>https://arxiv.org/abs/2502.12370</guid>
<content:encoded><![CDATA[
arXiv:2502.12370v2 Announce Type: replace 
Abstract: Recent advancements in transformer-based models have greatly improved time series analysis, providing robust solutions for tasks such as forecasting, anomaly detection, and classification. A crucial element of these models is positional encoding, which allows transformers to capture the intrinsic sequential nature of time series data. This survey systematically examines existing techniques for positional encoding in transformer-based time series models. We investigate a variety of methods, including fixed, learnable, relative, and hybrid approaches, and evaluate their effectiveness in different time series classification tasks. Our findings indicate that data characteristics like sequence length, signal complexity, and dimensionality significantly influence method effectiveness. Advanced positional encoding methods exhibit performance gains in terms of prediction accuracy, however, they come at the cost of increased computational complexity. Furthermore, we outline key challenges and suggest potential research directions to enhance positional encoding strategies. By delivering a comprehensive overview and quantitative benchmarking, this survey intends to assist researchers and practitioners in selecting and designing effective positional encoding methods for transformer-based time series models.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision Language Model Reasoning</title>
<link>https://arxiv.org/abs/2502.20587</link>
<guid>https://arxiv.org/abs/2502.20587</guid>
<content:encoded><![CDATA[
arXiv:2502.20587v2 Announce Type: replace 
Abstract: Vision Language Models (VLMs) have achieved remarkable success in a wide range of vision applications of increasing complexity and scales, yet choosing the right VLM model size involves a trade-off between response quality and cost. While smaller VLMs are cheaper to run, they typically produce responses only marginally better than random guessing on benchmarks such as MMMU.
  In this paper, we propose Cache of Thought (CoT), a master apprentice framework for collaborative inference between large and small VLMs. CoT manages high quality query results from large VLMs (master) in a cache, which are then selected via a novel multi modal retrieval and in-context learning to aid the performance of small VLMs (apprentice). We extensively evaluate CoT on various widely recognized and challenging general reasoning benchmarks, and show that CoT increases overall reasoning performance by up to 7.7% under the same budget, and specifically boosts the performance of apprentice VLMs by up to 36.6%. Our code is available at https://github.com/UIUC-MONET/Cache-of-Thoughts
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GIN-Graph: A Generative Interpretation Network for Model-Level Explanation of Graph Neural Networks</title>
<link>https://arxiv.org/abs/2503.06352</link>
<guid>https://arxiv.org/abs/2503.06352</guid>
<content:encoded><![CDATA[
arXiv:2503.06352v2 Announce Type: replace 
Abstract: One significant challenge of exploiting Graph neural networks (GNNs) in real-life scenarios is that they are always treated as black boxes, therefore leading to the requirement of interpretability. To address this, model-level interpretation methods have been developed to explain what patterns maximize probability of predicting to a certain class. However, existing model-level interpretation methods pose several limitations such as generating invalid explanation graphs and lacking reliability. In this paper, we propose a new Generative Interpretation Network for Model-Level Explanation of Graph Neural Networks (GIN-Graph), to generate reliable and high-quality model-level explanation graphs. The implicit and likelihood-free generative adversarial networks are exploited to construct the explanation graphs which are similar to original graphs, meanwhile maximizing the prediction probability for a certain class by adopting a novel objective function for generator with dynamic loss weight scheme. Experimental results indicate that GIN-Graph can be applied to interpret GNNs trained on a variety of graph datasets and generate high-quality explanation graphs with high stability and reliability.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StFT: Spatio-temporal Fourier Transformer for Long-term Dynamics Prediction</title>
<link>https://arxiv.org/abs/2503.11899</link>
<guid>https://arxiv.org/abs/2503.11899</guid>
<content:encoded><![CDATA[
arXiv:2503.11899v2 Announce Type: replace 
Abstract: Simulating the long-term dynamics of multi-scale and multi-physics systems poses a significant challenge in understanding complex phenomena across science and engineering. The complexity arises from the intricate interactions between scales and the interplay of diverse physical processes, which manifest in PDEs through coupled, nonlinear terms that govern the evolution of multiple physical fields across scales. Neural operators have shown potential in short-term prediction of such complex spatio-temporal dynamics; however, achieving stable high-fidelity predictions and providing robust uncertainty quantification over extended time horizons remains an open and unsolved area of research. These limitations often lead to stability degradation with rapid error accumulation, particularly in long-term forecasting of systems characterized by multi-scale behaviors involving dynamics of different orders. To address these challenges, we propose an autoregressive Spatio-temporal Fourier Transformer (StFT), in which each transformer block is designed to learn the system dynamics at a distinct scale through a dual-path architecture that integrates frequency-domain and spatio-temporal representations. By leveraging a structured hierarchy of \ours blocks, the resulting model explicitly captures the underlying dynamics across both macro- and micro- spatial scales. Furthermore, a generative residual correction mechanism is introduced to learn a probabilistic refinement temporally while simultaneously quantifying prediction uncertainties, enhancing both the accuracy and reliability of long-term probabilistic forecasting. Evaluations conducted on three benchmark datasets (plasma, fluid, and atmospheric dynamics) demonstrate the advantages of our approach over state-of-the-art ML methods.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Highly Efficient Direct Analytics on Semantic-aware Time Series Data Compression</title>
<link>https://arxiv.org/abs/2503.13246</link>
<guid>https://arxiv.org/abs/2503.13246</guid>
<content:encoded><![CDATA[
arXiv:2503.13246v3 Announce Type: replace 
Abstract: Semantic communication has emerged as a promising paradigm to tackle the challenges of massive growing data traffic and sustainable data communication. It shifts the focus from data fidelity to goal-oriented or task-oriented semantic transmission. While deep learning-based methods are commonly used for semantic encoding and decoding, they struggle with the sequential nature of time series data and high computation cost, particularly in resource-constrained IoT environments. Data compression plays a crucial role in reducing transmission and storage costs, yet traditional data compression methods fall short of the demands of goal-oriented communication systems. In this paper, we propose a novel method for direct analytics on time series data compressed by the SHRINK compression algorithm. Through experimentation using outlier detection as a case study, we show that our method outperforms baselines running on uncompressed data in multiple cases, with merely 1% difference in the worst case. Additionally, it achieves four times lower runtime on average and accesses approximately 10% of the data volume, which enables edge analytics with limited storage and computation power. These results demonstrate that our approach offers reliable, high-speed outlier detection analytics for diverse IoT applications while extracting semantics from time-series data, achieving high compression, and reducing data transmission.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Black Box Anymore: Demystifying Clinical Predictive Modeling with Temporal-Feature Cross Attention Mechanism</title>
<link>https://arxiv.org/abs/2503.19285</link>
<guid>https://arxiv.org/abs/2503.19285</guid>
<content:encoded><![CDATA[
arXiv:2503.19285v3 Announce Type: replace 
Abstract: Despite the outstanding performance of deep learning models in clinical prediction tasks, explainability remains a significant challenge. Inspired by transformer architectures, we introduce the Temporal-Feature Cross Attention Mechanism (TFCAM), a novel deep learning framework designed to capture dynamic interactions among clinical features across time, enhancing both predictive accuracy and interpretability. In an experiment with 1,422 patients with Chronic Kidney Disease, predicting progression to End-Stage Renal Disease, TFCAM outperformed LSTM and RETAIN baselines, achieving an AUROC of 0.95 and an F1-score of 0.69. Beyond performance gains, TFCAM provides multi-level explainability by identifying critical temporal periods, ranking feature importance, and quantifying how features influence each other across time before affecting predictions. Our approach addresses the "black box" limitations of deep learning in healthcare, offering clinicians transparent insights into disease progression mechanisms while maintaining state-of-the-art predictive performance.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPACE: SPike-Aware Consistency Enhancement for Test-Time Adaptation in Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2504.02298</link>
<guid>https://arxiv.org/abs/2504.02298</guid>
<content:encoded><![CDATA[
arXiv:2504.02298v3 Announce Type: replace 
Abstract: Spiking Neural Networks (SNNs), as a biologically plausible alternative to Artificial Neural Networks (ANNs), have demonstrated advantages in terms of energy efficiency, temporal processing, and biological plausibility. However, SNNs are highly sensitive to distribution shifts, which can significantly degrade their performance in real-world scenarios. Traditional test-time adaptation (TTA) methods designed for ANNs often fail to address the unique computational dynamics of SNNs, such as sparsity and temporal spiking behavior. To address these challenges, we propose SPike-Aware Consistency Enhancement (SPACE), the first source-free and single-instance TTA method specifically designed for SNNs. SPACE leverages the inherent spike dynamics of SNNs to maximize the consistency of spike-behavior-based local feature maps across augmented versions of a single test sample, enabling robust adaptation without requiring source data. We evaluate SPACE on multiple datasets. Furthermore, SPACE exhibits robust generalization across diverse network architectures, consistently enhancing the performance of SNNs on CNNs, Transformer, and ConvLSTM architectures. Experimental results show that SPACE outperforms state-of-the-art ANN methods while maintaining lower computational cost, highlighting its effectiveness and robustness for SNNs in real-world settings. The code will be available at https://github.com/ethanxyluo/SPACE.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compound Fault Diagnosis for Train Transmission Systems Using Deep Learning with Fourier-enhanced Representation</title>
<link>https://arxiv.org/abs/2504.07155</link>
<guid>https://arxiv.org/abs/2504.07155</guid>
<content:encoded><![CDATA[
arXiv:2504.07155v3 Announce Type: replace 
Abstract: Fault diagnosis prevents train disruptions by ensuring the stability and reliability of their transmission systems. Data-driven fault diagnosis models have several advantages over traditional methods in terms of dealing with non-linearity, adaptability, scalability, and automation. However, existing data-driven models are trained on separate transmission components and only consider single faults due to the limitations of existing datasets. These models will perform worse in scenarios where components operate with each other at the same time, affecting each component's vibration signals. To address some of these challenges, we propose a frequency domain representation and a 1-dimensional convolutional neural network for compound fault diagnosis and applied it on the PHM Beijing 2024 dataset, which includes 21 sensor channels, 17 single faults, and 42 compound faults from 4 interacting components, that is, motor, gearbox, left axle box, and right axle box. Our proposed model achieved 97.67% and 93.93% accuracies on the test set with 17 single faults and on the test set with 42 compound faults, respectively.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revealing Human Internal Attention Patterns from Gameplay Analysis for Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.11118</link>
<guid>https://arxiv.org/abs/2504.11118</guid>
<content:encoded><![CDATA[
arXiv:2504.11118v2 Announce Type: replace 
Abstract: This study introduces a novel method for revealing human internal attention patterns from gameplay data alone, leveraging offline attention techniques from reinforcement learning (RL). We propose contextualized, task-relevant (CTR) attention networks, which generate attention maps from both human and RL agent gameplay in Atari environments. To evaluate whether the human CTR maps reveal internal attention, we validate our model by quantitative and qualitative comparison to the agent maps as well as to a temporally integrated overt attention (TIOA) model based on human eye-tracking data. Our results show that human CTR maps are more sparse than the agent ones and align better with the TIOA maps. Following a qualitative visual comparison we conclude that they likely capture patterns of internal attention. As a further application, we use these maps to guide RL agents, finding that human internal attention-guided agents achieve slightly improved and more stable learning compared to baselines. This work advances the understanding of human-agent attention differences and provides a new approach for extracting and validating internal attention from behavioral data.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TSCAN: Context-Aware Uplift Modeling via Two-Stage Training for Online Merchant Business Diagnosis</title>
<link>https://arxiv.org/abs/2504.18881</link>
<guid>https://arxiv.org/abs/2504.18881</guid>
<content:encoded><![CDATA[
arXiv:2504.18881v2 Announce Type: replace 
Abstract: A primary challenge in ITE estimation is sample selection bias. Traditional approaches utilize treatment regularization techniques such as the Integral Probability Metrics (IPM), re-weighting, and propensity score modeling to mitigate this bias. However, these regularizations may introduce undesirable information loss and limit the performance of the model. Furthermore, treatment effects vary across different external contexts, and the existing methods are insufficient in fully interacting with and utilizing these contextual features. To address these issues, we propose a Context-Aware uplift model based on the Two-Stage training approach (TSCAN), comprising CAN-U and CAN-D sub-models. In the first stage, we train an uplift model, called CAN-U, which includes the treatment regularizations of IPM and propensity score prediction, to generate a complete dataset with counterfactual uplift labels. In the second stage, we train a model named CAN-D, which utilizes an isotonic output layer to directly model uplift effects, thereby eliminating the reliance on the regularization components. CAN-D adaptively corrects the errors estimated by CAN-U through reinforcing the factual samples, while avoiding the negative impacts associated with the aforementioned regularizations. Additionally, we introduce a Context-Aware Attention Layer throughout the two-stage process to manage the interactions between treatment, merchant, and contextual features, thereby modeling the varying treatment effect in different contexts. We conduct extensive experiments on two real-world datasets to validate the effectiveness of TSCAN. Ultimately, the deployment of our model for real-world merchant diagnosis on one of China's largest online food ordering platforms validates its practical utility and impact.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gaussian process policy iteration with additive Schwarz acceleration for forward and inverse HJB and mean field game problems</title>
<link>https://arxiv.org/abs/2505.00909</link>
<guid>https://arxiv.org/abs/2505.00909</guid>
<content:encoded><![CDATA[
arXiv:2505.00909v2 Announce Type: replace 
Abstract: We propose a Gaussian Process (GP)-based policy iteration framework for addressing both forward and inverse problems in Hamilton--Jacobi--Bellman (HJB) equations and mean field games (MFGs). Policy iteration is formulated as an alternating procedure between solving the value function under a fixed control policy and updating the policy based on the resulting value function. By exploiting the linear structure of GPs for function approximation, each policy evaluation step admits an explicit closed-form solution, eliminating the need for numerical optimization. To improve convergence, we incorporate the additive Schwarz acceleration as a preconditioning step following each policy update. Numerical experiments demonstrate the effectiveness of Schwarz acceleration in improving computational efficiency.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConCISE: Confidence-guided Compression in Step-by-step Efficient Reasoning</title>
<link>https://arxiv.org/abs/2505.04881</link>
<guid>https://arxiv.org/abs/2505.04881</guid>
<content:encoded><![CDATA[
arXiv:2505.04881v2 Announce Type: replace 
Abstract: Large Reasoning Models (LRMs) perform strongly in complex reasoning tasks via Chain-of-Thought (CoT) prompting, but often suffer from verbose outputs, increasing computational overhead. Existing fine-tuning-based compression methods either operate post-hoc pruning, risking disruption to reasoning coherence, or rely on sampling-based selection, which fails to remove redundant content thoroughly. To address these limitations, this work begins by framing two key patterns of redundant reflection in LRMs--Confidence Deficit, wherein the model reflects on correct intermediate steps, and Termination Delay, where reflection continues after a verified, confident answer--through a confidence-guided perspective. Based on this, we introduce ConCISE (Confidence-guided Compression In Step-by-step Efficient Reasoning), a framework designed to generate concise reasoning chains, integrating Confidence Injection to boost reasoning confidence, and Early Stopping to terminate reasoning when confidence is sufficient. Extensive experiments demonstrate that compared to baseline methods, fine-tuning LRMs on ConCISE-generated data yields a better balance between compression and task performance, reducing length by up to approximately 50% under SimPO, while maintaining high task accuracy.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Schreier-Coset Graph Propagation</title>
<link>https://arxiv.org/abs/2505.10392</link>
<guid>https://arxiv.org/abs/2505.10392</guid>
<content:encoded><![CDATA[
arXiv:2505.10392v2 Announce Type: replace 
Abstract: Graph Neural Networks (GNNs) offer a principled framework for learning over graph-structured data, yet their expressive capacity is often hindered by over-squashing, wherein information from distant nodes is compressed into fixed-size vectors. Existing solutions, including graph rewiring and bottleneck-resistant architectures such as Cayley and expander graphs, avoid this problem but introduce scalability bottlenecks. In particular, the Cayley graphs constructed over $SL(2,\mathbb{Z}_n)$ exhibit strong theoretical properties, yet suffer from cubic node growth $O(n^3)$, leading to high memory usage. To address this, this work introduces Schrier-Coset Graph Propagation (SCGP), a group-theoretic augmentation method that enriches node features through Schreier-coset embeddings without altering the input graph topology. SCGP embeds bottleneck-free connectivity patterns into a compact feature space, improving long-range message passing while maintaining computational efficiency. Empirical evaluations across standard node and graph classification benchmarks demonstrate that SCGP achieves performance comparable to, or exceeding, expander graph and rewired GNN baselines. Furthermore, SCGP exhibits particular advantages in processing hierarchical and modular graph structures, offering reduced inference latency, improved scalability, and a low memory footprint, making it suitable for real-time and resource-constrained applications.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context parroting: A simple but tough-to-beat baseline for foundation models in scientific machine learning</title>
<link>https://arxiv.org/abs/2505.11349</link>
<guid>https://arxiv.org/abs/2505.11349</guid>
<content:encoded><![CDATA[
arXiv:2505.11349v2 Announce Type: replace 
Abstract: Recent time-series foundation models exhibit strong abilities to predict physical systems. These abilities include zero-shot forecasting, in which a model forecasts future states of a system given only a short trajectory as context, without knowledge of the underlying physics. Here, we show that foundation models often forecast through a simple parroting strategy, and when they are not parroting they exhibit some shared failure modes such as converging to the mean. As a result, a naive context parroting model that copies directly from the context scores higher than leading time-series foundation models on predicting a diverse range of dynamical systems, including low-dimensional chaos, turbulence, coupled oscillators, and electrocardiograms -- and at a tiny fraction of the computational cost. We draw a parallel between context parroting and induction heads, which explains recent works showing that large language models can often be repurposed for time series forecasting. Our dynamical systems perspective also ties the scaling between forecast accuracy and context length to the fractal dimension of the underlying chaotic attractor, providing insight into previously observed in-context neural scaling laws. By revealing the performance gaps and failure modes of current time-series foundation models, context parroting can guide the design of future foundation models and help identify in-context learning strategies beyond parroting.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automating Versatile Time-Series Analysis with Tiny Transformers on Embedded FPGAs</title>
<link>https://arxiv.org/abs/2505.17662</link>
<guid>https://arxiv.org/abs/2505.17662</guid>
<content:encoded><![CDATA[
arXiv:2505.17662v5 Announce Type: replace 
Abstract: Transformer-based models have shown strong performance across diverse time-series tasks, but their deployment on resource-constrained devices remains challenging due to high memory and computational demand. While prior work targeting Microcontroller Units (MCUs) has explored hardware-specific optimizations, such approaches are often task-specific and limited to 8-bit fixed-point precision. Field-Programmable Gate Arrays (FPGAs) offer greater flexibility, enabling fine-grained control over data precision and architecture. However, existing FPGA-based deployments of Transformers for time-series analysis typically focus on high-density platforms with manual configuration. This paper presents a unified and fully automated deployment framework for Tiny Transformers on embedded FPGAs. Our framework supports a compact encoder-only Transformer architecture across three representative time-series tasks (forecasting, classification, and anomaly detection). It combines quantization-aware training (down to 4 bits), hardware-aware hyperparameter search using Optuna, and automatic VHDL generation for seamless deployment. We evaluate our framework on six public datasets across two embedded FPGA platforms. Results show that our framework produces integer-only, task-specific Transformer accelerators achieving as low as 0.033 mJ per inference with millisecond latency on AMD Spartan-7, while also providing insights into deployment feasibility on Lattice iCE40. All source code will be released in the GitHub repository (https://github.com/Edwina1030/TinyTransformer4TS).
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Large Language Models for Data Challenges in Graphs</title>
<link>https://arxiv.org/abs/2505.18475</link>
<guid>https://arxiv.org/abs/2505.18475</guid>
<content:encoded><![CDATA[
arXiv:2505.18475v2 Announce Type: replace 
Abstract: Graphs are a widely used paradigm for representing non-Euclidean data, with applications ranging from social network analysis to biomolecular prediction. While graph learning has achieved remarkable progress, real-world graph data presents a number of challenges that significantly hinder the learning process. In this survey, we focus on four fundamental data-centric challenges: (1) Incompleteness, real-world graphs have missing nodes, edges, or attributes; (2) Imbalance, the distribution of the labels of nodes or edges and their structures for real-world graphs are highly skewed; (3) Cross-domain Heterogeneity, graphs from different domains exhibit incompatible feature spaces or structural patterns; and (4) Dynamic Instability, graphs evolve over time in unpredictable ways. Recently, Large Language Models (LLMs) offer the potential to tackle these challenges by leveraging rich semantic reasoning and external knowledge. This survey focuses on how LLMs can address four fundamental data-centric challenges in graph-structured data, thereby improving the effectiveness of graph learning. For each challenge, we review both traditional solutions and modern LLM-driven approaches, highlighting how LLMs contribute unique advantages. Finally, we discuss open research questions and promising future directions in this emerging interdisciplinary field. To support further exploration, we have curated a repository of recent advances on graph learning challenges: https://github.com/limengran98/Awesome-Literature-Graph-Learning-Challenges.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Noise-Robustness Through Noise: Asymmetric LoRA Adaption with Poisoning Expert</title>
<link>https://arxiv.org/abs/2505.23868</link>
<guid>https://arxiv.org/abs/2505.23868</guid>
<content:encoded><![CDATA[
arXiv:2505.23868v4 Announce Type: replace 
Abstract: Current parameter-efficient fine-tuning methods for adapting pre-trained language models to downstream tasks are susceptible to interference from noisy data. Conventional noise-handling approaches either rely on laborious data pre-processing or employ model architecture modifications prone to error accumulation. In contrast to existing noise-process paradigms, we propose a noise-robust adaptation method via asymmetric LoRA poisoning experts (LoPE), a novel framework that enhances model robustness to noise only with generated noisy data. Drawing inspiration from the mixture-of-experts architecture, LoPE strategically integrates a dedicated poisoning expert in an asymmetric LoRA configuration. Through a two-stage paradigm, LoPE performs noise injection on the poisoning expert during fine-tuning to enhance its noise discrimination and processing ability. During inference, we selectively mask the dedicated poisoning expert to leverage purified knowledge acquired by normal experts for noise-robust output. Extensive experiments demonstrate that LoPE achieves strong performance and robustness purely through the low-cost noise injection, which completely eliminates the requirement of data cleaning.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Linear Steering: Unified Multi-Attribute Control for Language Models</title>
<link>https://arxiv.org/abs/2505.24535</link>
<guid>https://arxiv.org/abs/2505.24535</guid>
<content:encoded><![CDATA[
arXiv:2505.24535v2 Announce Type: replace 
Abstract: Controlling multiple behavioral attributes in large language models (LLMs) at inference time is a challenging problem due to interference between attributes and the limitations of linear steering methods, which assume additive behavior in activation space and require per-attribute tuning. We introduce K-Steering, a unified and flexible approach that trains a single non-linear multi-label classifier on hidden activations and computes intervention directions via gradients at inference time. This avoids linearity assumptions, removes the need for storing and tuning separate attribute vectors, and allows dynamic composition of behaviors without retraining. To evaluate our method, we propose two new benchmarks, ToneBank and DebateMix, targeting compositional behavioral control. Empirical results across 3 model families, validated by both activation-based classifiers and LLM-based judges, demonstrate that K-Steering outperforms strong baselines in accurately steering multiple behaviors.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Channel-Imposed Fusion: A Simple yet Effective Method for Medical Time Series Classification</title>
<link>https://arxiv.org/abs/2506.00337</link>
<guid>https://arxiv.org/abs/2506.00337</guid>
<content:encoded><![CDATA[
arXiv:2506.00337v2 Announce Type: replace 
Abstract: The automatic classification of medical time series signals, such as electroencephalogram (EEG) and electrocardiogram (ECG), plays a pivotal role in clinical decision support and early detection of diseases. Although Transformer based models have achieved notable performance by implicitly modeling temporal dependencies through self-attention mechanisms, their inherently complex architectures and opaque reasoning processes undermine their trustworthiness in high stakes clinical settings. In response to these limitations, this study shifts focus toward a modeling paradigm that emphasizes structural transparency, aligning more closely with the intrinsic characteristics of medical data. We propose a novel method, Channel Imposed Fusion (CIF), which enhances the signal-to-noise ratio through cross-channel information fusion, effectively reduces redundancy, and improves classification performance. Furthermore, we integrate CIF with the Temporal Convolutional Network (TCN), known for its structural simplicity and controllable receptive field, to construct an efficient and explicit classification framework. Experimental results on multiple publicly available EEG and ECG datasets demonstrate that the proposed method not only outperforms existing state-of-the-art (SOTA) approaches in terms of various classification metrics, but also significantly enhances the transparency of the classification process, offering a novel perspective for medical time series classification.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perception-R1: Advancing Multimodal Reasoning Capabilities of MLLMs via Visual Perception Reward</title>
<link>https://arxiv.org/abs/2506.07218</link>
<guid>https://arxiv.org/abs/2506.07218</guid>
<content:encoded><![CDATA[
arXiv:2506.07218v2 Announce Type: replace 
Abstract: Enhancing the multimodal reasoning capabilities of Multimodal Large Language Models (MLLMs) is a challenging task that has attracted increasing attention in the community. Recently, several studies have applied Reinforcement Learning with Verifiable Rewards (RLVR) to the multimodal domain in order to enhance the reasoning abilities of MLLMs. However, these works largely overlook the enhancement of multimodal perception capabilities in MLLMs, which serve as a core prerequisite and foundational component of complex multimodal reasoning. Through McNemar's test, we find that existing RLVR method fails to effectively enhance the multimodal perception capabilities of MLLMs, thereby limiting their further improvement in multimodal reasoning. To address this limitation, we propose Perception-R1, which introduces a novel visual perception reward that explicitly encourages MLLMs to perceive the visual content accurately, thereby can effectively incentivizing both their multimodal perception and reasoning capabilities. Specifically, we first collect textual visual annotations from the CoT trajectories of multimodal problems, which will serve as visual references for reward assignment. During RLVR training, we employ a judging LLM to assess the consistency between the visual annotations and the responses generated by MLLM, and assign the visual perception reward based on these consistency judgments. Extensive experiments on several multimodal reasoning benchmarks demonstrate the effectiveness of our Perception-R1, which achieves state-of-the-art performance on most benchmarks using only 1,442 training data.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Algorithmic Fairness: Not a Purely Technical but Socio-Technical Property</title>
<link>https://arxiv.org/abs/2506.12556</link>
<guid>https://arxiv.org/abs/2506.12556</guid>
<content:encoded><![CDATA[
arXiv:2506.12556v2 Announce Type: replace 
Abstract: The rapid trend of deploying artificial intelligence (AI) and machine learning (ML) systems in socially consequential domains has raised growing concerns about their trustworthiness, including potential discriminatory behaviours. Research in algorithmic fairness has generated a proliferation of mathematical definitions and metrics, yet persistent misconceptions and limitations -- both within and beyond the fairness community -- limit their effectiveness, such as an unreached consensus on its understanding, prevailing measures primarily tailored to binary group settings, and superficial handling for intersectional contexts. Here we critically remark on these misconceptions and argue that fairness cannot be reduced to purely technical constraints on models; we also examine the limitations of existing fairness measures through conceptual analysis and empirical illustrations, showing their limited applicability in the face of complex real-world scenarios, challenging prevailing views on the incompatibility between accuracy and fairness as well as that among fairness measures themselves, and outlining three worth-considering principles in the design of fairness measures. We believe these findings will help bridge the gap between technical formalisation and social realities and meet the challenges of real-world AI/ML deployment.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discrete Diffusion in Large Language and Multimodal Models: A Survey</title>
<link>https://arxiv.org/abs/2506.13759</link>
<guid>https://arxiv.org/abs/2506.13759</guid>
<content:encoded><![CDATA[
arXiv:2506.13759v5 Announce Type: replace 
Abstract: In this work, we provide a systematic survey of Discrete Diffusion Language Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs). Unlike autoregressive (AR) models, dLLMs and dMLLMs adopt a multi-token, parallel decoding paradigm using full attention and a denoising-based generation strategy. This paradigm naturally enables parallel generation, fine-grained output control, and dynamic perception. These capabilities are previously difficult to achieve with AR models. A growing number of industrial-scale proprietary d(M)LLMs, as well as a large number of open-source academic d(M)LLMs, have demonstrated performance comparable to their autoregressive counterparts, while achieving up to 10$\times$ acceleration in inference speed. These developments position discrete diffusion models as a promising alternative to intelligence based on the traditional autoregressive approach. In this work, we present a comprehensive overview of the research in the dLLM and dMLLM domains. We trace the historical development of dLLMs and dMLLMs, formalize the underlying mathematical frameworks, list commonly-used modeling methods, and categorize representative models. We further analyze key techniques for training, inference, quantization. We also discuss the trustworthy issues and summarize emerging applications across language, vision-language, and biological domains and etc.. We conclude by discussing future directions for research and deployment. Relative papers are collected in https://github.com/LiQiiiii/Awesome-Discrete-Diffusion-LLM_MLLM
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Capturing Polysemanticity with PRISM: A Multi-Concept Feature Description Framework</title>
<link>https://arxiv.org/abs/2506.15538</link>
<guid>https://arxiv.org/abs/2506.15538</guid>
<content:encoded><![CDATA[
arXiv:2506.15538v3 Announce Type: replace 
Abstract: Automated interpretability research aims to identify concepts encoded in neural network features to enhance human understanding of model behavior. Within the context of large language models (LLMs) for natural language processing (NLP), current automated neuron-level feature description methods face two key challenges: limited robustness and the assumption that each neuron encodes a single concept (monosemanticity), despite increasing evidence of polysemanticity. This assumption restricts the expressiveness of feature descriptions and limits their ability to capture the full range of behaviors encoded in model internals. To address this, we introduce Polysemantic FeatuRe Identification and Scoring Method (PRISM), a novel framework specifically designed to capture the complexity of features in LLMs. Unlike approaches that assign a single description per neuron, common in many automated interpretability methods in NLP, PRISM produces more nuanced descriptions that account for both monosemantic and polysemantic behavior. We apply PRISM to LLMs and, through extensive benchmarking against existing methods, demonstrate that our approach produces more accurate and faithful feature descriptions, improving both overall description quality (via a description score) and the ability to capture distinct concepts when polysemanticity is present (via a polysemanticity score).
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TESSERA: Precomputed FAIR Global Pixel Embeddings for Earth Representation and Analysis</title>
<link>https://arxiv.org/abs/2506.20380</link>
<guid>https://arxiv.org/abs/2506.20380</guid>
<content:encoded><![CDATA[
arXiv:2506.20380v5 Announce Type: replace 
Abstract: Petabytes of satellite Earth Observation (EO) data are freely available and can address critical global challenges. However, EO data quality is poor due to clouds and variable lighting conditions. To address this, practitioners typically use compositing, but this critically removes the temporal phenological signal. Moreover, supervised machine learning to map composited pixels to task-specific classes requires accurately labelled data that are rarely available. We present TESSERA, a pixel-oriented foundation model for EO time series that creates 128-dimensional latent embeddings requiring only a few labels for task-specific training to achieve state-of-the-art performance across diverse complex tasks. TESSERA uses two encoders that combine optical data with synthetic aperture radar backscatter coefficients at 10m resolution, creating embeddings fused with a multilayer perceptron to generate annual global embedding maps. TESSERA closely matches or outperforms state-of-the-art task-specific models and other foundation models across five diverse downstream tasks. It is unprecedented in ease of use, scale, and accuracy: no other open foundation model provides precomputed outputs with global, annual coverage at 10m resolution.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Both Asymptotic and Non-Asymptotic Convergence of Quasi-Hyperbolic Momentum using Increasing Batch Size</title>
<link>https://arxiv.org/abs/2506.23544</link>
<guid>https://arxiv.org/abs/2506.23544</guid>
<content:encoded><![CDATA[
arXiv:2506.23544v3 Announce Type: replace 
Abstract: Momentum methods were originally introduced for their superiority to stochastic gradient descent (SGD) in deterministic settings with convex objective functions. However, despite their widespread application to deep neural networks -- a representative case of stochastic nonconvex optimization -- the theoretical justification for their effectiveness in such settings remains limited. Quasi-hyperbolic momentum (QHM) is an algorithm that generalizes various momentum methods and has been studied to better understand the class of momentum-based algorithms as a whole. In this paper, we provide both asymptotic and non-asymptotic convergence results for mini-batch QHM with an increasing batch size. We show that achieving asymptotic convergence requires either a decaying learning rate or an increasing batch size. Since a decaying learning rate adversely affects non-asymptotic convergence, we demonstrate that using mini-batch QHM with an increasing batch size -- without decaying the learning rate -- can be a more effective strategy. Our experiments show that even a finite increase in batch size can provide benefits for training neural networks.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning with Gradient Eligibility Traces</title>
<link>https://arxiv.org/abs/2507.09087</link>
<guid>https://arxiv.org/abs/2507.09087</guid>
<content:encoded><![CDATA[
arXiv:2507.09087v2 Announce Type: replace 
Abstract: Achieving fast and stable off-policy learning in deep reinforcement learning (RL) is challenging. Most existing methods rely on semi-gradient temporal-difference (TD) methods for their simplicity and efficiency, but are consequently susceptible to divergence. While more principled approaches like Gradient TD (GTD) methods have strong convergence guarantees, they have rarely been used in deep RL. Recent work introduced the generalized Projected Bellman Error ($\overline{\text{PBE}}$), enabling GTD methods to work efficiently with nonlinear function approximation. However, this work is limited to one-step methods, which are slow at credit assignment and require a large number of samples. In this paper, we extend the generalized $\overline{\text{PBE}}$ objective to support multistep credit assignment based on the $\lambda$-return and derive three gradient-based methods that optimize this new objective. We provide both a forward-view formulation compatible with experience replay and a backward-view formulation compatible with streaming algorithms. Finally, we evaluate the proposed algorithms and show that they outperform both PPO and StreamQ in MuJoCo and MinAtar environments, respectively. Code available at https://github.com/esraaelelimy/gtd\_algos
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Merging Memory and Space: A State Space Neural Operator</title>
<link>https://arxiv.org/abs/2507.23428</link>
<guid>https://arxiv.org/abs/2507.23428</guid>
<content:encoded><![CDATA[
arXiv:2507.23428v2 Announce Type: replace 
Abstract: We propose the State Space Neural Operator (SS-NO), a compact architecture for learning solution operators of time-dependent partial differential equations (PDEs). Our formulation extends structured state space models (SSMs) to joint spatiotemporal modeling, introducing two key mechanisms: adaptive damping, which stabilizes learning by localizing receptive fields, and learnable frequency modulation, which enables data-driven spectral selection. These components provide a unified framework for capturing long-range dependencies with parameter efficiency. Theoretically, we establish connections between SSMs and neural operators, proving a universality theorem for convolutional architectures with full field-of-view. Empirically, SS-NO achieves state-of-the-art performance across diverse PDE benchmarks-including 1D Burgers' and Kuramoto-Sivashinsky equations, and 2D Navier-Stokes and compressible Euler flows-while using significantly fewer parameters than competing approaches. A factorized variant of SS-NO further demonstrates scalable performance on challenging 2D problems. Our results highlight the effectiveness of damping and frequency learning in operator modeling, while showing that lightweight factorization provides a complementary path toward efficient large-scale PDE learning.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the (In)Significance of Feature Selection in High-Dimensional Datasets</title>
<link>https://arxiv.org/abs/2508.03593</link>
<guid>https://arxiv.org/abs/2508.03593</guid>
<content:encoded><![CDATA[
arXiv:2508.03593v2 Announce Type: replace 
Abstract: Feature selection (FS) is assumed to improve predictive performance and identify meaningful features in high-dimensional datasets. Surprisingly, small random subsets of features (0.02-1%) match or outperform the predictive performance of both full feature sets and FS across 28 out of 30 diverse datasets (microarray, bulk and single-cell RNA-Seq, mass spectrometry, imaging, etc.). In short, any arbitrary set of features is as good as any other (with surprisingly low variance in results) - so how can a particular set of selected features be "important" if they perform no better than an arbitrary set? These results challenge the assumption that computationally selected features reliably capture meaningful signals, emphasizing the importance of rigorous validation before interpreting selected features as actionable, particularly in computational genomics.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PAC Apprenticeship Learning with Bayesian Active Inverse Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.03693</link>
<guid>https://arxiv.org/abs/2508.03693</guid>
<content:encoded><![CDATA[
arXiv:2508.03693v2 Announce Type: replace 
Abstract: As AI systems become increasingly autonomous, reliably aligning their decision-making with human preferences is essential. Inverse reinforcement learning (IRL) offers a promising approach to infer preferences from demonstrations. These preferences can then be used to produce an apprentice policy that performs well on the demonstrated task. However, in domains like autonomous driving or robotics, where errors can have serious consequences, we need not just good average performance but reliable policies with formal guarantees -- yet obtaining sufficient human demonstrations for reliability guarantees can be costly. Active IRL addresses this challenge by strategically selecting the most informative scenarios for human demonstration. We introduce PAC-EIG, an information-theoretic acquisition function that directly targets probably-approximately-correct (PAC) guarantees for the learned policy -- providing the first such theoretical guarantee for active IRL with noisy expert demonstrations. Our method maximises information gain about the regret of the apprentice policy, efficiently identifying states requiring further demonstration. We also present Reward-EIG as an alternative when learning the reward itself is the primary objective. Focusing on finite state-action spaces, we prove convergence bounds, illustrate failure modes of prior heuristic methods, and demonstrate our method's advantages experimentally.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CARD: A Cache-Assisted Parallel Speculative Decoding Framework via Query-and-Correct Paradigm for Accelerating LLM Inference</title>
<link>https://arxiv.org/abs/2508.04462</link>
<guid>https://arxiv.org/abs/2508.04462</guid>
<content:encoded><![CDATA[
arXiv:2508.04462v2 Announce Type: replace 
Abstract: Speculative decoding (SD), where a draft model provides multiple candidate tokens for the target model to verify in parallel, has demonstrated significant potential for accelerating LLM inference. Yet, existing SD approaches adhere to a strict draft-then-verify paradigm, enforcing a sequential process that hampers performance and constrains the draft model's capacity. Moreover, rejecting a token in the candidate sequence invalidates all subsequent tokens, leading to wasted computation during drafting. To overcome these limitations, we propose a cache-assisted parallel speculative decoding framework called CARD, which employs a novel query-and-correct paradigm. Our approach decouples drafting from verification: the draft model populates a shared cache with candidate tokens, while the target model concurrently refines the draft's trajectory. This enables inference at near-draft-speed, effectively leveraging the draft model's efficiency without additional fine-tuning. Experimental results show that CARD significantly outperforms existing state-of-the-art methods, achieving up to a 4.83x acceleration over vanilla autoregressive decoding, with no fine-tuning required for either models.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KVCompose: Efficient Structured KV Cache Compression with Composite Tokens</title>
<link>https://arxiv.org/abs/2509.05165</link>
<guid>https://arxiv.org/abs/2509.05165</guid>
<content:encoded><![CDATA[
arXiv:2509.05165v2 Announce Type: replace 
Abstract: Large language models (LLMs) rely on key-value (KV) caches for efficient autoregressive decoding; however, cache size grows linearly with context length and model depth, becoming a major bottleneck in long-context inference. Prior KV cache compression methods either enforce rigid heuristics, disrupt tensor layouts with per-attention-head variability, or require specialized compute kernels.
  We propose a simple, yet effective, KV cache compression framework based on attention-guided, layer-adaptive composite tokens. Our method aggregates attention scores to estimate token importance, selects head-specific tokens independently, and aligns them into composite tokens that respect the uniform cache structure required by existing inference engines. A global allocation mechanism further adapts retention budgets across layers, assigning more capacity to layers with informative tokens. This approach achieves significant memory reduction while preserving accuracy, consistently outperforming prior structured and semi-structured methods. Crucially, our approach remains fully compatible with standard inference pipelines, offering a practical and scalable solution for efficient long-context LLM deployment.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI for Scientific Discovery is a Social Problem</title>
<link>https://arxiv.org/abs/2509.06580</link>
<guid>https://arxiv.org/abs/2509.06580</guid>
<content:encoded><![CDATA[
arXiv:2509.06580v2 Announce Type: replace 
Abstract: Artificial intelligence promises to accelerate scientific discovery, yet its benefits remain unevenly distributed. While technical obstacles such as scarce data, fragmented standards, and unequal access to computation are significant, we argue that the primary barriers are social and institutional. Narratives that defer progress to speculative "AI scientists," the undervaluing of data and infrastructure contributions, misaligned incentives, and gaps between domain experts and machine learning researchers all constrain impact. We highlight four interconnected challenges: community dysfunction, research priorities misaligned with upstream needs, data fragmentation, and infrastructure inequities. We argue that their roots lie in cultural and organizational practices. Addressing them requires not only technical innovation but also intentional community-building, cross-disciplinary education, shared benchmarks, and accessible infrastructure. We call for reframing AI for science as a collective social project, where sustainable collaboration and equitable participation are treated as prerequisites for technical progress.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Riemannian Batch Normalization: A Gyro Approach</title>
<link>https://arxiv.org/abs/2509.07115</link>
<guid>https://arxiv.org/abs/2509.07115</guid>
<content:encoded><![CDATA[
arXiv:2509.07115v2 Announce Type: replace 
Abstract: Normalization layers are crucial for deep learning, but their Euclidean formulations are inadequate for data on manifolds. On the other hand, many Riemannian manifolds in machine learning admit gyro-structures, enabling principled extensions of Euclidean neural networks to non-Euclidean domains. Inspired by this, we introduce GyroBN, a principled Riemannian batch normalization framework for gyrogroups. We establish two necessary conditions, namely \emph{pseudo-reduction} and \emph{gyroisometric gyrations}, that guarantee GyroBN with theoretical control over sample statistics, and show that these conditions hold for all known gyrogroups in machine learning. Our framework also incorporates several existing Riemannian normalization methods as special cases. We further instantiate GyroBN on seven representative geometries, including the Grassmannian, five constant curvature spaces, and the correlation manifold, and derive novel gyro and Riemannian structures to enable these instantiations. Experiments across these geometries demonstrate the effectiveness of GyroBN. The code is available at https://github.com/GitZH-Chen/GyroBN.git.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two Facets of the Same Optimization Coin: Model Degradation and Representation Collapse in Graph Foundation Models</title>
<link>https://arxiv.org/abs/2509.08401</link>
<guid>https://arxiv.org/abs/2509.08401</guid>
<content:encoded><![CDATA[
arXiv:2509.08401v4 Announce Type: replace 
Abstract: Inspired by the success of LLMs, GFMs are designed to learn the optimal embedding functions from multi-domain text-attributed graphs for the downstream cross-task generalization capability. Among the diverse architectures, graph VQ-MAE stands out among the increasingly diverse landscape of GFM. This is attributed to its ability to jointly encode topology and textual attributes from multiple domains into discrete embedding spaces with clear semantic boundaries. Despite its potential, domain generalization conflicts cause imperceptible pitfalls. In this paper, we instantiate two of them, and they are just like two sides of the same GFM optimization coin - Side 1 Model Degradation: The encoder and codebook fail to capture the diversity of inputs; Side 2 Representation Collapse: The hidden embedding and codebook vector fail to preserve semantic separability due to constraints from narrow representation subspaces. These two pitfalls (sides) collectively impair the decoder and generate the low-quality reconstructed supervision, causing the GFM optimization dilemma during pre-training (coin). Through empirical investigation, we attribute the above challenges to Information Bottleneck and Regularization Deficit. To address them, we propose MoT - (1) Information Tinker for Two Pitfalls, which utilizes an edge-wise semantic fusion strategy and a mixture-of-codebooks with domain-aware routing to improve information capacity. (2) Regularization Tinker for Optimization Coin, which utilizes two additional regularizations to further improve gradient supervision in our proposed Information Tinker. Notably, as a flexible architecture, MoT adheres to the scaling laws of GFM, offering a controllable model scale. Compared to SOTA baselines, experiments on 22 datasets across 6 domains demonstrate that MoT achieves significant improvements in supervised, few-shot, and zero-shot scenarios.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-modal Adaptive Estimation for Temporal Respiratory Disease Outbreak</title>
<link>https://arxiv.org/abs/2509.08578</link>
<guid>https://arxiv.org/abs/2509.08578</guid>
<content:encoded><![CDATA[
arXiv:2509.08578v3 Announce Type: replace 
Abstract: Timely and robust influenza incidence forecasting is critical for public health decision-making. This paper presents MAESTRO (Multi-modal Adaptive Estimation for Temporal Respiratory Disease Outbreak), a novel, unified framework that synergistically integrates advanced spectro-temporal modeling with multi-modal data fusion, including surveillance, web search trends, and meteorological data. By adaptively weighting heterogeneous data sources and decomposing complex time series patterns, the model achieves robust and accurate forecasts. Evaluated on over 11 years of Hong Kong influenza data (excluding the COVID-19 period), MAESTRO demonstrates state-of-the-art performance, achieving a superior model fit with an R-square of 0.956. Extensive ablations confirm the significant contributions of its multi-modal and spectro-temporal components. The modular and reproducible pipeline is made publicly available to facilitate deployment and extension to other regions and pathogens, presenting a powerful tool for epidemiological forecasting.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structure Matters: Brain Graph Augmentation via Learnable Edge Masking for Data-efficient Psychiatric Diagnosis</title>
<link>https://arxiv.org/abs/2509.09744</link>
<guid>https://arxiv.org/abs/2509.09744</guid>
<content:encoded><![CDATA[
arXiv:2509.09744v2 Announce Type: replace 
Abstract: The limited availability of labeled brain network data makes it challenging to achieve accurate and interpretable psychiatric diagnoses. While self-supervised learning (SSL) offers a promising solution, existing methods often rely on augmentation strategies that can disrupt crucial structural semantics in brain graphs. To address this, we propose SAM-BG, a two-stage framework for learning brain graph representations with structural semantic preservation. In the pre-training stage, an edge masker is trained on a small labeled subset to capture key structural semantics. In the SSL stage, the extracted structural priors guide a structure-aware augmentation process, enabling the model to learn more semantically meaningful and robust representations. Experiments on two real-world psychiatric datasets demonstrate that SAM-BG outperforms state-of-the-art methods, particularly in small-labeled data settings, and uncovers clinically relevant connectivity patterns that enhance interpretability. Our code is available at https://github.com/mjliu99/SAM-BG.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ForestColl: Throughput-Optimal Collective Communications on Heterogeneous Network Fabrics</title>
<link>https://arxiv.org/abs/2402.06787</link>
<guid>https://arxiv.org/abs/2402.06787</guid>
<content:encoded><![CDATA[
arXiv:2402.06787v4 Announce Type: replace-cross 
Abstract: As modern DNN models grow ever larger, collective communications between the accelerators (allreduce, etc.) emerge as a significant performance bottleneck. Designing efficient communication schedules is challenging, given today's heterogeneous and diverse network fabrics. We present ForestColl, a tool that generates throughput-optimal schedules for any network topology. ForestColl constructs broadcast/aggregation spanning trees as the communication schedule, achieving theoretical optimality. Its schedule generation runs in polynomial time and is highly scalable. ForestColl supports any network fabric, including both switching fabrics and direct accelerator connections. We evaluated ForestColl on AMD MI250 and NVIDIA DGX A100 & H100 clusters. ForestColl showed significant improvements over the vendors' own optimized communication libraries across various settings and in LLM training. ForestColl also outperformed other state-of-the-art schedule generation techniques with both more efficient generated schedules and substantially faster generation speed.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Navigate Beyond Shortcuts: Debiased Learning through the Lens of Neural Collapse</title>
<link>https://arxiv.org/abs/2405.05587</link>
<guid>https://arxiv.org/abs/2405.05587</guid>
<content:encoded><![CDATA[
arXiv:2405.05587v2 Announce Type: replace-cross 
Abstract: Recent studies have noted an intriguing phenomenon termed Neural Collapse, that is, when the neural networks establish the right correlation between feature spaces and the training targets, their last-layer features, together with the classifier weights, will collapse into a stable and symmetric structure. In this paper, we extend the investigation of Neural Collapse to the biased datasets with imbalanced attributes. We observe that models will easily fall into the pitfall of shortcut learning and form a biased, non-collapsed feature space at the early period of training, which is hard to reverse and limits the generalization capability. To tackle the root cause of biased classification, we follow the recent inspiration of prime training, and propose an avoid-shortcut learning framework without additional training complexity. With well-designed shortcut primes based on Neural Collapse structure, the models are encouraged to skip the pursuit of simple shortcuts and naturally capture the intrinsic correlations. Experimental results demonstrate that our method induces better convergence properties during training, and achieves state-of-the-art generalization performance on both synthetic and real-world biased datasets. Our code is available at https://github.com/RachelWolowitz/Navigate-beyond-Shortcuts/tree/main.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training More Robust Classification Model via Discriminative Loss and Gaussian Noise Injection</title>
<link>https://arxiv.org/abs/2405.18499</link>
<guid>https://arxiv.org/abs/2405.18499</guid>
<content:encoded><![CDATA[
arXiv:2405.18499v3 Announce Type: replace-cross 
Abstract: Robustness of deep neural networks to input noise remains a critical challenge, as naive noise injection often degrades accuracy on clean (uncorrupted) data. We propose a novel training framework that addresses this trade-off through two complementary objectives. First, we introduce a loss function applied at the penultimate layer that explicitly enforces intra-class compactness and increases the margin to analytically defined decision boundaries. This enhances feature discriminativeness and class separability for clean data. Second, we propose a class-wise feature alignment mechanism that brings noisy data clusters closer to their clean counterparts. Furthermore, we provide a theoretical analysis demonstrating that improving feature stability under additive Gaussian noise implicitly reduces the curvature of the softmax loss landscape in input space, as measured by Hessian eigenvalues.This thus naturally enhances robustness without explicit curvature penalties. Conversely, we also theoretically show that lower curvatures lead to more robust models. We validate the effectiveness of our method on standard benchmarks and our custom dataset. Our approach significantly reinforces model robustness to various perturbations while maintaining high accuracy on clean data, advancing the understanding and practice of noise-robust deep learning.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Speaker Embeddings for Speech Generation: Sub-Center Modeling for Capturing Intra-Speaker Diversity</title>
<link>https://arxiv.org/abs/2407.04291</link>
<guid>https://arxiv.org/abs/2407.04291</guid>
<content:encoded><![CDATA[
arXiv:2407.04291v3 Announce Type: replace-cross 
Abstract: Modeling the rich prosodic variations inherent in human speech is essential for generating natural-sounding speech. While speaker embeddings are commonly used as conditioning inputs in personalized speech generation, they are typically optimized for speaker recognition, which encourages the loss of intra-speaker variation. This strategy makes them suboptimal for speech generation in terms of modeling the rich variations at the output speech distribution. In this work, we propose a novel speaker embedding network that employs multiple sub-centers per speaker class during training, instead of a single center as in conventional approaches. This sub-center modeling allows the embedding to capture a broader range of speaker-specific variations while maintaining speaker classification performance. We demonstrate the effectiveness of the proposed embeddings on a voice conversion task, showing improved naturalness and prosodic expressiveness in the synthesized speech.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FOVAL: Calibration-Free and Subject-Invariant Fixation Depth Estimation Across Diverse Eye-Tracking Datasets</title>
<link>https://arxiv.org/abs/2408.03591</link>
<guid>https://arxiv.org/abs/2408.03591</guid>
<content:encoded><![CDATA[
arXiv:2408.03591v2 Announce Type: replace-cross 
Abstract: Accurate fixation depth estimation is essential for applications in extended reality (XR), robotics, and human-computer interaction. However, current methods heavily depend on user-specific calibration, which limits their scalability and usability. We introduce FOVAL, a robust calibration-free approach that combines spatiotemporal sequence modelling via Long Short-Term Memory (LSTM) networks with subject-invariant feature engineering and normalisation. Compared to Transformers, Temporal Convolutional Networks (TCNs), and CNNs, FOVAL achieves superior performance, particularly in scenarios with limited and noisy gaze data. Evaluations across three benchmark datasets using Leave-One-Out Cross-Validation (LOOCV) and cross-dataset validation show a mean absolute error (MAE) of 9.1 cm and strong generalisation without calibration. We further analyse inter-subject variability and domain shifts, providing insight into model robustness and adaptation. FOVAL's scalability and accuracy make it highly suitable for real-world deployment.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Learning for Linear Properties of Bounded-Gate Quantum Circuits</title>
<link>https://arxiv.org/abs/2408.12199</link>
<guid>https://arxiv.org/abs/2408.12199</guid>
<content:encoded><![CDATA[
arXiv:2408.12199v2 Announce Type: replace-cross 
Abstract: The vast and complicated large-qubit state space forbids us to comprehensively capture the dynamics of modern quantum computers via classical simulations or quantum tomography. Recent progress in quantum learning theory prompts a crucial question: can linear properties of a large-qubit circuit with d tunable RZ gates and G-d Clifford gates be efficiently learned from measurement data generated by varying classical inputs? In this work, we prove that the sample complexity scaling linearly in $d$ is required to achieve a small prediction error, while the corresponding computational complexity may scale exponentially in d. To address this challenge, we propose a kernel-based method leveraging classical shadows and truncated trigonometric expansions, enabling a controllable trade-off between prediction accuracy and computational overhead. Our results advance two crucial realms in quantum computation: the exploration of quantum algorithms with practical utilities and learning-based quantum system certification. We conduct numerical simulations to validate our proposals across diverse scenarios, encompassing quantum information processing protocols, Hamiltonian simulation, and variational quantum algorithms up to 60 qubits.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Policy Fusion for User Alignment Without Re-Interaction</title>
<link>https://arxiv.org/abs/2409.20016</link>
<guid>https://arxiv.org/abs/2409.20016</guid>
<content:encoded><![CDATA[
arXiv:2409.20016v4 Announce Type: replace-cross 
Abstract: Deep reinforcement learning (RL) policies, although optimal in terms of task rewards, may not align with the personal preferences of human users. To ensure this alignment, a naive solution would be to retrain the agent using a reward function that encodes the user's specific preferences. However, such a reward function is typically not readily available, and as such, retraining the agent from scratch can be prohibitively expensive. We propose a more practical approach - to adapt the already trained policy to user-specific needs with the help of human feedback. To this end, we infer the user's intent through trajectory-level feedback and combine it with the trained task policy via a theoretically grounded dynamic policy fusion approach. As our approach collects human feedback on the very same trajectories used to learn the task policy, it does not require any additional interactions with the environment, making it a zero-shot approach. We empirically demonstrate in a number of environments that our proposed dynamic policy fusion approach consistently achieves the intended task while simultaneously adhering to user-specific needs.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangling Latent Shifts of In-Context Learning with Weak Supervision</title>
<link>https://arxiv.org/abs/2410.01508</link>
<guid>https://arxiv.org/abs/2410.01508</guid>
<content:encoded><![CDATA[
arXiv:2410.01508v2 Announce Type: replace-cross 
Abstract: In-context learning (ICL) enables large language models to perform few-shot learning by conditioning on labeled examples in the prompt. Despite its flexibility, ICL suffers from instability -- especially as prompt length increases with more demonstrations. To address this, we treat ICL as a source of weak supervision and propose a parameter-efficient method that disentangles demonstration-induced latent shifts from those of the query. An ICL-based teacher generates pseudo-labels on unlabeled queries, while a student predicts them using only the query input, updating a lightweight adapter. This captures demonstration effects in a compact, reusable form, enabling efficient inference while remaining composable with new demonstrations. Although trained on noisy teacher outputs, the student often outperforms its teacher through pseudo-label correction and coverage expansion, consistent with the weak-to-strong generalization effect. Empirically, our method improves generalization, stability, and efficiency across both in-domain and out-of-domain tasks, surpassing standard ICL and prior disentanglement methods.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLARE: Faithful Logic-Aided Reasoning and Exploration</title>
<link>https://arxiv.org/abs/2410.11900</link>
<guid>https://arxiv.org/abs/2410.11900</guid>
<content:encoded><![CDATA[
arXiv:2410.11900v5 Announce Type: replace-cross 
Abstract: Modern Question Answering (QA) and Reasoning approaches based on Large Language Models (LLMs) commonly use prompting techniques, such as Chain-of-Thought (CoT), assuming the resulting generation will have a more granular exploration and reasoning over the question space and scope. However, such methods struggle with generating outputs that are faithful to the intermediate chain of reasoning produced by the model. On the other end of the spectrum, neuro-symbolic methods such as Faithful CoT (F-CoT) propose to combine LLMs with external symbolic solvers. While such approaches boast a high degree of faithfulness, they usually require a model trained for code generation and struggle with tasks that are ambiguous or hard to formalise strictly. We introduce $\textbf{F}$aithful $\textbf{L}$ogic-$\textbf{A}$ided $\textbf{R}$easoning and $\textbf{E}$xploration ($\textbf{FLARE}$), a novel interpretable approach for traversing the problem space using task decompositions. We use the LLM to plan a solution, soft-formalise the query into facts and predicates using a logic programming code and simulate that code execution using an exhaustive multi-hop search over the defined space. Our method allows us to compute the faithfulness of the reasoning process w.r.t. the generated code and analyse the steps of the multi-hop search without relying on external solvers. Our methods achieve SOTA results on $\mathbf{7}$ out of $\mathbf{9}$ diverse reasoning benchmarks. We also show that model faithfulness positively correlates with overall performance and further demonstrate that $\textbf{FLARE}$ allows pinpointing the decisive factors sufficient for and leading to the correct answer with optimal reasoning during the multi-hop search.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling the Human Visual System: Comparative Insights from Response-Optimized and Task-Optimized Vision Models, Language Models, and different Readout Mechanisms</title>
<link>https://arxiv.org/abs/2410.14031</link>
<guid>https://arxiv.org/abs/2410.14031</guid>
<content:encoded><![CDATA[
arXiv:2410.14031v5 Announce Type: replace-cross 
Abstract: Over the past decade, predictive modeling of neural responses in the primate visual system has advanced significantly, largely driven by various DNN approaches. These include models optimized directly for visual recognition, cross-modal alignment through contrastive objectives, neural response prediction from scratch, and large language model embeddings.Likewise, different readout mechanisms, ranging from fully linear to spatial-feature factorized methods have been explored for mapping network activations to neural responses. Despite the diversity of these approaches, it remains unclear which method performs best across different visual regions. In this study, we systematically compare these approaches for modeling the human visual system and investigate alternative strategies to improve response predictions. Our findings reveal that for early to mid-level visual areas, response-optimized models with visual inputs offer superior prediction accuracy, while for higher visual regions, embeddings from LLMs based on detailed contextual descriptions of images and task-optimized models pretrained on large vision datasets provide the best fit. Through comparative analysis of these modeling approaches, we identified three distinct regions in the visual cortex: one sensitive primarily to perceptual features of the input that are not captured by linguistic descriptions, another attuned to fine-grained visual details representing semantic information, and a third responsive to abstract, global meanings aligned with linguistic content. We also highlight the critical role of readout mechanisms, proposing a novel scheme that modulates receptive fields and feature maps based on semantic content, resulting in an accuracy boost of 3-23% over existing SOTAs for all models and brain regions. Together, these findings offer key insights into building more precise models of the visual system.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>G2D2: Gradient-Guided Discrete Diffusion for Inverse Problem Solving</title>
<link>https://arxiv.org/abs/2410.14710</link>
<guid>https://arxiv.org/abs/2410.14710</guid>
<content:encoded><![CDATA[
arXiv:2410.14710v2 Announce Type: replace-cross 
Abstract: Recent literature has effectively leveraged diffusion models trained on continuous variables as priors for solving inverse problems. Notably, discrete diffusion models with discrete latent codes have shown strong performance, particularly in modalities suited for discrete compressed representations, such as image and motion generation. However, their discrete and non-differentiable nature has limited their application to inverse problems formulated in continuous spaces. This paper presents a novel method for addressing linear inverse problems by leveraging generative models based on discrete diffusion as priors. We overcome these limitations by approximating the true posterior distribution with a variational distribution constructed from categorical distributions and continuous relaxation techniques. Furthermore, we employ a star-shaped noise process to mitigate the drawbacks of traditional discrete diffusion models with absorbing states, demonstrating that our method performs comparably to continuous diffusion techniques with a lower GPU memory consumption. Our code is available at https://github.com/sony/g2d2.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Neural Curiosity Enhances Learning Flexibility for Autonomous Goal Discovery</title>
<link>https://arxiv.org/abs/2412.00152</link>
<guid>https://arxiv.org/abs/2412.00152</guid>
<content:encoded><![CDATA[
arXiv:2412.00152v2 Announce Type: replace-cross 
Abstract: The autonomous learning of new goals in robotics remains a complex issue to address. Here, we propose a model where curiosity influence learning flexibility. To do so, this paper proposes to root curiosity and attention together by taking inspiration from the Locus Coeruleus-Norepinephrine system along with various cognitive processes such as cognitive persistence and visual habituation. We apply our approach by experimenting with a simulated robotic arm on a set of objects with varying difficulty. The robot first discovers new goals via bottom-up attention through motor babbling with an inhibition of return mechanism, then engage to the learning of goals due to neural activity arising within the curiosity mechanism. The architecture is modelled with dynamic neural fields and the learning of goals such as pushing the objects in diverse directions is supported by the use of forward and inverse models implemented by multi-layer perceptrons. The adoption of dynamic neural fields to model curiosity, habituation and persistence allows the robot to demonstrate various learning trajectories depending on the object. In addition, the approach exhibits interesting properties regarding the learning of similar goals as well as the continuous switch between exploration and exploitation.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking Portrait</title>
<link>https://arxiv.org/abs/2412.01064</link>
<guid>https://arxiv.org/abs/2412.01064</guid>
<content:encoded><![CDATA[
arXiv:2412.01064v5 Announce Type: replace-cross 
Abstract: With the rapid advancement of diffusion-based generative models, portrait image animation has achieved remarkable results. However, it still faces challenges in temporally consistent video generation and fast sampling due to its iterative sampling nature. This paper presents FLOAT, an audio-driven talking portrait video generation method based on flow matching generative model. Instead of a pixel-based latent space, we take advantage of a learned orthogonal motion latent space, enabling efficient generation and editing of temporally consistent motion. To achieve this, we introduce a transformer-based vector field predictor with an effective frame-wise conditioning mechanism. Additionally, our method supports speech-driven emotion enhancement, enabling a natural incorporation of expressive motions. Extensive experiments demonstrate that our method outperforms state-of-the-art audio-driven talking portrait methods in terms of visual quality, motion fidelity, and efficiency.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Permutation recovery of spikes in noisy high-dimensional tensor estimation</title>
<link>https://arxiv.org/abs/2412.14650</link>
<guid>https://arxiv.org/abs/2412.14650</guid>
<content:encoded><![CDATA[
arXiv:2412.14650v3 Announce Type: replace-cross 
Abstract: We study the dynamics of gradient flow in high dimensions for the multi-spiked tensor problem, where the goal is to estimate $r$ unknown signal vectors (spikes) from noisy Gaussian tensor observations. Specifically, we analyze the maximum likelihood estimation procedure, which involves optimizing a highly nonconvex random function. We determine the sample complexity required for gradient flow to efficiently recover all spikes, without imposing any assumptions on the separation of the signal-to-noise ratios (SNRs). More precisely, our results provide the sample complexity required to guarantee recovery of the spikes up to a permutation. Our work builds on our companion paper [Ben Arous, Gerbelot, Piccolo 2024], which studies Langevin dynamics and determines the sample complexity and separation conditions for the SNRs necessary for ensuring exact recovery of the spikes (where the recovered permutation matches the identity). During the recovery process, the correlations between the estimators and the hidden vectors increase in a sequential manner. The order in which these correlations become significant depends on their initial values and the corresponding SNRs, which ultimately determines the permutation of the recovered spikes.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cosmology with Persistent Homology: Parameter Inference via Machine Learning</title>
<link>https://arxiv.org/abs/2412.15405</link>
<guid>https://arxiv.org/abs/2412.15405</guid>
<content:encoded><![CDATA[
arXiv:2412.15405v2 Announce Type: replace-cross 
Abstract: Building upon [2308.02636], we investigate the constraining power of persistent homology on cosmological parameters and primordial non-Gaussianity in a likelihood-free inference pipeline utilizing machine learning. We evaluate the ability of Persistence Images (PIs) to infer parameters, comparing them to the combined Power Spectrum and Bispectrum (PS/BS). We also compare two classes of models: neural-based and tree-based. PIs consistently lead to better predictions compared to the combined PS/BS for parameters that can be constrained, i.e., for $\{\Omega_{\rm m}, \sigma_8, n_{\rm s}, f_{\rm NL}^{\rm loc}\}$. PIs perform particularly well for $f_{\rm NL}^{\rm loc}$, highlighting the potential of persistent homology for constraining primordial non-Gaussianity. Our results indicate that combining PIs with PS/BS provides only marginal gains, indicating that the PS/BS contains little additional or complementary information to the PIs. Finally, we provide a visualization of the most important topological features for $f_{\rm NL}^{\rm loc}$ and for $\Omega_{\rm m}$. This reveals that clusters and voids (0-cycles and 2-cycles) are most informative for $\Omega_{\rm m}$, while $f_{\rm NL}^{\rm loc}$ is additionally informed by filaments (1-cycles).
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>iCBIR-Sli: Interpretable Content-Based Image Retrieval with 2D Slice Embeddings</title>
<link>https://arxiv.org/abs/2501.01642</link>
<guid>https://arxiv.org/abs/2501.01642</guid>
<content:encoded><![CDATA[
arXiv:2501.01642v2 Announce Type: replace-cross 
Abstract: Current methods for searching brain MR images rely on text-based approaches, highlighting a significant need for content-based image retrieval (CBIR) systems. Directly applying 3D brain MR images to machine learning models offers the benefit of effectively learning the brain's structure; however, building the generalized model necessitates a large amount of training data. While models that consider depth direction and utilize continuous 2D slices have demonstrated success in segmentation and classification tasks involving 3D data, concerns remain. Specifically, using general 2D slices may lead to the oversight of pathological features and discontinuities in depth direction information. Furthermore, to the best of the authors' knowledge, there have been no attempts to develop a practical CBIR system that preserves the entire brain's structural information. In this study, we propose an interpretable CBIR method for brain MR images, named iCBIR-Sli (Interpretable CBIR with 2D Slice Embedding), which, for the first time globally, utilizes a series of 2D slices. iCBIR-Sli addresses the challenges associated with using 2D slices by effectively aggregating slice information, thereby achieving low-dimensional representations with high completeness, usability, robustness, and interoperability, which are qualities essential for effective CBIR. In retrieval evaluation experiments utilizing five publicly available brain MR datasets (ADNI2/3, OASIS3/4, AIBL) for Alzheimer's disease and cognitively normal, iCBIR-Sli demonstrated top-1 retrieval performance (macro F1 = 0.859), comparable to existing deep learning models explicitly designed for classification, without the need for an external classifier. Additionally, the method provided high interpretability by clearly identifying the brain regions indicative of the searched-for disease.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Real-time Refinement of Language Model Text Generation</title>
<link>https://arxiv.org/abs/2501.07824</link>
<guid>https://arxiv.org/abs/2501.07824</guid>
<content:encoded><![CDATA[
arXiv:2501.07824v5 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have shown remarkable performance across a wide range of natural language tasks. However, a critical challenge remains in that they sometimes generate factually incorrect answers. To address this, while many previous work has focused on identifying errors in their generation and further refining them, they are slow in deployment since they are designed to verify the response from LLMs only after their entire generation (from the first to last tokens) is done. Further, we observe that once LLMs generate incorrect tokens early on, there is a higher likelihood that subsequent tokens will also be factually incorrect. To this end, in this work, we propose Streaming-VR (Streaming Verification and Refinement), a novel approach designed to enhance the efficiency of verification and refinement of LLM outputs. Specifically, the proposed Streaming-VR enables on-the-fly verification and correction of tokens as they are being generated, similar to a streaming process, ensuring that each subset of tokens is checked and refined in real-time by another LLM as the LLM constructs its response. Through comprehensive evaluations on multiple datasets, we demonstrate that our approach not only enhances the factual accuracy of LLMs, but also offers a more efficient solution compared to prior refinement methods.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Double descent in quantum kernel methods</title>
<link>https://arxiv.org/abs/2501.10077</link>
<guid>https://arxiv.org/abs/2501.10077</guid>
<content:encoded><![CDATA[
arXiv:2501.10077v2 Announce Type: replace-cross 
Abstract: The double descent phenomenon challenges traditional statistical learning theory by revealing scenarios where larger models do not necessarily lead to reduced performance on unseen data. While this counterintuitive behavior has been observed in a variety of classical machine learning models, particularly modern neural network architectures, it remains elusive within the context of quantum machine learning. In this work, we analytically demonstrate that linear regression models in quantum feature spaces can exhibit double descent behavior by drawing on insights from classical linear regression and random matrix theory. Additionally, our numerical experiments on quantum kernel methods across different real-world datasets and system sizes further confirm the existence of a test error peak, a characteristic feature of double descent. Our findings provide evidence that quantum models can operate in the modern, overparameterized regime without experiencing overfitting, potentially opening pathways to improved learning performance beyond traditional statistical learning theory.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Copyright and Competition: Estimating Supply and Demand with Unstructured Data</title>
<link>https://arxiv.org/abs/2501.16120</link>
<guid>https://arxiv.org/abs/2501.16120</guid>
<content:encoded><![CDATA[
arXiv:2501.16120v2 Announce Type: replace-cross 
Abstract: We study the competitive and welfare effects of copyright in creative industries in the face of cost-reducing technologies such as generative artificial intelligence. Creative products often feature unstructured attributes (e.g., images and text) that are complex and high-dimensional. To address this challenge, we study a stylized design product -- fonts -- using data from the world's largest font marketplace. We construct neural network embeddings to quantify unstructured attributes and measure visual similarity in a manner consistent with human perception. Spatial regression and event-study analyses demonstrate that competition is local in the visual characteristics space. Building on this evidence, we develop a structural model of supply and demand that incorporates embeddings and captures product positioning under copyright-based similarity constraints. Our estimates reveal consumers' heterogeneous design preferences and producers' cost-effective mimicry advantages. Counterfactual analyses show that copyright protection can raise consumer welfare by encouraging product relocation, and that the optimal policy depends on the interaction between copyright and cost-reducing technologies.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advances in Multimodal Adaptation and Generalization: From Traditional Approaches to Foundation Models</title>
<link>https://arxiv.org/abs/2501.18592</link>
<guid>https://arxiv.org/abs/2501.18592</guid>
<content:encoded><![CDATA[
arXiv:2501.18592v4 Announce Type: replace-cross 
Abstract: In real-world scenarios, achieving domain adaptation and generalization poses significant challenges, as models must adapt to or generalize across unknown target distributions. Extending these capabilities to unseen multimodal distributions, i.e., multimodal domain adaptation and generalization, is even more challenging due to the distinct characteristics of different modalities. Significant progress has been made over the years, with applications ranging from action recognition to semantic segmentation. Besides, the recent advent of large-scale pre-trained multimodal foundation models, such as CLIP, has inspired works leveraging these models to enhance adaptation and generalization performances or adapting them to downstream tasks. This survey provides the first comprehensive review of recent advances from traditional approaches to foundation models, covering: (1) Multimodal domain adaptation; (2) Multimodal test-time adaptation; (3) Multimodal domain generalization; (4) Domain adaptation and generalization with the help of multimodal foundation models; and (5) Adaptation of multimodal foundation models. For each topic, we formally define the problem and thoroughly review existing methods. Additionally, we analyze relevant datasets and applications, highlighting open challenges and potential future research directions. We maintain an active repository that contains up-to-date literature at https://github.com/donghao51/Awesome-Multimodal-Adaptation.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCoT: Straight Consistent Trajectory for Pre-Trained Diffusion Model Distillations</title>
<link>https://arxiv.org/abs/2502.16972</link>
<guid>https://arxiv.org/abs/2502.16972</guid>
<content:encoded><![CDATA[
arXiv:2502.16972v2 Announce Type: replace-cross 
Abstract: Pre-trained diffusion models are commonly used to generate clean data (e.g., images) from random noises, effectively forming pairs of noises and corresponding clean images. Distillation on these pre-trained models can be viewed as the process of constructing advanced trajectories within the pair to accelerate sampling. For instance, consistency model distillation develops consistent projection functions to regulate trajectories, although sampling efficiency remains a concern. Rectified flow method enforces straight trajectories to enable faster sampling, yet relies on numerical ODE solvers, which may introduce approximation errors. In this work, we bridge the gap between the consistency model and the rectified flow method by proposing a Straight Consistent Trajectory~(SCoT) model. SCoT enjoys the benefits of both approaches for fast sampling, producing trajectories with consistent and straight properties simultaneously. These dual properties are strategically balanced by targeting two critical objectives: (1) regulating the gradient of SCoT's mapping to a constant, (2) ensuring trajectory consistency. Extensive experimental results demonstrate the effectiveness and efficiency of SCoT.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards deployment-centric multimodal AI beyond vision and language</title>
<link>https://arxiv.org/abs/2504.03603</link>
<guid>https://arxiv.org/abs/2504.03603</guid>
<content:encoded><![CDATA[
arXiv:2504.03603v2 Announce Type: replace-cross 
Abstract: Multimodal artificial intelligence (AI) integrates diverse types of data via machine learning to improve understanding, prediction, and decision-making across disciplines such as healthcare, science, and engineering. However, most multimodal AI advances focus on models for vision and language data, while their deployability remains a key challenge. We advocate a deployment-centric workflow that incorporates deployment constraints early to reduce the likelihood of undeployable solutions, complementing data-centric and model-centric approaches. We also emphasise deeper integration across multiple levels of multimodality and multidisciplinary collaboration to significantly broaden the research scope beyond vision and language. To facilitate this approach, we identify common multimodal-AI-specific challenges shared across disciplines and examine three real-world use cases: pandemic response, self-driving car design, and climate change adaptation, drawing expertise from healthcare, social science, engineering, science, sustainability, and finance. By fostering multidisciplinary dialogue and open research practices, our community can accelerate deployment-centric development for broad societal impact.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Temporal Differential Consistency Autoencoder for Efficient and Sustainable Anomaly Detection in Cyber-Physical Systems</title>
<link>https://arxiv.org/abs/2504.06320</link>
<guid>https://arxiv.org/abs/2504.06320</guid>
<content:encoded><![CDATA[
arXiv:2504.06320v2 Announce Type: replace-cross 
Abstract: Cyberattacks on critical infrastructure, particularly water distribution systems, have increased due to rapid digitalization and the integration of IoT devices and industrial control systems (ICS). These cyber-physical systems (CPS) introduce new vulnerabilities, requiring robust and automated intrusion detection systems (IDS) to mitigate potential threats. This study addresses key challenges in anomaly detection by leveraging time correlations in sensor data, integrating physical principles into machine learning models, and optimizing computational efficiency for edge applications. We build upon the concept of temporal differential consistency (TDC) loss to capture the dynamics of the system, ensuring meaningful relationships between dynamic states. Expanding on this foundation, we propose a hybrid autoencoder-based approach, referred to as hybrid TDC-AE, which extends TDC by incorporating both deterministic nodes and conventional statistical nodes. This hybrid structure enables the model to account for non-deterministic processes. Our approach achieves state-of-the-art classification performance while improving time to detect anomalies by 3%, outperforming the BATADAL challenge leader without requiring domain-specific knowledge, making it broadly applicable. Additionally, it maintains the computational efficiency of conventional autoencoders while reducing the number of fully connected layers, resulting in a more sustainable and efficient solution. The method demonstrates how leveraging physics-inspired consistency principles enhances anomaly detection and strengthens the resilience of cyber-physical systems.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AttentionDrop: A Novel Regularization Method for Transformer Models</title>
<link>https://arxiv.org/abs/2504.12088</link>
<guid>https://arxiv.org/abs/2504.12088</guid>
<content:encoded><![CDATA[
arXiv:2504.12088v2 Announce Type: replace-cross 
Abstract: Transformer-based architectures achieve state-of-the-art performance across a wide range of tasks in natural language processing, computer vision, and speech processing. However, their immense capacity often leads to overfitting, especially when training data is limited or noisy. In this research, a unified family of stochastic regularization techniques has been proposed, i.e. AttentionDrop with its three different variants, which operate directly on the self-attention distributions. Hard Attention Masking randomly zeroes out top-k attention logits per query to encourage diverse context utilization, Blurred Attention Smoothing applies a dynamic Gaussian convolution over attention logits to diffuse overly peaked distributions, and Consistency-Regularized AttentionDrop enforces output stability under multiple independent AttentionDrop perturbations via a KL-based consistency loss. Results achieved in the study demonstrate that AttentionDrop consistently improves accuracy, calibration, and adversarial robustness over standard Dropout, DropConnect, and R-Drop baselines
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradient-Free Sequential Bayesian Experimental Design via Interacting Particle Systems</title>
<link>https://arxiv.org/abs/2504.13320</link>
<guid>https://arxiv.org/abs/2504.13320</guid>
<content:encoded><![CDATA[
arXiv:2504.13320v3 Announce Type: replace-cross 
Abstract: We introduce a gradient-free framework for Bayesian Optimal Experimental Design (BOED) in sequential settings, aimed at complex systems where gradient information is unavailable. Our method combines Ensemble Kalman Inversion (EKI) for design optimization with the Affine-Invariant Langevin Dynamics (ALDI) sampler for efficient posterior sampling-both of which are derivative-free and ensemble-based. To address the computational challenges posed by nested expectations in BOED, we propose variational Gaussian and parametrized Laplace approximations that provide tractable upper and lower bounds on the Expected Information Gain (EIG). These approximations enable scalable utility estimation in high-dimensional spaces and PDE-constrained inverse problems. We demonstrate the performance of our framework through numerical experiments ranging from linear Gaussian models to PDE-based inference tasks, highlighting the method's robustness, accuracy, and efficiency in information-driven experimental design.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Examining Deployment and Refinement of the VIOLA-AI Intracranial Hemorrhage Model Using an Interactive NeoMedSys Platform</title>
<link>https://arxiv.org/abs/2505.09380</link>
<guid>https://arxiv.org/abs/2505.09380</guid>
<content:encoded><![CDATA[
arXiv:2505.09380v2 Announce Type: replace-cross 
Abstract: Background: There are many challenges and opportunities in the clinical deployment of AI tools in radiology. The current study describes a radiology software platform called NeoMedSys that can enable efficient deployment and refinements of AI models. We evaluated the feasibility and effectiveness of running NeoMedSys for three months in real-world clinical settings and focused on improvement performance of an in-house developed AI model (VIOLA-AI) designed for intracranial hemorrhage (ICH) detection.
  Methods: NeoMedSys integrates tools for deploying, testing, and optimizing AI models with a web-based medical image viewer, annotation system, and hospital-wide radiology information systems. A prospective pragmatic investigation was deployed using clinical cases of patients presenting to the largest Emergency Department in Norway (site-1) with suspected traumatic brain injury (TBI) or patients with suspected stroke (site-2). We assessed ICH classification performance as VIOLA-AI encountered new data and underwent pre-planned model retraining. Performance metrics included sensitivity, specificity, accuracy, and the area under the receiver operating characteristic curve (AUC).
  Results: NeoMedSys facilitated iterative improvements in the AI model, significantly enhancing its diagnostic accuracy. Automated bleed detection and segmentation were reviewed in near real-time to facilitate re-training VIOLA-AI. The iterative refinement process yielded a marked improvement in classification sensitivity, rising to 90.3% (from 79.2%), and specificity that reached 89.3% (from 80.7%). The bleed detection ROC analysis for the entire sample demonstrated a high area-under-the-curve (AUC) of 0.949 (from 0.873). Model refinement stages were associated with notable gains, highlighting the value of real-time radiologist feedback.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Large Language Models Infer Causal Relationships from Real-World Text?</title>
<link>https://arxiv.org/abs/2505.18931</link>
<guid>https://arxiv.org/abs/2505.18931</guid>
<content:encoded><![CDATA[
arXiv:2505.18931v2 Announce Type: replace-cross 
Abstract: Understanding and inferring causal relationships from texts is a core aspect of human cognition and is essential for advancing large language models (LLMs) towards artificial general intelligence. Existing work evaluating LLM causal reasoning primarily focuses on synthetically generated texts which involve straightforward causal relationships that are explicitly mentioned in the text. This fails to reflect the complexities of real-world tasks. In this paper, we investigate whether LLMs are capable of inferring causal relationships from real-world texts. We develop a benchmark drawn from real-world academic literature which includes diverse texts with respect to length, complexity of relationships (different levels of explicitness, number of nodes, and causal relationships), and domains and sub-domains. To the best of our knowledge, our benchmark is the first-ever real-world dataset for this task. Our experiments on this dataset show that LLMs face significant challenges in inferring causal relationships from real-world text, with the best-performing model achieving an average F1 score of only 0.477. Through systematic analysis across aspects of real-world text (degree of confounding, size of graph, length of text, domain), our benchmark offers targeted insights for further research into advancing LLM causal reasoning.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fairness-in-the-Workflow: How Machine Learning Practitioners at Big Tech Companies Approach Fairness in Recommender Systems</title>
<link>https://arxiv.org/abs/2505.19441</link>
<guid>https://arxiv.org/abs/2505.19441</guid>
<content:encoded><![CDATA[
arXiv:2505.19441v2 Announce Type: replace-cross 
Abstract: Recommender systems (RS), which are widely deployed across high-stakes domains, are susceptible to biases that can cause large-scale societal impacts. Researchers have proposed methods to measure and mitigate such biases -- but translating academic theory into practice is inherently challenging. RS practitioners must balance the competing interests of diverse stakeholders, including providers and users, and operate in dynamic environments. Through a semi-structured interview study (N=11), we map the RS practitioner workflow within large technology companies, focusing on how technical teams consider fairness internally and in collaboration with other (legal, data, and fairness) teams. We identify key challenges to incorporating fairness into existing RS workflows: defining fairness in RS contexts, particularly when navigating multi-stakeholder and dynamic fairness considerations. We also identify key organization-wide challenges: making time for fairness work and facilitating cross-team communication. Finally, we offer actionable recommendations for the RS community, including HCI researchers and practitioners.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>cadrille: Multi-modal CAD Reconstruction with Online Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.22914</link>
<guid>https://arxiv.org/abs/2505.22914</guid>
<content:encoded><![CDATA[
arXiv:2505.22914v2 Announce Type: replace-cross 
Abstract: Computer-Aided Design (CAD) plays a central role in engineering and manufacturing, making it possible to create precise and editable 3D models. Using a variety of sensor or user-provided data as inputs for CAD reconstruction can democratize access to design applications. However, existing methods typically focus on a single input modality, such as point clouds, images, or text, which limits their generalizability and robustness. Leveraging recent advances in vision-language models (VLM), we propose a multi-modal CAD reconstruction model that simultaneously processes all three input modalities. Inspired by large language model (LLM) training paradigms, we adopt a two-stage pipeline: supervised fine-tuning (SFT) on large-scale procedurally generated data, followed by reinforcement learning (RL) fine-tuning using online feedback, obtained programatically. Furthermore, we are the first to explore RL fine-tuning of LLMs for CAD tasks demonstrating that online RL algorithms such as Group Relative Preference Optimization (GRPO) outperform offline alternatives. In the DeepCAD benchmark, our SFT model outperforms existing single-modal approaches in all three input modalities simultaneously. More importantly, after RL fine-tuning, cadrille sets new state-of-the-art on three challenging datasets, including a real-world one.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoCA: Multi-modal Cross-masked Autoencoder for Digital Health Measurements</title>
<link>https://arxiv.org/abs/2506.02260</link>
<guid>https://arxiv.org/abs/2506.02260</guid>
<content:encoded><![CDATA[
arXiv:2506.02260v3 Announce Type: replace-cross 
Abstract: Wearable devices enable continuous multi-modal physiological and behavioral monitoring, yet analysis of these data streams faces fundamental challenges including the lack of gold-standard labels and incomplete sensor data. While self-supervised learning approaches have shown promise for addressing these issues, existing multi-modal extensions present opportunities to better leverage the rich temporal and cross-modal correlations inherent in simultaneously recorded wearable sensor data. We propose the Multi-modal Cross-masked Autoencoder (MoCA), a self-supervised learning framework that combines transformer architecture with masked autoencoder (MAE) methodology, using a principled cross-modality masking scheme that explicitly leverages correlation structures between sensor modalities. MoCA demonstrates strong performance boosts across reconstruction and downstream classification tasks on diverse benchmark datasets. We further establish theoretical guarantees by establishing a fundamental connection between multi-modal MAE loss and kernelized canonical correlation analysis through a Reproducing Kernel Hilbert Space framework, providing principled guidance for correlation-aware masking strategy design. Our approach offers a novel solution for leveraging unlabeled multi-modal wearable data while handling missing modalities, with broad applications across digital health domains.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>World Modelling Improves Language Model Agents</title>
<link>https://arxiv.org/abs/2506.02918</link>
<guid>https://arxiv.org/abs/2506.02918</guid>
<content:encoded><![CDATA[
arXiv:2506.02918v2 Announce Type: replace-cross 
Abstract: Tool use in stateful environments presents unique challenges for large language models (LLMs), where existing test-time compute strategies relying on repeated trials in the environment are impractical. We propose dynamics modelling (DyMo), a method that augments LLMs with a state prediction capability alongside function calling during post-training. This enables LLMs to predict the future states of their actions through an internal environment model. On the Berkeley Function Calling Leaderboard V2, DyMo improves success rates and significantly reduces hallucinations. We further integrate the internal environment model into self-verification sampling (SVS), and show that this substantially improves pass^k over number of trials k, and allows the model to refuse unreliable outputs. Together, DyMo and SVS greatly enhance the effectiveness and reliability of LLMs for tool use. We believe this work charts a path towards scalable planning RL methods for LLM inference without repeatedly querying the oracle environment.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Torsion in Persistent Homology and Neural Networks</title>
<link>https://arxiv.org/abs/2506.03049</link>
<guid>https://arxiv.org/abs/2506.03049</guid>
<content:encoded><![CDATA[
arXiv:2506.03049v3 Announce Type: replace-cross 
Abstract: We explore the role of torsion in hybrid deep learning models that incorporate topological data analysis, focusing on autoencoders. While most TDA tools use field coefficients, this conceals torsional features present in integer homology. We show that torsion can be lost during encoding, altered in the latent space, and in many cases, not reconstructed by standard decoders. Using both synthetic and high-dimensional data, we evaluate torsion sensitivity to perturbations and assess its recoverability across several autoencoder architectures. Our findings reveal key limitations of field-based approaches and underline the need for architectures or loss terms that preserve torsional information for robust data representation.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Impact of Automatic Speech Transcription on Speaker Attribution</title>
<link>https://arxiv.org/abs/2507.08660</link>
<guid>https://arxiv.org/abs/2507.08660</guid>
<content:encoded><![CDATA[
arXiv:2507.08660v2 Announce Type: replace-cross 
Abstract: Speaker attribution from speech transcripts is the task of identifying a speaker from the transcript of their speech based on patterns in their language use. This task is especially useful when the audio is unavailable (e.g. deleted) or unreliable (e.g. anonymized speech). Prior work in this area has primarily focused on the feasibility of attributing speakers using transcripts produced by human annotators. However, in real-world settings, one often only has more errorful transcripts produced by automatic speech recognition (ASR) systems. In this paper, we conduct what is, to our knowledge, the first comprehensive study of the impact of automatic transcription on speaker attribution performance. In particular, we study the extent to which speaker attribution performance degrades in the face of transcription errors, as well as how properties of the ASR system impact attribution. We find that attribution is surprisingly resilient to word-level transcription errors and that the objective of recovering the true transcript is minimally correlated with attribution performance. Overall, our findings suggest that speaker attribution on more errorful transcripts produced by ASR is as good, if not better, than attribution based on human-transcribed data, possibly because ASR transcription errors can capture speaker-specific features revealing of speaker identity.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robustifying Learning-Augmented Caching Efficiently without Compromising 1-Consistency</title>
<link>https://arxiv.org/abs/2507.16242</link>
<guid>https://arxiv.org/abs/2507.16242</guid>
<content:encoded><![CDATA[
arXiv:2507.16242v3 Announce Type: replace-cross 
Abstract: The online caching problem aims to minimize cache misses when serving a sequence of requests under a limited cache size. While naive learning-augmented caching algorithms achieve ideal $1$-consistency, they lack robustness guarantees. Existing robustification methods either sacrifice $1$-consistency or introduce significant computational overhead. In this paper, we introduce Guard, a lightweight robustification framework that enhances the robustness of a broad class of learning-augmented caching algorithms to $2H_k + 2$, while preserving their $1$-consistency. Guard achieves the current best-known trade-off between consistency and robustness, with only $O(1)$ additional per-request overhead, thereby maintaining the original time complexity of the base algorithm. Extensive experiments across multiple real-world datasets and prediction models validate the effectiveness of Guard in practice.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tiny is not small enough: High-quality, low-resource facial animation models through hybrid knowledge distillation</title>
<link>https://arxiv.org/abs/2507.18352</link>
<guid>https://arxiv.org/abs/2507.18352</guid>
<content:encoded><![CDATA[
arXiv:2507.18352v2 Announce Type: replace-cross 
Abstract: The training of high-quality, robust machine learning models for speech-driven 3D facial animation requires a large, diverse dataset of high-quality audio-animation pairs. To overcome the lack of such a dataset, recent work has introduced large pre-trained speech encoders that are robust to variations in the input audio and, therefore, enable the facial animation model to generalize across speakers, audio quality, and languages. However, the resulting facial animation models are prohibitively large and lend themselves only to offline inference on a dedicated machine. In this work, we explore on-device, real-time facial animation models in the context of game development. We overcome the lack of large datasets by using hybrid knowledge distillation with pseudo-labeling. Given a large audio dataset, we employ a high-performing teacher model to train very small student models. In contrast to the pre-trained speech encoders, our student models only consist of convolutional and fully-connected layers, removing the need for attention context or recurrent updates. In our experiments, we demonstrate that we can reduce the memory footprint to up to 3.4 MB and required future audio context to up to 81 ms while maintaining high-quality animations. This paves the way for on-device inference, an important step towards realistic, model-driven digital characters.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MountainLion: A Multi-Modal LLM-Based Agent System for Interpretable and Adaptive Financial Trading</title>
<link>https://arxiv.org/abs/2507.20474</link>
<guid>https://arxiv.org/abs/2507.20474</guid>
<content:encoded><![CDATA[
arXiv:2507.20474v3 Announce Type: replace-cross 
Abstract: Cryptocurrency trading is a challenging task requiring the integration of heterogeneous data from multiple modalities. Traditional deep learning and reinforcement learning approaches typically demand large training datasets and encode diverse inputs into numerical representations, often at the cost of interpretability. Recent progress in large language model (LLM)-based agents has demonstrated the capacity to process multi-modal data and support complex investment decision-making. Building on these advances, we present \textbf{MountainLion}, a multi-modal, multi-agent system for financial trading that coordinates specialized LLM-based agents to interpret financial data and generate investment strategies. MountainLion processes textual news, candlestick charts, and trading signal charts to produce high-quality financial reports, while also enabling modification of reports and investment recommendations through data-driven user interaction and question answering. A central reflection module analyzes historical trading signals and outcomes to continuously refine decision processes, and the system is capable of real-time report analysis, summarization, and dynamic adjustment of investment strategies. Empirical results confirm that MountainLion systematically enriches technical price triggers with contextual macroeconomic and capital flow signals, providing a more interpretable, robust, and actionable investment framework that improves returns and strengthens investor confidence.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundational Design Principles and Patterns for Building Robust and Adaptive GenAI-Native Systems</title>
<link>https://arxiv.org/abs/2508.15411</link>
<guid>https://arxiv.org/abs/2508.15411</guid>
<content:encoded><![CDATA[
arXiv:2508.15411v2 Announce Type: replace-cross 
Abstract: Generative AI (GenAI) has emerged as a transformative technology, demonstrating remarkable capabilities across diverse application domains. However, GenAI faces several major challenges in developing reliable and efficient GenAI-empowered systems due to its unpredictability and inefficiency. This paper advocates for a paradigm shift: future GenAI-native systems should integrate GenAI's cognitive capabilities with traditional software engineering principles to create robust, adaptive, and efficient systems.
  We introduce foundational GenAI-native design principles centered around five key pillars -- reliability, excellence, evolvability, self-reliance, and assurance -- and propose architectural patterns such as GenAI-native cells, organic substrates, and programmable routers to guide the creation of resilient and self-evolving systems. Additionally, we outline the key ingredients of a GenAI-native software stack and discuss the impact of these systems from technical, user adoption, economic, and legal perspectives, underscoring the need for further validation and experimentation. Our work aims to inspire future research and encourage relevant communities to implement and refine this conceptual framework.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SyGra: A Unified Graph-Based Framework for Scalable Generation, Quality Tagging, and Management of Synthetic Data</title>
<link>https://arxiv.org/abs/2508.15432</link>
<guid>https://arxiv.org/abs/2508.15432</guid>
<content:encoded><![CDATA[
arXiv:2508.15432v2 Announce Type: replace-cross 
Abstract: The advancement of large language models (LLMs) is critically dependent on the availability of high-quality datasets for Supervised Fine-Tuning (SFT), alignment tasks like Direct Preference Optimization (DPO), etc. In this work, we present a comprehensive synthetic data generation framework that facilitates scalable, configurable, and high-fidelity generation of synthetic data tailored for these training paradigms. Our approach employs a modular and configuration-based pipeline capable of modeling complex dialogue flows with minimal manual intervention. This framework uses a dual-stage quality tagging mechanism, combining heuristic rules and LLM-based evaluations, to automatically filter and score data extracted from OASST-formatted conversations, ensuring the curation of high-quality dialogue samples. The resulting datasets are structured under a flexible schema supporting both SFT and DPO use cases, enabling seamless integration into diverse training workflows. Together, these innovations offer a robust solution for generating and managing synthetic conversational data at scale, significantly reducing the overhead of data preparation in LLM training pipelines.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PickleBall: Secure Deserialization of Pickle-based Machine Learning Models (Extended Report)</title>
<link>https://arxiv.org/abs/2508.15987</link>
<guid>https://arxiv.org/abs/2508.15987</guid>
<content:encoded><![CDATA[
arXiv:2508.15987v2 Announce Type: replace-cross 
Abstract: Machine learning model repositories such as the Hugging Face Model Hub facilitate model exchanges. However, bad actors can deliver malware through compromised models. Existing defenses such as safer model formats, restrictive (but inflexible) loading policies, and model scanners have shortcomings: 44.9% of popular models on Hugging Face still use the insecure pickle format, 15% of these cannot be loaded by restrictive loading policies, and model scanners have both false positives and false negatives. Pickle remains the de facto standard for model exchange, and the ML community lacks a tool that offers transparent safe loading.
  We present PickleBall to help machine learning engineers load pickle-based models safely. PickleBall statically analyzes the source code of a given machine learning library and computes a custom policy that specifies a safe load-time behavior for benign models. PickleBall then dynamically enforces the policy during load time as a drop-in replacement for the pickle module. PickleBall generates policies that correctly load 79.8% of benign pickle-based models in our dataset, while rejecting all (100%) malicious examples in our dataset. In comparison, evaluated model scanners fail to identify known malicious models, and the state-of-art loader loads 22% fewer benign models than PickleBall. PickleBall removes the threat of arbitrary function invocation from malicious pickle-based models, raising the bar for attackers to depend on code reuse techniques.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LongCat-Flash Technical Report</title>
<link>https://arxiv.org/abs/2509.01322</link>
<guid>https://arxiv.org/abs/2509.01322</guid>
<content:encoded><![CDATA[
arXiv:2509.01322v2 Announce Type: replace-cross 
Abstract: We introduce LongCat-Flash, a 560-billion-parameter Mixture-of-Experts (MoE) language model designed for both computational efficiency and advanced agentic capabilities. Stemming from the need for scalable efficiency, LongCat-Flash adopts two novel designs: (a) Zero-computation Experts, which enables dynamic computational budget allocation and activates 18.6B-31.3B (27B on average) per token depending on contextual demands, optimizing resource usage. (b) Shortcut-connected MoE, which enlarges the computation-communication overlap window, demonstrating notable gains in inference efficiency and throughput compared to models of a comparable scale. We develop a comprehensive scaling framework for large models that combines hyperparameter transfer, model-growth initialization, a multi-pronged stability suite, and deterministic computation to achieve stable and reproducible training. Notably, leveraging the synergy among scalable architectural design and infrastructure efforts, we complete model training on more than 20 trillion tokens within 30 days, while achieving over 100 tokens per second (TPS) for inference at a cost of \$0.70 per million output tokens. To cultivate LongCat-Flash towards agentic intelligence, we conduct a large-scale pre-training on optimized mixtures, followed by targeted mid- and post-training on reasoning, code, and instructions, with further augmentation from synthetic data and tool use tasks. Comprehensive evaluations demonstrate that, as a non-thinking foundation model, LongCat-Flash delivers highly competitive performance among other leading models, with exceptional strengths in agentic tasks. The model checkpoint of LongCat-Flash is open-sourced to foster community research.
  LongCat Chat: https://longcat.ai
  Hugging Face: https://huggingface.co/meituan-longcat
  GitHub: https://github.com/meituan-longcat
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Secure Isn't: Assessing the Security of Machine Learning Model Sharing</title>
<link>https://arxiv.org/abs/2509.06703</link>
<guid>https://arxiv.org/abs/2509.06703</guid>
<content:encoded><![CDATA[
arXiv:2509.06703v2 Announce Type: replace-cross 
Abstract: The rise of model-sharing through frameworks and dedicated hubs makes Machine Learning significantly more accessible. Despite their benefits, these tools expose users to underexplored security risks, while security awareness remains limited among both practitioners and developers. To enable a more security-conscious culture in Machine Learning model sharing, in this paper we evaluate the security posture of frameworks and hubs, assess whether security-oriented mechanisms offer real protection, and survey how users perceive the security narratives surrounding model sharing. Our evaluation shows that most frameworks and hubs address security risks partially at best, often by shifting responsibility to the user. More concerningly, our analysis of frameworks advertising security-oriented settings and complete model sharing uncovered six 0-day vulnerabilities enabling arbitrary code execution. Through this analysis, we debunk the misconceptions that the model-sharing problem is largely solved and that its security can be guaranteed by the file format used for sharing. As expected, our survey shows that the surrounding security narrative leads users to consider security-oriented settings as trustworthy, despite the weaknesses shown in this work. From this, we derive takeaways and suggestions to strengthen the security of model-sharing ecosystems.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Characterizing the Efficiency of Distributed Training: A Power, Performance, and Thermal Perspective</title>
<link>https://arxiv.org/abs/2509.10371</link>
<guid>https://arxiv.org/abs/2509.10371</guid>
<content:encoded><![CDATA[
arXiv:2509.10371v2 Announce Type: replace-cross 
Abstract: The rapid scaling of Large Language Models (LLMs) has pushed training workloads far beyond the limits of single-node analysis, demanding a deeper understanding of how these models behave across large-scale, multi-GPU systems. In this paper, we present a comprehensive characterization of LLM training across diverse real-world workloads and hardware platforms, including NVIDIA H100/H200 and AMD MI250 GPUs. We analyze dense and sparse models under various parallelism strategies -- tensor, pipeline, data, and expert -- and evaluate their effects on hardware utilization, power consumption, and thermal behavior. We further evaluate the effectiveness of optimizations such as activation recomputation and compute-communication overlap. Our findings show that performance is not determined solely by scaling hardware capacity. Scale-up systems with fewer, higher-memory GPUs can outperform scale-out systems in communication-bound regimes, but only under carefully tuned configurations; in other cases, scale-out deployments achieve superior throughput. We also show that certain parallelism combinations, such as tensor with pipeline, lead to bandwidth underutilization due to inefficient data chunking, while increasing microbatch sizes beyond a certain point induces bursty execution and peak power excursions that worsen thermal throttling. These insights reveal how training performance is shaped by complex interactions between hardware, system topology, and model execution. We conclude by offering recommendations for system and hardware design to improve the scalability and reliability of future LLM systems and workloads. The source code of this project is available at https://github.com/sitar-lab/CharLLM-PPT.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.09135</link>
<guid>https://arxiv.org/abs/2509.09135</guid>
<content:encoded><![CDATA[
<div> CTRL, continuous-time RL, CT-MARL, physics-informed neural networks, HJB equation <br />
Summary:
This paper introduces a continuous-time multi-agent reinforcement learning (CT-MARL) framework that utilizes physics-informed neural networks (PINNs) to approximate HJB-based value functions. The framework addresses challenges faced by existing reinforcement learning methods in high-dimensional systems by aligning value learning with value-gradient learning through a Value Gradient Iteration (VGI) module. By iteratively refining value gradients along trajectories, the approach improves gradient fidelity, leading to more accurate values and stronger policy learning. The proposed method is evaluated on continuous-time variants of standard benchmarks, such as the multi-agent particle environment (MPE) and multi-agent MuJoCo, demonstrating superior performance compared to existing continuous-time RL baselines and scalability to complex multi-agent dynamics. This research contributes to advancing the field of continuous-time reinforcement learning, particularly in the multi-agent domain. <br /> <div>
arXiv:2509.09135v2 Announce Type: replace 
Abstract: Existing reinforcement learning (RL) methods struggle with complex dynamical systems that demand interactions at high frequencies or irregular time intervals. Continuous-time RL (CTRL) has emerged as a promising alternative by replacing discrete-time Bellman recursion with differential value functions defined as viscosity solutions of the Hamilton--Jacobi--Bellman (HJB) equation. While CTRL has shown promise, its applications have been largely limited to the single-agent domain. This limitation stems from two key challenges: (i) conventional solution methods for HJB equations suffer from the curse of dimensionality (CoD), making them intractable in high-dimensional systems; and (ii) even with HJB-based learning approaches, accurately approximating centralized value functions in multi-agent settings remains difficult, which in turn destabilizes policy training. In this paper, we propose a CT-MARL framework that uses physics-informed neural networks (PINNs) to approximate HJB-based value functions at scale. To ensure the value is consistent with its differential structure, we align value learning with value-gradient learning by introducing a Value Gradient Iteration (VGI) module that iteratively refines value gradients along trajectories. This improves gradient fidelity, in turn yielding more accurate values and stronger policy learning. We evaluate our method using continuous-time variants of standard benchmarks, including multi-agent particle environment (MPE) and multi-agent MuJoCo. Our results demonstrate that our approach consistently outperforms existing continuous-time RL baselines and scales to complex multi-agent dynamics.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Survivability of Backdoor Attacks on Unconstrained Face Recognition Systems</title>
<link>https://arxiv.org/abs/2507.01607</link>
<guid>https://arxiv.org/abs/2507.01607</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep Learning, Face Recognition Systems, Backdoor Attacks, Security Concerns, Pipeline Configuration <br />
<br />
Summary: This paper presents a detailed analysis of Backdoor Attacks targeting Face Recognition Systems. It highlights the vulnerability of face feature extractors trained with large margin metric learning losses to such attacks. The study investigates multiple pipeline configurations and attack scenarios, demonstrating how a single backdoor can compromise the entire system. The research not only identifies these vulnerabilities but also proposes effective best practices and countermeasures for stakeholders to mitigate the risks associated with Backdoor Attacks on Face Recognition Systems. This comprehensive system-level analysis provides valuable insights for enhancing the security and robustness of face recognition technology in real-world applications. <div>
arXiv:2507.01607v4 Announce Type: replace-cross 
Abstract: The widespread deployment of Deep Learning-based Face Recognition Systems raises multiple security concerns. While prior research has identified backdoor vulnerabilities on isolated components, Backdoor Attacks on real-world, unconstrained pipelines remain underexplored. This paper presents the first comprehensive system-level analysis of Backdoor Attacks targeting Face Recognition Systems and provides three contributions. We first show that face feature extractors trained with large margin metric learning losses are susceptible to Backdoor Attacks. By analyzing 20 pipeline configurations and 15 attack scenarios, we then reveal that a single backdoor can compromise an entire Face Recognition System. Finally, we propose effective best practices and countermeasures for stakeholders.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ALIGNS: Unlocking nomological networks in psychological measurement through a large language model</title>
<link>https://arxiv.org/abs/2509.09723</link>
<guid>https://arxiv.org/abs/2509.09723</guid>
<content:encoded><![CDATA[
<div> Keywords: Psychological measurement, nomological networks, large language models, validation, ALIGNS

Summary: ALIGNS, a system based on large language models, addresses the challenge of building nomological networks in psychological measurement. It provides comprehensive nomological networks in various fields, with over 550,000 indicators. The system was tested for classification accuracy and evaluated in three ways. Firstly, it showed convergence between two widely used NIH PROMIS instruments. Secondly, it identified potential dimensions in child temperament measures that current frameworks may not capture. Lastly, expert psychometricians assessed its importance, accessibility, and suitability. ALIGNS is a valuable tool freely available at nomologicalnetwork.org, enhancing traditional validation methods with large-scale nomological analysis. <br /><br />Summary: <div>
arXiv:2509.09723v2 Announce Type: replace-cross 
Abstract: Psychological measurement is critical to many disciplines. Despite advances in measurement, building nomological networks, theoretical maps of how concepts and measures relate to establish validity, remains a challenge 70 years after Cronbach and Meehl proposed them as fundamental to validation. This limitation has practical consequences: clinical trials may fail to detect treatment effects, and public policy may target the wrong outcomes. We introduce Analysis of Latent Indicators to Generate Nomological Structures (ALIGNS), a large language model-based system trained with validated questionnaire measures. ALIGNS provides three comprehensive nomological networks containing over 550,000 indicators across psychology, medicine, social policy, and other fields. This represents the first application of large language models to solve a foundational problem in measurement validation. We report classification accuracy tests used to develop the model, as well as three evaluations. In the first evaluation, the widely used NIH PROMIS anxiety and depression instruments are shown to converge into a single dimension of emotional distress. The second evaluation examines child temperament measures and identifies four potential dimensions not captured by current frameworks, and questions one existing dimension. The third evaluation, an applicability check, engages expert psychometricians who assess the system's importance, accessibility, and suitability. ALIGNS is freely available at nomologicalnetwork.org, complementing traditional validation methods with large-scale nomological analysis.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering New Theorems via LLMs with In-Context Proof Learning in Lean</title>
<link>https://arxiv.org/abs/2509.14274</link>
<guid>https://arxiv.org/abs/2509.14274</guid>
<content:encoded><![CDATA[
<div> theorem proving, large language models, conjectures, in-context learning, neural theorem proving  
Summary:  
Large Language Models (LLMs) have shown promise in formal theorem proving, with previous works focusing on solving existing problems. This paper introduces the Conjecturing-Proving Loop pipeline, which utilizes LLMs to generate and prove mathematical conjectures in Lean 4 format. A key feature is the ability to generate and prove further conjectures with context from previously generated theorems and proofs, allowing for more challenging proofs through in-context learning of proof strategies without adjusting LLM parameters. The framework successfully rediscovered theorems from past mathematical papers and formalized them. In some cases, the LLM required in-context learning to prove certain theorems, showcasing the effectiveness of neural theorem proving with this approach. The source code for the framework is available on GitHub at https://github.com/auto-res/ConjecturingProvingLoop.  
<br /><br />Summary: <div>
arXiv:2509.14274v1 Announce Type: new 
Abstract: Large Language Models have demonstrated significant promise in formal theorem proving. However, previous works mainly focus on solving existing problems. In this paper, we focus on the ability of LLMs to find novel theorems. We propose Conjecturing-Proving Loop pipeline for automatically generating mathematical conjectures and proving them in Lean 4 format. A feature of our approach is that we generate and prove further conjectures with context including previously generated theorems and their proofs, which enables the generation of more difficult proofs by in-context learning of proof strategies without changing parameters of LLMs. We demonstrated that our framework rediscovered theorems with verification, which were published in past mathematical papers and have not yet formalized. Moreover, at least one of these theorems could not be proved by the LLM without in-context learning, even in natural language, which means that in-context learning was effective for neural theorem proving. The source code is available at https://github.com/auto-res/ConjecturingProvingLoop.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Neural Network for the Identical Kuramoto Equation: Architectural Considerations and Performance Evaluation</title>
<link>https://arxiv.org/abs/2509.14384</link>
<guid>https://arxiv.org/abs/2509.14384</guid>
<content:encoded><![CDATA[
<div> Activation function, Deep Neural Networks, Nonlocal conservation law, Architectural choice, Computational trade-offs

Summary:
This paper explores the efficiency of Deep Neural Networks (DNNs) in approximating the solution of a nonlocal conservation law from the identical-oscillator Kuramoto model. The study focuses on analyzing the impact of different architectural parameters such as activation functions (tanh, sin, ReLU), network depth (4-8 layers), width (64-256 neurons), and training methodologies on solution accuracy and computation time. It is found that tanh activation generally leads to stable convergence, while sine activation can offer slightly lower errors and training times but may introduce nonphysical artifacts. Comparisons with traditional numerical methods show that optimally configured DNNs can provide competitive accuracy with varying computational trade-offs. The study also highlights limitations of standard feed-forward architectures in handling singular or piecewise-constant solutions, indicating the need to overcome inherent oversmoothing of sharp features. This research provides practical guidelines for implementing DNNs in scientific computing and sheds light on theoretical constraints for extending their applicability to complex physical systems with discontinuities.<br /><br />Summary: <div>
arXiv:2509.14384v1 Announce Type: new 
Abstract: In this paper, we investigate the efficiency of Deep Neural Networks (DNNs) to approximate the solution of a nonlocal conservation law derived from the identical-oscillator Kuramoto model, focusing on the evaluation of an architectural choice and its impact on solution accuracy based on the energy norm and computation time. Through systematic experimentation, we demonstrate that network configuration parameters-specifically, activation function selection (tanh vs. sin vs. ReLU), network depth (4-8 hidden layers), width (64-256 neurons), and training methodology (collocation points, epoch count)-significantly influence convergence characteristics. We observe that tanh activation yields stable convergence across configurations, whereas sine activation can attain marginally lower errors and training times in isolated cases, but occasionally produce nonphysical artefacts. Our comparative analysis with traditional numerical methods shows that optimally configured DNNs offer competitive accuracy with notably different computational trade-offs. Furthermore, we identify fundamental limitations of standard feed-forward architectures when handling singular or piecewise-constant solutions, providing empirical evidence that such networks inherently oversmooth sharp features due to the natural function space limitations of standard activation functions. This work contributes to the growing body of research on neural network-based scientific computing by providing practitioners with empirical guidelines for DNN implementation while illuminating fundamental theoretical constraints that must be overcome to expand their applicability to more challenging physical systems with discontinuities.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disproving the Feasibility of Learned Confidence Calibration Under Binary Supervision: An Information-Theoretic Impossibility</title>
<link>https://arxiv.org/abs/2509.14386</link>
<guid>https://arxiv.org/abs/2509.14386</guid>
<content:encoded><![CDATA[
<div> impossibility theorem, neural networks, confidence estimates, binary supervision, calibration diversity <br />
<br />
Summary: 
The article presents an impossibility theorem that neural networks cannot achieve well-calibrated confidence estimates and meaningful diversity when trained using binary correct/incorrect supervision. Through rigorous mathematical analysis and empirical evaluation, it is shown that negative rewards result in extreme underconfidence and lack of diversity in confidence. Symmetric loss functions fail to distinguish between different confidence levels, while post-hoc calibration methods achieve calibration by compressing the confidence distribution rather than learning. The study demonstrates universal failure patterns across MNIST, Fashion-MNIST, and CIFAR-10 datasets, highlighting the necessity of post-hoc calibration. The research suggests novel supervision paradigms such as ensemble disagreement and adaptive multi-agent learning as potential solutions to overcome these fundamental limitations without the need for human confidence annotations. <div>
arXiv:2509.14386v1 Announce Type: new 
Abstract: We prove a fundamental impossibility theorem: neural networks cannot simultaneously learn well-calibrated confidence estimates with meaningful diversity when trained using binary correct/incorrect supervision. Through rigorous mathematical analysis and comprehensive empirical evaluation spanning negative reward training, symmetric loss functions, and post-hoc calibration methods, we demonstrate this is an information-theoretic constraint, not a methodological failure. Our experiments reveal universal failure patterns: negative rewards produce extreme underconfidence (ECE greater than 0.8) while destroying confidence diversity (std less than 0.05), symmetric losses fail to escape binary signal averaging, and post-hoc methods achieve calibration (ECE less than 0.02) only by compressing the confidence distribution. We formalize this as an underspecified mapping problem where binary signals cannot distinguish between different confidence levels for correct predictions: a 60 percent confident correct answer receives identical supervision to a 90 percent confident one. Crucially, our real-world validation shows 100 percent failure rate for all training methods across MNIST, Fashion-MNIST, and CIFAR-10, while post-hoc calibration's 33 percent success rate paradoxically confirms our theorem by achieving calibration through transformation rather than learning. This impossibility directly explains neural network hallucinations and establishes why post-hoc calibration is mathematically necessary, not merely convenient. We propose novel supervision paradigms using ensemble disagreement and adaptive multi-agent learning that could overcome these fundamental limitations without requiring human confidence annotations.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Q-ROAR: Outlier-Aware Rescaling for RoPE Position Interpolation in Quantized Long-Context LLMs</title>
<link>https://arxiv.org/abs/2509.14391</link>
<guid>https://arxiv.org/abs/2509.14391</guid>
<content:encoded><![CDATA[
<div> RoPE-based, position interpolation, post-training quantization, accuracy degradation, position-dependent logit noise <br />
Summary: 
- Extending context windows for long-range tasks is essential.
- Combining position interpolation with post-training quantization leads to accuracy degradation due to various factors.
- Two diagnostics, Interpolation Pressure and Tail Inflation Ratios, are introduced to analyze the effects.
- Q-ROAR, a proposed method, stabilizes RoPE dimensions, groups frequencies, and performs a search to improve accuracy.
- Empirical results show up to 0.7% accuracy recovery on standard tasks and a more than 10% reduction in perplexity for GovReport, maintaining short-context performance and compatibility with existing inference stacks.<br /><br /> <div>
arXiv:2509.14391v1 Announce Type: new 
Abstract: Extending LLM context windows is crucial for long range tasks. RoPE-based position interpolation (PI) methods like linear and frequency-aware scaling extend input lengths without retraining, while post-training quantization (PTQ) enables practical deployment. We show that combining PI with PTQ degrades accuracy due to coupled effects long context aliasing, dynamic range dilation, axis grid anisotropy, and outlier shifting that induce position-dependent logit noise. We provide the first systematic analysis of PI plus PTQ and introduce two diagnostics: Interpolation Pressure (per-band phase scaling sensitivity) and Tail Inflation Ratios (outlier shift from short to long contexts). To address this, we propose Q-ROAR, a RoPE-aware, weight-only stabilization that groups RoPE dimensions into a few frequency bands and performs a small search over per-band scales for W_Q,W_K, with an optional symmetric variant to preserve logit scale. The diagnostics guided search uses a tiny long-context dev set and requires no fine-tuning, kernel, or architecture changes. Empirically, Q-ROAR recovers up to 0.7% accuracy on standard tasks and reduces GovReport perplexity by more than 10%, while preserving short-context performance and compatibility with existing inference stacks.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hashing-Baseline: Rethinking Hashing in the Age of Pretrained Models</title>
<link>https://arxiv.org/abs/2509.14427</link>
<guid>https://arxiv.org/abs/2509.14427</guid>
<content:encoded><![CDATA[
<div> pretrained encoders, compact binary embeddings, hashing, information retrieval, training-free<br />
Summary:<br />
Hashing-Baseline is introduced as a training-free hashing method using pretrained encoders to generate rich embeddings. It combines classical techniques like principal component analysis, random orthogonal projection, and threshold binarization with frozen embeddings from advanced vision and audio encoders. This approach achieves competitive retrieval performance without additional training or fine-tuning. The method is evaluated on standard image retrieval benchmarks and a new audio hashing benchmark, demonstrating its versatility and effectiveness in information retrieval tasks. <div>
arXiv:2509.14427v1 Announce Type: new 
Abstract: Information retrieval with compact binary embeddings, also referred to as hashing, is crucial for scalable fast search applications, yet state-of-the-art hashing methods require expensive, scenario-specific training. In this work, we introduce Hashing-Baseline, a strong training-free hashing method leveraging powerful pretrained encoders that produce rich pretrained embeddings. We revisit classical, training-free hashing techniques: principal component analysis, random orthogonal projection, and threshold binarization, to produce a strong baseline for hashing. Our approach combines these techniques with frozen embeddings from state-of-the-art vision and audio encoders to yield competitive retrieval performance without any additional learning or fine-tuning. To demonstrate the generality and effectiveness of this approach, we evaluate it on standard image retrieval benchmarks as well as a newly introduced benchmark for audio hashing.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedAVOT: Exact Distribution Alignment in Federated Learning via Masked Optimal Transport</title>
<link>https://arxiv.org/abs/2509.14444</link>
<guid>https://arxiv.org/abs/2509.14444</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated Learning, FedAVOT, Optimal Transport, Availability Distribution, Importance Distribution

Summary:
FedAVOT introduces a new approach to Federated Learning called Federated Average with Optimal Transport. It addresses the issue of biased and unstable updates that occur when the availability distribution of users does not align with the importance distribution defining the optimization objective. By formulating aggregation as a masked optimal transport problem, FedAVOT aligns these distributions using Sinkhorn scaling to compute transport-based aggregation weights with guaranteed convergence. This method achieves a standard convergence rate independent of the number of participating users per round, making it highly efficient in nonsmooth convex FL settings. Experimental results demonstrate significantly improved performance compared to FedAvg, particularly in heterogeneous, fairness-sensitive, and low-availability scenarios, even with only two clients participating per round. FedAVOT shows promise for enhancing the efficiency and effectiveness of distributed model training in federated learning settings. 

<br /><br />Summary: <div>
arXiv:2509.14444v1 Announce Type: new 
Abstract: Federated Learning (FL) allows distributed model training without sharing raw data, but suffers when client participation is partial. In practice, the distribution of available users (\emph{availability distribution} $q$) rarely aligns with the distribution defining the optimization objective (\emph{importance distribution} $p$), leading to biased and unstable updates under classical FedAvg. We propose \textbf{Fereated AVerage with Optimal Transport (\textbf{FedAVOT})}, which formulates aggregation as a masked optimal transport problem aligning $q$ and $p$. Using Sinkhorn scaling, \textbf{FedAVOT} computes transport-based aggregation weights with provable convergence guarantees. \textbf{FedAVOT} achieves a standard $\mathcal{O}(1/\sqrt{T})$ rate under a nonsmooth convex FL setting, independent of the number of participating users per round. Our experiments confirm drastically improved performance compared to FedAvg across heterogeneous, fairness-sensitive, and low-availability regimes, even when only two clients participate per round.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>H-Alpha Anomalyzer: An Explainable Anomaly Detector for Solar H-Alpha Observations</title>
<link>https://arxiv.org/abs/2509.14472</link>
<guid>https://arxiv.org/abs/2509.14472</guid>
<content:encoded><![CDATA[
<div> Anomaly detection, H-Alpha observations, GONG network, lightweight algorithm, explainability <br />
<br />Summary:
Astrophysicists are facing a data processing challenge due to the abundance of space and ground-based observatories. The quality of data input into machine learning models is crucial, especially for H$\alpha$ observations from the GONG network. A new algorithm, the H-Alpha Anomalyzer, has been introduced for anomaly detection in these observations. This lightweight algorithm allows for the identification of anomalous observations based on user-defined criteria, highlighting specific regions that triggered the anomaly flag and quantifying the anomaly likelihood. A dataset of 2,000 observations, evenly split between anomalous and non-anomalous cases, was created for comparative analysis. The results show that the proposed model not only outperforms existing methods but also provides explainability, allowing for qualitative evaluation by domain experts. <div>
arXiv:2509.14472v1 Announce Type: new 
Abstract: The plethora of space-borne and ground-based observatories has provided astrophysicists with an unprecedented volume of data, which can only be processed at scale using advanced computing algorithms. Consequently, ensuring the quality of data fed into machine learning (ML) models is critical. The H$\alpha$ observations from the GONG network represent one such data stream, producing several observations per minute, 24/7, since 2010. In this study, we introduce a lightweight (non-ML) anomaly-detection algorithm, called H-Alpha Anomalyzer, designed to identify anomalous observations based on user-defined criteria. Unlike many black-box algorithms, our approach highlights exactly which regions triggered the anomaly flag and quantifies the corresponding anomaly likelihood. For our comparative analysis, we also created and released a dataset of 2,000 observations, equally divided between anomalous and non-anomalous cases. Our results demonstrate that the proposed model not only outperforms existing methods but also provides explainability, enabling qualitative evaluation by domain experts.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Optimization with Topology-Independent Communication</title>
<link>https://arxiv.org/abs/2509.14488</link>
<guid>https://arxiv.org/abs/2509.14488</guid>
<content:encoded><![CDATA[
<div> Keywords: Distributed optimization, Regularizers, Partial separability, Graph-guided regularizers, Communication efficiency <br />
Summary: 
This paper introduces a new approach to distributed optimization that reduces the communication burden by implementing randomized local coordination. Instead of full synchronization, nodes independently sample a single regularizer and only communicate with nodes sharing that term. This method exploits partial separability, where each regularizer depends on a specific subset of nodes. For graph-guided regularizers with subsets of size two, communication is reduced to two messages per iteration. The proposed approach achieves efficient convergence rates for convex objectives, with a complexity of $\tilde{\mathcal{O}}(\varepsilon^{-2})$. It also shows fast convergence to $\varepsilon$-solutions under strong convexity, requiring only $\mathcal{O}(\varepsilon^{-1})$ iterations. By replacing the proximal map of the global regularizer sum with a single randomly selected regularizer's proximal map, the method preserves convergence while eliminating the need for global coordination. Experimental results on synthetic and real-world datasets validate the effectiveness of the approach in terms of both convergence rates and communication efficiency. <br /><br />Summary: <div>
arXiv:2509.14488v1 Announce Type: new 
Abstract: Distributed optimization requires nodes to coordinate, yet full synchronization scales poorly. When $n$ nodes collaborate through $m$ pairwise regularizers, standard methods demand $\mathcal{O}(m)$ communications per iteration. This paper proposes randomized local coordination: each node independently samples one regularizer uniformly and coordinates only with nodes sharing that term. This exploits partial separability, where each regularizer $G_j$ depends on a subset $S_j \subseteq \{1,\ldots,n\}$ of nodes. For graph-guided regularizers where $|S_j|=2$, expected communication drops to exactly 2 messages per iteration. This method achieves $\tilde{\mathcal{O}}(\varepsilon^{-2})$ iterations for convex objectives and under strong convexity, $\mathcal{O}(\varepsilon^{-1})$ to an $\varepsilon$-solution and $\mathcal{O}(\log(1/\varepsilon))$ to a neighborhood. Replacing the proximal map of the sum $\sum_j G_j$ with the proximal map of a single randomly selected regularizer $G_j$ preserves convergence while eliminating global coordination. Experiments validate both convergence rates and communication efficiency across synthetic and real-world datasets.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BEACON: Behavioral Malware Classification with Large Language Model Embeddings and Deep Learning</title>
<link>https://arxiv.org/abs/2509.14519</link>
<guid>https://arxiv.org/abs/2509.14519</guid>
<content:encoded><![CDATA[
<div> Keywords: Malware, Behavioral detection, Deep learning, Large language models, Convolutional neural network

Summary:
Malware detection is facing challenges from increasingly complex threats that use evasion techniques such as code obfuscation and polymorphism. Traditional static analysis methods are often inadequate in addressing these modern threats. Behavioral malware detection, which monitors runtime activities, offers a more reliable approach. BEACON is a new deep learning framework that utilizes large language models to generate contextual embeddings from raw behavior reports. These embeddings capture semantic and structural patterns of malware samples and are fed into a one-dimensional convolutional neural network for classification. The framework, evaluated on the Avast-CTU Public CAPE Dataset, consistently outperforms existing methods. This highlights the effectiveness of leveraging large language models for behavioral embeddings and the overall design of BEACON for robust malware classification. 

<br /><br />Summary: 
Keywords: Malware, Behavioral detection, Deep learning, Large language models, Convolutional neural network <div>
arXiv:2509.14519v1 Announce Type: new 
Abstract: Malware is becoming increasingly complex and widespread, making it essential to develop more effective and timely detection methods. Traditional static analysis often fails to defend against modern threats that employ code obfuscation, polymorphism, and other evasion techniques. In contrast, behavioral malware detection, which monitors runtime activities, provides a more reliable and context-aware solution. In this work, we propose BEACON, a novel deep learning framework that leverages large language models (LLMs) to generate dense, contextual embeddings from raw sandbox-generated behavior reports. These embeddings capture semantic and structural patterns of each sample and are processed by a one-dimensional convolutional neural network (1D CNN) for multi-class malware classification. Evaluated on the Avast-CTU Public CAPE Dataset, our framework consistently outperforms existing methods, highlighting the effectiveness of LLM-based behavioral embeddings and the overall design of BEACON for robust malware classification.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Case Suffixes With Activity Start and End Times: A Sweep-Line Based Approach</title>
<link>https://arxiv.org/abs/2509.14536</link>
<guid>https://arxiv.org/abs/2509.14536</guid>
<content:encoded><![CDATA[
<div> predictive process monitoring, case suffix prediction, resource capacity planning, sweep-line approach, multi-model approach

Summary:
The paper introduces a new technique for case suffix prediction in business processes, aiming to predict both the waiting time and processing time of activities with start and end timestamps. Unlike existing approaches that only predict the end timestamp of activities, this technique enables resource capacity planning by considering periods when resources will be busy. By adopting a sweep-line approach, the technique predicts suffixes of ongoing cases simultaneously, taking into account how resource availability in one case may affect the waiting time in another. Evaluation on real-life and synthetic datasets shows the effectiveness of the multi-model approach in improving the accuracy of case suffix prediction. <div>
arXiv:2509.14536v1 Announce Type: new 
Abstract: Predictive process monitoring techniques support the operational decision making by predicting future states of ongoing cases of a business process. A subset of these techniques predict the remaining sequence of activities of an ongoing case (case suffix prediction). Existing approaches for case suffix prediction generate sequences of activities with a single timestamp (e.g. the end timestamp). This output is insufficient for resource capacity planning, where we need to reason about the periods of time when resources will be busy performing work. This paper introduces a technique for predicting case suffixes consisting of activities with start and end timestamps. In other words, the proposed technique predicts both the waiting time and the processing time of each activity. Since the waiting time of an activity in a case depends on how busy resources are in other cases, the technique adopts a sweep-line approach, wherein the suffixes of all ongoing cases in the process are predicted in lockstep, rather than predictions being made for each case in isolation. An evaluation on real-life and synthetic datasets compares the accuracy of different instantiations of this approach, demonstrating the advantages of a multi-model approach to case suffix prediction.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiMuon: Light and Fast Muon Optimizer for Large Models</title>
<link>https://arxiv.org/abs/2509.14562</link>
<guid>https://arxiv.org/abs/2509.14562</guid>
<content:encoded><![CDATA[
<div> Efficient training, large models, Muon optimizer, randomized Singular Value Decomposition, sample complexity <br />
<br />
Summary: 
The article introduces a new optimizer, LiMuon, designed for efficient training of large models in artificial intelligence. It addresses the limitations of existing Muon optimizers by utilizing a momentum-based variance reduced technique and randomized Singular Value Decomposition. LiMuon offers lower memory usage and improved sample complexity for finding stationary solutions in non-convex stochastic optimization problems, even under generalized smooth conditions. The optimizer is proven to have a sample complexity of O(epsilon^-3) and is particularly suitable for tasks like training large language models. Experimental results on DistilGPT2 and ViT models demonstrate the effectiveness of LiMuon in practice. <div>
arXiv:2509.14562v1 Announce Type: new 
Abstract: Large models recently are widely applied in artificial intelligence, so efficient training of large models has received widespread attention. More recently, a useful Muon optimizer is specifically designed for matrix-structured parameters of large models. Although some works have begun to studying Muon optimizer, the existing Muon and its variants still suffer from high sample complexity or high memory for large models. To fill this gap, we propose a light and fast Muon (LiMuon) optimizer for training large models, which builds on the momentum-based variance reduced technique and randomized Singular Value Decomposition (SVD). Our LiMuon optimizer has a lower memory than the current Muon and its variants. Moreover, we prove that our LiMuon has a lower sample complexity of $O(\epsilon^{-3})$ for finding an $\epsilon$-stationary solution of non-convex stochastic optimization under the smooth condition. Recently, the existing convergence analysis of Muon optimizer mainly relies on the strict Lipschitz smooth assumption, while some artificial intelligence tasks such as training large language models (LLMs) do not satisfy this condition. We also proved that our LiMuon optimizer has a sample complexity of $O(\epsilon^{-3})$ under the generalized smooth condition. Numerical experimental results on training DistilGPT2 and ViT models verify efficiency of our LiMuon optimizer.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Retrieve for Environmental Knowledge Discovery: An Augmentation-Adaptive Self-Supervised Learning Framework</title>
<link>https://arxiv.org/abs/2509.14563</link>
<guid>https://arxiv.org/abs/2509.14563</guid>
<content:encoded><![CDATA[
<div> framework, task-specific data, machine learning, data augmentation, freshwater ecosystems
Summary: 
The article introduces the Augmentation-Adaptive Self-Supervised Learning (A$^2$SL) framework, which aims to enhance modeling of target ecosystems by retrieving relevant observational samples. A multi-level pairwise learning loss is used to train a scenario encoder to capture similarities among scenarios and drive a retrieval mechanism to supplement target scenarios with relevant data. Additionally, an augmentation-adaptive mechanism selectively enhances scenarios through targeted data augmentation, particularly in atypical or extreme conditions. The framework is evaluated in modeling water temperature and dissolved oxygen dynamics in real-world lakes, showing improved predictive accuracy and robustness in data-scarce and atypical scenarios. The A$^2$SL framework offers a broadly applicable solution in various scientific domains. <div>
arXiv:2509.14563v1 Announce Type: new 
Abstract: The discovery of environmental knowledge depends on labeled task-specific data, but is often constrained by the high cost of data collection. Existing machine learning approaches usually struggle to generalize in data-sparse or atypical conditions. To this end, we propose an Augmentation-Adaptive Self-Supervised Learning (A$^2$SL) framework, which retrieves relevant observational samples to enhance modeling of the target ecosystem. Specifically, we introduce a multi-level pairwise learning loss to train a scenario encoder that captures varying degrees of similarity among scenarios. These learned similarities drive a retrieval mechanism that supplements a target scenario with relevant data from different locations or time periods. Furthermore, to better handle variable scenarios, particularly under atypical or extreme conditions where traditional models struggle, we design an augmentation-adaptive mechanism that selectively enhances these scenarios through targeted data augmentation. Using freshwater ecosystems as a case study, we evaluate A$^2$SL in modeling water temperature and dissolved oxygen dynamics in real-world lakes. Experimental results show that A$^2$SL significantly improves predictive accuracy and enhances robustness in data-scarce and atypical scenarios. Although this study focuses on freshwater ecosystems, the A$^2$SL framework offers a broadly applicable solution in various scientific domains.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evidential Physics-Informed Neural Networks for Scientific Discovery</title>
<link>https://arxiv.org/abs/2509.14568</link>
<guid>https://arxiv.org/abs/2509.14568</guid>
<content:encoded><![CDATA[
<div> uncertainty-aware PINN, evidential deep learning, posterior distribution, calibration, clinical glucose-insulin datasets <br />
<br />Summary: 
The article introduces Evidential Physics-Informed Neural Network (E-PINN), a novel approach for uncertainty-aware physics-informed neural networks. E-PINN utilizes the marginal distribution loss function from evidential deep learning to estimate output uncertainty and learns a posterior distribution to infer unknown PDE parameters. Through validation on 1D and 2D case studies, E-PINN demonstrates improved calibration of empirical coverage probabilities compared to Bayesian PINN and Deep Ensemble methods. The model's real-world applicability is demonstrated by analyzing clinical glucose-insulin datasets relevant to diabetes pathophysiology research. E-PINN offers a promising framework for incorporating uncertainty estimation in physics-informed neural networks, with potential applications in a wide range of scientific and engineering domains. <br /> <div>
arXiv:2509.14568v1 Announce Type: new 
Abstract: We present the fundamental theory and implementation guidelines underlying Evidential Physics-Informed Neural Network (E-PINN) -- a novel class of uncertainty-aware PINN. It leverages the marginal distribution loss function of evidential deep learning for estimating uncertainty of outputs, and infers unknown parameters of the PDE via a learned posterior distribution. Validating our model on two illustrative case studies -- the 1D Poisson equation with a Gaussian source and the 2D Fisher-KPP equation, we found that E-PINN generated empirical coverage probabilities that were calibrated significantly better than Bayesian PINN and Deep Ensemble methods. To demonstrate real-world applicability, we also present a brief case study on applying E-PINN to analyze clinical glucose-insulin datasets that have featured in medical research on diabetes pathophysiology.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structure-Preserving Margin Distribution Learning for High-Order Tensor Data with Low-Rank Decomposition</title>
<link>https://arxiv.org/abs/2509.14577</link>
<guid>https://arxiv.org/abs/2509.14577</guid>
<content:encoded><![CDATA[
<div> Tensor Data, Margin Distribution Learning, Low-Rank Decomposition, Classification, Structural Preservation

Summary:
The article introduces a new approach, SPMD-LRT, for learning margin distributions in high-order tensor data without the need for vectorization. By incorporating first and second-order tensor statistics and utilizing low-rank tensor decomposition techniques, SPMD-LRT preserves the spatial structure of the data. An alternating optimization algorithm efficiently solves the objective by updating factor matrices and core tensor iteratively. Experimental results on various datasets show that SPMD-LRT outperforms conventional SVM and vector-based LMDM, as well as prior tensor-based SVM extensions. SPMD-LRT with Tucker decomposition demonstrates the highest accuracy, emphasizing the importance of structure preservation in handling high-dimensional tensor data for classification. <br /><br />Summary: <div>
arXiv:2509.14577v1 Announce Type: new 
Abstract: The Large Margin Distribution Machine (LMDM) is a recent advancement in classifier design that optimizes not just the minimum margin (as in SVM) but the entire margin distribution, thereby improving generalization. However, existing LMDM formulations are limited to vectorized inputs and struggle with high-dimensional tensor data due to the need for flattening, which destroys the data's inherent multi-mode structure and increases computational burden. In this paper, we propose a Structure-Preserving Margin Distribution Learning for High-Order Tensor Data with Low-Rank Decomposition (SPMD-LRT) that operates directly on tensor representations without vectorization. The SPMD-LRT preserves multi-dimensional spatial structure by incorporating first-order and second-order tensor statistics (margin mean and variance) into the objective, and it leverages low-rank tensor decomposition techniques including rank-1(CP), higher-rank CP, and Tucker decomposition to parameterize the weight tensor. An alternating optimization (double-gradient descent) algorithm is developed to efficiently solve the SPMD-LRT, iteratively updating factor matrices and core tensor. This approach enables SPMD-LRT to maintain the structural information of high-order data while optimizing margin distribution for improved classification. Extensive experiments on diverse datasets (including MNIST, images and fMRI neuroimaging) demonstrate that SPMD-LRT achieves superior classification accuracy compared to conventional SVM, vector-based LMDM, and prior tensor-based SVM extensions (Support Tensor Machines and Support Tucker Machines). Notably, SPMD-LRT with Tucker decomposition attains the highest accuracy, highlighting the benefit of structure preservation. These results confirm the effectiveness and robustness of SPMD-LRT in handling high-dimensional tensor data for classification.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online reinforcement learning via sparse Gaussian mixture model Q-functions</title>
<link>https://arxiv.org/abs/2509.14585</link>
<guid>https://arxiv.org/abs/2509.14585</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Online Policy-Iteration Framework, Sparse Gaussian Mixture Model Q-functions, Model Complexity Regulation, Numerical Tests

Summary:
This paper presents a novel online policy-iteration framework for reinforcement learning (RL) using sparse Gaussian mixture model Q-functions (S-GMM-QFs). The framework incorporates an online scheme that utilizes streaming data to promote exploration and regulates model complexity through sparsification. By leveraging Hadamard overparametrization, overfitting is minimized while maintaining expressiveness. The parameter space of S-GMM-QFs is structured as a Riemannian manifold, allowing for principled parameter updates through online gradient descent on a smooth objective. Results from numerical tests demonstrate that S-GMM-QFs achieve comparable performance to dense deep RL methods on standard benchmarks with significantly fewer parameters. Moreover, S-GMM-QFs maintain strong performance in low-parameter-count scenarios where sparsified DeepRL methods struggle to generalize. <div>
arXiv:2509.14585v1 Announce Type: new 
Abstract: This paper introduces a structured and interpretable online policy-iteration framework for reinforcement learning (RL), built around the novel class of sparse Gaussian mixture model Q-functions (S-GMM-QFs). Extending earlier work that trained GMM-QFs offline, the proposed framework develops an online scheme that leverages streaming data to encourage exploration. Model complexity is regulated through sparsification by Hadamard overparametrization, which mitigates overfitting while preserving expressiveness. The parameter space of S-GMM-QFs is naturally endowed with a Riemannian manifold structure, allowing for principled parameter updates via online gradient descent on a smooth objective. Numerical tests show that S-GMM-QFs match the performance of dense deep RL (DeepRL) methods on standard benchmarks while using significantly fewer parameters, and maintain strong performance even in low-parameter-count regimes where sparsified DeepRL methods fail to generalize.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TICA-Based Free Energy Matching for Machine-Learned Molecular Dynamics</title>
<link>https://arxiv.org/abs/2509.14600</link>
<guid>https://arxiv.org/abs/2509.14600</guid>
<content:encoded><![CDATA[
<div> Machine learning, molecular dynamics, biomolecular systems, coarse-grained models, Chignolin protein <br />
<br />
Summary:
- Molecular dynamics simulations can provide detailed insights into biomolecular systems but are often limited by computational costs.
- Coarse-grained machine learning models show promise in accelerating sampling processes.
- Conventional force matching approaches may not fully capture the thermodynamic landscape.
- Incorporating an energy matching term in the loss function can offer new insights into how models generalize the free energy surface.
- While energy matching did not show significant improvements in accuracy for the Chignolin protein, it highlighted potential opportunities for enhancing coarse-grained modeling through improved energy estimation techniques and multi-modal loss formulations.  <br /> <div>
arXiv:2509.14600v1 Announce Type: new 
Abstract: Molecular dynamics (MD) simulations provide atomistic insight into biomolecular systems but are often limited by high computational costs required to access long timescales. Coarse-grained machine learning models offer a promising avenue for accelerating sampling, yet conventional force matching approaches often fail to capture the full thermodynamic landscape as fitting a model on the gradient may not fit the absolute differences between low-energy conformational states. In this work, we incorporate a complementary energy matching term into the loss function. We evaluate our framework on the Chignolin protein using the CGSchNet model, systematically varying the weight of the energy loss term. While energy matching did not yield statistically significant improvements in accuracy, it revealed distinct tendencies in how models generalize the free energy surface. Our results suggest future opportunities to enhance coarse-grained modeling through improved energy estimation techniques and multi-modal loss formulations.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Privacy-Preserving and Heterogeneity-aware Split Federated Learning via Probabilistic Masking</title>
<link>https://arxiv.org/abs/2509.14603</link>
<guid>https://arxiv.org/abs/2509.14603</guid>
<content:encoded><![CDATA[
<div> scalable, privacy-preserving, Federated Learning, Probabilistic Mask, structured randomness

Summary:
PM-SFL is a novel Split Federated Learning framework that enhances privacy protection and maintains model utility by incorporating Probabilistic Mask training to add structured randomness. This approach effectively mitigates data reconstruction risks while addressing data and system heterogeneity. Personalized mask learning tailors submodel structures to each client's local data, while layer-wise knowledge compensation ensures effective participation of clients with varying resources. Theoretical analysis validates the privacy protection of PM-SFL, and experiments show improved accuracy, communication efficiency, and robustness to privacy attacks, especially under data and system heterogeneity. PM-SFL presents a promising alternative to traditional Federated Learning by providing a scalable and privacy-preserving solution without compromising model performance. 

<br /><br /> <div>
arXiv:2509.14603v1 Announce Type: new 
Abstract: Split Federated Learning (SFL) has emerged as an efficient alternative to traditional Federated Learning (FL) by reducing client-side computation through model partitioning. However, exchanging of intermediate activations and model updates introduces significant privacy risks, especially from data reconstruction attacks that recover original inputs from intermediate representations. Existing defenses using noise injection often degrade model performance. To overcome these challenges, we present PM-SFL, a scalable and privacy-preserving SFL framework that incorporates Probabilistic Mask training to add structured randomness without relying on explicit noise. This mitigates data reconstruction risks while maintaining model utility. To address data heterogeneity, PM-SFL employs personalized mask learning that tailors submodel structures to each client's local data. For system heterogeneity, we introduce a layer-wise knowledge compensation mechanism, enabling clients with varying resources to participate effectively under adaptive model splitting. Theoretical analysis confirms its privacy protection, and experiments on image and wireless sensing tasks demonstrate that PM-SFL consistently improves accuracy, communication efficiency, and robustness to privacy attacks, with particularly strong performance under data and system heterogeneity.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HD3C: Efficient Medical Data Classification for Embedded Devices</title>
<link>https://arxiv.org/abs/2509.14617</link>
<guid>https://arxiv.org/abs/2509.14617</guid>
<content:encoded><![CDATA[
<div> Keywords: Hyperdimensional Computing, Class-Wise Clustering, Energy-efficient, Medical Data Classification, Low-power Environments

Summary:
Hyperdimensional Computing with Class-Wise Clustering (HD3C) is a lightweight classification framework designed for energy-efficient medical data classification on low-power devices. It encodes data into high-dimensional hypervectors, aggregates them into cluster-specific prototypes, and performs classification through similarity search in hyperspace. HD3C achieves exceptional energy efficiency, being 350 times more efficient than Bayesian ResNet with minimal accuracy difference in heart sound classification. The framework also demonstrates robustness to noise, limited training data, and hardware errors, making it suitable for real-world deployment. Theoretical analysis and empirical results substantiate its reliability in various environments. Code for HD3C is available on GitHub for further exploration and implementation purposes.

<br /><br />Summary: Hyperdimensional Computing with Class-Wise Clustering (HD3C) offers an energy-efficient solution for medical data classification, particularly in low-power environments. By encoding data into high-dimensional vectors and utilizing cluster-specific prototypes, HD3C enables accurate classification through similarity search in hyperspace. It outperforms existing methods significantly in terms of energy efficiency and showcases robustness to noise, limited data, and hardware errors, making it a reliable choice for real-world applications. The availability of code on GitHub further facilitates the adoption and customization of HD3C for various medical classification tasks. <div>
arXiv:2509.14617v1 Announce Type: new 
Abstract: Energy-efficient medical data classification is essential for modern disease screening, particularly in home and field healthcare where embedded devices are prevalent. While deep learning models achieve state-of-the-art accuracy, their substantial energy consumption and reliance on GPUs limit deployment on such platforms. We present Hyperdimensional Computing with Class-Wise Clustering (HD3C), a lightweight classification framework designed for low-power environments. HD3C encodes data into high-dimensional hypervectors, aggregates them into multiple cluster-specific prototypes, and performs classification through similarity search in hyperspace. We evaluate HD3C across three medical classification tasks; on heart sound classification, HD3C is $350\times$ more energy-efficient than Bayesian ResNet with less than 1% accuracy difference. Moreover, HD3C demonstrates exceptional robustness to noise, limited training data, and hardware error, supported by both theoretical analysis and empirical results, highlighting its potential for reliable deployment in real-world settings. Code is available at https://github.com/jianglanwei/HD3C.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CUFG: Curriculum Unlearning Guided by the Forgetting Gradient</title>
<link>https://arxiv.org/abs/2509.14633</link>
<guid>https://arxiv.org/abs/2509.14633</guid>
<content:encoded><![CDATA[
<div> framework, unlearning, stability, curriculum, gradients  
Summary:  
CUFG (Curriculum Unlearning via Forgetting Gradients) is introduced as a novel framework that enhances the stability of approximate unlearning in AI models. Existing methods of unlearning tend to prioritize efficiency and aggressive forgetting, which can lead to destabilization of model weights and reduced reliability. CUFG addresses this issue by incorporating a gradient corrector guided by forgetting gradients and a curriculum unlearning paradigm that progressively forgets from easy to hard examples. These innovations result in more stable and progressive unlearning, improving both effectiveness and reliability compared to traditional methods. The concept of curriculum unlearning is shown to have significant research potential and offers insights for the development of the AI field. Extensive experiments across various scenarios validate the effectiveness of CUFG. <div>
arXiv:2509.14633v1 Announce Type: new 
Abstract: As privacy and security take center stage in AI, machine unlearning, the ability to erase specific knowledge from models, has garnered increasing attention. However, existing methods overly prioritize efficiency and aggressive forgetting, which introduces notable limitations. In particular, radical interventions like gradient ascent, influence functions, and random label noise can destabilize model weights, leading to collapse and reduced reliability. To address this, we propose CUFG (Curriculum Unlearning via Forgetting Gradients), a novel framework that enhances the stability of approximate unlearning through innovations in both forgetting mechanisms and data scheduling strategies. Specifically, CUFG integrates a new gradient corrector guided by forgetting gradients for fine-tuning-based unlearning and a curriculum unlearning paradigm that progressively forgets from easy to hard. These innovations narrow the gap with the gold-standard Retrain method by enabling more stable and progressive unlearning, thereby improving both effectiveness and reliability. Furthermore, we believe that the concept of curriculum unlearning has substantial research potential and offers forward-looking insights for the development of the MU field. Extensive experiments across various forgetting scenarios validate the rationale and effectiveness of our approach and CUFG. Codes are available at https://anonymous.4open.science/r/CUFG-6375.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DyWPE: Signal-Aware Dynamic Wavelet Positional Encoding for Time Series Transformers</title>
<link>https://arxiv.org/abs/2509.14640</link>
<guid>https://arxiv.org/abs/2509.14640</guid>
<content:encoded><![CDATA[
<div> wavelet encoding, positional encoding, transformers, time series analysis, signal characteristics
<br />
Summary:
The article introduces Dynamic Wavelet Positional Encoding (DyWPE), a new positional encoding method that considers signal characteristics in transformers for time series analysis. Existing methods are limited in capturing complex, non-stationary signal dynamics. DyWPE utilizes the Discrete Wavelet Transform (DWT) to generate positional embeddings directly from input time series, outperforming eight state-of-the-art methods across ten diverse datasets. In biomedical signals, DyWPE shows an average relative improvement of 9.1% compared to baseline sinusoidal absolute position encoding while maintaining computational efficiency. This approach addresses the limitation of signal-agnostic positional encoding methods, offering a more effective way to capture temporal information in transformers. DyWPE demonstrates superior performance in capturing diverse signal characteristics and dynamic temporal scales, making it a promising advancement for time series analysis in transformer models.
<br /> <div>
arXiv:2509.14640v1 Announce Type: new 
Abstract: Existing positional encoding methods in transformers are fundamentally signal-agnostic, deriving positional information solely from sequence indices while ignoring the underlying signal characteristics. This limitation is particularly problematic for time series analysis, where signals exhibit complex, non-stationary dynamics across multiple temporal scales. We introduce Dynamic Wavelet Positional Encoding (DyWPE), a novel signal-aware framework that generates positional embeddings directly from input time series using the Discrete Wavelet Transform (DWT). Comprehensive experiments in ten diverse time series datasets demonstrate that DyWPE consistently outperforms eight existing state-of-the-art positional encoding methods, achieving average relative improvements of 9.1\% compared to baseline sinusoidal absolute position encoding in biomedical signals, while maintaining competitive computational efficiency.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeCoP: Enhancing Self-Supervised Time Series Representation with Dependency Controlled Pre-training</title>
<link>https://arxiv.org/abs/2509.14642</link>
<guid>https://arxiv.org/abs/2509.14642</guid>
<content:encoded><![CDATA[
<div> Instance-wise Patch Normalization, Dependency Controlled Pre-training, multi-scale dependencies, representation learning, state-of-the-art results

Summary:
- Dynamic temporal dependencies in time series pre-training pose a challenge due to distribution shifts and multi-scale patterns.
- Existing frameworks lack the ability to capture short- and long-term dependencies effectively, leading to degraded generalization.
- DeCoP introduces Instance-wise Patch Normalization (IPN) at the input level to mitigate distributional shifts while preserving patch characteristics.
- DeCoP employs a hierarchical Dependency Controlled Learning (DCL) strategy to model inter-patch dependencies across multiple temporal scales.
- An Instance-level Contrastive Module (ICM) enhances global generalization by learning instance-discriminative representations from time-invariant positive pairs.
- DeCoP outperforms existing frameworks on ten datasets with lower computing resources, improving Mean Squared Error (MSE) by 3% on ETTh1 over PatchTST using only 37% of the FLOPs.<br /><br />Summary: <div>
arXiv:2509.14642v1 Announce Type: new 
Abstract: Modeling dynamic temporal dependencies is a critical challenge in time series pre-training, which evolve due to distribution shifts and multi-scale patterns. This temporal variability severely impairs the generalization of pre-trained models to downstream tasks. Existing frameworks fail to capture the complex interactions of short- and long-term dependencies, making them susceptible to spurious correlations that degrade generalization. To address these limitations, we propose DeCoP, a Dependency Controlled Pre-training framework that explicitly models dynamic, multi-scale dependencies by simulating evolving inter-patch dependencies. At the input level, DeCoP introduces Instance-wise Patch Normalization (IPN) to mitigate distributional shifts while preserving the unique characteristics of each patch, creating a robust foundation for representation learning. At the latent level, a hierarchical Dependency Controlled Learning (DCL) strategy explicitly models inter-patch dependencies across multiple temporal scales, with an Instance-level Contrastive Module (ICM) enhances global generalization by learning instance-discriminative representations from time-invariant positive pairs. DeCoP achieves state-of-the-art results on ten datasets with lower computing resources, improving MSE by 3% on ETTh1 over PatchTST using only 37% of the FLOPs.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic Clock Attention for Aligning Continuous and Ordered Sequences</title>
<link>https://arxiv.org/abs/2509.14678</link>
<guid>https://arxiv.org/abs/2509.14678</guid>
<content:encoded><![CDATA[
<div> alignment mechanism, continuous sequences, sequence-to-sequence tasks, learned clocks, Transformer text-to-speech

Summary: 
This study introduces an attention mechanism for continuous and ordered sequences that functions as an alignment model for sequence-to-sequence tasks. By using learned nonnegative clocks for source and target sequences, the attention mechanism calculates the meeting probability of these clocks to generate smooth and causal alignments. The proposed framework supports both parallel and autoregressive decoding without the need for external positional regularizers. In a Transformer text-to-speech experiment, this approach enhances alignment stability and robustness while maintaining or improving accuracy compared to traditional methods. The researchers suggest that this method could have applications in modeling other continuous targets such as videos and temporal signals. <div>
arXiv:2509.14678v1 Announce Type: new 
Abstract: We formulate an attention mechanism for continuous and ordered sequences that explicitly functions as an alignment model, which serves as the core of many sequence-to-sequence tasks. Standard scaled dot-product attention relies on positional encodings and masks but does not enforce continuity or monotonicity, which are crucial for frame-synchronous targets. We propose learned nonnegative \emph{clocks} to source and target and model attention as the meeting probability of these clocks; a path-integral derivation yields a closed-form, Gaussian-like scoring rule with an intrinsic bias toward causal, smooth, near-diagonal alignments, without external positional regularizers. The framework supports two complementary regimes: normalized clocks for parallel decoding when a global length is available, and unnormalized clocks for autoregressive decoding -- both nearly-parameter-free, drop-in replacements. In a Transformer text-to-speech testbed, this construction produces more stable alignments and improved robustness to global time-scaling while matching or improving accuracy over scaled dot-product baselines. We hypothesize applicability to other continuous targets, including video and temporal signal modeling.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToolSample: Dual Dynamic Sampling Methods with Curriculum Learning for RL-based Tool Learning</title>
<link>https://arxiv.org/abs/2509.14718</link>
<guid>https://arxiv.org/abs/2509.14718</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, tool learning, dynamic sampling, curriculum learning, multi-task structure<br />
Summary: <br />
This paper introduces Dynamic Sampling with Curriculum Learning (DSCL), a framework designed to improve the efficiency of reinforcement learning-based tool learning. DSCL targets the unique characteristics of tool learning by utilizing Reward-Based Dynamic Sampling and Task-Based Dynamic Curriculum Learning. By prioritizing valuable data based on multi-dimensional reward statistics and adaptively focusing training on less-mastered sub-tasks, DSCL significantly enhances training efficiency and model performance. Through extensive experiments, DSCL achieved a 3.29% improvement on the BFCLv3 benchmark. The method effectively leverages the complex reward signals and sub-task dynamics within tool learning to yield superior results. <div>
arXiv:2509.14718v1 Announce Type: new 
Abstract: While reinforcement learning (RL) is increasingly used for LLM-based tool learning, its efficiency is often hampered by an overabundance of simple samples that provide diminishing learning value as training progresses. Existing dynamic sampling techniques are ill-suited for the multi-task structure and fine-grained reward mechanisms inherent to tool learning. This paper introduces Dynamic Sampling with Curriculum Learning (DSCL), a framework specifically designed to address this challenge by targeting the unique characteristics of tool learning: its multiple interdependent sub-tasks and multi-valued reward functions. DSCL features two core components: Reward-Based Dynamic Sampling, which uses multi-dimensional reward statistics (mean and variance) to prioritize valuable data, and Task-Based Dynamic Curriculum Learning, which adaptively focuses training on less-mastered sub-tasks. Through extensive experiments, we demonstrate that DSCL significantly improves training efficiency and model performance over strong baselines, achieving a 3.29\% improvement on the BFCLv3 benchmark. Our method provides a tailored solution that effectively leverages the complex reward signals and sub-task dynamics within tool learning to achieve superior results.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Pre-trained Graph Condensation via Optimal Transport</title>
<link>https://arxiv.org/abs/2509.14722</link>
<guid>https://arxiv.org/abs/2509.14722</guid>
<content:encoded><![CDATA[
<div> Graph condensation, GNN training, optimization consistency, pre-trained, optimal transport<br />
Summary:<br />
Graph condensation (GC) aims to distill large graphs into smaller ones to speed up GNN training. Traditional GC methods require specific GNNs and supervision, limiting their reusability. This work introduces PreGC using optimal transport to create task-independent condensed graphs. A hybrid-interval graph diffusion augmentation improves generalization across architectures. Matching optimal transport plans maintain semantic consistency between original and condensed graphs. A semantic harmonizer facilitates adapting condensed graphs to different tasks. Experimental results show PreGC's superiority and versatility, highlighting its task-independent nature and compatibility with various GNNs. <div>
arXiv:2509.14722v1 Announce Type: new 
Abstract: Graph condensation (GC) aims to distill the original graph into a small-scale graph, mitigating redundancy and accelerating GNN training. However, conventional GC approaches heavily rely on rigid GNNs and task-specific supervision. Such a dependency severely restricts their reusability and generalization across various tasks and architectures. In this work, we revisit the goal of ideal GC from the perspective of GNN optimization consistency, and then a generalized GC optimization objective is derived, by which those traditional GC methods can be viewed nicely as special cases of this optimization paradigm. Based on this, Pre-trained Graph Condensation (PreGC) via optimal transport is proposed to transcend the limitations of task- and architecture-dependent GC methods. Specifically, a hybrid-interval graph diffusion augmentation is presented to suppress the weak generalization ability of the condensed graph on particular architectures by enhancing the uncertainty of node states. Meanwhile, the matching between optimal graph transport plan and representation transport plan is tactfully established to maintain semantic consistencies across source graph and condensed graph spaces, thereby freeing graph condensation from task dependencies. To further facilitate the adaptation of condensed graphs to various downstream tasks, a traceable semantic harmonizer from source nodes to condensed nodes is proposed to bridge semantic associations through the optimized representation transport plan in pre-training. Extensive experiments verify the superiority and versatility of PreGC, demonstrating its task-independent nature and seamless compatibility with arbitrary GNNs.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transcoder-based Circuit Analysis for Interpretable Single-Cell Foundation Models</title>
<link>https://arxiv.org/abs/2509.14723</link>
<guid>https://arxiv.org/abs/2509.14723</guid>
<content:encoded><![CDATA[
<div> Keywords: Single-cell foundation models, gene regulatory networks, transcoders, interpretable decision-making, biological mechanisms

Summary: 
Single-cell foundation models (scFMs) have shown superior performance in tasks like cell-type annotation and perturbation response prediction through learning gene regulatory networks from transcriptome data. However, interpreting their decision-making processes remains a challenge. A new approach involves training a transcoder on the cell2sentence (C2S) model, a leading scFM, to extract interpretable decision circuits. The extracted circuits align with real biological mechanisms, revealing the potential of transcoders in uncovering biologically plausible pathways within complex single-cell models. This method enhances the interpretability of scFMs, aiding in understanding the underlying biology and improving the utility of these models in biological research.<br /><br />Summary: <div>
arXiv:2509.14723v1 Announce Type: new 
Abstract: Single-cell foundation models (scFMs) have demonstrated state-of-the-art performance on various tasks, such as cell-type annotation and perturbation response prediction, by learning gene regulatory networks from large-scale transcriptome data. However, a significant challenge remains: the decision-making processes of these models are less interpretable compared to traditional methods like differential gene expression analysis. Recently, transcoders have emerged as a promising approach for extracting interpretable decision circuits from large language models (LLMs). In this work, we train a transcoder on the cell2sentence (C2S) model, a state-of-the-art scFM. By leveraging the trained transcoder, we extract internal decision-making circuits from the C2S model. We demonstrate that the discovered circuits correspond to real-world biological mechanisms, confirming the potential of transcoders to uncover biologically plausible pathways within complex single-cell models.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One-step Multi-view Clustering With Adaptive Low-rank Anchor-graph Learning</title>
<link>https://arxiv.org/abs/2509.14724</link>
<guid>https://arxiv.org/abs/2509.14724</guid>
<content:encoded><![CDATA[
<div> Keywords: anchor graph, multi-view clustering, consensus anchor graph, low-rank learning, clustering effectiveness.

Summary: 
OMCAL is a novel one-step multi-view clustering method that addresses the limitations of existing anchor graph-based methods. By incorporating adaptive low-rank anchor-graph learning, OMCAL effectively constructs a high-quality consensus anchor graph (CAG) by reducing information redundancy and noise interference. Unlike previous methods, OMCAL integrates category indicator acquisition and CAG learning into a unified framework, improving clustering effectiveness and efficiency. Experimental results on various datasets demonstrate that OMCAL outperforms state-of-the-art methods in terms of clustering performance. With its ability to capture structural information while reducing computing complexity, OMCAL presents a promising approach for large-scale clustering problems. 

<br /><br />Summary: <div>
arXiv:2509.14724v1 Announce Type: new 
Abstract: In light of their capability to capture structural information while reducing computing complexity, anchor graph-based multi-view clustering (AGMC) methods have attracted considerable attention in large-scale clustering problems. Nevertheless, existing AGMC methods still face the following two issues: 1) They directly embedded diverse anchor graphs into a consensus anchor graph (CAG), and hence ignore redundant information and numerous noises contained in these anchor graphs, leading to a decrease in clustering effectiveness; 2) They drop effectiveness and efficiency due to independent post-processing to acquire clustering indicators. To overcome the aforementioned issues, we deliver a novel one-step multi-view clustering method with adaptive low-rank anchor-graph learning (OMCAL). To construct a high-quality CAG, OMCAL provides a nuclear norm-based adaptive CAG learning model against information redundancy and noise interference. Then, to boost clustering effectiveness and efficiency substantially, we incorporate category indicator acquisition and CAG learning into a unified framework. Numerous studies conducted on ordinary and large-scale datasets indicate that OMCAL outperforms existing state-of-the-art methods in terms of clustering effectiveness and efficiency.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlowCast-ODE: Continuous Hourly Weather Forecasting with Dynamic Flow Matching and ODE Integration</title>
<link>https://arxiv.org/abs/2509.14775</link>
<guid>https://arxiv.org/abs/2509.14775</guid>
<content:encoded><![CDATA[
<div> FlowCast-ODE, hourly weather forecasting, deep learning, atmospheric state evolution, continuous flow <br />
<br />Summary: FlowCast-ODE addresses challenges in hourly weather forecasting by modeling atmospheric state evolution as a continuous flow, aligning with physical dynamic systems. A coarse-to-fine training strategy integrates dynamic flow matching and an Ordinary Differential Equation (ODE) solver to improve accuracy and temporal coherence. A low-rank AdaLN-Zero modulation mechanism reduces model size without sacrificing accuracy. Experiments show FlowCast-ODE outperforms baselines, with lower RMSE and better energy conservation, preserving fine-scale details. It also performs comparably to state-of-the-art models in forecasting extreme events. Notably, FlowCast-ODE mitigates temporal discontinuities associated with data assimilation cycle transitions. <div>
arXiv:2509.14775v1 Announce Type: new 
Abstract: Accurate hourly weather forecasting is critical for numerous applications. Recent deep learning models have demonstrated strong capability on 6-hour intervals, yet achieving accurate and stable hourly predictions remains a critical challenge. This is primarily due to the rapid accumulation of errors in autoregressive rollouts and temporal discontinuities within the ERA5 data's 12-hour assimilation cycle. To address these issues, we propose FlowCast-ODE, a framework that models atmospheric state evolution as a continuous flow. FlowCast-ODE learns the conditional flow path directly from the previous state, an approach that aligns more naturally with physical dynamic systems and enables efficient computation. A coarse-to-fine strategy is introduced to train the model on 6-hour data using dynamic flow matching and then refined on hourly data that incorporates an Ordinary Differential Equation (ODE) solver to achieve temporally coherent forecasts. In addition, a lightweight low-rank AdaLN-Zero modulation mechanism is proposed and reduces model size by 15% without compromising accuracy. Experiments demonstrate that FlowCast-ODE outperforms strong baselines, yielding lower root mean square error (RMSE) and better energy conservation, which reduces blurring and preserves more fine-scale spatial details. It also shows comparable performance to the state-of-the-art model in forecasting extreme events like typhoons. Furthermore, the model alleviates temporal discontinuities associated with assimilation cycle transitions.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pre-training under infinite compute</title>
<link>https://arxiv.org/abs/2509.14786</link>
<guid>https://arxiv.org/abs/2509.14786</guid>
<content:encoded><![CDATA[
<div> Approach, Pre-training, Regularization, Data scaling, Ensemble

Summary:<br /><br />
In this study, the focus is on pre-training language models under fixed data and no compute constraints. The researchers investigate the effects of increasing epoch count and parameter count on overfitting and find that proper regularization is key, with an optimal weight decay significantly higher than standard practice. By tuning various factors such as epoching, regularization, parameter scaling, and ensemble scaling, they achieve a lower loss asymptote with higher data efficiency. The study shows that ensembling independently trained models outperforms individual models, leading to significant improvements. Additionally, they demonstrate that data efficiency gains can be maintained even at smaller parameter counts by distilling an ensemble into a smaller student model. The interventions designed for validation loss also translate effectively to downstream benchmarks, showcasing improved performance and data efficiency. Overall, the study suggests that simple algorithmic improvements can enable more data-efficient pre-training in a compute-rich future. <div>
arXiv:2509.14786v1 Announce Type: new 
Abstract: Since compute grows much faster than web text available for language model pre-training, we ask how one should approach pre-training under fixed data and no compute constraints. We first show that existing data-constrained approaches of increasing epoch count and parameter count eventually overfit, and we significantly improve upon such recipes by properly tuning regularization, finding that the optimal weight decay is $30\times$ larger than standard practice. Since our regularized recipe monotonically decreases loss following a simple power law in parameter count, we estimate its best possible performance via the asymptote of its scaling law rather than the performance at a fixed compute budget. We then identify that ensembling independently trained models achieves a significantly lower loss asymptote than the regularized recipe. Our best intervention combining epoching, regularization, parameter scaling, and ensemble scaling achieves an asymptote at 200M tokens using $5.17\times$ less data than our baseline, and our data scaling laws predict that this improvement persists at higher token budgets. We find that our data efficiency gains can be realized at much smaller parameter counts as we can distill an ensemble into a student model that is 8$\times$ smaller and retains $83\%$ of the ensembling benefit. Finally, our interventions designed for validation loss generalize to downstream benchmarks, achieving a $9\%$ improvement for pre-training evals and a $17.5\times$ data efficiency improvement over continued pre-training on math mid-training data. Our results show that simple algorithmic improvements can enable significantly more data-efficient pre-training in a compute-rich future.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structure-Aware Contrastive Learning with Fine-Grained Binding Representations for Drug Discovery</title>
<link>https://arxiv.org/abs/2509.14788</link>
<guid>https://arxiv.org/abs/2509.14788</guid>
<content:encoded><![CDATA[
<div> Framework, Drug-Target Interactions, Sequence-based methods, Machine Learning, Virtual Screening  
Summary:  
- A new sequence-based drug-target interaction framework is introduced, incorporating structural priors into protein representations to enhance prediction accuracy.  
- The model demonstrates superior performance on Human and BioSNAP datasets, and competitive results on BindingDB, showcasing its state-of-the-art capabilities.  
- In virtual screening tasks, the framework outperforms previous methods on LIT-PCBA, showing significant improvements in accuracy metrics.  
- Ablation studies underscore the importance of learned aggregation, bilinear attention, and contrastive alignment in enhancing predictive robustness.  
- Visualizations of embeddings reveal improved spatial alignment with known binding pockets and highlight interpretable attention patterns for ligand-residue contacts.  
<br /><br />Summary: <div>
arXiv:2509.14788v1 Announce Type: new 
Abstract: Accurate identification of drug-target interactions (DTI) remains a central challenge in computational pharmacology, where sequence-based methods offer scalability. This work introduces a sequence-based drug-target interaction framework that integrates structural priors into protein representations while maintaining high-throughput screening capability. Evaluated across multiple benchmarks, the model achieves state-of-the-art performance on Human and BioSNAP datasets and remains competitive on BindingDB. In virtual screening tasks, it surpasses prior methods on LIT-PCBA, yielding substantial gains in AUROC and BEDROC. Ablation studies confirm the critical role of learned aggregation, bilinear attention, and contrastive alignment in enhancing predictive robustness. Embedding visualizations reveal improved spatial correspondence with known binding pockets and highlight interpretable attention patterns over ligand-residue contacts. These results validate the framework's utility for scalable and structure-aware DTI prediction.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STEP: Structured Training and Evaluation Platform for benchmarking trajectory prediction models</title>
<link>https://arxiv.org/abs/2509.14801</link>
<guid>https://arxiv.org/abs/2509.14801</guid>
<content:encoded><![CDATA[
<div> benchmarking framework, trajectory prediction, automated vehicles, model evaluation, multi-agent settings<br />
Summary:<br />
The article introduces a new benchmarking framework called STEP to evaluate trajectory prediction models for automated vehicles. Existing evaluation practices lack standardization for heterogeneous traffic scenarios, joint prediction models, and user documentation. STEP addresses these limitations by providing a unified interface for multiple datasets, enforcing consistent training and evaluation conditions, and supporting various prediction models. Experiments using STEP reveal the limitations of current testing procedures, the importance of joint modeling for better predictions, and the vulnerabilities of state-of-the-art models against distribution shifts and adversarial attacks. The framework aims to shift focus from leaderboard comparisons to gaining deeper insights into model behavior and generalization in complex multi-agent settings. <div>
arXiv:2509.14801v1 Announce Type: new 
Abstract: While trajectory prediction plays a critical role in enabling safe and effective path-planning in automated vehicles, standardized practices for evaluating such models remain underdeveloped. Recent efforts have aimed to unify dataset formats and model interfaces for easier comparisons, yet existing frameworks often fall short in supporting heterogeneous traffic scenarios, joint prediction models, or user documentation. In this work, we introduce STEP -- a new benchmarking framework that addresses these limitations by providing a unified interface for multiple datasets, enforcing consistent training and evaluation conditions, and supporting a wide range of prediction models. We demonstrate the capabilities of STEP in a number of experiments which reveal 1) the limitations of widely-used testing procedures, 2) the importance of joint modeling of agents for better predictions of interactions, and 3) the vulnerability of current state-of-the-art models against both distribution shifts and targeted attacks by adversarial agents. With STEP, we aim to shift the focus from the ``leaderboard'' approach to deeper insights about model behavior and generalization in complex multi-agent settings.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Precision Neural Networks: Joint Graph And Relational Learning</title>
<link>https://arxiv.org/abs/2509.14821</link>
<guid>https://arxiv.org/abs/2509.14821</guid>
<content:encoded><![CDATA[
<div> VNNs, Precision Neural Networks, covariance matrix, precision matrix, alternating optimization

Summary: 
Precision Neural Networks (PNNs) are proposed as an alternative to CoVariance Neural Networks (VNNs) for covariance-based learning. PNNs operate on the precision matrix, which offers advantages such as encoding statistical independence, sparsity, and preserving covariance spectral structure. A joint optimization problem is formulated to learn both the network parameters and the precision matrix simultaneously. By employing alternating optimization, updates are made to the network weights and the precision estimate sequentially. The distance between the estimated and true precision matrices is theoretically bounded at each iteration. The effectiveness of the proposed joint estimation approach is demonstrated on synthetic and real-world data, showcasing its superiority over traditional two-step methods.
<br /><br />Summary: <div>
arXiv:2509.14821v1 Announce Type: new 
Abstract: CoVariance Neural Networks (VNNs) perform convolutions on the graph determined by the covariance matrix of the data, which enables expressive and stable covariance-based learning. However, covariance matrices are typically dense, fail to encode conditional independence, and are often precomputed in a task-agnostic way, which may hinder performance. To overcome these limitations, we study Precision Neural Networks (PNNs), i.e., VNNs on the precision matrix -- the inverse covariance. The precision matrix naturally encodes statistical independence, often exhibits sparsity, and preserves the covariance spectral structure. To make precision estimation task-aware, we formulate an optimization problem that jointly learns the network parameters and the precision matrix, and solve it via alternating optimization, by sequentially updating the network weights and the precision estimate. We theoretically bound the distance between the estimated and true precision matrices at each iteration, and demonstrate the effectiveness of joint estimation compared to two-step approaches on synthetic and real-world data.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion-Based Scenario Tree Generation for Multivariate Time Series Prediction and Multistage Stochastic Optimization</title>
<link>https://arxiv.org/abs/2509.14832</link>
<guid>https://arxiv.org/abs/2509.14832</guid>
<content:encoded><![CDATA[
<div> diffusion-based forecasting, stochastic forecasting, scenario trees, energy arbitrage, decision-making 

Summary: 
The article introduces the Diffusion Scenario Tree (DST) framework for constructing scenario trees in multivariate prediction tasks using diffusion-based probabilistic forecasting models. The DST framework recursively samples future trajectories and organizes them into a tree via clustering, ensuring non-anticipativity at each stage. It is evaluated on the energy arbitrage optimization task in New York State's day-ahead electricity market and shows consistent outperformance compared to conventional models and Model-Free Reinforcement Learning baselines. The use of DST for stochastic optimization results in more efficient decision policies, achieving higher performance by better handling uncertainty than deterministic and stochastic MPC variants using the same diffusion-based forecaster. <div>
arXiv:2509.14832v1 Announce Type: new 
Abstract: Stochastic forecasting is critical for efficient decision-making in uncertain systems, such as energy markets and finance, where estimating the full distribution of future scenarios is essential. We propose Diffusion Scenario Tree (DST), a general framework for constructing scenario trees for multivariate prediction tasks using diffusion-based probabilistic forecasting models. DST recursively samples future trajectories and organizes them into a tree via clustering, ensuring non-anticipativity (decisions depending only on observed history) at each stage. We evaluate the framework on the optimization task of energy arbitrage in New York State's day-ahead electricity market. Experimental results show that our approach consistently outperforms the same optimization algorithms that use scenario trees from more conventional models and Model-Free Reinforcement Learning baselines. Furthermore, using DST for stochastic optimization yields more efficient decision policies, achieving higher performance by better handling uncertainty than deterministic and stochastic MPC variants using the same diffusion-based forecaster.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization</title>
<link>https://arxiv.org/abs/2509.14848</link>
<guid>https://arxiv.org/abs/2509.14848</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, policy optimization, offline RL, hybrid RL, multi-fidelity

Summary:
The article introduces multi-fidelity hybrid RL via information gain maximization (MF-HRL-IGM), an algorithm that combines offline and online RL for policy optimization under a fixed cost budget. The algorithm leverages multiple simulators with varying levels of fidelity and computational cost. Theoretical analysis proves the no-regret property of MF-HRL-IGM. Empirical evaluations demonstrate its superior performance compared to existing benchmarks. This approach addresses the challenge of optimizing RL policies without extensive interactions with high-fidelity simulators, making it more cost-effective and practical for real-world scenarios. <div>
arXiv:2509.14848v1 Announce Type: new 
Abstract: Optimizing a reinforcement learning (RL) policy typically requires extensive interactions with a high-fidelity simulator of the environment, which are often costly or impractical. Offline RL addresses this problem by allowing training from pre-collected data, but its effectiveness is strongly constrained by the size and quality of the dataset. Hybrid offline-online RL leverages both offline data and interactions with a single simulator of the environment. In many real-world scenarios, however, multiple simulators with varying levels of fidelity and computational cost are available. In this work, we study multi-fidelity hybrid RL for policy optimization under a fixed cost budget. We introduce multi-fidelity hybrid RL via information gain maximization (MF-HRL-IGM), a hybrid offline-online RL algorithm that implements fidelity selection based on information gain maximization through a bootstrapping approach. Theoretical analysis establishes the no-regret property of MF-HRL-IGM, while empirical evaluations demonstrate its superior performance compared to existing benchmarks.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Global-to-Local Attention Scheme in Graph Transformers: An Empirical Study</title>
<link>https://arxiv.org/abs/2509.14863</link>
<guid>https://arxiv.org/abs/2509.14863</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Transformers, Graph Neural Networks, Global Attention Mechanisms, G2LFormer, Information Fusion

Summary:
G2LFormer is proposed as a graph representation learning model that features a novel global-to-local attention scheme. The architecture integrates global attention mechanisms in shallow layers to capture global information and Graph Neural Network modules in deeper layers to learn local structural information. This prevents nodes from ignoring their immediate neighbors and alleviates information loss. An effective cross-layer information fusion strategy allows local layers to retain beneficial information from global layers. G2LFormer outperforms state-of-the-art linear Graph Transformers and Graph Neural Networks on both node-level and graph-level tasks while maintaining linear complexity. The global-to-local attention scheme in G2LFormer shows excellent performance and addresses the issue of information loss in traditional integration schemes. The proposed model offers a promising approach for graph representation learning by effectively combining local and global attention mechanisms. 

Summary: <div>
arXiv:2509.14863v1 Announce Type: new 
Abstract: Graph Transformers (GTs) show considerable potential in graph representation learning. The architecture of GTs typically integrates Graph Neural Networks (GNNs) with global attention mechanisms either in parallel or as a precursor to attention mechanisms, yielding a local-and-global or local-to-global attention scheme. However, as the global attention mechanism primarily captures long-range dependencies between nodes, these integration schemes may suffer from information loss, where the local neighborhood information learned by GNN could be diluted by the attention mechanism. Therefore, we propose G2LFormer, featuring a novel global-to-local attention scheme where the shallow network layers use attention mechanisms to capture global information, while the deeper layers employ GNN modules to learn local structural information, thereby preventing nodes from ignoring their immediate neighbors. An effective cross-layer information fusion strategy is introduced to allow local layers to retain beneficial information from global layers and alleviate information loss, with acceptable trade-offs in scalability. To validate the feasibility of the global-to-local attention scheme, we compare G2LFormer with state-of-the-art linear GTs and GNNs on node-level and graph-level tasks. The results indicate that G2LFormer exhibits excellent performance while keeping linear complexity.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DPANet: Dual Pyramid Attention Network for Multivariate Time Series Forecasting</title>
<link>https://arxiv.org/abs/2509.14868</link>
<guid>https://arxiv.org/abs/2509.14868</guid>
<content:encoded><![CDATA[
<div> ablation studies, DPANet, temporal-Only model, Frequency-Only model, cross-attention mechanism  
Summary:<br /><br />We conducted rigorous ablation studies to validate DPANet's key components, confirming that the fusion of heterogeneous temporal and frequency information is crucial for performance. Two specialized versions, the Temporal-Only model and Frequency-Only model, underperformed significantly compared to the full model. Additionally, replacing the cross-attention mechanism with a simpler method led to the most severe performance degradation, highlighting the importance of the interactive fusion block in DPANet. These findings support the dual-domain hypothesis and demonstrate the effectiveness of the full model in integrating both temporal and spectral information for improved performance. <div>
arXiv:2509.14868v1 Announce Type: new 
Abstract: We conducted rigorous ablation studies to validate DPANet's key components (Table \ref{tab:ablation-study}). The full model consistently outperforms all variants. To test our dual-domain hypothesis, we designed two specialized versions: a Temporal-Only model (fusing two identical temporal pyramids) and a Frequency-Only model (fusing two spectral pyramids). Both variants underperformed significantly, confirming that the fusion of heterogeneous temporal and frequency information is critical. Furthermore, replacing the cross-attention mechanism with a simpler method (w/o Cross-Fusion) caused the most severe performance degradation. This result underscores that our interactive fusion block is the most essential component.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Graph from Smooth Signals under Partial Observation: A Robustness Analysis</title>
<link>https://arxiv.org/abs/2509.14887</link>
<guid>https://arxiv.org/abs/2509.14887</guid>
<content:encoded><![CDATA[
<div> Keywords: graph learning, networked system, hidden nodes, graph signal processing, Dirichlet energy

Summary: 
This paper explores the robustness of vanilla graph learning methods to partial observations of low-pass filtered graph signals in the presence of hidden nodes. The study demonstrates that traditional graph topology learning approaches can implicitly recover the ground truth graph structure even when only a subset of nodes are observed. By extending the restricted isometry property (RIP) to the Dirichlet energy function used in graph learning objectives, the authors prove that smoothness-based graph learning formulations can accurately estimate the underlying graph topology. Experimental results on synthetic and real data further support this finding, showing that methods like the GL-SigRep approach can successfully reconstruct the true graph network from incomplete signal observations. This analysis provides valuable insights for improving graph signal processing and machine learning tasks in networked systems with hidden nodes. 

<br /><br />Summary: <div>
arXiv:2509.14887v1 Announce Type: new 
Abstract: Learning the graph underlying a networked system from nodal signals is crucial to downstream tasks in graph signal processing and machine learning. The presence of hidden nodes whose signals are not observable might corrupt the estimated graph. While existing works proposed various robustifications of vanilla graph learning objectives by explicitly accounting for the presence of these hidden nodes, a robustness analysis of "naive", hidden-node agnostic approaches is still underexplored. This work demonstrates that vanilla graph topology learning methods are implicitly robust to partial observations of low-pass filtered graph signals. We achieve this theoretical result through extending the restricted isometry property (RIP) to the Dirichlet energy function used in graph learning objectives. We show that smoothness-based graph learning formulation (e.g., the GL-SigRep method) on partial observations can recover the ground truth graph topology corresponding to the observed nodes. Synthetic and real data experiments corroborate our findings.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Reinforcement Learning, Genetic Algorithms and Transformers for background determination in particle physics</title>
<link>https://arxiv.org/abs/2509.14894</link>
<guid>https://arxiv.org/abs/2509.14894</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Genetic Algorithms, beauty hadron decays, particle physics, transformer architecture

Summary: 
This paper introduces a novel approach using Reinforcement Learning (RL) to analyze background processes affecting beauty hadron decay measurements in particle physics. It addresses challenges such as computational limitations and the need for expertise to ascertain critical backgrounds accurately. The method also combines Genetic Algorithms (GAs) with RL to efficiently explore trajectory space and guide the training process in environments with sparse rewards. The proposed strategy is adaptable to various particle physics measurements beyond beauty hadron physics. Additionally, a transformer architecture is integrated into the RL agent to handle token sequences representing decays, enhancing the algorithm's performance in analyzing complex final state particles and kinematic overlaps. This innovative approach offers a systematic method for identifying relevant background processes in experimental studies of beauty hadron decays, contributing to advancements in particle physics research. 

<br /><br />Summary: <div>
arXiv:2509.14894v1 Announce Type: new 
Abstract: Experimental studies of beauty hadron decays face significant challenges due to a wide range of backgrounds arising from the numerous possible decay channels with similar final states. For a particular signal decay, the process for ascertaining the most relevant background processes necessitates a detailed analysis of final state particles, potential misidentifications, and kinematic overlaps, which, due to computational limitations, is restricted to the simulation of only the most relevant backgrounds. Moreover, this process typically relies on the physicist's intuition and expertise, as no systematic method exists.
  This paper has two primary goals. First, from a particle physics perspective, we present a novel approach that utilises Reinforcement Learning (RL) to overcome the aforementioned challenges by systematically determining the critical backgrounds affecting beauty hadron decay measurements. While beauty hadron physics serves as the case study in this work, the proposed strategy is broadly adaptable to other types of particle physics measurements. Second, from a Machine Learning perspective, we introduce a novel algorithm which exploits the synergy between RL and Genetic Algorithms (GAs) for environments with highly sparse rewards and a large trajectory space. This strategy leverages GAs to efficiently explore the trajectory space and identify successful trajectories, which are used to guide the RL agent's training. Our method also incorporates a transformer architecture for the RL agent to handle token sequences representing decays.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Barycenters of Persistence Diagrams</title>
<link>https://arxiv.org/abs/2509.14904</link>
<guid>https://arxiv.org/abs/2509.14904</guid>
<content:encoded><![CDATA[
<div> Keywords: Wasserstein barycenters, persistence diagrams, robustness, clustering, dictionary encoding

Summary:
This paper introduces a novel approach for calculating robust Wasserstein barycenters of persistence diagrams. While the traditional method is limited to a $q$-Wasserstein distance with $q=2$, this new method extends the computation to generic transportation costs ($q > 1$), especially those resilient to outliers ($q \in (1,2)$, showcasing improved robustness. The proposed technique is applied to both clustering of persistence diagrams in their metric space and dictionary encoding, demonstrating enhanced outlier resistance. The Python implementation of this approach is accessible at the provided GitHub link. Overall, this work offers a versatile framework for computing barycenters that can handle a broader range of transportation costs, thereby enhancing robustness in various applications involving persistence diagrams.<br /><br />Summary: <div>
arXiv:2509.14904v1 Announce Type: new 
Abstract: This short paper presents a general approach for computing robust Wasserstein barycenters of persistence diagrams. The classical method consists in computing assignment arithmetic means after finding the optimal transport plans between the barycenter and the persistence diagrams. However, this procedure only works for the transportation cost related to the $q$-Wasserstein distance $W_q$ when $q=2$. We adapt an alternative fixed-point method to compute a barycenter diagram for generic transportation costs ($q > 1$), in particular those robust to outliers, $q \in (1,2)$. We show the utility of our work in two applications: \emph{(i)} the clustering of persistence diagrams on their metric space and \emph{(ii)} the dictionary encoding of persistence diagrams. In both scenarios, we demonstrate the added robustness to outliers provided by our generalized framework. Our Python implementation is available at this address: https://github.com/Keanu-Sisouk/RobustBarycenter .
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Explaining Reinforcement Learning for Mobile Network Resource Allocation</title>
<link>https://arxiv.org/abs/2509.14925</link>
<guid>https://arxiv.org/abs/2509.14925</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Deep Neural Networks, Self-Explaining Neural Networks, Interpretability, Resource Allocation

Summary: 
Self-Explaining Neural Networks (SENNs) are proposed for Reinforcement Learning tasks with deep neural networks to enhance interpretability while maintaining predictive accuracy. The black-box nature of RL methods with DNNs can hinder trust and transparency, especially in critical domains. The approach focuses on low-dimensionality problems to provide robust local and global explanations for model behavior. Evaluation on a resource allocation problem in mobile networks shows that SENNs offer interpretable solutions with competitive performance compared to existing methods. The study indicates SENNs' potential to increase transparency and trust in AI-driven decision-making for tasks with low dimensions. The method demonstrates strong performance on par with the current state-of-the-art techniques, delivering robust explanations along with predictive accuracy.<br /><br />Summary: <div>
arXiv:2509.14925v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) methods that incorporate deep neural networks (DNN), though powerful, often lack transparency. Their black-box characteristic hinders interpretability and reduces trustworthiness, particularly in critical domains. To address this challenge in RL tasks, we propose a solution based on Self-Explaining Neural Networks (SENNs) along with explanation extraction methods to enhance interpretability while maintaining predictive accuracy. Our approach targets low-dimensionality problems to generate robust local and global explanations of the model's behaviour. We evaluate the proposed method on the resource allocation problem in mobile networks, demonstrating that SENNs can constitute interpretable solutions with competitive performance. This work highlights the potential of SENNs to improve transparency and trust in AI-driven decision-making for low-dimensional tasks. Our approach strong performance on par with the existing state-of-the-art methods, while providing robust explanations.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DAG: A Dual Causal Network for Time Series Forecasting with Exogenous Variables</title>
<link>https://arxiv.org/abs/2509.14933</link>
<guid>https://arxiv.org/abs/2509.14933</guid>
<content:encoded><![CDATA[
<div> Keywords: Time series forecasting, Exogenous variables, Causal relationships, DAG framework, Temporal and Channel dimensions <br />
<br />
Summary: 
The study introduces a framework called DAG for time series forecasting with exogenous variables, addressing the limitations of existing methods. The framework utilizes dual causal networks in both temporal and channel dimensions to improve forecasting accuracy. It includes the Temporal Causal Module, which captures how historical exogenous variables affect future exogenous variables through a causal discovery module. The causal injection module incorporates these relationships into forecasting future endogenous variables. The Channel Causal Module follows a similar approach, with a causal discovery module modeling the influence of historical exogenous variables on historical endogenous variables. The causal injection module then enhances the prediction of future endogenous variables based on future exogenous variables. This comprehensive framework aims to better leverage exogenous variables, especially future ones, for more accurate time series forecasting. <br /> <div>
arXiv:2509.14933v1 Announce Type: new 
Abstract: Time series forecasting is crucial in various fields such as economics, traffic, and AIOps. However, in real-world applications, focusing solely on the endogenous variables (i.e., target variables), is often insufficient to ensure accurate predictions. Considering exogenous variables (i.e., covariates) provides additional predictive information, thereby improving forecasting accuracy. However, existing methods for time series forecasting with exogenous variables (TSF-X) have the following shortcomings: 1) they do not leverage future exogenous variables, 2) they fail to account for the causal relationships between endogenous and exogenous variables. As a result, their performance is suboptimal. In this study, to better leverage exogenous variables, especially future exogenous variable, we propose a general framework DAG, which utilizes dual causal network along both the temporal and channel dimensions for time series forecasting with exogenous variables. Specifically, we first introduce the Temporal Causal Module, which includes a causal discovery module to capture how historical exogenous variables affect future exogenous variables. Following this, we construct a causal injection module that incorporates the discovered causal relationships into the process of forecasting future endogenous variables based on historical endogenous variables. Next, we propose the Channel Causal Module, which follows a similar design principle. It features a causal discovery module models how historical exogenous variables influence historical endogenous variables, and a causal injection module incorporates the discovered relationships to enhance the prediction of future endogenous variables based on future exogenous variables.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comparative Analysis of Transformer Models in Social Bot Detection</title>
<link>https://arxiv.org/abs/2509.14936</link>
<guid>https://arxiv.org/abs/2509.14936</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, artificial users, bot detection models, encoder transformers, decoder transformers

Summary: 
Social media has become a significant communication platform, with the proliferation of artificial users or bots creating challenges in spreading misinformation. This paper evaluates bot detection models using encoder and decoder transformers to address this issue. The study indicates that encoder-based classifiers outperform decoder-based models in terms of accuracy and robustness. However, decoder-based models show potential for task-specific alignment and generalization across various use cases, highlighting their adaptability advantages. These findings are crucial in the ongoing efforts to combat digital manipulation and protect the authenticity of online conversations. 

<br /><br />Summary: <div>
arXiv:2509.14936v1 Announce Type: new 
Abstract: Social media has become a key medium of communication in today's society. This realisation has led to many parties employing artificial users (or bots) to mislead others into believing untruths or acting in a beneficial manner to such parties. Sophisticated text generation tools, such as large language models, have further exacerbated this issue. This paper aims to compare the effectiveness of bot detection models based on encoder and decoder transformers. Pipelines are developed to evaluate the performance of these classifiers, revealing that encoder-based classifiers demonstrate greater accuracy and robustness. However, decoder-based models showed greater adaptability through task-specific alignment, suggesting more potential for generalisation across different use cases in addition to superior observa. These findings contribute to the ongoing effort to prevent digital environments being manipulated while protecting the integrity of online discussion.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Federated Learning for Social Network with Mobility</title>
<link>https://arxiv.org/abs/2509.14938</link>
<guid>https://arxiv.org/abs/2509.14938</guid>
<content:encoded><![CDATA[
<div> Federated Learning, Hierarchical Framework, Social Network, Mobility, Energy Consumption <br />
<br />
Summary: 
The paper introduces a novel approach, HFL-SNM, for Federated Learning that incorporates the social network structure and mobility patterns of clients. By considering data sharing among clients and client mobility, the framework aims to minimize energy consumption during the FL process. The optimization problem of resource allocation and client scheduling is formulated to achieve this objective under resource constraints. The concepts of Effective Data Coverage Rate and Redundant Data Coverage Rate are introduced to analyze the impact of data on model performance. The authors propose the DO-SNM algorithm, which decouples the optimization problem into sub-problems and significantly improves model performance while reducing energy consumption compared to traditional algorithms. Experimental results validate the effectiveness of the proposed approach in achieving superior model performance in FL scenarios. <br /> <div>
arXiv:2509.14938v1 Announce Type: new 
Abstract: Federated Learning (FL) offers a decentralized solution that allows collaborative local model training and global aggregation, thereby protecting data privacy. In conventional FL frameworks, data privacy is typically preserved under the assumption that local data remains absolutely private, whereas the mobility of clients is frequently neglected in explicit modeling. In this paper, we propose a hierarchical federated learning framework based on the social network with mobility namely HFL-SNM that considers both data sharing among clients and their mobility patterns. Under the constraints of limited resources, we formulate a joint optimization problem of resource allocation and client scheduling, which objective is to minimize the energy consumption of clients during the FL process. In social network, we introduce the concepts of Effective Data Coverage Rate and Redundant Data Coverage Rate. We analyze the impact of effective data and redundant data on the model performance through preliminary experiments. We decouple the optimization problem into multiple sub-problems, analyze them based on preliminary experimental results, and propose Dynamic Optimization in Social Network with Mobility (DO-SNM) algorithm. Experimental results demonstrate that our algorithm achieves superior model performance while significantly reducing energy consumption, compared to traditional baseline algorithms.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Prediction of Maternal Nutritional Status in Ethiopia Using Ensemble Machine Learning Models</title>
<link>https://arxiv.org/abs/2509.14945</link>
<guid>https://arxiv.org/abs/2509.14945</guid>
<content:encoded><![CDATA[
<div> Keywords: malnutrition, pregnant women, ensemble machine learning, Ethiopia, predictive model

Summary: 
The study addresses malnutrition among pregnant women in Ethiopia using ensemble machine learning techniques on data from the Ethiopian Demographic and Health Survey. The model considers 30 socio-demographic and health attributes to classify women into four nutritional status categories. Data preprocessing techniques were applied, and the Random Forest model outperformed others, achieving 97.87% accuracy, precision, recall, F1-score, and 99.86% ROC AUC. The study demonstrates the efficacy of ensemble learning in identifying hidden patterns in complex datasets and provides insights for early detection of nutritional risks. These findings have practical implications for healthcare providers, policymakers, and researchers to implement data-driven strategies for improving maternal nutrition and health outcomes in Ethiopia.<br /><br />Summary: <div>
arXiv:2509.14945v1 Announce Type: new 
Abstract: Malnutrition among pregnant women is a major public health challenge in Ethiopia, increasing the risk of adverse maternal and neonatal outcomes. Traditional statistical approaches often fail to capture the complex and multidimensional determinants of nutritional status. This study develops a predictive model using ensemble machine learning techniques, leveraging data from the Ethiopian Demographic and Health Survey (2005-2020), comprising 18,108 records with 30 socio-demographic and health attributes. Data preprocessing included handling missing values, normalization, and balancing with SMOTE, followed by feature selection to identify key predictors. Several supervised ensemble algorithms including XGBoost, Random Forest, CatBoost, and AdaBoost were applied to classify nutritional status. Among them, the Random Forest model achieved the best performance, classifying women into four categories (normal, moderate malnutrition, severe malnutrition, and overnutrition) with 97.87% accuracy, 97.88% precision, 97.87% recall, 97.87% F1-score, and 99.86% ROC AUC. These findings demonstrate the effectiveness of ensemble learning in capturing hidden patterns from complex datasets and provide timely insights for early detection of nutritional risks. The results offer practical implications for healthcare providers, policymakers, and researchers, supporting data-driven strategies to improve maternal nutrition and health outcomes in Ethiopia.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic Bilevel Optimization with Heavy-Tailed Noise</title>
<link>https://arxiv.org/abs/2509.14952</link>
<guid>https://arxiv.org/abs/2509.14952</guid>
<content:encoded><![CDATA[
<div> smooth bilevel optimization, stochastic setting, nested-loop normalized stochastic bilevel approximation, nonconvex-strongly-concave minimax optimization, SFO complexity

Summary:
The paper addresses smooth bilevel optimization problems where the lower-level problem is strongly convex and the upper-level problem is nonconvex. It focuses on the stochastic setting with unbiased stochastic gradient evaluation and heavy-tailed noise, common in machine learning applications. The proposed algorithm, N$^2$SBA, aims to find an $\epsilon$-stationary point with SFO complexity dependent on the condition number, the order of central moment for the noise, and the noise level. Specializing for nonconvex-strongly-concave minimax optimization, the algorithm achieves an $\epsilon$-stationary point with improved SFO complexity. The results match the best-known bounds for the bounded variance setting ($p=2), demonstrating the efficiency and effectiveness of the proposed approach. <br /><br />Summary: <div>
arXiv:2509.14952v1 Announce Type: new 
Abstract: This paper considers the smooth bilevel optimization in which the lower-level problem is strongly convex and the upper-level problem is possibly nonconvex. We focus on the stochastic setting that the algorithm can access the unbiased stochastic gradient evaluation with heavy-tailed noise, which is prevalent in many machine learning applications such as training large language models and reinforcement learning. We propose a nested-loop normalized stochastic bilevel approximation (N$^2$SBA) for finding an $\epsilon$-stationary point with the stochastic first-order oracle (SFO) complexity of $\tilde{\mathcal{O}}\big(\kappa^{\frac{7p-3}{p-1}} \sigma^{\frac{p}{p-1}} \epsilon^{-\frac{4 p - 2}{p-1}}\big)$, where $\kappa$ is the condition number, $p\in(1,2]$ is the order of central moment for the noise, and $\sigma$ is the noise level. Furthermore, we specialize our idea to solve the nonconvex-strongly-concave minimax optimization problem, achieving an $\epsilon$-stationary point with the SFO complexity of $\tilde{\mathcal O}\big(\kappa^{\frac{2p-1}{p-1}} \sigma^{\frac{p}{p-1}} \epsilon^{-\frac{3p-2}{p-1}}\big)$. All above upper bounds match the best-known results under the special case of the bounded variance setting, i.e., $p=2$.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FAWN: A MultiEncoder Fusion-Attention Wave Network for Integrated Sensing and Communication Indoor Scene Inference</title>
<link>https://arxiv.org/abs/2509.14968</link>
<guid>https://arxiv.org/abs/2509.14968</guid>
<content:encoded><![CDATA[
<div> Keywords: Integrated Sensing and Communication, Passive sensing, MultiEncoder Fusion-Attention Wave Network, Wi-Fi, 5G

Summary: 
Integrated Sensing and Communication (ISAC) aims to make wireless networks understand the physical world better using passive sensing techniques. However, existing solutions often focus on a single technology, limiting accuracy. To address this, the authors propose FAWN, a MultiEncoder Fusion-Attention Wave Network that combines Wi-Fi and 5G technologies for indoor scene inference. FAWN leverages the transformers architecture to fuse information from both technologies without disrupting current communications. A prototype was built and tested in a real scenario, achieving an error rate below 0.6m around 84% of the time. This integrated approach shows promise in enhancing the coverage and accuracy of passive sensing systems in wireless networks. 

<br /><br />Summary: <div>
arXiv:2509.14968v1 Announce Type: new 
Abstract: The upcoming generations of wireless technologies promise an era where everything is interconnected and intelligent. As the need for intelligence grows, networks must learn to better understand the physical world. However, deploying dedicated hardware to perceive the environment is not always feasible, mainly due to costs and/or complexity. Integrated Sensing and Communication (ISAC) has made a step forward in addressing this challenge. Within ISAC, passive sensing emerges as a cost-effective solution that reuses wireless communications to sense the environment, without interfering with existing communications. Nevertheless, the majority of current solutions are limited to one technology (mostly Wi-Fi or 5G), constraining the maximum accuracy reachable. As different technologies work with different spectrums, we see a necessity in integrating more than one technology to augment the coverage area. Hence, we take the advantage of ISAC passive sensing, to present FAWN, a MultiEncoder Fusion-Attention Wave Network for ISAC indoor scene inference. FAWN is based on the original transformers architecture, to fuse information from Wi-Fi and 5G, making the network capable of understanding the physical world without interfering with the current communication. To test our solution, we have built a prototype and integrated it in a real scenario. Results show errors below 0.6 m around 84% of times.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic Adaptive Gradient Descent Without Descent</title>
<link>https://arxiv.org/abs/2509.14969</link>
<guid>https://arxiv.org/abs/2509.14969</guid>
<content:encoded><![CDATA[
<div> Keywords: convex optimization, stochastic gradient, adaptive step-size, local geometry, theoretical convergence

Summary: 
The introduced method in the article presents a new adaptive step-size strategy for convex optimization with stochastic gradient descent. This approach leverages the local geometry of the objective function using only a first-order stochastic oracle, eliminating the need for hyper-parameter tuning. The method is based on adapting the Adaptive Gradient Descent Without Descent technique to the stochastic setting. The article provides theoretical proof of convergence for stochastic gradient descent with this step-size approach under various assumptions. Empirical results demonstrate that the proposed method competes favorably against tuned baseline methods. The method offers a promising approach for optimizing convex functions in a stochastic setting without the need for manual hyper-parameter adjustments. 

<br /><br />Summary: <div>
arXiv:2509.14969v1 Announce Type: new 
Abstract: We introduce a new adaptive step-size strategy for convex optimization with stochastic gradient that exploits the local geometry of the objective function only by means of a first-order stochastic oracle and without any hyper-parameter tuning. The method comes from a theoretically-grounded adaptation of the Adaptive Gradient Descent Without Descent method to the stochastic setting. We prove the convergence of stochastic gradient descent with our step-size under various assumptions, and we show that it empirically competes against tuned baselines.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention Beyond Neighborhoods: Reviving Transformer for Graph Clustering</title>
<link>https://arxiv.org/abs/2509.15024</link>
<guid>https://arxiv.org/abs/2509.15024</guid>
<content:encoded><![CDATA[
<div> Keywords: Attention mechanisms, Graph Neural Networks, Transformer, Graph clustering, Attentive Graph Clustering Network

Summary: 
The article explores the application of attention mechanisms in graph clustering tasks and compares their performance with Graph Neural Networks and Transformers. It identifies the weaknesses of GNN and Transformer in capturing both local and global information on graph structured data. The study proposes a new architecture called the Attentive Graph Clustering Network (AGCN) that incorporates attention mechanisms directly into the graph structure. AGCN aims to enhance global information extraction while maintaining sensitivity to local topological cues. The framework includes a KV cache mechanism for efficiency and a pairwise margin contrastive loss to improve the discriminative capacity of the attention space. Experimental results demonstrate that AGCN outperforms existing methods in graph clustering tasks. <div>
arXiv:2509.15024v1 Announce Type: new 
Abstract: Attention mechanisms have become a cornerstone in modern neural networks, driving breakthroughs across diverse domains. However, their application to graph structured data, where capturing topological connections is essential, remains underexplored and underperforming compared to Graph Neural Networks (GNNs), particularly in the graph clustering task. GNN tends to overemphasize neighborhood aggregation, leading to a homogenization of node representations. Conversely, Transformer tends to over globalize, highlighting distant nodes at the expense of meaningful local patterns. This dichotomy raises a key question: Is attention inherently redundant for unsupervised graph learning? To address this, we conduct a comprehensive empirical analysis, uncovering the complementary weaknesses of GNN and Transformer in graph clustering. Motivated by these insights, we propose the Attentive Graph Clustering Network (AGCN) a novel architecture that reinterprets the notion that graph is attention. AGCN directly embeds the attention mechanism into the graph structure, enabling effective global information extraction while maintaining sensitivity to local topological cues. Our framework incorporates theoretical analysis to contrast AGCN behavior with GNN and Transformer and introduces two innovations: (1) a KV cache mechanism to improve computational efficiency, and (2) a pairwise margin contrastive loss to boost the discriminative capacity of the attention space. Extensive experimental results demonstrate that AGCN outperforms state-of-the-art methods.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sample Efficient Experience Replay in Non-stationary Environments</title>
<link>https://arxiv.org/abs/2509.15032</link>
<guid>https://arxiv.org/abs/2509.15032</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, non-stationary environments, experience replay, environment dynamics, prioritized experience replay

Summary:
Reinforcement learning (RL) faces challenges in non-stationary environments where dynamics and rewards frequently change. Traditional experience replay (ER) methods struggle to adapt efficiently to these changes. This paper introduces a metric called Discrepancy of Environment Dynamics (DoE) to isolate environment-induced changes in value functions. Based on DoE, the paper proposes Discrepancy of Environment Prioritized Experience Replay (DEER), an adaptive ER framework that prioritizes transitions based on policy updates and environmental shifts. By using a binary classifier to detect environment changes, DEER can apply different prioritization strategies before and after each shift, leading to more sample-efficient learning. Experimental results on four non-stationary benchmarks show that DEER improves the performance of off-policy algorithms by 11.54% compared to state-of-the-art ER methods. <br /><br />Summary: Reinforcement learning in non-stationary environments is made more effective through the use of the Discrepancy of Environment Dynamics (DoE) metric and the Discrepancy of Environment Prioritized Experience Replay (DEER) framework. DEER adapts experience replay to prioritize transitions based on both policy updates and environmental shifts, resulting in significant performance gains in experimental setups. <div>
arXiv:2509.15032v1 Announce Type: new 
Abstract: Reinforcement learning (RL) in non-stationary environments is challenging, as changing dynamics and rewards quickly make past experiences outdated. Traditional experience replay (ER) methods, especially those using TD-error prioritization, struggle to distinguish between changes caused by the agent's policy and those from the environment, resulting in inefficient learning under dynamic conditions. To address this challenge, we propose the Discrepancy of Environment Dynamics (DoE), a metric that isolates the effects of environment shifts on value functions. Building on this, we introduce Discrepancy of Environment Prioritized Experience Replay (DEER), an adaptive ER framework that prioritizes transitions based on both policy updates and environmental changes. DEER uses a binary classifier to detect environment changes and applies distinct prioritization strategies before and after each shift, enabling more sample-efficient learning. Experiments on four non-stationary benchmarks demonstrate that DEER further improves the performance of off-policy algorithms by 11.54 percent compared to the best-performing state-of-the-art ER methods.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Marginals: Learning Joint Spatio-Temporal Patterns for Multivariate Anomaly Detection</title>
<link>https://arxiv.org/abs/2509.15033</link>
<guid>https://arxiv.org/abs/2509.15033</guid>
<content:encoded><![CDATA[
<div> Keywords: multivariate anomaly detection, time-varying non-linear spatio-temporal correlations, transformer encoder, multivariate likelihood, copula<br />
<br />
Summary: 
This paper proposes an approach to enhance multivariate anomaly detection by considering time-varying non-linear spatio-temporal correlations in multivariate time series data. Traditional methods often oversimplify interactions by assuming independence among time series variables. To address this, the proposed approach models joint dependencies in a latent space while decoupling marginal distributions, temporal dynamics, and inter-variable dependencies. Temporal patterns are captured using a transformer encoder, while spatial dependencies are modeled through a multivariate likelihood and copula. The temporal and spatial components are trained jointly in a latent space using a self-supervised contrastive learning objective. This enables the learning of meaningful feature representations to distinguish between normal and anomaly samples effectively. <div>
arXiv:2509.15033v1 Announce Type: new 
Abstract: In this paper, we aim to improve multivariate anomaly detection (AD) by modeling the \textit{time-varying non-linear spatio-temporal correlations} found in multivariate time series data . In multivariate time series data, an anomaly may be indicated by the simultaneous deviation of interrelated time series from their expected collective behavior, even when no individual time series exhibits a clearly abnormal pattern on its own. In many existing approaches, time series variables are assumed to be (conditionally) independent, which oversimplifies real-world interactions. Our approach addresses this by modeling joint dependencies in the latent space and decoupling the modeling of \textit{marginal distributions, temporal dynamics, and inter-variable dependencies}. We use a transformer encoder to capture temporal patterns, and to model spatial (inter-variable) dependencies, we fit a multi-variate likelihood and a copula. The temporal and the spatial components are trained jointly in a latent space using a self-supervised contrastive learning objective to learn meaningful feature representations to separate normal and anomaly samples.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Patterns to Predictions: A Shapelet-Based Framework for Directional Forecasting in Noisy Financial Markets</title>
<link>https://arxiv.org/abs/2509.15040</link>
<guid>https://arxiv.org/abs/2509.15040</guid>
<content:encoded><![CDATA[
<div> Keywords: directional forecasting, deep learning, pattern extraction, interpretable forecasting, financial markets

Summary:
This study introduces a two-stage framework for directional forecasting in financial markets, aiming to combine accuracy with interpretability. The first stage, SIMPC, extracts recurrent patterns from multivariate time series data, while also addressing issues like scale ambiguity and temporal distortion. The second stage, JISC-Net, uses shapelet-based classification to forecast short-term directional movement based on the extracted patterns. Experimental results on Bitcoin and S&amp;P 500 equities show that the proposed method outperforms baseline models in terms of predictive accuracy. Unlike traditional deep learning models, the framework offers transparency in decision-making by revealing the underlying pattern structures driving the forecasting outcomes. This integration of unsupervised pattern extraction with interpretable forecasting presents a promising approach for improving forecasting performance in financial markets. 

<br /><br />Summary: <div>
arXiv:2509.15040v1 Announce Type: new 
Abstract: Directional forecasting in financial markets requires both accuracy and interpretability. Before the advent of deep learning, interpretable approaches based on human-defined patterns were prevalent, but their structural vagueness and scale ambiguity hindered generalization. In contrast, deep learning models can effectively capture complex dynamics, yet often offer limited transparency. To bridge this gap, we propose a two-stage framework that integrates unsupervised pattern extracion with interpretable forecasting. (i) SIMPC segments and clusters multivariate time series, extracting recurrent patterns that are invariant to amplitude scaling and temporal distortion, even under varying window sizes. (ii) JISC-Net is a shapelet-based classifier that uses the initial part of extracted patterns as input and forecasts subsequent partial sequences for short-term directional movement. Experiments on Bitcoin and three S&amp;P 500 equities demonstrate that our method ranks first or second in 11 out of 12 metric--dataset combinations, consistently outperforming baselines. Unlike conventional deep learning models that output buy-or-sell signals without interpretable justification, our approach enables transparent decision-making by revealing the underlying pattern structures that drive predictive outcomes.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning Agent for a 2D Shooter Game</title>
<link>https://arxiv.org/abs/2509.15042</link>
<guid>https://arxiv.org/abs/2509.15042</guid>
<content:encoded><![CDATA[
arXiv:2509.15042v1 Announce Type: new 
Abstract: Reinforcement learning agents in complex game environments often suffer from sparse rewards, training instability, and poor sample efficiency. This paper presents a hybrid training approach that combines offline imitation learning with online reinforcement learning for a 2D shooter game agent. We implement a multi-head neural network with separate outputs for behavioral cloning and Q-learning, unified by shared feature extraction layers with attention mechanisms. Initial experiments using pure deep Q-Networks exhibited significant instability, with agents frequently reverting to poor policies despite occasional good performance. To address this, we developed a hybrid methodology that begins with behavioral cloning on demonstration data from rule-based agents, then transitions to reinforcement learning. Our hybrid approach achieves consistently above 70% win rate against rule-based opponents, substantially outperforming pure reinforcement learning methods which showed high variance and frequent performance degradation. The multi-head architecture enables effective knowledge transfer between learning modes while maintaining training stability. Results demonstrate that combining demonstration-based initialization with reinforcement learning optimization provides a robust solution for developing game AI agents in complex multi-agent environments where pure exploration proves insufficient.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Credit Card Fraud Detection</title>
<link>https://arxiv.org/abs/2509.15044</link>
<guid>https://arxiv.org/abs/2509.15044</guid>
<content:encoded><![CDATA[
arXiv:2509.15044v1 Announce Type: new 
Abstract: Credit card fraud remains a significant challenge due to class imbalance and fraudsters mimicking legitimate behavior. This study evaluates five machine learning models - Logistic Regression, Random Forest, XGBoost, K-Nearest Neighbors (KNN), and Multi-Layer Perceptron (MLP) on a real-world dataset using undersampling, SMOTE, and a hybrid approach. Our models are evaluated on the original imbalanced test set to better reflect real-world performance. Results show that the hybrid method achieves the best balance between recall and precision, especially improving MLP and KNN performance.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Balancing Sparse RNNs with Hyperparameterization Benefiting Meta-Learning</title>
<link>https://arxiv.org/abs/2509.15057</link>
<guid>https://arxiv.org/abs/2509.15057</guid>
<content:encoded><![CDATA[
arXiv:2509.15057v1 Announce Type: new 
Abstract: This paper develops alternative hyperparameters for specifying sparse Recurrent Neural Networks (RNNs). These hyperparameters allow for varying sparsity within the trainable weight matrices of the model while improving overall performance. This architecture enables the definition of a novel metric, hidden proportion, which seeks to balance the distribution of unknowns within the model and provides significant explanatory power of model performance. Together, the use of the varied sparsity RNN architecture combined with the hidden proportion metric generates significant performance gains while improving performance expectations on an a priori basis. This combined approach provides a path forward towards generalized meta-learning applications and model optimization based on intrinsic characteristics of the data set, including input and output dimensions.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Communication Efficient Split Learning of ViTs with Attention-based Double Compression</title>
<link>https://arxiv.org/abs/2509.15058</link>
<guid>https://arxiv.org/abs/2509.15058</guid>
<content:encoded><![CDATA[
arXiv:2509.15058v1 Announce Type: new 
Abstract: This paper proposes a novel communication-efficient Split Learning (SL) framework, named Attention-based Double Compression (ADC), which reduces the communication overhead required for transmitting intermediate Vision Transformers activations during the SL training process. ADC incorporates two parallel compression strategies. The first one merges samples' activations that are similar, based on the average attention score calculated in the last client layer; this strategy is class-agnostic, meaning that it can also merge samples having different classes, without losing generalization ability nor decreasing final results. The second strategy follows the first and discards the least meaningful tokens, further reducing the communication cost. Combining these strategies not only allows for sending less during the forward pass, but also the gradients are naturally compressed, allowing the whole model to be trained without additional tuning or approximations of the gradients. Simulation results demonstrate that Attention-based Double Compression outperforms state-of-the-art SL frameworks by significantly reducing communication overheads while maintaining high accuracy.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic and nonlinear compressive sensing</title>
<link>https://arxiv.org/abs/2509.15060</link>
<guid>https://arxiv.org/abs/2509.15060</guid>
<content:encoded><![CDATA[
arXiv:2509.15060v1 Announce Type: new 
Abstract: We present a smooth probabilistic reformulation of $\ell_0$ regularized regression that does not require Monte Carlo sampling and allows for the computation of exact gradients, facilitating rapid convergence to local optima of the best subset selection problem. The method drastically improves convergence speed compared to similar Monte Carlo based approaches. Furthermore, we empirically demonstrate that it outperforms compressive sensing algorithms such as IHT and (Relaxed-) Lasso across a wide range of settings and signal-to-noise ratios. The implementation runs efficiently on both CPUs and GPUs and is freely available at https://github.com/L0-and-behold/probabilistic-nonlinear-cs.
  We also contribute to research on nonlinear generalizations of compressive sensing by investigating when parameter recovery of a nonlinear teacher network is possible through compression of a student network. Building upon theorems of Fefferman and Markel, we show theoretically that the global optimum in the infinite-data limit enforces recovery up to certain symmetries. For empirical validation, we implement a normal-form algorithm that selects a canonical representative within each symmetry class. However, while compression can help to improve test loss, we find that exact parameter recovery is not even possible up to symmetries. In particular, we observe a surprising rebound effect where teacher and student configurations initially converge but subsequently diverge despite continuous decrease in test loss. These findings indicate fundamental differences between linear and nonlinear compressive sensing.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Internet Traffic Matrix Prediction via Time Series Clustering</title>
<link>https://arxiv.org/abs/2509.15072</link>
<guid>https://arxiv.org/abs/2509.15072</guid>
<content:encoded><![CDATA[
arXiv:2509.15072v1 Announce Type: new 
Abstract: We present a novel framework that leverages time series clustering to improve internet traffic matrix (TM) prediction using deep learning (DL) models. Traffic flows within a TM often exhibit diverse temporal behaviors, which can hinder prediction accuracy when training a single model across all flows. To address this, we propose two clustering strategies, source clustering and histogram clustering, that group flows with similar temporal patterns prior to model training. Clustering creates more homogeneous data subsets, enabling models to capture underlying patterns more effectively and generalize better than global prediction approaches that fit a single model to the entire TM. Compared to existing TM prediction methods, our method reduces RMSE by up to 92\% for Abilene and 75\% for G\'EANT. In routing scenarios, our clustered predictions also reduce maximum link utilization (MLU) bias by 18\% and 21\%, respectively, demonstrating the practical benefits of clustering when TMs are used for network optimization.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constrained Feedback Learning for Non-Stationary Multi-Armed Bandits</title>
<link>https://arxiv.org/abs/2509.15073</link>
<guid>https://arxiv.org/abs/2509.15073</guid>
<content:encoded><![CDATA[
arXiv:2509.15073v1 Announce Type: new 
Abstract: Non-stationary multi-armed bandits enable agents to adapt to changing environments by incorporating mechanisms to detect and respond to shifts in reward distributions, making them well-suited for dynamic settings. However, existing approaches typically assume that reward feedback is available at every round - an assumption that overlooks many real-world scenarios where feedback is limited. In this paper, we take a significant step forward by introducing a new model of constrained feedback in non-stationary multi-armed bandits, where the availability of reward feedback is restricted. We propose the first prior-free algorithm - that is, one that does not require prior knowledge of the degree of non-stationarity - that achieves near-optimal dynamic regret in this setting. Specifically, our algorithm attains a dynamic regret of $\tilde{\mathcal{O}}({K^{1/3} V_T^{1/3} T }/{ B^{1/3}})$, where $T$ is the number of rounds, $K$ is the number of arms, $B$ is the query budget, and $V_T$ is the variation budget capturing the degree of non-stationarity.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forecasting and Visualizing Air Quality from Sky Images with Vision-Language Models</title>
<link>https://arxiv.org/abs/2509.15076</link>
<guid>https://arxiv.org/abs/2509.15076</guid>
<content:encoded><![CDATA[
arXiv:2509.15076v1 Announce Type: new 
Abstract: Air pollution remains a critical threat to public health and environmental sustainability, yet conventional monitoring systems are often constrained by limited spatial coverage and accessibility. This paper proposes an AI-driven agent that predicts ambient air pollution levels from sky images and synthesizes realistic visualizations of pollution scenarios using generative modeling. Our approach combines statistical texture analysis with supervised learning for pollution classification, and leverages vision-language model (VLM)-guided image generation to produce interpretable representations of air quality conditions. The generated visuals simulate varying degrees of pollution, offering a foundation for user-facing interfaces that improve transparency and support informed environmental decision-making. These outputs can be seamlessly integrated into intelligent applications aimed at enhancing situational awareness and encouraging behavioral responses based on real-time forecasts. We validate our method using a dataset of urban sky images and demonstrate its effectiveness in both pollution level estimation and semantically consistent visual synthesis. The system design further incorporates human-centered user experience principles to ensure accessibility, clarity, and public engagement in air quality forecasting. To support scalable and energy-efficient deployment, future iterations will incorporate a green CNN architecture enhanced with FPGA-based incremental learning, enabling real-time inference on edge platforms.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive LoRA Experts Allocation and Selection for Federated Fine-Tuning</title>
<link>https://arxiv.org/abs/2509.15087</link>
<guid>https://arxiv.org/abs/2509.15087</guid>
<content:encoded><![CDATA[
arXiv:2509.15087v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities across various tasks, but fine-tuning them for domain-specific applications often requires substantial domain-specific data that may be distributed across multiple organizations. Federated Learning (FL) offers a privacy-preserving solution, but faces challenges with computational constraints when applied to LLMs. Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient fine-tuning approach, though a single LoRA module often struggles with heterogeneous data across diverse domains. This paper addresses two critical challenges in federated LoRA fine-tuning: 1. determining the optimal number and allocation of LoRA experts across heterogeneous clients, and 2. enabling clients to selectively utilize these experts based on their specific data characteristics. We propose FedLEASE (Federated adaptive LoRA Expert Allocation and SElection), a novel framework that adaptively clusters clients based on representation similarity to allocate and train domain-specific LoRA experts. It also introduces an adaptive top-$M$ Mixture-of-Experts mechanism that allows each client to select the optimal number of utilized experts. Our extensive experiments on diverse benchmark datasets demonstrate that FedLEASE significantly outperforms existing federated fine-tuning approaches in heterogeneous client settings while maintaining communication efficiency.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergent Alignment via Competition</title>
<link>https://arxiv.org/abs/2509.15090</link>
<guid>https://arxiv.org/abs/2509.15090</guid>
<content:encoded><![CDATA[
arXiv:2509.15090v1 Announce Type: new 
Abstract: Aligning AI systems with human values remains a fundamental challenge, but does our inability to create perfectly aligned models preclude obtaining the benefits of alignment? We study a strategic setting where a human user interacts with multiple differently misaligned AI agents, none of which are individually well-aligned. Our key insight is that when the users utility lies approximately within the convex hull of the agents utilities, a condition that becomes easier to satisfy as model diversity increases, strategic competition can yield outcomes comparable to interacting with a perfectly aligned model. We model this as a multi-leader Stackelberg game, extending Bayesian persuasion to multi-round conversations between differently informed parties, and prove three results: (1) when perfect alignment would allow the user to learn her Bayes-optimal action, she can also do so in all equilibria under the convex hull condition (2) under weaker assumptions requiring only approximate utility learning, a non-strategic user employing quantal response achieves near-optimal utility in all equilibria and (3) when the user selects the best single AI after an evaluation period, equilibrium guarantees remain near-optimal without further distributional assumptions. We complement the theory with two sets of experiments.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning</title>
<link>https://arxiv.org/abs/2509.15097</link>
<guid>https://arxiv.org/abs/2509.15097</guid>
<content:encoded><![CDATA[
arXiv:2509.15097v1 Announce Type: new 
Abstract: The rising computational and energy demands of deep learning, particularly in large-scale architectures such as foundation models and large language models (LLMs), pose significant challenges to sustainability. Traditional gradient-based training methods are inefficient, requiring numerous iterative updates and high power consumption. To address these limitations, we propose a hybrid framework that combines hierarchical decomposition with FPGA-based direct equation solving and incremental learning. Our method divides the neural network into two functional tiers: lower layers are optimized via single-step equation solving on FPGAs for efficient and parallelizable feature extraction, while higher layers employ adaptive incremental learning to support continual updates without full retraining. Building upon this foundation, we introduce the Compound LLM framework, which explicitly deploys LLM modules across both hierarchy levels. The lower-level LLM handles reusable representation learning with minimal energy overhead, while the upper-level LLM performs adaptive decision-making through energy-aware updates. This integrated design enhances scalability, reduces redundant computation, and aligns with the principles of sustainable AI. Theoretical analysis and architectural insights demonstrate that our method reduces computational costs significantly while preserving high model performance, making it well-suited for edge deployment and real-time adaptation in energy-constrained environments.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Super-Linear: A Lightweight Pretrained Mixture of Linear Experts for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2509.15105</link>
<guid>https://arxiv.org/abs/2509.15105</guid>
<content:encoded><![CDATA[
arXiv:2509.15105v1 Announce Type: new 
Abstract: Time series forecasting (TSF) is critical in domains like energy, finance, healthcare, and logistics, requiring models that generalize across diverse datasets. Large pre-trained models such as Chronos and Time-MoE show strong zero-shot (ZS) performance but suffer from high computational costs. In this work, We introduce Super-Linear, a lightweight and scalable mixture-of-experts (MoE) model for general forecasting. It replaces deep architectures with simple frequency-specialized linear experts, trained on resampled data across multiple frequency regimes. A lightweight spectral gating mechanism dynamically selects relevant experts, enabling efficient, accurate forecasting. Despite its simplicity, Super-Linear matches state-of-the-art performance while offering superior efficiency, robustness to various sampling rates, and enhanced interpretability. The implementation of Super-Linear is available at \href{https://github.com/azencot-group/SuperLinear}{https://github.com/azencot-group/SuperLinear}
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Limitations of Public Chest Radiography Datasets for Artificial Intelligence: Label Quality, Domain Shift, Bias and Evaluation Challenges</title>
<link>https://arxiv.org/abs/2509.15107</link>
<guid>https://arxiv.org/abs/2509.15107</guid>
<content:encoded><![CDATA[
arXiv:2509.15107v1 Announce Type: new 
Abstract: Artificial intelligence has shown significant promise in chest radiography, where deep learning models can approach radiologist-level diagnostic performance. Progress has been accelerated by large public datasets such as MIMIC-CXR, ChestX-ray14, PadChest, and CheXpert, which provide hundreds of thousands of labelled images with pathology annotations. However, these datasets also present important limitations. Automated label extraction from radiology reports introduces errors, particularly in handling uncertainty and negation, and radiologist review frequently disagrees with assigned labels. In addition, domain shift and population bias restrict model generalisability, while evaluation practices often overlook clinically meaningful measures. We conduct a systematic analysis of these challenges, focusing on label quality, dataset bias, and domain shift. Our cross-dataset domain shift evaluation across multiple model architectures revealed substantial external performance degradation, with pronounced reductions in AUPRC and F1 scores relative to internal testing. To assess dataset bias, we trained a source-classification model that distinguished datasets with near-perfect accuracy, and performed subgroup analyses showing reduced performance for minority age and sex groups. Finally, expert review by two board-certified radiologists identified significant disagreement with public dataset labels. Our findings highlight important clinical weaknesses of current benchmarks and emphasise the need for clinician-validated datasets and fairer evaluation frameworks.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference</title>
<link>https://arxiv.org/abs/2509.15110</link>
<guid>https://arxiv.org/abs/2509.15110</guid>
<content:encoded><![CDATA[
arXiv:2509.15110v1 Announce Type: new 
Abstract: Reward models are central to both reinforcement learning (RL) with language models and inference-time verification. However, existing reward models often lack temporal consistency, leading to ineffective policy updates and unstable RL training. We introduce TDRM, a method for learning smoother and more reliable reward models by minimizing temporal differences during training. This temporal-difference (TD) regularization produces smooth rewards and improves alignment with long-term objectives. Incorporating TDRM into the actor-critic style online RL loop yields consistent empirical gains. It is worth noting that TDRM is a supplement to verifiable reward methods, and both can be used in series. Experiments show that TD-trained process reward models (PRMs) improve performance across Best-of-N (up to 6.6%) and tree-search (up to 23.7%) settings. When combined with Reinforcement Learning with Verifiable Rewards (RLVR), TD-trained PRMs lead to more data-efficient RL -- achieving comparable performance with just 2.5k data to what baseline methods require 50.1k data to attain -- and yield higher-quality language model policies on 8 model variants (5 series), e.g., Qwen2.5-(0.5B, 1,5B), GLM4-9B-0414, GLM-Z1-9B-0414, Qwen2.5-Math-(1.5B, 7B), and DeepSeek-R1-Distill-Qwen-(1.5B, 7B). We release all code at https://github.com/THUDM/TDRM.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Low-rank surrogate modeling and stochastic zero-order optimization for training of neural networks with black-box layers</title>
<link>https://arxiv.org/abs/2509.15113</link>
<guid>https://arxiv.org/abs/2509.15113</guid>
<content:encoded><![CDATA[
arXiv:2509.15113v1 Announce Type: new 
Abstract: The growing demand for energy-efficient, high-performance AI systems has led to increased attention on alternative computing platforms (e.g., photonic, neuromorphic) due to their potential to accelerate learning and inference. However, integrating such physical components into deep learning pipelines remains challenging, as physical devices often offer limited expressiveness, and their non-differentiable nature renders on-device backpropagation difficult or infeasible. This motivates the development of hybrid architectures that combine digital neural networks with reconfigurable physical layers, which effectively behave as black boxes. In this work, we present a framework for the end-to-end training of such hybrid networks. This framework integrates stochastic zeroth-order optimization for updating the physical layer's internal parameters with a dynamic low-rank surrogate model that enables gradient propagation through the physical layer. A key component of our approach is the implicit projector-splitting integrator algorithm, which updates the lightweight surrogate model after each forward pass with minimal hardware queries, thereby avoiding costly full matrix reconstruction. We demonstrate our method across diverse deep learning tasks, including: computer vision, audio classification, and language modeling. Notably, across all modalities, the proposed approach achieves near-digital baseline accuracy and consistently enables effective end-to-end training of hybrid models incorporating various non-differentiable physical components (spatial light modulators, microring resonators, and Mach-Zehnder interferometers). This work bridges hardware-aware deep learning and gradient-free optimization, thereby offering a practical pathway for integrating non-differentiable physical components into scalable, end-to-end trainable AI systems.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Conformal Prediction for Regression Models under Label Noise</title>
<link>https://arxiv.org/abs/2509.15120</link>
<guid>https://arxiv.org/abs/2509.15120</guid>
<content:encoded><![CDATA[
arXiv:2509.15120v1 Announce Type: new 
Abstract: In high-stakes scenarios, such as medical imaging applications, it is critical to equip the predictions of a regression model with reliable confidence intervals. Recently, Conformal Prediction (CP) has emerged as a powerful statistical framework that, based on a labeled calibration set, generates intervals that include the true labels with a pre-specified probability. In this paper, we address the problem of applying CP for regression models when the calibration set contains noisy labels. We begin by establishing a mathematically grounded procedure for estimating the noise-free CP threshold. Then, we turn it into a practical algorithm that overcomes the challenges arising from the continuous nature of the regression problem. We evaluate the proposed method on two medical imaging regression datasets with Gaussian label noise. Our method significantly outperforms the existing alternative, achieving performance close to the clean-label setting.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Learning from Label Proportions with General Loss Functions</title>
<link>https://arxiv.org/abs/2509.15145</link>
<guid>https://arxiv.org/abs/2509.15145</guid>
<content:encoded><![CDATA[
arXiv:2509.15145v1 Announce Type: new 
Abstract: Motivated by problems in online advertising, we address the task of Learning from Label Proportions (LLP). In this partially-supervised setting, training data consists of groups of examples, termed bags, for which we only observe the average label value. The main goal, however, remains the design of a predictor for the labels of individual examples. We introduce a novel and versatile low-variance de-biasing methodology to learn from aggregate label information, significantly advancing the state of the art in LLP. Our approach exhibits remarkable flexibility, seamlessly accommodating a broad spectrum of practically relevant loss functions across both binary and multi-class classification settings. By carefully combining our estimators with standard techniques, we substantially improve sample complexity guarantees for a large class of losses of practical relevance. We also empirically validate the efficacy of our proposed approach across a diverse array of benchmark datasets, demonstrating compelling empirical advantages over standard baselines.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who to Trust? Aggregating Client Knowledge in Logit-Based Federated Learning</title>
<link>https://arxiv.org/abs/2509.15147</link>
<guid>https://arxiv.org/abs/2509.15147</guid>
<content:encoded><![CDATA[
arXiv:2509.15147v1 Announce Type: new 
Abstract: Federated learning (FL) usually shares model weights or gradients, which is costly for large models. Logit-based FL reduces this cost by sharing only logits computed on a public proxy dataset. However, aggregating information from heterogeneous clients is still challenging. This paper studies this problem, introduces and compares three logit aggregation methods: simple averaging, uncertainty-weighted averaging, and a learned meta-aggregator. Evaluated on MNIST and CIFAR-10, these methods reduce communication overhead, improve robustness under non-IID data, and achieve accuracy competitive with centralized training.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Improving Embodied Foundation Models</title>
<link>https://arxiv.org/abs/2509.15155</link>
<guid>https://arxiv.org/abs/2509.15155</guid>
<content:encoded><![CDATA[
arXiv:2509.15155v1 Announce Type: new 
Abstract: Foundation models trained on web-scale data have revolutionized robotics, but their application to low-level control remains largely limited to behavioral cloning. Drawing inspiration from the success of the reinforcement learning stage in fine-tuning large language models, we propose a two-stage post-training approach for robotics. The first stage, Supervised Fine-Tuning (SFT), fine-tunes pretrained foundation models using both: a) behavioral cloning, and b) steps-to-go prediction objectives. In the second stage, Self-Improvement, steps-to-go prediction enables the extraction of a well-shaped reward function and a robust success detector, enabling a fleet of robots to autonomously practice downstream tasks with minimal human supervision. Through extensive experiments on real-world and simulated robot embodiments, our novel post-training recipe unveils significant results on Embodied Foundation Models. First, we demonstrate that the combination of SFT and Self-Improvement is significantly more sample-efficient than scaling imitation data collection for supervised learning, and that it leads to policies with significantly higher success rates. Further ablations highlight that the combination of web-scale pretraining and Self-Improvement is the key to this sample-efficiency. Next, we demonstrate that our proposed combination uniquely unlocks a capability that current methods cannot achieve: autonomously practicing and acquiring novel skills that generalize far beyond the behaviors observed in the imitation learning datasets used during training. These findings highlight the transformative potential of combining pretrained foundation models with online Self-Improvement to enable autonomous skill acquisition in robotics. Our project website can be found at https://self-improving-efms.github.io .
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind the Gap: Data Rewriting for Stable Off-Policy Supervised Fine-Tuning</title>
<link>https://arxiv.org/abs/2509.15157</link>
<guid>https://arxiv.org/abs/2509.15157</guid>
<content:encoded><![CDATA[
arXiv:2509.15157v1 Announce Type: new 
Abstract: Supervised fine-tuning (SFT) of large language models can be viewed as an off-policy learning problem, where expert demonstrations come from a fixed behavior policy while training aims to optimize a target policy. Importance sampling is the standard tool for correcting this distribution mismatch, but large policy gaps lead to high variance and training instability. Existing approaches mitigate this issue using KL penalties or clipping, which passively constrain updates rather than actively reducing the gap. We propose a simple yet effective data rewriting framework that proactively shrinks the policy gap by keeping correct solutions as on-policy data and rewriting incorrect ones with guided re-solving, falling back to expert demonstrations only when needed. This aligns the training distribution with the target policy before optimization, reducing importance sampling variance and stabilizing off-policy fine-tuning. Experiments on five mathematical reasoning benchmarks demonstrate consistent and significant gains over both vanilla SFT and the state-of-the-art Dynamic Fine-Tuning (DFT) approach. The data and code will be released at https://github.com/NKU-HLT/Off-Policy-SFT.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MaRVIn: A Cross-Layer Mixed-Precision RISC-V Framework for DNN Inference, from ISA Extension to Hardware Acceleration</title>
<link>https://arxiv.org/abs/2509.15187</link>
<guid>https://arxiv.org/abs/2509.15187</guid>
<content:encoded><![CDATA[
arXiv:2509.15187v1 Announce Type: new 
Abstract: The evolution of quantization and mixed-precision techniques has unlocked new possibilities for enhancing the speed and energy efficiency of NNs. Several recent studies indicate that adapting precision levels across different parameters can maintain accuracy comparable to full-precision models while significantly reducing computational demands. However, existing embedded microprocessors lack sufficient architectural support for efficiently executing mixed-precision NNs, both in terms of ISA extensions and hardware design, resulting in inefficiencies such as excessive data packing/unpacking and underutilized arithmetic units. In this work, we propose novel ISA extensions and a micro-architecture implementation specifically designed to optimize mixed-precision execution, enabling energy-efficient deep learning inference on RISC-V architectures. We introduce MaRVIn, a cross-layer hardware-software co-design framework that enhances power efficiency and performance through a combination of hardware improvements, mixed-precision quantization, ISA-level optimizations, and cycle-accurate emulation. At the hardware level, we enhance the ALU with configurable mixed-precision arithmetic (2, 4, 8 bits) for weights/activations and employ multi-pumping to reduce execution latency while implementing soft SIMD for efficient 2-bit ops. At the software level, we integrate a pruning-aware fine-tuning method to optimize model compression and a greedy-based DSE approach to efficiently search for Pareto-optimal mixed-quantized models. Additionally, we incorporate voltage scaling to boost the power efficiency of our system. Our experimental evaluation over widely used DNNs and datasets, such as CIFAR10 and ImageNet, demonstrates that our framework can achieve, on average, 17.6x speedup for less than 1% accuracy loss and outperforms the ISA-agnostic state-of-the-art RISC-V cores, delivering up to 1.8 TOPs/W.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation</title>
<link>https://arxiv.org/abs/2509.15194</link>
<guid>https://arxiv.org/abs/2509.15194</guid>
<content:encoded><![CDATA[
arXiv:2509.15194v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly trained with reinforcement learning from verifiable rewards (RLVR), yet real-world deployment demands models that can self-improve without labels or external judges. Existing label-free methods, confidence minimization, self-consistency, or majority-vote objectives, stabilize learning but steadily shrink exploration, causing an entropy collapse: generations become shorter, less diverse, and brittle. Unlike prior approaches such as Test-Time Reinforcement Learning (TTRL), which primarily adapt models to the immediate unlabeled dataset at hand, our goal is broader: to enable general improvements without sacrificing the model's inherent exploration capacity and generalization ability, i.e., evolving. We formalize this issue and propose EVolution-Oriented and Label-free Reinforcement Learning (EVOL-RL), a simple rule that couples stability with variation under a label-free setting. EVOL-RL keeps the majority-voted answer as a stable anchor (selection) while adding a novelty-aware reward that favors responses whose reasoning differs from what has already been produced (variation), measured in semantic space. Implemented with GRPO, EVOL-RL also uses asymmetric clipping to preserve strong signals and an entropy regularizer to sustain search. This majority-for-selection + novelty-for-variation design prevents collapse, maintains longer and more informative chains of thought, and improves both pass@1 and pass@n. EVOL-RL consistently outperforms the majority-only TTRL baseline; e.g., training on label-free AIME24 lifts Qwen3-4B-Base AIME25 pass@1 from TTRL's 4.6% to 16.4%, and pass@16 from 18.5% to 37.9%. EVOL-RL not only prevents diversity collapse but also unlocks stronger generalization across domains (e.g., GPQA). Furthermore, we demonstrate that EVOL-RL also boosts performance in the RLVR setting, highlighting its broad applicability.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explaining deep learning for ECG using time-localized clusters</title>
<link>https://arxiv.org/abs/2509.15198</link>
<guid>https://arxiv.org/abs/2509.15198</guid>
<content:encoded><![CDATA[
arXiv:2509.15198v1 Announce Type: new 
Abstract: Deep learning has significantly advanced electrocardiogram (ECG) analysis, enabling automatic annotation, disease screening, and prognosis beyond traditional clinical capabilities. However, understanding these models remains a challenge, limiting interpretation and gaining knowledge from these developments. In this work, we propose a novel interpretability method for convolutional neural networks applied to ECG analysis. Our approach extracts time-localized clusters from the model's internal representations, segmenting the ECG according to the learned characteristics while quantifying the uncertainty of these representations. This allows us to visualize how different waveform regions contribute to the model's predictions and assess the certainty of its decisions. By providing a structured and interpretable view of deep learning models for ECG, our method enhances trust in AI-driven diagnostics and facilitates the discovery of clinically relevant electrophysiological patterns.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CausalPre: Scalable and Effective Data Pre-processing for Causal Fairness</title>
<link>https://arxiv.org/abs/2509.15199</link>
<guid>https://arxiv.org/abs/2509.15199</guid>
<content:encoded><![CDATA[
arXiv:2509.15199v1 Announce Type: new 
Abstract: Causal fairness in databases is crucial to preventing biased and inaccurate outcomes in downstream tasks. While most prior work assumes a known causal model, recent efforts relax this assumption by enforcing additional constraints. However, these approaches often fail to capture broader attribute relationships that are critical to maintaining utility. This raises a fundamental question: Can we harness the benefits of causal reasoning to design efficient and effective fairness solutions without relying on strong assumptions about the underlying causal model? In this paper, we seek to answer this question by introducing CausalPre, a scalable and effective causality-guided data pre-processing framework that guarantees justifiable fairness, a strong causal notion of fairness. CausalPre extracts causally fair relationships by reformulating the originally complex and computationally infeasible extraction task into a tailored distribution estimation problem. To ensure scalability, CausalPre adopts a carefully crafted variant of low-dimensional marginal factorization to approximate the joint distribution, complemented by a heuristic algorithm that efficiently tackles the associated computational challenge. Extensive experiments on benchmark datasets demonstrate that CausalPre is both effective and scalable, challenging the conventional belief that achieving causal fairness requires trading off relationship coverage for relaxed model assumptions.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlowRL: Matching Reward Distributions for LLM Reasoning</title>
<link>https://arxiv.org/abs/2509.15207</link>
<guid>https://arxiv.org/abs/2509.15207</guid>
<content:encoded><![CDATA[
arXiv:2509.15207v1 Announce Type: new 
Abstract: We propose FlowRL: matching the full reward distribution via flow balancing instead of maximizing rewards in large language model (LLM) reinforcement learning (RL). Recent advanced reasoning models adopt reward-maximizing methods (\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while neglecting less frequent but valid reasoning paths, thus reducing diversity. In contrast, we transform scalar rewards into a normalized target distribution using a learnable partition function, and then minimize the reverse KL divergence between the policy and the target distribution. We implement this idea as a flow-balanced optimization method that promotes diverse exploration and generalizable reasoning trajectories. We conduct experiments on math and code reasoning tasks: FlowRL achieves a significant average improvement of $10.0\%$ over GRPO and $5.1\%$ over PPO on math benchmarks, and performs consistently better on code reasoning tasks. These results highlight reward distribution-matching as a key step toward efficient exploration and diverse reasoning in LLM reinforcement learning.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resolve Highway Conflict in Multi-Autonomous Vehicle Controls with Local State Attention</title>
<link>https://arxiv.org/abs/2506.11445</link>
<guid>https://arxiv.org/abs/2506.11445</guid>
<content:encoded><![CDATA[
arXiv:2506.11445v1 Announce Type: cross 
Abstract: In mixed-traffic environments, autonomous vehicles must adapt to human-controlled vehicles and other unusual driving situations. This setting can be framed as a multi-agent reinforcement learning (MARL) environment with full cooperative reward among the autonomous vehicles. While methods such as Multi-agent Proximal Policy Optimization can be effective in training MARL tasks, they often fail to resolve local conflict between agents and are unable to generalize to stochastic events. In this paper, we propose a Local State Attention module to assist the input state representation. By relying on the self-attention operator, the module is expected to compress the essential information of nearby agents to resolve the conflict in traffic situations. Utilizing a simulated highway merging scenario with the priority vehicle as the unexpected event, our approach is able to prioritize other vehicles' information to manage the merging process. The results demonstrate significant improvements in merging efficiency compared to popular baselines, especially in high-density traffic settings.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Novel Phase-Noise-Tolerant Variational-Autoencoder-Based Equalization Suitable for Space-Division-Multiplexed Transmission</title>
<link>https://arxiv.org/abs/2509.14072</link>
<guid>https://arxiv.org/abs/2509.14072</guid>
<content:encoded><![CDATA[
arXiv:2509.14072v1 Announce Type: cross 
Abstract: We demonstrate the effectiveness of a novel phase-noise-tolerant, variational-autoencoder-based equalization scheme for space-division-multiplexed (SDM) transmission in an experiment over 150km of randomly-coupled multi-core fibers.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Intelligence-derived Cardiotocography Age as a Digital Biomarker for Predicting Future Adverse Pregnancy Outcomes</title>
<link>https://arxiv.org/abs/2509.14242</link>
<guid>https://arxiv.org/abs/2509.14242</guid>
<content:encoded><![CDATA[
arXiv:2509.14242v1 Announce Type: cross 
Abstract: Cardiotocography (CTG) is a low-cost, non-invasive fetal health assessment technique used globally, especially in underdeveloped countries. However, it is currently mainly used to identify the fetus's current status (e.g., fetal acidosis or hypoxia), and the potential of CTG in predicting future adverse pregnancy outcomes has not been fully explored. We aim to develop an AI-based model that predicts biological age from CTG time series (named CTGage), then calculate the age gap between CTGage and actual age (named CTGage-gap), and use this gap as a new digital biomarker for future adverse pregnancy outcomes. The CTGage model is developed using 61,140 records from 11,385 pregnant women, collected at Peking University People's Hospital between 2018 and 2022. For model training, a structurally designed 1D convolutional neural network is used, incorporating distribution-aligned augmented regression technology. The CTGage-gap is categorized into five groups: < -21 days (underestimation group), -21 to -7 days, -7 to 7 days (normal group), 7 to 21 days, and > 21 days (overestimation group). We further defined the underestimation group and overestimation group together as the high-risk group. We then compare the incidence of adverse outcomes and maternal diseases across these groups. The average absolute error of the CTGage model is 10.91 days. When comparing the overestimation group with the normal group, premature infants incidence is 5.33% vs. 1.42% (p < 0.05) and gestational diabetes mellitus (GDM) incidence is 31.93% vs. 20.86% (p < 0.05). When comparing the underestimation group with the normal group, low birth weight incidence is 0.17% vs. 0.15% (p < 0.05) and anaemia incidence is 37.51% vs. 34.74% (p < 0.05). Artificial intelligence-derived CTGage can predict the future risk of adverse pregnancy outcomes and hold potential as a novel, non-invasive, and easily accessible digital biomarker.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Defining, Understanding, and Detecting Online Toxicity: Challenges and Machine Learning Approaches</title>
<link>https://arxiv.org/abs/2509.14264</link>
<guid>https://arxiv.org/abs/2509.14264</guid>
<content:encoded><![CDATA[
arXiv:2509.14264v1 Announce Type: cross 
Abstract: Online toxic content has grown into a pervasive phenomenon, intensifying during times of crisis, elections, and social unrest. A significant amount of research has been focused on detecting or analyzing toxic content using machine-learning approaches. The proliferation of toxic content across digital platforms has spurred extensive research into automated detection mechanisms, primarily driven by advances in machine learning and natural language processing. Overall, the present study represents the synthesis of 140 publications on different types of toxic content on digital platforms. We present a comprehensive overview of the datasets used in previous studies focusing on definitions, data sources, challenges, and machine learning approaches employed in detecting online toxicity, such as hate speech, offensive language, and harmful discourse. The dataset encompasses content in 32 languages, covering topics such as elections, spontaneous events, and crises. We examine the possibility of using existing cross-platform data to improve the performance of classification models. We present the recommendations and guidelines for new research on online toxic consent and the use of content moderation for mitigation. Finally, we present some practical guidelines to mitigate toxic content from online platforms.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpeechWeave: Diverse Multilingual Synthetic Text &amp; Audio Data Generation Pipeline for Training Text to Speech Models</title>
<link>https://arxiv.org/abs/2509.14270</link>
<guid>https://arxiv.org/abs/2509.14270</guid>
<content:encoded><![CDATA[
arXiv:2509.14270v1 Announce Type: cross 
Abstract: High-quality Text-to-Speech (TTS) model training requires extensive and diverse text and speech data. It is challenging to procure such data from real sources due to issues of domain specificity, licensing, and scalability. Large language models (LLMs) can certainly generate textual data, but they create repetitive text with insufficient variation in the prompt during the generation process. Another important aspect in TTS training data is text normalization. Tools for normalization might occasionally introduce anomalies or overlook valuable patterns, and thus impact data quality. Furthermore, it is also impractical to rely on voice artists for large scale speech recording in commercial TTS systems with standardized voices. To address these challenges, we propose SpeechWeave, a synthetic speech data generation pipeline that is capable of automating the generation of multilingual, domain-specific datasets for training TTS models. Our experiments reveal that our pipeline generates data that is 10-48% more diverse than the baseline across various linguistic and phonetic metrics, along with speaker-standardized speech audio while generating approximately 97% correctly normalized text. Our approach enables scalable, high-quality data generation for TTS training, improving diversity, normalization, and voice consistency in the generated datasets.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Early Approaches to Adversarial Fine-Tuning for Prompt Injection Defense: A 2022 Study of GPT-3 and Contemporary Models</title>
<link>https://arxiv.org/abs/2509.14271</link>
<guid>https://arxiv.org/abs/2509.14271</guid>
<content:encoded><![CDATA[
arXiv:2509.14271v1 Announce Type: cross 
Abstract: This paper documents early research conducted in 2022 on defending against prompt injection attacks in large language models, providing historical context for the evolution of this critical security domain. This research focuses on two adversarial attacks against Large Language Models (LLMs): prompt injection and goal hijacking. We examine how to construct these attacks, test them on various LLMs, and compare their effectiveness. We propose and evaluate a novel defense technique called Adversarial Fine-Tuning. Our results show that, without this defense, the attacks succeeded 31\% of the time on GPT-3 series models. When using our Adversarial Fine-Tuning approach, attack success rates were reduced to near zero for smaller GPT-3 variants (Ada, Babbage, Curie), though we note that subsequent research has revealed limitations of fine-tuning-based defenses. We also find that more flexible models exhibit greater vulnerability to these attacks. Consequently, large models such as GPT-3 Davinci are more vulnerable than smaller models like GPT-2. While the specific models tested are now superseded, the core methodology and empirical findings contributed to the foundation of modern prompt injection defense research, including instruction hierarchy systems and constitutional AI approaches.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedMentor: Domain-Aware Differential Privacy for Heterogeneous Federated LLMs in Mental Health</title>
<link>https://arxiv.org/abs/2509.14275</link>
<guid>https://arxiv.org/abs/2509.14275</guid>
<content:encoded><![CDATA[
arXiv:2509.14275v1 Announce Type: cross 
Abstract: Privacy-preserving adaptation of Large Language Models (LLMs) in sensitive domains (e.g., mental health) requires balancing strict confidentiality with model utility and safety. We propose FedMentor, a federated fine-tuning framework that integrates Low-Rank Adaptation (LoRA) and domain-aware Differential Privacy (DP) to meet per-domain privacy budgets while maintaining performance. Each client (domain) applies a custom DP noise scale proportional to its data sensitivity, and the server adaptively reduces noise when utility falls below a threshold. In experiments on three mental health datasets, we show that FedMentor improves safety over standard Federated Learning without privacy, raising safe output rates by up to three points and lowering toxicity, while maintaining utility (BERTScore F1 and ROUGE-L) within 0.5% of the non-private baseline and close to the centralized upper bound. The framework scales to backbones with up to 1.7B parameters on single-GPU clients, requiring < 173 MB of communication per round. FedMentor demonstrates a practical approach to privately fine-tune LLMs for safer deployments in healthcare and other sensitive fields.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Data Privacy: New Privacy Risks for Large Language Models</title>
<link>https://arxiv.org/abs/2509.14278</link>
<guid>https://arxiv.org/abs/2509.14278</guid>
<content:encoded><![CDATA[
arXiv:2509.14278v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have achieved remarkable progress in natural language understanding, reasoning, and autonomous decision-making. However, these advancements have also come with significant privacy concerns. While significant research has focused on mitigating the data privacy risks of LLMs during various stages of model training, less attention has been paid to new threats emerging from their deployment. The integration of LLMs into widely used applications and the weaponization of their autonomous abilities have created new privacy vulnerabilities. These vulnerabilities provide opportunities for both inadvertent data leakage and malicious exfiltration from LLM-powered systems. Additionally, adversaries can exploit these systems to launch sophisticated, large-scale privacy attacks, threatening not only individual privacy but also financial security and societal trust. In this paper, we systematically examine these emerging privacy risks of LLMs. We also discuss potential mitigation strategies and call for the research community to broaden its focus beyond data privacy risks, developing new defenses to address the evolving threats posed by increasingly powerful LLMs and LLM-powered systems.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Robust Agentic CUDA Kernel Benchmarking, Verification, and Optimization</title>
<link>https://arxiv.org/abs/2509.14279</link>
<guid>https://arxiv.org/abs/2509.14279</guid>
<content:encoded><![CDATA[
arXiv:2509.14279v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) demonstrate their effectiveness in scaling test-time compute for software engineering tasks. However, these approaches often focus on high-level solutions, with limited attention to optimizing low-level CUDA kernel implementations. Additionally, existing kernel generation benchmarks suffer from exploitable loopholes and insufficient diversity in testing conditions, hindering true generalization assessment. To address these limitations, we introduce robust-kbench, a new benchmark for rigorous evaluation of kernel performance and correctness across varied scenarios. Furthermore, we present a comprehensive agentic framework that automates CUDA kernel discovery, verification, and optimization. This pipeline enables frontier LLMs to translate torch code to CUDA kernels and iteratively improve their runtime within our robust evaluation setting. Our sequential workflow first translates PyTorch code into equivalent CUDA kernels. It then optimizes their runtime using a novel evolutionary meta-generation procedure tailored to the CUDA ecosystem, guided by LLM-based verifiers for correctness and efficient filtering. Evaluated on robust-kbench, our approach produces CUDA kernels outperforming torch implementations for practical applications, including forward and backward passes. It can fuse operations and deploy various runtime optimization strategies. The verifier workflow accurately classifies incorrect kernels, enhancing hardware verification efficiency.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks</title>
<link>https://arxiv.org/abs/2509.14285</link>
<guid>https://arxiv.org/abs/2509.14285</guid>
<content:encoded><![CDATA[
arXiv:2509.14285v1 Announce Type: cross 
Abstract: Prompt injection attacks represent a major vulnerability in Large Language Model (LLM) deployments, where malicious instructions embedded in user inputs can override system prompts and induce unintended behaviors. This paper presents a novel multi-agent defense framework that employs specialized LLM agents in coordinated pipelines to detect and neutralize prompt injection attacks in real-time. We evaluate our approach using two distinct architectures: a sequential chain-of-agents pipeline and a hierarchical coordinator-based system. Our comprehensive evaluation on 55 unique prompt injection attacks, grouped into 8 categories and totaling 400 attack instances across two LLM platforms (ChatGLM and Llama2), demonstrates significant security improvements. Without defense mechanisms, baseline Attack Success Rates (ASR) reached 30% for ChatGLM and 20% for Llama2. Our multi-agent pipeline achieved 100% mitigation, reducing ASR to 0% across all tested scenarios. The framework demonstrates robustness across multiple attack categories including direct overrides, code execution attempts, data exfiltration, and obfuscation techniques, while maintaining system functionality for legitimate queries.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Property-Isometric Variational Autoencoders for Sequence Modeling and Design</title>
<link>https://arxiv.org/abs/2509.14287</link>
<guid>https://arxiv.org/abs/2509.14287</guid>
<content:encoded><![CDATA[
arXiv:2509.14287v1 Announce Type: cross 
Abstract: Biological sequence design (DNA, RNA, or peptides) with desired functional properties has applications in discovering novel nanomaterials, biosensors, antimicrobial drugs, and beyond. One common challenge is the ability to optimize complex high-dimensional properties such as target emission spectra of DNA-mediated fluorescent nanoparticles, photo and chemical stability, and antimicrobial activity of peptides across target microbes. Existing models rely on simple binary labels (e.g., binding/non-binding) rather than high-dimensional complex properties. To address this gap, we propose a geometry-preserving variational autoencoder framework, called PrIVAE, which learns latent sequence embeddings that respect the geometry of their property space. Specifically, we model the property space as a high-dimensional manifold that can be locally approximated by a nearest neighbor graph, given an appropriately defined distance measure. We employ the property graph to guide the sequence latent representations using (1) graph neural network encoder layers and (2) an isometric regularizer. PrIVAE learns a property-organized latent space that enables rational design of new sequences with desired properties by employing the trained decoder. We evaluate the utility of our framework for two generative tasks: (1) design of DNA sequences that template fluorescent metal nanoclusters and (2) design of antimicrobial peptides. The trained models retain high reconstruction accuracy while organizing the latent space according to properties. Beyond in silico experiments, we also employ sampled sequences for wet lab design of DNA nanoclusters, resulting in up to 16.1-fold enrichment of rare-property nanoclusters compared to their abundance in training data, demonstrating the practical utility of our framework.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Capabilities to Performance: Evaluating Key Functional Properties of LLM Architectures in Penetration Testing</title>
<link>https://arxiv.org/abs/2509.14289</link>
<guid>https://arxiv.org/abs/2509.14289</guid>
<content:encoded><![CDATA[
arXiv:2509.14289v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly used to automate or augment penetration testing, but their effectiveness and reliability across attack phases remain unclear. We present a comprehensive evaluation of multiple LLM-based agents, from single-agent to modular designs, across realistic penetration testing scenarios, measuring empirical performance and recurring failure patterns. We also isolate the impact of five core functional capabilities via targeted augmentations: Global Context Memory (GCM), Inter-Agent Messaging (IAM), Context-Conditioned Invocation (CCI), Adaptive Planning (AP), and Real-Time Monitoring (RTM). These interventions support, respectively: (i) context coherence and retention, (ii) inter-component coordination and state management, (iii) tool use accuracy and selective execution, (iv) multi-step strategic planning, error detection, and recovery, and (v) real-time dynamic responsiveness. Our results show that while some architectures natively exhibit subsets of these properties, targeted augmentations substantially improve modular agent performance, especially in complex, multi-step, and real-time penetration testing tasks.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Monitoring Machine Learning Systems: A Multivocal Literature Review</title>
<link>https://arxiv.org/abs/2509.14294</link>
<guid>https://arxiv.org/abs/2509.14294</guid>
<content:encoded><![CDATA[
arXiv:2509.14294v1 Announce Type: cross 
Abstract: Context: Dynamic production environments make it challenging to maintain reliable machine learning (ML) systems. Runtime issues, such as changes in data patterns or operating contexts, that degrade model performance are a common occurrence in production settings. Monitoring enables early detection and mitigation of these runtime issues, helping maintain users' trust and prevent unwanted consequences for organizations. Aim: This study aims to provide a comprehensive overview of the ML monitoring literature. Method: We conducted a multivocal literature review (MLR) following the well established guidelines by Garousi to investigate various aspects of ML monitoring approaches in 136 papers. Results: We analyzed selected studies based on four key areas: (1) the motivations, goals, and context; (2) the monitored aspects, specific techniques, metrics, and tools; (3) the contributions and benefits; and (4) the current limitations. We also discuss several insights found in the studies, their implications, and recommendations for future research and practice. Conclusion: Our MLR identifies and summarizes ML monitoring practices and gaps, emphasizing similarities and disconnects between formal and gray literature. Our study is valuable for both academics and practitioners, as it helps select appropriate solutions, highlights limitations in current approaches, and provides future directions for research and tool development.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpeechOp: Inference-Time Task Composition for Generative Speech Processing</title>
<link>https://arxiv.org/abs/2509.14298</link>
<guid>https://arxiv.org/abs/2509.14298</guid>
<content:encoded><![CDATA[
arXiv:2509.14298v1 Announce Type: cross 
Abstract: While generative Text-to-Speech (TTS) systems leverage vast ``in-the-wild" data to achieve remarkable success, speech-to-speech processing tasks like enhancement face data limitations, which lead data-hungry generative approaches to distort speech content and speaker identity. To bridge this gap, we present SpeechOp, a multi-task latent diffusion model that transforms pre-trained TTS models into a universal speech processor capable of performing a wide range of speech tasks and composing them in novel ways at inference time. By adapting a pre-trained TTS model, SpeechOp inherits a rich understanding of natural speech, accelerating training and improving S2S task quality, while simultaneously enhancing core TTS performance. Finally, we introduce Implicit Task Composition (ITC), a novel pipeline where ASR-derived transcripts (e.g., from Whisper) guide SpeechOp's enhancement via our principled inference-time task composition. ITC achieves state-of-the-art content preservation by robustly combining web-scale speech understanding with SpeechOp's generative capabilities. Audio samples are available at https://justinlovelace.github.io/projects/speechop
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Normalized Square Root: Sharper Matrix Factorization Bounds for Differentially Private Continual Counting</title>
<link>https://arxiv.org/abs/2509.14334</link>
<guid>https://arxiv.org/abs/2509.14334</guid>
<content:encoded><![CDATA[
arXiv:2509.14334v1 Announce Type: cross 
Abstract: The factorization norms of the lower-triangular all-ones $n \times n$ matrix, $\gamma_2(M_{count})$ and $\gamma_{F}(M_{count})$, play a central role in differential privacy as they are used to give theoretical justification of the accuracy of the only known production-level private training algorithm of deep neural networks by Google. Prior to this work, the best known upper bound on $\gamma_2(M_{count})$ was $1 + \frac{\log n}{\pi}$ by Mathias (Linear Algebra and Applications, 1993), and the best known lower bound was $\frac{1}{\pi}(2 + \log(\frac{2n+1}{3})) \approx 0.507 + \frac{\log n}{\pi}$ (Matou\v{s}ek, Nikolov, Talwar, IMRN 2020), where $\log$ denotes the natural logarithm. Recently, Henzinger and Upadhyay (SODA 2025) gave the first explicit factorization that meets the bound of Mathias (1993) and asked whether there exists an explicit factorization that improves on Mathias' bound. We answer this question in the affirmative. Additionally, we improve the lower bound significantly. More specifically, we show that $$
  0.701 + \frac{\log n}{\pi} + o(1) \;\leq\; \gamma_2(M_{count}) \;\leq\; 0.846 + \frac{\log n}{\pi} + o(1). $$ That is, we reduce the gap between the upper and lower bound to $0.14 + o(1)$.
  We also show that our factors achieve a better upper bound for $\gamma_{F}(M_{count})$ compared to prior work, and we establish an improved lower bound: $$
  0.701 + \frac{\log n}{\pi} + o(1) \;\leq\; \gamma_{F}(M_{count}) \;\leq\; 0.748 + \frac{\log n}{\pi} + o(1). $$ That is, the gap between the lower and upper bound provided by our explicit factorization is $0.047 + o(1)$.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DreamControl: Human-Inspired Whole-Body Humanoid Control for Scene Interaction via Guided Diffusion</title>
<link>https://arxiv.org/abs/2509.14353</link>
<guid>https://arxiv.org/abs/2509.14353</guid>
<content:encoded><![CDATA[
arXiv:2509.14353v1 Announce Type: cross 
Abstract: We introduce DreamControl, a novel methodology for learning autonomous whole-body humanoid skills. DreamControl leverages the strengths of diffusion models and Reinforcement Learning (RL): our core innovation is the use of a diffusion prior trained on human motion data, which subsequently guides an RL policy in simulation to complete specific tasks of interest (e.g., opening a drawer or picking up an object). We demonstrate that this human motion-informed prior allows RL to discover solutions unattainable by direct RL, and that diffusion models inherently promote natural looking motions, aiding in sim-to-real transfer. We validate DreamControl's effectiveness on a Unitree G1 robot across a diverse set of challenging tasks involving simultaneous lower and upper body control and object interaction.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion-Based Unsupervised Audio-Visual Speech Separation in Noisy Environments with Noise Prior</title>
<link>https://arxiv.org/abs/2509.14379</link>
<guid>https://arxiv.org/abs/2509.14379</guid>
<content:encoded><![CDATA[
arXiv:2509.14379v1 Announce Type: cross 
Abstract: In this paper, we address the problem of single-microphone speech separation in the presence of ambient noise. We propose a generative unsupervised technique that directly models both clean speech and structured noise components, training exclusively on these individual signals rather than noisy mixtures. Our approach leverages an audio-visual score model that incorporates visual cues to serve as a strong generative speech prior. By explicitly modelling the noise distribution alongside the speech distribution, we enable effective decomposition through the inverse problem paradigm. We perform speech separation by sampling from the posterior distributions via a reverse diffusion process, which directly estimates and removes the modelled noise component to recover clean constituent signals. Experimental results demonstrate promising performance, highlighting the effectiveness of our direct noise modelling approach in challenging acoustic environments.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>eIQ Neutron: Redefining Edge-AI Inference with Integrated NPU and Compiler Innovations</title>
<link>https://arxiv.org/abs/2509.14388</link>
<guid>https://arxiv.org/abs/2509.14388</guid>
<content:encoded><![CDATA[
arXiv:2509.14388v1 Announce Type: cross 
Abstract: Neural Processing Units (NPUs) are key to enabling efficient AI inference in resource-constrained edge environments. While peak tera operations per second (TOPS) is often used to gauge performance, it poorly reflects real-world performance and typically rather correlates with higher silicon cost. To address this, architects must focus on maximizing compute utilization, without sacrificing flexibility. This paper presents the eIQ Neutron efficient-NPU, integrated into a commercial flagship MPU, alongside co-designed compiler algorithms. The architecture employs a flexible, data-driven design, while the compiler uses a constrained programming approach to optimize compute and data movement based on workload characteristics. Compared to the leading embedded NPU and compiler stack, our solution achieves an average speedup of 1.8x (4x peak) at equal TOPS and memory resources across standard AI-benchmarks. Even against NPUs with double the compute and memory resources, Neutron delivers up to 3.3x higher performance.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Class-invariant Test-Time Augmentation for Domain Generalization</title>
<link>https://arxiv.org/abs/2509.14420</link>
<guid>https://arxiv.org/abs/2509.14420</guid>
<content:encoded><![CDATA[
arXiv:2509.14420v1 Announce Type: cross 
Abstract: Deep models often suffer significant performance degradation under distribution shifts. Domain generalization (DG) seeks to mitigate this challenge by enabling models to generalize to unseen domains. Most prior approaches rely on multi-domain training or computationally intensive test-time adaptation. In contrast, we propose a complementary strategy: lightweight test-time augmentation. Specifically, we develop a novel Class-Invariant Test-Time Augmentation (CI-TTA) technique. The idea is to generate multiple variants of each input image through elastic and grid deformations that nevertheless belong to the same class as the original input. Their predictions are aggregated through a confidence-guided filtering scheme that remove unreliable outputs, ensuring the final decision relies on consistent and trustworthy cues. Extensive Experiments on PACS and Office-Home datasets demonstrate consistent gains across different DG algorithms and backbones, highlighting the effectiveness and generality of our approach.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Indoor Airflow Imaging Using Physics-Informed Background-Oriented Schlieren Tomography</title>
<link>https://arxiv.org/abs/2509.14442</link>
<guid>https://arxiv.org/abs/2509.14442</guid>
<content:encoded><![CDATA[
arXiv:2509.14442v1 Announce Type: cross 
Abstract: We develop a framework for non-invasive volumetric indoor airflow estimation from a single viewpoint using background-oriented schlieren (BOS) measurements and physics-informed reconstruction. Our framework utilizes a light projector that projects a pattern onto a target back-wall and a camera that observes small distortions in the light pattern. While the single-view BOS tomography problem is severely ill-posed, our proposed framework addresses this using: (1) improved ray tracing, (2) a physics-based light rendering approach and loss formulation, and (3) a physics-based regularization using a physics-informed neural network (PINN) to ensure that the reconstructed airflow is consistent with the governing equations for buoyancy-driven flows.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficiently learning depth-3 circuits via quantum agnostic boosting</title>
<link>https://arxiv.org/abs/2509.14461</link>
<guid>https://arxiv.org/abs/2509.14461</guid>
<content:encoded><![CDATA[
arXiv:2509.14461v1 Announce Type: cross 
Abstract: We initiate the study of quantum agnostic learning of phase states with respect to a function class $\mathsf{C}\subseteq \{c:\{0,1\}^n\rightarrow \{0,1\}\}$: given copies of an unknown $n$-qubit state $|\psi\rangle$ which has fidelity $\textsf{opt}$ with a phase state $|\phi_c\rangle=\frac{1}{\sqrt{2^n}}\sum_{x\in \{0,1\}^n}(-1)^{c(x)}|x\rangle$ for some $c\in \mathsf{C}$, output $|\phi\rangle$ which has fidelity $|\langle \phi | \psi \rangle|^2 \geq \textsf{opt}-\varepsilon$. To this end, we give agnostic learning protocols for the following classes: (i) Size-$t$ decision trees which runs in time $\textsf{poly}(n,t,1/\varepsilon)$. This also implies $k$-juntas can be agnostically learned in time $\textsf{poly}(n,2^k,1/\varepsilon)$. (ii) $s$-term DNF formulas in near-polynomial time $\textsf{poly}(n,(s/\varepsilon)^{\log \log s/\varepsilon})$.
  Our main technical contribution is a quantum agnostic boosting protocol which converts a weak agnostic learner, which outputs a parity state $|\phi\rangle$ such that $|\langle \phi|\psi\rangle|^2\geq \textsf{opt}/\textsf{poly}(n)$, into a strong learner which outputs a superposition of parity states $|\phi'\rangle$ such that $|\langle \phi'|\psi\rangle|^2\geq \textsf{opt} - \varepsilon$.
  Using quantum agnostic boosting, we obtain the first near-polynomial time $n^{O(\log \log n)}$ algorithm for learning $\textsf{poly}(n)$-sized depth-$3$ circuits (consisting of $\textsf{AND}$, $\textsf{OR}$, $\textsf{NOT}$ gates) in the uniform quantum $\textsf{PAC}$ model using quantum examples. Classically, the analogue of efficient learning depth-$3$ circuits (and even depth-$2$ circuits) in the uniform $\textsf{PAC}$ model has been a longstanding open question in computational learning theory. Our work nearly settles this question, when the learner is given quantum examples.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating Semantic Alphabet Size for LLM Uncertainty Quantification</title>
<link>https://arxiv.org/abs/2509.14478</link>
<guid>https://arxiv.org/abs/2509.14478</guid>
<content:encoded><![CDATA[
arXiv:2509.14478v1 Announce Type: cross 
Abstract: Many black-box techniques for quantifying the uncertainty of large language models (LLMs) rely on repeated LLM sampling, which can be computationally expensive. Therefore, practical applicability demands reliable estimation from few samples. Semantic entropy (SE) is a popular sample-based uncertainty estimator with a discrete formulation attractive for the black-box setting. Recent extensions of semantic entropy exhibit improved LLM hallucination detection, but do so with less interpretable methods that admit additional hyperparameters. For this reason, we revisit the canonical discrete semantic entropy estimator, finding that it underestimates the "true" semantic entropy, as expected from theory. We propose a modified semantic alphabet size estimator, and illustrate that using it to adjust discrete semantic entropy for sample coverage results in more accurate semantic entropy estimation in our setting of interest. Furthermore, our proposed alphabet size estimator flags incorrect LLM responses as well or better than recent top-performing approaches, with the added benefit of remaining highly interpretable.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data coarse graining can improve model performance</title>
<link>https://arxiv.org/abs/2509.14498</link>
<guid>https://arxiv.org/abs/2509.14498</guid>
<content:encoded><![CDATA[
arXiv:2509.14498v1 Announce Type: cross 
Abstract: Lossy data transformations by definition lose information. Yet, in modern machine learning, methods like data pruning and lossy data augmentation can help improve generalization performance. We study this paradox using a solvable model of high-dimensional, ridge-regularized linear regression under 'data coarse graining.' Inspired by the renormalization group in statistical physics, we analyze coarse-graining schemes that systematically discard features based on their relevance to the learning task. Our results reveal a nonmonotonic dependence of the prediction risk on the degree of coarse graining. A 'high-pass' scheme--which filters out less relevant, lower-signal features--can help models generalize better. By contrast, a 'low-pass' scheme that integrates out more relevant, higher-signal features is purely detrimental. Crucially, using optimal regularization, we demonstrate that this nonmonotonicity is a distinct effect of data coarse graining and not an artifact of double descent. Our framework offers a clear, analytical explanation for why careful data augmentation works: it strips away less relevant degrees of freedom and isolates more predictive signals. Our results highlight a complex, nonmonotonic risk landscape shaped by the structure of the data, and illustrate how ideas from statistical physics provide a principled lens for understanding modern machine learning phenomena.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Introducing OmniGEC: A Silver Multilingual Dataset for Grammatical Error Correction</title>
<link>https://arxiv.org/abs/2509.14504</link>
<guid>https://arxiv.org/abs/2509.14504</guid>
<content:encoded><![CDATA[
arXiv:2509.14504v1 Announce Type: cross 
Abstract: In this paper, we introduce OmniGEC, a collection of multilingual silver-standard datasets for the task of Grammatical Error Correction (GEC), covering eleven languages: Czech, English, Estonian, German, Greek, Icelandic, Italian, Latvian, Slovene, Swedish, and Ukrainian. These datasets facilitate the development of multilingual GEC solutions and help bridge the data gap in adapting English GEC solutions to multilingual GEC. The texts in the datasets originate from three sources: Wikipedia edits for the eleven target languages, subreddits from Reddit in the eleven target languages, and the Ukrainian-only UberText 2.0 social media corpus. While Wikipedia edits were derived from human-made corrections, the Reddit and UberText 2.0 data were automatically corrected with the GPT-4o-mini model. The quality of the corrections in the datasets was evaluated both automatically and manually. Finally, we fine-tune two open-source large language models - Aya-Expanse (8B) and Gemma-3 (12B) - on the multilingual OmniGEC corpora and achieve state-of-the-art (SOTA) results for paragraph-level multilingual GEC. The dataset collection and the best-performing models are available on Hugging Face.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Delta Knowledge Distillation for Large Language Models</title>
<link>https://arxiv.org/abs/2509.14526</link>
<guid>https://arxiv.org/abs/2509.14526</guid>
<content:encoded><![CDATA[
arXiv:2509.14526v1 Announce Type: cross 
Abstract: Knowledge distillation (KD) is a widely adopted approach for compressing large neural networks by transferring knowledge from a large teacher model to a smaller student model. In the context of large language models, token level KD, typically minimizing the KL divergence between student output distribution and teacher output distribution, has shown strong empirical performance. However, prior work assumes student output distribution and teacher output distribution share the same optimal representation space, a premise that may not hold in many cases. To solve this problem, we propose Delta Knowledge Distillation (Delta-KD), a novel extension of token level KD that encourages the student to approximate an optimal representation space by explicitly preserving the distributional shift Delta introduced during the teacher's supervised finetuning (SFT). Empirical results on ROUGE metrics demonstrate that Delta KD substantially improves student performance while preserving more of the teacher's knowledge.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Radiolunadiff: Estimation of wireless network signal strength in lunar terrain</title>
<link>https://arxiv.org/abs/2509.14559</link>
<guid>https://arxiv.org/abs/2509.14559</guid>
<content:encoded><![CDATA[
arXiv:2509.14559v1 Announce Type: cross 
Abstract: In this paper, we propose a novel physics-informed deep learning architecture for predicting radio maps over lunar terrain. Our approach integrates a physics-based lunar terrain generator, which produces realistic topography informed by publicly available NASA data, with a ray-tracing engine to create a high-fidelity dataset of radio propagation scenarios. Building on this dataset, we introduce a triplet-UNet architecture, consisting of two standard UNets and a diffusion network, to model complex propagation effects. Experimental results demonstrate that our method outperforms existing deep learning approaches on our terrain dataset across various metrics.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VisMoDAl: Visual Analytics for Evaluating and Improving Corruption Robustness of Vision-Language Models</title>
<link>https://arxiv.org/abs/2509.14571</link>
<guid>https://arxiv.org/abs/2509.14571</guid>
<content:encoded><![CDATA[
arXiv:2509.14571v1 Announce Type: cross 
Abstract: Vision-language (VL) models have shown transformative potential across various critical domains due to their capability to comprehend multi-modal information. However, their performance frequently degrades under distribution shifts, making it crucial to assess and improve robustness against real-world data corruption encountered in practical applications. While advancements in VL benchmark datasets and data augmentation (DA) have contributed to robustness evaluation and improvement, there remain challenges due to a lack of in-depth comprehension of model behavior as well as the need for expertise and iterative efforts to explore data patterns. Given the achievement of visualization in explaining complex models and exploring large-scale data, understanding the impact of various data corruption on VL models aligns naturally with a visual analytics approach. To address these challenges, we introduce VisMoDAl, a visual analytics framework designed to evaluate VL model robustness against various corruption types and identify underperformed samples to guide the development of effective DA strategies. Grounded in the literature review and expert discussions, VisMoDAl supports multi-level analysis, ranging from examining performance under specific corruptions to task-driven inspection of model behavior and corresponding data slice. Unlike conventional works, VisMoDAl enables users to reason about the effects of corruption on VL models, facilitating both model behavior understanding and DA strategy formulation. The utility of our system is demonstrated through case studies and quantitative evaluations focused on corruption robustness in the image captioning task.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Distilled Retrieval-Augmented Guarding Model for Online Malicious Intent Detection</title>
<link>https://arxiv.org/abs/2509.14622</link>
<guid>https://arxiv.org/abs/2509.14622</guid>
<content:encoded><![CDATA[
arXiv:2509.14622v1 Announce Type: cross 
Abstract: With the deployment of Large Language Models (LLMs) in interactive applications, online malicious intent detection has become increasingly critical. However, existing approaches fall short of handling diverse and complex user queries in real time. To address these challenges, we introduce ADRAG (Adversarial Distilled Retrieval-Augmented Guard), a two-stage framework for robust and efficient online malicious intent detection. In the training stage, a high-capacity teacher model is trained on adversarially perturbed, retrieval-augmented inputs to learn robust decision boundaries over diverse and complex user queries. In the inference stage, a distillation scheduler transfers the teacher's knowledge into a compact student model, with a continually updated knowledge base collected online. At deployment, the compact student model leverages top-K similar safety exemplars retrieved from the online-updated knowledge base to enable both online and real-time malicious query detection. Evaluations across ten safety benchmarks demonstrate that ADRAG, with a 149M-parameter model, achieves 98.5% of WildGuard-7B's performance, surpasses GPT-4 by 3.3% and Llama-Guard-3-8B by 9.5% on out-of-distribution detection, while simultaneously delivering up to 5.6x lower latency at 300 queries per second (QPS) in real-time applications.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reveal and Release: Iterative LLM Unlearning with Self-generated Data</title>
<link>https://arxiv.org/abs/2509.14624</link>
<guid>https://arxiv.org/abs/2509.14624</guid>
<content:encoded><![CDATA[
arXiv:2509.14624v1 Announce Type: cross 
Abstract: Large language model (LLM) unlearning has demonstrated effectiveness in removing the influence of undesirable data (also known as forget data). Existing approaches typically assume full access to the forget dataset, overlooking two key challenges: (1) Forget data is often privacy-sensitive, rare, or legally regulated, making it expensive or impractical to obtain (2) The distribution of available forget data may not align with how that information is represented within the model. To address these limitations, we propose a ``Reveal-and-Release'' method to unlearn with self-generated data, where we prompt the model to reveal what it knows using optimized instructions. To fully utilize the self-generated forget data, we propose an iterative unlearning framework, where we make incremental adjustments to the model's weight space with parameter-efficient modules trained on the forget data. Experimental results demonstrate that our method balances the tradeoff between forget quality and utility preservation.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Audio Captions with Human Preferences</title>
<link>https://arxiv.org/abs/2509.14659</link>
<guid>https://arxiv.org/abs/2509.14659</guid>
<content:encoded><![CDATA[
arXiv:2509.14659v1 Announce Type: cross 
Abstract: Current audio captioning systems rely heavily on supervised learning with paired audio-caption datasets, which are expensive to curate and may not reflect human preferences in real-world scenarios. To address this limitation, we propose a preference-aligned audio captioning framework based on Reinforcement Learning from Human Feedback (RLHF). To effectively capture nuanced human preferences, we train a Contrastive Language-Audio Pretraining (CLAP)-based reward model using human-labeled pairwise preference data. This reward model is integrated into a reinforcement learning framework to fine-tune any baseline captioning system without relying on ground-truth caption annotations. Extensive human evaluations across multiple datasets show that our method produces captions preferred over those from baseline models, particularly in cases where the baseline models fail to provide correct and natural captions. Furthermore, our framework achieves performance comparable to supervised approaches with ground-truth data, demonstrating its effectiveness in aligning audio captioning with human preferences and its scalability in real-world scenarios.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding the Thinking Process of Reasoning Models: A Perspective from Schoenfeld's Episode Theory</title>
<link>https://arxiv.org/abs/2509.14662</link>
<guid>https://arxiv.org/abs/2509.14662</guid>
<content:encoded><![CDATA[
arXiv:2509.14662v1 Announce Type: cross 
Abstract: While Large Reasoning Models (LRMs) generate extensive chain-of-thought reasoning, we lack a principled framework for understanding how these thoughts are structured. In this paper, we introduce a novel approach by applying Schoenfeld's Episode Theory, a classic cognitive framework for human mathematical problem-solving, to analyze the reasoning traces of LRMs. We annotated thousands of sentences and paragraphs from model-generated solutions to math problems using seven cognitive labels (e.g., Plan, Implement, Verify). The result is the first publicly available benchmark for the fine-grained analysis of machine reasoning, including a large annotated corpus and detailed annotation guidebooks. Our preliminary analysis reveals distinct patterns in LRM reasoning, such as the transition dynamics between cognitive states. This framework provides a theoretically grounded methodology for interpreting LRM cognition and enables future work on more controllable and transparent reasoning systems.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TableDART: Dynamic Adaptive Multi-Modal Routing for Table Understanding</title>
<link>https://arxiv.org/abs/2509.14671</link>
<guid>https://arxiv.org/abs/2509.14671</guid>
<content:encoded><![CDATA[
arXiv:2509.14671v1 Announce Type: cross 
Abstract: Modeling semantic and structural information from tabular data remains a core challenge for effective table understanding. Existing Table-as-Text approaches flatten tables for large language models (LLMs), but lose crucial structural cues, while Table-as-Image methods preserve structure yet struggle with fine-grained semantics. Recent Table-as-Multimodality strategies attempt to combine textual and visual views, but they (1) statically process both modalities for every query-table pair within a large multimodal LLMs (MLLMs), inevitably introducing redundancy and even conflicts, and (2) depend on costly fine-tuning of MLLMs. In light of this, we propose TableDART, a training-efficient framework that integrates multimodal views by reusing pretrained single-modality models. TableDART introduces a lightweight 2.59M-parameter MLP gating network that dynamically selects the optimal path (either Text-only, Image-only, or Fusion) for each table-query pair, effectively reducing redundancy and conflicts from both modalities. In addition, we propose a novel agent to mediate cross-modal knowledge integration by analyzing outputs from text- and image-based models, either selecting the best result or synthesizing a new answer through reasoning. This design avoids the prohibitive costs of full MLLM fine-tuning. Extensive experiments on seven benchmarks show that TableDART establishes new state-of-the-art performance among open-source models, surpassing the strongest baseline by an average of 4.02%. The code is available at: https://anonymous.4open.science/r/TableDART-C52B
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LEED: A Highly Efficient and Scalable LLM-Empowered Expert Demonstrations Framework for Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.14680</link>
<guid>https://arxiv.org/abs/2509.14680</guid>
<content:encoded><![CDATA[
arXiv:2509.14680v1 Announce Type: cross 
Abstract: Multi-agent reinforcement learning (MARL) holds substantial promise for intelligent decision-making in complex environments. However, it suffers from a coordination and scalability bottleneck as the number of agents increases. To address these issues, we propose the LLM-empowered expert demonstrations framework for multi-agent reinforcement learning (LEED). LEED consists of two components: a demonstration generation (DG) module and a policy optimization (PO) module. Specifically, the DG module leverages large language models to generate instructions for interacting with the environment, thereby producing high-quality demonstrations. The PO module adopts a decentralized training paradigm, where each agent utilizes the generated demonstrations to construct an expert policy loss, which is then integrated with its own policy loss. This enables each agent to effectively personalize and optimize its local policy based on both expert knowledge and individual experience. Experimental results show that LEED achieves superior sample efficiency, time efficiency, and robust scalability compared to state-of-the-art baselines.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Designing Latent Safety Filters using Pre-Trained Vision Models</title>
<link>https://arxiv.org/abs/2509.14758</link>
<guid>https://arxiv.org/abs/2509.14758</guid>
<content:encoded><![CDATA[
arXiv:2509.14758v1 Announce Type: cross 
Abstract: Ensuring safety of vision-based control systems remains a major challenge hindering their deployment in critical settings. Safety filters have gained increased interest as effective tools for ensuring the safety of classical control systems, but their applications in vision-based control settings have so far been limited. Pre-trained vision models (PVRs) have been shown to be effective perception backbones for control in various robotics domains. In this paper, we are interested in examining their effectiveness when used for designing vision-based safety filters. We use them as backbones for classifiers defining failure sets, for Hamilton-Jacobi (HJ) reachability-based safety filters, and for latent world models. We discuss the trade-offs between training from scratch, fine-tuning, and freezing the PVRs when training the models they are backbones for. We also evaluate whether one of the PVRs is superior across all tasks, evaluate whether learned world models or Q-functions are better for switching decisions to safe policies, and discuss practical considerations for deploying these PVRs on resource-constrained devices.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution</title>
<link>https://arxiv.org/abs/2509.14816</link>
<guid>https://arxiv.org/abs/2509.14816</guid>
<content:encoded><![CDATA[
arXiv:2509.14816v1 Announce Type: cross 
Abstract: Reinforcement Learning (RL) robot controllers usually aggregate many task objectives into one scalar reward. While large-scale proximal policy optimisation (PPO) has enabled impressive results such as robust robot locomotion in the real world, many tasks still require careful reward tuning and are brittle to local optima. Tuning cost and sub-optimality grow with the number of objectives, limiting scalability. Modelling reward vectors and their trade-offs can address these issues; however, multi-objective methods remain underused in RL for robotics because of computational cost and optimisation difficulty. In this work, we investigate the conflict between gradient contributions for each objective that emerge from scalarising the task objectives. In particular, we explicitly address the conflict between task-based rewards and terms that regularise the policy towards realistic behaviour. We propose GCR-PPO, a modification to actor-critic optimisation that decomposes the actor update into objective-wise gradients using a multi-headed critic and resolves conflicts based on the objective priority. Our methodology, GCR-PPO, is evaluated on the well-known IsaacLab manipulation and locomotion benchmarks and additional multi-objective modifications on two related tasks. We show superior scalability compared to parallel PPO (p = 0.04), without significant computational overhead. We also show higher performance with more conflicting tasks. GCR-PPO improves on large-scale PPO with an average improvement of 9.5%, with high-conflict tasks observing a greater improvement. The code is available at https://github.com/humphreymunn/GCR-PPO.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Template-Based Cortical Surface Reconstruction with Minimal Energy Deformation</title>
<link>https://arxiv.org/abs/2509.14827</link>
<guid>https://arxiv.org/abs/2509.14827</guid>
<content:encoded><![CDATA[
arXiv:2509.14827v1 Announce Type: cross 
Abstract: Cortical surface reconstruction (CSR) from magnetic resonance imaging (MRI) is fundamental to neuroimage analysis, enabling morphological studies of the cerebral cortex and functional brain mapping. Recent advances in learning-based CSR have dramatically accelerated processing, allowing for reconstructions through the deformation of anatomical templates within seconds. However, ensuring the learned deformations are optimal in terms of deformation energy and consistent across training runs remains a particular challenge. In this work, we design a Minimal Energy Deformation (MED) loss, acting as a regularizer on the deformation trajectories and complementing the widely used Chamfer distance in CSR. We incorporate it into the recent V2C-Flow model and demonstrate considerable improvements in previously neglected training consistency and reproducibility without harming reconstruction accuracy and topological correctness.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProtoMedX: Towards Explainable Multi-Modal Prototype Learning for Bone Health Classification</title>
<link>https://arxiv.org/abs/2509.14830</link>
<guid>https://arxiv.org/abs/2509.14830</guid>
<content:encoded><![CDATA[
arXiv:2509.14830v1 Announce Type: cross 
Abstract: Bone health studies are crucial in medical practice for the early detection and treatment of Osteopenia and Osteoporosis. Clinicians usually make a diagnosis based on densitometry (DEXA scans) and patient history. The applications of AI in this field are ongoing research. Most successful methods rely on deep learning models that use vision alone (DEXA/X-ray imagery) and focus on prediction accuracy, while explainability is often disregarded and left to post hoc assessments of input contributions. We propose ProtoMedX, a multi-modal model that uses both DEXA scans of the lumbar spine and patient records. ProtoMedX's prototype-based architecture is explainable by design, which is crucial for medical applications, especially in the context of the upcoming EU AI Act, as it allows explicit analysis of model decisions, including incorrect ones. ProtoMedX demonstrates state-of-the-art performance in bone health classification while also providing explanations that can be visually understood by clinicians. Using a dataset of 4,160 real NHS patients, the proposed ProtoMedX achieves 87.58% accuracy in vision-only tasks and 89.8% in its multi-modal variant, both surpassing existing published methods.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sampling Method for Generalized Graph Signals with Pre-selected Vertices via DC Optimization</title>
<link>https://arxiv.org/abs/2509.14836</link>
<guid>https://arxiv.org/abs/2509.14836</guid>
<content:encoded><![CDATA[
arXiv:2509.14836v1 Announce Type: cross 
Abstract: This paper proposes a method for vertex-wise flexible sampling of a broad class of graph signals, designed to attain the best possible recovery based on the generalized sampling theory. This is achieved by designing a sampling operator by an optimization problem, which is inherently non-convex, as the best possible recovery imposes a rank constraint. An existing method for vertex-wise flexible sampling is able to control the number of active vertices but cannot incorporate prior knowledge of mandatory or forbidden vertices. To address these challenges, we formulate the operator design as a problem that handles a constraint of the number of active vertices and prior knowledge on specific vertices for sampling, mandatory inclusion or exclusion. We transformed this constrained problem into a difference-of-convex (DC) optimization problem by using the nuclear norm and a DC penalty for vertex selection. To solve this, we develop a convergent solver based on the general double-proximal gradient DC algorithm. The effectiveness of our method is demonstrated through experiments on various graph signal models, including real-world data, showing superior performance in the recovery accuracy by comparing to existing methods.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-Intrusive Parametrized-Background Data-Weak Reconstruction of Cardiac Displacement Fields from Sparse MRI-like Observations</title>
<link>https://arxiv.org/abs/2509.14844</link>
<guid>https://arxiv.org/abs/2509.14844</guid>
<content:encoded><![CDATA[
arXiv:2509.14844v1 Announce Type: cross 
Abstract: Personalized cardiac diagnostics require accurate reconstruction of myocardial displacement fields from sparse clinical imaging data, yet current methods often demand intrusive access to computational models. In this work, we apply the non-intrusive Parametrized-Background Data-Weak (PBDW) approach to three-dimensional (3D) cardiac displacement field reconstruction from limited Magnetic Resonance Image (MRI)-like observations. Our implementation requires only solution snapshots -- no governing equations, assembly routines, or solver access -- enabling immediate deployment across commercial and research codes using different constitutive models. Additionally, we introduce two enhancements: an H-size minibatch worst-case Orthogonal Matching Pursuit (wOMP) algorithm that improves Sensor Selection (SS) computational efficiency while maintaining reconstruction accuracy, and memory optimization techniques exploiting block matrix structures in vectorial problems. We demonstrate the effectiveness of the method through validation on a 3D left ventricular model with simulated scar tissue. Starting with noise-free reconstruction, we systematically incorporate Gaussian noise and spatial sparsity mimicking realistic MRI acquisition protocols. Results show exceptional accuracy in noise-free conditions (relative L2 error of order O(1e-5)), robust performance with 10% noise (relative L2 error of order O(1e-2)), and effective reconstruction from sparse measurements (relative L2 error of order O(1e-2)). The online reconstruction achieves four-order-of-magnitude computational speed-up compared to full Finite Element (FE) simulations, with reconstruction times under one tenth of second for sparse scenarios, demonstrating significant potential for integration into clinical cardiac modeling workflows.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Spherical geometry: Unraveling complex features of objects orbiting around stars from its transit light curve using deep learning</title>
<link>https://arxiv.org/abs/2509.14875</link>
<guid>https://arxiv.org/abs/2509.14875</guid>
<content:encoded><![CDATA[
arXiv:2509.14875v1 Announce Type: cross 
Abstract: Characterizing the geometry of an object orbiting around a star from its transit light curve is a powerful tool to uncover various complex phenomena. This problem is inherently ill-posed, since similar or identical light curves can be produced by multiple different shapes. In this study, we investigate the extent to which the features of a shape can be embedded in a transit light curve. We generate a library of two-dimensional random shapes and simulate their transit light curves with light curve simulator, Yuti. Each shape is decomposed into a series of elliptical components expressed in the form of Fourier coefficients that adds increasingly diminishing perturbations to an ideal ellipse. We train deep neural networks to predict these Fourier coefficients directly from simulated light curves. Our results demonstrate that the neural network can successfully reconstruct the low-order ellipses, which describe overall shape, orientation and large-scale perturbations. For higher order ellipses the scale is successfully determined but the inference of eccentricity and orientation is limited, demonstrating the extent of shape information in the light curve. We explore the impact of non-convex shape features in reconstruction, and show its dependence on shape orientation. The level of reconstruction achieved by the neural network underscores the utility of using light curves as a means to extract geometric information from transiting systems.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CARGO: A Framework for Confidence-Aware Routing of Large Language Models</title>
<link>https://arxiv.org/abs/2509.14899</link>
<guid>https://arxiv.org/abs/2509.14899</guid>
<content:encoded><![CDATA[
arXiv:2509.14899v1 Announce Type: cross 
Abstract: As large language models (LLMs) proliferate in scale, specialization, and latency profiles, the challenge of routing user prompts to the most appropriate model has become increasingly critical for balancing performance and cost. We introduce CARGO (Category-Aware Routing with Gap-based Optimization), a lightweight, confidence-aware framework for dynamic LLM selection. CARGO employs a single embedding-based regressor trained on LLM-judged pairwise comparisons to predict model performance, with an optional binary classifier invoked when predictions are uncertain. This two-stage design enables precise, cost-aware routing without the need for human-annotated supervision. To capture domain-specific behavior, CARGO also supports category-specific regressors trained across five task groups: mathematics, coding, reasoning, summarization, and creative writing. Evaluated on four competitive LLMs (GPT-4o, Claude 3.5 Sonnet, DeepSeek V3, and Perplexity Sonar), CARGO achieves a top-1 routing accuracy of 76.4% and win rates ranging from 72% to 89% against individual experts. These results demonstrate that confidence-guided, lightweight routing can achieve expert-level performance with minimal overhead, offering a practical solution for real-world, multi-model LLM deployments.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inspired by machine learning optimization: can gradient-based optimizers solve cycle skipping in full waveform inversion given sufficient iterations?</title>
<link>https://arxiv.org/abs/2509.14919</link>
<guid>https://arxiv.org/abs/2509.14919</guid>
<content:encoded><![CDATA[
arXiv:2509.14919v1 Announce Type: cross 
Abstract: Full waveform inversion (FWI) iteratively updates the velocity model by minimizing the difference between observed and simulated data. Due to the high computational cost and memory requirements associated with global optimization algorithms, FWI is typically implemented using local optimization methods. However, when the initial velocity model is inaccurate and low-frequency seismic data (e.g., below 3 Hz) are absent, the mismatch between simulated and observed data may exceed half a cycle, a phenomenon known as cycle skipping. In such cases, local optimization algorithms (e.g., gradient-based local optimizers) tend to converge to local minima, leading to inaccurate inversion results. In machine learning, neural network training is also an optimization problem prone to local minima. It often employs gradient-based optimizers with a relatively large learning rate (beyond the theoretical limits of local optimization that are usually determined numerically by a line search), which allows the optimization to behave like a quasi-global optimizer. Consequently, after training for several thousand iterations, we can obtain a neural network model with strong generative capability. In this study, we also employ gradient-based optimizers with a relatively large learning rate for FWI. Results from both synthetic and field data experiments show that FWI may initially converge to a local minimum; however, with sufficient additional iterations, the inversion can gradually approach the global minimum, slowly from shallow subsurface to deep, ultimately yielding an accurate velocity model. Furthermore, numerical examples indicate that, given sufficient iterations, reasonable velocity inversion results can still be achieved even when low-frequency data below 5 Hz are missing.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Patent Language Model Pretraining with ModernBERT</title>
<link>https://arxiv.org/abs/2509.14926</link>
<guid>https://arxiv.org/abs/2509.14926</guid>
<content:encoded><![CDATA[
arXiv:2509.14926v1 Announce Type: cross 
Abstract: Transformer-based language models such as BERT have become foundational in NLP, yet their performance degrades in specialized domains like patents, which contain long, technical, and legally structured text. Prior approaches to patent NLP have primarily relied on fine-tuning general-purpose models or domain-adapted variants pretrained with limited data. In this work, we pretrain 3 domain-specific masked language models for patents, using the ModernBERT architecture and a curated corpus of over 60 million patent records. Our approach incorporates architectural optimizations, including FlashAttention, rotary embeddings, and GLU feed-forward layers. We evaluate our models on four downstream patent classification tasks. Our model, ModernBERT-base-PT, consistently outperforms the general-purpose ModernBERT baseline on three out of four datasets and achieves competitive performance with a baseline PatentBERT. Additional experiments with ModernBERT-base-VX and Mosaic-BERT-large demonstrate that scaling the model size and customizing the tokenizer further enhance performance on selected tasks. Notably, all ModernBERT variants retain substantially faster inference over - 3x that of PatentBERT - underscoring their suitability for time-sensitive applications. These results underscore the benefits of domain-specific pretraining and architectural improvements for patent-focused NLP tasks.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robot Control Stack: A Lean Ecosystem for Robot Learning at Scale</title>
<link>https://arxiv.org/abs/2509.14932</link>
<guid>https://arxiv.org/abs/2509.14932</guid>
<content:encoded><![CDATA[
arXiv:2509.14932v1 Announce Type: cross 
Abstract: Vision-Language-Action models (VLAs) mark a major shift in robot learning. They replace specialized architectures and task-tailored components of expert policies with large-scale data collection and setup-specific fine-tuning. In this machine learning-focused workflow that is centered around models and scalable training, traditional robotics software frameworks become a bottleneck, while robot simulations offer only limited support for transitioning from and to real-world experiments. In this work, we close this gap by introducing Robot Control Stack (RCS), a lean ecosystem designed from the ground up to support research in robot learning with large-scale generalist policies. At its core, RCS features a modular and easily extensible layered architecture with a unified interface for simulated and physical robots, facilitating sim-to-real transfer. Despite its minimal footprint and dependencies, it offers a complete feature set, enabling both real-world experiments and large-scale training in simulation. Our contribution is twofold: First, we introduce the architecture of RCS and explain its design principles. Second, we evaluate its usability and performance along the development cycle of VLA and RL policies. Our experiments also provide an extensive evaluation of Octo, OpenVLA, and Pi Zero on multiple robots and shed light on how simulation data can improve real-world policy performance. Our code, datasets, weights, and videos are available at: https://robotcontrolstack.github.io/
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating data replication in text-to-audio generative diffusion models through anti-memorization guidance</title>
<link>https://arxiv.org/abs/2509.14934</link>
<guid>https://arxiv.org/abs/2509.14934</guid>
<content:encoded><![CDATA[
arXiv:2509.14934v1 Announce Type: cross 
Abstract: A persistent challenge in generative audio models is data replication, where the model unintentionally generates parts of its training data during inference. In this work, we address this issue in text-to-audio diffusion models by exploring the use of anti-memorization strategies. We adopt Anti-Memorization Guidance (AMG), a technique that modifies the sampling process of pre-trained diffusion models to discourage memorization. Our study explores three types of guidance within AMG, each designed to reduce replication while preserving generation quality. We use Stable Audio Open as our backbone, leveraging its fully open-source architecture and training dataset. Our comprehensive experimental analysis suggests that AMG significantly mitigates memorization in diffusion-based text-to-audio generation without compromising audio fidelity or semantic alignment.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable AI for Infection Prevention and Control: Modeling CPE Acquisition and Patient Outcomes in an Irish Hospital with Transformers</title>
<link>https://arxiv.org/abs/2509.14942</link>
<guid>https://arxiv.org/abs/2509.14942</guid>
<content:encoded><![CDATA[
arXiv:2509.14942v1 Announce Type: cross 
Abstract: Carbapenemase-Producing Enterobacteriace poses a critical concern for infection prevention and control in hospitals. However, predictive modeling of previously highlighted CPE-associated risks such as readmission, mortality, and extended length of stay (LOS) remains underexplored, particularly with modern deep learning approaches. This study introduces an eXplainable AI modeling framework to investigate CPE impact on patient outcomes from Electronic Medical Records data of an Irish hospital. We analyzed an inpatient dataset from an Irish acute hospital, incorporating diagnostic codes, ward transitions, patient demographics, infection-related variables and contact network features. Several Transformer-based architectures were benchmarked alongside traditional machine learning models. Clinical outcomes were predicted, and XAI techniques were applied to interpret model decisions. Our framework successfully demonstrated the utility of Transformer-based models, with TabTransformer consistently outperforming baselines across multiple clinical prediction tasks, especially for CPE acquisition (AUROC and sensitivity). We found infection-related features, including historical hospital exposure, admission context, and network centrality measures, to be highly influential in predicting patient outcomes and CPE acquisition risk. Explainability analyses revealed that features like "Area of Residence", "Admission Ward" and prior admissions are key risk factors. Network variables like "Ward PageRank" also ranked highly, reflecting the potential value of structural exposure information. This study presents a robust and explainable AI framework for analyzing complex EMR data to identify key risk factors and predict CPE-related outcomes. Our findings underscore the superior performance of the Transformer models and highlight the importance of diverse clinical and network features.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards universal property prediction in Cartesian space: TACE is all you need</title>
<link>https://arxiv.org/abs/2509.14961</link>
<guid>https://arxiv.org/abs/2509.14961</guid>
<content:encoded><![CDATA[
arXiv:2509.14961v1 Announce Type: cross 
Abstract: Machine learning has revolutionized atomistic simulations and materials science, yet current approaches often depend on spherical-harmonic representations. Here we introduce the Tensor Atomic Cluster Expansion and Tensor Moment Potential, the first unified framework formulated entirely in Cartesian space for the systematic prediction of arbitrary structure-determined tensorial properties. TACE achieves this by decomposing atomic environments into a complete hierarchy of (irreducible) Cartesian tensors, ensuring symmetry-consistent representations that naturally encode invariance and equivariance constraints. Beyond geometry, TACE incorporates universal embeddings that flexibly integrate diverse attributes including basis sets, charges, magnetic moments and field perturbations. This allows explicit control over external invariants and equivariants in the prediction process. Long-range interactions are also accurately described through the Latent Ewald Summation module within the short-range approximation, providing a rigorous yet computationally efficient treatment of electrostatic interactions. We demonstrate that TACE attains accuracy, stability, and efficiency on par with or surpassing leading equivariant frameworks across finite molecules and extended materials, including in-domain and out-of-domain benchmarks, spectra, hessians, external-field response, charged systems, magnetic systems, multi-fidelity training, and heterogeneous catalytic systems. Crucially, TACE bridges scalar and tensorial modeling and establishes a Cartesian-space paradigm that unifies and extends beyond the design space of spherical-harmonic-based methods. This work lays the foundation for a new generation of universal atomistic machine learning models capable of systematically capturing the rich interplay of geometry, fields and material properties within a single coherent framework.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Role of Touch: Towards Optimal Tactile Sensing Distribution in Anthropomorphic Hands for Dexterous In-Hand Manipulation</title>
<link>https://arxiv.org/abs/2509.14984</link>
<guid>https://arxiv.org/abs/2509.14984</guid>
<content:encoded><![CDATA[
arXiv:2509.14984v1 Announce Type: cross 
Abstract: In-hand manipulation tasks, particularly in human-inspired robotic systems, must rely on distributed tactile sensing to achieve precise control across a wide variety of tasks. However, the optimal configuration of this network of sensors is a complex problem, and while the fingertips are a common choice for placing sensors, the contribution of tactile information from other regions of the hand is often overlooked. This work investigates the impact of tactile feedback from various regions of the fingers and palm in performing in-hand object reorientation tasks. We analyze how sensory feedback from different parts of the hand influences the robustness of deep reinforcement learning control policies and investigate the relationship between object characteristics and optimal sensor placement. We identify which tactile sensing configurations contribute to improving the efficiency and accuracy of manipulation. Our results provide valuable insights for the design and use of anthropomorphic end-effectors with enhanced manipulation capabilities.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Blockchain-Enabled Explainable AI for Trusted Healthcare Systems</title>
<link>https://arxiv.org/abs/2509.14987</link>
<guid>https://arxiv.org/abs/2509.14987</guid>
<content:encoded><![CDATA[
arXiv:2509.14987v1 Announce Type: cross 
Abstract: This paper introduces a Blockchain-Integrated Explainable AI Framework (BXHF) for healthcare systems to tackle two essential challenges confronting health information networks: safe data exchange and comprehensible AI-driven clinical decision-making. Our architecture incorporates blockchain, ensuring patient records are immutable, auditable, and tamper-proof, alongside Explainable AI (XAI) methodologies that yield transparent and clinically relevant model predictions. By incorporating security assurances and interpretability requirements into a unified optimization pipeline, BXHF ensures both data-level trust (by verified and encrypted record sharing) and decision-level trust (with auditable and clinically aligned explanations). Its hybrid edge-cloud architecture allows for federated computation across different institutions, enabling collaborative analytics while protecting patient privacy. We demonstrate the framework's applicability through use cases such as cross-border clinical research networks, uncommon illness detection and high-risk intervention decision support. By ensuring transparency, auditability, and regulatory compliance, BXHF improves the credibility, uptake, and effectiveness of AI in healthcare, laying the groundwork for safer and more reliable clinical decision-making.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BabyHuBERT: Multilingual Self-Supervised Learning for Segmenting Speakers in Child-Centered Long-Form Recordings</title>
<link>https://arxiv.org/abs/2509.15001</link>
<guid>https://arxiv.org/abs/2509.15001</guid>
<content:encoded><![CDATA[
arXiv:2509.15001v1 Announce Type: cross 
Abstract: Child-centered long-form recordings are essential for studying early language development, but existing speech models trained on clean adult data perform poorly due to acoustic and linguistic differences. We introduce BabyHuBERT, the first self-supervised speech representation model trained on 13,000 hours of multilingual child-centered long-form recordings spanning over 40 languages. We evaluate BabyHuBERT on speaker segmentation, identifying when target children speak versus female adults, male adults, or other children -- a fundamental preprocessing step for analyzing naturalistic language experiences. BabyHuBERT achieves F1-scores from 52.1% to 74.4% across six diverse datasets, consistently outperforming W2V2-LL4300 (trained on English long-forms) and standard HuBERT (trained on clean adult speech). Notable improvements include 13.2 absolute F1 points over HuBERT on Vanuatu and 15.9 points on Solomon Islands corpora, demonstrating effectiveness on underrepresented languages. By sharing code and models, BabyHuBERT serves as a foundation model for child speech research, enabling fine-tuning on diverse downstream tasks.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Undersampled Phase Retrieval with Image Priors</title>
<link>https://arxiv.org/abs/2509.15026</link>
<guid>https://arxiv.org/abs/2509.15026</guid>
<content:encoded><![CDATA[
arXiv:2509.15026v1 Announce Type: cross 
Abstract: Phase retrieval seeks to recover a complex signal from amplitude-only measurements, a challenging nonlinear inverse problem. Current theory and algorithms often ignore signal priors. By contrast, we evaluate here a variety of image priors in the context of severe undersampling with structured random Fourier measurements. Our results show that those priors significantly improve reconstruction, allowing accurate reconstruction even below the weak recovery threshold.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed GCN-LSTM Framework for Long-Term Forecasting of 2D and 3D Microstructure Evolution</title>
<link>https://arxiv.org/abs/2509.15029</link>
<guid>https://arxiv.org/abs/2509.15029</guid>
<content:encoded><![CDATA[
arXiv:2509.15029v1 Announce Type: cross 
Abstract: This paper presents a physics-informed framework that integrates graph convolutional networks (GCN) with long short-term memory (LSTM) architecture to forecast microstructure evolution over long time horizons in both 2D and 3D with remarkable performance across varied metrics. The proposed framework is composition-aware, trained jointly on datasets with different compositions, and operates in latent graph space, which enables the model to capture compositions and morphological dynamics while remaining computationally efficient. Compressing and encoding phase-field simulation data with convolutional autoencoders and operating in Latent graph space facilitates efficient modeling of microstructural evolution across composition, dimensions, and long-term horizons. The framework captures the spatial and temporal patterns of evolving microstructures while enabling long-range forecasting at reduced computational cost after training.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic-to-Real Object Detection using YOLOv11 and Domain Randomization Strategies</title>
<link>https://arxiv.org/abs/2509.15045</link>
<guid>https://arxiv.org/abs/2509.15045</guid>
<content:encoded><![CDATA[
arXiv:2509.15045v1 Announce Type: cross 
Abstract: This paper addresses the synthetic-to-real domain gap in object detection, focusing on training a YOLOv11 model to detect a specific object (a soup can) using only synthetic data and domain randomization strategies. The methodology involves extensive experimentation with data augmentation, dataset composition, and model scaling. While synthetic validation metrics were consistently high, they proved to be poor predictors of real-world performance. Consequently, models were also evaluated qualitatively, through visual inspection of predictions, and quantitatively, on a manually labeled real-world test set, to guide development. Final mAP@50 scores were provided by the official Kaggle competition. Key findings indicate that increasing synthetic dataset diversity, specifically by including varied perspectives and complex backgrounds, combined with carefully tuned data augmentation, were crucial in bridging the domain gap. The best performing configuration, a YOLOv11l model trained on an expanded and diverse dataset, achieved a final mAP@50 of 0.910 on the competition's hidden test set. This result demonstrates the potential of a synthetic-only training approach while also highlighting the remaining challenges in fully capturing real-world variability.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time Streaming Mel Vocoding with Generative Flow Matching</title>
<link>https://arxiv.org/abs/2509.15085</link>
<guid>https://arxiv.org/abs/2509.15085</guid>
<content:encoded><![CDATA[
arXiv:2509.15085v1 Announce Type: cross 
Abstract: The task of Mel vocoding, i.e., the inversion of a Mel magnitude spectrogram to an audio waveform, is still a key component in many text-to-speech (TTS) systems today. Based on generative flow matching, our prior work on generative STFT phase retrieval (DiffPhase), and the pseudoinverse operator of the Mel filterbank, we develop MelFlow, a streaming-capable generative Mel vocoder for speech sampled at 16 kHz with an algorithmic latency of only 32 ms and a total latency of 48 ms. We show real-time streaming capability at this latency not only in theory, but in practice on a consumer laptop GPU. Furthermore, we show that our model achieves substantially better PESQ and SI-SDR values compared to well-established not streaming-capable baselines for Mel vocoding including HiFi-GAN.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shedding Light on Dark Matter at the LHC with Machine Learning</title>
<link>https://arxiv.org/abs/2509.15121</link>
<guid>https://arxiv.org/abs/2509.15121</guid>
<content:encoded><![CDATA[
arXiv:2509.15121v1 Announce Type: cross 
Abstract: We investigate a WIMP dark matter (DM) candidate in the form of a singlino-dominated lightest supersymmetric particle (LSP) within the $Z_3$-symmetric Next-to-Minimal Supersymmetric Standard Model. This framework gives rise to regions of parameter space where DM is obtained via co-annihilation with nearby higgsino-like electroweakinos and DM direct detection~signals are suppressed, the so-called ``blind spots". On the other hand, collider signatures remain promising due to enhanced radiative decay modes of higgsinos into the singlino-dominated LSP and a photon, rather than into leptons or hadrons. This motivates searches for radiatively decaying neutralinos, however, these signals face substantial background challenges, as the decay products are typically soft due to the small mass-splits ($\Delta m$) between the LSP and the higgsino-like coannihilation partners. We apply a data-driven Machine Learning (ML) analysis that improves sensitivity to these subtle signals, offering a powerful complement to traditional search strategies to discover a new physics scenario. Using an LHC integrated luminosity of $100~\mathrm{fb}^{-1}$ at $14~\mathrm{TeV}$, the method achieves a $5\sigma$ discovery reach for higgsino masses up to $225~\mathrm{GeV}$ with $\Delta m\!\lesssim\!12~\mathrm{GeV}$, and a $2\sigma$ exclusion up to $285~\mathrm{GeV}$ with $\Delta m\!\lesssim\!20~\mathrm{GeV}$. These results highlight the power of collider searches to probe DM candidates that remain hidden from current direct detection experiments, and provide a motivation for a search by the LHC collaborations using ML methods.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Mechanistic Subtypes of Neurodegeneration with a Physics-Informed Variational Autoencoder Mixture Model</title>
<link>https://arxiv.org/abs/2509.15124</link>
<guid>https://arxiv.org/abs/2509.15124</guid>
<content:encoded><![CDATA[
arXiv:2509.15124v1 Announce Type: cross 
Abstract: Modelling the underlying mechanisms of neurodegenerative diseases demands methods that capture heterogeneous and spatially varying dynamics from sparse, high-dimensional neuroimaging data. Integrating partial differential equation (PDE) based physics knowledge with machine learning provides enhanced interpretability and utility over classic numerical methods. However, current physics-integrated machine learning methods are limited to considering a single PDE, severely limiting their application to diseases where multiple mechanisms are responsible for different groups (i.e., subtypes) and aggravating problems with model misspecification and degeneracy. Here, we present a deep generative model for learning mixtures of latent dynamic models governed by physics-based PDEs, going beyond traditional approaches that assume a single PDE structure. Our method integrates reaction-diffusion PDEs within a variational autoencoder (VAE) mixture model framework, supporting inference of subtypes of interpretable latent variables (e.g. diffusivity and reaction rates) from neuroimaging data. We evaluate our method on synthetic benchmarks and demonstrate its potential for uncovering mechanistic subtypes of Alzheimer's disease progression from positron emission tomography (PET) data.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Rate Should Scale Inversely with High-Order Data Moments in High-Dimensional Online Independent Component Analysis</title>
<link>https://arxiv.org/abs/2509.15127</link>
<guid>https://arxiv.org/abs/2509.15127</guid>
<content:encoded><![CDATA[
arXiv:2509.15127v1 Announce Type: cross 
Abstract: We investigate the impact of high-order moments on the learning dynamics of an online Independent Component Analysis (ICA) algorithm under a high-dimensional data model composed of a weighted sum of two non-Gaussian random variables. This model allows precise control of the input moment structure via a weighting parameter. Building on an existing ordinary differential equation (ODE)-based analysis in the high-dimensional limit, we demonstrate that as the high-order moments increase, the algorithm exhibits slower convergence and demands both a lower learning rate and greater initial alignment to achieve informative solutions. Our findings highlight the algorithm's sensitivity to the statistical structure of the input data, particularly its moment characteristics. Furthermore, the ODE framework reveals a critical learning rate threshold necessary for learning when moments approach their maximum. These insights motivate future directions in moment-aware initialization and adaptive learning rate strategies to counteract the degradation in learning speed caused by high non-Gaussianity, thereby enhancing the robustness and efficiency of ICA in complex, high-dimensional settings.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benefits of Online Tilted Empirical Risk Minimization: A Case Study of Outlier Detection and Robust Regression</title>
<link>https://arxiv.org/abs/2509.15141</link>
<guid>https://arxiv.org/abs/2509.15141</guid>
<content:encoded><![CDATA[
arXiv:2509.15141v1 Announce Type: cross 
Abstract: Empirical Risk Minimization (ERM) is a foundational framework for supervised learning but primarily optimizes average-case performance, often neglecting fairness and robustness considerations. Tilted Empirical Risk Minimization (TERM) extends ERM by introducing an exponential tilt hyperparameter $t$ to balance average-case accuracy with worst-case fairness and robustness. However, in online or streaming settings where data arrive one sample at a time, the classical TERM objective degenerates to standard ERM, losing tilt sensitivity. We address this limitation by proposing an online TERM formulation that removes the logarithm from the classical objective, preserving tilt effects without additional computational or memory overhead. This formulation enables a continuous trade-off controlled by $t$, smoothly interpolating between ERM ($t \to 0$), fairness emphasis ($t > 0$), and robustness to outliers ($t < 0$). We empirically validate online TERM on two representative streaming tasks: robust linear regression with adversarial outliers and minority-class detection in binary classification. Our results demonstrate that negative tilting effectively suppresses outlier influence, while positive tilting improves recall with minimal impact on precision, all at per-sample computational cost equivalent to ERM. Online TERM thus recovers the full robustness-fairness spectrum of classical TERM in an efficient single-sample learning regime.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Next-Depth Lookahead Tree</title>
<link>https://arxiv.org/abs/2509.15143</link>
<guid>https://arxiv.org/abs/2509.15143</guid>
<content:encoded><![CDATA[
arXiv:2509.15143v1 Announce Type: cross 
Abstract: This paper proposes the Next-Depth Lookahead Tree (NDLT), a single-tree model designed to improve performance by evaluating node splits not only at the node being optimized but also by evaluating the quality of the next depth level.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Asymptotic Study of In-context Learning with Random Transformers through Equivalent Models</title>
<link>https://arxiv.org/abs/2509.15152</link>
<guid>https://arxiv.org/abs/2509.15152</guid>
<content:encoded><![CDATA[
arXiv:2509.15152v1 Announce Type: cross 
Abstract: We study the in-context learning (ICL) capabilities of pretrained Transformers in the setting of nonlinear regression. Specifically, we focus on a random Transformer with a nonlinear MLP head where the first layer is randomly initialized and fixed while the second layer is trained. Furthermore, we consider an asymptotic regime where the context length, input dimension, hidden dimension, number of training tasks, and number of training samples jointly grow. In this setting, we show that the random Transformer behaves equivalent to a finite-degree Hermite polynomial model in terms of ICL error. This equivalence is validated through simulations across varying activation functions, context lengths, hidden layer widths (revealing a double-descent phenomenon), and regularization settings. Our results offer theoretical and empirical insights into when and how MLP layers enhance ICL, and how nonlinearity and over-parameterization influence model performance.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AnoF-Diff: One-Step Diffusion-Based Anomaly Detection for Forceful Tool Use</title>
<link>https://arxiv.org/abs/2509.15153</link>
<guid>https://arxiv.org/abs/2509.15153</guid>
<content:encoded><![CDATA[
arXiv:2509.15153v1 Announce Type: cross 
Abstract: Multivariate time-series anomaly detection, which is critical for identifying unexpected events, has been explored in the field of machine learning for several decades. However, directly applying these methods to data from forceful tool use tasks is challenging because streaming sensor data in the real world tends to be inherently noisy, exhibits non-stationary behavior, and varies across different tasks and tools. To address these challenges, we propose a method, AnoF-Diff, based on the diffusion model to extract force-torque features from time-series data and use force-torque features to detect anomalies. We compare our method with other state-of-the-art methods in terms of F1-score and Area Under the Receiver Operating Characteristic curve (AUROC) on four forceful tool-use tasks, demonstrating that our method has better performance and is more robust to a noisy dataset. We also propose the method of parallel anomaly score evaluation based on one-step diffusion and demonstrate how our method can be used for online anomaly detection in several forceful tool use experiments.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semi-Supervised 3D Medical Segmentation from 2D Natural Images Pretrained Model</title>
<link>https://arxiv.org/abs/2509.15167</link>
<guid>https://arxiv.org/abs/2509.15167</guid>
<content:encoded><![CDATA[
arXiv:2509.15167v1 Announce Type: cross 
Abstract: This paper explores the transfer of knowledge from general vision models pretrained on 2D natural images to improve 3D medical image segmentation. We focus on the semi-supervised setting, where only a few labeled 3D medical images are available, along with a large set of unlabeled images. To tackle this, we propose a model-agnostic framework that progressively distills knowledge from a 2D pretrained model to a 3D segmentation model trained from scratch. Our approach, M&amp;N, involves iterative co-training of the two models using pseudo-masks generated by each other, along with our proposed learning rate guided sampling that adaptively adjusts the proportion of labeled and unlabeled data in each training batch to align with the models' prediction accuracy and stability, minimizing the adverse effect caused by inaccurate pseudo-masks. Extensive experiments on multiple publicly available datasets demonstrate that M&amp;N achieves state-of-the-art performance, outperforming thirteen existing semi-supervised segmentation approaches under all different settings. Importantly, ablation studies show that M&amp;N remains model-agnostic, allowing seamless integration with different architectures. This ensures its adaptability as more advanced models emerge. The code is available at https://github.com/pakheiyeung/M-N.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast and Fluent Diffusion Language Models via Convolutional Decoding and Rejective Fine-tuning</title>
<link>https://arxiv.org/abs/2509.15188</link>
<guid>https://arxiv.org/abs/2509.15188</guid>
<content:encoded><![CDATA[
arXiv:2509.15188v1 Announce Type: cross 
Abstract: Autoregressive (AR) language models generate text one token at a time, which limits their inference speed. Diffusion-based language models offer a promising alternative, as they can decode multiple tokens in parallel. However, we identify a key bottleneck in current diffusion LMs: the long decoding-window problem, where tokens generated far from the input context often become irrelevant or repetitive. Previous solutions like semi-autoregressive address this issue by splitting windows into blocks, but this sacrifices speed and bidirectionality, eliminating the main advantage of diffusion models. To overcome this, we propose Convolutional decoding (Conv), a normalization-based method that narrows the decoding window without hard segmentation, leading to better fluency and flexibility. Additionally, we introduce Rejecting Rule-based Fine-Tuning (R2FT), a post-hoc training scheme that better aligns tokens at positions far from context. Our methods achieve state-of-the-art results on open-ended generation benchmarks (e.g., AlpacaEval) among diffusion LM baselines, with significantly lower step size than previous works, demonstrating both speed and quality improvements.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explicit Context-Driven Neural Acoustic Modeling for High-Fidelity RIR Generation</title>
<link>https://arxiv.org/abs/2509.15210</link>
<guid>https://arxiv.org/abs/2509.15210</guid>
<content:encoded><![CDATA[
arXiv:2509.15210v1 Announce Type: cross 
Abstract: Realistic sound simulation plays a critical role in many applications. A key element in sound simulation is the room impulse response (RIR), which characterizes how sound propagates from a source to a listener within a given space. Recent studies have applied neural implicit methods to learn RIR using context information collected from the environment, such as scene images. However, these approaches do not effectively leverage explicit geometric information from the environment. To further exploit the potential of neural implicit models with direct geometric features, we present Mesh-infused Neural Acoustic Field (MiNAF), which queries a rough room mesh at given locations and extracts distance distributions as an explicit representation of local context. Our approach demonstrates that incorporating explicit local geometric features can better guide the neural network in generating more accurate RIR predictions. Through comparisons with conventional and state-of-the-art baseline methods, we show that MiNAF performs competitively across various evaluation metrics. Furthermore, we verify the robustness of MiNAF in datasets with limited training samples, demonstrating an advance in high-fidelity sound simulation.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizable Geometric Image Caption Synthesis</title>
<link>https://arxiv.org/abs/2509.15217</link>
<guid>https://arxiv.org/abs/2509.15217</guid>
<content:encoded><![CDATA[
arXiv:2509.15217v1 Announce Type: cross 
Abstract: Multimodal large language models have various practical applications that demand strong reasoning abilities. Despite recent advancements, these models still struggle to solve complex geometric problems. A key challenge stems from the lack of high-quality image-text pair datasets for understanding geometric images. Furthermore, most template-based data synthesis pipelines typically fail to generalize to questions beyond their predefined templates. In this paper, we bridge this gap by introducing a complementary process of Reinforcement Learning with Verifiable Rewards (RLVR) into the data generation pipeline. By adopting RLVR to refine captions for geometric images synthesized from 50 basic geometric relations and using reward signals derived from mathematical problem-solving tasks, our pipeline successfully captures the key features of geometry problem-solving. This enables better task generalization and yields non-trivial improvements. Furthermore, even in out-of-distribution scenarios, the generated dataset enhances the general reasoning capabilities of multimodal large language models, yielding accuracy improvements of $2.8\%\text{-}4.8\%$ in statistics, arithmetic, algebraic, and numerical tasks with non-geometric input images of MathVista and MathVerse, along with $2.4\%\text{-}3.9\%$ improvements in Art, Design, Tech, and Engineering tasks in MMMU.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Out-of-Sight Trajectories: Tracking, Fusion, and Prediction</title>
<link>https://arxiv.org/abs/2509.15219</link>
<guid>https://arxiv.org/abs/2509.15219</guid>
<content:encoded><![CDATA[
arXiv:2509.15219v1 Announce Type: cross 
Abstract: Trajectory prediction is a critical task in computer vision and autonomous systems, playing a key role in autonomous driving, robotics, surveillance, and virtual reality. Existing methods often rely on complete and noise-free observational data, overlooking the challenges associated with out-of-sight objects and the inherent noise in sensor data caused by limited camera coverage, obstructions, and the absence of ground truth for denoised trajectories. These limitations pose safety risks and hinder reliable prediction in real-world scenarios. In this extended work, we present advancements in Out-of-Sight Trajectory (OST), a novel task that predicts the noise-free visual trajectories of out-of-sight objects using noisy sensor data. Building on our previous research, we broaden the scope of Out-of-Sight Trajectory Prediction (OOSTraj) to include pedestrians and vehicles, extending its applicability to autonomous driving, robotics, surveillance, and virtual reality. Our enhanced Vision-Positioning Denoising Module leverages camera calibration to establish a vision-positioning mapping, addressing the lack of visual references, while effectively denoising noisy sensor data in an unsupervised manner. Through extensive evaluations on the Vi-Fi and JRDB datasets, our approach achieves state-of-the-art performance in both trajectory denoising and prediction, significantly surpassing previous baselines. Additionally, we introduce comparisons with traditional denoising methods, such as Kalman filtering, and adapt recent trajectory prediction models to our task, providing a comprehensive benchmark. This work represents the first initiative to integrate vision-positioning projection for denoising noisy sensor trajectories of out-of-sight agents, paving the way for future advances. The code and preprocessed datasets are available at github.com/Hai-chao-Zhang/OST
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combining Minkowski and Chebyshev: New distance proposal and survey of distance metrics using k-nearest neighbours classifier</title>
<link>https://arxiv.org/abs/2112.12549</link>
<guid>https://arxiv.org/abs/2112.12549</guid>
<content:encoded><![CDATA[
arXiv:2112.12549v2 Announce Type: replace 
Abstract: This work proposes a distance that combines Minkowski and Chebyshev distances and can be seen as an intermediary distance. This combination not only achieves efficient run times in neighbourhood iteration tasks in Z^2, but also obtains good accuracies when coupled with the k-Nearest Neighbours (k-NN) classifier. The proposed distance is approximately 1.3 times faster than Manhattan distance and 329.5 times faster than Euclidean distance in discrete neighbourhood iterations. An accuracy analysis of the k-NN classifier using a total of 33 datasets from the UCI repository, 15 distances and values assigned to k that vary from 1 to 200 is presented. In this experiment, the proposed distance obtained accuracies that were better than the average more often than its counterparts (in 26 cases out of 33), and also obtained the best accuracy more frequently (in 9 out of 33 cases).
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rule-Based Error Detection and Correction to Operationalize Movement Trajectory Classification</title>
<link>https://arxiv.org/abs/2308.14250</link>
<guid>https://arxiv.org/abs/2308.14250</guid>
<content:encoded><![CDATA[
arXiv:2308.14250v5 Announce Type: replace 
Abstract: Classification of movement trajectories has many applications in transportation and is a key component for large-scale movement trajectory generation and anomaly detection which has key safety applications in the aftermath of a disaster or other external shock. However, the current state-of-the-art (SOTA) are based on supervised deep learning - which leads to challenges when the distribution of trajectories changes due to such a shock. We provide a neuro-symbolic rule-based framework to conduct error correction and detection of these models to integrate into our movement trajectory platform. We provide a suite of experiments on several recent SOTA models where we show highly accurate error detection, the ability to improve accuracy with a changing test distribution, and accuracy improvement for the base use case in addition to a suite of theoretical properties that informed algorithm development. Specifically, we show an F1 scores for predicting errors of up to 0.984, significant performance increase for out-of distribution accuracy (8.51% improvement over SOTA for zero-shot accuracy), and accuracy improvement over the SOTA model.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatio-Temporal Anomaly Detection with Graph Networks for Data Quality Monitoring of the Hadron Calorimeter</title>
<link>https://arxiv.org/abs/2311.04190</link>
<guid>https://arxiv.org/abs/2311.04190</guid>
<content:encoded><![CDATA[
arXiv:2311.04190v2 Announce Type: replace 
Abstract: The Compact Muon Solenoid (CMS) experiment is a general-purpose detector for high-energy collision at the Large Hadron Collider (LHC) at CERN. It employs an online data quality monitoring (DQM) system to promptly spot and diagnose particle data acquisition problems to avoid data quality loss. In this study, we present a semi-supervised spatio-temporal anomaly detection (AD) monitoring system for the physics particle reading channels of the Hadron Calorimeter (HCAL) of the CMS using three-dimensional digi-occupancy map data of the DQM. We propose the GraphSTAD system, which employs convolutional and graph neural networks to learn local spatial characteristics induced by particles traversing the detector and the global behavior owing to shared backend circuit connections and housing boxes of the channels, respectively. Recurrent neural networks capture the temporal evolution of the extracted spatial features. We validate the accuracy of the proposed AD system in capturing diverse channel fault types using the LHC collision data sets. The GraphSTAD system achieves production-level accuracy and is being integrated into the CMS core production system for real-time monitoring of the HCAL. We provide a quantitative performance comparison with alternative benchmark models to demonstrate the promising leverage of the presented system. Code: \href{https://github.com/muleina/CMS_HCAL_ML_OnlineDQM}{https://github.com/muleina/CMS\_HCAL\_ML\_OnlineDQM}
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Learning to Optimize to Learning Optimization Algorithms</title>
<link>https://arxiv.org/abs/2405.18222</link>
<guid>https://arxiv.org/abs/2405.18222</guid>
<content:encoded><![CDATA[
arXiv:2405.18222v3 Announce Type: replace 
Abstract: Towards designing learned optimization algorithms that are usable beyond their training setting, we identify key principles that classical algorithms obey, but have up to now, not been used for Learning to Optimize (L2O). Following these principles, we provide a general design pipeline, taking into account data, architecture and learning strategy, and thereby enabling a synergy between classical optimization and L2O, resulting in a philosophy of Learning Optimization Algorithms. As a consequence our learned algorithms perform well far beyond problems from the training distribution. We demonstrate the success of these novel principles by designing a new learning-enhanced BFGS algorithm and provide numerical experiments evidencing its adaptation to many settings at test time.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learn while Unlearn: An Iterative Unlearning Framework for Generative Language Models</title>
<link>https://arxiv.org/abs/2407.20271</link>
<guid>https://arxiv.org/abs/2407.20271</guid>
<content:encoded><![CDATA[
arXiv:2407.20271v5 Announce Type: replace 
Abstract: Recent advances in machine learning, particularly in Natural Language Processing (NLP), have produced powerful models trained on vast datasets. However, these models risk leaking sensitive information, raising privacy concerns. In response, regulatory measures such as the European Union's General Data Protection Regulation (GDPR) have driven increasing interest in Machine Unlearning techniques, which enable models to selectively forget specific data entries. Early unlearning approaches primarily relied on pre-processing methods, while more recent research has shifted towards training-based solutions. Despite their effectiveness, a key limitation persists: most methods require access to original training data, which is often unavailable. Additionally, directly applying unlearning techniques bears the cost of undermining the model's expressive capabilities. To address these challenges, we introduce the Iterative Contrastive Unlearning (ICU) framework, which consists of three core components: A Knowledge Unlearning Induction module designed to target specific knowledge for removal using an unlearning loss; A Contrastive Learning Enhancement module to preserve the model's expressive capabilities against the pure unlearning goal; And an Iterative Unlearning Refinement module that dynamically adjusts the unlearning process through ongoing evaluation and updates. Experimental results demonstrate the efficacy of our ICU method in unlearning sensitive information while maintaining the model's overall performance, offering a promising solution for privacy-conscious machine learning applications.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Top K Enhanced Reinforcement Learning Attacks on Heterogeneous Graph Node Classification</title>
<link>https://arxiv.org/abs/2408.01964</link>
<guid>https://arxiv.org/abs/2408.01964</guid>
<content:encoded><![CDATA[
arXiv:2408.01964v2 Announce Type: replace 
Abstract: Graph Neural Networks (GNNs) have attracted substantial interest due to their exceptional performance on graph-based data. However, their robustness, especially on heterogeneous graphs, remains underexplored, particularly against adversarial attacks. This paper proposes HeteroKRLAttack, a targeted evasion black-box attack method for heterogeneous graphs. By integrating reinforcement learning with a Top-K algorithm to reduce the action space, our method efficiently identifies effective attack strategies to disrupt node classification tasks. We validate the effectiveness of HeteroKRLAttack through experiments on multiple heterogeneous graph datasets, showing significant reductions in classification accuracy compared to baseline methods. An ablation study underscores the critical role of the Top-K algorithm in enhancing attack performance. Our findings highlight potential vulnerabilities in current models and provide guidance for future defense strategies against adversarial attacks on heterogeneous graphs.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Hypergraph Learning with Local Differential Privacy: Toward Privacy-Aware Hypergraph Structure Completion</title>
<link>https://arxiv.org/abs/2408.05160</link>
<guid>https://arxiv.org/abs/2408.05160</guid>
<content:encoded><![CDATA[
arXiv:2408.05160v3 Announce Type: replace 
Abstract: The rapid growth of graph-structured data necessitates partitioning and distributed storage across decentralized systems, driving the emergence of federated graph learning to collaboratively train Graph Neural Networks (GNNs) without compromising privacy. However, current methods exhibit limited performance when handling hypergraphs, which inherently represent complex high-order relationships beyond pairwise connections. Partitioning hypergraph structures across federated subsystems amplifies structural complexity, hindering high-order information mining and compromising local information integrity. To bridge the gap between hypergraph learning and federated systems, we develop FedHGL, a first-of-its-kind framework for federated hypergraph learning on disjoint and privacy-constrained hypergraph partitions. Beyond collaboratively training a comprehensive hypergraph neural network across multiple clients, FedHGL introduces a pre-propagation hyperedge completion mechanism to preserve high-order structural integrity within each client. This procedure leverages the federated central server to perform cross-client hypergraph convolution without exposing internal topological information, effectively mitigating the high-order information loss induced by subgraph partitioning. Furthermore, by incorporating two kinds of local differential privacy (LDP) mechanisms, we provide formal privacy guarantees for this process, ensuring that sensitive node features remain protected against inference attacks from potentially malicious servers or clients. Experimental results on seven real-world datasets confirm the effectiveness of our approach and demonstrate its performance advantages over traditional federated graph learning methods.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Quality Monitoring for the Hadron Calorimeters Using Transfer Learning for Anomaly Detection</title>
<link>https://arxiv.org/abs/2408.16612</link>
<guid>https://arxiv.org/abs/2408.16612</guid>
<content:encoded><![CDATA[
arXiv:2408.16612v2 Announce Type: replace 
Abstract: The proliferation of sensors brings an immense volume of spatio-temporal (ST) data in many domains, including monitoring, diagnostics, and prognostics applications. Data curation is a time-consuming process for a large volume of data, making it challenging and expensive to deploy data analytics platforms in new environments. Transfer learning (TL) mechanisms promise to mitigate data sparsity and model complexity by utilizing pre-trained models for a new task. Despite the triumph of TL in fields like computer vision and natural language processing, efforts on complex ST models for anomaly detection (AD) applications are limited. In this study, we present the potential of TL within the context of high-dimensional ST AD with a hybrid autoencoder architecture, incorporating convolutional, graph, and recurrent neural networks. Motivated by the need for improved model accuracy and robustness, particularly in scenarios with limited training data on systems with thousands of sensors, this research investigates the transferability of models trained on different sections of the Hadron Calorimeter of the Compact Muon Solenoid experiment at CERN. The key contributions of the study include exploring TL's potential and limitations within the context of encoder and decoder networks, revealing insights into model initialization and training configurations that enhance performance while substantially reducing trainable parameters and mitigating data contamination effects. Code: \href{https://github.com/muleina/CMS_HCAL_ML_OnlineDQM}{https://github.com/muleina/CMS\_HCAL\_ML\_OnlineDQM}
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Role of Graph Topology in the Performance of Biomedical Knowledge Graph Completion Models</title>
<link>https://arxiv.org/abs/2409.04103</link>
<guid>https://arxiv.org/abs/2409.04103</guid>
<content:encoded><![CDATA[
arXiv:2409.04103v2 Announce Type: replace 
Abstract: Knowledge Graph Completion has been increasingly adopted as a useful method for helping address several tasks in biomedical research, such as drug repurposing or drug-target identification. To that end, a variety of datasets and Knowledge Graph Embedding models have been proposed over the years. However, little is known about the properties that render a dataset, and associated modelling choices, useful for a given task. Moreover, even though theoretical properties of Knowledge Graph Embedding models are well understood, their practical utility in this field remains controversial. In this work, we conduct a comprehensive investigation into the topological properties of publicly available biomedical Knowledge Graphs and establish links to the accuracy observed in real-world tasks. By releasing all model predictions and a new suite of analysis tools we invite the community to build upon our work and continue improving the understanding of these crucial applications.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3DS: Medical Domain Adaptation of LLMs via Decomposed Difficulty-based Data Selection</title>
<link>https://arxiv.org/abs/2410.10901</link>
<guid>https://arxiv.org/abs/2410.10901</guid>
<content:encoded><![CDATA[
arXiv:2410.10901v2 Announce Type: replace 
Abstract: Large Language Models(LLMs) excel in general tasks but struggle in specialized domains like healthcare due to limited domain-specific knowledge.Supervised Fine-Tuning(SFT) data construction for domain adaptation often relies on heuristic methods, such as GPT-4 annotation or manual data selection, with a data-centric focus on presumed diverse, high-quality datasets. However, these methods overlook the model's inherent knowledge distribution, introducing noise, redundancy, and irrelevant data, leading to a mismatch between the selected data and the model's learning task, resulting in suboptimal performance. To address this, we propose a two-stage model-centric data selection framework, Decomposed Difficulty Data Selection (3DS), which aligns data with the model's knowledge distribution for optimized adaptation. In Stage1, we apply Prompt-Driven Data Selection via Explicit Alignment, where the the model filters irrelevant or redundant data based on its internal knowledge. In Stage2, we perform Decomposed Difficulty Data Selection, where data selection is guided by our defined difficulty decomposition, using three metrics: Instruction Understanding, Response Confidence, and Response Correctness. Additionally, an attention-based importance weighting mechanism captures token importance for more accurate difficulty calibration. This two-stage approach ensures the selected data is not only aligned with the model's knowledge and preferences but also appropriately challenging for the model to learn, leading to more effective and targeted domain adaptation. In the case study of the medical domain, our extensive experiments on real-world healthcare datasets demonstrate the superiority of 3DS over exisiting methods in accuracy by over 5.29%. Our dataset and code has been open-sourced at https://github.com/PuppyKnightUniversity/3DS.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaTrading: An Immersion-Aware Model Trading Framework for Vehicular Metaverse Services</title>
<link>https://arxiv.org/abs/2410.19665</link>
<guid>https://arxiv.org/abs/2410.19665</guid>
<content:encoded><![CDATA[
arXiv:2410.19665v3 Announce Type: replace 
Abstract: Timely updating of Internet of Things data is crucial for achieving immersion in vehicular metaverse services. However, challenges such as latency caused by massive data transmissions, privacy risks associated with user data, and computational burdens on metaverse service providers (MSPs) hinder the continuous collection of high-quality data. To address these challenges, we propose an immersion-aware model trading framework that enables efficient and privacy-preserving data provisioning through federated learning (FL). Specifically, we first develop a novel multi-dimensional evaluation metric for the immersion of models (IoM). The metric considers the freshness and accuracy of the local model, and the amount and potential value of raw training data. Building on the IoM, we design an incentive mechanism to encourage metaverse users (MUs) to participate in FL by providing local updates to MSPs under resource constraints. The trading interactions between MSPs and MUs are modeled as an equilibrium problem with equilibrium constraints (EPEC) to analyze and balance their costs and gains, where MSPs as leaders determine rewards, while MUs as followers optimize resource allocation. To ensure privacy and adapt to dynamic network conditions, we develop a distributed dynamic reward algorithm based on deep reinforcement learning, without acquiring any private information from MUs and other MSPs. Experimental results show that the proposed framework outperforms state-of-the-art benchmarks, achieving improvements in IoM of 38.3% and 37.2%, and reductions in training time to reach the target accuracy of 43.5% and 49.8%, on average, for the MNIST and GTSRB datasets, respectively. These findings validate the effectiveness of our approach in incentivizing MUs to contribute high-value local models to MSPs, providing a flexible and adaptive scheme for data provisioning in vehicular metaverse services.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval-Retro: Retrieval-based Inorganic Retrosynthesis with Expert Knowledge</title>
<link>https://arxiv.org/abs/2410.21341</link>
<guid>https://arxiv.org/abs/2410.21341</guid>
<content:encoded><![CDATA[
arXiv:2410.21341v2 Announce Type: replace 
Abstract: While inorganic retrosynthesis planning is essential in the field of chemical science, the application of machine learning in this area has been notably less explored compared to organic retrosynthesis planning. In this paper, we propose Retrieval-Retro for inorganic retrosynthesis planning, which implicitly extracts the precursor information of reference materials that are retrieved from the knowledge base regarding domain expertise in the field. Specifically, instead of directly employing the precursor information of reference materials, we propose implicitly extracting it with various attention layers, which enables the model to learn novel synthesis recipes more effectively. Moreover, during retrieval, we consider the thermodynamic relationship between target material and precursors, which is essential domain expertise in identifying the most probable precursor set among various options. Extensive experiments demonstrate the superiority of Retrieval-Retro in retrosynthesis planning, especially in discovering novel synthesis recipes, which is crucial for materials discovery. The source code for Retrieval-Retro is available at https://github.com/HeewoongNoh/Retrieval-Retro.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>General Geospatial Inference with a Population Dynamics Foundation Model</title>
<link>https://arxiv.org/abs/2411.07207</link>
<guid>https://arxiv.org/abs/2411.07207</guid>
<content:encoded><![CDATA[
arXiv:2411.07207v5 Announce Type: replace 
Abstract: Supporting the health and well-being of dynamic populations around the world requires governmental agencies, organizations and researchers to understand and reason over complex relationships between human behavior and local contexts in order to identify high-risk groups and strategically allocate limited resources. Traditional approaches to these classes of problems often entail developing manually curated, task-specific features and models to represent human behavior and the natural and built environment, which can be challenging to adapt to new, or even, related tasks. To address this, we introduce a Population Dynamics Foundation Model (PDFM) that aims to capture the relationships between diverse data modalities and is applicable to a broad range of geospatial tasks. We first construct a geo-indexed dataset for postal codes and counties across the United States, capturing rich aggregated information on human behavior from maps, busyness, and aggregated search trends, and environmental factors such as weather and air quality. We then model this data and the complex relationships between locations using a graph neural network, producing embeddings that can be adapted to a wide range of downstream tasks using relatively simple models. We evaluate the effectiveness of our approach by benchmarking it on 27 downstream tasks spanning three distinct domains: health indicators, socioeconomic factors, and environmental measurements. The approach achieves state-of-the-art performance on all 27 geospatial interpolation tasks, and on 25 out of the 27 extrapolation and super-resolution tasks. We combined the PDFM with a state-of-the-art forecasting foundation model, TimesFM, to predict unemployment and poverty, achieving performance that surpasses fully supervised forecasting. The full set of embeddings and sample code are publicly available for researchers.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Reinforcement Learning under Diffusion Models for Data with Jumps</title>
<link>https://arxiv.org/abs/2411.11697</link>
<guid>https://arxiv.org/abs/2411.11697</guid>
<content:encoded><![CDATA[
arXiv:2411.11697v2 Announce Type: replace 
Abstract: Reinforcement Learning (RL) has proven effective in solving complex decision-making tasks across various domains, but challenges remain in continuous-time settings, particularly when state dynamics are governed by stochastic differential equations (SDEs) with jump components. In this paper, we address this challenge by introducing the Mean-Square Bipower Variation Error (MSBVE) algorithm, which enhances robustness and convergence in scenarios involving significant stochastic noise and jumps. We first revisit the Mean-Square TD Error (MSTDE) algorithm, commonly used in continuous-time RL, and highlight its limitations in handling jumps in state dynamics. The proposed MSBVE algorithm minimizes the mean-square quadratic variation error, offering improved performance over MSTDE in environments characterized by SDEs with jumps. Simulations and formal proofs demonstrate that the MSBVE algorithm reliably estimates the value function in complex settings, surpassing MSTDE's performance when faced with jump processes. These findings underscore the importance of alternative error metrics to improve the resilience and effectiveness of RL algorithms in continuous-time frameworks.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Communication-Efficient and Privacy-Adaptable Mechanism for Federated Learning</title>
<link>https://arxiv.org/abs/2501.12046</link>
<guid>https://arxiv.org/abs/2501.12046</guid>
<content:encoded><![CDATA[
arXiv:2501.12046v2 Announce Type: replace 
Abstract: Training machine learning models on decentralized private data via federated learning (FL) poses two key challenges: communication efficiency and privacy protection. In this work, we address these challenges within the trusted aggregator model by introducing a novel approach called the Communication-Efficient and Privacy-Adaptable Mechanism (CEPAM), achieving both objectives simultaneously. In particular, CEPAM leverages the rejection-sampled universal quantizer (RSUQ), a construction of randomized vector quantizer whose resulting distortion is equivalent to a prescribed noise, such as Gaussian or Laplace noise, enabling joint differential privacy and compression. Our CEPAM provides the additional benefit of privacy adaptability, allowing clients and the server to customize privacy protection based on required accuracy and protection. We theoretically analyze the privacy guarantee of CEPAM and investigate the trade-offs among user privacy and accuracy of CEPAM through experimental evaluations. Moreover, we assess CEPAM's utility performance using MNIST dataset, demonstrating that CEPAM surpasses baseline models in terms of learning accuracy.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced Physics-Informed Neural Network with Residuals for Solving Complex Integral Equations</title>
<link>https://arxiv.org/abs/2501.16370</link>
<guid>https://arxiv.org/abs/2501.16370</guid>
<content:encoded><![CDATA[
arXiv:2501.16370v3 Announce Type: replace 
Abstract: In this paper, we present the Residual Integral Solver Network (RISN), a novel neural network architecture designed to solve a wide range of integral and integro-differential equations, including one-dimensional, multi-dimensional, ordinary and partial integro-differential, systems, fractional types, and Helmholtz-type integral equations involving oscillatory kernels. RISN integrates residual connections with high-accuracy numerical methods such as Gaussian quadrature and fractional derivative operational matrices, enabling it to achieve higher accuracy and stability than traditional Physics-Informed Neural Networks (PINN). The residual connections help mitigate vanishing gradient issues, allowing RISN to handle deeper networks and more complex kernels, particularly in multi-dimensional problems. Through extensive experiments, we demonstrate that RISN consistently outperforms not only classical PINNs but also advanced variants such as Auxiliary PINN (A-PINN) and Self-Adaptive PINN (SA-PINN), achieving significantly lower Mean Absolute Errors (MAE) across various types of equations. These results highlight RISN's robustness and efficiency in solving challenging integral and integro-differential problems, making it a valuable tool for real-world applications where traditional methods often struggle.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Ensemble Kalman Update is an Empirical Matheron Update</title>
<link>https://arxiv.org/abs/2502.03048</link>
<guid>https://arxiv.org/abs/2502.03048</guid>
<content:encoded><![CDATA[
arXiv:2502.03048v4 Announce Type: replace 
Abstract: The Ensemble Kalman Filter (EnKF) is a widely used method for data assimilation in high-dimensional systems, with an ensemble update step equivalent to an empirical version of the Matheron update popular in Gaussian process regression -- a connection that links half a century of data-assimilation engineering to modern path-wise GP sampling.
  This paper provides a compact introduction to this simple but under-exploited connection, with necessary definitions accessible to all fields involved.
  Source code is available at https://github.com/danmackinlay/paper_matheron_equals_enkf .
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Superpose Task-specific Features for Model Merging</title>
<link>https://arxiv.org/abs/2502.10698</link>
<guid>https://arxiv.org/abs/2502.10698</guid>
<content:encoded><![CDATA[
arXiv:2502.10698v2 Announce Type: replace 
Abstract: Model merging enables powerful capabilities in neural networks without requiring additional training. In this paper, we introduce a novel perspective on model merging by leveraging the fundamental mechanisms of neural network representation. Our approach is motivated by the linear representation hypothesis, which states that neural networks encode information through linear combinations of feature vectors. We propose a method that superposes task-specific features from individual models into a merged model. Our approach specifically targets linear transformation matrices, which are crucial for feature activation and extraction in deep networks. By formulating the merging process as a linear system, we can preserve task-specific features from individual models and create merged models that effectively maintain multi-task capabilities compared to existing methods. Extensive experiments across diverse benchmarks and models demonstrate that our method outperforms existing techniques. Code is available at https://github.com/LARS-research/STF.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning the symmetric group: large from small</title>
<link>https://arxiv.org/abs/2502.12717</link>
<guid>https://arxiv.org/abs/2502.12717</guid>
<content:encoded><![CDATA[
arXiv:2502.12717v2 Announce Type: replace 
Abstract: Machine learning explorations can make significant inroads into solving difficult problems in pure mathematics. One advantage of this approach is that mathematical datasets do not suffer from noise, but a challenge is the amount of data required to train these models and that this data can be computationally expensive to generate. Key challenges further comprise difficulty in a posteriori interpretation of statistical models and the implementation of deep and abstract mathematical problems.
  We propose a method for scalable tasks, by which models trained on simpler versions of a task can then generalize to the full task. Specifically, we demonstrate that a transformer neural-network trained on predicting permutations from words formed by general transpositions in the symmetric group $S_{10}$ can generalize to the symmetric group $S_{25}$ with near 100\% accuracy. We also show that $S_{10}$ generalizes to $S_{16}$ with similar performance if we only use adjacent transpositions. We employ identity augmentation as a key tool to manage variable word lengths, and partitioned windows for training on adjacent transpositions. Finally we compare variations of the method used and discuss potential challenges with extending the method to other tasks.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Birds look like cars: Adversarial analysis of intrinsically interpretable deep learning</title>
<link>https://arxiv.org/abs/2503.08636</link>
<guid>https://arxiv.org/abs/2503.08636</guid>
<content:encoded><![CDATA[
arXiv:2503.08636v2 Announce Type: replace 
Abstract: A common belief is that intrinsically interpretable deep learning models ensure a correct, intuitive understanding of their behavior and offer greater robustness against accidental errors or intentional manipulation. However, these beliefs have not been comprehensively verified, and growing evidence casts doubt on them. In this paper, we highlight the risks related to overreliance and susceptibility to adversarial manipulation of these so-called "intrinsically (aka inherently) interpretable" models by design. We introduce two strategies for adversarial analysis with prototype manipulation and backdoor attacks against prototype-based networks, and discuss how concept bottleneck models defend against these attacks. Fooling the model's reasoning by exploiting its use of latent prototypes manifests the inherent uninterpretability of deep neural networks, leading to a false sense of security reinforced by a visual confirmation bias. The reported limitations of part-prototype networks put their trustworthiness and applicability into question, motivating further work on the robustness and alignment of (deep) interpretable models.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning Agents Trained For Avoidance Behave Like Hawks And Doves</title>
<link>https://arxiv.org/abs/2503.11452</link>
<guid>https://arxiv.org/abs/2503.11452</guid>
<content:encoded><![CDATA[
arXiv:2503.11452v2 Announce Type: replace 
Abstract: We present heuristically optimal strategies expressed by deep learning agents playing a simple avoidance game. We analyse the learning and behaviour of two agents within a symmetrical grid world that must cross paths to reach a target destination without crashing into each other or straying off of the grid world in the wrong direction. The agent policy is determined by one neural network that is employed in both agents. Our findings indicate that the fully trained network exhibits behaviour similar to that of the game Hawks and Doves, in that one agent employs an aggressive strategy to reach the target while the other learns how to avoid the aggressive agent.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot LLMs in Human-in-the-Loop RL: Replacing Human Feedback for Reward Shaping</title>
<link>https://arxiv.org/abs/2503.22723</link>
<guid>https://arxiv.org/abs/2503.22723</guid>
<content:encoded><![CDATA[
arXiv:2503.22723v2 Announce Type: replace 
Abstract: Reinforcement learning (RL) often struggles with reward misalignment, where agents optimize given rewards but fail to exhibit the desired behaviors. This arises when the reward function incentivizes proxy behaviors misaligned with the true objective. While human-in-the-loop (HITL) methods can mitigate this issue, they also introduce biases, leading to inconsistent and subjective feedback that complicates learning. To address these challenges, we propose two key contributions. First, we extend the use of zero-shot, off-the-shelf large language models (LLMs) for reward shaping beyond natural language processing (NLP) to continuous control tasks. Using LLMs as direct feedback providers eliminates the need for surrogate models trained on human feedback, which often inherit biases from training data. Second, we introduce a hybrid framework (LLM-HFBF) that enables LLMs to identify and correct biases in human feedback while incorporating this feedback into the reward shaping process. The LLM-HFBF framework creates a more balanced and reliable system by addressing both the limitations of LLMs (e.g., lack of domain-specific knowledge) and human supervision (e.g., inherent biases). By enabling human feedback bias flagging and correction, our approach improves reinforcement learning performance and reduces reliance on potentially biased human feedback. Empirical experiments show that biased human feedback significantly reduces performance, with Average Episodic Reward dropping by nearly 94% compared to unbiased approaches. In contrast, LLM-based methods sustain performance at a similar level to unbiased feedback, even in challenging edge-case scenarios.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modular Machine Learning: An Indispensable Path towards New-Generation Large Language Models</title>
<link>https://arxiv.org/abs/2504.20020</link>
<guid>https://arxiv.org/abs/2504.20020</guid>
<content:encoded><![CDATA[
arXiv:2504.20020v2 Announce Type: replace 
Abstract: Large language models (LLMs) have substantially advanced machine learning research, including natural language processing, computer vision, data mining, etc., yet they still exhibit critical limitations in explainability, reliability, adaptability, and extensibility. In this paper, we overview a promising learning paradigm, i.e., Modular Machine Learning (MML), as an essential approach toward new-generation LLMs capable of addressing these issues. We begin by systematically and comprehensively surveying the existing literature on modular machine learning, with a particular focus on modular data representation and modular models. Then, we propose a unified MML framework for LLMs, which decomposes the complex structure of LLMs into three interdependent components: modular representation, modular model, and modular reasoning. Specifically, the MML paradigm discussed in this article is able to: i) clarify the internal working mechanism of LLMs through the disentanglement of semantic components; ii) allow for flexible and task-adaptive model design; iii) enable an interpretable and logic-driven decision-making process. We further elaborate a feasible implementation of MML-based LLMs via leveraging advanced techniques such as disentangled representation learning, neural architecture search and neuro-symbolic learning. Last but not least, we critically identify the remaining key challenges, such as the integration of continuous neural and discrete symbolic processes, joint optimization, and computational scalability, present promising future research directions that deserve further exploration. Ultimately, we believe the integration of the MML with LLMs has the potential to bridge the gap between statistical (deep) learning and formal (logical) reasoning, thereby paving the way for robust, adaptable, and trustworthy AI systems across a wide range of real-world applications.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Conservative Neural Control Barrier Functions from Offline Data</title>
<link>https://arxiv.org/abs/2505.00908</link>
<guid>https://arxiv.org/abs/2505.00908</guid>
<content:encoded><![CDATA[
arXiv:2505.00908v2 Announce Type: replace 
Abstract: Safety filters, particularly those based on control barrier functions, have gained increased interest as effective tools for safe control of dynamical systems. Existing correct-by-construction synthesis algorithms for such filters, however, suffer from the curse-of-dimensionality. Deep learning approaches have been proposed in recent years to address this challenge. In this paper, we add to this set of approaches an algorithm for training neural control barrier functions from offline datasets. Such functions can be used to design constraints for quadratic programs that are then used as safety filters. Our algorithm trains these functions so that the system is not only prevented from reaching unsafe states but is also disincentivized from reaching out-of-distribution ones, at which they would be less reliable. It is inspired by Conservative Q-learning, an offline reinforcement learning algorithm. We call its outputs Conservative Control Barrier Functions (CCBFs). Our empirical results demonstrate that CCBFs outperform existing methods in maintaining safety while minimally affecting task performance. Source code is available at https://github.com/tabz23/CCBF.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preference Isolation Forest for Structure-based Anomaly Detection</title>
<link>https://arxiv.org/abs/2505.10876</link>
<guid>https://arxiv.org/abs/2505.10876</guid>
<content:encoded><![CDATA[
arXiv:2505.10876v2 Announce Type: replace 
Abstract: We address the problem of detecting anomalies as samples that do not conform to structured patterns represented by low-dimensional manifolds. To this end, we conceive a general anomaly detection framework called Preference Isolation Forest (PIF), that combines the benefits of adaptive isolation-based methods with the flexibility of preference embedding. The key intuition is to embed the data into a high-dimensional preference space by fitting low-dimensional manifolds, and to identify anomalies as isolated points. We propose three isolation approaches to identify anomalies: $i$) Voronoi-iForest, the most general solution, $ii$) RuzHash-iForest, that avoids explicit computation of distances via Local Sensitive Hashing, and $iii$) Sliding-PIF, that leverages a locality prior to improve efficiency and effectiveness.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reconstructing Physics-Informed Machine Learning for Traffic Flow Modeling: a Multi-Gradient Descent and Pareto Learning Approach</title>
<link>https://arxiv.org/abs/2505.13241</link>
<guid>https://arxiv.org/abs/2505.13241</guid>
<content:encoded><![CDATA[
arXiv:2505.13241v3 Announce Type: replace 
Abstract: Physics-informed machine learning (PIML) is crucial in modern traffic flow modeling because it combines the benefits of both physics-based and data-driven approaches. In conventional PIML, physical information is typically incorporated by constructing a hybrid loss function that combines data-driven loss and physics loss through linear scalarization. The goal is to find a trade-off between these two objectives to improve the accuracy of model predictions. However, from a mathematical perspective, linear scalarization is limited to identifying only the convex region of the Pareto front, as it treats data-driven and physics losses as separate objectives. Given that most PIML loss functions are non-convex, linear scalarization restricts the achievable trade-off solutions. Moreover, tuning the weighting coefficients for the two loss components can be both time-consuming and computationally challenging. To address these limitations, this paper introduces a paradigm shift in PIML by reformulating the training process as a multi-objective optimization problem, treating data-driven loss and physics loss independently. We apply several multi-gradient descent algorithms (MGDAs), including traditional multi-gradient descent (TMGD) and dual cone gradient descent (DCGD), to explore the Pareto front in this multi-objective setting. These methods are evaluated on both macroscopic and microscopic traffic flow models. In the macroscopic case, MGDAs achieved comparable performance to traditional linear scalarization methods. Notably, in the microscopic case, MGDAs significantly outperformed their scalarization-based counterparts, demonstrating the advantages of a multi-objective optimization approach in complex PIML scenarios.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Binarized Neural Networks Converge Toward Algorithmic Simplicity: Empirical Support for the Learning-as-Compression Hypothesis</title>
<link>https://arxiv.org/abs/2505.20646</link>
<guid>https://arxiv.org/abs/2505.20646</guid>
<content:encoded><![CDATA[
arXiv:2505.20646v3 Announce Type: replace 
Abstract: Understanding and controlling the informational complexity of neural networks is a central challenge in machine learning, with implications for generalization, optimization, and model capacity. While most approaches rely on entropy-based loss functions and statistical metrics, these measures often fail to capture deeper, causally relevant algorithmic regularities embedded in network structure. We propose a shift toward algorithmic information theory, using Binarized Neural Networks (BNNs) as a first proxy. Grounded in algorithmic probability (AP) and the universal distribution it defines, our approach characterizes learning dynamics through a formal, causally grounded lens. We apply the Block Decomposition Method (BDM) -- a scalable approximation of algorithmic complexity based on AP -- and demonstrate that it more closely tracks structural changes during training than entropy, consistently exhibiting stronger correlations with training loss across varying model sizes and randomized training runs. These results support the view of training as a process of algorithmic compression, where learning corresponds to the progressive internalization of structured regularities. In doing so, our work offers a principled estimate of learning progression and suggests a framework for complexity-aware learning and regularization, grounded in first principles from information theory, complexity, and computability.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Supervised Learning Models for Fraud Detection: A Comparative Study of Classical and Deep Architectures on Imbalanced Transaction Data</title>
<link>https://arxiv.org/abs/2505.22521</link>
<guid>https://arxiv.org/abs/2505.22521</guid>
<content:encoded><![CDATA[
arXiv:2505.22521v2 Announce Type: replace 
Abstract: Fraud detection remains a critical task in high-stakes domains such as finance and e-commerce, where undetected fraudulent transactions can lead to significant economic losses. In this study, we systematically compare the performance of four supervised learning models - Logistic Regression, Random Forest, Light Gradient Boosting Machine (LightGBM), and a Gated Recurrent Unit (GRU) network - on a large-scale, highly imbalanced online transaction dataset. While ensemble methods such as Random Forest and LightGBM demonstrated superior performance in both overall and class-specific metrics, Logistic Regression offered a reliable and interpretable baseline. The GRU model showed strong recall for the minority fraud class, though at the cost of precision, highlighting a trade-off relevant for real-world deployment. Our evaluation emphasizes not only weighted averages but also per-class precision, recall, and F1-scores, providing a nuanced view of each model's effectiveness in detecting rare but consequential fraudulent activity. The findings underscore the importance of choosing models based on the specific risk tolerance and operational needs of fraud detection systems.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Empirical Study of Federated Prompt Learning for Vision Language Model</title>
<link>https://arxiv.org/abs/2505.23024</link>
<guid>https://arxiv.org/abs/2505.23024</guid>
<content:encoded><![CDATA[
arXiv:2505.23024v2 Announce Type: replace 
Abstract: The Vision Language Model (VLM) excels in aligning vision and language representations, and prompt learning has emerged as a key technique for adapting such models to downstream tasks. However, the application of prompt learning with VLM in federated learning (FL) scenarios remains underexplored. This paper systematically investigates the behavioral differences between language prompt learning (LPT) and vision prompt learning (VPT) under data heterogeneity challenges, including label skew and domain shift. We conduct extensive experiments to evaluate the impact of various FL and prompt configurations, such as client scale, aggregation strategies, and prompt length, to assess the robustness of Federated Prompt Learning (FPL). Furthermore, we explore strategies for enhancing prompt learning in complex scenarios where label skew and domain shift coexist, including leveraging both prompt types when computational resources allow. Our findings offer practical insights into optimizing prompt learning in federated settings, contributing to the broader deployment of VLMs in privacy-preserving environments.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>carps: A Framework for Comparing N Hyperparameter Optimizers on M Benchmarks</title>
<link>https://arxiv.org/abs/2506.06143</link>
<guid>https://arxiv.org/abs/2506.06143</guid>
<content:encoded><![CDATA[
arXiv:2506.06143v2 Announce Type: replace 
Abstract: Hyperparameter Optimization (HPO) is crucial to develop well-performing machine learning models. In order to ease prototyping and benchmarking of HPO methods, we propose carps, a benchmark framework for Comprehensive Automated Research Performance Studies allowing to evaluate N optimizers on M benchmark tasks. In this first release of carps, we focus on the four most important types of HPO task types: blackbox, multi-fidelity, multi-objective and multi-fidelity-multi-objective. With 3 336 tasks from 5 community benchmark collections and 28 variants of 9 optimizer families, we offer the biggest go-to library to date to evaluate and compare HPO methods. The carps framework relies on a purpose-built, lightweight interface, gluing together optimizers and benchmark tasks. It also features an analysis pipeline, facilitating the evaluation of optimizers on benchmarks. However, navigating a huge number of tasks while developing and comparing methods can be computationally infeasible. To address this, we obtain a subset of representative tasks by minimizing the star discrepancy of the subset, in the space spanned by the full set. As a result, we propose an initial subset of 10 to 30 diverse tasks for each task type, and include functionality to re-compute subsets as more benchmarks become available, enabling efficient evaluations. We also establish a first set of baseline results on these tasks as a measure for future comparisons. With carps (https://www.github.com/automl/CARP-S), we make an important step in the standardization of HPO evaluation.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Adapting Language Models</title>
<link>https://arxiv.org/abs/2506.10943</link>
<guid>https://arxiv.org/abs/2506.10943</guid>
<content:encoded><![CDATA[
arXiv:2506.10943v2 Announce Type: replace 
Abstract: Large language models (LLMs) are powerful but static; they lack mechanisms to adapt their weights in response to new tasks, knowledge, or examples. We introduce Self-Adapting LLMs (SEAL), a framework that enables LLMs to self-adapt by generating their own finetuning data and update directives. Given a new input, the model produces a self-edit-a generation that may restructure the information in different ways, specify optimization hyperparameters, or invoke tools for data augmentation and gradient-based updates. Through supervised finetuning (SFT), these self-edits result in persistent weight updates, enabling lasting adaptation. To train the model to produce effective self-edits, we use a reinforcement learning loop with the downstream performance of the updated model as the reward signal. Unlike prior approaches that rely on separate adaptation modules or auxiliary networks, SEAL directly uses the model's own generation to control its adaptation process. Experiments on knowledge incorporation and few-shot generalization show that SEAL is a promising step toward language models capable of self-directed adaptation. Our website and code is available at https://jyopari.github.io/posts/seal.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Explainable AI Framework for Dynamic Resource Management in Vehicular Network Slicing</title>
<link>https://arxiv.org/abs/2506.11882</link>
<guid>https://arxiv.org/abs/2506.11882</guid>
<content:encoded><![CDATA[
arXiv:2506.11882v2 Announce Type: replace 
Abstract: Effective resource management and network slicing are essential to meet the diverse service demands of vehicular networks, including Enhanced Mobile Broadband (eMBB) and Ultra-Reliable and Low-Latency Communications (URLLC). This paper introduces an Explainable Deep Reinforcement Learning (XRL) framework for dynamic network slicing and resource allocation in vehicular networks, built upon a near-real-time RAN intelligent controller. By integrating a feature-based approach that leverages Shapley values and an attention mechanism, we interpret and refine the decisions of our reinforcementlearning agents, addressing key reliability challenges in vehicular communication systems. Simulation results demonstrate that our approach provides clear, real-time insights into the resource allocation process and achieves higher interpretability precision than a pure attention mechanism. Furthermore, the Quality of Service (QoS) satisfaction for URLLC services increased from 78.0% to 80.13%, while that for eMBB services improved from 71.44% to 73.21%.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Test-Function Approach to Incremental Stability</title>
<link>https://arxiv.org/abs/2507.00695</link>
<guid>https://arxiv.org/abs/2507.00695</guid>
<content:encoded><![CDATA[
arXiv:2507.00695v2 Announce Type: replace 
Abstract: This paper presents a novel framework for analyzing Incremental-Input-to-State Stability ($\delta$ISS) based on the idea of using rewards as "test functions." Whereas control theory traditionally deals with Lyapunov functions that satisfy a time-decrease condition, reinforcement learning (RL) value functions are constructed by exponentially decaying a Lipschitz reward function that may be non-smooth and unbounded on both sides. Thus, these RL-style value functions cannot be directly understood as Lyapunov certificates. We develop a new equivalence between a variant of incremental input-to-state stability of a closed-loop system under given a policy, and the regularity of RL-style value functions under adversarial selection of a H\"older-continuous reward function. This result highlights that the regularity of value functions, and their connection to incremental stability, can be understood in a way that is distinct from the traditional Lyapunov-based approach to certifying stability in control theory.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Interconnect Learning in Boolean Networks</title>
<link>https://arxiv.org/abs/2507.02585</link>
<guid>https://arxiv.org/abs/2507.02585</guid>
<content:encoded><![CDATA[
arXiv:2507.02585v2 Announce Type: replace 
Abstract: Learned Differentiable Boolean Logic Networks (DBNs) already deliver efficient inference on resource-constrained hardware. We extend them with a trainable, differentiable interconnect whose parameter count remains constant as input width grows, allowing DBNs to scale to far wider layers than earlier learnable-interconnect designs while preserving their advantageous accuracy. To further reduce model size, we propose two complementary pruning stages: an SAT-based logic equivalence pass that removes redundant gates without affecting performance, and a similarity-based, data-driven pass that outperforms a magnitude-style greedy baseline and offers a superior compression-accuracy trade-off.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Logic Networks for Interpretable Classification</title>
<link>https://arxiv.org/abs/2508.08172</link>
<guid>https://arxiv.org/abs/2508.08172</guid>
<content:encoded><![CDATA[
arXiv:2508.08172v3 Announce Type: replace 
Abstract: Traditional neural networks have an impressive classification performance, but what they learn cannot be inspected, verified or extracted. Neural Logic Networks on the other hand have an interpretable structure that enables them to learn a logical mechanism relating the inputs and outputs with AND and OR operations. We generalize these networks with NOT operations and biases that take into account unobserved data and develop a rigorous logical and probabilistic modeling in terms of concept combinations to motivate their use. We also propose a novel factorized IF-THEN rule structure for the model as well as a modified learning algorithm. Our method improves the state-of-the-art in Boolean networks discovery and is able to learn relevant, interpretable rules in tabular classification, notably on examples from the medical and industrial fields where interpretability has tangible value.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resource-Aware Aggregation and Sparsification in Heterogeneous Ensemble Federated Learning</title>
<link>https://arxiv.org/abs/2508.08552</link>
<guid>https://arxiv.org/abs/2508.08552</guid>
<content:encoded><![CDATA[
arXiv:2508.08552v2 Announce Type: replace 
Abstract: Federated learning (FL) enables distributed training with private client data, but its convergence is hindered by system heterogeneity under realistic communication scenarios. Most FL schemes addressing system heterogeneity utilize global pruning or ensemble distillation, yet often overlook typical constraints required for communication efficiency. Meanwhile, deep ensembles can aggregate predictions from individually trained models to improve performance, but current ensemble-based FL methods fall short in fully capturing diversity of model predictions. In this work, we propose \textbf{SHEFL}, a global ensemble-based FL framework suited for clients with diverse computational capacities. We allocate different numbers of global models to clients based on their available resources. We introduce a novel aggregation scheme that mitigates the training bias between clients and dynamically adjusts the sparsification ratio across clients to reduce the computational burden of training deep ensembles. Extensive experiments demonstrate that our method effectively addresses computational heterogeneity, significantly improving accuracy and stability compared to existing approaches.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mini-Batch Robustness Verification of Deep Neural Networks</title>
<link>https://arxiv.org/abs/2508.15454</link>
<guid>https://arxiv.org/abs/2508.15454</guid>
<content:encoded><![CDATA[
arXiv:2508.15454v2 Announce Type: replace 
Abstract: Neural network image classifiers are ubiquitous in many safety-critical applications. However, they are susceptible to adversarial attacks. To understand their robustness to attacks, many local robustness verifiers have been proposed to analyze $\epsilon$-balls of inputs. Yet, existing verifiers introduce a long analysis time or lose too much precision, making them less effective for a large set of inputs. In this work, we propose a new approach to local robustness: group local robustness verification. The key idea is to leverage the similarity of the network computations of certain $\epsilon$-balls to reduce the overall analysis time. We propose BaVerLy, a sound and complete verifier that boosts the local robustness verification of a set of $\epsilon$-balls by dynamically constructing and verifying mini-batches. BaVerLy adaptively identifies successful mini-batch sizes, accordingly constructs mini-batches of $\epsilon$-balls that have similar network computations, and verifies them jointly. If a mini-batch is verified, all its $\epsilon$-balls are proven robust. Otherwise, one $\epsilon$-ball is suspected as not being robust, guiding the refinement. BaVerLy leverages the analysis results to expedite the analysis of that $\epsilon$-ball as well as the analysis of the mini-batch with the other $\epsilon$-balls. We evaluate BaVerLy on fully connected and convolutional networks for MNIST and CIFAR-10. Results show that BaVerLy scales the common one by one verification by 2.3x on average and up to 4.1x, in which case it reduces the total analysis time from 24 hours to 6 hours.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Let's Grow an Unbiased Community: Guiding the Fairness of Graphs via New Links</title>
<link>https://arxiv.org/abs/2508.15499</link>
<guid>https://arxiv.org/abs/2508.15499</guid>
<content:encoded><![CDATA[
arXiv:2508.15499v2 Announce Type: replace 
Abstract: Graph Neural Networks (GNNs) have achieved remarkable success across diverse applications. However, due to the biases in the graph structures, graph neural networks face significant challenges in fairness. Although the original user graph structure is generally biased, it is promising to guide these existing structures toward unbiased ones by introducing new links. The fairness guidance via new links could foster unbiased communities, thereby enhancing fairness in downstream applications. To address this issue, we propose a novel framework named FairGuide. Specifically, to ensure fairness in downstream tasks trained on fairness-guided graphs, we introduce a differentiable community detection task as a pseudo downstream task. Our theoretical analysis further demonstrates that optimizing fairness within this pseudo task effectively enhances structural fairness, promoting fairness generalization across diverse downstream applications. Moreover, FairGuide employs an effective strategy which leverages meta-gradients derived from the fairness-guidance objective to identify new links that significantly enhance structural fairness. Extensive experimental results demonstrate the effectiveness and generalizability of our proposed method across a variety of graph-based fairness tasks.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CbLDM: A Diffusion Model for recovering nanostructure from pair distribution function</title>
<link>https://arxiv.org/abs/2509.01370</link>
<guid>https://arxiv.org/abs/2509.01370</guid>
<content:encoded><![CDATA[
arXiv:2509.01370v2 Announce Type: replace 
Abstract: Nowadays, the nanostructure inverse problem is an attractive problem that helps researchers to understand the relationship between the properties and the structure of nanomaterials. This article focuses on the problem of using PDF to recover the nanostructure, which this article views as a conditional generation problem. This article propose a deep learning model CbLDM, Condition-based Latent Diffusion Model. Based on the original latent diffusion model, the sampling steps of the diffusion model are reduced and the sample generation efficiency is improved by using the conditional prior to estimate conditional posterior distribution, which is the approximated distribution of p(z|x). In addition, this article uses the Laplacian matrix instead of the distance matrix to recover the nanostructure, which can reduce the reconstruction error. Finally, this article compares CbLDM with existing models which were used to solve the nanostructure inverse problem, and find that CbLDM demonstrates significantly higher prediction accuracy than these models, which reflects the ability of CbLDM to solve the nanostructure inverse problem and the potential to cope with other continuous conditional generation tasks.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AR-KAN: Autoregressive-Weight-Enhanced Kolmogorov-Arnold Network for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2509.02967</link>
<guid>https://arxiv.org/abs/2509.02967</guid>
<content:encoded><![CDATA[
arXiv:2509.02967v2 Announce Type: replace 
Abstract: Traditional neural networks struggle to capture the spectral structure of complex signals. Fourier neural networks (FNNs) attempt to address this by embedding Fourier series components, yet many real-world signals are almost-periodic with non-commensurate frequencies, posing additional challenges. Building on prior work showing that ARIMA outperforms large language models (LLMs) for forecasting, we extend the comparison to neural predictors and find ARIMA still superior. We therefore propose the Autoregressive-Weight-Enhanced Kolmogorov-Arnold Network (AR-KAN), which integrates a pre-trained AR module for temporal memory with a KAN for nonlinear representation. The AR module preserves essential temporal features while reducing redundancy. Experiments demonstrate that AR-KAN matches ARIMA on almost-periodic functions and achieves the best results on $72\%$ of Rdatasets series, with a clear advantage on data with periodic structure. These results highlight AR-KAN as a robust and effective framework for time series forecasting.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring the Measures: Discriminative Capacity of Representational Similarity Metrics Across Model Families</title>
<link>https://arxiv.org/abs/2509.04622</link>
<guid>https://arxiv.org/abs/2509.04622</guid>
<content:encoded><![CDATA[
arXiv:2509.04622v2 Announce Type: replace 
Abstract: Representational similarity metrics are fundamental tools in neuroscience and AI, yet we lack systematic comparisons of their discriminative power across model families. We introduce a quantitative framework to evaluate representational similarity measures based on their ability to separate model families-across architectures (CNNs, Vision Transformers, Swin Transformers, ConvNeXt) and training regimes (supervised vs. self-supervised). Using three complementary separability measures-dprime from signal detection theory, silhouette coefficients and ROC-AUC, we systematically assess the discriminative capacity of commonly used metrics including RSA, linear predictivity, Procrustes, and soft matching. We show that separability systematically increases as metrics impose more stringent alignment constraints. Among mapping-based approaches, soft-matching achieves the highest separability, followed by Procrustes alignment and linear predictivity. Non-fitting methods such as RSA also yield strong separability across families. These results provide the first systematic comparison of similarity metrics through a separability lens, clarifying their relative sensitivity and guiding metric choice for large-scale model and brain comparisons.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cloud-Edge Collaborative Data Anomaly Detection in Industrial Sensor Networks</title>
<link>https://arxiv.org/abs/2204.09942</link>
<guid>https://arxiv.org/abs/2204.09942</guid>
<content:encoded><![CDATA[
arXiv:2204.09942v2 Announce Type: replace-cross 
Abstract: Existing research on sensor data anomaly detection for industrial sensor networks still has several inherent limitations. First, most detection models usually consider centralized detection. Thus, all sensor data have to be uploaded to the control center for analysis, leading to a heavy traffic load. However, industrial sensor networks have high requirements for reliable and real-time communication. The heavy traffic load may cause communication delays or packets lost by corruption. Second, there are complex spatial and temporal features in industrial sensor data. The full extraction of such features plays a key role in improving detection performance.To solve the limitations above, this paper develops a cloud-edge collaborative data anomaly detection approach for industrial sensor networks that mainly consists of a sensor data detection model deployed at individual edges and a sensor data analysis model deployed in the cloud. The former is implemented using Gaussian and Bayesian algorithms, which effectively filter the substantial volume of sensor data generated during the normal operation of the industrial sensor network, thereby reducing traffic load. It only uploads all the sensor data to the sensor data analysis model for further analysis when the network is in an anomalous state. The latter based on GCRL is developed by inserting Long Short-Term Memory network (LSTM) into Graph Convolutional Network (GCN), which can effectively extract the spatial and temporal features of the sensor data for anomaly detection.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentially private multivariate medians</title>
<link>https://arxiv.org/abs/2210.06459</link>
<guid>https://arxiv.org/abs/2210.06459</guid>
<content:encoded><![CDATA[
arXiv:2210.06459v3 Announce Type: replace-cross 
Abstract: Statistical tools which satisfy rigorous privacy guarantees are necessary for modern data analysis. It is well-known that robustness against contamination is linked to differential privacy. Despite this fact, using multivariate medians for differentially private and robust multivariate location estimation has not been systematically studied. We develop novel finite-sample performance guarantees for differentially private multivariate depth-based medians, which are essentially sharp. Our results cover commonly used depth functions, such as the halfspace (or Tukey) depth, spatial depth, and the integrated dual depth. We show that under Cauchy marginals, the cost of heavy-tailed location estimation outweighs the cost of privacy. We demonstrate our results numerically using a Gaussian contamination model in dimensions up to $d = 100$, and compare them to a state-of-the-art private mean estimation algorithm. As a by-product of our investigation, we prove concentration inequalities for the output of the exponential mechanism about the maximizer of the population objective function. This bound applies to objective functions that satisfy a mild regularity condition.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heterogeneous Directed Hypergraph Neural Network over abstract syntax tree (AST) for Code Classification</title>
<link>https://arxiv.org/abs/2305.04228</link>
<guid>https://arxiv.org/abs/2305.04228</guid>
<content:encoded><![CDATA[
arXiv:2305.04228v5 Announce Type: replace-cross 
Abstract: Code classification is a difficult issue in program understanding and automatic coding. Due to the elusive syntax and complicated semantics in programs, most existing studies use techniques based on abstract syntax tree (AST) and graph neural networks (GNN) to create code representations for code classification. These techniques utilize the structure and semantic information of the code, but they only take into account pairwise associations and neglect the high-order data correlations that already exist between nodes of the same field or called attribute in the AST, which may result in the loss of code structural information. On the other hand, while a general hypergraph can encode high-order data correlations, it is homogeneous and undirected which will result in a lack of semantic and structural information such as node types, edge types, and directions between child nodes and parent nodes when modeling AST. In this study, we propose a heterogeneous directed hypergraph (HDHG) to represent AST and a heterogeneous directed hypergraph neural network (HDHGN) to process the graph for code classification. Our method improves code understanding and can represent high-order data correlations beyond paired interactions. We assess our heterogeneous directed hypergraph neural network (HDHGN) on public datasets of Python and Java programs. Our method outperforms previous AST-based and GNN-based methods, which demonstrates the capability of our model.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Multipole Attention: A Scalable Multilevel Attention Mechanism for Text and Images</title>
<link>https://arxiv.org/abs/2310.11960</link>
<guid>https://arxiv.org/abs/2310.11960</guid>
<content:encoded><![CDATA[
arXiv:2310.11960v4 Announce Type: replace-cross 
Abstract: While Transformer networks benefit from a global receptive field, their quadratic cost relative to sequence length restricts their application to long sequences and high-resolution inputs. We introduce Fast Multipole Attention (FMA), a divide-and-conquer mechanism for self-attention inspired by the Fast Multipole Method from n-body physics. FMA reduces the time and memory complexity of self-attention from $\mathcal{O}\left(n^2\right)$ to $\mathcal{O}(n \log n)$ and $\mathcal{O}(n)$ while preserving full-context interactions.
  FMA contains a learned hierarchy with $\mathcal{O}(\log n)$ levels of resolution. In this hierarchy, nearby tokens interact at full resolution, while distant tokens engage through progressively coarser, learned basis functions. We have developed both 1D and 2D implementations of FMA for language and vision tasks, respectively. On autoregressive and bidirectional language modeling benchmarks, the 1D variant either matches or outperforms leading efficient attention baselines with substantially lower memory use. With linear complexity, the 2D variant demonstrates superior performance over strong vision transformer baselines in classification and semantic segmentation tasks.
  Our results confirm that the multilevel attention implemented by FMA allows Transformer-based models to scale to much longer sequences and higher-resolution inputs without loss in accuracy. This provides a principled, physics-inspired approach for developing scalable neural networks suitable for language, vision, and multimodal tasks. Our code will be available at https://github.com/epoch98/FMA.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Utility Optimization via a GAN Approach</title>
<link>https://arxiv.org/abs/2403.15243</link>
<guid>https://arxiv.org/abs/2403.15243</guid>
<content:encoded><![CDATA[
arXiv:2403.15243v2 Announce Type: replace-cross 
Abstract: Robust utility optimization enables an investor to deal with market uncertainty in a structured way, with the goal of maximizing the worst-case outcome. In this work, we propose a generative adversarial network (GAN) approach to (approximately) solve robust utility optimization problems in general and realistic settings. In particular, we model both the investor and the market by neural networks (NN) and train them in a mini-max zero-sum game. This approach is applicable for any continuous utility function and in realistic market settings with trading costs, where only observable information of the market can be used. A large empirical study shows the versatile usability of our method. Whenever an optimal reference strategy is available, our method performs on par with it and in the (many) settings without known optimal strategy, our method outperforms all other reference strategies. Moreover, we can conclude from our study that the trained path-dependent strategies do not outperform Markovian ones. Lastly, we uncover that our generative approach for learning optimal, (non-) robust investments under trading costs generates universally applicable alternatives to well known asymptotic strategies of idealized settings.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Double Descent: Understanding Linear Model Estimation of Nonidentifiable Parameters and a Model for Overfitting</title>
<link>https://arxiv.org/abs/2408.13235</link>
<guid>https://arxiv.org/abs/2408.13235</guid>
<content:encoded><![CDATA[
arXiv:2408.13235v2 Announce Type: replace-cross 
Abstract: We consider ordinary least squares estimation and variations on least squares estimation such as penalized (regularized) least squares and spectral shrinkage estimates for problems with p > n and associated problems with prediction of new observations. After the introduction of Section 1, Section 2 examines a number of commonly used estimators for p > n. Section 3 introduces prediction with p > n. Section 4 introduces notational changes to facilitate discussion of overfitting and Section 5 illustrates the phenomenon of double descent. We conclude with some final comments.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixture of Multicenter Experts in Multimodal AI for Debiased Radiotherapy Target Delineation</title>
<link>https://arxiv.org/abs/2410.00046</link>
<guid>https://arxiv.org/abs/2410.00046</guid>
<content:encoded><![CDATA[
arXiv:2410.00046v3 Announce Type: replace-cross 
Abstract: Clinical decision-making reflects diverse strategies shaped by regional patient populations and institutional protocols. However, most existing medical artificial intelligence (AI) models are trained on highly prevalent data patterns, which reinforces biases and fails to capture the breadth of clinical expertise. Inspired by the recent advances in Mixture of Experts (MoE), we propose a Mixture of Multicenter Experts (MoME) framework to address AI bias in the medical domain without requiring data sharing across institutions. MoME integrates specialized expertise from diverse clinical strategies to enhance model generalizability and adaptability across medical centers. We validate this framework using a multimodal target volume delineation model for prostate cancer radiotherapy. With few-shot training that combines imaging and clinical notes from each center, the model outperformed baselines, particularly in settings with high inter-center variability or limited data availability. Furthermore, MoME enables model customization to local clinical preferences without cross-institutional data exchange, making it especially suitable for resource-constrained settings while promoting broadly generalizable medical AI.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reconstruction of Differentially Private Text Sanitization via Large Language Models</title>
<link>https://arxiv.org/abs/2410.12443</link>
<guid>https://arxiv.org/abs/2410.12443</guid>
<content:encoded><![CDATA[
arXiv:2410.12443v3 Announce Type: replace-cross 
Abstract: Differential privacy (DP) is the de facto privacy standard against privacy leakage attacks, including many recently discovered ones against large language models (LLMs). However, we discovered that LLMs could reconstruct the altered/removed privacy from given DP-sanitized prompts. We propose two attacks (black-box and white-box) based on the accessibility to LLMs and show that LLMs could connect the pair of DP-sanitized text and the corresponding private training data of LLMs by giving sample text pairs as instructions (in the black-box attacks) or fine-tuning data (in the white-box attacks). To illustrate our findings, we conduct comprehensive experiments on modern LLMs (e.g., LLaMA-2, LLaMA-3, ChatGPT-3.5, ChatGPT-4, ChatGPT-4o, Claude-3, Claude-3.5, OPT, GPT-Neo, GPT-J, Gemma-2, and Pythia) using commonly used datasets (such as WikiMIA, Pile-CC, and Pile-Wiki) against both word-level and sentence-level DP. The experimental results show promising recovery rates, e.g., the black-box attacks against the word-level DP over WikiMIA dataset gave 72.18% on LLaMA-2 (70B), 82.39% on LLaMA-3 (70B), 75.35% on Gemma-2, 91.2% on ChatGPT-4o, and 94.01% on Claude-3.5 (Sonnet). More urgently, this study indicates that these well-known LLMs have emerged as a new security risk for existing DP text sanitization approaches in the current environment.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compactly-supported nonstationary kernels for computing exact Gaussian processes on big data</title>
<link>https://arxiv.org/abs/2411.05869</link>
<guid>https://arxiv.org/abs/2411.05869</guid>
<content:encoded><![CDATA[
arXiv:2411.05869v3 Announce Type: replace-cross 
Abstract: The Gaussian process (GP) is a widely used probabilistic machine learning method with implicit uncertainty characterization for stochastic function approximation, stochastic modeling, and analyzing real-world measurements of nonlinear processes. Traditional implementations of GPs involve stationary kernels (also termed covariance functions) that limit their flexibility, and exact methods for inference that prevent application to data sets with more than about ten thousand points. Modern approaches to address stationarity assumptions generally fail to accommodate large data sets, while all attempts to address scalability focus on approximating the Gaussian likelihood, which can involve subjectivity and lead to inaccuracies. In this work, we explicitly derive an alternative kernel that can discover and encode both sparsity and nonstationarity. We embed the kernel within a fully Bayesian GP model and leverage high-performance computing resources to enable the analysis of massive data sets. We demonstrate the favorable performance of our novel kernel relative to existing exact and approximate GP methods across a variety of synthetic data examples. Furthermore, we conduct space-time prediction based on more than one million measurements of daily maximum temperature and verify that our results outperform state-of-the-art methods in the Earth sciences. More broadly, having access to exact GPs that use ultra-scalable, sparsity-discovering, nonstationary kernels allows GP methods to truly compete with a wide variety of machine learning methods.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Debias your Large Multi-Modal Model at Test-Time via Non-Contrastive Visual Attribute Steering</title>
<link>https://arxiv.org/abs/2411.12590</link>
<guid>https://arxiv.org/abs/2411.12590</guid>
<content:encoded><![CDATA[
arXiv:2411.12590v3 Announce Type: replace-cross 
Abstract: Large Multi-Modal Models (LMMs) have demonstrated impressive capabilities as general-purpose chatbots able to engage in conversations about visual inputs. However, their responses are influenced by societal biases present in their training datasets, leading to undesirable differences in how the model responds when presented with images depicting people of different demographics. In this work, we propose a training-free debiasing framework for LMMs that intervenes on the model's representations during text generation by constructing a steering vector that reduces reference on protected attributes. Our framework introduces two complementary methods: (1) a dataset-based approach that constructs a steering vector by contrasting model activations on biased and neutral inputs, and (2) a novel optimization-based approach designed for low-resource settings, which constructs the steering vector using a single step of gradient-based perturbation without requiring additional data. Our experiments show that these interventions effectively reduce the propensity of LMMs to generate text related to protected attributes while maintaining sentiment and fluency. Furthermore, we demonstrate that debiased LMMs achieve comparable accuracy to their unmodified counterparts on downstream tasks, indicating that bias mitigation can be achieved without sacrificing model performance.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tight PAC-Bayesian Risk Certificates for Contrastive Learning</title>
<link>https://arxiv.org/abs/2412.03486</link>
<guid>https://arxiv.org/abs/2412.03486</guid>
<content:encoded><![CDATA[
arXiv:2412.03486v3 Announce Type: replace-cross 
Abstract: Contrastive representation learning is a modern paradigm for learning representations of unlabeled data via augmentations -- precisely, contrastive models learn to embed semantically similar pairs of samples (positive pairs) closer than independently drawn samples (negative samples). In spite of its empirical success and widespread use in foundation models, statistical theory for contrastive learning remains less explored. Recent works have developed generalization error bounds for contrastive losses, but the resulting risk certificates are either vacuous (certificates based on Rademacher complexity or $f$-divergence) or require strong assumptions about samples that are unreasonable in practice. The present paper develops non-vacuous PAC-Bayesian risk certificates for contrastive representation learning, considering the practical considerations of the popular SimCLR framework. Notably, we take into account that SimCLR reuses positive pairs of augmented data as negative samples for other data, thereby inducing strong dependence and making classical PAC or PAC-Bayesian bounds inapplicable. We further refine existing bounds on the downstream classification loss by incorporating SimCLR-specific factors, including data augmentation and temperature scaling, and derive risk certificates for the contrastive zero-one risk. The resulting bounds for contrastive loss and downstream prediction are much tighter than those of previous risk certificates, as demonstrated by experiments on CIFAR-10.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Traffic Co-Simulation Framework Empowered by Infrastructure Camera Sensing and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2412.03925</link>
<guid>https://arxiv.org/abs/2412.03925</guid>
<content:encoded><![CDATA[
arXiv:2412.03925v2 Announce Type: replace-cross 
Abstract: Traffic simulations are commonly used to optimize urban traffic flow, with reinforcement learning (RL) showing promising potential for automated traffic signal control, particularly in intelligent transportation systems involving connected automated vehicles. Multi-agent reinforcement learning (MARL) is particularly effective for learning control strategies for traffic lights in a network using iterative simulations. However, existing methods often assume perfect vehicle detection, which overlooks real-world limitations related to infrastructure availability and sensor reliability. This study proposes a co-simulation framework integrating CARLA and SUMO, which combines high-fidelity 3D modeling with large-scale traffic flow simulation. Cameras mounted on traffic light poles within the CARLA environment use a YOLO-based computer vision system to detect and count vehicles, providing real-time traffic data as input for adaptive signal control in SUMO. MARL agents trained with four different reward structures leverage this visual feedback to optimize signal timings and improve network-wide traffic flow. Experiments in a multi-intersection test-bed demonstrate the effectiveness of the proposed MARL approach in enhancing traffic conditions using real-time camera based detection. The framework also evaluates the robustness of MARL under faulty or sparse sensing and compares the performance of YOLOv5 and YOLOv8 for vehicle detection. Results show that while better accuracy improves performance, MARL agents can still achieve significant improvements with imperfect detection, demonstrating scalability and adaptability for real-world scenarios.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DINAMO: Dynamic and INterpretable Anomaly MOnitoring for Large-Scale Particle Physics Experiments</title>
<link>https://arxiv.org/abs/2501.19237</link>
<guid>https://arxiv.org/abs/2501.19237</guid>
<content:encoded><![CDATA[
arXiv:2501.19237v2 Announce Type: replace-cross 
Abstract: Ensuring reliable data collection in large-scale particle physics experiments demands Data Quality Monitoring (DQM) procedures to detect possible detector malfunctions and preserve data integrity. Traditionally, this resource-intensive task has been handled by human shifters who struggle with frequent changes in operational conditions. We present DINAMO: a novel, interpretable, robust, and scalable DQM framework designed to automate anomaly detection in time-dependent settings. Our approach constructs evolving histogram templates with built-in uncertainties, featuring both a statistical variant - extending the classical Exponentially Weighted Moving Average (EWMA) - and a machine learning (ML)-enhanced version that leverages a transformer encoder for improved adaptability. Experimental validations on synthetic datasets demonstrate the high accuracy, adaptability, and interpretability of these methods. The statistical variant is being commissioned in the LHCb experiment at the Large Hadron Collider, underscoring its real-world impact. The code used in this study is available at https://github.com/ArseniiGav/DINAMO.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gap-Dependent Bounds for Federated $Q$-learning</title>
<link>https://arxiv.org/abs/2502.02859</link>
<guid>https://arxiv.org/abs/2502.02859</guid>
<content:encoded><![CDATA[
arXiv:2502.02859v2 Announce Type: replace-cross 
Abstract: We present the first gap-dependent analysis of regret and communication cost for on-policy federated $Q$-Learning in tabular episodic finite-horizon Markov decision processes (MDPs). Existing FRL methods focus on worst-case scenarios, leading to $\sqrt{T}$-type regret bounds and communication cost bounds with a $\log T$ term scaling with the number of agents $M$, states $S$, and actions $A$, where $T$ is the average total number of steps per agent. In contrast, our novel framework leverages the benign structures of MDPs, such as a strictly positive suboptimality gap, to achieve a $\log T$-type regret bound and a refined communication cost bound that disentangles exploration and exploitation. Our gap-dependent regret bound reveals a distinct multi-agent speedup pattern, and our gap-dependent communication cost bound removes the dependence on $MSA$ from the $\log T$ term. Notably, our gap-dependent communication cost bound also yields a better global switching cost when $M=1$, removing $SA$ from the $\log T$ term.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combining physics education and machine learning research to measure evidence of students' mechanistic sensemaking</title>
<link>https://arxiv.org/abs/2503.15638</link>
<guid>https://arxiv.org/abs/2503.15638</guid>
<content:encoded><![CDATA[
arXiv:2503.15638v3 Announce Type: replace-cross 
Abstract: Advances in machine learning (ML) offer new possibilities for science education research. We report on early progress in the design of an ML-based tool to analyze students' mechanistic sensemaking, working from a coding scheme that is aligned with previous work in physics education research (PER) and amenable to recently developed ML classification strategies using language encoders. We describe pilot tests of the tool, in three versions with different language encoders, to analyze sensemaking evident in college students' written responses to brief conceptual questions. The results show, first, that the tool's measurements of sensemaking can achieve useful agreement with a human coder, and, second, that encoder design choices entail a tradeoff between accuracy and computational expense. We discuss the promise and limitations of this approach, providing examples as to how this measurement scheme may serve PER in the future. We conclude with reflections on the use of ML to support PER research, with cautious optimism for strategies of co-design between PER and ML.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extracting memorized pieces of (copyrighted) books from open-weight language models</title>
<link>https://arxiv.org/abs/2505.12546</link>
<guid>https://arxiv.org/abs/2505.12546</guid>
<content:encoded><![CDATA[
arXiv:2505.12546v3 Announce Type: replace-cross 
Abstract: Plaintiffs and defendants in copyright lawsuits over generative AI often make sweeping, opposing claims about the extent to which large language models (LLMs) have memorized plaintiffs' protected expression in their training data. Drawing on both machine learning and copyright law, we show that these polarized positions dramatically oversimplify the relationship between memorization and copyright. To do so, we extend a recent probabilistic extraction technique to measure memorization of 50 books in 17 open-weight LLMs. Through thousands of experiments, we show that the extent of memorization varies both by model and by book. With respect to our specific extraction methodology, we find that most LLMs do not memorize most books -- either in whole or in part. However, we also find that Llama 3.1 70B entirely memorizes some books, like the first Harry Potter book and 1984. In fact, the first Harry Potter is so memorized that, using a seed prompt consisting of just the first few tokens of the first chapter, we can deterministically generate the entire book near-verbatim. We discuss why our results have significant implications for copyright cases, though not ones that unambiguously favor either side.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hamiltonian Descent Algorithms for Optimization: Accelerated Rates via Randomized Integration Time</title>
<link>https://arxiv.org/abs/2505.12553</link>
<guid>https://arxiv.org/abs/2505.12553</guid>
<content:encoded><![CDATA[
arXiv:2505.12553v2 Announce Type: replace-cross 
Abstract: We study the Hamiltonian flow for optimization (HF-opt), which simulates the Hamiltonian dynamics for some integration time and resets the velocity to $0$ to decrease the objective function; this is the optimization analogue of the Hamiltonian Monte Carlo algorithm for sampling. For short integration time, HF-opt has the same convergence rates as gradient descent for minimizing strongly and weakly convex functions. We show that by randomizing the integration time in HF-opt, the resulting randomized Hamiltonian flow (RHF) achieves accelerated convergence rates in continuous time, similar to the rates for the accelerated gradient flow. We study a discrete-time implementation of RHF as the randomized Hamiltonian gradient descent (RHGD) algorithm. We prove that RHGD achieves the same accelerated convergence rates as Nesterov's accelerated gradient descent (AGD) for minimizing smooth strongly and weakly convex functions. We provide numerical experiments to demonstrate that RHGD is competitive with classical accelerated methods such as AGD across all settings and outperforms them in certain regimes.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PMPO: Probabilistic Metric Prompt Optimization for Small and Large Language Models</title>
<link>https://arxiv.org/abs/2505.16307</link>
<guid>https://arxiv.org/abs/2505.16307</guid>
<content:encoded><![CDATA[
arXiv:2505.16307v2 Announce Type: replace-cross 
Abstract: Prompt optimization is a practical and widely applicable alternative to fine tuning for improving large language model performance. Yet many existing methods evaluate candidate prompts by sampling full outputs, often coupled with self critique or human annotated preferences, which limits scalability, especially for smaller models or models that are not instruction tuned. We present PMPO (Probabilistic Metric Prompt Optimization), a unified framework that uses token level cross entropy as a direct, lightweight evaluation signal. PMPO locates low quality prompt segments via a masking based analysis and iteratively rewrites them to propose improved variants. Crucially, during evaluation, PMPO selects among variants by minimizing loss in a single forward pass, eliminating output sampling and human or judge based scoring for selection while still using standard generation only to propose rewrites. This unified, loss based strategy supports both supervised and preference based tasks. Across model sizes and datasets, PMPO outperforms prior prompt optimizers: it achieves the highest average accuracy on BBH, performs strongly on GSM8K and AQUA RAT, and raises AlpacaEval 2.0 win rates by over 19 points. These results demonstrate PMPO's effectiveness, efficiency, and broad applicability.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Style Transfer with Diffusion Models for Synthetic-to-Real Domain Adaptation</title>
<link>https://arxiv.org/abs/2505.16360</link>
<guid>https://arxiv.org/abs/2505.16360</guid>
<content:encoded><![CDATA[
arXiv:2505.16360v2 Announce Type: replace-cross 
Abstract: Semantic segmentation models trained on synthetic data often perform poorly on real-world images due to domain gaps, particularly in adverse conditions where labeled data is scarce. Yet, recent foundation models enable to generate realistic images without any training. This paper proposes to leverage such diffusion models to improve the performance of vision models when learned on synthetic data. We introduce two novel techniques for semantically consistent style transfer using diffusion models: Class-wise Adaptive Instance Normalization and Cross-Attention (CACTI) and its extension with selective attention Filtering (CACTIF). CACTI applies statistical normalization selectively based on semantic classes, while CACTIF further filters cross-attention maps based on feature similarity, preventing artifacts in regions with weak cross-attention correspondences. Our methods transfer style characteristics while preserving semantic boundaries and structural coherence, unlike approaches that apply global transformations or generate content without constraints. Experiments using GTA5 as source and Cityscapes/ACDC as target domains show that our approach produces higher quality images with lower FID scores and better content preservation. Our work demonstrates that class-aware diffusion-based style transfer effectively bridges the synthetic-to-real domain gap even with minimal target domain data, advancing robust perception systems for challenging real-world applications. The source code is available at: https://github.com/echigot/cactif.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A deep solver for backward stochastic Volterra integral equations</title>
<link>https://arxiv.org/abs/2505.18297</link>
<guid>https://arxiv.org/abs/2505.18297</guid>
<content:encoded><![CDATA[
arXiv:2505.18297v3 Announce Type: replace-cross 
Abstract: We present the first deep-learning solver for backward stochastic Volterra integral equations (BSVIEs) and their fully-coupled forward-backward variants. The method trains a neural network to approximate the two solution fields in a single stage, avoiding the use of nested time-stepping cycles that limit classical algorithms. For the decoupled case we prove a non-asymptotic error bound composed of an a posteriori residual plus the familiar square root dependence on the time step. Numerical experiments confirm this rate and reveal two key properties: \emph{scalability}, in the sense that accuracy remains stable from low dimension up to 500 spatial variables while GPU batching keeps wall-clock time nearly constant; and \emph{generality}, since the same method handles coupled systems whose forward dynamics depend on the backward solution. These results open practical access to a family of high-dimensional, path-dependent problems in stochastic control and quantitative finance.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ActivePusher: Active Learning and Planning with Residual Physics for Nonprehensile Manipulation</title>
<link>https://arxiv.org/abs/2506.04646</link>
<guid>https://arxiv.org/abs/2506.04646</guid>
<content:encoded><![CDATA[
arXiv:2506.04646v2 Announce Type: replace-cross 
Abstract: Planning with learned dynamics models offers a promising approach toward versatile real-world manipulation, particularly in nonprehensile settings such as pushing or rolling, where accurate analytical models are difficult to obtain. However, collecting training data for learning-based methods can be costly and inefficient, as it often relies on randomly sampled interactions that are not necessarily the most informative. Furthermore, learned models tend to exhibit high uncertainty in underexplored regions of the skill space, undermining the reliability of long-horizon planning. To address these challenges, we propose ActivePusher, a novel framework that combines residual-physics modeling with uncertainty-based active learning, to focus data acquisition on the most informative skill parameters. Additionally, ActivePusher seamlessly integrates with model-based kinodynamic planners, leveraging uncertainty estimates to bias control sampling toward more reliable actions. We evaluate our approach in both simulation and real-world environments, and demonstrate that it consistently improves data efficiency and achieves higher planning success rates in comparison to baseline methods. The source code is available at https://github.com/elpis-lab/ActivePusher.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiCoRe: Enhancing Zero-shot Event Detection via Divergent-Convergent LLM Reasoning</title>
<link>https://arxiv.org/abs/2506.05128</link>
<guid>https://arxiv.org/abs/2506.05128</guid>
<content:encoded><![CDATA[
arXiv:2506.05128v2 Announce Type: replace-cross 
Abstract: Zero-shot Event Detection (ED), the task of identifying event mentions in natural language text without any training data, is critical for document understanding in specialized domains. Understanding the complex event ontology, extracting domain-specific triggers from the passage, and structuring them appropriately overloads and limits the utility of Large Language Models (LLMs) for zero-shot ED. To this end, we propose DiCoRe, a divergent-convergent reasoning framework that decouples the task of ED using Dreamer and Grounder. Dreamer encourages divergent reasoning through open-ended event discovery, which helps to boost event coverage. Conversely, Grounder introduces convergent reasoning to align the free-form predictions with the task-specific instructions using finite-state machine guided constrained decoding. Additionally, an LLM-Judge verifies the final outputs to ensure high precision. Through extensive experiments on six datasets across five domains and nine LLMs, we demonstrate how DiCoRe consistently outperforms prior zero-shot, transfer-learning, and reasoning baselines, achieving 4-7% average F1 gains over the best baseline -- establishing DiCoRe as a strong zero-shot ED framework.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Plea for History and Philosophy of Statistics and Machine Learning</title>
<link>https://arxiv.org/abs/2506.22236</link>
<guid>https://arxiv.org/abs/2506.22236</guid>
<content:encoded><![CDATA[
arXiv:2506.22236v3 Announce Type: replace-cross 
Abstract: The integration of the history and philosophy of statistics was initiated at least by Hacking (1975) and advanced by Hacking (1990), Mayo (1996), and Zabell (2005), but it has not received sustained follow-up. Yet such integration is more urgent than ever, as the recent success of artificial intelligence has been driven largely by machine learning -- a field historically developed alongside statistics. Today, the boundary between statistics and machine learning is increasingly blurred. What we now need is integration, twice over: of history and philosophy, and of two fields they engage -- statistics and machine learning. I present a case study of a philosophical idea in machine learning (and in formal epistemology) whose root can be traced back to an often under-appreciated insight in Neyman and Pearson's 1936 work (a follow-up to their 1933 classic). This leads to the articulation of an epistemological principle -- largely implicit in, but shared by, the practices of frequentist statistics and machine learning -- which I call achievabilism: the thesis that the correct standard for assessing non-deductive inference methods should not be fixed, but should instead be sensitive to what is achievable in specific problem contexts. Another integration also emerges at the level of methodology, combining two ends of the philosophy of science spectrum: history and philosophy of science on the one hand, and formal epistemology on the other hand.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedVAL: Toward Expert-Level Medical Text Validation with Language Models</title>
<link>https://arxiv.org/abs/2507.03152</link>
<guid>https://arxiv.org/abs/2507.03152</guid>
<content:encoded><![CDATA[
arXiv:2507.03152v4 Announce Type: replace-cross 
Abstract: With the growing use of language models (LMs) in clinical environments, there is an immediate need to evaluate the accuracy and safety of LM-generated medical text. Currently, such evaluation relies solely on manual physician review. However, detecting errors in LM-generated text is challenging because 1) manual review is costly and 2) expert-composed reference outputs are often unavailable in real-world settings. While the "LM-as-judge" paradigm (a LM evaluating another LM) offers scalable evaluation, even frontier LMs can miss subtle but clinically significant errors. To address these challenges, we propose MedVAL, a novel, self-supervised, data-efficient distillation method that leverages synthetic data to train evaluator LMs to assess whether LM-generated medical outputs are factually consistent with inputs, without requiring physician labels or reference outputs. To evaluate LM performance, we introduce MedVAL-Bench, a dataset of 840 physician-annotated outputs across 6 diverse medical tasks capturing real-world challenges. Across 10 state-of-the-art LMs spanning open-source and proprietary models, MedVAL distillation significantly improves (p < 0.001) alignment with physicians across seen and unseen tasks, increasing average F1 scores from 66% to 83%. Despite strong baseline performance, MedVAL improves the best-performing proprietary LM (GPT-4o) by 8% without training on physician-labeled data, demonstrating a performance statistically non-inferior to a single human expert (p < 0.001). To support a scalable, risk-aware pathway towards clinical integration, we open-source: 1) Codebase (https://github.com/StanfordMIMI/MedVAL), 2) MedVAL-Bench (https://huggingface.co/datasets/stanfordmimi/MedVAL-Bench), 3) MedVAL-4B (https://huggingface.co/stanfordmimi/MedVAL-4B). Our benchmark provides evidence of LMs approaching expert-level ability in validating AI-generated medical text.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMART: Simulated Students Aligned with Item Response Theory for Question Difficulty Prediction</title>
<link>https://arxiv.org/abs/2507.05129</link>
<guid>https://arxiv.org/abs/2507.05129</guid>
<content:encoded><![CDATA[
arXiv:2507.05129v2 Announce Type: replace-cross 
Abstract: Item (question) difficulties play a crucial role in educational assessments, enabling accurate and efficient assessment of student abilities and personalization to maximize learning outcomes. Traditionally, estimating item difficulties can be costly, requiring real students to respond to items, followed by fitting an item response theory (IRT) model to get difficulty estimates. This approach cannot be applied to the cold-start setting for previously unseen items either. In this work, we present SMART (Simulated Students Aligned with IRT), a novel method for aligning simulated students with instructed ability, which can then be used in simulations to predict the difficulty of open-ended items. We achieve this alignment using direct preference optimization (DPO), where we form preference pairs based on how likely responses are under a ground-truth IRT model. We perform a simulation by generating thousands of responses, evaluating them with a large language model (LLM)-based scoring model, and fit the resulting data to an IRT model to obtain item difficulty estimates. Through extensive experiments on two real-world student response datasets, we show that SMART outperforms other item difficulty prediction methods by leveraging its improved ability alignment.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trainability of Quantum Models Beyond Known Classical Simulability</title>
<link>https://arxiv.org/abs/2507.06344</link>
<guid>https://arxiv.org/abs/2507.06344</guid>
<content:encoded><![CDATA[
arXiv:2507.06344v2 Announce Type: replace-cross 
Abstract: Variational Quantum Algorithms (VQAs) are promising candidates for near-term quantum computing, yet they face scalability challenges due to barren plateaus, where gradients vanish exponentially in the system size. Recent conjectures suggest that avoiding barren plateaus might inherently lead to classical simulability, thus limiting the opportunities for quantum advantage. In this work, we advance the theoretical understanding of the relationship between the trainability and computational complexity of VQAs, thus directly addressing the conjecture. We introduce the Linear Clifford Encoder (LCE), a novel technique that ensures constant-scaling gradient statistics on optimization landscape regions that are close to Clifford circuits. Additionally, we leverage classical Taylor surrogates to reveal computational complexity phase transitions from polynomial to super-polynomial as the initialization region size increases. Combining these results, we reveal a deeper link between trainability and computational complexity, and analytically prove that barren plateaus can be avoided in regions for which no classical surrogate is known to exist. Furthermore, numerical experiments on LCE transformed landscapes confirm in practice the existence of a super-polynomially complex ``transition zone'' where gradients decay polynomially. These findings indicate a plausible path to practically relevant, barren plateau-free variational models with potential for quantum advantage.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parametrized Quantum Circuit Learning for Quantum Chemical Applications</title>
<link>https://arxiv.org/abs/2507.08183</link>
<guid>https://arxiv.org/abs/2507.08183</guid>
<content:encoded><![CDATA[
arXiv:2507.08183v2 Announce Type: replace-cross 
Abstract: In the field of quantum machine learning (QML), parametrized quantum circuits (PQCs) -- constructed using a combination of fixed and tunable quantum gates -- provide a promising hybrid framework for tackling complex machine learning problems. Despite numerous proposed applications, there remains limited exploration of datasets relevant to quantum chemistry. In this study, we investigate the potential benefits and limitations of PQCs on two chemically meaningful datasets: (1) the BSE49 dataset, containing bond separation energies for 49 different classes of chemical bonds, and (2) a dataset of water conformations, where coupled-cluster singles and doubles (CCSD) wavefunctions are predicted from lower-level electronic structure methods using the data-driven coupled-cluster (DDCC) approach. We construct a comprehensive set of 168 PQCs by combining 14 data encoding strategies with 12 variational ans{\"a}tze, and evaluate their performance on circuits with 5 and 16 qubits. Our initial analysis examines the impact of circuit structure on model performance using state-vector simulations. We then explore how circuit depth and training set size influence model performance. Finally, we assess the performance of the best-performing PQCs on current quantum hardware, using both noisy simulations ("fake" backends) and real quantum devices. Our findings underscore the challenges of applying PQCs to chemically relevant problems that are straightforward for classical machine learning methods but remain non-trivial for quantum approaches.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time Adaptive Motion Planning via Point Cloud-Guided, Energy-Based Diffusion and Potential Fields</title>
<link>https://arxiv.org/abs/2507.09383</link>
<guid>https://arxiv.org/abs/2507.09383</guid>
<content:encoded><![CDATA[
arXiv:2507.09383v2 Announce Type: replace-cross 
Abstract: Motivated by the problem of pursuit-evasion, we present a motion planning framework that combines energy-based diffusion models with artificial potential fields for robust real time trajectory generation in complex environments. Our approach processes obstacle information directly from point clouds, enabling efficient planning without requiring complete geometric representations. The framework employs classifier-free guidance training and integrates local potential fields during sampling to enhance obstacle avoidance. In dynamic scenarios, the system generates initial trajectories using the diffusion model and continuously refines them through potential field-based adaptation, demonstrating effective performance in pursuit-evasion scenarios with partial pursuer observability.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning</title>
<link>https://arxiv.org/abs/2507.16815</link>
<guid>https://arxiv.org/abs/2507.16815</guid>
<content:encoded><![CDATA[
arXiv:2507.16815v2 Announce Type: replace-cross 
Abstract: Vision-language-action (VLA) reasoning tasks require agents to interpret multimodal instructions, perform long-horizon planning, and act adaptively in dynamic environments. Existing approaches typically train VLA models in an end-to-end fashion, directly mapping inputs to actions without explicit reasoning, which hinders their ability to plan over multiple steps or adapt to complex task variations. In this paper, we propose ThinkAct, a dual-system framework that bridges high-level reasoning with low-level action execution via reinforced visual latent planning. ThinkAct trains a multimodal LLM to generate embodied reasoning plans guided by reinforcing action-aligned visual rewards based on goal completion and trajectory consistency. These reasoning plans are compressed into a visual plan latent that conditions a downstream action model for robust action execution on target environments. Extensive experiments on embodied reasoning and robot manipulation benchmarks demonstrate that ThinkAct enables few-shot adaptation, long-horizon planning, and self-correction behaviors in complex embodied AI tasks.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Provable Non-Convex Euclidean Distance Matrix Completion: Geometry, Reconstruction, and Robustness</title>
<link>https://arxiv.org/abs/2508.00091</link>
<guid>https://arxiv.org/abs/2508.00091</guid>
<content:encoded><![CDATA[
arXiv:2508.00091v2 Announce Type: replace-cross 
Abstract: The problem of recovering the configuration of points from their partial pairwise distances, referred to as the Euclidean Distance Matrix Completion (EDMC) problem, arises in a broad range of applications, including sensor network localization, molecular conformation, and manifold learning. In this paper, we propose a Riemannian optimization framework for solving the EDMC problem by formulating it as a low-rank matrix completion task over the space of positive semi-definite Gram matrices. The available distance measurements are encoded as expansion coefficients in a non-orthogonal basis, and optimization over the Gram matrix implicitly enforces geometric consistency through nonnegativity and the triangle inequality, a structure inherited from classical multidimensional scaling. Under a Bernoulli sampling model for observed distances, we prove that Riemannian gradient descent on the manifold of rank-$r$ matrices locally converges linearly with high probability when the sampling probability satisfies $p\geq O(\nu^2 r^2\log(n)/n)$, where $\nu$ is an EDMC-specific incoherence parameter. Furthermore, we provide an initialization candidate using a one-step hard thresholding procedure that yields convergence, provided the sampling probability satisfies $p \geq O(\nu r^{3/2}\log^{3/4}(n)/n^{1/4})$. A key technical contribution of this work is the analysis of a symmetric linear operator arising from a dual basis expansion in the non-orthogonal basis, which requires a novel application of the Hanson-Wright inequality to establish an optimal restricted isometry property in the presence of coupled terms. Empirical evaluations on synthetic data demonstrate that our algorithm achieves competitive performance relative to state-of-the-art methods. Moreover, we provide a geometric interpretation of matrix incoherence tailored to the EDMC setting and provide robustness guarantees for our method.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two tales for a geometric Jensen--Shannon divergence</title>
<link>https://arxiv.org/abs/2508.05066</link>
<guid>https://arxiv.org/abs/2508.05066</guid>
<content:encoded><![CDATA[
arXiv:2508.05066v3 Announce Type: replace-cross 
Abstract: The geometric Jensen--Shannon divergence (G-JSD) gained popularity in machine learning and information sciences thanks to its closed-form expression between Gaussian distributions. In this work, we introduce an alternative definition of the geometric Jensen--Shannon divergence tailored to positive densities which does not normalize geometric mixtures. This novel divergence is termed the extended G-JSD as it applies to the more general case of positive measures. We report explicitly the gap between the extended G-JSD and the G-JSD when considering probability densities, and show how to express the G-JSD and extended G-JSD using the Jeffreys divergence and the Bhattacharyya distance or Bhattacharyya coefficient. The extended G-JSD is proven to be a $f$-divergence which is a separable divergence satisfying information monotonicity and invariance in information geometry. We derive corresponding closed-form formula for the two types of G-JSDs when considering the case of multivariate Gaussian distributions often met in applications. We consider Monte Carlo stochastic estimations and approximations of the two types of G-JSD using the projective $\gamma$-divergences. Although the square root of the JSD yields a metric distance, we show that this is not anymore the case for the two types of G-JSD. Finally, we explain how these two types of geometric JSDs can be interpreted as regularizations of the ordinary JSD.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DSperse: A Framework for Targeted Verification in Zero-Knowledge Machine Learning</title>
<link>https://arxiv.org/abs/2508.06972</link>
<guid>https://arxiv.org/abs/2508.06972</guid>
<content:encoded><![CDATA[
arXiv:2508.06972v3 Announce Type: replace-cross 
Abstract: DSperse is a modular framework for distributed machine learning inference with strategic cryptographic verification. Operating within the emerging paradigm of distributed zero-knowledge machine learning, DSperse avoids the high cost and rigidity of full-model circuitization by enabling targeted verification of strategically chosen subcomputations. These verifiable segments, or "slices", may cover part or all of the inference pipeline, with global consistency enforced through audit, replication, or economic incentives. This architecture supports a pragmatic form of trust minimization, localizing zero-knowledge proofs to the components where they provide the greatest value. We evaluate DSperse using multiple proving systems and report empirical results on memory usage, runtime, and circuit behavior under sliced and unsliced configurations. By allowing proof boundaries to align flexibly with the model's logical structure, DSperse supports scalable, targeted verification strategies suited to diverse deployment needs.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probing the Representational Power of Sparse Autoencoders in Vision Models</title>
<link>https://arxiv.org/abs/2508.11277</link>
<guid>https://arxiv.org/abs/2508.11277</guid>
<content:encoded><![CDATA[
arXiv:2508.11277v2 Announce Type: replace-cross 
Abstract: Sparse Autoencoders (SAEs) have emerged as a popular tool for interpreting the hidden states of large language models (LLMs). By learning to reconstruct activations from a sparse bottleneck layer, SAEs discover interpretable features from the high-dimensional internal representations of LLMs. Despite their popularity with language models, SAEs remain understudied in the visual domain. In this work, we provide an extensive evaluation the representational power of SAEs for vision models using a broad range of image-based tasks. Our experimental results demonstrate that SAE features are semantically meaningful, improve out-of-distribution generalization, and enable controllable generation across three vision model architectures: vision embedding models, multi-modal LMMs and diffusion models. In vision embedding models, we find that learned SAE features can be used for OOD detection and provide evidence that they recover the ontological structure of the underlying model. For diffusion models, we demonstrate that SAEs enable semantic steering through text encoder manipulation and develop an automated pipeline for discovering human-interpretable attributes. Finally, we conduct exploratory experiments on multi-modal LLMs, finding evidence that SAE features reveal shared representations across vision and language modalities. Our study provides a foundation for SAE evaluation in vision models, highlighting their strong potential improving interpretability, generalization, and steerability in the visual domain.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Moment- and Power-Spectrum-Based Gaussianity Regularization for Text-to-Image Models</title>
<link>https://arxiv.org/abs/2509.07027</link>
<guid>https://arxiv.org/abs/2509.07027</guid>
<content:encoded><![CDATA[
arXiv:2509.07027v3 Announce Type: replace-cross 
Abstract: We propose a novel regularization loss that enforces standard Gaussianity, encouraging samples to align with a standard Gaussian distribution. This facilitates a range of downstream tasks involving optimization in the latent space of text-to-image models. We treat elements of a high-dimensional sample as one-dimensional standard Gaussian variables and define a composite loss that combines moment-based regularization in the spatial domain with power spectrum-based regularization in the spectral domain. Since the expected values of moments and power spectrum distributions are analytically known, the loss promotes conformity to these properties. To ensure permutation invariance, the losses are applied to randomly permuted inputs. Notably, existing Gaussianity-based regularizations fall within our unified framework: some correspond to moment losses of specific orders, while the previous covariance-matching loss is equivalent to our spectral loss but incurs higher time complexity due to its spatial-domain computation. We showcase the application of our regularization in generative modeling for test-time reward alignment with a text-to-image model, specifically to enhance aesthetics and text alignment. Our regularization outperforms previous Gaussianity regularization, effectively prevents reward hacking and accelerates convergence.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Statistical Methods in Generative AI</title>
<link>https://arxiv.org/abs/2509.07054</link>
<guid>https://arxiv.org/abs/2509.07054</guid>
<content:encoded><![CDATA[
arXiv:2509.07054v2 Announce Type: replace-cross 
Abstract: Generative Artificial Intelligence is emerging as an important technology, promising to be transformative in many areas. At the same time, generative AI techniques are based on sampling from probabilistic models, and by default, they come with no guarantees about correctness, safety, fairness, or other properties. Statistical methods offer a promising potential approach to improve the reliability of generative AI techniques. In addition, statistical methods are also promising for improving the quality and efficiency of AI evaluation, as well as for designing interventions and experiments in AI. In this paper, we review some of the existing work on these topics, explaining both the general statistical techniques used, as well as their applications to generative AI. We also discuss limitations and potential future directions.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reconstruction Alignment Improves Unified Multimodal Models</title>
<link>https://arxiv.org/abs/2509.07295</link>
<guid>https://arxiv.org/abs/2509.07295</guid>
<content:encoded><![CDATA[
arXiv:2509.07295v2 Announce Type: replace-cross 
Abstract: Unified multimodal models (UMMs) unify visual understanding and generation within a single architecture. However, conventional training relies on image-text pairs (or sequences) whose captions are typically sparse and miss fine-grained visual details--even when they use hundreds of words to describe a simple image. We introduce Reconstruction Alignment (RecA), a resource-efficient post-training method that leverages visual understanding encoder embeddings as dense "text prompts," providing rich supervision without captions. Concretely, RecA conditions a UMM on its own visual understanding embeddings and optimizes it to reconstruct the input image with a self-supervised reconstruction loss, thereby realigning understanding and generation. Despite its simplicity, RecA is broadly applicable: across autoregressive, masked-autoregressive, and diffusion-based UMMs, it consistently improves generation and editing fidelity. With only 27 GPU-hours, post-training with RecA substantially improves image generation performance on GenEval (0.73$\rightarrow$0.90) and DPGBench (80.93$\rightarrow$88.15), while also boosting editing benchmarks (ImgEdit 3.38$\rightarrow$3.75, GEdit 6.94$\rightarrow$7.25). Notably, RecA surpasses much larger open-source models and applies broadly across diverse UMM architectures, establishing it as an efficient and general post-training alignment strategy for UMMs
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Reinforcement Learning for Large Reasoning Models</title>
<link>https://arxiv.org/abs/2509.08827</link>
<guid>https://arxiv.org/abs/2509.08827</guid>
<content:encoded><![CDATA[
arXiv:2509.08827v2 Announce Type: replace-cross 
Abstract: In this paper, we survey recent advances in Reinforcement Learning (RL) for reasoning with Large Language Models (LLMs). RL has achieved remarkable success in advancing the frontier of LLM capabilities, particularly in addressing complex logical tasks such as mathematics and coding. As a result, RL has emerged as a foundational methodology for transforming LLMs into LRMs. With the rapid progress of the field, further scaling of RL for LRMs now faces foundational challenges not only in computational resources but also in algorithm design, training data, and infrastructure. To this end, it is timely to revisit the development of this domain, reassess its trajectory, and explore strategies to enhance the scalability of RL toward Artificial SuperIntelligence (ASI). In particular, we examine research applying RL to LLMs and LRMs for reasoning abilities, especially since the release of DeepSeek-R1, including foundational components, core problems, training resources, and downstream applications, to identify future opportunities and directions for this rapidly evolving area. We hope this review will promote future research on RL for broader reasoning models. Github: https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perfectly-Private Analog Secure Aggregation in Federated Learning</title>
<link>https://arxiv.org/abs/2509.08683</link>
<guid>https://arxiv.org/abs/2509.08683</guid>
<content:encoded><![CDATA[
<div> Keywords: federated learning, secure aggregation, secure multiparty computation, privacy, torus<br />
Summary:<br />
This paper addresses the privacy concerns in federated learning by proposing a novel secure parameter aggregation method using the torus. Traditional secure aggregation methods utilizing finite fields face accuracy complexity tradeoff issues, but the torus-based approach guarantees perfect privacy while maintaining model accuracy. By leveraging the uniform distribution on the torus, each party's data is kept private. Experimental results demonstrate that the new protocol performs similarly to non-secure aggregation models, with added privacy benefits. Additionally, the torus-based protocol outperforms finite field secure aggregation in terms of model accuracy and cosine similarity in some cases, making it a safer option. Overall, this innovative approach offers a solution to the challenges of maintaining privacy in federated learning without sacrificing accuracy. <br /><br />Summary: <div>
arXiv:2509.08683v2 Announce Type: replace 
Abstract: In federated learning, multiple parties train models locally and share their parameters with a central server, which aggregates them to update a global model. To address the risk of exposing sensitive data through local models, secure aggregation via secure multiparty computation has been proposed to enhance privacy. At the same time, perfect privacy can only be achieved by a uniform distribution of the masked local models to be aggregated. This raises a problem when working with real valued data, as there is no measure on the reals that is invariant under the masking operation, and hence information leakage is bound to occur. Shifting the data to a finite field circumvents this problem, but as a downside runs into an inherent accuracy complexity tradeoff issue due to fixed point modular arithmetic as opposed to floating point numbers that can simultaneously handle numbers of varying magnitudes. In this paper, a novel secure parameter aggregation method is proposed that employs the torus rather than a finite field. This approach guarantees perfect privacy for each party's data by utilizing the uniform distribution on the torus, while avoiding accuracy losses. Experimental results show that the new protocol performs similarly to the model without secure aggregation while maintaining perfect privacy. Compared to the finite field secure aggregation, the torus-based protocol can in some cases significantly outperform it in terms of model accuracy and cosine similarity, hence making it a safer choice.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contextualize-then-Aggregate: Circuits for In-Context Learning in Gemma-2 2B</title>
<link>https://arxiv.org/abs/2504.00132</link>
<guid>https://arxiv.org/abs/2504.00132</guid>
<content:encoded><![CDATA[
<div> in-context learning, large language models, Gemma-2 2B, causal interventions, information flow

Summary: 
The study explores the mechanism behind In-Context Learning (ICL) in large language models (LLMs) using causal interventions on Gemma-2 2B. It uncovers a two-step strategy termed contextualize-then-aggregate used by the model: at lower layers, individual fewshot examples are represented and contextualized by preceding examples, while at higher layers, these representations are aggregated to identify tasks and make predictions. The contextualization step's importance varies across tasks and becomes more crucial with ambiguous examples. Through rigorous causal analysis, the research illuminates how ICL occurs in language models, providing insights into the intricate process of task inference from fewshot prompts. <div>
arXiv:2504.00132v4 Announce Type: replace-cross 
Abstract: In-Context Learning (ICL) is an intriguing ability of large language models (LLMs). Despite a substantial amount of work on its behavioral aspects and how it emerges in miniature setups, it remains unclear which mechanism assembles task information from the individual examples in a fewshot prompt. We use causal interventions to identify information flow in Gemma-2 2B for five naturalistic ICL tasks. We find that the model infers task information using a two-step strategy we call contextualize-then-aggregate: In the lower layers, the model builds up representations of individual fewshot examples, which are contextualized by preceding examples through connections between fewshot input and output tokens across the sequence. In the higher layers, these representations are aggregated to identify the task and prepare prediction of the next output. The importance of the contextualization step differs between tasks, and it may become more important in the presence of ambiguous examples. Overall, by providing rigorous causal analysis, our results shed light on the mechanisms through which ICL happens in language models.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified Spatiotemopral Physics-Informed Learning (USPIL): A Framework for Modeling Complex Predator-Prey Dynamics</title>
<link>https://arxiv.org/abs/2509.13425</link>
<guid>https://arxiv.org/abs/2509.13425</guid>
<content:encoded><![CDATA[
<div> Framework, neural networks, predator-prey dynamics, physics-informed learning, multi-scale ecological modeling 

Summary:
The Unified Spatiotemporal Physics-Informed Learning (USPIL) framework integrates physics-informed neural networks (PINNs) and conservation laws to model predator-prey dynamics across dimensional scales. It provides a unified solution for ordinary and partial differential equation systems, capturing temporal cycles and reaction-diffusion patterns within a single neural network architecture. USPIL achieves high correlation for 1D temporal dynamics and successfully captures complex spiral waves in 2D systems. It enforces physics constraints using automatic differentiation and adaptive loss weighting, ensuring both data fidelity and physical consistency. The framework shows a significant computational speedup for inference compared to numerical solvers and enables mechanistic understanding through interpretable physics constraints. USPIL's ability to transition between dimensional formulations opens new avenues for multi-scale ecological modeling, making it a transformative tool for ecological forecasting, conservation planning, and ecosystem resilience understanding. <div>
arXiv:2509.13425v1 Announce Type: new 
Abstract: Ecological systems exhibit complex multi-scale dynamics that challenge traditional modeling. New methods must capture temporal oscillations and emergent spatiotemporal patterns while adhering to conservation principles. We present the Unified Spatiotemporal Physics-Informed Learning (USPIL) framework, a deep learning architecture integrating physics-informed neural networks (PINNs) and conservation laws to model predator-prey dynamics across dimensional scales. The framework provides a unified solution for both ordinary (ODE) and partial (PDE) differential equation systems, describing temporal cycles and reaction-diffusion patterns within a single neural network architecture. Our methodology uses automatic differentiation to enforce physics constraints and adaptive loss weighting to balance data fidelity with physical consistency. Applied to the Lotka-Volterra system, USPIL achieves 98.9% correlation for 1D temporal dynamics (loss: 0.0219, MAE: 0.0184) and captures complex spiral waves in 2D systems (loss: 4.7656, pattern correlation: 0.94). Validation confirms conservation law adherence within 0.5% and shows a 10-50x computational speedup for inference compared to numerical solvers. USPIL also enables mechanistic understanding through interpretable physics constraints, facilitating parameter discovery and sensitivity analysis not possible with purely data-driven methods. Its ability to transition between dimensional formulations opens new avenues for multi-scale ecological modeling. These capabilities make USPIL a transformative tool for ecological forecasting, conservation planning, and understanding ecosystem resilience, establishing physics-informed deep learning as a powerful and scientifically rigorous paradigm.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Analysis of Optimizer Choice on Energy Efficiency and Performance in Neural Network Training</title>
<link>https://arxiv.org/abs/2509.13516</link>
<guid>https://arxiv.org/abs/2509.13516</guid>
<content:encoded><![CDATA[
<div> optimizer, energy efficiency, neural network training, sustainability, trade-offs 

Summary: 
The study investigates the impact of optimizer choice on energy efficiency in neural network training. It conducted experiments across three datasets using eight popular optimizers and measured training duration, memory usage, carbon emissions, and model performance. Results show trade-offs between training speed, accuracy, and environmental impact, with AdamW and NAdam identified as efficient choices. Despite higher emissions, SGD performs well on complex datasets. The findings offer insights for practitioners aiming to balance performance and sustainability in machine learning workflows. <div>
arXiv:2509.13516v1 Announce Type: new 
Abstract: As machine learning models grow increasingly complex and computationally demanding, understanding the environmental impact of training decisions becomes critical for sustainable AI development. This paper presents a comprehensive empirical study investigating the relationship between optimizer choice and energy efficiency in neural network training. We conducted 360 controlled experiments across three benchmark datasets (MNIST, CIFAR-10, CIFAR-100) using eight popular optimizers (SGD, Adam, AdamW, RMSprop, Adagrad, Adadelta, Adamax, NAdam) with 15 random seeds each. Using CodeCarbon for precise energy tracking on Apple M1 Pro hardware, we measured training duration, peak memory usage, carbon dioxide emissions, and final model performance. Our findings reveal substantial trade-offs between training speed, accuracy, and environmental impact that vary across datasets and model complexity. We identify AdamW and NAdam as consistently efficient choices, while SGD demonstrates superior performance on complex datasets despite higher emissions. These results provide actionable insights for practitioners seeking to balance performance and sustainability in machine learning workflows.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Nonlinear Responses in PET Bottle Buckling with a Hybrid DeepONet-Transolver Framework</title>
<link>https://arxiv.org/abs/2509.13520</link>
<guid>https://arxiv.org/abs/2509.13520</guid>
<content:encoded><![CDATA[
<div> surrogate modeling, neural networks, partial differential equations, packaging design, finite element analysis <br />
Summary: 
This study introduces a hybrid framework, combining DeepONet and Transolver, to address the challenge of generalizing solutions for Polyethylene Terephthalate (PET) bottle buckling analysis across different geometric domains. The framework predicts nodal displacement fields and reaction forces during top load compression, achieving low errors in displacement fields and reaction forces for various bottle geometries. The model accurately captures buckling behavior and other physical phenomena, making it a scalable and efficient surrogate for design evaluation in computational mechanics. This approach has the potential for multi-task predictions and rapid design assessment, showcasing its applicability in diverse engineering applications. <br /><br /> <div>
arXiv:2509.13520v1 Announce Type: new 
Abstract: Neural surrogates and operator networks for solving partial differential equation (PDE) problems have attracted significant research interest in recent years. However, most existing approaches are limited in their ability to generalize solutions across varying non-parametric geometric domains. In this work, we address this challenge in the context of Polyethylene Terephthalate (PET) bottle buckling analysis, a representative packaging design problem conventionally solved using computationally expensive finite element analysis (FEA). We introduce a hybrid DeepONet-Transolver framework that simultaneously predicts nodal displacement fields and the time evolution of reaction forces during top load compression. Our methodology is evaluated on two families of bottle geometries parameterized by two and four design variables. Training data is generated using nonlinear FEA simulations in Abaqus for 254 unique designs per family. The proposed framework achieves mean relative $L^{2}$ errors of 2.5-13% for displacement fields and approximately 2.4% for time-dependent reaction forces for the four-parameter bottle family. Point-wise error analyses further show absolute displacement errors on the order of $10^{-4}$-$10^{-3}$, with the largest discrepancies confined to localized geometric regions. Importantly, the model accurately captures key physical phenomena, such as buckling behavior, across diverse bottle geometries. These results highlight the potential of our framework as a scalable and computationally efficient surrogate, particularly for multi-task predictions in computational mechanics and applications requiring rapid design evaluation.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AERIS: Argonne Earth Systems Model for Reliable and Skillful Predictions</title>
<link>https://arxiv.org/abs/2509.13523</link>
<guid>https://arxiv.org/abs/2509.13523</guid>
<content:encoded><![CDATA[
<div> diffusion-based methods, Swin diffusion transformer, ensemble calibration, high resolution, weather forecasting<br />
Summary:<br />
The article introduces AERIS, a pixel-level Swin diffusion transformer with 1.3 to 80B parameters designed to improve ensemble calibration in weather forecasting at high resolutions. It also introduces SWiPe, a technique that combines window parallelism with sequence and pipeline parallelism to shard window-based transformers efficiently. AERIS achieves 10.21 ExaFLOPS in mixed precision on the ERA5 dataset, demonstrating impressive weak and strong scaling efficiencies. The model outperforms existing methods and remains stable on seasonal scales, indicating potential for billion-parameter diffusion models in weather and climate prediction.<br /> <div>
arXiv:2509.13523v1 Announce Type: new 
Abstract: Generative machine learning offers new opportunities to better understand complex Earth system dynamics. Recent diffusion-based methods address spectral biases and improve ensemble calibration in weather forecasting compared to deterministic methods, yet have so far proven difficult to scale stably at high resolutions. We introduce AERIS, a 1.3 to 80B parameter pixel-level Swin diffusion transformer to address this gap, and SWiPe, a generalizable technique that composes window parallelism with sequence and pipeline parallelism to shard window-based transformers without added communication cost or increased global batch size. On Aurora (10,080 nodes), AERIS sustains 10.21 ExaFLOPS (mixed precision) and a peak performance of 11.21 ExaFLOPS with $1 \times 1$ patch size on the 0.25{\deg} ERA5 dataset, achieving 95.5% weak scaling efficiency, and 81.6% strong scaling efficiency. AERIS outperforms the IFS ENS and remains stable on seasonal scales to 90 days, highlighting the potential of billion-parameter diffusion models for weather and climate prediction.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-Learning Linear Models for Molecular Property Prediction</title>
<link>https://arxiv.org/abs/2509.13527</link>
<guid>https://arxiv.org/abs/2509.13527</guid>
<content:encoded><![CDATA[
<div> Machine Learning, Chemical Sciences, Explainable AI, Predictive Accuracy, Interpretability
Summary:
LAMeL, a Linear Algorithm for Meta-Learning, is introduced to improve prediction accuracy while maintaining interpretability in chemical property prediction tasks. By leveraging a meta-learning framework, LAMeL identifies shared model parameters across related tasks, enhancing performance from 1.1- to 25-fold compared to standard ridge regression. It outperforms or matches traditional linear methods consistently, making it a reliable tool for chemists seeking accurate and interpretable predictions. The method addresses the challenge of limited high-quality datasets by learning a common functional manifold across tasks, even when data is not shared. This approach bridges the gap between predictive accuracy and human comprehensibility in chemical sciences, supporting the growing demand for explainable AI. LAMeL's ability to deliver performance improvements and preserve interpretability makes it a valuable tool for researchers in search of structure-property relationships in chemical systems. 
<br /><br />Summary: <div>
arXiv:2509.13527v1 Announce Type: new 
Abstract: Chemists in search of structure-property relationships face great challenges due to limited high quality, concordant datasets. Machine learning (ML) has significantly advanced predictive capabilities in chemical sciences, but these modern data-driven approaches have increased the demand for data. In response to the growing demand for explainable AI (XAI) and to bridge the gap between predictive accuracy and human comprehensibility, we introduce LAMeL - a Linear Algorithm for Meta-Learning that preserves interpretability while improving the prediction accuracy across multiple properties. While most approaches treat each chemical prediction task in isolation, LAMeL leverages a meta-learning framework to identify shared model parameters across related tasks, even if those tasks do not share data, allowing it to learn a common functional manifold that serves as a more informed starting point for new unseen tasks. Our method delivers performance improvements ranging from 1.1- to 25-fold over standard ridge regression, depending on the domain of the dataset. While the degree of performance enhancement varies across tasks, LAMeL consistently outperforms or matches traditional linear methods, making it a reliable tool for chemical property prediction where both accuracy and interpretability are critical.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is GPT-4o mini Blinded by its Own Safety Filters? Exposing the Multimodal-to-Unimodal Bottleneck in Hate Speech Detection</title>
<link>https://arxiv.org/abs/2509.13608</link>
<guid>https://arxiv.org/abs/2509.13608</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Multimodal Models, GPT-4o mini, hate speech detection, Unimodal Bottleneck, safety architectures

Summary: 
The paper analyzes OpenAI's GPT-4o mini model on hate speech detection using the Hateful Memes Challenge dataset. It uncovers a critical flaw termed the "Unimodal Bottleneck," where the model's advanced multimodal reasoning is hindered by context-blind safety filters. The study reveals that the safety system is triggered equally by visual and textual content, leading to both high-risk and benign content being blocked. This results in predictable false positives, highlighting the challenge of balancing capability and safety in state-of-the-art Large Multimodal Models. The findings emphasize the importance of implementing more integrated, context-aware alignment strategies to ensure the safe and effective deployment of AI systems.<br /><br />Summary: <div>
arXiv:2509.13608v1 Announce Type: new 
Abstract: As Large Multimodal Models (LMMs) become integral to daily digital life, understanding their safety architectures is a critical problem for AI Alignment. This paper presents a systematic analysis of OpenAI's GPT-4o mini, a globally deployed model, on the difficult task of multimodal hate speech detection. Using the Hateful Memes Challenge dataset, we conduct a multi-phase investigation on 500 samples to probe the model's reasoning and failure modes. Our central finding is the experimental identification of a "Unimodal Bottleneck," an architectural flaw where the model's advanced multimodal reasoning is systematically preempted by context-blind safety filters. A quantitative validation of 144 content policy refusals reveals that these overrides are triggered in equal measure by unimodal visual 50% and textual 50% content. We further demonstrate that this safety system is brittle, blocking not only high-risk imagery but also benign, common meme formats, leading to predictable false positives. These findings expose a fundamental tension between capability and safety in state-of-the-art LMMs, highlighting the need for more integrated, context-aware alignment strategies to ensure AI systems can be deployed both safely and effectively.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Anomaly Detection in ALS EPICS Event Logs</title>
<link>https://arxiv.org/abs/2509.13621</link>
<guid>https://arxiv.org/abs/2509.13621</guid>
<content:encoded><![CDATA[
<div> Keywords: automated fault analysis, Advanced Light Source, EPICS control system, semantic embedding, neural network

Summary: 
An automated fault analysis framework is introduced for the Advanced Light Source (ALS) using real-time event logs from its EPICS control system. Log entries are processed as natural language and transformed into contextual vector representations through semantic embedding techniques. A sequence-aware neural network, trained on normal operation data, assigns real-time anomaly scores to each event, flagging deviations from baseline behavior. This enables operators to rapidly identify critical event sequences that precede complex system failures. The framework utilizes automated processes to streamline fault analysis and improve operational efficiency at the ALS facility. <div>
arXiv:2509.13621v1 Announce Type: new 
Abstract: This paper introduces an automated fault analysis framework for the Advanced Light Source (ALS) that processes real-time event logs from its EPICS control system. By treating log entries as natural language, we transform them into contextual vector representations using semantic embedding techniques. A sequence-aware neural network, trained on normal operational data, assigns a real-time anomaly score to each event. This method flags deviations from baseline behavior, enabling operators to rapidly identify the critical event sequences that precede complex system failures.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Aware In-Context Learning for Large Language Models</title>
<link>https://arxiv.org/abs/2509.13625</link>
<guid>https://arxiv.org/abs/2509.13625</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, privacy-preserving text generation, in-context-learning, information leakage, synthetic text<br />
Summary: 
This study focuses on addressing privacy concerns related to Large Language Models (LLMs) by introducing a private prediction framework that leverages Differential Privacy (DP). The framework ensures strong privacy guarantees without requiring fine-tuning of models. By performing inference on private records and aggregating output distributions, the method generates high-quality synthetic text while minimizing information leakage. A blending operation that combines private and public inference further enhances utility. Empirical evaluations show that the proposed approach surpasses previous methods in in-context-learning tasks. This research offers a promising direction for privacy-preserving text generation, maintaining both privacy guarantees and high utility.<br /><br />Summary: <div>
arXiv:2509.13625v1 Announce Type: new 
Abstract: Large language models (LLMs) have significantly transformed natural language understanding and generation, but they raise privacy concerns due to potential exposure of sensitive information. Studies have highlighted the risk of information leakage, where adversaries can extract sensitive information embedded in the prompts. In this work, we introduce a novel private prediction framework for generating high-quality synthetic text with strong privacy guarantees. Our approach leverages the Differential Privacy (DP) framework to ensure worst-case theoretical bounds on information leakage without requiring any fine-tuning of the underlying models.The proposed method performs inference on private records and aggregates the resulting per-token output distributions. This enables the generation of longer and coherent synthetic text while maintaining privacy guarantees. Additionally, we propose a simple blending operation that combines private and public inference to further enhance utility. Empirical evaluations demonstrate that our approach outperforms previous state-of-the-art methods on in-context-learning (ICL) tasks, making it a promising direction for privacy-preserving text generation while maintaining high utility.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepLogit: A sequentially constrained explainable deep learning modeling approach for transport policy analysis</title>
<link>https://arxiv.org/abs/2509.13633</link>
<guid>https://arxiv.org/abs/2509.13633</guid>
<content:encoded><![CDATA[
<div> CNN, deep learning, policy analysis, transport, interpretability

Summary:
The study presents a novel approach called DeepLogit models, which aim to bridge the gap between deep learning models and policy analysis in transportation. By sequentially constraining the parameters of the deep learning models, the approach maintains interpretability while enhancing accuracy beyond traditional discrete choice models. The method involves estimating a linear-in-parameter CNN model as a baseline, then expanding to more complex models such as Transformers while preserving interpretability where needed. Using real-world transit data from Singapore, the approach demonstrates improved predictive power without sacrificing the interpretability required for policy decisions. By combining the strengths of theory-based discrete choice models and data-driven AI models, this approach shows promise in enhancing model accuracy in planning and policy contexts. The code for the approach is available on GitHub for replication and further development. <div>
arXiv:2509.13633v1 Announce Type: new 
Abstract: Despite the significant progress of deep learning models in multitude of applications, their adaption in planning and policy related areas remains challenging due to the black-box nature of these models. In this work, we develop a set of DeepLogit models that follow a novel sequentially constrained approach in estimating deep learning models for transport policy analysis. In the first step of the proposed approach, we estimate a convolutional neural network (CNN) model with only linear terms, which is equivalent of a linear-in-parameter multinomial logit model. We then estimate other deep learning models by constraining the parameters that need interpretability at the values obtained in the linear-in-parameter CNN model and including higher order terms or by introducing advanced deep learning architectures like Transformers. Our approach can retain the interpretability of the selected parameters, yet provides significantly improved model accuracy than the discrete choice model. We demonstrate our approach on a transit route choice example using real-world transit smart card data from Singapore. This study shows the potential for a unifying approach, where theory-based discrete choice model (DCM) and data-driven AI models can leverage each other's strengths in interpretability and predictive power. With the availability of larger datasets and more complex constructions, such approach can lead to more accurate models using discrete choice models while maintaining its applicability in planning and policy-related areas. Our code is available on https://github.com/jeremyoon/route-choice/ .
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Secure UAV-assisted Federated Learning: A Digital Twin-Driven Approach with Zero-Knowledge Proofs</title>
<link>https://arxiv.org/abs/2509.13634</link>
<guid>https://arxiv.org/abs/2509.13634</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated learning, UAV-assisted systems, Digital Twin technology, Zero-Knowledge Federated Learning, energy efficiency<br />
Summary: <br />
This paper introduces a novel framework that combines Digital Twin (DT) technology and Zero-Knowledge Federated Learning (zkFed) to address challenges in UAV-assisted Federated Learning (FL) systems. UAVs serve as mobile base stations, facilitating decentralized model training while ensuring privacy. The integration of DT technology enables real-time monitoring and predictive maintenance for improved network efficiency. Zero-Knowledge Proofs (ZKPs) enhance security by allowing model verification without exposing sensitive data. A dynamic allocation strategy adjusts UAV flight paths, transmission power, and processing rates based on network conditions to optimize energy efficiency and resource management. By utilizing block coordinate descent and convex optimization techniques, the proposed approach achieves a significant reduction in energy consumption compared to traditional FL methods. Simulation results demonstrate enhanced learning performance, security, and scalability, making this framework a promising solution for future UAV-based intelligent networks. <br /><br />Summary: <div>
arXiv:2509.13634v1 Announce Type: new 
Abstract: Federated learning (FL) has gained popularity as a privacy-preserving method of training machine learning models on decentralized networks. However to ensure reliable operation of UAV-assisted FL systems, issues like as excessive energy consumption, communication inefficiencies, and security vulnerabilities must be solved. This paper proposes an innovative framework that integrates Digital Twin (DT) technology and Zero-Knowledge Federated Learning (zkFed) to tackle these challenges. UAVs act as mobile base stations, allowing scattered devices to train FL models locally and upload model updates for aggregation. By incorporating DT technology, our approach enables real-time system monitoring and predictive maintenance, improving UAV network efficiency. Additionally, Zero-Knowledge Proofs (ZKPs) strengthen security by allowing model verification without exposing sensitive data. To optimize energy efficiency and resource management, we introduce a dynamic allocation strategy that adjusts UAV flight paths, transmission power, and processing rates based on network conditions. Using block coordinate descent and convex optimization techniques, our method significantly reduces system energy consumption by up to 29.6% compared to conventional FL approaches. Simulation results demonstrate improved learning performance, security, and scalability, positioning this framework as a promising solution for next-generation UAV-based intelligent networks.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal signal fusion for stress detection using deep neural networks: a novel approach for converting 1D signals to unified 2D images</title>
<link>https://arxiv.org/abs/2509.13636</link>
<guid>https://arxiv.org/abs/2509.13636</guid>
<content:encoded><![CDATA[
<div> Keywords: physiological signals, stress detection, convolutional neural networks, multimodal, data augmentation

Summary:
This study presents a novel method that transforms multimodal physiological signals such as PPG, GSR, and ACC into structured image matrices to enhance stress detection using CNNs. By fusing these signals into image representations, CNNs can better capture temporal and cross-signal dependencies, leading to improved interpretability and robust data augmentation. The technique also involves systematically reorganizing the fused signals into multiple formats and utilizing a multi-stage training pipeline to enhance generalization and model robustness, resulting in significantly improved classification performance. This approach, while demonstrated in stress detection, has broad applications in domains involving multimodal physiological signals, enabling more accurate, personalized, and real-time health monitoring through wearable technologies.

<br /><br />Summary: <div>
arXiv:2509.13636v1 Announce Type: new 
Abstract: This study introduces a novel method that transforms multimodal physiological signalsphotoplethysmography (PPG), galvanic skin response (GSR), and acceleration (ACC) into 2D image matrices to enhance stress detection using convolutional neural networks (CNNs). Unlike traditional approaches that process these signals separately or rely on fixed encodings, our technique fuses them into structured image representations that enable CNNs to capture temporal and cross signal dependencies more effectively. This image based transformation not only improves interpretability but also serves as a robust form of data augmentation. To further enhance generalization and model robustness, we systematically reorganize the fused signals into multiple formats, combining them in a multi stage training pipeline. This approach significantly boosts classification performance. While demonstrated here in the context of stress detection, the proposed method is broadly applicable to any domain involving multimodal physiological signals, paving the way for more accurate, personalized, and real time health monitoring through wearable technologies.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-I: LLMs are Naturally Interleaved Multimodal Creators</title>
<link>https://arxiv.org/abs/2509.13642</link>
<guid>https://arxiv.org/abs/2509.13642</guid>
<content:encoded><![CDATA[
<div> Reframe interleaved image-text generation tool-use problem flexible dynamic framework LLM-I<br />
LLM-I is a flexible and dynamic framework that reframes interleaved image-text generation as a tool-use problem. It overcomes the limitations of current unified models by allowing a central LLM or MLLM agent to orchestrate a diverse toolkit of specialized visual tools using RL. The agent is trained to select and apply these tools proficiently, leading to state-of-the-art performance on four benchmarks. A hybrid reward system and a novel test-time scaling strategy contribute to further performance gains. The framework is trained on a new dataset using four different model backbones and outperforms existing methods by a large margin. Project page: https://github.com/ByteDance-BandAI/LLM-I.<br /><br />Summary: <div>
arXiv:2509.13642v1 Announce Type: new 
Abstract: We propose LLM-Interleaved (LLM-I), a flexible and dynamic framework that reframes interleaved image-text generation as a tool-use problem. LLM-I is designed to overcome the "one-tool" bottleneck of current unified models, which are limited to synthetic imagery and struggle with tasks requiring factual grounding or programmatic precision. Our framework empowers a central LLM or MLLM agent to intelligently orchestrate a diverse toolkit of specialized visual tools, including online image search, diffusion-based generation, code execution, and image editing. The agent is trained to select and apply these tools proficiently via a Reinforcement Learning (RL) framework that features a hybrid reward system combining rule-based logic with judgments from LLM and MLLM evaluators. Trained on a diverse new dataset using four different model backbones, LLM-I demonstrates state-of-the-art performance, outperforming existing methods by a large margin across four benchmarks. We also introduce a novel test-time scaling strategy that provides further performance gains. Project Page: https://github.com/ByteDance-BandAI/LLM-I.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sequential Data Augmentation for Generative Recommendation</title>
<link>https://arxiv.org/abs/2509.13648</link>
<guid>https://arxiv.org/abs/2509.13648</guid>
<content:encoded><![CDATA[
<div> data augmentation, generative recommendation, personalized systems, training distribution, model generalization

Summary:
- Data augmentation is crucial in generative recommendation for personalized systems, impacting model performance.
- Existing work often lacks a systematic understanding of data augmentation effects.
- Different augmentation strategies can lead to significant performance differences.
- GenPAS is a proposed framework that models data augmentation as a stochastic sampling process with bias control.
- GenPAS outperforms existing strategies in accuracy, data efficiency, and parameter efficiency in both benchmark and industrial datasets.
- This study provides practical guidance for principled training data construction in generative recommendation systems.<br /><br /> <div>
arXiv:2509.13648v1 Announce Type: new 
Abstract: Generative recommendation plays a crucial role in personalized systems, predicting users' future interactions from their historical behavior sequences. A critical yet underexplored factor in training these models is data augmentation, the process of constructing training data from user interaction histories. By shaping the training distribution, data augmentation directly and often substantially affects model generalization and performance. Nevertheless, in much of the existing work, this process is simplified, applied inconsistently, or treated as a minor design choice, without a systematic and principled understanding of its effects.
  Motivated by our empirical finding that different augmentation strategies can yield large performance disparities, we conduct an in-depth analysis of how they reshape training distributions and influence alignment with future targets and generalization to unseen inputs. To systematize this design space, we propose GenPAS, a generalized and principled framework that models augmentation as a stochastic sampling process over input-target pairs with three bias-controlled steps: sequence sampling, target sampling, and input sampling. This formulation unifies widely used strategies as special cases and enables flexible control of the resulting training distribution. Our extensive experiments on benchmark and industrial datasets demonstrate that GenPAS yields superior accuracy, data efficiency, and parameter efficiency compared to existing strategies, providing practical guidance for principled training data construction in generative recommendation.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controllable Pareto Trade-off between Fairness and Accuracy</title>
<link>https://arxiv.org/abs/2509.13651</link>
<guid>https://arxiv.org/abs/2509.13651</guid>
<content:encoded><![CDATA[
<div> trade-off, fairness, accuracy, multi-objective optimization, Controllable Pareto Trade-off

Summary:
The article discusses the fairness-accuracy trade-off in NLP tasks and proposes a Controllable Pareto Trade-off (CPT) approach to provide controllable trade-offs based on user-defined reference vectors. By using multi-objective optimization, CPT can find solutions from various regions of the Pareto front. The method stabilizes fairness updates by using a moving average of stochastic gradients and prunes gradients to keep only critical parameters. Evaluation on hate speech detection and occupation classification tasks shows that CPT outperforms baseline methods in achieving higher-quality solutions on the Pareto front. It also demonstrates better controllability and can precisely follow human-defined reference vectors.<br /><br />Summary: <div>
arXiv:2509.13651v1 Announce Type: new 
Abstract: The fairness-accuracy trade-off is a key challenge in NLP tasks. Current work focuses on finding a single "optimal" solution to balance the two objectives, which is limited considering the diverse solutions on the Pareto front. This work intends to provide controllable trade-offs according to the user's preference of the two objectives, which is defined as a reference vector. To achieve this goal, we apply multi-objective optimization (MOO), which can find solutions from various regions of the Pareto front. However, it is challenging to precisely control the trade-off due to the stochasticity of the training process and the high dimentional gradient vectors. Thus, we propose Controllable Pareto Trade-off (CPT) that can effectively train models to perform different trade-offs according to users' preferences. CPT 1) stabilizes the fairness update with a moving average of stochastic gradients to determine the update direction, and 2) prunes the gradients by only keeping the gradients of the critical parameters. We evaluate CPT on hate speech detection and occupation classification tasks. Experiments show that CPT can achieve a higher-quality set of solutions on the Pareto front than the baseline methods. It also exhibits better controllability and can precisely follow the human-defined reference vectors.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RF-LSCM: Pushing Radiance Fields to Multi-Domain Localized Statistical Channel Modeling for Cellular Network Optimization</title>
<link>https://arxiv.org/abs/2509.13686</link>
<guid>https://arxiv.org/abs/2509.13686</guid>
<content:encoded><![CDATA[
<div> radiance field, multi-domain LSCM, Attenuation Model, point-cloud-aided method, Hierarchical Tensor Angular Modeling 

Summary: 
RF-LSCM is a novel framework for localized wireless channel modeling in cellular networks. It addresses limitations of traditional methods by jointly representing signal attenuation and multipath components within a radiance field. By incorporating a physics-informed Attenuation Model and a point-cloud-aided method, RF-LSCM enables multi-cell and multi-grid channel modeling with cross-frequency generalization. The framework also introduces a Hierarchical Tensor Angular Modeling algorithm for computational efficiency, reducing GPU memory requirements and training time while maintaining accuracy. Extensive experiments on real-world datasets show that RF-LSCM outperforms existing methods, achieving significant improvements in coverage prediction and multi-frequency data fusion. <div>
arXiv:2509.13686v1 Announce Type: new 
Abstract: Accurate localized wireless channel modeling is a cornerstone of cellular network optimization, enabling reliable prediction of network performance during parameter tuning. Localized statistical channel modeling (LSCM) is the state-of-the-art channel modeling framework tailored for cellular network optimization. However, traditional LSCM methods, which infer the channel's Angular Power Spectrum (APS) from Reference Signal Received Power (RSRP) measurements, suffer from critical limitations: they are typically confined to single-cell, single-grid and single-carrier frequency analysis and fail to capture complex cross-domain interactions. To overcome these challenges, we propose RF-LSCM, a novel framework that models the channel APS by jointly representing large-scale signal attenuation and multipath components within a radiance field. RF-LSCM introduces a multi-domain LSCM formulation with a physics-informed frequency-dependent Attenuation Model (FDAM) to facilitate the cross frequency generalization as well as a point-cloud-aided environment enhanced method to enable multi-cell and multi-grid channel modeling. Furthermore, to address the computational inefficiency of typical neural radiance fields, RF-LSCM leverages a low-rank tensor representation, complemented by a novel Hierarchical Tensor Angular Modeling (HiTAM) algorithm. This efficient design significantly reduces GPU memory requirements and training time while preserving fine-grained accuracy. Extensive experiments on real-world multi-cell datasets demonstrate that RF-LSCM significantly outperforms state-of-the-art methods, achieving up to a 30% reduction in mean absolute error (MAE) for coverage prediction and a 22% MAE improvement by effectively fusing multi-frequency data.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Conformal Prediction Framework for Uncertainty Quantification in Physics-Informed Neural Networks</title>
<link>https://arxiv.org/abs/2509.13717</link>
<guid>https://arxiv.org/abs/2509.13717</guid>
<content:encoded><![CDATA[
<div> Neural Networks, Physics-Informed, Uncertainty Quantification, Distribution-Free, Conformal Prediction <br />
Summary: <br />
This work introduces a distribution-free conformal prediction framework for uncertainty quantification in Physics-Informed Neural Networks (PINNs). The framework calibrates prediction intervals using nonconformity scores on a calibration set, providing rigorous finite-sample coverage guarantees. To address spatial heteroskedasticity, local conformal quantile estimation is introduced, allowing for spatially adaptive uncertainty bands with theoretical guarantees. Through evaluations on various PDEs, including damped harmonic oscillator and Poisson equations, the proposed framework demonstrates reliable calibration and locally adaptive uncertainty intervals, surpassing heuristic UQ approaches. By bridging PINNs with distribution-free uncertainty quantification, this work enhances calibration, reliability, and opens up new possibilities for uncertainty-aware modeling in complex PDE systems. <br /> <div>
arXiv:2509.13717v1 Announce Type: new 
Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework for solving PDEs, yet existing uncertainty quantification (UQ) approaches for PINNs generally lack rigorous statistical guarantees. In this work, we bridge this gap by introducing a distribution-free conformal prediction (CP) framework for UQ in PINNs. This framework calibrates prediction intervals by constructing nonconformity scores on a calibration set, thereby yielding distribution-free uncertainty estimates with rigorous finite-sample coverage guarantees for PINNs. To handle spatial heteroskedasticity, we further introduce local conformal quantile estimation, enabling spatially adaptive uncertainty bands while preserving theoretical guarantee. Through systematic evaluations on typical PDEs (damped harmonic oscillator, Poisson, Allen-Cahn, and Helmholtz equations) and comprehensive testing across multiple uncertainty metrics, our results demonstrate that the proposed framework achieves reliable calibration and locally adaptive uncertainty intervals, consistently outperforming heuristic UQ approaches. By bridging PINNs with distribution-free UQ, this work introduces a general framework that not only enhances calibration and reliability, but also opens new avenues for uncertainty-aware modeling of complex PDE systems.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WatchAnxiety: A Transfer Learning Approach for State Anxiety Prediction from Smartwatch Data</title>
<link>https://arxiv.org/abs/2509.13725</link>
<guid>https://arxiv.org/abs/2509.13725</guid>
<content:encoded><![CDATA[
<div> Keywords: Social Anxiety, Ecological Momentary Assessments, Smartwatch, Just-In-Time Adaptive Interventions, Machine Learning

Summary: 
- The study focused on measuring fluctuations in state anxiety throughout the day among socially anxious college students using a smartwatch-based system and ecological momentary assessments.
- A base model was developed using external heart rate data and fine-tuned to predict state anxiety, achieving a balanced accuracy of 60.4% in the study dataset.
- The training approach was also applied to a separate hold-out set from the TILES-18 dataset, achieving a balanced accuracy of 59.1% and outperforming previous work by at least 7%.
- The study highlights the potential for machine learning algorithms to accurately predict state anxiety levels in real-time, which could inform the development of personalized interventions for individuals with social anxiety.
- These findings have implications for designing Just-In-Time Adaptive Interventions that could help individuals manage their social anxiety more effectively. 

<br /><br />Summary: <div>
arXiv:2509.13725v1 Announce Type: new 
Abstract: Social anxiety is a common mental health condition linked to significant challenges in academic, social, and occupational functioning. A core feature is elevated momentary (state) anxiety in social situations, yet little prior work has measured or predicted fluctuations in this anxiety throughout the day. Capturing these intra-day dynamics is critical for designing real-time, personalized interventions such as Just-In-Time Adaptive Interventions (JITAIs). To address this gap, we conducted a study with socially anxious college students (N=91; 72 after exclusions) using our custom smartwatch-based system over an average of 9.03 days (SD = 2.95). Participants received seven ecological momentary assessments (EMAs) per day to report state anxiety. We developed a base model on over 10,000 days of external heart rate data, transferred its representations to our dataset, and fine-tuned it to generate probabilistic predictions. These were combined with trait-level measures in a meta-learner. Our pipeline achieved 60.4% balanced accuracy in state anxiety detection in our dataset. To evaluate generalizability, we applied the training approach to a separate hold-out set from the TILES-18 dataset-the same dataset used for pretraining. On 10,095 once-daily EMAs, our method achieved 59.1% balanced accuracy, outperforming prior work by at least 7%.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>State Space Models over Directed Graphs</title>
<link>https://arxiv.org/abs/2509.13735</link>
<guid>https://arxiv.org/abs/2509.13735</guid>
<content:encoded><![CDATA[
<div> Graph state space models, directed graphs, graph neural networks, causal dependencies, message-passing mechanism<br />
Summary:<br />
- The article introduces DirEgo2Token, a method for sequentializing directed graphs using k-hop ego graphs. This approach extends state space models to directed graph learning for the first time.
- DirGraphSSM is a novel directed graph neural network architecture that implements state space models on directed graphs through message-passing.
- Experimental results show that DirGraphSSM achieves state-of-the-art performance on three directed graph learning tasks and competitive performance on two additional tasks.
- The model also demonstrates 1.5x to 2x training speed improvements compared to existing state-of-the-art models.
- This research addresses challenges in capturing long-range causal dependencies in directed graphs and balancing accuracy and training efficiency for large-scale graph datasets. <br /> 
Summary: <div>
arXiv:2509.13735v1 Announce Type: new 
Abstract: Directed graphs are ubiquitous across numerous domains, where the directionality of edges encodes critical causal dependencies. However, existing GNNs and graph Transformers tailored for directed graphs face two major challenges: (1) effectively capturing long-range causal dependencies derived from directed edges; (2) balancing accuracy and training efficiency when processing large-scale graph datasets. In recent years, state space models (SSMs) have achieved substantial progress in causal sequence tasks, and their variants designed for graphs have demonstrated state-of-the-art accuracy while maintaining high efficiency across various graph learning benchmarks. However, existing graph state space models are exclusively designed for undirected graphs, which limits their performance in directed graph learning. To this end, we propose an innovative approach DirEgo2Token which sequentializes directed graphs via k-hop ego graphs. This marks the first systematic extension of state space models to the field of directed graph learning. Building upon this, we develop DirGraphSSM, a novel directed graph neural network architecture that implements state space models on directed graphs via the message-passing mechanism. Experimental results demonstrate that DirGraphSSM achieves state-of-the-art performance on three representative directed graph learning tasks while attaining competitive performance on two additional tasks with 1.5$\times $ to 2$\times $ training speed improvements compared to existing state-of-the-art models.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ParaAegis: Parallel Protection for Flexible Privacy-preserved Federated Learning</title>
<link>https://arxiv.org/abs/2509.13739</link>
<guid>https://arxiv.org/abs/2509.13739</guid>
<content:encoded><![CDATA[
<div> framework, federated learning, differential privacy, homomorphic encryption, privacy-utility-efficiency balance

Summary:
ParaAegis is introduced as a parallel protection framework for federated learning, addressing the trade-off between model utility and computational efficiency. The core innovation lies in a model partitioning scheme that applies differential privacy to the less critical portion and homomorphic encryption to the rest. This allows for a tunable system, enabling flexible control over the privacy-utility-efficiency balance. The distributed voting mechanism ensures consensus on the partitioning, leading to adjustments between efficiency and utility with the same level of privacy. Theoretical analysis confirms the feasibility of balancing efficiency and utility. Experimental results show that by adjusting hyperparameters, ParaAegis enables practitioners to prioritize between model accuracy and training time based on their specific requirements. <br /><br />Summary: <div>
arXiv:2509.13739v1 Announce Type: new 
Abstract: Federated learning (FL) faces a critical dilemma: existing protection mechanisms like differential privacy (DP) and homomorphic encryption (HE) enforce a rigid trade-off, forcing a choice between model utility and computational efficiency. This lack of flexibility hinders the practical implementation. To address this, we introduce ParaAegis, a parallel protection framework designed to give practitioners flexible control over the privacy-utility-efficiency balance. Our core innovation is a strategic model partitioning scheme. By applying lightweight DP to the less critical, low norm portion of the model while protecting the remainder with HE, we create a tunable system. A distributed voting mechanism ensures consensus on this partitioning. Theoretical analysis confirms the adjustments between efficiency and utility with the same privacy. Crucially, the experimental results demonstrate that by adjusting the hyperparameters, our method enables flexible prioritization between model accuracy and training time.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ST-LINK: Spatially-Aware Large Language Models for Spatio-Temporal Forecasting</title>
<link>https://arxiv.org/abs/2509.13753</link>
<guid>https://arxiv.org/abs/2509.13753</guid>
<content:encoded><![CDATA[
<div> Keywords: Traffic forecasting, Large Language Models, Spatially-Enhanced Attention, Memory Retrieval Feed-Forward Network, Spatio-temporal dependencies

Summary: 
ST-LINK is introduced as a novel framework for enhancing Large Language Models' ability to capture spatio-temporal dependencies in traffic forecasting. It addresses the limitations of LLMs in modeling spatial relationships and their compatibility with graph-structured spatial data using Spatially-Enhanced Attention and the Memory Retrieval Feed-Forward Network. SE-Attention integrates spatial correlations within the attention mechanism, maximizing spatial learning while maintaining the LLM's sequential structure. MRFFN dynamically retrieves historical patterns to capture temporal dependencies and improve long-term forecasting stability. Experimental results show that ST-LINK outperforms conventional deep learning and LLM approaches, effectively capturing both regular traffic patterns and sudden changes.

<br /><br />Summary: ST-LINK is a novel framework that enhances Large Language Models' ability to capture spatio-temporal dependencies in traffic forecasting by utilizing Spatially-Enhanced Attention and Memory Retrieval Feed-Forward Network. It overcomes the limitations of LLMs in modeling spatial relationships and graph-structured data compatibility, outperforming traditional methods and effectively capturing both regular traffic patterns and abrupt changes. <div>
arXiv:2509.13753v1 Announce Type: new 
Abstract: Traffic forecasting represents a crucial problem within intelligent transportation systems. In recent research, Large Language Models (LLMs) have emerged as a promising method, but their intrinsic design, tailored primarily for sequential token processing, introduces notable challenges in effectively capturing spatial dependencies. Specifically, the inherent limitations of LLMs in modeling spatial relationships and their architectural incompatibility with graph-structured spatial data remain largely unaddressed. To overcome these limitations, we introduce ST-LINK, a novel framework that enhances the capability of Large Language Models to capture spatio-temporal dependencies. Its key components are Spatially-Enhanced Attention (SE-Attention) and the Memory Retrieval Feed-Forward Network (MRFFN). SE-Attention extends rotary position embeddings to integrate spatial correlations as direct rotational transformations within the attention mechanism. This approach maximizes spatial learning while preserving the LLM's inherent sequential processing structure. Meanwhile, MRFFN dynamically retrieves and utilizes key historical patterns to capture complex temporal dependencies and improve the stability of long-term forecasting. Comprehensive experiments on benchmark datasets demonstrate that ST-LINK surpasses conventional deep learning and LLM approaches, and effectively captures both regular traffic patterns and abrupt changes.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Correlation: Causal Multi-View Unsupervised Feature Selection Learning</title>
<link>https://arxiv.org/abs/2509.13763</link>
<guid>https://arxiv.org/abs/2509.13763</guid>
<content:encoded><![CDATA[
<div> causal, multi-view, unsupervised, feature selection, confounders
<br />
Summary:<br />
The paper introduces a new method for multi-view unsupervised feature selection (MUFS) called CAUSA. It analyzes existing MUFS methods from a causal perspective, highlighting the issue of selecting irrelevant features due to spurious correlations caused by confounders. CAUSA addresses this by utilizing a causal regularization module to separate confounders and learn sample weights for balancing distributions. This approach enhances the selection of causally informative features, outperforming current state-of-the-art methods in comprehensive experiments. This study represents the first in-depth exploration of causal multi-view feature selection in the unsupervised setting. <div>
arXiv:2509.13763v1 Announce Type: new 
Abstract: Multi-view unsupervised feature selection (MUFS) has recently received increasing attention for its promising ability in dimensionality reduction on multi-view unlabeled data. Existing MUFS methods typically select discriminative features by capturing correlations between features and clustering labels. However, an important yet underexplored question remains: \textit{Are such correlations sufficiently reliable to guide feature selection?} In this paper, we analyze MUFS from a causal perspective by introducing a novel structural causal model, which reveals that existing methods may select irrelevant features because they overlook spurious correlations caused by confounders. Building on this causal perspective, we propose a novel MUFS method called CAusal multi-view Unsupervised feature Selection leArning (CAUSA). Specifically, we first employ a generalized unsupervised spectral regression model that identifies informative features by capturing dependencies between features and consensus clustering labels. We then introduce a causal regularization module that can adaptively separate confounders from multi-view data and simultaneously learn view-shared sample weights to balance confounder distributions, thereby mitigating spurious correlations. Thereafter, integrating both into a unified learning framework enables CAUSA to select causally informative features. Comprehensive experiments demonstrate that CAUSA outperforms several state-of-the-art methods. To our knowledge, this is the first in-depth study of causal multi-view feature selection in the unsupervised setting.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Floating-Body Hydrodynamic Neural Networks</title>
<link>https://arxiv.org/abs/2509.13783</link>
<guid>https://arxiv.org/abs/2509.13783</guid>
<content:encoded><![CDATA[
<div> Fluid-structure interaction, Hydrodynamic parameters, Neural networks, Interpretability, System identification
<br />
Summary:
Floating-Body Hydrodynamic Neural Networks (FHNN) present a framework that predicts hydrodynamic parameters like added masses and drag coefficients while integrating them with equations of motion. This approach enhances interpretability and stability in predicting floating-body motion, bridging the gap between traditional models and black-box neural networks. FHNN outperforms Neural ODEs on synthetic vortex datasets, achieving lower errors and recovering physically consistent flow fields. Compared to Hamiltonian and Lagrangian neural networks, FHNN effectively handles dissipative dynamics while maintaining interpretability. This study demonstrates the potential for FHNN to improve understanding and prediction of fluid-structure interaction in engineering and natural systems. 
<br /><br /> <div>
arXiv:2509.13783v1 Announce Type: new 
Abstract: Fluid-structure interaction is common in engineering and natural systems, where floating-body motion is governed by added mass, drag, and background flows. Modeling these dissipative dynamics is difficult: black-box neural models regress state derivatives with limited interpretability and unstable long-horizon predictions. We propose Floating-Body Hydrodynamic Neural Networks (FHNN), a physics-structured framework that predicts interpretable hydrodynamic parameters such as directional added masses, drag coefficients, and a streamfunction-based flow, and couples them with analytic equations of motion. This design constrains the hypothesis space, enhances interpretability, and stabilizes integration. On synthetic vortex datasets, FHNN achieves up to an order-of-magnitude lower error than Neural ODEs, recovers physically consistent flow fields. Compared with Hamiltonian and Lagrangian neural networks, FHNN more effectively handles dissipative dynamics while preserving interpretability, which bridges the gap between black-box learning and transparent system identification.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Physics Foundation Model</title>
<link>https://arxiv.org/abs/2509.13805</link>
<guid>https://arxiv.org/abs/2509.13805</guid>
<content:encoded><![CDATA[
<div> Foundation models, Physics Foundation Model, General Physics Transformer, simulation data, transformers<br />
<br />Summary:
The article introduces the concept of a Physics Foundation Model (PFM) that could revolutionize scientific simulations by eliminating the need for specialized solver development. The General Physics Transformer (GPhyT) is presented as a model trained on diverse simulation data, showcasing its ability to simulate various physics domains without prior knowledge of the underlying equations. GPhyT outperforms specialized architectures, achieves zero-shot generalization to unseen systems, and provides stable long-term predictions through 50-timestep rollouts. This work demonstrates the feasibility of a universal PFM that can learn generalizable physical principles from data alone, potentially transforming computational science and engineering. <div>
arXiv:2509.13805v1 Announce Type: new 
Abstract: Foundation models have revolutionized natural language processing through a ``train once, deploy anywhere'' paradigm, where a single pre-trained model adapts to countless downstream tasks without retraining. Access to a Physics Foundation Model (PFM) would be transformative -- democratizing access to high-fidelity simulations, accelerating scientific discovery, and eliminating the need for specialized solver development. Yet current physics-aware machine learning approaches remain fundamentally limited to single, narrow domains and require retraining for each new system. We present the General Physics Transformer (GPhyT), trained on 1.8 TB of diverse simulation data, that demonstrates foundation model capabilities are achievable for physics. Our key insight is that transformers can learn to infer governing dynamics from context, enabling a single model to simulate fluid-solid interactions, shock waves, thermal convection, and multi-phase dynamics without being told the underlying equations. GPhyT achieves three critical breakthroughs: (1) superior performance across multiple physics domains, outperforming specialized architectures by up to 29x, (2) zero-shot generalization to entirely unseen physical systems through in-context learning, and (3) stable long-term predictions through 50-timestep rollouts. By establishing that a single model can learn generalizable physical principles from data alone, this work opens the path toward a universal PFM that could transform computational science and engineering.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Quantum-Classical Neural Networks for Few-Shot Credit Risk Assessment</title>
<link>https://arxiv.org/abs/2509.13818</link>
<guid>https://arxiv.org/abs/2509.13818</guid>
<content:encoded><![CDATA[
<div> Machine Learning, Quantum Computing, Credit Risk Assessment, Financial Data, Quantum Neural Network

Summary:<br />
- Quantum Machine Learning (QML) is used to address the challenge of few-shot credit risk assessment, a critical issue in finance with data scarcity.
- A hybrid quantum-classical workflow is designed, combining classical machine learning models for feature engineering and a Quantum Neural Network (QNN) as the core classifier.
- The QNN trained via the parameter-shift rule achieved a robust average AUC of 0.852 in simulations and an impressive AUC of 0.88 in hardware experiments.
- The performance of the QNN surpassed classical benchmarks, particularly excelling in the recall metric, on a real-world credit dataset.
- This study showcases the potential of quantum computing in data-constrained financial scenarios during the NISQ era, offering a pragmatic blueprint for inclusive finance applications. 

<br /><br />Summary: <div>
arXiv:2509.13818v1 Announce Type: new 
Abstract: Quantum Machine Learning (QML) offers a new paradigm for addressing complex financial problems intractable for classical methods. This work specifically tackles the challenge of few-shot credit risk assessment, a critical issue in inclusive finance where data scarcity and imbalance limit the effectiveness of conventional models. To address this, we design and implement a novel hybrid quantum-classical workflow. The methodology first employs an ensemble of classical machine learning models (Logistic Regression, Random Forest, XGBoost) for intelligent feature engineering and dimensionality reduction. Subsequently, a Quantum Neural Network (QNN), trained via the parameter-shift rule, serves as the core classifier. This framework was evaluated through numerical simulations and deployed on the Quafu Quantum Cloud Platform's ScQ-P21 superconducting processor. On a real-world credit dataset of 279 samples, our QNN achieved a robust average AUC of 0.852 +/- 0.027 in simulations and yielded an impressive AUC of 0.88 in the hardware experiment. This performance surpasses a suite of classical benchmarks, with a particularly strong result on the recall metric. This study provides a pragmatic blueprint for applying quantum computing to data-constrained financial scenarios in the NISQ era and offers valuable empirical evidence supporting its potential in high-stakes applications like inclusive finance.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An End-to-End Differentiable, Graph Neural Network-Embedded Pore Network Model for Permeability Prediction</title>
<link>https://arxiv.org/abs/2509.13841</link>
<guid>https://arxiv.org/abs/2509.13841</guid>
<content:encoded><![CDATA[
<div> Graph Neural Network, Pore Network Model, Permeability Prediction, Porous Media, End-to-end Training

Summary:
- Accurate prediction of permeability in porous media is crucial for subsurface flow modeling.
- Data-driven models lack generalization across scales, while Pore Network Models (PNMs) rely on idealized geometric assumptions.
- An end-to-end differentiable hybrid framework is proposed, combining a Graph Neural Network (GNN) with a PNM.
- The GNN predicts conductances based on pore and throat features, improving accuracy without idealized assumptions.
- The model is trained using a single scalar permeability target, achieving high accuracy and generalization across scales.
- The framework enables fully coupled, end-to-end training by backpropagating gradients through the GNN and PNM solver.
- Gradient-based sensitivity analysis enhances model interpretability by revealing physically consistent feature influences.
- This approach offers a scalable and physically informed framework for permeability prediction, reducing uncertainty and improving accuracy.

<br /><br />Summary: <div>
arXiv:2509.13841v1 Announce Type: new 
Abstract: Accurate prediction of permeability in porous media is essential for modeling subsurface flow. While pure data-driven models offer computational efficiency, they often lack generalization across scales and do not incorporate explicit physical constraints. Pore network models (PNMs), on the other hand, are physics-based and efficient but rely on idealized geometric assumptions to estimate pore-scale hydraulic conductance, limiting their accuracy in complex structures. To overcome these limitations, we present an end-to-end differentiable hybrid framework that embeds a graph neural network (GNN) into a PNM. In this framework, the analytical formulas used for conductance calculations are replaced by GNN-based predictions derived from pore and throat features. The predicted conductances are then passed to the PNM solver for permeability computation. In this way, the model avoids the idealized geometric assumptions of PNM while preserving the physics-based flow calculations. The GNN is trained without requiring labeled conductance data, which can number in the thousands per pore network; instead, it learns conductance values by using a single scalar permeability as the training target. This is made possible by backpropagating gradients through both the GNN (via automatic differentiation) and the PNM solver (via a discrete adjoint method), enabling fully coupled, end-to-end training. The resulting model achieves high accuracy and generalizes well across different scales, outperforming both pure data-driven and traditional PNM approaches. Gradient-based sensitivity analysis further reveals physically consistent feature influences, enhancing model interpretability. This approach offers a scalable and physically informed framework for permeability prediction in complex porous media, reducing model uncertainty and improving accuracy.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-Regularized Learning of Gaussian Mixture Models</title>
<link>https://arxiv.org/abs/2509.13855</link>
<guid>https://arxiv.org/abs/2509.13855</guid>
<content:encoded><![CDATA[
<div> Gaussian Mixture Models; Graph-regularized learning; Distributed settings; Parameter sharing; Heterogeneous data

Summary:
- The article introduces a method for learning Gaussian Mixture Models (GMMs) in distributed settings with limited and heterogeneous local data.
- It utilizes a similarity graph to guide the sharing of parameters among nodes, eliminating the need to transfer raw data.
- This approach allows for flexible aggregation of parameters from neighboring nodes.
- The proposed model demonstrates superior performance compared to both centrally trained and locally trained GMMs, especially in scenarios with low-sample and heterogeneous data.
- By leveraging graph regularization, the method enhances the learning process by effectively utilizing the available data resources across distributed nodes.<br /><br />Summary: <div>
arXiv:2509.13855v1 Announce Type: new 
Abstract: We present a graph-regularized learning of Gaussian Mixture Models (GMMs) in distributed settings with heterogeneous and limited local data. The method exploits a provided similarity graph to guide parameter sharing among nodes, avoiding the transfer of raw data. The resulting model allows for flexible aggregation of neighbors' parameters and outperforms both centralized and locally trained GMMs in heterogeneous, low-sample regimes.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Masked Diffusion Models as Energy Minimization</title>
<link>https://arxiv.org/abs/2509.13866</link>
<guid>https://arxiv.org/abs/2509.13866</guid>
<content:encoded><![CDATA[
<div> masked diffusion models, energy minimization, discrete optimal transport, Beta distributions, sampling improvement <br />
Summary:
This study introduces a theoretical framework that interprets masked diffusion models (MDMs) as solutions to energy minimization problems in discrete optimal transport. The research demonstrates that MDMs minimize three distinct energy formulations  kinetic, conditional kinetic, and geodesic energy  when the mask schedule meets a closed-form optimality condition. By parameterizing interpolation schedules using Beta distributions, the study enables efficient post-training tuning without modifying the model, resulting in improved sampling performance. Experimental results on synthetic and real-world benchmarks show that the energy-inspired schedules outperform hand-crafted baselines, especially in low-step sampling scenarios. This unification not only clarifies the theoretical foundations of MDMs but also motivates practical enhancements in sampling strategies. <br /><br />Summary: <div>
arXiv:2509.13866v1 Announce Type: new 
Abstract: We present a systematic theoretical framework that interprets masked diffusion models (MDMs) as solutions to energy minimization problems in discrete optimal transport. Specifically, we prove that three distinct energy formulations--kinetic, conditional kinetic, and geodesic energy--are mathematically equivalent under the structure of MDMs, and that MDMs minimize all three when the mask schedule satisfies a closed-form optimality condition. This unification not only clarifies the theoretical foundations of MDMs, but also motivates practical improvements in sampling. By parameterizing interpolation schedules via Beta distributions, we reduce the schedule design space to a tractable 2D search, enabling efficient post-training tuning without model modification. Experiments on synthetic and real-world benchmarks demonstrate that our energy-inspired schedules outperform hand-crafted baselines, particularly in low-step sampling settings.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedSSG: Expectation-Gated and History-Aware Drift Alignment for Federated Learning</title>
<link>https://arxiv.org/abs/2509.13895</link>
<guid>https://arxiv.org/abs/2509.13895</guid>
<content:encoded><![CDATA[
<div> client drift, partial participation, federated learning, FedSSG, history-aware drift alignment<br />
<br />
Summary: <br />
FedSSG is introduced as a method to address issues of client drift and inconsistent local optima in federated learning due to non-IID data and partial participation. It maintains a per-client drift memory to accumulate local model differences and uses a gating function based on participation statistics to align drift and optimize convergence. By leveraging sampling statistics as a phase control, FedSSG outperforms existing baselines and accelerates convergence on benchmarks such as CIFAR-10/100 with improved test accuracy. The method is lightweight, adding minimal client memory and constant-time gate, and gracefully adapts to different sampling scenarios. FedSSG demonstrates the potential of using sampling statistics as a principled approach to stabilize and enhance federated training. <div>
arXiv:2509.13895v1 Announce Type: new 
Abstract: Non-IID data and partial participation induce client drift and inconsistent local optima in federated learning, causing unstable convergence and accuracy loss. We present FedSSG, a stochastic sampling-guided, history-aware drift alignment method. FedSSG maintains a per-client drift memory that accumulates local model differences as a lightweight sketch of historical gradients; crucially, it gates both the memory update and the local alignment term by a smooth function of the observed/expected participation ratio (a phase-by-expectation signal derived from the server sampler). This statistically grounded gate stays weak and smooth when sampling noise dominates early, then strengthens once participation statistics stabilize, contracting the local-global gap without extra communication. Across CIFAR-10/100 with 100/500 clients and 2-15 percent participation, FedSSG consistently outperforms strong drift-aware baselines and accelerates convergence; on our benchmarks it improves test accuracy by up to a few points (e.g., about +0.9 on CIFAR-10 and about +2.7 on CIFAR-100 on average over the top-2 baseline) and yields about 4.5x faster target-accuracy convergence on average. The method adds only O(d) client memory and a constant-time gate, and degrades gracefully to a mild regularizer under near-IID or uniform sampling. FedSSG shows that sampling statistics can be turned into a principled, history-aware phase control to stabilize and speed up federated training.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TFMAdapter: Lightweight Instance-Level Adaptation of Foundation Models for Forecasting with Covariates</title>
<link>https://arxiv.org/abs/2509.13906</link>
<guid>https://arxiv.org/abs/2509.13906</guid>
<content:encoded><![CDATA[
<div> Time Series Foundation Models, TSFMs, covariates, TFMAdapter, forecasting
<br />
Summary:
Time Series Foundation Models have excelled in univariate forecasting by learning from diverse domains and temporal patterns. TFMAdapter enhances TSFMs by incorporating covariate information without requiring fine-tuning, resulting in improved forecasting accuracy. TFMAdapter utilizes a two-stage method involving generating pseudo-forecasts and refining predictions with a Gaussian Process regressor. Experimental results demonstrate TFMAdapter outperforms both foundation models and supervised baselines by 24-27%. This lightweight adapter bridges the gap between generic foundation models and domain-specific forecasting needs, showcasing its potential in enhancing forecasting accuracy efficiently. 
<br /><br /> <div>
arXiv:2509.13906v1 Announce Type: new 
Abstract: Time Series Foundation Models (TSFMs) have recently achieved state-of-the-art performance in univariate forecasting on new time series simply by conditioned on a brief history of past values. Their success demonstrates that large-scale pretraining across diverse domains can acquire the inductive bias to generalize from temporal patterns in a brief history. However, most TSFMs are unable to leverage covariates -- future-available exogenous variables critical for accurate forecasting in many applications -- due to their domain-specific nature and the lack of associated inductive bias. We propose TFMAdapter, a lightweight, instance-level adapter that augments TSFMs with covariate information without fine-tuning. Instead of retraining, TFMAdapter operates on the limited history provided during a single model call, learning a non-parametric cascade that combines covariates with univariate TSFM forecasts. However, such learning would require univariate forecasts at all steps in the history, requiring too many calls to the TSFM. To enable training on the full historical context while limiting TSFM invocations, TFMAdapter uses a two-stage method: (1) generating pseudo-forecasts with a simple regression model, and (2) training a Gaussian Process regressor to refine predictions using both pseudo- and TSFM forecasts alongside covariates. Extensive experiments on real-world datasets demonstrate that TFMAdapter consistently outperforms both foundation models and supervised baselines, achieving a 24-27\% improvement over base foundation models with minimal data and computational overhead. Our results highlight the potential of lightweight adapters to bridge the gap between generic foundation models and domain-specific forecasting needs.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>APFEx: Adaptive Pareto Front Explorer for Intersectional Fairness</title>
<link>https://arxiv.org/abs/2509.13908</link>
<guid>https://arxiv.org/abs/2509.13908</guid>
<content:encoded><![CDATA[
<div> Keywords: fairness, machine learning, intersectional, adaptive, optimization 

Summary: 
Adaptive Pareto Front Explorer (APFEx) is introduced as a framework to address fairness in machine learning models, particularly focusing on intersectional biases. It combines an adaptive multi-objective optimizer, differentiable intersectional fairness metrics, and theoretical convergence guarantees to optimize fairness-accuracy trade-offs. By modeling intersectional fairness as a joint optimization problem over multiple sensitive attributes, APFEx outperforms existing methods in reducing fairness violations while maintaining competitive accuracy. The framework offers a scalable and model-agnostic solution for addressing intersectional biases in machine learning models. <div>
arXiv:2509.13908v1 Announce Type: new 
Abstract: Ensuring fairness in machine learning models is critical, especially when biases compound across intersecting protected attributes like race, gender, and age. While existing methods address fairness for single attributes, they fail to capture the nuanced, multiplicative biases faced by intersectional subgroups. We introduce Adaptive Pareto Front Explorer (APFEx), the first framework to explicitly model intersectional fairness as a joint optimization problem over the Cartesian product of sensitive attributes. APFEx combines three key innovations- (1) an adaptive multi-objective optimizer that dynamically switches between Pareto cone projection, gradient weighting, and exploration strategies to navigate fairness-accuracy trade-offs, (2) differentiable intersectional fairness metrics enabling gradient-based optimization of non-smooth subgroup disparities, and (3) theoretical guarantees of convergence to Pareto-optimal solutions. Experiments on four real-world datasets demonstrate APFEx's superiority, reducing fairness violations while maintaining competitive accuracy. Our work bridges a critical gap in fair ML, providing a scalable, model-agnostic solution for intersectional fairness.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ensemble of Pre-Trained Models for Long-Tailed Trajectory Prediction</title>
<link>https://arxiv.org/abs/2509.13914</link>
<guid>https://arxiv.org/abs/2509.13914</guid>
<content:encoded><![CDATA[
<div> Keywords: ensemble modeling, trajectory prediction, autonomous driving, deep learning, urban environments

Summary: 
Ensemble modeling is applied to enhance trajectory prediction for vehicles in urban settings without the need for re-training expensive models. The study combines state-of-the-art deep learning models with a simple confidence-weighted average method, resulting in a 10% performance improvement over the best standalone model. Surprisingly, this approach proves effective without retraining or fine-tuning the models. The performance boost is particularly significant in long-tailed metrics, benefiting both NuScenes and Argoverse datasets. The improvement in prediction accuracy is consistent across the dataset distribution. The code for this study is open source, allowing for transparent and reproducible research practices. <div>
arXiv:2509.13914v1 Announce Type: new 
Abstract: This work explores the application of ensemble modeling to the multidimensional regression problem of trajectory prediction for vehicles in urban environments. As newer and bigger state-of-the-art prediction models for autonomous driving continue to emerge, an important open challenge is the problem of how to combine the strengths of these big models without the need for costly re-training. We show how, perhaps surprisingly, combining state-of-the-art deep learning models out-of-the-box (without retraining or fine-tuning) with a simple confidence-weighted average method can enhance the overall prediction. Indeed, while combining trajectory prediction models is not straightforward, this simple approach enhances performance by 10% over the best prediction model, especially in the long-tailed metrics. We show that this performance improvement holds on both the NuScenes and Argoverse datasets, and that these improvements are made across the dataset distribution. The code for our work is open source.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Client Selection via Q-Learning-based Whittle Index in Wireless Federated Learning</title>
<link>https://arxiv.org/abs/2509.13933</link>
<guid>https://arxiv.org/abs/2509.13933</guid>
<content:encoded><![CDATA[
<div> client selection, wireless Federated Learning, restless multi-armed bandit problem, Whittle Index Learning, Q-learning

Summary: 
In the context of wireless Federated Learning (FL), the client selection problem is addressed with the goal of minimizing the time required to achieve a certain learning accuracy level. This issue is formulated as a restless multi-armed bandit problem, given the dynamic nature of client states affecting computation and communication efficiency. The proposed solution, Whittle Index Learning in Federated Q-learning (WILF-Q), utilizes Q-learning to adaptively determine and update an approximated Whittle index for each client, enabling the selection of clients with the highest indices. Unlike existing methods, WILF-Q does not necessitate explicit knowledge of client state transitions or data distributions, rendering it suitable for practical FL implementations. Experimental results indicate that WILF-Q outperforms baseline policies in terms of learning efficiency, offering a robust and effective strategy for client selection in wireless FL. <br /><br />Summary: <div>
arXiv:2509.13933v1 Announce Type: new 
Abstract: We consider the client selection problem in wireless Federated Learning (FL), with the objective of reducing the total required time to achieve a certain level of learning accuracy. Since the server cannot observe the clients' dynamic states that can change their computation and communication efficiency, we formulate client selection as a restless multi-armed bandit problem. We propose a scalable and efficient approach called the Whittle Index Learning in Federated Q-learning (WILF-Q), which uses Q-learning to adaptively learn and update an approximated Whittle index associated with each client, and then selects the clients with the highest indices. Compared to existing approaches, WILF-Q does not require explicit knowledge of client state transitions or data distributions, making it well-suited for deployment in practical FL settings. Experiment results demonstrate that WILF-Q significantly outperforms existing baseline policies in terms of learning efficiency, providing a robust and efficient approach to client selection in wireless FL.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>eXtended Physics Informed Neural Network Method for Fracture Mechanics Problems</title>
<link>https://arxiv.org/abs/2509.13952</link>
<guid>https://arxiv.org/abs/2509.13952</guid>
<content:encoded><![CDATA[
<div> Keywords: fracture mechanics, neural network, XFEM, multiple cracks, domain decomposition <br />
Summary: <br />
This paper introduces the eXtended Physics-Informed Neural Network (X-PINN) framework, designed to address fracture mechanics problems involving multiple cracks in fractured media. The framework incorporates an energy-based loss function, customized integration schemes, and domain decomposition procedures. Inspired by the Extended Finite Element Method (XFEM), X-PINN enriches the neural network solution space with specialized functions to capture crack body discontinuities and singularities at crack tips. A structured framework is proposed where standard and enriched solution components are modeled using distinct neural networks, allowing for flexible simulations of complex multiple-crack problems in 1D and 2D domains, with potential extension to 3D. Numerical experiments demonstrate the effectiveness and robustness of X-PINN in simulating multiple-crack problems. <br /> <div>
arXiv:2509.13952v1 Announce Type: new 
Abstract: This paper presents eXtended Physics-Informed Neural Network (X-PINN), a novel and robust framework for addressing fracture mechanics problems involving multiple cracks in fractured media. To address this, an energy-based loss function, customized integration schemes, and domain decomposition procedures are proposed. Inspired by the Extended Finite Element Method (XFEM), the neural network solution space is enriched with specialized functions that allow crack body discontinuities and singularities at crack tips to be explicitly captured. Furthermore, a structured framework is introduced in which standard and enriched solution components are modeled using distinct neural networks, enabling flexible and effective simulations of complex multiple-crack problems in 1D and 2D domains, with convenient extensibility to 3D problems. Numerical experiments are conducted to validate the effectiveness and robustness of the proposed method.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalization on a Budget: Minimally-Labeled Continual Learning for Resource-Efficient Seizure Detection</title>
<link>https://arxiv.org/abs/2509.13974</link>
<guid>https://arxiv.org/abs/2509.13974</guid>
<content:encoded><![CDATA[
<div> deep learning, epileptic seizure detection, continual learning, electroencephalography, personalized

Summary:
EpiSMART introduces a novel approach to automated epileptic seizure detection using deep learning, focusing on personalized continual learning models that adapt to evolving patient-specific electroencephalography signal features. The framework addresses the challenge of integrating new data without catastrophic forgetting, utilizing a size-constrained replay buffer and informed sample selection strategy. Results from validation on the CHB-MIT dataset show a significant improvement in F1 score compared to a baseline model, with minimal memory and computational requirements. EpiSMART requires only 6.46 minutes of labeled data and 6.28 updates per day on average, making it suitable for real-time deployment in wearable systems. This framework advances the field by providing a continual learning approach that supports patient-specific adaptation and practical deployment in healthcare systems. 

<br /><br />Summary: <div>
arXiv:2509.13974v1 Announce Type: new 
Abstract: Objective: Epilepsy, a prevalent neurological disease, demands careful diagnosis and continuous care. Seizure detection remains challenging, as current clinical practice relies on expert analysis of electroencephalography, which is a time-consuming process and requires specialized knowledge. Addressing this challenge, this paper explores automated epileptic seizure detection using deep learning, focusing on personalized continual learning models that adapt to each patient's unique electroencephalography signal features, which evolve over time. Methods: In this context, our approach addresses the challenge of integrating new data into existing models without catastrophic forgetting, a common issue in static deep learning models. We propose EpiSMART, a continual learning framework for seizure detection that uses a size-constrained replay buffer and an informed sample selection strategy to incrementally adapt to patient-specific electroencephalography signals. By selectively retaining high-entropy and seizure-predicted samples, our method preserves critical past information while maintaining high performance with minimal memory and computational requirements. Results: Validation on the CHB-MIT dataset, shows that EpiSMART achieves a 21% improvement in the F1 score over a trained baseline without updates in all other patients. On average, EpiSMART requires only 6.46 minutes of labeled data and 6.28 updates per day, making it suitable for real-time deployment in wearable systems. Conclusion:EpiSMART enables robust and personalized seizure detection under realistic and resource-constrained conditions by effectively integrating new data into existing models without degrading past knowledge. Significance: This framework advances automated seizure detection by providing a continual learning approach that supports patient-specific adaptation and practical deployment in wearable healthcare systems.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Temporal Graph Networks for Real-Time Correction of GNSS Jamming-Induced Deviations</title>
<link>https://arxiv.org/abs/2509.14000</link>
<guid>https://arxiv.org/abs/2509.14000</guid>
<content:encoded><![CDATA[
<div> Keywords: GNSS, jamming mitigation, deep temporal graph network, heterogeneous graph, receiver-centric.

Summary: 
Global Navigation Satellite Systems (GNSS) face increasing disruption from intentional jamming, affecting positioning and timing accuracy. This study introduces a receiver-centric deep temporal graph network that predicts and corrects horizontal deviations caused by jamming in real time. By representing the receiver environment as a heterogeneous star graph with time-varying attributes, the proposed model, HeteroGCLSTM, outperforms strong multivariate time series baselines in predicting deviation vectors for on-the-fly correction. Evaluation on datasets from two receivers under different jammer profiles and power levels shows the model consistently achieves the lowest mean absolute error. It shows superior data efficiency, requiring only 10% of training data to outperform baselines significantly. The approach demonstrates promising results in mitigating the impact of jamming on GNSS positioning accuracy. 

<br /><br />Summary: <div>
arXiv:2509.14000v1 Announce Type: new 
Abstract: Global Navigation Satellite Systems (GNSS) are increasingly disrupted by intentional jamming, degrading availability precisely when positioning and timing must remain operational. We address this by reframing jamming mitigation as dynamic graph regression and introducing a receiver-centric deep temporal graph network that predicts, and thus corrects, the receivers horizontal deviation in real time. At each 1 Hz epoch, the satellite receiver environment is represented as a heterogeneous star graph (receiver center, tracked satellites as leaves) with time varying attributes (e.g., SNR, azimuth, elevation, latitude/longitude). A single layer Heterogeneous Graph ConvLSTM (HeteroGCLSTM) aggregates one hop spatial context and temporal dynamics over a short history to output the 2D deviation vector applied for on the fly correction.
  We evaluate on datasets from two distinct receivers under three jammer profiles, continuous wave (cw), triple tone (cw3), and wideband FM, each exercised at six power levels between -45 and -70 dBm, with 50 repetitions per scenario (prejam/jam/recovery). Against strong multivariate time series baselines (MLP, uniform CNN, and Seq2Point CNN), our model consistently attains the lowest mean absolute error (MAE). At -45 dBm, it achieves 3.64 cm (GP01/cw), 7.74 cm (GP01/cw3), 4.41 cm (ublox/cw), 4.84 cm (ublox/cw3), and 4.82 cm (ublox/FM), improving to 1.65-2.08 cm by -60 to -70 dBm. On mixed mode datasets pooling all powers, MAE is 3.78 cm (GP01) and 4.25 cm (ublox10), outperforming Seq2Point, MLP, and CNN. A split study shows superior data efficiency: with only 10\% training data our approach remains well ahead of baselines (20 cm vs. 36-42 cm).
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentially private federated learning for localized control of infectious disease dynamics</title>
<link>https://arxiv.org/abs/2509.14024</link>
<guid>https://arxiv.org/abs/2509.14024</guid>
<content:encoded><![CDATA[
<div> Privacy-preserving forecasting, federated learning, differential privacy, COVID-19, machine learning  

Summary:  
Faced with the challenge of limited data availability for training localized machine learning models during epidemics, this study proposes a privacy-preserving forecasting method based on German counties and communities managed by local health authorities. Utilizing federated learning with client-level differential privacy, a shared multilayer perceptron model is trained on recent case counts to forecast the number of COVID-19 cases. Evaluations during two phases show that the approach achieves high accuracy with strong privacy guarantees. The DP model closely approximates the non-DP model with a high $R^2$ value and low mean absolute percentage error, demonstrating its usefulness in delivering county-level predictions while protecting privacy. Viable privacy budgets vary depending on the epidemic phase, allowing for effective collaboration among health authorities for localized forecasting. 

<br /><br />Summary: <div>
arXiv:2509.14024v1 Announce Type: new 
Abstract: In times of epidemics, swift reaction is necessary to mitigate epidemic spreading. For this reaction, localized approaches have several advantages, limiting necessary resources and reducing the impact of interventions on a larger scale. However, training a separate machine learning (ML) model on a local scale is often not feasible due to limited available data. Centralizing the data is also challenging because of its high sensitivity and privacy constraints. In this study, we consider a localized strategy based on the German counties and communities managed by the related local health authorities (LHA). For the preservation of privacy to not oppose the availability of detailed situational data, we propose a privacy-preserving forecasting method that can assist public health experts and decision makers. ML methods with federated learning (FL) train a shared model without centralizing raw data. Considering the counties, communities or LHAs as clients and finding a balance between utility and privacy, we study a FL framework with client-level differential privacy (DP). We train a shared multilayer perceptron on sliding windows of recent case counts to forecast the number of cases, while clients exchange only norm-clipped updates and the server aggregated updates with DP noise. We evaluate the approach on COVID-19 data on county-level during two phases. As expected, very strict privacy yields unstable, unusable forecasts. At a moderately strong level, the DP model closely approaches the non-DP model: $R^2= 0.94$ (vs. 0.95) and mean absolute percentage error (MAPE) of 26 % in November 2020; $R^2= 0.88$ (vs. 0.93) and MAPE of 21 % in March 2022. Overall, client-level DP-FL can deliver useful county-level predictions with strong privacy guarantees, and viable privacy budgets depend on epidemic phase, allowing privacy-compliant collaboration among health authorities for local forecasting.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning-Driven Peptide Classification in Biological Nanopores</title>
<link>https://arxiv.org/abs/2509.14029</link>
<guid>https://arxiv.org/abs/2509.14029</guid>
<content:encoded><![CDATA[
<div> Keywords: nanopore devices, real-time classification, protein identification, wavelet transforms, machine learning

Summary:
Nanopore devices show promise for real-time protein classification in clinical settings. By measuring current signals when proteins enter nanometer-scale pores, these devices offer a potential method for disease diagnosis. However, existing techniques face challenges due to signal complexity. To address this, researchers have developed a novel approach that converts current signals into scaleogram images using wavelet transforms. This method captures amplitude, frequency, and time information, making it suitable for machine learning algorithms. Testing on 42 peptides demonstrated a classification accuracy of approximately 81%, a new state-of-the-art achievement in the field. The proposed technique not only advances peptide and protein diagnostics but also lays the foundation for practical application at the point of care. Moreover, model transfer techniques have been introduced to facilitate real hardware deployment, thus opening up possibilities for real-time disease diagnosis.<br /><br />Summary: <div>
arXiv:2509.14029v1 Announce Type: new 
Abstract: A device capable of performing real time classification of proteins in a clinical setting would allow for inexpensive and rapid disease diagnosis. One such candidate for this technology are nanopore devices. These devices work by measuring a current signal that arises when a protein or peptide enters a nanometer-length-scale pore. Should this current be uniquely related to the structure of the peptide and its interactions with the pore, the signals can be used to perform identification. While such a method would allow for real time identification of peptides and proteins in a clinical setting, to date, the complexities of these signals limit their accuracy. In this work, we tackle the issue of classification by converting the current signals into scaleogram images via wavelet transforms, capturing amplitude, frequency, and time information in a modality well-suited to machine learning algorithms. When tested on 42 peptides, our method achieved a classification accuracy of ~$81\,\%$, setting a new state-of-the-art in the field and taking a step toward practical peptide/protein diagnostics at the point of care. In addition, we demonstrate model transfer techniques that will be critical when deploying these models into real hardware, paving the way to a new method for real-time disease diagnosis.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Queen Detection in Beehives via Environmental Sensor Fusion for Low-Power Edge Computing</title>
<link>https://arxiv.org/abs/2509.14061</link>
<guid>https://arxiv.org/abs/2509.14061</guid>
<content:encoded><![CDATA[
<div> queen bee, honeybee colonies, environmental sensor fusion, real-time, edge computing

Summary:<br /><br />Queen bee presence is crucial for honeybee colony health and stability. Current manual monitoring methods are labor-intensive and impractical for large-scale beekeeping. To address this issue, a lightweight multimodal system for queen detection has been developed. This system utilizes environmental sensor fusion, specifically temperature, humidity, and pressure differentials between the inside and outside of the hive. By employing quantized decision tree inference on a commercial STM32 microcontroller, real-time, low-power edge computing with high accuracy is achieved. The system demonstrates over 99% queen detection accuracy using only environmental inputs, without significant performance improvement from audio features. This innovative approach offers a scalable and sustainable solution for non-invasive hive monitoring, enabling autonomous, precision beekeeping through the use of off-the-shelf, energy-efficient hardware. <div>
arXiv:2509.14061v1 Announce Type: new 
Abstract: Queen bee presence is essential for the health and stability of honeybee colonies, yet current monitoring methods rely on manual inspections that are labor-intensive, disruptive, and impractical for large-scale beekeeping. While recent audio-based approaches have shown promise, they often require high power consumption, complex preprocessing, and are susceptible to ambient noise. To overcome these limitations, we propose a lightweight, multimodal system for queen detection based on environmental sensor fusion-specifically, temperature, humidity, and pressure differentials between the inside and outside of the hive. Our approach employs quantized decision tree inference on a commercial STM32 microcontroller, enabling real-time, low-power edge computing without compromising accuracy. We show that our system achieves over 99% queen detection accuracy using only environmental inputs, with audio features offering no significant performance gain. This work presents a scalable and sustainable solution for non-invasive hive monitoring, paving the way for autonomous, precision beekeeping using off-the-shelf, energy-efficient hardware.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Bayesian Risk-Averse Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.14077</link>
<guid>https://arxiv.org/abs/2509.14077</guid>
<content:encoded><![CDATA[
<div> Bayesian risk-averse reinforcement learning, epistemic uncertainty, Bayesian Risk Markov Decision Process, regret bound, posterior sampling <br />
Summary:
In this paper, the authors explore the Bayesian risk-averse formulation in reinforcement learning (RL) to address epistemic uncertainty. They use the Bayesian Risk Markov Decision Process (BRMDP) to account for parameter uncertainty, deriving an asymptotic normality that reveals the Bayesian risk-averse approach tends to pessimistically underestimate the original value function. This discrepancy diminishes with more data. The authors propose two procedures using posterior sampling for online RL and online contextual multi-arm bandits (CMAB), establishing sub-linear regret bounds in both scenarios. They also demonstrate the algorithm's effectiveness in handling epistemic uncertainty through numerical experiments, validating the theoretical properties. Overall, the study contributes valuable insights into adaptive risk-averse RL algorithms and their applications in addressing model uncertainty. <br /> <div>
arXiv:2509.14077v1 Announce Type: new 
Abstract: In this paper, we study the Bayesian risk-averse formulation in reinforcement learning (RL). To address the epistemic uncertainty due to a lack of data, we adopt the Bayesian Risk Markov Decision Process (BRMDP) to account for the parameter uncertainty of the unknown underlying model. We derive the asymptotic normality that characterizes the difference between the Bayesian risk value function and the original value function under the true unknown distribution. The results indicate that the Bayesian risk-averse approach tends to pessimistically underestimate the original value function. This discrepancy increases with stronger risk aversion and decreases as more data become available. We then utilize this adaptive property in the setting of online RL as well as online contextual multi-arm bandits (CMAB), a special case of online RL. We provide two procedures using posterior sampling for both the general RL problem and the CMAB problem. We establish a sub-linear regret bound, with the regret defined as the conventional regret for both the RL and CMAB settings. Additionally, we establish a sub-linear regret bound for the CMAB setting with the regret defined as the Bayesian risk regret. Finally, we conduct numerical experiments to demonstrate the effectiveness of the proposed algorithm in addressing epistemic uncertainty and verifying the theoretical properties.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Relationship between Brain Hemisphere States and Frequency Bands through Deep Learning Optimization Techniques</title>
<link>https://arxiv.org/abs/2509.14078</link>
<guid>https://arxiv.org/abs/2509.14078</guid>
<content:encoded><![CDATA[
<div> Keywords: EEG frequency bands, optimizer selection, neural network architectures, SHAP plots, neuroimaging-based classification

Summary:
This study evaluates classifier performance in EEG frequency bands using various optimizers and neural network architectures. Adagrad and RMSprop optimizers consistently perform well across different frequency bands, with Adadelta also showing robust performance. Adagrad excels in the beta band, while RMSprop performs best in the gamma band. The CNN model captures spatial features effectively, while the deep dense network learns complex patterns and the shallow three-layer network provides computational efficiency. SHAP plots are utilized to determine efficient class prediction and understand the contributions of EEG frequency bands to model accuracy. The study emphasizes the importance of optimizer selection, model architecture, and EEG frequency band analysis in enhancing classifier performance and interpreting feature importance in neuroimaging-based classification tasks. 

<br /><br />Summary: <div>
arXiv:2509.14078v1 Announce Type: new 
Abstract: This study investigates classifier performance across EEG frequency bands using various optimizers and evaluates efficient class prediction for the left and right hemispheres. Three neural network architectures - a deep dense network, a shallow three-layer network, and a convolutional neural network (CNN) - are implemented and compared using the TensorFlow and PyTorch frameworks. Results indicate that the Adagrad and RMSprop optimizers consistently perform well across different frequency bands, with Adadelta exhibiting robust performance in cross-model evaluations. Specifically, Adagrad excels in the beta band, while RMSprop achieves superior performance in the gamma band. Conversely, SGD and FTRL exhibit inconsistent performance. Among the models, the CNN demonstrates the second highest accuracy, particularly in capturing spatial features of EEG data. The deep dense network shows competitive performance in learning complex patterns, whereas the shallow three-layer network, sometimes being less accurate, provides computational efficiency. SHAP (Shapley Additive Explanations) plots are employed to identify efficient class prediction, revealing nuanced contributions of EEG frequency bands to model accuracy. Overall, the study highlights the importance of optimizer selection, model architecture, and EEG frequency band analysis in enhancing classifier performance and understanding feature importance in neuroimaging-based classification tasks.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Distributional to Quantile Neural Basis Models: the case of Electricity Price Forecasting</title>
<link>https://arxiv.org/abs/2509.14113</link>
<guid>https://arxiv.org/abs/2509.14113</guid>
<content:encoded><![CDATA[
<div> Neural networks, multi-horizon probabilistic forecasting, Quantile Neural Basis Model, interpretability, day-ahead electricity price forecasting<br />
Summary: 
The article introduces the Quantile Neural Basis Model, a framework that combines the interpretability principles of Quantile Generalized Additive Models with neural network training. By incorporating shared basis decomposition and weight factorization, the model avoids parametric distributional assumptions, offering insights into feature-conditioned outputs in forecasting. Tested on day-ahead electricity price forecasting, the model achieves comparable predictive performance to existing neural networks while providing valuable understanding of the nonlinear mappings from input features to output predictions across the forecast horizon. <div>
arXiv:2509.14113v1 Announce Type: new 
Abstract: While neural networks are achieving high predictive accuracy in multi-horizon probabilistic forecasting, understanding the underlying mechanisms that lead to feature-conditioned outputs remains a significant challenge for forecasters. In this work, we take a further step toward addressing this critical issue by introducing the Quantile Neural Basis Model, which incorporates the interpretability principles of Quantile Generalized Additive Models into an end-to-end neural network training framework. To this end, we leverage shared basis decomposition and weight factorization, complementing Neural Models for Location, Scale, and Shape by avoiding any parametric distributional assumptions. We validate our approach on day-ahead electricity price forecasting, achieving predictive performance comparable to distributional and quantile regression neural networks, while offering valuable insights into model behavior through the learned nonlinear mappings from input features to output predictions across the horizon.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking the Cycle of Incarceration With Targeted Mental Health Outreach: A Case Study in Machine Learning for Public Policy</title>
<link>https://arxiv.org/abs/2509.14129</link>
<guid>https://arxiv.org/abs/2509.14129</guid>
<content:encoded><![CDATA[
<div> Keywords: incarcerated individuals, mental health outreach, predictive modeling, reincarceration rates, community-driven approaches

Summary:<br /><br />Many incarcerated individuals face challenges such as mental illness, substance dependence, and homelessness, often leading to a cycle of incarceration. To address this issue, criminal justice stakeholders are exploring innovative approaches like proactive mental health outreach. A collaboration between Johnson County, Kansas, and Carnegie Mellon University focused on targeted outreach to reduce reincarceration rates. The study utilized predictive modeling to identify individuals at highest risk of returning to jail. Results showed that over half of the highest-risk group did return to jail within a year. The outreach efforts were most effective with this high-risk population, leading to positive impacts on mental health utilization, EMS dispatches, and criminal justice involvement. This trial highlights the potential of targeted outreach initiatives in reducing reincarceration rates and addressing the complex needs of incarcerated individuals. <br /><br />Summary: <div>
arXiv:2509.14129v1 Announce Type: new 
Abstract: Many incarcerated individuals face significant and complex challenges, including mental illness, substance dependence, and homelessness, yet jails and prisons are often poorly equipped to address these needs. With little support from the existing criminal justice system, these needs can remain untreated and worsen, often leading to further offenses and a cycle of incarceration with adverse outcomes both for the individual and for public safety, with particularly large impacts on communities of color that continue to widen the already extensive racial disparities in criminal justice outcomes. Responding to these failures, a growing number of criminal justice stakeholders are seeking to break this cycle through innovative approaches such as community-driven and alternative approaches to policing, mentoring, community building, restorative justice, pretrial diversion, holistic defense, and social service connections. Here we report on a collaboration between Johnson County, Kansas, and Carnegie Mellon University to perform targeted, proactive mental health outreach in an effort to reduce reincarceration rates.
  This paper describes the data used, our predictive modeling approach and results, as well as the design and analysis of a field trial conducted to confirm our model's predictive power, evaluate the impact of this targeted outreach, and understand at what level of reincarceration risk outreach might be most effective. Through this trial, we find that our model is highly predictive of new jail bookings, with more than half of individuals in the trial's highest-risk group returning to jail in the following year. Outreach was most effective among these highest-risk individuals, with impacts on mental health utilization, EMS dispatches, and criminal justice involvement.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Compositional Kernel Model for Feature Learning</title>
<link>https://arxiv.org/abs/2509.14158</link>
<guid>https://arxiv.org/abs/2509.14158</guid>
<content:encoded><![CDATA[
<div> Kernel ridge regression, compositional variant, coordinate-wise reweighting, feature learning, variable selection <br />
Summary: <br />
The article explores a compositional variant of kernel ridge regression, where the predictor is applied to a reweighting of inputs. This model serves as a testbed for feature learning in compositional architectures and effectively recovers relevant variables while eliminating noise variables. Guarantees are established, showing that noise coordinates are discarded by both global minimizers and stationary points when the noise variables are Gaussian distributed. The study highlights that $\ell_1$-type kernels like the Laplace kernel excel in recovering nonlinear feature contributions at stationary points, whereas Gaussian kernels mainly recover linear effects. Overall, the research emphasizes the importance of kernel selection in effectively recovering features contributing to both linear and nonlinear effects in compositional models. <br /> <div>
arXiv:2509.14158v1 Announce Type: new 
Abstract: We study a compositional variant of kernel ridge regression in which the predictor is applied to a coordinate-wise reweighting of the inputs. Formulated as a variational problem, this model provides a simple testbed for feature learning in compositional architectures. From the perspective of variable selection, we show how relevant variables are recovered while noise variables are eliminated. We establish guarantees showing that both global minimizers and stationary points discard noise coordinates when the noise variables are Gaussian distributed. A central finding is that $\ell_1$-type kernels, such as the Laplace kernel, succeed in recovering features contributing to nonlinear effects at stationary points, whereas Gaussian kernels recover only linear ones.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deconstructing Intraocular Pressure: A Non-invasive Multi-Stage Probabilistic Inverse Framework</title>
<link>https://arxiv.org/abs/2509.14167</link>
<guid>https://arxiv.org/abs/2509.14167</guid>
<content:encoded><![CDATA[
<div> Framework, Glaucoma, Computational, Artificial intelligence, Data generation <br />
Summary: 
The article introduces a new framework for estimating unmeasurable variables in glaucoma using artificial intelligence. It addresses the challenge of measuring trabecular meshwork permeability, crucial for determining intraocular pressure (IOP). By deconstructing routine data into fundamental components, the framework estimates tissue permeability and patient outflow facility. It incorporates a novel data generation strategy, reducing computational time significantly. The estimated outflow facility aligns well with tonography results and provides a valuable biomarker for diagnosing disease risk. This framework sets a precedent for solving similar inverse problems in data-scarce and computationally-intensive fields. <div>
arXiv:2509.14167v1 Announce Type: new 
Abstract: Many critical healthcare decisions are challenged by the inability to measure key underlying parameters. Glaucoma, a leading cause of irreversible blindness driven by elevated intraocular pressure (IOP), provides a stark example. The primary determinant of IOP, a tissue property called trabecular meshwork permeability, cannot be measured in vivo, forcing clinicians to depend on indirect surrogates. This clinical challenge is compounded by a broader computational one: developing predictive models for such ill-posed inverse problems is hindered by a lack of ground-truth data and prohibitive cost of large-scale, high-fidelity simulations. We address both challenges with an end-to-end framework to noninvasively estimate unmeasurable variables from sparse, routine data. Our approach combines a multi-stage artificial intelligence architecture to functionally separate the problem; a novel data generation strategy we term PCDS that obviates the need for hundreds of thousands of costly simulations, reducing the effective computational time from years to hours; and a Bayesian engine to quantify predictive uncertainty. Our framework deconstructs a single IOP measurement into its fundamental components from routine inputs only, yielding estimates for the unmeasurable tissue permeability and a patient's outflow facility. Our noninvasively estimated outflow facility achieved excellent agreement with state-of-the-art tonography with precision comparable to direct physical instruments. Furthermore, the newly derived permeability biomarker demonstrates high accuracy in stratifying clinical cohorts by disease risk, highlighting its diagnostic potential. More broadly, our framework establishes a generalizable blueprint for solving similar inverse problems in other data-scarce, computationally-intensive domains.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TopoSizing: An LLM-aided Framework of Topology-based Understanding and Sizing for AMS Circuits</title>
<link>https://arxiv.org/abs/2509.14169</link>
<guid>https://arxiv.org/abs/2509.14169</guid>
<content:encoded><![CDATA[
<div> challenging, analog, mixed-signal, circuit design, TopoSizing

Summary:
TopoSizing is a new framework for analog and mixed-signal circuit design that addresses challenges in optimizing circuits efficiently. It combines graph algorithms to organize circuits into a hierarchical structure and utilizes Large Language Models (LLM) for circuit understanding and optimization. The framework follows an iterative hypothesis-verification-refinement loop to generate explicit annotations and integrates them into Bayesian optimization. This approach improves efficiency by guiding initial sampling and updating trust-regions when stagnation is detected. TopoSizing aims to enhance circuit optimization by leveraging structural knowledge and automation, bridging the gap between traditional black-box optimization and learning-based methods. By directly analyzing raw netlists, TopoSizing offers transparency and generality in the design process, making it a promising tool for improving analog and mixed-signal circuit designs. 

<br /><br />Summary: <div>
arXiv:2509.14169v1 Announce Type: new 
Abstract: Analog and mixed-signal circuit design remains challenging due to the shortage of high-quality data and the difficulty of embedding domain knowledge into automated flows. Traditional black-box optimization achieves sampling efficiency but lacks circuit understanding, which often causes evaluations to be wasted in low-value regions of the design space. In contrast, learning-based methods embed structural knowledge but are case-specific and costly to retrain. Recent attempts with large language models show potential, yet they often rely on manual intervention, limiting generality and transparency. We propose TopoSizing, an end-to-end framework that performs robust circuit understanding directly from raw netlists and translates this knowledge into optimization gains. Our approach first applies graph algorithms to organize circuits into a hierarchical device-module-stage representation. LLM agents then execute an iterative hypothesis-verification-refinement loop with built-in consistency checks, producing explicit annotations. Verified insights are integrated into Bayesian optimization through LLM-guided initial sampling and stagnation-triggered trust-region updates, improving efficiency while preserving feasibility.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TGPO: Tree-Guided Preference Optimization for Robust Web Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.14172</link>
<guid>https://arxiv.org/abs/2509.14172</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, web agents, large language models, credit assignment misallocation, reward sparsity 

Summary:
Tree-Guided Preference Optimization (TGPO) is proposed as an offline reinforcement learning framework for training Web Agents. The framework utilizes a tree-structured trajectory representation to address challenges such as credit assignment misallocation and prohibitively high annotation costs. It incorporates a Process Reward Model that generates fine-grained rewards through subgoal progress, redundancy detection, and action verification. Dynamic weighting is used to prioritize high-impact decision points during training. Experiments on Online-Mind2Web and C-WebShop datasets show that TGPO outperforms existing methods, achieving higher success rates with fewer redundant steps. <div>
arXiv:2509.14172v1 Announce Type: new 
Abstract: With the rapid advancement of large language models and vision-language models, employing large models as Web Agents has become essential for automated web interaction. However, training Web Agents with reinforcement learning faces critical challenges including credit assignment misallocation, prohibitively high annotation costs, and reward sparsity. To address these issues, we propose Tree-Guided Preference Optimization (TGPO), an offline reinforcement learning framework that proposes a tree-structured trajectory representation merging semantically identical states across trajectories to eliminate label conflicts. Our framework incorporates a Process Reward Model that automatically generates fine-grained rewards through subgoal progress, redundancy detection, and action verification. Additionally, a dynamic weighting mechanism prioritizes high-impact decision points during training. Experiments on Online-Mind2Web and our self-constructed C-WebShop datasets demonstrate that TGPO significantly outperforms existing methods, achieving higher success rates with fewer redundant steps.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Past and Future: Distribution-Aware Alignment for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2509.14181</link>
<guid>https://arxiv.org/abs/2509.14181</guid>
<content:encoded><![CDATA[
<div> Keywords: representation learning, time series forecasting, contrastive learning, auxiliary features, frequency mismatches

Summary:
TimeAlign introduces a lightweight framework for time series forecasting that utilizes representation learning techniques to align the historical input data with future targets. By learning auxiliary features through a simple reconstruction task and incorporating them back into the base forecaster, TimeAlign demonstrates superior performance across multiple benchmarks. The framework corrects frequency mismatches between input histories and future outputs, leading to improved forecasting accuracy. The theoretical explanation provided elucidates how TimeAlign increases the mutual information between learned representations and predicted targets. As an architecture-agnostic module with minimal computational overhead, TimeAlign can enhance modern deep learning-based time series forecasting systems. The code for implementing TimeAlign is available on GitHub for the research community to explore and utilize. 

<br /><br />Summary: TimeAlign is a novel framework for time series forecasting that leverages representation learning to align historical input data and future targets. Through a simple reconstruction task, auxiliary features are learned and incorporated into the base forecaster, resulting in superior performance on multiple benchmarks. The framework addresses frequency mismatches between input histories and future outputs, improving forecasting accuracy. The theoretical justification provided elucidates how TimeAlign enhances the mutual information between learned representations and predicted targets. As an architecture-agnostic and lightweight module, TimeAlign can effectively enhance modern deep learning-based time series forecasting systems. The availability of the code on GitHub encourages further exploration and adoption of TimeAlign in forecasting research. <div>
arXiv:2509.14181v1 Announce Type: new 
Abstract: Representation learning techniques like contrastive learning have long been explored in time series forecasting, mirroring their success in computer vision and natural language processing. Yet recent state-of-the-art (SOTA) forecasters seldom adopt these representation approaches because they have shown little performance advantage. We challenge this view and demonstrate that explicit representation alignment can supply critical information that bridges the distributional gap between input histories and future targets. To this end, we introduce TimeAlign, a lightweight, plug-and-play framework that learns auxiliary features via a simple reconstruction task and feeds them back to any base forecaster. Extensive experiments across eight benchmarks verify its superior performance. Further studies indicate that the gains arises primarily from correcting frequency mismatches between historical inputs and future outputs. We also provide a theoretical justification for the effectiveness of TimeAlign in increasing the mutual information between learned representations and predicted targets. As it is architecture-agnostic and incurs negligible overhead, TimeAlign can serve as a general alignment module for modern deep learning time-series forecasting systems. The code is available at https://github.com/TROUBADOUR000/TimeAlign.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Variational Framework for Residual-Based Adaptivity in Neural PDE Solvers and Operator Learning</title>
<link>https://arxiv.org/abs/2509.14198</link>
<guid>https://arxiv.org/abs/2509.14198</guid>
<content:encoded><![CDATA[
<div> Variational framework, residual-based adaptive strategies, convex transformations, objective functionals, adaptive weighting <br />
<br />
Summary: 
The article introduces a variational framework that formalizes residual-based adaptive strategies through convex transformations. Different transformations correspond to distinct objective functionals: exponential weights target uniform error minimization, while linear weights aim for quadratic error minimization. This approach enables systematic design of adaptive schemes across norms, reduces discretization error through variance reduction of the loss estimator, and enhances learning dynamics by improving the gradient signal-to-noise ratio. The framework extends to operator learning, demonstrating performance gains across optimizers and architectures. The results offer a theoretical justification for residual-based adaptivity and establish a foundation for principled discretization and training strategies. <div>
arXiv:2509.14198v1 Announce Type: new 
Abstract: Residual-based adaptive strategies are widely used in scientific machine learning but remain largely heuristic. We introduce a unifying variational framework that formalizes these methods by integrating convex transformations of the residual. Different transformations correspond to distinct objective functionals: exponential weights target the minimization of uniform error, while linear weights recover the minimization of quadratic error. Within this perspective, adaptive weighting is equivalent to selecting sampling distributions that optimize the primal objective, thereby linking discretization choices directly to error metrics. This principled approach yields three benefits: (1) it enables systematic design of adaptive schemes across norms, (2) reduces discretization error through variance reduction of the loss estimator, and (3) enhances learning dynamics by improving the gradient signal-to-noise ratio. Extending the framework to operator learning, we demonstrate substantial performance gains across optimizers and architectures. Our results provide a theoretical justification of residual-based adaptivity and establish a foundation for principled discretization and training strategies.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Universal Banach--Bregman Framework for Stochastic Iterations: Unifying Stochastic Mirror Descent, Learning and LLM Training</title>
<link>https://arxiv.org/abs/2509.14216</link>
<guid>https://arxiv.org/abs/2509.14216</guid>
<content:encoded><![CDATA[
<div> Banach--Bregman framework, stochastic optimization, non-Euclidean settings, optimization theory, AI paradigms <br />
Summary: <br />
This paper introduces a pioneering Banach--Bregman framework for stochastic optimization, breaking away from traditional Hilbert space approaches to accommodate non-Euclidean settings. The framework uses Bregman geometry as a foundation for optimization, allowing for flexible geometries and accelerated convergence in various AI paradigms. It provides a unified template for stochastic iterations, encompassing mirror descent, natural gradient descent, and adaptive methods. The introduction of super-relaxations ($\lambda > 2$) in non-Hilbert settings enables faster convergence rates. The theoretical framework is validated through convergence theorems and empirical studies across machine learning, deep learning, reinforcement learning, and large language models, showing improved convergence speed, reduced variance, and enhanced accuracy over classical baselines. This work positions Banach--Bregman geometry as a cornerstone for optimization theory and practice in AI. <br /> <div>
arXiv:2509.14216v1 Announce Type: new 
Abstract: Stochastic optimization powers the scalability of modern artificial intelligence, spanning machine learning, deep learning, reinforcement learning, and large language model training. Yet, existing theory remains largely confined to Hilbert spaces, relying on inner-product frameworks and orthogonality. This paradigm fails to capture non-Euclidean settings, such as mirror descent on simplices, Bregman proximal methods for sparse learning, natural gradient descent in information geometry, or Kullback--Leibler-regularized language model training. Unlike Euclidean-based Hilbert-space methods, this approach embraces general Banach spaces. This work introduces a pioneering Banach--Bregman framework for stochastic iterations, establishing Bregman geometry as a foundation for next-generation optimization. It (i) provides a unified template via Bregman projections and Bregman--Fejer monotonicity, encompassing stochastic approximation, mirror descent, natural gradient, adaptive methods, and mirror-prox; (ii) establishes super-relaxations ($\lambda > 2$) in non-Hilbert settings, enabling flexible geometries and elucidating their acceleration effect; and (iii) delivers convergence theorems spanning almost-sure boundedness to geometric rates, validated on synthetic and real-world tasks. Empirical studies across machine learning (UCI benchmarks), deep learning (e.g., Transformer training), reinforcement learning (actor--critic), and large language models (WikiText-2 with distilGPT-2) show up to 20% faster convergence, reduced variance, and enhanced accuracy over classical baselines. These results position Banach--Bregman geometry as a cornerstone unifying optimization theory and practice across core AI paradigms.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Denoising and Derivative Estimation for Data-Driven Modeling of Nonlinear Dynamical Systems</title>
<link>https://arxiv.org/abs/2509.14219</link>
<guid>https://arxiv.org/abs/2509.14219</guid>
<content:encoded><![CDATA[
<div> Keywords: data-driven modeling, nonlinear dynamical systems, denoising framework, implicit neural representation, Sparse Identification of Nonlinear Dynamics (SINDy) <br />
<br />
Summary: 
The paper introduces a denoising framework, RKTV-INR, for modeling nonlinear dynamical systems in the presence of measurement noise. By utilizing an implicit neural representation (INR) directly fitted to noisy observations and enforcing constraints such as Runge-Kutta integration and total variation, the framework generates a clean, continuous trajectory that closely matches the original data. The INR also provides accurate first-order derivatives through automatic differentiation. The denoised states and derivatives obtained are fed into the Sparse Identification of Nonlinear Dynamics (SINDy) method to accurately recover the governing equations of the system. Experimental results demonstrate the framework's effectiveness in suppressing noise, estimating derivatives precisely, and identifying system dynamics reliably. <div>
arXiv:2509.14219v1 Announce Type: new 
Abstract: Data-driven modeling of nonlinear dynamical systems is often hampered by measurement noise. We propose a denoising framework, called Runge-Kutta and Total Variation Based Implicit Neural Representation (RKTV-INR), that represents the state trajectory with an implicit neural representation (INR) fitted directly to noisy observations. Runge-Kutta integration and total variation are imposed as constraints to ensure that the reconstructed state is a trajectory of a dynamical system that remains close to the original data. The trained INR yields a clean, continuous trajectory and provides accurate first-order derivatives via automatic differentiation. These denoised states and derivatives are then supplied to Sparse Identification of Nonlinear Dynamics (SINDy) to recover the governing equations. Experiments demonstrate effective noise suppression, precise derivative estimation, and reliable system identification.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language models' activations linearly encode training-order recency</title>
<link>https://arxiv.org/abs/2509.14223</link>
<guid>https://arxiv.org/abs/2509.14223</guid>
<content:encoded><![CDATA[
arXiv:2509.14223v1 Announce Type: new 
Abstract: We show that language models' activations linearly encode when information was learned during training. Our setup involves creating a model with a known training order by sequentially fine-tuning Llama-3.2-1B on six disjoint but otherwise similar datasets about named entities. We find that the average activations of test samples for the six training datasets encode the training order: when projected into a 2D subspace, these centroids are arranged exactly in the order of training and lie on a straight line. Further, we show that linear probes can accurately (~90%) distinguish "early" vs. "late" entities, generalizing to entities unseen during the probes' own training. The model can also be fine-tuned to explicitly report an unseen entity's training stage (~80% accuracy). Interestingly, this temporal signal does not seem attributable to simple differences in activation magnitudes, losses, or model confidence. Our paper demonstrates that models are capable of differentiating information by its acquisition time, and carries significant implications for how they might manage conflicting data and respond to knowledge modifications.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Defending Diffusion Models Against Membership Inference Attacks via Higher-Order Langevin Dynamics</title>
<link>https://arxiv.org/abs/2509.14225</link>
<guid>https://arxiv.org/abs/2509.14225</guid>
<content:encoded><![CDATA[
arXiv:2509.14225v1 Announce Type: new 
Abstract: Recent advances in generative artificial intelligence applications have raised new data security concerns. This paper focuses on defending diffusion models against membership inference attacks. This type of attack occurs when the attacker can determine if a certain data point was used to train the model. Although diffusion models are intrinsically more resistant to membership inference attacks than other generative models, they are still susceptible. The defense proposed here utilizes critically-damped higher-order Langevin dynamics, which introduces several auxiliary variables and a joint diffusion process along these variables. The idea is that the presence of auxiliary variables mixes external randomness that helps to corrupt sensitive input data earlier on in the diffusion process. This concept is theoretically investigated and validated on a toy dataset and a speech dataset using the Area Under the Receiver Operating Characteristic (AUROC) curves and the FID metric.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NIRVANA: Structured pruning reimagined for large language models compression</title>
<link>https://arxiv.org/abs/2509.14230</link>
<guid>https://arxiv.org/abs/2509.14230</guid>
<content:encoded><![CDATA[
arXiv:2509.14230v1 Announce Type: new 
Abstract: Structured pruning of large language models (LLMs) offers substantial efficiency improvements by removing entire hidden units, yet current approaches often suffer from significant performance degradation, particularly in zero-shot settings, and necessitate costly recovery techniques such as supervised fine-tuning (SFT) or adapter insertion. To address these critical shortcomings, we introduce NIRVANA, a novel pruning method explicitly designed to balance immediate zero-shot accuracy preservation with robust fine-tuning capability. Leveraging a first-order saliency criterion derived from the Neural Tangent Kernel under Adam optimization dynamics, NIRVANA provides a theoretically grounded pruning strategy that respects essential model training behaviors. To further address the unique challenges posed by structured pruning, NIRVANA incorporates an adaptive sparsity allocation mechanism across layers and modules (attention vs. MLP), which adjusts pruning intensity between modules in a globally balanced manner. Additionally, to mitigate the high sensitivity of pruning decisions to calibration data quality, we propose a simple yet effective KL divergence-based calibration data selection strategy, ensuring more reliable and task-agnostic pruning outcomes. Comprehensive experiments conducted on Llama3, Qwen, and T5 models demonstrate that NIRVANA outperforms existing structured pruning methods under equivalent sparsity constraints, providing a theoretically sound and practical approach to LLM compression. The code is available at https://github.com/iDEA-iSAIL-Lab-UIUC/NIRVANA.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compute as Teacher: Turning Inference Compute Into Reference-Free Supervision</title>
<link>https://arxiv.org/abs/2509.14234</link>
<guid>https://arxiv.org/abs/2509.14234</guid>
<content:encoded><![CDATA[
arXiv:2509.14234v1 Announce Type: new 
Abstract: Where do learning signals come from when there is no ground truth in post-training? We propose turning exploration into supervision through Compute as Teacher (CaT), which converts the model's own exploration at inference-time into reference-free supervision by synthesizing a single reference from a group of parallel rollouts and then optimizing toward it. Concretely, the current policy produces a group of rollouts; a frozen anchor (the initial policy) reconciles omissions and contradictions to estimate a reference, turning extra inference-time compute into a teacher signal. We turn this into rewards in two regimes: (i) verifiable tasks use programmatic equivalence on final answers; (ii) non-verifiable tasks use self-proposed rubrics-binary, auditable criteria scored by an independent LLM judge, with reward given by the fraction satisfied. Unlike selection methods (best-of-N, majority, perplexity, or judge scores), synthesis may disagree with the majority and be correct even when all rollouts are wrong; performance scales with the number of rollouts. As a test-time procedure, CaT improves Gemma 3 4B, Qwen 3 4B, and Llama 3.1 8B (up to +27% on MATH-500; +12% on HealthBench). With reinforcement learning (CaT-RL), we obtain further gains (up to +33% and +30%), with the trained policy surpassing the initial teacher signal.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Supervised and Topological Signal-Quality Assessment for Any PPG Device</title>
<link>https://arxiv.org/abs/2509.12510</link>
<guid>https://arxiv.org/abs/2509.12510</guid>
<content:encoded><![CDATA[
arXiv:2509.12510v1 Announce Type: cross 
Abstract: Wearable photoplethysmography (PPG) is embedded in billions of devices, yet its optical waveform is easily corrupted by motion, perfusion loss, and ambient light, jeopardizing downstream cardiometric analytics. Existing signal-quality assessment (SQA) methods rely either on brittle heuristics or on data-hungry supervised models. We introduce the first fully unsupervised SQA pipeline for wrist PPG. Stage 1 trains a contrastive 1-D ResNet-18 on 276 h of raw, unlabeled data from heterogeneous sources (varying in device and sampling frequency), yielding optical-emitter- and motion-invariant embeddings (i.e., the learned representation is stable across differences in LED wavelength, drive intensity, and device optics, as well as wrist motion). Stage 2 converts each 512-D encoder embedding into a 4-D topological signature via persistent homology (PH) and clusters these signatures with HDBSCAN. To produce a binary signal-quality index (SQI), the acceptable PPG signals are represented by the densest cluster while the remaining clusters are assumed to mainly contain poor-quality PPG signals. Without re-tuning, the SQI attains Silhouette, Davies-Bouldin, and Calinski-Harabasz scores of 0.72, 0.34, and 6173, respectively, on a stratified sample of 10,000 windows. In this study, we propose a hybrid self-supervised-learning--topological-data-analysis (SSL--TDA) framework that offers a drop-in, scalable, cross-device quality gate for PPG signals.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Chatbot-Creation Approaches</title>
<link>https://arxiv.org/abs/2509.13326</link>
<guid>https://arxiv.org/abs/2509.13326</guid>
<content:encoded><![CDATA[
arXiv:2509.13326v1 Announce Type: cross 
Abstract: This full research-to-practice paper explores approaches for developing course chatbots by comparing low-code platforms and custom-coded solutions in educational contexts. With the rise of Large Language Models (LLMs) like GPT-4 and LLaMA, LLM-based chatbots are being integrated into teaching workflows to automate tasks, provide assistance, and offer scalable support. However, selecting the optimal development strategy requires balancing ease of use, customization, data privacy, and scalability. This study compares two development approaches: low-code platforms like AnythingLLM and Botpress, with custom-coded solutions using LangChain, FAISS, and FastAPI. The research uses Prompt engineering, Retrieval-augmented generation (RAG), and personalization to evaluate chatbot prototypes across technical performance, scalability, and user experience. Findings indicate that while low-code platforms enable rapid prototyping, they face limitations in customization and scaling, while custom-coded systems offer more control but require significant technical expertise. Both approaches successfully implement key research principles such as adaptive feedback loops and conversational continuity. The study provides a framework for selecting the appropriate development strategy based on institutional goals and resources. Future work will focus on hybrid solutions that combine low-code accessibility with modular customization and incorporate multimodal input for intelligent tutoring systems.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FRIT: Using Causal Importance to Improve Chain-of-Thought Faithfulness</title>
<link>https://arxiv.org/abs/2509.13334</link>
<guid>https://arxiv.org/abs/2509.13334</guid>
<content:encoded><![CDATA[
arXiv:2509.13334v1 Announce Type: cross 
Abstract: Chain-of-thought (CoT) reasoning has emerged as a powerful tool for improving large language model performance on complex tasks, but recent work shows that reasoning steps often fail to causally influence the final answer, creating brittle and untrustworthy outputs. Prior approaches focus primarily on measuring faithfulness, while methods for systematically improving it remain limited. We introduce Faithful Reasoning via Intervention Training (FRIT), a scalable alignment method that trains models to produce causally consistent reasoning by learning from systematically corrupted examples. FRIT generates synthetic training data by intervening on individual reasoning steps in model-generated CoTs, creating faithful/unfaithful pairs that highlight when reasoning breaks down. We then apply Direct Preference Optimization to teach models to prefer causally consistent reasoning paths. Evaluating on Qwen3-8B and Mistral-7B-v0.1 across factual and symbolic reasoning tasks, FRIT increases faithful reasoning by $3.4$ percentage points for Mistral on GSM8K while improving accuracy by $7.6$ percentage points. Our approach provides the first scalable, supervision-free method for training language models to produce more reliable and interpretable reasoning, addressing a critical gap between reasoning performance and trustworthiness. We release our code at \href{https://github.com/Anut-py/frit}.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Maximizing UAV Cellular Connectivity with Reinforcement Learning for BVLoS Path Planning</title>
<link>https://arxiv.org/abs/2509.13336</link>
<guid>https://arxiv.org/abs/2509.13336</guid>
<content:encoded><![CDATA[
arXiv:2509.13336v1 Announce Type: cross 
Abstract: This paper presents a reinforcement learning (RL) based approach for path planning of cellular connected unmanned aerial vehicles (UAVs) operating beyond visual line of sight (BVLoS). The objective is to minimize travel distance while maximizing the quality of cellular link connectivity by considering real world aerial coverage constraints and employing an empirical aerial channel model. The proposed solution employs RL techniques to train an agent, using the quality of communication links between the UAV and base stations (BSs) as the reward function. Simulation results demonstrate the effectiveness of the proposed method in training the agent and generating feasible UAV path plans. The proposed approach addresses the challenges due to limitations in UAV cellular communications, highlighting the need for investigations and considerations in this area. The RL algorithm efficiently identifies optimal paths, ensuring maximum connectivity with ground BSs to ensure safe and reliable BVLoS flight operation. Moreover, the solution can be deployed as an offline path planning module that can be integrated into future ground control systems (GCS) for UAV operations, enhancing their capabilities and safety. The method holds potential for complex long range UAV applications, advancing the technology in the field of cellular connected UAV path planning.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proximity-Based Evidence Retrieval for Uncertainty-Aware Neural Networks</title>
<link>https://arxiv.org/abs/2509.13338</link>
<guid>https://arxiv.org/abs/2509.13338</guid>
<content:encoded><![CDATA[
arXiv:2509.13338v1 Announce Type: cross 
Abstract: This work proposes an evidence-retrieval mechanism for uncertainty-aware decision-making that replaces a single global cutoff with an evidence-conditioned, instance-adaptive criterion. For each test instance, proximal exemplars are retrieved in an embedding space; their predictive distributions are fused via Dempster-Shafer theory. The resulting fused belief acts as a per-instance thresholding mechanism. Because the supporting evidences are explicit, decisions are transparent and auditable. Experiments on CIFAR-10/100 with BiT and ViT backbones show higher or comparable uncertainty-aware performance with materially fewer confidently incorrect outcomes and a sustainable review load compared with applying threshold on prediction entropy. Notably, only a few evidences are sufficient to realize these gains; increasing the evidence set yields only modest changes. These results indicate that evidence-conditioned tagging provides a more reliable and interpretable alternative to fixed prediction entropy thresholds for operational uncertainty-aware decision-making.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Imagined Autocurricula</title>
<link>https://arxiv.org/abs/2509.13341</link>
<guid>https://arxiv.org/abs/2509.13341</guid>
<content:encoded><![CDATA[
arXiv:2509.13341v1 Announce Type: cross 
Abstract: Training agents to act in embodied environments typically requires vast training data or access to accurate simulation, neither of which exists for many cases in the real world. Instead, world models are emerging as an alternative leveraging offline, passively collected data, they make it possible to generate diverse worlds for training agents in simulation. In this work, we harness world models to generate imagined environments to train robust agents capable of generalizing to novel task variations. One of the challenges in doing this is ensuring the agent trains on useful generated data. We thus propose a novel approach, IMAC (Imagined Autocurricula), leveraging Unsupervised Environment Design (UED), which induces an automatic curriculum over generated worlds. In a series of challenging, procedurally generated environments, we show it is possible to achieve strong transfer performance on held-out environments, having trained only inside a world model learned from a narrower dataset. We believe this opens the path to utilizing larger-scale, foundation world models for generally capable agents.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Dimensionality Reduction Techniques for Spatial Transcriptomics</title>
<link>https://arxiv.org/abs/2509.13344</link>
<guid>https://arxiv.org/abs/2509.13344</guid>
<content:encoded><![CDATA[
arXiv:2509.13344v1 Announce Type: cross 
Abstract: We introduce a unified framework for evaluating dimensionality reduction techniques in spatial transcriptomics beyond standard PCA approaches. We benchmark six methods PCA, NMF, autoencoder, VAE, and two hybrid embeddings on a cholangiocarcinoma Xenium dataset, systematically varying latent dimensions ($k$=5-40) and clustering resolutions ($\rho$=0.1-1.2). Each configuration is evaluated using complementary metrics including reconstruction error, explained variance, cluster cohesion, and two novel biologically-motivated measures: Cluster Marker Coherence (CMC) and Marker Exclusion Rate (MER). Our results demonstrate distinct performance profiles: PCA provides a fast baseline, NMF maximizes marker enrichment, VAE balances reconstruction and interpretability, while autoencoders occupy a middle ground. We provide systematic hyperparameter selection using Pareto optimal analysis and demonstrate how MER-guided reassignment improves biological fidelity across all methods, with CMC scores improving by up to 12\% on average. This framework enables principled selection of dimensionality reduction methods tailored to specific spatial transcriptomics analyses.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accuracy Paradox in Large Language Models: Regulating Hallucination Risks in Generative AI</title>
<link>https://arxiv.org/abs/2509.13345</link>
<guid>https://arxiv.org/abs/2509.13345</guid>
<content:encoded><![CDATA[
arXiv:2509.13345v1 Announce Type: cross 
Abstract: As Large Language Models (LLMs) permeate everyday decision-making, their epistemic and societal risks demand urgent scrutiny. Hallucinations, the generation of fabricated, misleading, oversimplified or untrustworthy outputs, has emerged as imperative challenges. While regulatory, academic, and technical discourse position accuracy as the principal benchmark for mitigating such harms, this article contends that overreliance on accuracy misdiagnoses the problem and has counterproductive effect: the accuracy paradox. Drawing on interdisciplinary literatures, this article develops a taxonomy of hallucination types and shows the paradox along three intertwining dimensions: outputs, individuals and society. First, accuracy functions as a superficial proxy for reliability, incentivising the optimisation of rhetorical fluency and surface-level correctness over epistemic trustworthiness. This encourages passive user trust in outputs that appear accurate but epistemically untenable. Second, accuracy as a singular metric fails to detect harms that are not factually false but are nonetheless misleading, value-laden, or socially distorting, including consensus illusions, sycophantic alignment, and subtle manipulation. Third, regulatory overemphasis on accuracy obscures the wider societal consequences of hallucination, including social sorting, privacy violations, equity harms, epistemic convergence that marginalises dissent, reduces pluralism, and causes social deskilling. By examining the EU AI Act, GDPR, and DSA, the article argues that current regulations are not yet structurally equipped to address these epistemic, relational, and systemic harms and exacerbated by the overreliance on accuracy. By exposing such conceptual and practical challenges, this article calls for a fundamental shift towards pluralistic, context-aware, and manipulation-resilient approaches to AI trustworthy governance.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Label-Efficient Grasp Joint Prediction with Point-JEPA</title>
<link>https://arxiv.org/abs/2509.13349</link>
<guid>https://arxiv.org/abs/2509.13349</guid>
<content:encoded><![CDATA[
arXiv:2509.13349v1 Announce Type: cross 
Abstract: We investigate whether 3D self-supervised pretraining with a Joint-Embedding Predictive Architecture (Point-JEPA) enables label-efficient grasp joint-angle prediction. Using point clouds tokenized from meshes and a ShapeNet-pretrained Point-JEPA encoder, we train a lightweight multi-hypothesis head with winner-takes-all and evaluate by top-logit selection. On DLR-Hand II with object-level splits, Point-JEPA reduces RMSE by up to 26% in low-label regimes and reaches parity with full supervision. These results suggest JEPA-style pretraining is a practical approach for data-efficient grasp learning.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Quantum-Classical Model for Image Classification</title>
<link>https://arxiv.org/abs/2509.13353</link>
<guid>https://arxiv.org/abs/2509.13353</guid>
<content:encoded><![CDATA[
arXiv:2509.13353v1 Announce Type: cross 
Abstract: This study presents a systematic comparison between hybrid quantum-classical neural networks and purely classical models across three benchmark datasets (MNIST, CIFAR100, and STL10) to evaluate their performance, efficiency, and robustness. The hybrid models integrate parameterized quantum circuits with classical deep learning architectures, while the classical counterparts use conventional convolutional neural networks (CNNs). Experiments were conducted over 50 training epochs for each dataset, with evaluations on validation accuracy, test accuracy, training time, computational resource usage, and adversarial robustness (tested with $\epsilon=0.1$ perturbations).Key findings demonstrate that hybrid models consistently outperform classical models in final accuracy, achieving {99.38\% (MNIST), 41.69\% (CIFAR100), and 74.05\% (STL10) validation accuracy, compared to classical benchmarks of 98.21\%, 32.25\%, and 63.76\%, respectively. Notably, the hybrid advantage scales with dataset complexity, showing the most significant gains on CIFAR100 (+9.44\%) and STL10 (+10.29\%). Hybrid models also train 5--12$\times$ faster (e.g., 21.23s vs. 108.44s per epoch on MNIST) and use 6--32\% fewer parameters} while maintaining superior generalization to unseen test data.Adversarial robustness tests reveal that hybrid models are significantly more resilient on simpler datasets (e.g., 45.27\% robust accuracy on MNIST vs. 10.80\% for classical) but show comparable fragility on complex datasets like CIFAR100 ($\sim$1\% robustness for both). Resource efficiency analyses indicate that hybrid models consume less memory (4--5GB vs. 5--6GB for classical) and lower CPU utilization (9.5\% vs. 23.2\% on average).These results suggest that hybrid quantum-classical architectures offer compelling advantages in accuracy, training efficiency, and parameter scalability, particularly for complex vision tasks.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic Data and the Shifting Ground of Truth</title>
<link>https://arxiv.org/abs/2509.13355</link>
<guid>https://arxiv.org/abs/2509.13355</guid>
<content:encoded><![CDATA[
arXiv:2509.13355v1 Announce Type: cross 
Abstract: The emergence of synthetic data for privacy protection, training data generation, or simply convenient access to quasi-realistic data in any shape or volume complicates the concept of ground truth. Synthetic data mimic real-world observations, but do not refer to external features. This lack of a representational relationship, however, not prevent researchers from using synthetic data as training data for AI models and ground truth repositories. It is claimed that the lack of data realism is not merely an acceptable tradeoff, but often leads to better model performance than realistic data: compensate for known biases, prevent overfitting and support generalization, and make the models more robust in dealing with unexpected outliers. Indeed, injecting noisy and outright implausible data into training sets can be beneficial for the model. This greatly complicates usual assumptions based on which representational accuracy determines data fidelity (garbage in - garbage out). Furthermore, ground truth becomes a self-referential affair, in which the labels used as a ground truth repository are themselves synthetic products of a generative model and as such not connected to real-world observations. My paper examines how ML researchers and practitioners bootstrap ground truth under such paradoxical circumstances without relying on the stable ground of representation and real-world reference. It will also reflect on the broader implications of a shift from a representational to what could be described as a mimetic or iconic concept of data.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PREDICT-GBM: Platform for Robust Evaluation and Development of Individualized Computational Tumor Models in Glioblastoma</title>
<link>https://arxiv.org/abs/2509.13360</link>
<guid>https://arxiv.org/abs/2509.13360</guid>
<content:encoded><![CDATA[
arXiv:2509.13360v1 Announce Type: cross 
Abstract: Glioblastoma is the most prevalent primary brain malignancy, distinguished by its highly invasive behavior and exceptionally high rates of recurrence. Conventional radiation therapy, which employs uniform treatment margins, fails to account for patient-specific anatomical and biological factors that critically influence tumor cell migration. To address this limitation, numerous computational models of glioblastoma growth have been developed, enabling generation of tumor cell distribution maps extending beyond radiographically visible regions and thus informing more precise treatment strategies. However, despite encouraging preliminary findings, the clinical adoption of these growth models remains limited. To bridge this translational gap and accelerate both model development and clinical validation, we introduce PREDICT-GBM, a comprehensive integrated pipeline and dataset for modeling and evaluation. This platform enables systematic benchmarking of state-of-the-art tumor growth models using an expert-curated clinical dataset comprising 255 subjects with complete tumor segmentations and tissue characterization maps. Our analysis demonstrates that personalized radiation treatment plans derived from tumor growth predictions achieved superior recurrence coverage compared to conventional uniform margin approaches for two of the evaluated models. This work establishes a robust platform for advancing and systematically evaluating cutting-edge tumor growth modeling approaches, with the ultimate goal of facilitating clinical translation and improving patient outcomes.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$Agent^2$: An Agent-Generates-Agent Framework for Reinforcement Learning Automation</title>
<link>https://arxiv.org/abs/2509.13368</link>
<guid>https://arxiv.org/abs/2509.13368</guid>
<content:encoded><![CDATA[
arXiv:2509.13368v1 Announce Type: cross 
Abstract: Reinforcement learning agent development traditionally requires extensive expertise and lengthy iterations, often resulting in high failure rates and limited accessibility. This paper introduces $Agent^2$, a novel agent-generates-agent framework that achieves fully automated RL agent design through intelligent LLM-driven generation. The system autonomously transforms natural language task descriptions and environment code into comprehensive, high-performance reinforcement learning solutions without human intervention. $Agent^2$ features a revolutionary dual-agent architecture. The Generator Agent serves as an autonomous AI designer that analyzes tasks and generates executable RL agents, while the Target Agent is the resulting automatically generated RL agent. The framework decomposes RL development into two distinct stages: MDP modeling and algorithmic optimization, enabling more targeted and effective agent generation. Built on the Model Context Protocol, $Agent^2$ provides a unified framework that standardizes intelligent agent creation across diverse environments and algorithms, while incorporating adaptive training management and intelligent feedback analysis for continuous improvement. Extensive experiments on a wide range of benchmarks, including MuJoCo, MetaDrive, MPE, and SMAC, demonstrate that $Agent^2$ consistently outperforms manually designed solutions across all tasks, achieving up to 55% performance improvement and substantial gains on average. By enabling truly end-to-end, closed-loop automation, this work establishes a new paradigm in which intelligent agents design and optimize other agents, marking a fundamental breakthrough for automated AI systems.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A novel approach of day-ahead cooling load prediction and optimal control for ice-based thermal energy storage (TES) system in commercial buildings</title>
<link>https://arxiv.org/abs/2509.13371</link>
<guid>https://arxiv.org/abs/2509.13371</guid>
<content:encoded><![CDATA[
arXiv:2509.13371v1 Announce Type: cross 
Abstract: Thermal energy storage (TES) is an effective method for load shifting and demand response in buildings. Optimal TES control and management are essential to improve the performance of the cooling system. Most existing TES systems operate on a fixed schedule, which cannot take full advantage of its load shifting capability, and requires extensive investigation and optimization. This study proposed a novel integrated load prediction and optimized control approach for ice-based TES in commercial buildings. A cooling load prediction model was developed and a mid-day modification mechanism was introduced into the prediction model to improve the accuracy. Based on the predictions, a rule-based control strategy was proposed according to the time-of-use tariff; the mid-day control adjustment mechanism was introduced in accordance with the mid-day prediction modifications. The proposed approach was applied in the ice-based TES system of a commercial complex in Beijing, and achieved a mean absolute error (MAE) of 389 kW and coefficient of variance of MAE of 12.5%. The integrated prediction-based control strategy achieved an energy cost saving rate of 9.9%. The proposed model was deployed in the realistic building automation system of the case building and significantly improved the efficiency and automation of the cooling system.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Valuation of Exotic Options and Counterparty Games Based on Conditional Diffusion</title>
<link>https://arxiv.org/abs/2509.13374</link>
<guid>https://arxiv.org/abs/2509.13374</guid>
<content:encoded><![CDATA[
arXiv:2509.13374v1 Announce Type: cross 
Abstract: This paper addresses the challenges of pricing exotic options and structured products, which traditional models often fail to handle due to their inability to capture real-world market phenomena like fat-tailed distributions and volatility clustering. We introduce a Diffusion-Conditional Probability Model (DDPM) to generate more realistic price paths. Our method incorporates a composite loss function with financial-specific features, and we propose a P-Q dynamic game framework for evaluating the model's economic value through adversarial backtesting. Static validation shows our P-model effectively matches market mean and volatility. In dynamic games, it demonstrates significantly higher profitability than a traditional Monte Carlo-based model for European and Asian options. However, the model shows limitations in pricing products highly sensitive to extreme events, such as snowballs and accumulators, because it tends to underestimate tail risks. The study concludes that diffusion models hold significant potential for enhancing pricing accuracy, though further research is needed to improve their ability to model extreme market risks.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unleashing the power of computational insights in revealing the complexity of biological systems in the new era of spatial multi-omics</title>
<link>https://arxiv.org/abs/2509.13376</link>
<guid>https://arxiv.org/abs/2509.13376</guid>
<content:encoded><![CDATA[
arXiv:2509.13376v1 Announce Type: cross 
Abstract: Recent advances in spatial omics technologies have revolutionized our ability to study biological systems with unprecedented resolution. By preserving the spatial context of molecular measurements, these methods enable comprehensive mapping of cellular heterogeneity, tissue architecture, and dynamic biological processes in developmental biology, neuroscience, oncology, and evolutionary studies. This review highlights a systematic overview of the continuous advancements in both technology and computational algorithms that are paving the way for a deeper, more systematic comprehension of the structure and mechanisms of mammalian tissues and organs by using spatial multi-omics. Our viewpoint demonstrates how advanced machine learning algorithms and multi-omics integrative modeling can decode complex biological processes, including the spatial organization and topological relationships of cells during organ development, as well as key molecular signatures and regulatory networks underlying tumorigenesis and metastasis. Finally, we outline future directions for technological innovation and modeling insights of spatial omics in precision medicine.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>