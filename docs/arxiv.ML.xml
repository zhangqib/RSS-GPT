<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.LG updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.LG</link>


<item>
<title>CLARIFY: Contrastive Preference Reinforcement Learning for Untangling Ambiguous Queries</title>
<link>https://arxiv.org/abs/2506.00388</link>
<guid>https://arxiv.org/abs/2506.00388</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, preference-based, human feedback, trajectory embedding, PbRL  

Summary:  
Contrastive Learning for Resolving Ambiguous Feedback (CLARIFY) is introduced as an offline Preference-based reinforcement learning (PbRL) method to address the issue of human struggles in labelling clear preferences between similar segments. By learning a trajectory embedding space that includes preference information, CLARIFY ensures that clearly distinguished segments are spaced apart, making it easier to select unambiguous queries. Extensive experiments show that CLARIFY surpasses baseline methods in scenarios involving non-ideal teachers and real human feedback. The approach not only improves query selection but also generates meaningful trajectory embeddings. <div>
arXiv:2506.00388v3 Announce Type: replace 
Abstract: Preference-based reinforcement learning (PbRL) bypasses explicit reward engineering by inferring reward functions from human preference comparisons, enabling better alignment with human intentions. However, humans often struggle to label a clear preference between similar segments, reducing label efficiency and limiting PbRL's real-world applicability. To address this, we propose an offline PbRL method: Contrastive LeArning for ResolvIng Ambiguous Feedback (CLARIFY), which learns a trajectory embedding space that incorporates preference information, ensuring clearly distinguished segments are spaced apart, thus facilitating the selection of more unambiguous queries. Extensive experiments demonstrate that CLARIFY outperforms baselines in both non-ideal teachers and real human feedback settings. Our approach not only selects more distinguished queries but also learns meaningful trajectory embeddings.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gumbel-max List Sampling for Distribution Coupling with Multiple Samples</title>
<link>https://arxiv.org/abs/2506.05632</link>
<guid>https://arxiv.org/abs/2506.05632</guid>
<content:encoded><![CDATA[
<div> sampling, probability distributions, multi-draft speculative sampling, distributed lossy compression, Gumbel-max<br />
Summary: 
This article presents a novel method for generating samples to solve the problem of coupling probability distributions. The proposed method extends Gumbel-max sampling and is supported by the list matching lemma lower bound on acceptance probability. In the context of language tasks, a new mechanism for multi-draft speculative sampling is introduced, ensuring drafter invariance and competitive performance. Theoretical lower bounds on token level acceptance probability are provided. In the field of distributed lossy compression with side information, a compression technique based on the extended Gumbel-max sampling method is proposed. Experimental results demonstrate significant gains in compression effectiveness for synthetic Gaussian sources and the MNIST image dataset. <div>
arXiv:2506.05632v2 Announce Type: replace 
Abstract: We study a relaxation of the problem of coupling probability distributions -- a list of samples is generated from one distribution and an accept is declared if any one of these samples is identical to the sample generated from the other distribution. We propose a novel method for generating samples, which extends the Gumbel-max sampling suggested in Daliri et al. (arXiv:2408.07978) for coupling probability distributions. We also establish a corresponding lower bound on the acceptance probability, which we call the list matching lemma. We next discuss two applications of our setup. First, we develop a new mechanism for multi-draft speculative sampling that is simple to implement and achieves performance competitive with baselines such as SpecTr and SpecInfer across a range of language tasks. Our method also guarantees a certain degree of drafter invariance with respect to the output tokens which is not supported by existing schemes. We also provide a theoretical lower bound on the token level acceptance probability. As our second application, we consider distributed lossy compression with side information in a setting where a source sample is compressed and available to multiple decoders, each with independent side information. We propose a compression technique that is based on our generalization of Gumbel-max sampling and show that it provides significant gains in experiments involving synthetic Gaussian sources and the MNIST image dataset.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Microstructural Dynamics in Cryptocurrency Limit Order Books: Better Inputs Matter More Than Stacking Another Hidden Layer</title>
<link>https://arxiv.org/abs/2506.05764</link>
<guid>https://arxiv.org/abs/2506.05764</guid>
<content:encoded><![CDATA[
<div> deep learning, cryptocurrency, limit order book, forecasting, data preprocessing <br />
Summary: 
The study explores the use of deep learning models for short-term price forecasting in the cryptocurrency market, specifically focusing on supply and demand imbalances in the limit order book (LOB). By comparing various models, including simpler ones like logistic regression and XGBoost, with more complex deep architectures like DeepLOB and Conv1D+LSTM, the researchers investigate the impact of model complexity on predictive performance. They introduce two data filtering pipelines and evaluate binary and ternary labeling schemes. The results show that, with proper data preprocessing and hyperparameter tuning, simpler models can achieve comparable or better forecasting performance than complex neural networks. Simpler models also offer advantages such as faster inference and enhanced interpretability, highlighting the importance of effective data preprocessing and feature engineering in cryptocurrency price forecasting. <br /><br />Summary: <div>
arXiv:2506.05764v2 Announce Type: replace 
Abstract: Cryptocurrency price dynamics are driven largely by microstructural supply demand imbalances in the limit order book (LOB), yet the highly noisy nature of LOB data complicates the signal extraction process. Prior research has demonstrated that deep-learning architectures can yield promising predictive performance on pre-processed equity and futures LOB data, but they often treat model complexity as an unqualified virtue. In this paper, we aim to examine whether adding extra hidden layers or parameters to "blackbox ish" neural networks genuinely enhances short term price forecasting, or if gains are primarily attributable to data preprocessing and feature engineering. We benchmark a spectrum of models from interpretable baselines, logistic regression, XGBoost to deep architectures (DeepLOB, Conv1D+LSTM) on BTC/USDT LOB snapshots sampled at 100 ms to multi second intervals using publicly available Bybit data. We introduce two data filtering pipelines (Kalman, Savitzky Golay) and evaluate both binary (up/down) and ternary (up/flat/down) labeling schemes. Our analysis compares models on out of sample accuracy, latency, and robustness to noise. Results reveal that, with data preprocessing and hyperparameter tuning, simpler models can match and even exceed the performance of more complex networks, offering faster inference and greater interpretability.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pruning Spurious Subgraphs for Graph Out-of-Distribtuion Generalization</title>
<link>https://arxiv.org/abs/2506.05957</link>
<guid>https://arxiv.org/abs/2506.05957</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, GNNs, distribution shifts, out-of-distribution generalization challenge, invariant subgraph <br />
<br />
Summary: 
The paper introduces PrunE, a novel pruning-based method for improving out-of-distribution (OOD) generalizability in Graph Neural Networks (GNNs). Existing methods struggle to identify spurious edges in the invariant subgraph that are not informative. PrunE addresses this issue by effectively eliminating such spurious edges, enhancing the retention of the invariant subgraph crucial for OOD generalization. It utilizes two regularization terms - graph size constraints and $\epsilon$-probability alignment - to prune spurious edges. Theoretical analysis and extensive experiments demonstrate that PrunE surpasses previous state-of-the-art methods in achieving superior OOD performance. The availability of the code repository enhances reproducibility and facilitates further research in the field. <div>
arXiv:2506.05957v2 Announce Type: replace 
Abstract: Graph Neural Networks (GNNs) often encounter significant performance degradation under distribution shifts between training and test data, hindering their applicability in real-world scenarios. Recent studies have proposed various methods to address the out-of-distribution generalization challenge, with many methods in the graph domain focusing on directly identifying an invariant subgraph that is predictive of the target label. However, we argue that identifying the edges from the invariant subgraph directly is challenging and error-prone, especially when some spurious edges exhibit strong correlations with the targets. In this paper, we propose PrunE, the first pruning-based graph OOD method that eliminates spurious edges to improve OOD generalizability. By pruning spurious edges, PrunE retains the invariant subgraph more comprehensively, which is critical for OOD generalization. Specifically, PrunE employs two regularization terms to prune spurious edges: 1) graph size constraint to exclude uninformative spurious edges, and 2) $\epsilon$-probability alignment to further suppress the occurrence of spurious edges. Through theoretical analysis and extensive experiments, we show that PrunE achieves superior OOD performance and outperforms previous state-of-the-art methods significantly. Codes are available at: \href{https://github.com/tianyao-aka/PrunE-GraphOOD}{https://github.com/tianyao-aka/PrunE-GraphOOD}.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Voice Impression Control in Zero-Shot TTS</title>
<link>https://arxiv.org/abs/2506.05688</link>
<guid>https://arxiv.org/abs/2506.05688</guid>
<content:encoded><![CDATA[
<div> Keywords: zero-shot text-to-speech, voice impression control, para/non-linguistic information, speaker fidelity, language model

Summary: 
Zero-shot text-to-speech (TTS) technology has advanced in achieving high speaker fidelity, but modulating para/non-linguistic information for controlling perceived voice characteristics remains a challenge. This study introduces a novel method for voice impression control in zero-shot TTS using a low-dimensional vector to represent intensities of different voice impression pairs. The method effectively manipulates voice impressions as demonstrated by both objective and subjective evaluations. By generating the vector through a large language model, it enables the generation of target impressions based on natural language descriptions, eliminating the need for manual optimization. The approach allows for precise control of voice impressions, such as dark or bright qualities, enhancing the overall quality and expressiveness of synthesized speech. Visit the demo page for audio examples showcasing the effectiveness of this innovative voice impression control technique. 

<br /><br />Summary: <div>
arXiv:2506.05688v2 Announce Type: replace-cross 
Abstract: Para-/non-linguistic information in speech is pivotal in shaping the listeners' impression. Although zero-shot text-to-speech (TTS) has achieved high speaker fidelity, modulating subtle para-/non-linguistic information to control perceived voice characteristics, i.e., impressions, remains challenging. We have therefore developed a voice impression control method in zero-shot TTS that utilizes a low-dimensional vector to represent the intensities of various voice impression pairs (e.g., dark-bright). The results of both objective and subjective evaluations have demonstrated our method's effectiveness in impression control. Furthermore, generating this vector via a large language model enables target-impression generation from a natural language description of the desired impression, thus eliminating the need for manual optimization. Audio examples are available on our demo page (https://ntt-hilab-gensp.github.io/is2025voiceimpression/).
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BEAST: Efficient Tokenization of B-Splines Encoded Action Sequences for Imitation Learning</title>
<link>https://arxiv.org/abs/2506.06072</link>
<guid>https://arxiv.org/abs/2506.06072</guid>
<content:encoded><![CDATA[
<div> Keywords: B-spline, Action Tokenizer, Trajectory Generation, Pretrained Models, Robotics

Summary:
BEAST is a novel action tokenizer that uses B-splines to encode action sequences efficiently and generate tokens of uniform length without the need for separate tokenizer training. It ensures smooth trajectories and enables fast action sequence generation through parallel decoding. BEAST is compatible with various model architectures, including VAE, Transformers, and pretrained Vision-Language Models like Florence-2. Experimental results show that BEAST significantly reduces computational costs for training and inference, producing smooth control signals suitable for continuous control tasks and achieving competitive task success rates. The evaluation on 166 simulated tasks and 8 real-world tasks across different robot settings demonstrates the effectiveness and scalability of BEAST in enhancing overall performance in action sequence encoding and generation.<br /><br />Summary: <div>
arXiv:2506.06072v2 Announce Type: replace-cross 
Abstract: We present the B-spline Encoded Action Sequence Tokenizer (BEAST), a novel action tokenizer that encodes action sequences into compact discrete or continuous tokens using B-splines. In contrast to existing action tokenizers based on vector quantization or byte pair encoding, BEAST requires no separate tokenizer training and consistently produces tokens of uniform length, enabling fast action sequence generation via parallel decoding. Leveraging our B-spline formulation, BEAST inherently ensures generating smooth trajectories without discontinuities between adjacent segments. We extensively evaluate BEAST by integrating it with three distinct model architectures: a Variational Autoencoder (VAE) with continuous tokens, a decoder-only Transformer with discrete tokens, and Florence-2, a pretrained Vision-Language Model with an encoder-decoder architecture, demonstrating BEAST's compatibility and scalability with large pretrained models. We evaluate BEAST across three established benchmarks consisting of 166 simulated tasks and on three distinct robot settings with a total of 8 real-world tasks. Experimental results demonstrate that BEAST (i) significantly reduces both training and inference computational costs, and (ii) consistently generates smooth, high-frequency control signals suitable for continuous control tasks while (iii) reliably achieves competitive task success rates compared to state-of-the-art methods.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BiAssemble: Learning Collaborative Affordance for Bimanual Geometric Assembly</title>
<link>https://arxiv.org/abs/2506.06221</link>
<guid>https://arxiv.org/abs/2506.06221</guid>
<content:encoded><![CDATA[
<div> assembly, robot, geometric, bimanual collaboration, affordance  
Summary:  
Shape assembly is a crucial robotic skill, particularly in geometric assembly tasks where broken parts are reassembled into their original form. The robot must recognize geometric cues for grasping, assembly, and collaborative manipulation on varied fragments. This paper introduces a point-level affordance approach for learning bimanual collaboration in geometric assembly with long-horizon action sequences. To address evaluation ambiguity due to geometry diversity, a real-world benchmark with geometric variety and global reproducibility is proposed. Extensive experiments show the superiority of this approach over previous methods based on affordance and imitation. <div>
arXiv:2506.06221v2 Announce Type: replace-cross 
Abstract: Shape assembly, the process of combining parts into a complete whole, is a crucial robotic skill with broad real-world applications. Among various assembly tasks, geometric assembly--where broken parts are reassembled into their original form (e.g., reconstructing a shattered bowl)--is particularly challenging. This requires the robot to recognize geometric cues for grasping, assembly, and subsequent bimanual collaborative manipulation on varied fragments. In this paper, we exploit the geometric generalization of point-level affordance, learning affordance aware of bimanual collaboration in geometric assembly with long-horizon action sequences. To address the evaluation ambiguity caused by geometry diversity of broken parts, we introduce a real-world benchmark featuring geometric variety and global reproducibility. Extensive experiments demonstrate the superiority of our approach over both previous affordance-based and imitation-based methods. Project page: https://sites.google.com/view/biassembly/.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KVmix: Gradient-Based Layer Importance-Aware Mixed-Precision Quantization for KV Cache</title>
<link>https://arxiv.org/abs/2506.08018</link>
<guid>https://arxiv.org/abs/2506.08018</guid>
<content:encoded><![CDATA[
<div> Keyword: Key-Value Cache, Large Language Models, Quantization, Mixed-Precision, Inference Throughput

Summary:
KVmix is a novel mixed-precision quantization method for Key-Value (KV) Cache in Large Language Models (LLMs). It uses gradient-based importance analysis to allocate layer-specific bit-width dynamically, prioritizing higher precision for important layers and aggressively quantizing less influential ones. This approach allows for a tunable balance between accuracy and efficiency. KVmix also introduces a dynamic long-context optimization strategy that keeps full-precision KV pairs for recent pivotal tokens and compresses older ones, enabling high-quality sequence generation with low memory usage. Furthermore, efficient low-bit quantization and CUDA kernels are provided to optimize computational overhead. On LLMs such as Llama and Mistral, KVmix achieves near-lossless inference with extremely low quantization configuration (Key 2.19bit Value 2.38bit), resulting in a remarkable 4.9x memory compression and 5.3x speedup in inference throughput.

<br /><br />Summary: 
KVmix is a mixed-precision quantization method for Key-Value Cache in Large Language Models that dynamically allocates bit-width based on layer importance, achieving a balance between accuracy and efficiency. The method also includes a dynamic long-context optimization strategy and efficient low-bit quantization to optimize computational overhead. On LLMs like Llama and Mistral, KVmix achieves near-lossless inference with low quantization configuration, providing significant memory compression and speedup in inference throughput. <div>
arXiv:2506.08018v1 Announce Type: new 
Abstract: The high memory demands of the Key-Value (KV) Cache during the inference of Large Language Models (LLMs) severely restrict their deployment in resource-constrained platforms. Quantization can effectively alleviate the memory pressure caused by KV Cache. However, existing methods either rely on static one-size-fits-all precision allocation or fail to dynamically prioritize critical KV in long-context tasks, forcing memory-accuracy-throughput tradeoffs. In this work, we propose a novel mixed-precision quantization method for KV Cache named KVmix. KVmix leverages gradient-based importance analysis to evaluate how individual Key and Value projection matrices affect the model loss, enabling layer-specific bit-width allocation for mix-precision quantization. It dynamically prioritizes higher precision for important layers while aggressively quantizing less influential ones, achieving a tunable balance between accuracy and efficiency. KVmix also introduces a dynamic long-context optimization strategy that adaptively keeps full-precision KV pairs for recent pivotal tokens and compresses older ones, achieving high-quality sequence generation with low memory usage. Additionally, KVmix provides efficient low-bit quantization and CUDA kernels to optimize computational overhead. On LLMs such as Llama and Mistral, KVmix achieves near-lossless inference performance with extremely low quantization configuration (Key 2.19bit Value 2.38bit), while delivering a remarkable 4.9x memory compression and a 5.3x speedup in inference throughput.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gridding Forced Displacement using Semi-Supervised Learning</title>
<link>https://arxiv.org/abs/2506.08019</link>
<guid>https://arxiv.org/abs/2506.08019</guid>
<content:encoded><![CDATA[
<div> Keywords: semi-supervised approach, refugee statistics, spatial disaggregation, ProGres registration data, displacement patterns

Summary: 
This article presents a semi-supervised approach to disaggregating refugee statistics in Sub-Saharan African countries to 0.5-degree grid cells. By utilizing UNHCR's ProGres registration data and satellite-derived building footprints from Google Open Buildings, the algorithm achieves a high accuracy rate of 92.9% in placing over 10 million refugee observations into grid cells. This methodology allows for the identification of localized displacement patterns that were previously obscured in broader regional and national statistics. The resulting high-resolution dataset provides valuable insights into understanding displacement drivers and can contribute to more targeted and effective responses to refugee crises. <div>
arXiv:2506.08019v1 Announce Type: new 
Abstract: We present a semi-supervised approach that disaggregates refugee statistics from administrative boundaries to 0.5-degree grid cells across 25 Sub-Saharan African countries. By integrating UNHCR's ProGres registration data with satellite-derived building footprints from Google Open Buildings and location coordinates from OpenStreetMap Populated Places, our label spreading algorithm creates spatially explicit refugee statistics at high granularity.This methodology achieves 92.9% average accuracy in placing over 10 million refugee observations into appropriate grid cells, enabling the identification of localized displacement patterns previously obscured in broader regional and national statistics. The resulting high-resolution dataset provides a foundation for a deeper understanding of displacement drivers.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bi-level Unbalanced Optimal Transport for Partial Domain Adaptation</title>
<link>https://arxiv.org/abs/2506.08020</link>
<guid>https://arxiv.org/abs/2506.08020</guid>
<content:encoded><![CDATA[
<div> Keywords: Partial domain adaptation, outlier classes, weighting framework, optimal transport, cluster structures

Summary: 
The article introduces a Bi-level Unbalanced Optimal Transport (BUOT) model to address the issue of partial domain adaptation (PDA) and outlier class detection in cross-domain sample alignment. The current weighting framework used in PDA struggles to accurately identify outlier classes due to its focus on sample-wise relations. The BUOT model offers a unified transport framework that considers both sample-wise and class-wise relations, improving the alignment process. By incorporating a label-aware transport cost, the model ensures local transport structure and enhances efficiency. Extensive experiments on benchmark datasets demonstrate the effectiveness and competitiveness of BUOT in handling outlier classes and improving knowledge transfer in PDA scenarios.<br /><br />Summary: <div>
arXiv:2506.08020v1 Announce Type: new 
Abstract: Partial domain adaptation (PDA) problem requires aligning cross-domain samples while distinguishing the outlier classes for accurate knowledge transfer. The widely used weighting framework tries to address the outlier classes by introducing the reweighed source domain with a similar label distribution to the target domain. However, the empirical modeling of weights can only characterize the sample-wise relations, which leads to insufficient exploration of cluster structures, and the weights could be sensitive to the inaccurate prediction and cause confusion on the outlier classes. To tackle these issues, we propose a Bi-level Unbalanced Optimal Transport (BUOT) model to simultaneously characterize the sample-wise and class-wise relations in a unified transport framework. Specifically, a cooperation mechanism between sample-level and class-level transport is introduced, where the sample-level transport provides essential structure information for the class-level knowledge transfer, while the class-level transport supplies discriminative information for the outlier identification. The bi-level transport plan provides guidance for the alignment process. By incorporating the label-aware transport cost, the local transport structure is ensured and a fast computation formulation is derived to improve the efficiency. Extensive experiments on benchmark datasets validate the competitiveness of BUOT.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlowBERT: Prompt-tuned BERT for variable flow field prediction</title>
<link>https://arxiv.org/abs/2506.08021</link>
<guid>https://arxiv.org/abs/2506.08021</guid>
<content:encoded><![CDATA[
<div> Pretrained LLM, Proper Orthogonal Decomposition, fine-tuning, fluid dynamics, knowledge transfer<br />
Summary:<br />
This study introduces a universal flow field prediction framework that leverages knowledge transfer from large language models (LLMs) to address the computational costs of traditional computational fluid dynamics methods and the limitations of existing deep learning models. By integrating Proper Orthogonal Decomposition (POD) with fine-tuning strategies for pretrained LLMs, the framework effectively captures flow field features in a compressed representation while learning system dynamics in state space. Designed fluid dynamics-oriented text templates enhance predictive performance by providing enriched semantic information. Experimental results show that the proposed framework surpasses conventional Transformer models in few-shot learning and exhibits strong generalization across different flow conditions and airfoil geometries. With reduced prediction time and high accuracy, this approach opens up new possibilities for rapid fluid dynamics prediction in areas like aerodynamic optimization and flow control.<br /> <div>
arXiv:2506.08021v1 Announce Type: new 
Abstract: This study proposes a universal flow field prediction framework based on knowledge transfer
  from large language model (LLM), addressing the high computational costs of traditional
  computational fluid dynamics (CFD) methods and the limited cross-condition transfer capability
  of existing deep learning models. The framework innovatively integrates Proper Orthogonal
  Decomposition (POD) dimensionality reduction with fine-tuning strategies for pretrained LLM,
  where POD facilitates compressed representation of flow field features while the fine-tuned model
  learns to encode system dynamics in state space. To enhance the model's adaptability to flow field
  data, we specifically designed fluid dynamics-oriented text templates that improve predictive
  performance through enriched contextual semantic information. Experimental results demonstrate
  that our framework outperforms conventional Transformer models in few-shot learning scenarios while
  exhibiting exceptional generalization across various inflow conditions and airfoil geometries.
  Ablation studies reveal the contributions of key components in the FlowBERT architecture. Compared
  to traditional Navier-Stokes equation solvers requiring hours of computation, our approach reduces
  prediction time to seconds while maintaining over 90% accuracy. The developed knowledge transfer
  paradigm establishes a new direction for rapid fluid dynamics prediction, with potential
  applications extending to aerodynamic optimization, flow control, and other engineering domains.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modality-Balancing Preference Optimization of Large Multimodal Models by Adversarial Negative Mining</title>
<link>https://arxiv.org/abs/2506.08022</link>
<guid>https://arxiv.org/abs/2506.08022</guid>
<content:encoded><![CDATA[
<div> modality imbalance, large multimodal models, preference optimization, group relative policy optimization, hallucinations  
Summary:  
Modality-Balancing Preference Optimization (MBPO) addresses modality imbalance in Large Multimodal Models (LMMs) by generating hard negatives through adversarial perturbation of input images. MBPO uses offline-online hybrid data for training, leveraging Group Relative Policy Optimization (GRPO) to improve reasoning capabilities of LMMs. The framework enhances LMM performance on vision-language tasks and reduces hallucinations by curating training data effectively and adapting to dynamic distributional shifts during training. <div>
arXiv:2506.08022v1 Announce Type: new 
Abstract: The task adaptation and alignment of Large Multimodal Models (LMMs) have been significantly advanced by instruction tuning and further strengthened by recent preference optimization. Yet, most LMMs still suffer from severe modality imbalance during reasoning, i.e., outweighing language prior biases over visual inputs, which bottlenecks their generalization to downstream tasks and causes hallucinations. However, existing preference optimization approaches for LMMs do not focus on restraining the internal biases of their Large Language Model (LLM) backbones when curating the training data. Moreover, they heavily rely on offline data and lack the capacity to explore diverse responses adaptive to dynamic distributional shifts during training. Meanwhile, Group Relative Policy Optimization (GRPO), a recent method using online-generated data and verified rewards to improve reasoning capabilities, remains largely underexplored in LMM alignment. In this paper, we propose a novel preference learning framework, Modality-Balancing Preference Optimization (MBPO), to address the modality imbalance in LMMs. MBPO constructs a more effective offline preference dataset by generating hard negatives, i.e., rejected responses misled by LLM biases due to limited usage of visual information, through adversarial perturbation of input images. Moreover, MBPO leverages the easy-to-verify nature of close-ended tasks to generate online responses with verified rewards. GRPO is then employed to train the model with offline-online hybrid data. Extensive experiments demonstrate that MBPO can enhance LMM performance on challenging vision-language tasks and effectively reduce hallucinations.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recipes for Pre-training LLMs with MXFP8</title>
<link>https://arxiv.org/abs/2506.08027</link>
<guid>https://arxiv.org/abs/2506.08027</guid>
<content:encoded><![CDATA[
<div> precision scaling, model parameters, tensors, pre-training, GPU efficiency

Summary:
Precision scaling using fewer bits for model parameters and tensors is an effective technique for improving GPU efficiency without losing accuracy. The Microscaling formats in NVIDIA's Blackwell GPUs, featuring narrow floating-point data types with per-block scaling factors, facilitate precise quantization of tensors. While MX-formats offer improved numeric stability, the suggested rounding mode in the OCP specification can cause divergence during LLM pre-training on a multi-trillion token dataset. Introducing an enhanced rounding mode that employs round-to-infinity for computing scaling factors allows successful pre-training with MXFP8 for an 8B model on 15T tokens. <br /><br />Summary: <div>
arXiv:2506.08027v1 Announce Type: new 
Abstract: Precision scaling - using fewer bits to represent model parameters and related tensors during pre-training - has emerged as a compelling technique for improving GPU efficiency without sacrificing accuracy. Microscaling (MX) formats in NVIDIA's latest Blackwell GPUs represent a major leap in enabling this precision scaling aspect. These formats combine narrow floating-point data types with per-block scaling factors, offering a fine-grained approach to quantizing tensors.
  Although MX-formats offer the promise of improved numeric stability compared to other reduced-precision representations, in practice they must be used carefully in order to successfully converge an LLM on a multi-trillion token dataset. In this paper, we show that the rounding mode suggested in OCP specification can lead to divergence when pre-training an LLM. We show an improved rounding mode, which uses round-to-infinity to compute scaling factors, enables successful pre-training in MXFP8 for an 8B model on 15T tokens.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ST-GraphNet: A Spatio-Temporal Graph Neural Network for Understanding and Predicting Automated Vehicle Crash Severity</title>
<link>https://arxiv.org/abs/2506.08051</link>
<guid>https://arxiv.org/abs/2506.08051</guid>
<content:encoded><![CDATA[
<div> graph neural network, automated vehicle crashes, spatio-temporal dynamics, crash severity, spatial aggregation 

Summary:
ST-GraphNet is introduced as a framework to model and predict automated vehicle crash severity using spatio-temporal graph neural networks. The study utilizes real-world AV crash data from Texas, constructing fine-grained and region-aggregated spatial graphs. Multimodal data including spatial coordinates, crash times, and narrative descriptions are integrated into the graph representation. Different graph neural network architectures are evaluated, with DSTGCN on the coarse-grained H3 graph showing promising results. The ST-GraphNet achieves a high test accuracy of 97.74%, surpassing fine-grained models. The study underscores the significance of spatial aggregation, dynamic message passing, and multimodal feature integration in understanding the complex spatio-temporal patterns of AV crash severity. <div>
arXiv:2506.08051v1 Announce Type: new 
Abstract: Understanding the spatial and temporal dynamics of automated vehicle (AV) crash severity is critical for advancing urban mobility safety and infrastructure planning. In this work, we introduce ST-GraphNet, a spatio-temporal graph neural network framework designed to model and predict AV crash severity by using both fine-grained and region-aggregated spatial graphs. Using a balanced dataset of 2,352 real-world AV-related crash reports from Texas (2024), including geospatial coordinates, crash timestamps, SAE automation levels, and narrative descriptions, we construct two complementary graph representations: (1) a fine-grained graph with individual crash events as nodes, where edges are defined via spatio-temporal proximity; and (2) a coarse-grained graph where crashes are aggregated into Hexagonal Hierarchical Spatial Indexing (H3)-based spatial cells, connected through hexagonal adjacency. Each node in the graph is enriched with multimodal data, including semantic, spatial, and temporal attributes, including textual embeddings from crash narratives using a pretrained Sentence-BERT model. We evaluate various graph neural network (GNN) architectures, such as Graph Convolutional Networks (GCN), Graph Attention Networks (GAT), and Dynamic Spatio-Temporal GCN (DSTGCN), to classify crash severity and predict high-risk regions. Our proposed ST-GraphNet, which utilizes a DSTGCN backbone on the coarse-grained H3 graph, achieves a test accuracy of 97.74\%, substantially outperforming the best fine-grained model (64.7\% test accuracy). These findings highlight the effectiveness of spatial aggregation, dynamic message passing, and multi-modal feature integration in capturing the complex spatio-temporal patterns underlying AV crash severity.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STAMImputer: Spatio-Temporal Attention MoE for Traffic Data Imputation</title>
<link>https://arxiv.org/abs/2506.08054</link>
<guid>https://arxiv.org/abs/2506.08054</guid>
<content:encoded><![CDATA[
<div> MoE, Mixture of Experts, LrSGAT, dynamic graphs, traffic data imputation <br />
Summary: <br />
The paper introduces STAMImputer, a novel SpatioTemporal Attention Mixture of experts network for traffic data imputation. It addresses the limitations of existing methods by utilizing a Mixture of Experts framework to capture latent spatio-temporal features and their influence weights for effective imputation in block missing scenarios. A unique Low-rank guided Sampling Graph ATtention (LrSGAT) mechanism dynamically balances local and global correlations in road networks, generating dynamic graphs that capture real-time spatial correlations. Experimental results on four traffic datasets demonstrate the superior performance of STAMImputer compared to state-of-the-art approaches. The code for STAMImputer is available on GitHub for further exploration and implementation. <br /> <div>
arXiv:2506.08054v1 Announce Type: new 
Abstract: Traffic data imputation is fundamentally important to support various applications in intelligent transportation systems such as traffic flow prediction. However, existing time-to-space sequential methods often fail to effectively extract features in block-wise missing data scenarios. Meanwhile, the static graph structure for spatial feature propagation significantly constrains the models flexibility in handling the distribution shift issue for the nonstationary traffic data. To address these issues, this paper proposes a SpatioTemporal Attention Mixture of experts network named STAMImputer for traffic data imputation. Specifically, we introduce a Mixture of Experts (MoE) framework to capture latent spatio-temporal features and their influence weights, effectively imputing block missing. A novel Low-rank guided Sampling Graph ATtention (LrSGAT) mechanism is designed to dynamically balance the local and global correlations across road networks. The sampled attention vectors are utilized to generate dynamic graphs that capture real-time spatial correlations. Extensive experiments are conducted on four traffic datasets for evaluation. The result shows STAMImputer achieves significantly performance improvement compared with existing SOTA approaches. Our codes are available at https://github.com/RingBDStack/STAMImupter.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Eliciting Fine-Tuned Transformer Capabilities via Inference-Time Techniques</title>
<link>https://arxiv.org/abs/2506.08060</link>
<guid>https://arxiv.org/abs/2506.08060</guid>
<content:encoded><![CDATA[
<div> supervised fine-tuning, large language models, in-context learning, text generation, linear classification  
Summary:  
Supervised fine-tuning (SFT) for large language models can be approximated by base transformer models using inference-time techniques like in-context learning (ICL), without changing model parameters. The study proves that with ideal assumptions of unlimited computational resources and fine-tuning dataset access, SFT capabilities can be mimicked. Practical scenarios with finite context lengths and partial dataset access are also examined. For text generation tasks with fixed output length or linear classification, datasets of specific sizes can approximate fine-tuned behavior within a given error margin. These results support the resource-efficient deployment of large language models and suggest practical techniques for real-world applications, such as retrieval-augmented generation.  
<br /><br />Summary: <div>
arXiv:2506.08060v1 Announce Type: new 
Abstract: Large language models have transformed natural language processing, yet supervised fine-tuning (SFT) remains computationally intensive. This paper formally proves that capabilities acquired through SFT can be approximated by a base transformer model using inference-time techniques, specifically in-context learning (ICL), without altering model parameters, under idealized assumptions including unbounded computational resources and access to the fine-tuning dataset. We extend these results to practical scenarios with finite context lengths and partial dataset access. For text generation tasks with fixed output length $l$, datasets of size $\mathrm{O}\left( \frac{m V}{\varepsilon^2} \log \frac{m}{\delta} \right)$ or, with bounded context, $\mathrm{O}\left( \frac{l \log V}{\varepsilon^2} \log \frac{1}{\delta} \right)$ suffice to approximate fine-tuned behavior across $m$ contexts within error $\varepsilon$, where $V$ is the vocabulary size and $\delta$ is the failure probability. For linear classification, datasets of size $\mathrm{O}\left( \frac{d}{\varepsilon} \right)$ or, with fixed context, $\mathrm{O}\left( \frac{1}{\varepsilon^2} \log \frac{1}{\delta} \right)$ are sufficient, where $d$ is the input dimension. Grounded in the Turing completeness of transformers, these results provide a theoretical foundation for resource-efficient deployment of large language models, with practical techniques like retrieval-augmented generation bridging theory to real-world applications.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FairDICE: Fairness-Driven Offline Multi-Objective Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.08062</link>
<guid>https://arxiv.org/abs/2506.08062</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, multi-objective optimization, fairness, welfare maximization, offline learning 
Summary:<br /><br />Multi-objective reinforcement learning (MORL) is a framework that aims to optimize policies when faced with conflicting objectives. Linear scalarization is commonly used in MORL to reduce vector-valued returns into scalar signals. However, this approach falls short when it comes to capturing fairness-oriented goals, which require non-linear and non-additive trade-offs. In this work, FairDICE is introduced as the first offline MORL framework that directly optimizes nonlinear welfare objectives. FairDICE utilizes distribution correction estimation to simultaneously consider welfare maximization and distributional regularization. It does not require explicit preference weights or exhaustive weight search, making it stable and sample-efficient. Through various offline benchmarks, FairDICE shows strong performance in terms of fairness compared to existing baselines.<br /> <div>
arXiv:2506.08062v1 Announce Type: new 
Abstract: Multi-objective reinforcement learning (MORL) aims to optimize policies in the presence of conflicting objectives, where linear scalarization is commonly used to reduce vector-valued returns into scalar signals. While effective for certain preferences, this approach cannot capture fairness-oriented goals such as Nash social welfare or max-min fairness, which require nonlinear and non-additive trade-offs. Although several online algorithms have been proposed for specific fairness objectives, a unified approach for optimizing nonlinear welfare criteria in the offline setting-where learning must proceed from a fixed dataset-remains unexplored. In this work, we present FairDICE, the first offline MORL framework that directly optimizes nonlinear welfare objective. FairDICE leverages distribution correction estimation to jointly account for welfare maximization and distributional regularization, enabling stable and sample-efficient learning without requiring explicit preference weights or exhaustive weight search. Across multiple offline benchmarks, FairDICE demonstrates strong fairness-aware performance compared to existing baselines.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lite-RVFL: A Lightweight Random Vector Functional-Link Neural Network for Learning Under Concept Drift</title>
<link>https://arxiv.org/abs/2506.08063</link>
<guid>https://arxiv.org/abs/2506.08063</guid>
<content:encoded><![CDATA[
<div> Keywords: concept drift, online learning, random vector functional-link network, lightweight, real-time applications

Summary: 
Lite-RVFL is introduced as a lightweight, fast, and efficient random vector functional-link network designed to adapt to concept drift without the need for drift detection or model retraining. It utilizes a novel objective function that assigns exponentially increasing weights to new samples, giving more importance to recent data and allowing for timely adaptation. Theoretical analysis validates the feasibility of this objective function for drift adaptation, and an efficient incremental update rule is derived. Experimental results on a real-world safety assessment task demonstrate Lite-RVFL's efficiency, effectiveness in adapting to drift, and potential to capture temporal patterns. The source code for Lite-RVFL is available on GitHub at https://github.com/songqiaohu/Lite-RVFL. 

<br /><br />Summary: Lite-RVFL is a lightweight and efficient random vector functional-link network that can adapt to concept drift in real-time applications without the need for drift detection or retraining. It achieves this through a novel objective function that prioritizes recent data, ensuring timely adaptation. Theoretical analysis supports the effectiveness of this approach, and experimental results demonstrate Lite-RVFL's ability to efficiently adapt to changing data distributions and capture temporal patterns. The source code for Lite-RVFL is publicly available for further research and applications. <div>
arXiv:2506.08063v1 Announce Type: new 
Abstract: The change in data distribution over time, also known as concept drift, poses a significant challenge to the reliability of online learning methods. Existing methods typically require model retraining or drift detection, both of which demand high computational costs and are often unsuitable for real-time applications. To address these limitations, a lightweight, fast and efficient random vector functional-link network termed Lite-RVFL is proposed, capable of adapting to concept drift without drift detection and retraining. Lite-RVFL introduces a novel objective function that assigns weights exponentially increasing to new samples, thereby emphasizing recent data and enabling timely adaptation. Theoretical analysis confirms the feasibility of this objective function for drift adaptation, and an efficient incremental update rule is derived. Experimental results on a real-world safety assessment task validate the efficiency, effectiveness in adapting to drift, and potential to capture temporal patterns of Lite-RVFL. The source code is available at https://github.com/songqiaohu/Lite-RVFL.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Info-Coevolution: An Efficient Framework for Data Model Coevolution</title>
<link>https://arxiv.org/abs/2506.08070</link>
<guid>https://arxiv.org/abs/2506.08070</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, active learning, dataset construction, annotation, Info-Coevolution

Summary: 
Info-Coevolution addresses the challenge of dataset construction and training efficiency in machine learning. By selectively annotating data through online selective annotation, it allows models and data to coevolve without introducing bias or increasing pipeline complexity. This approach reduces annotation and training costs by 32% for datasets like ImageNet-1K without sacrificing performance. The framework can automatically determine the saving ratio without the need for manual tuning and can further reduce the annotation ratio to 50% with semi-supervised learning. Additionally, Info-Coevolution explores the use of unlabeled open-source data for dataset enhancement through a retrieval-based approach. The code for Info-Coevolution is available on GitHub for further exploration and implementation.<br /><br />Summary: <div>
arXiv:2506.08070v1 Announce Type: new 
Abstract: Machine learning relies heavily on data, yet the continuous growth of real-world data poses challenges for efficient dataset construction and training. A fundamental yet unsolved question is: given our current model and data, does a new data (sample/batch) need annotation/learning? Conventional approaches retain all available data, leading to non-optimal data and training efficiency. Active learning aims to reduce data redundancy by selecting a subset of samples to annotate, while it increases pipeline complexity and introduces bias. In this work, we propose Info-Coevolution, a novel framework that efficiently enables models and data to coevolve through online selective annotation with no bias. Leveraging task-specific models (and open-source models), it selectively annotates and integrates online and web data to improve datasets efficiently. For real-world datasets like ImageNet-1K, Info-Coevolution reduces annotation and training costs by 32\% without performance loss. It is able to automatically give the saving ratio without tuning the ratio. It can further reduce the annotation ratio to 50\% with semi-supervised learning. We also explore retrieval-based dataset enhancement using unlabeled open-source data. Code is available at https://github.com/NUS-HPC-AI-Lab/Info-Coevolution/.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Pre-Trained Time Series Models for Electricity Price Forecasting</title>
<link>https://arxiv.org/abs/2506.08113</link>
<guid>https://arxiv.org/abs/2506.08113</guid>
<content:encoded><![CDATA[
<div> pretrained models, electricity price forecasting, time series forecasting, power trading, state-of-the-art

Summary:
- Accurate electricity price forecasting is essential for effective decision-making in power trading on the spot market.
- Several state-of-the-art pretrained models were benchmarked against established statistical and machine learning methods for electricity price forecasting.
- Chronos-Bolt and Time-MoE were identified as the strongest among the time series foundation models, performing similarly to traditional models.
- The biseasonal MSTL model, capturing daily and weekly seasonality, consistently outperformed other models across different countries and evaluation metrics.
- The study suggests that while pretrained models show promise in electricity price forecasting, models like the biseasonal MSTL remain a robust choice for accurate predictions. 

<br /><br />Summary: <div>
arXiv:2506.08113v1 Announce Type: new 
Abstract: Accurate electricity price forecasting (EPF) is crucial for effective decision-making in power trading on the spot market. While recent advances in generative artificial intelligence (GenAI) and pre-trained large language models (LLMs) have inspired the development of numerous time series foundation models (TSFMs) for time series forecasting, their effectiveness in EPF remains uncertain. To address this gap, we benchmark several state-of-the-art pretrained models--Chronos-Bolt, Chronos-T5, TimesFM, Moirai, Time-MoE, and TimeGPT--against established statistical and machine learning (ML) methods for EPF. Using 2024 day-ahead auction (DAA) electricity prices from Germany, France, the Netherlands, Austria, and Belgium, we generate daily forecasts with a one-day horizon. Chronos-Bolt and Time-MoE emerge as the strongest among the TSFMs, performing on par with traditional models. However, the biseasonal MSTL model, which captures daily and weekly seasonality, stands out for its consistent performance across countries and evaluation metrics, with no TSFM statistically outperforming it.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bingo: Boosting Efficient Reasoning of LLMs via Dynamic and Significance-based Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.08125</link>
<guid>https://arxiv.org/abs/2506.08125</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, language models, reasoning, efficiency, Bingo

Summary:
- Large language models often produce verbose or redundant outputs, leading to inefficiencies.
- Current reinforcement learning approaches focus on accuracy without considering reasoning efficiency.
- Bingo is a novel RL framework that incorporates a significance-aware length reward and dynamic length reward to enhance efficient reasoning.
- The significance-aware length reward guides the model to reduce only insignificant tokens gradually.
- The dynamic length reward initially encourages elaborate reasoning for hard questions but decays over time to improve overall efficiency.
<br /><br />Summary: <div>
arXiv:2506.08125v1 Announce Type: new 
Abstract: Large language models have demonstrated impressive reasoning capabilities, yet they often suffer from inefficiencies due to unnecessarily verbose or redundant outputs. While many works have explored reinforcement learning (RL) to enhance reasoning abilities, most primarily focus on improving accuracy, with limited attention to reasoning efficiency. Some existing approaches introduce direct length-based rewards to encourage brevity, but this often leads to noticeable drops in accuracy. In this paper, we propose Bingo, an RL framework that advances length-based reward design to boost efficient reasoning. Bingo incorporates two key mechanisms: a significance-aware length reward, which gradually guides the model to reduce only insignificant tokens, and a dynamic length reward, which initially encourages elaborate reasoning for hard questions but decays over time to improve overall efficiency. Experiments across multiple reasoning benchmarks show that Bingo improves both accuracy and efficiency. It outperforms the vanilla reward and several other length-based reward baselines in RL, achieving a favorable trade-off between accuracy and efficiency. These results underscore the potential of training LLMs explicitly for efficient reasoning.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nearness of Neighbors Attention for Regression in Supervised Finetuning</title>
<link>https://arxiv.org/abs/2506.08139</link>
<guid>https://arxiv.org/abs/2506.08139</guid>
<content:encoded><![CDATA[
<div> Neural Networks, Supervised Machine Learning, Feature Extraction, k-nearest neighbors, Support Vector Machines
Summary:
NONA is a new regression layer that combines neural network attention mechanics with a learned attention-masking scheme to create a differentiable version of the k-NN regression algorithm. By incorporating traditional algorithms like k-NN into supervised fine-tuning (SFT), NONA outperforms both dense layer prediction and k-NN on SFT embeddings for regression across multiple unstructured datasets. The study highlights the potential benefits of directly integrating traditional algorithms into feature extraction processes in supervised machine learning tasks. This approach demonstrates improved performance in prediction tasks, showcasing the value of combining the feature extraction capabilities of neural networks with the predictive power of traditional algorithms like k-nearest neighbors. <div>
arXiv:2506.08139v1 Announce Type: new 
Abstract: It is common in supervised machine learning to combine the feature extraction capabilities of neural networks with the predictive power of traditional algorithms, such as k-nearest neighbors (k-NN) or support vector machines. This procedure involves performing supervised fine-tuning (SFT) on a domain-appropriate feature extractor, followed by training a traditional predictor on the resulting SFT embeddings. When used in this manner, traditional predictors often deliver increased performance over the SFT model itself, despite the fine-tuned feature extractor yielding embeddings specifically optimized for prediction by the neural network's final dense layer. This suggests that directly incorporating traditional algorithms into SFT as prediction layers may further improve performance. However, many traditional algorithms have not been implemented as neural network layers due to their non-differentiable nature and their unique optimization requirements. As a step towards solving this problem, we introduce the Nearness of Neighbors Attention (NONA) regression layer. NONA uses the mechanics of neural network attention and a novel learned attention-masking scheme to yield a differentiable proxy of the k-NN regression algorithm. Results on multiple unstructured datasets show improved performance over both dense layer prediction and k-NN on SFT embeddings for regression.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoSDT: Scaling Data-Driven Discovery Tasks Toward Open Co-Scientists</title>
<link>https://arxiv.org/abs/2506.08140</link>
<guid>https://arxiv.org/abs/2506.08140</guid>
<content:encoded><![CDATA[
<div> Pipeline, AutoSDT, dataset, coding tasks, LLMs
Summary:
AutoSDT introduces an automatic pipeline, AutoSDT, that collects high-quality coding tasks for data-driven scientific discovery. Leveraging the coding abilities of LLMs, it gathers a dataset of 5,404 diverse coding tasks covering multiple scientific disciplines and Python packages. Expert feedback indicates that 93% of tasks are ecologically valid, and 92.2% of synthesized programs are functionally correct. The trained AutoSDT-Coder models show significant performance improvements on challenging data-driven discovery benchmarks, matching the success rate of GPT-4o on ScienceAgentBench and improving the hypothesis matching score on DiscoveryBench by 17.4%. This work addresses the data scarcity issue in building AI co-scientists and demonstrates the effectiveness of AutoSDT in enhancing AI capabilities for scientific discovery.<br /><br />Summary: <div>
arXiv:2506.08140v1 Announce Type: new 
Abstract: Despite long-standing efforts in accelerating scientific discovery with AI, building AI co-scientists remains challenging due to limited high-quality data for training and evaluation. To tackle this data scarcity issue, we present AutoSDT, an automatic pipeline that collects high-quality coding tasks in real-world data-driven discovery workflows. AutoSDT leverages the coding capabilities and parametric knowledge of LLMs to search for diverse sources, select ecologically valid tasks, and synthesize accurate task instructions and code solutions. Using our pipeline, we construct AutoSDT-5K, a dataset of 5,404 coding tasks for data-driven discovery that covers four scientific disciplines and 756 unique Python packages. To the best of our knowledge, AutoSDT-5K is the only automatically collected and the largest open dataset for data-driven scientific discovery. Expert feedback on a subset of 256 tasks shows the effectiveness of AutoSDT: 93% of the collected tasks are ecologically valid, and 92.2% of the synthesized programs are functionally correct. Trained on AutoSDT-5K, the Qwen2.5-Coder-Instruct LLM series, dubbed AutoSDT-Coder, show substantial improvement on two challenging data-driven discovery benchmarks, ScienceAgentBench and DiscoveryBench. Most notably, AutoSDT-Coder-32B reaches the same level of performance as GPT-4o on ScienceAgentBench with a success rate of 7.8%, doubling the performance of its base model. On DiscoveryBench, it lifts the hypothesis matching score to 8.1, bringing a 17.4% relative improvement and closing the gap between open-weight models and GPT-4o.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Spectral Clustering under Fairness Constraints</title>
<link>https://arxiv.org/abs/2506.08143</link>
<guid>https://arxiv.org/abs/2506.08143</guid>
<content:encoded><![CDATA[
<div> Keywords: spectral clustering, group fairness constraints, fairness, difference of convex functions, computational efficiency

Summary: 
In this paper, the focus is on fair decision-making algorithms, specifically spectral clustering with group fairness constraints. The new method, Fair SC, is introduced using a difference of convex functions (DC) framework, augmented with a variable strategy and an alternating direction method of multipliers algorithm. This approach improves computational efficiency by avoiding the need for costly eigendecomposition. Numerical experiments demonstrate the effectiveness of Fair SC on various benchmarks, showing significant speedups in computation time as problem size increases. The results indicate a substantial advancement towards implementing fair clustering in practical applications. <div>
arXiv:2506.08143v1 Announce Type: new 
Abstract: Fairness of decision-making algorithms is an increasingly important issue. In this paper, we focus on spectral clustering with group fairness constraints, where every demographic group is represented in each cluster proportionally as in the general population. We present a new efficient method for fair spectral clustering (Fair SC) by casting the Fair SC problem within the difference of convex functions (DC) framework. To this end, we introduce a novel variable augmentation strategy and employ an alternating direction method of multipliers type of algorithm adapted to DC problems. We show that each associated subproblem can be solved efficiently, resulting in higher computational efficiency compared to prior work, which required a computationally expensive eigendecomposition. Numerical experiments demonstrate the effectiveness of our approach on both synthetic and real-world benchmarks, showing significant speedups in computation time over prior art, especially as the problem size grows. This work thus represents a considerable step forward towards the adoption of fair clustering in real-world applications.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fully data-driven inverse hyperelasticity with hyper-network neural ODE fields</title>
<link>https://arxiv.org/abs/2506.08146</link>
<guid>https://arxiv.org/abs/2506.08146</guid>
<content:encoded><![CDATA[
<div> Fourier features, neural network, ordinary neural differential equations (NODEs), hyper-network, mechanical properties<br />
Summary:<br />
The article introduces a new framework for identifying mechanical properties of heterogeneous materials using a continuous approximation of the strain field obtained from displacement measurements. It employs a neural network with Fourier features to capture sharp gradients. The approach uses ordinary neural differential equations (NODEs) to discover constitutive equations, allowing for representation of arbitrary materials while adhering to theory constraints. A hyper-network is defined to account for heterogeneity, with parameters optimized through a multi-objective loss function. The framework is showcased with numerical examples demonstrating its robustness in identifying material properties in various scenarios, including noise presence and application to experimental data. The proposed method offers a versatile and effective alternative to traditional inverse methods, making it valuable for material characterization studies. <br /> <div>
arXiv:2506.08146v1 Announce Type: new 
Abstract: We propose a new framework for identifying mechanical properties of heterogeneous materials without a closed-form constitutive equation. Given a full-field measurement of the displacement field, for instance as obtained from digital image correlation (DIC), a continuous approximation of the strain field is obtained by training a neural network that incorporates Fourier features to effectively capture sharp gradients in the data. A physics-based data-driven method built upon ordinary neural differential equations (NODEs) is employed to discover constitutive equations. The NODE framework can represent arbitrary materials while satisfying constraints in the theory of constitutive equations by default. To account for heterogeneity, a hyper-network is defined, where the input is the material coordinate system, and the output is the NODE-based constitutive equation. The parameters of the hyper-network are optimized by minimizing a multi-objective loss function that includes penalty terms for violations of the strong form of the equilibrium equations of elasticity and the associated Neumann boundary conditions. We showcase the framework with several numerical examples, including heterogeneity arising from variations in material parameters, spatial transitions from isotropy to anisotropy, material identification in the presence of noise, and, ultimately, application to experimental data. As the numerical results suggest, the proposed approach is robust and general in identifying the mechanical properties of heterogeneous materials with very few assumptions, making it a suitable alternative to classical inverse methods.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BLUR: A Bi-Level Optimization Approach for LLM Unlearning</title>
<link>https://arxiv.org/abs/2506.08164</link>
<guid>https://arxiv.org/abs/2506.08164</guid>
<content:encoded><![CDATA[
<div> Algorithm, Unlearning, Large Language Models, Hierarchical Structure, Bi-Level Optimization

Summary: 
The article discusses the importance of unlearning knowledge and capabilities in large language models (LLMs) for compliance with data regulations and ethical AI practices. The current popular formulation using a weighted sum of forget and retain loss results in performance degradation due to the forget-retain loss trade-off. The authors propose a hierarchical structure for the unlearning problem, prioritizing the forget problem over the retain problem. This leads to a bi-level optimization formulation where the lower-level objective minimizes forget loss while the upper-level objective maintains model utility. The novel Bi-Level UnleaRning (BLUR) algorithm based on this formulation outperforms existing algorithms in various unlearning tasks, models, and metrics. The algorithm not only offers strong theoretical guarantees but also demonstrates superior performance in extensive experiments. The code for BLUR is available on GitHub for further exploration and implementation. 

Summary: <br /><br /> <div>
arXiv:2506.08164v1 Announce Type: new 
Abstract: Enabling large language models (LLMs) to unlearn knowledge and capabilities acquired during training has proven vital for ensuring compliance with data regulations and promoting ethical practices in generative AI. Although there are growing interests in developing various unlearning algorithms, it remains unclear how to best formulate the unlearning problem. The most popular formulation uses a weighted sum of forget and retain loss, but it often leads to performance degradation due to the inherent trade-off between forget and retain losses. In this work, we argue that it is important to model the hierarchical structure of the unlearning problem, where the forget problem (which \textit{unlearns} certain knowledge and/or capabilities) takes priority over the retain problem (which preserves model utility). This hierarchical structure naturally leads to a bi-level optimization formulation where the lower-level objective focuses on minimizing the forget loss, while the upper-level objective aims to maintain the model's utility. Based on this new formulation, we propose a novel algorithm, termed Bi-Level UnleaRning (\texttt{BLUR}), which not only possesses strong theoretical guarantees but more importantly, delivers superior performance. In particular, our extensive experiments demonstrate that \texttt{BLUR} consistently outperforms all the state-of-the-art algorithms across various unlearning tasks, models, and metrics. Codes are available at https://github.com/OptimAI-Lab/BLURLLMUnlearning.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniVarFL: Uniformity and Variance Regularized Federated Learning for Heterogeneous Data</title>
<link>https://arxiv.org/abs/2506.08167</link>
<guid>https://arxiv.org/abs/2506.08167</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated Learning, Non-IID data, Classifier Bias, Regularization, UniVarFL 

Summary: 
UniVarFL is a novel Federated Learning framework designed to address the challenge of non-IID data by directly addressing local classifier bias. The framework utilizes two key regularization strategies: Classifier Variance Regularization, which aligns class-wise probability distributions to mitigate bias, and Hyperspherical Uniformity Regularization, which promotes a uniform distribution of feature representations. By implementing these strategies at the client level during local training, UniVarFL eliminates the need for global model dependencies. Extensive experiments on various benchmark datasets have shown that UniVarFL surpasses existing methods in terms of accuracy. This positions UniVarFL as a scalable and efficient solution for real-world Federated Learning deployments, particularly in resource-constrained environments. The framework not only improves model performance but also enhances generalization capabilities under diverse data distributions. The code for UniVarFL is available on GitHub for further exploration and implementation. 

<br /><br />Summary: <div>
arXiv:2506.08167v1 Announce Type: new 
Abstract: Federated Learning (FL) often suffers from severe performance degradation when faced with non-IID data, largely due to local classifier bias. Traditional remedies such as global model regularization or layer freezing either incur high computational costs or struggle to adapt to feature shifts. In this work, we propose UniVarFL, a novel FL framework that emulates IID-like training dynamics directly at the client level, eliminating the need for global model dependency. UniVarFL leverages two complementary regularization strategies during local training: Classifier Variance Regularization, which aligns class-wise probability distributions with those expected under IID conditions, effectively mitigating local classifier bias; and Hyperspherical Uniformity Regularization, which encourages a uniform distribution of feature representations across the hypersphere, thereby enhancing the model's ability to generalize under diverse data distributions. Extensive experiments on multiple benchmark datasets demonstrate that UniVarFL outperforms existing methods in accuracy, highlighting its potential as a highly scalable and efficient solution for real-world FL deployments, especially in resource-constrained settings. Code: https://github.com/sunnyinAI/UniVarFL
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Learning on Stochastic Neural Networks</title>
<link>https://arxiv.org/abs/2506.08169</link>
<guid>https://arxiv.org/abs/2506.08169</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated learning, edge computing, stochastic neural networks, latent noise, data accuracy<br />
Summary:<br />
Federated learning, a machine learning approach utilizing edge computing, aims to optimize models while preserving user privacy. However, the presence of latent noise in local datasets may lead to inaccuracies. To tackle this issue, the use of stochastic neural networks as local models within the federated learning framework is proposed. These networks not only aid in estimating the true data states but also help quantify latent noise. Referred to as Federated stochastic neural networks, this approach demonstrates effectiveness in handling non-independent and identically distributed data through numerical experiments. By incorporating stochastic neural networks, this method enhances data accuracy and improves the performance of federated learning systems. <div>
arXiv:2506.08169v1 Announce Type: new 
Abstract: Federated learning is a machine learning paradigm that leverages edge computing on client devices to optimize models while maintaining user privacy by ensuring that local data remains on the device. However, since all data is collected by clients, federated learning is susceptible to latent noise in local datasets. Factors such as limited measurement capabilities or human errors may introduce inaccuracies in client data. To address this challenge, we propose the use of a stochastic neural network as the local model within the federated learning framework. Stochastic neural networks not only facilitate the estimation of the true underlying states of the data but also enable the quantification of latent noise. We refer to our federated learning approach, which incorporates stochastic neural networks as local models, as Federated stochastic neural networks. We will present numerical experiments demonstrating the performance and effectiveness of our method, particularly in handling non-independent and identically distributed data.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedGA-Tree: Federated Decision Tree using Genetic Algorithm</title>
<link>https://arxiv.org/abs/2506.08176</link>
<guid>https://arxiv.org/abs/2506.08176</guid>
<content:encoded><![CDATA[
<div> Federated Learning, Decision Trees, Genetic Algorithm, Data Privacy, Collaborative Training
<br />
Summary: 
This paper explores the use of Genetic Algorithm in Federated Learning to construct personalized decision trees that can handle both categorical and numerical data for classification and regression tasks. Existing methods in Federated Learning have primarily focused on parametric gradient-based models, neglecting nonparametric models like decision trees. The proposed approach outperforms traditional decision trees trained on local data and a benchmark algorithm in comprehensive experiments. The use of Genetic Algorithm allows for the creation of personalized decision trees, catering to individual client data while maintaining privacy and data security. This method presents a promising solution for Federated Learning tasks that require the use of decision trees while ensuring data privacy and model accuracy. 
<br /> <div>
arXiv:2506.08176v1 Announce Type: new 
Abstract: In recent years, with rising concerns for data privacy, Federated Learning has gained prominence, as it enables collaborative training without the aggregation of raw data from participating clients. However, much of the current focus has been on parametric gradient-based models, while nonparametric counterparts such as decision tree are relatively understudied. Existing methods for adapting decision trees to Federated Learning generally combine a greedy tree-building algorithm with differential privacy to produce a global model for all clients. These methods are limited to classification trees and categorical data due to the constraints of differential privacy. In this paper, we explore an alternative approach that utilizes Genetic Algorithm to facilitate the construction of personalized decision trees and accommodate categorical and numerical data, thus allowing for both classification and regression trees. Comprehensive experiments demonstrate that our method surpasses decision trees trained solely on local data and a benchmark algorithm.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Correlated Noise Mechanisms for Differentially Private Learning</title>
<link>https://arxiv.org/abs/2506.08201</link>
<guid>https://arxiv.org/abs/2506.08201</guid>
<content:encoded><![CDATA[
<div> DP, correlated noise, differential privacy, AI, machine learning <br />
<br />
Summary: This monograph delves into the design and analysis of correlated noise mechanisms for differential privacy (DP) in the context of private training of AI and machine learning models. Unlike traditional DP mechanisms that introduce independent noise at each step of a stochastic gradient learning algorithm, recent research has shown the benefits of introducing correlations in the noise to enhance privacy-utility trade-offs. These correlated noise mechanisms, also known as matrix or factorization mechanisms, and DP-Follow-the-Regularized-Leader (DP-FTRL) when applied to learning algorithms, have proven to be effective in practice and have been deployed globally on an industrial scale. By carefully balancing the noise added at different steps, correlated noise mechanisms can improve the privacy-utility trade-off and provide better protection for training data in AI and machine learning algorithms. <div>
arXiv:2506.08201v1 Announce Type: new 
Abstract: This monograph explores the design and analysis of correlated noise mechanisms for differential privacy (DP), focusing on their application to private training of AI and machine learning models via the core primitive of estimation of weighted prefix sums. While typical DP mechanisms inject independent noise into each step of a stochastic gradient (SGD) learning algorithm in order to protect the privacy of the training data, a growing body of recent research demonstrates that introducing (anti-)correlations in the noise can significantly improve privacy-utility trade-offs by carefully canceling out some of the noise added on earlier steps in subsequent steps. Such correlated noise mechanisms, known variously as matrix mechanisms, factorization mechanisms, and DP-Follow-the-Regularized-Leader (DP-FTRL) when applied to learning algorithms, have also been influential in practice, with industrial deployment at a global scale.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Machine Learning Approach to Generate Residual Stress Distributions using Sparse Characterization Data in Friction-Stir Processed Parts</title>
<link>https://arxiv.org/abs/2506.08205</link>
<guid>https://arxiv.org/abs/2506.08205</guid>
<content:encoded><![CDATA[
<div> machine learning, residual stress, full-field distribution, structural integrity, experimental effort

Summary:
- Residual stresses can negatively impact performance and longevity of components.
- Accurately determining full-field stress distributions is crucial but experimentally challenging.
- A Residual Stress Generator (RSG) using machine learning was proposed to infer stresses from limited measurements.
- An extensive dataset was constructed through process simulations with diverse parameters.
- A U-Net-based ML model was trained and evaluated for generating simulated stresses, showing high predictive accuracy and generalization.
- The RSG successfully predicted experimentally characterized data, showcasing its effectiveness in reducing experimental efforts. 

<br /><br />Summary: <div>
arXiv:2506.08205v1 Announce Type: new 
Abstract: Residual stresses, which remain within a component after processing, can deteriorate performance. Accurately determining their full-field distributions is essential for optimizing the structural integrity and longevity. However, the experimental effort required for full-field characterization is impractical. Given these challenges, this work proposes a machine learning (ML) based Residual Stress Generator (RSG) to infer full-field stresses from limited measurements. An extensive dataset was initially constructed by performing numerous process simulations with a diverse parameter set. A ML model based on U-Net architecture was then trained to learn the underlying structure through systematic hyperparameter tuning. Then, the model's ability to generate simulated stresses was evaluated, and it was ultimately tested on actual characterization data to validate its effectiveness. The model's prediction of simulated stresses shows that it achieved excellent predictive accuracy and exhibited a significant degree of generalization, indicating that it successfully learnt the latent structure of residual stress distribution. The RSG's performance in predicting experimentally characterized data highlights the feasibility of the proposed approach in providing a comprehensive understanding of residual stress distributions from limited measurements, thereby significantly reducing experimental efforts.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What makes an Ensemble (Un) Interpretable?</title>
<link>https://arxiv.org/abs/2506.08216</link>
<guid>https://arxiv.org/abs/2506.08216</guid>
<content:encoded><![CDATA[
<div> interpretability, ensemble models, computational complexity theory, base models, explanations

Summary:
This study explores the interpretability of ensemble models in machine learning, highlighting the challenges and factors that influence it. By applying concepts from computational complexity theory, the researchers examine the impact of factors such as the number, size, and type of base models on interpretability. The analysis reveals that interpreting ensembles can be intractable even with small numbers of base models, with the complexity varying significantly depending on the type of models used. Surprisingly, ensembles of decision trees are more efficiently interpretable compared to ensembles with linear models. The findings emphasize the importance of considering interpretability through a computational complexity lens for a better understanding of ensemble models in machine learning. 

<br /><br />Summary: <div>
arXiv:2506.08216v1 Announce Type: new 
Abstract: Ensemble models are widely recognized in the ML community for their limited interpretability. For instance, while a single decision tree is considered interpretable, ensembles of trees (e.g., boosted trees) are often treated as black-boxes. Despite this folklore recognition, there remains a lack of rigorous mathematical understanding of what particularly makes an ensemble (un)-interpretable, including how fundamental factors like the (1) *number*, (2) *size*, and (3) *type* of base models influence its interpretability. In this work, we seek to bridge this gap by applying concepts from computational complexity theory to study the challenges of generating explanations for various ensemble configurations. Our analysis uncovers nuanced complexity patterns influenced by various factors. For example, we demonstrate that under standard complexity assumptions like P$\neq$NP, interpreting ensembles remains intractable even when base models are of constant size. Surprisingly, the complexity changes drastically with the number of base models: small ensembles of decision trees are efficiently interpretable, whereas interpreting ensembles with even a constant number of linear models remains intractable. We believe that our findings provide a more robust foundation for understanding the interpretability of ensembles, emphasizing the benefits of examining it through a computational complexity lens.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mondrian: Transformer Operators via Domain Decomposition</title>
<link>https://arxiv.org/abs/2506.08226</link>
<guid>https://arxiv.org/abs/2506.08226</guid>
<content:encoded><![CDATA[
<div> Keywords: Operator learning, Partial differential equations, Transformer-based models, Domain decomposition, Attention mechanisms

Summary: 
Mondrian introduces a novel approach for scaling transformer-based operator models to high-resolution, multiscale domains. By decomposing the domain into non-overlapping subdomains and applying attention over sequences of subdomain-restricted functions, Mondrian effectively decouples attention from discretization, enabling efficient modeling of PDEs. The formulation allows for both local and global interactions through hierarchical windowed and neighborhood attention. Mondrian outperforms existing methods on Allen-Cahn and Navier-Stokes PDEs, showcasing its ability to scale resolution without the need for retraining. This innovative approach demonstrates the potential of domain-decomposed attention for developing scalable and versatile neural operators in the context of data-driven modeling of PDEs. <br /><br />Summary: <div>
arXiv:2506.08226v1 Announce Type: new 
Abstract: Operator learning enables data-driven modeling of partial differential equations (PDEs) by learning mappings between function spaces. However, scaling transformer-based operator models to high-resolution, multiscale domains remains a challenge due to the quadratic cost of attention and its coupling to discretization. We introduce \textbf{Mondrian}, transformer operators that decompose a domain into non-overlapping subdomains and apply attention over sequences of subdomain-restricted functions. Leveraging principles from domain decomposition, Mondrian decouples attention from discretization. Within each subdomain, it replaces standard layers with expressive neural operators, and attention across subdomains is computed via softmax-based inner products over functions. The formulation naturally extends to hierarchical windowed and neighborhood attention, supporting both local and global interactions. Mondrian achieves strong performance on Allen-Cahn and Navier-Stokes PDEs, demonstrating resolution scaling without retraining. These results highlight the promise of domain-decomposed attention for scalable and general-purpose neural operators.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Laws of Motion Forecasting and Planning -- A Technical Report</title>
<link>https://arxiv.org/abs/2506.08228</link>
<guid>https://arxiv.org/abs/2506.08228</guid>
<content:encoded><![CDATA[
<div> Keywords: encoder-decoder, autoregressive transformer models, motion forecasting, autonomous driving, scaling laws 

Summary: 
- The study explores scaling laws of encoder-decoder autoregressive transformer models in joint motion forecasting and planning for autonomous driving, showing improved performance with increased compute budget.
- Model training loss is strongly correlated with evaluation metrics, indicating the importance of monitoring training progress for model development.
- Closed-loop metrics also improve with scaling, suggesting the relevance of open-loop metrics for model refinement.
- Optimal scaling of transformer parameters and training data size reveals that model size should increase 1.5x faster than dataset size with growing compute budget.
- Inference-time compute scaling demonstrates the competitiveness of smaller models through sampling and clustering, with larger models becoming more efficient beyond a certain point.
- Training on general driving data of other agents can enhance the ego-agent's performance, addressing data scarcity for training large capacity models in robotics. 

<br /><br />Summary: <div>
arXiv:2506.08228v1 Announce Type: new 
Abstract: We study the empirical scaling laws of a family of encoder-decoder autoregressive transformer models on the task of joint motion forecasting and planning in the autonomous driving domain. Using a 500 thousand hours driving dataset, we demonstrate that, similar to language modeling, model performance improves as a power-law function of the total compute budget, and we observe a strong correlation between model training loss and model evaluation metrics. Most interestingly, closed-loop metrics also improve with scaling, which has important implications for the suitability of open-loop metrics for model development and hill climbing. We also study the optimal scaling of the number of transformer parameters and the training data size for a training compute-optimal model. We find that as the training compute budget grows, optimal scaling requires increasing the model size 1.5x as fast as the dataset size. We also study inference-time compute scaling, where we observe that sampling and clustering the output of smaller models makes them competitive with larger models, up to a crossover point beyond which a larger models becomes more inference-compute efficient. Overall, our experimental results demonstrate that optimizing the training and inference-time scaling properties of motion forecasting and planning models is a key lever for improving their performance to address a wide variety of driving scenarios. Finally, we briefly study the utility of training on general logged driving data of other agents to improve the performance of the ego-agent, an important research area to address the scarcity of robotics data for large capacity models training.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ensuring Reliability of Curated EHR-Derived Data: The Validation of Accuracy for LLM/ML-Extracted Information and Data (VALID) Framework</title>
<link>https://arxiv.org/abs/2506.08231</link>
<guid>https://arxiv.org/abs/2506.08231</guid>
<content:encoded><![CDATA[
<div> framework, LLM, clinical data, quality assurance, oncology

Summary:<br />
Large language models (LLMs) are being used for extracting clinical data from electronic health records (EHRs) in oncology. A new comprehensive framework is proposed in this paper to evaluate the quality of data extracted by LLMs. The framework includes benchmarking against expert human abstraction, automated verification checks, replication analyses, and bias assessment. It aims to ensure reliability, accuracy, and fairness in the extracted data, addressing the unique error modes associated with LLMs. By identifying areas for improvement, detecting latent errors, and confirming dataset fitness-for-purpose, this framework enhances industry standards and promotes transparent and trustworthy use of AI-powered evidence generation in oncology research and practice. <div>
arXiv:2506.08231v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used to extract clinical data from electronic health records (EHRs), offering significant improvements in scalability and efficiency for real-world data (RWD) curation in oncology. However, the adoption of LLMs introduces new challenges in ensuring the reliability, accuracy, and fairness of extracted data, which are essential for research, regulatory, and clinical applications. Existing quality assurance frameworks for RWD and artificial intelligence do not fully address the unique error modes and complexities associated with LLM-extracted data. In this paper, we propose a comprehensive framework for evaluating the quality of clinical data extracted by LLMs. The framework integrates variable-level performance benchmarking against expert human abstraction, automated verification checks for internal consistency and plausibility, and replication analyses comparing LLM-extracted data to human-abstracted datasets or external standards. This multidimensional approach enables the identification of variables most in need of improvement, systematic detection of latent errors, and confirmation of dataset fitness-for-purpose in real-world research. Additionally, the framework supports bias assessment by stratifying metrics across demographic subgroups. By providing a rigorous and transparent method for assessing LLM-extracted RWD, this framework advances industry standards and supports the trustworthy use of AI-powered evidence generation in oncology research and practice.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dealing with the Evil Twins: Improving Random Augmentation by Addressing Catastrophic Forgetting of Diverse Augmentations</title>
<link>https://arxiv.org/abs/2506.08240</link>
<guid>https://arxiv.org/abs/2506.08240</guid>
<content:encoded><![CDATA[
<div> generalization, data augmentation, random augmentation, catastrophic forgetting, single source domain generalization

Summary:
This paper discusses the role of data augmentation in enhancing out-of-distribution generalization. While targeted augmentations are effective but costly, random augmentation is considered suboptimal due to limited effects. The study reveals that random augmentation's stochastic nature can lead to colliding augmentations, causing feature distortion akin to catastrophic forgetting. To mitigate this, the paper proposes a simple solution to improve random augmentation's generalization effect by addressing forgetting. The solution proves to be highly effective across various single source domain generalization benchmarks. The research aims to shed light on enhancing generalization through random augmentation, offering a promising approach to improving model performance in challenging domains. <div>
arXiv:2506.08240v1 Announce Type: new 
Abstract: Data augmentation is a promising tool for enhancing out-of-distribution generalization, where the key is to produce diverse, challenging variations of the source domain via costly targeted augmentations that maximize its generalization effect. Conversely, random augmentation is inexpensive but is deemed suboptimal due to its limited effect. In this paper, we revisit random augmentation and explore methods to address its shortcomings. We show that the stochastic nature of random augmentation can produce a set of colliding augmentations that distorts the learned features, similar to catastrophic forgetting. We propose a simple solution that improves the generalization effect of random augmentation by addressing forgetting, which displays strong generalization performance across various single source domain generalization (sDG) benchmarks.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporalizing Confidence: Evaluation of Chain-of-Thought Reasoning with Signal Temporal Logic</title>
<link>https://arxiv.org/abs/2506.08243</link>
<guid>https://arxiv.org/abs/2506.08243</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Chain-of-Thought prompting, Signal Temporal Logic, uncertainty estimates, calibration metrics

Summary: 
Large Language Models (LLMs) have shown strong performance in mathematical reasoning tasks with Chain-of-Thought prompting. However, they often produce confidently incorrect outputs, presenting risks in education. To address this, a framework is proposed that models stepwise confidence as a temporal signal using Signal Temporal Logic (STL). Formal constraints based on STL capture desirable temporal properties, providing structured and interpretable confidence estimates. Uncertainty reshaping strategies enforce smoothness, monotonicity, and causal consistency in the reasoning trajectory. Experiments demonstrate that this approach enhances calibration metrics and offers more reliable uncertainty estimates compared to traditional methods. <br /><br />Summary: <div>
arXiv:2506.08243v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown impressive performance in mathematical reasoning tasks when guided by Chain-of-Thought (CoT) prompting. However, they tend to produce highly confident yet incorrect outputs, which poses significant risks in domains like education, where users may lack the expertise to assess reasoning steps. To address this, we propose a structured framework that models stepwise confidence as a temporal signal and evaluates it using Signal Temporal Logic (STL). In particular, we define formal STL-based constraints to capture desirable temporal properties and compute robustness scores that serve as structured, interpretable confidence estimates. Our approach also introduces a set of uncertainty reshaping strategies to enforce smoothness, monotonicity, and causal consistency across the reasoning trajectory. Experiments show that our approach consistently improves calibration metrics and provides more reliable uncertainty estimates than conventional confidence aggregation and post-hoc calibration.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parameter-free approximate equivariance for tasks with finite group symmetry</title>
<link>https://arxiv.org/abs/2506.08244</link>
<guid>https://arxiv.org/abs/2506.08244</guid>
<content:encoded><![CDATA[
<div> Equivariant neural networks; symmetries; group actions; inductive bias; performance improvement<br />
<br />
Summary:<br />
Equivariant neural networks leverage symmetries through group actions to enhance performance on various tasks. Prior methods are computationally demanding with high parameter counts and are often tied to specific architectures. A new zero-parameter approach introduces approximate equivariance for a finite group in the latent representation through an additional loss function term. Experiment results demonstrate that the network tends to learn the regular representation in the latent space. By fixing this action, the method effectively enforces approximate equivariance as an added loss penalty. Benchmarking on three datasets against existing equivariant methods reveals comparable or superior performance with significantly fewer parameters. <div>
arXiv:2506.08244v1 Announce Type: new 
Abstract: Equivariant neural networks incorporate symmetries through group actions, embedding them as an inductive bias to improve performance on a wide variety of tasks. However, existing equivariant methods can be computationally intensive, with high parameter counts, and are often tied to a specific architecture. We propose a simple zero-parameter approach that imposes approximate equivariance for a finite group in the latent representation, as an additional term in the loss function. We conduct experiments which allow the network to learn a group representation on the latent space, and show in every case it prefers to learn the regular representation. Fixing this action on the latent space, this yields a simple method to impose approximate equivariance as an additional loss penalty. We benchmark our approach on three datasets and compare it against several existing equivariant methods, showing that in many cases it achieves similar or better performance for a fraction of the parameters.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SHIELD: Secure Hypernetworks for Incremental Expansion Learning Defense</title>
<link>https://arxiv.org/abs/2506.08255</link>
<guid>https://arxiv.org/abs/2506.08255</guid>
<content:encoded><![CDATA[
<div> Keywords: deep neural networks, catastrophic forgetting, adversarial attacks, SHIELD, hypernetworks <br />
<br />
Summary: 
Traditional deep neural networks face challenges such as catastrophic forgetting and vulnerability to adversarial attacks. Existing solutions address these issues separately, but none can tackle both simultaneously. The SHIELD approach combines hypernetwork-based continual learning with interval arithmetic to address these challenges. SHIELD uses hypernetworks to transfer task embedding vectors to specific data weights, enabling dynamic generation of separate networks for each subtask. The hypernetwork aggregates information across tasks, providing strict guarantees against attacks for data within defined interval ranges. This approach enhances security without compromising network adaptability, addressing the overlooked safety aspect in continual learning. <div>
arXiv:2506.08255v1 Announce Type: new 
Abstract: Traditional deep neural networks suffer from several limitations, including catastrophic forgetting. When models are adapted to new datasets, they tend to quickly forget previously learned knowledge. Another significant issue is the lack of robustness to even small perturbations in the input data. In practice, we can often easily perform adversarial attacks and change the network's predictions, adding minimal noise to the input. Dedicated architectures and training procedures can solve each of the above problems separately. Unfortunately, currently, no model can simultaneously address both catastrophic forgetting and vulnerability to adversarial attacks. We introduce SHIELD (Secure Hypernetworks for Incremental Expansion and Learning Defense), a novel approach that integrates a hypernetwork-based continual learning approach with interval arithmetic. SHIELD use the hypernetwork to transfer trainable task embedding vectors into the weights of a target model dedicated to specific data. This paradigm allows for the dynamic generation of separate networks for each subtask, while the hypernetwork aggregates and analyzes information across all tasks. The target model takes in the input a data sample with a defined interval range, and by creating a hypercube, produces a prediction for the given range. Therefore, such target models provide strict guarantees against all possible attacks for data samples within the interval range. Our approach enhances security without sacrificing network adaptability, addressing the overlooked challenge of safety in continual learning.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning from Human Feedback with High-Confidence Safety Constraints</title>
<link>https://arxiv.org/abs/2506.08266</link>
<guid>https://arxiv.org/abs/2506.08266</guid>
<content:encoded><![CDATA[
<div> Keywords: language model alignment, safety, reinforcement learning, human feedback, high-confidence guarantees

Summary:
High-Confidence Safe Reinforcement Learning from Human Feedback (HC-RLHF) is proposed to address the tradeoff between safety and helpfulness in language model alignment. It ensures reliable performance in sensitive domains by decoupling human preferences into helpfulness and harmlessness, learned through a reward and cost model. HC-RLHF optimizes the reward function under a pessimistic cost constraint and undergoes a safety test to verify performance within an upper-confidence bound. The theoretical analysis guarantees that HC-RLHF will not return an unsafe solution with a probability greater than a user-specified threshold. Empirical results aligning language models with human preferences show HC-RLHF produces safe models with high probability and improves harmlessness and helpfulness compared to previous methods. 

<br /><br />Summary: <div>
arXiv:2506.08266v1 Announce Type: new 
Abstract: Existing approaches to language model alignment often treat safety as a tradeoff against helpfulness, which can lead to unacceptable responses in sensitive domains. To ensure reliable performance in such settings, we propose High-Confidence Safe Reinforcement Learning from Human Feedback (HC-RLHF), a method that provides high-confidence safety guarantees while maximizing helpfulness. Similar to previous methods, HC-RLHF explicitly decouples human preferences into helpfulness and harmlessness (safety), which are learned by training a reward model and a cost model, respectively. It then employs a two-step process to find safe solutions. In the first step, it optimizes the reward function under an intentionally pessimistic version of the cost constraint. In the second step, the trained model undergoes a safety test to verify whether its performance stays within an upper-confidence bound of the actual cost constraint. We provide a theoretical analysis of HC-RLHF, including proof that it will not return an unsafe solution with a probability greater than a user-specified threshold. For our empirical analysis, we apply HC-RLHF to align three different language models (Qwen2-1.5B, Qwen2.5-3B, and LLaMa3.2-3B) with human preferences. Our results demonstrate that HC-RLHF produces safe models with high probability and can improve harmlessness and helpfulness compared to previous methods.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Interpretable Deep Learning with LIES Networks for Symbolic Regression</title>
<link>https://arxiv.org/abs/2506.08267</link>
<guid>https://arxiv.org/abs/2506.08267</guid>
<content:encoded><![CDATA[
<div> Symbolic regression, LIES framework, neural network, interpretable, oversampling strategy<br />
Summary:<br />
The study introduces LIES, a fixed neural network architecture for symbolic regression, aiming to discover compact and accurate mathematical expressions. LIES utilizes interpretable primitive activations such as logarithm, identity, exponential, and sine functions. The framework incorporates an oversampling strategy and tailored loss function to promote sparsity and prevent gradient instability, leading to simplified formulae. Experiments on symbolic regression benchmarks demonstrate that LIES consistently outperforms existing methods in producing sparse and accurate symbolic formulae. Ablation studies highlight the significance of each design component in the framework. <div>
arXiv:2506.08267v1 Announce Type: new 
Abstract: Symbolic regression (SR) aims to discover closed-form mathematical expressions that accurately describe data, offering interpretability and analytical insight beyond standard black-box models. Existing SR methods often rely on population-based search or autoregressive modeling, which struggle with scalability and symbolic consistency. We introduce LIES (Logarithm, Identity, Exponential, Sine), a fixed neural network architecture with interpretable primitive activations that are optimized to model symbolic expressions. We develop a framework to extract compact formulae from LIES networks by training with an appropriate oversampling strategy and a tailored loss function to promote sparsity and to prevent gradient instability. After training, it applies additional pruning strategies to further simplify the learned expressions into compact formulae. Our experiments on SR benchmarks show that the LIES framework consistently produces sparse and accurate symbolic formulae outperforming all baselines. We also demonstrate the importance of each design component through ablation studies.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SWAT-NN: Simultaneous Weights and Architecture Training for Neural Networks in a Latent Space</title>
<link>https://arxiv.org/abs/2506.08270</link>
<guid>https://arxiv.org/abs/2506.08270</guid>
<content:encoded><![CDATA[
<div> Optimization, Neural Networks, Architecture, Weight Training, Autoencoder
Summary:
The paper introduces a novel approach for designing neural networks that optimizes both architecture and weights simultaneously. It involves training a universal multi-scale autoencoder that embeds architectural and parametric information into a continuous latent space. This allows for mapping functionally similar neural networks closer together. By randomly initializing a point in the embedding space and updating it via gradient descent, the optimal neural network is obtained, jointly optimizing its structure and weights. The optimization process integrates sparsity and compactness penalties to encourage efficient models. Experimental results on synthetic regression tasks show that the proposed method effectively uncovers sparse and compact neural networks with strong performance. <div>
arXiv:2506.08270v1 Announce Type: new 
Abstract: Designing neural networks typically relies on manual trial and error or a neural architecture search (NAS) followed by weight training. The former is time-consuming and labor-intensive, while the latter often discretizes architecture search and weight optimization. In this paper, we propose a fundamentally different approach that simultaneously optimizes both the architecture and the weights of a neural network. Our framework first trains a universal multi-scale autoencoder that embeds both architectural and parametric information into a continuous latent space, where functionally similar neural networks are mapped closer together. Given a dataset, we then randomly initialize a point in the embedding space and update it via gradient descent to obtain the optimal neural network, jointly optimizing its structure and weights. The optimization process incorporates sparsity and compactness penalties to promote efficient models. Experiments on synthetic regression tasks demonstrate that our method effectively discovers sparse and compact neural networks with strong performance.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal Differential Equations for Scientific Machine Learning of Node-Wise Battery Dynamics in Smart Grids</title>
<link>https://arxiv.org/abs/2506.08272</link>
<guid>https://arxiv.org/abs/2506.08272</guid>
<content:encoded><![CDATA[
<div> neural networks, physical differential equations, scientific machine learning, smart grid systems, battery modeling

Summary:
Universal Differential Equations (UDEs) merge neural networks with physical differential equations, enabling data-efficient, interpretable, and physically consistent modeling in scientific machine learning. The challenge of modeling node-wise battery dynamics in smart grid systems due to solar input stochasticity and load profile variability is addressed by proposing a UDE-based approach. This approach embeds a neural residual into a physically inspired battery ODE to learn node-specific battery evolution. Synthetic solar generation and load demand data are utilized to simulate battery dynamics, with the neural component capturing unobserved or stochastic corrections. The experiments demonstrate that the trained UDE closely aligns with actual battery trajectories, displays smooth convergence, and maintains stability in long-term forecasts. The success of the UDE-based approach in battery modeling for decentralized energy networks showcases its potential for real-time control and optimization in renewable-integrated smart grids. 

<br /><br />Summary: <div>
arXiv:2506.08272v1 Announce Type: new 
Abstract: Universal Differential Equations (UDEs), which blend neural networks with physical differential equations, have emerged as a powerful framework for scientific machine learning (SciML), enabling data-efficient, interpretable, and physically consistent modeling. In the context of smart grid systems, modeling node-wise battery dynamics remains a challenge due to the stochasticity of solar input and variability in household load profiles. Traditional approaches often struggle with generalization and fail to capture unmodeled residual dynamics. This work proposes a UDE-based approach to learn node-specific battery evolution by embedding a neural residual into a physically inspired battery ODE. Synthetic yet realistic solar generation and load demand data are used to simulate battery dynamics over time. The neural component learns to model unobserved or stochastic corrections arising from heterogeneity in node demand and environmental conditions. Comprehensive experiments reveal that the trained UDE aligns closely with ground truth battery trajectories, exhibits smooth convergence behavior, and maintains stability in long-term forecasts. These findings affirm the viability of UDE-based SciML approaches for battery modeling in decentralized energy networks and suggest broader implications for real-time control and optimization in renewable-integrated smart grids.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Impact of Feature Scaling In Machine Learning: Effects on Regression and Classification Tasks</title>
<link>https://arxiv.org/abs/2506.08274</link>
<guid>https://arxiv.org/abs/2506.08274</guid>
<content:encoded><![CDATA[
<div> ensemble methods, feature scaling, machine learning, predictive performance, computational costs <br />
Summary:
This research evaluates 12 feature scaling techniques across 14 machine learning algorithms and 16 datasets for classification and regression tasks. The study finds that ensemble methods like Random Forest and XGBoost are less affected by scaling, while models like Logistic Regression and SVMs show significant performance differences based on the scaler chosen. The research emphasizes the need for practitioners to carefully select feature scaling techniques for specific models to optimize performance. All experimental data, source code, and model parameters are publicly available to ensure transparency and reproducibility. The analysis provides valuable insights for practitioners looking to improve model performance through optimal feature scaling. <br /><br />Summary: <div>
arXiv:2506.08274v1 Announce Type: new 
Abstract: This research addresses the critical lack of comprehensive studies on feature scaling by systematically evaluating 12 scaling techniques - including several less common transformations - across 14 different Machine Learning algorithms and 16 datasets for classification and regression tasks. We meticulously analyzed impacts on predictive performance (using metrics such as accuracy, MAE, MSE, and $R^2$) and computational costs (training time, inference time, and memory usage). Key findings reveal that while ensemble methods (such as Random Forest and gradient boosting models like XGBoost, CatBoost and LightGBM) demonstrate robust performance largely independent of scaling, other widely used models such as Logistic Regression, SVMs, TabNet, and MLPs show significant performance variations highly dependent on the chosen scaler. This extensive empirical analysis, with all source code, experimental results, and model parameters made publicly available to ensure complete transparency and reproducibility, offers model-specific crucial guidance to practitioners on the need for an optimal selection of feature scaling techniques.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Debate to Equilibrium: Belief-Driven Multi-Agent LLM Reasoning via Bayesian Nash Equilibrium</title>
<link>https://arxiv.org/abs/2506.08292</link>
<guid>https://arxiv.org/abs/2506.08292</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent frameworks, large language models, Bayesian Nash equilibrium, hierarchical reinforcement-learning paradigm, ECON <br />
Summary: 
Multi-agent frameworks can enhance large language models by recasting coordination as an incomplete-information game and seeking a Bayesian Nash equilibrium. The Efficient Coordination via Nash Equilibrium (ECON) uses hierarchical reinforcement learning to enable distributed reasoning with centralized final output. ECON achieves a tighter regret bound and outperforms existing multi-LLM approaches by 11.2% on average across various benchmarks. It does not require costly inter-agent exchanges and is scalable to incorporate additional models, showing promise for larger and more powerful multi-LLM ensembles. The code for ECON is publicly available at https://github.com/tmlr-group/ECON.<br /><br />Summary: <div>
arXiv:2506.08292v1 Announce Type: new 
Abstract: Multi-agent frameworks can substantially boost the reasoning power of large language models (LLMs), but they typically incur heavy computational costs and lack convergence guarantees. To overcome these challenges, we recast multi-LLM coordination as an incomplete-information game and seek a Bayesian Nash equilibrium (BNE), in which each agent optimally responds to its probabilistic beliefs about the strategies of others. We introduce Efficient Coordination via Nash Equilibrium (ECON), a hierarchical reinforcement-learning paradigm that marries distributed reasoning with centralized final output. Under ECON, each LLM independently selects responses that maximize its expected reward, conditioned on its beliefs about co-agents, without requiring costly inter-agent exchanges. We mathematically prove that ECON attains a markedly tighter regret bound than non-equilibrium multi-agent schemes. Empirically, ECON outperforms existing multi-LLM approaches by 11.2% on average across six benchmarks spanning complex reasoning and planning tasks. Further experiments demonstrate ECON's ability to flexibly incorporate additional models, confirming its scalability and paving the way toward larger, more powerful multi-LLM ensembles. The code is publicly available at: https://github.com/tmlr-group/ECON.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Passive to Active Reasoning: Can Large Language Models Ask the Right Questions under Incomplete Information?</title>
<link>https://arxiv.org/abs/2506.08295</link>
<guid>https://arxiv.org/abs/2506.08295</guid>
<content:encoded><![CDATA[
<div> Keywords: AR-Bench, active reasoning, large language models, benchmark, reasoning abilities

Summary: 
The article introduces AR-Bench, a new benchmark designed to evaluate the active reasoning skills of large language models (LLMs). Existing benchmarks mainly focus on passive reasoning, where all information is provided, but active reasoning involves interacting with external systems to acquire missing data. AR-Bench consists of three task families that simulate real-world scenarios and measure reasoning abilities. Empirical evaluation shows that current LLMs struggle with active reasoning tasks, indicating a gap between passive and active reasoning abilities. Advanced strategies like tree-based searching offer limited improvements. The study suggests a need to develop methodologies for active reasoning, such as interactive learning and real-time feedback loops. The AR-Bench benchmark is publicly available for further research and evaluation. 

<br /><br />Summary: <div>
arXiv:2506.08295v1 Announce Type: new 
Abstract: While existing benchmarks probe the reasoning abilities of large language models (LLMs) across diverse domains, they predominantly assess passive reasoning, providing models with all the information needed to reach a solution. By contrast, active reasoning-where an LLM must interact with external systems to acquire missing evidence or data-has received little systematic attention. To address this shortfall, we present AR-Bench, a novel benchmark designed explicitly to evaluate an LLM's active reasoning skills. AR-Bench comprises three task families-detective cases, situation puzzles, and guessing numbers-that together simulate real-world, agentic scenarios and measure performance across commonsense, logical, and symbolic reasoning challenges. Empirical evaluation on AR-Bench demonstrates that contemporary LLMs exhibit pronounced difficulties with active reasoning: they frequently fail to acquire or leverage the information needed to solve tasks. This gap highlights a stark divergence between their passive and active reasoning abilities. Moreover, ablation studies indicate that even advanced strategies, such as tree-based searching or post-training approaches, yield only modest gains and fall short of the levels required for real-world deployment. Collectively, these findings highlight the critical need to advance methodology for active reasoning, e.g., incorporating interactive learning, real-time feedback loops, and environment-aware objectives for training. The benchmark is publicly available at: https://github.com/tmlr-group/AR-Bench.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>H$^2$GFM: Towards unifying Homogeneity and Heterogeneity on Text-Attributed Graphs</title>
<link>https://arxiv.org/abs/2506.08298</link>
<guid>https://arxiv.org/abs/2506.08298</guid>
<content:encoded><![CDATA[
<div> Keywords: graph learning, Graph Foundation Model, text-attributed graphs, heterogeneous TAGs, context-adaptive graph transformer

Summary:
The article introduces H$^2$GFM, a framework designed to generalize across both homogeneous and heterogeneous text-attributed graphs (TAGs). This new model leverages diverse meta-relations among graphs in a unified textual space and employs context encoding to capture spatial and higher-order semantic relationships. A novel context-adaptive graph transformer (CGT) is proposed to achieve robust node representations by capturing information from context neighbors and their relationships. Additionally, a mixture of CGT experts is utilized to handle the heterogeneity in structural patterns among graph types. Experimental results on various types of TAGs demonstrate the effectiveness of H$^2$GFM in learning scenarios. This innovative approach enhances the capabilities and applicability of the Graph Foundation Model in diverse domains. <div>
arXiv:2506.08298v1 Announce Type: new 
Abstract: The growing interests and applications of graph learning in diverse domains have propelled the development of a unified model generalizing well across different graphs and tasks, known as the Graph Foundation Model (GFM). Existing research has leveraged text-attributed graphs (TAGs) to tackle the heterogeneity in node features among graphs. However, they primarily focus on homogeneous TAGs (HoTAGs), leaving heterogeneous TAGs (HeTAGs), where multiple types of nodes/edges reside, underexplored. To enhance the capabilities and applications of GFM, we introduce H$^2$GFM, a novel framework designed to generalize across both HoTAGs and HeTAGs. Our model projects diverse meta-relations among graphs under a unified textual space, and employs a context encoding to capture spatial and higher-order semantic relationships. To achieve robust node representations, we propose a novel context-adaptive graph transformer (CGT), effectively capturing information from both context neighbors and their relationships. Furthermore, we employ a mixture of CGT experts to capture the heterogeneity in structural patterns among graph types. Comprehensive experiments on a wide range of HoTAGs and HeTAGs as well as learning scenarios demonstrate the effectiveness of our model.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learnable Spatial-Temporal Positional Encoding for Link Prediction</title>
<link>https://arxiv.org/abs/2506.08309</link>
<guid>https://arxiv.org/abs/2506.08309</guid>
<content:encoded><![CDATA[
<div> Keywords: graph deep learning, positional encoding, spatial-temporal, link prediction, benchmark

Summary:
Learnable Spatial-Temporal Positional Encoding is developed to address limitations in existing positional encoding methods for graph neural networks and graph transformers. The proposed model, L-STEP, utilizes a positional learning scheme that preserves graph properties from a spatial-temporal spectral viewpoint. By leveraging Multi-Layer Perceptrons (MLPs) instead of dense or relational attention mechanisms, L-STEP achieves competitive performance with reduced computational complexity. The model demonstrates robustness to different initial positional encoding inputs and outperforms 10 algorithms on 13 classic datasets for temporal link prediction tasks in both transductive and inductive settings using various sampling strategies. Additionally, L-STEP achieves state-of-the-art performance in the large-scale TGB benchmark. The code for L-STEP is publicly available on GitHub at https://github.com/kthrn22/L-STEP. 

<br /><br />Summary: Accurate predictions in graph deep learning frameworks rely on effective positional encoding. The proposed Learnable Spatial-Temporal Positional Encoding (L-STEP) addresses limitations in current methods by preserving graph properties spectrally and using MLPs for enhanced expressiveness. L-STEP demonstrates robustness, efficiency, and superior performance in temporal link prediction tasks across diverse datasets and achieves top results in the TGB benchmark. <div>
arXiv:2506.08309v1 Announce Type: new 
Abstract: Accurate predictions rely on the expressiveness power of graph deep learning frameworks like graph neural networks and graph transformers, where a positional encoding mechanism has become much more indispensable in recent state-of-the-art works to record the canonical position information. However, the current positional encoding is limited in three aspects: (1) most positional encoding methods use pre-defined, and fixed functions, which are inadequate to adapt to the complex attributed graphs; (2) a few pioneering works proposed the learnable positional encoding but are still limited to the structural information, not considering the real-world time-evolving topological and feature information; (3) most positional encoding methods are equipped with transformers' attention mechanism to fully leverage their capabilities, where the dense or relational attention is often unaffordable on large-scale structured data. Hence, we aim to develop Learnable Spatial-Temporal Positional Encoding in an effective and efficient manner and propose a simple temporal link prediction model named L-STEP. Briefly, for L-STEP, we (1) prove the proposed positional learning scheme can preserve the graph property from the spatial-temporal spectral viewpoint, (2) verify that MLPs can fully exploit the expressiveness and reach transformers' performance on that encoding, (3) change different initial positional encoding inputs to show robustness, (4) analyze the theoretical complexity and obtain less empirical running time than SOTA, and (5) demonstrate its temporal link prediction out-performance on 13 classic datasets and with 10 algorithms in both transductive and inductive settings using 3 different sampling strategies. Also, \name\ obtains the leading performance in the newest large-scale TGB benchmark. Our code is available at https://github.com/kthrn22/L-STEP.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Private Evolution Converges</title>
<link>https://arxiv.org/abs/2506.08312</link>
<guid>https://arxiv.org/abs/2506.08312</guid>
<content:encoded><![CDATA[
<div> Private Evolution, Differentail Privacy, Synthetic Data Generation, Theoretical Analysis, Convergence<br />
<br />
Summary:<br />
The article introduces Private Evolution (PE), a training-free method for differentially private synthetic data generation. While PE shows strong performance in image and text domains, its behavior in tabular data is inconsistent. Existing theoretical analyses of PE rely on unrealistic assumptions. The study presents a new theoretical framework to explain PE's practical behavior and identifies conditions for convergence. For sensitive datasets with n data points in a bounded domain, PE produces (, )-DP synthetic data with expected 1-Wasserstein distance of order O(d(n)^-1/d) from the original. The analysis extends to general Banach spaces. PE is connected to the Private Signed Measure Mechanism for DP synthetic data generation. The theoretical findings are validated in simulations, demonstrating the practical relevance of the study. <br /><br /> <div>
arXiv:2506.08312v1 Announce Type: new 
Abstract: Private Evolution (PE) is a promising training-free method for differentially private (DP) synthetic data generation. While it achieves strong performance in some domains (e.g., images and text), its behavior in others (e.g., tabular data) is less consistent. To date, the only theoretical analysis of the convergence of PE depends on unrealistic assumptions about both the algorithm's behavior and the structure of the sensitive dataset. In this work, we develop a new theoretical framework to explain PE's practical behavior and identify sufficient conditions for its convergence. For $d$-dimensional sensitive datasets with $n$ data points from a bounded domain, we prove that PE produces an $(\epsilon, \delta)$-DP synthetic dataset with expected 1-Wasserstein distance of order $\tilde{O}(d(n\epsilon)^{-1/d})$ from the original, establishing worst-case convergence of the algorithm as $n \to \infty$. Our analysis extends to general Banach spaces as well. We also connect PE to the Private Signed Measure Mechanism, a method for DP synthetic data generation that has thus far not seen much practical adoption. We demonstrate the practical relevance of our theoretical findings in simulations.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Masking Diffusion Works: Condition on the Jump Schedule for Improved Discrete Diffusion</title>
<link>https://arxiv.org/abs/2506.08316</link>
<guid>https://arxiv.org/abs/2506.08316</guid>
<content:encoded><![CDATA[
<div> jump times, discrete diffusion models, masking diffusion, SCUD, inductive biases

Summary:
Discrete diffusion models, like continuous diffusion models, utilize Markov processes to gradually undo noise applied to data points, offering conceptual benefits such as incorporating inductive biases and improved sampling algorithms. Despite the theory supporting gradual generation, the consistently best performing discrete diffusion model, masking diffusion, does not denoise gradually. The superior performance of masking diffusion is attributed to its utilization of the known distribution of jump times in discrete Markov processes, enabling it to learn where to jump to. By introducing schedule-conditioned discrete diffusion (SCUD) models that embed the known distribution of jump times, a generalized approach combining classical discrete diffusion and masking diffusion is achieved. Applying SCUD to models incorporating inductive biases for images, text, and protein data results in models that outperform masking diffusion. <div>
arXiv:2506.08316v1 Announce Type: new 
Abstract: Discrete diffusion models, like continuous diffusion models, generate high-quality samples by gradually undoing noise applied to datapoints with a Markov process. Gradual generation in theory comes with many conceptual benefits; for example, inductive biases can be incorporated into the noising Markov process, and access to improved sampling algorithms. In practice, however, the consistently best performing discrete diffusion model is, surprisingly, masking diffusion, which does not denoise gradually. Here we explain the superior performance of masking diffusion by noting that it makes use of a fundamental difference between continuous and discrete Markov processes: discrete Markov processes evolve by discontinuous jumps at a fixed rate and, unlike other discrete diffusion models, masking diffusion builds in the known distribution of jump times and only learns where to jump to. We show that we can similarly bake in the known distribution of jump times into any discrete diffusion model. The resulting models - schedule-conditioned discrete diffusion (SCUD) - generalize classical discrete diffusion and masking diffusion. By applying SCUD to models with noising processes that incorporate inductive biases on images, text, and protein data, we build models that outperform masking.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Prompting for Graph Learning Models: Recent Advances and Future Directions</title>
<link>https://arxiv.org/abs/2506.08326</link>
<guid>https://arxiv.org/abs/2506.08326</guid>
<content:encoded><![CDATA[
<div> Graph learning models, graph prompting, pre-training, adaptation, self-supervised learning 

Summary: 
Graph learning models are effective in learning representations from large-scale graph data. The "pre-training, adaptation" scheme involves pre-training models on unlabeled data and then adapting them to specific tasks. Graph prompting, a technique in adaptation, involves learning prompts for pre-trained models. This review discusses graph pre-training methods, mainstream graph prompting techniques, and real-world applications. The challenges and future directions in graph prompting are also explored. 

<br /><br />Summary: <div>
arXiv:2506.08326v1 Announce Type: new 
Abstract: Graph learning models have demonstrated great prowess in learning expressive representations from large-scale graph data in a wide variety of real-world scenarios. As a prevalent strategy for training powerful graph learning models, the "pre-training, adaptation" scheme first pre-trains graph learning models on unlabeled graph data in a self-supervised manner and then adapts them to specific downstream tasks. During the adaptation phase, graph prompting emerges as a promising approach that learns trainable prompts while keeping the pre-trained graph learning models unchanged. In this paper, we present a systematic review of recent advancements in graph prompting. First, we introduce representative graph pre-training methods that serve as the foundation step of graph prompting. Next, we review mainstream techniques in graph prompting and elaborate on how they design learnable prompts for graph prompting. Furthermore, we summarize the real-world applications of graph prompting from different domains. Finally, we discuss several open challenges in existing studies with promising future directions in this field.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Simple Analysis of Discretization Error in Diffusion Models</title>
<link>https://arxiv.org/abs/2506.08337</link>
<guid>https://arxiv.org/abs/2506.08337</guid>
<content:encoded><![CDATA[
<div> Diffusion models, Euler-Maruyama discretization, variance-preserving SDEs, Denoising Diffusion Probabilistic Models, convergence rate<br />
Summary: <br />
This work presents a simplified theoretical framework for analyzing the Euler-Maruyama discretization of variance-preserving SDEs in Denoising Diffusion Probabilistic Models. By leveraging Gr\"onwall's inequality, the convergence rate is derived as $ \mathcal{O}(1/T^{1/2}) $ under Lipschitz assumptions, streamlining prior proofs. The study also shows that replacing Gaussian noise with discrete random variables like Rademacher or uniform noise does not affect convergence guarantees, offering practical advantages for efficient sampling. Experimental results validate the theory, demonstrating that the error scales as predicted, discrete noise achieves similar sample quality to Gaussian noise, and incorrect noise scaling leads to performance degradation. This work unifies simplified analysis and discrete noise substitution, bridging theoretical rigor with practical efficiency in diffusion-based generative modeling. <br /> <div>
arXiv:2506.08337v1 Announce Type: new 
Abstract: Diffusion models, formulated as discretizations of stochastic differential equations (SDEs), achieve state-of-the-art generative performance. However, existing analyses of their discretization error often rely on complex probabilistic tools. In this work, we present a simplified theoretical framework for analyzing the Euler--Maruyama discretization of variance-preserving SDEs (VP-SDEs) in Denoising Diffusion Probabilistic Models (DDPMs), where $ T $ denotes the number of denoising steps in the diffusion process. Our approach leverages Gr\"onwall's inequality to derive a convergence rate of $ \mathcal{O}(1/T^{1/2}) $ under Lipschitz assumptions, significantly streamlining prior proofs. Furthermore, we demonstrate that the Gaussian noise in the discretization can be replaced by a discrete random variable (e.g., Rademacher or uniform noise) without sacrificing convergence guarantees-an insight with practical implications for efficient sampling. Experiments validate our theory, showing that (1) the error scales as predicted, (2) discrete noise achieves comparable sample quality to Gaussian noise, and (3) incorrect noise scaling degrades performance. By unifying simplified analysis and discrete noise substitution, our work bridges theoretical rigor with practical efficiency in diffusion-based generative modeling.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamical System Optimization</title>
<link>https://arxiv.org/abs/2506.08340</link>
<guid>https://arxiv.org/abs/2506.08340</guid>
<content:encoded><![CDATA[
<div> optimization framework, policy optimization, autonomous dynamical system, policy parameters, generative AI models

Summary: 
The article introduces an optimization framework that focuses on transferring control authority to a specified policy, allowing for the autonomous operation of a dynamical system. By optimizing policy parameters without the need for direct control or action, the framework simplifies algorithms at the autonomous system level. These algorithms are shown to compute the same quantities as traditional approaches such as policy gradients, natural gradients, and proximal methods. Analogous methods to approximate policy iteration and off-policy learning are also discussed. The framework treats policy parameters and system parameters equally, enabling application to various tasks like behavioral cloning, mechanism design, and system identification. The approach also highlights the potential for tuning generative AI models, positioning it closer conceptually to the framework than traditional Reinforcement Learning methods. <div>
arXiv:2506.08340v1 Announce Type: new 
Abstract: We develop an optimization framework centered around a core idea: once a (parametric) policy is specified, control authority is transferred to the policy, resulting in an autonomous dynamical system. Thus we should be able to optimize policy parameters without further reference to controls or actions, and without directly using the machinery of approximate Dynamic Programming and Reinforcement Learning. Here we derive simpler algorithms at the autonomous system level, and show that they compute the same quantities as policy gradients and Hessians, natural gradients, proximal methods. Analogs to approximate policy iteration and off-policy learning are also available. Since policy parameters and other system parameters are treated uniformly, the same algorithms apply to behavioral cloning, mechanism design, system identification, learning of state estimators. Tuning of generative AI models is not only possible, but is conceptually closer to the present framework than to Reinforcement Learning.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentially Private Relational Learning with Entity-level Privacy Guarantees</title>
<link>https://arxiv.org/abs/2506.08347</link>
<guid>https://arxiv.org/abs/2506.08347</guid>
<content:encoded><![CDATA[
<div> Framework, Differential Privacy, Sensitivity Analysis, Gradient Clipping, Privacy Amplification <br />
Summary: 
This paper introduces a framework for learning with relational and network-structured data while ensuring formal entity-level Differential Privacy guarantees. The challenges of applying Differential Privacy Stochastic Gradient Descent (DP-SGD) to relational learning are addressed through a rigorous sensitivity analysis and an adaptive gradient clipping scheme that adjusts clipping thresholds based on entity occurrence frequency. Additionally, privacy amplification results are extended to a subset of coupled sampling scenarios. These contributions result in a tailored variant of DP-SGD for relational data with proven privacy guarantees. Experimental results on text encoders trained on text-attributed network-structured relational data showcase the effective utility-privacy trade-offs of the proposed approach. The code for this framework is publicly available for further exploration. <br /> <div>
arXiv:2506.08347v1 Announce Type: new 
Abstract: Learning with relational and network-structured data is increasingly vital in sensitive domains where protecting the privacy of individual entities is paramount. Differential Privacy (DP) offers a principled approach for quantifying privacy risks, with DP-SGD emerging as a standard mechanism for private model training. However, directly applying DP-SGD to relational learning is challenging due to two key factors: (i) entities often participate in multiple relations, resulting in high and difficult-to-control sensitivity; and (ii) relational learning typically involves multi-stage, potentially coupled (interdependent) sampling procedures that make standard privacy amplification analyses inapplicable. This work presents a principled framework for relational learning with formal entity-level DP guarantees. We provide a rigorous sensitivity analysis and introduce an adaptive gradient clipping scheme that modulates clipping thresholds based on entity occurrence frequency. We also extend the privacy amplification results to a tractable subclass of coupled sampling, where the dependence arises only through sample sizes. These contributions lead to a tailored DP-SGD variant for relational data with provable privacy guarantees. Experiments on fine-tuning text encoders over text-attributed network-structured relational data demonstrate the strong utility-privacy trade-offs of our approach. Our code is available at https://github.com/Graph-COM/Node_DP.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Adaptive Method Stabilizing Activations for Enhanced Generalization</title>
<link>https://arxiv.org/abs/2506.08353</link>
<guid>https://arxiv.org/abs/2506.08353</guid>
<content:encoded><![CDATA[
<div> Keywords: AdaAct, optimization algorithm, learning rates, activation variance, image classification benchmarks

Summary:<br /><br />AdaAct is a new optimization algorithm introduced to improve learning rates based on activation variance, enhancing the stability of neuron outputs. This approach, incorporating neuron-wise adaptivity during training, leads to better generalization in image classification tasks. The experimental results on CIFAR and ImageNet datasets demonstrate AdaAct's competitive performance compared to other state-of-the-art methods. AdaAct effectively balances the convergence speed of Adam and the strong generalization capabilities of SGD while maintaining competitive execution times. The code for AdaAct is available on GitHub for further exploration and implementation. <div>
arXiv:2506.08353v1 Announce Type: new 
Abstract: We introduce AdaAct, a novel optimization algorithm that adjusts learning rates according to activation variance. Our method enhances the stability of neuron outputs by incorporating neuron-wise adaptivity during the training process, which subsequently leads to better generalization -- a complementary approach to conventional activation regularization methods. Experimental results demonstrate AdaAct's competitive performance across standard image classification benchmarks. We evaluate AdaAct on CIFAR and ImageNet, comparing it with other state-of-the-art methods. Importantly, AdaAct effectively bridges the gap between the convergence speed of Adam and the strong generalization capabilities of SGD, all while maintaining competitive execution times. Code is available at https://github.com/hseung88/adaact.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NysAct: A Scalable Preconditioned Gradient Descent using Nystrom Approximation</title>
<link>https://arxiv.org/abs/2506.08360</link>
<guid>https://arxiv.org/abs/2506.08360</guid>
<content:encoded><![CDATA[
<div> Keywords: Adaptive gradient methods, second-order methods, NysAct, activation covariance matrix, test accuracy

Summary: 
Adaptive gradient methods are efficient but suffer from poor generalization, while second-order methods enhance convergence and generalization at the cost of high computational and memory usage. In response, the NysAct method is introduced as a scalable first-order gradient preconditioning approach that balances the benefits of first and second-order optimization methods. By leveraging an eigenvalue-shifted Nystrom method, NysAct approximates the activation covariance matrix for efficient preconditioning, reducing time and memory complexities without affecting test accuracy significantly. Experimental results demonstrate that NysAct outperforms both first-order and second-order methods in test accuracy while requiring fewer computational resources than traditional second-order methods.<br /><br />Summary: <div>
arXiv:2506.08360v1 Announce Type: new 
Abstract: Adaptive gradient methods are computationally efficient and converge quickly, but they often suffer from poor generalization. In contrast, second-order methods enhance convergence and generalization but typically incur high computational and memory costs. In this work, we introduce NysAct, a scalable first-order gradient preconditioning method that strikes a balance between state-of-the-art first-order and second-order optimization methods. NysAct leverages an eigenvalue-shifted Nystrom method to approximate the activation covariance matrix, which is used as a preconditioning matrix, significantly reducing time and memory complexities with minimal impact on test accuracy. Our experiments show that NysAct not only achieves improved test accuracy compared to both first-order and second-order methods but also demands considerably less computational resources than existing second-order methods. Code is available at https://github.com/hseung88/nysact.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlphaFold Database Debiasing for Robust Inverse Folding</title>
<link>https://arxiv.org/abs/2506.08365</link>
<guid>https://arxiv.org/abs/2506.08365</guid>
<content:encoded><![CDATA[
<div> database, protein structure, debiasing, AutoEncoder, inverse folding <br />
<br />
Summary: 
The article introduces the AlphaFold Protein Structure Database (AFDB) and identifies a systematic geometric bias in the structural features of AFDB compared to experimentally determined structures. To address this bias, the researchers propose a Debiasing Structure AutoEncoder (DeSAE) that learns to reconstruct native-like conformations from corrupted backbone geometries. By training DeSAE to recover plausible structural states, a more robust and natural structural manifold is implicitly captured. Applying DeSAE to AFDB structures produces debiased structures that significantly enhance performance in inverse folding tasks. This study emphasizes the impact of subtle biases in predicted protein structures and demonstrates the effectiveness of debiasing methods in improving structure-based learning tasks like inverse folding. The DeSAE model offers a principled framework for addressing systematic biases in protein structure prediction, leading to more accurate and reliable structural representations. <br /> <div>
arXiv:2506.08365v1 Announce Type: new 
Abstract: The AlphaFold Protein Structure Database (AFDB) offers unparalleled structural coverage at near-experimental accuracy, positioning it as a valuable resource for data-driven protein design. However, its direct use in training deep models that are sensitive to fine-grained atomic geometry, such as inverse folding, exposes a critical limitation. Comparative analysis of structural feature distributions reveals that AFDB structures exhibit distinct statistical regularities, reflecting a systematic geometric bias that deviates from the conformational diversity found in experimentally determined structures from the Protein Data Bank (PDB). While AFDB structures are cleaner and more idealized, PDB structures capture the intrinsic variability and physical realism essential for generalization in downstream tasks. To address this discrepancy, we introduce a Debiasing Structure AutoEncoder (DeSAE) that learns to reconstruct native-like conformations from intentionally corrupted backbone geometries. By training the model to recover plausible structural states, DeSAE implicitly captures a more robust and natural structural manifold. At inference, applying DeSAE to AFDB structures produces debiased structures that significantly improve inverse folding performance across multiple benchmarks. This work highlights the critical impact of subtle systematic biases in predicted structures and presents a principled framework for debiasing, significantly boosting the performance of structure-based learning tasks like inverse folding.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforce LLM Reasoning through Multi-Agent Reflection</title>
<link>https://arxiv.org/abs/2506.08379</link>
<guid>https://arxiv.org/abs/2506.08379</guid>
<content:encoded><![CDATA[
arXiv:2506.08379v1 Announce Type: new 
Abstract: Leveraging more test-time computation has proven to be an effective way to boost the reasoning capabilities of large language models (LLMs). Among various methods, the verify-and-improve paradigm stands out for enabling dynamic solution exploration and feedback incorporation. However, existing approaches often suffer from restricted feedback spaces and lack of coordinated training of different parties, leading to suboptimal performance. To address this, we model this multi-turn refinement process as a Markov Decision Process and introduce DPSDP (Direct Policy Search by Dynamic Programming), a reinforcement learning algorithm that trains an actor-critic LLM system to iteratively refine answers via direct preference learning on self-generated data. Theoretically, DPSDP can match the performance of any policy within the training distribution. Empirically, we instantiate DPSDP with various base models and show improvements on both in- and out-of-distribution benchmarks. For example, on benchmark MATH 500, majority voting over five refinement steps increases first-turn accuracy from 58.2% to 63.2% with Ministral-based models. An ablation study further confirms the benefits of multi-agent collaboration and out-of-distribution generalization.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Network Threat Detection: Addressing Class Imbalanced Data with Deep Forest</title>
<link>https://arxiv.org/abs/2506.08383</link>
<guid>https://arxiv.org/abs/2506.08383</guid>
<content:encoded><![CDATA[
arXiv:2506.08383v1 Announce Type: new 
Abstract: With the rapid expansion of Internet of Things (IoT) networks, detecting malicious traffic in real-time has become a critical cybersecurity challenge. This research addresses the detection challenges by presenting a comprehensive empirical analysis of machine learning techniques for malware detection using the IoT-23 dataset provided by the Stratosphere Laboratory. We address the significant class imbalance within the dataset through three resampling strategies. We implement and compare a few machine learning techniques. Our findings demonstrate that the combination of appropriate imbalance treatment techniques with ensemble methods, particularly gcForest, achieves better detection performance compared to traditional approaches. This work contributes significantly to the development of more intelligent and efficient automated threat detection systems for IoT environments, helping to secure critical infrastructure against sophisticated cyber attacks while optimizing computational resource usage.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning Teachers of Test Time Scaling</title>
<link>https://arxiv.org/abs/2506.08388</link>
<guid>https://arxiv.org/abs/2506.08388</guid>
<content:encoded><![CDATA[
arXiv:2506.08388v1 Announce Type: new 
Abstract: Training reasoning language models (LMs) with reinforcement learning (RL) for one-hot correctness inherently relies on the LM being able to explore and solve its task with some chance at initialization. Furthermore, a key use case of reasoning LMs is to act as teachers for distilling new students and cold-starting future RL iterations rather than being deployed themselves. From these considerations, we introduce a new framework that avoids RL's exploration challenge by training a new class of Reinforcement-Learned Teachers (RLTs) focused on yielding the most effective downstream distillation. RLTs are prompted with both the question and solution to each problem, and tasked to simply "connect-the-dots" with detailed explanations tailored for their students. We train RLTs with dense rewards obtained by feeding each explanation to the student and testing its understanding of the problem's solution. In practice, the raw outputs of a 7B RLT provide higher final performance on competition and graduate-level tasks than existing distillation and cold-starting pipelines that collect and postprocess the reasoning traces of orders of magnitude larger LMs. Furthermore, RLTs maintain their effectiveness when training larger students and when applied zero-shot to out-of-distribution tasks, unlocking new levels of efficiency and re-usability for the RL reasoning framework.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatiotemporal deep learning models for detection of rapid intensification in cyclones</title>
<link>https://arxiv.org/abs/2506.08397</link>
<guid>https://arxiv.org/abs/2506.08397</guid>
<content:encoded><![CDATA[
arXiv:2506.08397v1 Announce Type: new 
Abstract: Cyclone rapid intensification is the rapid increase in cyclone wind intensity, exceeding a threshold of 30 knots, within 24 hours. Rapid intensification is considered an extreme event during a cyclone, and its occurrence is relatively rare, contributing to a class imbalance in the dataset. A diverse array of factors influences the likelihood of a cyclone undergoing rapid intensification, further complicating the task for conventional machine learning models. In this paper, we evaluate deep learning, ensemble learning and data augmentation frameworks to detect cyclone rapid intensification based on wind intensity and spatial coordinates. We note that conventional data augmentation methods cannot be utilised for generating spatiotemporal patterns replicating cyclones that undergo rapid intensification. Therefore, our framework employs deep learning models to generate spatial coordinates and wind intensity that replicate cyclones to address the class imbalance problem of rapid intensification. We also use a deep learning model for the classification module within the data augmentation framework to differentiate between rapid and non-rapid intensification events during a cyclone. Our results show that data augmentation improves the results for rapid intensification detection in cyclones, and spatial coordinates play a critical role as input features to the given models. This paves the way for research in synthetic data generation for spatiotemporal data with extreme events.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FUSE: Measure-Theoretic Compact Fuzzy Set Representation for Taxonomy Expansion</title>
<link>https://arxiv.org/abs/2506.08409</link>
<guid>https://arxiv.org/abs/2506.08409</guid>
<content:encoded><![CDATA[
arXiv:2506.08409v1 Announce Type: new 
Abstract: Taxonomy Expansion, which models complex concepts and their relations, can be formulated as a set representation learning task. The generalization of set, fuzzy set, incorporates uncertainty and measures the information within a semantic concept, making it suitable for concept modeling. Existing works usually model sets as vectors or geometric objects such as boxes, which are not closed under set operations. In this work, we propose a sound and efficient formulation of set representation learning based on its volume approximation as a fuzzy set. The resulting embedding framework, Fuzzy Set Embedding (FUSE), satisfies all set operations and compactly approximates the underlying fuzzy set, hence preserving information while being efficient to learn, relying on minimum neural architecture. We empirically demonstrate the power of FUSE on the task of taxonomy expansion, where FUSE achieves remarkable improvements up to 23% compared with existing baselines. Our work marks the first attempt to understand and efficiently compute the embeddings of fuzzy sets.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Hear Broken Motors: Signature-Guided Data Augmentation for Induction-Motor Diagnostics</title>
<link>https://arxiv.org/abs/2506.08412</link>
<guid>https://arxiv.org/abs/2506.08412</guid>
<content:encoded><![CDATA[
arXiv:2506.08412v1 Announce Type: new 
Abstract: The application of machine learning (ML) algorithms in the intelligent diagnosis of three-phase engines has the potential to significantly enhance diagnostic performance and accuracy. Traditional methods largely rely on signature analysis, which, despite being a standard practice, can benefit from the integration of advanced ML techniques. In our study, we innovate by combining ML algorithms with a novel unsupervised anomaly generation methodology that takes into account the engine physics model. We propose Signature-Guided Data Augmentation (SGDA), an unsupervised framework that synthesizes physically plausible faults directly in the frequency domain of healthy current signals. Guided by Motor Current Signature Analysis, SGDA creates diverse and realistic anomalies without resorting to computationally intensive simulations. This hybrid approach leverages the strengths of both supervised ML and unsupervised signature analysis, achieving superior diagnostic accuracy and reliability along with wide industrial application. The findings highlight the potential of our approach to contribute significantly to the field of engine diagnostics, offering a robust and efficient solution for real-world applications.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Scaling Laws in Linear Regression via Data Reuse</title>
<link>https://arxiv.org/abs/2506.08415</link>
<guid>https://arxiv.org/abs/2506.08415</guid>
<content:encoded><![CDATA[
arXiv:2506.08415v1 Announce Type: new 
Abstract: Neural scaling laws suggest that the test error of large language models trained online decreases polynomially as the model size and data size increase. However, such scaling can be unsustainable when running out of new data. In this work, we show that data reuse can improve existing scaling laws in linear regression. Specifically, we derive sharp test error bounds on $M$-dimensional linear models trained by multi-pass stochastic gradient descent (multi-pass SGD) on $N$ data with sketched features. Assuming that the data covariance has a power-law spectrum of degree $a$, and that the true parameter follows a prior with an aligned power-law spectrum of degree $b-a$ (with $a > b > 1$), we show that multi-pass SGD achieves a test error of $\Theta(M^{1-b} + L^{(1-b)/a})$, where $L \lesssim N^{a/b}$ is the number of iterations. In the same setting, one-pass SGD only attains a test error of $\Theta(M^{1-b} + N^{(1-b)/a})$ (see e.g., Lin et al., 2024). This suggests an improved scaling law via data reuse (i.e., choosing $L>N$) in data-constrained regimes. Numerical simulations are also provided to verify our theoretical findings.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Offline RL with Smooth OOD Generalization in Convex Hull and its Neighborhood</title>
<link>https://arxiv.org/abs/2506.08417</link>
<guid>https://arxiv.org/abs/2506.08417</guid>
<content:encoded><![CDATA[
arXiv:2506.08417v1 Announce Type: new 
Abstract: Offline Reinforcement Learning (RL) struggles with distributional shifts, leading to the $Q$-value overestimation for out-of-distribution (OOD) actions. Existing methods address this issue by imposing constraints; however, they often become overly conservative when evaluating OOD regions, which constrains the $Q$-function generalization. This over-constraint issue results in poor $Q$-value estimation and hinders policy improvement. In this paper, we introduce a novel approach to achieve better $Q$-value estimation by enhancing $Q$-function generalization in OOD regions within Convex Hull and its Neighborhood (CHN). Under the safety generalization guarantees of the CHN, we propose the Smooth Bellman Operator (SBO), which updates OOD $Q$-values by smoothing them with neighboring in-sample $Q$-values. We theoretically show that SBO approximates true $Q$-values for both in-sample and OOD actions within the CHN. Our practical algorithm, Smooth Q-function OOD Generalization (SQOG), empirically alleviates the over-constraint issue, achieving near-accurate $Q$-value estimation. On the D4RL benchmarks, SQOG outperforms existing state-of-the-art methods in both performance and computational efficiency.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Learning-guided Learning Rate Adaptation via Gradient Alignment</title>
<link>https://arxiv.org/abs/2506.08419</link>
<guid>https://arxiv.org/abs/2506.08419</guid>
<content:encoded><![CDATA[
arXiv:2506.08419v1 Announce Type: new 
Abstract: The performance of an optimizer on large-scale deep learning models depends critically on fine-tuning the learning rate, often requiring an extensive grid search over base learning rates, schedules, and other hyperparameters. In this paper, we propose a principled framework called GALA (Gradient Alignment-based Learning rate Adaptation), which dynamically adjusts the learning rate by tracking the alignment between consecutive gradients and using a local curvature estimate. Guided by the convergence analysis, we formulate the problem of selecting the learning rate as a one-dimensional online learning problem. When paired with an online learning algorithm such as Follow-the-Regularized-Leader, our method produces a flexible, adaptive learning rate schedule that tends to increase when consecutive gradients are aligned and decrease otherwise. We establish a data-adaptive convergence rate for normalized SGD equipped with GALA in the smooth, nonconvex setting. Empirically, common optimizers such as SGD and Adam, when augmented with GALA, demonstrate robust performance across a wide range of initial learning rates and perform competitively without the need for tuning.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HASFL: Heterogeneity-aware Split Federated Learning over Edge Computing Systems</title>
<link>https://arxiv.org/abs/2506.08426</link>
<guid>https://arxiv.org/abs/2506.08426</guid>
<content:encoded><![CDATA[
arXiv:2506.08426v1 Announce Type: new 
Abstract: Split federated learning (SFL) has emerged as a promising paradigm to democratize machine learning (ML) on edge devices by enabling layer-wise model partitioning. However, existing SFL approaches suffer significantly from the straggler effect due to the heterogeneous capabilities of edge devices. To address the fundamental challenge, we propose adaptively controlling batch sizes (BSs) and model splitting (MS) for edge devices to overcome resource heterogeneity. We first derive a tight convergence bound of SFL that quantifies the impact of varied BSs and MS on learning performance. Based on the convergence bound, we propose HASFL, a heterogeneity-aware SFL framework capable of adaptively controlling BS and MS to balance communication-computing latency and training convergence in heterogeneous edge networks. Extensive experiments with various datasets validate the effectiveness of HASFL and demonstrate its superiority over state-of-the-art benchmarks.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting Gradient Leakage Attacks: Data Reconstruction in Realistic FL Settings</title>
<link>https://arxiv.org/abs/2506.08435</link>
<guid>https://arxiv.org/abs/2506.08435</guid>
<content:encoded><![CDATA[
arXiv:2506.08435v1 Announce Type: new 
Abstract: Federated learning (FL) enables collaborative model training among multiple clients without the need to expose raw data. Its ability to safeguard privacy, at the heart of FL, has recently been a hot-button debate topic. To elaborate, several studies have introduced a type of attacks known as gradient leakage attacks (GLAs), which exploit the gradients shared during training to reconstruct clients' raw data. On the flip side, some literature, however, contends no substantial privacy risk in practical FL environments due to the effectiveness of such GLAs being limited to overly relaxed conditions, such as small batch sizes and knowledge of clients' data distributions.
  This paper bridges this critical gap by empirically demonstrating that clients' data can still be effectively reconstructed, even within realistic FL environments. Upon revisiting GLAs, we recognize that their performance failures stem from their inability to handle the gradient matching problem. To alleviate the performance bottlenecks identified above, we develop FedLeak, which introduces two novel techniques, partial gradient matching and gradient regularization. Moreover, to evaluate the performance of FedLeak in real-world FL environments, we formulate a practical evaluation protocol grounded in a thorough review of extensive FL literature and industry practices. Under this protocol, FedLeak can still achieve high-fidelity data reconstruction, thereby underscoring the significant vulnerability in FL systems and the urgent need for more effective defense methods.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Lead: Incentivizing Strategic Agents in the Dark</title>
<link>https://arxiv.org/abs/2506.08438</link>
<guid>https://arxiv.org/abs/2506.08438</guid>
<content:encoded><![CDATA[
arXiv:2506.08438v1 Announce Type: new 
Abstract: We study an online learning version of the generalized principal-agent model, where a principal interacts repeatedly with a strategic agent possessing private types, private rewards, and taking unobservable actions. The agent is non-myopic, optimizing a discounted sum of future rewards and may strategically misreport types to manipulate the principal's learning. The principal, observing only her own realized rewards and the agent's reported types, aims to learn an optimal coordination mechanism that minimizes strategic regret. We develop the first provably sample-efficient algorithm for this challenging setting. Our approach features a novel pipeline that combines (i) a delaying mechanism to incentivize approximately myopic agent behavior, (ii) an innovative reward angle estimation framework that uses sector tests and a matching procedure to recover type-dependent reward functions, and (iii) a pessimistic-optimistic LinUCB algorithm that enables the principal to explore efficiently while respecting the agent's incentive constraints. We establish a near optimal $\tilde{O}(\sqrt{T}) $ regret bound for learning the principal's optimal policy, where $\tilde{O}(\cdot) $ omits logarithmic factors. Our results open up new avenues for designing robust online learning algorithms for a wide range of game-theoretic settings involving private types and strategic agents.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time-Aware World Model for Adaptive Prediction and Control</title>
<link>https://arxiv.org/abs/2506.08441</link>
<guid>https://arxiv.org/abs/2506.08441</guid>
<content:encoded><![CDATA[
arXiv:2506.08441v1 Announce Type: new 
Abstract: In this work, we introduce the Time-Aware World Model (TAWM), a model-based approach that explicitly incorporates temporal dynamics. By conditioning on the time-step size, {\Delta}t, and training over a diverse range of {\Delta}t values -- rather than sampling at a fixed time-step -- TAWM learns both high- and low-frequency task dynamics across diverse control problems. Grounded in the information-theoretic insight that the optimal sampling rate depends on a system's underlying dynamics, this time-aware formulation improves both performance and data efficiency. Empirical evaluations show that TAWM consistently outperforms conventional models across varying observation rates in a variety of control tasks, using the same number of training samples and iterations. Our code can be found online at: github.com/anh-nn01/Time-Aware-World-Model.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOBODY: Model Based Off-Dynamics Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.08460</link>
<guid>https://arxiv.org/abs/2506.08460</guid>
<content:encoded><![CDATA[
arXiv:2506.08460v1 Announce Type: new 
Abstract: We study the off-dynamics offline reinforcement learning problem, where the goal is to learn a policy from offline datasets collected from source and target domains with mismatched transition. Existing off-dynamics offline RL methods typically either filter source transitions that resemble those of the target domain or apply reward augmentation to source data, both constrained by the limited transitions available from the target domain. As a result, the learned policy is unable to explore target domain beyond the offline datasets. We propose MOBODY, a Model-Based Off-Dynamics offline RL algorithm that addresses this limitation by enabling exploration of the target domain via learned dynamics. MOBODY generates new synthetic transitions in the target domain through model rollouts, which are used as data augmentation during offline policy learning. Unlike existing model-based methods that learn dynamics from a single domain, MOBODY tackles the challenge of mismatched dynamics by leveraging both source and target datasets. Directly merging these datasets can bias the learned model toward source dynamics. Instead, MOBODY learns target dynamics by discovering a shared latent representation of states and transitions across domains through representation learning. To stabilize training, MOBODY incorporates a behavior cloning loss that regularizes the policy. Specifically, we introduce a Q-weighted behavior cloning loss that regularizes the policy toward actions with high target-domain Q-values, rather than uniformly imitating all actions in the dataset. These Q-values are learned from an enhanced target dataset composed of offline target data, augmented source data, and rollout data from the learned target dynamics. We evaluate MOBODY on MuJoCo benchmarks and show that it significantly outperforms state-of-the-art baselines, with especially pronounced improvements in challenging scenarios.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to Provably Improve Return Conditioned Supervised Learning?</title>
<link>https://arxiv.org/abs/2506.08463</link>
<guid>https://arxiv.org/abs/2506.08463</guid>
<content:encoded><![CDATA[
arXiv:2506.08463v1 Announce Type: new 
Abstract: In sequential decision-making problems, Return-Conditioned Supervised Learning (RCSL) has gained increasing recognition for its simplicity and stability in modern decision-making tasks. Unlike traditional offline reinforcement learning (RL) algorithms, RCSL frames policy learning as a supervised learning problem by taking both the state and return as input. This approach eliminates the instability often associated with temporal difference (TD) learning in offline RL. However, RCSL has been criticized for lacking the stitching property, meaning its performance is inherently limited by the quality of the policy used to generate the offline dataset. To address this limitation, we propose a principled and simple framework called Reinforced RCSL. The key innovation of our framework is the introduction of a concept we call the in-distribution optimal return-to-go. This mechanism leverages our policy to identify the best achievable in-dataset future return based on the current state, avoiding the need for complex return augmentation techniques. Our theoretical analysis demonstrates that Reinforced RCSL can consistently outperform the standard RCSL approach. Empirical results further validate our claims, showing significant performance improvements across a range of benchmarks.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAC: An Efficient Gradient Preconditioning using Mean Activation Approximated Curvature</title>
<link>https://arxiv.org/abs/2506.08464</link>
<guid>https://arxiv.org/abs/2506.08464</guid>
<content:encoded><![CDATA[
arXiv:2506.08464v1 Announce Type: new 
Abstract: Second-order optimization methods for training neural networks, such as KFAC, exhibit superior convergence by utilizing curvature information of loss landscape. However, it comes at the expense of high computational burden. In this work, we analyze the two components that constitute the layer-wise Fisher information matrix (FIM) used in KFAC: the Kronecker factors related to activations and pre-activation gradients. Based on empirical observations on their eigenspectra, we propose efficient approximations for them, resulting in a computationally efficient optimization method called MAC. To the best of our knowledge, MAC is the first algorithm to apply the Kronecker factorization to the FIM of attention layers used in transformers and explicitly integrate attention scores into the preconditioning. We also study the convergence property of MAC on nonlinear neural networks and provide two conditions under which it converges to global minima. Our extensive evaluations on various network architectures and datasets show that the proposed method outperforms KFAC and other state-of-the-art methods in terms of accuracy, end-to-end training time, and memory usage. Code is available at https://github.com/hseung88/mac.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AsFT: Anchoring Safety During LLM Fine-Tuning Within Narrow Safety Basin</title>
<link>https://arxiv.org/abs/2506.08473</link>
<guid>https://arxiv.org/abs/2506.08473</guid>
<content:encoded><![CDATA[
arXiv:2506.08473v1 Announce Type: new 
Abstract: Large language models (LLMs) are vulnerable to safety risks during fine-tuning, where small amounts of malicious or harmless data can compromise safeguards. In this paper, building on the concept of alignment direction -- defined by the weight difference between aligned and unaligned models -- we observe that perturbations along this direction preserve model safety. In contrast, perturbations along directions orthogonal to this alignment are strongly linked to harmful direction perturbations, rapidly degrading safety and framing the parameter space as a narrow safety basin. Based on this insight, we propose a methodology for safety fine-tuning called AsFT (Anchoring Safety in Fine-Tuning), which integrates a regularization term into the training objective. This term uses the alignment direction as an anchor to suppress updates in harmful directions, ensuring that fine-tuning is constrained within the narrow safety basin. Extensive experiments on multiple datasets show that AsFT outperforms Safe LoRA, reducing harmful behavior by 7.60 percent, improving model performance by 3.44 percent, and maintaining robust performance across various experimental settings. Code is available at https://github.com/PKU-YuanGroup/AsFT
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thermodynamically Consistent Latent Dynamics Identification for Parametric Systems</title>
<link>https://arxiv.org/abs/2506.08475</link>
<guid>https://arxiv.org/abs/2506.08475</guid>
<content:encoded><![CDATA[
arXiv:2506.08475v1 Announce Type: new 
Abstract: We propose an efficient thermodynamics-informed latent space dynamics identification (tLaSDI) framework for the reduced-order modeling of parametric nonlinear dynamical systems. This framework integrates autoencoders for dimensionality reduction with newly developed parametric GENERIC formalism-informed neural networks (pGFINNs), which enable efficient learning of parametric latent dynamics while preserving key thermodynamic principles such as free energy conservation and entropy generation across the parameter space. To further enhance model performance, a physics-informed active learning strategy is incorporated, leveraging a greedy, residual-based error indicator to adaptively sample informative training data, outperforming uniform sampling at equivalent computational cost. Numerical experiments on the Burgers' equation and the 1D/1V Vlasov-Poisson equation demonstrate that the proposed method achieves up to 3,528x speed-up with 1-3% relative errors, and significant reduction in training (50-90%) and inference (57-61%) cost. Moreover, the learned latent space dynamics reveal the underlying thermodynamic behavior of the system, offering valuable insights into the physical-space dynamics.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explaining, Fast and Slow: Abstraction and Refinement of Provable Explanations</title>
<link>https://arxiv.org/abs/2506.08505</link>
<guid>https://arxiv.org/abs/2506.08505</guid>
<content:encoded><![CDATA[
arXiv:2506.08505v1 Announce Type: new 
Abstract: Despite significant advancements in post-hoc explainability techniques for neural networks, many current methods rely on heuristics and do not provide formally provable guarantees over the explanations provided. Recent work has shown that it is possible to obtain explanations with formal guarantees by identifying subsets of input features that are sufficient to determine that predictions remain unchanged using neural network verification techniques. Despite the appeal of these explanations, their computation faces significant scalability challenges. In this work, we address this gap by proposing a novel abstraction-refinement technique for efficiently computing provably sufficient explanations of neural network predictions. Our method abstracts the original large neural network by constructing a substantially reduced network, where a sufficient explanation of the reduced network is also provably sufficient for the original network, hence significantly speeding up the verification process. If the explanation is in sufficient on the reduced network, we iteratively refine the network size by gradually increasing it until convergence. Our experiments demonstrate that our approach enhances the efficiency of obtaining provably sufficient explanations for neural network predictions while additionally providing a fine-grained interpretation of the network's predictions across different abstraction levels.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffGradCAM: A Universal Class Activation Map Resistant to Adversarial Training</title>
<link>https://arxiv.org/abs/2506.08514</link>
<guid>https://arxiv.org/abs/2506.08514</guid>
<content:encoded><![CDATA[
arXiv:2506.08514v1 Announce Type: new 
Abstract: Class Activation Mapping (CAM) and its gradient-based variants (e.g., GradCAM) have become standard tools for explaining Convolutional Neural Network (CNN) predictions. However, these approaches typically focus on individual logits, while for neural networks using softmax, the class membership probability estimates depend \textit{only} on the \textit{differences} between logits, not on their absolute values. This disconnect leaves standard CAMs vulnerable to adversarial manipulation, such as passive fooling, where a model is trained to produce misleading CAMs without affecting decision performance. We introduce \textbf{Salience-Hoax Activation Maps (SHAMs)}, an \emph{entropy-aware form of passive fooling} that serves as a benchmark for CAM robustness under adversarial conditions. To address the passive fooling vulnerability, we then propose \textbf{DiffGradCAM}, a novel, lightweight, and contrastive approach to class activation mapping that is both non-suceptible to passive fooling, but also matches the output of standard CAM methods such as GradCAM in the non-adversarial case. Together, SHAM and DiffGradCAM establish a new framework for probing and improving the robustness of saliency-based explanations. We validate both contributions across multi-class tasks with few and many classes.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeurIPS 2024 ML4CFD Competition: Results and Retrospective Analysis</title>
<link>https://arxiv.org/abs/2506.08516</link>
<guid>https://arxiv.org/abs/2506.08516</guid>
<content:encoded><![CDATA[
arXiv:2506.08516v1 Announce Type: new 
Abstract: The integration of machine learning (ML) into the physical sciences is reshaping computational paradigms, offering the potential to accelerate demanding simulations such as computational fluid dynamics (CFD). Yet, persistent challenges in accuracy, generalization, and physical consistency hinder the practical deployment of ML models in scientific domains. To address these limitations and systematically benchmark progress, we organized the ML4CFD competition, centered on surrogate modeling for aerodynamic simulations over two-dimensional airfoils. The competition attracted over 240 teams, who were provided with a curated dataset generated via OpenFOAM and evaluated through a multi-criteria framework encompassing predictive accuracy, physical fidelity, computational efficiency, and out-of-distribution generalization. This retrospective analysis reviews the competition outcomes, highlighting several approaches that outperformed baselines under our global evaluation score. Notably, the top entry exceeded the performance of the original OpenFOAM solver on aggregate metrics, illustrating the promise of ML-based surrogates to outperform traditional solvers under tailored criteria. Drawing from these results, we analyze the key design principles of top submissions, assess the robustness of our evaluation framework, and offer guidance for future scientific ML challenges.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging chaos in the training of artificial neural networks</title>
<link>https://arxiv.org/abs/2506.08523</link>
<guid>https://arxiv.org/abs/2506.08523</guid>
<content:encoded><![CDATA[
arXiv:2506.08523v1 Announce Type: new 
Abstract: Traditional algorithms to optimize artificial neural networks when confronted with a supervised learning task are usually exploitation-type relaxational dynamics such as gradient descent (GD). Here, we explore the dynamics of the neural network trajectory along training for unconventionally large learning rates. We show that for a region of values of the learning rate, the GD optimization shifts away from purely exploitation-like algorithm into a regime of exploration-exploitation balance, as the neural network is still capable of learning but the trajectory shows sensitive dependence on initial conditions -- as characterized by positive network maximum Lyapunov exponent --. Interestingly, the characteristic training time required to reach an acceptable accuracy in the test set reaches a minimum precisely in such learning rate region, further suggesting that one can accelerate the training of artificial neural networks by locating at the onset of chaos. Our results -- initially illustrated for the MNIST classification task -- qualitatively hold for a range of supervised learning tasks, learning architectures and other hyperparameters, and showcase the emergent, constructive role of transient chaotic dynamics in the training of artificial neural networks.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Evolutionary Multi-Objective Network Architecture Search for Reinforcement Learning (EMNAS-RL)</title>
<link>https://arxiv.org/abs/2506.08533</link>
<guid>https://arxiv.org/abs/2506.08533</guid>
<content:encoded><![CDATA[
arXiv:2506.08533v1 Announce Type: new 
Abstract: This paper introduces Evolutionary Multi-Objective Network Architecture Search (EMNAS) for the first time to optimize neural network architectures in large-scale Reinforcement Learning (RL) for Autonomous Driving (AD). EMNAS uses genetic algorithms to automate network design, tailored to enhance rewards and reduce model size without compromising performance. Additionally, parallelization techniques are employed to accelerate the search, and teacher-student methodologies are implemented to ensure scalable optimization. This research underscores the potential of transfer learning as a robust framework for optimizing performance across iterative learning processes by effectively leveraging knowledge from earlier generations to enhance learning efficiency and stability in subsequent generations. Experimental results demonstrate that tailored EMNAS outperforms manually designed models, achieving higher rewards with fewer parameters. The findings of these strategies contribute positively to EMNAS for RL in autonomous driving, advancing the field toward better-performing networks suitable for real-world scenarios.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepForm: Reasoning Large Language Model for Communication System Formulation</title>
<link>https://arxiv.org/abs/2506.08551</link>
<guid>https://arxiv.org/abs/2506.08551</guid>
<content:encoded><![CDATA[
arXiv:2506.08551v1 Announce Type: new 
Abstract: Communication system formulation is critical for advancing 6G and future wireless technologies, yet it remains a complex, expertise-intensive task. While Large Language Models (LLMs) offer potential, existing general-purpose models often lack the specialized domain knowledge, nuanced reasoning capabilities, and access to high-quality, domain-specific training data required for adapting a general LLM into an LLM specially for communication system formulation. To bridge this gap, we introduce DeepForm, the first reasoning LLM specially for automated communication system formulation. We propose the world-first large-scale, open-source dataset meticulously curated for this domain called Communication System Formulation Reasoning Corpus (CSFRC). Our framework employs a two-stage training strategy: first, Supervised Fine-Tuning (SFT) with Chain-of-Thought (CoT) data to distill domain knowledge; second, a novel rule-based Reinforcement Learning (RL) algorithm, C-ReMax based on ReMax, to cultivate advanced modeling capabilities and elicit sophisticated reasoning patterns like self-correction and verification. Extensive experiments demonstrate that our model achieves state-of-the-art performance, significantly outperforming larger proprietary LLMs on diverse senerios. We will release related resources to foster further research in this area after the paper is accepted.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Geometries of Truth Are Orthogonal Across Tasks</title>
<link>https://arxiv.org/abs/2506.08572</link>
<guid>https://arxiv.org/abs/2506.08572</guid>
<content:encoded><![CDATA[
arXiv:2506.08572v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated impressive generalization capabilities across various tasks, but their claim to practical relevance is still mired by concerns on their reliability. Recent works have proposed examining the activations produced by an LLM at inference time to assess whether its answer to a question is correct. Some works claim that a "geometry of truth" can be learned from examples, in the sense that the activations that generate correct answers can be distinguished from those leading to mistakes with a linear classifier. In this work, we underline a limitation of these approaches: we observe that these "geometries of truth" are intrinsically task-dependent and fail to transfer across tasks. More precisely, we show that linear classifiers trained across distinct tasks share little similarity and, when trained with sparsity-enforcing regularizers, have almost disjoint supports. We show that more sophisticated approaches (e.g., using mixtures of probes and tasks) fail to overcome this limitation, likely because activation vectors commonly used to classify answers form clearly separated clusters when examined across tasks.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLEEPYLAND: trust begins with fair evaluation of automatic sleep staging models</title>
<link>https://arxiv.org/abs/2506.08574</link>
<guid>https://arxiv.org/abs/2506.08574</guid>
<content:encoded><![CDATA[
arXiv:2506.08574v1 Announce Type: new 
Abstract: Despite advances in deep learning for automatic sleep staging, clinical adoption remains limited due to challenges in fair model evaluation, generalization across diverse datasets, model bias, and variability in human annotations. We present SLEEPYLAND, an open-source sleep staging evaluation framework designed to address these barriers. It includes more than 22'0000 hours in-domain (ID) sleep recordings, and more than 84'000 hours out-of-domain (OOD) sleep recordings, spanning a broad range of ages, sleep-wake disorders, and hardware setups. We release pre-trained models based on high-performing SoA architectures and evaluate them under standardized conditions across single- and multi-channel EEG/EOG configurations. We introduce SOMNUS, an ensemble combining models across architectures and channel setups via soft voting. SOMNUS achieves robust performance across twenty-four different datasets, with macro-F1 scores between 68.7% and 87.2%, outperforming individual models in 94.9% of cases. Notably, SOMNUS surpasses previous SoA methods, even including cases where compared models were trained ID while SOMNUS treated the same data as OOD. Using a subset of the BSWR (N=6'633), we quantify model biases linked to age, gender, AHI, and PLMI, showing that while ensemble improves robustness, no model architecture consistently minimizes bias in performance and clinical markers estimation. In evaluations on OOD multi-annotated datasets (DOD-H, DOD-O), SOMNUS exceeds the best human scorer, i.e., MF1 85.2% vs 80.8% on DOD-H, and 80.2% vs 75.9% on DOD-O, better reproducing the scorer consensus than any individual expert (k = 0.89/0.85 and ACS = 0.95/0.94 for healthy/OSA cohorts). Finally, we introduce ensemble disagreement metrics - entropy and inter-model divergence based - predicting regions of scorer disagreement with ROC AUCs up to 0.828, offering a data-driven proxy for human uncertainty.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion-based Time Series Forecasting for Sewerage Systems</title>
<link>https://arxiv.org/abs/2506.08577</link>
<guid>https://arxiv.org/abs/2506.08577</guid>
<content:encoded><![CDATA[
arXiv:2506.08577v1 Announce Type: new 
Abstract: We introduce a novel deep learning approach that harnesses the power of generative artificial intelligence to enhance the accuracy of contextual forecasting in sewerage systems. By developing a diffusion-based model that processes multivariate time series data, our system excels at capturing complex correlations across diverse environmental signals, enabling robust predictions even during extreme weather events. To strengthen the model's reliability, we further calibrate its predictions with a conformal inference technique, tailored for probabilistic time series data, ensuring that the resulting prediction intervals are statistically reliable and cover the true target values with a desired confidence level. Our empirical tests on real sewerage system data confirm the model's exceptional capability to deliver reliable contextual predictions, maintaining accuracy even under severe weather conditions.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CALT: A Library for Computer Algebra with Transformer</title>
<link>https://arxiv.org/abs/2506.08600</link>
<guid>https://arxiv.org/abs/2506.08600</guid>
<content:encoded><![CDATA[
arXiv:2506.08600v1 Announce Type: new 
Abstract: Recent advances in artificial intelligence have demonstrated the learnability of symbolic computation through end-to-end deep learning. Given a sufficient number of examples of symbolic expressions before and after the target computation, Transformer models - highly effective learners of sequence-to-sequence functions - can be trained to emulate the computation. This development opens up several intriguing challenges and new research directions, which require active contributions from the symbolic computation community. In this work, we introduce Computer Algebra with Transformer (CALT), a user-friendly Python library designed to help non-experts in deep learning train models for symbolic computation tasks.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flow Matching Meets PDEs: A Unified Framework for Physics-Constrained Generation</title>
<link>https://arxiv.org/abs/2506.08604</link>
<guid>https://arxiv.org/abs/2506.08604</guid>
<content:encoded><![CDATA[
arXiv:2506.08604v1 Announce Type: new 
Abstract: Generative machine learning methods, such as diffusion models and flow matching, have shown great potential in modeling complex system behaviors and building efficient surrogate models. However, these methods typically learn the underlying physics implicitly from data. We propose Physics-Based Flow Matching (PBFM), a novel generative framework that explicitly embeds physical constraints, both PDE residuals and algebraic relations, into the flow matching objective. We also introduce temporal unrolling at training time that improves the accuracy of the final, noise-free sample prediction. Our method jointly minimizes the flow matching loss and the physics-based residual loss without requiring hyperparameter tuning of their relative weights. Additionally, we analyze the role of the minimum noise level, $\sigma_{\min}$, in the context of physical constraints and evaluate a stochastic sampling strategy that helps to reduce physical residuals. Through extensive benchmarks on three representative PDE problems, we show that our approach yields up to an $8\times$ more accurate physical residuals compared to FM, while clearly outperforming existing algorithms in terms of distributional accuracy. PBFM thus provides a principled and efficient framework for surrogate modeling, uncertainty quantification, and accelerated simulation in physics and engineering applications.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sample Efficient Demonstration Selection for In-Context Learning</title>
<link>https://arxiv.org/abs/2506.08607</link>
<guid>https://arxiv.org/abs/2506.08607</guid>
<content:encoded><![CDATA[
arXiv:2506.08607v1 Announce Type: new 
Abstract: The in-context learning paradigm with LLMs has been instrumental in advancing a wide range of natural language processing tasks. The selection of few-shot examples (exemplars / demonstration samples) is essential for constructing effective prompts under context-length budget constraints. In this paper, we formulate the exemplar selection task as a top-m best arms identification problem. A key challenge in this setup is the exponentially large number of arms that need to be evaluated to identify the m-best arms. We propose CASE (Challenger Arm Sampling for Exemplar selection), a novel sample-efficient selective exploration strategy that maintains a shortlist of "challenger" arms, which are current candidates for the top-m arms. In each iteration, only one of the arms from this shortlist or the current topm set is pulled, thereby reducing sample complexity and, consequently, the number of LLM evaluations. Furthermore, we model the scores of exemplar subsets (arms) using a parameterized linear scoring function, leading to stochastic linear bandits setting. CASE achieves remarkable efficiency gains of up to 7x speedup in runtime while requiring 7x fewer LLM calls (87% reduction) without sacrificing performance compared to state-of-the-art exemplar selection methods. We release our code and data at https://github.com/kiranpurohit/CASE
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HSG-12M: A Large-Scale Spatial Multigraph Dataset</title>
<link>https://arxiv.org/abs/2506.08618</link>
<guid>https://arxiv.org/abs/2506.08618</guid>
<content:encoded><![CDATA[
arXiv:2506.08618v1 Announce Type: new 
Abstract: Existing graph benchmarks assume non-spatial, simple edges, collapsing physically distinct paths into a single link. We introduce HSG-12M, the first large-scale dataset of $\textbf{spatial multigraphs}-$graphs embedded in a metric space where multiple geometrically distinct trajectories between two nodes are retained as separate edges. HSG-12M contains 11.6 million static and 5.1 million dynamic $\textit{Hamiltonian spectral graphs}$ across 1401 characteristic-polynomial classes, derived from 177 TB of spectral potential data. Each graph encodes the full geometry of a 1-D crystal's energy spectrum on the complex plane, producing diverse, physics-grounded topologies that transcend conventional node-coordinate datasets. To enable future extensions, we release $\texttt{Poly2Graph}$: a high-performance, open-source pipeline that maps arbitrary 1-D crystal Hamiltonians to spectral graphs. Benchmarks with popular GNNs expose new challenges in learning from multi-edge geometry at scale. Beyond its practical utility, we show that spectral graphs serve as universal topological fingerprints of polynomials, vectors, and matrices, forging a new algebra-to-graph link. HSG-12M lays the groundwork for geometry-aware graph learning and new opportunities of data-driven scientific discovery in condensed matter physics and beyond.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time Series Representations for Classification Lie Hidden in Pretrained Vision Transformers</title>
<link>https://arxiv.org/abs/2506.08641</link>
<guid>https://arxiv.org/abs/2506.08641</guid>
<content:encoded><![CDATA[
arXiv:2506.08641v1 Announce Type: new 
Abstract: Time series classification is a fundamental task in healthcare and industry, yet the development of time series foundation models (TSFMs) remains limited by the scarcity of publicly available time series datasets. In this work, we propose Time Vision Transformer (TiViT), a framework that converts time series into images to leverage the representational power of frozen Vision Transformers (ViTs) pretrained on large-scale image datasets. First, we theoretically motivate our approach by analyzing the 2D patching of ViTs for time series, showing that it can increase the number of label-relevant tokens and reduce the sample complexity. Second, we empirically demonstrate that TiViT achieves state-of-the-art performance on standard time series classification benchmarks by utilizing the hidden representations of large OpenCLIP models. We explore the structure of TiViT representations and find that intermediate layers with high intrinsic dimension are the most effective for time series classification. Finally, we assess the alignment between TiViT and TSFM representation spaces and identify a strong complementarity, with further performance gains achieved by combining their features. Our findings reveal yet another direction for reusing vision representations in a non-visual domain.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semi-gradient DICE for Offline Constrained Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.08644</link>
<guid>https://arxiv.org/abs/2506.08644</guid>
<content:encoded><![CDATA[
arXiv:2506.08644v1 Announce Type: new 
Abstract: Stationary Distribution Correction Estimation (DICE) addresses the mismatch between the stationary distribution induced by a policy and the target distribution required for reliable off-policy evaluation (OPE) and policy optimization. DICE-based offline constrained RL particularly benefits from the flexibility of DICE, as it simultaneously maximizes return while estimating costs in offline settings. However, we have observed that recent approaches designed to enhance the offline RL performance of the DICE framework inadvertently undermine its ability to perform OPE, making them unsuitable for constrained RL scenarios. In this paper, we identify the root cause of this limitation: their reliance on a semi-gradient optimization, which solves a fundamentally different optimization problem and results in failures in cost estimation. Building on these insights, we propose a novel method to enable OPE and constrained RL through semi-gradient DICE. Our method ensures accurate cost estimation and achieves state-of-the-art performance on the offline constrained RL benchmark, DSRL.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fusing Cross-modal and Uni-modal Representations: A Kronecker Product Approach</title>
<link>https://arxiv.org/abs/2506.08645</link>
<guid>https://arxiv.org/abs/2506.08645</guid>
<content:encoded><![CDATA[
arXiv:2506.08645v1 Announce Type: new 
Abstract: Cross-modal embeddings, such as CLIP, BLIP and their variants, have achieved promising results in aligning representations across modalities. However, these embeddings could underperform compared to state-of-the-art single-modality embeddings on modality-specific tasks. On the other hand, single-modality embeddings excel in their domains but lack cross-modal alignment capabilities. In this work, we focus on the problem of unifying cross-modality and single-modality embeddings to achieve the performance of modality-expert embedding within individual modalities while preserving cross-modal alignment. To this end, we propose RP-KrossFuse, a method that leverages a random projection-based Kronecker product to integrate cross-modal embeddings with single-modality embeddings. RP-KrossFuse aims to fuse the sample-pairwise similarity scores of the fused embeddings and operates efficiently in a specified kernel space and supports scalable implementations via random Fourier features for shift-invariant kernels such as the Gaussian kernel. We demonstrate the effectiveness of RP-KrossFuse through several numerical experiments, combining CLIP embeddings with uni-modal image and text embeddings. Our numerical results indicate that RP-KrossFuse achieves competitive modality-specific performance while retaining cross-modal alignment, bridging the gap between cross-modal and single-modality embeddings.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JoFormer (Journey-based Transformer): Theory and Empirical Analysis on the Tiny Shakespeare Dataset</title>
<link>https://arxiv.org/abs/2506.08652</link>
<guid>https://arxiv.org/abs/2506.08652</guid>
<content:encoded><![CDATA[
arXiv:2506.08652v1 Announce Type: new 
Abstract: Transformers have demonstrated remarkable success in sequence modeling, yet effectively incorporating positional information remains a challenging and active area of research. In this paper, we introduce JoFormer, a journey-based Transformer architecture grounded in a recently proposed non-commutative algebra for composing transformations across positions. JoFormer represents relative positions through learnable directional transforms that are sequentially composed along the input, thereby extending and generalizing existing approaches based on relative position representations. We derive the JoFormer attention mechanism from first principles and show that it subsumes standard methods such as rotary transformations as special cases. To evaluate its effectiveness, we compare JoFormer to the RoFormer baseline on the Tiny Shakespeare character-level language modeling task. Our results demonstrate that
  JoFormer consistently achieves lower perplexity and faster convergence, highlighting the advantages of its more expressive, journey-based treatment of position. Notably, the per-token JoFormer is still a primitive, conceptual variant with layer-independent angles, yet it already demonstrates strong performance-underscoring its promise as a proof of concept for more expressive architectures. We conclude by discussing how JoFormer offers a principled approach to integrating positional structure into Transformer architectures. The code used in this work is available at https://github.com/mahesh-godavarti/joformer.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Simple Model Just Works: Is Network Traffic Classification in Crisis?</title>
<link>https://arxiv.org/abs/2506.08655</link>
<guid>https://arxiv.org/abs/2506.08655</guid>
<content:encoded><![CDATA[
arXiv:2506.08655v1 Announce Type: new 
Abstract: Machine learning has been applied to network traffic classification (TC) for over two decades. While early efforts used shallow models, the latter 2010s saw a shift toward complex neural networks, often reporting near-perfect accuracy. However, it was recently revealed that a simple k-NN baseline using packet sequences metadata (sizes, times, and directions) can be on par or even outperform more complex methods. In this paper, we investigate this phenomenon further and evaluate this baseline across 12 datasets and 15 TC tasks, and investigate why it performs so well. Our analysis shows that most datasets contain over 50% redundant samples (identical packet sequences), which frequently appear in both training and test sets due to common splitting practices. This redundancy can lead to overestimated model performance and reduce the theoretical maximum accuracy when identical flows have conflicting labels. Given its distinct characteristics, we further argue that standard machine learning practices adapted from domains like NLP or computer vision may be ill-suited for TC. Finally, we propose new directions for task formulation and evaluation to address these challenges and help realign the field.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Robust Real-World Multivariate Time Series Forecasting: A Unified Framework for Dependency, Asynchrony, and Missingness</title>
<link>https://arxiv.org/abs/2506.08660</link>
<guid>https://arxiv.org/abs/2506.08660</guid>
<content:encoded><![CDATA[
arXiv:2506.08660v1 Announce Type: new 
Abstract: Real-world time series data are inherently multivariate, often exhibiting complex inter-channel dependencies. Each channel is typically sampled at its own period and is prone to missing values due to various practical and operational constraints. These characteristics pose fundamental challenges related to channel dependency, sampling asynchrony, and missingness, all of which must be addressed to enable robust and reliable forecasting in practical settings. However, most existing architectures are built on oversimplified assumptions, such as identical sampling periods across channels and fully observed inputs at test time, which often do not hold in real-world scenarios. To bridge this gap, we propose ChannelTokenFormer, a Transformer-based forecasting model with a flexible architecture designed to explicitly capture cross-channel interactions, accommodate channel-wise asynchronous sampling, and effectively handle missing values. Extensive experiments on three benchmark datasets modified to reflect practical settings, along with one real-world industrial dataset, demonstrate the superior robustness and accuracy of ChannelTokenFormer under challenging real-world conditions.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Learned Image Compression on Scalar and Entropy-Constraint Quantization</title>
<link>https://arxiv.org/abs/2506.08662</link>
<guid>https://arxiv.org/abs/2506.08662</guid>
<content:encoded><![CDATA[
arXiv:2506.08662v1 Announce Type: new 
Abstract: The continuous improvements on image compression with variational autoencoders have lead to learned codecs competitive with conventional approaches in terms of rate-distortion efficiency. Nonetheless, taking the quantization into account during the training process remains a problem, since it produces zero derivatives almost everywhere and needs to be replaced with a differentiable approximation which allows end-to-end optimization. Though there are different methods for approximating the quantization, none of them model the quantization noise correctly and thus, result in suboptimal networks. Hence, we propose an additional finetuning training step: After conventional end-to-end training, parts of the network are retrained on quantized latents obtained at the inference stage. For entropy-constraint quantizers like Trellis-Coded Quantization, the impact of the quantizer is particularly difficult to approximate by rounding or adding noise as the quantized latents are interdependently chosen through a trellis search based on both the entropy model and a distortion measure. We show that retraining on correctly quantized data consistently yields additional coding gain for both uniform scalar and especially for entropy-constraint quantization, without increasing inference complexity. For the Kodak test set, we obtain average savings between 1% and 2%, and for the TecNick test set up to 2.2% in terms of Bj{\o}ntegaard-Delta bitrate.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Reasoning Capabilities of Small Language Models with Blueprints and Prompt Template Search</title>
<link>https://arxiv.org/abs/2506.08669</link>
<guid>https://arxiv.org/abs/2506.08669</guid>
<content:encoded><![CDATA[
arXiv:2506.08669v1 Announce Type: new 
Abstract: Small language models (SLMs) offer promising and efficient alternatives to large language models (LLMs). However, SLMs' limited capacity restricts their reasoning capabilities and makes them sensitive to prompt variations. To address these challenges, we propose a novel framework that enhances SLM reasoning capabilities through LLM generated blueprints. The blueprints provide structured, high-level reasoning guides that help SLMs systematically tackle related problems. Furthermore, our framework integrates a prompt template search mechanism to mitigate the SLMs' sensitivity to prompt variations. Our framework demonstrates improved SLM performance across various tasks, including math (GSM8K), coding (MBPP), and logic reasoning (BBH). Our approach improves the reasoning capabilities of SLMs without increasing model size or requiring additional training, offering a lightweight and deployment-friendly solution for on-device or resource-constrained environments.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Fair Representation: Clustering and Consensus</title>
<link>https://arxiv.org/abs/2506.08673</link>
<guid>https://arxiv.org/abs/2506.08673</guid>
<content:encoded><![CDATA[
arXiv:2506.08673v1 Announce Type: new 
Abstract: Consensus clustering, a fundamental task in machine learning and data analysis, aims to aggregate multiple input clusterings of a dataset, potentially based on different non-sensitive attributes, into a single clustering that best represents the collective structure of the data. In this work, we study this fundamental problem through the lens of fair clustering, as introduced by Chierichetti et al. [NeurIPS'17], which incorporates the disparate impact doctrine to ensure proportional representation of each protected group in the dataset within every cluster. Our objective is to find a consensus clustering that is not only representative but also fair with respect to specific protected attributes. To the best of our knowledge, we are the first to address this problem and provide a constant-factor approximation.
  As part of our investigation, we examine how to minimally modify an existing clustering to enforce fairness -- an essential postprocessing step in many clustering applications that require fair representation. We develop an optimal algorithm for datasets with equal group representation and near-linear time constant factor approximation algorithms for more general scenarios with different proportions of two group sizes. We complement our approximation result by showing that the problem is NP-hard for two unequal-sized groups. Given the fundamental nature of this problem, we believe our results on Closest Fair Clustering could have broader implications for other clustering problems, particularly those for which no prior approximation guarantees exist for their fair variants.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Reward Over-optimization in Direct Alignment Algorithms with Importance Sampling</title>
<link>https://arxiv.org/abs/2506.08681</link>
<guid>https://arxiv.org/abs/2506.08681</guid>
<content:encoded><![CDATA[
arXiv:2506.08681v1 Announce Type: new 
Abstract: Direct Alignment Algorithms (DAAs) such as Direct Preference Optimization (DPO) have emerged as alternatives to the standard Reinforcement Learning from Human Feedback (RLHF) for aligning large language models (LLMs) with human values. However, these methods are more susceptible to over-optimization, in which the model drifts away from the reference policy, leading to degraded performance as training progresses. This paper proposes a novel importance-sampling approach to mitigate the over-optimization problem of offline DAAs. This approach, called (IS-DAAs), multiplies the DAA objective with an importance ratio that accounts for the reference policy distribution. IS-DAAs additionally avoid the high variance issue associated with importance sampling by clipping the importance ratio to a maximum value. Our extensive experiments demonstrate that IS-DAAs can effectively mitigate over-optimization, especially under low regularization strength, and achieve better performance than other methods designed to address this problem. Our implementations are provided publicly at this link.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variational Autoencoder-Based Approach to Latent Feature Analysis on Efficient Representation of Power Load Monitoring Data</title>
<link>https://arxiv.org/abs/2506.08698</link>
<guid>https://arxiv.org/abs/2506.08698</guid>
<content:encoded><![CDATA[
arXiv:2506.08698v1 Announce Type: new 
Abstract: With the development of smart grids, High-Dimensional and Incomplete (HDI) Power Load Monitoring (PLM) data challenges the performance of Power Load Forecasting (PLF) models. In this paper, we propose a potential characterization model VAE-LF based on Variational Autoencoder (VAE) for efficiently representing and complementing PLM missing data. VAE-LF learns a low-dimensional latent representation of the data using an Encoder-Decoder structure by splitting the HDI PLM data into vectors and feeding them sequentially into the VAE-LF model, and generates the complementary data. Experiments on the UK-DALE dataset show that VAE-LF outperforms other benchmark models in both 5% and 10% sparsity test cases, with significantly lower RMSE and MAE, and especially outperforms on low sparsity ratio data. The method provides an efficient data-completion solution for electric load management in smart grids.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking the ICE: Exploring promises and challenges of benchmarks for Inference Carbon &amp; Energy estimation for LLMs</title>
<link>https://arxiv.org/abs/2506.08727</link>
<guid>https://arxiv.org/abs/2506.08727</guid>
<content:encoded><![CDATA[
arXiv:2506.08727v1 Announce Type: new 
Abstract: While Generative AI stands to be one of the fastest adopted technologies ever, studies have made evident that the usage of Large Language Models (LLMs) puts significant burden on energy grids and our environment. It may prove a hindrance to the Sustainability goals of any organization. A crucial step in any Sustainability strategy is monitoring or estimating the energy consumption of various components. While there exist multiple tools for monitoring energy consumption, there is a dearth of tools/frameworks for estimating the consumption or carbon emissions. Current drawbacks of both monitoring and estimation tools include high input data points, intrusive nature, high error margin, etc. We posit that leveraging emerging LLM benchmarks and related data points can help overcome aforementioned challenges while balancing accuracy of the emission estimations. To that extent, we discuss the challenges of current approaches and present our evolving framework, R-ICE, which estimates prompt level inference carbon emissions by leveraging existing state-of-the-art(SOTA) benchmark. This direction provides a more practical and non-intrusive way to enable emerging use-cases like dynamic LLM routing, carbon accounting, etc. Our promising validation results suggest that benchmark-based modelling holds great potential for inference emission estimation and warrants further exploration from the scientific community.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploration by Random Reward Perturbation</title>
<link>https://arxiv.org/abs/2506.08737</link>
<guid>https://arxiv.org/abs/2506.08737</guid>
<content:encoded><![CDATA[
arXiv:2506.08737v1 Announce Type: new 
Abstract: We introduce Random Reward Perturbation (RRP), a novel exploration strategy for reinforcement learning (RL). Our theoretical analyses demonstrate that adding zero-mean noise to environmental rewards effectively enhances policy diversity during training, thereby expanding the range of exploration. RRP is fully compatible with the action-perturbation-based exploration strategies, such as $\epsilon$-greedy, stochastic policies, and entropy regularization, providing additive improvements to exploration effects. It is general, lightweight, and can be integrated into existing RL algorithms with minimal implementation effort and negligible computational overhead. RRP establishes a theoretical connection between reward shaping and noise-driven exploration, highlighting their complementary potential. Experiments show that RRP significantly boosts the performance of Proximal Policy Optimization and Soft Actor-Critic, achieving higher sample efficiency and escaping local optima across various tasks, under both sparse and dense reward scenarios.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Urban Incident Prediction with Graph Neural Networks: Integrating Government Ratings and Crowdsourced Reports</title>
<link>https://arxiv.org/abs/2506.08740</link>
<guid>https://arxiv.org/abs/2506.08740</guid>
<content:encoded><![CDATA[
arXiv:2506.08740v1 Announce Type: new 
Abstract: Graph neural networks (GNNs) are widely used in urban spatiotemporal forecasting, such as predicting infrastructure problems. In this setting, government officials wish to know in which neighborhoods incidents like potholes or rodent issues occur. The true state of incidents (e.g., street conditions) for each neighborhood is observed via government inspection ratings. However, these ratings are only conducted for a sparse set of neighborhoods and incident types. We also observe the state of incidents via crowdsourced reports, which are more densely observed but may be biased due to heterogeneous reporting behavior. First, for such settings, we propose a multiview, multioutput GNN-based model that uses both unbiased rating data and biased reporting data to predict the true latent state of incidents. Second, we investigate a case study of New York City urban incidents and collect, standardize, and make publicly available a dataset of 9,615,863 crowdsourced reports and 1,041,415 government inspection ratings over 3 years and across 139 types of incidents. Finally, we show on both real and semi-synthetic data that our model can better predict the latent state compared to models that use only reporting data or models that use only rating data, especially when rating data is sparse and reports are predictive of ratings. We also quantify demographic biases in crowdsourced reporting, e.g., higher-income neighborhoods report problems at higher rates. Our analysis showcases a widely applicable approach for latent state prediction using heterogeneous, sparse, and biased data.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Stability of the Jacobian Matrix in Deep Neural Networks</title>
<link>https://arxiv.org/abs/2506.08764</link>
<guid>https://arxiv.org/abs/2506.08764</guid>
<content:encoded><![CDATA[
arXiv:2506.08764v1 Announce Type: new 
Abstract: Deep neural networks are known to suffer from exploding or vanishing gradients as depth increases, a phenomenon closely tied to the spectral behavior of the input-output Jacobian. Prior work has identified critical initialization schemes that ensure Jacobian stability, but these analyses are typically restricted to fully connected networks with i.i.d. weights. In this work, we go significantly beyond these limitations: we establish a general stability theorem for deep neural networks that accommodates sparsity (such as that introduced by pruning) and non-i.i.d., weakly correlated weights (e.g. induced by training). Our results rely on recent advances in random matrix theory, and provide rigorous guarantees for spectral stability in a much broader class of network models. This extends the theoretical foundation for initialization schemes in modern neural networks with structured and dependent randomness.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Design Patterns for Securing LLM Agents against Prompt Injections</title>
<link>https://arxiv.org/abs/2506.08837</link>
<guid>https://arxiv.org/abs/2506.08837</guid>
<content:encoded><![CDATA[
arXiv:2506.08837v1 Announce Type: new 
Abstract: As AI agents powered by Large Language Models (LLMs) become increasingly versatile and capable of addressing a broad spectrum of tasks, ensuring their security has become a critical challenge. Among the most pressing threats are prompt injection attacks, which exploit the agent's resilience on natural language inputs -- an especially dangerous threat when agents are granted tool access or handle sensitive information. In this work, we propose a set of principled design patterns for building AI agents with provable resistance to prompt injection. We systematically analyze these patterns, discuss their trade-offs in terms of utility and security, and illustrate their real-world applicability through a series of case studies.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IMAGIC-500: IMputation benchmark on A Generative Imaginary Country (500k samples)</title>
<link>https://arxiv.org/abs/2506.08844</link>
<guid>https://arxiv.org/abs/2506.08844</guid>
<content:encoded><![CDATA[
arXiv:2506.08844v1 Announce Type: new 
Abstract: Missing data imputation in tabular datasets remains a pivotal challenge in data science and machine learning, particularly within socioeconomic research. However, real-world socioeconomic datasets are typically subject to strict data protection protocols, which often prohibit public sharing, even for synthetic derivatives. This severely limits the reproducibility and accessibility of benchmark studies in such settings. Further, there are very few publicly available synthetic datasets. Thus, there is limited availability of benchmarks for systematic evaluation of imputation methods on socioeconomic datasets, whether real or synthetic. In this study, we utilize the World Bank's publicly available synthetic dataset, Synthetic Data for an Imaginary Country, which closely mimics a real World Bank household survey while being fully public, enabling broad access for methodological research. With this as a starting point, we derived the IMAGIC-500 dataset: we select a subset of 500k individuals across approximately 100k households with 19 socioeconomic features, designed to reflect the hierarchical structure of real-world household surveys. This paper introduces a comprehensive missing data imputation benchmark on IMAGIC-500 under various missing mechanisms (MCAR, MAR, MNAR) and missingness ratios (10\%, 20\%, 30\%, 40\%, 50\%). Our evaluation considers the imputation accuracy for continuous and categorical variables, computational efficiency, and impact on downstream predictive tasks, such as estimating educational attainment at the individual level. The results highlight the strengths and weaknesses of statistical, traditional machine learning, and deep learning imputation techniques, including recent diffusion-based methods. The IMAGIC-500 dataset and benchmark aim to facilitate the development of robust imputation algorithms and foster reproducible social science research.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agile Reinforcement Learning for Real-Time Task Scheduling in Edge Computing</title>
<link>https://arxiv.org/abs/2506.08850</link>
<guid>https://arxiv.org/abs/2506.08850</guid>
<content:encoded><![CDATA[
arXiv:2506.08850v1 Announce Type: new 
Abstract: Soft real-time applications are becoming increasingly complex, posing significant challenges for scheduling offloaded tasks in edge computing environments while meeting task timing constraints. Moreover, the exponential growth of the search space, presence of multiple objectives and parameters, and highly dynamic nature of edge computing environments further exacerbate the complexity of task scheduling. As a result, schedulers based on heuristic and metaheuristic algorithms frequently encounter difficulties in generating optimal or near-optimal task schedules due to their constrained ability to adapt to the dynamic conditions and complex environmental characteristics of edge computing. Accordingly, reinforcement learning algorithms have been incorporated into schedulers to address the complexity and dynamic conditions inherent in task scheduling in edge computing. However, a significant limitation of reinforcement learning algorithms is the prolonged learning time required to adapt to new environments and to address medium- and large-scale problems. This challenge arises from the extensive global action space and frequent random exploration of irrelevant actions. Therefore, this study proposes Agile Reinforcement learning (aRL), in which the RL-agent performs informed exploration and executes only relevant actions. Consequently, the predictability of the RL-agent is enhanced, leading to rapid adaptation and convergence, which positions aRL as a suitable candidate for scheduling the tasks of soft real-time applications in edge computing. The experiments demonstrate that the combination of informed exploration and action-masking methods enables aRL to achieve a higher hit-ratio and converge faster than the baseline approaches.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapting to Heterophilic Graph Data with Structure-Guided Neighbor Discovery</title>
<link>https://arxiv.org/abs/2506.08871</link>
<guid>https://arxiv.org/abs/2506.08871</guid>
<content:encoded><![CDATA[
arXiv:2506.08871v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) often struggle with heterophilic data, where connected nodes may have dissimilar labels, as they typically assume homophily and rely on local message passing. To address this, we propose creating alternative graph structures by linking nodes with similar structural attributes (e.g., role-based or global), thereby fostering higher label homophily on these new graphs. We theoretically prove that GNN performance can be improved by utilizing graphs with fewer false positive edges (connections between nodes of different classes) and that considering multiple graph views increases the likelihood of finding such beneficial structures. Building on these insights, we introduce Structure-Guided GNN (SG-GNN), an architecture that processes the original graph alongside the newly created structural graphs, adaptively learning to weigh their contributions. Extensive experiments on various benchmark datasets, particularly those with heterophilic characteristics, demonstrate that our SG-GNN achieves state-of-the-art or highly competitive performance, highlighting the efficacy of exploiting structural information to guide GNNs.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Filling in the Blanks: Applying Data Imputation in incomplete Water Metering Data</title>
<link>https://arxiv.org/abs/2506.08882</link>
<guid>https://arxiv.org/abs/2506.08882</guid>
<content:encoded><![CDATA[
arXiv:2506.08882v1 Announce Type: new 
Abstract: In this work, we explore the application of recent data imputation techniques to enhance monitoring and management of water distribution networks using smart water meters, based on data derived from a real-world IoT water grid monitoring deployment. Despite the detailed data produced by such meters, data gaps due to technical issues can significantly impact operational decisions and efficiency. Our results, by comparing various imputation methods, such as k-Nearest Neighbors, MissForest, Transformers, and Recurrent Neural Networks, indicate that effective data imputation can substantially enhance the quality of the insights derived from water consumption data as we study their effect on accuracy and reliability of water metering data to provide solutions in applications like leak detection and predictive maintenance scheduling.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfoDPCCA: Information-Theoretic Dynamic Probabilistic Canonical Correlation Analysis</title>
<link>https://arxiv.org/abs/2506.08884</link>
<guid>https://arxiv.org/abs/2506.08884</guid>
<content:encoded><![CDATA[
arXiv:2506.08884v1 Announce Type: new 
Abstract: Extracting meaningful latent representations from high-dimensional sequential data is a crucial challenge in machine learning, with applications spanning natural science and engineering. We introduce InfoDPCCA, a dynamic probabilistic Canonical Correlation Analysis (CCA) framework designed to model two interdependent sequences of observations. InfoDPCCA leverages a novel information-theoretic objective to extract a shared latent representation that captures the mutual structure between the data streams and balances representation compression and predictive sufficiency while also learning separate latent components that encode information specific to each sequence. Unlike prior dynamic CCA models, such as DPCCA, our approach explicitly enforces the shared latent space to encode only the mutual information between the sequences, improving interpretability and robustness. We further introduce a two-step training scheme to bridge the gap between information-theoretic representation learning and generative modeling, along with a residual connection mechanism to enhance training stability. Through experiments on synthetic and medical fMRI data, we demonstrate that InfoDPCCA excels as a tool for representation learning. Code of InfoDPCCA is available at https://github.com/marcusstang/InfoDPCCA.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SeerAttention-R: Sparse Attention Adaptation for Long Reasoning</title>
<link>https://arxiv.org/abs/2506.08889</link>
<guid>https://arxiv.org/abs/2506.08889</guid>
<content:encoded><![CDATA[
arXiv:2506.08889v1 Announce Type: new 
Abstract: We introduce SeerAttention-R, a sparse attention framework specifically tailored for the long decoding of reasoning models. Extended from SeerAttention, SeerAttention-R retains the design of learning attention sparsity through a self-distilled gating mechanism, while removing query pooling to accommodate auto-regressive decoding. With a lightweight plug-in gating, SeerAttention-R is flexible and can be easily integrated into existing pretrained model without modifying the original parameters. We demonstrate that SeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning accuracy with 4K token budget in AIME benchmark under large sparse attention block sizes (64/128). Using TileLang, we develop a highly optimized sparse decoding kernel that achieves near-theoretical speedups of up to 9x over FlashAttention-3 on H100 GPU at 90% sparsity. Code is available at: https://github.com/microsoft/SeerAttention.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intention-Conditioned Flow Occupancy Models</title>
<link>https://arxiv.org/abs/2506.08902</link>
<guid>https://arxiv.org/abs/2506.08902</guid>
<content:encoded><![CDATA[
arXiv:2506.08902v1 Announce Type: new 
Abstract: Large-scale pre-training has fundamentally changed how machine learning research is done today: large foundation models are trained once, and then can be used by anyone in the community (including those without data or compute resources to train a model from scratch) to adapt and fine-tune to specific tasks. Applying this same framework to reinforcement learning (RL) is appealing because it offers compelling avenues for addressing core challenges in RL, including sample efficiency and robustness. However, there remains a fundamental challenge to pre-train large models in the context of RL: actions have long-term dependencies, so training a foundation model that reasons across time is important. Recent advances in generative AI have provided new tools for modeling highly complex distributions. In this paper, we build a probabilistic model to predict which states an agent will visit in the temporally distant future (i.e., an occupancy measure) using flow matching. As large datasets are often constructed by many distinct users performing distinct tasks, we include in our model a latent variable capturing the user intention. This intention increases the expressivity of our model, and enables adaptation with generalized policy improvement. We call our proposed method intention-conditioned flow occupancy models (InFOM). Comparing with alternative methods for pre-training, our experiments on $36$ state-based and $4$ image-based benchmark tasks demonstrate that the proposed method achieves $1.8 \times$ median improvement in returns and increases success rates by $36\%$. Website: https://chongyi-zheng.github.io/infom Code: https://github.com/chongyi-zheng/infom
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing generalizability of model discovery across parameter space with multi-experiment equation learning (ME-EQL)</title>
<link>https://arxiv.org/abs/2506.08916</link>
<guid>https://arxiv.org/abs/2506.08916</guid>
<content:encoded><![CDATA[
arXiv:2506.08916v1 Announce Type: new 
Abstract: Agent-based modeling (ABM) is a powerful tool for understanding self-organizing biological systems, but it is computationally intensive and often not analytically tractable. Equation learning (EQL) methods can derive continuum models from ABM data, but they typically require extensive simulations for each parameter set, raising concerns about generalizability. In this work, we extend EQL to Multi-experiment equation learning (ME-EQL) by introducing two methods: one-at-a-time ME-EQL (OAT ME-EQL), which learns individual models for each parameter set and connects them via interpolation, and embedded structure ME-EQL (ES ME-EQL), which builds a unified model library across parameters. We demonstrate these methods using a birth--death mean-field model and an on-lattice agent-based model of birth, death, and migration with spatial structure. Our results show that both methods significantly reduce the relative error in recovering parameters from agent-based simulations, with OAT ME-EQL offering better generalizability across parameter space. Our findings highlight the potential of equation learning from multiple experiments to enhance the generalizability and interpretability of learned models for complex biological systems.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Local MDI+: Local Feature Importances for Tree-Based Models</title>
<link>https://arxiv.org/abs/2506.08928</link>
<guid>https://arxiv.org/abs/2506.08928</guid>
<content:encoded><![CDATA[
arXiv:2506.08928v1 Announce Type: new 
Abstract: Tree-based ensembles such as random forests remain the go-to for tabular data over deep learning models due to their prediction performance and computational efficiency. These advantages have led to their widespread deployment in high-stakes domains, where interpretability is essential for ensuring trustworthy predictions. This has motivated the development of popular local (i.e. sample-specific) feature importance (LFI) methods such as LIME and TreeSHAP. However, these approaches rely on approximations that ignore the model's internal structure and instead depend on potentially unstable perturbations. These issues are addressed in the global setting by MDI+, a feature importance method which exploits an equivalence between decision trees and linear models on a transformed node basis. However, the global MDI+ scores are not able to explain predictions when faced with heterogeneous individual characteristics. To address this gap, we propose Local MDI+ (LMDI+), a novel extension of the MDI+ framework to the sample specific setting. LMDI+ outperforms existing baselines LIME and TreeSHAP in identifying instance-specific signal features, averaging a 10% improvement in downstream task performance across twelve real-world benchmark datasets. It further demonstrates greater stability by consistently producing similar instance-level feature importance rankings across multiple random forest fits. Finally, LMDI+ enables local interpretability use cases, including the identification of closer counterfactuals and the discovery of homogeneous subgroups.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BioLangFusion: Multimodal Fusion of DNA, mRNA, and Protein Language Models</title>
<link>https://arxiv.org/abs/2506.08936</link>
<guid>https://arxiv.org/abs/2506.08936</guid>
<content:encoded><![CDATA[
arXiv:2506.08936v1 Announce Type: new 
Abstract: We present BioLangFusion, a simple approach for integrating pre-trained DNA, mRNA, and protein language models into unified molecular representations. Motivated by the central dogma of molecular biology (information flow from gene to transcript to protein), we align per-modality embeddings at the biologically meaningful codon level (three nucleotides encoding one amino acid) to ensure direct cross-modal correspondence. BioLangFusion studies three standard fusion techniques: (i) codon-level embedding concatenation, (ii) entropy-regularized attention pooling inspired by multiple-instance learning, and (iii) cross-modal multi-head attention -- each technique providing a different inductive bias for combining modality-specific signals. These methods require no additional pre-training or modification of the base models, allowing straightforward integration with existing sequence-based foundation models. Across five molecular property prediction tasks, BioLangFusion outperforms strong unimodal baselines, showing that even simple fusion of pre-trained models can capture complementary multi-omic information with minimal overhead.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KARMA: A Multilevel Decomposition Hybrid Mamba Framework for Multivariate Long-Term Time Series Forecasting</title>
<link>https://arxiv.org/abs/2506.08939</link>
<guid>https://arxiv.org/abs/2506.08939</guid>
<content:encoded><![CDATA[
arXiv:2506.08939v1 Announce Type: new 
Abstract: Multivariate long-term and efficient time series forecasting is a key requirement for a variety of practical applications, and there are complex interleaving time dynamics in time series data that require decomposition modeling. Traditional time series decomposition methods are single and rely on fixed rules, which are insufficient for mining the potential information of the series and adapting to the dynamic characteristics of complex series. On the other hand, the Transformer-based models for time series forecasting struggle to effectively model long sequences and intricate dynamic relationships due to their high computational complexity. To overcome these limitations, we introduce KARMA, with an Adaptive Time Channel Decomposition module (ATCD) to dynamically extract trend and seasonal components. It further integrates a Hybrid Frequency-Time Decomposition module (HFTD) to further decompose Series into frequency-domain and time-domain. These components are coupled with multi-scale Mamba-based KarmaBlock to efficiently process global and local information in a coordinated manner. Experiments on eight real-world datasets from diverse domains well demonstrated that KARMA significantly outperforms mainstream baseline methods in both predictive accuracy and computational efficiency. Code and full results are available at this repository: https://github.com/yedadasd/KARMA
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Robust Deep Reinforcement Learning against Environmental State Perturbation</title>
<link>https://arxiv.org/abs/2506.08961</link>
<guid>https://arxiv.org/abs/2506.08961</guid>
<content:encoded><![CDATA[
arXiv:2506.08961v1 Announce Type: new 
Abstract: Adversarial attacks and robustness in Deep Reinforcement Learning (DRL) have been widely studied in various threat models; however, few consider environmental state perturbations, which are natural in embodied scenarios. To improve the robustness of DRL agents, we formulate the problem of environmental state perturbation, introducing a preliminary non-targeted attack method as a calibration adversary, and then propose a defense framework, named Boosted Adversarial Training (BAT), which first tunes the agents via supervised learning to avoid catastrophic failure and subsequently adversarially trains the agent with reinforcement learning. Extensive experimental results substantiate the vulnerability of mainstream agents under environmental state perturbations and the effectiveness of our proposed attack. The defense results demonstrate that while existing robust reinforcement learning algorithms may not be suitable, our BAT framework can significantly enhance the robustness of agents against environmental state perturbations across various situations.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GFRIEND: Generative Few-shot Reward Inference through EfficieNt DPO</title>
<link>https://arxiv.org/abs/2506.08965</link>
<guid>https://arxiv.org/abs/2506.08965</guid>
<content:encoded><![CDATA[
arXiv:2506.08965v1 Announce Type: new 
Abstract: The ability to train high-performing reward models with few-shot data is critical for enhancing the efficiency and scalability of Reinforcement Learning from Human Feedback (RLHF). We propose a data augmentation and expansion framework that enables generative reward models trained on small datasets to achieve comparable performance to those trained on large-scale datasets. Traditional methods to train a generative reward model, such as Direct Preference Optimization (DPO), are constrained by inefficiencies in sample pairing and limited data diversity. This work introduces preference refinement, which employs Chain-of-Thought (CoT) sampling to uncover diverse and high-quality preference relationships. It also incorporates a perplexity-based scoring mechanism to assign nuanced preference levels and utilizes Multi-level Direct Preference Optimization (M-DPO) to enable the model to capture finer-grained preference differences between samples. Experimental results demonstrate that the proposed method significantly enhances data efficiency and model performance, enabling reward models trained in a few-shot setting to achieve results on par with those trained on large-scale datasets. This study underscores the potential of data-efficient strategies in advancing reward model optimization, offering a robust solution for low-resource RLHF applications.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tailored Architectures for Time Series Forecasting: Evaluating Deep Learning Models on Gaussian Process-Generated Data</title>
<link>https://arxiv.org/abs/2506.08977</link>
<guid>https://arxiv.org/abs/2506.08977</guid>
<content:encoded><![CDATA[
arXiv:2506.08977v1 Announce Type: new 
Abstract: Developments in Deep Learning have significantly improved time series forecasting by enabling more accurate modeling of complex temporal dependencies inherent in sequential data. The effectiveness of such models is often demonstrated on limited sets of specific real-world data. Although this allows for comparative analysis, it still does not demonstrate how specific data characteristics align with the architectural strengths of individual models. Our research aims at uncovering clear connections between time series characteristics and particular models. We introduce a novel dataset generated using Gaussian Processes, specifically designed to display distinct, known characteristics for targeted evaluations of model adaptability to them. Furthermore, we present TimeFlex, a new model that incorporates a modular architecture tailored to handle diverse temporal dynamics, including trends and periodic patterns. This model is compared to current state-of-the-art models, offering a deeper understanding of how models perform under varied time series conditions.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Propositional Logic for Probing Generalization in Neural Networks</title>
<link>https://arxiv.org/abs/2506.08978</link>
<guid>https://arxiv.org/abs/2506.08978</guid>
<content:encoded><![CDATA[
arXiv:2506.08978v1 Announce Type: new 
Abstract: The extent to which neural networks are able to acquire and represent symbolic rules remains a key topic of research and debate. Much current work focuses on the impressive capabilities of large language models, as well as their often ill-understood failures on a wide range of reasoning tasks. In this paper, in contrast, we investigate the generalization behavior of three key neural architectures (Transformers, Graph Convolution Networks and LSTMs) in a controlled task rooted in propositional logic. The task requires models to generate satisfying assignments for logical formulas, making it a structured and interpretable setting for studying compositionality. We introduce a balanced extension of an existing dataset to eliminate superficial patterns and enable testing on unseen operator combinations. Using this dataset, we evaluate the ability of the three architectures to generalize beyond the training distribution. While all models perform well in-distribution, we find that generalization to unseen patterns, particularly those involving negation, remains a significant challenge. Transformers fail to apply negation compositionally, unless structural biases are introduced. Our findings highlight persistent limitations in the ability of standard architectures to learn systematic representations of logical operators, suggesting the need for stronger inductive biases to support robust rule-based reasoning.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Finetuning Tabular Foundation Models</title>
<link>https://arxiv.org/abs/2506.08982</link>
<guid>https://arxiv.org/abs/2506.08982</guid>
<content:encoded><![CDATA[
arXiv:2506.08982v1 Announce Type: new 
Abstract: Foundation models are an emerging research direction in tabular deep learning. Notably, TabPFNv2 recently claimed superior performance over traditional GBDT-based methods on small-scale datasets using an in-context learning paradigm, which does not adapt model parameters to target datasets. However, the optimal finetuning approach for adapting tabular foundational models, and how this adaptation reshapes their internal mechanisms, remains underexplored. While prior works studied finetuning for earlier foundational models, inconsistent findings and TabPFNv2's unique architecture necessitate fresh investigation. To address these questions, we first systematically evaluate various finetuning strategies on diverse datasets. Our findings establish full finetuning as the most practical solution for TabPFNv2 in terms of time-efficiency and effectiveness. We then investigate how finetuning alters TabPFNv2's inner mechanisms, drawing an analogy to retrieval-augmented models. We reveal that the success of finetuning stems from the fact that after gradient-based adaptation, the dot products of the query-representations of test objects and the key-representations of in-context training objects more accurately reflect their target similarity. This improved similarity allows finetuned TabPFNv2 to better approximate target dependency by appropriately weighting relevant in-context samples, improving the retrieval-based prediction logic. From the practical perspective, we managed to finetune TabPFNv2 on datasets with up to 50K objects, observing performance improvements on almost all tasks. More precisely, on academic datasets with I.I.D. splits, finetuning allows TabPFNv2 to achieve state-of-the-art results, while on datasets with gradual temporal shifts and rich feature sets, TabPFNv2 is less stable and prior methods remain better.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning</title>
<link>https://arxiv.org/abs/2506.08989</link>
<guid>https://arxiv.org/abs/2506.08989</guid>
<content:encoded><![CDATA[
arXiv:2506.08989v1 Announce Type: new 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective for training large language models (LLMs) on complex reasoning tasks, such as mathematical problem solving. A prerequisite for the scalability of RLVR is a high-quality problem set with precise and verifiable answers. However, the scarcity of well-crafted human-labeled math problems and limited-verification answers in existing distillation-oriented synthetic datasets limit their effectiveness in RL. Additionally, most problem synthesis strategies indiscriminately expand the problem set without considering the model's capabilities, leading to low efficiency in generating useful questions. To mitigate this issue, we introduce a Self-aware Weakness-driven problem Synthesis framework (SwS) that systematically identifies model deficiencies and leverages them for problem augmentation. Specifically, we define weaknesses as questions that the model consistently fails to learn through its iterative sampling during RL training. We then extract the core concepts from these failure cases and synthesize new problems to strengthen the model's weak areas in subsequent augmented training, enabling it to focus on and gradually overcome its weaknesses. Without relying on external knowledge distillation, our framework enables robust generalization byempowering the model to self-identify and address its weaknesses in RL, yielding average performance gains of 10.0% and 7.7% on 7B and 32B models across eight mainstream reasoning benchmarks.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Branched Schr\"odinger Bridge Matching</title>
<link>https://arxiv.org/abs/2506.09007</link>
<guid>https://arxiv.org/abs/2506.09007</guid>
<content:encoded><![CDATA[
arXiv:2506.09007v1 Announce Type: new 
Abstract: Predicting the intermediate trajectories between an initial and target distribution is a central problem in generative modeling. Existing approaches, such as flow matching and Schr\"odinger Bridge Matching, effectively learn mappings between two distributions by modeling a single stochastic path. However, these methods are inherently limited to unimodal transitions and cannot capture branched or divergent evolution from a common origin to multiple distinct outcomes. To address this, we introduce Branched Schr\"odinger Bridge Matching (BranchSBM), a novel framework that learns branched Schr\"odinger bridges. BranchSBM parameterizes multiple time-dependent velocity fields and growth processes, enabling the representation of population-level divergence into multiple terminal distributions. We show that BranchSBM is not only more expressive but also essential for tasks involving multi-path surface navigation, modeling cell fate bifurcations from homogeneous progenitor states, and simulating diverging cellular responses to perturbations.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effective Data Pruning through Score Extrapolation</title>
<link>https://arxiv.org/abs/2506.09010</link>
<guid>https://arxiv.org/abs/2506.09010</guid>
<content:encoded><![CDATA[
arXiv:2506.09010v1 Announce Type: new 
Abstract: Training advanced machine learning models demands massive datasets, resulting in prohibitive computational costs. To address this challenge, data pruning techniques identify and remove redundant training samples while preserving model performance. Yet, existing pruning techniques predominantly require a full initial training pass to identify removable samples, negating any efficiency benefits for single training runs. To overcome this limitation, we introduce a novel importance score extrapolation framework that requires training on only a small subset of data. We present two initial approaches in this framework - k-nearest neighbors and graph neural networks - to accurately predict sample importance for the entire dataset using patterns learned from this minimal subset. We demonstrate the effectiveness of our approach for 2 state-of-the-art pruning methods (Dynamic Uncertainty and TDDS), 4 different datasets (CIFAR-10, CIFAR-100, Places-365, and ImageNet), and 3 training paradigms (supervised, unsupervised, and adversarial). Our results indicate that score extrapolation is a promising direction to scale expensive score calculation methods, such as pruning, data attribution, or other tasks.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPEED-RL: Faster Training of Reasoning Models via Online Curriculum Learning</title>
<link>https://arxiv.org/abs/2506.09016</link>
<guid>https://arxiv.org/abs/2506.09016</guid>
<content:encoded><![CDATA[
arXiv:2506.09016v1 Announce Type: new 
Abstract: Training large language models with reinforcement learning (RL) against verifiable rewards significantly enhances their reasoning abilities, yet remains computationally expensive due to inefficient uniform prompt sampling. We introduce Selective Prompting with Efficient Estimation of Difficulty (SPEED), an adaptive online RL curriculum that selectively chooses training examples of intermediate difficulty to maximize learning efficiency. Theoretically, we establish that intermediate-difficulty prompts improve the gradient estimator's signal-to-noise ratio, accelerating convergence. Empirically, our efficient implementation leads to 2x to 6x faster training without degrading accuracy, requires no manual tuning, and integrates seamlessly into standard RL algorithms.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Edit Flows: Flow Matching with Edit Operations</title>
<link>https://arxiv.org/abs/2506.09018</link>
<guid>https://arxiv.org/abs/2506.09018</guid>
<content:encoded><![CDATA[
arXiv:2506.09018v1 Announce Type: new 
Abstract: Autoregressive generative models naturally generate variable-length sequences, while non-autoregressive models struggle, often imposing rigid, token-wise structures. We propose Edit Flows, a non-autoregressive model that overcomes these limitations by defining a discrete flow over sequences through edit operations-insertions, deletions, and substitutions. By modeling these operations within a Continuous-time Markov Chain over the sequence space, Edit Flows enable flexible, position-relative generation that aligns more closely with the structure of sequence data. Our training method leverages an expanded state space with auxiliary variables, making the learning process efficient and tractable. Empirical results show that Edit Flows outperforms both autoregressive and mask models on image captioning and significantly outperforms the mask construction in text and code generation.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>e3: Learning to Explore Enables Extrapolation of Test-Time Compute for LLMs</title>
<link>https://arxiv.org/abs/2506.09026</link>
<guid>https://arxiv.org/abs/2506.09026</guid>
<content:encoded><![CDATA[
arXiv:2506.09026v1 Announce Type: new 
Abstract: Test-time scaling offers a promising path to improve LLM reasoning by utilizing more compute at inference time; however, the true promise of this paradigm lies in extrapolation (i.e., improvement in performance on hard problems as LLMs keep "thinking" for longer, beyond the maximum token budget they were trained on). Surprisingly, we find that most existing reasoning models do not extrapolate well. We show that one way to enable extrapolation is by training the LLM to perform in-context exploration: training the LLM to effectively spend its test time budget by chaining operations (such as generation, verification, refinement, etc.), or testing multiple hypotheses before it commits to an answer. To enable in-context exploration, we identify three key ingredients as part of our recipe e3: (1) chaining skills that the base LLM has asymmetric competence in, e.g., chaining verification (easy) with generation (hard), as a way to implement in-context search; (2) leveraging "negative" gradients from incorrect traces to amplify exploration during RL, resulting in longer search traces that chains additional asymmetries; and (3) coupling task difficulty with training token budget during training via a specifically-designed curriculum to structure in-context exploration. Our recipe e3 produces the best known 1.7B model according to AIME'25 and HMMT'25 scores, and extrapolates to 2x the training token budget. Our e3-1.7B model not only attains high pass@1 scores, but also improves pass@k over the base model.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FZOO: Fast Zeroth-Order Optimizer for Fine-Tuning Large Language Models towards Adam-Scale Speed</title>
<link>https://arxiv.org/abs/2506.09034</link>
<guid>https://arxiv.org/abs/2506.09034</guid>
<content:encoded><![CDATA[
arXiv:2506.09034v1 Announce Type: new 
Abstract: Fine-tuning large language models (LLMs) often faces GPU memory bottlenecks: the backward pass of first-order optimizers like Adam increases memory usage to more than 10 times the inference level (e.g., 633 GB for OPT-30B). Zeroth-order (ZO) optimizers avoid this cost by estimating gradients only from forward passes, yet existing methods like MeZO usually require many more steps to converge. Can this trade-off between speed and memory in ZO be fundamentally improved? Normalized-SGD demonstrates strong empirical performance with greater memory efficiency than Adam. In light of this, we introduce FZOO, a Fast Zeroth-Order Optimizer toward Adam-Scale Speed. FZOO reduces the total forward passes needed for convergence by employing batched one-sided estimates that adapt step sizes based on the standard deviation of batch losses. It also accelerates per-batch computation through the use of Rademacher random vector perturbations coupled with CUDA's parallel processing. Extensive experiments on diverse models, including RoBERTa-large, OPT (350M-66B), Phi-2, and Llama3, across 11 tasks validate FZOO's effectiveness. On average, FZOO outperforms MeZO by 3 percent in accuracy while requiring 3 times fewer forward passes. For RoBERTa-large, FZOO achieves average improvements of 5.6 percent in accuracy and an 18 times reduction in forward passes compared to MeZO, achieving convergence speeds comparable to Adam. We also provide theoretical analysis proving FZOO's formal equivalence to a normalized-SGD update rule and its convergence guarantees. FZOO integrates smoothly into PEFT techniques, enabling even larger memory savings. Overall, our results make single-GPU, high-speed, full-parameter fine-tuning practical and point toward future work on memory-efficient pre-training.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Decoupled Risk Landscape in Performative Prediction</title>
<link>https://arxiv.org/abs/2506.09044</link>
<guid>https://arxiv.org/abs/2506.09044</guid>
<content:encoded><![CDATA[
arXiv:2506.09044v1 Announce Type: new 
Abstract: Performative Prediction addresses scenarios where deploying a model induces a distribution shift in the input data, such as individuals modifying their features and reapplying for a bank loan after rejection. Literature has had a theoretical perspective giving mathematical guarantees for convergence (either to the stable or optimal point). We believe that visualization of the loss landscape can complement this theoretical advances with practical insights. Therefore, (1) we introduce a simple decoupled risk visualization method inspired in the two-step process that performative prediction is. Our approach visualizes the risk landscape with respect to two parameter vectors: model parameters and data parameters. We use this method to propose new properties of the interest points, to examine how existing algorithms traverse the risk landscape and perform under more realistic conditions, including strategic classification with non-linear models. (2) Building on this decoupled risk visualization, we introduce a novel setting - extended Performative Prediction - which captures scenarios where the distribution reacts to a model different from the decision-making one, reflecting the reality that agents often lack full access to the deployed model.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Neural Networks: Self-Evolving Multi-Agent Systems via Textual Backpropagation</title>
<link>https://arxiv.org/abs/2506.09046</link>
<guid>https://arxiv.org/abs/2506.09046</guid>
<content:encoded><![CDATA[
arXiv:2506.09046v1 Announce Type: new 
Abstract: Leveraging multiple Large Language Models(LLMs) has proven effective for addressing complex, high-dimensional tasks, but current approaches often rely on static, manually engineered multi-agent configurations. To overcome these constraints, we present the Agentic Neural Network(ANN), a framework that conceptualizes multi-agent collaboration as a layered neural network architecture. In this design, each agent operates as a node, and each layer forms a cooperative "team" focused on a specific subtask. Agentic Neural Network follows a two-phase optimization strategy: (1) Forward Phase-Drawing inspiration from neural network forward passes, tasks are dynamically decomposed into subtasks, and cooperative agent teams with suitable aggregation methods are constructed layer by layer. (2) Backward Phase-Mirroring backpropagation, we refine both global and local collaboration through iterative feedback, allowing agents to self-evolve their roles, prompts, and coordination. This neuro-symbolic approach enables ANN to create new or specialized agent teams post-training, delivering notable gains in accuracy and adaptability. Across four benchmark datasets, ANN surpasses leading multi-agent baselines under the same configurations, showing consistent performance improvements. Our findings indicate that ANN provides a scalable, data-driven framework for multi-agent systems, combining the collaborative capabilities of LLMs with the efficiency and flexibility of neural network principles. We plan to open-source the entire framework.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Task Vectors in In-Context Learning: Emergence, Functionality, and Limitations</title>
<link>https://arxiv.org/abs/2506.09048</link>
<guid>https://arxiv.org/abs/2506.09048</guid>
<content:encoded><![CDATA[
arXiv:2506.09048v1 Announce Type: new 
Abstract: Task vectors offer a compelling mechanism for accelerating inference in in-context learning (ICL) by distilling task-specific information into a single, reusable representation. Despite their empirical success, the underlying principles governing their emergence and functionality remain unclear. This work proposes the Linear Combination Conjecture, positing that task vectors act as single in-context demonstrations formed through linear combinations of the original ones. We provide both theoretical and empirical support for this conjecture. First, we show that task vectors naturally emerge in linear transformers trained on triplet-formatted prompts through loss landscape analysis. Next, we predict the failure of task vectors on representing high-rank mappings and confirm this on practical LLMs. Our findings are further validated through saliency analyses and parameter visualization, suggesting an enhancement of task vectors by injecting multiple ones into few-shot prompts. Together, our results advance the understanding of task vectors and shed light on the mechanisms underlying ICL in transformer-based models.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Werewolf: A Straightforward Game Framework with TTS for Improved User Engagement</title>
<link>https://arxiv.org/abs/2506.00160</link>
<guid>https://arxiv.org/abs/2506.00160</guid>
<content:encoded><![CDATA[
arXiv:2506.00160v1 Announce Type: cross 
Abstract: The growing popularity of social deduction game systems for both business applications and AI research has greatly benefited from the rapid advancements in Large Language Models (LLMs), which now demonstrate stronger reasoning and persuasion capabilities. Especially with the raise of DeepSeek R1 and V3 models, LLMs should enable a more engaging experience for human players in LLM-agent-based social deduction games like Werewolf. Previous works either fine-tuning, advanced prompting engineering, or additional experience pool to achieve engaging text-format Werewolf game experience. We propose a novel yet straightforward LLM-based Werewolf game system with tuned Text-to-Speech(TTS) models designed for enhanced compatibility with various LLM models, and improved user engagement. We argue with ever enhancing LLM reasoning, extra components will be unnecessary in the case of Werewolf.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Proteins and Language: A Foundation Model for Protein Retrieval</title>
<link>https://arxiv.org/abs/2506.08023</link>
<guid>https://arxiv.org/abs/2506.08023</guid>
<content:encoded><![CDATA[
arXiv:2506.08023v1 Announce Type: cross 
Abstract: This paper aims to retrieve proteins with similar structures and semantics from large-scale protein dataset, facilitating the functional interpretation of protein structures derived by structural determination methods like cryo-Electron Microscopy (cryo-EM). Motivated by the recent progress of vision-language models (VLMs), we propose a CLIP-style framework for aligning 3D protein structures with functional annotations using contrastive learning. For model training, we propose a large-scale dataset of approximately 200,000 protein-caption pairs with rich functional descriptors. We evaluate our model in both in-domain and more challenging cross-database retrieval on Protein Data Bank (PDB) and Electron Microscopy Data Bank (EMDB) dataset, respectively. In both cases, our approach demonstrates promising zero-shot retrieval performance, highlighting the potential of multimodal foundation models for structure-function understanding in protein biology.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TIP-Search: Time-Predictable Inference Scheduling for Market Prediction under Uncertain Load</title>
<link>https://arxiv.org/abs/2506.08026</link>
<guid>https://arxiv.org/abs/2506.08026</guid>
<content:encoded><![CDATA[
arXiv:2506.08026v1 Announce Type: cross 
Abstract: This paper proposes TIP-Search, a time-predictable inference scheduling framework for real-time market prediction under uncertain workloads. Motivated by the strict latency demands in high-frequency financial systems, TIP-Search dynamically selects a deep learning model from a heterogeneous pool, aiming to maximize predictive accuracy while satisfying per-task deadline constraints. Our approach profiles latency and generalization performance offline, then performs online task-aware selection without relying on explicit input domain labels. We evaluate TIP-Search on three real-world limit order book datasets (FI-2010, Binance BTC/USDT, LOBSTER AAPL) and demonstrate that it outperforms static baselines with up to 8.5% improvement in accuracy and 100% deadline satisfaction. Our results highlight the effectiveness of TIP-Search in robust low-latency financial inference under uncertainty.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inverse Design in Distributed Circuits Using Single-Step Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.08029</link>
<guid>https://arxiv.org/abs/2506.08029</guid>
<content:encoded><![CDATA[
arXiv:2506.08029v1 Announce Type: cross 
Abstract: The goal of inverse design in distributed circuits is to generate near-optimal designs that meet a desirable transfer function specification. Existing design exploration methods use some combination of strategies involving artificial grids, differentiable evaluation procedures, and specific template topologies. However, real-world design practices often require non-differentiable evaluation procedures, varying topologies, and near-continuous placement spaces. In this paper, we propose DCIDA, a design exploration framework that learns a near-optimal design sampling policy for a target transfer function. DCIDA decides all design factors in a compound single-step action by sampling from a set of jointly-trained conditional distributions generated by the policy. Utilizing an injective interdependent ``map", DCIDA transforms raw sampled design ``actions" into uniquely equivalent physical representations, enabling the framework to learn the conditional dependencies among joint ``raw'' design decisions. Our experiments demonstrate DCIDA's Transformer-based policy network achieves significant reductions in design error compared to state-of-the-art approaches, with significantly better fit in cases involving more complex transfer functions.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOSS: Multi-Objective Optimization for Stable Rule Sets</title>
<link>https://arxiv.org/abs/2506.08030</link>
<guid>https://arxiv.org/abs/2506.08030</guid>
<content:encoded><![CDATA[
arXiv:2506.08030v1 Announce Type: cross 
Abstract: We present MOSS, a multi-objective optimization framework for constructing stable sets of decision rules. MOSS incorporates three important criteria for interpretability: sparsity, accuracy, and stability, into a single multi-objective optimization framework. Importantly, MOSS allows a practitioner to rapidly evaluate the trade-off between accuracy and stability in sparse rule sets in order to select an appropriate model. We develop a specialized cutting plane algorithm in our framework to rapidly compute the Pareto frontier between these two objectives, and our algorithm scales to problem instances beyond the capabilities of commercial optimization solvers. Our experiments show that MOSS outperforms state-of-the-art rule ensembles in terms of both predictive performance and stability.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feasibility Study of CNNs and MLPs for Radiation Heat Transfer in 2-D Furnaces with Spectrally Participative Gases</title>
<link>https://arxiv.org/abs/2506.08033</link>
<guid>https://arxiv.org/abs/2506.08033</guid>
<content:encoded><![CDATA[
arXiv:2506.08033v1 Announce Type: cross 
Abstract: Aiming to reduce the computational cost of numerical simulations, a convolutional neural network (CNN) and a multi-layer perceptron (MLP) are introduced to build a surrogate model to approximate radiative heat transfer solutions in a 2-D walled domain with participative gases. The originality of this work lays in the adaptation of the inputs of the problem (gas and wall properties) in order to fit with the CNN architecture, more commonly used for image processing. Two precision datasets have been created with the classical solver, ICARUS2D, that uses the discrete transfer radiation method with the statistical narrow bands model. The performance of the CNN architecture is compared to a more classical MLP architecture in terms of speed and accuracy. Thanks to Optuna, all results are obtained using the optimized hyper parameters networks. The results show a significant speedup with industrially acceptable relative errors compared to the classical solver for both architectures. Additionally, the CNN outperforms the MLP in terms of precision and is more robust and stable to changes in hyper-parameters. A performance analysis on the dataset size of the samples have also been carried out to gain a deeper understanding of the model behavior.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural-Augmented Kelvinlet: Real-Time Soft Tissue Deformation with Multiple Graspers</title>
<link>https://arxiv.org/abs/2506.08043</link>
<guid>https://arxiv.org/abs/2506.08043</guid>
<content:encoded><![CDATA[
arXiv:2506.08043v1 Announce Type: cross 
Abstract: Fast and accurate simulation of soft tissue deformation is a critical factor for surgical robotics and medical training. In this paper, we introduce a novel physics-informed neural simulator that approximates soft tissue deformations in a realistic and real-time manner. Our framework integrates Kelvinlet-based priors into neural simulators, making it the first approach to leverage Kelvinlets for residual learning and regularization in data-driven soft tissue modeling. By incorporating large-scale Finite Element Method (FEM) simulations of both linear and nonlinear soft tissue responses, our method improves neural network predictions across diverse architectures, enhancing accuracy and physical consistency while maintaining low latency for real-time performance. We demonstrate the effectiveness of our approach by performing accurate surgical maneuvers that simulate the use of standard laparoscopic tissue grasping tools with high fidelity. These results establish Kelvinlet-augmented learning as a powerful and efficient strategy for real-time, physics-aware soft tissue simulation in surgical applications.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of Machine Learning Models in Student Academic Performance Prediction</title>
<link>https://arxiv.org/abs/2506.08047</link>
<guid>https://arxiv.org/abs/2506.08047</guid>
<content:encoded><![CDATA[
arXiv:2506.08047v1 Announce Type: cross 
Abstract: This research investigates the use of machine learning methods to forecast students' academic performance in a school setting. Students' data with behavioral, academic, and demographic details were used in implementations with standard classical machine learning models including multi-layer perceptron classifier (MLPC). MLPC obtained 86.46% maximum accuracy for test set across all implementations. Under 10-fold cross validation, MLPC obtained 79.58% average accuracy for test set while for train set, it was 99.65%. MLP's better performance over other machine learning models strongly suggest the potential use of neural networks as data-efficient models. Feature selection approach played a crucial role in improving the performance and multiple evaluation approaches were used in order to compare with existing literature. Explainable machine learning methods were utilized to demystify the black box models and to validate the feature selection approach.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed Teleconnection-Aware Transformer for Global Subseasonal-to-Seasonal Forecasting</title>
<link>https://arxiv.org/abs/2506.08049</link>
<guid>https://arxiv.org/abs/2506.08049</guid>
<content:encoded><![CDATA[
arXiv:2506.08049v1 Announce Type: cross 
Abstract: Subseasonal-to-seasonal (S2S) forecasting, which predicts climate conditions from several weeks to months in advance, presents significant challenges due to the chaotic dynamics of atmospheric systems and complex interactions across multiple scales. Current approaches often fail to explicitly model underlying physical processes and teleconnections that are crucial at S2S timescales. We introduce TelePiT, a novel deep learning architecture that enhances global S2S forecasting through integrated multi-scale physics and teleconnection awareness. Our approach consists of three key components: (1) Spherical Harmonic Embedding, which accurately encodes global atmospheric variables onto spherical geometry; (2) Multi-Scale Physics-Informed Neural ODE, which explicitly captures atmospheric physical processes across multiple learnable frequency bands; (3) Teleconnection-Aware Transformer, which models critical global climate interactions through tactfully injecting teleconnection patterns into the self-attention. Extensive experiments demonstrate that TelePiT significantly outperforms state-of-the-art data-driven baselines and operational numerical weather prediction systems, with remarkable improvements for atmospheric variables including a 57.7% reduction in RMSE for 2-meter temperature compared to previous best models.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CaliciBoost: Performance-Driven Evaluation of Molecular Representations for Caco-2 Permeability Prediction</title>
<link>https://arxiv.org/abs/2506.08059</link>
<guid>https://arxiv.org/abs/2506.08059</guid>
<content:encoded><![CDATA[
arXiv:2506.08059v1 Announce Type: cross 
Abstract: Caco-2 permeability serves as a critical in vitro indicator for predicting the oral absorption of drug candidates during early-stage drug discovery. To enhance the accuracy and efficiency of computational predictions, we systematically investigated the impact of eight molecular feature representation types including 2D/3D descriptors, structural fingerprints, and deep learning-based embeddings combined with automated machine learning techniques to predict Caco-2 permeability. Using two datasets of differing scale and diversity (TDC benchmark and curated OCHEM data), we assessed model performance across representations and identified PaDEL, Mordred, and RDKit descriptors as particularly effective for Caco-2 prediction. Notably, the AutoML-based model CaliciBoost achieved the best MAE performance. Furthermore, for both PaDEL and Mordred representations, the incorporation of 3D descriptors resulted in a 15.73% reduction in MAE compared to using 2D features alone, as confirmed by feature importance analysis. These findings highlight the effectiveness of AutoML approaches in ADMET modeling and offer practical guidance for feature selection in data-limited prediction tasks.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Diffusion Schr\"odinger Bridge in Astrophysical Observational Inversions</title>
<link>https://arxiv.org/abs/2506.08065</link>
<guid>https://arxiv.org/abs/2506.08065</guid>
<content:encoded><![CDATA[
arXiv:2506.08065v1 Announce Type: cross 
Abstract: We study Diffusion Schr\"odinger Bridge (DSB) models in the context of dynamical astrophysical systems, specifically tackling observational inverse prediction tasks within Giant Molecular Clouds (GMCs) for star formation. We introduce the Astro-DSB model, a variant of DSB with the pairwise domain assumption tailored for astrophysical dynamics. By investigating its learning process and prediction performance in both physically simulated data and in real observations (the Taurus B213 data), we present two main takeaways. First, from the astrophysical perspective, our proposed paired DSB method improves interpretability, learning efficiency, and prediction performance over conventional astrostatistical and other machine learning methods. Second, from the generative modeling perspective, probabilistic generative modeling reveals improvements over discriminative pixel-to-pixel modeling in Out-Of-Distribution (OOD) testing cases of physical simulations with unseen initial conditions and different dominant physical processes. Our study expands research into diffusion models beyond the traditional visual synthesis application and provides evidence of the models' learning abilities beyond pure data statistics, paving a path for future physics-aware generative models which can align dynamics between machine learning and real (astro)physical systems.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WWAggr: A Window Wasserstein-based Aggregation for Ensemble Change Point Detection</title>
<link>https://arxiv.org/abs/2506.08066</link>
<guid>https://arxiv.org/abs/2506.08066</guid>
<content:encoded><![CDATA[
arXiv:2506.08066v1 Announce Type: cross 
Abstract: Change Point Detection (CPD) aims to identify moments of abrupt distribution shifts in data streams. Real-world high-dimensional CPD remains challenging due to data pattern complexity and violation of common assumptions. Resorting to standalone deep neural networks, the current state-of-the-art detectors have yet to achieve perfect quality. Concurrently, ensembling provides more robust solutions, boosting the performance. In this paper, we investigate ensembles of deep change point detectors and realize that standard prediction aggregation techniques, e.g., averaging, are suboptimal and fail to account for problem peculiarities. Alternatively, we introduce WWAggr -- a novel task-specific method of ensemble aggregation based on the Wasserstein distance. Our procedure is versatile, working effectively with various ensembles of deep CPD models. Moreover, unlike existing solutions, we practically lift a long-standing problem of the decision threshold selection for CPD.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain Switching on the Pareto Front: Multi-Objective Deep Kernel Learning in Automated Piezoresponse Force Microscopy</title>
<link>https://arxiv.org/abs/2506.08073</link>
<guid>https://arxiv.org/abs/2506.08073</guid>
<content:encoded><![CDATA[
arXiv:2506.08073v1 Announce Type: cross 
Abstract: Ferroelectric polarization switching underpins the functional performance of a wide range of materials and devices, yet its dependence on complex local microstructural features renders systematic exploration by manual or grid-based spectroscopic measurements impractical. Here, we introduce a multi-objective kernel-learning workflow that infers the microstructural rules governing switching behavior directly from high-resolution imaging data. Applied to automated piezoresponse force microscopy (PFM) experiments, our framework efficiently identifies the key relationships between domain-wall configurations and local switching kinetics, revealing how specific wall geometries and defect distributions modulate polarization reversal. Post-experiment analysis projects abstract reward functions, such as switching ease and domain symmetry, onto physically interpretable descriptors including domain configuration and proximity to boundaries. This enables not only high-throughput active learning, but also mechanistic insight into the microstructural control of switching phenomena. While demonstrated for ferroelectric domain switching, our approach provides a powerful, generalizable tool for navigating complex, non-differentiable design spaces, from structure-property correlations in molecular discovery to combinatorial optimization across diverse imaging modalities.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuous Policy and Value Iteration for Stochastic Control Problems and Its Convergence</title>
<link>https://arxiv.org/abs/2506.08121</link>
<guid>https://arxiv.org/abs/2506.08121</guid>
<content:encoded><![CDATA[
arXiv:2506.08121v1 Announce Type: cross 
Abstract: We introduce a continuous policy-value iteration algorithm where the approximations of the value function of a stochastic control problem and the optimal control are simultaneously updated through Langevin-type dynamics. This framework applies to both the entropy-regularized relaxed control problems and the classical control problems, with infinite horizon. We establish policy improvement and demonstrate convergence to the optimal control under the monotonicity condition of the Hamiltonian. By utilizing Langevin-type stochastic differential equations for continuous updates along the policy iteration direction, our approach enables the use of distribution sampling and non-convex learning techniques in machine learning to optimize the value function and identify the optimal control simultaneously.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constrained Pareto Set Identification with Bandit Feedback</title>
<link>https://arxiv.org/abs/2506.08127</link>
<guid>https://arxiv.org/abs/2506.08127</guid>
<content:encoded><![CDATA[
arXiv:2506.08127v1 Announce Type: cross 
Abstract: In this paper, we address the problem of identifying the Pareto Set under feasibility constraints in a multivariate bandit setting. Specifically, given a $K$-armed bandit with unknown means $\mu_1, \dots, \mu_K \in \mathbb{R}^d$, the goal is to identify the set of arms whose mean is not uniformly worse than that of another arm (i.e., not smaller for all objectives), while satisfying some known set of linear constraints, expressing, for example, some minimal performance on each objective. Our focus lies in fixed-confidence identification, for which we introduce an algorithm that significantly outperforms racing-like algorithms and the intuitive two-stage approach that first identifies feasible arms and then their Pareto Set. We further prove an information-theoretic lower bound on the sample complexity of any algorithm for constrained Pareto Set identification, showing that the sample complexity of our approach is near-optimal. Our theoretical results are supported by an extensive empirical evaluation on a series of benchmarks.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilingual Hate Speech Detection in Social Media Using Translation-Based Approaches with Large Language Models</title>
<link>https://arxiv.org/abs/2506.08147</link>
<guid>https://arxiv.org/abs/2506.08147</guid>
<content:encoded><![CDATA[
arXiv:2506.08147v1 Announce Type: cross 
Abstract: Social media platforms are critical spaces for public discourse, shaping opinions and community dynamics, yet their widespread use has amplified harmful content, particularly hate speech, threatening online safety and inclusivity. While hate speech detection has been extensively studied in languages like English and Spanish, Urdu remains underexplored, especially using translation-based approaches. To address this gap, we introduce a trilingual dataset of 10,193 tweets in English (3,834 samples), Urdu (3,197 samples), and Spanish (3,162 samples), collected via keyword filtering, with a balanced distribution of 4,849 Hateful and 5,344 Not-Hateful labels. Our methodology leverages attention layers as a precursor to transformer-based models and large language models (LLMs), enhancing feature extraction for multilingual hate speech detection. For non-transformer models, we use TF-IDF for feature extraction. The dataset is benchmarked using state-of-the-art models, including GPT-3.5 Turbo and Qwen 2.5 72B, alongside traditional machine learning models like SVM and other transformers (e.g., BERT, RoBERTa). Three annotators, following rigorous guidelines, ensured high dataset quality, achieving a Fleiss' Kappa of 0.821. Our approach, integrating attention layers with GPT-3.5 Turbo and Qwen 2.5 72B, achieves strong performance, with macro F1 scores of 0.87 for English (GPT-3.5 Turbo), 0.85 for Spanish (GPT-3.5 Turbo), 0.81 for Urdu (Qwen 2.5 72B), and 0.88 for the joint multilingual model (Qwen 2.5 72B). These results reflect improvements of 8.75% in English (over SVM baseline 0.80), 8.97% in Spanish (over SVM baseline 0.78), 5.19% in Urdu (over SVM baseline 0.77), and 7.32% in the joint multilingual model (over SVM baseline 0.82). Our framework offers a robust solution for multilingual hate speech detection, fostering safer digital communities worldwide.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Metrics-Oriented Architectural Model to Characterize Complexity on Machine Learning-Enabled Systems</title>
<link>https://arxiv.org/abs/2506.08153</link>
<guid>https://arxiv.org/abs/2506.08153</guid>
<content:encoded><![CDATA[
arXiv:2506.08153v1 Announce Type: cross 
Abstract: How can the complexity of ML-enabled systems be managed effectively? The goal of this research is to investigate how complexity affects ML-Enabled Systems (MLES). To address this question, this research aims to introduce a metrics-based architectural model to characterize the complexity of MLES. The goal is to support architectural decisions, providing a guideline for the inception and growth of these systems. This paper showcases the first step for creating the metrics-based architectural model: an extension of a reference architecture that can describe MLES to collect their metrics.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpreting Agent Behaviors in Reinforcement-Learning-Based Cyber-Battle Simulation Platforms</title>
<link>https://arxiv.org/abs/2506.08192</link>
<guid>https://arxiv.org/abs/2506.08192</guid>
<content:encoded><![CDATA[
arXiv:2506.08192v1 Announce Type: cross 
Abstract: We analyze two open source deep reinforcement learning agents submitted to the CAGE Challenge 2 cyber defense challenge, where each competitor submitted an agent to defend a simulated network against each of several provided rules-based attack agents. We demonstrate that one can gain interpretability of agent successes and failures by simplifying the complex state and action spaces and by tracking important events, shedding light on the fine-grained behavior of both the defense and attack agents in each experimental scenario. By analyzing important events within an evaluation episode, we identify patterns in infiltration and clearing events that tell us how well the attacker and defender played their respective roles; for example, defenders were generally able to clear infiltrations within one or two timesteps of a host being exploited. By examining transitions in the environment's state caused by the various possible actions, we determine which actions tended to be effective and which did not, showing that certain important actions are between 40% and 99% ineffective. We examine how decoy services affect exploit success, concluding for instance that decoys block up to 94% of exploits that would directly grant privileged access to a host. Finally, we discuss the realism of the challenge and ways that the CAGE Challenge 4 has addressed some of our concerns.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2506.08210</link>
<guid>https://arxiv.org/abs/2506.08210</guid>
<content:encoded><![CDATA[
arXiv:2506.08210v1 Announce Type: cross 
Abstract: Both text-to-image generation and large language models (LLMs) have made significant advancements. However, many text-to-image models still employ the somewhat outdated T5 and CLIP as their text encoders. In this work, we investigate the effectiveness of using modern decoder-only LLMs as text encoders for text-to-image diffusion models. We build a standardized training and evaluation pipeline that allows us to isolate and evaluate the effect of different text embeddings. We train a total of 27 text-to-image models with 12 different text encoders to analyze the critical aspects of LLMs that could impact text-to-image generation, including the approaches to extract embeddings, different LLMs variants, and model sizes. Our experiments reveal that the de facto way of using last-layer embeddings as conditioning leads to inferior performance. Instead, we explore embeddings from various layers and find that using layer-normalized averaging across all layers significantly improves alignment with complex prompts. Most LLMs with this conditioning outperform the baseline T5 model, showing enhanced performance in advanced visio-linguistic reasoning skills.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning-Based Multiuser Scheduling in MIMO-OFDM Systems with Hybrid Beamforming</title>
<link>https://arxiv.org/abs/2506.08263</link>
<guid>https://arxiv.org/abs/2506.08263</guid>
<content:encoded><![CDATA[
arXiv:2506.08263v1 Announce Type: cross 
Abstract: We investigate the multiuser scheduling problem in multiple-input multiple-output (MIMO) systems using orthogonal frequency division multiplexing (OFDM) and hybrid beamforming in which a base station (BS) communicates with multiple users over millimeter wave (mmWave) channels in the downlink. Improved scheduling is critical for enhancing spectral efficiency and the long-term performance of the system from the perspective of proportional fairness (PF) metric in hybrid beamforming systems due to its limited multiplexing gain. Our objective is to maximize PF by properly designing the analog and digital precoders within the hybrid beamforming and selecting the users subject to the number of radio frequency (RF) chains. Leveraging the characteristics of mmWave channels, we apply a two-timescale protocol. On a long timescale, we assign an analog beam to each user. Scheduling the users and designing the digital precoder are done accordingly on a short timescale. To conduct scheduling, we propose combinatorial solutions, such as greedy and sorting algorithms, followed by a machine learning (ML) approach. Our numerical results highlight the trade-off between the performance and complexity of the proposed approaches. Consequently, we show that the choice of approach depends on the specific criteria within a given scenario.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LEANN: A Low-Storage Vector Index</title>
<link>https://arxiv.org/abs/2506.08276</link>
<guid>https://arxiv.org/abs/2506.08276</guid>
<content:encoded><![CDATA[
arXiv:2506.08276v1 Announce Type: cross 
Abstract: Embedding-based search is widely used in applications such as recommendation and retrieval-augmented generation (RAG). Recently, there is a growing demand to support these capabilities over personal data stored locally on devices. However, maintaining the necessary data structure associated with the embedding-based search is often infeasible due to its high storage overhead. For example, indexing 100 GB of raw data requires 150 to 700 GB of storage, making local deployment impractical. Reducing this overhead while maintaining search quality and latency becomes a critical challenge. In this paper, we present LEANN, a storage-efficient approximate nearest neighbor (ANN) search index optimized for resource-constrained personal devices. LEANN combines a compact graph-based structure with an efficient on-the-fly recomputation strategy to enable fast and accurate retrieval with minimal storage overhead. Our evaluation shows that LEANN reduces index size to under 5% of the original raw data, achieving up to 50 times smaller storage than standard indexes, while maintaining 90% top-3 recall in under 2 seconds on real-world question answering benchmarks.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instruction-Tuned Video-Audio Models Elucidate Functional Specialization in the Brain</title>
<link>https://arxiv.org/abs/2506.08277</link>
<guid>https://arxiv.org/abs/2506.08277</guid>
<content:encoded><![CDATA[
arXiv:2506.08277v1 Announce Type: cross 
Abstract: Recent voxel-wise multimodal brain encoding studies have shown that multimodal large language models (MLLMs) exhibit a higher degree of brain alignment compared to unimodal models in both unimodal and multimodal stimulus settings. More recently, instruction-tuned multimodal models have shown to generate task-specific representations that align strongly with brain activity. However, prior work evaluating the brain alignment of MLLMs has primarily focused on unimodal settings or relied on non-instruction-tuned multimodal models for multimodal stimuli. To address this gap, we investigated brain alignment, that is, measuring the degree of predictivity of neural activity recorded while participants were watching naturalistic movies (video along with audio) with representations derived from MLLMs. We utilized instruction-specific embeddings from six video and two audio instruction-tuned MLLMs. Experiments with 13 video task-specific instructions show that instruction-tuned video MLLMs significantly outperform non-instruction-tuned multimodal (by 15%) and unimodal models (by 20%). Our evaluation of MLLMs for both video and audio tasks using language-guided instructions shows clear disentanglement in task-specific representations from MLLMs, leading to precise differentiation of multimodal functional processing in the brain. We also find that MLLM layers align hierarchically with the brain, with early sensory areas showing strong alignment with early layers, while higher-level visual and language regions align more with middle to late layers. These findings provide clear evidence for the role of task-specific instructions in improving the alignment between brain activity and MLLMs, and open new avenues for mapping joint information processing in both the systems. We make the code publicly available [https://github.com/subbareddy248/mllm_videos].
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing Voices: Generating A-Roll Video from Audio with Mirage</title>
<link>https://arxiv.org/abs/2506.08279</link>
<guid>https://arxiv.org/abs/2506.08279</guid>
<content:encoded><![CDATA[
arXiv:2506.08279v1 Announce Type: cross 
Abstract: From professional filmmaking to user-generated content, creators and consumers have long recognized that the power of video depends on the harmonious integration of what we hear (the video's audio track) with what we see (the video's image sequence). Current approaches to video generation either ignore sound to focus on general-purpose but silent image sequence generation or address both visual and audio elements but focus on restricted application domains such as re-dubbing. We introduce Mirage, an audio-to-video foundation model that excels at generating realistic, expressive output imagery from scratch given an audio input. When integrated with existing methods for speech synthesis (text-to-speech, or TTS), Mirage results in compelling multimodal video. When trained on audio-video footage of people talking (A-roll) and conditioned on audio containing speech, Mirage generates video of people delivering a believable interpretation of the performance implicit in input audio. Our central technical contribution is a unified method for training self-attention-based audio-to-video generation models, either from scratch or given existing weights. This methodology allows Mirage to retain generality as an approach to audio-to-video generation while producing outputs of superior subjective quality to methods that incorporate audio-specific architectures or loss components specific to people, speech, or details of how images or audio are captured. We encourage readers to watch and listen to the results of Mirage for themselves (see paper and comments for links).
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-Free Kernel Conformal Depth Measures Algorithm for Uncertainty Quantification in Regression Models in Separable Hilbert Spaces</title>
<link>https://arxiv.org/abs/2506.08325</link>
<guid>https://arxiv.org/abs/2506.08325</guid>
<content:encoded><![CDATA[
arXiv:2506.08325v1 Announce Type: cross 
Abstract: Depth measures are powerful tools for defining level sets in emerging, non--standard, and complex random objects such as high-dimensional multivariate data, functional data, and random graphs. Despite their favorable theoretical properties, the integration of depth measures into regression modeling to provide prediction regions remains a largely underexplored area of research. To address this gap, we propose a novel, model-free uncertainty quantification algorithm based on conditional depth measures--specifically, conditional kernel mean embeddings and an integrated depth measure. These new algorithms can be used to define prediction and tolerance regions when predictors and responses are defined in separable Hilbert spaces. The use of kernel mean embeddings ensures faster convergence rates in prediction region estimation. To enhance the practical utility of the algorithms with finite samples, we also introduce a conformal prediction variant that provides marginal, non-asymptotic guarantees for the derived prediction regions. Additionally, we establish both conditional and unconditional consistency results, as well as fast convergence rates in certain homoscedastic settings. We evaluate the finite--sample performance of our model in extensive simulation studies involving various types of functional data and traditional Euclidean scenarios. Finally, we demonstrate the practical relevance of our approach through a digital health application related to physical activity, aiming to provide personalized recommendations
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Your Agent Can Defend Itself against Backdoor Attacks</title>
<link>https://arxiv.org/abs/2506.08336</link>
<guid>https://arxiv.org/abs/2506.08336</guid>
<content:encoded><![CDATA[
arXiv:2506.08336v1 Announce Type: cross 
Abstract: Despite their growing adoption across domains, large language model (LLM)-powered agents face significant security risks from backdoor attacks during training and fine-tuning. These compromised agents can subsequently be manipulated to execute malicious operations when presented with specific triggers in their inputs or environments. To address this pressing risk, we present ReAgent, a novel defense against a range of backdoor attacks on LLM-based agents. Intuitively, backdoor attacks often result in inconsistencies among the user's instruction, the agent's planning, and its execution. Drawing on this insight, ReAgent employs a two-level approach to detect potential backdoors. At the execution level, ReAgent verifies consistency between the agent's thoughts and actions; at the planning level, ReAgent leverages the agent's capability to reconstruct the instruction based on its thought trajectory, checking for consistency between the reconstructed instruction and the user's instruction. Extensive evaluation demonstrates ReAgent's effectiveness against various backdoor attacks across tasks. For instance, ReAgent reduces the attack success rate by up to 90\% in database operation tasks, outperforming existing defenses by large margins. This work reveals the potential of utilizing compromised agents themselves to mitigate backdoor risks.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>midr: Learning from Black-Box Models by Maximum Interpretation Decomposition</title>
<link>https://arxiv.org/abs/2506.08338</link>
<guid>https://arxiv.org/abs/2506.08338</guid>
<content:encoded><![CDATA[
arXiv:2506.08338v1 Announce Type: cross 
Abstract: The use of appropriate methods of Interpretable Machine Learning (IML) and eXplainable Artificial Intelligence (XAI) is essential for adopting black-box predictive models in fields where model and prediction explainability is required. As a novel tool for interpreting black-box models, we introduce the R package midr, which implements Maximum Interpretation Decomposition (MID). MID is a functional decomposition approach that derives a low-order additive representation of a black-box model by minimizing the squared error between the model's prediction function and this additive representation. midr enables learning from black-box models by constructing a global surrogate model with advanced analytical capabilities. After reviewing related work and the theoretical foundation of MID, we demonstrate the package's usage and discuss some of its key features.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Re4MPC: Reactive Nonlinear MPC for Multi-model Motion Planning via Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.08344</link>
<guid>https://arxiv.org/abs/2506.08344</guid>
<content:encoded><![CDATA[
arXiv:2506.08344v1 Announce Type: cross 
Abstract: Traditional motion planning methods for robots with many degrees-of-freedom, such as mobile manipulators, are often computationally prohibitive for real-world settings. In this paper, we propose a novel multi-model motion planning pipeline, termed Re4MPC, which computes trajectories using Nonlinear Model Predictive Control (NMPC). Re4MPC generates trajectories in a computationally efficient manner by reactively selecting the model, cost, and constraints of the NMPC problem depending on the complexity of the task and robot state. The policy for this reactive decision-making is learned via a Deep Reinforcement Learning (DRL) framework. We introduce a mathematical formulation to integrate NMPC into this DRL framework. To validate our methodology and design choices, we evaluate DRL training and test outcomes in a physics-based simulation involving a mobile manipulator. Experimental results demonstrate that Re4MPC is more computationally efficient and achieves higher success rates in reaching end-effector goals than the NMPC baseline, which computes whole-body trajectories without our learning mechanism.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving Convex-Concave Problems with $\tilde{\mathcal{O}}(\epsilon^{-4/7})$ Second-Order Oracle Complexity</title>
<link>https://arxiv.org/abs/2506.08362</link>
<guid>https://arxiv.org/abs/2506.08362</guid>
<content:encoded><![CDATA[
arXiv:2506.08362v1 Announce Type: cross 
Abstract: Previous algorithms can solve convex-concave minimax problems $\min_{x \in \mathcal{X}} \max_{y \in \mathcal{Y}} f(x,y)$ with $\mathcal{O}(\epsilon^{-2/3})$ second-order oracle calls using Newton-type methods. This result has been speculated to be optimal because the upper bound is achieved by a natural generalization of the optimal first-order method. In this work, we show an improved upper bound of $\tilde{\mathcal{O}}(\epsilon^{-4/7})$ by generalizing the optimal second-order method for convex optimization to solve the convex-concave minimax problem. We further apply a similar technique to lazy Hessian algorithms and show that our proposed algorithm can also be seen as a second-order ``Catalyst'' framework (Lin et al., JMLR 2018) that could accelerate any globally convergent algorithms for solving minimax problems.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TS-PIELM: Time-Stepping Physics-Informed Extreme Learning Machine Facilitates Soil Consolidation Analyses</title>
<link>https://arxiv.org/abs/2506.08381</link>
<guid>https://arxiv.org/abs/2506.08381</guid>
<content:encoded><![CDATA[
arXiv:2506.08381v1 Announce Type: cross 
Abstract: Accuracy and efficiency of the conventional physics-informed neural network (PINN) need to be improved before it can be a competitive alternative for soil consolidation analyses. This paper aims to overcome these limitations by proposing a highly accurate and efficient physics-informed machine learning (PIML) approach, termed time-stepping physics-informed extreme learning machine (TS-PIELM). In the TS-PIELM framework the consolidation process is divided into numerous time intervals, which helps overcome the limitation of PIELM in solving differential equations with sharp gradients. To accelerate network training, the solution is approximated by a single-layer feedforward extreme learning machine (ELM), rather than using a fully connected neural network in PINN. The input layer weights of the ELM network are generated randomly and fixed during the training process. Subsequently, the output layer weights are directly computed by solving a system of linear equations, which significantly enhances the training efficiency compared to the time-consuming gradient descent method in PINN. Finally, the superior performance of TS-PIELM is demonstrated by solving three typical Terzaghi consolidation problems. Compared to PINN, results show that the computational efficiency and accuracy of the novel TS-PIELM framework are improved by more than 1000 times and 100 times for one-dimensional cases, respectively. This paper provides compelling evidence that PIML can be a powerful tool for computational geotechnics.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafeCoT: Improving VLM Safety with Minimal Reasoning</title>
<link>https://arxiv.org/abs/2506.08399</link>
<guid>https://arxiv.org/abs/2506.08399</guid>
<content:encoded><![CDATA[
arXiv:2506.08399v1 Announce Type: cross 
Abstract: Ensuring safe and appropriate responses from vision-language models (VLMs) remains a critical challenge, particularly in high-risk or ambiguous scenarios. We introduce SafeCoT, a lightweight, interpretable framework that leverages rule-based chain-of-thought (CoT) supervision to improve refusal behavior in VLMs. Unlike prior methods that rely on large-scale safety annotations or complex modeling, SafeCoT uses minimal supervision to help models reason about safety risks and make context-aware refusals. Experiments across multiple benchmarks show that SafeCoT significantly reduces overrefusal and enhances generalization, even with limited training data. Our approach offers a scalable solution for aligning VLMs with safety-critical objectives.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>mSTEB: Massively Multilingual Evaluation of LLMs on Speech and Text Tasks</title>
<link>https://arxiv.org/abs/2506.08400</link>
<guid>https://arxiv.org/abs/2506.08400</guid>
<content:encoded><![CDATA[
arXiv:2506.08400v1 Announce Type: cross 
Abstract: Large Language models (LLMs) have demonstrated impressive performance on a wide range of tasks, including in multimodal settings such as speech. However, their evaluation is often limited to English and a few high-resource languages. For low-resource languages, there is no standardized evaluation benchmark. In this paper, we address this gap by introducing mSTEB, a new benchmark to evaluate the performance of LLMs on a wide range of tasks covering language identification, text classification, question answering, and translation tasks on both speech and text modalities. We evaluated the performance of leading LLMs such as Gemini 2.0 Flash and GPT-4o (Audio) and state-of-the-art open models such as Qwen 2 Audio and Gemma 3 27B. Our evaluation shows a wide gap in performance between high-resource and low-resource languages, especially for languages spoken in Africa and Americas/Oceania. Our findings show that more investment is needed to address their under-representation in LLMs coverage.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mic-hackathon 2024: Hackathon on Machine Learning for Electron and Scanning Probe Microscopy</title>
<link>https://arxiv.org/abs/2506.08423</link>
<guid>https://arxiv.org/abs/2506.08423</guid>
<content:encoded><![CDATA[
arXiv:2506.08423v1 Announce Type: cross 
Abstract: Microscopy is a primary source of information on materials structure and functionality at nanometer and atomic scales. The data generated is often well-structured, enriched with metadata and sample histories, though not always consistent in detail or format. The adoption of Data Management Plans (DMPs) by major funding agencies promotes preservation and access. However, deriving insights remains difficult due to the lack of standardized code ecosystems, benchmarks, and integration strategies. As a result, data usage is inefficient and analysis time is extensive. In addition to post-acquisition analysis, new APIs from major microscope manufacturers enable real-time, ML-based analytics for automated decision-making and ML-agent-controlled microscope operation. Yet, a gap remains between the ML and microscopy communities, limiting the impact of these methods on physics, materials discovery, and optimization. Hackathons help bridge this divide by fostering collaboration between ML researchers and microscopy experts. They encourage the development of novel solutions that apply ML to microscopy, while preparing a future workforce for instrumentation, materials science, and applied ML. This hackathon produced benchmark datasets and digital twins of microscopes to support community growth and standardized workflows. All related code is available at GitHub: https://github.com/KalininGroup/Mic-hackathon-2024-codes-publication/tree/1.0.0.1
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sharper Convergence Rates for Nonconvex Optimisation via Reduction Mappings</title>
<link>https://arxiv.org/abs/2506.08428</link>
<guid>https://arxiv.org/abs/2506.08428</guid>
<content:encoded><![CDATA[
arXiv:2506.08428v1 Announce Type: cross 
Abstract: Many high-dimensional optimisation problems exhibit rich geometric structures in their set of minimisers, often forming smooth manifolds due to over-parametrisation or symmetries. When this structure is known, at least locally, it can be exploited through reduction mappings that reparametrise part of the parameter space to lie on the solution manifold. These reductions naturally arise from inner optimisation problems and effectively remove redundant directions, yielding a lower-dimensional objective. In this work, we introduce a general framework to understand how such reductions influence the optimisation landscape. We show that well-designed reduction mappings improve curvature properties of the objective, leading to better-conditioned problems and theoretically faster convergence for gradient-based methods. Our analysis unifies a range of scenarios where structural information at optimality is leveraged to accelerate convergence, offering a principled explanation for the empirical gains observed in such optimisation algorithms.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Low-resource domain adaptation while minimizing energy and hardware resource consumption</title>
<link>https://arxiv.org/abs/2506.08433</link>
<guid>https://arxiv.org/abs/2506.08433</guid>
<content:encoded><![CDATA[
arXiv:2506.08433v1 Announce Type: cross 
Abstract: Training Large Language Models (LLMs) is costly in terms of energy, hardware, and annotated data, often resulting in a positionality rooted in predominant cultures and values (Santy et al., 2023). Domain adaptation has emerged as a promising strategy to better align models with diverse cultural and value contexts (Hershcovich et al., 2022), but its computational cost remains a significant barrier, particularly for research groups lacking access to large-scale infrastructure. In this paper, we evaluate how the use of different numerical precisions and data parallelization strategies impacts both training speed (as a proxy to energy and hardware consumption) and model accuracy, with the goal of facilitating domain adaptation in low-resource environments. Our findings are relevant to any setting where energy efficiency, accessibility, or limited hardware availability are key concerns.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Olica: Efficient Structured Pruning of Large Language Models without Retraining</title>
<link>https://arxiv.org/abs/2506.08436</link>
<guid>https://arxiv.org/abs/2506.08436</guid>
<content:encoded><![CDATA[
arXiv:2506.08436v1 Announce Type: cross 
Abstract: Most existing structured pruning methods for Large Language Models (LLMs) require substantial computational and data resources for retraining to reestablish the corrupted correlations, making them prohibitively expensive. To address this, we propose a pruning framework for LLMs called Orthogonal decomposition and Linear Calibration (Olica), which eliminates the need for retraining. A key observation is that the multi-head attention (MHA) layer depends on two types of matrix products. By treating these matrix products as unified entities and applying principal component analysis (PCA), we extract the most important information to compress LLMs without sacrificing accuracy or disrupting their original structure. Consequently, retraining becomes unnecessary. A fast decomposition method is devised, reducing the complexity of PCA by a factor of the square of the number of attention heads. Additionally, to mitigate error accumulation problem caused by pruning the feed-forward network (FFN) layer, we introduce a linear calibration method to reconstruct the residual errors of pruned layers using low-rank matrices. By leveraging singular value decomposition (SVD) on the solution of the least-squares problem, these matrices are obtained without requiring retraining. Extensive experiments show that the proposed Olica is efficient in terms of data usage, GPU memory, and running time, while delivering superior performance across multiple benchmarks.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Systematic and Efficient Construction of Quadratic Unconstrained Binary Optimization Forms for High-order and Dense Interactions</title>
<link>https://arxiv.org/abs/2506.08448</link>
<guid>https://arxiv.org/abs/2506.08448</guid>
<content:encoded><![CDATA[
arXiv:2506.08448v1 Announce Type: cross 
Abstract: Quantum Annealing (QA) can efficiently solve combinatorial optimization problems whose objective functions are represented by Quadratic Unconstrained Binary Optimization (QUBO) formulations. For broader applicability of QA, quadratization methods are used to transform higher-order problems into QUBOs. However, quadratization methods for complex problems involving Machine Learning (ML) remain largely unknown. In these problems, strong nonlinearity and dense interactions prevent conventional methods from being applied. Therefore, we model target functions by the sum of rectified linear unit bases, which not only have the ability of universal approximation, but also have an equivalent quadratic-polynomial representation. In this study, the proof of concept is verified both numerically and analytically. In addition, by combining QA with the proposed quadratization, we design a new black-box optimization scheme, in which ML surrogate regressors are inputted to QA after the quadratization process.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The interplay of robustness and generalization in quantum machine learning</title>
<link>https://arxiv.org/abs/2506.08455</link>
<guid>https://arxiv.org/abs/2506.08455</guid>
<content:encoded><![CDATA[
arXiv:2506.08455v1 Announce Type: cross 
Abstract: While adversarial robustness and generalization have individually received substantial attention in the recent literature on quantum machine learning, their interplay is much less explored. In this chapter, we address this interplay for variational quantum models, which were recently proposed as function approximators in supervised learning. We discuss recent results quantifying both robustness and generalization via Lipschitz bounds, which explicitly depend on model parameters. Thus, they give rise to a regularization-based training approach for robust and generalizable quantum models, highlighting the importance of trainable data encoding strategies. The practical implications of the theoretical results are demonstrated with an application to time series analysis.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRAGged into Conflicts: Detecting and Addressing Conflicting Sources in Search-Augmented LLMs</title>
<link>https://arxiv.org/abs/2506.08500</link>
<guid>https://arxiv.org/abs/2506.08500</guid>
<content:encoded><![CDATA[
arXiv:2506.08500v1 Announce Type: cross 
Abstract: Retrieval Augmented Generation (RAG) is a commonly used approach for enhancing large language models (LLMs) with relevant and up-to-date information. However, the retrieved sources can often contain conflicting information and it remains unclear how models should address such discrepancies. In this work, we first propose a novel taxonomy of knowledge conflict types in RAG, along with the desired model behavior for each type. We then introduce CONFLICTS, a high-quality benchmark with expert annotations of conflict types in a realistic RAG setting. CONFLICTS is the first benchmark that enables tracking progress on how models address a wide range of knowledge conflicts. We conduct extensive experiments on this benchmark, showing that LLMs often struggle to appropriately resolve conflicts between sources. While prompting LLMs to explicitly reason about the potential conflict in the retrieved documents significantly improves the quality and appropriateness of their responses, substantial room for improvement in future research remains.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in conversations</title>
<link>https://arxiv.org/abs/2506.08504</link>
<guid>https://arxiv.org/abs/2506.08504</guid>
<content:encoded><![CDATA[
arXiv:2506.08504v1 Announce Type: cross 
Abstract: Discourse parsing is an important task useful for NLU applications such as summarization, machine comprehension, and emotion recognition. The current discourse parsing datasets based on conversations consists of written English dialogues restricted to a single domain. In this resource paper, we introduce CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in conversations. The corpus (code-mixed in Hindi and English) has both audio and transcribed text and is annotated with nine discourse relations. We experiment with various SoTA baseline models; the poor performance of SoTA models highlights the challenges of multi-domain code-mixed corpus, pointing towards the need for developing better models for such realistic settings.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MasHost Builds It All: Autonomous Multi-Agent System Directed by Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.08507</link>
<guid>https://arxiv.org/abs/2506.08507</guid>
<content:encoded><![CDATA[
arXiv:2506.08507v1 Announce Type: cross 
Abstract: Large Language Model (LLM)-driven Multi-agent systems (Mas) have recently emerged as a powerful paradigm for tackling complex real-world tasks. However, existing Mas construction methods typically rely on manually crafted interaction mechanisms or heuristic rules, introducing human biases and constraining the autonomous ability. Even with recent advances in adaptive Mas construction, existing systems largely remain within the paradigm of semi-autonomous patterns. In this work, we propose MasHost, a Reinforcement Learning (RL)-based framework for autonomous and query-adaptive Mas design. By formulating Mas construction as a graph search problem, our proposed MasHost jointly samples agent roles and their interactions through a unified probabilistic sampling mechanism. Beyond the accuracy and efficiency objectives pursued in prior works, we introduce component rationality as an additional and novel design principle in Mas. To achieve this multi-objective optimization, we propose Hierarchical Relative Policy Optimization (HRPO), a novel RL strategy that collaboratively integrates group-relative advantages and action-wise rewards. To our knowledge, our proposed MasHost is the first RL-driven framework for autonomous Mas graph construction. Extensive experiments on six benchmarks demonstrate that MasHost consistently outperforms most competitive baselines, validating its effectiveness, efficiency, and structure rationality.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FEDTAIL: Federated Long-Tailed Domain Generalization with Sharpness-Guided Gradient Matching</title>
<link>https://arxiv.org/abs/2506.08518</link>
<guid>https://arxiv.org/abs/2506.08518</guid>
<content:encoded><![CDATA[
arXiv:2506.08518v1 Announce Type: cross 
Abstract: Domain Generalization (DG) seeks to train models that perform reliably on unseen target domains without access to target data during training. While recent progress in smoothing the loss landscape has improved generalization, existing methods often falter under long-tailed class distributions and conflicting optimization objectives. We introduce FedTAIL, a federated domain generalization framework that explicitly addresses these challenges through sharpness-guided, gradient-aligned optimization. Our method incorporates a gradient coherence regularizer to mitigate conflicts between classification and adversarial objectives, leading to more stable convergence. To combat class imbalance, we perform class-wise sharpness minimization and propose a curvature-aware dynamic weighting scheme that adaptively emphasizes underrepresented tail classes. Furthermore, we enhance conditional distribution alignment by integrating sharpness-aware perturbations into entropy regularization, improving robustness under domain shift. FedTAIL unifies optimization harmonization, class-aware regularization, and conditional alignment into a scalable, federated-compatible framework. Extensive evaluations across standard domain generalization benchmarks demonstrate that FedTAIL achieves state-of-the-art performance, particularly in the presence of domain shifts and label imbalance, validating its effectiveness in both centralized and federated settings. Code: https://github.com/sunnyinAI/FedTail
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PerfTracker: Online Performance Troubleshooting for Large-scale Model Training in Production</title>
<link>https://arxiv.org/abs/2506.08528</link>
<guid>https://arxiv.org/abs/2506.08528</guid>
<content:encoded><![CDATA[
arXiv:2506.08528v1 Announce Type: cross 
Abstract: Troubleshooting performance problems of large model training (LMT) is immensely challenging, due to unprecedented scales of modern GPU clusters, the complexity of software-hardware interactions, and the data intensity of the training process. Existing troubleshooting approaches designed for traditional distributed systems or datacenter networks fall short and can hardly apply to real-world training systems. In this paper, we present PerfTracker, the first online troubleshooting system utilizing fine-grained profiling, to diagnose performance issues of large-scale model training in production. PerfTracker can diagnose performance issues rooted in both hardware (e.g., GPUs and their interconnects) and software (e.g., Python functions and GPU operations). It scales to LMT on modern GPU clusters. PerfTracker effectively summarizes runtime behavior patterns of fine-grained LMT functions via online profiling, and leverages differential observability to localize the root cause with minimal production impact. PerfTracker has been deployed as a production service for large-scale GPU clusters of O(10, 000) GPUs (product homepage https://help.aliyun.com/zh/pai/user-guide/perftracker-online-performance-analysis-diagnostic-tool). It has been used to diagnose a variety of difficult performance issues.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structured Variational $D$-Decomposition for Accurate and Stable Low-Rank Approximation</title>
<link>https://arxiv.org/abs/2506.08535</link>
<guid>https://arxiv.org/abs/2506.08535</guid>
<content:encoded><![CDATA[
arXiv:2506.08535v1 Announce Type: cross 
Abstract: We introduce the $D$-decomposition, a non-orthogonal matrix factorization of the form $A \approx P D Q$, where $P \in \mathbb{R}^{n \times k}$, $D \in \mathbb{R}^{k \times k}$, and $Q \in \mathbb{R}^{k \times n}$. The decomposition is defined variationally by minimizing a regularized Frobenius loss, allowing control over rank, sparsity, and conditioning. Unlike algebraic factorizations such as LU or SVD, it is computed by alternating minimization. We establish existence and perturbation stability of the solution and show that each update has complexity $\mathcal{O}(n^2k)$. Benchmarks against truncated SVD, CUR, and nonnegative matrix factorization show improved reconstruction accuracy on MovieLens, MNIST, Olivetti Faces, and gene expression matrices, particularly under sparsity and noise.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Asymptotic Normality of Infinite Centered Random Forests -Application to Imbalanced Classification</title>
<link>https://arxiv.org/abs/2506.08548</link>
<guid>https://arxiv.org/abs/2506.08548</guid>
<content:encoded><![CDATA[
arXiv:2506.08548v1 Announce Type: cross 
Abstract: Many classification tasks involve imbalanced data, in which a class is largely underrepresented. Several techniques consists in creating a rebalanced dataset on which a classifier is trained. In this paper, we study theoretically such a procedure, when the classifier is a Centered Random Forests (CRF). We establish a Central Limit Theorem (CLT) on the infinite CRF with explicit rates and exact constant. We then prove that the CRF trained on the rebalanced dataset exhibits a bias, which can be removed with appropriate techniques. Based on an importance sampling (IS) approach, the resulting debiased estimator, called IS-ICRF, satisfies a CLT centered at the prediction function value. For high imbalance settings, we prove that the IS-ICRF estimator enjoys a variance reduction compared to the ICRF trained on the original data. Therefore, our theoretical analysis highlights the benefits of training random forests on a rebalanced dataset (followed by a debiasing procedure) compared to using the original data. Our theoretical results, especially the variance rates and the variance reduction, appear to be valid for Breiman's random forests in our experiments.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimization over Sparse Support-Preserving Sets: Two-Step Projection with Global Optimality Guarantees</title>
<link>https://arxiv.org/abs/2506.08558</link>
<guid>https://arxiv.org/abs/2506.08558</guid>
<content:encoded><![CDATA[
arXiv:2506.08558v1 Announce Type: cross 
Abstract: In sparse optimization, enforcing hard constraints using the $\ell_0$ pseudo-norm offers advantages like controlled sparsity compared to convex relaxations. However, many real-world applications demand not only sparsity constraints but also some extra constraints. While prior algorithms have been developed to address this complex scenario with mixed combinatorial and convex constraints, they typically require the closed form projection onto the mixed constraints which might not exist, and/or only provide local guarantees of convergence which is different from the global guarantees commonly sought in sparse optimization. To fill this gap, in this paper, we study the problem of sparse optimization with extra \qw{\textit{support-preserving}} constraints commonly encountered in the literature. We present a new variant of iterative hard-thresholding algorithm equipped with a two-step consecutive projection operator customized for these mixed constraints, serving as a simple alternative to the Euclidean projection onto the mixed constraint. By introducing a novel trade-off between sparsity relaxation and sub-optimality, we provide global guarantees in objective value for the output of our algorithm, in the deterministic, stochastic, and zeroth-order settings, under the conventional restricted strong-convexity/smoothness assumptions. As a fundamental contribution in proof techniques, we develop a novel extension of the classic three-point lemma to the considered two-step non-convex projection operator, which allows us to analyze the convergence in objective value in an elegant way that has not been possible with existing techniques. In the zeroth-order case, such technique also improves upon the state-of-the-art result from de Vazelhes et. al. (2022), even in the case without additional constraints, by allowing us to remove a non-vanishing system error present in their work.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Auto-Regressive vs Flow-Matching: a Comparative Study of Modeling Paradigms for Text-to-Music Generation</title>
<link>https://arxiv.org/abs/2506.08570</link>
<guid>https://arxiv.org/abs/2506.08570</guid>
<content:encoded><![CDATA[
arXiv:2506.08570v1 Announce Type: cross 
Abstract: Recent progress in text-to-music generation has enabled models to synthesize high-quality musical segments, full compositions, and even respond to fine-grained control signals, e.g. chord progressions. State-of-the-art (SOTA) systems differ significantly across many dimensions, such as training datasets, modeling paradigms, and architectural choices. This diversity complicates efforts to evaluate models fairly and pinpoint which design choices most influence performance. While factors like data and architecture are important, in this study we focus exclusively on the modeling paradigm. We conduct a systematic empirical analysis to isolate its effects, offering insights into associated trade-offs and emergent behaviors that can guide future text-to-music generation systems. Specifically, we compare the two arguably most common modeling paradigms: Auto-Regressive decoding and Conditional Flow-Matching. We conduct a controlled comparison by training all models from scratch using identical datasets, training configurations, and similar backbone architectures. Performance is evaluated across multiple axes, including generation quality, robustness to inference configurations, scalability, adherence to both textual and temporally aligned conditioning, and editing capabilities in the form of audio inpainting. This comparative study sheds light on distinct strengths and limitations of each paradigm, providing actionable insights that can inform future architectural and training decisions in the evolving landscape of text-to-music generation. Audio sampled examples are available at: https://huggingface.co/spaces/ortal1602/ARvsFM
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diversity-Guided MLP Reduction for Efficient Large Vision Transformers</title>
<link>https://arxiv.org/abs/2506.08591</link>
<guid>https://arxiv.org/abs/2506.08591</guid>
<content:encoded><![CDATA[
arXiv:2506.08591v1 Announce Type: cross 
Abstract: Transformer models achieve excellent scaling property, where the performance is improved with the increment of model capacity. However, large-scale model parameters lead to an unaffordable cost of computing and memory. We analyze popular transformer architectures and find that multilayer perceptron (MLP) modules take up the majority of model parameters.
To this end, we focus on the recoverability of the compressed models and propose a Diversity-Guided MLP Reduction (DGMR) method to significantly reduce the parameters of large vision transformers with only negligible performance degradation. Specifically, we conduct a Gram-Schmidt weight pruning strategy to eliminate redundant neurons of MLP hidden layer, while preserving weight diversity for better performance recover during distillation. Compared to the model trained from scratch, our pruned model only requires 0.06\% data of LAION-2B (for the training of large vision transformers) without labels (ImageNet-1K) to recover the original performance. Experimental results on several state-of-the-art large vision transformers demonstrate that our method achieves a more than 57.0\% parameter and FLOPs reduction in a near lossless manner. Notably, for EVA-CLIP-E (4.4B), our method accomplishes a 71.5\% parameter and FLOPs reduction without performance degradation. The source code and trained weights are available at https://github.com/visresearch/DGMR.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity Dilemma of Embeddings</title>
<link>https://arxiv.org/abs/2506.08592</link>
<guid>https://arxiv.org/abs/2506.08592</guid>
<content:encoded><![CDATA[
arXiv:2506.08592v1 Announce Type: cross 
Abstract: This work focuses on an observed limitation of text encoders: embeddings may not be able to recognize fine-grained entities or events within the semantics, resulting in failed dense retrieval on even simple cases. To examine such behaviors, we first introduce a new evaluation dataset in Chinese, named CapRetrieval, whose passages are image captions, and queries are phrases inquiring entities or events in various forms. Zero-shot evaluation suggests that encoders may fail on these fine-grained matching, regardless of training sources or model sizes. Aiming for enhancement, we proceed to finetune encoders with our proposed data generation strategies, which obtains the best performance on CapRetrieval. Within this process, we further identify an issue of granularity dilemma, a challenge for embeddings to express fine-grained salience while aligning with overall semantics. Our dataset, code and models in this work are publicly released at https://github.com/lxucs/CapRetrieval.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving excited states for long-range interacting trapped ions with neural networks</title>
<link>https://arxiv.org/abs/2506.08594</link>
<guid>https://arxiv.org/abs/2506.08594</guid>
<content:encoded><![CDATA[
arXiv:2506.08594v1 Announce Type: cross 
Abstract: The computation of excited states in strongly interacting quantum many-body systems is of fundamental importance. Yet, it is notoriously challenging due to the exponential scaling of the Hilbert space dimension with the system size. Here, we introduce a neural network-based algorithm that can simultaneously output multiple low-lying excited states of a quantum many-body spin system in an accurate and efficient fashion. This algorithm, dubbed the neural quantum excited-state (NQES) algorithm, requires no explicit orthogonalization of the states and is generally applicable to higher dimensions. We demonstrate, through concrete examples including the Haldane-Shastry model with all-to-all interactions, that the NQES algorithm is capable of efficiently computing multiple excited states and their related observable expectations. In addition, we apply the NQES algorithm to two classes of long-range interacting trapped-ion systems in a two-dimensional Wigner crystal. For non-decaying all-to-all interactions with alternating signs, our computed low-lying excited states bear spatial correlation patterns similar to those of the ground states, which closely match recent experimental observations that the quasi-adiabatically prepared state accurately reproduces analytical ground-state correlations. For a system of up to 300 ions with power-law decaying antiferromagnetic interactions, we successfully uncover its gap scaling and correlation features. Our results establish a scalable and efficient algorithm for computing excited states of interacting quantum many-body systems, which holds potential applications ranging from benchmarking quantum devices to photoisomerization.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizing while preserving monotonicity in comparison-based preference learning models</title>
<link>https://arxiv.org/abs/2506.08616</link>
<guid>https://arxiv.org/abs/2506.08616</guid>
<content:encoded><![CDATA[
arXiv:2506.08616v1 Announce Type: cross 
Abstract: If you tell a learning model that you prefer an alternative $a$ over another alternative $b$, then you probably expect the model to be monotone, that is, the valuation of $a$ increases, and that of $b$ decreases. Yet, perhaps surprisingly, many widely deployed comparison-based preference learning models, including large language models, fail to have this guarantee. Until now, the only comparison-based preference learning algorithms that were proved to be monotone are the Generalized Bradley-Terry models. Yet, these models are unable to generalize to uncompared data. In this paper, we advance the understanding of the set of models with generalization ability that are monotone. Namely, we propose a new class of Linear Generalized Bradley-Terry models with Diffusion Priors, and identify sufficient conditions on alternatives' embeddings that guarantee monotonicity. Our experiments show that this monotonicity is far from being a general guarantee, and that our new class of generalizing models improves accuracy, especially when the dataset is limited.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TableDreamer: Progressive and Weakness-guided Data Synthesis from Scratch for Table Instruction Tuning</title>
<link>https://arxiv.org/abs/2506.08646</link>
<guid>https://arxiv.org/abs/2506.08646</guid>
<content:encoded><![CDATA[
arXiv:2506.08646v1 Announce Type: cross 
Abstract: Despite the commendable progress of recent LLM-based data synthesis methods, they face two limitations in generating table instruction tuning data. First, they can not thoroughly explore the vast input space of table understanding tasks, leading to limited data diversity. Second, they ignore the weaknesses in table understanding ability of the target LLM and blindly pursue the increase of data quantity, resulting in suboptimal data efficiency. In this paper, we introduce a progressive and weakness-guided data synthesis framework tailored for table instruction tuning, named TableDreamer, to mitigate the above issues. Specifically, we first synthesize diverse tables and related instructions as seed data, and then perform an iterative exploration of the input space under the guidance of the newly identified weakness data, which eventually serve as the final training data for fine-tuning the target LLM. Extensive experiments on 10 tabular benchmarks demonstrate the effectiveness of the proposed framework, which boosts the average accuracy of Llama3.1-8B-instruct by 11.62% (49.07% to 60.69%) with 27K GPT-4o synthetic data and outperforms state-of-the-art data synthesis baselines which use more training data. The code and data is available at https://github.com/SpursGoZmy/TableDreamer
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Privacy-Preserving Federated Learning Framework for Generalizable CBCT to Synthetic CT Translation in Head and Neck</title>
<link>https://arxiv.org/abs/2506.08654</link>
<guid>https://arxiv.org/abs/2506.08654</guid>
<content:encoded><![CDATA[
arXiv:2506.08654v1 Announce Type: cross 
Abstract: Shortened Abstract
  Cone-beam computed tomography (CBCT) has become a widely adopted modality for image-guided radiotherapy (IGRT). However, CBCT suffers from increased noise, limited soft-tissue contrast, and artifacts, resulting in unreliable Hounsfield unit values and hindering direct dose calculation. Synthetic CT (sCT) generation from CBCT addresses these issues, especially using deep learning (DL) methods. Existing approaches are limited by institutional heterogeneity, scanner-dependent variations, and data privacy regulations that prevent multi-center data sharing.
  To overcome these challenges, we propose a cross-silo horizontal federated learning (FL) approach for CBCT-to-sCT synthesis in the head and neck region, extending our FedSynthCT framework. A conditional generative adversarial network was collaboratively trained on data from three European medical centers in the public SynthRAD2025 challenge dataset.
  The federated model demonstrated effective generalization across centers, with mean absolute error (MAE) ranging from $64.38\pm13.63$ to $85.90\pm7.10$ HU, structural similarity index (SSIM) from $0.882\pm0.022$ to $0.922\pm0.039$, and peak signal-to-noise ratio (PSNR) from $32.86\pm0.94$ to $34.91\pm1.04$ dB. Notably, on an external validation dataset of 60 patients, comparable performance was achieved (MAE: $75.22\pm11.81$ HU, SSIM: $0.904\pm0.034$, PSNR: $33.52\pm2.06$ dB) without additional training, confirming robust generalization despite protocol, scanner differences and registration errors.
  These findings demonstrate the technical feasibility of FL for CBCT-to-sCT synthesis while preserving data privacy and offer a collaborative solution for developing generalizable models across institutions without centralized data sharing or site-specific fine-tuning.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>sparseGeoHOPCA: A Geometric Solution to Sparse Higher-Order PCA Without Covariance Estimation</title>
<link>https://arxiv.org/abs/2506.08670</link>
<guid>https://arxiv.org/abs/2506.08670</guid>
<content:encoded><![CDATA[
arXiv:2506.08670v1 Announce Type: cross 
Abstract: We propose sparseGeoHOPCA, a novel framework for sparse higher-order principal component analysis (SHOPCA) that introduces a geometric perspective to high-dimensional tensor decomposition. By unfolding the input tensor along each mode and reformulating the resulting subproblems as structured binary linear optimization problems, our method transforms the original nonconvex sparse objective into a tractable geometric form. This eliminates the need for explicit covariance estimation and iterative deflation, enabling significant gains in both computational efficiency and interpretability, particularly in high-dimensional and unbalanced data scenarios. We theoretically establish the equivalence between the geometric subproblems and the original SHOPCA formulation, and derive worst-case approximation error bounds based on classical PCA residuals, providing data-dependent performance guarantees. The proposed algorithm achieves a total computational complexity of $O\left(\sum_{n=1}^{N} (k_n^3 + J_n k_n^2)\right)$, which scales linearly with tensor size. Extensive experiments demonstrate that sparseGeoHOPCA accurately recovers sparse supports in synthetic settings, preserves classification performance under 10$\times$ compression, and achieves high-quality image reconstruction on ImageNet, highlighting its robustness and versatility.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConfPO: Exploiting Policy Model Confidence for Critical Token Selection in Large Language Model Preference Optimization</title>
<link>https://arxiv.org/abs/2506.08712</link>
<guid>https://arxiv.org/abs/2506.08712</guid>
<content:encoded><![CDATA[
arXiv:2506.08712v1 Announce Type: cross 
Abstract: We introduce ConfPO, a method for preference learning in Large Language Models (LLMs) that identifies and optimizes preference-critical tokens based solely on the training policy's confidence, without requiring any auxiliary models or compute. Unlike prior Direct Alignment Algorithms (DAAs) such as Direct Preference Optimization (DPO), which uniformly adjust all token probabilities regardless of their relevance to preference, ConfPO focuses optimization on the most impactful tokens. This targeted approach improves alignment quality while mitigating overoptimization (i.e., reward hacking) by using the KL divergence budget more efficiently. In contrast to recent token-level methods that rely on credit-assignment models or AI annotators, raising concerns about scalability and reliability, ConfPO is simple, lightweight, and model-free. Experimental results on challenging alignment benchmarks, including AlpacaEval 2 and Arena-Hard, demonstrate that ConfPO consistently outperforms uniform DAAs across various LLMs, delivering better alignment with zero additional computational overhead.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stop Misusing t-SNE and UMAP for Visual Analytics</title>
<link>https://arxiv.org/abs/2506.08725</link>
<guid>https://arxiv.org/abs/2506.08725</guid>
<content:encoded><![CDATA[
arXiv:2506.08725v1 Announce Type: cross 
Abstract: Misuses of t-SNE and UMAP in visual analytics have become increasingly common. For example, although t-SNE and UMAP projections often do not faithfully reflect true distances between clusters, practitioners frequently use them to investigate inter-cluster relationships. In this paper, we bring this issue to the surface and comprehensively investigate why such misuse occurs and how to prevent it. We conduct a literature review of 114 papers to verify the prevalence of the misuse and analyze the reasonings behind it. We then execute an interview study to uncover practitioners' implicit motivations for using these techniques -- rationales often undisclosed in the literature. Our findings indicate that misuse of t-SNE and UMAP primarily stems from limited discourse on their appropriate use in visual analytics. We conclude by proposing future directions and concrete action items to promote more reasonable use of DR.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flexible and Efficient Drift Detection without Labels</title>
<link>https://arxiv.org/abs/2506.08734</link>
<guid>https://arxiv.org/abs/2506.08734</guid>
<content:encoded><![CDATA[
arXiv:2506.08734v1 Announce Type: cross 
Abstract: Machine learning models are being increasingly used to automate decisions in almost every domain, and ensuring the performance of these models is crucial for ensuring high quality machine learning enabled services. Ensuring concept drift is detected early is thus of the highest importance. A lot of research on concept drift has focused on the supervised case that assumes the true labels of supervised tasks are available immediately after making predictions. Controlling for false positives while monitoring the performance of predictive models used to make inference from extremely large datasets periodically, where the true labels are not instantly available, becomes extremely challenging. We propose a flexible and efficient concept drift detection algorithm that uses classical statistical process control in a label-less setting to accurately detect concept drifts. We shown empirically that under computational constraints, our approach has better statistical power than previous known methods. Furthermore, we introduce a new drift detection framework to model the scenario of detecting drift (without labels) given prior detections, and show our how our drift detection algorithm can be incorporated effectively into this framework. We demonstrate promising performance via numerical simulations.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging RDF Knowledge Graphs with Graph Neural Networks for Semantically-Rich Recommender Systems</title>
<link>https://arxiv.org/abs/2506.08743</link>
<guid>https://arxiv.org/abs/2506.08743</guid>
<content:encoded><![CDATA[
arXiv:2506.08743v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) have substantially advanced the field of recommender systems. However, despite the creation of more than a thousand knowledge graphs (KGs) under the W3C standard RDF, their rich semantic information has not yet been fully leveraged in GNN-based recommender systems. To address this gap, we propose a comprehensive integration of RDF KGs with GNNs that utilizes both the topological information from RDF object properties and the content information from RDF datatype properties. Our main focus is an in-depth evaluation of various GNNs, analyzing how different semantic feature initializations and types of graph structure heterogeneity influence their performance in recommendation tasks. Through experiments across multiple recommendation scenarios involving multi-million-node RDF graphs, we demonstrate that harnessing the semantic richness of RDF KGs significantly improves recommender systems and lays the groundwork for GNN-based recommender systems for the Linked Open Data cloud. The code and data are available on our GitHub repository: https://github.com/davidlamprecht/rdf-gnn-recommendation
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Secure and Private Language Models for Nuclear Power Plants</title>
<link>https://arxiv.org/abs/2506.08746</link>
<guid>https://arxiv.org/abs/2506.08746</guid>
<content:encoded><![CDATA[
arXiv:2506.08746v1 Announce Type: cross 
Abstract: This paper introduces a domain-specific Large Language Model for nuclear applications, built from the publicly accessible Essential CANDU textbook. Drawing on a compact Transformer-based architecture, the model is trained on a single GPU to protect the sensitive data inherent in nuclear operations. Despite relying on a relatively small dataset, it shows encouraging signs of capturing specialized nuclear vocabulary, though the generated text sometimes lacks syntactic coherence. By focusing exclusively on nuclear content, this approach demonstrates the feasibility of in-house LLM solutions that align with rigorous cybersecurity and data confidentiality standards. Early successes in text generation underscore the model's utility for specialized tasks, while also revealing the need for richer corpora, more sophisticated preprocessing, and instruction fine-tuning to enhance domain accuracy. Future directions include extending the dataset to cover diverse nuclear subtopics, refining tokenization to reduce noise, and systematically evaluating the model's readiness for real-world applications in nuclear domain.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Superposed Parameterised Quantum Circuits</title>
<link>https://arxiv.org/abs/2506.08749</link>
<guid>https://arxiv.org/abs/2506.08749</guid>
<content:encoded><![CDATA[
arXiv:2506.08749v1 Announce Type: cross 
Abstract: Quantum machine learning has shown promise for high-dimensional data analysis, yet many existing approaches rely on linear unitary operations and shared trainable parameters across outputs. These constraints limit expressivity and scalability relative to the multi-layered, non-linear architectures of classical deep networks. We introduce superposed parameterised quantum circuits to overcome these limitations. By combining flip-flop quantum random-access memory with repeat-until-success protocols, a superposed parameterised quantum circuit embeds an exponential number of parameterised sub-models in a single circuit and induces polynomial activation functions through amplitude transformations and post-selection. We provide an analytic description of the architecture, showing how multiple parameter sets are trained in parallel while non-linear amplitude transformations broaden representational power beyond conventional quantum kernels. Numerical experiments underscore these advantages: on a 1D step-function regression a two-qubit superposed parameterised quantum circuit cuts the mean-squared error by three orders of magnitude versus a parameter-matched variational baseline; on a 2D star-shaped two-dimensional classification task, introducing a quadratic activation lifts accuracy to 81.4% and reduces run-to-run variance three-fold. These results position superposed parameterised quantum circuits as a hardware-efficient route toward deeper, more versatile parameterised quantum circuits capable of learning complex decision boundaries.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Accuracy and Maintainability in Nuclear Plant Data Retrieval: A Function-Calling LLM Approach Over NL-to-SQL</title>
<link>https://arxiv.org/abs/2506.08757</link>
<guid>https://arxiv.org/abs/2506.08757</guid>
<content:encoded><![CDATA[
arXiv:2506.08757v1 Announce Type: cross 
Abstract: Retrieving operational data from nuclear power plants requires exceptional accuracy and transparency due to the criticality of the decisions it supports. Traditionally, natural language to SQL (NL-to-SQL) approaches have been explored for querying such data. While NL-to-SQL promises ease of use, it poses significant risks: end-users cannot easily validate generated SQL queries, and legacy nuclear plant databases -- often complex and poorly structured -- complicate query generation due to decades of incremental modifications. These challenges increase the likelihood of inaccuracies and reduce trust in the approach. In this work, we propose an alternative paradigm: leveraging function-calling large language models (LLMs) to address these challenges. Instead of directly generating SQL queries, we define a set of pre-approved, purpose-specific functions representing common use cases. Queries are processed by invoking these functions, which encapsulate validated SQL logic. This hybrid approach mitigates the risks associated with direct NL-to-SQL translations by ensuring that SQL queries are reviewed and optimized by experts before deployment. While this strategy introduces the upfront cost of developing and maintaining the function library, we demonstrate how NL-to-SQL tools can assist in the initial generation of function code, allowing experts to focus on validation rather than creation. Our study includes a performance comparison between direct NL-to-SQL generation and the proposed function-based approach, highlighting improvements in accuracy and maintainability. This work underscores the importance of balancing user accessibility with operational safety and provides a novel, actionable framework for robust data retrieval in critical systems.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EDINET-Bench: Evaluating LLMs on Complex Financial Tasks using Japanese Financial Statements</title>
<link>https://arxiv.org/abs/2506.08762</link>
<guid>https://arxiv.org/abs/2506.08762</guid>
<content:encoded><![CDATA[
arXiv:2506.08762v1 Announce Type: cross 
Abstract: Financial analysis presents complex challenges that could leverage large language model (LLM) capabilities. However, the scarcity of challenging financial datasets, particularly for Japanese financial data, impedes academic innovation in financial analytics. As LLMs advance, this lack of accessible research resources increasingly hinders their development and evaluation in this specialized domain. To address this gap, we introduce EDINET-Bench, an open-source Japanese financial benchmark designed to evaluate the performance of LLMs on challenging financial tasks including accounting fraud detection, earnings forecasting, and industry prediction. EDINET-Bench is constructed by downloading annual reports from the past 10 years from Japan's Electronic Disclosure for Investors' NETwork (EDINET) and automatically assigning labels corresponding to each evaluation task. Our experiments reveal that even state-of-the-art LLMs struggle, performing only slightly better than logistic regression in binary classification for fraud detection and earnings forecasting. These results highlight significant challenges in applying LLMs to real-world financial applications and underscore the need for domain-specific adaptation. Our dataset, benchmark construction code, and evaluation code is publicly available to facilitate future research in finance with LLMs.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Paths to Causality: Finding Informative Subgraphs Within Knowledge Graphs for Knowledge-Based Causal Discovery</title>
<link>https://arxiv.org/abs/2506.08771</link>
<guid>https://arxiv.org/abs/2506.08771</guid>
<content:encoded><![CDATA[
arXiv:2506.08771v1 Announce Type: cross 
Abstract: Inferring causal relationships between variable pairs is crucial for understanding multivariate interactions in complex systems. Knowledge-based causal discovery -- which involves inferring causal relationships by reasoning over the metadata of variables (e.g., names or textual context) -- offers a compelling alternative to traditional methods that rely on observational data. However, existing methods using Large Language Models (LLMs) often produce unstable and inconsistent results, compromising their reliability for causal inference. To address this, we introduce a novel approach that integrates Knowledge Graphs (KGs) with LLMs to enhance knowledge-based causal discovery. Our approach identifies informative metapath-based subgraphs within KGs and further refines the selection of these subgraphs using Learning-to-Rank-based models. The top-ranked subgraphs are then incorporated into zero-shot prompts, improving the effectiveness of LLMs in inferring the causal relationship. Extensive experiments on biomedical and open-domain datasets demonstrate that our method outperforms most baselines by up to 44.4 points in F1 scores, evaluated across diverse LLMs and KGs. Our code and datasets are available on GitHub: https://github.com/susantiyuni/path-to-causality
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Landsat-Bench: Datasets and Benchmarks for Landsat Foundation Models</title>
<link>https://arxiv.org/abs/2506.08780</link>
<guid>https://arxiv.org/abs/2506.08780</guid>
<content:encoded><![CDATA[
arXiv:2506.08780v1 Announce Type: cross 
Abstract: The Landsat program offers over 50 years of globally consistent Earth imagery. However, the lack of benchmarks for this data constrains progress towards Landsat-based Geospatial Foundation Models (GFM). In this paper, we introduce Landsat-Bench, a suite of three benchmarks with Landsat imagery that adapt from existing remote sensing datasets -- EuroSAT-L, BigEarthNet-L, and LC100-L. We establish baseline and standardized evaluation methods across both common architectures and Landsat foundation models pretrained on the SSL4EO-L dataset. Notably, we provide evidence that SSL4EO-L pretrained GFMs extract better representations for downstream tasks in comparison to ImageNet, including performance gains of +4% OA and +5.1% mAP on EuroSAT-L and BigEarthNet-L.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>syren-baryon: Analytic emulators for the impact of baryons on the matter power spectrum</title>
<link>https://arxiv.org/abs/2506.08783</link>
<guid>https://arxiv.org/abs/2506.08783</guid>
<content:encoded><![CDATA[
arXiv:2506.08783v1 Announce Type: cross 
Abstract: Baryonic physics has a considerable impact on the distribution of matter in our Universe on scales probed by current and future cosmological surveys, acting as a key systematic in such analyses. We seek simple symbolic parametrisations for the impact of baryonic physics on the matter power spectrum for a range of physically motivated models, as a function of wavenumber, redshift, cosmology, and parameters controlling the baryonic feedback. We use symbolic regression to construct analytic approximations for the ratio of the matter power spectrum in the presence of baryons to that without such effects. We obtain separate functions of each of four distinct sub-grid prescriptions of baryonic physics from the CAMELS suite of hydrodynamical simulations (Astrid, IllustrisTNG, SIMBA and Swift-EAGLE) as well as for a baryonification algorithm. We also provide functions which describe the uncertainty on these predictions, due to both the stochastic nature of baryonic physics and the errors on our fits. The error on our approximations to the hydrodynamical simulations is comparable to the sample variance estimated through varying initial conditions, and our baryonification expression has a root mean squared error of better than one percent, although this increases on small scales. These errors are comparable to those of previous numerical emulators for these models. Our expressions are enforced to have the physically correct behaviour on large scales and at high redshift. Due to their analytic form, we are able to directly interpret the impact of varying cosmology and feedback parameters, and we can identify parameters which have little to no effect. Each function is based on a different implementation of baryonic physics, and can therefore be used to discriminate between these models when applied to real data. We provide publicly available code for all symbolic approximations found.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On The Impact of Merge Request Deviations on Code Review Practices</title>
<link>https://arxiv.org/abs/2506.08860</link>
<guid>https://arxiv.org/abs/2506.08860</guid>
<content:encoded><![CDATA[
arXiv:2506.08860v1 Announce Type: cross 
Abstract: Code review is a key practice in software engineering, ensuring quality and collaboration. However, industrial Merge Request (MR) workflows often deviate from standardized review processes, with many MRs serving non-review purposes (e.g., drafts, rebases, or dependency updates). We term these cases deviations and hypothesize that ignoring them biases analytics and undermines ML models for review analysis.
  We identify seven deviation categories, occurring in 37.02% of MRs, and propose a few-shot learning detection method (91% accuracy). By excluding deviations, ML models predicting review completion time improve performance in 53.33% of cases (up to 2.25x) and exhibit significant shifts in feature importance (47% overall, 60% top-*k*).
  Our contributions include: (1) a taxonomy of MR deviations, (2) an AI-driven detection approach, and (3) empirical evidence of their impact on ML-based review analytics. This work aids practitioners in optimizing review efforts and ensuring reliable insights.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated Video Streams</title>
<link>https://arxiv.org/abs/2506.08862</link>
<guid>https://arxiv.org/abs/2506.08862</guid>
<content:encoded><![CDATA[
arXiv:2506.08862v1 Announce Type: cross 
Abstract: Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams is crucial for numerous real-world applications. However, existing methods struggle to jointly address three key challenges: 1) processing uncalibrated inputs in real time, 2) accurately modeling dynamic scene evolution, and 3) maintaining long-term stability and computational efficiency. To this end, we introduce StreamSplat, the first fully feed-forward framework that transforms uncalibrated video streams of arbitrary length into dynamic 3D Gaussian Splatting (3DGS) representations in an online manner, capable of recovering scene dynamics from temporally local observations. We propose two key technical innovations: a probabilistic sampling mechanism in the static encoder for 3DGS position prediction, and a bidirectional deformation field in the dynamic decoder that enables robust and efficient dynamic modeling. Extensive experiments on static and dynamic benchmarks demonstrate that StreamSplat consistently outperforms prior works in both reconstruction quality and dynamic scene modeling, while uniquely supporting online reconstruction of arbitrarily long video streams. Code and models are available at https://github.com/nickwzk/StreamSplat.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdversariaL attacK sAfety aLIgnment(ALKALI): Safeguarding LLMs through GRACE: Geometric Representation-Aware Contrastive Enhancement- Introducing Adversarial Vulnerability Quality Index (AVQI)</title>
<link>https://arxiv.org/abs/2506.08885</link>
<guid>https://arxiv.org/abs/2506.08885</guid>
<content:encoded><![CDATA[
arXiv:2506.08885v1 Announce Type: cross 
Abstract: Adversarial threats against LLMs are escalating faster than current defenses can adapt. We expose a critical geometric blind spot in alignment: adversarial prompts exploit latent camouflage, embedding perilously close to the safe representation manifold while encoding unsafe intent thereby evading surface level defenses like Direct Preference Optimization (DPO), which remain blind to the latent geometry. We introduce ALKALI, the first rigorously curated adversarial benchmark and the most comprehensive to date spanning 9,000 prompts across three macro categories, six subtypes, and fifteen attack families. Evaluation of 21 leading LLMs reveals alarmingly high Attack Success Rates (ASRs) across both open and closed source models, exposing an underlying vulnerability we term latent camouflage, a structural blind spot where adversarial completions mimic the latent geometry of safe ones. To mitigate this vulnerability, we introduce GRACE - Geometric Representation Aware Contrastive Enhancement, an alignment framework coupling preference learning with latent space regularization. GRACE enforces two constraints: latent separation between safe and adversarial completions, and adversarial cohesion among unsafe and jailbreak behaviors. These operate over layerwise pooled embeddings guided by a learned attention profile, reshaping internal geometry without modifying the base model, and achieve up to 39% ASR reduction. Moreover, we introduce AVQI, a geometry aware metric that quantifies latent alignment failure via cluster separation and compactness. AVQI reveals when unsafe completions mimic the geometry of safe ones, offering a principled lens into how models internally encode safety. We make the code publicly available at https://anonymous.4open.science/r/alkali-B416/README.md.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time Cascade Mitigation in Power Systems Using Influence Graph Improved by Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.08893</link>
<guid>https://arxiv.org/abs/2506.08893</guid>
<content:encoded><![CDATA[
arXiv:2506.08893v1 Announce Type: cross 
Abstract: Despite high reliability, modern power systems with growing renewable penetration face an increasing risk of cascading outages. Real-time cascade mitigation requires fast, complex operational decisions under uncertainty. In this work, we extend the influence graph into a Markov decision process model (MDP) for real-time mitigation of cascading outages in power transmission systems, accounting for uncertainties in generation, load, and initial contingencies. The MDP includes a do-nothing action to allow for conservative decision-making and is solved using reinforcement learning. We present a policy gradient learning algorithm initialized with a policy corresponding to the unmitigated case and designed to handle invalid actions. The proposed learning method converges faster than the conventional algorithm. Through careful reward design, we learn a policy that takes conservative actions without deteriorating system conditions. The model is validated on the IEEE 14-bus and IEEE 118-bus systems. The results show that proactive line disconnections can effectively reduce cascading risk, and certain lines consistently emerge as critical in mitigating cascade propagation.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Implementing Keyword Spotting on the MCUX947 Microcontroller with Integrated NPU</title>
<link>https://arxiv.org/abs/2506.08911</link>
<guid>https://arxiv.org/abs/2506.08911</guid>
<content:encoded><![CDATA[
arXiv:2506.08911v1 Announce Type: cross 
Abstract: This paper presents a keyword spotting (KWS) system implemented on the NXP MCXN947 microcontroller with an integrated Neural Processing Unit (NPU), enabling real-time voice interaction on resource-constrained devices. The system combines MFCC feature extraction with a CNN classifier, optimized using Quantization Aware Training to reduce model size with minimal accuracy drop. Experimental results demonstrate a 59x speedup in inference time when leveraging the NPU compared to CPU-only execution, achieving 97.06% accuracy with a model size of 30.58 KB, demonstrating the feasibility of efficient, low-power voice interfaces on embedded platforms.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PropMEND: Hypernetworks for Knowledge Propagation in LLMs</title>
<link>https://arxiv.org/abs/2506.08920</link>
<guid>https://arxiv.org/abs/2506.08920</guid>
<content:encoded><![CDATA[
arXiv:2506.08920v1 Announce Type: cross 
Abstract: Knowledge editing techniques for large language models (LLMs) can inject knowledge that is later reproducible verbatim, but they fall short on propagating that knowledge: models cannot answer questions that require reasoning with the injected knowledge. We present a hypernetwork-based approach for knowledge propagation, named PropMEND, where we meta-learn how to modify gradients of a language modeling loss to encourage injected information to propagate. Our approach extends the meta-objective of MEND [29] so that gradient updates on knowledge are transformed to enable answering multi-hop questions involving that knowledge. We show improved performance on the RippleEdit dataset, showing almost 2x accuracy on challenging multi-hop questions whose answers are not explicitly stated in the injected fact. We further introduce a new dataset, Controlled RippleEdit, to evaluate the generalization of our hypernetwork, testing knowledge propagation along relations and entities unseen during hypernetwork training. PropMEND still outperforms existing approaches in unseen entity-relation pairs, yet the performance gap decreases substantially, suggesting future work in propagating knowledge to a wide range of relations.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can A Gamer Train A Mathematical Reasoning Model?</title>
<link>https://arxiv.org/abs/2506.08935</link>
<guid>https://arxiv.org/abs/2506.08935</guid>
<content:encoded><![CDATA[
arXiv:2506.08935v1 Announce Type: cross 
Abstract: While large language models (LLMs) have achieved remarkable performance in various tasks including mathematical reasoning, their development typically demands prohibitive computational resources. Recent advancements have reduced costs for training capable models, yet even these approaches rely on high-end hardware clusters. In this paper, we demonstrate that a single average gaming GPU can train a solid mathematical reasoning model, by integrating reinforcement learning and memory optimization techniques. Specifically, we train a 1.5B parameter mathematical reasoning model on RTX 3080 Ti of 16GB memory that achieves comparable or better performance on mathematical reasoning benchmarks than models several times larger, in resource-constrained environments. Our results challenge the paradigm that state-of-the-art mathematical reasoning necessitates massive infrastructure, democratizing access to high-performance AI research. https://github.com/shinandrew/YouronMath.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Protriever: End-to-End Differentiable Protein Homology Search for Fitness Prediction</title>
<link>https://arxiv.org/abs/2506.08954</link>
<guid>https://arxiv.org/abs/2506.08954</guid>
<content:encoded><![CDATA[
arXiv:2506.08954v1 Announce Type: cross 
Abstract: Retrieving homologous protein sequences is essential for a broad range of protein modeling tasks such as fitness prediction, protein design, structure modeling, and protein-protein interactions. Traditional workflows have relied on a two-step process: first retrieving homologs via Multiple Sequence Alignments (MSA), then training models on one or more of these alignments. However, MSA-based retrieval is computationally expensive, struggles with highly divergent sequences or complex insertions & deletions patterns, and operates independently of the downstream modeling objective. We introduce Protriever, an end-to-end differentiable framework that learns to retrieve relevant homologs while simultaneously training for the target task. When applied to protein fitness prediction, Protriever achieves state-of-the-art performance compared to sequence-based models that rely on MSA-based homolog retrieval, while being two orders of magnitude faster through efficient vector search. Protriever is both architecture- and task-agnostic, and can flexibly adapt to different retrieval strategies and protein databases at inference time -- offering a scalable alternative to alignment-centric approaches.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Segment Concealed Objects with Incomplete Supervision</title>
<link>https://arxiv.org/abs/2506.08955</link>
<guid>https://arxiv.org/abs/2506.08955</guid>
<content:encoded><![CDATA[
arXiv:2506.08955v1 Announce Type: cross 
Abstract: Incompletely-Supervised Concealed Object Segmentation (ISCOS) involves segmenting objects that seamlessly blend into their surrounding environments, utilizing incompletely annotated data, such as weak and semi-annotations, for model training. This task remains highly challenging due to (1) the limited supervision provided by the incompletely annotated training data, and (2) the difficulty of distinguishing concealed objects from the background, which arises from the intrinsic similarities in concealed scenarios. In this paper, we introduce the first unified method for ISCOS to address these challenges. To tackle the issue of incomplete supervision, we propose a unified mean-teacher framework, SEE, that leverages the vision foundation model, ``\emph{Segment Anything Model (SAM)}'', to generate pseudo-labels using coarse masks produced by the teacher model as prompts. To mitigate the effect of low-quality segmentation masks, we introduce a series of strategies for pseudo-label generation, storage, and supervision. These strategies aim to produce informative pseudo-labels, store the best pseudo-labels generated, and select the most reliable components to guide the student model, thereby ensuring robust network training. Additionally, to tackle the issue of intrinsic similarity, we design a hybrid-granularity feature grouping module that groups features at different granularities and aggregates these results. By clustering similar features, this module promotes segmentation coherence, facilitating more complete segmentation for both single-object and multiple-object images. We validate the effectiveness of our approach across multiple ISCOS tasks, and experimental results demonstrate that our method achieves state-of-the-art performance. Furthermore, SEE can serve as a plug-and-play solution, enhancing the performance of existing models.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Augmentation For Small Object using Fast AutoAugment</title>
<link>https://arxiv.org/abs/2506.08956</link>
<guid>https://arxiv.org/abs/2506.08956</guid>
<content:encoded><![CDATA[
arXiv:2506.08956v1 Announce Type: cross 
Abstract: In recent years, there has been tremendous progress in object detection performance. However, despite these advances, the detection performance for small objects is significantly inferior to that of large objects. Detecting small objects is one of the most challenging and important problems in computer vision. To improve the detection performance for small objects, we propose an optimal data augmentation method using Fast AutoAugment. Through our proposed method, we can quickly find optimal augmentation policies that can overcome degradation when detecting small objects, and we achieve a 20% performance improvement on the DOTA dataset.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IntTrajSim: Trajectory Prediction for Simulating Multi-Vehicle driving at Signalized Intersections</title>
<link>https://arxiv.org/abs/2506.08957</link>
<guid>https://arxiv.org/abs/2506.08957</guid>
<content:encoded><![CDATA[
arXiv:2506.08957v1 Announce Type: cross 
Abstract: Traffic simulators are widely used to study the operational efficiency of road infrastructure, but their rule-based approach limits their ability to mimic real-world driving behavior. Traffic intersections are critical components of the road infrastructure, both in terms of safety risk (nearly 28% of fatal crashes and 58% of nonfatal crashes happen at intersections) as well as the operational efficiency of a road corridor. This raises an important question: can we create a data-driven simulator that can mimic the macro- and micro-statistics of the driving behavior at a traffic intersection? Deep Generative Modeling-based trajectory prediction models provide a good starting point to model the complex dynamics of vehicles at an intersection. But they are not tested in a "live" micro-simulation scenario and are not evaluated on traffic engineering-related metrics. In this study, we propose traffic engineering-related metrics to evaluate generative trajectory prediction models and provide a simulation-in-the-loop pipeline to do so. We also provide a multi-headed self-attention-based trajectory prediction model that incorporates the signal information, which outperforms our previous models on the evaluation metrics.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers</title>
<link>https://arxiv.org/abs/2506.08966</link>
<guid>https://arxiv.org/abs/2506.08966</guid>
<content:encoded><![CDATA[
arXiv:2506.08966v1 Announce Type: cross 
Abstract: Pretrained language models (LMs) are prone to arithmetic errors. Existing work showed limited success in probing numeric values from models' representations, indicating that these errors can be attributed to the inherent unreliability of distributionally learned embeddings in representing exact quantities. However, we observe that previous probing methods are inadequate for the emergent structure of learned number embeddings with sinusoidal patterns.
  In response, we propose a novel probing technique that decodes numeric values from input embeddings with near-perfect accuracy across a range of open-source LMs. This proves that after the sole pre-training, LMs represent numbers with remarkable precision. Finally, we find that the embeddings' preciseness judged by our probe's accuracy explains a large portion of LM's errors in elementary arithmetic, and show that aligning the embeddings with the pattern discovered by our probe can mitigate these errors.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Medical Vision-Language Alignment Through Adapting Masked Vision Models</title>
<link>https://arxiv.org/abs/2506.08990</link>
<guid>https://arxiv.org/abs/2506.08990</guid>
<content:encoded><![CDATA[
arXiv:2506.08990v1 Announce Type: cross 
Abstract: Medical vision-language alignment through cross-modal contrastive learning shows promising performance in image-text matching tasks, such as retrieval and zero-shot classification. However, conventional cross-modal contrastive learning (CLIP-based) methods suffer from suboptimal visual representation capabilities, which also limits their effectiveness in vision-language alignment. In contrast, although the models pretrained via multimodal masked modeling struggle with direct cross-modal matching, they excel in visual representation. To address this contradiction, we propose ALTA (ALign Through Adapting), an efficient medical vision-language alignment method that utilizes only about 8% of the trainable parameters and less than 1/5 of the computational consumption required for masked record modeling. ALTA achieves superior performance in vision-language matching tasks like retrieval and zero-shot classification by adapting the pretrained vision model from masked record modeling. Additionally, we integrate temporal-multiview radiograph inputs to enhance the information consistency between radiographs and their corresponding descriptions in reports, further improving the vision-language alignment. Experimental evaluations show that ALTA outperforms the best-performing counterpart by over 4% absolute points in text-to-image accuracy and approximately 6% absolute points in image-to-text retrieval accuracy. The adaptation of vision-language models during efficient alignment also promotes better vision and language understanding. Code is publicly available at https://github.com/DopamineLcy/ALTA.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DIsoN: Decentralized Isolation Networks for Out-of-Distribution Detection in Medical Imaging</title>
<link>https://arxiv.org/abs/2506.09024</link>
<guid>https://arxiv.org/abs/2506.09024</guid>
<content:encoded><![CDATA[
arXiv:2506.09024v1 Announce Type: cross 
Abstract: Safe deployment of machine learning (ML) models in safety-critical domains such as medical imaging requires detecting inputs with characteristics not seen during training, known as out-of-distribution (OOD) detection, to prevent unreliable predictions. Effective OOD detection after deployment could benefit from access to the training data, enabling direct comparison between test samples and the training data distribution to identify differences. State-of-the-art OOD detection methods, however, either discard training data after deployment or assume that test samples and training data are centrally stored together, an assumption that rarely holds in real-world settings. This is because shipping training data with the deployed model is usually impossible due to the size of training databases, as well as proprietary or privacy constraints. We introduce the Isolation Network, an OOD detection framework that quantifies the difficulty of separating a target test sample from the training data by solving a binary classification task. We then propose Decentralized Isolation Networks (DIsoN), which enables the comparison of training and test data when data-sharing is impossible, by exchanging only model parameters between the remote computational nodes of training and deployment. We further extend DIsoN with class-conditioning, comparing a target sample solely with training data of its predicted class. We evaluate DIsoN on four medical imaging datasets (dermatology, chest X-ray, breast ultrasound, histopathology) across 12 OOD detection tasks. DIsoN performs favorably against existing methods while respecting data-privacy. This decentralized OOD detection framework opens the way for a new type of service that ML developers could provide along with their models: providing remote, secure utilization of their training data for OOD detection services. Code will be available upon acceptance at: *****
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffuse and Disperse: Image Generation with Representation Regularization</title>
<link>https://arxiv.org/abs/2506.09027</link>
<guid>https://arxiv.org/abs/2506.09027</guid>
<content:encoded><![CDATA[
arXiv:2506.09027v1 Announce Type: cross 
Abstract: The development of diffusion-based generative models over the past decade has largely proceeded independently of progress in representation learning. These diffusion models typically rely on regression-based objectives and generally lack explicit regularization. In this work, we propose \textit{Dispersive Loss}, a simple plug-and-play regularizer that effectively improves diffusion-based generative models. Our loss function encourages internal representations to disperse in the hidden space, analogous to contrastive self-supervised learning, with the key distinction that it requires no positive sample pairs and therefore does not interfere with the sampling process used for regression. Compared to the recent method of representation alignment (REPA), our approach is self-contained and minimalist, requiring no pre-training, no additional parameters, and no external data. We evaluate Dispersive Loss on the ImageNet dataset across a range of models and report consistent improvements over widely used and strong baselines. We hope our work will help bridge the gap between generative modeling and representation learning.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.09033</link>
<guid>https://arxiv.org/abs/2506.09033</guid>
<content:encoded><![CDATA[
arXiv:2506.09033v1 Announce Type: cross 
Abstract: The rapid emergence of diverse large language models (LLMs) has spurred the development of LLM routers that assign user queries to the most suitable model. However, existing LLM routers typically perform a single-round, one-to-one mapping (\textit{i.e.}, assigning each query to a single model in isolation), which limits their capability to tackle complex tasks that demand the complementary strengths of multiple LLMs. In this paper, we present \textbf{Router-R1}, a reinforcement learning (RL)-based framework that formulates multi-LLM routing and aggregation as a sequential decision process. Router-R1 instantiates the router itself as a capable LLM, leveraging its reasoning ability to interleave "think" actions (internal deliberation) with "route" actions (dynamic model invocation), and integrates each response into its evolving context. To guide learning, we employ a lightweight rule-based reward comprising format rewards, final outcome rewards, and a novel cost reward for performance and cost trade-off optimization, opening a pathway toward optimizing performance-cost tradeoffs via RL. Router-R1 also conditions only on simple model descriptors such as pricing, latency, and example performance, enabling strong generalization to unseen model selection. Experiments on seven general and multi-hop QA benchmarks show that Router-R1 outperforms over several strong baselines, achieving superior performance while maintaining robust generalization and cost management.Code is available at https://github.com/ulab-uiuc/Router-R1.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Refiner: Data Refining against Gradient Leakage Attacks in Federated Learning</title>
<link>https://arxiv.org/abs/2212.02042</link>
<guid>https://arxiv.org/abs/2212.02042</guid>
<content:encoded><![CDATA[
arXiv:2212.02042v3 Announce Type: replace 
Abstract: Recent works have brought attention to the vulnerability of Federated Learning (FL) systems to gradient leakage attacks. Such attacks exploit clients' uploaded gradients to reconstruct their sensitive data, thereby compromising the privacy protection capability of FL. In response, various defense mechanisms have been proposed to mitigate this threat by manipulating the uploaded gradients. Unfortunately, empirical evaluations have demonstrated limited resilience of these defenses against sophisticated attacks, indicating an urgent need for more effective defenses. In this paper, we explore a novel defensive paradigm that departs from conventional gradient perturbation approaches and instead focuses on the construction of robust data. Intuitively, if robust data exhibits low semantic similarity with clients' raw data, the gradients associated with robust data can effectively obfuscate attackers. To this end, we design Refiner that jointly optimizes two metrics for privacy protection and performance maintenance. The utility metric is designed to promote consistency between the gradients of key parameters associated with robust data and those derived from clients' data, thus maintaining model performance. Furthermore, the privacy metric guides the generation of robust data towards enlarging the semantic gap with clients' data. Theoretical analysis supports the effectiveness of Refiner, and empirical evaluations on multiple benchmark datasets demonstrate the superior defense effectiveness of Refiner at defending against state-of-the-art attacks.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalization analysis of an unfolding network for analysis-based Compressed Sensing</title>
<link>https://arxiv.org/abs/2303.05582</link>
<guid>https://arxiv.org/abs/2303.05582</guid>
<content:encoded><![CDATA[
arXiv:2303.05582v3 Announce Type: replace 
Abstract: Unfolding networks have shown promising results in the Compressed Sensing (CS) field. Yet, the investigation of their generalization ability is still in its infancy. In this paper, we perform a generalization analysis of a state-of-the-art ADMM-based unfolding network, which jointly learns a decoder for CS and a sparsifying redundant analysis operator. To this end, we first impose a structural constraint on the learnable sparsifier, which parametrizes the network's hypothesis class. For the latter, we estimate its Rademacher complexity. With this estimate in hand, we deliver generalization error bounds -- which scale like the square root of the number of layers -- for the examined network. Finally, the validity of our theory is assessed and numerical comparisons to a state-of-the-art unfolding network are made, on synthetic and real-world datasets. Our experimental results demonstrate that our proposed framework complies with our theoretical findings and outperforms the baseline, consistently for all datasets.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating fairwashing using Two-Source Audits</title>
<link>https://arxiv.org/abs/2305.13883</link>
<guid>https://arxiv.org/abs/2305.13883</guid>
<content:encoded><![CDATA[
arXiv:2305.13883v2 Announce Type: replace 
Abstract: Recent legislation requires online platforms to provide dedicated APIs to assess the compliance of their decision-making algorithms with the law. Research has nevertheless shown that the auditors of such platforms are prone to manipulation (a practice referred to as \textit{fairwashing}). To address this salient problem, recent work has considered audits under the assumption of partial knowledge of the platform's internal mechanisms. In this paper, we propose a more pragmatic approach with the \textit{Two-Source Audit} setup: while still leveraging the API, we advocate for the adjunction of a second source of data to both perform the audit of a platform and the detection of fairwashing attempts. Our method is based on identifying discrepancies between the two data sources, using data proxies at use in the fairness literature. We formally demonstrate the conditions for success in this fairwashing mitigation task. We then validate our method empirically, demonstrating that Two-Source Audits can achieve a Pareto-optimal balance between the two objectives. We believe this paper sets the stage for reliable audits in manipulation-prone setups, under mild assumptions.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stabilizing Contrastive RL: Techniques for Robotic Goal Reaching from Offline Data</title>
<link>https://arxiv.org/abs/2306.03346</link>
<guid>https://arxiv.org/abs/2306.03346</guid>
<content:encoded><![CDATA[
arXiv:2306.03346v3 Announce Type: replace 
Abstract: Robotic systems that rely primarily on self-supervised learning have the potential to decrease the amount of human annotation and engineering effort required to learn control strategies. In the same way that prior robotic systems have leveraged self-supervised techniques from computer vision (CV) and natural language processing (NLP), our work builds on prior work showing that the reinforcement learning (RL) itself can be cast as a self-supervised problem: learning to reach any goal without human-specified rewards or labels. Despite the seeming appeal, little (if any) prior work has demonstrated how self-supervised RL methods can be practically deployed on robotic systems. By first studying a challenging simulated version of this task, we discover design decisions about architectures and hyperparameters that increase the success rate by $2 \times$. These findings lay the groundwork for our main result: we demonstrate that a self-supervised RL algorithm based on contrastive learning can solve real-world, image-based robotic manipulation tasks, with tasks being specified by a single goal image provided after training.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Provably Cost-Sensitive Adversarial Defense via Randomized Smoothing</title>
<link>https://arxiv.org/abs/2310.08732</link>
<guid>https://arxiv.org/abs/2310.08732</guid>
<content:encoded><![CDATA[
arXiv:2310.08732v3 Announce Type: replace 
Abstract: As ML models are increasingly deployed in critical applications, robustness against adversarial perturbations is crucial. While numerous defenses have been proposed to counter such attacks, they typically assume that all adversarial transformations are equally important, an assumption that rarely aligns with real-world applications. To address this, we study the problem of robust learning against adversarial perturbations under cost-sensitive scenarios, where the potential harm of different types of misclassifications is encoded in a cost matrix. Our solution introduces a provably robust learning algorithm to certify and optimize for cost-sensitive robustness, building on the scalable certification framework of randomized smoothing. Specifically, we formalize the definition of cost-sensitive certified radius and propose our novel adaptation of the standard certification algorithm to generate tight robustness certificates tailored to any cost matrix. In addition, we design a robust training method that improves certified cost-sensitive robustness without compromising model accuracy. Extensive experiments on benchmark datasets, including challenging ones unsolvable by existing methods, demonstrate the effectiveness of our certification algorithm and training method across various cost-sensitive scenarios.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Innate-Values-driven Reinforcement Learning based Cooperative Multi-Agent Cognitive Modeling</title>
<link>https://arxiv.org/abs/2401.05572</link>
<guid>https://arxiv.org/abs/2401.05572</guid>
<content:encoded><![CDATA[
arXiv:2401.05572v2 Announce Type: replace 
Abstract: In multi-agent systems (MAS), the dynamic interaction among multiple decision-makers is driven by their innate values, affecting the environment's state, and can cause specific behavioral patterns to emerge. On the other hand, innate values in cognitive modeling reflect individual interests and preferences for specific tasks and drive them to develop diverse skills and plans, satisfying their various needs and achieving common goals in cooperation. Therefore, building the awareness of AI agents to balance the group utilities and system costs and meet group members' needs in their cooperation is a crucial problem for individuals learning to support their community and even integrate into human society in the long term. However, the current MAS reinforcement learning domain lacks a general intrinsic model to describe agents' dynamic motivation for decision-making and learning from an individual needs perspective in their cooperation. To address the gap, this paper proposes a general MAS innate-values reinforcement learning (IVRL) architecture from the individual preferences angle. We tested the Multi-Agent IVRL Actor-Critic Model in different StarCraft Multi-Agent Challenge (SMAC) settings, which demonstrated its potential to organize the group's behaviours to achieve better performance.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Spectral Training and Inference on Euclidean and Hyperbolic Neural Networks</title>
<link>https://arxiv.org/abs/2405.15481</link>
<guid>https://arxiv.org/abs/2405.15481</guid>
<content:encoded><![CDATA[
arXiv:2405.15481v2 Announce Type: replace 
Abstract: The growing demands on GPU memory posed by the increasing number of neural network parameters call for training approaches that are more memory-efficient. Previous memory reduction training techniques, such as Low-Rank Adaptation (LoRA) and ReLoRA, face challenges, with LoRA being constrained by its low-rank structure, particularly during intensive tasks like pre-training, and ReLoRA suffering from saddle point issues. In this paper, we propose Sparse Spectral Training (SST) to optimize memory usage for pre-training. SST updates all singular values and selectively updates singular vectors through a multinomial sampling method weighted by the magnitude of the singular values. Furthermore, SST employs singular value decomposition to initialize and periodically reinitialize low-rank parameters, reducing distortion relative to full-rank training compared to other low-rank methods. Through comprehensive testing on both Euclidean and hyperbolic neural networks across various tasks, SST demonstrates its ability to outperform existing memory reduction training methods and is comparable to full-rank training in various cases. On LLaMA-1.3B, with only 18.7\% of the parameters trainable compared to full-rank training (using a rank equivalent to 6\% of the embedding dimension), SST reduces the perplexity gap between other low-rank methods and full-rank training by 97.4\%. This result highlights SST as an effective parameter-efficient technique for model pre-training.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapting Prediction Sets to Distribution Shifts Without Labels</title>
<link>https://arxiv.org/abs/2406.01416</link>
<guid>https://arxiv.org/abs/2406.01416</guid>
<content:encoded><![CDATA[
arXiv:2406.01416v2 Announce Type: replace 
Abstract: Recently there has been a surge of interest to deploy confidence set predictions rather than point predictions in machine learning. Unfortunately, the effectiveness of such prediction sets is frequently impaired by distribution shifts in practice, and the challenge is often compounded by the lack of ground truth labels at test time. Focusing on a standard set-valued prediction framework called conformal prediction (CP), this paper studies how to improve its practical performance using only unlabeled data from the shifted test domain. This is achieved by two new methods called ECP and EACP, whose main idea is to adjust the score function in CP according to its base model's own uncertainty evaluation. Through extensive experiments on a number of large-scale datasets and neural network architectures, we show that our methods provide consistent improvement over existing baselines and nearly match the performance of fully supervised methods.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Hardness of Sampling from Mixture Distributions via Langevin Dynamics</title>
<link>https://arxiv.org/abs/2406.02017</link>
<guid>https://arxiv.org/abs/2406.02017</guid>
<content:encoded><![CDATA[
arXiv:2406.02017v3 Announce Type: replace 
Abstract: The Langevin Dynamics (LD), which aims to sample from a probability distribution using its score function, has been widely used for analyzing and developing score-based generative modeling algorithms. While the convergence behavior of LD in sampling from a uni-modal distribution has been extensively studied in the literature, the analysis of LD under a mixture distribution with distinct modes remains underexplored in the literature. In this work, we analyze LD in sampling from a mixture distribution and theoretically study its convergence properties. Our theoretical results indicate that for general mixture distributions of sub-Gaussian components, LD could fail in finding all the components within a sub-exponential number of steps in the data dimension. Following our result on the complexity of LD in sampling from high-dimensional variables, we propose Chained Langevin Dynamics (Chained-LD), which divides the data vector into patches of smaller sizes and generates every patch sequentially conditioned on the previous patches. Our theoretical analysis of Chained-LD indicates its faster convergence speed to the components of a mixture distribution. We present the results of several numerical experiments on synthetic and real image datasets, validating our theoretical results on the iteration complexities of sample generation from mixture distributions using the vanilla and chained LD algorithms.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Approximation-Aware Bayesian Optimization</title>
<link>https://arxiv.org/abs/2406.04308</link>
<guid>https://arxiv.org/abs/2406.04308</guid>
<content:encoded><![CDATA[
arXiv:2406.04308v2 Announce Type: replace 
Abstract: High-dimensional Bayesian optimization (BO) tasks such as molecular design often require 10,000 function evaluations before obtaining meaningful results. While methods like sparse variational Gaussian processes (SVGPs) reduce computational requirements in these settings, the underlying approximations result in suboptimal data acquisitions that slow the progress of optimization. In this paper we modify SVGPs to better align with the goals of BO: targeting informed data acquisition rather than global posterior fidelity. Using the framework of utility-calibrated variational inference, we unify GP approximation and data acquisition into a joint optimization problem, thereby ensuring optimal decisions under a limited computational budget. Our approach can be used with any decision-theoretic acquisition function and is compatible with trust region methods like TuRBO. We derive efficient joint objectives for the expected improvement and knowledge gradient acquisition functions in both the standard and batch BO settings. Our approach outperforms standard SVGPs on high-dimensional benchmark tasks in control and molecular design.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Laws in Linear Regression: Compute, Parameters, and Data</title>
<link>https://arxiv.org/abs/2406.08466</link>
<guid>https://arxiv.org/abs/2406.08466</guid>
<content:encoded><![CDATA[
arXiv:2406.08466v3 Announce Type: replace 
Abstract: Empirically, large-scale deep learning models often satisfy a neural scaling law: the test error of the trained model improves polynomially as the model size and data size grow. However, conventional wisdom suggests the test error consists of approximation, bias, and variance errors, where the variance error increases with model size. This disagrees with the general form of neural scaling laws, which predict that increasing model size monotonically improves performance.
  We study the theory of scaling laws in an infinite dimensional linear regression setup. Specifically, we consider a model with $M$ parameters as a linear function of sketched covariates. The model is trained by one-pass stochastic gradient descent (SGD) using $N$ data. Assuming the optimal parameter satisfies a Gaussian prior and the data covariance matrix has a power-law spectrum of degree $a>1$, we show that the reducible part of the test error is $\Theta(M^{-(a-1)} + N^{-(a-1)/a})$. The variance error, which increases with $M$, is dominated by the other errors due to the implicit regularization of SGD, thus disappearing from the bound. Our theory is consistent with the empirical neural scaling laws and verified by numerical simulation.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chip Placement with Diffusion Models</title>
<link>https://arxiv.org/abs/2407.12282</link>
<guid>https://arxiv.org/abs/2407.12282</guid>
<content:encoded><![CDATA[
arXiv:2407.12282v3 Announce Type: replace 
Abstract: Macro placement is a vital step in digital circuit design that defines the physical location of large collections of components, known as macros, on a 2D chip. Because key performance metrics of the chip are determined by the placement, optimizing it is crucial. Existing learning-based methods typically fall short because of their reliance on reinforcement learning (RL), which is slow and struggles to generalize, requiring online training on each new circuit. Instead, we train a diffusion model capable of placing new circuits zero-shot, using guided sampling in lieu of RL to optimize placement quality. To enable such models to train at scale, we designed a capable yet efficient architecture for the denoising model, and propose a novel algorithm to generate large synthetic datasets for pre-training. To allow zero-shot transfer to real circuits, we empirically study the design decisions of our dataset generation algorithm, and identify several key factors enabling generalization. When trained on our synthetic data, our models generate high-quality placements on unseen, realistic circuits, achieving competitive performance on placement benchmarks compared to state-of-the-art methods.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifiable Latent Bandits: Leveraging observational data for personalized decision-making</title>
<link>https://arxiv.org/abs/2407.16239</link>
<guid>https://arxiv.org/abs/2407.16239</guid>
<content:encoded><![CDATA[
arXiv:2407.16239v3 Announce Type: replace 
Abstract: For many decision-making tasks, such as precision medicine, historical data alone are insufficient to determine the right choice for a new problem instance or patient. Online algorithms like multi-armed bandits can find optimal personalized decisions but are notoriously sample-hungry. In practice, training a bandit for a new individual from scratch is often infeasible, as the number of trials required is larger than the practical number of decision points. Latent bandits offer rapid exploration and personalization beyond what context variables can reveal, provided that a latent variable model can be learned consistently. In this work, we propose an identifiable latent bandit framework that leads to optimal decision-making with a shorter exploration time than classical bandits by learning from historical records of decisions and outcomes. Our method is based on nonlinear independent component analysis that provably identifies representations from observational data sufficient to infer the optimal action in new bandit instances. We verify this strategy in simulated and semi-synthetic environments, showing substantial improvement over online and offline learning baselines when identifying conditions are satisfied.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Directed Exploration in Reinforcement Learning from Linear Temporal Logic</title>
<link>https://arxiv.org/abs/2408.09495</link>
<guid>https://arxiv.org/abs/2408.09495</guid>
<content:encoded><![CDATA[
arXiv:2408.09495v2 Announce Type: replace 
Abstract: Linear temporal logic (LTL) is a powerful language for task specification in reinforcement learning, as it allows describing objectives beyond the expressivity of conventional discounted return formulations. Nonetheless, recent works have shown that LTL formulas can be translated into a variable rewarding and discounting scheme, whose optimization produces a policy maximizing a lower bound on the probability of formula satisfaction. However, the synthesized reward signal remains fundamentally sparse, making exploration challenging. We aim to overcome this limitation, which can prevent current algorithms from scaling beyond low-dimensional, short-horizon problems. We show how better exploration can be achieved by further leveraging the LTL specification and casting its corresponding Limit Deterministic B\"uchi Automaton (LDBA) as a Markov reward process, thus enabling a form of high-level value estimation. By taking a Bayesian perspective over LDBA dynamics and proposing a suitable prior distribution, we show that the values estimated through this procedure can be treated as a shaping potential and mapped to informative intrinsic rewards. Empirically, we demonstrate applications of our method from tabular settings to high-dimensional continuous systems, which have so far represented a significant challenge for LTL-based reinforcement learning algorithms.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How transformers learn structured data: insights from hierarchical filtering</title>
<link>https://arxiv.org/abs/2408.15138</link>
<guid>https://arxiv.org/abs/2408.15138</guid>
<content:encoded><![CDATA[
arXiv:2408.15138v3 Announce Type: replace 
Abstract: Understanding the learning process and the embedded computation in transformers is becoming a central goal for the development of interpretable AI. In the present study, we introduce a hierarchical filtering procedure for data models of sequences on trees, allowing us to hand-tune the range of positional correlations in the data. Leveraging this controlled setting, we provide evidence that vanilla encoder-only transformers can approximate the exact inference algorithm when trained on root classification and masked language modeling tasks, and study how this computation is discovered and implemented. We find that correlations at larger distances, corresponding to increasing layers of the hierarchy, are sequentially included by the network during training. By comparing attention maps from models trained with varying degrees of filtering and by probing the different encoder levels, we find clear evidence of a reconstruction of correlations on successive length scales corresponding to the various levels of the hierarchy, which we relate to a plausible implementation of the exact inference algorithm within the same architecture.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Causal Information Bottleneck and Optimal Causal Variable Abstractions</title>
<link>https://arxiv.org/abs/2410.00535</link>
<guid>https://arxiv.org/abs/2410.00535</guid>
<content:encoded><![CDATA[
arXiv:2410.00535v4 Announce Type: replace 
Abstract: To effectively study complex causal systems, it is often useful to construct abstractions of parts of the system by discarding irrelevant details while preserving key features. The Information Bottleneck (IB) method is a widely used approach to construct variable abstractions by compressing random variables while retaining predictive power over a target variable. Traditional methods like IB are purely statistical and ignore underlying causal structures, making them ill-suited for causal tasks. We propose the Causal Information Bottleneck (CIB), a causal extension of the IB, which compresses a set of chosen variables while maintaining causal control over a target variable. This method produces abstractions of (sets of) variables which are causally interpretable, give us insight about the interactions between the abstracted variables and the target variable, and can be used when reasoning about interventions. We present experimental results demonstrating that the learned abstractions accurately capture causal relations as intended.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Positional Attention: Expressivity and Learnability of Algorithmic Computation</title>
<link>https://arxiv.org/abs/2410.01686</link>
<guid>https://arxiv.org/abs/2410.01686</guid>
<content:encoded><![CDATA[
arXiv:2410.01686v3 Announce Type: replace 
Abstract: There is a growing interest in the ability of neural networks to execute algorithmic tasks (e.g., arithmetic, summary statistics, and sorting). The goal of this work is to better understand the role of attention in Transformers for algorithmic execution. Its importance for algorithmic execution has been studied theoretically and empirically using parallel computational models. Notably, many parallel algorithms communicate between processors solely using positional information. Inspired by this observation, we investigate how Transformers can execute algorithms using positional attention, where attention weights depend exclusively on positional encodings. We prove that Transformers with positional attention (positional Transformers) maintain the same expressivity of parallel computational models, incurring a logarithmic depth cost relative to the input length. We analyze their in-distribution learnability and explore how parameter norms in positional attention affect sample complexity. Our results show that positional Transformers introduce a learning trade-off: while they exhibit better theoretical dependence on parameter norms, certain tasks may require more layers, which can, in turn, increase sample complexity. Finally, we empirically explore the out-of-distribution performance of positional Transformers and find that they perform well in tasks where their underlying algorithmic solution relies on positional information.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TPP-LLM: Modeling Temporal Point Processes by Efficiently Fine-Tuning Large Language Models</title>
<link>https://arxiv.org/abs/2410.02062</link>
<guid>https://arxiv.org/abs/2410.02062</guid>
<content:encoded><![CDATA[
arXiv:2410.02062v2 Announce Type: replace 
Abstract: Temporal point processes (TPPs) are widely used to model the timing and occurrence of events in domains such as social networks, transportation systems, and e-commerce. In this paper, we introduce TPP-LLM, a novel framework that integrates large language models (LLMs) with TPPs to capture both the semantic and temporal aspects of event sequences. Unlike traditional methods that rely on categorical event type representations, TPP-LLM directly utilizes the textual descriptions of event types, enabling the model to capture rich semantic information embedded in the text. While LLMs excel at understanding event semantics, they are less adept at capturing temporal patterns. To address this, TPP-LLM incorporates temporal embeddings and employs parameter-efficient fine-tuning (PEFT) methods to effectively learn temporal dynamics without extensive retraining. This approach improves both predictive accuracy and computational efficiency. Experimental results across diverse real-world datasets demonstrate that TPP-LLM outperforms state-of-the-art baselines in sequence modeling and event prediction, highlighting the benefits of combining LLMs with TPPs.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Variational Inference in Discrete VAEs using Error Correcting Codes</title>
<link>https://arxiv.org/abs/2410.07840</link>
<guid>https://arxiv.org/abs/2410.07840</guid>
<content:encoded><![CDATA[
arXiv:2410.07840v2 Announce Type: replace 
Abstract: Despite advances in deep probabilistic models, learning discrete latent representations remains challenging. This work introduces a novel method to improve inference in discrete Variational Autoencoders by reframing the inference problem through a generative perspective. We conceptualize the model as a communication system, and propose to leverage Error-Correcting Codes (ECCs) to introduce redundancy in latent representations, allowing the variational posterior to produce more accurate estimates and reduce the variational gap. We present a proof-of-concept using a Discrete Variational Autoencoder with binary latent variables and low-complexity repetition codes, extending it to a hierarchical structure for disentangling global and local data features. Our approach significantly improves generation quality, data reconstruction, and uncertainty calibration, outperforming the uncoded models even when trained with tighter bounds such as the Importance Weighted Autoencoder objective. We also outline the properties that ECCs should possess to be effectively utilized for improved discrete variational inference.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MF-LAL: Drug Compound Generation Using Multi-Fidelity Latent Space Active Learning</title>
<link>https://arxiv.org/abs/2410.11226</link>
<guid>https://arxiv.org/abs/2410.11226</guid>
<content:encoded><![CDATA[
arXiv:2410.11226v3 Announce Type: replace 
Abstract: Current generative models for drug discovery primarily use molecular docking as an oracle to guide the generation of active compounds. However, such models are often not useful in practice because even compounds with high docking scores do not consistently show real-world experimental activity. More accurate methods for activity prediction exist, such as molecular dynamics based binding free energy calculations, but they are too computationally expensive to use in a generative model. To address this challenge, we propose Multi-Fidelity Latent space Active Learning (MF-LAL), a generative modeling framework that integrates a set of oracles with varying cost-accuracy tradeoffs. Using active learning, we train a surrogate model for each oracle and use these surrogates to guide generation of compounds with high predicted activity. Unlike previous approaches that separately learn the surrogate model and generative model, MF-LAL combines the generative and multi-fidelity surrogate models into a single framework, allowing for more accurate activity prediction and higher quality samples. Our experiments on two disease-relevant proteins show that MF-LAL produces compounds with significantly better binding free energy scores than other single and multi-fidelity approaches (~50% improvement in mean binding free energy score). The code is available at https://github.com/Rose-STL-Lab/MF-LAL.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlickerFusion: Intra-trajectory Domain Generalizing Multi-Agent RL</title>
<link>https://arxiv.org/abs/2410.15876</link>
<guid>https://arxiv.org/abs/2410.15876</guid>
<content:encoded><![CDATA[
arXiv:2410.15876v4 Announce Type: replace 
Abstract: Multi-agent reinforcement learning has demonstrated significant potential in addressing complex cooperative tasks across various real-world applications. However, existing MARL approaches often rely on the restrictive assumption that the number of entities (e.g., agents, obstacles) remains constant between training and inference. This overlooks scenarios where entities are dynamically removed or added during the inference trajectory -- a common occurrence in real-world environments like search and rescue missions and dynamic combat situations. In this paper, we tackle the challenge of intra-trajectory dynamic entity composition under zero-shot out-of-domain (OOD) generalization, where such dynamic changes cannot be anticipated beforehand. Our empirical studies reveal that existing MARL methods suffer significant performance degradation and increased uncertainty in these scenarios. In response, we propose FlickerFusion, a novel OOD generalization method that acts as a universally applicable augmentation technique for MARL backbone methods. FlickerFusion stochastically drops out parts of the observation space, emulating being in-domain when inferenced OOD. The results show that FlickerFusion not only achieves superior inference rewards but also uniquely reduces uncertainty vis-\`a-vis the backbone, compared to existing methods. Benchmarks, implementations, and model weights are organized and open-sourced at flickerfusion305.github.io, accompanied by ample demo video renderings.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffLM: Controllable Synthetic Data Generation via Diffusion Language Models</title>
<link>https://arxiv.org/abs/2411.03250</link>
<guid>https://arxiv.org/abs/2411.03250</guid>
<content:encoded><![CDATA[
arXiv:2411.03250v2 Announce Type: replace 
Abstract: Recent advancements in large language models (LLMs) have significantly enhanced their knowledge and generative capabilities, leading to a surge of interest in leveraging LLMs for high-quality data synthesis. However, synthetic data generation via prompting LLMs remains challenging due to LLMs' limited understanding of target data distributions and the complexity of prompt engineering, especially for structured formatted data. To address these issues, we introduce DiffLM, a controllable data synthesis framework based on variational autoencoder (VAE), which further (1) leverages diffusion models to reserve more information of original distribution and format structure in the learned latent distribution and (2) decouples the learning of target distribution knowledge from the LLM's generative objectives via a plug-and-play latent feature injection module. As we observed significant discrepancies between the VAE's latent representations and the real data distribution, the latent diffusion module is introduced into our framework to learn a fully expressive latent distribution. Evaluations on seven real-world datasets with structured formatted data (i.e., Tabular, Code, and Tool data) demonstrate that DiffLM generates high-quality data, with performance on downstream tasks surpassing that of real data by 2%-7% in certain cases. Data and code are available at https://github.com/bytedance/DiffLM.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BioNeMo Framework: a modular, high-performance library for AI model development in drug discovery</title>
<link>https://arxiv.org/abs/2411.10548</link>
<guid>https://arxiv.org/abs/2411.10548</guid>
<content:encoded><![CDATA[
arXiv:2411.10548v2 Announce Type: replace 
Abstract: Artificial Intelligence models encoding biology and chemistry are opening new routes to high-throughput and high-quality in-silico drug development. However, their training increasingly relies on computational scale, with recent protein language models (pLM) training on hundreds of graphical processing units (GPUs). We introduce the BioNeMo Framework to facilitate the training of computational biology and chemistry AI models across hundreds of GPUs. Its modular design allows the integration of individual components, such as data loaders, into existing workflows and is open to community contributions. We detail technical features of the BioNeMo Framework through use cases such as pLM pre-training and fine-tuning. On 256 NVIDIA A100s, BioNeMo Framework trains a three billion parameter BERT-based pLM on over one trillion tokens in 4.2 days. The BioNeMo Framework is open-source and free for everyone to use.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Mechanistic Explanation of Diffusion Model Generalization</title>
<link>https://arxiv.org/abs/2411.19339</link>
<guid>https://arxiv.org/abs/2411.19339</guid>
<content:encoded><![CDATA[
arXiv:2411.19339v3 Announce Type: replace 
Abstract: We propose a simple, training-free mechanism which explains the generalization behaviour of diffusion models. By comparing pre-trained diffusion models to their theoretically optimal empirical counterparts, we identify a shared local inductive bias across a variety of network architectures. From this observation, we hypothesize that network denoisers generalize through localized denoising operations, as these operations approximate the training objective well over much of the training distribution. To validate our hypothesis, we introduce novel denoising algorithms which aggregate local empirical denoisers to replicate network behaviour. Comparing these algorithms to network denoisers across forward and reverse diffusion processes, our approach exhibits consistent visual similarity to neural network outputs, with lower mean squared error than previously proposed methods.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tight Lower Bounds and Improved Convergence in Performative Prediction</title>
<link>https://arxiv.org/abs/2412.03671</link>
<guid>https://arxiv.org/abs/2412.03671</guid>
<content:encoded><![CDATA[
arXiv:2412.03671v2 Announce Type: replace 
Abstract: Performative prediction is a framework accounting for the shift in the data distribution induced by the prediction of a model deployed in the real world. Ensuring rapid convergence to a stable solution where the data distribution remains the same after the model deployment is crucial, especially in evolving environments. This paper extends the Repeated Risk Minimization (RRM) framework by utilizing historical datasets from previous retraining snapshots, yielding a class of algorithms that we call Affine Risk Minimizers and enabling convergence to a performatively stable point for a broader class of problems. We introduce a new upper bound for methods that use only the final iteration of the dataset and prove for the first time the tightness of both this new bound and the previous existing bounds within the same regime. We also prove that utilizing historical datasets can surpass the lower bound for last iterate RRM, and empirically observe faster convergence to the stable point on various performative prediction benchmarks. We offer at the same time the first lower bound analysis for RRM within the class of Affine Risk Minimizers, quantifying the potential improvements in convergence speed that could be achieved with other variants in our framework.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRAM: Generalization in Deep RL with a Robust Adaptation Module</title>
<link>https://arxiv.org/abs/2412.04323</link>
<guid>https://arxiv.org/abs/2412.04323</guid>
<content:encoded><![CDATA[
arXiv:2412.04323v2 Announce Type: replace 
Abstract: The reliable deployment of deep reinforcement learning in real-world settings requires the ability to generalize across a variety of conditions, including both in-distribution scenarios seen during training as well as novel out-of-distribution scenarios. In this work, we present a framework for dynamics generalization in deep reinforcement learning that unifies these two distinct types of generalization within a single architecture. We introduce a robust adaptation module that provides a mechanism for identifying and reacting to both in-distribution and out-of-distribution environment dynamics, along with a joint training pipeline that combines the goals of in-distribution adaptation and out-of-distribution robustness. Our algorithm GRAM achieves strong generalization performance across in-distribution and out-of-distribution scenarios upon deployment, which we demonstrate through extensive simulation and hardware locomotion experiments on a quadruped robot.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CENTAUR: Bridging the Impossible Trinity of Privacy, Efficiency, and Performance in Privacy-Preserving Transformer Inference</title>
<link>https://arxiv.org/abs/2412.10652</link>
<guid>https://arxiv.org/abs/2412.10652</guid>
<content:encoded><![CDATA[
arXiv:2412.10652v2 Announce Type: replace 
Abstract: With the growing deployment of pre-trained models like Transformers on cloud platforms, privacy concerns about model parameters and inference data are intensifying. Existing Privacy-Preserving Transformer Inference (PPTI) frameworks face the "impossible trinity" of balancing privacy, efficiency, and performance: Secure Multi-Party Computation (SMPC)-based approaches ensure strong privacy but suffer from high computational overhead and performance losses; Conversely, permutation-based methods achieve near-plaintext efficiency and accuracy but compromise privacy by exposing sensitive model parameters and intermediate results. Bridging this gap with a single approach presents substantial challenges, motivating the introduction of CENTAUR, a groundbreaking PPTI framework that seamlessly integrates random permutations and SMPC to address the "impossible trinity". By designing efficient PPTI algorithms tailored to the structural properties of Transformer models, CENTAUR achieves an unprecedented balance among privacy, efficiency, and performance. Our experiments demonstrate CENTAUR's ability to resist diverse data reconstruction attacks, achieve plaintext-level inference accuracy, and boost inference speed by 5.0-30.4 times, unlocking new possibilities for secure and efficient AI deployment.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving the Noise Estimation of Latent Neural Stochastic Differential Equations</title>
<link>https://arxiv.org/abs/2412.17499</link>
<guid>https://arxiv.org/abs/2412.17499</guid>
<content:encoded><![CDATA[
arXiv:2412.17499v2 Announce Type: replace 
Abstract: Latent neural stochastic differential equations (SDEs) have recently emerged as a promising approach for learning generative models from stochastic time series data. However, they systematically underestimate the noise level inherent in such data, limiting their ability to capture stochastic dynamics accurately. We investigate this underestimation in detail and propose a straightforward solution: by including an explicit additional noise regularization in the loss function, we are able to learn a model that accurately captures the diffusion component of the data. We demonstrate our results on a conceptual model system that highlights the improved latent neural SDE's capability to model stochastic bistable dynamics.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation</title>
<link>https://arxiv.org/abs/2501.08617</link>
<guid>https://arxiv.org/abs/2501.08617</guid>
<content:encoded><![CDATA[
arXiv:2501.08617v3 Announce Type: replace 
Abstract: While Reinforcement Learning from Human Feedback (RLHF) has shown promise in aligning generative AI, we present empirical evidence that it can also cause severe, systematic misalignment. We hypothesize that this stems from evaluator feedback depending on downstream outcome predictions (foresight) that can be influenced by the AI's output, inducing Goodhart's law dynamics. We present a theoretical analysis showing that conditioning evaluator feedback on downstream observations (hindsight) inhibits this effect by decoupling the alignment signal from potentially compromised predictions--crucially, the result holds even if the observed outcomes are sampled from the AI's own world model. Building on this insight, we introduce Reinforcement Learning from Hindsight Simulation (RLHS), which presents plausible simulated outcomes to evaluators before eliciting feedback. We validate RLHS across three consultancy settings--marketplace interactions, restaurant recommendations, and online course advising--using both online (PPO) and offline (DPO) fine-tuning methods, and show that it substantially improves alignment over RLHF in experiments and human evaluations. We perform post-hoc benchmark evaluations on TruthfulQA, HaluEval, and TrustLLM, finding that even after single-task fine-tuning, RLHF misalignment persists, whereas RLHS consistently outperforms baselines and demonstrates robust alignment generalization. The project webpage and code are available at https://rl-hindsight.github.io.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pivoting Factorization: A Compact Meta Low-Rank Representation of Sparsity for Efficient Inference in Large Language Models</title>
<link>https://arxiv.org/abs/2501.19090</link>
<guid>https://arxiv.org/abs/2501.19090</guid>
<content:encoded><![CDATA[
arXiv:2501.19090v2 Announce Type: replace 
Abstract: The rapid growth of Large Language Models has driven demand for effective model compression techniques to reduce memory and computation costs. Low-rank pruning has gained attention for its GPU compatibility across all densities. However, low-rank pruning struggles to match the performance of semi-structured pruning, often doubling perplexity at similar densities. In this paper, we propose Pivoting Factorization (PIFA), a novel lossless meta low-rank representation that unsupervisedly learns a compact form of any low-rank representation, effectively eliminating redundant information. PIFA identifies pivot rows (linearly independent rows) and expresses non-pivot rows as linear combinations, achieving 24.2% additional memory savings and 24.6% faster inference over low-rank layers at rank = 50% of dimension. To mitigate the performance degradation caused by low-rank pruning, we introduce a novel, retraining-free reconstruction method that minimizes error accumulation (M). MPIFA, combining M and PIFA into an end-to-end framework, significantly outperforms existing low-rank pruning methods, and achieves performance comparable to semi-structured pruning, while surpassing it in GPU efficiency and compatibility. Our code is available at https://github.com/biomedical-cybernetics/pivoting-factorization.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regularized Langevin Dynamics for Combinatorial Optimization</title>
<link>https://arxiv.org/abs/2502.00277</link>
<guid>https://arxiv.org/abs/2502.00277</guid>
<content:encoded><![CDATA[
arXiv:2502.00277v2 Announce Type: replace 
Abstract: This work proposes a simple yet effective sampling framework for combinatorial optimization (CO). Our method builds on discrete Langevin dynamics (LD), an efficient gradient-guided generative paradigm. However, we observe that directly applying LD often leads to limited exploration. To overcome this limitation, we propose the Regularized Langevin Dynamics (RLD), which enforces an expected distance between the sampled and current solutions, effectively avoiding local minima. We develop two CO solvers on top of RLD, one based on simulated annealing (SA), and the other one based on neural network (NN). Empirical results on three classic CO problems demonstrate that both of our methods can achieve comparable or better performance against the previous state-of-the-art (SOTA) SA- and NN-based solvers. In particular, our SA algorithm reduces the runtime of the previous SOTA SA method by up to 80\%, while achieving equal or superior performance. In summary, RLD offers a promising framework for enhancing both traditional heuristics and NN models to solve CO problems. Our code is available at https://github.com/Shengyu-Feng/RLD4CO.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Generalised Variational Inference: A Robust Probabilistic Federated Learning Framework</title>
<link>https://arxiv.org/abs/2502.00846</link>
<guid>https://arxiv.org/abs/2502.00846</guid>
<content:encoded><![CDATA[
arXiv:2502.00846v3 Announce Type: replace 
Abstract: We introduce FedGVI, a probabilistic Federated Learning (FL) framework that is robust to both prior and likelihood misspecification. FedGVI addresses limitations in both frequentist and Bayesian FL by providing unbiased predictions under model misspecification, with calibrated uncertainty quantification. Our approach generalises previous FL approaches, specifically Partitioned Variational Inference (Ashman et al., 2022), by allowing robust and conjugate updates, decreasing computational complexity at the clients. We offer theoretical analysis in terms of fixed-point convergence, optimality of the cavity distribution, and provable robustness to likelihood misspecification. Further, we empirically demonstrate the effectiveness of FedGVI in terms of improved robustness and predictive performance on multiple synthetic and real world classification data sets.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DIME:Diffusion-Based Maximum Entropy Reinforcement Learning</title>
<link>https://arxiv.org/abs/2502.02316</link>
<guid>https://arxiv.org/abs/2502.02316</guid>
<content:encoded><![CDATA[
arXiv:2502.02316v2 Announce Type: replace 
Abstract: Maximum entropy reinforcement learning (MaxEnt-RL) has become the standard approach to RL due to its beneficial exploration properties. Traditionally, policies are parameterized using Gaussian distributions, which significantly limits their representational capacity. Diffusion-based policies offer a more expressive alternative, yet integrating them into MaxEnt-RL poses challenges-primarily due to the intractability of computing their marginal entropy. To overcome this, we propose Diffusion-Based Maximum Entropy RL (DIME). \emph{DIME} leverages recent advances in approximate inference with diffusion models to derive a lower bound on the maximum entropy objective. Additionally, we propose a policy iteration scheme that provably converges to the optimal diffusion policy. Our method enables the use of expressive diffusion-based policies while retaining the principled exploration benefits of MaxEnt-RL, significantly outperforming other diffusion-based methods on challenging high-dimensional control benchmarks. It is also competitive with state-of-the-art non-diffusion based RL methods while requiring fewer algorithmic design choices and smaller update-to-data ratios, reducing computational complexity.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Spectral Transitions in High-Dimensional Multi-Index Models</title>
<link>https://arxiv.org/abs/2502.02545</link>
<guid>https://arxiv.org/abs/2502.02545</guid>
<content:encoded><![CDATA[
arXiv:2502.02545v2 Announce Type: replace 
Abstract: We consider the problem of how many samples from a Gaussian multi-index model are required to weakly reconstruct the relevant index subspace. Despite its increasing popularity as a testbed for investigating the computational complexity of neural networks, results beyond the single-index setting remain elusive. In this work, we introduce spectral algorithms based on the linearization of a message passing scheme tailored to this problem. Our main contribution is to show that the proposed methods achieve the optimal reconstruction threshold. Leveraging a high-dimensional characterization of the algorithms, we show that above the critical threshold the leading eigenvector correlates with the relevant index subspace, a phenomenon reminiscent of the Baik-Ben Arous-Peche (BBP) transition in spiked models arising in random matrix theory. Supported by numerical experiments and a rigorous theoretical framework, our work bridges critical gaps in the computational limits of weak learnability in multi-index model.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Calibrated Physics-Informed Uncertainty Quantification</title>
<link>https://arxiv.org/abs/2502.04406</link>
<guid>https://arxiv.org/abs/2502.04406</guid>
<content:encoded><![CDATA[
arXiv:2502.04406v2 Announce Type: replace 
Abstract: Simulating complex physical systems is crucial for understanding and predicting phenomena across diverse fields, such as fluid dynamics and heat transfer, as well as plasma physics and structural mechanics. Traditional approaches rely on solving partial differential equations (PDEs) using numerical methods, which are computationally expensive and often prohibitively slow for real-time applications or large-scale simulations. Neural PDEs have emerged as efficient alternatives to these costly numerical solvers, offering significant computational speed-ups. However, their lack of robust uncertainty quantification (UQ) limits deployment in critical applications. We introduce a model-agnostic, physics-informed conformal prediction (CP) framework that provides guaranteed uncertainty estimates without requiring labelled data. By utilising a physics-based approach, we can quantify and calibrate the model's inconsistencies with the physics rather than the uncertainty arising from the data. Our approach utilises convolutional layers as finite-difference stencils and leverages physics residual errors as nonconformity scores, enabling data-free UQ with marginal and joint coverage guarantees across prediction domains for a range of complex PDEs. We further validate the efficacy of our method on neural PDE models for plasma modelling and shot design in fusion reactors.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-shot Meta-learning for Tabular Prediction Tasks with Adversarially Pre-trained Transformer</title>
<link>https://arxiv.org/abs/2502.04573</link>
<guid>https://arxiv.org/abs/2502.04573</guid>
<content:encoded><![CDATA[
arXiv:2502.04573v2 Announce Type: replace 
Abstract: We present an Adversarially Pre-trained Transformer (APT) that is able to perform zero-shot meta-learning on tabular prediction tasks without pre-training on any real-world dataset, extending on the recent development of Prior-Data Fitted Networks (PFNs) and TabPFN. Specifically, APT is pre-trained with adversarial synthetic data agents, who continue to shift their underlying data generating distribution and deliberately challenge the model with different synthetic datasets. In addition, we propose a mixture block architecture that is able to handle classification tasks with arbitrary number of classes, addressing the class size limitation -- a crucial weakness of prior deep tabular zero-shot learners. In experiments, we show that our framework matches state-of-the-art performance on small classification tasks without filtering on dataset characteristics such as number of classes and number of missing values, while maintaining an average runtime under one second. On common benchmark dataset suites in both classification and regression, we show that adversarial pre-training was able to enhance TabPFN's performance. In our analysis, we demonstrate that the adversarial synthetic data agents were able to generate a more diverse collection of data compared to the ordinary random generator in TabPFN. In addition, we demonstrate that our mixture block neural design has improved generalizability and greatly accelerated pre-training.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightweight Dataset Pruning without Full Training via Example Difficulty and Prediction Uncertainty</title>
<link>https://arxiv.org/abs/2502.06905</link>
<guid>https://arxiv.org/abs/2502.06905</guid>
<content:encoded><![CDATA[
arXiv:2502.06905v2 Announce Type: replace 
Abstract: Recent advances in deep learning rely heavily on massive datasets, leading to substantial storage and training costs. Dataset pruning aims to alleviate this demand by discarding redundant examples. However, many existing methods require training a model with a full dataset over a large number of epochs before being able to prune the dataset, which ironically makes the pruning process more expensive than just training the model on the entire dataset. To overcome this limitation, we introduce a Difficulty and Uncertainty-Aware Lightweight (DUAL) score, which aims to identify important samples from the early training stage by considering both example difficulty and prediction uncertainty. To address a catastrophic accuracy drop at an extreme pruning, we further propose a ratio-adaptive sampling using Beta distribution. Experiments on various datasets and learning scenarios such as image classification with label noise and image corruption, and model architecture generalization demonstrate the superiority of our method over previous state-of-the-art (SOTA) approaches. Specifically, on ImageNet-1k, our method reduces the time cost for pruning to 66% compared to previous methods while achieving a SOTA, specifically 60% test accuracy at a 90% pruning ratio. On CIFAR datasets, the time cost is reduced to just 15% while maintaining SOTA performance.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Curvature Tuning: Provable Training-free Model Steering From a Single Parameter</title>
<link>https://arxiv.org/abs/2502.07783</link>
<guid>https://arxiv.org/abs/2502.07783</guid>
<content:encoded><![CDATA[
arXiv:2502.07783v3 Announce Type: replace 
Abstract: The scaling of model and data sizes has reshaped the AI landscape, establishing finetuning pretrained models as the standard paradigm for solving downstream tasks. However, dominant finetuning methods typically rely on weight adaptation, often lack interpretability, and depend on heuristically chosen hyperparameters. In this paper, we take a different perspective and shift the focus from weights to activation functions, viewing them through the lens of spline operators. We propose Curvature Tuning (CT), an interpretable and principled steering method that modulates a model's decision boundary by injecting a single hyperparameter into its activation functions. We show that CT provably adjusts model decision boundary curvature and, more fundamentally, projects a model onto a space of smooth functions-thereby complementing current finetuning methods, whose effect lies primarily in feature adaptation. Making this hyperparameter trainable gives rise to a novel and highly parameter-efficient finetuning method. Empirically, CT improves both generalization and robustness. For example, it boosts downstream accuracy of ResNet-50/152 by 7.14%/8.46% over linear probing and 4.64%/1.70% over LoRA across 12 datasets, and improves robust accuracy on the $\ell_\infty$ benchmark from RobustBench by 1032.64%/1494.46%. Our code is available at https://github.com/Leon-Leyang/curvature-tuning.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding High-Dimensional Bayesian Optimization</title>
<link>https://arxiv.org/abs/2502.09198</link>
<guid>https://arxiv.org/abs/2502.09198</guid>
<content:encoded><![CDATA[
arXiv:2502.09198v2 Announce Type: replace 
Abstract: Recent work reported that simple Bayesian optimization (BO) methods perform well for high-dimensional real-world tasks, seemingly contradicting prior work and tribal knowledge. This paper investigates why. We identify underlying challenges that arise in high-dimensional BO and explain why recent methods succeed. Our empirical analysis shows that vanishing gradients caused by Gaussian process (GP) initialization schemes play a major role in the failures of high-dimensional Bayesian optimization (HDBO) and that methods that promote local search behaviors are better suited for the task. We find that maximum likelihood estimation (MLE) of GP length scales suffices for state-of-the-art performance. Based on this, we propose a simple variant of MLE called MSR that leverages these findings to achieve state-of-the-art performance on a comprehensive set of real-world applications. We present targeted experiments to illustrate and confirm our findings.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comprehensive Review of Neural Differential Equations for Time Series Analysis</title>
<link>https://arxiv.org/abs/2502.09885</link>
<guid>https://arxiv.org/abs/2502.09885</guid>
<content:encoded><![CDATA[
arXiv:2502.09885v2 Announce Type: replace 
Abstract: Time series modeling and analysis have become critical in various domains. Conventional methods such as RNNs and Transformers, while effective for discrete-time and regularly sampled data, face significant challenges in capturing the continuous dynamics and irregular sampling patterns inherent in real-world scenarios. Neural Differential Equations (NDEs) represent a paradigm shift by combining the flexibility of neural networks with the mathematical rigor of differential equations. This paper presents a comprehensive review of NDE-based methods for time series analysis, including neural ordinary differential equations, neural controlled differential equations, and neural stochastic differential equations. We provide a detailed discussion of their mathematical formulations, numerical methods, and applications, highlighting their ability to model continuous-time dynamics. Furthermore, we address key challenges and future research directions. This survey serves as a foundation for researchers and practitioners seeking to leverage NDEs for advanced time series analysis.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memory, Benchmark &amp; Robots: A Benchmark for Solving Complex Tasks with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2502.10550</link>
<guid>https://arxiv.org/abs/2502.10550</guid>
<content:encoded><![CDATA[
arXiv:2502.10550v2 Announce Type: replace 
Abstract: Memory is crucial for enabling agents to tackle complex tasks with temporal and spatial dependencies. While many reinforcement learning (RL) algorithms incorporate memory, the field lacks a universal benchmark to assess an agent's memory capabilities across diverse scenarios. This gap is particularly evident in tabletop robotic manipulation, where memory is essential for solving tasks with partial observability and ensuring robust performance, yet no standardized benchmarks exist. To address this, we introduce MIKASA (Memory-Intensive Skills Assessment Suite for Agents), a comprehensive benchmark for memory RL, with three key contributions: (1) we propose a comprehensive classification framework for memory-intensive RL tasks, (2) we collect MIKASA-Base -- a unified benchmark that enables systematic evaluation of memory-enhanced agents across diverse scenarios, and (3) we develop MIKASA-Robo (pip install mikasa-robo-suite) -- a novel benchmark of 32 carefully designed memory-intensive tasks that assess memory capabilities in tabletop robotic manipulation. Our work introduces a unified framework to advance memory RL research, enabling more robust systems for real-world use. MIKASA is available at https://tinyurl.com/membenchrobots.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training-Free Guidance Beyond Differentiability: Scalable Path Steering with Tree Search in Diffusion and Flow Models</title>
<link>https://arxiv.org/abs/2502.11420</link>
<guid>https://arxiv.org/abs/2502.11420</guid>
<content:encoded><![CDATA[
arXiv:2502.11420v2 Announce Type: replace 
Abstract: Training-free guidance enables controlled generation in diffusion and flow models, but most methods rely on gradients and assume differentiable objectives. This work focuses on training-free guidance addressing challenges from non-differentiable objectives and discrete data distributions. We propose TreeG: Tree Search-Based Path Steering Guidance, applicable to both continuous and discrete settings in diffusion and flow models. TreeG offers a unified framework for training-free guidance by proposing, evaluating, and selecting candidates at each step, enhanced with tree search over active paths and parallel exploration. We comprehensively investigate the design space of TreeG over the candidate proposal module and the evaluation function, instantiating TreeG into three novel algorithms. Our experiments show that TreeG consistently outperforms top guidance baselines in symbolic music generation, small molecule design, and enhancer DNA design with improvements of 29.01%, 16.6%, and 18.43%. Additionally, we identify an inference-time scaling law showing TreeG's scalability in inference-time computation.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exact Upper and Lower Bounds for the Output Distribution of Neural Networks with Random Inputs</title>
<link>https://arxiv.org/abs/2502.11672</link>
<guid>https://arxiv.org/abs/2502.11672</guid>
<content:encoded><![CDATA[
arXiv:2502.11672v2 Announce Type: replace 
Abstract: We derive exact upper and lower bounds for the cumulative distribution function (cdf) of the output of a neural network (NN) over its entire support subject to noisy (stochastic) inputs. The upper and lower bounds converge to the true cdf over its domain as the resolution increases. Our method applies to any feedforward NN using continuous monotonic piecewise twice continuously differentiable activation functions (e.g., ReLU, tanh and softmax) and convolutional NNs, which were beyond the scope of competing approaches. The novelty and instrumental tool of our approach is to bound general NNs with ReLU NNs. The ReLU NN-based bounds are then used to derive the upper and lower bounds of the cdf of the NN output. Experiments demonstrate that our method delivers guaranteed bounds of the predictive output distribution over its support, thus providing exact error guarantees, in contrast to competing approaches.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Privacy Risks of Spiking Neural Networks: A Membership Inference Analysis</title>
<link>https://arxiv.org/abs/2502.13191</link>
<guid>https://arxiv.org/abs/2502.13191</guid>
<content:encoded><![CDATA[
arXiv:2502.13191v3 Announce Type: replace 
Abstract: Spiking Neural Networks (SNNs) are increasingly explored for their energy efficiency and robustness in real-world applications, yet their privacy risks remain largely unexamined. In this work, we investigate the susceptibility of SNNs to Membership Inference Attacks (MIAs) -- a major privacy threat where an adversary attempts to determine whether a given sample was part of the training dataset. While prior work suggests that SNNs may offer inherent robustness due to their discrete, event-driven nature, we find that its resilience diminishes as latency (T) increases. Furthermore, we introduce an input dropout strategy under black box setting, that significantly enhances membership inference in SNNs. Our findings challenge the assumption that SNNs are inherently more secure, and even though they are expected to be better, our results reveal that SNNs exhibit privacy vulnerabilities that are equally comparable to Artificial Neural Networks (ANNs). Our code is available at https://anonymous.4open.science/r/MIA_SNN-3610.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Supervised Transformers as Iterative Solution Improvers for Constraint Satisfaction</title>
<link>https://arxiv.org/abs/2502.15794</link>
<guid>https://arxiv.org/abs/2502.15794</guid>
<content:encoded><![CDATA[
arXiv:2502.15794v2 Announce Type: replace 
Abstract: We present a Transformer-based framework for Constraint Satisfaction Problems (CSPs). CSPs find use in many applications and thus accelerating their solution with machine learning is of wide interest. Most existing approaches rely on supervised learning from feasible solutions or reinforcement learning, paradigms that require either feasible solutions to these NP-Complete CSPs or large training budgets and a complex expert-designed reward signal. To address these challenges, we propose ConsFormer, a self-supervised framework that leverages a Transformer as a solution refiner. ConsFormer constructs a solution to a CSP iteratively in a process that mimics local search. Instead of using feasible solutions as labeled data, we devise differentiable approximations to the discrete constraints of a CSP to guide model training. Our model is trained to improve random assignments for a single step but is deployed iteratively at test time, circumventing the bottlenecks of supervised and reinforcement learning. Experiments on Sudoku, Graph Coloring, Nurse Rostering, and MAXCUT demonstrate that our method can tackle out-of-distribution CSPs simply through additional iterations.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Implicit Neural Representations for Chemical Reaction Paths</title>
<link>https://arxiv.org/abs/2502.15843</link>
<guid>https://arxiv.org/abs/2502.15843</guid>
<content:encoded><![CDATA[
arXiv:2502.15843v2 Announce Type: replace 
Abstract: We show that neural networks can be optimized to represent minimum energy paths as continuous functions, offering a flexible alternative to discrete path-search methods like Nudged Elastic Band (NEB). Our approach parameterizes reaction paths with a network trained on a loss function that discards tangential energy gradients and enables instant estimation of the transition state. We first validate the method on two-dimensional potentials and then demonstrate its advantages over NEB on challenging atomistic systems where (i) poor initial guesses yield unphysical paths, (ii) multiple competing paths exist, or (iii) the reaction follows a complex multi-step mechanism. Results highlight the versatility of the method: for instance, a simple adjustment to the sampling strategy during optimization can help escape local-minimum solutions. Finally, in a low-dimensional setting, we demonstrate that a single neural network can learn from existing paths and generalize to unseen systems, showing promise for a universal reaction path representation.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Equilibrium Sampling with Sequential Boltzmann Generators</title>
<link>https://arxiv.org/abs/2502.18462</link>
<guid>https://arxiv.org/abs/2502.18462</guid>
<content:encoded><![CDATA[
arXiv:2502.18462v2 Announce Type: replace 
Abstract: Scalable sampling of molecular states in thermodynamic equilibrium is a long-standing challenge in statistical physics. Boltzmann generators tackle this problem by pairing normalizing flows with importance sampling to obtain uncorrelated samples under the target distribution. In this paper, we extend the Boltzmann generator framework with two key contributions, denoting our framework Sequential Boltzmann Generators (SBG). The first is a highly efficient Transformer-based normalizing flow operating directly on all-atom Cartesian coordinates. In contrast to the equivariant continuous flows of prior methods, we leverage exactly invertible non-equivariant architectures which are highly efficient during both sample generation and likelihood evaluation. This efficiency unlocks more sophisticated inference strategies beyond standard importance sampling. In particular, we perform inference-time scaling of flow samples using a continuous-time variant of sequential Monte Carlo, in which flow samples are transported towards the target distribution with annealed Langevin dynamics. SBG achieves state-of-the-art performance w.r.t. all metrics on peptide systems, demonstrating the first equilibrium sampling in Cartesian coordinates of tri-, tetra- and hexa-peptides that were thus far intractable for prior Boltzmann generators.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Graph Attention-based Instance Selection via Mini-Batch Sampling and Hierarchical Hashing</title>
<link>https://arxiv.org/abs/2502.20293</link>
<guid>https://arxiv.org/abs/2502.20293</guid>
<content:encoded><![CDATA[
arXiv:2502.20293v2 Announce Type: replace 
Abstract: Instance selection (IS) addresses the critical challenge of reducing dataset size while keeping informative characteristics, becoming increasingly important as datasets grow to millions of instances. Current IS methods often struggle with capturing complex relationships in high-dimensional spaces and scale with large datasets. This paper introduces a graph attention-based instance selection (GAIS) method that uses attention mechanisms to identify informative instances through their structural relationships in graph representations. We present two approaches for scalable graph construction: a distance-based mini-batch sampling technique that achieves dataset-size-independent complexity through strategic batch processing, and a hierarchical hashing approach that enables efficient similarity computation through random projections. The mini-batch approach keeps class distributions through stratified sampling, while the hierarchical hashing method captures relationships at multiple granularities through single-level, multi-level, and multi-view variants. Experiments across 39 datasets show that GAIS achieves reduction rates above 96\% while maintaining or improving model performance relative to state-of-the-art IS methods. The findings show that the distance-based mini-batch approach offers an optimal efficiency for large-scale datasets, while multi-view variants excel on complex, high-dimensional data, demonstrating that attention-based importance scoring can effectively identify instances important for maintaining decision boundaries while avoiding computationally prohibitive pairwise comparisons.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning surrogate equations for the analysis of an agent-based cancer model</title>
<link>https://arxiv.org/abs/2503.01718</link>
<guid>https://arxiv.org/abs/2503.01718</guid>
<content:encoded><![CDATA[
arXiv:2503.01718v2 Announce Type: replace 
Abstract: In this paper, we adapt a two-species agent-based cancer model that describes the interaction between cancer cells and healthy cells on a uniform grid to include the interaction with a third species -- namely immune cells. We run six different scenarios to explore the competition between cancer and immune cells and the initial concentration of the immune cells on cancer dynamics. We then use coupled equation learning to construct a population-based reaction model for each scenario. We show how they can be unified into a single surrogate population-based reaction model, whose underlying three coupled ordinary differential equations are much easier to analyse than the original agent-based model. As an example, by finding the single steady state of the cancer concentration, we are able to find a linear relationship between this concentration and the initial concentration of the immune cells. This then enables us to estimate suitable values for the competition and initial concentration to reduce the cancer substantially without performing additional complex and expensive simulations from an agent-based stochastic model.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nearly Optimal Differentially Private ReLU Regression</title>
<link>https://arxiv.org/abs/2503.06009</link>
<guid>https://arxiv.org/abs/2503.06009</guid>
<content:encoded><![CDATA[
arXiv:2503.06009v2 Announce Type: replace 
Abstract: In this paper, we investigate one of the most fundamental nonconvex learning problems, ReLU regression, in the Differential Privacy (DP) model. Previous studies on private ReLU regression heavily rely on stringent assumptions, such as constant bounded norms for feature vectors and labels. We relax these assumptions to a more standard setting, where data can be i.i.d. sampled from $O(1)$-sub-Gaussian distributions. We first show that when $\varepsilon = \tilde{O}(\sqrt{\frac{1}{N}})$ and there is some public data, it is possible to achieve an upper bound of $\tilde{O}(\frac{d^2}{N^2 \varepsilon^2})$ for the excess population risk in $(\epsilon, \delta)$-DP, where $d$ is the dimension and $N$ is the number of data samples. Moreover, we relax the requirement of $\epsilon$ and public data by proposing and analyzing a one-pass mini-batch Generalized Linear Model Perceptron algorithm (DP-MBGLMtron). Additionally, using the tracing attack argument technique, we demonstrate that the minimax rate of the estimation error for $(\varepsilon, \delta)$-DP algorithms is lower bounded by $\Omega(\frac{d^2}{N^2 \varepsilon^2})$. This shows that DP-MBGLMtron achieves the optimal utility bound up to logarithmic factors. Experiments further support our theoretical results.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting and Understanding College Student Mental Health with Interpretable Machine Learning</title>
<link>https://arxiv.org/abs/2503.08002</link>
<guid>https://arxiv.org/abs/2503.08002</guid>
<content:encoded><![CDATA[
arXiv:2503.08002v3 Announce Type: replace 
Abstract: Mental health issues among college students have reached critical levels, significantly impacting academic performance and overall wellbeing. Predicting and understanding mental health status among college students is challenging due to three main factors: the necessity for large-scale longitudinal datasets, the prevalence of black-box machine learning models lacking transparency, and the tendency of existing approaches to provide aggregated insights at the population level rather than individualized understanding.
  To tackle these challenges, this paper presents I-HOPE, the first Interpretable Hierarchical mOdel for Personalized mEntal health prediction. I-HOPE is a two-stage hierarchical model that connects raw behavioral features to mental health status through five defined behavioral categories as interaction labels. We evaluate I-HOPE on the College Experience Study, the longest longitudinal mobile sensing dataset. This dataset spans five years and captures data from both pre-pandemic periods and the COVID-19 pandemic. I-HOPE achieves a prediction accuracy of 91%, significantly surpassing the 60-70% accuracy of baseline methods. In addition, I-HOPE distills complex patterns into interpretable and individualized insights, enabling the future development of tailored interventions and improving mental health support. The code is available at https://github.com/roycmeghna/I-HOPE.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ASIDE: Architectural Separation of Instructions and Data in Language Models</title>
<link>https://arxiv.org/abs/2503.10566</link>
<guid>https://arxiv.org/abs/2503.10566</guid>
<content:encoded><![CDATA[
arXiv:2503.10566v3 Announce Type: replace 
Abstract: Despite their remarkable performance, large language models lack elementary safety features, making them susceptible to numerous malicious attacks. In particular, previous work has identified the absence of an intrinsic separation between instructions and data as a root cause of the success of prompt injection attacks. In this work, we propose a new architectural element, ASIDE, that allows language models to clearly separate instructions and data at the level of embeddings. ASIDE applies an orthogonal rotation to the embeddings of data tokens, thus creating clearly distinct representations of instructions and data tokens without introducing any additional parameters. As we demonstrate experimentally across a range of models, instruction-tuning LLMs with ASIDE (1) leads to highly increased instruction-data separation without a loss in model utility and (2) makes the models more robust to prompt injection benchmarks, even without dedicated safety training. Additionally, we provide insights into the mechanism underlying our method through an analysis of the model representations. The source code and training scripts are openly accessible at https://github.com/egozverev/aside.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Bias Reinforcement in LLM Agents Debate</title>
<link>https://arxiv.org/abs/2503.16814</link>
<guid>https://arxiv.org/abs/2503.16814</guid>
<content:encoded><![CDATA[
arXiv:2503.16814v3 Announce Type: replace 
Abstract: Large Language Models $($LLMs$)$ solve complex problems using training-free methods like prompt engineering and in-context learning, yet ensuring reasoning correctness remains challenging. While self-correction methods such as self-consistency and self-refinement aim to improve reliability, they often reinforce biases due to the lack of effective feedback mechanisms. Multi-Agent Debate $($MAD$)$ has emerged as an alternative, but we identify two key limitations: bias reinforcement, where debate amplifies model biases instead of correcting them, and lack of perspective diversity, as all agents share the same model and reasoning patterns, limiting true debate effectiveness. To systematically evaluate these issues, we introduce $\textit{MetaNIM Arena}$, a benchmark designed to assess LLMs in adversarial strategic decision-making, where dynamic interactions influence optimal decisions. To overcome MAD's limitations, we propose $\textbf{DReaMAD}$ $($$\textbf{D}$iverse $\textbf{Rea}$soning via $\textbf{M}$ulti-$\textbf{A}$gent $\textbf{D}$ebate with Refined Prompt$)$, a novel framework that $(1)$ refines LLM's strategic prior knowledge to improve reasoning quality and $(2)$ promotes diverse viewpoints within a single model by systematically modifying prompts, reducing bias. Empirical results show that $\textbf{DReaMAD}$ significantly improves decision accuracy, reasoning diversity, and bias mitigation across multiple strategic tasks, establishing it as a more effective approach for LLM-based decision-making.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>k-NN as a Simple and Effective Estimator of Transferability</title>
<link>https://arxiv.org/abs/2503.18528</link>
<guid>https://arxiv.org/abs/2503.18528</guid>
<content:encoded><![CDATA[
arXiv:2503.18528v2 Announce Type: replace 
Abstract: How well can one expect transfer learning to work in a new setting where the domain is shifted, the task is different, and the architecture changes? Many transfer learning metrics have been proposed to answer this question. But how accurate are their predictions in a realistic new setting? We conducted an extensive evaluation involving over 42,000 experiments comparing 23 transferability metrics across 16 different datasets to assess their ability to predict transfer performance. Our findings reveal that none of the existing metrics perform well across the board. However, we find that a simple k-nearest neighbor evaluation -- as is commonly used to evaluate feature quality for self-supervision -- not only surpasses existing metrics, but also offers better computational efficiency and ease of implementation.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolutionary Policy Optimization</title>
<link>https://arxiv.org/abs/2503.19037</link>
<guid>https://arxiv.org/abs/2503.19037</guid>
<content:encoded><![CDATA[
arXiv:2503.19037v2 Announce Type: replace 
Abstract: On-policy reinforcement learning (RL) algorithms are widely used for their strong asymptotic performance and training stability, but they struggle to scale with larger batch sizes, as additional parallel environments yield redundant data due to limited policy-induced diversity. In contrast, Evolutionary Algorithms (EAs) scale naturally and encourage exploration via randomized population-based search, but are often sample-inefficient. We propose Evolutionary Policy Optimization (EPO), a hybrid algorithm that combines the scalability and diversity of EAs with the performance and stability of policy gradients. EPO maintains a population of agents conditioned on latent variables, shares actor-critic network parameters for coherence and memory efficiency, and aggregates diverse experiences into a master agent. Across tasks in dexterous manipulation, legged locomotion, and classic control, EPO outperforms state-of-the-art baselines in sample efficiency, asymptotic performance, and scalability.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quamba2: A Robust and Scalable Post-training Quantization Framework for Selective State Space Models</title>
<link>https://arxiv.org/abs/2503.22879</link>
<guid>https://arxiv.org/abs/2503.22879</guid>
<content:encoded><![CDATA[
arXiv:2503.22879v3 Announce Type: replace 
Abstract: State Space Models (SSMs) are emerging as a compelling alternative to Transformers because of their consistent memory usage and high performance. Despite this, scaling up SSMs on cloud services or limited-resource devices is challenging due to their storage requirements and computational power. To overcome this, quantizing SSMs with low bit-width data formats can reduce model size and benefit from hardware acceleration. As SSMs are prone to quantization-induced errors, recent efforts have focused on optimizing a particular model or bit-width for efficiency without sacrificing performance. However, distinct bit-width configurations are essential for different scenarios, like W4A8 for boosting large-batch decoding speed, and W4A16 for enhancing generation speed in short prompt applications for a single user. To this end, we present Quamba2, compatible with W8A8, W4A8, and W4A16 for both Mamba1 and Mamba2 backbones, addressing the growing demand for SSM deployment on various platforms. Based on the channel order preserving and activation persistence of SSMs, we propose an offline approach to quantize inputs of a linear recurrence in 8-bit by sorting and clustering for input $x$, combined with a per-state-group quantization for input-dependent parameters $B$ and $C$. To ensure compute-invariance in the SSM output, we rearrange weights offline according to the clustering sequence. The experiments show that Quamba2-8B outperforms two state-of-the-art SSM quantization methods and delivers 1.3$\times$ and 3$\times$ speed-ups in the pre-filling and generation stages, respectively, while offering 4$\times$ memory reduction with only a $1.6\%$ average accuracy drop. The evaluation on MMLU shows the generalizability and robustness of our framework. The code and quantized models will be released at: https://github.com/enyac-group/Quamba.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Theory of Machine Understanding via the Minimum Description Length Principle</title>
<link>https://arxiv.org/abs/2504.00395</link>
<guid>https://arxiv.org/abs/2504.00395</guid>
<content:encoded><![CDATA[
arXiv:2504.00395v3 Announce Type: replace 
Abstract: Deep neural networks trained through end-to-end learning have achieved remarkable success across various domains in the past decade. However, the end-to-end learning strategy, originally designed to minimize predictive loss in a black-box manner, faces two fundamental limitations: the struggle to form explainable representations in a self-supervised manner, and the inability to compress information rigorously following the Minimum Description Length (MDL) principle. These two limitations point to a deeper issue: an end-to-end learning model is not able to "understand" what it learns. In this paper, we establish a novel theory connecting these two limitations. We design the Spectrum VAE, a novel deep learning architecture whose minimum description length (MDL) can be rigorously evaluated. Then, we introduce the concept of latent dimension combinations, or what we term spiking patterns, and demonstrate that the observed spiking patterns should be as few as possible based on the training data in order for the Spectrum VAE to achieve the MDL. Finally, our theory demonstrates that when the MDL is achieved with respect to the given data distribution, the Spectrum VAE will naturally produce explainable latent representations of the data. In other words, explainable representations--or "understanding"--can emerge in a self-supervised manner simply by making the deep network obey the MDL principle. In our opinion, this also implies a deeper insight: To understand is to compress. At its core, our theory advocates for a shift in the training objective of deep networks: not only to minimize predictive loss, but also to minimize the description length regarding the given data. That is, a deep network should not only learn, but also understand what it learns. This work is entirely theoretical and aims to inspire future research toward self-supervised, explainable AI grounded in the MDL principle.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Provably Accurate Adaptive Sampling for Collocation Points in Physics-informed Neural Networks</title>
<link>https://arxiv.org/abs/2504.00910</link>
<guid>https://arxiv.org/abs/2504.00910</guid>
<content:encoded><![CDATA[
arXiv:2504.00910v2 Announce Type: replace 
Abstract: Despite considerable scientific advances in numerical simulation, efficiently solving PDEs remains a complex and often expensive problem. Physics-informed Neural Networks (PINN) have emerged as an efficient way to learn surrogate solvers by embedding the PDE in the loss function and minimizing its residuals using automatic differentiation at so-called collocation points. Originally uniformly sampled, the choice of the latter has been the subject of recent advances leading to adaptive sampling refinements for PINNs. In this paper, leveraging a new quadrature method for approximating definite integrals, we introduce a provably accurate sampling method for collocation points based on the Hessian of the PDE residuals. Comparative experiments conducted on a set of 1D and 2D PDEs demonstrate the benefits of our method.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optuna vs Code Llama: Are LLMs a New Paradigm for Hyperparameter Tuning?</title>
<link>https://arxiv.org/abs/2504.06006</link>
<guid>https://arxiv.org/abs/2504.06006</guid>
<content:encoded><![CDATA[
arXiv:2504.06006v3 Announce Type: replace 
Abstract: Optimal hyperparameter selection is critical for maximizing neural network performance, especially as models grow in complexity. This work investigates the viability of leveraging large language models (LLMs) for hyperparameter optimization by fine-tuning a parameter-efficient version of Code Llama using LoRA. The adapted LLM is capable of generating accurate and efficient hyperparameter recommendations tailored to diverse neural network architectures. Unlike traditional approaches such as Optuna, which rely on computationally intensive trial-and-error procedures, our method achieves competitive or superior results in terms of Root Mean Square Error (RMSE) while significantly reducing computational overhead. Our findings demonstrate that LLM-based optimization not only matches the performance of state-of-the-art techniques like Tree-structured Parzen Estimators (TPE) but also substantially accelerates the tuning process. This positions LLMs as a promising alternative for rapid experimentation, particularly in resource-constrained environments such as edge devices and mobile platforms, where computational efficiency is essential. In addition to improved efficiency, the method offers time savings and consistent performance across various tasks, highlighting its robustness and generalizability. All generated hyperparameters are included in the LEMUR Neural Network (NN) Dataset, which is publicly available and serves as an open-source benchmark for hyperparameter optimization research.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PatchTrAD: A Patch-Based Transformer focusing on Patch-Wise Reconstruction Error for Time Series Anomaly Detection</title>
<link>https://arxiv.org/abs/2504.08827</link>
<guid>https://arxiv.org/abs/2504.08827</guid>
<content:encoded><![CDATA[
arXiv:2504.08827v2 Announce Type: replace 
Abstract: Time series anomaly detection (TSAD) focuses on identifying whether observations in streaming data deviate significantly from normal patterns. With the prevalence of connected devices, anomaly detection on time series has become paramount, as it enables real-time monitoring and early detection of irregular behaviors across various application domains. In this work, we introduce PatchTrAD, a Patch-based Transformer model for time series anomaly detection. Our approach leverages a Transformer encoder along with the use of patches under a reconstructionbased framework for anomaly detection. Empirical evaluations on multiple benchmark datasets show that PatchTrAD is on par, in terms of detection performance, with state-of-the-art deep learning models for anomaly detection while being time efficient during inference.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Large-scale Evaluation of Embedding Models for Knowledge Graph Completion</title>
<link>https://arxiv.org/abs/2504.08970</link>
<guid>https://arxiv.org/abs/2504.08970</guid>
<content:encoded><![CDATA[
arXiv:2504.08970v2 Announce Type: replace 
Abstract: Knowledge graph embedding (KGE) models are extensively studied for knowledge graph completion, yet their evaluation remains constrained by unrealistic benchmarks. Standard evaluation metrics rely on the closed-world assumption, which penalizes models for correctly predicting missing triples, contradicting the fundamental goals of link prediction. These metrics often compress accuracy assessment into a single value, obscuring models' specific strengths and weaknesses. The prevailing evaluation protocol, link prediction, operates under the unrealistic assumption that an entity's properties, for which values are to be predicted, are known in advance. While alternative protocols such as property prediction, entity-pair ranking, and triple classification address some of these limitations, they remain underutilized. Moreover, commonly used datasets are either faulty or too small to reflect real-world data. Few studies examine the role of mediator nodes, which are essential for modeling n-ary relationships, or investigate model performance variation across domains. This paper conducts a comprehensive evaluation of four representative KGE models on large-scale datasets FB-CVT-REV and FB+CVT-REV. Our analysis reveals critical insights, including substantial performance variations between small and large datasets, both in relative rankings and absolute metrics, systematic overestimation of model capabilities when n-ary relations are binarized, and fundamental limitations in current evaluation protocols and metrics.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Activated LoRA: Fine-tuned LLMs for Intrinsics</title>
<link>https://arxiv.org/abs/2504.12397</link>
<guid>https://arxiv.org/abs/2504.12397</guid>
<content:encoded><![CDATA[
arXiv:2504.12397v4 Announce Type: replace 
Abstract: Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for finetuning the weights of large foundation models, and has become the go-to method for data-driven customization of LLMs. Despite the promise of highly customized behaviors and capabilities, switching between relevant LoRAs in a multiturn setting is inefficient, as the key-value (KV) cache of the entire turn history must be recomputed with the LoRA weights before generation can begin. To address this problem, we propose Activated LoRA (aLoRA), an adapter architecture which modifies the LoRA framework to only adapt weights for the tokens in the sequence \emph{after} the aLoRA is invoked. This change crucially allows aLoRA to accept the base model's KV cache of the input string, meaning that aLoRA can be instantly activated whenever needed in a chain without recomputing the cache. This enables building what we call \emph{intrinsics}, i.e. specialized models invoked to perform well-defined operations on portions of an input chain or conversation that otherwise uses the base model by default. We train a set of aLoRA-based intrinsics models, demonstrating competitive accuracy with standard LoRA while achieving significant inference benefits. The codebase is at https://github.com/IBM/activated-lora.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Meta-Learning via Mixed-Mode Differentiation</title>
<link>https://arxiv.org/abs/2505.00793</link>
<guid>https://arxiv.org/abs/2505.00793</guid>
<content:encoded><![CDATA[
arXiv:2505.00793v2 Announce Type: replace 
Abstract: Gradient-based bilevel optimisation is a powerful technique with applications in hyperparameter optimisation, task adaptation, algorithm discovery, meta-learning more broadly, and beyond. It often requires differentiating through the gradient-based optimisation itself, leading to "gradient-of-a-gradient" calculations with computationally expensive second-order and mixed derivatives. While modern automatic differentiation libraries provide a convenient way to write programs for calculating these derivatives, they oftentimes cannot fully exploit the specific structure of these problems out-of-the-box, leading to suboptimal performance. In this paper, we analyse such cases and propose Mixed-Flow Meta-Gradients, or MixFlow-MG -- a practical algorithm that uses mixed-mode differentiation to construct more efficient and scalable computational graphs yielding over 10x memory and up to 25% wall-clock time improvements over standard implementations in modern meta-learning setups.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Low-Loss Space in Neural Networks is Continuous and Fully Connected</title>
<link>https://arxiv.org/abs/2505.02604</link>
<guid>https://arxiv.org/abs/2505.02604</guid>
<content:encoded><![CDATA[
arXiv:2505.02604v2 Announce Type: replace 
Abstract: Visualizations of the loss landscape in neural networks suggest that minima are isolated points. However, both theoretical and empirical studies indicate that it is possible to connect two different minima with a path consisting of intermediate points that also have low loss. In this study, we propose a new algorithm which investigates low-loss paths in the full parameter space, not only between two minima. Our experiments on LeNet5, ResNet18, and Compact Convolutional Transformer architectures consistently demonstrate the existence of such continuous paths in the parameter space. These results suggest that the low-loss region is a fully connected and continuous space in the parameter space. Our findings provide theoretical insight into neural network over-parameterization, highlighting that parameters collectively define a high-dimensional low-loss space, implying parameter redundancy exists only within individual models and not throughout the entire low-loss space. Additionally, our work also provides new visualization methods and opportunities to improve model generalization by exploring the low-loss space that is closer to the origin.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Fine-Tuning of Quantized Models via Adaptive Rank and Bitwidth</title>
<link>https://arxiv.org/abs/2505.03802</link>
<guid>https://arxiv.org/abs/2505.03802</guid>
<content:encoded><![CDATA[
arXiv:2505.03802v3 Announce Type: replace 
Abstract: QLoRA effectively combines low-bit quantization and LoRA to achieve memory-friendly fine-tuning for large language models (LLM). Recently, methods based on SVD for continuous update iterations to initialize LoRA matrices to accommodate quantization errors have generally failed to consistently improve performance. Dynamic mixed precision is a natural idea for continuously improving the fine-tuning performance of quantized models, but previous methods often optimize low-rank subspaces or quantization components separately, without considering their synergy. To address this, we propose \textbf{QR-Adaptor}, a unified, gradient-free strategy that uses partial calibration data to jointly search the quantization components and the rank of low-rank spaces for each layer, thereby continuously improving model performance. QR-Adaptor does not minimize quantization error but treats precision and rank allocation as a discrete optimization problem guided by actual downstream performance and memory usage. Compared to state-of-the-art (SOTA) quantized LoRA fine-tuning methods, our approach achieves a 4.89\% accuracy improvement on GSM8K, and in some cases even outperforms the 16-bit fine-tuned model while maintaining the memory footprint of the 4-bit setting.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Training from Random Initialization: Aligning Lottery Ticket Masks using Weight Symmetry</title>
<link>https://arxiv.org/abs/2505.05143</link>
<guid>https://arxiv.org/abs/2505.05143</guid>
<content:encoded><![CDATA[
arXiv:2505.05143v2 Announce Type: replace 
Abstract: The Lottery Ticket Hypothesis (LTH) suggests there exists a sparse LTH mask and weights that achieve the same generalization performance as the dense model while using significantly fewer parameters. However, finding a LTH solution is computationally expensive, and a LTH sparsity mask does not generalize to other random weight initializations. Recent work has suggested that neural networks trained from random initialization find solutions within the same basin modulo permutation, and proposes a method to align trained models within the same loss basin. We hypothesize that misalignment of basins is the reason why LTH masks do not generalize to new random initializations and propose permuting the LTH mask to align with the new optimization basin when performing sparse training from a different random init. We empirically show a significant increase in generalization when sparse training from random initialization with the permuted mask as compared to using the non-permuted LTH mask, on multiple datasets (CIFAR-10, CIFAR-100 and ImageNet) and models (VGG11, ResNet20 and ResNet50).
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DisCO: Reinforcing Large Reasoning Models with Discriminative Constrained Optimization</title>
<link>https://arxiv.org/abs/2505.12366</link>
<guid>https://arxiv.org/abs/2505.12366</guid>
<content:encoded><![CDATA[
arXiv:2505.12366v2 Announce Type: replace 
Abstract: The recent success and openness of DeepSeek-R1 have brought widespread attention to Group Relative Policy Optimization (GRPO) as a reinforcement learning method for large reasoning models (LRMs). In this work, we analyze the GRPO objective under a binary reward setting and reveal an inherent limitation of question-level difficulty bias. We also identify a connection between GRPO and traditional discriminative methods in supervised learning. Motivated by these insights, we introduce a new Discriminative Constrained Optimization (DisCO) framework for reinforcing LRMs, grounded in the principle of discriminative learning. The main differences between DisCO and GRPO and its recent variants are: (1) it replaces the group relative objective with a discriminative objective defined by a scoring function; (2) it abandons clipping-based surrogates in favor of non-clipping RL surrogate objectives used as scoring functions; (3) it employs a simple yet effective constrained optimization approach to enforce the KL divergence constraint, ensuring stable training. As a result, DisCO offers notable advantages over GRPO and its variants: (i) it completely eliminates difficulty bias by adopting discriminative objectives; (ii) it addresses the entropy instability in GRPO and its variants through the use of non-clipping scoring functions and a constrained optimization approach; (iii) it allows the incorporation of advanced discriminative learning techniques to address data imbalance, where a significant number of questions have more negative than positive generated answers during training. Our experiments on enhancing the mathematical reasoning capabilities of SFT-finetuned models show that DisCO significantly outperforms GRPO and its improved variants such as DAPO, achieving average gains of 7\% over GRPO and 6\% over DAPO across six benchmark tasks for an 1.5B model.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Two-Stage Data Selection Framework for Data-Efficient Model Training on Edge Devices</title>
<link>https://arxiv.org/abs/2505.16563</link>
<guid>https://arxiv.org/abs/2505.16563</guid>
<content:encoded><![CDATA[
arXiv:2505.16563v2 Announce Type: replace 
Abstract: The demand for machine learning (ML) model training on edge devices is escalating due to data privacy and personalized service needs. However, we observe that current on-device model training is hampered by the under-utilization of on-device data, due to low training throughput, limited storage and diverse data importance. To improve data resource utilization, we propose a two-stage data selection framework {\sf Titan} to select the most important data batch from streaming data for model training with guaranteed efficiency and effectiveness. Specifically, in the first stage, {\sf Titan} filters out a candidate dataset with potentially high importance in a coarse-grained manner.In the second stage of fine-grained selection, we propose a theoretically optimal data selection strategy to identify the data batch with the highest model performance improvement to current training round. To further enhance time-and-resource efficiency, {\sf Titan} leverages a pipeline to co-execute data selection and model training, and avoids resource conflicts by exploiting idle computing resources. We evaluate {\sf Titan} on real-world edge devices and three representative edge computing tasks with diverse models and data modalities. Empirical results demonstrate that {\sf Titan} achieves up to $43\%$ reduction in training time and $6.2\%$ increase in final accuracy with minor system overhead, such as data processing delay, memory footprint and energy consumption.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Shortfall Risk Metric for Learning Regression Models</title>
<link>https://arxiv.org/abs/2505.17777</link>
<guid>https://arxiv.org/abs/2505.17777</guid>
<content:encoded><![CDATA[
arXiv:2505.17777v2 Announce Type: replace 
Abstract: We consider the problem of estimating and optimizing utility-based shortfall risk (UBSR) of a loss, say $(Y - \hat Y)^2$, in the context of a regression problem. Empirical risk minimization with a UBSR objective is challenging since UBSR is a non-linear function of the underlying distribution. We first derive a concentration bound for UBSR estimation using independent and identically distributed (i.i.d.) samples. We then frame the UBSR optimization problem as minimization of a pseudo-linear function in the space of achievable distributions $\mathcal D$ of the loss $(Y- \hat Y)^2$. We construct a gradient oracle for the UBSR objective and a linear minimization oracle (LMO) for the set $\mathcal D$. Using these oracles, we devise a bisection-type algorithm, and establish convergence to the UBSR-optimal solution.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Learning: From Theory to Practice</title>
<link>https://arxiv.org/abs/2505.19183</link>
<guid>https://arxiv.org/abs/2505.19183</guid>
<content:encoded><![CDATA[
arXiv:2505.19183v2 Announce Type: replace 
Abstract: This book offers a hands-on introduction to building and understanding federated learning (FL) systems. FL enables multiple devices -- such as smartphones, sensors, or local computers -- to collaboratively train machine learning (ML) models, while keeping their data private and local. It is a powerful solution when data cannot or should not be centralized due to privacy, regulatory, or technical reasons. The book is designed for students, engineers, and researchers who want to learn how to design scalable, privacy preserving FL systems. Our main focus is on personalization: enabling each device to train its own model while still benefiting from collaboration with relevant devices. This is achieved by leveraging similarities between (the learning tasks associated with) devices that are encoded by the weighted edges (or links) of a federated learning network (FL network). The key idea is to represent real-world FL systems as networks of devices, where nodes correspond to device and edges represent communication links and data similarities between them. The training of personalized models for these devices can be naturally framed as a distributed optimization problem. This optimization problem is referred to as generalized total variation minimization (GTVMin) and ensures that devices with similar learning tasks learn similar model parameters. Our approach is both mathematically principled and practically motivated. While we introduce some advanced ideas from optimization theory and graph-based learning, we aim to keep the book accessible. Readers are guided through the core ideas step by step, with intuitive explanations.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Navigating the Latent Space Dynamics of Neural Models</title>
<link>https://arxiv.org/abs/2505.22785</link>
<guid>https://arxiv.org/abs/2505.22785</guid>
<content:encoded><![CDATA[
arXiv:2505.22785v2 Announce Type: replace 
Abstract: Neural networks transform high-dimensional data into compact, structured representations, often modeled as elements of a lower dimensional latent space. In this paper, we present an alternative interpretation of neural models as dynamical systems acting on the latent manifold. Specifically, we show that autoencoder models implicitly define a latent vector field on the manifold, derived by iteratively applying the encoding-decoding map, without any additional training. We observe that standard training procedures introduce inductive biases that lead to the emergence of attractor points within this vector field. Drawing on this insight, we propose to leverage the vector field as a representation for the network, providing a novel tool to analyze the properties of the model and the data. This representation enables to: (i) analyze the generalization and memorization regimes of neural models, even throughout training; (ii) extract prior knowledge encoded in the network's parameters from the attractors, without requiring any input data; (iii) identify out-of-distribution samples from their trajectories in the vector field. We further validate our approach on vision foundation models, showcasing the applicability and effectiveness of our method in real-world scenarios.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equivalence of stochastic and deterministic policy gradients</title>
<link>https://arxiv.org/abs/2505.23244</link>
<guid>https://arxiv.org/abs/2505.23244</guid>
<content:encoded><![CDATA[
arXiv:2505.23244v2 Announce Type: replace 
Abstract: Policy gradients in continuous control have been derived for both stochastic and deterministic policies. Here we study the relationship between the two. In a widely-used family of MDPs involving Gaussian control noise and quadratic control costs, we show that the stochastic and deterministic policy gradients, natural gradients, and state value functions are identical; while the state-control value functions are different. We then develop a general procedure for constructing an MDP with deterministic policy that is equivalent to a given MDP with stochastic policy. The controls of this new MDP are the sufficient statistics of the stochastic policy in the original MDP. Our results suggest that policy gradient methods can be unified by approximating state value functions rather than state-control value functions.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stepsize anything: A unified learning rate schedule for budgeted-iteration training</title>
<link>https://arxiv.org/abs/2505.24452</link>
<guid>https://arxiv.org/abs/2505.24452</guid>
<content:encoded><![CDATA[
arXiv:2505.24452v2 Announce Type: replace 
Abstract: The expanding computational costs and limited resources underscore the critical need for budgeted-iteration training, which aims to achieve optimal learning within predetermined iteration budgets. While learning rate schedules fundamentally govern the performance of different networks and tasks, particularly in budgeted-iteration scenarios, their design remains largely heuristic, lacking theoretical foundations. In addition, the optimal learning rate schedule requires extensive trial-and-error selection, making the training process inefficient. In this work, we propose the Unified Budget-Aware (UBA) schedule, a theoretically grounded learning rate schedule that consistently outperforms commonly-used schedules among diverse architectures and tasks under different constrained training budgets. First, we bridge the gap by constructing a novel training budget-aware optimization framework, which explicitly accounts for the robustness to landscape curvature variations. From this framework, we derive the UBA schedule, controlled by a single hyper-parameter \varphi that provides a trade-off between flexibility and simplicity, eliminating the need for per-network numerical optimization. Moreover, we establish a theoretical connection between \varphi and the condition number, adding interpretation and justification to our approach. Besides, we prove the convergence for different values of \varphi. We offer practical guidelines for its selection via theoretical analysis and empirical results. Extensive experimental results show that UBA consistently surpasses the commonly-used schedules across diverse vision and language tasks, spanning network architectures (e.g., ResNet, OLMo) and scales, under different training-iteration budgets.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Slow-thinking LLMs Reason Over Time? Empirical Studies in Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.24511</link>
<guid>https://arxiv.org/abs/2505.24511</guid>
<content:encoded><![CDATA[
arXiv:2505.24511v2 Announce Type: replace 
Abstract: Time series forecasting (TSF) is a fundamental and widely studied task, spanning methods from classical statistical approaches to modern deep learning and multimodal language modeling. Despite their effectiveness, these methods often follow a fast thinking paradigm emphasizing pattern extraction and direct value mapping, while overlooking explicit reasoning over temporal dynamics and contextual dependencies. Meanwhile, emerging slow-thinking LLMs (e.g., ChatGPT-o1, DeepSeek-R1) have demonstrated impressive multi-step reasoning capabilities across diverse domains, suggesting a new opportunity for reframing TSF as a structured reasoning task. This motivates a key question: can slow-thinking LLMs effectively reason over temporal patterns to support time series forecasting, even in zero-shot manner? To investigate this, in this paper, we propose TimeReasoner, an extensive empirical study that formulates TSF as a conditional reasoning task. We design a series of prompting strategies to elicit inference-time reasoning from pretrained slow-thinking LLMs and evaluate their performance across diverse TSF benchmarks. Our findings reveal that slow-thinking LLMs exhibit non-trivial zero-shot forecasting capabilities, especially in capturing high-level trends and contextual shifts. While preliminary, our study surfaces important insights into the reasoning behaviors of LLMs in temporal domains highlighting both their potential and limitations. We hope this work catalyzes further research into reasoning-based forecasting paradigms and paves the way toward more interpretable and generalizable TSF frameworks.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Gradients Rapidly Increase Near the End of Training</title>
<link>https://arxiv.org/abs/2506.02285</link>
<guid>https://arxiv.org/abs/2506.02285</guid>
<content:encoded><![CDATA[
arXiv:2506.02285v2 Announce Type: replace 
Abstract: During long-duration Large Language Model (LLM) training runs the gradient norm increases rapidly near the end of training. In this short note, we show that this increase is due to an unintended interaction between weight decay, normalization layers, and the learning rate schedule. We propose a simple correction that fixes this behavior while also resulting in lower loss values throughout training.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Through a Steerable Lens: Magnifying Neural Network Interpretability via Phase-Based Extrapolation</title>
<link>https://arxiv.org/abs/2506.02300</link>
<guid>https://arxiv.org/abs/2506.02300</guid>
<content:encoded><![CDATA[
arXiv:2506.02300v2 Announce Type: replace 
Abstract: Understanding the internal representations and decision mechanisms of deep neural networks remains a critical open challenge. While existing interpretability methods often identify influential input regions, they may not elucidate how a model distinguishes between classes or what specific changes would transition an input from one category to another. To address these limitations, we propose a novel framework that visualizes the implicit path between classes by treating the network gradient as a form of infinitesimal motion. Drawing inspiration from phase-based motion magnification, we first decompose images using invertible transforms-specifically the Complex Steerable Pyramid-then compute class-conditional gradients in the transformed space. Rather than iteratively integrating the gradient to trace a full path, we amplify the one-step gradient to the input and perform a linear extrapolation to expose how the model moves from source to target class. By operating in the steerable pyramid domain, these amplified gradients produce semantically meaningful, spatially coherent morphs that highlight the classifier's most sensitive directions, giving insight into the geometry of its decision boundaries. Experiments on both synthetic and real-world datasets demonstrate that our phase-focused extrapolation yields perceptually aligned, semantically meaningful transformations, offering a novel, interpretable lens into neural classifiers' internal representations.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Selective Matching Losses -- Not All Scores Are Created Equal</title>
<link>https://arxiv.org/abs/2506.04446</link>
<guid>https://arxiv.org/abs/2506.04446</guid>
<content:encoded><![CDATA[
arXiv:2506.04446v2 Announce Type: replace 
Abstract: Learning systems match predicted scores to observations over some domain. Often, it is critical to produce accurate predictions in some subset (or region) of the domain, yet less important to accurately predict in other regions. We construct selective matching loss functions by design of increasing link functions over score domains. A matching loss is an integral over the link. A link defines loss sensitivity as function of the score, emphasizing high slope high sensitivity regions over flat ones. Loss asymmetry drives a model and resolves its underspecification to predict better in high sensitivity regions where it is more important, and to distinguish between high and low importance regions. A large variety of selective scalar losses can be designed with scaled and shifted Sigmoid and hyperbolic sine links. Their properties, however, do not extend to multi-class. Applying them per dimension lacks ranking sensitivity that assigns importance according to class score ranking. Utilizing composite Softmax functions, we develop a framework for multidimensional selective losses. We overcome limitations of the standard Softmax function, that is good for classification, but not for distinction between adjacent scores. Selective losses have substantial advantage over traditional losses in applications with more important score regions, including dwell-time prediction, retrieval, ranking with either pointwise, contrastive pairwise, or listwise losses, distillation problems, and fine-tuning alignment of Large Language Models (LLMs).
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High-Dimensional Independence Testing via Maximum and Average Distance Correlations</title>
<link>https://arxiv.org/abs/2001.01095</link>
<guid>https://arxiv.org/abs/2001.01095</guid>
<content:encoded><![CDATA[
arXiv:2001.01095v4 Announce Type: replace-cross 
Abstract: This paper investigates the utilization of maximum and average distance correlations for multivariate independence testing. We characterize their consistency properties in high-dimensional settings with respect to the number of marginally dependent dimensions, compare the advantages of each test statistic, examine their respective null distributions, and present a fast chi-square-based testing procedure. The resulting tests are non-parametric and applicable to both Euclidean distance and the Gaussian kernel as the underlying metric. To better understand the practical use cases of the proposed tests, we evaluate the empirical performance of the maximum distance correlation, average distance correlation, and the original distance correlation across various multivariate dependence scenarios, as well as conduct a real data experiment to test the presence of various cancer types and peptide levels in human plasma.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Precise High-Dimensional Asymptotics for Quantifying Heterogeneous Transfers</title>
<link>https://arxiv.org/abs/2010.11750</link>
<guid>https://arxiv.org/abs/2010.11750</guid>
<content:encoded><![CDATA[
arXiv:2010.11750v5 Announce Type: replace-cross 
Abstract: The problem of learning one task using samples from another task is central to transfer learning. In this paper, we focus on answering the following question: when does combining the samples from two related tasks perform better than learning with one target task alone? This question is motivated by an empirical phenomenon known as negative transfer, which has been observed in practice. While the transfer effect from one task to another depends on factors such as their sample sizes and the spectrum of their covariance matrices, precisely quantifying this dependence has remained a challenging problem. In order to compare a transfer learning estimator to single-task learning, one needs to compare the risks between the two estimators precisely. Further, the comparison depends on the distribution shifts between the two tasks. This paper applies recent developments of random matrix theory to tackle this challenge in a high-dimensional linear regression setting with two tasks. We show precise high-dimensional asymptotics for the bias and variance of a classical hard parameter sharing (HPS) estimator in the proportional limit, where the sample sizes of both tasks increase proportionally with dimension at fixed ratios. The precise asymptotics apply to various types of distribution shifts, including covariate shifts, model shifts, and combinations of both. We illustrate these results in a random-effects model to mathematically prove a phase transition from positive to negative transfer as the number of source task samples increases. One insight from the analysis is that a rebalanced HPS estimator, which downsizes the source task when the model shift is high, achieves the minimax optimal rate. The finding regarding phase transition also applies to multiple tasks when covariates are shared across tasks. Simulations validate the accuracy of the high-dimensional asymptotics for finite dimensions.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>General Loss Functions Lead to (Approximate) Interpolation in High Dimensions</title>
<link>https://arxiv.org/abs/2303.07475</link>
<guid>https://arxiv.org/abs/2303.07475</guid>
<content:encoded><![CDATA[
arXiv:2303.07475v2 Announce Type: replace-cross 
Abstract: We provide a unified framework that applies to a general family of convex losses across binary and multiclass settings in the overparameterized regime to approximately characterize the implicit bias of gradient descent in closed form. Specifically, we show that the implicit bias is approximated (but not exactly equal to) the minimum-norm interpolation in high dimensions, which arises from training on the squared loss. In contrast to prior work, which was tailored to exponentially-tailed losses and used the intermediate support-vector-machine formulation, our framework directly builds on the primal-dual analysis of Ji and Telgarsky (2021), allowing us to provide new approximate equivalences for general convex losses through a novel sensitivity analysis. Our framework also recovers existing exact equivalence results for exponentially-tailed losses across binary and multiclass settings. Finally, we provide evidence for the tightness of our techniques and use our results to demonstrate the effect of certain loss functions designed for out-of-distribution problems on the closed-form solution.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Face of Populism: Examining Differences in Facial Emotional Expressions of Political Leaders Using Machine Learning</title>
<link>https://arxiv.org/abs/2304.09914</link>
<guid>https://arxiv.org/abs/2304.09914</guid>
<content:encoded><![CDATA[
arXiv:2304.09914v5 Announce Type: replace-cross 
Abstract: Populist rhetoric employed on online media is characterized as deeply impassioned and often imbued with strong emotions. The aim of this paper is to empirically investigate the differences in affective nonverbal communication of political leaders. We use a deep-learning approach to process a sample of 220 YouTube videos of political leaders from 15 different countries, analyze their facial expressions of emotion and then examine differences in average emotion scores representing the relative presence of 6 emotional states (anger, disgust, fear, happiness, sadness, and surprise) and a neutral expression for each frame of the YouTube video. Based on a sample of manually coded images, we find that this deep-learning approach has 53-60\% agreement with human labels. We observe statistically significant differences in the average score of negative emotions between groups of leaders with varying degrees of populist rhetoric.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Quantum Machine Learning: Current Trends, Challenges, Opportunities, and the Road Ahead</title>
<link>https://arxiv.org/abs/2310.10315</link>
<guid>https://arxiv.org/abs/2310.10315</guid>
<content:encoded><![CDATA[
arXiv:2310.10315v4 Announce Type: replace-cross 
Abstract: Quantum Computing (QC) claims to improve the efficiency of solving complex problems, compared to classical computing. When QC is integrated with Machine Learning (ML), it creates a Quantum Machine Learning (QML) system. This paper aims to provide a thorough understanding of the foundational concepts of QC and its notable advantages over classical computing. Following this, we delve into the key aspects of QML in a detailed and comprehensive manner.
  In this survey, we investigate a variety of QML algorithms, discussing their applicability across different domains. We examine quantum datasets, highlighting their unique characteristics and advantages. The survey also covers the current state of hardware technologies, providing insights into the latest advancements and their implications for QML. Additionally, we review the software tools and simulators available for QML development, discussing their features and usability.
  Furthermore, we explore practical applications of QML, illustrating how it can be leveraged to solve real-world problems more efficiently than classical ML methods. This survey aims to consolidate the current landscape of QML and outline key opportunities and challenges for future research.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Edge Computing based Human-Robot Cognitive Fusion: A Medical Case Study in the Autism Spectrum Disorder Therapy</title>
<link>https://arxiv.org/abs/2401.00776</link>
<guid>https://arxiv.org/abs/2401.00776</guid>
<content:encoded><![CDATA[
arXiv:2401.00776v2 Announce Type: replace-cross 
Abstract: In recent years, edge computing has served as a paradigm that enables many future technologies like AI, Robotics, IoT, and high-speed wireless sensor networks (like 5G) by connecting cloud computing facilities and services to the end users. Especially in medical and healthcare applications, it provides remote patient monitoring and increases voluminous multimedia. From the robotics angle, robot-assisted therapy (RAT) is an active-assistive robotic technology in rehabilitation robotics, attracting researchers to study and benefit people with disability like autism spectrum disorder (ASD) children. However, the main challenge of RAT is that the model capable of detecting the affective states of ASD people exists and can recall individual preferences. Moreover, involving expert diagnosis and recommendations to guide robots in updating the therapy approach to adapt to different statuses and scenarios is a crucial part of the ASD therapy process. This paper proposes the architecture of edge cognitive computing by combining human experts and assisted robots collaborating in the same framework to achieve a seamless remote diagnosis, round-the-clock symptom monitoring, emergency warning, therapy alteration, and advanced assistance.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Architecture Search with Unsupervised Representation Learning</title>
<link>https://arxiv.org/abs/2401.11576</link>
<guid>https://arxiv.org/abs/2401.11576</guid>
<content:encoded><![CDATA[
arXiv:2401.11576v5 Announce Type: replace-cross 
Abstract: Unsupervised representation learning presents new opportunities for advancing Quantum Architecture Search (QAS) on Noisy Intermediate-Scale Quantum (NISQ) devices. QAS is designed to optimize quantum circuits for Variational Quantum Algorithms (VQAs). Most QAS algorithms tightly couple the search space and search algorithm, typically requiring the evaluation of numerous quantum circuits, resulting in high computational costs and limiting scalability to larger quantum circuits. Predictor-based QAS algorithms mitigate this issue by estimating circuit performance based on structure or embedding. However, these methods often demand time-intensive labeling to optimize gate parameters across many circuits, which is crucial for training accurate predictors. Inspired by the classical neural architecture search algorithm Arch2vec, we investigate the potential of unsupervised representation learning for QAS without relying on predictors. Our framework decouples unsupervised architecture representation learning from the search process, enabling the learned representations to be applied across various downstream tasks. Additionally, it integrates an improved quantum circuit graph encoding scheme, addressing the limitations of existing representations and enhancing search efficiency. This predictor-free approach removes the need for large labeled datasets. During the search, we employ REINFORCE and Bayesian Optimization to explore the latent representation space and compare their performance against baseline methods. We further validate our approach by executing the best-discovered MaxCut circuits on IBM's ibm_sherbrooke quantum processor, confirming that the architectures retain optimal performance even under real hardware noise. Our results demonstrate that the framework efficiently identifies high-performing quantum circuits with fewer search iterations.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Discovery of PDEs via the Adjoint Method</title>
<link>https://arxiv.org/abs/2401.17177</link>
<guid>https://arxiv.org/abs/2401.17177</guid>
<content:encoded><![CDATA[
arXiv:2401.17177v4 Announce Type: replace-cross 
Abstract: In this work, we present an adjoint-based method for discovering the underlying governing partial differential equations (PDEs) given data. The idea is to consider a parameterized PDE in a general form and formulate a PDE-constrained optimization problem aimed at minimizing the error of the PDE solution from data. Using variational calculus, we obtain an evolution equation for the Lagrange multipliers (adjoint equations) allowing us to compute the gradient of the objective function with respect to the parameters of PDEs given data in a straightforward manner. In particular, we consider a family of parameterized PDEs encompassing linear, nonlinear, and spatial derivative candidate terms, and elegantly derive the corresponding adjoint equations. We show the efficacy of the proposed approach in identifying the form of the PDE up to machine accuracy, enabling the accurate discovery of PDEs from data. We also compare its performance with the famous PDE Functional Identification of Nonlinear Dynamics method known as PDE-FIND (Rudy et al., 2017), on both smooth and noisy data sets. Even though the proposed adjoint method relies on forward/backward solvers, it outperforms PDE-FIND for large data sets thanks to the analytic expressions for gradients of the cost function with respect to each PDE parameter.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Unsupervised Domain Generalization by Retrieving Across the Modality Gap</title>
<link>https://arxiv.org/abs/2402.04416</link>
<guid>https://arxiv.org/abs/2402.04416</guid>
<content:encoded><![CDATA[
arXiv:2402.04416v3 Announce Type: replace-cross 
Abstract: Domain generalization (DG) is an important problem that learns a model which generalizes to unseen test domains leveraging one or more source domains, under the assumption of shared label spaces. However, most DG methods assume access to abundant source data in the target label space, a requirement that proves overly stringent for numerous real-world applications, where acquiring the same label space as the target task is prohibitively expensive. For this setting, we tackle the multimodal version of the unsupervised domain generalization (MUDG) problem, which uses a large task-agnostic unlabeled source dataset during finetuning. Our framework does not explicitly assume any relationship between the source dataset and target task. Instead, it relies only on the premise that the source dataset can be accurately and efficiently searched in a joint vision-language space. We make three contributions in the MUDG setting. Firstly, we show theoretically that cross-modal approximate nearest neighbor search suffers from low recall due to the large distance between text queries and the image centroids used for coarse quantization. Accordingly, we propose paired k-means, a simple clustering algorithm that improves nearest neighbor recall by storing centroids in query space instead of image space. Secondly, we propose an adaptive text augmentation scheme for target labels designed to improve zero-shot accuracy and diversify retrieved image data. Lastly, we present two simple but effective components to further improve downstream target accuracy. We compare against state-of-the-art name-only transfer, source-free DG and zero-shot (ZS) methods on their respective benchmarks and show consistent improvement in accuracy on 20 diverse datasets. Code is available: https://github.com/Chris210634/mudg
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectral invariance and maximality properties of the frequency spectrum of quantum neural networks</title>
<link>https://arxiv.org/abs/2402.14515</link>
<guid>https://arxiv.org/abs/2402.14515</guid>
<content:encoded><![CDATA[
arXiv:2402.14515v3 Announce Type: replace-cross 
Abstract: Quantum Neural Networks (QNNs) are a popular approach in Quantum Machine Learning.
  We analyze this frequency spectrum using the Minkowski sum for sets and the set of differences, which makes it particularly easy to express and calculate the frequency spectrum algebraically, and prove different maximality results for a large class of models.
  Furthermore, we prove that under some mild conditions there exists a bijection between classes of models with the same area $A:=R\cdot L$ that preserves the frequency spectrum, where $R$ denotes the number of qubits and $L$ the number of layers, which we consequently call spectral invariance under area-preserving transformations. With this we explain the symmetry in $R$ and $L$ in the results often observed in the literature and show that the maximal frequency spectrum depends only on the area $A=RL$ and not on the individual values of $R$ and $L$. Moreover, we collect and extend existing results and specify the maximum possible frequency spectrum of a QNN with arbitrarily many layers as a function of the spectrum of its generators. In the case of arbitrary dimensional generators, where our two introduces notions of maximality differ, we extend existing results based on the so-called Golomb ruler and introduce a second novel approach based on a variation of the turnpike problem, which we call the relaxed turnpike problem.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing the Continuous Structure: Utilizing the First-order Approach in Online Contract Design</title>
<link>https://arxiv.org/abs/2403.07143</link>
<guid>https://arxiv.org/abs/2403.07143</guid>
<content:encoded><![CDATA[
arXiv:2403.07143v3 Announce Type: replace-cross 
Abstract: This work studies the online contract design problem. The principal's goal is to learn the optimal contract that maximizes her utility through repeated interactions, without prior knowledge of the agent's type (i.e., the agent's cost and production functions). We leverage the structure provided by continuous action spaces, which allows the application of first-order conditions (FOC) to characterize the agent's behavior. In some cases, we utilize conditions from the first-order approach (FOA) in economics, but in certain settings, we are able to apply FOC without additional assumptions, leading to simpler and more principled algorithms.
  We illustrate this approach in three problem settings. Firstly, we study the problem of learning the optimal contract when there can be many outcomes. In contrast to prior works that design highly specialized algorithms, we show that the problem can be directly reduced to Lipschitz bandits. Secondly, we study the problem of learning linear contracts. While the contracting problem involves hidden action (moral hazard) and the pricing problem involves hidden value (adverse selection), the two problems share a similar optimization structure, which enables direct reduction between the problem of learning linear contracts and dynamic pricing. Thirdly, we study the problem of learning contracts with many outcomes when agents are identical and provide an algorithm with polynomial sample complexity.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Textual Unlearning Gives a False Sense of Unlearning</title>
<link>https://arxiv.org/abs/2406.13348</link>
<guid>https://arxiv.org/abs/2406.13348</guid>
<content:encoded><![CDATA[
arXiv:2406.13348v3 Announce Type: replace-cross 
Abstract: Language Models (LMs) are prone to ''memorizing'' training data, including substantial sensitive user information. To mitigate privacy risks and safeguard the right to be forgotten, machine unlearning has emerged as a promising approach for enabling LMs to efficiently ''forget'' specific texts. However, despite the good intentions, is textual unlearning really as effective and reliable as expected? To address the concern, we first propose Unlearning Likelihood Ratio Attack+ (U-LiRA+), a rigorous textual unlearning auditing method, and find that unlearned texts can still be detected with very high confidence after unlearning. Further, we conduct an in-depth investigation on the privacy risks of textual unlearning mechanisms in deployment and present the Textual Unlearning Leakage Attack (TULA), along with its variants in both black- and white-box scenarios. We show that textual unlearning mechanisms could instead reveal more about the unlearned texts, exposing them to significant membership inference and data reconstruction risks. Our findings highlight that existing textual unlearning actually gives a false sense of unlearning, underscoring the need for more robust and secure unlearning mechanisms.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-like object concept representations emerge naturally in multimodal large language models</title>
<link>https://arxiv.org/abs/2407.01067</link>
<guid>https://arxiv.org/abs/2407.01067</guid>
<content:encoded><![CDATA[
arXiv:2407.01067v2 Announce Type: replace-cross 
Abstract: Understanding how humans conceptualize and categorize natural objects offers critical insights into perception and cognition. With the advent of Large Language Models (LLMs), a key question arises: can these models develop human-like object representations from linguistic and multimodal data? In this study, we combined behavioral and neuroimaging analyses to explore the relationship between object concept representations in LLMs and human cognition. We collected 4.7 million triplet judgments from LLMs and Multimodal LLMs (MLLMs) to derive low-dimensional embeddings that capture the similarity structure of 1,854 natural objects. The resulting 66-dimensional embeddings were stable, predictive, and exhibited semantic clustering similar to human mental representations. Remarkably, the dimensions underlying these embeddings were interpretable, suggesting that LLMs and MLLMs develop human-like conceptual representations of objects. Further analysis showed strong alignment between model embeddings and neural activity patterns in brain regions such as EBA, PPA, RSC, and FFA. This provides compelling evidence that the object representations in LLMs, while not identical to human ones, share fundamental similarities that reflect key aspects of human conceptual knowledge. Our findings advance the understanding of machine intelligence and inform the development of more human-like artificial cognitive systems.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributionally and Adversarially Robust Logistic Regression via Intersecting Wasserstein Balls</title>
<link>https://arxiv.org/abs/2407.13625</link>
<guid>https://arxiv.org/abs/2407.13625</guid>
<content:encoded><![CDATA[
arXiv:2407.13625v3 Announce Type: replace-cross 
Abstract: Adversarially robust optimization (ARO) has emerged as the *de facto* standard for training models that hedge against adversarial attacks in the test stage. While these models are robust against adversarial attacks, they tend to suffer severely from overfitting. To address this issue, some successful methods replace the empirical distribution in the training stage with alternatives including *(i)* a worst-case distribution residing in an ambiguity set, resulting in a distributionally robust (DR) counterpart of ARO; *(ii)* a mixture of the empirical distribution with a distribution induced by an auxiliary (*e.g.*, synthetic, external, out-of-domain) dataset. Inspired by the former, we study the Wasserstein DR counterpart of ARO for logistic regression and show it admits a tractable convex optimization reformulation. Adopting the latter setting, we revise the DR approach by intersecting its ambiguity set with another ambiguity set built using the auxiliary dataset, which offers a significant improvement whenever the Wasserstein distance between the data generating and auxiliary distributions can be estimated. We study the underlying optimization problem, develop efficient solution algorithms, and demonstrate that the proposed method outperforms benchmark approaches on standard datasets.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Explainable Vision Transformer with Transfer Learning Combined with Support Vector Machine Based Efficient Drought Stress Identification</title>
<link>https://arxiv.org/abs/2407.21666</link>
<guid>https://arxiv.org/abs/2407.21666</guid>
<content:encoded><![CDATA[
arXiv:2407.21666v2 Announce Type: replace-cross 
Abstract: Early detection of drought stress is critical for taking timely measures for reducing crop loss before the drought impact becomes irreversible. The subtle phenotypical and physiological changes in response to drought stress are captured by non-invasive imaging techniques and these imaging data serve as valuable resource for machine learning methods to identify drought stress. While convolutional neural networks (CNNs) are in wide use, vision transformers (ViTs) present a promising alternative in capturing long-range dependencies and intricate spatial relationships, thereby enhancing the detection of subtle indicators of drought stress. We propose an explainable deep learning pipeline that leverages the power of ViTs for drought stress detection in potato crops using aerial imagery. We applied two distinct approaches: a synergistic combination of ViT and support vector machine (SVM), where ViT extracts intricate spatial features from aerial images, and SVM classifies the crops as stressed or healthy and an end-to-end approach using a dedicated classification layer within ViT to directly detect drought stress. Our key findings explain the ViT model's decision-making process by visualizing attention maps. These maps highlight the specific spatial features within the aerial images that the ViT model focuses as the drought stress signature. Our findings demonstrate that the proposed methods not only achieve high accuracy in drought stress identification but also shedding light on the diverse subtle plant features associated with drought stress. This offers a robust and interpretable solution for drought stress monitoring for farmers to undertake informed decisions for improved crop management.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Penalty Learning for Optimal Partitioning using Multilayer Perceptron</title>
<link>https://arxiv.org/abs/2408.00856</link>
<guid>https://arxiv.org/abs/2408.00856</guid>
<content:encoded><![CDATA[
arXiv:2408.00856v4 Announce Type: replace-cross 
Abstract: Changepoint detection is a technique used to identify significant shifts in sequences and is widely used in fields such as finance, genomics, and medicine. To identify the changepoints, dynamic programming (DP) algorithms, particularly Optimal Partitioning (OP) family, are widely used. To control the changepoints count, these algorithms use a fixed penalty to penalize the changepoints presence. To predict the optimal value of that penalty, existing methods used simple models such as linear or tree-based, which may limit predictive performance. To address this issue, this study proposes using a multilayer perceptron (MLP) with a ReLU activation function to predict the penalty. The proposed model generates continuous predictions -- as opposed to the stepwise ones in tree-based models -- and handles non-linearity better than linear models. Experiments on large benchmark genomic datasets demonstrate that the proposed model improves accuracy and F1 score compared to existing models.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Relational decomposition for program synthesis</title>
<link>https://arxiv.org/abs/2408.12212</link>
<guid>https://arxiv.org/abs/2408.12212</guid>
<content:encoded><![CDATA[
arXiv:2408.12212v3 Announce Type: replace-cross 
Abstract: We introduce a relational approach to program synthesis. The key idea is to decompose synthesis tasks into simpler relational synthesis subtasks. Specifically, our representation decomposes a training input-output example into sets of input and output facts respectively. We then learn relations between the input and output facts. We demonstrate our approach using an off-the-shelf inductive logic programming (ILP) system on four challenging synthesis datasets. Our results show that (i) our representation can outperform a standard one, and (ii) an off-the-shelf ILP system with our representation can outperform domain-specific approaches.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of the Self Supervised Learning Mechanisms for Vision Transformers</title>
<link>https://arxiv.org/abs/2408.17059</link>
<guid>https://arxiv.org/abs/2408.17059</guid>
<content:encoded><![CDATA[
arXiv:2408.17059v5 Announce Type: replace-cross 
Abstract: Vision Transformers (ViTs) have recently demonstrated remarkable performance in computer vision tasks. However, their parameter-intensive nature and reliance on large amounts of data for effective performance have shifted the focus from traditional human-annotated labels to unsupervised learning and pretraining strategies that uncover hidden structures within the data. In response to this challenge, self-supervised learning (SSL) has emerged as a promising paradigm. SSL leverages inherent relationships within the data itself as a form of supervision, eliminating the need for manual labeling and offering a more scalable and resource-efficient alternative for model training. Given these advantages, it is imperative to explore the integration of SSL techniques with ViTs, particularly in scenarios with limited labeled data. Inspired by this evolving trend, this survey aims to systematically review SSL mechanisms tailored for ViTs. We propose a comprehensive taxonomy to classify SSL techniques based on their representations and pre-training tasks. Additionally, we discuss the motivations behind SSL, review prominent pre-training tasks, and highlight advancements and challenges in this field. Furthermore, we conduct a comparative analysis of various SSL methods designed for ViTs, evaluating their strengths, limitations, and applicability to different scenarios.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Efficient Representations of Neutrino Telescope Events</title>
<link>https://arxiv.org/abs/2410.13148</link>
<guid>https://arxiv.org/abs/2410.13148</guid>
<content:encoded><![CDATA[
arXiv:2410.13148v2 Announce Type: replace-cross 
Abstract: Neutrino telescopes detect rare interactions of particles produced in some of the most extreme environments in the Universe. This is accomplished by instrumenting a cubic-kilometer volume of naturally occurring transparent medium with light sensors. Given their substantial size and the high frequency of background interactions, these telescopes amass an enormous quantity of large variance, high-dimensional data. These attributes create substantial challenges for analyzing and reconstructing interactions, particularly when utilizing machine learning (ML) techniques. In this paper, we present a novel approach, called om2vec, that employs transformer-based variational autoencoders to efficiently represent neutrino telescope events by learning compact and descriptive latent representations. We demonstrate that these latent representations offer enhanced flexibility and improved computational efficiency, thereby facilitating downstream tasks in data analysis.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video Even in VLMs</title>
<link>https://arxiv.org/abs/2410.16267</link>
<guid>https://arxiv.org/abs/2410.16267</guid>
<content:encoded><![CDATA[
arXiv:2410.16267v2 Announce Type: replace-cross 
Abstract: We present xGen-MM-Vid (BLIP-3-Video): a multimodal language model for videos, particularly designed to efficiently capture temporal information over multiple frames. BLIP-3-Video takes advantage of the 'temporal encoder' in addition to the conventional visual tokenizer, which maps a sequence of tokens over multiple frames into a compact set of visual tokens. This enables BLIP3-Video to use much fewer visual tokens than its competing models (e.g., 32 vs. 4608 tokens). We explore different types of temporal encoders, including learnable spatio-temporal pooling as well as sequential models like Token Turing Machines. We experimentally confirm that BLIP-3-Video obtains video question-answering accuracies comparable to much larger state-of-the-art models (e.g., 34B), while being much smaller (i.e., 4B) and more efficient by using fewer visual tokens. The project website is at https://www.salesforceairesearch.com/opensource/xGen-MM-Vid/index.html
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UnCLe: Benchmarking Unsupervised Continual Learning for Depth Completion</title>
<link>https://arxiv.org/abs/2410.18074</link>
<guid>https://arxiv.org/abs/2410.18074</guid>
<content:encoded><![CDATA[
arXiv:2410.18074v4 Announce Type: replace-cross 
Abstract: We propose UnCLe, the first standardized benchmark for Unsupervised Continual Learning of a multimodal 3D reconstruction task: Depth completion aims to infer a dense depth map from a pair of synchronized RGB image and sparse depth map. We benchmark depth completion models under the practical scenario of unsupervised learning over continuous streams of data. While unsupervised learning of depth boasts the possibility continual learning of novel data distributions over time, existing methods are typically trained on a static, or stationary, dataset. However, when adapting to novel nonstationary distributions, they ``catastrophically forget'' previously learned information. UnCLe simulates these non-stationary distributions by adapting depth completion models to sequences of datasets containing diverse scenes captured from distinct domains using different visual and range sensors. We adopt representative methods from continual learning paradigms and translate them to enable unsupervised continual learning of depth completion. We benchmark these models across indoor and outdoor environments, and investigate the degree of catastrophic forgetting through standard quantitative metrics. We find that unsupervised continual learning of depth completion is an open problem, and we invite researchers to leverage UnCLe as a development platform.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cooperative and Collaborative Multi-Task Semantic Communication for Distributed Sources</title>
<link>https://arxiv.org/abs/2411.02150</link>
<guid>https://arxiv.org/abs/2411.02150</guid>
<content:encoded><![CDATA[
arXiv:2411.02150v2 Announce Type: replace-cross 
Abstract: In this paper, we explore a multi-task semantic communication (SemCom) system for distributed sources, extending the existing focus on collaborative single-task execution. We build on the cooperative multi-task processing introduced in [1], which divides the encoder into a common unit (CU) and multiple specific units (SUs). While earlier studies in multi-task SemCom focused on full observation settings, our research explores a more realistic case where only distributed partial observations are available, such as in a production line monitored by multiple sensing nodes. To address this, we propose an SemCom system that supports multi-task processing through cooperation on the transmitter side via split structure and collaboration on the receiver side. We have used an information-theoretic perspective with variational approximations for our end-to-end data-driven approach. Simulation results demonstrate that the proposed cooperative and collaborative multi-task (CCMT) SemCom system significantly improves task execution accuracy, particularly in complex datasets, if the noise introduced from the communication channel is not limiting the task performance too much. Our findings contribute to a more general SemCom framework capable of handling distributed sources and multiple tasks simultaneously, advancing the applicability of SemCom systems in real-world scenarios.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grouped Discrete Representation for Object-Centric Learning</title>
<link>https://arxiv.org/abs/2411.02299</link>
<guid>https://arxiv.org/abs/2411.02299</guid>
<content:encoded><![CDATA[
arXiv:2411.02299v2 Announce Type: replace-cross 
Abstract: Object-Centric Learning (OCL) aims to discover objects in images or videos by reconstructing the input. Representative methods achieve this by reconstructing the input as its Variational Autoencoder (VAE) discrete representations, which suppress (super-)pixel noise and enhance object separability. However, these methods treat features as indivisible units, overlooking their compositional attributes, and discretize features via scalar code indexes, losing attribute-level similarities and differences. We propose Grouped Discrete Representation (GDR) for OCL. For better generalization, features are decomposed into combinatorial attributes by organized channel grouping. For better convergence, features are quantized into discrete representations via tuple code indexes. Experiments demonstrate that GDR consistently improves both mainstream and state-of-the-art OCL methods across various datasets. Visualizations further highlight GDR's superior object separability and interpretability. The source code is available on https://github.com/Genera1Z/GroupedDiscreteRepresentation.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Innate-Values-driven Reinforcement Learning based Cognitive Modeling</title>
<link>https://arxiv.org/abs/2411.09160</link>
<guid>https://arxiv.org/abs/2411.09160</guid>
<content:encoded><![CDATA[
arXiv:2411.09160v2 Announce Type: replace-cross 
Abstract: Innate values describe agents' intrinsic motivations, which reflect their inherent interests and preferences for pursuing goals and drive them to develop diverse skills that satisfy their various needs. Traditional reinforcement learning (RL) is learning from interaction based on the feedback rewards of the environment. However, in real scenarios, the rewards are generated by agents' innate value systems, which differ vastly from individuals based on their needs and requirements. In other words, considering the AI agent as a self-organizing system, developing its awareness through balancing internal and external utilities based on its needs in different tasks is a crucial problem for individuals learning to support others and integrate community with safety and harmony in the long term. To address this gap, we propose a new RL model termed innate-values-driven RL (IVRL) based on combined motivations' models and expected utility theory to mimic its complex behaviors in the evolution through decision-making and learning. Then, we introduce two IVRL-based models: IV-DQN and IV-A2C. By comparing them with benchmark algorithms such as DQN, DDQN, A2C, and PPO in the Role-Playing Game (RPG) reinforcement learning test platform VIZDoom, we demonstrated that the IVRL-based models can help the agent rationally organize various needs, achieve better performance effectively.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PrisonBreak: Jailbreaking Large Language Models with Fewer Than Twenty-Five Targeted Bit-flips</title>
<link>https://arxiv.org/abs/2412.07192</link>
<guid>https://arxiv.org/abs/2412.07192</guid>
<content:encoded><![CDATA[
arXiv:2412.07192v2 Announce Type: replace-cross 
Abstract: We introduce a new class of attacks on commercial-scale (human-aligned) language models that induce jailbreaking through targeted bitwise corruptions in model parameters. Our adversary can jailbreak billion-parameter language models with fewer than 25 bit-flips in all cases$-$and as few as 5 in some$-$using up to 40$\times$ less bit-flips than existing attacks on computer vision models at least 100$\times$ smaller. Unlike prompt-based jailbreaks, our attack renders these models in memory 'uncensored' at runtime, allowing them to generate harmful responses without any input modifications. Our attack algorithm efficiently identifies target bits to flip, offering up to 20$\times$ more computational efficiency than previous methods. This makes it practical for language models with billions of parameters. We show an end-to-end exploitation of our attack using software-induced fault injection, Rowhammer (RH). Our work examines 56 DRAM RH profiles from DDR4 and LPDDR4X devices with different RH vulnerabilities. We show that our attack can reliably induce jailbreaking in systems similar to those affected by prior bit-flip attacks. Moreover, our approach remains effective even against highly RH-secure systems (e.g., 46$\times$ more secure than previously tested systems). Our analyses further reveal that: (1) models with less post-training alignment require fewer bit flips to jailbreak; (2) certain model components, such as value projection layers, are substantially more vulnerable than others; and (3) our method is mechanistically different than existing jailbreaks. Our findings highlight a pressing, practical threat to the language model ecosystem and underscore the need for research to protect these models from bit-flip attacks.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JuStRank: Benchmarking LLM Judges for System Ranking</title>
<link>https://arxiv.org/abs/2412.09569</link>
<guid>https://arxiv.org/abs/2412.09569</guid>
<content:encoded><![CDATA[
arXiv:2412.09569v2 Announce Type: replace-cross 
Abstract: Given the rapid progress of generative AI, there is a pressing need to systematically compare and choose between the numerous models and configurations available. The scale and versatility of such evaluations make the use of LLM-based judges a compelling solution for this challenge. Crucially, this approach requires first to validate the quality of the LLM judge itself. Previous work has focused on instance-based assessment of LLM judges, where a judge is evaluated over a set of responses, or response pairs, while being agnostic to their source systems. We argue that this setting overlooks critical factors affecting system-level ranking, such as a judge's positive or negative bias towards certain systems. To address this gap, we conduct the first large-scale study of LLM judges as system rankers. System scores are generated by aggregating judgment scores over multiple system outputs, and the judge's quality is assessed by comparing the resulting system ranking to a human-based ranking. Beyond overall judge assessment, our analysis provides a fine-grained characterization of judge behavior, including their decisiveness and bias.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SnapGen-V: Generating a Five-Second Video within Five Seconds on a Mobile Device</title>
<link>https://arxiv.org/abs/2412.10494</link>
<guid>https://arxiv.org/abs/2412.10494</guid>
<content:encoded><![CDATA[
arXiv:2412.10494v2 Announce Type: replace-cross 
Abstract: We have witnessed the unprecedented success of diffusion-based video generation over the past year. Recently proposed models from the community have wielded the power to generate cinematic and high-resolution videos with smooth motions from arbitrary input prompts. However, as a supertask of image generation, video generation models require more computation and are thus hosted mostly on cloud servers, limiting broader adoption among content creators. In this work, we propose a comprehensive acceleration framework to bring the power of the large-scale video diffusion model to the hands of edge users. From the network architecture scope, we initialize from a compact image backbone and search out the design and arrangement of temporal layers to maximize hardware efficiency. In addition, we propose a dedicated adversarial fine-tuning algorithm for our efficient model and reduce the denoising steps to 4. Our model, with only 0.6B parameters, can generate a 5-second video on an iPhone 16 PM within 5 seconds. Compared to server-side models that take minutes on powerful GPUs to generate a single video, we accelerate the generation by magnitudes while delivering on-par quality.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-SpaCE: Multi-Objective Subsequence-based Sparse Counterfactual Explanations for Multivariate Time Series Classification</title>
<link>https://arxiv.org/abs/2501.04009</link>
<guid>https://arxiv.org/abs/2501.04009</guid>
<content:encoded><![CDATA[
arXiv:2501.04009v2 Announce Type: replace-cross 
Abstract: Deep Learning systems excel in complex tasks but often lack transparency, limiting their use in critical applications. Counterfactual explanations, a core tool within eXplainable Artificial Intelligence (XAI), offer insights into model decisions by identifying minimal changes to an input to alter its predicted outcome. However, existing methods for time series data are limited by univariate assumptions, rigid constraints on modifications, or lack of validity guarantees. This paper introduces Multi-SpaCE, a multi-objective counterfactual explanation method for multivariate time series. Using non-dominated ranking genetic algorithm II (NSGA-II), Multi-SpaCE balances proximity, sparsity, plausibility, and contiguity. Unlike most methods, it ensures perfect validity, supports multivariate data and provides a Pareto front of solutions, enabling flexibility to different end-user needs. Comprehensive experiments in diverse datasets demonstrate the ability of Multi-SpaCE to consistently achieve perfect validity and deliver superior performance compared to existing methods.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generation from Noisy Examples</title>
<link>https://arxiv.org/abs/2501.04179</link>
<guid>https://arxiv.org/abs/2501.04179</guid>
<content:encoded><![CDATA[
arXiv:2501.04179v2 Announce Type: replace-cross 
Abstract: We continue to study the learning-theoretic foundations of generation by extending the results from Kleinberg and Mullainathan [2024] and Li et al. [2024] to account for noisy example streams. In the noiseless setting of Kleinberg and Mullainathan [2024] and Li et al. [2024], an adversary picks a hypothesis from a binary hypothesis class and provides a generator with a sequence of its positive examples. The goal of the generator is to eventually output new, unseen positive examples. In the noisy setting, an adversary still picks a hypothesis and a sequence of its positive examples. But, before presenting the stream to the generator, the adversary inserts a finite number of negative examples. Unaware of which examples are noisy, the goal of the generator is to still eventually output new, unseen positive examples. In this paper, we provide necessary and sufficient conditions for when a binary hypothesis class can be noisily generatable. We provide such conditions with respect to various constraints on the number of distinct examples that need to be seen before perfect generation of positive examples. Interestingly, for finite and countable classes we show that generatability is largely unaffected by the presence of a finite number of noisy examples.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Digital Twin Synchronization: Bridging the Sim-RL Agent to a Real-Time Robotic Additive Manufacturing Control</title>
<link>https://arxiv.org/abs/2501.18016</link>
<guid>https://arxiv.org/abs/2501.18016</guid>
<content:encoded><![CDATA[
arXiv:2501.18016v2 Announce Type: replace-cross 
Abstract: With the rapid development of deep reinforcement learning technology, it gradually demonstrates excellent potential and is becoming the most promising solution in the robotics. However, in the smart manufacturing domain, there is still not too much research involved in dynamic adaptive control mechanisms optimizing complex processes. This research advances the integration of Soft Actor-Critic (SAC) with digital twins for industrial robotics applications, providing a framework for enhanced adaptive real-time control for smart additive manufacturing processing. The system architecture combines Unity's simulation environment with ROS2 for seamless digital twin synchronization, while leveraging transfer learning to efficiently adapt trained models across tasks. We demonstrate our methodology using a Viper X300s robot arm with the proposed hierarchical reward structure to address the common reinforcement learning challenges in two distinct control scenarios. The results show rapid policy convergence and robust task execution in both simulated and physical environments demonstrating the effectiveness of our approach.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectral Estimators for Multi-Index Models: Precise Asymptotics and Optimal Weak Recovery</title>
<link>https://arxiv.org/abs/2502.01583</link>
<guid>https://arxiv.org/abs/2502.01583</guid>
<content:encoded><![CDATA[
arXiv:2502.01583v2 Announce Type: replace-cross 
Abstract: Multi-index models provide a popular framework to investigate the learnability of functions with low-dimensional structure and, also due to their connections with neural networks, they have been object of recent intensive study. In this paper, we focus on recovering the subspace spanned by the signals via spectral estimators -- a family of methods routinely used in practice, often as a warm-start for iterative algorithms. Our main technical contribution is a precise asymptotic characterization of the performance of spectral methods, when sample size and input dimension grow proportionally and the dimension $p$ of the space to recover is fixed. Specifically, we locate the top-$p$ eigenvalues of the spectral matrix and establish the overlaps between the corresponding eigenvectors (which give the spectral estimators) and a basis of the signal subspace. Our analysis unveils a phase transition phenomenon in which, as the sample complexity grows, eigenvalues escape from the bulk of the spectrum and, when that happens, eigenvectors recover directions of the desired subspace. The precise characterization we put forward enables the optimization of the data preprocessing, thus allowing to identify the spectral estimator that requires the minimal sample size for weak recovery.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-Aligned Image Models Improve Visual Decoding from the Brain</title>
<link>https://arxiv.org/abs/2502.03081</link>
<guid>https://arxiv.org/abs/2502.03081</guid>
<content:encoded><![CDATA[
arXiv:2502.03081v3 Announce Type: replace-cross 
Abstract: Decoding visual images from brain activity has significant potential for advancing brain-computer interaction and enhancing the understanding of human perception. Recent approaches align the representation spaces of images and brain activity to enable visual decoding. In this paper, we introduce the use of human-aligned image encoders to map brain signals to images. We hypothesize that these models more effectively capture perceptual attributes associated with the rapid visual stimuli presentations commonly used in visual brain data recording experiments. Our empirical results support this hypothesis, demonstrating that this simple modification improves image retrieval accuracy by up to 21% compared to state-of-the-art methods. Comprehensive experiments confirm consistent performance improvements across diverse EEG architectures, image encoders, alignment methods, participants, and brain imaging modalities
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In Praise of Stubbornness: An Empirical Case for Cognitive-Dissonance Aware Continual Update of Knowledge in LLMs</title>
<link>https://arxiv.org/abs/2502.04390</link>
<guid>https://arxiv.org/abs/2502.04390</guid>
<content:encoded><![CDATA[
arXiv:2502.04390v2 Announce Type: replace-cross 
Abstract: Through systematic empirical investigation, we uncover a fundamental and concerning property of Large Language Models: while they can safely learn facts that don't contradict their knowledge, attempting to update facts with contradictory information triggers catastrophic corruption of unrelated knowledge. Unlike humans, who naturally resist contradictory information, these models indiscriminately accept contradictions, leading to devastating interference, destroying up to 80% of unrelated knowledge even when learning as few as 10-100 contradicting facts. To understand whether this interference could be mitigated through selective plasticity, we experiment with targeted network updates, distinguishing between previously used (stubborn) and rarely used (plastic) neurons. We uncover another asymmetry: while sparing frequently-used neurons significantly improves retention of existing knowledge for non-contradictory updates (98% vs 93% with standard updates), contradictory updates trigger catastrophic interference regardless of targeting strategy. This effect which persists across tested model scales (GPT-2 to GPT-J-6B), suggests a fundamental limitation in how neural networks handle contradictions. Finally, we demonstrate that contradictory information can be reliably detected (95%+ accuracy) using simple model features, offering a potential protective mechanism. These findings motivate new architectures that can, like humans, naturally resist contradictions rather than allowing destructive overwrites.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distinguishing Cause from Effect with Causal Velocity Models</title>
<link>https://arxiv.org/abs/2502.05122</link>
<guid>https://arxiv.org/abs/2502.05122</guid>
<content:encoded><![CDATA[
arXiv:2502.05122v2 Announce Type: replace-cross 
Abstract: Bivariate structural causal models (SCM) are often used to infer causal direction by examining their goodness-of-fit under restricted model classes. In this paper, we describe a parametrization of bivariate SCMs in terms of a causal velocity by viewing the cause variable as time in a dynamical system. The velocity implicitly defines counterfactual curves via the solution of initial value problems where the observation specifies the initial condition. Using tools from measure transport, we obtain a unique correspondence between SCMs and the score function of the generated distribution via its causal velocity. Based on this, we derive an objective function that directly regresses the velocity against the score function, the latter of which can be estimated non-parametrically from observational data. We use this to develop a method for bivariate causal discovery that extends beyond known model classes such as additive or location scale noise, and that requires no assumptions on the noise distributions. When the score is estimated well, the objective is also useful for detecting model non-identifiability and misspecification. We present positive results in simulation and benchmark experiments where many existing methods fail, and perform ablation studies to examine the method's sensitivity to accurate score estimation.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating LLM Inference with Lossless Speculative Decoding Algorithms for Heterogeneous Vocabularies</title>
<link>https://arxiv.org/abs/2502.05202</link>
<guid>https://arxiv.org/abs/2502.05202</guid>
<content:encoded><![CDATA[
arXiv:2502.05202v2 Announce Type: replace-cross 
Abstract: Accelerating the inference of large language models (LLMs) is a critical challenge in generative AI. Speculative decoding (SD) methods offer substantial efficiency gains by generating multiple tokens using a single target forward pass. However, existing SD approaches require the drafter and target models to share the same vocabulary, thus limiting the pool of possible drafters, often necessitating the training of a drafter from scratch. We present three new SD methods that remove this shared-vocabulary constraint. All three methods preserve the target distribution (i.e., they are lossless) and work with off-the-shelf models without requiring additional training or modifications. Empirically, on summarization, programming, and long-context tasks, our algorithms demonstrate significant speedups of up to 2.8x over standard autoregressive decoding. By enabling any off-the-shelf model to serve as a drafter and requiring no retraining, this work substantially broadens the applicability of the SD framework in practice.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Epistemic Uncertainty in Conformal Scores: A Unified Approach</title>
<link>https://arxiv.org/abs/2502.06995</link>
<guid>https://arxiv.org/abs/2502.06995</guid>
<content:encoded><![CDATA[
arXiv:2502.06995v2 Announce Type: replace-cross 
Abstract: Conformal prediction methods create prediction bands with distribution-free guarantees but do not explicitly capture epistemic uncertainty, which can lead to overconfident predictions in data-sparse regions. Although recent conformal scores have been developed to address this limitation, they are typically designed for specific tasks, such as regression or quantile regression. Moreover, they rely on particular modeling choices for epistemic uncertainty, restricting their applicability. We introduce $\texttt{EPICSCORE}$, a model-agnostic approach that enhances any conformal score by explicitly integrating epistemic uncertainty. Leveraging Bayesian techniques such as Gaussian Processes, Monte Carlo Dropout, or Bayesian Additive Regression Trees, $\texttt{EPICSCORE}$ adaptively expands predictive intervals in regions with limited data while maintaining compact intervals where data is abundant. As with any conformal method, it preserves finite-sample marginal coverage. Additionally, it also achieves asymptotic conditional coverage. Experiments demonstrate its good performance compared to existing methods. Designed for compatibility with any Bayesian model, but equipped with distribution-free guarantees, $\texttt{EPICSCORE}$ provides a general-purpose framework for uncertainty quantification in prediction problems.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Monte Carlo Tree Diffusion for System 2 Planning</title>
<link>https://arxiv.org/abs/2502.07202</link>
<guid>https://arxiv.org/abs/2502.07202</guid>
<content:encoded><![CDATA[
arXiv:2502.07202v3 Announce Type: replace-cross 
Abstract: Diffusion models have recently emerged as a powerful tool for planning. However, unlike Monte Carlo Tree Search (MCTS)-whose performance naturally improves with inference-time computation scaling-standard diffusion-based planners offer only limited avenues for the scalability. In this paper, we introduce Monte Carlo Tree Diffusion (MCTD), a novel framework that integrates the generative strength of diffusion models with the adaptive search capabilities of MCTS. Our method reconceptualizes denoising as a tree-structured process, allowing partially denoised plans to be iteratively evaluated, pruned, and refined. By selectively expanding promising trajectories while retaining the flexibility to revisit and improve suboptimal branches, MCTD achieves the benefits of MCTS such as controlling exploration-exploitation trade-offs within the diffusion framework. Empirical results on challenging long-horizon tasks show that MCTD outperforms diffusion baselines, yielding higher-quality solutions as inference-time computation increases.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRAVEL: Training-Free Retrieval and Alignment for Vision-and-Language Navigation</title>
<link>https://arxiv.org/abs/2502.07306</link>
<guid>https://arxiv.org/abs/2502.07306</guid>
<content:encoded><![CDATA[
arXiv:2502.07306v2 Announce Type: replace-cross 
Abstract: In this work, we propose a modular approach for the Vision-Language Navigation (VLN) task by decomposing the problem into four sub-modules that use state-of-the-art Large Language Models (LLMs) and Vision-Language Models (VLMs) in a zero-shot setting. Given navigation instruction in natural language, we first prompt LLM to extract the landmarks and the order in which they are visited. Assuming the known model of the environment, we retrieve the top-k locations of the last landmark and generate $k$ path hypotheses from the starting location to the last landmark using the shortest path algorithm on the topological map of the environment. Each path hypothesis is represented by a sequence of panoramas. We then use dynamic programming to compute the alignment score between the sequence of panoramas and the sequence of landmark names, which match scores obtained from VLM. Finally, we compute the nDTW metric between the hypothesis that yields the highest alignment score to evaluate the path fidelity. We demonstrate superior performance compared to other approaches that use joint semantic maps like VLMaps on the complex R2R-Habitat instruction dataset and quantify in detail the effect of visual grounding on navigation performance.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robot Pouring: Identifying Causes of Spillage and Selecting Alternative Action Parameters Using Probabilistic Actual Causation</title>
<link>https://arxiv.org/abs/2502.09395</link>
<guid>https://arxiv.org/abs/2502.09395</guid>
<content:encoded><![CDATA[
arXiv:2502.09395v3 Announce Type: replace-cross 
Abstract: In everyday life, we perform tasks (e.g., cooking or cleaning) that involve a large variety of objects and goals. When confronted with an unexpected or unwanted outcome, we take corrective actions and try again until achieving the desired result. The reasoning performed to identify a cause of the observed outcome and to select an appropriate corrective action is a crucial aspect of human reasoning for successful task execution. Central to this reasoning is the assumption that a factor is responsible for producing the observed outcome. In this paper, we investigate the use of probabilistic actual causation to determine whether a factor is the cause of an observed undesired outcome. Furthermore, we show how the actual causation probabilities can be used to find alternative actions to change the outcome. We apply the probabilistic actual causation analysis to a robot pouring task. When spillage occurs, the analysis indicates whether a task parameter is the cause and how it should be changed to avoid spillage. The analysis requires a causal graph of the task and the corresponding conditional probability distributions. To fulfill these requirements, we perform a complete causal modeling procedure (i.e., task analysis, definition of variables, determination of the causal graph structure, and estimation of conditional probability distributions) using data from a realistic simulation of the robot pouring task, covering a large combinatorial space of task parameters. Based on the results, we discuss the implications of the variables' representation and how the alternative actions suggested by the actual causation analysis would compare to the alternative solutions proposed by a human observer. The practical use of the analysis of probabilistic actual causation to select alternative action parameters is demonstrated.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Computational Advantage of Depth: Learning High-Dimensional Hierarchical Functions with Gradient Descent</title>
<link>https://arxiv.org/abs/2502.13961</link>
<guid>https://arxiv.org/abs/2502.13961</guid>
<content:encoded><![CDATA[
arXiv:2502.13961v2 Announce Type: replace-cross 
Abstract: Understanding the advantages of deep neural networks trained by gradient descent (GD) compared to shallow models remains an open theoretical challenge. In this paper, we introduce a class of target functions (single and multi-index Gaussian hierarchical targets) that incorporate a hierarchy of latent subspace dimensionalities. This framework enables us to analytically study the learning dynamics and generalization performance of deep networks compared to shallow ones in the high-dimensional limit. Specifically, our main theorem shows that feature learning with GD successively reduces the effective dimensionality, transforming a high-dimensional problem into a sequence of lower-dimensional ones. This enables learning the target function with drastically less samples than with shallow networks. While the results are proven in a controlled training setting, we also discuss more common training procedures and argue that they learn through the same mechanisms.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EquivaMap: Leveraging LLMs for Automatic Equivalence Checking of Optimization Formulations</title>
<link>https://arxiv.org/abs/2502.14760</link>
<guid>https://arxiv.org/abs/2502.14760</guid>
<content:encoded><![CDATA[
arXiv:2502.14760v2 Announce Type: replace-cross 
Abstract: A fundamental problem in combinatorial optimization is identifying equivalent formulations. Despite the growing need for automated equivalence checks -- driven, for example, by optimization copilots, which generate problem formulations from natural language descriptions -- current approaches rely on simple heuristics that fail to reliably check formulation equivalence. Inspired by Karp reductions, in this work we introduce Quasi-Karp equivalence, a formal criterion for determining when two optimization formulations are equivalent based on the existence of a mapping between their decision variables. We propose EquivaMap, a framework that leverages large language models to automatically discover such mappings for scalable, reliable equivalence checking, with a verification stage that ensures mapped solutions preserve feasibility and optimality without additional solver calls. To evaluate our approach, we construct EquivaFormulation, the first open-source dataset of equivalent optimization formulations, generated by applying transformations such as adding slack variables or valid inequalities to existing formulations. Empirically, EquivaMap significantly outperforms existing methods, achieving substantial improvements in correctly identifying formulation equivalence.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Training Elicits Concise Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2502.20122</link>
<guid>https://arxiv.org/abs/2502.20122</guid>
<content:encoded><![CDATA[
arXiv:2502.20122v3 Announce Type: replace-cross 
Abstract: Chain-of-thought (CoT) reasoning has enabled large language models (LLMs) to utilize additional computation through intermediate tokens to solve complex tasks. However, we posit that typical reasoning traces contain many redundant tokens, incurring extraneous inference costs. Upon examination of the output distribution of current LLMs, we find evidence on their latent ability to reason more concisely, relative to their default behavior. To elicit this capability, we propose simple fine-tuning methods which leverage self-generated concise reasoning paths obtained by best-of-N sampling and few-shot conditioning, in task-specific settings. Our combined method achieves a 30% reduction in output tokens on average, across five model families on GSM8K and MATH, while maintaining average accuracy. By exploiting the fundamental stochasticity and in-context learning capabilities of LLMs, our self-training approach robustly elicits concise reasoning on a wide range of models, including those with extensive post-training. Code is available at https://github.com/TergelMunkhbat/concise-reasoning
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compositional Causal Reasoning Evaluation in Language Models</title>
<link>https://arxiv.org/abs/2503.04556</link>
<guid>https://arxiv.org/abs/2503.04556</guid>
<content:encoded><![CDATA[
arXiv:2503.04556v4 Announce Type: replace-cross 
Abstract: Causal reasoning and compositional reasoning are two core aspirations in AI. Measuring the extent of these behaviors requires principled evaluation methods. We explore a unified perspective that considers both behaviors simultaneously, termed compositional causal reasoning (CCR): the ability to infer how causal measures compose and, equivalently, how causal quantities propagate through graphs. We instantiate a framework for the systematic evaluation of CCR for the average treatment effect and the probability of necessity and sufficiency. As proof of concept, we demonstrate CCR evaluation for language models in the LLama, Phi, and GPT families. On a math word problem, our framework revealed a range of taxonomically distinct error patterns. CCR errors increased with the complexity of causal paths for all models except o1.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strain Problems got you in a Twist? Try StrainRelief: A Quantum-Accurate Tool for Ligand Strain Calculations</title>
<link>https://arxiv.org/abs/2503.13352</link>
<guid>https://arxiv.org/abs/2503.13352</guid>
<content:encoded><![CDATA[
arXiv:2503.13352v2 Announce Type: replace-cross 
Abstract: Ligand strain energy, the energy difference between the bound and unbound conformations of a ligand, is an important component of structure-based small molecule drug design. A large majority of observed ligands in protein-small molecule co-crystal structures bind in low-strain conformations, making strain energy a useful filter for structure-based drug design. In this work we present a tool for calculating ligand strain with a high accuracy. StrainRelief uses a MACE Neural Network Potential (NNP), trained on a large database of Density Functional Theory (DFT) calculations to estimate ligand strain of neutral molecules with quantum accuracy. We show that this tool estimates strain energy differences relative to DFT to within 1.4 kcal/mol, more accurately than alternative NNPs. These results highlight the utility of NNPs in drug discovery, and provide a useful tool for drug discovery teams.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fusing Global and Local: Transformer-CNN Synergy for Next-Gen Current Estimation</title>
<link>https://arxiv.org/abs/2504.07996</link>
<guid>https://arxiv.org/abs/2504.07996</guid>
<content:encoded><![CDATA[
arXiv:2504.07996v2 Announce Type: replace-cross 
Abstract: This paper presents a hybrid model combining Transformer and CNN for predicting the current waveform in signal lines. Unlike traditional approaches such as current source models, driver linear representations, waveform functional fitting, or equivalent load capacitance methods, our model does not rely on fixed simplified models of standard-cell drivers or RC loads. Instead, it replaces the complex Newton iteration process used in traditional SPICE simulations, leveraging the powerful sequence modeling capabilities of the Transformer framework to directly predict current responses without iterative solving steps. The hybrid architecture effectively integrates the global feature-capturing ability of Transformers with the local feature extraction advantages of CNNs, significantly improving the accuracy of current waveform predictions. Experimental results demonstrate that, compared to traditional SPICE simulations, the proposed algorithm achieves an error of only 0.0098. These results highlight the algorithm's superior capabilities in predicting signal line current waveforms, timing analysis, and power evaluation, making it suitable for a wide range of technology nodes, from 40nm to 3nm.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>polyGen: A Learning Framework for Atomic-level Polymer Structure Generation</title>
<link>https://arxiv.org/abs/2504.17656</link>
<guid>https://arxiv.org/abs/2504.17656</guid>
<content:encoded><![CDATA[
arXiv:2504.17656v2 Announce Type: replace-cross 
Abstract: Synthetic polymeric materials underpin fundamental technologies in the energy, electronics, consumer goods, and medical sectors, yet their development still suffers from prolonged design timelines. Although polymer informatics tools have supported speedup, polymer simulation protocols continue to face significant challenges in the on-demand generation of realistic 3D atomic structures that respect the conformational diversity of polymers. Generative algorithms for 3D structures of inorganic crystals, bio-polymers, and small molecules exist, but have not addressed synthetic polymers because of challenges in representation and dataset constraints. In this work, we introduce polyGen, the first generative model designed specifically for polymer structures from minimal inputs such as the repeat unit chemistry alone. polyGen combines graph-based encodings with a latent diffusion transformer using positional biased attention for realistic conformation generation. Given the limited dataset of 3,855 DFT-optimized polymer structures, we incorporate joint training with small molecule data to enhance generation quality. We also establish structure matching criteria to benchmark our approach on this novel problem. polyGen overcomes the limitations of traditional crystal structure prediction methods for polymers, successfully generating realistic and diverse linear and branched conformations, with promising performance even on challenging large repeat units. As the first atomic-level proof-of-concept capturing intrinsic polymer flexibility, it marks a new capability in material structure generation.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VIST-GPT: Ushering in the Era of Visual Storytelling with LLMs?</title>
<link>https://arxiv.org/abs/2504.19267</link>
<guid>https://arxiv.org/abs/2504.19267</guid>
<content:encoded><![CDATA[
arXiv:2504.19267v3 Announce Type: replace-cross 
Abstract: Visual storytelling is an interdisciplinary field combining computer vision and natural language processing to generate cohesive narratives from sequences of images. This paper presents a novel approach that leverages recent advancements in multimodal models, specifically adapting transformer-based architectures and large multimodal models, for the visual storytelling task. Leveraging the large-scale Visual Storytelling (VIST) dataset, our VIST-GPT model produces visually grounded, contextually appropriate narratives. We address the limitations of traditional evaluation metrics, such as BLEU, METEOR, ROUGE, and CIDEr, which are not suitable for this task. Instead, we utilize RoViST and GROOVIST, novel reference-free metrics designed to assess visual storytelling, focusing on visual grounding, coherence, and non-redundancy. These metrics provide a more nuanced evaluation of narrative quality, aligning closely with human judgment.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Analysis of Hyper-Parameter Optimization Methods for Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2505.03452</link>
<guid>https://arxiv.org/abs/2505.03452</guid>
<content:encoded><![CDATA[
arXiv:2505.03452v2 Announce Type: replace-cross 
Abstract: Finding the optimal Retrieval-Augmented Generation (RAG) configuration for a given use case can be complex and expensive. Motivated by this challenge, frameworks for RAG hyper-parameter optimization (HPO) have recently emerged, yet their effectiveness has not been rigorously benchmarked. To address this gap, we present a comprehensive study involving 5 HPO algorithms over 5 datasets from diverse domains, including a new one collected for this work on real-world product documentation. Our study explores the largest HPO search space considered to date, with three evaluation metrics as optimization targets. Analysis of the results shows that RAG HPO can be done efficiently, either greedily or with random search, and that it significantly boosts RAG performance for all datasets. For greedy HPO approaches, we show that optimizing model selection first is preferable to the prevalent practice of optimizing according to RAG pipeline order.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-based learning for joint channel estimationand hybrid MIMO precoding</title>
<link>https://arxiv.org/abs/2505.04255</link>
<guid>https://arxiv.org/abs/2505.04255</guid>
<content:encoded><![CDATA[
arXiv:2505.04255v2 Announce Type: replace-cross 
Abstract: Hybrid precoding is a key ingredient of cost-effective massive multiple-input multiple-output transceivers. However, setting jointly digital and analog precoders to optimally serve multiple users is a difficult optimization problem. Moreover, it relies heavily on precise knowledge of the channels, which is difficult to obtain, especially when considering realistic systems comprising hardware impairments. In this paper, a joint channel estimation and hybrid precoding method is proposed, which consists in an end-to-end architecture taking received pilots as inputs and outputting pre-coders. The resulting neural network is fully model-based, making it lightweight and interpretable with very few learnable parameters. The channel estimation step is performed using the unfolded matching pursuit algorithm, accounting for imperfect knowledge of the antenna system, while the precoding step is done via unfolded projected gradient ascent. The great potential of the proposed method is empirically demonstrated on realistic synthetic channels.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuous Temporal Learning of Probability Distributions via Neural ODEs with Applications in Continuous Glucose Monitoring Data</title>
<link>https://arxiv.org/abs/2505.08698</link>
<guid>https://arxiv.org/abs/2505.08698</guid>
<content:encoded><![CDATA[
arXiv:2505.08698v2 Announce Type: replace-cross 
Abstract: Modeling the dynamics of probability distributions from time-dependent data samples is a fundamental problem in many fields, including digital health. The goal is to analyze how the distribution of a biomarker, such as glucose, changes over time and how these changes may reflect the progression of chronic diseases like diabetes. We introduce a probabilistic model based on a Gaussian mixture that captures the evolution of a continuous-time stochastic process. Our approach combines a non-parametric estimate of the distribution, obtained with Maximum Mean Discrepancy (MMD), and a Neural Ordinary Differential Equation (Neural ODE) that governs the temporal evolution of the mixture weights. The model is highly interpretable, detects subtle distribution shifts, and remains computationally efficient. Simulation studies show that our method matches or surpasses the estimation accuracy of state-of-the-art, less interpretable techniques such as normalizing flows and non-parametric kernel density estimators. We further demonstrate its utility using data from a digital clinical trial, revealing how interventions affect the time-dependent distribution of glucose levels. The proposed method enables rigorous comparisons between control and treatment groups from both mathematical and clinical perspectives, offering novel longitudinal characterizations that existing approaches cannot achieve.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DISCO Balances the Scales: Adaptive Domain- and Difficulty-Aware Reinforcement Learning on Imbalanced Data</title>
<link>https://arxiv.org/abs/2505.15074</link>
<guid>https://arxiv.org/abs/2505.15074</guid>
<content:encoded><![CDATA[
arXiv:2505.15074v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are increasingly aligned with human preferences through Reinforcement Learning from Human Feedback (RLHF). Among RLHF methods, Group Relative Policy Optimization (GRPO) has gained attention for its simplicity and strong performance, notably eliminating the need for a learned value function. However, GRPO implicitly assumes a balanced domain distribution and uniform semantic alignment across groups - assumptions that rarely hold in real-world datasets. When applied to multi-domain, imbalanced data, GRPO disproportionately optimizes for dominant domains, neglecting underrepresented ones and resulting in poor generalization and fairness. We propose Domain-Informed Self-Consistency Policy Optimization (DISCO), a principled extension to GRPO that addresses inter-group imbalance with two key innovations. Domain-aware reward scaling counteracts frequency bias by reweighting optimization based on domain prevalence. Difficulty-aware reward scaling leverages prompt-level self-consistency to identify and prioritize uncertain prompts that offer greater learning value. Together, these strategies promote more equitable and effective policy learning across domains. Extensive experiments across multiple LLMs and skewed training distributions show that DISCO improves generalization, outperforms existing GRPO variants by 5% on Qwen3 models, and sets new state-of-the-art results on multi-domain alignment benchmarks.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO</title>
<link>https://arxiv.org/abs/2505.17017</link>
<guid>https://arxiv.org/abs/2505.17017</guid>
<content:encoded><![CDATA[
arXiv:2505.17017v2 Announce Type: replace-cross 
Abstract: Recent advancements underscore the significant role of Reinforcement Learning (RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large language models (LLMs). Two prominent RL algorithms, Direct Preference Optimization (DPO) and Group Relative Policy Optimization (GRPO), are central to these developments, showcasing different pros and cons. Autoregressive image generation, also interpretable as a sequential CoT reasoning process, presents unique challenges distinct from LLM-based CoT reasoning. These encompass ensuring text-image consistency, improving image aesthetic quality, and designing sophisticated reward models, rather than relying on simpler rule-based rewards. While recent efforts have extended RL to this domain, these explorations typically lack an in-depth analysis of the domain-specific challenges and the characteristics of different RL strategies. To bridge this gap, we provide the first comprehensive investigation of the GRPO and DPO algorithms in autoregressive image generation, evaluating their in-domain performance and out-of-domain generalization, while scrutinizing the impact of different reward models on their respective capabilities. Our findings reveal that GRPO and DPO exhibit distinct advantages, and crucially, that reward models possessing stronger intrinsic generalization capabilities potentially enhance the generalization potential of the applied RL algorithms. Furthermore, we systematically explore three prevalent scaling strategies to enhance both their in-domain and out-of-domain proficiency, deriving unique insights into efficiently scaling performance for each paradigm. We hope our study paves a new path for inspiring future work on developing more effective RL algorithms to achieve robust CoT reasoning in the realm of autoregressive image generation. Code is released at https://github.com/ZiyuGuo99/Image-Generation-CoT
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAVEN: Query-Guided Representation Alignment for Question Answering over Audio, Video, Embedded Sensors, and Natural Language</title>
<link>https://arxiv.org/abs/2505.17114</link>
<guid>https://arxiv.org/abs/2505.17114</guid>
<content:encoded><![CDATA[
arXiv:2505.17114v2 Announce Type: replace-cross 
Abstract: Multimodal question answering (QA) often requires identifying which video, audio, or sensor tokens are relevant to the question. Yet modality disagreements are common: off-camera speech, background noise, or motion outside the field of view often mislead fusion models that weight all streams equally. We present RAVEN, a unified QA architecture whose core is QuART, a query-conditioned cross-modal gating module that assigns scalar relevance scores to each token across modalities, enabling the model to amplify informative signals and suppress distractors before fusion. RAVEN is trained through a three-stage pipeline comprising unimodal pretraining, query-aligned fusion, and disagreement-oriented fine-tuning -- each stage targeting a distinct challenge in multi-modal reasoning: representation quality, cross-modal relevance, and robustness to modality mismatch. To support training and evaluation, we release AVS-QA, a dataset of 300K synchronized Audio--Video-Sensor streams paired with automatically generated question-answer pairs. Experimental results on seven multi-modal QA benchmarks -- including egocentric and exocentric tasks -- show that RAVEN achieves up to 14.5\% and 8.0\% gains in accuracy compared to state-of-the-art multi-modal large language models, respectively. Incorporating sensor data provides an additional 16.4\% boost, and the model remains robust under modality corruption, outperforming SOTA baselines by 50.23\%. Our code and dataset are available at https://github.com/BASHLab/RAVEN.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Distributed Estimation: Extending Gossip Algorithms to Ranking and Trimmed Means</title>
<link>https://arxiv.org/abs/2505.17836</link>
<guid>https://arxiv.org/abs/2505.17836</guid>
<content:encoded><![CDATA[
arXiv:2505.17836v4 Announce Type: replace-cross 
Abstract: This paper addresses the problem of robust estimation in gossip algorithms over arbitrary communication graphs. Gossip algorithms are fully decentralized, relying only on local neighbor-to-neighbor communication, making them well-suited for situations where communication is constrained. A fundamental challenge in existing mean-based gossip algorithms is their vulnerability to malicious or corrupted nodes. In this paper, we show that an outlier-robust mean can be computed by globally estimating a robust statistic. More specifically, we propose a novel gossip algorithm for rank estimation, referred to as \textsc{GoRank}, and leverage it to design a gossip procedure dedicated to trimmed mean estimation, coined \textsc{GoTrim}. In addition to a detailed description of the proposed methods, a key contribution of our work is a precise convergence analysis: we establish an $\mathcal{O}(1/t)$ rate for rank estimation and an $\mathcal{O}((\log t)/\sqrt{t})$ rate for trimmed mean estimation, where by $t$ is meant the number of iterations. Moreover, we provide a breakdown point analysis of \textsc{GoTrim}. We empirically validate our theoretical results through experiments on diverse network topologies, data distributions and contamination schemes.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Do Images Align and Complement LiDAR? Towards a Harmonized Multi-modal 3D Panoptic Segmentation</title>
<link>https://arxiv.org/abs/2505.18956</link>
<guid>https://arxiv.org/abs/2505.18956</guid>
<content:encoded><![CDATA[
arXiv:2505.18956v2 Announce Type: replace-cross 
Abstract: LiDAR-based 3D panoptic segmentation often struggles with the inherent sparsity of data from LiDAR sensors, which makes it challenging to accurately recognize distant or small objects. Recently, a few studies have sought to overcome this challenge by integrating LiDAR inputs with camera images, leveraging the rich and dense texture information provided by the latter. While these approaches have shown promising results, they still face challenges, such as misalignment during data augmentation and the reliance on post-processing steps. To address these issues, we propose Image-Assists-LiDAR (IAL), a novel multi-modal 3D panoptic segmentation framework. In IAL, we first introduce a modality-synchronized data augmentation strategy, PieAug, to ensure alignment between LiDAR and image inputs from the start. Next, we adopt a transformer decoder to directly predict panoptic segmentation results. To effectively fuse LiDAR and image features into tokens for the decoder, we design a Geometric-guided Token Fusion (GTF) module. Additionally, we leverage the complementary strengths of each modality as priors for query initialization through a Prior-based Query Generation (PQG) module, enhancing the decoder's ability to generate accurate instance masks. Our IAL framework achieves state-of-the-art performance compared to previous multi-modal 3D panoptic segmentation methods on two widely used benchmarks. Code and models are publicly available at .
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimized Text Embedding Models and Benchmarks for Amharic Passage Retrieval</title>
<link>https://arxiv.org/abs/2505.19356</link>
<guid>https://arxiv.org/abs/2505.19356</guid>
<content:encoded><![CDATA[
arXiv:2505.19356v2 Announce Type: replace-cross 
Abstract: Neural retrieval methods using transformer-based pre-trained language models have advanced multilingual and cross-lingual retrieval. However, their effectiveness for low-resource, morphologically rich languages such as Amharic remains underexplored due to data scarcity and suboptimal tokenization. We address this gap by introducing Amharic-specific dense retrieval models based on pre-trained Amharic BERT and RoBERTa backbones. Our proposed RoBERTa-Base-Amharic-Embed model (110M parameters) achieves a 17.6% relative improvement in MRR@10 and a 9.86% gain in Recall@10 over the strongest multilingual baseline, Arctic Embed 2.0 (568M parameters). More compact variants, such as RoBERTa-Medium-Amharic-Embed (42M), remain competitive while being over 13x smaller. Additionally, we train a ColBERT-based late interaction retrieval model that achieves the highest MRR@10 score (0.843) among all evaluated models. We benchmark our proposed models against both sparse and dense retrieval baselines to systematically assess retrieval effectiveness in Amharic. Our analysis highlights key challenges in low-resource settings and underscores the importance of language-specific adaptation. To foster future research in low-resource IR, we publicly release our dataset, codebase, and trained models at https://github.com/kidist-amde/amharic-ir-benchmarks.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Curse of High Dimensionality Issue in Transformer for Long-context Modeling</title>
<link>https://arxiv.org/abs/2505.22107</link>
<guid>https://arxiv.org/abs/2505.22107</guid>
<content:encoded><![CDATA[
arXiv:2505.22107v3 Announce Type: replace-cross 
Abstract: Transformer-based large language models (LLMs) excel in natural language processing tasks by capturing long-range dependencies through self-attention mechanisms. However, long-context modeling faces significant computational inefficiencies due to \textit{redundant} attention computations: while attention weights are often \textit{sparse}, all tokens consume \textit{equal} computational resources. In this paper, we reformulate traditional probabilistic sequence modeling as a \textit{supervised learning task}, enabling the separation of relevant and irrelevant tokens and providing a clearer understanding of redundancy. Based on this reformulation, we theoretically analyze attention sparsity, revealing that only a few tokens significantly contribute to predictions. Building on this, we formulate attention optimization as a linear coding problem and propose a \textit{group coding strategy}, theoretically showing its ability to improve robustness against random noise and enhance learning efficiency. Motivated by this, we propose \textit{Dynamic Group Attention} (DGA), which leverages the group coding to explicitly reduce redundancy by aggregating less important tokens during attention computation. Empirical results show that our DGA significantly reduces computational costs while maintaining competitive performance.Code is available at https://github.com/bolixinyu/DynamicGroupAttention.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TextAtari: 100K Frames Game Playing with Language Agents</title>
<link>https://arxiv.org/abs/2506.04098</link>
<guid>https://arxiv.org/abs/2506.04098</guid>
<content:encoded><![CDATA[
arXiv:2506.04098v2 Announce Type: replace-cross 
Abstract: We present TextAtari, a benchmark for evaluating language agents on very long-horizon decision-making tasks spanning up to 100,000 steps. By translating the visual state representations of classic Atari games into rich textual descriptions, TextAtari creates a challenging test bed that bridges sequential decision-making with natural language processing. The benchmark includes nearly 100 distinct tasks with varying complexity, action spaces, and planning horizons, all rendered as text through an unsupervised representation learning framework (AtariARI). We evaluate three open-source large language models (Qwen2.5-7B, Gemma-7B, and Llama3.1-8B) across three agent frameworks (zero-shot, few-shot chain-of-thought, and reflection reasoning) to assess how different forms of prior knowledge affect performance on these long-horizon challenges. Four scenarios-Basic, Obscured, Manual Augmentation, and Reference-based-investigate the impact of semantic understanding, instruction comprehension, and expert demonstrations on agent decision-making. Our results reveal significant performance gaps between language agents and human players in extensive planning tasks, highlighting challenges in sequential reasoning, state tracking, and strategic planning across tens of thousands of steps. TextAtari provides standardized evaluation protocols, baseline implementations, and a framework for advancing research at the intersection of language models and planning. Our code is available at https://github.com/Lww007/Text-Atari-Agents.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training-Free Constrained Generation With Stable Diffusion Models</title>
<link>https://arxiv.org/abs/2502.05625</link>
<guid>https://arxiv.org/abs/2502.05625</guid>
<content:encoded><![CDATA[
<div> stabile diffusion models, data synthesis, physics-based constraints, generative models, constrained optimization<br />
Summary:<br />
This paper introduces a novel approach that integrates stable diffusion models with constrained optimization frameworks to generate outputs that adhere to strict physical and functional requirements. The proposed method is demonstrated through material design experiments requiring precise morphometric properties, challenging inverse design tasks for materials with specific stress-strain responses, and copyright-constrained content generation tasks. This integration allows for improved data synthesis in various domains, facilitating the discovery of novel solutions and simulating complex systems that are difficult to model explicitly. By combining stable diffusion models with domain-specific constraints, this approach enhances the effectiveness and applicability of generative models in science and engineering applications. <div>
arXiv:2502.05625v3 Announce Type: replace 
Abstract: Stable diffusion models represent the state-of-the-art in data synthesis across diverse domains and hold transformative potential for applications in science and engineering, e.g., by facilitating the discovery of novel solutions and simulating systems that are computationally intractable to model explicitly. While there is increasing effort to incorporate physics-based constraints into generative models, existing techniques are either limited in their applicability to latent diffusion frameworks or lack the capability to strictly enforce domain-specific constraints. To address this limitation this paper proposes a novel integration of stable diffusion models with constrained optimization frameworks, enabling the generation of outputs satisfying stringent physical and functional requirements. The effectiveness of this approach is demonstrated through material design experiments requiring adherence to precise morphometric properties, challenging inverse design tasks involving the generation of materials inducing specific stress-strain responses, and copyright-constrained content generation tasks.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning-at-Criticality in Large Language Models for Quantum Field Theory and Beyond</title>
<link>https://arxiv.org/abs/2506.03703</link>
<guid>https://arxiv.org/abs/2506.03703</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, large language models, criticality, artificial intelligence, quantum field theory
Summary: 
- The study introduces learning at criticality (LaC), a reinforcement learning approach that tunes Large Language Models (LLMs) to a sharp learning transition, allowing peak generalization with minimal data.
- LLMs operating at the critical point exhibit power-law distributed solution path lengths, suggesting a second-order phase transition.
- The critical thinking pattern maximized at this critical point enables generalization by extracting underlying operational rules.
- LaC is demonstrated in quantum field theory, where an 8B-parameter LLM significantly outperforms larger models by solving higher-order problems using only a few exemplars of symbolic Matsubara sums.
- By leveraging critical phenomena, LaC harnesses a physical principle to enhance artificial intelligence for information-scarce challenges in fundamental physics.<br /><br />Summary: <div>
arXiv:2506.03703v2 Announce Type: replace 
Abstract: Fundamental physics often confronts complex symbolic problems with few guiding exemplars or established principles. While artificial intelligence (AI) offers promise, its typical need for vast datasets to learn from hinders its use in these information-scarce frontiers. We introduce learning at criticality (LaC), a reinforcement learning (RL) scheme that tunes Large Language Models (LLMs) to a sharp learning transition, addressing this information scarcity. At this transition, LLMs achieve peak generalization from minimal data, exemplified by 7-digit base-7 addition -- a test of nontrivial arithmetic reasoning. To elucidate this peak, we analyze a minimal concept-network model (CoNet) designed to capture the essence of how LLMs might link tokens. Trained on a single exemplar, this model also undergoes a sharp learning transition. This transition exhibits hallmarks of a second-order phase transition, notably power-law distributed solution path lengths. At this critical point, the system maximizes a ``critical thinking pattern" crucial for generalization, enabled by the underlying scale-free exploration. This suggests LLMs reach peak performance by operating at criticality, where such explorative dynamics enable the extraction of underlying operational rules. We demonstrate LaC in quantum field theory: an 8B-parameter LLM, tuned to its critical point by LaC using a few exemplars of symbolic Matsubara sums, solves unseen, higher-order problems, significantly outperforming far larger models. LaC thus leverages critical phenomena, a physical principle, to empower AI for complex, data-sparse challenges in fundamental physics.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Horizon Reduction Makes RL Scalable</title>
<link>https://arxiv.org/abs/2506.04168</link>
<guid>https://arxiv.org/abs/2506.04168</guid>
<content:encoded><![CDATA[
<div> scalability, offline reinforcement learning, horizon, horizon reduction techniques, SHARSA <br />
Summary: <br />
This work examines the scalability of offline reinforcement learning (RL) algorithms, aiming to solve complex, unsolved tasks with larger datasets. Despite increased data sizes, many current offline RL algorithms show poor scaling, potentially due to long horizons. Experimental analysis confirms that long horizons are a key barrier to scalability in offline RL. Various horizon reduction techniques are effective in improving scalability on challenging tasks. The proposed minimal method, SHARSA, significantly reduces the horizon and outperforms other evaluation methods, demonstrating enhanced scalability. Explicitly reducing the horizon is shown to unlock the scalability of offline RL, highlighting the importance of addressing horizon-related challenges in offline RL algorithms. <div>
arXiv:2506.04168v2 Announce Type: replace 
Abstract: In this work, we study the scalability of offline reinforcement learning (RL) algorithms. In principle, a truly scalable offline RL algorithm should be able to solve any given problem, regardless of its complexity, given sufficient data, compute, and model capacity. We investigate if and how current offline RL algorithms match up to this promise on diverse, challenging, previously unsolved tasks, using datasets up to 1000x larger than typical offline RL datasets. We observe that despite scaling up data, many existing offline RL algorithms exhibit poor scaling behavior, saturating well below the maximum performance. We hypothesize that the horizon is the main cause behind the poor scaling of offline RL. We empirically verify this hypothesis through several analysis experiments, showing that long horizons indeed present a fundamental barrier to scaling up offline RL. We then show that various horizon reduction techniques substantially enhance scalability on challenging tasks. Based on our insights, we also introduce a minimal yet scalable method named SHARSA that effectively reduces the horizon. SHARSA achieves the best asymptotic performance and scaling behavior among our evaluation methods, showing that explicitly reducing the horizon unlocks the scalability of offline RL. Code: https://github.com/seohongpark/horizon-reduction
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reliably detecting model failures in deployment without labels</title>
<link>https://arxiv.org/abs/2506.05047</link>
<guid>https://arxiv.org/abs/2506.05047</guid>
<content:encoded><![CDATA[
<div> Keywords: data distribution, dynamic environments, retraining, post-deployment deterioration monitoring, predictive models <br />
Summary: 
The paper addresses the challenge of monitoring post-deployment deterioration (PDD) in machine learning models operating in dynamic environments where data distribution changes over time. Retraining models is necessary, but knowing when to do so without access to labels is difficult. The proposed D3M algorithm, based on model disagreement, effectively monitors PDD with low false positive rates in non-deteriorating shifts and provides sample complexity bounds for high true positive rates in deteriorating shifts. Empirical results on benchmark and real-world medical datasets demonstrate the framework's efficacy as an alert mechanism for critical machine learning pipelines. The algorithm's efficiency and practicality make it a valuable tool for maintaining model performance over time. <br /><br />Summary: <div>
arXiv:2506.05047v2 Announce Type: replace 
Abstract: The distribution of data changes over time; models operating operating in dynamic environments need retraining. But knowing when to retrain, without access to labels, is an open challenge since some, but not all shifts degrade model performance. This paper formalizes and addresses the problem of post-deployment deterioration (PDD) monitoring. We propose D3M, a practical and efficient monitoring algorithm based on the disagreement of predictive models, achieving low false positive rates under non-deteriorating shifts and provides sample complexity bounds for high true positive rates under deteriorating shifts. Empirical results on both standard benchmark and a real-world large-scale internal medicine dataset demonstrate the effectiveness of the framework and highlight its viability as an alert mechanism for high-stakes machine learning pipelines.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High-Dimensional Learning in Finance</title>
<link>https://arxiv.org/abs/2506.03780</link>
<guid>https://arxiv.org/abs/2506.03780</guid>
<content:encoded><![CDATA[
<div> machine learning, financial prediction, high-dimensional learning, Random Fourier Features, information-theoretic lower bounds

Summary:
- The article explores the use of large, over-parameterized machine learning models for financial prediction.
- It discusses the impact of within-sample standardization on Random Fourier Features implementations in financial learning.
- The author establishes information-theoretic lower bounds that highlight the limitations of achieving reliable learning in high-dimensional finance.
- A quantitative calibration of the polynomial lower bound reveals the extensive amount of data required to escape the bound.
- The findings suggest that observed out-of-sample success in financial prediction may be attributed to lower-complexity artefacts rather than the intended high-dimensional mechanism. 

Summary:<br />
Recent advancements in machine learning show promise for financial prediction using large models. The article delves into the effects of within-sample standardization in Random Fourier Features on financial learning and presents information-theoretic lower bounds that outline the challenges of reliable high-dimensional finance learning. A quantitative assessment indicates a substantial amount of data is needed to surpass these bounds. The study suggests that success in out-of-sample financial prediction may stem from simpler factors rather than complex mechanisms. <div>
arXiv:2506.03780v2 Announce Type: replace-cross 
Abstract: Recent advances in machine learning have shown promising results for financial prediction using large, over-parameterized models. This paper provides theoretical foundations and empirical validation for understanding when and how these methods achieve predictive success. I examine two key aspects of high-dimensional learning in finance. First, I prove that within-sample standardization in Random Fourier Features implementations fundamentally alters the underlying Gaussian kernel approximation, replacing shift-invariant kernels with training-set dependent alternatives. Second, I establish information-theoretic lower bounds that identify when reliable learning is impossible no matter how sophisticated the estimator. A detailed quantitative calibration of the polynomial lower bound shows that with typical parameter choices, e.g., 12,000 features, 12 monthly observations, and R-square 2-3%, the required sample size to escape the bound exceeds 25-30 years of data--well beyond any rolling-window actually used. Thus, observed out-of-sample success must originate from lower-complexity artefacts rather than from the intended high-dimensional mechanism.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structured Pruning for Diverse Best-of-N Reasoning Optimization</title>
<link>https://arxiv.org/abs/2506.03978</link>
<guid>https://arxiv.org/abs/2506.03978</guid>
<content:encoded><![CDATA[
<div> pruning, transformer-based language models, reasoning capabilities, SPRINT, contrastive learning

Summary:
Model pruning in transformer-based language models can enhance reasoning capabilities by selectively pruning attention heads. The new framework called SPRINT dynamically selects the optimal head and layer to prune during inference by aligning question embeddings with head embeddings. This approach identifies pruned-head configurations that lead to more accurate reasoning. Extensive experiments show significant performance improvements over traditional strategies on the MATH500 and GSM8K datasets.<br /><br />Summary: <div>
arXiv:2506.03978v2 Announce Type: replace-cross 
Abstract: Model pruning in transformer-based language models, traditionally viewed as a means of achieving computational savings, can enhance the model's reasoning capabilities. In this work, we uncover a surprising phenomenon: the selective pruning of certain attention heads leads to improvements in reasoning performance, particularly on challenging tasks. Motivated by this observation, we propose SPRINT, a novel contrastive learning framework that dynamically selects the optimal head and layer to prune during inference. By aligning question embeddings with head embeddings, SPRINT identifies those pruned-head configurations that result in more accurate reasoning. Extensive experiments demonstrate that our method significantly outperforms traditional best-of-$N$ and random head selection strategies on the MATH500 and GSM8K datasets.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAID: A Dataset for Testing the Adversarial Robustness of AI-Generated Image Detectors</title>
<link>https://arxiv.org/abs/2506.03988</link>
<guid>https://arxiv.org/abs/2506.03988</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-generated images, detection, adversarial robustness, dataset, transferability

Summary:
AI-generated images have reached a quality level that is indistinguishable from real images, posing a challenge for detecting fraud and disinformation. The need for robust detection methods is crucial, with current approaches often overlooking adversarial robustness. To address this, the RAID dataset is introduced, containing diverse and highly transferable adversarial examples generated against state-of-the-art detectors and text-to-image models. The dataset demonstrates the ease with which current detectors can be deceived by adversarial images, underscoring the necessity for more robust detection methods. The use of the RAID dataset can quickly assess a detector's adversarial robustness, providing a reliable estimate of its efficacy. This work emphasizes the critical importance of developing robust AI-generated image detectors to combat the growing threat of fraudulent content. The RAID dataset and evaluation code are publicly available for further research. 

<br /><br />Summary: <div>
arXiv:2506.03988v3 Announce Type: replace-cross 
Abstract: AI-generated images have reached a quality level at which humans are incapable of reliably distinguishing them from real images. To counteract the inherent risk of fraud and disinformation, the detection of AI-generated images is a pressing challenge and an active research topic. While many of the presented methods claim to achieve high detection accuracy, they are usually evaluated under idealized conditions. In particular, the adversarial robustness is often neglected, potentially due to a lack of awareness or the substantial effort required to conduct a comprehensive robustness analysis. In this work, we tackle this problem by providing a simpler means to assess the robustness of AI-generated image detectors. We present RAID (Robust evaluation of AI-generated image Detectors), a dataset of 72k diverse and highly transferable adversarial examples. The dataset is created by running attacks against an ensemble of seven state-of-the-art detectors and images generated by four different text-to-image models. Extensive experiments show that our methodology generates adversarial images that transfer with a high success rate to unseen detectors, which can be used to quickly provide an approximate yet still reliable estimate of a detector's adversarial robustness. Our findings indicate that current state-of-the-art AI-generated image detectors can be easily deceived by adversarial examples, highlighting the critical need for the development of more robust methods. We release our dataset at https://huggingface.co/datasets/aimagelab/RAID and evaluation code at https://github.com/pralab/RAID.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLAC: Simulation-Pretrained Latent Action Space for Whole-Body Real-World RL</title>
<link>https://arxiv.org/abs/2506.04147</link>
<guid>https://arxiv.org/abs/2506.04147</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, mobile manipulators, high-degree-of-freedom systems, sim-to-real transfer, SLAC  
Summary:  
SLAC introduces a method for enabling real-world reinforcement learning (RL) for complex robots with high degrees of freedom. It utilizes a low-fidelity simulator to pretrain a task-agnostic latent action space, promoting temporal abstraction, disentanglement, and safety. This approach facilitates efficient learning in downstream tasks by using the learned latent action space as the interface for off-policy RL algorithms. SLAC achieves state-of-the-art performance on bimanual mobile manipulation tasks, learning contact-rich whole-body tasks in under an hour of real-world interactions without relying on demonstrations or hand-crafted behavior priors. The method addresses the challenges of safe exploration and sample efficiency in direct RL, as well as the brittleness of sim-to-real transfer methods, making it a promising solution for mastering control of versatile robot systems. <br /><br />Summary: <div>
arXiv:2506.04147v2 Announce Type: replace-cross 
Abstract: Building capable household and industrial robots requires mastering the control of versatile, high-degree-of-freedom (DoF) systems such as mobile manipulators. While reinforcement learning (RL) holds promise for autonomously acquiring robot control policies, scaling it to high-DoF embodiments remains challenging. Direct RL in the real world demands both safe exploration and high sample efficiency, which are difficult to achieve in practice. Sim-to-real RL, on the other hand, is often brittle due to the reality gap. This paper introduces SLAC, a method that renders real-world RL feasible for complex embodiments by leveraging a low-fidelity simulator to pretrain a task-agnostic latent action space. SLAC trains this latent action space via a customized unsupervised skill discovery method designed to promote temporal abstraction, disentanglement, and safety, thereby facilitating efficient downstream learning. Once a latent action space is learned, SLAC uses it as the action interface for a novel off-policy RL algorithm to autonomously learn downstream tasks through real-world interactions. We evaluate SLAC against existing methods on a suite of bimanual mobile manipulation tasks, where it achieves state-of-the-art performance. Notably, SLAC learns contact-rich whole-body tasks in under an hour of real-world interactions, without relying on any demonstrations or hand-crafted behavior priors. More information, code, and videos at robo-rl.github.io
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable LLMs for Credit Risk: A Systematic Review and Taxonomy</title>
<link>https://arxiv.org/abs/2506.04290</link>
<guid>https://arxiv.org/abs/2506.04290</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, credit risk assessment, interpretability, model architectures, data types

Summary: 
This paper presents a systematic review and taxonomy of Large Language Model (LLM) based approaches in credit risk estimation. The study selected 60 relevant papers from 2020-2025 using the PRISMA research strategy to determine basic model architectures and data used for credit default prediction and risk analysis scenarios. The focus is on interpretability, classifying concepts such as explainability mechanisms, chain of thought prompts, and natural language justifications for LLM-based credit models. The taxonomy categorizes literature under model architectures, data types, explainability mechanisms, and application areas. The analysis highlights future trends and research gaps in LLM-based credit scoring systems, aiming to serve as a reference for artificial intelligence and financial researchers. <div>
arXiv:2506.04290v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLM), which have developed in recent years, enable credit risk assessment through the analysis of financial texts such as analyst reports and corporate disclosures. This paper presents the first systematic review and taxonomy focusing on LLMbased approaches in credit risk estimation. We determined the basic model architectures by selecting 60 relevant papers published between 2020-2025 with the PRISMA research strategy. And we examined the data used for scenarios such as credit default prediction and risk analysis. Since the main focus of the paper is interpretability, we classify concepts such as explainability mechanisms, chain of thought prompts and natural language justifications for LLM-based credit models. The taxonomy organizes the literature under four main headings: model architectures, data types, explainability mechanisms and application areas. Based on this analysis, we highlight the main future trends and research gaps for LLM-based credit scoring systems. This paper aims to be a reference paper for artificial intelligence and financial researchers.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BridgeNet: A Hybrid, Physics-Informed Machine Learning Framework for Solving High-Dimensional Fokker-Planck Equations</title>
<link>https://arxiv.org/abs/2506.04354</link>
<guid>https://arxiv.org/abs/2506.04354</guid>
<content:encoded><![CDATA[
<div> Framework, convolutional neural networks, physics-informed neural networks, Fokker-Planck equations, high-dimensional

Summary:
BridgeNet is a novel hybrid framework that combines convolutional neural networks and physics-informed neural networks to efficiently solve non-linear, high-dimensional Fokker-Planck equations. Traditional PINNs struggle with complex spatial hierarchies and boundary conditions, but BridgeNet uses adaptive CNN layers for local feature extraction and a dynamically weighted loss function to enforce physical constraints. Through extensive numerical experiments, BridgeNet outperforms conventional PINN approaches by achieving lower error metrics, faster convergence, and robust stability in high-dimensional settings. This advancement in computational physics offers a scalable and accurate solution methodology with applications in fields like financial mathematics and complex system dynamics.

<br /><br />Summary: <div>
arXiv:2506.04354v2 Announce Type: replace-cross 
Abstract: BridgeNet is a novel hybrid framework that integrates convolutional neural networks with physics-informed neural networks to efficiently solve non-linear, high-dimensional Fokker-Planck equations (FPEs). Traditional PINNs, which typically rely on fully connected architectures, often struggle to capture complex spatial hierarchies and enforce intricate boundary conditions. In contrast, BridgeNet leverages adaptive CNN layers for effective local feature extraction and incorporates a dynamically weighted loss function that rigorously enforces physical constraints. Extensive numerical experiments across various test cases demonstrate that BridgeNet not only achieves significantly lower error metrics and faster convergence compared to conventional PINN approaches but also maintains robust stability in high-dimensional settings. This work represents a substantial advancement in computational physics, offering a scalable and accurate solution methodology with promising applications in fields ranging from financial mathematics to complex system dynamics.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlashDMoE: Fast Distributed MoE in a Single Kernel</title>
<link>https://arxiv.org/abs/2506.04667</link>
<guid>https://arxiv.org/abs/2506.04667</guid>
<content:encoded><![CDATA[
<div> GPU utilization, latency overhead, task locality, FlashDMoE, Mixture-of-Experts (MoE) models <br />
Summary:<br />FlashDMoE is a new GPU-resident Mixture-of-Experts (MoE) operator designed to address the low GPU utilization, latency overhead, and inability to leverage task locality in existing implementations. By fusing expert computation and inter-GPU communication into a single persistent GPU kernel, FlashDMoE enables fine-grained pipelining and eliminates launch overheads, resulting in higher GPU utilization, lower latency, higher throughput, and better overlap efficiency compared to state-of-the-art baselines. The use of FP32 in FlashDMoE also outperforms baselines using FP16. FlashDMoE showcases the importance of GPU kernel-hardware co-design for unlocking performance potential in large-scale distributed machine learning workloads.<br /> <div>
arXiv:2506.04667v2 Announce Type: replace-cross 
Abstract: The computational sparsity of Mixture-of-Experts (MoE) models enables sub-linear growth in compute cost as model size increases, thus offering a scalable path to training massive neural networks. However, existing implementations suffer from \emph{low GPU utilization}, \emph{significant latency overhead}, and a fundamental \emph{inability to leverage task locality}, primarily due to CPU-managed scheduling, host-initiated communication, and frequent kernel launches. To overcome these limitations, we develop FlashDMoE, a fully GPU-resident MoE operator that fuses expert computation and inter-GPU communication into a \emph{single persistent GPU kernel}. FlashDMoE enables fine-grained pipelining of dispatch, compute, and combine phases, eliminating launch overheads and reducing idle gaps. Unlike existing work, FlashDMoE obviates bulk-synchronous collectives for one-sided, device-initiated, inter-GPU (R)DMA transfers, thus unlocking \emph{payload efficiency}, where we eliminate bloated or redundant network payloads in sparsely activated layers. When evaluated on a single 8-H100 GPU node with MoE models having up to 128 experts and 16K token sequences, FlashDMoE achieves up to \textbf{9}$\times$ higher GPU utilization, \textbf{6}$\times$ lower latency, \textbf{5.7}$\times$ higher throughput, and \textbf{4}$\times$ better overlap efficiency compared to state-of-the-art baselines, despite using FP32 while baselines use FP16. FlashDMoE demonstrates that principled GPU kernel-hardware co-design is key to unlocking the performance ceiling of large-scale distributed ML workloads.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context Is Not Comprehension</title>
<link>https://arxiv.org/abs/2506.04907</link>
<guid>https://arxiv.org/abs/2506.04907</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Verbose ListOps, multi-step computation, narrative, benchmark

Summary: 

The article introduces the Verbose ListOps (VLO) benchmark, which evaluates the ability of Large Language Models (LLMs) to handle multi-step computation within coherent stories. While current LLMs excel at retrieving explicit facts, they struggle when required to track and update internal state in narrative-based tasks. VLO consists of deterministic, nested computations embedded within stories, challenging models to comprehend and process information beyond simple value retrieval. Experimental results reveal that top LLMs, proficient at solving basic ListOps equations, falter on VLO tasks as the complexity increases. The VLO framework can be applied to various reasoning tasks, serving as a crucial tool to improve models' capacity for complex knowledge work. This research highlights the need for LLMs to enhance their stateful comprehension abilities to excel in tasks involving intricate narratives and computations. 

<br /><br />Summary: <div>
arXiv:2506.04907v2 Announce Type: replace-cross 
Abstract: The dominant evaluation of Large Language Models has centered on their ability to surface explicit facts from increasingly vast contexts. While today's best models demonstrate near-perfect recall on these tasks, this apparent success masks a fundamental failure in multi-step computation when information is embedded in a narrative. We introduce Verbose ListOps (VLO), a novel benchmark designed to isolate this failure. VLO programmatically weaves deterministic, nested computations into coherent stories, forcing models to track and update internal state rather than simply locate explicit values. Our experiments show that leading LLMs, capable of solving the raw ListOps equations with near-perfect accuracy, collapse in performance on VLO at just 10k tokens. The VLO framework is extensible to any verifiable reasoning task, providing a critical tool to move beyond simply expanding context windows and begin building models with the robust, stateful comprehension required for complex knowledge work.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CellCLIP -- Learning Perturbation Effects in Cell Painting via Text-Guided Contrastive Learning</title>
<link>https://arxiv.org/abs/2506.06290</link>
<guid>https://arxiv.org/abs/2506.06290</guid>
<content:encoded><![CDATA[
<div> CellCLIP, High-content screening, HCS data, Cross-modal contrastive learning, Perturbations<br />
Summary:<br />
- High-content screening assays using Cell Painting and high-throughput microscopy enable large-scale analysis of cellular responses to perturbations. 
- Learning a unified latent space aligning perturbations with morphological effects through cross-modal contrastive learning is challenging due to differences in Cell Painting images and perturbation semantics. 
- CellCLIP is introduced as a framework for HCS data, utilizing pre-trained image encoders and a novel channel encoding method for better capturing image relationships and natural language encoders for perturbation representation. 
- CellCLIP outperforms existing models in cross-modal retrieval and biological downstream tasks, with improved performance and reduced computation time. 
- The framework demonstrates superior performance in learning relationships between microscopy channels and perturbations, contributing to a better understanding of cellular responses in high-content screening assays. <br /> <div>
arXiv:2506.06290v1 Announce Type: new 
Abstract: High-content screening (HCS) assays based on high-throughput microscopy techniques such as Cell Painting have enabled the interrogation of cells' morphological responses to perturbations at an unprecedented scale. The collection of such data promises to facilitate a better understanding of the relationships between different perturbations and their effects on cellular state. Towards achieving this goal, recent advances in cross-modal contrastive learning could, in theory, be leveraged to learn a unified latent space that aligns perturbations with their corresponding morphological effects. However, the application of such methods to HCS data is not straightforward due to substantial differences in the semantics of Cell Painting images compared to natural images, and the difficulty of representing different classes of perturbations (e.g., small molecule vs CRISPR gene knockout) in a single latent space. In response to these challenges, here we introduce CellCLIP, a cross-modal contrastive learning framework for HCS data. CellCLIP leverages pre-trained image encoders coupled with a novel channel encoding scheme to better capture relationships between different microscopy channels in image embeddings, along with natural language encoders for representing perturbations. Our framework outperforms current open-source models, demonstrating the best performance in both cross-modal retrieval and biologically meaningful downstream tasks while also achieving significant reductions in computation time.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improvement of Optimization using Learning Based Models in Mixed Integer Linear Programming Tasks</title>
<link>https://arxiv.org/abs/2506.06291</link>
<guid>https://arxiv.org/abs/2506.06291</guid>
<content:encoded><![CDATA[
<div> Keywords: Mixed Integer Linear Programs, Behavior Cloning, Reinforcement Learning, Graph Neural Networks, Multi-Agent Task Allocation

Summary: 
Mixed Integer Linear Programs (MILPs) are crucial for solving complex planning and scheduling problems in various industries. However, their adoption is limited due to long computational times, especially in real-time scenarios. To tackle this issue, a learning-based framework combining Behavior Cloning (BC) and Reinforcement Learning (RL) to train Graph Neural Networks (GNNs) has been proposed. This framework aims to generate high-quality initial solutions for warm-starting MILP solvers in Multi-Agent Task Allocation and Scheduling Problems. Experimental results show that this method significantly reduces optimization time and variance compared to traditional techniques, while still maintaining solution quality and feasibility. This approach has the potential to enhance efficiency and effectiveness in solving challenging optimization problems in critical industries. 

<br /><br />Summary: <div>
arXiv:2506.06291v1 Announce Type: new 
Abstract: Mixed Integer Linear Programs (MILPs) are essential tools for solving planning and scheduling problems across critical industries such as construction, manufacturing, and logistics. However, their widespread adoption is limited by long computational times, especially in large-scale, real-time scenarios. To address this, we present a learning-based framework that leverages Behavior Cloning (BC) and Reinforcement Learning (RL) to train Graph Neural Networks (GNNs), producing high-quality initial solutions for warm-starting MILP solvers in Multi-Agent Task Allocation and Scheduling Problems. Experimental results demonstrate that our method reduces optimization time and variance compared to traditional techniques while maintaining solution quality and feasibility.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mutual-Taught for Co-adapting Policy and Reward Models</title>
<link>https://arxiv.org/abs/2506.06292</link>
<guid>https://arxiv.org/abs/2506.06292</guid>
<content:encoded><![CDATA[
<div> self-training, large language models, preference optimization, Mutual-Taught, distribution shifts  
Summary:  
Mutual-Taught is a self-training method designed to address distribution shifts in large language models during preference optimization. The approach iteratively improves both the policy model (PM) and reward model (RM) without human annotation. It utilizes an expectation-maximization (EM) algorithm-like process, updating the PM using feedback from the RM to approximate the optimal preference distribution better. The RM is updated by constructing training data from PM outputs before and after the update, ensuring adaptation to the evolving policy distribution. Experimental results show consistent improvements in both models, with LLaMA-3-8B-Instruct-MT achieving a 54.1% length-controlled win rate on AlpacaEval-2 and FsfairX-LLaMA3-RM-MT performing comparably to GPT-4o-2024-08-06 on RewardBench.  
<br /><br />Summary: <div>
arXiv:2506.06292v1 Announce Type: new 
Abstract: During the preference optimization of large language models (LLMs), distribution shifts may arise between newly generated model samples and the data used to train the reward model (RM). This shift reduces the efficacy of the RM, which in turn negatively impacts the performance of the policy model (PM). To address this challenge, we propose Mutual-Taught, a self-training method that iteratively improves both the PM and RM without requiring additional human annotation. Our approach mirrors the expectation-maximization (EM) algorithm. In the E-step, the PM is updated using feedback from the current RM, guiding the PM toward a better approximation of the latent optimal preference distribution. In the M-step, we update the RM by constructing training data from the outputs of the PM before and after the E-step update. This process ensures that the RM adapts to the evolving policy distribution. Experimental results demonstrate that this iterative approach leads to consistent improvements in both models. Specifically, our 8B policy model, LLaMA-3-8B-Instruct-MT, achieves a length-controlled win rate of 54.1\% on AlpacaEval-2, while our 8B reward model, FsfairX-LLaMA3-RM-MT, performs on par with GPT-4o-2024-08-06 on RewardBench.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prediction of Bank Credit Ratings using Heterogeneous Topological Graph Neural Networks</title>
<link>https://arxiv.org/abs/2506.06293</link>
<guid>https://arxiv.org/abs/2506.06293</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks; credit ratings; interbank connections; persistent homology; heterogeneous network <br />
<br />
Summary: <br />
This research focuses on improving bank credit rating predictions by utilizing persistent homology to construct a network capturing relationships among banks. Due to privacy concerns, a complete interbank connection graph is often unavailable, hindering the direct application of Graph Neural Networks. By combining information from a traditional lending network with the constructed network, a heterogeneous network is created. This integration leads to improved predictions for bank credit ratings. Experiments conducted on a real-world global dataset validate the effectiveness of this approach, known as HTGNN. The research has implications for investors and regulatory bodies by enhancing proactive risk mitigation and enabling the implementation of effective market interventions. The code for this research can be accessed at https://github.com/Liu-Jun-Yi/HTGNN. <div>
arXiv:2506.06293v1 Announce Type: new 
Abstract: Agencies such as Standard & Poor's and Moody's provide bank credit ratings that influence economic stability and decision-making by stakeholders. Accurate and timely predictions support informed decision-making, regulatory actions, and investor protection. However, a complete interbank connection graph is often unavailable due to privacy concerns, complicating the direct application of Graph Neural Networks (GNNs) for rating prediction. our research utilizes persistent homology to construct a network that captures relationships among banks and combines this with a traditional lending network to create a heterogeneous network that integrates information from both sources, leading to improved predictions. Experiments on a global, real-world dataset validate the effectiveness of HTGNN. This research has implications for investors and regulatory bodies in enhancing proactive risk mitigation and the implementation of effective market interventions.The code can be find at https://github.com/Liu-Jun-Yi/HTGNN.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GLProtein: Global-and-Local Structure Aware Protein Representation Learning</title>
<link>https://arxiv.org/abs/2506.06294</link>
<guid>https://arxiv.org/abs/2506.06294</guid>
<content:encoded><![CDATA[
<div> Keywords: Proteins, Structure, GLProtein, Protein-protein interaction, Bioinformatics <br />
<br />
Summary: Proteins play a key role in biological systems and understanding their functions is crucial. The GLProtein framework is introduced to integrate both global structural similarity and local amino acid details in protein pre-training. This novel approach combines protein-masked modelling, triplet structure similarity scoring, protein 3D distance encoding, and substructure-based amino acid molecule encoding. Experimental results demonstrate the superior performance of GLProtein in various bioinformatics tasks such as predicting protein-protein interaction and contact prediction. By incorporating both local and global protein structural information, GLProtein enhances prediction accuracy and provides valuable functional insights, surpassing previous methods in the field. <div>
arXiv:2506.06294v1 Announce Type: new 
Abstract: Proteins are central to biological systems, participating as building blocks across all forms of life. Despite advancements in understanding protein functions through protein sequence analysis, there remains potential for further exploration in integrating protein structural information. We argue that the structural information of proteins is not only limited to their 3D information but also encompasses information from amino acid molecules (local information) to protein-protein structure similarity (global information). To address this, we propose \textbf{GLProtein}, the first framework in protein pre-training that incorporates both global structural similarity and local amino acid details to enhance prediction accuracy and functional insights. GLProtein innovatively combines protein-masked modelling with triplet structure similarity scoring, protein 3D distance encoding and substructure-based amino acid molecule encoding. Experimental results demonstrate that GLProtein outperforms previous methods in several bioinformatics tasks, including predicting protein-protein interaction, contact prediction, and so on.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>dLLM-Cache: Accelerating Diffusion Large Language Models with Adaptive Caching</title>
<link>https://arxiv.org/abs/2506.06295</link>
<guid>https://arxiv.org/abs/2506.06295</guid>
<content:encoded><![CDATA[
<div> Keywords: Autoregressive Models, Large Language Models, diffusion-based, caching, inference latency 

Summary: 
The paper introduces a new framework called dLLM-Cache to address the high inference latency issues faced by diffusion-based Large Language Models (dLLMs). These models generate text by denoising masked segments iteratively. Traditional acceleration techniques like Key-Value caching are not compatible due to their bidirectional attention mechanism. The dLLM-Cache framework leverages the static prompt and partially dynamic response nature of dLLMs to reuse intermediate computations efficiently. By combining long-interval prompt caching with partial response updates based on feature similarity, dLLM-Cache achieves up to 9.1x speedup in inference without compromising output quality. Extensive experiments on popular dLLMs like LLaDA 8B and Dream 7B demonstrate that dLLM-Cache brings inference latency closer to Autoregressive Models, making it a promising solution for improving the efficiency of large language models. The code for dLLM-Cache is provided in the supplementary material and will be publicly released on GitHub. 

<br /><br />Summary: <div>
arXiv:2506.06295v1 Announce Type: new 
Abstract: Autoregressive Models (ARMs) have long dominated the landscape of Large Language Models. Recently, a new paradigm has emerged in the form of diffusion-based Large Language Models (dLLMs), which generate text by iteratively denoising masked segments. This approach has shown significant advantages and potential. However, dLLMs suffer from high inference latency. Traditional ARM acceleration techniques, such as Key-Value caching, are incompatible with dLLMs due to their bidirectional attention mechanism. To address this specific challenge, our work begins with a key observation that dLLM inference involves a static prompt and a partially dynamic response, where most tokens remain stable across adjacent denoising steps. Based on this, we propose dLLM-Cache, a training-free adaptive caching framework that combines long-interval prompt caching with partial response updates guided by feature similarity. This design enables efficient reuse of intermediate computations without compromising model performance. Extensive experiments on representative dLLMs, including LLaDA 8B and Dream 7B, show that dLLM-Cache achieves up to 9.1 x speedup over standard inference without compromising output quality. Notably, our method brings dLLM inference latency close to that of ARMs under many settings. Codes are provided in the supplementary material and will be released publicly on GitHub.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Graph CNN with Jacobi Kolmogorov-Arnold Networks for 3D Classification of Point Sets</title>
<link>https://arxiv.org/abs/2506.06296</link>
<guid>https://arxiv.org/abs/2506.06296</guid>
<content:encoded><![CDATA[
<div> Jacobi-KAN-DGCNN, Dynamic Graph Convolutional Neural Network, Jacobi Kolmogorov-Arnold Networks, classification, three-dimensional point clouds <br />
<br />
Summary: <br />
Jacobi-KAN-DGCNN combines DGCNN with KAN for 3D point cloud classification, replacing MLP layers with adaptable polynomial expansions. The method outperforms the traditional DGCNN baseline on ModelNet40 dataset in accuracy and convergence speed. Higher polynomial degrees do not always improve performance, indicating the need for further investigation into polynomial bases and degrees in graph-based learning. <div>
arXiv:2506.06296v1 Announce Type: new 
Abstract: We introduce Jacobi-KAN-DGCNN, a framework that integrates Dynamic Graph Convolutional Neural Network (DGCNN) with Jacobi Kolmogorov-Arnold Networks (KAN) for the classification of three-dimensional point clouds. This method replaces Multi-Layer Perceptron (MLP) layers with adaptable univariate polynomial expansions within a streamlined DGCNN architecture, circumventing deep levels for both MLP and KAN to facilitate a layer-by-layer comparison. In comparative experiments on the ModelNet40 dataset, KAN layers employing Jacobi polynomials outperform the traditional linear layer-based DGCNN baseline in terms of accuracy and convergence speed, while maintaining parameter efficiency. Our results demonstrate that higher polynomial degrees do not automatically improve performance, highlighting the need for further theoretical and empirical investigation to fully understand the interactions between polynomial bases, degrees, and the mechanisms of graph-based learning.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal patient allocation for echocardiographic assessments</title>
<link>https://arxiv.org/abs/2506.06297</link>
<guid>https://arxiv.org/abs/2506.06297</guid>
<content:encoded><![CDATA[
<div> scheduling, echocardiographic exams, stochastic simulation, resource allocation, reinforcement learning
<br />
Summary:
This study focuses on optimizing the scheduling of echocardiographic exams in a hospital setting where non-deterministic factors and resource constraints pose challenges. By analyzing operational data and using simulation models, the researchers compared on-the-fly and reservation-based allocation strategies. They found that on-the-fly allocation generally performed better in adapting to patient variability and resource constraints. Using reinforcement learning, an optimal dynamic allocation policy was derived and benchmarked against rule-based strategies. The results provide insights for improving efficiency in echo lab resource management through data-driven approaches. <div>
arXiv:2506.06297v1 Announce Type: new 
Abstract: Scheduling echocardiographic exams in a hospital presents significant challenges due to non-deterministic factors (e.g., patient no-shows, patient arrival times, diverse exam durations, etc.) and asymmetric resource constraints between fetal and non-fetal patient streams. To address these challenges, we first conducted extensive pre-processing on one week of operational data from the Echo Laboratory at Stanford University's Lucile Packard Children's Hospital, to estimate patient no-show probabilities and derive empirical distributions of arrival times and exam durations. Based on these inputs, we developed a discrete-event stochastic simulation model using SimPy, and integrate it with the open source Gymnasium Python library. As a baseline for policy optimization, we developed a comparative framework to evaluate on-the-fly versus reservation-based allocation strategies, in which different proportions of resources are reserved in advance. Considering a hospital configuration with a 1:6 ratio of fetal to non-fetal rooms and a 4:2 ratio of fetal to non-fetal sonographers, we show that on-the-fly allocation generally yields better performance, more effectively adapting to patient variability and resource constraints. Building on this foundation, we apply reinforcement learning (RL) to derive an approximated optimal dynamic allocation policy. This RL-based policy is benchmarked against the best-performing rule-based strategies, allowing us to quantify their differences and provide actionable insights for improving echo lab efficiency through intelligent, data-driven resource management.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pairwise Calibrated Rewards for Pluralistic Alignment</title>
<link>https://arxiv.org/abs/2506.06298</link>
<guid>https://arxiv.org/abs/2506.06298</guid>
<content:encoded><![CDATA[
<div> Keywords: alignment pipelines, diverse preferences, pairwise preference, calibration, pluralistic values

Summary: 
This article challenges the assumption of a universal notion of desirable behavior in current alignment pipelines. It proposes a novel approach that reflects diverse human preferences through a distribution over multiple reward functions, each leading to a unique aligned policy. By learning this distribution directly from pairwise preferences, without predefined groups or annotator identifiers, the algorithm aims to capture the full spectrum of user perspectives. The central criterion is pairwise calibration, ensuring that the proportion of reward functions favoring a candidate response aligns with the fraction of annotators expressing that preference. The authors prove that a small ensemble can accurately represent diverse preference distributions without outliers. Through empirical validation, a practical training heuristic is introduced to enhance this approach, leading to improved calibration and a more accurate representation of pluralistic values. <br /><br />Summary: <div>
arXiv:2506.06298v1 Announce Type: new 
Abstract: Current alignment pipelines presume a single, universal notion of desirable behavior. However, human preferences often diverge across users, contexts, and cultures. As a result, disagreement collapses into the majority signal and minority perspectives are discounted. To address this, we propose reflecting diverse human preferences through a distribution over multiple reward functions, each inducing a distinct aligned policy. The distribution is learned directly from pairwise preference without annotator identifiers or predefined groups. Instead, annotator disagreements are treated as informative soft labels. Our central criterion is pairwise calibration: for every pair of candidate responses, the proportion of reward functions preferring one response matches the fraction of annotators with that preference. We prove that even a small outlier-free ensemble can accurately represent diverse preference distributions. Empirically, we introduce and validate a practical training heuristic to learn such ensembles, and demonstrate its effectiveness through improved calibration, implying a more faithful representation of pluralistic values.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LT-PINN: Lagrangian Topology-conscious Physics-informed Neural Network for Boundary-focused Engineering Optimization</title>
<link>https://arxiv.org/abs/2506.06300</link>
<guid>https://arxiv.org/abs/2506.06300</guid>
<content:encoded><![CDATA[
<div> Lagrangian topology-conscious PINNs, boundary-focused, engineering optimization, PDEs, topology boundary curves, learnable parameters

Summary:
LT-PINNs, a new approach for engineering optimization, remove the need for manual interpolation by parameterizing topology boundary curves as learnable parameters. Specialized loss functions ensure accurate boundary representations for intricate topologies in two types of PDEs. LT-PINNs outperform DT-PINNs in reducing errors, handle arbitrary boundary conditions, and infer clear topology boundaries without manual interpolation. These advantages make LT-PINNs suitable for a wide range of PDEs and showcase their potential in flow velocity rearrangement applications, transforming uniform upstream velocities into sine-shaped downstream profiles. <div>
arXiv:2506.06300v1 Announce Type: new 
Abstract: Physics-informed neural networks (PINNs) have emerged as a powerful meshless tool for topology optimization, capable of simultaneously determining optimal topologies and physical solutions. However, conventional PINNs rely on density-based topology descriptions, which necessitate manual interpolation and limit their applicability to complex geometries. To address this, we propose Lagrangian topology-conscious PINNs (LT-PINNs), a novel framework for boundary-focused engineering optimization. By parameterizing the control variables of topology boundary curves as learnable parameters, LT-PINNs eliminate the need for manual interpolation and enable precise boundary determination. We further introduce specialized boundary condition loss function and topology loss function to ensure sharp and accurate boundary representations, even for intricate topologies. The accuracy and robustness of LT-PINNs are validated via two types of partial differential equations (PDEs), including elastic equation with Dirichlet boundary conditions and Laplace's equation with Neumann boundary conditions. Furthermore, we demonstrate effectiveness of LT-PINNs on more complex time-dependent and time-independent flow problems without relying on measurement data, and showcase their engineering application potential in flow velocity rearrangement, transforming a uniform upstream velocity into a sine-shaped downstream profile. The results demonstrate (1) LT-PINNs achieve substantial reductions in relative L2 errors compared with the state-of-art density topology-oriented PINNs (DT-PINNs), (2) LT-PINNs can handle arbitrary boundary conditions, making them suitable for a wide range of PDEs, and (3) LT-PINNs can infer clear topology boundaries without manual interpolation, especially for complex topologies.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reward Is Enough: LLMs Are In-Context Reinforcement Learners</title>
<link>https://arxiv.org/abs/2506.06303</link>
<guid>https://arxiv.org/abs/2506.06303</guid>
<content:encoded><![CDATA[
<div> RL, LLM, ICRL, prompting, benchmarks <br />
Summary: <br />
This work introduces the concept of in-context reinforcement learning (ICRL) emerging in Large Language Models (LLMs) during inference time. The proposed ICRL prompting framework involves multiple rounds of prompting the LLM to complete a task, providing numerical rewards for its responses, and incorporating previous context in subsequent prompts. Through experiments in Game of 24, creative writing, and ScienceWorld benchmarks, ICRL prompting shows significant performance improvements compared to baseline methods. It is observed that the LLM can maximize scalar reward signals during inference, similar to an RL algorithm. Interestingly, some experiments show that the LLM generates its own reward signals, yet still benefits from ICRL prompting, showcasing its potential for enhancing test-time compute scalability. <div>
arXiv:2506.06303v1 Announce Type: new 
Abstract: Reinforcement learning (RL) is a human-designed framework for solving sequential decision making problems. In this work, we demonstrate that, surprisingly, RL emerges in LLM's (Large Language Model) inference time -- a phenomenon known as in-context RL (ICRL). Specifically, we propose a novel multi-round prompting framework called ICRL prompting. The goal is to prompt the LLM to complete a task. After the LLM generates a response at the current round, we give numerical scalar feedbacks for the response, called the rewards. At the next round, we prompt the LLM again with the same task and a context consisting of all previous responses and rewards. We observe that the quality of the LLM's response increases as the context grows. In other words, the LLM is able to maximize the scalar reward signal in the inference time, just like an RL algorithm. We evaluate ICRL prompting in three benchmarks (Game of 24, creative writing, and ScienceWorld) and demonstrate significant performance improvements over baseline methods such as Self-Refine and Reflexion. Surprisingly, in some experiments the reward signals are generated by the LLM itself, yet performance improvements are still observed from ICRL prompting, offering a promising paradigm for scaling test-time compute.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wine Quality Prediction with Ensemble Trees: A Unified, Leak-Free Comparative Study</title>
<link>https://arxiv.org/abs/2506.06327</link>
<guid>https://arxiv.org/abs/2506.06327</guid>
<content:encoded><![CDATA[
<div> Keywords: ensemble learners, wine-quality assessment, physicochemical attributes, feature selection, benchmarking

Summary:
The study presents a benchmark of five ensemble learners for wine-quality assessment using the Vinho Verde red- and white-wine datasets. The workflow includes train-test split, resampling techniques, hyper-parameter search, and feature selection. Gradient Boosting achieved the highest accuracy, followed closely by Random Forest and XGBoost. Limiting models to the top-ranked variables reduces dimensionality while maintaining predictive performance. Alcohol, volatile acidity, sulphates, free SO2, and chlorides are identified as key predictors. Runtime profiling shows varying efficiency among the models, with Random Forest being the most cost-effective option. The study recommends Random Forest for production models, XGBoost and LightGBM for efficiency, and Gradient Boosting for accuracy. The pipeline and metrics provided establish a reproducible baseline for future research on wine-quality prediction. 

<br /><br />Summary: <div>
arXiv:2506.06327v1 Announce Type: new 
Abstract: Accurate and reproducible wine-quality assessment is critical for production control yet remains dominated by subjective, labour-intensive tasting panels. We present the first unified benchmark of five ensemble learners (Random Forest, Gradient Boosting, XGBoost, LightGBM, CatBoost) on the canonical Vinho Verde red- and white-wine datasets (1,599 and 4,898 instances, 11 physicochemical attributes). Our leakage-free workflow employs an 80:20 stratified train-test split, five-fold StratifiedGroupKFold within the training set, per-fold standardisation, SMOTE-Tomek resampling, inverse-frequency cost weighting, Optuna hyper-parameter search (120-200 trials per model) and a two-stage feature-selection refit. Final scores on untouched test sets are reported with weighted F1 as the headline metric. Gradient Boosting achieves the highest accuracy (weighted F1 0.693 +/- 0.028 for red and 0.664 +/- 0.016 for white), followed within three percentage points by Random Forest and XGBoost. Limiting each model to its five top-ranked variables lowers dimensionality by 55 percent while reducing weighted F1 by only 2.6 percentage points for red and 3.0 percentage points for white, indicating that alcohol, volatile acidity, sulphates, free SO2 and chlorides capture most predictive signal. Runtime profiling on an EPYC 9K84/H20 node reveals a steep efficiency gradient: Gradient Boosting averages 12 h per five-fold study, XGBoost and LightGBM require 2-3 h, CatBoost 1 h, and Random Forest under 50 min. We therefore recommend Random Forest as the most cost-effective production model, XGBoost and LightGBM as GPU-efficient alternatives, and Gradient Boosting as the accuracy ceiling for offline benchmarking. The fully documented pipeline and metric set provide a reproducible baseline for future work on imbalanced multi-class wine-quality prediction.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ExplainBench: A Benchmark Framework for Local Model Explanations in Fairness-Critical Applications</title>
<link>https://arxiv.org/abs/2506.06330</link>
<guid>https://arxiv.org/abs/2506.06330</guid>
<content:encoded><![CDATA[
<div> machine learning, local explanations, fairness, ExplainBench, interpretability  

Summary:  
ExplainBench is an open-source benchmarking suite designed for the systematic evaluation of local model explanations in ethically consequential datasets. It offers unified wrappers for popular explanation algorithms, end-to-end pipelines for model training and explanation generation, and supports evaluation through fidelity, sparsity, and robustness metrics. The framework includes a graphical interface for interactive exploration and is packaged as a Python module for easy integration into research workflows. By showcasing different explanation methods on datasets commonly used in fairness research, such as COMPAS and UCI Adult Income, ExplainBench advances the methodological foundations of interpretable machine learning. This tool enables reproducible, comparative analysis of local explanations, contributing to accountability in real-world AI systems. <div>
arXiv:2506.06330v1 Announce Type: new 
Abstract: As machine learning systems are increasingly deployed in high-stakes domains such as criminal justice, finance, and healthcare, the demand for interpretable and trustworthy models has intensified. Despite the proliferation of local explanation techniques, including SHAP, LIME, and counterfactual methods, there exists no standardized, reproducible framework for their comparative evaluation, particularly in fairness-sensitive settings.
  We introduce ExplainBench, an open-source benchmarking suite for systematic evaluation of local model explanations across ethically consequential datasets. ExplainBench provides unified wrappers for popular explanation algorithms, integrates end-to-end pipelines for model training and explanation generation, and supports evaluation via fidelity, sparsity, and robustness metrics. The framework includes a Streamlit-based graphical interface for interactive exploration and is packaged as a Python module for seamless integration into research workflows.
  We demonstrate ExplainBench on datasets commonly used in fairness research, such as COMPAS, UCI Adult Income, and LendingClub, and showcase how different explanation methods behave under a shared experimental protocol. By enabling reproducible, comparative analysis of local explanations, ExplainBench advances the methodological foundations of interpretable machine learning and facilitates accountability in real-world AI systems.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extending AALpy with Passive Learning: A Generalized State-Merging Approach</title>
<link>https://arxiv.org/abs/2506.06333</link>
<guid>https://arxiv.org/abs/2506.06333</guid>
<content:encoded><![CDATA[
<div> automata learning, AALpy, Python, active learning, state-merging

Summary:
AALpy is a popular open-source automata learning library in Python that focuses on active learning of systems with input and output behavior. The library offers state-of-the-art algorithms for various automaton types, including deterministic and probabilistic automata. Recently, a generalized implementation of the state-merging method in the red-blue framework has been added to AALpy, allowing for a more configurable implementation. This new feature simplifies the implementation of state-merging algorithms by defining compatibility criteria and scoring. Users can easily define and execute state-merging algorithms using AALpy, making it simpler to implement both existing and novel algorithms. With AALpy, implementing state-merging algorithms from the literature only requires a few lines of code, demonstrating the ease and flexibility of the library. <div>
arXiv:2506.06333v1 Announce Type: new 
Abstract: AALpy is a well-established open-source automata learning library written in Python with a focus on active learning of systems with IO behavior. It provides a wide range of state-of-the-art algorithms for different automaton types ranging from fully deterministic to probabilistic automata. In this work, we present the recent addition of a generalized implementation of an important method from the domain of passive automata learning: state-merging in the red-blue framework. Using a common internal representation for different automaton types allows for a general and highly configurable implementation of the red-blue framework. We describe how to define and execute state-merging algorithms using AALpy, which reduces the implementation effort for state-merging algorithms mainly to the definition of compatibility criteria and scoring. This aids the implementation of both existing and novel algorithms. In particular, defining some existing state-merging algorithms from the literature with AALpy only takes a few lines of code.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimized Local Updates in Federated Learning via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.06337</link>
<guid>https://arxiv.org/abs/2506.06337</guid>
<content:encoded><![CDATA[
<div> framework, federated learning, deep reinforcement learning, data privacy, performance drop
<br />
Summary:<br />
The paper introduces a novel framework for Federated Learning (FL) that utilizes Deep Reinforcement Learning (DRL) to optimize the amount of training data needed for client models without compromising data privacy. By using a DRL agent that learns from the change in training loss as a reward signal, the framework improves client performance by selecting an optimal amount of data for training. This approach addresses the issue of non-IID data distribution across clients, ensuring better overall performance. The DRL agent outputs optimized weights for each class in the training data, creating a policy that enhances the partitioning of the local training dataset during FL rounds. After FL, clients can further improve their performance on their own data distribution, mitigating the non-IID effects of aggregation. Experimental results demonstrate superior performance on various benchmark datasets and FL frameworks using the proposed algorithm. <div>
arXiv:2506.06337v1 Announce Type: new 
Abstract: Federated Learning (FL) is a distributed framework for collaborative model training over large-scale distributed data, enabling higher performance while maintaining client data privacy. However, the nature of model aggregation at the centralized server can result in a performance drop in the presence of non-IID data across different clients. We remark that training a client locally on more data than necessary does not benefit the overall performance of all clients. In this paper, we devise a novel framework that leverages a Deep Reinforcement Learning (DRL) agent to select an optimized amount of data necessary to train a client model without oversharing information with the server. Starting without awareness of the client's performance, the DRL agent utilizes the change in training loss as a reward signal and learns to optimize the amount of training data necessary for improving the client's performance. Specifically, after each aggregation round, the DRL algorithm considers the local performance as the current state and outputs the optimized weights for each class, in the training data, to be used during the next round of local training. In doing so, the agent learns a policy that creates an optimized partition of the local training dataset during the FL rounds. After FL, the client utilizes the entire local training dataset to further enhance its performance on its own data distribution, mitigating the non-IID effects of aggregation. Through extensive experiments, we demonstrate that training FL clients through our algorithm results in superior performance on multiple benchmark datasets and FL frameworks. Our code is available at https://github.com/amuraddd/optimized_client_training.git.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Transformers to Large Language Models: A systematic review of AI applications in the energy sector towards Agentic Digital Twins</title>
<link>https://arxiv.org/abs/2506.06359</link>
<guid>https://arxiv.org/abs/2506.06359</guid>
<content:encoded><![CDATA[
<div> Transformer architecture, Large Language Models, energy management, smart grids, artificial intelligence<br />
<br />
Summary: 
Artificial intelligence has promised to revolutionize energy management in smart grids by improving situational awareness and decision-making. Traditional machine learning has shown limitations in generalization and data integration, but recent advancements in Transformers and Large Language Models (LLMs) have enhanced modeling capabilities. This review explores applications of Transformers in various forecasting and grid management tasks, as well as the emerging role of LLMs in the energy sector. These developments indicate a shift towards Generative AI impacting day-to-day operations like forecasting and grid balancing. The concept of Agentic Digital Twin, integrating LLMs, is introduced for enhanced autonomy and proactive decision-making in energy management systems. <div>
arXiv:2506.06359v1 Announce Type: new 
Abstract: Artificial intelligence (AI) has long promised to improve energy management in smart grids by enhancing situational awareness and supporting more effective decision-making. While traditional machine learning has demonstrated notable results in forecasting and optimization, it often struggles with generalization, situational awareness, and heterogeneous data integration. Recent advances in foundation models such as Transformer architecture and Large Language Models (LLMs) have demonstrated improved capabilities in modelling complex temporal and contextual relationships, as well as in multi-modal data fusion which is essential for most AI applications in the energy sector. In this review we synthesize the rapid expanding field of AI applications in the energy domain focusing on Transformers and LLMs. We examine the architectural foundations, domain-specific adaptations and practical implementations of transformer models across various forecasting and grid management tasks. We then explore the emerging role of LLMs in the field: adaptation and fine tuning for the energy sector, the type of tasks they are suited for, and the new challenges they introduce. Along the way, we highlight practical implementations, innovations, and areas where the research frontier is rapidly expanding. These recent developments reviewed underscore a broader trend: Generative AI (GenAI) is beginning to augment decision-making not only in high-level planning but also in day-to-day operations, from forecasting and grid balancing to workforce training and asset onboarding. Building on these developments, we introduce the concept of the Agentic Digital Twin, a next-generation model that integrates LLMs to bring autonomy, proactivity, and social interaction into digital twin-based energy management systems.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Norm: A Survey of Synthetic Data Generation for Rare Events</title>
<link>https://arxiv.org/abs/2506.06380</link>
<guid>https://arxiv.org/abs/2506.06380</guid>
<content:encoded><![CDATA[
<div> Generative modeling, large language models, statistical theory, specialized training, heavy-tailed distributions<br />
<br />
Summary: This article discusses the challenge of predicting extreme events with limited data and the importance of synthetic data generation in addressing this issue. It provides an overview of generative modeling techniques and large language models that can capture heavy-tailed distributions. The article introduces a tailored evaluation framework covering various metrics for assessing model performance in extreme event scenarios. It categorizes key application domains and highlights underexplored areas such as behavioral finance, wildfires, earthquakes, windstorms, and infectious outbreaks. The article also outlines open challenges in synthetic rare-event research, providing a structured foundation for future advancements. <div>
arXiv:2506.06380v1 Announce Type: new 
Abstract: Extreme events, such as market crashes, natural disasters, and pandemics, are rare but catastrophic, often triggering cascading failures across interconnected systems. Accurate prediction and early warning can help minimize losses and improve preparedness. While data-driven methods offer powerful capabilities for extreme event modeling, they require abundant training data, yet extreme event data is inherently scarce, creating a fundamental challenge. Synthetic data generation has emerged as a powerful solution. However, existing surveys focus on general data with privacy preservation emphasis, rather than extreme events' unique performance requirements. This survey provides the first overview of synthetic data generation for extreme events. We systematically review generative modeling techniques and large language models, particularly those enhanced by statistical theory as well as specialized training and sampling mechanisms to capture heavy-tailed distributions. We summarize benchmark datasets and introduce a tailored evaluation framework covering statistical, dependence, visual, and task-oriented metrics. A central contribution is our in-depth analysis of each metric's applicability in extremeness and domain-specific adaptations, providing actionable guidance for model evaluation in extreme settings. We categorize key application domains and identify underexplored areas like behavioral finance, wildfires, earthquakes, windstorms, and infectious outbreaks. Finally, we outline open challenges, providing a structured foundation for advancing synthetic rare-event research.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Theoretical Analysis of Positional Encodings in Transformer Models: Impact on Expressiveness and Generalization</title>
<link>https://arxiv.org/abs/2506.06398</link>
<guid>https://arxiv.org/abs/2506.06398</guid>
<content:encoded><![CDATA[
<div> Keywords: Positional encoding, Transformer models, Expressiveness, Generalization, Extrapolation

Summary:
Positional encodings play a vital role in transformer-based models for processing sequential data efficiently. This paper introduces a theoretical framework to examine the impact of various positional encoding methods on a transformer's capabilities. It evaluates the expressiveness, generalization ability, and extrapolation potential of different encoding techniques such as sinusoidal, learned, relative, and bias-based methods like Attention with Linear Biases (ALiBi). The study defines expressiveness through function approximation and establishes generalization bounds using Rademacher complexity. Additionally, new encoding methods based on orthogonal functions like wavelets and Legendre polynomials are proposed. Experimental results on synthetic sequence-to-sequence tasks demonstrate that orthogonal transform-based encodings outperform traditional sinusoidal encodings in terms of generalization and extrapolation. This research fills a crucial gap in transformer theory, offering valuable insights for designing transformer models in various applications such as natural language processing and computer vision.<br /><br />Summary: <div>
arXiv:2506.06398v1 Announce Type: new 
Abstract: Positional encodings are a core part of transformer-based models, enabling processing of sequential data without recurrence. This paper presents a theoretical framework to analyze how various positional encoding methods, including sinusoidal, learned, relative, and bias-based methods like Attention with Linear Biases (ALiBi), impact a transformer's expressiveness, generalization ability, and extrapolation to longer sequences. Expressiveness is defined via function approximation, generalization bounds are established using Rademacher complexity, and new encoding methods based on orthogonal functions, such as wavelets and Legendre polynomials, are proposed. The extrapolation capacity of existing and proposed encodings is analyzed, extending ALiBi's biasing approach to a unified theoretical context. Experimental evaluation on synthetic sequence-to-sequence tasks shows that orthogonal transform-based encodings outperform traditional sinusoidal encodings in generalization and extrapolation. This work addresses a critical gap in transformer theory, providing insights for design choices in natural language processing, computer vision, and other transformer applications.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoxNTF: A New Approach for Joint Clustering and Prediction in Survival Analysis</title>
<link>https://arxiv.org/abs/2506.06411</link>
<guid>https://arxiv.org/abs/2506.06411</guid>
<content:encoded><![CDATA[
<div> non-negative tensor factorization, CoxNTF, survival analysis, latent factor representations, Coxnet model

Summary:
The article introduces CoxNTF, a novel approach that incorporates survival information into non-negative tensor factorization (NTF) for more accurate survival analysis predictions. By utilizing a weighted covariate tensor where survival probabilities from the Coxnet model guide the tensorization process, CoxNTF generates meaningful latent representations closely linked to survival outcomes. Results demonstrate that CoxNTF achieves comparable survival prediction performance to using Coxnet with original covariates while offering a structured and interpretable clustering framework. Moreover, the approach effectively manages feature redundancy, making it a valuable tool for joint clustering and prediction in survival analysis.<br /><br />Summary: <div>
arXiv:2506.06411v1 Announce Type: new 
Abstract: The interpretation of the results of survival analysis often benefits from latent factor representations of baseline covariates. However, existing methods, such as Nonnegative Matrix Factorization (NMF), do not incorporate survival information, limiting their predictive power. We present CoxNTF, a novel approach that uses non-negative tensor factorization (NTF) to derive meaningful latent representations that are closely associated with survival outcomes. CoxNTF constructs a weighted covariate tensor in which survival probabilities derived from the Coxnet model are used to guide the tensorization process. Our results show that CoxNTF achieves survival prediction performance comparable to using Coxnet with the original covariates, while providing a structured and interpretable clustering framework. In addition, the new approach effectively handles feature redundancy, making it a powerful tool for joint clustering and prediction in survival analysis.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeurNCD: Novel Class Discovery via Implicit Neural Representation</title>
<link>https://arxiv.org/abs/2506.06412</link>
<guid>https://arxiv.org/abs/2506.06412</guid>
<content:encoded><![CDATA[
<div> Keywords: novel class discovery, Embedding-NeRF model, semantic segmentation, feature augmentation, neural representations   

Summary:  
NeurNCD is introduced as a novel framework for discovering novel classes in open-world settings using the Embedding-NeRF model and KL divergence to aggregate semantic embedding and entropy in visual embedding space. It integrates feature query, modulation, and clustering for efficient feature augmentation and information exchange between pre-trained semantic segmentation networks and neural representations. The framework achieves superior segmentation performance in both open and closed-world scenarios without requiring densely labeled datasets or human interaction for sparse label supervision. Extensive experiments on NYUv2 and Replica datasets show that NeurNCD outperforms existing methods. <div>
arXiv:2506.06412v1 Announce Type: new 
Abstract: Discovering novel classes in open-world settings is crucial for real-world applications. Traditional explicit representations, such as object descriptors or 3D segmentation maps, are constrained by their discrete, hole-prone, and noisy nature, which hinders accurate novel class discovery. To address these challenges, we introduce NeurNCD, the first versatile and data-efficient framework for novel class discovery that employs the meticulously designed Embedding-NeRF model combined with KL divergence as a substitute for traditional explicit 3D segmentation maps to aggregate semantic embedding and entropy in visual embedding space. NeurNCD also integrates several key components, including feature query, feature modulation and clustering, facilitating efficient feature augmentation and information exchange between the pre-trained semantic segmentation network and implicit neural representations. As a result, our framework achieves superior segmentation performance in both open and closed-world settings without relying on densely labelled datasets for supervised training or human interaction to generate sparse label supervision. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art approaches on the NYUv2 and Replica datasets.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlocking Chemical Insights: Superior Molecular Representations from Intermediate Encoder Layers</title>
<link>https://arxiv.org/abs/2506.06443</link>
<guid>https://arxiv.org/abs/2506.06443</guid>
<content:encoded><![CDATA[
<div> Keywords: molecular encoders, ADMET property prediction, intermediate layers, downstream performance, state-of-the-art results 

Summary: 
Pretrained molecular encoders are essential in computational chemistry, but relying solely on final-layer embeddings may overlook valuable information. A layer-wise analysis of five molecular encoders across 22 ADMET property prediction tasks shows that intermediate layer embeddings consistently outperform final-layer representations. Utilizing fixed embeddings from optimal intermediate layers improved downstream performance by an average of 5.4%, with gains up to 28.6%. Finetuning up to these layers led to even greater average improvements of 8.5%, with performance increases as high as 40.8%, achieving new state-of-the-art results on various benchmarks. A strong positive correlation between fixed embedding performance and finetuning outcomes supports an efficient evaluate-then-finetune approach, enabling identification of optimal layers with reduced computation cost. It is crucial to explore the full representational depth of molecular encoders to achieve significant performance enhancements and computational efficiency.

<br /><br />Summary: <div>
arXiv:2506.06443v1 Announce Type: new 
Abstract: Pretrained molecular encoders have become indispensable in computational chemistry for tasks such as property prediction and molecular generation. However, the standard practice of relying solely on final-layer embeddings for downstream tasks may discard valuable information. In this work, we challenge this convention by conducting a comprehensive layer-wise analysis of five diverse molecular encoders across 22 ADMET property prediction tasks. Our results demonstrate that embeddings from intermediate layers consistently outperform final-layer representations. Specifically, using fixed embeddings from the optimal intermediate layers improved downstream performance by an average of 5.4%, reaching gains up to 28.6%. Furthermore, finetuning up to these intermediate layers yielded even greater average improvements of 8.5%, with performance increases as high as 40.8%, achieving new state-of-the-art results on several benchmarks. Additionally, a strong positive correlation between fixed embedding performance and finetuning outcomes supports an efficient evaluate-then-finetune approach, enabling identification of optimal layers with reduced computational cost. These findings highlight the importance of exploring the full representational depth of molecular encoders to achieve substantial performance improvements and computational efficiency. The code is made publicly available at https://github.com/luispintoc/Unlocking-Chemical-Insights.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety Assurance</title>
<link>https://arxiv.org/abs/2506.06444</link>
<guid>https://arxiv.org/abs/2506.06444</guid>
<content:encoded><![CDATA[
<div> keywords: safety assurance, inference scaling, multifurcation reward model, exploration efficiency dilemma, LLM

Summary:
Existing safety assurance research on Large Language Models (LLMs) has focused on training-phase alignment for instilling safe behaviors. However, this approach has been found vulnerable to jailbreak attacks. In this study, the authors introduce inference scaling for enhancing LLM safety against emerging threats. They identify the exploration-efficiency dilemma as a challenge in conventional inference scaling techniques and propose SAFFRON, a novel paradigm tailored for safety assurance. SAFFRON includes a multifurcation reward model (MRM) to reduce the computational overhead of reward model evaluations. Additional components of their approach include a partial supervision training objective for MRM, a conservative exploration constraint, and a Trie-based key-value caching strategy for tree search. Extensive experiments validate the effectiveness of SAFFRON, and the authors provide their trained MRM (Saffron-1) and token-level safety reward dataset (Safety4M) for further research. The code, model, and data are publicly available on GitHub. 

<br /><br />Summary: <div>
arXiv:2506.06444v1 Announce Type: new 
Abstract: Existing safety assurance research has primarily focused on training-phase alignment to instill safe behaviors into LLMs. However, recent studies have exposed these methods' susceptibility to diverse jailbreak attacks. Concurrently, inference scaling has significantly advanced LLM reasoning capabilities but remains unexplored in the context of safety assurance. Addressing this gap, our work pioneers inference scaling for robust and effective LLM safety against emerging threats. We reveal that conventional inference scaling techniques, despite their success in reasoning tasks, perform poorly in safety contexts, even falling short of basic approaches like Best-of-N Sampling. We attribute this inefficiency to a newly identified challenge, the exploration--efficiency dilemma, arising from the high computational overhead associated with frequent process reward model (PRM) evaluations. To overcome this dilemma, we propose SAFFRON, a novel inference scaling paradigm tailored explicitly for safety assurance. Central to our approach is the introduction of a multifurcation reward model (MRM) that significantly reduces the required number of reward model evaluations. To operationalize this paradigm, we further propose: (i) a partial supervision training objective for MRM, (ii) a conservative exploration constraint to prevent out-of-distribution explorations, and (iii) a Trie-based key--value caching strategy that facilitates cache sharing across sequences during tree search. Extensive experiments validate the effectiveness of our method. Additionally, we publicly release our trained multifurcation reward model (Saffron-1) and the accompanying token-level safety reward dataset (Safety4M) to accelerate future research in LLM safety. Our code, model, and data are publicly available at https://github.com/q-rz/saffron , and our project homepage is at https://q-rz.github.io/p/saffron .
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LETS Forecast: Learning Embedology for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2506.06454</link>
<guid>https://arxiv.org/abs/2506.06454</guid>
<content:encoded><![CDATA[
<div> framework, DeepEDM, nonlinear dynamical systems modeling, deep neural networks, forecasting accuracy <br />
Summary: <br />
The article introduces DeepEDM, a framework that combines nonlinear dynamical systems modeling with deep neural networks for time series forecasting. DeepEDM learns a latent space from time-delayed embeddings and uses kernel regression to model the underlying dynamics. It incorporates efficient softmax attention and accurately predicts future time steps. Experimental results on synthetic data and real-world time series demonstrate the robustness of DeepEDM to input noise and its superior forecasting accuracy compared to existing methods. The code for DeepEDM is available for use. <div>
arXiv:2506.06454v1 Announce Type: new 
Abstract: Real-world time series are often governed by complex nonlinear dynamics. Understanding these underlying dynamics is crucial for precise future prediction. While deep learning has achieved major success in time series forecasting, many existing approaches do not explicitly model the dynamics. To bridge this gap, we introduce DeepEDM, a framework that integrates nonlinear dynamical systems modeling with deep neural networks. Inspired by empirical dynamic modeling (EDM) and rooted in Takens' theorem, DeepEDM presents a novel deep model that learns a latent space from time-delayed embeddings, and employs kernel regression to approximate the underlying dynamics, while leveraging efficient implementation of softmax attention and allowing for accurate prediction of future time steps. To evaluate our method, we conduct comprehensive experiments on synthetic data of nonlinear dynamical systems as well as real-world time series across domains. Our results show that DeepEDM is robust to input noise, and outperforms state-of-the-art methods in forecasting accuracy. Our code is available at: https://abrarmajeedi.github.io/deep_edm.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WISCA: A Consensus-Based Approach to Harmonizing Interpretability in Tabular Datasets</title>
<link>https://arxiv.org/abs/2506.06455</link>
<guid>https://arxiv.org/abs/2506.06455</guid>
<content:encoded><![CDATA[
<div> Consensus explanations, ML models, interpretability, synthetic datasets, WISCA <br />
<br />
Summary: 
In the study, six machine learning models were trained on synthetic datasets with known ground truths, and various interpretability techniques were applied to them. The aim was to address the issue of conflicting explanations generated by different interpretability algorithms. A novel approach called WISCA (Weighted Scaled Consensus Attributions) was introduced to create consensus explanations by integrating class probability and normalized attributions. The results showed that WISCA consistently aligned with the most reliable individual method, demonstrating the effectiveness of robust consensus strategies in improving the reliability of explanations in machine learning models. This study emphasizes the importance of interpretability in scientific and high-stakes domains, where understanding the inner workings of ML models is crucial for decision-making and trust. <div>
arXiv:2506.06455v1 Announce Type: new 
Abstract: While predictive accuracy is often prioritized in machine learning (ML) models, interpretability remains essential in scientific and high-stakes domains. However, diverse interpretability algorithms frequently yield conflicting explanations, highlighting the need for consensus to harmonize results. In this study, six ML models were trained on six synthetic datasets with known ground truths, utilizing various model-agnostic interpretability techniques. Consensus explanations were generated using established methods and a novel approach: WISCA (Weighted Scaled Consensus Attributions), which integrates class probability and normalized attributions. WISCA consistently aligned with the most reliable individual method, underscoring the value of robust consensus strategies in improving explanation reliability.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Infant Sleep-Optimized Driving: Synergizing Wearable and Vehicle Sensing in Intelligent Cruise Control</title>
<link>https://arxiv.org/abs/2506.06459</link>
<guid>https://arxiv.org/abs/2506.06459</guid>
<content:encoded><![CDATA[
<div> Keywords: Automated driving, Infant sleep, Reinforcement learning, Wearable sensing, Neural networks

Summary: 
This study introduces a novel approach to enhance infant sleep quality in vehicles equipped with automated driving technology. By integrating reinforcement learning with wearable sensing and neural networks, the proposed intelligent cruise control system can personalize driving behavior to optimize passenger comfort and travel efficiency. The system dynamically adjusts driving aggressiveness based on real-time data from wearable sensors, vehicle controllers, and map applications. This allows for the optimization of acceleration, lane changes, and overtaking maneuvers to minimize disruptions to infant sleep. Simulation results demonstrate that the solution significantly improves infant sleep quality while maintaining travel efficiency. This research sheds light on the potential of using advanced technologies to address passenger well-being concerns in the context of automated driving. 

<br /><br />Summary: <div>
arXiv:2506.06459v1 Announce Type: new 
Abstract: Automated driving (AD) has substantially improved vehicle safety and driving comfort, but their impact on passenger well-being, particularly infant sleep, is not sufficiently studied. Sudden acceleration, abrupt braking, and sharp maneuvers can disrupt infant sleep, compromising both passenger comfort and parental convenience. To solve this problem, this paper explores the integration of reinforcement learning (RL) within AD to personalize driving behavior and optimally balance occupant comfort and travel efficiency. In particular, we propose an intelligent cruise control framework that adapts to varying driving conditions to enhance infant sleep quality by effectively synergizing wearable sensing and vehicle data. Long short-term memory (LSTM) and transformer-based neural networks are integrated with RL to model the relationship between driving behavior and infant sleep quality under diverse traffic and road conditions. Based on the sleep quality indicators from the wearable sensors, driving action data from vehicle controllers, and map data from map applications, the model dynamically computes the optimal driving aggressiveness level, which is subsequently translated into specific AD control strategies, e.g., the magnitude and frequency of acceleration, lane change, and overtaking. Simulation results demonstrate that the proposed solution significantly improves infant sleep quality compared to baseline methods, while preserving desirable travel efficiency.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TimeRecipe: A Time-Series Forecasting Recipe via Benchmarking Module Level Effectiveness</title>
<link>https://arxiv.org/abs/2506.06482</link>
<guid>https://arxiv.org/abs/2506.06482</guid>
<content:encoded><![CDATA[
<div> benchmarking, time-series forecasting, deep learning, model architectures, design components

Summary:
TimeRecipe is a new benchmarking framework for time-series forecasting that evaluates models at the module level. It conducts over 10,000 experiments to analyze the effectiveness of individual components across various datasets, forecasting horizons, and task settings. The results show that thorough exploration of the design space can lead to models surpassing existing state-of-the-art methods. Insights gained from the experiments help link specific design choices to different forecasting scenarios. Additionally, a practical toolkit within TimeRecipe is released to recommend suitable model architectures based on empirical insights. The framework provides valuable guidance for researchers and practitioners in choosing the most effective design components for their time-series forecasting tasks. <div>
arXiv:2506.06482v1 Announce Type: new 
Abstract: Time-series forecasting is an essential task with wide real-world applications across domains. While recent advances in deep learning have enabled time-series forecasting models with accurate predictions, there remains considerable debate over which architectures and design components, such as series decomposition or normalization, are most effective under varying conditions. Existing benchmarks primarily evaluate models at a high level, offering limited insight into why certain designs work better. To mitigate this gap, we propose TimeRecipe, a unified benchmarking framework that systematically evaluates time-series forecasting methods at the module level. TimeRecipe conducts over 10,000 experiments to assess the effectiveness of individual components across a diverse range of datasets, forecasting horizons, and task settings. Our results reveal that exhaustive exploration of the design space can yield models that outperform existing state-of-the-art methods and uncover meaningful intuitions linking specific design choices to forecasting scenarios. Furthermore, we release a practical toolkit within TimeRecipe that recommends suitable model architectures based on these empirical insights. The benchmark is available at: https://github.com/AdityaLab/TimeRecipe.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Certified Unlearning Approach without Access to Source Data</title>
<link>https://arxiv.org/abs/2506.06486</link>
<guid>https://arxiv.org/abs/2506.06486</guid>
<content:encoded><![CDATA[
<div> privacy regulations, data unlearning, certified framework, noise scaling, model behavior <br />
<br />
Summary: 
The article introduces a certified unlearning framework to erase private or copyrighted information from trained models without access to the original training data samples. The framework utilizes a surrogate dataset to approximate the statistical properties of the source data, allowing for controlled noise scaling based on the statistical distance between the two datasets. The approach provides strong guarantees on the model's behavior post-unlearning while maintaining utility. Theoretical bounds are established, and practical noise calibration techniques are introduced to ensure privacy guarantees, even with approximate statistical distances. Extensive experiments on synthetic and real-world datasets validate the method's effectiveness and reliability in privacy-sensitive settings. <div>
arXiv:2506.06486v1 Announce Type: new 
Abstract: With the growing adoption of data privacy regulations, the ability to erase private or copyrighted information from trained models has become a crucial requirement. Traditional unlearning methods often assume access to the complete training dataset, which is unrealistic in scenarios where the source data is no longer available. To address this challenge, we propose a certified unlearning framework that enables effective data removal \final{without access to the original training data samples}. Our approach utilizes a surrogate dataset that approximates the statistical properties of the source data, allowing for controlled noise scaling based on the statistical distance between the two. \updated{While our theoretical guarantees assume knowledge of the exact statistical distance, practical implementations typically approximate this distance, resulting in potentially weaker but still meaningful privacy guarantees.} This ensures strong guarantees on the model's behavior post-unlearning while maintaining its overall utility. We establish theoretical bounds, introduce practical noise calibration techniques, and validate our method through extensive experiments on both synthetic and real-world datasets. The results demonstrate the effectiveness and reliability of our approach in privacy-sensitive settings.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Membership Inference Attacks for Unseen Classes</title>
<link>https://arxiv.org/abs/2506.06488</link>
<guid>https://arxiv.org/abs/2506.06488</guid>
<content:encoded><![CDATA[
<div> Shadow model attacks, Membership inference attacks, Machine learning, Quantile regression, Distribution shift<br />
Summary:<br />
- The article discusses membership inference attacks on machine learning models, focusing on the scenario where the adversary does not have access to the entire subclass from the data distribution used to train the model.
- It shows that shadow model attacks perform poorly in such extreme distribution shift scenarios.
- The study introduces quantile regression as an alternative approach, demonstrating its superior performance compared to shadow model attacks.
- Quantile regression attacks outperform shadow model attacks in the class dropout setting, achieving significantly higher True Positive Rates (TPR) on unseen classes in datasets like CIFAR-100.
- The research also includes a theoretical model to explain the potential and limitations of the quantile regression approach. <br /> <br />Summary: <div>
arXiv:2506.06488v1 Announce Type: new 
Abstract: Shadow model attacks are the state-of-the-art approach for membership inference attacks on machine learning models. However, these attacks typically assume an adversary has access to a background (nonmember) data distribution that matches the distribution the target model was trained on. We initiate a study of membership inference attacks where the adversary or auditor cannot access an entire subclass from the distribution -- a more extreme but realistic version of distribution shift than has been studied previously. In this setting, we first show that the performance of shadow model attacks degrades catastrophically, and then demonstrate the promise of another approach, quantile regression, that does not have the same limitations. We show that quantile regression attacks consistently outperform shadow model attacks in the class dropout setting -- for example, quantile regression attacks achieve up to 11$\times$ the TPR of shadow models on the unseen class on CIFAR-100, and achieve nontrivial TPR on ImageNet even with 90% of training classes removed. We also provide a theoretical model that illustrates the potential and limitations of this approach.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alternating Gradient Flows: A Theory of Feature Learning in Two-layer Neural Networks</title>
<link>https://arxiv.org/abs/2506.06489</link>
<guid>https://arxiv.org/abs/2506.06489</guid>
<content:encoded><![CDATA[
<div> Algorithmic framework, feature learning, neural networks, two-layer networks, Alternating Gradient Flows<br />
<br />
Summary: AGF introduces an algorithmic framework that describes the dynamics of feature learning in two-layer networks from small initialization. It portrays a two-step process of maximizing utility over dormant neurons and minimizing cost over active ones, resembling the staircase-like loss curve seen in gradient flow. AGF quantifies the timing, order, and magnitude of feature acquisition and loss decreases, aligning with experimental results. It unifies existing analyses in linear networks and transformers, showing convergence to gradient flow in diagonal linear networks. AGF applied to quadratic networks revealed learning Fourier features in decreasing order of coefficient magnitude, providing a comprehensive understanding of training dynamics. Overall, AGF offers insights into feature learning in neural networks and paves the way for further exploration in this field. <br /> <div>
arXiv:2506.06489v1 Announce Type: new 
Abstract: What features neural networks learn, and how, remains an open question. In this paper, we introduce Alternating Gradient Flows (AGF), an algorithmic framework that describes the dynamics of feature learning in two-layer networks trained from small initialization. Prior works have shown that gradient flow in this regime exhibits a staircase-like loss curve, alternating between plateaus where neurons slowly align to useful directions and sharp drops where neurons rapidly grow in norm. AGF approximates this behavior as an alternating two-step process: maximizing a utility function over dormant neurons and minimizing a cost function over active ones. AGF begins with all neurons dormant. At each round, a dormant neuron activates, triggering the acquisition of a feature and a drop in the loss. AGF quantifies the order, timing, and magnitude of these drops, matching experiments across architectures. We show that AGF unifies and extends existing saddle-to-saddle analyses in fully connected linear networks and attention-only linear transformers, where the learned features are singular modes and principal components, respectively. In diagonal linear networks, we prove AGF converges to gradient flow in the limit of vanishing initialization. Applying AGF to quadratic networks trained to perform modular addition, we give the first complete characterization of the training dynamics, revealing that networks learn Fourier features in decreasing order of coefficient magnitude. Altogether, AGF offers a promising step towards understanding feature learning in neural networks.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic Problem Generation for Reasoning via Quality-Diversity Algorithms</title>
<link>https://arxiv.org/abs/2506.06499</link>
<guid>https://arxiv.org/abs/2506.06499</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language model, synthetic data generation, problem difficulty, model generalization, quality-diversity algorithms

Summary:
SPARQ is a new approach for generating diverse and high-quality synthetic math problems and solutions using a single model. By measuring a problem's solve-rate as a proxy for difficulty, over 20 million new problem-solution pairs were generated from a small seed dataset. Filtering generated data based on difficulty and fine-tuning the model on the resulting data improved model performance by up to 24%. High-quality data, determined by problem difficulty, led to better in-distribution performance, while more diverse data facilitated robust out-of-distribution (OOD) generalization. The study also confirmed the existence of model and data scaling laws in synthetically generated problems, which positively impacted downstream model generalization.<br /><br />Summary: <div>
arXiv:2506.06499v1 Announce Type: new 
Abstract: Large language model (LLM) driven synthetic data generation has emerged as a powerful method for improving model reasoning capabilities. However, most methods either distill large state-of-the-art models into small students or use natural ground-truth problem statements to guarantee problem statement quality. This limits the scalability of these approaches to more complex and diverse problem domains. To address this, we present SPARQ: Synthetic Problem Generation for Reasoning via Quality-Diversity Algorithms, a novel approach for generating high-quality and diverse synthetic math problem and solution pairs using only a single model by measuring a problem's solve-rate: a proxy for problem difficulty. Starting from a seed dataset of 7.5K samples, we generate over 20 million new problem-solution pairs. We show that filtering the generated data by difficulty and then fine-tuning the same model on the resulting data improves relative model performance by up to 24\%. Additionally, we conduct ablations studying the impact of synthetic data quantity, quality and diversity on model generalization. We find that higher quality, as measured by problem difficulty, facilitates better in-distribution performance. Further, while generating diverse synthetic data does not as strongly benefit in-distribution performance, filtering for more diverse data facilitates more robust OOD generalization. We also confirm the existence of model and data scaling laws for synthetically generated problems, which positively benefit downstream model generalization.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Rates in Continual Linear Regression via Increasing Regularization</title>
<link>https://arxiv.org/abs/2506.06501</link>
<guid>https://arxiv.org/abs/2506.06501</guid>
<content:encoded><![CDATA[
<div> regularization, continual learning, linear regression, stochastic gradient descent, forgetting <br />
<br />
Summary: 
The article delves into realizable continual linear regression in random task orderings. It addresses the gap in prior work concerning the expected loss after k learning iterations. By utilizing isotropic 2 regularization and finite step budgets as regularization schemes, the research shows that stochastic gradient descent on specific surrogate losses can narrow or eliminate this gap. A fixed regularization strength results in an optimal rate of O(log k / k), while an increasing regularization strength schedule achieves an optimal rate of O(1/k) through a generalized SGD for time-varying functions. The study suggests that increasing the regularization coefficient or decreasing the number of steps per task can be advantageous in worst-case scenarios. <div>
arXiv:2506.06501v1 Announce Type: new 
Abstract: We study realizable continual linear regression under random task orderings, a common setting for developing continual learning theory. In this setup, the worst-case expected loss after $k$ learning iterations admits a lower bound of $\Omega(1/k)$. However, prior work using an unregularized scheme has only established an upper bound of $O(1/k^{1/4})$, leaving a significant gap. Our paper proves that this gap can be narrowed, or even closed, using two frequently used regularization schemes: (1) explicit isotropic $\ell_2$ regularization, and (2) implicit regularization via finite step budgets. We show that these approaches, which are used in practice to mitigate forgetting, reduce to stochastic gradient descent (SGD) on carefully defined surrogate losses. Through this lens, we identify a fixed regularization strength that yields a near-optimal rate of $O(\log k / k)$. Moreover, formalizing and analyzing a generalized variant of SGD for time-varying functions, we derive an increasing regularization strength schedule that provably achieves an optimal rate of $O(1/k)$. This suggests that schedules that increase the regularization coefficient or decrease the number of steps per task are beneficial, at least in the worst case.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InstantFT: An FPGA-Based Runtime Subsecond Fine-tuning of CNN Models</title>
<link>https://arxiv.org/abs/2506.06505</link>
<guid>https://arxiv.org/abs/2506.06505</guid>
<content:encoded><![CDATA[
<div> FPGA, CNN, fine-tuning, IoT platforms, on-the-fly adaptation 
Summary:
FPGA-based method InstantFT proposed for ultra-fast CNN fine-tuning on IoT devices optimizing forward and backward computations in parameter-efficient fine-tuning (PEFT). InstantFT fine-tunes pre-trained CNN 17.4x faster than existing Low-Rank Adaptation (LoRA)-based approaches with comparable accuracy. Experiments on datasets with concept drift show InstantFT reduces fine-tuning time to just 0.36s and improves energy-efficiency by 16.3x. Enables on-the-fly adaptation of CNNs to non-stationary data distributions. <div>
arXiv:2506.06505v1 Announce Type: new 
Abstract: Training deep neural networks (DNNs) requires significantly more computation and memory than inference, making runtime adaptation of DNNs challenging on resource-limited IoT platforms. We propose InstantFT, an FPGA-based method for ultra-fast CNN fine-tuning on IoT devices, by optimizing the forward and backward computations in parameter-efficient fine-tuning (PEFT). Experiments on datasets with concept drift demonstrate that InstantFT fine-tunes a pre-trained CNN 17.4x faster than existing Low-Rank Adaptation (LoRA)-based approaches, while achieving comparable accuracy. Our FPGA-based InstantFT reduces the fine-tuning time to just 0.36s and improves energy-efficiency by 16.3x, enabling on-the-fly adaptation of CNNs to non-stationary data distributions.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sharp Gap-Dependent Variance-Aware Regret Bounds for Tabular MDPs</title>
<link>https://arxiv.org/abs/2506.06521</link>
<guid>https://arxiv.org/abs/2506.06521</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Episodic MDPs, Regret Bounds, Monotonic Value Propagation, Variance Awareness  
<br />  
Summary:  
The article discusses the gap-dependent regret bounds for episodic Markov Decision Processes (MDPs), focusing on the Monotonic Value Propagation (MVP) algorithm. The algorithm achieves a variance-aware gap-dependent regret bound, considering the suboptimality gap and maximum conditional total variance. The analysis highlights the importance of accounting for variance in regret bounds for learning algorithms. The results provide insights into the trade-off between suboptimality gaps, variance, and learning efficiency in episodic MDPs. The study demonstrates the necessity of considering variance even when the maximum unconditional total variance is minimal, emphasizing the significance of variance-aware algorithms in reinforcement learning tasks. The findings offer a novel perspective on regret bounds and provide a basis for potential adaptation to enhance the performance of learning algorithms in dynamic decision-making environments.  
<br /> <div>
arXiv:2506.06521v1 Announce Type: new 
Abstract: We consider the gap-dependent regret bounds for episodic MDPs. We show that the Monotonic Value Propagation (MVP) algorithm achieves a variance-aware gap-dependent regret bound of $$\tilde{O}\left(\left(\sum_{\Delta_h(s,a)>0} \frac{H^2 \log K \land \mathtt{Var}_{\max}^{\text{c}}}{\Delta_h(s,a)} +\sum_{\Delta_h(s,a)=0}\frac{ H^2 \land \mathtt{Var}_{\max}^{\text{c}}}{\Delta_{\mathrm{min}}} + SAH^4 (S \lor H) \right) \log K\right),$$ where $H$ is the planning horizon, $S$ is the number of states, $A$ is the number of actions, and $K$ is the number of episodes. Here, $\Delta_h(s,a) =V_h^* (a) - Q_h^* (s, a)$ represents the suboptimality gap and $\Delta_{\mathrm{min}} := \min_{\Delta_h (s,a) > 0} \Delta_h(s,a)$. The term $\mathtt{Var}_{\max}^{\text{c}}$ denotes the maximum conditional total variance, calculated as the maximum over all $(\pi, h, s)$ tuples of the expected total variance under policy $\pi$ conditioned on trajectories visiting state $s$ at step $h$. $\mathtt{Var}_{\max}^{\text{c}}$ characterizes the maximum randomness encountered when learning any $(h, s)$ pair. Our result stems from a novel analysis of the weighted sum of the suboptimality gap and can be potentially adapted for other algorithms. To complement the study, we establish a lower bound of $$\Omega \left( \sum_{\Delta_h(s,a)>0} \frac{H^2 \land \mathtt{Var}_{\max}^{\text{c}}}{\Delta_h(s,a)}\cdot \log K\right),$$ demonstrating the necessity of dependence on $\mathtt{Var}_{\max}^{\text{c}}$ even when the maximum unconditional total variance (without conditioning on $(h, s)$) approaches zero.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical and Collaborative LLM-Based Control for Multi-UAV Motion and Communication in Integrated Terrestrial and Non-Terrestrial Networks</title>
<link>https://arxiv.org/abs/2506.06532</link>
<guid>https://arxiv.org/abs/2506.06532</guid>
<content:encoded><![CDATA[
<div> Keywords: UAVs, multi-agent systems, communication control, motion planning, hierarchical collaboration

Summary: 
This research focuses on optimizing the control and communication of multiple unmanned aerial vehicles (UAVs) in dynamic environments using a novel hierarchical and collaborative approach based on large language models (LLMs). The study considers a scenario where UAVs operate within integrated terrestrial and non-terrestrial networks, such as high-altitude platform stations (HAPS), in an aerial highway setting. An LLM deployed on the HAPS manages UAV access control, while each UAV utilizes an onboard LLM for motion planning and control. By leveraging pre-trained models, this knowledge-driven framework enables strategic planning and tactical decision-making, leading to improved system rewards, lower operational costs, and reduced collision rates compared to traditional methods. Experimental results demonstrate the effectiveness of the proposed LLM-based approach in enhancing the performance of multi-UAV systems in complex environments.<br /><br />Summary: <div>
arXiv:2506.06532v1 Announce Type: new 
Abstract: Unmanned aerial vehicles (UAVs) have been widely adopted in various real-world applications. However, the control and optimization of multi-UAV systems remain a significant challenge, particularly in dynamic and constrained environments. This work explores the joint motion and communication control of multiple UAVs operating within integrated terrestrial and non-terrestrial networks that include high-altitude platform stations (HAPS). Specifically, we consider an aerial highway scenario in which UAVs must accelerate, decelerate, and change lanes to avoid collisions and maintain overall traffic flow. Different from existing studies, we propose a novel hierarchical and collaborative method based on large language models (LLMs). In our approach, an LLM deployed on the HAPS performs UAV access control, while another LLM onboard each UAV handles motion planning and control. This LLM-based framework leverages the rich knowledge embedded in pre-trained models to enable both high-level strategic planning and low-level tactical decisions. This knowledge-driven paradigm holds great potential for the development of next-generation 3D aerial highway systems. Experimental results demonstrate that our proposed collaborative LLM-based method achieves higher system rewards, lower operational costs, and significantly reduced UAV collision rates compared to baseline approaches.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoClip: Geometry-Aware Clipping for Differentially Private SGD</title>
<link>https://arxiv.org/abs/2506.06549</link>
<guid>https://arxiv.org/abs/2506.06549</guid>
<content:encoded><![CDATA[
<div> privacy, differential privacy, stochastic gradient descent, machine learning, GeoClip

Summary:<br />
- Differentially private stochastic gradient descent (DP-SGD) is a widely used method for training machine learning models with privacy guarantees.
- DP-SGD requires setting the per-sample gradient clipping threshold, impacting the privacy-utility trade-off.
- GeoClip is a geometry-aware framework that adapts gradient clipping based on the gradient distribution's geometry.
- GeoClip estimates the optimal transformation using previously released noisy gradients without additional privacy cost.
- Experiments demonstrate GeoClip's superior performance over existing adaptive clipping methods under the same privacy budget. 

<br />Summary: <div>
arXiv:2506.06549v1 Announce Type: new 
Abstract: Differentially private stochastic gradient descent (DP-SGD) is the most widely used method for training machine learning models with provable privacy guarantees. A key challenge in DP-SGD is setting the per-sample gradient clipping threshold, which significantly affects the trade-off between privacy and utility. While recent adaptive methods improve performance by adjusting this threshold during training, they operate in the standard coordinate system and fail to account for correlations across the coordinates of the gradient. We propose GeoClip, a geometry-aware framework that clips and perturbs gradients in a transformed basis aligned with the geometry of the gradient distribution. GeoClip adaptively estimates this transformation using only previously released noisy gradients, incurring no additional privacy cost. We provide convergence guarantees for GeoClip and derive a closed-form solution for the optimal transformation that minimizes the amount of noise added while keeping the probability of gradient clipping under control. Experiments on both tabular and image datasets demonstrate that GeoClip consistently outperforms existing adaptive clipping methods under the same privacy budget.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SDN-Based False Data Detection With Its Mitigation and Machine Learning Robustness for In-Vehicle Networks</title>
<link>https://arxiv.org/abs/2506.06556</link>
<guid>https://arxiv.org/abs/2506.06556</guid>
<content:encoded><![CDATA[
<div> SDN-based, False Data Detection, Mitigation System, in-vehicle networks, Controller Area Network<br />
Summary:<br />
The paper introduces a robust SDN-based False Data Detection and Mitigation System (FDDMS) for in-vehicle networks to enhance safety and security. It focuses on brake-related ECUs, utilizing SDN capabilities to monitor and detect false data injection attacks in real-time. By decoding raw CAN data, an attack model is created to illustrate potential vulnerabilities. The FDDMS incorporates an LSTM-based detection model to identify false data injection attacks and employs a variant of the DeepFool attack to evaluate robustness. A re-training technique with a threshold selection strategy is enhanced to countermeasure impacts of adversarial attacks. Furthermore, a mitigation scheme is implemented to redirect attack traffic by dynamically updating flow rules through SDN. Experimental results demonstrate the FDDMS's effectiveness in detecting and mitigating false data injection attacks in real-time, showcasing its robustness against various adversarial attacks.<br /><br />Summary: <div>
arXiv:2506.06556v1 Announce Type: new 
Abstract: As the development of autonomous and connected vehicles advances, the complexity of modern vehicles increases, with numerous Electronic Control Units (ECUs) integrated into the system. In an in-vehicle network, these ECUs communicate with one another using an standard protocol called Controller Area Network (CAN). Securing communication among ECUs plays a vital role in maintaining the safety and security of the vehicle. This paper proposes a robust SDN-based False Data Detection and Mitigation System (FDDMS) for in-vehicle networks. Leveraging the unique capabilities of Software-Defined Networking (SDN), FDDMS is designed to monitor and detect false data injection attacks in real-time. Specifically, we focus on brake-related ECUs within an SDN-enabled in-vehicle network. First, we decode raw CAN data to create an attack model that illustrates how false data can be injected into the system. Then, FDDMS, incorporating a Long Short Term Memory (LSTM)-based detection model, is used to identify false data injection attacks. We further propose an effective variant of DeepFool attack to evaluate the model's robustness. To countermeasure the impacts of four adversarial attacks including Fast gradient descent method, Basic iterative method, DeepFool, and the DeepFool variant, we further enhance a re-training technique method with a threshold based selection strategy. Finally, a mitigation scheme is implemented to redirect attack traffic by dynamically updating flow rules through SDN. Our experimental results show that the proposed FDDMS is robust against adversarial attacks and effectively detects and mitigates false data injection attacks in real-time.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rapid training of Hamiltonian graph networks without gradient descent</title>
<link>https://arxiv.org/abs/2506.06558</link>
<guid>https://arxiv.org/abs/2506.06558</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, Physical Symmetries, Constraint, Data-driven Modeling, Hamiltonian Graph Networks
<br />
Summary: 
The article introduces a new approach for learning dynamical systems that respect physical symmetries and constraints using graph neural networks. Traditional gradient-based optimization algorithms for training these networks can be slow, but the Hamiltonian Graph Networks (HGN) proposed in this work offer up to 600x faster training with comparable accuracy. The HGN model demonstrates robust performance in various simulations, including N-body mass-spring systems with different geometries. It retains essential physical invariances such as permutation, rotation, and translation. The model can even generalize to larger systems without the need for retraining. This challenges the dominance of iterative gradient-descent-based optimization algorithms and offers a promising alternative for training neural network models for physical systems.
<br /> <div>
arXiv:2506.06558v1 Announce Type: new 
Abstract: Learning dynamical systems that respect physical symmetries and constraints remains a fundamental challenge in data-driven modeling. Integrating physical laws with graph neural networks facilitates principled modeling of complex N-body dynamics and yields accurate and permutation-invariant models. However, training graph neural networks with iterative, gradient-based optimization algorithms (e.g., Adam, RMSProp, LBFGS) often leads to slow training, especially for large, complex systems. In comparison to 15 different optimizers, we demonstrate that Hamiltonian Graph Networks (HGN) can be trained up to 600x faster--but with comparable accuracy--by replacing iterative optimization with random feature-based parameter construction. We show robust performance in diverse simulations, including N-body mass-spring systems in up to 3 dimensions with different geometries, while retaining essential physical invariances with respect to permutation, rotation, and translation. We reveal that even when trained on minimal 8-node systems, the model can generalize in a zero-shot manner to systems as large as 4096 nodes without retraining. Our work challenges the dominance of iterative gradient-descent-based optimization algorithms for training neural network models for physical systems.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Persistence goes Spectral</title>
<link>https://arxiv.org/abs/2506.06571</link>
<guid>https://arxiv.org/abs/2506.06571</guid>
<content:encoded><![CDATA[
<div> Keywords: topological information, message-passing graph neural networks, Persistent Homology, SpectRe, graph representation learning

Summary: 
Including intricate topological information, such as cycles, in message-passing graph neural networks (GNNs) enhances their expressivity beyond the Weisfeiler-Leman hierarchy. Persistent Homology (PH) methods are being used more frequently for graph representation learning, with recent approaches adding vertex and edge features to PH diagrams for better expressivity. However, these methods still struggle to capture basic graph structural information. This paper introduces SpectRe, a topological descriptor for graphs that blends spectral information into PH diagrams, offering increased expressivity compared to existing descriptors. The concept of stability is also introduced, showing that SpectRe is locally stable. Experiments on both synthetic and real-world datasets demonstrate the effectiveness of SpectRe and its potential to improve graph models in various learning tasks.<br /><br />Summary: <div>
arXiv:2506.06571v1 Announce Type: new 
Abstract: Including intricate topological information (e.g., cycles) provably enhances the expressivity of message-passing graph neural networks (GNNs) beyond the Weisfeiler-Leman (WL) hierarchy. Consequently, Persistent Homology (PH) methods are increasingly employed for graph representation learning. In this context, recent works have proposed decorating classical PH diagrams with vertex and edge features for improved expressivity. However, due to their dependence on features, these methods still fail to capture basic graph structural information. In this paper, we propose SpectRe -- a new topological descriptor for graphs that integrates spectral information into PH diagrams. Notably, SpectRe is strictly more expressive than existing descriptors on graphs. We also introduce notions of global and local stability to analyze existing descriptors and establish that SpectRe is locally stable. Finally, experiments on synthetic and real-world datasets demonstrate the effectiveness of SpectRe and its potential to enhance the capabilities of graph models in relevant learning tasks.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Efficient Multi-LLM Inference: Characterization and Analysis of LLM Routing and Hierarchical Techniques</title>
<link>https://arxiv.org/abs/2506.06579</link>
<guid>https://arxiv.org/abs/2506.06579</guid>
<content:encoded><![CDATA[
<div> Language Models, NLP, Inference, Efficient, Model Selection  
Summary:  
Recent advancements in Language Models (LMs) have improved natural language processing tasks but remain computationally expensive. To address limitations, new approaches employ multi LLM intelligent model selection strategies based on query complexity. This research survey examines routing and hierarchical inference methods for efficient LLM inference. Routing selects the most suitable model for a query, while HI escalates queries through a sequence of models for a confident response. Both approaches aim to reduce computation by utilizing lightweight models for simpler tasks and escalating only when necessary. Comparative analysis across key performance metrics is provided, along with discussion on benchmarking efforts and open challenges. Future research directions focus on achieving faster response times, adaptive model selection based on task complexity, and scalable deployment across diverse environments for enhanced efficiency and accessibility in real world applications.<br /><br />Summary: <div>
arXiv:2506.06579v1 Announce Type: new 
Abstract: Recent progress in Language Models (LMs) has dramatically advanced the field of natural language processing (NLP), excelling at tasks like text generation, summarization, and question answering. However, their inference remains computationally expensive and energy intensive, especially in settings with limited hardware, power, or bandwidth. This makes it difficult to deploy LMs in mobile, edge, or cost sensitive environments. To address these challenges, recent approaches have introduced multi LLM intelligent model selection strategies that dynamically allocate computational resources based on query complexity -- using lightweight models for simpler queries and escalating to larger models only when necessary. This survey explores two complementary strategies for efficient LLM inference: (i) routing, which selects the most suitable model based on the query, and (ii) cascading or hierarchical inference (HI), which escalates queries through a sequence of models until a confident response is found. Both approaches aim to reduce computation by using lightweight models for simpler tasks while offloading only when needed. We provide a comparative analysis of these techniques across key performance metrics, discuss benchmarking efforts, and outline open challenges. Finally, we outline future research directions to enable faster response times, adaptive model selection based on task complexity, and scalable deployment across heterogeneous environments, making LLM based systems more efficient and accessible for real world applications.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demystifying Topological Message-Passing with Relational Structures: A Case Study on Oversquashing in Simplicial Message-Passing</title>
<link>https://arxiv.org/abs/2506.06582</link>
<guid>https://arxiv.org/abs/2506.06582</guid>
<content:encoded><![CDATA[
<div> Keywords: Topological deep learning, oversquashing, message-passing, relational structures, simplicial networks

Summary:
This article introduces a new approach to address the issue of oversquashing in topological deep learning (TDL) by proposing a unifying axiomatic framework that connects graph and topological message-passing. By viewing simplicial and cellular complexes through the lens of relational structures, researchers can extend graph-theoretic algorithms to higher-order structures. This framework allows for the analysis and mitigation of oversquashing in topological message-passing networks. The theoretical analysis and empirical studies on simplicial networks demonstrate the potential of this approach to advance TDL. With a focus on higher-order interactions in relational data, the framework bridges the gap between graph-based methods and topological message-passing, offering new insights and strategies for improving TDL models. <br /><br />Summary: <div>
arXiv:2506.06582v1 Announce Type: new 
Abstract: Topological deep learning (TDL) has emerged as a powerful tool for modeling higher-order interactions in relational data. However, phenomena such as oversquashing in topological message-passing remain understudied and lack theoretical analysis. We propose a unifying axiomatic framework that bridges graph and topological message-passing by viewing simplicial and cellular complexes and their message-passing schemes through the lens of relational structures. This approach extends graph-theoretic results and algorithms to higher-order structures, facilitating the analysis and mitigation of oversquashing in topological message-passing networks. Through theoretical analysis and empirical studies on simplicial networks, we demonstrate the potential of this framework to advance TDL.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Global Convergence of Gradient EM for Over-Parameterized Gaussian Mixtures</title>
<link>https://arxiv.org/abs/2506.06584</link>
<guid>https://arxiv.org/abs/2506.06584</guid>
<content:encoded><![CDATA[
<div> Gaussian Mixture Models, Expectation-Maximization, gradient EM, over-parameterized setting, global convergence guarantee <br />
<br />
Summary: 
In the over-parameterized setting, where the learning model uses more components than the ground truth Gaussian Mixture Model (GMM), this paper establishes a global convergence guarantee for gradient EM. The study demonstrates that with mild over-parameterization (n = Omega(m log m)), gradient EM can converge globally to the ground truth GMM at a polynomial rate with polynomial samples. Using novel tools such as Hermite polynomials and tensor decomposition, the analysis explores the dynamics of gradient EM and characterizes the geometric landscape of the likelihood loss for well-separated GMMs in general position. This result extends the global convergence and recovery guarantees for EM algorithms beyond the traditional exact-parameterized setting, showing promise for more complex GMM setups. <br /> <div>
arXiv:2506.06584v1 Announce Type: new 
Abstract: Learning Gaussian Mixture Models (GMMs) is a fundamental problem in machine learning, with the Expectation-Maximization (EM) algorithm and its popular variant gradient EM being arguably the most widely used algorithms in practice. In the exact-parameterized setting, where both the ground truth GMM and the learning model have the same number of components $m$, a vast line of work has aimed to establish rigorous recovery guarantees for EM. However, global convergence has only been proven for the case of $m=2$, and EM is known to fail to recover the ground truth when $m\geq 3$.
  In this paper, we consider the $\textit{over-parameterized}$ setting, where the learning model uses $n>m$ components to fit an $m$-component ground truth GMM. In contrast to the exact-parameterized case, we provide a rigorous global convergence guarantee for gradient EM. Specifically, for any well separated GMMs in general position, we prove that with only mild over-parameterization $n = \Omega(m\log m)$, randomly initialized gradient EM converges globally to the ground truth at a polynomial rate with polynomial samples. Our analysis proceeds in two stages and introduces a suite of novel tools for Gaussian Mixture analysis. We use Hermite polynomials to study the dynamics of gradient EM and employ tensor decomposition to characterize the geometric landscape of the likelihood loss. This is the first global convergence and recovery result for EM or Gradient EM beyond the special case of $m=2$.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Direct Prediction Set Minimization via Bilevel Conformal Classifier Training</title>
<link>https://arxiv.org/abs/2506.06599</link>
<guid>https://arxiv.org/abs/2506.06599</guid>
<content:encoded><![CDATA[
<div> conformal prediction, uncertainty quantification, deep classifiers, prediction set minimization, DPSM <br />
<br />
Summary: 
The paper introduces Direct Prediction Set Minimization (DPSM), a novel algorithm that integrates conformal principles into the training process of deep classifiers to reduce the size of prediction sets. By formulating conformal training as a bilevel optimization problem, DPSM aims to minimize the prediction set size by learning the quantile of conformity scores. The algorithm exhibits a learning bound of O(1/sqrt(n)), which outperforms existing methods with a bound of Omega(1/s). Experimental results on benchmark datasets and deep models demonstrate a significant reduction in prediction set size (20.46%) compared to prior conformal training baselines. The theory behind DPSM's performance is validated through these experiments, showcasing its effectiveness in improving the usability of conformal prediction frameworks. <div>
arXiv:2506.06599v1 Announce Type: new 
Abstract: Conformal prediction (CP) is a promising uncertainty quantification framework which works as a wrapper around a black-box classifier to construct prediction sets (i.e., subset of candidate classes) with provable guarantees. However, standard calibration methods for CP tend to produce large prediction sets which makes them less useful in practice. This paper considers the problem of integrating conformal principles into the training process of deep classifiers to directly minimize the size of prediction sets. We formulate conformal training as a bilevel optimization problem and propose the {\em Direct Prediction Set Minimization (DPSM)} algorithm to solve it. The key insight behind DPSM is to minimize a measure of the prediction set size (upper level) that is conditioned on the learned quantile of conformity scores (lower level). We analyze that DPSM has a learning bound of $O(1/\sqrt{n})$ (with $n$ training samples), while prior conformal training methods based on stochastic approximation for the quantile has a bound of $\Omega(1/s)$ (with batch size $s$ and typically $s \ll \sqrt{n}$). Experiments on various benchmark datasets and deep models show that DPSM significantly outperforms the best prior conformal training baseline with $20.46\%\downarrow$ in the prediction set size and validates our theory.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAtCh: Cognitive Assessment through Cookie Thief</title>
<link>https://arxiv.org/abs/2506.06603</link>
<guid>https://arxiv.org/abs/2506.06603</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, Alzheimer's disease, cognitive impairment, speech analysis, multimodal sentiment analysis

Summary:
Machine learning algorithms have been developed to predict Alzheimer's disease and related dementia from speech, but not for broader cognitive impairment (CI). This study evaluated various speech-based methods for predicting CI from audio recordings of patients. Results showed that multimodal methods, especially those focusing on acoustics, outperformed unimodal approaches for CI prediction. Interpretable acoustic features related to affect and prosody were found to be more effective than BERT-based linguistic features. The code for this study is available on GitHub for further exploration and development. Overall, the study highlights the potential of incorporating multimodal sentiment analysis methods, particularly those emphasizing acoustics, for predicting cognitive impairment from speech data. 

<br /><br />Summary: <div>
arXiv:2506.06603v1 Announce Type: new 
Abstract: Several machine learning algorithms have been developed for the prediction of Alzheimer's disease and related dementia (ADRD) from spontaneous speech. However, none of these algorithms have been translated for the prediction of broader cognitive impairment (CI), which in some cases is a precursor and risk factor of ADRD. In this paper, we evaluated several speech-based open-source methods originally proposed for the prediction of ADRD, as well as methods from multimodal sentiment analysis for the task of predicting CI from patient audio recordings. Results demonstrated that multimodal methods outperformed unimodal ones for CI prediction, and that acoustics-based approaches performed better than linguistics-based ones. Specifically, interpretable acoustic features relating to affect and prosody were found to significantly outperform BERT-based linguistic features and interpretable linguistic features, respectively. All the code developed for this study is available at https://github.com/JTColonel/catch.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stacey: Promoting Stochastic Steepest Descent via Accelerated $\ell_p$-Smooth Nonconvex Optimization</title>
<link>https://arxiv.org/abs/2506.06606</link>
<guid>https://arxiv.org/abs/2506.06606</guid>
<content:encoded><![CDATA[
<div> accelerated $\ell_p$ steepest descent, non-Euclidean structure, smooth optimization tasks, Stacey algorithm, empirical comparison <br />
<br />Summary:
The article introduces the Stacey algorithm, an accelerated $\ell_p$ steepest descent method designed to address the non-Euclidean structure prevalent in modern deep network training. By utilizing interpolated primal-dual iterate sequences, Stacey effectively navigates non-Euclidean smooth optimization tasks. The algorithm is compared empirically against popular methods like SGD, AdamW, and Lion on image classification and language model pretraining, showing faster convergence and higher final accuracy. The theoretical guarantees of Stacey's foundations are also discussed. Varying values of $p$ across different models and datasets demonstrate the efficiency and importance of non-Euclidean approaches over standard Euclidean methods. The code for Stacey algorithm can be found on GitHub. <div>
arXiv:2506.06606v1 Announce Type: new 
Abstract: While popular optimization methods such as SGD, AdamW, and Lion depend on steepest descent updates in either $\ell_2$ or $\ell_\infty$ norms, there remains a critical gap in handling the non-Euclidean structure observed in modern deep networks training. In this work, we address this need by introducing a new accelerated $\ell_p$ steepest descent algorithm, called Stacey, which uses interpolated primal-dual iterate sequences to effectively navigate non-Euclidean smooth optimization tasks. In addition to providing novel theoretical guarantees for the foundations of our algorithm, we empirically compare our approach against these popular methods on tasks including image classification and language model (LLM) pretraining, demonstrating both faster convergence and higher final accuracy. We further evaluate different values of $p$ across various models and datasets, underscoring the importance and efficiency of non-Euclidean approaches over standard Euclidean methods. Code can be found at https://github.com/xinyuluo8561/Stacey .
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning</title>
<link>https://arxiv.org/abs/2506.06632</link>
<guid>https://arxiv.org/abs/2506.06632</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Language Models, Reasoning, Curriculum Learning, Finite-Sample Complexity Bounds
Summary: 
The study introduces a new method called E2H Reasoner to enhance the reasoning capabilities of language models through reinforcement learning (RL). By scheduling tasks from easy to hard (E2H), the method allows language models to gradually build reasoning skills. The research shows that while easy tasks are important initially, fading them out through appropriate scheduling is crucial to prevent overfitting. Theoretical analysis establishes convergence guarantees for E2H Reasoner within an approximate policy iteration framework and demonstrates that learning through curriculum stages requires fewer total samples than direct learning when tasks are appropriately decomposed and conditioned. Empirical experiments across various domains indicate that E2H Reasoner significantly improves the reasoning ability of small language models, which struggle when trained with vanilla RL alone, showcasing the effectiveness of the proposed method.<br /><br />Summary: <div>
arXiv:2506.06632v1 Announce Type: new 
Abstract: We aim to improve the reasoning capabilities of language models via reinforcement learning (RL). Recent RL post-trained models like DeepSeek-R1 have demonstrated reasoning abilities on mathematical and coding tasks. However, prior studies suggest that using RL alone to improve reasoning on inherently difficult tasks is less effective. Here, we draw inspiration from curriculum learning and propose to schedule tasks from easy to hard (E2H), allowing LLMs to build reasoning skills gradually. Our method is termed E2H Reasoner. Empirically, we observe that, although easy tasks are important initially, fading them out through appropriate scheduling is essential in preventing overfitting. Theoretically, we establish convergence guarantees for E2H Reasoner within an approximate policy iteration framework. We derive finite-sample complexity bounds and show that when tasks are appropriately decomposed and conditioned, learning through curriculum stages requires fewer total samples than direct learning. Experiments across multiple domains show that E2H Reasoner significantly improves the reasoning ability of small LLMs (1.5B to 3B), which otherwise struggle when trained with vanilla RL alone, highlighting the effectiveness of our method.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-QRWKV: Exploring Quantum-Enhanced RWKV Models for Image Classification</title>
<link>https://arxiv.org/abs/2506.06633</link>
<guid>https://arxiv.org/abs/2506.06633</guid>
<content:encoded><![CDATA[
<div> Quantum Machine Learning, Image Classification, Receptance Weighted Key Value Architecture, Variational Quantum Circuit, Performance Improvement<br />
Summary:<br />
The paper introduces Vision-QRWKV, a hybrid quantum-classical extension of the Receptance Weighted Key Value (RWKV) architecture applied to image classification tasks. By integrating a variational quantum circuit (VQC) into the RWKV model, the quantum-enhanced version shows improved performance on various medical and standard image classification benchmarks. The quantum model outperforms the classical model, especially on datasets with subtle or noisy class differences. This study demonstrates the potential of quantum models for enhancing visual representations and improving feature transformations. The results indicate the advantage of quantum-enhanced models in capturing complex features in high-dimensional visual data. The hybrid approach offers insights into the trade-offs and potential benefits of quantum models for lightweight and efficient vision tasks.<br /><br />Summary: <div>
arXiv:2506.06633v1 Announce Type: new 
Abstract: Recent advancements in quantum machine learning have shown promise in enhancing classical neural network architectures, particularly in domains involving complex, high-dimensional data. Building upon prior work in temporal sequence modeling, this paper introduces Vision-QRWKV, a hybrid quantum-classical extension of the Receptance Weighted Key Value (RWKV) architecture, applied for the first time to image classification tasks. By integrating a variational quantum circuit (VQC) into the channel mixing component of RWKV, our model aims to improve nonlinear feature transformation and enhance the expressive capacity of visual representations.
  We evaluate both classical and quantum RWKV models on a diverse collection of 14 medical and standard image classification benchmarks, including MedMNIST datasets, MNIST, and FashionMNIST. Our results demonstrate that the quantum-enhanced model outperforms its classical counterpart on a majority of datasets, particularly those with subtle or noisy class distinctions (e.g., ChestMNIST, RetinaMNIST, BloodMNIST). This study represents the first systematic application of quantum-enhanced RWKV in the visual domain, offering insights into the architectural trade-offs and future potential of quantum models for lightweight and efficient vision tasks.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-Intrusive Load Monitoring Based on Image Load Signatures and Continual Learning</title>
<link>https://arxiv.org/abs/2506.06637</link>
<guid>https://arxiv.org/abs/2506.06637</guid>
<content:encoded><![CDATA[
<div> Load Monitoring, Electrical Devices, Image Load Signature, Continual Learning, Deep Convolutional Neural Networks
Summary:<br /><br />This paper introduces a new method for Non-Intrusive Load Monitoring (NILM) that utilizes "image load signatures" and continual learning. By converting power signals into visual images and utilizing deep convolutional neural networks, multiple devices can be identified and classified. The method also incorporates self-supervised pre-training to enhance feature generalization and online learning strategies to prevent model forgetting and accommodate new loads. Experimental results on high-sampling rate load datasets demonstrate improved recognition accuracy compared to existing methods and model variants. <div>
arXiv:2506.06637v1 Announce Type: new 
Abstract: Non-Intrusive Load Monitoring (NILM) identifies the operating status and energy consumption of each electrical device in the circuit by analyzing the electrical signals at the bus, which is of great significance for smart power management. However, the complex and changeable load combinations and application environments lead to the challenges of poor feature robustness and insufficient model generalization of traditional NILM methods. To this end, this paper proposes a new non-intrusive load monitoring method that integrates "image load signature" and continual learning. This method converts multi-dimensional power signals such as current, voltage, and power factor into visual image load feature signatures, and combines deep convolutional neural networks to realize the identification and classification of multiple devices; at the same time, self-supervised pre-training is introduced to improve feature generalization, and continual online learning strategies are used to overcome model forgetting to adapt to the emergence of new loads. This paper conducts a large number of experiments on high-sampling rate load datasets, and compares a variety of existing methods and model variants. The results show that the proposed method has achieved significant improvements in recognition accuracy.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spark Transformer: Reactivating Sparsity in FFN and Attention</title>
<link>https://arxiv.org/abs/2506.06644</link>
<guid>https://arxiv.org/abs/2506.06644</guid>
<content:encoded><![CDATA[
<div> Keywords: Spark Transformer, activation sparsity, top-k masking, statistical top-k algorithm, FLOPs reduction

Summary:
The paper introduces the Spark Transformer architecture, which achieves high activation sparsity in both feed-forward networks (FFN) and attention mechanisms without compromising model quality or increasing parameter count. The method uses top-k masking for explicit control over sparsity levels, with the introduction of a hardware-friendly statistical top-k algorithm to avoid sorting and training slowdowns. By reallocating existing parameters and attention key embeddings, Spark Transformer maintains model quality while enhancing efficiency. Pretrained with the Gemma-2 recipe, the model shows competitive performance on standard benchmarks with significant sparsity - only 8% of FFN neurons are activated, and each token attends to a maximum of 256 tokens. This sparsity results in a 2.5x reduction in FLOPs, leading to decoding speedups of up to 1.79x on CPU and 1.40x on GPU.

<br /><br />Summary: <div>
arXiv:2506.06644v1 Announce Type: new 
Abstract: The discovery of the lazy neuron phenomenon in trained Transformers, where the vast majority of neurons in their feed-forward networks (FFN) are inactive for each token, has spurred tremendous interests in activation sparsity for enhancing large model efficiency. While notable progress has been made in translating such sparsity to wall-time benefits, modern Transformers have moved away from the ReLU activation function crucial to this phenomenon. Existing efforts on re-introducing activation sparsity often degrade model quality, increase parameter count, complicate or slow down training. Sparse attention, the application of sparse activation to the attention mechanism, often faces similar challenges.
  This paper introduces the Spark Transformer, a novel architecture that achieves a high level of activation sparsity in both FFN and the attention mechanism while maintaining model quality, parameter count, and standard training procedures. Our method realizes sparsity via top-k masking for explicit control over sparsity level. Crucially, we introduce statistical top-k, a hardware-accelerator-friendly, linear-time approximate algorithm that avoids costly sorting and mitigates significant training slowdown from standard top-$k$ operators. Furthermore, Spark Transformer reallocates existing FFN parameters and attention key embeddings to form a low-cost predictor for identifying activated entries. This design not only mitigates quality loss from enforced sparsity, but also enhances wall-time benefit. Pretrained with the Gemma-2 recipe, Spark Transformer demonstrates competitive performance on standard benchmarks while exhibiting significant sparsity: only 8% of FFN neurons are activated, and each token attends to a maximum of 256 tokens. This sparsity translates to a 2.5x reduction in FLOPs, leading to decoding wall-time speedups of up to 1.79x on CPU and 1.40x on GPU.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAFER: A Calibrated Risk-Aware Multimodal Recommendation Model for Dynamic Treatment Regimes</title>
<link>https://arxiv.org/abs/2506.06649</link>
<guid>https://arxiv.org/abs/2506.06649</guid>
<content:encoded><![CDATA[
<div> Keywords: Dynamic treatment regimes, precision medicine, clinical notes, conformal prediction, sepsis dataset 

Summary: 
SAFER is a new framework for dynamic treatment regimes (DTR) that combines structured electronic health record (EHR) data with clinical notes to provide personalized treatment recommendations in real-time. It addresses label uncertainty by considering ambiguous optimal treatment solutions for deceased patients. SAFER uses conformal prediction to ensure safe recommendations while filtering out uncertain predictions. Experimental results on sepsis datasets show that SAFER outperforms existing methods in recommendation metrics and counterfactual mortality rates. The framework offers statistical guarantees and robust formal assurances, making it a reliable solution for high-stakes DTR applications. <div>
arXiv:2506.06649v1 Announce Type: new 
Abstract: Dynamic treatment regimes (DTRs) are critical to precision medicine, optimizing long-term outcomes through personalized, real-time decision-making in evolving clinical contexts, but require careful supervision for unsafe treatment risks. Existing efforts rely primarily on clinician-prescribed gold standards despite the absence of a known optimal strategy, and predominantly using structured EHR data without extracting valuable insights from clinical notes, limiting their reliability for treatment recommendations. In this work, we introduce SAFER, a calibrated risk-aware tabular-language recommendation framework for DTR that integrates both structured EHR and clinical notes, enabling them to learn from each other, and addresses inherent label uncertainty by assuming ambiguous optimal treatment solution for deceased patients. Moreover, SAFER employs conformal prediction to provide statistical guarantees, ensuring safe treatment recommendations while filtering out uncertain predictions. Experiments on two publicly available sepsis datasets demonstrate that SAFER outperforms state-of-the-art baselines across multiple recommendation metrics and counterfactual mortality rate, while offering robust formal assurances. These findings underscore SAFER potential as a trustworthy and theoretically grounded solution for high-stakes DTR applications.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rescaled Influence Functions: Accurate Data Attribution in High Dimension</title>
<link>https://arxiv.org/abs/2506.06656</link>
<guid>https://arxiv.org/abs/2506.06656</guid>
<content:encoded><![CDATA[
<div> influence functions, data attribution, machine learning, rescaled influence functions, data poisoning attacks

Summary:
Rescaled influence functions (RIF) are introduced as a more accurate alternative to influence functions (IF) for data attribution in machine learning. IFs often underestimate the effect of sample removals, especially in high-dimensional settings, while RIFs provide significantly better predictions without much computational overhead. Real-world dataset comparisons demonstrate the superior accuracy of RIFs over IFs. A theoretical analysis is presented to explain the improved performance of RIF. Additionally, a class of data poisoning attacks that can deceive IF-based detections but are detectable by RIF is discussed. RIF offers a more precise method for understanding the impact of training data on model behavior and can be used effectively in various machine learning applications.<br /><br />Summary: <div>
arXiv:2506.06656v1 Announce Type: new 
Abstract: How does the training data affect a model's behavior? This is the question we seek to answer with data attribution. The leading practical approaches to data attribution are based on influence functions (IF). IFs utilize a first-order Taylor approximation to efficiently predict the effect of removing a set of samples from the training set without retraining the model, and are used in a wide variety of machine learning applications. However, especially in the high-dimensional regime (# params $\geq \Omega($# samples$)$), they are often imprecise and tend to underestimate the effect of sample removals, even for simple models such as logistic regression. We present rescaled influence functions (RIF), a new tool for data attribution which can be used as a drop-in replacement for influence functions, with little computational overhead but significant improvement in accuracy. We compare IF and RIF on a range of real-world datasets, showing that RIFs offer significantly better predictions in practice, and present a theoretical analysis explaining this improvement. Finally, we present a simple class of data poisoning attacks that would fool IF-based detections but would be detected by RIF.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SDP-CROWN: Efficient Bound Propagation for Neural Network Verification with Tightness of Semidefinite Programming</title>
<link>https://arxiv.org/abs/2506.06665</link>
<guid>https://arxiv.org/abs/2506.06665</guid>
<content:encoded><![CDATA[
arXiv:2506.06665v1 Announce Type: new 
Abstract: Neural network verifiers based on linear bound propagation scale impressively to massive models but can be surprisingly loose when neuron coupling is crucial. Conversely, semidefinite programming (SDP) verifiers capture inter-neuron coupling naturally, but their cubic complexity restricts them to only small models. In this paper, we propose SDP-CROWN, a novel hybrid verification framework that combines the tightness of SDP relaxations with the scalability of bound-propagation verifiers. At the core of SDP-CROWN is a new linear bound, derived via SDP principles, that explicitly captures $\ell_{2}$-norm-based inter-neuron coupling while adding only one extra parameter per layer. This bound can be integrated seamlessly into any linear bound-propagation pipeline, preserving the inherent scalability of such methods yet significantly improving tightness. In theory, we prove that our inter-neuron bound can be up to a factor of $\sqrt{n}$ tighter than traditional per-neuron bounds. In practice, when incorporated into the state-of-the-art $\alpha$-CROWN verifier, we observe markedly improved verification performance on large models with up to 65 thousand neurons and 2.47 million parameters, achieving tightness that approaches that of costly SDP-based methods.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Through the Gaps: Uncovering Tactical Line-Breaking Passes with Clustering</title>
<link>https://arxiv.org/abs/2506.06666</link>
<guid>https://arxiv.org/abs/2506.06666</guid>
<content:encoded><![CDATA[
arXiv:2506.06666v1 Announce Type: new 
Abstract: Line-breaking passes (LBPs) are crucial tactical actions in football, allowing teams to penetrate defensive lines and access high-value spaces. In this study, we present an unsupervised, clustering-based framework for detecting and analysing LBPs using synchronised event and tracking data from elite matches. Our approach models opponent team shape through vertical spatial segmentation and identifies passes that disrupt defensive lines within open play. Beyond detection, we introduce several tactical metrics, including the space build-up ratio (SBR) and two chain-based variants, LBPCh$^1$ and LBPCh$^2$, which quantify the effectiveness of LBPs in generating immediate or sustained attacking threats. We evaluate these metrics across teams and players in the 2022 FIFA World Cup, revealing stylistic differences in vertical progression and structural disruption. The proposed methodology is explainable, scalable, and directly applicable to modern performance analysis and scouting workflows.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Robust Heterogeneous Graph Representations via Contrastive-Reconstruction under Sparse Semantics</title>
<link>https://arxiv.org/abs/2506.06682</link>
<guid>https://arxiv.org/abs/2506.06682</guid>
<content:encoded><![CDATA[
arXiv:2506.06682v1 Announce Type: new 
Abstract: In graph self-supervised learning, masked autoencoders (MAE) and contrastive learning (CL) are two prominent paradigms. MAE focuses on reconstructing masked elements, while CL maximizes similarity between augmented graph views. Recent studies highlight their complementarity: MAE excels at local feature capture, and CL at global information extraction. Hybrid frameworks for homogeneous graphs have been proposed, but face challenges in designing shared encoders to meet the semantic requirements of both tasks. In semantically sparse scenarios, CL struggles with view construction, and gradient imbalance between positive and negative samples persists. This paper introduces HetCRF, a novel dual-channel self-supervised learning framework for heterogeneous graphs. HetCRF uses a two-stage aggregation strategy to adapt embedding semantics, making it compatible with both MAE and CL. To address semantic sparsity, it enhances encoder output for view construction instead of relying on raw features, improving efficiency. Two positive sample augmentation strategies are also proposed to balance gradient contributions. Node classification experiments on four real-world heterogeneous graph datasets demonstrate that HetCRF outperforms state-of-the-art baselines. On datasets with missing node features, such as Aminer and Freebase, at a 40% label rate in node classification, HetCRF improves the Macro-F1 score by 2.75% and 2.2% respectively compared to the second-best baseline, validating its effectiveness and superiority.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking Data Silos: Towards Open and Scalable Mobility Foundation Models via Generative Continual Learning</title>
<link>https://arxiv.org/abs/2506.06694</link>
<guid>https://arxiv.org/abs/2506.06694</guid>
<content:encoded><![CDATA[
arXiv:2506.06694v1 Announce Type: new 
Abstract: Foundation models have revolutionized fields such as natural language processing and computer vision by enabling general-purpose learning across diverse tasks and datasets. However, building analogous models for human mobility remains challenging due to the privacy-sensitive nature of mobility data and the resulting data silos across institutions. To bridge this gap, we propose MoveGCL, a scalable and privacy-preserving framework for training mobility foundation models via generative continual learning. Without sharing raw data, MoveGCL enables decentralized and progressive model evolution by replaying synthetic trajectories generated from a frozen teacher model, and reinforces knowledge retention through a tailored distillation strategy that mitigates catastrophic forgetting. To address the heterogeneity of mobility patterns, MoveGCL incorporates a Mixture-of-Experts Transformer with a mobility-aware expert routing mechanism, and employs a layer-wise progressive adaptation strategy to stabilize continual updates. Experiments on six real-world urban datasets demonstrate that MoveGCL achieves performance comparable to joint training and significantly outperforms federated learning baselines, while offering strong privacy protection. MoveGCL marks a crucial step toward unlocking foundation models for mobility, offering a practical blueprint for open, scalable, and privacy-preserving model development in the era of foundation models.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MarginSel : Max-Margin Demonstration Selection for LLMs</title>
<link>https://arxiv.org/abs/2506.06699</link>
<guid>https://arxiv.org/abs/2506.06699</guid>
<content:encoded><![CDATA[
arXiv:2506.06699v1 Announce Type: new 
Abstract: Large Language Models (LLMs) excel at few-shot learning via in-context learning (ICL). However, the effectiveness of ICL is often sensitive to the selection and ordering of demonstration examples. To address this, we present MarginSel: Max-Margin Demonstration Selection for LLMs, a two-step method that selects hard demonstration examples for the ICL prompt, adapting to each test instance. Our approach achieves 2-7% absolute improvement in F1-score across classification tasks, compared to a random selection of examples. We also provide theoretical insights and empirical evidence showing that MarginSel induces max-margin behavior in LLMs by effectively increasing the margin for hard examples, analogous to support vectors, thereby shifting the decision boundary in a beneficial direction.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Protein Transformers Have Biological Intelligence?</title>
<link>https://arxiv.org/abs/2506.06701</link>
<guid>https://arxiv.org/abs/2506.06701</guid>
<content:encoded><![CDATA[
arXiv:2506.06701v1 Announce Type: new 
Abstract: Deep neural networks, particularly Transformers, have been widely adopted for predicting the functional properties of proteins. In this work, we focus on exploring whether Protein Transformers can capture biological intelligence among protein sequences. To achieve our goal, we first introduce a protein function dataset, namely Protein-FN, providing over 9000 protein data with meaningful labels. Second, we devise a new Transformer architecture, namely Sequence Protein Transformers (SPT), for computationally efficient protein function predictions. Third, we develop a novel Explainable Artificial Intelligence (XAI) technique called Sequence Score, which can efficiently interpret the decision-making processes of protein models, thereby overcoming the difficulty of deciphering biological intelligence bided in Protein Transformers. Remarkably, even our smallest SPT-Tiny model, which contains only 5.4M parameters, demonstrates impressive predictive accuracy, achieving 94.3% on the Antibiotic Resistance (AR) dataset and 99.6% on the Protein-FN dataset, all accomplished by training from scratch. Besides, our Sequence Score technique helps reveal that our SPT models can discover several meaningful patterns underlying the sequence structures of protein data, with these patterns aligning closely with the domain knowledge in the biology community. We have officially released our Protein-FN dataset on Hugging Face Datasets https://huggingface.co/datasets/Protein-FN/Protein-FN. Our code is available at https://github.com/fudong03/BioIntelligence.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Framework for Controllable Multi-objective Learning with Annealed Stein Variational Hypernetworks</title>
<link>https://arxiv.org/abs/2506.06715</link>
<guid>https://arxiv.org/abs/2506.06715</guid>
<content:encoded><![CDATA[
arXiv:2506.06715v1 Announce Type: new 
Abstract: Pareto Set Learning (PSL) is popular as an efficient approach to obtaining the complete optimal solution in Multi-objective Learning (MOL). A set of optimal solutions approximates the Pareto set, and its mapping is a set of dense points in the Pareto front in objective space. However, some current methods face a challenge: how to make the Pareto solution is diverse while maximizing the hypervolume value. In this paper, we propose a novel method to address this challenge, which employs Stein Variational Gradient Descent (SVGD) to approximate the entire Pareto set. SVGD pushes a set of particles towards the Pareto set by applying a form of functional gradient descent, which helps to converge and diversify optimal solutions. Additionally, we employ diverse gradient direction strategies to thoroughly investigate a unified framework for SVGD in multi-objective optimization and adapt this framework with an annealing schedule to promote stability. We introduce our method, SVH-MOL, and validate its effectiveness through extensive experiments on multi-objective problems and multi-task learning, demonstrating its superior performance.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The OCR Quest for Generalization: Learning to recognize low-resource alphabets with model editing</title>
<link>https://arxiv.org/abs/2506.06761</link>
<guid>https://arxiv.org/abs/2506.06761</guid>
<content:encoded><![CDATA[
arXiv:2506.06761v1 Announce Type: new 
Abstract: Achieving robustness in recognition systems across diverse domains is crucial for their practical utility. While ample data availability is usually assumed, low-resource languages, such as ancient manuscripts and non-western languages, tend to be kept out of the equations of massive pretraining and foundational techniques due to an under representation. In this work, we aim for building models which can generalize to new distributions of data, such as alphabets, faster than centralized fine-tune strategies. For doing so, we take advantage of the recent advancements in model editing to enhance the incorporation of unseen scripts (low-resource learning). In contrast to state-of-the-art meta-learning, we showcase the effectiveness of domain merging in sparse distributions of data, with agnosticity of its relation to the overall distribution or any other prototyping necessity. Even when using the same exact training data, our experiments showcase significant performance boosts in \textbf{transfer learning} to new alphabets and \textbf{out-of-domain evaluation} in challenging domain shifts, including historical ciphered texts and non-Latin scripts. This research contributes a novel approach into building models that can easily adopt under-represented alphabets and, therefore, enable document recognition to a wider set of contexts and cultures.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feature-Based Instance Neighbor Discovery: Advanced Stable Test-Time Adaptation in Dynamic World</title>
<link>https://arxiv.org/abs/2506.06782</link>
<guid>https://arxiv.org/abs/2506.06782</guid>
<content:encoded><![CDATA[
arXiv:2506.06782v1 Announce Type: new 
Abstract: Despite progress, deep neural networks still suffer performance declines under distribution shifts between training and test domains, leading to a substantial decrease in Quality of Experience (QoE) for applications. Existing test-time adaptation (TTA) methods are challenged by dynamic, multiple test distributions within batches. We observe that feature distributions across different domains inherently cluster into distinct groups with varying means and variances. This divergence reveals a critical limitation of previous global normalization strategies in TTA, which inevitably distort the original data characteristics. Based on this insight, we propose Feature-based Instance Neighbor Discovery (FIND), which comprises three key components: Layer-wise Feature Disentanglement (LFD), Feature Aware Batch Normalization (FABN) and Selective FABN (S-FABN). LFD stably captures features with similar distributions at each layer by constructing graph structures. While FABN optimally combines source statistics with test-time distribution specific statistics for robust feature representation. Finally, S-FABN determines which layers require feature partitioning and which can remain unified, thereby enhancing inference efficiency. Extensive experiments demonstrate that FIND significantly outperforms existing methods, achieving a 30\% accuracy improvement in dynamic scenarios while maintaining computational efficiency.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Caterpillar GNN: Replacing Message Passing with Efficient Aggregation</title>
<link>https://arxiv.org/abs/2506.06784</link>
<guid>https://arxiv.org/abs/2506.06784</guid>
<content:encoded><![CDATA[
arXiv:2506.06784v1 Announce Type: new 
Abstract: Message-passing graph neural networks (MPGNNs) dominate modern graph learning, typically prioritizing maximal expressive power. In contrast, we introduce an \emph{efficient aggregation} mechanism, deliberately trading off some expressivity for stronger and more structured aggregation capabilities. Our approach allows seamless scaling between classical message-passing and simpler methods based on colored or plain walks. We rigorously characterize the expressive power at each intermediate step using homomorphism counts from a hierarchy of generalized \emph{caterpillar graphs}. Based on this foundation, we propose the \emph{Caterpillar GNN}, whose robust graph-level aggregation enables it to successfully tackle synthetic graph-level task specifically designed to be challenging for classical MPGNNs. Moreover, we demonstrate that, on real-world datasets, the Caterpillar GNN achieves comparable predictive performance while significantly reducing the number of nodes in the hidden layers of the computational graph.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FuncGNN: Learning Functional Semantics of Logic Circuits with Graph Neural Networks</title>
<link>https://arxiv.org/abs/2506.06787</link>
<guid>https://arxiv.org/abs/2506.06787</guid>
<content:encoded><![CDATA[
arXiv:2506.06787v1 Announce Type: new 
Abstract: As integrated circuit scale grows and design complexity rises, effective circuit representation helps support logic synthesis, formal verification, and other automated processes in electronic design automation. And-Inverter Graphs (AIGs), as a compact and canonical structure, are widely adopted for representing Boolean logic in these workflows. However, the increasing complexity and integration density of modern circuits introduce structural heterogeneity and global logic information loss in AIGs, posing significant challenges to accurate circuit modeling. To address these issues, we propose FuncGNN, which integrates hybrid feature aggregation to extract multi-granularity topological patterns, thereby mitigating structural heterogeneity and enhancing logic circuit representations. FuncGNN further introduces gate-aware normalization that adapts to circuit-specific gate distributions, improving robustness to structural heterogeneity. Finally, FuncGNN employs multi-layer integration to merge intermediate features across layers, effectively synthesizing local and global semantic information for comprehensive logic representations. Experimental results on two logic-level analysis tasks (i.e., signal probability prediction and truth-table distance prediction) demonstrate that FuncGNN outperforms existing state-of-the-art methods, achieving improvements of 2.06% and 18.71%, respectively, while reducing training time by approximately 50.6% and GPU memory usage by about 32.8%.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Optimal Transport Necessary for Inverse Reinforcement Learning?</title>
<link>https://arxiv.org/abs/2506.06793</link>
<guid>https://arxiv.org/abs/2506.06793</guid>
<content:encoded><![CDATA[
arXiv:2506.06793v1 Announce Type: new 
Abstract: Inverse Reinforcement Learning (IRL) aims to recover a reward function from expert demonstrations. Recently, Optimal Transport (OT) methods have been successfully deployed to align trajectories and infer rewards. While OT-based methods have shown strong empirical results, they introduce algorithmic complexity, hyperparameter sensitivity, and require solving the OT optimization problems. In this work, we challenge the necessity of OT in IRL by proposing two simple, heuristic alternatives: (1) Minimum-Distance Reward, which assigns rewards based on the nearest expert state regardless of temporal order; and (2) Segment-Matching Reward, which incorporates lightweight temporal alignment by matching agent states to corresponding segments in the expert trajectory. These methods avoid optimization, exhibit linear-time complexity, and are easy to implement. Through extensive evaluations across 32 online and offline benchmarks with three reinforcement learning algorithms, we show that our simple rewards match or outperform recent OT-based approaches. Our findings suggest that the core benefits of OT may arise from basic proximity alignment rather than its optimal coupling formulation, advocating for reevaluation of complexity in future IRL design.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IMPA-HGAE:Intra-Meta-Path Augmented Heterogeneous Graph Autoencoder</title>
<link>https://arxiv.org/abs/2506.06809</link>
<guid>https://arxiv.org/abs/2506.06809</guid>
<content:encoded><![CDATA[
arXiv:2506.06809v1 Announce Type: new 
Abstract: Self-supervised learning (SSL) methods have been increasingly applied to diverse downstream tasks due to their superior generalization capabilities and low annotation costs. However, most existing heterogeneous graph SSL models convert heterogeneous graphs into homogeneous ones via meta-paths for training, which only leverage information from nodes at both ends of meta-paths while underutilizing the heterogeneous node information along the meta-paths. To address this limitation, this paper proposes a novel framework named IMPA-HGAE to enhance target node embeddings by fully exploiting internal node information along meta-paths. Experimental results validate that IMPA-HGAE achieves superior performance on heterogeneous datasets. Furthermore, this paper introduce innovative masking strategies to strengthen the representational capacity of generative SSL models on heterogeneous graph data. Additionally, this paper discuss the interpretability of the proposed method and potential future directions for generative self-supervised learning in heterogeneous graphs. This work provides insights into leveraging meta-path-guided structural semantics for robust representation learning in complex graph scenarios.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Path Integral Optimiser: Global Optimisation via Neural Schr\"odinger-F\"ollmer Diffusion</title>
<link>https://arxiv.org/abs/2506.06815</link>
<guid>https://arxiv.org/abs/2506.06815</guid>
<content:encoded><![CDATA[
arXiv:2506.06815v1 Announce Type: new 
Abstract: We present an early investigation into the use of neural diffusion processes for global optimisation, focusing on Zhang et al.'s Path Integral Sampler. One can use the Boltzmann distribution to formulate optimization as solving a Schr\"odinger bridge sampling problem, then apply Girsanov's theorem with a simple (single-point) prior to frame it in stochastic control terms, and compute the solution's integral terms via a neural approximation (a Fourier MLP). We provide theoretical bounds for this optimiser, results on toy optimisation tasks, and a summary of the stochastic theory motivating the model. Ultimately, we found the optimiser to display promising per-step performance at optimisation tasks between 2 and 1,247 dimensions, but struggle to explore higher-dimensional spaces when faced with a 15.9k parameter model, indicating a need for work on adaptation in such environments.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Curvature Enhanced Data Augmentation for Regression</title>
<link>https://arxiv.org/abs/2506.06853</link>
<guid>https://arxiv.org/abs/2506.06853</guid>
<content:encoded><![CDATA[
arXiv:2506.06853v1 Announce Type: new 
Abstract: Deep learning models with a large number of parameters, often referred to as over-parameterized models, have achieved exceptional performance across various tasks. Despite concerns about overfitting, these models frequently generalize well to unseen data, thanks to effective regularization techniques, with data augmentation being among the most widely used. While data augmentation has shown great success in classification tasks using label-preserving transformations, its application in regression problems has received less attention. Recently, a novel \emph{manifold learning} approach for generating synthetic data was proposed, utilizing a first-order approximation of the data manifold. Building on this foundation, we present a theoretical framework and practical tools for approximating and sampling general data manifolds. Furthermore, we introduce the Curvature-Enhanced Manifold Sampling (CEMS) method for regression tasks. CEMS leverages a second-order representation of the data manifold to enable efficient sampling and reconstruction of new data points. Extensive evaluations across multiple datasets and comparisons with state-of-the-art methods demonstrate that CEMS delivers superior performance in both in-distribution and out-of-distribution scenarios, while introducing only minimal computational overhead. Code is available at https://github.com/azencot-group/CEMS.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High-Fidelity Scientific Simulation Surrogates via Adaptive Implicit Neural Representations</title>
<link>https://arxiv.org/abs/2506.06858</link>
<guid>https://arxiv.org/abs/2506.06858</guid>
<content:encoded><![CDATA[
arXiv:2506.06858v1 Announce Type: new 
Abstract: Effective surrogate models are critical for accelerating scientific simulations. Implicit neural representations (INRs) offer a compact and continuous framework for modeling spatially structured data, but they often struggle with complex scientific fields exhibiting localized, high-frequency variations. Recent approaches address this by introducing additional features along rigid geometric structures (e.g., grids), but at the cost of flexibility and increased model size. In this paper, we propose a simple yet effective alternative: Feature-Adaptive INR (FA-INR). FA-INR leverages cross-attention to an augmented memory bank to learn flexible feature representations, enabling adaptive allocation of model capacity based on data characteristics, rather than rigid structural assumptions. To further improve scalability, we introduce a coordinate-guided mixture of experts (MoE) that enhances the specialization and efficiency of feature representations. Experiments on three large-scale ensemble simulation datasets show that FA-INR achieves state-of-the-art fidelity while significantly reducing model size, establishing a new trade-off frontier between accuracy and compactness for INR-based surrogates.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentially Private Sparse Linear Regression with Heavy-tailed Responses</title>
<link>https://arxiv.org/abs/2506.06861</link>
<guid>https://arxiv.org/abs/2506.06861</guid>
<content:encoded><![CDATA[
arXiv:2506.06861v1 Announce Type: new 
Abstract: As a fundamental problem in machine learning and differential privacy (DP), DP linear regression has been extensively studied. However, most existing methods focus primarily on either regular data distributions or low-dimensional cases with irregular data. To address these limitations, this paper provides a comprehensive study of DP sparse linear regression with heavy-tailed responses in high-dimensional settings. In the first part, we introduce the DP-IHT-H method, which leverages the Huber loss and private iterative hard thresholding to achieve an estimation error bound of \(
  \tilde{O}\biggl(
  s^{* \frac{1 }{2}}
  \cdot \biggl(\frac{\log d}{n}\biggr)^{\frac{\zeta}{1 + \zeta}}
  +
  s^{* \frac{1 + 2\zeta}{2 + 2\zeta}}
  \cdot \biggl(\frac{\log^2 d}{n \varepsilon}\biggr)^{\frac{\zeta}{1 + \zeta}}
  \biggr) \) under the $(\varepsilon, \delta)$-DP model, where $n$ is the sample size, $d$ is the dimensionality, $s^*$ is the sparsity of the parameter, and $\zeta \in (0, 1]$ characterizes the tail heaviness of the data. In the second part, we propose DP-IHT-L, which further improves the error bound under additional assumptions on the response and achieves \(
  \tilde{O}\Bigl(\frac{(s^*)^{3/2} \log d}{n \varepsilon}\Bigr). \) Compared to the first result, this bound is independent of the tail parameter $\zeta$. Finally, through experiments on synthetic and real-world datasets, we demonstrate that our methods outperform standard DP algorithms designed for ``regular'' data.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAFE: Finding Sparse and Flat Minima to Improve Pruning</title>
<link>https://arxiv.org/abs/2506.06866</link>
<guid>https://arxiv.org/abs/2506.06866</guid>
<content:encoded><![CDATA[
arXiv:2506.06866v1 Announce Type: new 
Abstract: Sparsifying neural networks often suffers from seemingly inevitable performance degradation, and it remains challenging to restore the original performance despite much recent progress. Motivated by recent studies in robust optimization, we aim to tackle this problem by finding subnetworks that are both sparse and flat at the same time. Specifically, we formulate pruning as a sparsity-constrained optimization problem where flatness is encouraged as an objective. We solve it explicitly via an augmented Lagrange dual approach and extend it further by proposing a generalized projection operation, resulting in novel pruning methods called SAFE and its extension, SAFE$^+$. Extensive evaluations on standard image classification and language modeling tasks reveal that SAFE consistently yields sparse networks with improved generalization performance, which compares competitively to well-established baselines. In addition, SAFE demonstrates resilience to noisy data, making it well-suited for real-world conditions.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Log-Sum-Exponential Estimator for Off-Policy Evaluation and Learning</title>
<link>https://arxiv.org/abs/2506.06873</link>
<guid>https://arxiv.org/abs/2506.06873</guid>
<content:encoded><![CDATA[
arXiv:2506.06873v1 Announce Type: new 
Abstract: Off-policy learning and evaluation leverage logged bandit feedback datasets, which contain context, action, propensity score, and feedback for each data point. These scenarios face significant challenges due to high variance and poor performance with low-quality propensity scores and heavy-tailed reward distributions. We address these issues by introducing a novel estimator based on the log-sum-exponential (LSE) operator, which outperforms traditional inverse propensity score estimators. Our LSE estimator demonstrates variance reduction and robustness under heavy-tailed conditions. For off-policy evaluation, we derive upper bounds on the estimator's bias and variance. In the off-policy learning scenario, we establish bounds on the regret -- the performance gap between our LSE estimator and the optimal policy -- assuming bounded $(1+\epsilon)$-th moment of weighted reward. Notably, we achieve a convergence rate of $O(n^{-\epsilon/(1+ \epsilon)})$ for the regret bounds, where $\epsilon \in [0,1]$ and $n$ is the size of logged bandit feedback dataset. Theoretical analysis is complemented by comprehensive empirical evaluations in both off-policy learning and evaluation scenarios, confirming the practical advantages of our approach. The code for our estimator is available at the following link: https://github.com/armin-behnamnia/lse-offpolicy-learning.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FREE: Fast and Robust Vision Language Models with Early Exits</title>
<link>https://arxiv.org/abs/2506.06884</link>
<guid>https://arxiv.org/abs/2506.06884</guid>
<content:encoded><![CDATA[
arXiv:2506.06884v1 Announce Type: new 
Abstract: In recent years, Vision-Language Models (VLMs) have shown remarkable performance improvements in Vision-Language tasks. However, their large size poses challenges for real-world applications where inference latency is a concern. To tackle this issue, we propose employing Early Exit (EE) strategies in VLMs. However, training exit classifiers in VLMs is challenging, particularly with limited labeled training data. To address this, we introduce FREE, an adversarial training approach within a GAN-based framework. Here, each exit consists of a transformer layer and a classifier. The transformer layer is adversarially trained to produce feature representations similar to the final layer, while a feature classifier serves as the discriminator. Our method focuses on performing input-adaptive inference that increases inference speed with minimal drop in performance. Experimental results demonstrate the effectiveness of our approach in enhancing accuracy and model robustness by mitigating overthinking and the phenomenon of mid-crisis that we highlight. We experimentally validate that our method speeds up the inference process by more than 1.51x while retaining comparable performance. The source code is available at https://github.com/Div290/FREE.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can In-Context Reinforcement Learning Recover From Reward Poisoning Attacks?</title>
<link>https://arxiv.org/abs/2506.06891</link>
<guid>https://arxiv.org/abs/2506.06891</guid>
<content:encoded><![CDATA[
arXiv:2506.06891v1 Announce Type: new 
Abstract: We study the corruption-robustness of in-context reinforcement learning (ICRL), focusing on the Decision-Pretrained Transformer (DPT, Lee et al., 2023). To address the challenge of reward poisoning attacks targeting the DPT, we propose a novel adversarial training framework, called Adversarially Trained Decision-Pretrained Transformer (AT-DPT). Our method simultaneously trains an attacker to minimize the true reward of the DPT by poisoning environment rewards, and a DPT model to infer optimal actions from the poisoned data. We evaluate the effectiveness of our approach against standard bandit algorithms, including robust baselines designed to handle reward contamination. Our results show that the proposed method significantly outperforms these baselines in bandit settings, under a learned attacker. We additionally evaluate AT-DPT on an adaptive attacker, and observe similar results. Furthermore, we extend our evaluation to the MDP setting, confirming that the robustness observed in bandit scenarios generalizes to more complex environments.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Gaussian Processes with Latent Kronecker Structure</title>
<link>https://arxiv.org/abs/2506.06895</link>
<guid>https://arxiv.org/abs/2506.06895</guid>
<content:encoded><![CDATA[
arXiv:2506.06895v1 Announce Type: new 
Abstract: Applying Gaussian processes (GPs) to very large datasets remains a challenge due to limited computational scalability. Matrix structures, such as the Kronecker product, can accelerate operations significantly, but their application commonly entails approximations or unrealistic assumptions. In particular, the most common path to creating a Kronecker-structured kernel matrix is by evaluating a product kernel on gridded inputs that can be expressed as a Cartesian product. However, this structure is lost if any observation is missing, breaking the Cartesian product structure, which frequently occurs in real-world data such as time series. To address this limitation, we propose leveraging latent Kronecker structure, by expressing the kernel matrix of observed values as the projection of a latent Kronecker product. In combination with iterative linear system solvers and pathwise conditioning, our method facilitates inference of exact GPs while requiring substantially fewer computational resources than standard iterative methods. We demonstrate that our method outperforms state-of-the-art sparse and variational GPs on real-world datasets with up to five million examples, including robotics, automated machine learning, and climate applications.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty Estimation on Graphs with Structure Informed Stochastic Partial Differential Equations</title>
<link>https://arxiv.org/abs/2506.06907</link>
<guid>https://arxiv.org/abs/2506.06907</guid>
<content:encoded><![CDATA[
arXiv:2506.06907v1 Announce Type: new 
Abstract: Graph Neural Networks have achieved impressive results across diverse network modeling tasks, but accurately estimating uncertainty on graphs remains difficult, especially under distributional shifts. Unlike traditional uncertainty estimation, graph-based uncertainty must account for randomness arising from both the graph's structure and its label distribution, which adds complexity. In this paper, making an analogy between the evolution of a stochastic partial differential equation (SPDE) driven by Matern Gaussian Process and message passing using GNN layers, we present a principled way to design a novel message passing scheme that incorporates spatial-temporal noises motivated by the Gaussian Process approach to SPDE. Our method simultaneously captures uncertainty across space and time and allows explicit control over the covariance kernel smoothness, thereby enhancing uncertainty estimates on graphs with both low and high label informativeness. Our extensive experiments on Out-of-Distribution (OOD) detection on graph datasets with varying label informativeness demonstrate the soundness and superiority of our model to existing approaches.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-Based Physics-Guided Urban PM2.5 Air Quality Imputation with Constrained Monitoring Data</title>
<link>https://arxiv.org/abs/2506.06917</link>
<guid>https://arxiv.org/abs/2506.06917</guid>
<content:encoded><![CDATA[
arXiv:2506.06917v1 Announce Type: new 
Abstract: This work introduces GraPhy, a graph-based, physics-guided learning framework for high-resolution and accurate air quality modeling in urban areas with limited monitoring data. Fine-grained air quality monitoring information is essential for reducing public exposure to pollutants. However, monitoring networks are often sparse in socioeconomically disadvantaged regions, limiting the accuracy and resolution of air quality modeling. To address this, we propose a physics-guided graph neural network architecture called GraPhy with layers and edge features designed specifically for low-resolution monitoring data. Experiments using data from California's socioeconomically disadvantaged San Joaquin Valley show that GraPhy achieves the overall best performance evaluated by mean squared error (MSE), mean absolute error (MAE), and R-square value (R2), improving the performance by 9%-56% compared to various baseline models. Moreover, GraPhy consistently outperforms baselines across different spatial heterogeneity levels, demonstrating the effectiveness of our model design.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Basis Transformers for Multi-Task Tabular Regression</title>
<link>https://arxiv.org/abs/2506.06926</link>
<guid>https://arxiv.org/abs/2506.06926</guid>
<content:encoded><![CDATA[
arXiv:2506.06926v1 Announce Type: new 
Abstract: Dealing with tabular data is challenging due to partial information, noise, and heterogeneous structure. Existing techniques often struggle to simultaneously address key aspects of tabular data such as textual information, a variable number of columns, and unseen data without metadata besides column names. We propose a novel architecture, \textit{basis transformers}, specifically designed to tackle these challenges while respecting inherent invariances in tabular data, including hierarchical structure and the representation of numeric values. We evaluate our design on a multi-task tabular regression benchmark, achieving an improvement of 0.338 in the median $R^2$ score and the lowest standard deviation across 34 tasks from the OpenML-CTR23 benchmark. Furthermore, our model has five times fewer parameters than the best-performing baseline and surpasses pretrained large language model baselines -- even when initialized from randomized weights.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rewriting the Budget: A General Framework for Black-Box Attacks Under Cost Asymmetry</title>
<link>https://arxiv.org/abs/2506.06933</link>
<guid>https://arxiv.org/abs/2506.06933</guid>
<content:encoded><![CDATA[
arXiv:2506.06933v1 Announce Type: new 
Abstract: Traditional decision-based black-box adversarial attacks on image classifiers aim to generate adversarial examples by slightly modifying input images while keeping the number of queries low, where each query involves sending an input to the model and observing its output. Most existing methods assume that all queries have equal cost. However, in practice, queries may incur asymmetric costs; for example, in content moderation systems, certain output classes may trigger additional review, enforcement, or penalties, making them more costly than others. While prior work has considered such asymmetric cost settings, effective algorithms for this scenario remain underdeveloped. In this paper, we propose a general framework for decision-based attacks under asymmetric query costs, which we refer to as asymmetric black-box attacks. We modify two core components of existing attacks: the search strategy and the gradient estimation process. Specifically, we propose Asymmetric Search (AS), a more conservative variant of binary search that reduces reliance on high-cost queries, and Asymmetric Gradient Estimation (AGREST), which shifts the sampling distribution to favor low-cost queries. We design efficient algorithms that minimize total attack cost by balancing different query types, in contrast to earlier methods such as stealthy attacks that focus only on limiting expensive (high-cost) queries. Our method can be integrated into a range of existing black-box attacks with minimal changes. We perform both theoretical analysis and empirical evaluation on standard image classification benchmarks. Across various cost regimes, our method consistently achieves lower total query cost and smaller perturbations than existing approaches, with improvements of up to 40% in some settings.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Sharpness Dynamics in NN Training with a Minimalist Example: The Effects of Dataset Difficulty, Depth, Stochasticity, and More</title>
<link>https://arxiv.org/abs/2506.06940</link>
<guid>https://arxiv.org/abs/2506.06940</guid>
<content:encoded><![CDATA[
arXiv:2506.06940v1 Announce Type: new 
Abstract: When training deep neural networks with gradient descent, sharpness often increases -- a phenomenon known as progressive sharpening -- before saturating at the edge of stability. Although commonly observed in practice, the underlying mechanisms behind progressive sharpening remain poorly understood. In this work, we study this phenomenon using a minimalist model: a deep linear network with a single neuron per layer. We show that this simple model effectively captures the sharpness dynamics observed in recent empirical studies, offering a simple testbed to better understand neural network training. Moreover, we theoretically analyze how dataset properties, network depth, stochasticity of optimizers, and step size affect the degree of progressive sharpening in the minimalist model. We then empirically demonstrate how these theoretical insights extend to practical scenarios. This study offers a deeper understanding of sharpness dynamics in neural network training, highlighting the interplay between depth, training data, and optimizers.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safety-Aware Reinforcement Learning for Control via Risk-Sensitive Action-Value Iteration and Quantile Regression</title>
<link>https://arxiv.org/abs/2506.06954</link>
<guid>https://arxiv.org/abs/2506.06954</guid>
<content:encoded><![CDATA[
arXiv:2506.06954v1 Announce Type: new 
Abstract: Mainstream approximate action-value iteration reinforcement learning (RL) algorithms suffer from overestimation bias, leading to suboptimal policies in high-variance stochastic environments. Quantile-based action-value iteration methods reduce this bias by learning a distribution of the expected cost-to-go using quantile regression. However, ensuring that the learned policy satisfies safety constraints remains a challenge when these constraints are not explicitly integrated into the RL framework. Existing methods often require complex neural architectures or manual tradeoffs due to combined cost functions. To address this, we propose a risk-regularized quantile-based algorithm integrating Conditional Value-at-Risk (CVaR) to enforce safety without complex architectures. We also provide theoretical guarantees on the contraction properties of the risk-sensitive distributional Bellman operator in Wasserstein space, ensuring convergence to a unique cost distribution. Simulations of a mobile robot in a dynamic reach-avoid task show that our approach leads to more goal successes, fewer collisions, and better safety-performance trade-offs compared to risk-neutral methods.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UdonCare: Hierarchy Pruning for Unseen Domain Discovery in Predictive Healthcare</title>
<link>https://arxiv.org/abs/2506.06977</link>
<guid>https://arxiv.org/abs/2506.06977</guid>
<content:encoded><![CDATA[
arXiv:2506.06977v1 Announce Type: new 
Abstract: Domain generalization has become a critical challenge in clinical prediction, where patient cohorts often exhibit shifting data distributions that degrade model performance. Typical domain generalization approaches struggle in real-world healthcare settings for two main reasons: (1) patient-specific domain labels are typically unavailable, making domain discovery especially difficult; (2) purely data-driven approaches overlook key clinical insights, leading to a gap in medical knowledge integration. To address these problems, we leverage hierarchical medical ontologies like the ICD-9-CM hierarchy to group diseases into higher-level categories and discover more flexible latent domains. In this paper, we introduce UdonCare, a hierarchy-guided framework that iteratively prunes fine-grained domains, encodes these refined domains, and applies a Siamese-type inference mechanism to separate domain-related signals from patient-level features. Experimental results on clinical datasets (MIMIC-III and MIMIC-IV) show that the proposed model achieves higher performance compared to other domain generalization baselines when substantial domain gaps presents, highlighting the untapped potential of medical knowledge for enhancing domain generalization in practical healthcare applications.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Near Optimal Non-asymptotic Sample Complexity of 1-Identification</title>
<link>https://arxiv.org/abs/2506.06978</link>
<guid>https://arxiv.org/abs/2506.06978</guid>
<content:encoded><![CDATA[
arXiv:2506.06978v1 Announce Type: new 
Abstract: Motivated by an open direction in existing literature, we study the 1-identification problem, a fundamental multi-armed bandit formulation on pure exploration. The goal is to determine whether there exists an arm whose mean reward is at least a known threshold $\mu_0$, or to output None if it believes such an arm does not exist. The agent needs to guarantee its output is correct with probability at least $1-\delta$. Degenne & Koolen 2019 has established the asymptotically tight sample complexity for the 1-identification problem, but they commented that the non-asymptotic analysis remains unclear. We design a new algorithm Sequential-Exploration-Exploitation (SEE), and conduct theoretical analysis from the non-asymptotic perspective. Novel to the literature, we achieve near optimality, in the sense of matching upper and lower bounds on the pulling complexity. The gap between the upper and lower bounds is up to a polynomial logarithmic factor. The numerical result also indicates the effectiveness of our algorithm, compared to existing benchmarks.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoXGATE: Modality-aware cross-attention for multi-omic gastrointestinal cancer sub-type classification</title>
<link>https://arxiv.org/abs/2506.06980</link>
<guid>https://arxiv.org/abs/2506.06980</guid>
<content:encoded><![CDATA[
arXiv:2506.06980v1 Announce Type: new 
Abstract: Cancer subtype classification is crucial for personalized treatment and prognostic assessment. However, effectively integrating multi-omic data remains challenging due to the heterogeneous nature of genomic, epigenomic, and transcriptomic features. In this work, we propose Modality-Aware Cross-Attention MoXGATE, a novel deep-learning framework that leverages cross-attention and learnable modality weights to enhance feature fusion across multiple omics sources. Our approach effectively captures inter-modality dependencies, ensuring robust and interpretable integration. Through experiments on Gastrointestinal Adenocarcinoma (GIAC) and Breast Cancer (BRCA) datasets from TCGA, we demonstrate that MoXGATE outperforms existing methods, achieving 95\% classification accuracy. Ablation studies validate the effectiveness of cross-attention over simple concatenation and highlight the importance of different omics modalities. Moreover, our model generalizes well to unseen cancer types e.g., breast cancer, underscoring its adaptability. Key contributions include (1) a cross-attention-based multi-omic integration framework, (2) modality-weighted fusion for enhanced interpretability, (3) application of focal loss to mitigate data imbalance, and (4) validation across multiple cancer subtypes. Our results indicate that MoXGATE is a promising approach for multi-omic cancer subtype classification, offering improved performance and biological generalizability.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Certified Unlearning for Neural Networks</title>
<link>https://arxiv.org/abs/2506.06985</link>
<guid>https://arxiv.org/abs/2506.06985</guid>
<content:encoded><![CDATA[
arXiv:2506.06985v1 Announce Type: new 
Abstract: We address the problem of machine unlearning, where the goal is to remove the influence of specific training data from a model upon request, motivated by privacy concerns and regulatory requirements such as the "right to be forgotten." Unfortunately, existing methods rely on restrictive assumptions or lack formal guarantees. To this end, we propose a novel method for certified machine unlearning, leveraging the connection between unlearning and privacy amplification by stochastic post-processing. Our method uses noisy fine-tuning on the retain data, i.e., data that does not need to be removed, to ensure provable unlearning guarantees. This approach requires no assumptions about the underlying loss function, making it broadly applicable across diverse settings. We analyze the theoretical trade-offs in efficiency and accuracy and demonstrate empirically that our method not only achieves formal unlearning guarantees but also performs effectively in practice, outperforming existing baselines. Our code is available at https://github.com/stair-lab/certified-unlearningneural-networks-icml-2025
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fully Explainable Classification Models Using Hyperblocks</title>
<link>https://arxiv.org/abs/2506.06986</link>
<guid>https://arxiv.org/abs/2506.06986</guid>
<content:encoded><![CDATA[
arXiv:2506.06986v1 Announce Type: new 
Abstract: Building on existing work with Hyperblocks, which classify data using minimum and maximum bounds for each attribute, we focus on enhancing interpretability, decreasing training time, and reducing model complexity without sacrificing accuracy. This system allows subject matter experts (SMEs) to directly inspect and understand the model's decision logic without requiring extensive machine learning expertise. To reduce Hyperblock complexity while retaining performance, we introduce a suite of algorithms for Hyperblock simplification. These include removing redundant attributes, removing redundant blocks through overlap analysis, and creating disjunctive units. These methods eliminate unnecessary parameters, dramatically reducing model size without harming classification power. We increase robustness by introducing an interpretable fallback mechanism using k-Nearest Neighbor (k-NN) classifiers for points not covered by any block, ensuring complete data coverage while preserving model transparency. Our results demonstrate that interpretable models can scale to high-dimensional, large-volume datasets while maintaining competitive accuracy. On benchmark datasets such as WBC (9-D), we achieve strong predictive performance with significantly reduced complexity. On MNIST (784-D), our method continues to improve through tuning and simplification, showing promise as a transparent alternative to black-box models in domains where trust, clarity, and control are crucial.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modified K-means Algorithm with Local Optimality Guarantees</title>
<link>https://arxiv.org/abs/2506.06990</link>
<guid>https://arxiv.org/abs/2506.06990</guid>
<content:encoded><![CDATA[
arXiv:2506.06990v1 Announce Type: new 
Abstract: The K-means algorithm is one of the most widely studied clustering algorithms in machine learning. While extensive research has focused on its ability to achieve a globally optimal solution, there still lacks a rigorous analysis of its local optimality guarantees. In this paper, we first present conditions under which the K-means algorithm converges to a locally optimal solution. Based on this, we propose simple modifications to the K-means algorithm which ensure local optimality in both the continuous and discrete sense, with the same computational complexity as the original K-means algorithm. As the dissimilarity measure, we consider a general Bregman divergence, which is an extension of the squared Euclidean distance often used in the K-means algorithm. Numerical experiments confirm that the K-means algorithm does not always find a locally optimal solution in practice, while our proposed methods provide improved locally optimal solutions with reduced clustering loss. Our code is available at https://github.com/lmingyi/LO-K-means.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Physics-informed Diffusion for Anomaly Detection in Trajectories</title>
<link>https://arxiv.org/abs/2506.06999</link>
<guid>https://arxiv.org/abs/2506.06999</guid>
<content:encoded><![CDATA[
arXiv:2506.06999v1 Announce Type: new 
Abstract: Given trajectory data, a domain-specific study area, and a user-defined threshold, we aim to find anomalous trajectories indicative of possible GPS spoofing (e.g., fake trajectory). The problem is societally important to curb illegal activities in international waters, such as unauthorized fishing and illicit oil transfers. The problem is challenging due to advances in AI generated in deep fakes generation (e.g., additive noise, fake trajectories) and lack of adequate amount of labeled samples for ground-truth verification. Recent literature shows promising results for anomalous trajectory detection using generative models despite data sparsity. However, they do not consider fine-scale spatiotemporal dependencies and prior physical knowledge, resulting in higher false-positive rates. To address these limitations, we propose a physics-informed diffusion model that integrates kinematic constraints to identify trajectories that do not adhere to physical laws. Experimental results on real-world datasets in the maritime and urban domains show that the proposed framework results in higher prediction accuracy and lower estimation error rate for anomaly detection and trajectory generation methods, respectively. Our implementation is available at https://github.com/arunshar/Physics-Informed-Diffusion-Probabilistic-Model.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>End-to-End Probabilistic Framework for Learning with Hard Constraints</title>
<link>https://arxiv.org/abs/2506.07003</link>
<guid>https://arxiv.org/abs/2506.07003</guid>
<content:encoded><![CDATA[
arXiv:2506.07003v1 Announce Type: new 
Abstract: We present a general purpose probabilistic forecasting framework, ProbHardE2E, to learn systems that can incorporate operational/physical constraints as hard requirements. ProbHardE2E enforces hard constraints by exploiting variance information in a novel way; and thus it is also capable of performing uncertainty quantification (UQ) on the model. Our methodology uses a novel differentiable probabilistic projection layer (DPPL) that can be combined with a wide range of neural network architectures. This DPPL allows the model to learn the system in an end-to-end manner, compared to other approaches where the constraints are satisfied either through a post-processing step or at inference. In addition, ProbHardE2E can optimize a strictly proper scoring rule, without making any distributional assumptions on the target, which enables it to obtain robust distributional estimates (in contrast to existing approaches that generally optimize likelihood-based objectives, which are heavily biased by their distributional assumptions and model choices); and it can incorporate a range of non-linear constraints (increasing the power of modeling and flexibility). We apply ProbHardE2E to problems in learning partial differential equations with uncertainty estimates and to probabilistic time-series forecasting, showcasing it as a broadly applicable general setup that connects these seemingly disparate domains.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparison of Lightweight Methods for Vehicle Dynamics-Based Driver Drowsiness Detection</title>
<link>https://arxiv.org/abs/2506.07014</link>
<guid>https://arxiv.org/abs/2506.07014</guid>
<content:encoded><![CDATA[
arXiv:2506.07014v1 Announce Type: new 
Abstract: Driver drowsiness detection (DDD) prevents road accidents caused by driver fatigue. Vehicle dynamics-based DDD has been proposed as a method that is both economical and high performance. However, there are concerns about the reliability of performance metrics and the reproducibility of many of the existing methods. For instance, some previous studies seem to have a data leakage issue among training and test datasets, and many do not openly provide the datasets they used. To this end, this paper aims to compare the performance of representative vehicle dynamics-based DDD methods under a transparent and fair framework that uses a public dataset. We first develop a framework for extracting features from an open dataset by Aygun et al. and performing DDD with lightweight ML models; the framework is carefully designed to support a variety of onfigurations. Second, we implement three existing representative methods and a concise random forest (RF)-based method in the framework. Finally, we report the results of experiments to verify the reproducibility and clarify the performance of DDD based on common metrics. Among the evaluated methods, the RF-based method achieved the highest accuracy of 88 %. Our findings imply the issues inherent in DDD methods developed in a non-standard manner, and demonstrate a high performance method implemented appropriately.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlphaSteer: Learning Refusal Steering with Principled Null-Space Constraint</title>
<link>https://arxiv.org/abs/2506.07022</link>
<guid>https://arxiv.org/abs/2506.07022</guid>
<content:encoded><![CDATA[
arXiv:2506.07022v1 Announce Type: new 
Abstract: As LLMs are increasingly deployed in real-world applications, ensuring their ability to refuse malicious prompts, especially jailbreak attacks, is essential for safe and reliable use. Recently, activation steering has emerged as an effective approach for enhancing LLM safety by adding a refusal direction vector to internal activations of LLMs during inference, which will further induce the refusal behaviors of LLMs. However, indiscriminately applying activation steering fundamentally suffers from the trade-off between safety and utility, since the same steering vector can also lead to over-refusal and degraded performance on benign prompts. Although prior efforts, such as vector calibration and conditional steering, have attempted to mitigate this trade-off, their lack of theoretical grounding limits their robustness and effectiveness. To better address the trade-off between safety and utility, we present a theoretically grounded and empirically effective activation steering method called AlphaSteer. Specifically, it considers activation steering as a learnable process with two principled learning objectives: utility preservation and safety enhancement. For utility preservation, it learns to construct a nearly zero vector for steering benign data, with the null-space constraints. For safety enhancement, it learns to construct a refusal direction vector for steering malicious data, with the help of linear regression. Experiments across multiple jailbreak attacks and utility benchmarks demonstrate the effectiveness of AlphaSteer, which significantly improves the safety of LLMs without compromising general capabilities. Our codes are available at https://github.com/AlphaLab-USTC/AlphaSteer.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixture Experts with Test-Time Self-Supervised Aggregation for Tabular Imbalanced Regression</title>
<link>https://arxiv.org/abs/2506.07033</link>
<guid>https://arxiv.org/abs/2506.07033</guid>
<content:encoded><![CDATA[
arXiv:2506.07033v1 Announce Type: new 
Abstract: Tabular data serve as a fundamental and ubiquitous representation of structured information in numerous real-world applications, e.g., finance and urban planning. In the realm of tabular imbalanced applications, data imbalance has been investigated in classification tasks with insufficient instances in certain labels, causing the model's ineffective generalizability. However, the imbalance issue of tabular regression tasks is underexplored, and yet is critical due to unclear boundaries for continuous labels and simplifying assumptions in existing imbalance regression work, which often rely on known and balanced test distributions. Such assumptions may not hold in practice and can lead to performance degradation. To address these issues, we propose MATI: Mixture Experts with Test-Time Self-Supervised Aggregation for Tabular Imbalance Regression, featuring two key innovations: (i) the Region-Aware Mixture Expert, which adopts a Gaussian Mixture Model to capture the underlying related regions. The statistical information of each Gaussian component is then used to synthesize and train region-specific experts to capture the unique characteristics of their respective regions. (ii) Test-Time Self-Supervised Expert Aggregation, which dynamically adjusts region expert weights based on test data features to reinforce expert adaptation across varying test distributions. We evaluated MATI on four real-world tabular imbalance regression datasets, including house pricing, bike sharing, and age prediction. To reflect realistic deployment scenarios, we adopted three types of test distributions: a balanced distribution with uniform target frequencies, a normal distribution that follows the training data, and an inverse distribution that emphasizes rare target regions. On average across these three test distributions, MATI achieved a 7.1% improvement in MAE compared to existing methods.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient $Q$-Learning and Actor-Critic Methods for Robust Average Reward Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.07040</link>
<guid>https://arxiv.org/abs/2506.07040</guid>
<content:encoded><![CDATA[
arXiv:2506.07040v1 Announce Type: new 
Abstract: We present the first $Q$-learning and actor-critic algorithms for robust average reward Markov Decision Processes (MDPs) with non-asymptotic convergence under contamination, TV distance and Wasserstein distance uncertainty sets. We show that the robust $Q$ Bellman operator is a strict contractive mapping with respect to a carefully constructed semi-norm with constant functions being quotiented out. This property supports a stochastic approximation update, that learns the optimal robust $Q$ function in $\tilde{\cO}(\epsilon^{-2})$ samples. We also show that the same idea can be used for robust $Q$ function estimation, which can be further used for critic estimation. Coupling it with theories in robust policy mirror descent update, we present a natural actor-critic algorithm that attains an $\epsilon$-optimal robust policy in $\tilde{\cO}(\epsilon^{-3})$ samples. These results advance the theory of distributionally robust reinforcement learning in the average reward setting.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FairPFN: A Tabular Foundation Model for Causal Fairness</title>
<link>https://arxiv.org/abs/2506.07049</link>
<guid>https://arxiv.org/abs/2506.07049</guid>
<content:encoded><![CDATA[
arXiv:2506.07049v1 Announce Type: new 
Abstract: Machine learning (ML) systems are utilized in critical sectors, such as healthcare, law enforcement, and finance. However, these systems are often trained on historical data that contains demographic biases, leading to ML decisions that perpetuate or exacerbate existing social inequalities. Causal fairness provides a transparent, human-in-the-loop framework to mitigate algorithmic discrimination, aligning closely with legal doctrines of direct and indirect discrimination. However, current causal fairness frameworks hold a key limitation in that they assume prior knowledge of the correct causal model, restricting their applicability in complex fairness scenarios where causal models are unknown or difficult to identify. To bridge this gap, we propose FairPFN, a tabular foundation model pre-trained on synthetic causal fairness data to identify and mitigate the causal effects of protected attributes in its predictions. FairPFN's key contribution is that it requires no knowledge of the causal model and still demonstrates strong performance in identifying and removing protected causal effects across a diverse set of hand-crafted and real-world scenarios relative to robust baseline methods. FairPFN paves the way for promising future research, making causal fairness more accessible to a wider variety of complex fairness problems.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Policy Gradient with Tree Search: Avoiding Local Optimas through Lookahead</title>
<link>https://arxiv.org/abs/2506.07054</link>
<guid>https://arxiv.org/abs/2506.07054</guid>
<content:encoded><![CDATA[
arXiv:2506.07054v1 Announce Type: new 
Abstract: Classical policy gradient (PG) methods in reinforcement learning frequently converge to suboptimal local optima, a challenge exacerbated in large or complex environments. This work investigates Policy Gradient with Tree Search (PGTS), an approach that integrates an $m$-step lookahead mechanism to enhance policy optimization. We provide theoretical analysis demonstrating that increasing the tree search depth $m$-monotonically reduces the set of undesirable stationary points and, consequently, improves the worst-case performance of any resulting stationary policy. Critically, our analysis accommodates practical scenarios where policy updates are restricted to states visited by the current policy, rather than requiring updates across the entire state space. Empirical evaluations on diverse MDP structures, including Ladder, Tightrope, and Gridworld environments, illustrate PGTS's ability to exhibit "farsightedness," navigate challenging reward landscapes, escape local traps where standard PG fails, and achieve superior solutions.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>E-BATS: Efficient Backpropagation-Free Test-Time Adaptation for Speech Foundation Models</title>
<link>https://arxiv.org/abs/2506.07078</link>
<guid>https://arxiv.org/abs/2506.07078</guid>
<content:encoded><![CDATA[
arXiv:2506.07078v1 Announce Type: new 
Abstract: Speech Foundation Models encounter significant performance degradation when deployed in real-world scenarios involving acoustic domain shifts, such as background noise and speaker accents. Test-time adaptation (TTA) has recently emerged as a viable strategy to address such domain shifts at inference time without requiring access to source data or labels. However, existing TTA approaches, particularly those relying on backpropagation, are memory-intensive, limiting their applicability in speech tasks and resource-constrained settings. Although backpropagation-free methods offer improved efficiency, existing ones exhibit poor accuracy. This is because they are predominantly developed for vision tasks, which fundamentally differ from speech task formulations, noise characteristics, and model architecture, posing unique transferability challenges. In this paper, we introduce E-BATS, the first Efficient BAckpropagation-free TTA framework designed explicitly for speech foundation models. E-BATS achieves a balance between adaptation effectiveness and memory efficiency through three key components: (i) lightweight prompt adaptation for a forward-pass-based feature alignment, (ii) a multi-scale loss to capture both global (utterance-level) and local distribution shifts (token-level) and (iii) a test-time exponential moving average mechanism for stable adaptation across utterances. Experiments conducted on four noisy speech datasets spanning sixteen acoustic conditions demonstrate consistent improvements, with 4.1%-13.5% accuracy gains over backpropagation-free baselines and 2.0-6.4 times GPU memory savings compared to backpropagation-based methods. By enabling scalable and robust adaptation under acoustic variability, this work paves the way for developing more efficient adaptation approaches for practical speech processing systems in real-world environments.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>State Entropy Regularization for Robust Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.07085</link>
<guid>https://arxiv.org/abs/2506.07085</guid>
<content:encoded><![CDATA[
arXiv:2506.07085v1 Announce Type: new 
Abstract: State entropy regularization has empirically shown better exploration and sample complexity in reinforcement learning (RL). However, its theoretical guarantees have not been studied. In this paper, we show that state entropy regularization improves robustness to structured and spatially correlated perturbations. These types of variation are common in transfer learning but often overlooked by standard robust RL methods, which typically focus on small, uncorrelated changes. We provide a comprehensive characterization of these robustness properties, including formal guarantees under reward and transition uncertainty, as well as settings where the method performs poorly. Much of our analysis contrasts state entropy with the widely used policy entropy regularization, highlighting their different benefits. Finally, from a practical standpoint, we illustrate that compared with policy entropy, the robustness advantages of state entropy are more sensitive to the number of rollouts used for policy evaluation.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pointwise confidence estimation in the non-linear $\ell^2$-regularized least squares</title>
<link>https://arxiv.org/abs/2506.07088</link>
<guid>https://arxiv.org/abs/2506.07088</guid>
<content:encoded><![CDATA[
arXiv:2506.07088v1 Announce Type: new 
Abstract: We consider a high-probability non-asymptotic confidence estimation in the $\ell^2$-regularized non-linear least-squares setting with fixed design. In particular, we study confidence estimation for local minimizers of the regularized training loss. We show a pointwise confidence bound, meaning that it holds for the prediction on any given fixed test input $x$. Importantly, the proposed confidence bound scales with similarity of the test input to the training data in the implicit feature space of the predictor (for instance, becoming very large when the test input lies far outside of the training data). This desirable last feature is captured by the weighted norm involving the inverse-Hessian matrix of the objective function, which is a generalized version of its counterpart in the linear setting, $x^{\top} \text{Cov}^{-1} x$. Our generalized result can be regarded as a non-asymptotic counterpart of the classical confidence interval based on asymptotic normality of the MLE estimator. We propose an efficient method for computing the weighted norm, which only mildly exceeds the cost of a gradient computation of the loss function. Finally, we complement our analysis with empirical evidence showing that the proposed confidence bound provides better coverage/width trade-off compared to a confidence estimation by bootstrapping, which is a gold-standard method in many applications involving non-linear predictors such as neural networks.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Patient Similarity Computation for Clinical Decision Support: An Efficient Use of Data Transformation, Combining Static and Time Series Data</title>
<link>https://arxiv.org/abs/2506.07092</link>
<guid>https://arxiv.org/abs/2506.07092</guid>
<content:encoded><![CDATA[
arXiv:2506.07092v1 Announce Type: new 
Abstract: Patient similarity computation (PSC) is a fundamental problem in healthcare informatics. The aim of the patient similarity computation is to measure the similarity among patients according to their historical clinical records, which helps to improve clinical decision support. This paper presents a novel distributed patient similarity computation (DPSC) technique based on data transformation (DT) methods, utilizing an effective combination of time series and static data. Time series data are sensor-collected patients' information, including metrics like heart rate, blood pressure, Oxygen saturation, respiration, etc. The static data are mainly patient background and demographic data, including age, weight, height, gender, etc. Static data has been used for clustering the patients. Before feeding the static data to the machine learning model adaptive Weight-of-Evidence (aWOE) and Z-score data transformation (DT) methods have been performed, which improve the prediction performances. In aWOE-based patient similarity models, sensitive patient information has been processed using aWOE which preserves the data privacy of the trained models. We used the Dynamic Time Warping (DTW) approach, which is robust and very popular, for time series similarity. However, DTW is not suitable for big data due to the significant computational run-time. To overcome this problem, distributed DTW computation is used in this study. For Coronary Artery Disease, our DT based approach boosts prediction performance by as much as 11.4%, 10.20%, and 12.6% in terms of AUC, accuracy, and F-measure, respectively. In the case of Congestive Heart Failure (CHF), our proposed method achieves performance enhancement up to 15.9%, 10.5%, and 21.9% for the same measures, respectively. The proposed method reduces the computation time by as high as 40%.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Filling the Missings: Spatiotemporal Data Imputation by Conditional Diffusion</title>
<link>https://arxiv.org/abs/2506.07099</link>
<guid>https://arxiv.org/abs/2506.07099</guid>
<content:encoded><![CDATA[
arXiv:2506.07099v1 Announce Type: new 
Abstract: Missing data in spatiotemporal systems presents a significant challenge for modern applications, ranging from environmental monitoring to urban traffic management. The integrity of spatiotemporal data often deteriorates due to hardware malfunctions and software failures in real-world deployments. Current approaches based on machine learning and deep learning struggle to model the intricate interdependencies between spatial and temporal dimensions effectively and, more importantly, suffer from cumulative errors during the data imputation process, which propagate and amplify through iterations. To address these limitations, we propose CoFILL, a novel Conditional Diffusion Model for spatiotemporal data imputation. CoFILL builds on the inherent advantages of diffusion models to generate high-quality imputations without relying on potentially error-prone prior estimates. It incorporates an innovative dual-stream architecture that processes temporal and frequency domain features in parallel. By fusing these complementary features, CoFILL captures both rapid fluctuations and underlying patterns in the data, which enables more robust imputation. The extensive experiments reveal that CoFILL's noise prediction network successfully transforms random noise into meaningful values that align with the true data distribution. The results also show that CoFILL outperforms state-of-the-art methods in imputation accuracy. The source code is publicly available at https://github.com/joyHJL/CoFILL.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Universal Offline Black-Box Optimization via Learning Language Model Embeddings</title>
<link>https://arxiv.org/abs/2506.07109</link>
<guid>https://arxiv.org/abs/2506.07109</guid>
<content:encoded><![CDATA[
arXiv:2506.07109v1 Announce Type: new 
Abstract: The pursuit of universal black-box optimization (BBO) algorithms is a longstanding goal. However, unlike domains such as language or vision, where scaling structured data has driven generalization, progress in offline BBO remains hindered by the lack of unified representations for heterogeneous numerical spaces. Thus, existing offline BBO approaches are constrained to single-task and fixed-dimensional settings, failing to achieve cross-domain universal optimization. Recent advances in language models (LMs) offer a promising path forward: their embeddings capture latent relationships in a unifying way, enabling universal optimization across different data types possible. In this paper, we discuss multiple potential approaches, including an end-to-end learning framework in the form of next-token prediction, as well as prioritizing the learning of latent spaces with strong representational capabilities. To validate the effectiveness of these methods, we collect offline BBO tasks and data from open-source academic works for training. Experiments demonstrate the universality and effectiveness of our proposed methods. Our findings suggest that unifying language model priors and learning string embedding space can overcome traditional barriers in universal BBO, paving the way for general-purpose BBO algorithms. The code is provided at https://github.com/lamda-bbo/universal-offline-bbo.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quality-Diversity Red-Teaming: Automated Generation of High-Quality and Diverse Attackers for Large Language Models</title>
<link>https://arxiv.org/abs/2506.07121</link>
<guid>https://arxiv.org/abs/2506.07121</guid>
<content:encoded><![CDATA[
arXiv:2506.07121v1 Announce Type: new 
Abstract: Ensuring safety of large language models (LLMs) is important. Red teaming--a systematic approach to identifying adversarial prompts that elicit harmful responses from target LLMs--has emerged as a crucial safety evaluation method. Within this framework, the diversity of adversarial prompts is essential for comprehensive safety assessments. We find that previous approaches to red-teaming may suffer from two key limitations. First, they often pursue diversity through simplistic metrics like word frequency or sentence embedding similarity, which may not capture meaningful variation in attack strategies. Second, the common practice of training a single attacker model restricts coverage across potential attack styles and risk categories. This paper introduces Quality-Diversity Red-Teaming (QDRT), a new framework designed to address these limitations. QDRT achieves goal-driven diversity through behavior-conditioned training and implements a behavioral replay buffer in an open-ended manner. Additionally, it trains multiple specialized attackers capable of generating high-quality attacks across diverse styles and risk categories. Our empirical evaluation demonstrates that QDRT generates attacks that are both more diverse and more effective against a wide range of target LLMs, including GPT-2, Llama-3, Gemma-2, and Qwen2.5. This work advances the field of LLM safety by providing a systematic and effective approach to automated red-teaming, ultimately supporting the responsible deployment of LLMs.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reliable Critics: Monotonic Improvement and Convergence Guarantees for Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.07134</link>
<guid>https://arxiv.org/abs/2506.07134</guid>
<content:encoded><![CDATA[
arXiv:2506.07134v1 Announce Type: new 
Abstract: Despite decades of research, it remains challenging to correctly use Reinforcement Learning (RL) algorithms with function approximation. A prime example is policy iteration, whose fundamental guarantee of monotonic improvement collapses even under linear function approximation. To address this issue, we introduce Reliable Policy Iteration (RPI). It replaces the common projection or Bellman-error minimization during policy evaluation with a Bellman-based constrained optimization. We prove that not only does RPI confer textbook monotonicity on its value estimates but these estimates also lower bound the true return. Also, their limit partially satisfies the unprojected Bellman equation, emphasizing RPI's natural fit within RL. RPI is the first algorithm with such monotonicity and convergence guarantees under function approximation. For practical use, we provide a model-free variant of RPI that amounts to a novel critic. It can be readily integrated into primary model-free PI implementations such as DQN and DDPG. In classical control tasks, such RPI-enhanced variants consistently maintain their lower-bound guarantee while matching or surpassing the performance of all baseline methods.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMoPO: Adaptive Multi-objective Preference Optimization without Reward Models and Reference Models</title>
<link>https://arxiv.org/abs/2506.07165</link>
<guid>https://arxiv.org/abs/2506.07165</guid>
<content:encoded><![CDATA[
arXiv:2506.07165v1 Announce Type: new 
Abstract: Existing multi-objective preference alignment methods for large language models (LLMs) face limitations: (1) the inability to effectively balance various preference dimensions, and (2) reliance on auxiliary reward/reference models introduces computational complexity. To address these challenges, we propose Adaptive Multi-objective Preference Optimization (AMoPO), a novel framework that achieves dynamic balance across preference dimensions. By introducing the multi-objective optimization paradigm to use the dimension-aware generation metrics as implicit rewards, AMoPO aligns LLMs with diverse preferences without additional reward models or reference models. We introduce an adaptive weight assignment mechanism that models the generation space as a Gaussian distribution, allowing dynamic prioritization of preference dimensions. Empirical results demonstrate that AMoPO outperforms state-of-the-art baselines by 28.5%, and the experiments on 7B, 14B, and 32B models reveal the scaling ability of AMoPO. Moreover, additional analysis of multiple dimensions verifies its adaptability and effectiveness. These findings validate AMoPO's capability to achieve dimension-aware preference alignment, highlighting its superiority. Our codes and datasets are available at https://github.com/Javkonline/AMoPO.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Text-Attributed Graph Learning through Selective Annotation and Graph Alignment</title>
<link>https://arxiv.org/abs/2506.07168</link>
<guid>https://arxiv.org/abs/2506.07168</guid>
<content:encoded><![CDATA[
arXiv:2506.07168v1 Announce Type: new 
Abstract: In the realm of Text-attributed Graphs (TAGs), traditional graph neural networks (GNNs) often fall short due to the complex textual information associated with each node. Recent methods have improved node representations by leveraging large language models (LLMs) to enhance node text features, but these approaches typically require extensive annotations or fine-tuning across all nodes, which is both time-consuming and costly. To overcome these challenges, we introduce GAGA, an efficient framework for TAG representation learning. GAGA reduces annotation time and cost by focusing on annotating only representative nodes and edges. It constructs an annotation graph that captures the topological relationships among these annotations. Furthermore, GAGA employs a two-level alignment module to effectively integrate the annotation graph with the TAG, aligning their underlying structures. Experiments show that GAGA achieves classification accuracies on par with or surpassing state-of-the-art methods while requiring only 1% of the data to be annotated, demonstrating its high efficiency.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regularized Adaptive Graph Learning for Large-Scale Traffic Forecasting</title>
<link>https://arxiv.org/abs/2506.07179</link>
<guid>https://arxiv.org/abs/2506.07179</guid>
<content:encoded><![CDATA[
arXiv:2506.07179v1 Announce Type: new 
Abstract: Traffic prediction is a critical task in spatial-temporal forecasting with broad applications in travel planning and urban management. Adaptive graph convolution networks have emerged as mainstream solutions due to their ability to learn node embeddings in a data-driven manner and capture complex latent dependencies. However, existing adaptive graph learning methods for traffic forecasting often either ignore the regularization of node embeddings, which account for a significant proportion of model parameters, or face scalability issues from expensive graph convolution operations. To address these challenges, we propose a Regularized Adaptive Graph Learning (RAGL) model. First, we introduce a regularized adaptive graph learning framework that synergizes Stochastic Shared Embedding (SSE) and adaptive graph convolution via a residual difference mechanism, achieving both embedding regularization and noise suppression. Second, to ensure scalability on large road networks, we develop the Efficient Cosine Operator (ECO), which performs graph convolution based on the cosine similarity of regularized embeddings with linear time complexity. Extensive experiments on four large-scale real-world traffic datasets show that RAGL consistently outperforms state-of-the-art methods in terms of prediction accuracy and exhibits competitive computational efficiency.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning based on neurovectors for tabular data: a new neural network approach</title>
<link>https://arxiv.org/abs/2506.07185</link>
<guid>https://arxiv.org/abs/2506.07185</guid>
<content:encoded><![CDATA[
arXiv:2506.07185v1 Announce Type: new 
Abstract: In this paper, we present a novel learning approach based on Neurovectors, an innovative paradigm that structures information through interconnected nodes and vector relationships for tabular data processing. Unlike traditional artificial neural networks that rely on weight adjustment through backpropagation, Neurovectors encode information by structuring data in vector spaces where energy propagation, rather than traditional weight updates, drives the learning process, enabling a more adaptable and explainable learning process. Our method generates dynamic representations of knowledge through neurovectors, thereby improving both the interpretability and efficiency of the predictive model. Experimental results using datasets from well-established repositories such as the UCI machine learning repository and Kaggle are reported both for classification and regression. To evaluate its performance, we compare our approach with standard machine learning and deep learning models, showing that Neurovectors achieve competitive accuracy.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing Breast Cancer Survival Disparities by Race and Demographic Location: A Survival Analysis Approach</title>
<link>https://arxiv.org/abs/2506.07191</link>
<guid>https://arxiv.org/abs/2506.07191</guid>
<content:encoded><![CDATA[
arXiv:2506.07191v1 Announce Type: new 
Abstract: This study employs a robust analytical framework to uncover patterns in survival outcomes among breast cancer patients from diverse racial and geographical backgrounds. This research uses the SEER 2021 dataset to analyze breast cancer survival outcomes to identify and comprehend dissimilarities. Our approach integrates exploratory data analysis (EDA), through this we identify key variables that influence survival rates and employ survival analysis techniques, including the Kaplan-Meier estimator and log-rank test and the advanced modeling Cox Proportional Hazards model to determine how survival rates vary across racial groups and countries. Model validation and interpretation are undertaken to ensure the reliability of our findings, which are documented comprehensively to inform policymakers and healthcare professionals. The outcome of this paper is a detailed version of statistical analysis that not just highlights disparities in breast cancer treatment and care but also serves as a foundational tool for developing targeted interventions to address the inequalities effectively. Through this research, our aim is to contribute to the global efforts to improve breast cancer outcomes and reduce treatment disparities.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GGBall: Graph Generative Model on Poincar\'e Ball</title>
<link>https://arxiv.org/abs/2506.07198</link>
<guid>https://arxiv.org/abs/2506.07198</guid>
<content:encoded><![CDATA[
arXiv:2506.07198v1 Announce Type: new 
Abstract: Generating graphs with hierarchical structures remains a fundamental challenge due to the limitations of Euclidean geometry in capturing exponential complexity. Here we introduce \textbf{GGBall}, a novel hyperbolic framework for graph generation that integrates geometric inductive biases with modern generative paradigms. GGBall combines a Hyperbolic Vector-Quantized Autoencoder (HVQVAE) with a Riemannian flow matching prior defined via closed-form geodesics. This design enables flow-based priors to model complex latent distributions, while vector quantization helps preserve the curvature-aware structure of the hyperbolic space. We further develop a suite of hyperbolic GNN and Transformer layers that operate entirely within the manifold, ensuring stability and scalability. Empirically, our model reduces degree MMD by over 75\% on Community-Small and over 40\% on Ego-Small compared to state-of-the-art baselines, demonstrating an improved ability to preserve topological hierarchies. These results highlight the potential of hyperbolic geometry as a powerful foundation for the generative modeling of complex, structured, and hierarchical data domains. Our code is available at \href{https://github.com/AI4Science-WestlakeU/GGBall}{here}.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Multimodal Reasoning Capabilities of Multimodal Large Language Models via Visual Perception Reward</title>
<link>https://arxiv.org/abs/2506.07218</link>
<guid>https://arxiv.org/abs/2506.07218</guid>
<content:encoded><![CDATA[
arXiv:2506.07218v1 Announce Type: new 
Abstract: Enhancing the multimodal reasoning capabilities of Multimodal Large Language Models (MLLMs) is a challenging task that has attracted increasing attention in the community. Recently, several studies have applied Reinforcement Learning with Verifiable Rewards (RLVR) to the multimodal domain in order to enhance the reasoning abilities of MLLMs. However, these works largely overlook the enhancement of multimodal perception capabilities in MLLMs, which serve as a core prerequisite and foundational component of complex multimodal reasoning. Through McNemar's test, we find that existing RLVR method fails to effectively enhance the multimodal perception capabilities of MLLMs, thereby limiting their further improvement in multimodal reasoning. To address this limitation, we propose Perception-R1, which introduces a novel visual perception reward that explicitly encourages MLLMs to perceive the visual content accurately, thereby can effectively incentivizing both their multimodal perception and reasoning capabilities. Specifically, we first collect textual visual annotations from the CoT trajectories of multimodal problems, which will serve as visual references for reward assignment. During RLVR training, we employ a judging LLM to assess the consistency between the visual annotations and the responses generated by MLLM, and assign the visual perception reward based on these consistency judgments. Extensive experiments on several multimodal reasoning benchmarks demonstrate the effectiveness of our Perception-R1, which achieves state-of-the-art performance on most benchmarks using only 1,442 training data.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VARSHAP: Addressing Global Dependency Problems in Explainable AI with Variance-Based Local Feature Attribution</title>
<link>https://arxiv.org/abs/2506.07229</link>
<guid>https://arxiv.org/abs/2506.07229</guid>
<content:encoded><![CDATA[
arXiv:2506.07229v1 Announce Type: new 
Abstract: Existing feature attribution methods like SHAP often suffer from global dependence, failing to capture true local model behavior. This paper introduces VARSHAP, a novel model-agnostic local feature attribution method which uses the reduction of prediction variance as the key importance metric of features. Building upon Shapley value framework, VARSHAP satisfies the key Shapley axioms, but, unlike SHAP, is resilient to global data distribution shifts. Experiments on synthetic and real-world datasets demonstrate that VARSHAP outperforms popular methods such as KernelSHAP or LIME, both quantitatively and qualitatively.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overclocking LLM Reasoning: Monitoring and Controlling Thinking Path Lengths in LLMs</title>
<link>https://arxiv.org/abs/2506.07240</link>
<guid>https://arxiv.org/abs/2506.07240</guid>
<content:encoded><![CDATA[
arXiv:2506.07240v1 Announce Type: new 
Abstract: Recently, techniques such as explicit structured reasoning have demonstrated strong test-time scaling behavior by enforcing a separation between the model's internal "thinking" process and the final response. A key factor influencing answer quality in this setting is the length of the thinking stage. When the reasoning is too short, the model may fail to capture the complexity of the task. Conversely, when it is too long, the model may overthink, leading to unnecessary computation and degraded performance. This paper explores and exploits the underlying mechanisms by which LLMs understand and regulate the length of their reasoning during explicit thought processes. First, we show that LLMs encode their progress through the reasoning process and introduce an interactive progress bar visualization, which is then used to reveal insights on the model's planning dynamics. Second, we manipulate the internal progress encoding during inference to reduce unnecessary steps and generate a more concise and decisive chain of thoughts. Our empirical results demonstrate that this "overclocking" method mitigates overthinking, improves answer accuracy, and reduces inference latency. Our code is publicly available.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Promoting Ensemble Diversity with Interactive Bayesian Distributional Robustness for Fine-tuning Foundation Models</title>
<link>https://arxiv.org/abs/2506.07247</link>
<guid>https://arxiv.org/abs/2506.07247</guid>
<content:encoded><![CDATA[
arXiv:2506.07247v1 Announce Type: new 
Abstract: We introduce Interactive Bayesian Distributional Robustness (IBDR), a novel Bayesian inference framework that allows modeling the interactions between particles, thereby enhancing ensemble quality through increased particle diversity. IBDR is grounded in a generalized theoretical framework that connects the distributional population loss with the approximate posterior, motivating a practical dual optimization procedure that enforces distributional robustness while fostering particle diversity. We evaluate IBDR's performance against various baseline methods using the VTAB-1K benchmark and the common reasoning language task. The results consistently show that IBDR outperforms these baselines, underscoring its effectiveness in real-world applications.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Stable Whitening Optimizer for Efficient Neural Network Training</title>
<link>https://arxiv.org/abs/2506.07254</link>
<guid>https://arxiv.org/abs/2506.07254</guid>
<content:encoded><![CDATA[
arXiv:2506.07254v1 Announce Type: new 
Abstract: In this work, we take an experimentally grounded look at neural network optimization. Building on the Shampoo family of algorithms, we identify and alleviate three key issues, resulting in the proposed SPlus method. First, we find that naive Shampoo is prone to divergence when matrix-inverses are cached for long periods. We introduce an alternate bounded update combining a historical eigenbasis with instantaneous normalization, resulting in across-the-board stability and significantly lower computational requirements. Second, we adapt a shape-aware scaling to enable learning rate transfer across network width. Third, we find that high learning rates result in large parameter noise, and propose a simple iterate-averaging scheme which unblocks faster learning. To properly confirm these findings, we introduce a pointed Transformer training benchmark, considering three objectives (language modelling, image classification, and diffusion modelling) across different stages of training. On average, SPlus is able to reach the validation performance of Adam within 44% of the gradient steps and 62% of the wallclock time.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Cram\'er-von Mises Approach to Incentivizing Truthful Data Sharing</title>
<link>https://arxiv.org/abs/2506.07272</link>
<guid>https://arxiv.org/abs/2506.07272</guid>
<content:encoded><![CDATA[
arXiv:2506.07272v1 Announce Type: new 
Abstract: Modern data marketplaces and data sharing consortia increasingly rely on incentive mechanisms to encourage agents to contribute data. However, schemes that reward agents based on the quantity of submitted data are vulnerable to manipulation, as agents may submit fabricated or low-quality data to inflate their rewards. Prior work has proposed comparing each agent's data against others' to promote honesty: when others contribute genuine data, the best way to minimize discrepancy is to do the same. Yet prior implementations of this idea rely on very strong assumptions about the data distribution (e.g. Gaussian), limiting their applicability. In this work, we develop reward mechanisms based on a novel, two-sample test inspired by the Cram\'er-von Mises statistic. Our methods strictly incentivize agents to submit more genuine data, while disincentivizing data fabrication and other types of untruthful reporting. We establish that truthful reporting constitutes a (possibly approximate) Nash equilibrium in both Bayesian and prior-agnostic settings. We theoretically instantiate our method in three canonical data sharing problems and show that it relaxes key assumptions made by prior work. Empirically, we demonstrate that our mechanism incentivizes truthful data sharing via simulations and on real-world language and image data.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating the Relationship Between Physical Activity and Tailored Behavior Change Messaging: Connecting Contextual Bandit with Large Language Models</title>
<link>https://arxiv.org/abs/2506.07275</link>
<guid>https://arxiv.org/abs/2506.07275</guid>
<content:encoded><![CDATA[
arXiv:2506.07275v1 Announce Type: new 
Abstract: Machine learning approaches, such as contextual multi-armed bandit (cMAB) algorithms, offer a promising strategy to reduce sedentary behavior by delivering personalized interventions to encourage physical activity. However, cMAB algorithms typically require large participant samples to learn effectively and may overlook key psychological factors that are not explicitly encoded in the model. In this study, we propose a hybrid approach that combines cMAB for selecting intervention types with large language models (LLMs) to personalize message content. We evaluate four intervention types: behavioral self-monitoring, gain-framed, loss-framed, and social comparison, each delivered as a motivational message aimed at increasing motivation for physical activity and daily step count. Message content is further personalized using dynamic contextual factors including daily fluctuations in self-efficacy, social influence, and regulatory focus. Over a seven-day trial, participants receive daily messages assigned by one of four models: cMAB alone, LLM alone, combined cMAB with LLM personalization (cMABxLLM), or equal randomization (RCT). Outcomes include daily step count and message acceptance, assessed via ecological momentary assessments (EMAs). We apply a causal inference framework to evaluate the effects of each model. Our findings offer new insights into the complementary roles of LLM-based personalization and cMAB adaptation in promoting physical activity through personalized behavioral messaging.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tokenized Bandit for LLM Decoding and Alignment</title>
<link>https://arxiv.org/abs/2506.07276</link>
<guid>https://arxiv.org/abs/2506.07276</guid>
<content:encoded><![CDATA[
arXiv:2506.07276v1 Announce Type: new 
Abstract: We introduce the tokenized linear bandit (TLB) and multi-armed bandit (TMAB), variants of linear and stochastic multi-armed bandit problems inspired by LLM decoding and alignment. In these problems, at each round $t \in [T]$, a user submits a query (context), and the decision maker (DM) sequentially selects a token irrevocably from a token set. Once the sequence is complete, the DM observes a random utility from the user, whose expectation is presented by a sequence function mapping the chosen token sequence to a nonnegative real value that depends on the query.
  In both problems, we first show that learning is impossible without any structure on the sequence function. We introduce a natural assumption, diminishing distance with more commons (DDMC), and propose algorithms with regret $\tilde{O}(L\sqrt{T})$ and $\tilde{O}(L\sqrt{T^{2/3}})$ for TLB and TMAB, respectively. As a side product, we obtain an (almost) optimality of the greedy decoding for LLM decoding algorithm under DDMC, which justifies the unresaonable effectiveness of greedy decoding in several tasks. This also has an immediate application to decoding-time LLM alignment, when the misaligned utility can be represented as the frozen LLM's utility and a linearly realizable latent function. We finally validate our algorithm's performance empirically as well as verify our assumptions using synthetic and real-world datasets.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EviNet: Evidential Reasoning Network for Resilient Graph Learning in the Open and Noisy Environments</title>
<link>https://arxiv.org/abs/2506.07288</link>
<guid>https://arxiv.org/abs/2506.07288</guid>
<content:encoded><![CDATA[
arXiv:2506.07288v1 Announce Type: new 
Abstract: Graph learning has been crucial to many real-world tasks, but they are often studied with a closed-world assumption, with all possible labels of data known a priori. To enable effective graph learning in an open and noisy environment, it is critical to inform the model users when the model makes a wrong prediction to in-distribution data of a known class, i.e., misclassification detection or when the model encounters out-of-distribution from novel classes, i.e., out-of-distribution detection. This paper introduces Evidential Reasoning Network (EVINET), a framework that addresses these two challenges by integrating Beta embedding within a subjective logic framework. EVINET includes two key modules: Dissonance Reasoning for misclassification detection and Vacuity Reasoning for out-of-distribution detection. Extensive experiments demonstrate that EVINET outperforms state-of-the-art methods across multiple metrics in the tasks of in-distribution classification, misclassification detection, and out-of-distribution detection. EVINET demonstrates the necessity of uncertainty estimation and logical reasoning for misclassification detection and out-of-distribution detection and paves the way for open-world graph learning. Our code and data are available at https://github.com/SSSKJ/EviNET.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pre-trained Large Language Models Learn Hidden Markov Models In-context</title>
<link>https://arxiv.org/abs/2506.07298</link>
<guid>https://arxiv.org/abs/2506.07298</guid>
<content:encoded><![CDATA[
arXiv:2506.07298v1 Announce Type: new 
Abstract: Hidden Markov Models (HMMs) are foundational tools for modeling sequential data with latent Markovian structure, yet fitting them to real-world data remains computationally challenging. In this work, we show that pre-trained large language models (LLMs) can effectively model data generated by HMMs via in-context learning (ICL)$\unicode{x2013}$their ability to infer patterns from examples within a prompt. On a diverse set of synthetic HMMs, LLMs achieve predictive accuracy approaching the theoretical optimum. We uncover novel scaling trends influenced by HMM properties, and offer theoretical conjectures for these empirical observations. We also provide practical guidelines for scientists on using ICL as a diagnostic tool for complex data. On real-world animal decision-making tasks, ICL achieves competitive performance with models designed by human experts. To our knowledge, this is the first demonstration that ICL can learn and predict HMM-generated sequences$\unicode{x2013}$an advance that deepens our understanding of in-context learning in LLMs and establishes its potential as a powerful tool for uncovering hidden structure in complex scientific data.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PASS: Private Attributes Protection with Stochastic Data Substitution</title>
<link>https://arxiv.org/abs/2506.07308</link>
<guid>https://arxiv.org/abs/2506.07308</guid>
<content:encoded><![CDATA[
arXiv:2506.07308v1 Announce Type: new 
Abstract: The growing Machine Learning (ML) services require extensive collections of user data, which may inadvertently include people's private information irrelevant to the services. Various studies have been proposed to protect private attributes by removing them from the data while maintaining the utilities of the data for downstream tasks. Nevertheless, as we theoretically and empirically show in the paper, these methods reveal severe vulnerability because of a common weakness rooted in their adversarial training based strategies. To overcome this limitation, we propose a novel approach, PASS, designed to stochastically substitute the original sample with another one according to certain probabilities, which is trained with a novel loss function soundly derived from information-theoretic objective defined for utility-preserving private attributes protection. The comprehensive evaluation of PASS on various datasets of different modalities, including facial images, human activity sensory signals, and voice recording datasets, substantiates PASS's effectiveness and generalizability.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Paged Attention Meets FlexAttention: Unlocking Long-Context Efficiency in Deployed Inference</title>
<link>https://arxiv.org/abs/2506.07311</link>
<guid>https://arxiv.org/abs/2506.07311</guid>
<content:encoded><![CDATA[
arXiv:2506.07311v1 Announce Type: new 
Abstract: Large Language Models (LLMs) encounter severe memory inefficiencies during long-context inference due to conventional handling of key-value (KV) caches. In this work, we introduce a novel integration of PagedAttention with PyTorch's FlexAttention, addressing internal fragmentation and inefficiencies associated with monolithic KV cache allocations. Implemented within IBM's Foundation Model Stack (FMS), our fused attention kernel efficiently gathers scattered KV data. Our benchmarks on an NVIDIA L4 GPU (24GB) demonstrate significantly reduced inference latency, growing only linearly (~2x) with sequence length from 128 to 2048 tokens when utilizing a global KV cache, compared to exponential latency increases without caching. While peak memory usage remains largely unchanged for single-step evaluations (dominated by model weights and activations), paged attention causes minimal incremental memory usage, observable only at sequence lengths exceeding 2048 tokens due to its power-of-two cache allocations. We open-source the full implementation and discuss its implications for future long-context model deployment.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Modeling of Networked Time-Series via Transformer Architectures</title>
<link>https://arxiv.org/abs/2506.07312</link>
<guid>https://arxiv.org/abs/2506.07312</guid>
<content:encoded><![CDATA[
arXiv:2506.07312v1 Announce Type: new 
Abstract: Many security and network applications require having large datasets to train the machine learning models. Limited data access is a well-known problem in the security domain. Recent studies have shown the potential of Transformer models to enlarge the size of data by synthesizing new samples, but the synthesized samples don't improve the models over the real data. To address this issue, we design an efficient transformer-based model as a generative framework to generate time-series data, that can be used to boost the performance of existing and new ML workflows. Our new transformer model achieves the SOTA results. We style our model to be generalizable and work across different datasets, and produce high-quality samples.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DEF: Diffusion-augmented Ensemble Forecasting</title>
<link>https://arxiv.org/abs/2506.07324</link>
<guid>https://arxiv.org/abs/2506.07324</guid>
<content:encoded><![CDATA[
arXiv:2506.07324v1 Announce Type: new 
Abstract: We present DEF (\textbf{\ul{D}}iffusion-augmented \textbf{\ul{E}}nsemble \textbf{\ul{F}}orecasting), a novel approach for generating initial condition perturbations. Modern approaches to initial condition perturbations are primarily designed for numerical weather prediction (NWP) solvers, limiting their applicability in the rapidly growing field of machine learning for weather prediction. Consequently, stochastic models in this domain are often developed on a case-by-case basis. We demonstrate that a simple conditional diffusion model can (1) generate meaningful structured perturbations, (2) be applied iteratively, and (3) utilize a guidance term to intuitivey control the level of perturbation. This method enables the transformation of any deterministic neural forecasting system into a stochastic one. With our stochastic extended systems, we show that the model accumulates less error over long-term forecasts while producing meaningful forecast distributions. We validate our approach on the 5.625$^\circ$ ERA5 reanalysis dataset, which comprises atmospheric and surface variables over a discretized global grid, spanning from the 1960s to the present. On this dataset, our method demonstrates improved predictive performance along with reasonable spread estimates.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mobility-Aware Asynchronous Federated Learning with Dynamic Sparsification</title>
<link>https://arxiv.org/abs/2506.07328</link>
<guid>https://arxiv.org/abs/2506.07328</guid>
<content:encoded><![CDATA[
arXiv:2506.07328v1 Announce Type: new 
Abstract: Asynchronous Federated Learning (AFL) enables distributed model training across multiple mobile devices, allowing each device to independently update its local model without waiting for others. However, device mobility introduces intermittent connectivity, which necessitates gradient sparsification and leads to model staleness, jointly affecting AFL convergence. This paper develops a theoretical model to characterize the interplay among sparsification, model staleness and mobility-induced contact patterns, and their joint impact on AFL convergence. Based on the analysis, we propose a mobility-aware dynamic sparsification (MADS) algorithm that optimizes the sparsification degree based on contact time and model staleness. Closed-form solutions are derived, showing that under low-speed conditions, MADS increases the sparsification degree to enhance convergence, while under high-speed conditions, it reduces the sparsification degree to guarantee reliable uploads within limited contact time. Experimental results validate the theoretical findings. Compared with the state-of-the-art benchmarks, the MADS algorithm increases the image classification accuracy on the CIFAR-10 dataset by 8.76% and reduces the average displacement error in the Argoverse trajectory prediction dataset by 9.46%.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JavelinGuard: Low-Cost Transformer Architectures for LLM Security</title>
<link>https://arxiv.org/abs/2506.07330</link>
<guid>https://arxiv.org/abs/2506.07330</guid>
<content:encoded><![CDATA[
arXiv:2506.07330v1 Announce Type: new 
Abstract: We present JavelinGuard, a suite of low-cost, high-performance model architectures designed for detecting malicious intent in Large Language Model (LLM) interactions, optimized specifically for production deployment. Recent advances in transformer architectures, including compact BERT(Devlin et al. 2019) variants (e.g., ModernBERT (Warner et al. 2024)), allow us to build highly accurate classifiers with as few as approximately 400M parameters that achieve rapid inference speeds even on standard CPU hardware. We systematically explore five progressively sophisticated transformer-based architectures: Sharanga (baseline transformer classifier), Mahendra (enhanced attention-weighted pooling with deeper heads), Vaishnava and Ashwina (hybrid neural ensemble architectures), and Raudra (an advanced multi-task framework with specialized loss functions). Our models are rigorously benchmarked across nine diverse adversarial datasets, including popular sets like the NotInject series, BIPIA, Garak, ImprovedLLM, ToxicChat, WildGuard, and our newly introduced JavelinBench, specifically crafted to test generalization on challenging borderline and hard-negative cases. Additionally, we compare our architectures against leading open-source guardrail models as well as large decoder-only LLMs such as gpt-4o, demonstrating superior cost-performance trade-offs in terms of accuracy, and latency. Our findings reveal that while Raudra's multi-task design offers the most robust performance overall, each architecture presents unique trade-offs in speed, interpretability, and resource requirements, guiding practitioners in selecting the optimal balance of complexity and efficiency for real-world LLM security applications.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-KV: Breaking Sequence via Injecting Structural Biases into Large Language Models</title>
<link>https://arxiv.org/abs/2506.07334</link>
<guid>https://arxiv.org/abs/2506.07334</guid>
<content:encoded><![CDATA[
arXiv:2506.07334v1 Announce Type: new 
Abstract: Modern large language models (LLMs) are inherently auto-regressive, requiring input to be serialized into flat sequences regardless of their structural dependencies. This serialization hinders the model's ability to leverage structural inductive biases, especially in tasks such as retrieval-augmented generation (RAG) and reasoning on data with native graph structures, where inter-segment dependencies are crucial. We introduce Graph-KV with the potential to overcome this limitation. Graph-KV leverages the KV-cache of text segments as condensed representations and governs their interaction through structural inductive biases. In this framework, 'target' segments selectively attend only to the KV-caches of their designated 'source' segments, rather than all preceding segments in a serialized sequence. This approach induces a graph-structured block mask, sparsifying attention and enabling a message-passing-like step within the LLM. Furthermore, strategically allocated positional encodings for source and target segments reduce positional bias and context window consumption. We evaluate Graph-KV across three scenarios: (1) seven RAG benchmarks spanning direct inference, multi-hop reasoning, and long-document understanding; (2) Arxiv-QA, a novel academic paper QA task with full-text scientific papers structured as citation ego-graphs; and (3) paper topic classification within a citation network. By effectively reducing positional bias and harnessing structural inductive biases, Graph-KV substantially outperforms baselines, including standard costly sequential encoding, across various settings. Code and the Graph-KV data are publicly available.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SALT: A Lightweight Model Adaptation Method for Closed Split Computing Environments</title>
<link>https://arxiv.org/abs/2506.07355</link>
<guid>https://arxiv.org/abs/2506.07355</guid>
<content:encoded><![CDATA[
arXiv:2506.07355v1 Announce Type: new 
Abstract: We propose SALT (Split-Adaptive Lightweight Tuning), a lightweight model adaptation framework for Split Computing under closed constraints, where the head and tail networks are proprietary and inaccessible to users. In such closed environments, conventional adaptation methods are infeasible since they require access to model parameters or architectures. SALT addresses this challenge by introducing a compact, trainable adapter on the client side to refine latent features from the head network, enabling user-specific adaptation without modifying the original models or increasing communication overhead. We evaluate SALT on user-specific classification tasks with CIFAR-10 and CIFAR-100, demonstrating improved accuracy with lower training latency compared to fine-tuning methods. Furthermore, SALT facilitates model adaptation for robust inference over lossy networks, a common challenge in edge-cloud environments. With minimal deployment overhead, SALT offers a practical solution for personalized inference in edge AI systems under strict system constraints.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoE-GPS: Guidlines for Prediction Strategy for Dynamic Expert Duplication in MoE Load Balancing</title>
<link>https://arxiv.org/abs/2506.07366</link>
<guid>https://arxiv.org/abs/2506.07366</guid>
<content:encoded><![CDATA[
arXiv:2506.07366v1 Announce Type: new 
Abstract: In multi-GPU Mixture-of-Experts (MoE) network, experts are distributed across different GPUs, which creates load imbalance as each expert processes different number of tokens. Recent works improve MoE inference load balance by dynamically duplicating popular experts to more GPUs to process excessive tokens, which requires predicting the distribution before routing. In this paper, we discuss the tradeoff of prediction strategies, accuracies, overhead, and end-to-end system performance. We propose MoE-GPS, a framework that guides the selection of the optimal predictor design under various system configurations, by quantifying the performance impact to system-level model runtime. Specifically, we advocate for Distribution-Only Prediction, a prediction strategy that only predicts overall token distribution which significantly reduces overhead compared to the traditional Token-to-Expert Prediction. On Mixtral 8x7B MMLU dataset, MoE-GPS suggests Distribution-Only Prediction which improves end-to-end inference performance by more than 23% compared with Token-to-Expert Prediction.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Moment Alignment: Unifying Gradient and Hessian Matching for Domain Generalization</title>
<link>https://arxiv.org/abs/2506.07378</link>
<guid>https://arxiv.org/abs/2506.07378</guid>
<content:encoded><![CDATA[
arXiv:2506.07378v1 Announce Type: new 
Abstract: Domain generalization (DG) seeks to develop models that generalize well to unseen target domains, addressing the prevalent issue of distribution shifts in real-world applications. One line of research in DG focuses on aligning domain-level gradients and Hessians to enhance generalization. However, existing methods are computationally inefficient and the underlying principles of these approaches are not well understood. In this paper, we develop the theory of moment alignment for DG. Grounded in \textit{transfer measure}, a principled framework for quantifying generalizability between two domains, we first extend the definition of transfer measure to domain generalization that includes multiple source domains and establish a target error bound. Then, we prove that aligning derivatives across domains improves transfer measure both when the feature extractor induces an invariant optimal predictor across domains and when it does not. Notably, moment alignment provides a unifying understanding of Invariant Risk Minimization, gradient matching, and Hessian matching, three previously disconnected approaches to DG. We further connect feature moments and derivatives of the classifier head, and establish the duality between feature learning and classifier fitting. Building upon our theory, we introduce \textbf{C}losed-Form \textbf{M}oment \textbf{A}lignment (CMA), a novel DG algorithm that aligns domain-level gradients and Hessians in closed-form. Our method overcomes the computational inefficiencies of existing gradient and Hessian-based techniques by eliminating the need for repeated backpropagation or sampling-based Hessian estimation. We validate the efficacy of our approach through two sets of experiments: linear probing and full fine-tuning. CMA demonstrates superior performance in both settings compared to Empirical Risk Minimization and state-of-the-art algorithms.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RiemannFormer: A Framework for Attention in Curved Spaces</title>
<link>https://arxiv.org/abs/2506.07405</link>
<guid>https://arxiv.org/abs/2506.07405</guid>
<content:encoded><![CDATA[
arXiv:2506.07405v1 Announce Type: new 
Abstract: This research endeavors to offer insights into unlocking the further potential of transformer-based architectures. One of the primary motivations is to offer a geometric interpretation for the attention mechanism in transformers. In our framework, the attention mainly involves metric tensors, tangent spaces, inner product, and how they relate to each other. These quantities and structures at discrete positions are intricately interconnected via the parallel transport of tangent vectors. To make the learning process more efficient, we reduce the number of parameters through ingenious predefined configurations. Moreover, we introduce an explicit mechanism to highlight a neighborhood by attenuating the remote values, given that transformers inherently neglect local inductive bias. Experimental results demonstrate that our modules deliver significant performance improvements relative to the baseline. More evaluation experiments on visual and large language models will be launched successively.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InverseScope: Scalable Activation Inversion for Interpreting Large Language Models</title>
<link>https://arxiv.org/abs/2506.07406</link>
<guid>https://arxiv.org/abs/2506.07406</guid>
<content:encoded><![CDATA[
arXiv:2506.07406v1 Announce Type: new 
Abstract: Understanding the internal representations of large language models (LLMs) is a central challenge in interpretability research. Existing feature interpretability methods often rely on strong assumptions about the structure of representations that may not hold in practice. In this work, we introduce InverseScope, an assumption-light and scalable framework for interpreting neural activations via input inversion. Given a target activation, we define a distribution over inputs that generate similar activations and analyze this distribution to infer the encoded features. To address the inefficiency of sampling in high-dimensional spaces, we propose a novel conditional generation architecture that significantly improves sample efficiency compared to previous methods. We further introduce a quantitative evaluation protocol that tests interpretability hypotheses using feature consistency rate computed over the sampled inputs. InverseScope scales inversion-based interpretability methods to larger models and practical tasks, enabling systematic and quantitative analysis of internal representations in real-world LLMs.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anomaly Detection and Early Warning Mechanism for Intelligent Monitoring Systems in Multi-Cloud Environments Based on LLM</title>
<link>https://arxiv.org/abs/2506.07407</link>
<guid>https://arxiv.org/abs/2506.07407</guid>
<content:encoded><![CDATA[
arXiv:2506.07407v1 Announce Type: new 
Abstract: With the rapid development of multi-cloud environments, it is increasingly important to ensure the security and reliability of intelligent monitoring systems. In this paper, we propose an anomaly detection and early warning mechanism for intelligent monitoring system in multi-cloud environment based on Large-Scale Language Model (LLM). On the basis of the existing monitoring framework, the proposed model innovatively introduces a multi-level feature extraction method, which combines the natural language processing ability of LLM with traditional machine learning methods to enhance the accuracy of anomaly detection and improve the real-time response efficiency. By introducing the contextual understanding capabilities of LLMs, the model dynamically adapts to different cloud service providers and environments, so as to more effectively detect abnormal patterns and predict potential failures. Experimental results show that the proposed model is significantly better than the traditional anomaly detection system in terms of detection accuracy and latency, and significantly improves the resilience and active management ability of cloud infrastructure.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fractional-order Jacobian Matrix Differentiation and Its Application in Artificial Neural Networks</title>
<link>https://arxiv.org/abs/2506.07408</link>
<guid>https://arxiv.org/abs/2506.07408</guid>
<content:encoded><![CDATA[
arXiv:2506.07408v1 Announce Type: new 
Abstract: Fractional-order differentiation has many characteristics different from integer-order differentiation. These characteristics can be applied to the optimization algorithms of artificial neural networks to obtain better results. However, due to insufficient theoretical research, at present, there is no fractional-order matrix differentiation method that is perfectly compatible with automatic differentiation (Autograd) technology. Therefore, we propose a fractional-order matrix differentiation calculation method. This method is introduced by the definition of the integer-order Jacobian matrix. We denote it as fractional-order Jacobian matrix differentiation (${{\bf{J}}^\alpha }$). Through ${{\bf{J}}^\alpha }$, we can carry out the matrix-based fractional-order chain rule. Based on the Linear module and the fractional-order differentiation, we design the fractional-order Autograd technology to enable the use of fractional-order differentiation in hidden layers, thereby enhancing the practicality of fractional-order differentiation in deep learning. In the experiment, according to the PyTorch framework, we design fractional-order Linear (FLinear) and replace nn.Linear in the multilayer perceptron with FLinear. Through the qualitative analysis of the training set and validation set $Loss$, the quantitative analysis of the test set indicators, and the analysis of time consumption and GPU memory usage during model training, we verify the superior performance of ${{\bf{J}}^\alpha }$ and prove that it is an excellent fractional-order gradient descent method in the field of deep learning.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variational Supervised Contrastive Learning</title>
<link>https://arxiv.org/abs/2506.07413</link>
<guid>https://arxiv.org/abs/2506.07413</guid>
<content:encoded><![CDATA[
arXiv:2506.07413v1 Announce Type: new 
Abstract: Contrastive learning has proven to be highly efficient and adaptable in shaping representation spaces across diverse modalities by pulling similar samples together and pushing dissimilar ones apart. However, two key limitations persist: (1) Without explicit regulation of the embedding distribution, semantically related instances can inadvertently be pushed apart unless complementary signals guide pair selection, and (2) excessive reliance on large in-batch negatives and tailored augmentations hinders generalization. To address these limitations, we propose Variational Supervised Contrastive Learning (VarCon), which reformulates supervised contrastive learning as variational inference over latent class variables and maximizes a posterior-weighted evidence lower bound (ELBO) that replaces exhaustive pair-wise comparisons for efficient class-aware matching and grants fine-grained control over intra-class dispersion in the embedding space. Trained exclusively on image data, our experiments on CIFAR-10, CIFAR-100, ImageNet-100, and ImageNet-1K show that VarCon (1) achieves state-of-the-art performance for contrastive learning frameworks, reaching 79.36% Top-1 accuracy on ImageNet-1K and 78.29% on CIFAR-100 with a ResNet-50 encoder while converging in just 200 epochs; (2) yields substantially clearer decision boundaries and semantic organization in the embedding space, as evidenced by KNN classification, hierarchical clustering results, and transfer-learning assessments; and (3) demonstrates superior performance in few-shot learning than supervised baseline and superior robustness across various augmentation strategies.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiteVLM: A Low-Latency Vision-Language Model Inference Pipeline for Resource-Constrained Environments</title>
<link>https://arxiv.org/abs/2506.07416</link>
<guid>https://arxiv.org/abs/2506.07416</guid>
<content:encoded><![CDATA[
arXiv:2506.07416v1 Announce Type: new 
Abstract: This paper introduces an efficient Vision-Language Model (VLM) pipeline specifically optimized for deployment on embedded devices, such as those used in robotics and autonomous driving. The pipeline significantly reduces the computational overhead by jointly leveraging patch selection to filter irrelevant camera views, a token selection module to reduce input sequence length for the LLM, and speculative decoding to accelerate token generation. Evaluation on the NVIDIA DRIVE Thor platform for automonous driving application, our pipeline achieves $2.5\times$ end-to-end latency reduction without compromising task accuracy. The speed-up further increases to $3.2\times$ when applying FP8 post-training quantization. These results demonstrate our pipeline as a viable solution for enabling real-time VLM deployment in resource-constrained environments.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evidential Spectrum-Aware Contrastive Learning for OOD Detection in Dynamic Graphs</title>
<link>https://arxiv.org/abs/2506.07417</link>
<guid>https://arxiv.org/abs/2506.07417</guid>
<content:encoded><![CDATA[
arXiv:2506.07417v1 Announce Type: new 
Abstract: Recently, Out-of-distribution (OOD) detection in dynamic graphs, which aims to identify whether incoming data deviates from the distribution of the in-distribution (ID) training set, has garnered considerable attention in security-sensitive fields. Current OOD detection paradigms primarily focus on static graphs and confront two critical challenges: i) high bias and high variance caused by single-point estimation, which makes the predictions sensitive to randomness in the data; ii) score homogenization resulting from the lack of OOD training data, where the model only learns ID-specific patterns, resulting in overall low OOD scores and a narrow score gap between ID and OOD data. To tackle these issues, we first investigate OOD detection in dynamic graphs through the lens of Evidential Deep Learning (EDL). Specifically, we propose EviSEC, an innovative and effective OOD detector via Evidential Spectrum-awarE Contrastive Learning. We design an evidential neural network to redefine the output as the posterior Dirichlet distribution, explaining the randomness of inputs through the uncertainty of distribution, which is overlooked by single-point estimation. Moreover, spectrum-aware augmentation module generates OOD approximations to identify patterns with high OOD scores, thereby widening the score gap between ID and OOD data and mitigating score homogenization. Extensive experiments on real-world datasets demonstrate that EviSAC effectively detects OOD samples in dynamic graphs.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated In-Context Learning: Iterative Refinement for Improved Answer Quality</title>
<link>https://arxiv.org/abs/2506.07440</link>
<guid>https://arxiv.org/abs/2506.07440</guid>
<content:encoded><![CDATA[
arXiv:2506.07440v1 Announce Type: new 
Abstract: For question-answering (QA) tasks, in-context learning (ICL) enables language models to generate responses without modifying their parameters by leveraging examples provided in the input. However, the effectiveness of ICL heavily depends on the availability of high-quality examples, which are often scarce due to data privacy constraints, annotation costs, and distribution disparities. A natural solution is to utilize examples stored on client devices, but existing approaches either require transmitting model parameters - incurring significant communication overhead - or fail to fully exploit local datasets, limiting their effectiveness. To address these challenges, we propose Federated In-Context Learning (Fed-ICL), a general framework that enhances ICL through an iterative, collaborative process. Fed-ICL progressively refines responses by leveraging multi-round interactions between clients and a central server, improving answer quality without the need to transmit model parameters. We establish theoretical guarantees for the convergence of Fed-ICL and conduct extensive experiments on standard QA benchmarks, demonstrating that our proposed approach achieves strong performance while maintaining low communication costs.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extending Epistemic Uncertainty Beyond Parameters Would Assist in Designing Reliable LLMs</title>
<link>https://arxiv.org/abs/2506.07448</link>
<guid>https://arxiv.org/abs/2506.07448</guid>
<content:encoded><![CDATA[
arXiv:2506.07448v1 Announce Type: new 
Abstract: Although large language models (LLMs) are highly interactive and extendable, current approaches to ensure reliability in deployments remain mostly limited to rejecting outputs with high uncertainty in order to avoid misinformation. This conservative strategy reflects the current lack of tools to systematically distinguish and respond to different sources of uncertainty. In this paper, we advocate for the adoption of Bayesian Modeling of Experiments -- a framework that provides a coherent foundation to reason about uncertainty and clarify the reducibility of uncertainty -- for managing and proactively addressing uncertainty that arises in LLM deployments. This framework enables LLMs and their users to take contextually appropriate steps, such as requesting clarification, retrieving external information, or refining inputs. By supporting active resolution rather than passive avoidance, it opens the door to more reliable, transparent, and broadly applicable LLM systems, particularly in high-stakes, real-world settings.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Style Breaks Safety: Defending Language Models Against Superficial Style Alignment</title>
<link>https://arxiv.org/abs/2506.07452</link>
<guid>https://arxiv.org/abs/2506.07452</guid>
<content:encoded><![CDATA[
arXiv:2506.07452v1 Announce Type: new 
Abstract: Large language models (LLMs) can be prompted with specific styles (e.g., formatting responses as lists), including in jailbreak queries. Although these style patterns are semantically unrelated to the malicious intents behind jailbreak queries, their safety impact remains unclear. In this work, we seek to understand whether style patterns compromise LLM safety, how superficial style alignment increases model vulnerability, and how best to mitigate these risks during alignment. We evaluate 32 LLMs across seven jailbreak benchmarks, and find that malicious queries with style patterns inflate the attack success rate (ASR) for nearly all models. Notably, ASR inflation correlates with both the length of style patterns and the relative attention an LLM exhibits on them. We then investigate superficial style alignment, and find that fine-tuning with specific styles makes LLMs more vulnerable to jailbreaks of those same styles. Finally, we propose SafeStyle, a defense strategy that incorporates a small amount of safety training data augmented to match the distribution of style patterns in the fine-tuning data. Across three LLMs and five fine-tuning style settings, SafeStyle consistently outperforms baselines in maintaining LLM safety.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProteinZero: Self-Improving Protein Generation via Online Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.07459</link>
<guid>https://arxiv.org/abs/2506.07459</guid>
<content:encoded><![CDATA[
arXiv:2506.07459v1 Announce Type: new 
Abstract: Protein generative models have shown remarkable promise in protein design but still face limitations in success rate, due to the scarcity of high-quality protein datasets for supervised pretraining. We present ProteinZero, a novel framework that enables scalable, automated, and continuous self-improvement of the inverse folding model through online reinforcement learning. To achieve computationally tractable online feedback, we introduce efficient proxy reward models based on ESM-fold and a novel rapid ddG predictor that significantly accelerates evaluation speed. ProteinZero employs a general RL framework balancing multi-reward maximization, KL-divergence from a reference model, and a novel protein-embedding level diversity regularization that prevents mode collapse while promoting higher sequence diversity. Through extensive experiments, we demonstrate that ProteinZero substantially outperforms existing methods across every key metric in protein design, achieving significant improvements in structural accuracy, designability, thermodynamic stability, and sequence diversity. Most impressively, ProteinZero reduces design failure rates by approximately 36% - 48% compared to widely-used methods like ProteinMPNN, ESM-IF and InstructPLM, consistently achieving success rates exceeding 90% across diverse and complex protein folds. Notably, the entire RL run on CATH-4.3 can be done with a single 8 X GPU node in under 3 days, including reward computation. Our work establishes a new paradigm for protein design where models evolve continuously from their own generated outputs, opening new possibilities for exploring the vast protein design space.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Circumventing Backdoor Space via Weight Symmetry</title>
<link>https://arxiv.org/abs/2506.07467</link>
<guid>https://arxiv.org/abs/2506.07467</guid>
<content:encoded><![CDATA[
arXiv:2506.07467v1 Announce Type: new 
Abstract: Deep neural networks are vulnerable to backdoor attacks, where malicious behaviors are implanted during training. While existing defenses can effectively purify compromised models, they typically require labeled data or specific training procedures, making them difficult to apply beyond supervised learning settings. Notably, recent studies have shown successful backdoor attacks across various learning paradigms, highlighting a critical security concern. To address this gap, we propose Two-stage Symmetry Connectivity (TSC), a novel backdoor purification defense that operates independently of data format and requires only a small fraction of clean samples. Through theoretical analysis, we prove that by leveraging permutation invariance in neural networks and quadratic mode connectivity, TSC amplifies the loss on poisoned samples while maintaining bounded clean accuracy. Experiments demonstrate that TSC achieves robust performance comparable to state-of-the-art methods in supervised learning scenarios. Furthermore, TSC generalizes to self-supervised learning frameworks, such as SimCLR and CLIP, maintaining its strong defense capabilities. Our code is available at https://github.com/JiePeng104/TSC.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models</title>
<link>https://arxiv.org/abs/2506.07468</link>
<guid>https://arxiv.org/abs/2506.07468</guid>
<content:encoded><![CDATA[
arXiv:2506.07468v1 Announce Type: new 
Abstract: Conventional language model (LM) safety alignment relies on a reactive, disjoint procedure: attackers exploit a static model, followed by defensive fine-tuning to patch exposed vulnerabilities. This sequential approach creates a mismatch -- attackers overfit to obsolete defenses, while defenders perpetually lag behind emerging threats. To address this, we propose Self-RedTeam, an online self-play reinforcement learning algorithm where an attacker and defender agent co-evolve through continuous interaction. We cast safety alignment as a two-player zero-sum game, where a single model alternates between attacker and defender roles -- generating adversarial prompts and safeguarding against them -- while a reward LM adjudicates outcomes. This enables dynamic co-adaptation. Grounded in the game-theoretic framework of zero-sum games, we establish a theoretical safety guarantee which motivates the design of our method: if self-play converges to a Nash Equilibrium, the defender will reliably produce safe responses to any adversarial input. Empirically, Self-RedTeam uncovers more diverse attacks (+21.8% SBERT) compared to attackers trained against static defenders and achieves higher robustness on safety benchmarks (e.g., +65.5% on WildJailBreak) than defenders trained against static attackers. We further propose hidden Chain-of-Thought, allowing agents to plan privately, which boosts adversarial diversity and reduces over-refusals. Our results motivate a shift from reactive patching to proactive co-evolution in LM safety training, enabling scalable, autonomous, and robust self-improvement of LMs via multi-agent reinforcement learning (MARL).
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Premise Selection for a Lean Hammer</title>
<link>https://arxiv.org/abs/2506.07477</link>
<guid>https://arxiv.org/abs/2506.07477</guid>
<content:encoded><![CDATA[
arXiv:2506.07477v1 Announce Type: new 
Abstract: Neural methods are transforming automated reasoning for proof assistants, yet integrating these advances into practical verification workflows remains challenging. Hammers are tools that interface with external automatic theorem provers to automate tedious reasoning steps. They have dramatically improved productivity in proof assistants, but the Lean proof assistant still does not have a hammer despite its growing popularity. We present LeanHammer, the first end-to-end domain-general hammer for Lean, built on a novel neural premise selection system for a hammer in dependent type theory. Unlike existing Lean premise selectors, our approach dynamically adapts to user-specific contexts and combines with symbolic proof search and reconstruction to create a practical hammer. With comprehensive evaluations, we show that our premise selector enables LeanHammer to solve 21\% more goals relative to existing premise selectors, and generalize well to diverse domains. Our work bridges the gap between neural retrieval and symbolic reasoning, making formal verification more accessible to researchers and practitioners.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explicit Preference Optimization: No Need for an Implicit Reward Model</title>
<link>https://arxiv.org/abs/2506.07492</link>
<guid>https://arxiv.org/abs/2506.07492</guid>
<content:encoded><![CDATA[
arXiv:2506.07492v1 Announce Type: new 
Abstract: The generated responses of large language models (LLMs) are often fine-tuned to human preferences through a process called reinforcement learning from human feedback (RLHF). As RLHF relies on a challenging training sequence, whereby a separate reward model is independently learned and then later applied to LLM policy updates, ongoing research effort has targeted more straightforward alternatives. In this regard, direct preference optimization (DPO) and its many offshoots circumvent the need for a separate reward training step. Instead, through the judicious use of a reparameterization trick that induces an \textit{implicit} reward, DPO and related methods consolidate learning to the minimization of a single loss function. And yet despite demonstrable success in some real-world settings, we prove that DPO-based objectives are nonetheless subject to sub-optimal regularization and counter-intuitive interpolation behaviors, underappreciated artifacts of the reparameterizations upon which they are based. To this end, we introduce an \textit{explicit} preference optimization framework termed EXPO that requires no analogous reparameterization to achieve an implicit reward. Quite differently, we merely posit intuitively-appealing regularization factors from scratch that transparently avoid the potential pitfalls of key DPO variants, provably satisfying regularization desiderata that prior methods do not. Empirical results serve to corroborate our analyses and showcase the efficacy of EXPO.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind the Gap: Removing the Discretization Gap in Differentiable Logic Gate Networks</title>
<link>https://arxiv.org/abs/2506.07500</link>
<guid>https://arxiv.org/abs/2506.07500</guid>
<content:encoded><![CDATA[
arXiv:2506.07500v1 Announce Type: new 
Abstract: Modern neural networks demonstrate state-of-the-art performance on numerous existing benchmarks; however, their high computational requirements and energy consumption prompt researchers to seek more efficient solutions for real-world deployment. Logic gate networks (LGNs) learns a large network of logic gates for efficient image classification. However, learning a network that can solve a simple problem like CIFAR-10 can take days to weeks to train. Even then, almost half of the network remains unused, causing a discretization gap. This discretization gap hinders real-world deployment of LGNs, as the performance drop between training and inference negatively impacts accuracy. We inject Gumbel noise with a straight-through estimator during training to significantly speed up training, improve neuron utilization, and decrease the discretization gap. We theoretically show that this results from implicit Hessian regularization, which improves the convergence properties of LGNs. We train networks $4.5 \times$ faster in wall-clock time, reduce the discretization gap by $98\%$, and reduce the number of unused gates by $100\%$.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-of-Causal Evolution: Challenging Chain-of-Model for Reasoning</title>
<link>https://arxiv.org/abs/2506.07501</link>
<guid>https://arxiv.org/abs/2506.07501</guid>
<content:encoded><![CDATA[
arXiv:2506.07501v1 Announce Type: new 
Abstract: In view of the problem that each subchain in the chain-of-model (CoM) relies only on the information of the previous subchain and may lose long-range dependencies due to the causal mask blocking the global context flow between multi-level subchains, this work proposes a graph of causal evolution (GoCE). Its core principle is to map the implicit token representation into a differentiable and sparse causal adjacency matrix, then permeate causal constraints through each layer of calculation using causal-masked attention and causal-MoE. By combining intervention consistency loss test and self-evolution gate, the dynamic balance between causal structure learning and adaptive updating of transformer architecture is realized. The researcher built experimental environments in sandboxes built with Claude Sonnet 4, o4-mini-high, and DeepSeek R1 respectively with the transformer variant architecture introduced in GoCE. It is evaluated on publicly available datasets including CLUTRR, CLADDER, EX-FEVER, and CausalQA and compared with the baseline LLMs. The finding proves that GoCE strengthens the transformer's ability to capture long-range causal dependencies, while the ability to self-evolve is improved. It not only surpasses the design of CoM in terms of design principles, but also provides experience for future research on causal learning and continuous adaptive improvement.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning via Implicit Imitation Guidance</title>
<link>https://arxiv.org/abs/2506.07505</link>
<guid>https://arxiv.org/abs/2506.07505</guid>
<content:encoded><![CDATA[
arXiv:2506.07505v1 Announce Type: new 
Abstract: We study the problem of sample efficient reinforcement learning, where prior data such as demonstrations are provided for initialization in lieu of a dense reward signal. A natural approach is to incorporate an imitation learning objective, either as regularization during training or to acquire a reference policy. However, imitation learning objectives can ultimately degrade long-term performance, as it does not directly align with reward maximization. In this work, we propose to use prior data solely for guiding exploration via noise added to the policy, sidestepping the need for explicit behavior cloning constraints. The key insight in our framework, Data-Guided Noise (DGN), is that demonstrations are most useful for identifying which actions should be explored, rather than forcing the policy to take certain actions. Our approach achieves up to 2-3x improvement over prior reinforcement learning from offline data methods across seven simulated continuous control tasks.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing Correlated Latent Exogenous Variables in Debiased Recommender Systems</title>
<link>https://arxiv.org/abs/2506.07517</link>
<guid>https://arxiv.org/abs/2506.07517</guid>
<content:encoded><![CDATA[
arXiv:2506.07517v1 Announce Type: new 
Abstract: Recommendation systems (RS) aim to provide personalized content, but they face a challenge in unbiased learning due to selection bias, where users only interact with items they prefer. This bias leads to a distorted representation of user preferences, which hinders the accuracy and fairness of recommendations. To address the issue, various methods such as error imputation based, inverse propensity scoring, and doubly robust techniques have been developed. Despite the progress, from the structural causal model perspective, previous debiasing methods in RS assume the independence of the exogenous variables. In this paper, we release this assumption and propose a learning algorithm based on likelihood maximization to learn a prediction model. We first discuss the correlation and difference between unmeasured confounding and our scenario, then we propose a unified method that effectively handles latent exogenous variables. Specifically, our method models the data generation process with latent exogenous variables under mild normality assumptions. We then develop a Monte Carlo algorithm to numerically estimate the likelihood function. Extensive experiments on synthetic datasets and three real-world datasets demonstrate the effectiveness of our proposed method. The code is at https://github.com/WallaceSUI/kdd25-background-variable.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flowing Datasets with Wasserstein over Wasserstein Gradient Flows</title>
<link>https://arxiv.org/abs/2506.07534</link>
<guid>https://arxiv.org/abs/2506.07534</guid>
<content:encoded><![CDATA[
arXiv:2506.07534v1 Announce Type: new 
Abstract: Many applications in machine learning involve data represented as probability distributions. The emergence of such data requires radically novel techniques to design tractable gradient flows on probability distributions over this type of (infinite-dimensional) objects. For instance, being able to flow labeled datasets is a core task for applications ranging from domain adaptation to transfer learning or dataset distillation. In this setting, we propose to represent each class by the associated conditional distribution of features, and to model the dataset as a mixture distribution supported on these classes (which are themselves probability distributions), meaning that labeled datasets can be seen as probability distributions over probability distributions. We endow this space with a metric structure from optimal transport, namely the Wasserstein over Wasserstein (WoW) distance, derive a differential structure on this space, and define WoW gradient flows. The latter enables to design dynamics over this space that decrease a given objective functional. We apply our framework to transfer learning and dataset distillation tasks, leveraging our gradient flow construction as well as novel tractable functionals that take the form of Maximum Mean Discrepancies with Sliced-Wasserstein based kernels between probability distributions.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Memory Efficiency for Training KANs via Meta Learning</title>
<link>https://arxiv.org/abs/2506.07549</link>
<guid>https://arxiv.org/abs/2506.07549</guid>
<content:encoded><![CDATA[
arXiv:2506.07549v1 Announce Type: new 
Abstract: Inspired by the Kolmogorov-Arnold representation theorem, KANs offer a novel framework for function approximation by replacing traditional neural network weights with learnable univariate functions. This design demonstrates significant potential as an efficient and interpretable alternative to traditional MLPs. However, KANs are characterized by a substantially larger number of trainable parameters, leading to challenges in memory efficiency and higher training costs compared to MLPs. To address this limitation, we propose to generate weights for KANs via a smaller meta-learner, called MetaKANs. By training KANs and MetaKANs in an end-to-end differentiable manner, MetaKANs achieve comparable or even superior performance while significantly reducing the number of trainable parameters and maintaining promising interpretability. Extensive experiments on diverse benchmark tasks, including symbolic regression, partial differential equation solving, and image classification, demonstrate the effectiveness of MetaKANs in improving parameter efficiency and memory usage. The proposed method provides an alternative technique for training KANs, that allows for greater scalability and extensibility, and narrows the training cost gap with MLPs stated in the original paper of KANs. Our code is available at https://github.com/Murphyzc/MetaKAN.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChemAgent: Enhancing LLMs for Chemistry and Materials Science through Tree-Search Based Tool Learning</title>
<link>https://arxiv.org/abs/2506.07551</link>
<guid>https://arxiv.org/abs/2506.07551</guid>
<content:encoded><![CDATA[
arXiv:2506.07551v1 Announce Type: new 
Abstract: Large language models (LLMs) have recently demonstrated promising capabilities in chemistry tasks while still facing challenges due to outdated pretraining knowledge and the difficulty of incorporating specialized chemical expertise. To address these issues, we propose an LLM-based agent that synergistically integrates 137 external chemical tools created ranging from basic information retrieval to complex reaction predictions, and a dataset curation pipeline to generate the dataset ChemToolBench that facilitates both effective tool selection and precise parameter filling during fine-tuning and evaluation. We introduce a Hierarchical Evolutionary Monte Carlo Tree Search (HE-MCTS) framework, enabling independent optimization of tool planning and execution. By leveraging self-generated data, our approach supports step-level fine-tuning (FT) of the policy model and training task-adaptive PRM and ORM that surpass GPT-4o. Experimental evaluations demonstrate that our approach significantly improves performance in Chemistry QA and discovery tasks, offering a robust solution to integrate specialized tools with LLMs for advanced chemical applications. All datasets and code are available at https://github.com/AI4Chem/ChemistryAgent .
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Denoising the Future: Top-p Distributions for Moving Through Time</title>
<link>https://arxiv.org/abs/2506.07578</link>
<guid>https://arxiv.org/abs/2506.07578</guid>
<content:encoded><![CDATA[
arXiv:2506.07578v1 Announce Type: new 
Abstract: Inference in dynamic probabilistic models is a complex task involving expensive operations. In particular, for Hidden Markov Models, the whole state space has to be enumerated for advancing in time. Even states with negligible probabilities are considered, resulting in computational inefficiency and increased noise due to the propagation of unlikely probability mass. We propose to denoise the future and speed up inference by using only the top-p states, i.e., the most probable states with accumulated probability p. We show that the error introduced by using only the top-p states is bound by p and the so-called minimal mixing rate of the underlying model. Moreover, in our empirical evaluation, we show that we can expect speedups of at least an order of magnitude, while the error in terms of total variation distance is below 0.09.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedCGD: Collective Gradient Divergence Optimized Scheduling for Wireless Federated Learning</title>
<link>https://arxiv.org/abs/2506.07581</link>
<guid>https://arxiv.org/abs/2506.07581</guid>
<content:encoded><![CDATA[
arXiv:2506.07581v1 Announce Type: new 
Abstract: Federated learning (FL) is a promising paradigm for multiple devices to cooperatively train a model. When applied in wireless networks, two issues consistently affect the performance of FL, i.e., data heterogeneity of devices and limited bandwidth. Many papers have investigated device scheduling strategies considering the two issues. However, most of them recognize data heterogeneity as a property of individual devices. In this paper, we prove that the convergence speed of FL is affected by the sum of device-level and sample-level collective gradient divergence (CGD). The device-level CGD refers to the gradient divergence of the scheduled device group, instead of the sum of the individual device divergence. The sample-level CGD is statistically upper bounded by sampling variance, which is inversely proportional to the total number of samples scheduled for local update. To derive a tractable form of the device-level CGD, we further consider a classification problem and transform it into the weighted earth moving distance (WEMD) between the group distribution and the global distribution. Then we propose FedCGD algorithm to minimize the sum of multi-level CGDs by balancing WEMD and sampling variance, within polynomial time. Simulation shows that the proposed strategy increases classification accuracy on the CIFAR-10 dataset by up to 4.2\% while scheduling 41.8\% fewer devices, and flexibly switches between reducing WEMD and reducing sampling variance.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIRA: Medical Time Series Foundation Model for Real-World Health Data</title>
<link>https://arxiv.org/abs/2506.07584</link>
<guid>https://arxiv.org/abs/2506.07584</guid>
<content:encoded><![CDATA[
arXiv:2506.07584v1 Announce Type: new 
Abstract: A unified foundation model for medical time series -- pretrained on open access and ethics board-approved medical corpora -- offers the potential to reduce annotation burdens, minimize model customization, and enable robust transfer across clinical institutions, modalities, and tasks, particularly in data-scarce or privacy-constrained environments. However, existing generalist time series foundation models struggle to handle medical time series data due to their inherent challenges, including irregular intervals, heterogeneous sampling rates, and frequent missing values. To address these challenges, we introduce MIRA, a unified foundation model specifically designed for medical time series forecasting. MIRA incorporates a Continuous-Time Rotary Positional Encoding that enables fine-grained modeling of variable time intervals, a frequency-specific mixture-of-experts layer that routes computation across latent frequency regimes to further promote temporal specialization, and a Continuous Dynamics Extrapolation Block based on Neural ODE that models the continuous trajectory of latent states, enabling accurate forecasting at arbitrary target timestamps. Pretrained on a large-scale and diverse medical corpus comprising over 454 billion time points collect from publicly available datasets, MIRA achieves reductions in forecasting errors by an average of 10% and 7% in out-of-distribution and in-distribution scenarios, respectively, when compared to other zero-shot and fine-tuned baselines. We also introduce a comprehensive benchmark spanning multiple downstream clinical tasks, establishing a foundation for future research in medical time series modeling.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aircraft Trajectory Dataset Augmentation in Latent Space</title>
<link>https://arxiv.org/abs/2506.07585</link>
<guid>https://arxiv.org/abs/2506.07585</guid>
<content:encoded><![CDATA[
arXiv:2506.07585v1 Announce Type: new 
Abstract: Aircraft trajectory modeling plays a crucial role in Air Traffic Management (ATM) and is important for various downstream tasks, including conflict detection and landing time prediction. Dataset augmentation through the addition of synthetically generated trajectory data is necessary to develop a more robust aircraft trajectory model and ensure that the trajectory dataset is sufficient and balanced. In this work, we propose a novel framework called ATRADA for aircraft trajectory dataset augmentation. In the proposed framework, a Transformer encoder learns the underlying patterns in the original trajectory dataset and converts each data point into a context vector in the learned latent space. The converted dataset in the latent space is projected into reduced dimensions using principal component analysis (PCA), and a Gaussian mixture model (GMM) is applied to fit the probability distribution of the data points in the reduced-dimensional space. Finally, new samples are drawn from the fitted GMM, the dimension of the samples is reverted to the original dimension, and they are decoded with a Multi-Layer Perceptron (MLP). Several experiments demonstrate that the framework effectively generates new, high-quality synthetic aircraft trajectory data, which were compared to the results of several baselines.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PrunePEFT: Iterative Hybrid Pruning for Parameter-Efficient Fine-tuning of LLMs</title>
<link>https://arxiv.org/abs/2506.07587</link>
<guid>https://arxiv.org/abs/2506.07587</guid>
<content:encoded><![CDATA[
arXiv:2506.07587v1 Announce Type: new 
Abstract: Parameter Efficient Fine-Tuning (PEFT) methods have emerged as effective and promising approaches for fine-tuning pre-trained language models. Compared with Full parameter Fine-Tuning (FFT), PEFT achieved comparable task performance with a substantial reduction of trainable parameters, which largely saved the training and storage costs. However, using the PEFT method requires considering a vast design space, such as the type of PEFT modules and their insertion layers. Inadequate configurations can lead to sub-optimal results. Conventional solutions such as architectural search techniques, while effective, tend to introduce substantial additional overhead. In this paper, we propose a novel approach, PrunePEFT, which formulates the PEFT strategy search as a pruning problem and introduces a hybrid pruning strategy that capitalizes on the sensitivity of pruning methods to different PEFT modules. This method extends traditional pruning techniques by iteratively removing redundant or conflicting PEFT modules, thereby optimizing the fine-tuned configuration. By efficiently identifying the most relevant modules, our approach significantly reduces the computational burden typically associated with architectural search processes, making it a more scalable and efficient solution for fine-tuning large pre-trained models.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploiting Curvature in Online Convex Optimization with Delayed Feedback</title>
<link>https://arxiv.org/abs/2506.07595</link>
<guid>https://arxiv.org/abs/2506.07595</guid>
<content:encoded><![CDATA[
arXiv:2506.07595v1 Announce Type: new 
Abstract: In this work, we study the online convex optimization problem with curved losses and delayed feedback. When losses are strongly convex, existing approaches obtain regret bounds of order $d_{\max} \ln T$, where $d_{\max}$ is the maximum delay and $T$ is the time horizon. However, in many cases, this guarantee can be much worse than $\sqrt{d_{\mathrm{tot}}}$ as obtained by a delayed version of online gradient descent, where $d_{\mathrm{tot}}$ is the total delay. We bridge this gap by proposing a variant of follow-the-regularized-leader that obtains regret of order $\min\{\sigma_{\max}\ln T, \sqrt{d_{\mathrm{tot}}}\}$, where $\sigma_{\max}$ is the maximum number of missing observations. We then consider exp-concave losses and extend the Online Newton Step algorithm to handle delays with an adaptive learning rate tuning, achieving regret $\min\{d_{\max} n\ln T, \sqrt{d_{\mathrm{tot}}}\}$ where $n$ is the dimension. To our knowledge, this is the first algorithm to achieve such a regret bound for exp-concave losses. We further consider the problem of unconstrained online linear regression and achieve a similar guarantee by designing a variant of the Vovk-Azoury-Warmuth forecaster with a clipping trick. Finally, we implement our algorithms and conduct experiments under various types of delay and losses, showing an improved performance over existing methods.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts</title>
<link>https://arxiv.org/abs/2506.07596</link>
<guid>https://arxiv.org/abs/2506.07596</guid>
<content:encoded><![CDATA[
arXiv:2506.07596v1 Announce Type: new 
Abstract: Machine learning is advancing rapidly, with applications bringing notable benefits, such as improvements in translation and code generation. Models like ChatGPT, powered by Large Language Models (LLMs), are increasingly integrated into daily life. However, alongside these benefits, LLMs also introduce social risks. Malicious users can exploit LLMs by submitting harmful prompts, such as requesting instructions for illegal activities. To mitigate this, models often include a security mechanism that automatically rejects such harmful prompts. However, they can be bypassed through LLM jailbreaks. Current jailbreaks often require significant manual effort, high computational costs, or result in excessive model modifications that may degrade regular utility.
  We introduce TwinBreak, an innovative safety alignment removal method. Building on the idea that the safety mechanism operates like an embedded backdoor, TwinBreak identifies and prunes parameters responsible for this functionality. By focusing on the most relevant model layers, TwinBreak performs fine-grained analysis of parameters essential to model utility and safety. TwinBreak is the first method to analyze intermediate outputs from prompts with high structural and content similarity to isolate safety parameters. We present the TwinPrompt dataset containing 100 such twin prompts. Experiments confirm TwinBreak's effectiveness, achieving 89% to 98% success rates with minimal computational requirements across 16 LLMs from five vendors.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FuXi-Air: Urban Air Quality Forecasting Based on Emission-Meteorology-Pollutant multimodal Machine Learning</title>
<link>https://arxiv.org/abs/2506.07616</link>
<guid>https://arxiv.org/abs/2506.07616</guid>
<content:encoded><![CDATA[
arXiv:2506.07616v1 Announce Type: new 
Abstract: Air pollution has emerged as a major public health challenge in megacities. Numerical simulations and single-site machine learning approaches have been widely applied in air quality forecasting tasks. However, these methods face multiple limitations, including high computational costs, low operational efficiency, and limited integration with observational data. With the rapid advancement of artificial intelligence, there is an urgent need to develop a low-cost, efficient air quality forecasting model for smart urban management. An air quality forecasting model, named FuXi-Air, has been constructed in this study based on multimodal data fusion to support high-precision air quality forecasting and operated in typical megacities. The model integrates meteorological forecasts, emission inventories, and pollutant monitoring data under the guidance of air pollution mechanism. By combining an autoregressive prediction framework with a frame interpolation strategy, the model successfully completes 72-hour forecasts for six major air pollutants at an hourly resolution across multiple monitoring sites within 25-30 seconds. In terms of both computational efficiency and forecasting accuracy, it outperforms the mainstream numerical air quality models in operational forecasting work. Ablation experiments concerning key influencing factors show that although meteorological data contribute more to model accuracy than emission inventories do, the integration of multimodal data significantly improves forecasting precision and ensures that reliable predictions are obtained under differing pollution mechanisms across megacities. This study provides both a technical reference and a practical example for applying multimodal data-driven models to air quality forecasting and offers new insights into building hybrid forecasting systems to support air pollution risk warning in smart city management.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Catechol Benchmark: Time-series Solvent Selection Data for Few-shot Machine Learning</title>
<link>https://arxiv.org/abs/2506.07619</link>
<guid>https://arxiv.org/abs/2506.07619</guid>
<content:encoded><![CDATA[
arXiv:2506.07619v1 Announce Type: new 
Abstract: Machine learning has promised to change the landscape of laboratory chemistry, with impressive results in molecular property prediction and reaction retro-synthesis. However, chemical datasets are often inaccessible to the machine learning community as they tend to require cleaning, thorough understanding of the chemistry, or are simply not available. In this paper, we introduce a novel dataset for yield prediction, providing the first-ever transient flow dataset for machine learning benchmarking, covering over 1200 process conditions. While previous datasets focus on discrete parameters, our experimental set-up allow us to sample a large number of continuous process conditions, generating new challenges for machine learning models. We focus on solvent selection, a task that is particularly difficult to model theoretically and therefore ripe for machine learning applications. We showcase benchmarking for regression algorithms, transfer-learning approaches, feature engineering, and active learning, with important applications towards solvent replacement and sustainable manufacturing.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Return of ChebNet: Understanding and Improving an Overlooked GNN on Long Range Tasks</title>
<link>https://arxiv.org/abs/2506.07624</link>
<guid>https://arxiv.org/abs/2506.07624</guid>
<content:encoded><![CDATA[
arXiv:2506.07624v1 Announce Type: new 
Abstract: ChebNet, one of the earliest spectral GNNs, has largely been overshadowed by Message Passing Neural Networks (MPNNs), which gained popularity for their simplicity and effectiveness in capturing local graph structure. Despite their success, MPNNs are limited in their ability to capture long-range dependencies between nodes. This has led researchers to adapt MPNNs through rewiring or make use of Graph Transformers, which compromises the computational efficiency that characterized early spatial message-passing architectures, and typically disregards the graph structure. Almost a decade after its original introduction, we revisit ChebNet to shed light on its ability to model distant node interactions. We find that out-of-box, ChebNet already shows competitive advantages relative to classical MPNNs and GTs on long-range benchmarks, while maintaining good scalability properties for high-order polynomials. However, we uncover that this polynomial expansion leads ChebNet to an unstable regime during training. To address this limitation, we cast ChebNet as a stable and non-dissipative dynamical system, which we coin Stable-ChebNet. Our Stable-ChebNet model allows for stable information propagation, and has controllable dynamics which do not require the use of eigendecompositions, positional encodings, or graph rewiring. Across several benchmarks, Stable-ChebNet achieves near state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Universality Lens: Why Even Highly Over-Parametrized Models Learn Well</title>
<link>https://arxiv.org/abs/2506.07661</link>
<guid>https://arxiv.org/abs/2506.07661</guid>
<content:encoded><![CDATA[
arXiv:2506.07661v1 Announce Type: new 
Abstract: A fundamental question in modern machine learning is why large, over-parameterized models, such as deep neural networks and transformers, tend to generalize well, even when their number of parameters far exceeds the number of training samples.
  We investigate this phenomenon through the lens of information theory, grounded in universal learning theory. Specifically, we study a Bayesian mixture learner with log-loss and (almost) uniform prior over an expansive hypothesis class.
  Our key result shows that the learner's regret is not determined by the overall size of the hypothesis class, but rather by the cumulative probability of all models that are close, in Kullback-Leibler divergence distance, to the true data-generating process. We refer to this cumulative probability as the weight of the hypothesis.
  This leads to a natural notion of model simplicity: simple models are those with large weight and thus require fewer samples to generalize, while complex models have small weight and need more data. This perspective provides a rigorous and intuitive explanation for why over-parameterized models often avoid overfitting: the presence of simple hypotheses allows the posterior to concentrate on them when supported by the data.
  We further bridge theory and practice by recalling that stochastic gradient descent with Langevin dynamics samples from the correct posterior distribution, enabling our theoretical learner to be approximated using standard machine learning methods combined with ensemble learning.
  Our analysis yields non-uniform regret bounds and aligns with key practical concepts such as flat minima and model distillation. The results apply broadly across online, batch, and supervised learning settings, offering a unified and principled understanding of the generalization behavior of modern AI systems.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProARD: progressive adversarial robustness distillation: provide wide range of robust students</title>
<link>https://arxiv.org/abs/2506.07666</link>
<guid>https://arxiv.org/abs/2506.07666</guid>
<content:encoded><![CDATA[
arXiv:2506.07666v1 Announce Type: new 
Abstract: Adversarial Robustness Distillation (ARD) has emerged as an effective method to enhance the robustness of lightweight deep neural networks against adversarial attacks. Current ARD approaches have leveraged a large robust teacher network to train one robust lightweight student. However, due to the diverse range of edge devices and resource constraints, current approaches require training a new student network from scratch to meet specific constraints, leading to substantial computational costs and increased CO2 emissions. This paper proposes Progressive Adversarial Robustness Distillation (ProARD), enabling the efficient one-time training of a dynamic network that supports a diverse range of accurate and robust student networks without requiring retraining. We first make a dynamic deep neural network based on dynamic layers by encompassing variations in width, depth, and expansion in each design stage to support a wide range of architectures. Then, we consider the student network with the largest size as the dynamic teacher network. ProARD trains this dynamic network using a weight-sharing mechanism to jointly optimize the dynamic teacher network and its internal student networks. However, due to the high computational cost of calculating exact gradients for all the students within the dynamic network, a sampling mechanism is required to select a subset of students. We show that random student sampling in each iteration fails to produce accurate and robust students.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Benchmark Prediction from Fewer Data Misses the Mark</title>
<link>https://arxiv.org/abs/2506.07673</link>
<guid>https://arxiv.org/abs/2506.07673</guid>
<content:encoded><![CDATA[
arXiv:2506.07673v1 Announce Type: new 
Abstract: Large language model (LLM) evaluation is increasingly costly, prompting interest in methods that speed up evaluation by shrinking benchmark datasets. Benchmark prediction (also called efficient LLM evaluation) aims to select a small subset of evaluation points and predict overall benchmark performance from that subset. In this paper, we systematically assess the strengths and limitations of 11 benchmark prediction methods across 19 diverse benchmarks. First, we identify a highly competitive baseline: Take a random sample and fit a regression model on the sample to predict missing entries. Outperforming most existing methods, this baseline challenges the assumption that careful subset selection is necessary for benchmark prediction. Second, we discover that all existing methods crucially depend on model similarity. They work best when interpolating scores among similar models. The effectiveness of benchmark prediction sharply declines when new models have higher accuracy than previously seen models. In this setting of extrapolation, none of the previous methods consistently beat a simple average over random samples. To improve over the sample average, we introduce a new method inspired by augmented inverse propensity weighting. This method consistently outperforms the random sample average even for extrapolation. However, its performance still relies on model similarity and the gains are modest in general. This shows that benchmark prediction fails just when it is most needed: at the evaluation frontier, where the goal is to evaluate new models of unknown capabilities.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Robustness in Latent Diffusion Models via Embedding Level Augmentation</title>
<link>https://arxiv.org/abs/2506.07706</link>
<guid>https://arxiv.org/abs/2506.07706</guid>
<content:encoded><![CDATA[
arXiv:2506.07706v1 Announce Type: new 
Abstract: Latent diffusion models (LDMs) achieve state-of-the-art performance across various tasks, including image generation and video synthesis. However, they generally lack robustness, a limitation that remains not fully explored in current research. In this paper, we propose several methods to address this gap. First, we hypothesize that the robustness of LDMs primarily should be measured without their text encoder, because if we take and explore the whole architecture, the problems of image generator and text encoders wll be fused. Second, we introduce novel data augmentation techniques designed to reveal robustness shortcomings in LDMs when processing diverse textual prompts. We then fine-tune Stable Diffusion 3 and Stable Diffusion XL models using Dreambooth, incorporating these proposed augmentation methods across multiple tasks. Finally, we propose a novel evaluation pipeline specifically tailored to assess the robustness of LDMs fine-tuned via Dreambooth.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Embedding Meets Dynamic Graph: A New Exploration for Neural Architecture Representation Learning</title>
<link>https://arxiv.org/abs/2506.07735</link>
<guid>https://arxiv.org/abs/2506.07735</guid>
<content:encoded><![CDATA[
arXiv:2506.07735v1 Announce Type: new 
Abstract: Neural Architecture Representation Learning aims to transform network models into feature representations for predicting network attributes, playing a crucial role in deploying and designing networks for real-world applications. Recently, inspired by the success of transformers, transformer-based models integrated with Graph Neural Networks (GNNs) have achieved significant progress in representation learning. However, current methods still have some limitations. First, existing methods overlook hardware attribute information, which conflicts with the current trend of diversified deep learning hardware and limits the practical applicability of models. Second, current encoding approaches rely on static adjacency matrices to represent topological structures, failing to capture the structural differences between computational nodes, which ultimately compromises encoding effectiveness. In this paper, we introduce LeDG-Former, an innovative framework that addresses these limitations through the synergistic integration of language-based semantic embedding and dynamic graph representation learning. Specifically, inspired by large language models (LLMs), we propose a language embedding framework where both neural architectures and hardware platform specifications are projected into a unified semantic space through tokenization and LLM processing, enabling zero-shot prediction across different hardware platforms for the first time. Then, we propose a dynamic graph-based transformer for modeling neural architectures, resulting in improved neural architecture modeling performance. On the NNLQP benchmark, LeDG-Former surpasses previous methods, establishing a new SOTA while demonstrating the first successful cross-hardware latency prediction capability. Furthermore, our framework achieves superior performance on the cell-structured NAS-Bench-101 and NAS-Bench-201 datasets.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-Assisted Stitching for Offline Hierarchical Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.07744</link>
<guid>https://arxiv.org/abs/2506.07744</guid>
<content:encoded><![CDATA[
arXiv:2506.07744v1 Announce Type: new 
Abstract: Existing offline hierarchical reinforcement learning methods rely on high-level policy learning to generate subgoal sequences. However, their efficiency degrades as task horizons increase, and they lack effective strategies for stitching useful state transitions across different trajectories. We propose Graph-Assisted Stitching (GAS), a novel framework that formulates subgoal selection as a graph search problem rather than learning an explicit high-level policy. By embedding states into a Temporal Distance Representation (TDR) space, GAS clusters semantically similar states from different trajectories into unified graph nodes, enabling efficient transition stitching. A shortest-path algorithm is then applied to select subgoal sequences within the graph, while a low-level policy learns to reach the subgoals. To improve graph quality, we introduce the Temporal Efficiency (TE) metric, which filters out noisy or inefficient transition states, significantly enhancing task performance. GAS outperforms prior offline HRL methods across locomotion, navigation, and manipulation tasks. Notably, in the most stitching-critical task, it achieves a score of 88.3, dramatically surpassing the previous state-of-the-art score of 1.0. Our source code is available at: https://github.com/qortmdgh4141/GAS.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>E-LDA: Toward Interpretable LDA Topic Models with Strong Guarantees in Logarithmic Parallel Time</title>
<link>https://arxiv.org/abs/2506.07747</link>
<guid>https://arxiv.org/abs/2506.07747</guid>
<content:encoded><![CDATA[
arXiv:2506.07747v1 Announce Type: new 
Abstract: In this paper, we provide the first practical algorithms with provable guarantees for the problem of inferring the topics assigned to each document in an LDA topic model. This is the primary inference problem for many applications of topic models in social science, data exploration, and causal inference settings. We obtain this result by showing a novel non-gradient-based, combinatorial approach to estimating topic models. This yields algorithms that converge to near-optimal posterior probability in logarithmic parallel computation time (adaptivity) -- exponentially faster than any known LDA algorithm. We also show that our approach can provide interpretability guarantees such that each learned topic is formally associated with a known keyword. Finally, we show that unlike alternatives, our approach can maintain the independence assumptions necessary to use the learned topic model for downstream causal inference methods that allow researchers to study topics as treatments. In terms of practical performance, our approach consistently returns solutions of higher semantic quality than solutions from state-of-the-art LDA algorithms, neural topic models, and LLM-based topic models across a diverse range of text datasets and evaluation parameters.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing Credit Risk Estimates in the Gen-AI Era</title>
<link>https://arxiv.org/abs/2506.07754</link>
<guid>https://arxiv.org/abs/2506.07754</guid>
<content:encoded><![CDATA[
arXiv:2506.07754v1 Announce Type: new 
Abstract: Generative AI technologies have demonstrated significant potential across diverse applications. This study provides a comparative analysis of credit score modeling techniques, contrasting traditional approaches with those leveraging generative AI. Our findings reveal that current generative AI models fall short of matching the performance of traditional methods, regardless of the integration strategy employed. These results highlight the limitations in the current capabilities of generative AI for credit risk scoring, emphasizing the need for further research and development before the possibility of applying generative AI for this specific task, or equivalent ones.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clustered Federated Learning via Embedding Distributions</title>
<link>https://arxiv.org/abs/2506.07769</link>
<guid>https://arxiv.org/abs/2506.07769</guid>
<content:encoded><![CDATA[
arXiv:2506.07769v1 Announce Type: new 
Abstract: Federated learning (FL) is a widely used framework for machine learning in distributed data environments where clients hold data that cannot be easily centralised, such as for data protection reasons. FL, however, is known to be vulnerable to non-IID data. Clustered FL addresses this issue by finding more homogeneous clusters of clients. We propose a novel one-shot clustering method, EMD-CFL, using the Earth Mover's distance (EMD) between data distributions in embedding space. We theoretically motivate the use of EMDs using results from the domain adaptation literature and demonstrate empirically superior clustering performance in extensive comparisons against 16 baselines and on a range of challenging datasets.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Adversarial Robustness with Conformal Prediction: A Framework for Guaranteed Model Reliability</title>
<link>https://arxiv.org/abs/2506.07804</link>
<guid>https://arxiv.org/abs/2506.07804</guid>
<content:encoded><![CDATA[
arXiv:2506.07804v1 Announce Type: new 
Abstract: As deep learning models are increasingly deployed in high-risk applications, robust defenses against adversarial attacks and reliable performance guarantees become paramount. Moreover, accuracy alone does not provide sufficient assurance or reliable uncertainty estimates for these models. This study advances adversarial training by leveraging principles from Conformal Prediction. Specifically, we develop an adversarial attack method, termed OPSA (OPtimal Size Attack), designed to reduce the efficiency of conformal prediction at any significance level by maximizing model uncertainty without requiring coverage guarantees. Correspondingly, we introduce OPSA-AT (Adversarial Training), a defense strategy that integrates OPSA within a novel conformal training paradigm. Experimental evaluations demonstrate that our OPSA attack method induces greater uncertainty compared to baseline approaches for various defenses. Conversely, our OPSA-AT defensive model significantly enhances robustness not only against OPSA but also other adversarial attacks, and maintains reliable prediction. Our findings highlight the effectiveness of this integrated approach for developing trustworthy and resilient deep learning models for safety-critical domains. Our code is available at https://github.com/bjbbbb/Enhancing-Adversarial-Robustness-with-Conformal-Prediction.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifiable Object Representations under Spatial Ambiguities</title>
<link>https://arxiv.org/abs/2506.07806</link>
<guid>https://arxiv.org/abs/2506.07806</guid>
<content:encoded><![CDATA[
arXiv:2506.07806v1 Announce Type: new 
Abstract: Modular object-centric representations are essential for *human-like reasoning* but are challenging to obtain under spatial ambiguities, *e.g. due to occlusions and view ambiguities*. However, addressing challenges presents both theoretical and practical difficulties. We introduce a novel multi-view probabilistic approach that aggregates view-specific slots to capture *invariant content* information while simultaneously learning disentangled global *viewpoint-level* information. Unlike prior single-view methods, our approach resolves spatial ambiguities, provides theoretical guarantees for identifiability, and requires *no viewpoint annotations*. Extensive experiments on standard benchmarks and novel complex datasets validate our method's robustness and scalability.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Diffusion Models in Offline RL via Reward-Aware Consistency Trajectory Distillation</title>
<link>https://arxiv.org/abs/2506.07822</link>
<guid>https://arxiv.org/abs/2506.07822</guid>
<content:encoded><![CDATA[
arXiv:2506.07822v1 Announce Type: new 
Abstract: Although diffusion models have achieved strong results in decision-making tasks, their slow inference speed remains a key limitation. While the consistency model offers a potential solution, its applications to decision-making often struggle with suboptimal demonstrations or rely on complex concurrent training of multiple networks. In this work, we propose a novel approach to consistency distillation for offline reinforcement learning that directly incorporates reward optimization into the distillation process. Our method enables single-step generation while maintaining higher performance and simpler training. Empirical evaluations on the Gym MuJoCo benchmarks and long horizon planning demonstrate that our approach can achieve an 8.7% improvement over previous state-of-the-art while offering up to 142x speedup over diffusion counterparts in inference time.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralizing Multi-Agent Reinforcement Learning with Temporal Causal Information</title>
<link>https://arxiv.org/abs/2506.07829</link>
<guid>https://arxiv.org/abs/2506.07829</guid>
<content:encoded><![CDATA[
arXiv:2506.07829v1 Announce Type: new 
Abstract: Reinforcement learning (RL) algorithms can find an optimal policy for a single agent to accomplish a particular task. However, many real-world problems require multiple agents to collaborate in order to achieve a common goal. For example, a robot executing a task in a warehouse may require the assistance of a drone to retrieve items from high shelves. In Decentralized Multi-Agent RL (DMARL), agents learn independently and then combine their policies at execution time, but often must satisfy constraints on compatibility of local policies to ensure that they can achieve the global task when combined. In this paper, we study how providing high-level symbolic knowledge to agents can help address unique challenges of this setting, such as privacy constraints, communication limitations, and performance concerns. In particular, we extend the formal tools used to check the compatibility of local policies with the team task, making decentralized training with theoretical guarantees usable in more scenarios. Furthermore, we empirically demonstrate that symbolic knowledge about the temporal evolution of events in the environment can significantly expedite the learning process in DMARL.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving large language models with concept-aware fine-tuning</title>
<link>https://arxiv.org/abs/2506.07833</link>
<guid>https://arxiv.org/abs/2506.07833</guid>
<content:encoded><![CDATA[
arXiv:2506.07833v1 Announce Type: new 
Abstract: Large language models (LLMs) have become the cornerstone of modern AI. However, the existing paradigm of next-token prediction fundamentally limits their ability to form coherent, high-level concepts, making it a critical barrier to human-like understanding and reasoning. Take the phrase "ribonucleic acid" as an example: an LLM will first decompose it into tokens, i.e., artificial text fragments ("rib", "on", ...), then learn each token sequentially, rather than grasping the phrase as a unified, coherent semantic entity. This fragmented representation hinders deeper conceptual understanding and, ultimately, the development of truly intelligent systems. In response, we introduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method that redefines how LLMs are fine-tuned. By enabling the learning of sequences that span multiple tokens, this method fosters stronger concept-aware learning. Our experiments demonstrate significant improvements compared to conventional next-token finetuning methods across diverse tasks, including traditional applications like text summarization and domain-specific ones like de novo protein design. Multi-token prediction was previously only possible in the prohibitively expensive pretraining phase; CAFT, to our knowledge, is the first to bring the multi-token setting to the post-training phase, thus effectively democratizing its benefits for the broader community of practitioners and researchers. Finally, the unexpected effectiveness of our proposed method suggests wider implications for the machine learning research community. All code and data are available at https://github.com/michaelchen-lab/caft-llm
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Jarzynski Reweighting and Sampling Dynamics for Training Energy-Based Models: Theoretical Analysis of Different Transition Kernels</title>
<link>https://arxiv.org/abs/2506.07843</link>
<guid>https://arxiv.org/abs/2506.07843</guid>
<content:encoded><![CDATA[
arXiv:2506.07843v1 Announce Type: new 
Abstract: Energy-Based Models (EBMs) provide a flexible framework for generative modeling, but their training remains theoretically challenging due to the need to approximate normalization constants and efficiently sample from complex, multi-modal distributions. Traditional methods, such as contrastive divergence and score matching, introduce biases that can hinder accurate learning. In this work, we present a theoretical analysis of Jarzynski reweighting, a technique from non-equilibrium statistical mechanics, and its implications for training EBMs. We focus on the role of the choice of the kernel and we illustrate these theoretical considerations in two key generative frameworks: (i) flow-based diffusion models, where we reinterpret Jarzynski reweighting in the context of stochastic interpolants to mitigate discretization errors and improve sample quality, and (ii) Restricted Boltzmann Machines, where we analyze its role in correcting the biases of contrastive divergence. Our results provide insights into the interplay between kernel choice and model performance, highlighting the potential of Jarzynski reweighting as a principled tool for generative learning.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Residual Reweighted Conformal Prediction for Graph Neural Networks</title>
<link>https://arxiv.org/abs/2506.07854</link>
<guid>https://arxiv.org/abs/2506.07854</guid>
<content:encoded><![CDATA[
arXiv:2506.07854v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) excel at modeling relational data but face significant challenges in high-stakes domains due to unquantified uncertainty. Conformal prediction (CP) offers statistical coverage guarantees, but existing methods often produce overly conservative prediction intervals that fail to account for graph heteroscedasticity and structural biases. While residual reweighting CP variants address some of these limitations, they neglect graph topology, cluster-specific uncertainties, and risk data leakage by reusing training sets. To address these issues, we propose Residual Reweighted GNN (RR-GNN), a framework designed to generate minimal prediction sets with provable marginal coverage guarantees.
  RR-GNN introduces three major innovations to enhance prediction performance. First, it employs Graph-Structured Mondrian CP to partition nodes or edges into communities based on topological features, ensuring cluster-conditional coverage that reflects heterogeneity. Second, it uses Residual-Adaptive Nonconformity Scores by training a secondary GNN on a held-out calibration set to estimate task-specific residuals, dynamically adjusting prediction intervals according to node or edge uncertainty. Third, it adopts a Cross-Training Protocol, which alternates the optimization of the primary GNN and the residual predictor to prevent information leakage while maintaining graph dependencies. We validate RR-GNN on 15 real-world graphs across diverse tasks, including node classification, regression, and edge weight prediction. Compared to CP baselines, RR-GNN achieves improved efficiency over state-of-the-art methods, with no loss of coverage.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fairness Overfitting in Machine Learning: An Information-Theoretic Perspective</title>
<link>https://arxiv.org/abs/2506.07861</link>
<guid>https://arxiv.org/abs/2506.07861</guid>
<content:encoded><![CDATA[
arXiv:2506.07861v1 Announce Type: new 
Abstract: Despite substantial progress in promoting fairness in high-stake applications using machine learning models, existing methods often modify the training process, such as through regularizers or other interventions, but lack formal guarantees that fairness achieved during training will generalize to unseen data. Although overfitting with respect to prediction performance has been extensively studied, overfitting in terms of fairness loss has received far less attention. This paper proposes a theoretical framework for analyzing fairness generalization error through an information-theoretic lens. Our novel bounding technique is based on Efron-Stein inequality, which allows us to derive tight information-theoretic fairness generalization bounds with both Mutual Information (MI) and Conditional Mutual Information (CMI). Our empirical results validate the tightness and practical relevance of these bounds across diverse fairness-aware learning algorithms. Our framework offers valuable insights to guide the design of algorithms improving fairness generalization.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightweight Sequential Transformers for Blood Glucose Level Prediction in Type-1 Diabetes</title>
<link>https://arxiv.org/abs/2506.07864</link>
<guid>https://arxiv.org/abs/2506.07864</guid>
<content:encoded><![CDATA[
arXiv:2506.07864v1 Announce Type: new 
Abstract: Type 1 Diabetes (T1D) affects millions worldwide, requiring continuous monitoring to prevent severe hypo- and hyperglycemic events. While continuous glucose monitoring has improved blood glucose management, deploying predictive models on wearable devices remains challenging due to computational and memory constraints. To address this, we propose a novel Lightweight Sequential Transformer model designed for blood glucose prediction in T1D. By integrating the strengths of Transformers' attention mechanisms and the sequential processing of recurrent neural networks, our architecture captures long-term dependencies while maintaining computational efficiency. The model is optimized for deployment on resource-constrained edge devices and incorporates a balanced loss function to handle the inherent data imbalance in hypo- and hyperglycemic events. Experiments on two benchmark datasets, OhioT1DM and DiaTrend, demonstrate that the proposed model outperforms state-of-the-art methods in predicting glucose levels and detecting adverse events. This work fills the gap between high-performance modeling and practical deployment, providing a reliable and efficient T1D management solution.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Hessian-Based Insights Support Fault Diagnosis in Attention-based Models?</title>
<link>https://arxiv.org/abs/2506.07871</link>
<guid>https://arxiv.org/abs/2506.07871</guid>
<content:encoded><![CDATA[
arXiv:2506.07871v1 Announce Type: new 
Abstract: As attention-based deep learning models scale in size and complexity, diagnosing their faults becomes increasingly challenging. In this work, we conduct an empirical study to evaluate the potential of Hessian-based analysis for diagnosing faults in attention-based models. Specifically, we use Hessian-derived insights to identify fragile regions (via curvature analysis) and parameter interdependencies (via parameter interaction analysis) within attention mechanisms. Through experiments on three diverse models (HAN, 3D-CNN, DistilBERT), we show that Hessian-based metrics can localize instability and pinpoint fault sources more effectively than gradients alone. Our empirical findings suggest that these metrics could significantly improve fault diagnosis in complex neural architectures, potentially improving software debugging practices.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Counterfactual Generation with Semantic Abduction</title>
<link>https://arxiv.org/abs/2506.07883</link>
<guid>https://arxiv.org/abs/2506.07883</guid>
<content:encoded><![CDATA[
arXiv:2506.07883v1 Announce Type: new 
Abstract: Counterfactual image generation presents significant challenges, including preserving identity, maintaining perceptual quality, and ensuring faithfulness to an underlying causal model. While existing auto-encoding frameworks admit semantic latent spaces which can be manipulated for causal control, they struggle with scalability and fidelity. Advancements in diffusion models present opportunities for improving counterfactual image editing, having demonstrated state-of-the-art visual quality, human-aligned perception and representation learning capabilities. Here, we present a suite of diffusion-based causal mechanisms, introducing the notions of spatial, semantic and dynamic abduction. We propose a general framework that integrates semantic representations into diffusion models through the lens of Pearlian causality to edit images via a counterfactual reasoning process. To our knowledge, this is the first work to consider high-level semantic identity preservation for diffusion counterfactuals and to demonstrate how semantic control enables principled trade-offs between faithful causal control and identity preservation.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Schauder Bases for $C[0, 1]$ Using ReLU, Softplus and Two Sigmoidal Functions</title>
<link>https://arxiv.org/abs/2506.07884</link>
<guid>https://arxiv.org/abs/2506.07884</guid>
<content:encoded><![CDATA[
arXiv:2506.07884v1 Announce Type: new 
Abstract: We construct four Schauder bases for the space $C[0,1]$, one using ReLU functions, another using Softplus functions, and two more using sigmoidal versions of the ReLU and Softplus functions. This establishes the existence of a basis using these functions for the first time, and improves on the universal approximation property associated with them.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FunDiff: Diffusion Models over Function Spaces for Physics-Informed Generative Modeling</title>
<link>https://arxiv.org/abs/2506.07902</link>
<guid>https://arxiv.org/abs/2506.07902</guid>
<content:encoded><![CDATA[
arXiv:2506.07902v1 Announce Type: new 
Abstract: Recent advances in generative modeling -- particularly diffusion models and flow matching -- have achieved remarkable success in synthesizing discrete data such as images and videos. However, adapting these models to physical applications remains challenging, as the quantities of interest are continuous functions governed by complex physical laws. Here, we introduce $\textbf{FunDiff}$, a novel framework for generative modeling in function spaces. FunDiff combines a latent diffusion process with a function autoencoder architecture to handle input functions with varying discretizations, generate continuous functions evaluable at arbitrary locations, and seamlessly incorporate physical priors. These priors are enforced through architectural constraints or physics-informed loss functions, ensuring that generated samples satisfy fundamental physical laws. We theoretically establish minimax optimality guarantees for density estimation in function spaces, showing that diffusion-based estimators achieve optimal convergence rates under suitable regularity conditions. We demonstrate the practical effectiveness of FunDiff across diverse applications in fluid dynamics and solid mechanics. Empirical results show that our method generates physically consistent samples with high fidelity to the target distribution and exhibits robustness to noisy and low-resolution data. Code and datasets are publicly available at https://github.com/sifanexisted/fundiff.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffuse Everything: Multimodal Diffusion Models on Arbitrary State Spaces</title>
<link>https://arxiv.org/abs/2506.07903</link>
<guid>https://arxiv.org/abs/2506.07903</guid>
<content:encoded><![CDATA[
arXiv:2506.07903v1 Announce Type: new 
Abstract: Diffusion models have demonstrated remarkable performance in generating unimodal data across various tasks, including image, video, and text generation. On the contrary, the joint generation of multimodal data through diffusion models is still in the early stages of exploration. Existing approaches heavily rely on external preprocessing protocols, such as tokenizers and variational autoencoders, to harmonize varied data representations into a unified, unimodal format. This process heavily demands the high accuracy of encoders and decoders, which can be problematic for applications with limited data. To lift this restriction, we propose a novel framework for building multimodal diffusion models on arbitrary state spaces, enabling native generation of coupled data across different modalities. By introducing an innovative decoupled noise schedule for each modality, we enable both unconditional and modality-conditioned generation within a single model simultaneously. We empirically validate our approach for text-image generation and mixed-type tabular data synthesis, demonstrating that it achieves competitive performance.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CausalPFN: Amortized Causal Effect Estimation via In-Context Learning</title>
<link>https://arxiv.org/abs/2506.07918</link>
<guid>https://arxiv.org/abs/2506.07918</guid>
<content:encoded><![CDATA[
arXiv:2506.07918v1 Announce Type: new 
Abstract: Causal effect estimation from observational data is fundamental across various applications. However, selecting an appropriate estimator from dozens of specialized methods demands substantial manual effort and domain expertise. We present CausalPFN, a single transformer that amortizes this workflow: trained once on a large library of simulated data-generating processes that satisfy ignorability, it infers causal effects for new observational datasets out-of-the-box. CausalPFN combines ideas from Bayesian causal inference with the large-scale training protocol of prior-fitted networks (PFNs), learning to map raw observations directly to causal effects without any task-specific adjustment. Our approach achieves superior average performance on heterogeneous and average treatment effect estimation benchmarks (IHDP, Lalonde, ACIC). Moreover, it shows competitive performance for real-world policy making on uplift modeling tasks. CausalPFN provides calibrated uncertainty estimates to support reliable decision-making based on Bayesian principles. This ready-to-use model does not require any further training or tuning and takes a step toward automated causal inference (https://github.com/vdblm/CausalPFN).
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering the Functional Roles of Nonlinearity in Memory</title>
<link>https://arxiv.org/abs/2506.07919</link>
<guid>https://arxiv.org/abs/2506.07919</guid>
<content:encoded><![CDATA[
arXiv:2506.07919v1 Announce Type: new 
Abstract: Memory and long-range temporal processing are core requirements for sequence modeling tasks across natural language processing, time-series forecasting, speech recognition, and control. While nonlinear recurrence has long been viewed as essential for enabling such mechanisms, recent work suggests that linear dynamics may often suffice. In this study, we go beyond performance comparisons to systematically dissect the functional role of nonlinearity in recurrent networks--identifying both when it is computationally necessary, and what mechanisms it enables. We use Almost Linear Recurrent Neural Networks (AL-RNNs), which allow fine-grained control over nonlinearity, as both a flexible modeling tool and a probe into the internal mechanisms of memory. Across a range of classic sequence modeling tasks and a real-world stimulus selection task, we find that minimal nonlinearity is not only sufficient but often optimal, yielding models that are simpler, more robust, and more interpretable than their fully nonlinear or linear counterparts. Our results provide a principled framework for selectively introducing nonlinearity, bridging dynamical systems theory with the functional demands of long-range memory and structured computation in recurrent neural networks, with implications for both artificial and biological neural systems.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>W4S4: WaLRUS Meets S4 for Long-Range Sequence Modeling</title>
<link>https://arxiv.org/abs/2506.07920</link>
<guid>https://arxiv.org/abs/2506.07920</guid>
<content:encoded><![CDATA[
arXiv:2506.07920v1 Announce Type: new 
Abstract: State Space Models (SSMs) have emerged as powerful components for sequence modeling, enabling efficient handling of long-range dependencies via linear recurrence and convolutional computation. However, their effectiveness depends heavily on the choice and initialization of the state matrix. In this work, we build on the SaFARi framework and existing WaLRUS SSMs to introduce a new variant, W4S4 (WaLRUS for S4), a new class of SSMs constructed from redundant wavelet frames. WaLRUS admits a stable diagonalization and supports fast kernel computation without requiring low-rank approximations, making it both theoretically grounded and computationally efficient. We show that WaLRUS retains information over long horizons significantly better than HiPPO-based SSMs, both in isolation and when integrated into deep architectures such as S4. Our experiments demonstrate consistent improvements across delay reconstruction tasks, classification benchmarks, and long-range sequence modeling, confirming that high-quality, structured initialization enabled by wavelet-based state dynamic offers substantial advantages over existing alternatives. WaLRUS provides a scalable and versatile foundation for the next generation of deep SSM-based models.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Generative Physics-Informed Reinforcement Learning-Based Approach for Construction of Representative Drive Cycle</title>
<link>https://arxiv.org/abs/2506.07929</link>
<guid>https://arxiv.org/abs/2506.07929</guid>
<content:encoded><![CDATA[
arXiv:2506.07929v1 Announce Type: new 
Abstract: Accurate driving cycle construction is crucial for vehicle design, fuel economy analysis, and environmental impact assessments. A generative Physics-Informed Expected SARSA-Monte Carlo (PIESMC) approach that constructs representative driving cycles by capturing transient dynamics, acceleration, deceleration, idling, and road grade transitions while ensuring model fidelity is introduced. Leveraging a physics-informed reinforcement learning framework with Monte Carlo sampling, PIESMC delivers efficient cycle construction with reduced computational cost. Experimental evaluations on two real-world datasets demonstrate that PIESMC replicates key kinematic and energy metrics, achieving up to a 57.3% reduction in cumulative kinematic fragment errors compared to the Micro-trip-based (MTB) method and a 10.5% reduction relative to the Markov-chain-based (MCB) method. Moreover, it is nearly an order of magnitude faster than conventional techniques. Analyses of vehicle-specific power distributions and wavelet-transformed frequency content further confirm its ability to reproduce experimental central tendencies and variability.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ensemble-Based Survival Models with the Self-Attended Beran Estimator Predictions</title>
<link>https://arxiv.org/abs/2506.07933</link>
<guid>https://arxiv.org/abs/2506.07933</guid>
<content:encoded><![CDATA[
arXiv:2506.07933v1 Announce Type: new 
Abstract: Survival analysis predicts the time until an event of interest, such as failure or death, but faces challenges due to censored data, where some events remain unobserved. Ensemble-based models, like random survival forests and gradient boosting, are widely used but can produce unstable predictions due to variations in bootstrap samples. To address this, we propose SurvBESA (Survival Beran Estimators Self-Attended), a novel ensemble model that combines Beran estimators with a self-attention mechanism. Unlike traditional methods, SurvBESA applies self-attention to predicted survival functions, smoothing out noise by adjusting each survival function based on its similarity to neighboring survival functions. We also explore a special case using Huber's contamination model to define attention weights, simplifying training to a quadratic or linear optimization problem. Numerical experiments show that SurvBESA outperforms state-of-the-art models. The implementation of SurvBESA is publicly available.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TokenBreak: Bypassing Text Classification Models Through Token Manipulation</title>
<link>https://arxiv.org/abs/2506.07948</link>
<guid>https://arxiv.org/abs/2506.07948</guid>
<content:encoded><![CDATA[
arXiv:2506.07948v1 Announce Type: new 
Abstract: Natural Language Processing (NLP) models are used for text-related tasks such as classification and generation. To complete these tasks, input data is first tokenized from human-readable text into a format the model can understand, enabling it to make inferences and understand context. Text classification models can be implemented to guard against threats such as prompt injection attacks against Large Language Models (LLMs), toxic input and cybersecurity risks such as spam emails. In this paper, we introduce TokenBreak: a novel attack that can bypass these protection models by taking advantage of the tokenization strategy they use. This attack technique manipulates input text in such a way that certain models give an incorrect classification. Importantly, the end target (LLM or email recipient) can still understand and respond to the manipulated text and therefore be vulnerable to the very attack the protection model was put in place to prevent. The tokenizer is tied to model architecture, meaning it is possible to predict whether or not a model is vulnerable to attack based on family. We also present a defensive strategy as an added layer of protection that can be implemented without having to retrain the defensive model.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cost-Optimal Active AI Model Evaluation</title>
<link>https://arxiv.org/abs/2506.07949</link>
<guid>https://arxiv.org/abs/2506.07949</guid>
<content:encoded><![CDATA[
arXiv:2506.07949v1 Announce Type: new 
Abstract: The development lifecycle of generative AI systems requires continual evaluation, data acquisition, and annotation, which is costly in both resources and time. In practice, rapid iteration often makes it necessary to rely on synthetic annotation data because of the low cost, despite the potential for substantial bias. In this paper, we develop novel, cost-aware methods for actively balancing the use of a cheap, but often inaccurate, weak rater -- such as a model-based autorater that is designed to automatically assess the quality of generated content -- with a more expensive, but also more accurate, strong rater alternative such as a human. More specifically, the goal of our approach is to produce a low variance, unbiased estimate of the mean of the target "strong" rating, subject to some total annotation budget. Building on recent work in active and prediction-powered statistical inference, we derive a family of cost-optimal policies for allocating a given annotation budget between weak and strong raters so as to maximize statistical efficiency. Using synthetic and real-world data, we empirically characterize the conditions under which these policies yield improvements over prior methods. We find that, especially in tasks where there is high variability in the difficulty of examples, our policies can achieve the same estimation precision at a far lower total annotation budget than standard evaluation methods.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Tangent Kernel Analysis to Probe Convergence in Physics-informed Neural Solvers: PIKANs vs. PINNs</title>
<link>https://arxiv.org/abs/2506.07958</link>
<guid>https://arxiv.org/abs/2506.07958</guid>
<content:encoded><![CDATA[
arXiv:2506.07958v1 Announce Type: new 
Abstract: Physics-informed Kolmogorov-Arnold Networks (PIKANs), and in particular their Chebyshev-based variants (cPIKANs), have recently emerged as promising models for solving partial differential equations (PDEs). However, their training dynamics and convergence behavior remain largely unexplored both theoretically and numerically. In this work, we aim to advance the theoretical understanding of cPIKANs by analyzing them using Neural Tangent Kernel (NTK) theory. Our objective is to discern the evolution of kernel structure throughout gradient-based training and its subsequent impact on learning efficiency. We first derive the NTK of standard cKANs in a supervised setting, and then extend the analysis to the physics-informed context. We analyze the spectral properties of NTK matrices, specifically their eigenvalue distributions and spectral bias, for four representative PDEs: the steady-state Helmholtz equation, transient diffusion and Allen-Cahn equations, and forced vibrations governed by the Euler-Bernoulli beam equation. We also conduct an investigation into the impact of various optimization strategies, e.g., first-order, second-order, and hybrid approaches, on the evolution of the NTK and the resulting learning dynamics. Results indicate a tractable behavior for NTK in the context of cPIKANs, which exposes learning dynamics that standard physics-informed neural networks (PINNs) cannot capture. Spectral trends also reveal when domain decomposition improves training, directly linking kernel behavior to convergence rates under different setups. To the best of our knowledge, this is the first systematic NTK study of cPIKANs, providing theoretical insight that clarifies and predicts their empirical performance.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Two-Phase Deep Learning Framework for Adaptive Time-Stepping in High-Speed Flow Modeling</title>
<link>https://arxiv.org/abs/2506.07969</link>
<guid>https://arxiv.org/abs/2506.07969</guid>
<content:encoded><![CDATA[
arXiv:2506.07969v1 Announce Type: new 
Abstract: We consider the problem of modeling high-speed flows using machine learning methods. While most prior studies focus on low-speed fluid flows in which uniform time-stepping is practical, flows approaching and exceeding the speed of sound exhibit sudden changes such as shock waves. In such cases, it is essential to use adaptive time-stepping methods to allow a temporal resolution sufficient to resolve these phenomena while simultaneously balancing computational costs. Here, we propose a two-phase machine learning method, known as ShockCast, to model high-speed flows with adaptive time-stepping. In the first phase, we propose to employ a machine learning model to predict the timestep size. In the second phase, the predicted timestep is used as an input along with the current fluid fields to advance the system state by the predicted timestep. We explore several physically-motivated components for timestep prediction and introduce timestep conditioning strategies inspired by neural ODE and Mixture of Experts. As ShockCast is the first framework for learning high-speed flows, we evaluate our methods by generating two supersonic flow datasets, available at https://huggingface.co/datasets/divelab. Our code is publicly available as part of the AIRS library (https://github.com/divelab/AIRS).
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization</title>
<link>https://arxiv.org/abs/2506.07972</link>
<guid>https://arxiv.org/abs/2506.07972</guid>
<content:encoded><![CDATA[
arXiv:2506.07972v1 Announce Type: new 
Abstract: While Large Language Models (LLMs) have demonstrated significant advancements in reasoning and agent-based problem-solving, current evaluation methodologies fail to adequately assess their capabilities: existing benchmarks either rely on closed-ended questions prone to saturation and memorization, or subjective comparisons that lack consistency and rigor. In this work, we introduce HeuriGym, an agentic framework designed for evaluating heuristic algorithms generated by LLMs for combinatorial optimization problems, characterized by clearly defined objectives and expansive solution spaces. HeuriGym empowers LLMs to propose heuristics, receive evaluative feedback via code execution, and iteratively refine their solutions. We evaluate nine state-of-the-art models on nine problems across domains such as computer systems, logistics, and biology, exposing persistent limitations in tool use, planning, and adaptive reasoning. To quantify performance, we propose the Quality-Yield Index (QYI), a metric that captures both solution pass rate and quality. Even top models like GPT-o4-mini-high and Gemini-2.5-Pro attain QYI scores of only 0.6, well below the expert baseline of 1. Our open-source benchmark aims to guide the development of LLMs toward more effective and realistic problem-solving in scientific and engineering domains.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hyperpruning: Efficient Search through Pruned Variants of Recurrent Neural Networks Leveraging Lyapunov Spectrum</title>
<link>https://arxiv.org/abs/2506.07975</link>
<guid>https://arxiv.org/abs/2506.07975</guid>
<content:encoded><![CDATA[
arXiv:2506.07975v1 Announce Type: new 
Abstract: A variety of pruning methods have been introduced for over-parameterized Recurrent Neural Networks to improve efficiency in terms of power consumption and storage utilization. These advances motivate a new paradigm, termed `hyperpruning', which seeks to identify the most suitable pruning strategy for a given network architecture and application. Unlike conventional hyperparameter search, where the optimal configuration's accuracy remains uncertain, in the context of network pruning, the accuracy of the dense model sets the target for the accuracy of the pruned one. The goal, therefore, is to discover pruned variants that match or even surpass this established accuracy. However, exhaustive search over pruning configurations is computationally expensive and lacks early performance guarantees. To address this challenge, we propose a novel Lyapunov Spectrum (LS)-based distance metric that enables early comparison between pruned and dense networks, allowing accurate prediction of post-training performance. By integrating this LS-based distance with standard hyperparameter optimization algorithms, we introduce an efficient hyperpruning framework, termed LS-based Hyperpruning (LSH). LSH reduces search time by an order of magnitude compared to conventional approaches relying on full training. Experiments on stacked LSTM and RHN architectures using the Penn Treebank dataset, and on AWD-LSTM-MoS using WikiText-2, demonstrate that under fixed training budgets and target pruning ratios, LSH consistently identifies superior pruned models. Remarkably, these pruned variants not only outperform those selected by loss-based baseline but also exceed the performance of their dense counterpart.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction</title>
<link>https://arxiv.org/abs/2506.07976</link>
<guid>https://arxiv.org/abs/2506.07976</guid>
<content:encoded><![CDATA[
arXiv:2506.07976v1 Announce Type: new 
Abstract: The current paradigm of test-time scaling relies on generating long reasoning traces ("thinking" more) before producing a response. In agent problems that require interaction, this can be done by generating thinking traces before acting in the world. However, this process does not allow agents to acquire new information from the environment or adapt their behavior over time. In this work, we propose to scale test-time interaction, an untapped dimension of test-time scaling that increases the agent's interaction horizon to enable running rich behaviors such as exploration, backtracking, and dynamic re-planning within a single rollout. To demonstrate the promise of this scaling dimension, we study the domain of web agents. We first show that even prompting-based interaction scaling without any training can improve task success on web benchmarks non-trivially. Building on this, we introduce TTI (Test-Time Interaction), a curriculum-based online reinforcement learning (RL) approach that trains agents by adaptively adjusting their rollout lengths. Using a Gemma 3 12B model, TTI produces state-of-the-art open-source, open-data web agents on WebVoyager and WebArena benchmarks. We further show that TTI enables agents to balance exploration and exploitation adaptively. Our results establish interaction scaling as a powerful, complementary axis to scaling per-step compute, offering new avenues for training adaptive agents.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Realistic Urban Traffic Generator using Decentralized Federated Learning for the SUMO simulator</title>
<link>https://arxiv.org/abs/2506.07980</link>
<guid>https://arxiv.org/abs/2506.07980</guid>
<content:encoded><![CDATA[
arXiv:2506.07980v1 Announce Type: new 
Abstract: Realistic urban traffic simulation is essential for sustainable urban planning and the development of intelligent transportation systems. However, generating high-fidelity, time-varying traffic profiles that accurately reflect real-world conditions, especially in large-scale scenarios, remains a major challenge. Existing methods often suffer from limitations in accuracy, scalability, or raise privacy concerns due to centralized data processing. This work introduces DesRUTGe (Decentralized Realistic Urban Traffic Generator), a novel framework that integrates Deep Reinforcement Learning (DRL) agents with the SUMO simulator to generate realistic 24-hour traffic patterns. A key innovation of DesRUTGe is its use of Decentralized Federated Learning (DFL), wherein each traffic detector and its corresponding urban zone function as an independent learning node. These nodes train local DRL models using minimal historical data and collaboratively refine their performance by exchanging model parameters with selected peers (e.g., geographically adjacent zones), without requiring a central coordinator. Evaluated using real-world data from the city of Barcelona, DesRUTGe outperforms standard SUMO-based tools such as RouteSampler, as well as other centralized learning approaches, by delivering more accurate and privacy-preserving traffic pattern generation.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Modeling of Weights: Generalization or Memorization?</title>
<link>https://arxiv.org/abs/2506.07998</link>
<guid>https://arxiv.org/abs/2506.07998</guid>
<content:encoded><![CDATA[
arXiv:2506.07998v1 Announce Type: new 
Abstract: Generative models, with their success in image and video generation, have recently been explored for synthesizing effective neural network weights. These approaches take trained neural network checkpoints as training data, and aim to generate high-performing neural network weights during inference. In this work, we examine four representative methods on their ability to generate novel model weights, i.e., weights that are different from the checkpoints seen during training. Surprisingly, we find that these methods synthesize weights largely by memorization: they produce either replicas, or at best simple interpolations, of the training checkpoints. Current methods fail to outperform simple baselines, such as adding noise to the weights or taking a simple weight ensemble, in obtaining different and simultaneously high-performing models. We further show that this memorization cannot be effectively mitigated by modifying modeling factors commonly associated with memorization in image diffusion models, or applying data augmentations. Our findings provide a realistic assessment of what types of data current generative models can model, and highlight the need for more careful evaluation of generative models in new domains. Our code is available at https://github.com/boyazeng/weight_memorization.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reparameterized LLM Training via Orthogonal Equivalence Transformation</title>
<link>https://arxiv.org/abs/2506.08001</link>
<guid>https://arxiv.org/abs/2506.08001</guid>
<content:encoded><![CDATA[
arXiv:2506.08001v1 Announce Type: new 
Abstract: While large language models (LLMs) are driving the rapid advancement of artificial intelligence, effectively and reliably training these large models remains one of the field's most significant challenges. To address this challenge, we propose POET, a novel reParameterized training algorithm that uses Orthogonal Equivalence Transformation to optimize neurons. Specifically, POET reparameterizes each neuron with two learnable orthogonal matrices and a fixed random weight matrix. Because of its provable preservation of spectral properties of weight matrices, POET can stably optimize the objective function with improved generalization. We further develop efficient approximations that make POET flexible and scalable for training large-scale neural networks. Extensive experiments validate the effectiveness and scalability of POET in training LLMs.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Amatriciana: Exploiting Temporal GNNs for Robust and Efficient Money Laundering Detection</title>
<link>https://arxiv.org/abs/2506.00654</link>
<guid>https://arxiv.org/abs/2506.00654</guid>
<content:encoded><![CDATA[
arXiv:2506.00654v1 Announce Type: cross 
Abstract: Money laundering is a financial crime that poses a serious threat to financial integrity and social security. The growing number of transactions makes it necessary to use automatic tools that help law enforcement agencies detect such criminal activity. In this work, we present Amatriciana, a novel approach based on Graph Neural Networks to detect money launderers inside a graph of transactions by considering temporal information. Amatriciana uses the whole graph of transactions without splitting it into several time-based subgraphs, exploiting all relational information in the dataset. Our experiments on a public dataset reveal that the model can learn from a limited amount of data. Furthermore, when more data is available, the model outperforms other State-of-the-art approaches; in particular, Amatriciana decreases the number of False Positives (FPs) while detecting many launderers. In summary, Amatriciana achieves an F1 score of 0.76. In addition, it lowers the FPs by 55% with respect to other State-of-the-art models.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STARFlow: Scaling Latent Normalizing Flows for High-resolution Image Synthesis</title>
<link>https://arxiv.org/abs/2506.06276</link>
<guid>https://arxiv.org/abs/2506.06276</guid>
<content:encoded><![CDATA[
arXiv:2506.06276v1 Announce Type: cross 
Abstract: We present STARFlow, a scalable generative model based on normalizing flows that achieves strong performance in high-resolution image synthesis. The core of STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the expressive power of normalizing flows with the structured modeling capabilities of Autoregressive Transformers. We first establish the theoretical universality of TARFlow for modeling continuous distributions. Building on this foundation, we introduce several key architectural and algorithmic innovations to significantly enhance scalability: (1) a deep-shallow design, wherein a deep Transformer block captures most of the model representational capacity, complemented by a few shallow Transformer blocks that are computationally efficient yet substantially beneficial; (2) modeling in the latent space of pretrained autoencoders, which proves more effective than direct pixel-level modeling; and (3) a novel guidance algorithm that significantly boosts sample quality. Crucially, our model remains an end-to-end normalizing flow, enabling exact maximum likelihood training in continuous spaces without discretization. STARFlow achieves competitive performance in both class-conditional and text-conditional image generation tasks, approaching state-of-the-art diffusion models in sample quality. To our knowledge, this work is the first successful demonstration of normalizing flows operating effectively at this scale and resolution.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DELPHYNE: A Pre-Trained Model for General and Financial Time Series</title>
<link>https://arxiv.org/abs/2506.06288</link>
<guid>https://arxiv.org/abs/2506.06288</guid>
<content:encoded><![CDATA[
arXiv:2506.06288v1 Announce Type: cross 
Abstract: Time-series data is a vital modality within data science communities. This is particularly valuable in financial applications, where it helps in detecting patterns, understanding market behavior, and making informed decisions based on historical data. Recent advances in language modeling have led to the rise of time-series pre-trained models that are trained on vast collections of datasets and applied to diverse tasks across financial domains. However, across financial applications, existing time-series pre-trained models have not shown boosts in performance over simple finance benchmarks in both zero-shot and fine-tuning settings. This phenomenon occurs because of a i) lack of financial data within the pre-training stage, and ii) the negative transfer effect due to inherently different time-series patterns across domains. Furthermore, time-series data is continuous, noisy, and can be collected at varying frequencies and with varying lags across different variables, making this data more challenging to model than languages. To address the above problems, we introduce a Pre-trained MoDEL for FINance TimE-series (Delphyne). Delphyne achieves competitive performance to existing foundation and full-shot models with few fine-tuning steps on publicly available datasets, and also shows superior performances on various financial tasks.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Malicious AI Swarms Can Threaten Democracy</title>
<link>https://arxiv.org/abs/2506.06299</link>
<guid>https://arxiv.org/abs/2506.06299</guid>
<content:encoded><![CDATA[
arXiv:2506.06299v1 Announce Type: cross 
Abstract: Advances in AI portend a new era of sophisticated disinformation operations. While individual AI systems already create convincing -- and at times misleading -- information, an imminent development is the emergence of malicious AI swarms. These systems can coordinate covertly, infiltrate communities, evade traditional detectors, and run continuous A/B tests, with round-the-clock persistence. The result can include fabricated grassroots consensus, fragmented shared reality, mass harassment, voter micro-suppression or mobilization, contamination of AI training data, and erosion of institutional trust. With democratic processes worldwide increasingly vulnerable, we urge a three-pronged response: (1) platform-side defenses -- always-on swarm-detection dashboards, pre-election high-fidelity swarm-simulation stress-tests, transparency audits, and optional client-side "AI shields" for users; (2) model-side safeguards -- standardized persuasion-risk tests, provenance-authenticating passkeys, and watermarking; and (3) system-level oversight -- a UN-backed AI Influence Observatory.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Template-Guided 3D Molecular Pose Generation via Flow Matching and Differentiable Optimization</title>
<link>https://arxiv.org/abs/2506.06305</link>
<guid>https://arxiv.org/abs/2506.06305</guid>
<content:encoded><![CDATA[
arXiv:2506.06305v1 Announce Type: cross 
Abstract: Predicting the 3D conformation of small molecules within protein binding sites is a key challenge in drug design. When a crystallized reference ligand (template) is available, it provides geometric priors that can guide 3D pose prediction. We present a two-stage method for ligand conformation generation guided by such templates. In the first stage, we introduce a molecular alignment approach based on flow-matching to generate 3D coordinates for the ligand, using the template structure as a reference. In the second stage, a differentiable pose optimization procedure refines this conformation based on shape and pharmacophore similarities, internal energy, and, optionally, the protein binding pocket. We evaluate our approach on a new benchmark of ligand pairs co-crystallized with the same target and show that it outperforms standard docking tools and open-access alignment methods, especially in cases involving low similarity to the template or high ligand flexibility.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Early Agitation Prediction in Community-Dwelling People with Dementia Using Multimodal Sensors and Machine Learning</title>
<link>https://arxiv.org/abs/2506.06306</link>
<guid>https://arxiv.org/abs/2506.06306</guid>
<content:encoded><![CDATA[
arXiv:2506.06306v1 Announce Type: cross 
Abstract: Agitation is one of the most common responsive behaviors in people living with dementia, particularly among those residing in community settings without continuous clinical supervision. Timely prediction of agitation can enable early intervention, reduce caregiver burden, and improve the quality of life for both patients and caregivers. This study aimed to develop and benchmark machine learning approaches for the early prediction of agitation in community-dwelling older adults with dementia using multimodal sensor data. A new set of agitation-related contextual features derived from activity data was introduced and employed for agitation prediction. A wide range of machine learning and deep learning models was evaluated across multiple problem formulations, including binary classification for single-timestamp tabular sensor data and multi-timestamp sequential sensor data, as well as anomaly detection for single-timestamp tabular sensor data. The study utilized the Technology Integrated Health Management (TIHM) dataset, the largest publicly available dataset for remote monitoring of people living with dementia, comprising 2,803 days of in-home activity, physiology, and sleep data. The most effective setting involved binary classification of sensor data using the current 6-hour timestamp to predict agitation at the subsequent timestamp. Incorporating additional information, such as time of day and agitation history, further improved model performance, with the highest AUC-ROC of 0.9720 and AUC-PR of 0.4320 achieved by the light gradient boosting machine. This work presents the first comprehensive benchmarking of state-of-the-art techniques for agitation prediction in community-based dementia care using privacy-preserving sensor data. The approach enables accurate, explainable, and efficient agitation prediction, supporting proactive dementia care and aging in place.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scientific machine learning in Hydrology: a unified perspective</title>
<link>https://arxiv.org/abs/2506.06308</link>
<guid>https://arxiv.org/abs/2506.06308</guid>
<content:encoded><![CDATA[
arXiv:2506.06308v1 Announce Type: cross 
Abstract: Scientific machine learning (SciML) provides a structured approach to integrating physical knowledge into data-driven modeling, offering significant potential for advancing hydrological research. In recent years, multiple methodological families have emerged, including physics-informed machine learning, physics-guided machine learning, hybrid physics-machine learning, and data-driven physics discovery. Within each of these families, a proliferation of heterogeneous approaches has developed independently, often without conceptual coordination. This fragmentation complicates the assessment of methodological novelty and makes it difficult to identify where meaningful advances can still be made in the absence of a unified conceptual framework. This review, the first focused overview of SciML in hydrology, addresses these limitations by proposing a unified methodological framework for each SciML family, bringing together representative contributions into a coherent structure that fosters conceptual clarity and supports cumulative progress in hydrological modeling. Finally, we highlight the limitations and future opportunities of each unified family to guide systematic research in hydrology, where these methods remain underutilized.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Novel Ensemble Learning Techniques and Landsat Multispectral Data for Estimating Olive Yields in Tunisia</title>
<link>https://arxiv.org/abs/2506.06309</link>
<guid>https://arxiv.org/abs/2506.06309</guid>
<content:encoded><![CDATA[
arXiv:2506.06309v1 Announce Type: cross 
Abstract: Olive production is an important tree crop in Mediterranean climates. However, olive yield varies significantly due to climate change. Accurately estimating yield using remote sensing and machine learning remains a complex challenge. In this study, we developed a streamlined pipeline for olive yield estimation in the Kairouan and Sousse governorates of Tunisia. We extracted features from multispectral reflectance bands, vegetation indices derived from Landsat-8 OLI and Landsat-9 OLI-2 satellite imagery, along with digital elevation model data. These spatial features were combined with ground-based field survey data to form a structured tabular dataset. We then developed an automated ensemble learning framework, implemented using AutoGluon to train and evaluate multiple machine learning models, select optimal combinations through stacking, and generate robust yield predictions using five-fold cross-validation. The results demonstrate strong predictive performance from both sensors, with Landsat-8 OLI achieving R2 = 0.8635 and RMSE = 1.17 tons per ha, and Landsat-9 OLI-2 achieving R2 = 0.8378 and RMSE = 1.32 tons per ha. This study highlights a scalable, cost-effective, and accurate method for olive yield estimation, with potential applicability across diverse agricultural regions globally.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Contrastive Learning-based Electrocardiogram Pretrained Model with Patient Memory Queue</title>
<link>https://arxiv.org/abs/2506.06310</link>
<guid>https://arxiv.org/abs/2506.06310</guid>
<content:encoded><![CDATA[
arXiv:2506.06310v1 Announce Type: cross 
Abstract: In the field of automatic Electrocardiogram (ECG) diagnosis, due to the relatively limited amount of labeled data, how to build a robust ECG pretrained model based on unlabeled data is a key area of focus for researchers. Recent advancements in contrastive learning-based ECG pretrained models highlight the potential of exploiting the additional patient-level self-supervisory signals inherent in ECG. They are referred to as patient contrastive learning. Its rationale is that multiple physical recordings from the same patient may share commonalities, termed patient consistency, so redefining positive and negative pairs in contrastive learning as intrapatient and inter-patient samples provides more shared context to learn an effective representation. However, these methods still fail to efficiently exploit patient consistency due to the insufficient amount of intra-inter patient samples existing in a batch. Hence, we propose a contrastive learning-based ECG pretrained model enhanced by the Patient Memory Queue (PMQ), which incorporates a large patient memory queue to mitigate model degeneration that can arise from insufficient intra-inter patient samples. In order to further enhance the performance of the pretrained model, we introduce two extra data augmentation methods to provide more perspectives of positive and negative pairs for pretraining. Extensive experiments were conducted on three public datasets with three different data ratios. The experimental results show that the comprehensive performance of our method outperforms previous contrastive learning methods and exhibits greater robustness in scenarios with limited labeled data. The code is available at https://github.com/3hiuwoo/PMQ.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Novel Shape-Aware Topological Representation for GPR Data with DNN Integration</title>
<link>https://arxiv.org/abs/2506.06311</link>
<guid>https://arxiv.org/abs/2506.06311</guid>
<content:encoded><![CDATA[
arXiv:2506.06311v1 Announce Type: cross 
Abstract: Ground Penetrating Radar (GPR) is a widely used Non-Destructive Testing (NDT) technique for subsurface exploration, particularly in infrastructure inspection and maintenance. However, conventional interpretation methods are often limited by noise sensitivity and a lack of structural awareness. This study presents a novel framework that enhances the detection of underground utilities, especially pipelines, by integrating shape-aware topological features derived from B-scan GPR images using Topological Data Analysis (TDA), with the spatial detection capabilities of the YOLOv5 deep neural network (DNN). We propose a novel shape-aware topological representation that amplifies structural features in the input data, thereby improving the model's responsiveness to the geometrical features of buried objects. To address the scarcity of annotated real-world data, we employ a Sim2Real strategy that generates diverse and realistic synthetic datasets, effectively bridging the gap between simulated and real-world domains. Experimental results demonstrate significant improvements in mean Average Precision (mAP), validating the robustness and efficacy of our approach. This approach underscores the potential of TDA-enhanced learning in achieving reliable, real-time subsurface object detection, with broad applications in urban planning, safety inspection, and infrastructure management.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Open-Source Python Framework and Synthetic ECG Image Datasets for Digitization, Lead and Lead Name Detection, and Overlapping Signal Segmentation</title>
<link>https://arxiv.org/abs/2506.06315</link>
<guid>https://arxiv.org/abs/2506.06315</guid>
<content:encoded><![CDATA[
arXiv:2506.06315v1 Announce Type: cross 
Abstract: We introduce an open-source Python framework for generating synthetic ECG image datasets to advance critical deep learning-based tasks in ECG analysis, including ECG digitization, lead region and lead name detection, and pixel-level waveform segmentation. Using the PTB-XL signal dataset, our proposed framework produces four open-access datasets: (1) ECG images in various lead configurations paired with time-series signals for ECG digitization, (2) ECG images annotated with YOLO-format bounding boxes for detection of lead region and lead name, (3)-(4) cropped single-lead images with segmentation masks compatible with U-Net-based models in normal and overlapping versions. In the overlapping case, waveforms from neighboring leads are superimposed onto the target lead image, while the segmentation masks remain clean. The open-source Python framework and datasets are publicly available at https://github.com/rezakarbasi/ecg-image-and-signal-dataset and https://doi.org/10.5281/zenodo.15484519, respectively.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Composite Reward Design in PPO-Driven Adaptive Filtering</title>
<link>https://arxiv.org/abs/2506.06323</link>
<guid>https://arxiv.org/abs/2506.06323</guid>
<content:encoded><![CDATA[
arXiv:2506.06323v1 Announce Type: cross 
Abstract: Model-free and reinforcement learning-based adaptive filtering methods are gaining traction for denoising in dynamic, non-stationary environments such as wireless signal channels. Traditional filters like LMS, RLS, Wiener, and Kalman are limited by assumptions of stationary or requiring complex fine-tuning or exact noise statistics or fixed models. This letter proposes an adaptive filtering framework using Proximal Policy Optimization (PPO), guided by a composite reward that balances SNR improvement, MSE reduction, and residual smoothness. Experiments on synthetic signals with various noise types show that our PPO agent generalizes beyond its training distribution, achieving real-time performance and outperforming classical filters. This work demonstrates the viability of policy-gradient reinforcement learning for robust, low-latency adaptive signal filtering.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Introduction to Predictive Coding Networks for Machine Learning</title>
<link>https://arxiv.org/abs/2506.06332</link>
<guid>https://arxiv.org/abs/2506.06332</guid>
<content:encoded><![CDATA[
arXiv:2506.06332v1 Announce Type: cross 
Abstract: Predictive coding networks (PCNs) constitute a biologically inspired framework for understanding hierarchical computation in the brain, and offer an alternative to traditional feedforward neural networks in ML. This note serves as a quick, onboarding introduction to PCNs for machine learning practitioners. We cover the foundational network architecture, inference and learning update rules, and algorithmic implementation. A concrete image-classification task (CIFAR-10) is provided as a benchmark-smashing application, together with an accompanying Python notebook containing the PyTorch implementation.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preference-based learning for news headline recommendation</title>
<link>https://arxiv.org/abs/2506.06334</link>
<guid>https://arxiv.org/abs/2506.06334</guid>
<content:encoded><![CDATA[
arXiv:2506.06334v1 Announce Type: cross 
Abstract: This study explores strategies for optimizing news headline recommendations through preference-based learning. Using real-world data of user interactions with French-language online news posts, we learn a headline recommender agent under a contextual bandit setting. This allows us to explore the impact of translation on engagement predictions, as well as the benefits of different interactive strategies on user engagement during data collection. Our results show that explicit exploration may not be required in the presence of noisy contexts, opening the door to simpler but efficient strategies in practice.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Aware Multi-view Arrhythmia Classification from ECG</title>
<link>https://arxiv.org/abs/2506.06342</link>
<guid>https://arxiv.org/abs/2506.06342</guid>
<content:encoded><![CDATA[
arXiv:2506.06342v1 Announce Type: cross 
Abstract: We propose a deep neural architecture that performs uncertainty-aware multi-view classification of arrhythmia from ECG. Our method learns two different views (1D and 2D) of single-lead ECG to capture different types of information. We use a fusion technique to reduce the conflict between the different views caused by noise and artifacts in ECG data, thus incorporating uncertainty to obtain stronger final predictions. Our framework contains the following three modules (1) a time-series module to learn the morphological features from ECG; (2) an image-space learning module to learn the spatiotemporal features; and (3) the uncertainty-aware fusion module to fuse the information from the two different views. Experimental results on two real-world datasets demonstrate that our framework not only improves the performance on arrhythmia classification compared to the state-of-the-art but also shows better robustness to noise and artifacts present in ECG.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TESU-LLM: Training Speech-LLMs Without Speech via Unified Encoder Alignment</title>
<link>https://arxiv.org/abs/2506.06343</link>
<guid>https://arxiv.org/abs/2506.06343</guid>
<content:encoded><![CDATA[
arXiv:2506.06343v1 Announce Type: cross 
Abstract: Recent advances in speech-enabled language models have shown promising results in building intelligent voice assistants. However, most existing approaches rely on large-scale paired speech-text data and extensive computational resources, which pose challenges in terms of scalability and accessibility. In this paper, we present \textbf{TESU-LLM}, a novel framework that enables training speech-capable language models using only text data. Our key insight is to leverage a unified encoder that maps semantically equivalent text and speech inputs to a shared latent space. By aligning the encoder output with the embedding space of a LLM via a lightweight projection network, we enable the model to generalize from text-only supervision to speech-based inference. Despite being trained exclusively on text, TESU-LLM achieves strong performance on various speech-related benchmarks, comparable to baseline methods trained with large-scale multimodal datasets and substantial computational resources. These results highlight the effectiveness and efficiency of our approach, offering a scalable path toward building speech LLMs without speech data.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Reinforcement Learning Approach for RIS-aided Fair Communications</title>
<link>https://arxiv.org/abs/2506.06344</link>
<guid>https://arxiv.org/abs/2506.06344</guid>
<content:encoded><![CDATA[
arXiv:2506.06344v1 Announce Type: cross 
Abstract: Reconfigurable Intelligent Surfaces (RISs) are composed of physical elements that can dynamically alter electromagnetic wave properties to enhance beamforming and leading to improvements in areas with low coverage properties. They have the potential to be combined with Reinforcement Learning (RL) techniques to achieve network performance and energy efficiency via optimization techniques. In addition to performance and energy improvements, it is also crucial to consider the concept of fair communications. RISs must ensure that User Equipment (UE) units receive their signals with adequate strength, without other UE being deprived of service due to insufficient power. In this paper, we address such a problem. We explore the fairness properties of previous work and propose a novel method that aims at obtaining an efficient and fair duplex RIS-RL system for multiple legitimate UE units. We report and discuss our experimental work and simulation results. We also release our code and datasets to foster further research in the topic.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable-AI powered stock price prediction using time series transformers: A Case Study on BIST100</title>
<link>https://arxiv.org/abs/2506.06345</link>
<guid>https://arxiv.org/abs/2506.06345</guid>
<content:encoded><![CDATA[
arXiv:2506.06345v1 Announce Type: cross 
Abstract: Financial literacy is increasingly dependent on the ability to interpret complex financial data and utilize advanced forecasting tools. In this context, this study proposes a novel approach that combines transformer-based time series models with explainable artificial intelligence (XAI) to enhance the interpretability and accuracy of stock price predictions. The analysis focuses on the daily stock prices of the five highest-volume banks listed in the BIST100 index, along with XBANK and XU100 indices, covering the period from January 2015 to March 2025. Models including DLinear, LTSNet, Vanilla Transformer, and Time Series Transformer are employed, with input features enriched by technical indicators. SHAP and LIME techniques are used to provide transparency into the influence of individual features on model outputs. The results demonstrate the strong predictive capabilities of transformer models and highlight the potential of interpretable machine learning to empower individuals in making informed investment decisions and actively engaging in financial markets.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LD-RPMNet: Near-Sensor Diagnosis for Railway Point Machines</title>
<link>https://arxiv.org/abs/2506.06346</link>
<guid>https://arxiv.org/abs/2506.06346</guid>
<content:encoded><![CDATA[
arXiv:2506.06346v1 Announce Type: cross 
Abstract: Near-sensor diagnosis has become increasingly prevalent in industry. This study proposes a lightweight model named LD-RPMNet that integrates Transformers and Convolutional Neural Networks, leveraging both local and global feature extraction to optimize computational efficiency for a practical railway application. The LD-RPMNet introduces a Multi-scale Depthwise Separable Convolution (MDSC) module, which decomposes cross-channel convolutions into pointwise and depthwise convolutions while employing multi-scale kernels to enhance feature extraction. Meanwhile, a Broadcast Self-Attention (BSA) mechanism is incorporated to simplify complex matrix multiplications and improve computational efficiency. Experimental results based on collected sound signals during the operation of railway point machines demonstrate that the optimized model reduces parameter count and computational complexity by 50% while improving diagnostic accuracy by nearly 3%, ultimately achieving an accuracy of 98.86%. This demonstrates the possibility of near-sensor fault diagnosis applications in railway point machines.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Platform Methane Plume Detection via Model and Domain Adaptation</title>
<link>https://arxiv.org/abs/2506.06348</link>
<guid>https://arxiv.org/abs/2506.06348</guid>
<content:encoded><![CDATA[
arXiv:2506.06348v1 Announce Type: cross 
Abstract: Prioritizing methane for near-term climate action is crucial due to its significant impact on global warming. Previous work used columnwise matched filter products from the airborne AVIRIS-NG imaging spectrometer to detect methane plume sources; convolutional neural networks (CNNs) discerned anthropogenic methane plumes from false positive enhancements. However, as an increasing number of remote sensing platforms are used for methane plume detection, there is a growing need to address cross-platform alignment. In this work, we describe model- and data-driven machine learning approaches that leverage airborne observations to improve spaceborne methane plume detection, reconciling the distributional shifts inherent with performing the same task across platforms. We develop a spaceborne methane plume classifier using data from the EMIT imaging spectroscopy mission. We refine classifiers trained on airborne imagery from AVIRIS-NG campaigns using transfer learning, outperforming the standalone spaceborne model. Finally, we use CycleGAN, an unsupervised image-to-image translation technique, to align the data distributions between airborne and spaceborne contexts. Translating spaceborne EMIT data to the airborne AVIRIS-NG domain using CycleGAN and applying airborne classifiers directly yields the best plume detection results. This methodology is useful not only for data simulation, but also for direct data alignment. Though demonstrated on the task of methane plume detection, our work more broadly demonstrates a data-driven approach to align related products obtained from distinct remote sensing instruments.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heart Rate Classification in ECG Signals Using Machine Learning and Deep Learning</title>
<link>https://arxiv.org/abs/2506.06349</link>
<guid>https://arxiv.org/abs/2506.06349</guid>
<content:encoded><![CDATA[
arXiv:2506.06349v1 Announce Type: cross 
Abstract: This study addresses the classification of heartbeats from ECG signals through two distinct approaches: traditional machine learning utilizing hand-crafted features and deep learning via transformed images of ECG beats. The dataset underwent preprocessing steps, including downsampling, filtering, and normalization, to ensure consistency and relevance for subsequent analysis. In the first approach, features such as heart rate variability (HRV), mean, variance, and RR intervals were extracted to train various classifiers, including SVM, Random Forest, AdaBoost, LSTM, Bi-directional LSTM, and LightGBM. The second approach involved transforming ECG signals into images using Gramian Angular Field (GAF), Markov Transition Field (MTF), and Recurrence Plots (RP), with these images subsequently classified using CNN architectures like VGG and Inception.
  Experimental results demonstrate that the LightGBM model achieved the highest performance, with an accuracy of 99% and an F1 score of 0.94, outperforming the image-based CNN approach (F1 score of 0.85). Models such as SVM and AdaBoost yielded significantly lower scores, indicating limited suitability for this task. The findings underscore the superior ability of hand-crafted features to capture temporal and morphological variations in ECG signals compared to image-based representations of individual beats. Future investigations may benefit from incorporating multi-lead ECG signals and temporal dependencies across successive beats to enhance classification accuracy further.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep learning methods for modeling infrasound transmission loss in the middle atmosphere</title>
<link>https://arxiv.org/abs/2506.06351</link>
<guid>https://arxiv.org/abs/2506.06351</guid>
<content:encoded><![CDATA[
arXiv:2506.06351v1 Announce Type: cross 
Abstract: Accurate modeling of infrasound transmission losses (TLs) is essential to assess the performance of the global International Monitoring System infrasound network. Among existing propagation modeling tools, parabolic equation (PE) method enables TLs to be finely modeled, but its computational cost does not allow exploration of a large parameter space for operational monitoring applications. To reduce computation times, Brissaud et al. 2023 explored the potential of convolutional neural networks trained on a large set of regionally simulated wavefields (< 1000 km from the source) to predict TLs with negligible computation times compared to PE simulations. However, this method struggles in unfavorable initial wind conditions, especially at high frequencies, and causal issues with winds at large distances from the source affecting ground TLs close to the source. In this study, we have developed an optimized convolutional network designed to minimize prediction errors while predicting TLs from globally simulated combined temperature and wind fields spanning over propagation ranges of 4000 km. Our approach enhances the previously proposed one by implementing key optimizations that improve the overall architecture performance. The implemented model predicts TLs with an average error of 8.6 dB in the whole frequency band (0.1-3.2 Hz) and explored realistic atmospheric scenarios.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models for EEG: A Comprehensive Survey and Taxonomy</title>
<link>https://arxiv.org/abs/2506.06353</link>
<guid>https://arxiv.org/abs/2506.06353</guid>
<content:encoded><![CDATA[
arXiv:2506.06353v1 Announce Type: cross 
Abstract: The growing convergence between Large Language Models (LLMs) and electroencephalography (EEG) research is enabling new directions in neural decoding, brain-computer interfaces (BCIs), and affective computing. This survey offers a systematic review and structured taxonomy of recent advancements that utilize LLMs for EEG-based analysis and applications. We organize the literature into four domains: (1) LLM-inspired foundation models for EEG representation learning, (2) EEG-to-language decoding, (3) cross-modal generation including image and 3D object synthesis, and (4) clinical applications and dataset management tools. The survey highlights how transformer-based architectures adapted through fine-tuning, few-shot, and zero-shot learning have enabled EEG-based models to perform complex tasks such as natural language generation, semantic interpretation, and diagnostic assistance. By offering a structured overview of modeling strategies, system designs, and application areas, this work serves as a foundational resource for future work to bridge natural language processing and neural signal analysis through language models.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning Enhanced Multi-Day Turnover Quantitative Trading Algorithm for Chinese A-Share Market</title>
<link>https://arxiv.org/abs/2506.06356</link>
<guid>https://arxiv.org/abs/2506.06356</guid>
<content:encoded><![CDATA[
arXiv:2506.06356v1 Announce Type: cross 
Abstract: This paper presents a sophisticated multi-day turnover quantitative trading algorithm that integrates advanced deep learning techniques with comprehensive cross-sectional stock prediction for the Chinese A-share market. Our framework combines five interconnected modules: initial stock selection through deep cross-sectional prediction networks, opening signal distribution analysis using mixture models for arbitrage identification, market capitalization and liquidity-based dynamic position sizing, grid-search optimized profit-taking and stop-loss mechanisms, and multi-granularity volatility-based market timing models. The algorithm employs a novel approach to balance capital efficiency with risk management through adaptive holding periods and sophisticated entry/exit timing. Trained on comprehensive A-share data from 2010-2020 and rigorously backtested on 2021-2024 data, our method achieves remarkable performance with 15.2\% annualized returns, maximum drawdown constrained below 5\%, and a Sharpe ratio of 1.87. The strategy demonstrates exceptional scalability by maintaining 50-100 daily positions with a 9-day maximum holding period, incorporating dynamic profit-taking and stop-loss mechanisms that enhance capital turnover efficiency while preserving risk-adjusted returns. Our approach exhibits robust performance across various market regimes while maintaining high capital capacity suitable for institutional deployment.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Generalizable Drowsiness Monitoring with Physiological Sensors: A Preliminary Study</title>
<link>https://arxiv.org/abs/2506.06360</link>
<guid>https://arxiv.org/abs/2506.06360</guid>
<content:encoded><![CDATA[
arXiv:2506.06360v1 Announce Type: cross 
Abstract: Accurately detecting drowsiness is vital to driving safety. Among all measures, physiological-signal-based drowsiness monitoring can be more privacy-preserving than a camera-based approach. However, conflicts exist regarding how physiological metrics are associated with different drowsiness labels across datasets. Thus, we analyzed key features from electrocardiograms (ECG), electrodermal activity (EDA), and respiratory (RESP) signals across four datasets, where different drowsiness inducers (such as fatigue and low arousal) and assessment methods (subjective vs. objective) were used. Binary logistic regression models were built to identify the physiological metrics that are associated with drowsiness. Findings indicate that distinct different drowsiness inducers can lead to different physiological responses, and objective assessments were more sensitive than subjective ones in detecting drowsiness. Further, the increased heart rate stability, reduced respiratory amplitude, and decreased tonic EDA are robustly associated with increased drowsiness. The results enhance understanding of drowsiness detection and can inform future generalizable monitoring designs.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tactile MNIST: Benchmarking Active Tactile Perception</title>
<link>https://arxiv.org/abs/2506.06361</link>
<guid>https://arxiv.org/abs/2506.06361</guid>
<content:encoded><![CDATA[
arXiv:2506.06361v1 Announce Type: cross 
Abstract: Tactile perception has the potential to significantly enhance dexterous robotic manipulation by providing rich local information that can complement or substitute for other sensory modalities such as vision. However, because tactile sensing is inherently local, it is not well-suited for tasks that require broad spatial awareness or global scene understanding on its own. A human-inspired strategy to address this issue is to consider active perception techniques instead. That is, to actively guide sensors toward regions with more informative or significant features and integrate such information over time in order to understand a scene or complete a task. Both active perception and different methods for tactile sensing have received significant attention recently. Yet, despite advancements, both fields lack standardized benchmarks. To bridge this gap, we introduce the Tactile MNIST Benchmark Suite, an open-source, Gymnasium-compatible benchmark specifically designed for active tactile perception tasks, including localization, classification, and volume estimation. Our benchmark suite offers diverse simulation scenarios, from simple toy environments all the way to complex tactile perception tasks using vision-based tactile sensors. Furthermore, we also offer a comprehensive dataset comprising 13,500 synthetic 3D MNIST digit models and 153,600 real-world tactile samples collected from 600 3D printed digits. Using this dataset, we train a CycleGAN for realistic tactile simulation rendering. By providing standardized protocols and reproducible evaluation frameworks, our benchmark suite facilitates systematic progress in the fields of tactile sensing and active perception.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CR-BLEA: Contrastive Ranking for Adaptive Resource Allocation in Bilevel Evolutionary Algorithms</title>
<link>https://arxiv.org/abs/2506.06362</link>
<guid>https://arxiv.org/abs/2506.06362</guid>
<content:encoded><![CDATA[
arXiv:2506.06362v1 Announce Type: cross 
Abstract: Bilevel optimization poses a significant computational challenge due to its nested structure, where each upper-level candidate solution requires solving a corresponding lower-level problem. While evolutionary algorithms (EAs) are effective at navigating such complex landscapes, their high resource demands remain a key bottleneck -- particularly the redundant evaluation of numerous unpromising lower-level tasks. Despite recent advances in multitasking and transfer learning, resource waste persists. To address this issue, we propose a novel resource allocation framework for bilevel EAs that selectively identifies and focuses on promising lower-level tasks. Central to our approach is a contrastive ranking network that learns relational patterns between paired upper- and lower-level solutions online. This knowledge guides a reference-based ranking strategy that prioritizes tasks for optimization and adaptively controls resampling based on estimated population quality. Comprehensive experiments across five state-of-the-art bilevel algorithms show that our framework significantly reduces computational cost while preserving -- or even enhancing -- solution accuracy. This work offers a generalizable strategy to improve the efficiency of bilevel EAs, paving the way for more scalable bilevel optimization.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChemGraph: An Agentic Framework for Computational Chemistry Workflows</title>
<link>https://arxiv.org/abs/2506.06363</link>
<guid>https://arxiv.org/abs/2506.06363</guid>
<content:encoded><![CDATA[
arXiv:2506.06363v1 Announce Type: cross 
Abstract: Atomistic simulations are essential tools in chemistry and materials science, accelerating the discovery of novel catalysts, energy storage materials, and pharmaceuticals. However, running these simulations remains challenging due to the wide range of computational methods, diverse software ecosystems, and the need for expert knowledge and manual effort for the setup, execution, and validation stages. In this work, we present ChemGraph, an agentic framework powered by artificial intelligence and state-of-the-art simulation tools to streamline and automate computational chemistry and materials science workflows. ChemGraph leverages graph neural network-based foundation models for accurate yet computationally efficient calculations and large language models (LLMs) for natural language understanding, task planning, and scientific reasoning to provide an intuitive and interactive interface. Users can perform tasks such as molecular structure generation, single-point energy, geometry optimization, vibrational analysis, and thermochemistry calculations with methods ranging from tight-binding and machine learning interatomic potentials to density functional theory or wave function theory-based methods. We evaluate ChemGraph across 13 benchmark tasks and demonstrate that smaller LLMs (GPT-4o-mini, Claude-3.5-haiku, Qwen2.5-14B) perform well on simple workflows, while more complex tasks benefit from using larger models like GPT-4o. Importantly, we show that decomposing complex tasks into smaller subtasks through a multi-agent framework enables smaller LLM models to match or exceed GPT-4o's performance in specific scenarios.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>El0ps: An Exact L0-regularized Problems Solver</title>
<link>https://arxiv.org/abs/2506.06373</link>
<guid>https://arxiv.org/abs/2506.06373</guid>
<content:encoded><![CDATA[
arXiv:2506.06373v1 Announce Type: cross 
Abstract: This paper presents El0ps, a Python toolbox providing several utilities to handle L0-regularized problems related to applications in machine learning, statistics, and signal processing, among other fields. In contrast to existing toolboxes, El0ps allows users to define custom instances of these problems through a flexible framework, provides a dedicated solver achieving state-of-the-art performance, and offers several built-in machine learning pipelines. Our aim with El0ps is to provide a comprehensive tool which opens new perspectives for the integration of L0-regularized problems in practical applications.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Large Language Model Capabilities in Assessing Spatial Econometrics Research</title>
<link>https://arxiv.org/abs/2506.06377</link>
<guid>https://arxiv.org/abs/2506.06377</guid>
<content:encoded><![CDATA[
arXiv:2506.06377v1 Announce Type: cross 
Abstract: This paper investigates Large Language Models (LLMs) ability to assess the economic soundness and theoretical consistency of empirical findings in spatial econometrics. We created original and deliberately altered "counterfactual" summaries from 28 published papers (2005-2024), which were evaluated by a diverse set of LLMs. The LLMs provided qualitative assessments and structured binary classifications on variable choice, coefficient plausibility, and publication suitability. The results indicate that while LLMs can expertly assess the coherence of variable choices (with top models like GPT-4o achieving an overall F1 score of 0.87), their performance varies significantly when evaluating deeper aspects such as coefficient plausibility and overall publication suitability. The results further revealed that the choice of LLM, the specific characteristics of the paper and the interaction between these two factors significantly influence the accuracy of the assessment, particularly for nuanced judgments. These findings highlight LLMs' current strengths in assisting with initial, more surface-level checks and their limitations in performing comprehensive, deep economic reasoning, suggesting a potential assistive role in peer review that still necessitates robust human oversight.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformer-Based Decomposition of Electrodermal Activity for Real-World Mental Health Applications</title>
<link>https://arxiv.org/abs/2506.06378</link>
<guid>https://arxiv.org/abs/2506.06378</guid>
<content:encoded><![CDATA[
arXiv:2506.06378v1 Announce Type: cross 
Abstract: Decomposing Electrodermal Activity (EDA) into phasic (short-term, stimulus-linked responses) and tonic (longer-term baseline) components is essential for extracting meaningful emotional and physiological biomarkers. This study presents a comparative analysis of knowledge-driven, statistical, and deep learning-based methods for EDA signal decomposition, with a focus on in-the-wild data collected from wearable devices. In particular, the authors introduce the Feel Transformer, a novel Transformer-based model adapted from the Autoformer architecture, designed to separate phasic and tonic components without explicit supervision. The model leverages pooling and trend-removal mechanisms to enforce physiologically meaningful decompositions. Comparative experiments against methods such as Ledalab, cvxEDA, and conventional detrending show that the Feel Transformer achieves a balance between feature fidelity (SCR frequency, amplitude, and tonic slope) and robustness to noisy, real-world data. The model demonstrates potential for real-time biosignal analysis and future applications in stress prediction, digital mental health interventions, and physiological forecasting.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Fundamental Impossibility of Hallucination Control in Large Language Models</title>
<link>https://arxiv.org/abs/2506.06382</link>
<guid>https://arxiv.org/abs/2506.06382</guid>
<content:encoded><![CDATA[
arXiv:2506.06382v1 Announce Type: cross 
Abstract: This paper explains \textbf{why it is impossible to create large language models that do not hallucinate and what are the trade-offs we should be looking for}. It presents a formal \textbf{impossibility theorem} demonstrating that no inference mechanism can simultaneously satisfy four fundamental properties: \textbf{truthful (non-hallucinatory) generation, semantic information conservation, relevant knowledge revelation, and knowledge-constrained optimality}. By modeling LLM inference as an \textbf{auction of ideas} where neural components compete to contribute to responses, we prove the impossibility using the Green-Laffont theorem. That mathematical framework provides a rigorous foundation for understanding the nature of inference process, with implications for model architecture, training objectives, and evaluation methods.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-based Neural Data Augmentation for sub-wavelength Radio Localization</title>
<link>https://arxiv.org/abs/2506.06387</link>
<guid>https://arxiv.org/abs/2506.06387</guid>
<content:encoded><![CDATA[
arXiv:2506.06387v1 Announce Type: cross 
Abstract: The increasing deployment of large antenna arrays at base stations has significantly improved the spatial resolution and localization accuracy of radio-localization methods. However, traditional signal processing techniques struggle in complex radio environments, particularly in scenarios dominated by non line of sight (NLoS) propagation paths, resulting in degraded localization accuracy. Recent developments in machine learning have facilitated the development of machine learning-assisted localization techniques, enhancing localization accuracy in complex radio environments. However, these methods often involve substantial computational complexity during both the training and inference phases. This work extends the well-established fingerprinting-based localization framework by simultaneously reducing its memory requirements and improving its accuracy. Specifically, a model-based neural network is used to learn the location-to-channel mapping, and then serves as a generative neural channel model. This generative model augments the fingerprinting comparison dictionary while reducing the memory requirements. The proposed method outperforms fingerprinting baselines by achieving sub-wavelength localization accuracy, even in NLoS environments. Remarkably, it offers an improvement by several orders of magnitude in localization accuracy, while simultaneously reducing memory requirements by an order of magnitude compared to classical fingerprinting methods.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Adversarial Watermarking in Transformer-Based Models: Transferability and Robustness Against Defense Mechanism for Medical Images</title>
<link>https://arxiv.org/abs/2506.06389</link>
<guid>https://arxiv.org/abs/2506.06389</guid>
<content:encoded><![CDATA[
arXiv:2506.06389v1 Announce Type: cross 
Abstract: Deep learning models have shown remarkable success in dermatological image analysis, offering potential for automated skin disease diagnosis. Previously, convolutional neural network(CNN) based architectures have achieved immense popularity and success in computer vision (CV) based task like skin image recognition, generation and video analysis. But with the emergence of transformer based models, CV tasks are now are nowadays carrying out using these models. Vision Transformers (ViTs) is such a transformer-based models that have shown success in computer vision. It uses self-attention mechanisms to achieve state-of-the-art performance across various tasks. However, their reliance on global attention mechanisms makes them susceptible to adversarial perturbations. This paper aims to investigate the susceptibility of ViTs for medical images to adversarial watermarking-a method that adds so-called imperceptible perturbations in order to fool models. By generating adversarial watermarks through Projected Gradient Descent (PGD), we examine the transferability of such attacks to CNNs and analyze the performance defense mechanism -- adversarial training. Results indicate that while performance is not compromised for clean images, ViTs certainly become much more vulnerable to adversarial attacks: an accuracy drop of as low as 27.6%. Nevertheless, adversarial training raises it up to 90.0%.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models</title>
<link>https://arxiv.org/abs/2506.06395</link>
<guid>https://arxiv.org/abs/2506.06395</guid>
<content:encoded><![CDATA[
arXiv:2506.06395v1 Announce Type: cross 
Abstract: Large language models (LLMs) excel at reasoning, yet post-training remains critical for aligning their behavior with task goals. Existing reinforcement learning (RL) methods often depend on costly human annotations or external reward models. We propose Reinforcement Learning via Self-Confidence (RLSC), which uses the model's own confidence as reward signals-eliminating the need for labels, preference models, or reward engineering. Applied to Qwen2.5-Math-7B with only 8 samples per question and 4 training epochs, RLSC improves accuracy by +20.10% on AIME2024, +49.40% on MATH500, and +52.50% on AMC23. RLSC offers a simple, scalable post-training method for reasoning models with minimal supervision.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Direct Behavior Optimization: Unlocking the Potential of Lightweight LLMs</title>
<link>https://arxiv.org/abs/2506.06401</link>
<guid>https://arxiv.org/abs/2506.06401</guid>
<content:encoded><![CDATA[
arXiv:2506.06401v1 Announce Type: cross 
Abstract: Lightweight Large Language Models (LwLLMs) are reduced-parameter, optimized models designed to run efficiently on consumer-grade hardware, offering significant advantages in resource efficiency, cost-effectiveness, and data privacy. However, these models often struggle with limited inference and reasoning capabilities, which restrict their performance on complex tasks and limit their practical applicability. Moreover, existing prompt optimization methods typically rely on extensive manual effort or the meta-cognitive abilities of state-of-the-art LLMs, making them less effective for LwLLMs. To address these challenges, we introduce DeBoP, a new Direct Behavior Optimization Paradigm, original from the Chain-of-Thought (CoT) prompting technique. Unlike CoT Prompting, DeBoP is an automatic optimization method, which focuses on the optimization directly on the behavior of LwLLMs. In particular, DeBoP transforms the optimization of complex prompts into the optimization of discrete, quantifiable execution sequences using a gradient-free Monte Carlo Tree Search. We evaluate DeBoP on seven challenging tasks where state-of-the-art LLMs excel but LwLLMs generally underperform. Experimental results demonstrate that DeBoP significantly outperforms recent prompt optimization methods on most tasks. In particular, DeBoP-optimized LwLLMs surpass GPT-3.5 on most tasks while reducing computational time by approximately 60% compared to other automatic prompt optimization methods.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unintended Harms of Value-Aligned LLMs: Psychological and Empirical Insights</title>
<link>https://arxiv.org/abs/2506.06404</link>
<guid>https://arxiv.org/abs/2506.06404</guid>
<content:encoded><![CDATA[
arXiv:2506.06404v1 Announce Type: cross 
Abstract: The application scope of Large Language Models (LLMs) continues to expand, leading to increasing interest in personalized LLMs that align with human values. However, aligning these models with individual values raises significant safety concerns, as certain values may correlate with harmful information. In this paper, we identify specific safety risks associated with value-aligned LLMs and investigate the psychological principles behind these challenges. Our findings reveal two key insights. (1) Value-aligned LLMs are more prone to harmful behavior compared to non-fine-tuned models and exhibit slightly higher risks in traditional safety evaluations than other fine-tuned models. (2) These safety issues arise because value-aligned LLMs genuinely generate text according to the aligned values, which can amplify harmful outcomes. Using a dataset with detailed safety categories, we find significant correlations between value alignment and safety risks, supported by psychological hypotheses. This study offers insights into the "black box" of value alignment and proposes in-context alignment methods to enhance the safety of value-aligned LLMs.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TimeWak: Temporal Chained-Hashing Watermark for Time Series Data</title>
<link>https://arxiv.org/abs/2506.06407</link>
<guid>https://arxiv.org/abs/2506.06407</guid>
<content:encoded><![CDATA[
arXiv:2506.06407v1 Announce Type: cross 
Abstract: Synthetic time series generated by diffusion models enable sharing privacy-sensitive datasets, such as patients' functional MRI records. Key criteria for synthetic data include high data utility and traceability to verify the data source. Recent watermarking methods embed in homogeneous latent spaces, but state-of-the-art time series generators operate in real space, making latent-based watermarking incompatible. This creates the challenge of watermarking directly in real space while handling feature heterogeneity and temporal dependencies. We propose TimeWak, the first watermarking algorithm for multivariate time series diffusion models. To handle temporal dependence and spatial heterogeneity, TimeWak embeds a temporal chained-hashing watermark directly within the real temporal-feature space. The other unique feature is the $\epsilon$-exact inversion, which addresses the non-uniform reconstruction error distribution across features from inverting the diffusion process to detect watermarks. We derive the error bound of inverting multivariate time series and further maintain high watermark detectability. We extensively evaluate TimeWak on its impact on synthetic data quality, watermark detectability, and robustness under various post-editing attacks, against 5 datasets and baselines of different temporal lengths. Our results show that TimeWak achieves improvements of 61.96% in context-FID score, and 8.44% in correlational scores against the state-of-the-art baseline, while remaining consistently detectable.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HeavyWater and SimplexWater: Watermarking Low-Entropy Text Distributions</title>
<link>https://arxiv.org/abs/2506.06409</link>
<guid>https://arxiv.org/abs/2506.06409</guid>
<content:encoded><![CDATA[
arXiv:2506.06409v1 Announce Type: cross 
Abstract: Large language model (LLM) watermarks enable authentication of text provenance, curb misuse of machine-generated text, and promote trust in AI systems. Current watermarks operate by changing the next-token predictions output by an LLM. The updated (i.e., watermarked) predictions depend on random side information produced, for example, by hashing previously generated tokens. LLM watermarking is particularly challenging in low-entropy generation tasks - such as coding - where next-token predictions are near-deterministic. In this paper, we propose an optimization framework for watermark design. Our goal is to understand how to most effectively use random side information in order to maximize the likelihood of watermark detection and minimize the distortion of generated text. Our analysis informs the design of two new watermarks: HeavyWater and SimplexWater. Both watermarks are tunable, gracefully trading-off between detection accuracy and text distortion. They can also be applied to any LLM and are agnostic to side information generation. We examine the performance of HeavyWater and SimplexWater through several benchmarks, demonstrating that they can achieve high watermark detection accuracy with minimal compromise of text generation quality, particularly in the low-entropy regime. Our theoretical analysis also reveals surprising new connections between LLM watermarking and coding theory. The code implementation can be found in https://github.com/DorTsur/HeavyWater_SimplexWater
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving choice model specification using reinforcement learning</title>
<link>https://arxiv.org/abs/2506.06410</link>
<guid>https://arxiv.org/abs/2506.06410</guid>
<content:encoded><![CDATA[
arXiv:2506.06410v1 Announce Type: cross 
Abstract: Discrete choice modelling is a theory-driven modelling framework for understanding and forecasting choice behaviour. To obtain behavioural insights, modellers test several competing model specifications in their attempts to discover the 'true' data generation process. This trial-and-error process requires expertise, is time-consuming, and relies on subjective theoretical assumptions. Although metaheuristics have been proposed to assist choice modellers, they treat model specification as a classic optimisation problem, relying on static strategies, applying predefined rules, and neglecting outcomes from previous estimated models. As a result, current metaheuristics struggle to prioritise promising search regions, adapt exploration dynamically, and transfer knowledge to other modelling tasks. To address these limitations, we introduce a deep reinforcement learning-based framework where an 'agent' specifies models by estimating them and receiving rewards based on goodness-of-fit and parsimony. Results demonstrate the agent dynamically adapts its strategies to identify promising specifications across data generation processes, showing robustness and potential transferability, without prior domain knowledge.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Canonical Autoregressive Generation</title>
<link>https://arxiv.org/abs/2506.06446</link>
<guid>https://arxiv.org/abs/2506.06446</guid>
<content:encoded><![CDATA[
arXiv:2506.06446v1 Announce Type: cross 
Abstract: State of the art large language models are trained using large amounts of tokens derived from raw text using what is called a tokenizer. Crucially, the tokenizer determines the (token) vocabulary a model will use during inference as well as, in principle, the (token) language. This is because, while the token vocabulary may allow for different tokenizations of a string, the tokenizer always maps the string to only one of these tokenizations--the canonical tokenization. However, multiple lines of empirical evidence suggest that large language models do not always generate canonical token sequences, and this comes with several negative consequences. In this work, we first show that, to generate a canonical token sequence, a model needs to generate (partial) canonical token sequences at each step of the autoregressive generation process underpinning its functioning. Building upon this theoretical result, we introduce canonical sampling, a simple and efficient sampling method that precludes a given model from generating non-canonical token sequences. Further, we also show that, in comparison with standard sampling, the distribution of token sequences generated using canonical sampling is provably closer to the true distribution of token sequences used during training.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SIGMA: Refining Large Language Model Reasoning via Sibling-Guided Monte Carlo Augmentation</title>
<link>https://arxiv.org/abs/2506.06470</link>
<guid>https://arxiv.org/abs/2506.06470</guid>
<content:encoded><![CDATA[
arXiv:2506.06470v1 Announce Type: cross 
Abstract: Enhancing large language models by simply scaling up datasets has begun to yield diminishing returns, shifting the spotlight to data quality. Monte Carlo Tree Search (MCTS) has emerged as a powerful technique for generating high-quality chain-of-thought data, yet conventional approaches typically retain only the top-scoring trajectory from the search tree, discarding sibling nodes that often contain valuable partial insights, recurrent error patterns, and alternative reasoning strategies. This unconditional rejection of non-optimal reasoning branches may waste vast amounts of informative data in the whole search tree. We propose SIGMA (Sibling Guided Monte Carlo Augmentation), a novel framework that reintegrates these discarded sibling nodes to refine LLM reasoning. SIGMA forges semantic links among sibling nodes along each search path and applies a two-stage refinement: a critique model identifies overlooked strengths and weaknesses across the sibling set, and a revision model conducts text-based backpropagation to refine the top-scoring trajectory in light of this comparative feedback. By recovering and amplifying the underutilized but valuable signals from non-optimal reasoning branches, SIGMA substantially improves reasoning trajectories. On the challenging MATH benchmark, our SIGMA-tuned 7B model achieves 54.92% accuracy using only 30K samples, outperforming state-of-the-art models trained on 590K samples. This result highlights that our sibling-guided optimization not only significantly reduces data usage but also significantly boosts LLM reasoning.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cost-Efficient LLM Training with Lifetime-Aware Tensor Offloading via GPUDirect Storage</title>
<link>https://arxiv.org/abs/2506.06472</link>
<guid>https://arxiv.org/abs/2506.06472</guid>
<content:encoded><![CDATA[
arXiv:2506.06472v1 Announce Type: cross 
Abstract: We present the design and implementation of a new lifetime-aware tensor offloading framework for GPU memory expansion using low-cost PCIe-based solid-state drives (SSDs). Our framework, TERAIO, is developed explicitly for large language model (LLM) training with multiple GPUs and multiple SSDs. Its design is driven by our observation that the active tensors take only a small fraction (1.7% on average) of allocated GPU memory in each LLM training iteration, the inactive tensors are usually large and will not be used for a long period of time, creating ample opportunities for offloading/prefetching tensors to/from slow SSDs without stalling the GPU training process. TERAIO accurately estimates the lifetime (active period of time in GPU memory) of each tensor with the profiling of the first few iterations in the training process. With the tensor lifetime analysis, TERAIO will generate an optimized tensor offloading/prefetching plan and integrate it into the compiled LLM program via PyTorch. TERAIO has a runtime tensor migration engine to execute the offloading/prefetching plan via GPUDirect storage, which allows direct tensor migration between GPUs and SSDs for alleviating the CPU bottleneck and maximizing the SSD bandwidth utilization. In comparison with state-of-the-art studies such as ZeRO-Offload and ZeRO-Infinity, we show that TERAIO improves the training performance of various LLMs by 1.47x on average, and achieves 80.7% of the ideal performance assuming unlimited GPU memory.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Noise Consistency Regularization for Improved Subject-Driven Image Synthesis</title>
<link>https://arxiv.org/abs/2506.06483</link>
<guid>https://arxiv.org/abs/2506.06483</guid>
<content:encoded><![CDATA[
arXiv:2506.06483v1 Announce Type: cross 
Abstract: Fine-tuning Stable Diffusion enables subject-driven image synthesis by adapting the model to generate images containing specific subjects. However, existing fine-tuning methods suffer from two key issues: underfitting, where the model fails to reliably capture subject identity, and overfitting, where it memorizes the subject image and reduces background diversity. To address these challenges, we propose two auxiliary consistency losses for diffusion fine-tuning. First, a prior consistency regularization loss ensures that the predicted diffusion noise for prior (non-subject) images remains consistent with that of the pretrained model, improving fidelity. Second, a subject consistency regularization loss enhances the fine-tuned model's robustness to multiplicative noise modulated latent code, helping to preserve subject identity while improving diversity. Our experimental results demonstrate that incorporating these losses into fine-tuning not only preserves subject identity but also enhances image diversity, outperforming DreamBooth in terms of CLIP scores, background variation, and overall visual quality.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Economic Dispatch of Power-to-Gas Systems with Deep Reinforcement Learning:Tackling the Challenge of Delayed Rewards with Long-Term Energy Storage</title>
<link>https://arxiv.org/abs/2506.06484</link>
<guid>https://arxiv.org/abs/2506.06484</guid>
<content:encoded><![CDATA[
arXiv:2506.06484v1 Announce Type: cross 
Abstract: Power-to-Gas (P2G) technologies gain recognition for enabling the integration of intermittent renewables, such as wind and solar, into electricity grids. However, determining the most cost-effective operation of these systems is complex due to the volatile nature of renewable energy, electricity prices, and loads. Additionally, P2G systems are less efficient in converting and storing energy compared to battery energy storage systems (BESs), and the benefits of converting electricity into gas are not immediately apparent. Deep Reinforcement Learning (DRL) has shown promise in managing the operation of energy systems amidst these uncertainties. Yet, DRL techniques face difficulties with the delayed reward characteristic of P2G system operation. Previous research has mostly focused on short-term studies that look at the energy conversion process, neglecting the long-term storage capabilities of P2G.
  This study presents a new method by thoroughly examining how DRL can be applied to the economic operation of P2G systems, in combination with BESs and gas turbines, over extended periods. Through three progressively more complex case studies, we assess the performance of DRL algorithms, specifically Deep Q-Networks and Proximal Policy Optimization, and introduce modifications to enhance their effectiveness. These modifications include integrating forecasts, implementing penalties on the reward function, and applying strategic cost calculations, all aimed at addressing the issue of delayed rewards. Our findings indicate that while DRL initially struggles with the complex decision-making required for P2G system operation, the adjustments we propose significantly improve its capability to devise cost-effective operation strategies, thereby unlocking the potential for long-term energy storage in P2G technologies.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Systematic Review of Poisoning Attacks Against Large Language Models</title>
<link>https://arxiv.org/abs/2506.06518</link>
<guid>https://arxiv.org/abs/2506.06518</guid>
<content:encoded><![CDATA[
arXiv:2506.06518v1 Announce Type: cross 
Abstract: With the widespread availability of pretrained Large Language Models (LLMs) and their training datasets, concerns about the security risks associated with their usage has increased significantly. One of these security risks is the threat of LLM poisoning attacks where an attacker modifies some part of the LLM training process to cause the LLM to behave in a malicious way. As an emerging area of research, the current frameworks and terminology for LLM poisoning attacks are derived from earlier classification poisoning literature and are not fully equipped for generative LLM settings. We conduct a systematic review of published LLM poisoning attacks to clarify the security implications and address inconsistencies in terminology across the literature. We propose a comprehensive poisoning threat model applicable to categorize a wide range of LLM poisoning attacks. The poisoning threat model includes four poisoning attack specifications that define the logistics and manipulation strategies of an attack as well as six poisoning metrics used to measure key characteristics of an attack. Under our proposed framework, we organize our discussion of published LLM poisoning literature along four critical dimensions of LLM poisoning attacks: concept poisons, stealthy poisons, persistent poisons, and poisons for unique tasks, to better understand the current landscape of security risks.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Direct Fisher Score Estimation for Likelihood Maximization</title>
<link>https://arxiv.org/abs/2506.06542</link>
<guid>https://arxiv.org/abs/2506.06542</guid>
<content:encoded><![CDATA[
arXiv:2506.06542v1 Announce Type: cross 
Abstract: We study the problem of likelihood maximization when the likelihood function is intractable but model simulations are readily available. We propose a sequential, gradient-based optimization method that directly models the Fisher score based on a local score matching technique which uses simulations from a localized region around each parameter iterate. By employing a linear parameterization to the surrogate score model, our technique admits a closed-form, least-squares solution. This approach yields a fast, flexible, and efficient approximation to the Fisher score, effectively smoothing the likelihood objective and mitigating the challenges posed by complex likelihood landscapes. We provide theoretical guarantees for our score estimator, including bounds on the bias introduced by the smoothing. Empirical results on a range of synthetic and real-world problems demonstrate the superior performance of our method compared to existing benchmarks.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Infinity Search: Approximate Vector Search with Projections on q-Metric Spaces</title>
<link>https://arxiv.org/abs/2506.06557</link>
<guid>https://arxiv.org/abs/2506.06557</guid>
<content:encoded><![CDATA[
arXiv:2506.06557v1 Announce Type: cross 
Abstract: Despite the ubiquity of vector search applications, prevailing search algorithms overlook the metric structure of vector embeddings, treating it as a constraint rather than exploiting its underlying properties. In this paper, we demonstrate that in $q$-metric spaces, metric trees can leverage a stronger version of the triangle inequality to reduce comparisons for exact search. Notably, as $q$ approaches infinity, the search complexity becomes logarithmic. Therefore, we propose a novel projection method that embeds vector datasets with arbitrary dissimilarity measures into $q$-metric spaces while preserving the nearest neighbor. We propose to learn an approximation of this projection to efficiently transform query points to a space where euclidean distances satisfy the desired properties. Our experimental results with text and image vector embeddings show that learning $q$-metric approximations enables classic metric tree algorithms -- which typically underperform with high-dimensional data -- to achieve competitive performance against state-of-the-art search methods.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Securing Traffic Sign Recognition Systems in Autonomous Vehicles</title>
<link>https://arxiv.org/abs/2506.06563</link>
<guid>https://arxiv.org/abs/2506.06563</guid>
<content:encoded><![CDATA[
arXiv:2506.06563v1 Announce Type: cross 
Abstract: Deep Neural Networks (DNNs) are widely used for traffic sign recognition because they can automatically extract high-level features from images. These DNNs are trained on large-scale datasets obtained from unknown sources. Therefore, it is important to ensure that the models remain secure and are not compromised or poisoned during training. In this paper, we investigate the robustness of DNNs trained for traffic sign recognition. First, we perform the error-minimizing attacks on DNNs used for traffic sign recognition by adding imperceptible perturbations on training data. Then, we propose a data augmentation-based training method to mitigate the error-minimizing attacks. The proposed training method utilizes nonlinear transformations to disrupt the perturbations and improve the model robustness. We experiment with two well-known traffic sign datasets to demonstrate the severity of the attack and the effectiveness of our mitigation scheme. The error-minimizing attacks reduce the prediction accuracy of the DNNs from 99.90% to 10.6%. However, our mitigation scheme successfully restores the prediction accuracy to 96.05%. Moreover, our approach outperforms adversarial training in mitigating the error-minimizing attacks. Furthermore, we propose a detection model capable of identifying poisoned data even when the perturbations are imperceptible to human inspection. Our detection model achieves a success rate of over 99% in identifying the attack. This research highlights the need to employ advanced training methods for DNNs in traffic sign recognition systems to mitigate the effects of data poisoning attacks.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Future of Work with AI Agents: Auditing Automation and Augmentation Potential across the U.S. Workforce</title>
<link>https://arxiv.org/abs/2506.06576</link>
<guid>https://arxiv.org/abs/2506.06576</guid>
<content:encoded><![CDATA[
arXiv:2506.06576v1 Announce Type: cross 
Abstract: The rapid rise of compound AI systems (a.k.a., AI agents) is reshaping the labor market, raising concerns about job displacement, diminished human agency, and overreliance on automation. Yet, we lack a systematic understanding of the evolving landscape. In this paper, we address this gap by introducing a novel auditing framework to assess which occupational tasks workers want AI agents to automate or augment, and how those desires align with the current technological capabilities. Our framework features an audio-enhanced mini-interview to capture nuanced worker desires and introduces the Human Agency Scale (HAS) as a shared language to quantify the preferred level of human involvement. Using this framework, we construct the WORKBank database, building on the U.S. Department of Labor's O*NET database, to capture preferences from 1,500 domain workers and capability assessments from AI experts across over 844 tasks spanning 104 occupations. Jointly considering the desire and technological capability divides tasks in WORKBank into four zones: Automation "Green Light" Zone, Automation "Red Light" Zone, R&amp;D Opportunity Zone, Low Priority Zone. This highlights critical mismatches and opportunities for AI agent development. Moving beyond a simple automate-or-not dichotomy, our results reveal diverse HAS profiles across occupations, reflecting heterogeneous expectations for human involvement. Moreover, our study offers early signals of how AI agent integration may reshape the core human competencies, shifting from information-focused skills to interpersonal ones. These findings underscore the importance of aligning AI agent development with human desires and preparing workers for evolving workplace dynamics.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Model-Based and Adaptive Control to Evolving Fuzzy Control</title>
<link>https://arxiv.org/abs/2506.06594</link>
<guid>https://arxiv.org/abs/2506.06594</guid>
<content:encoded><![CDATA[
arXiv:2506.06594v1 Announce Type: cross 
Abstract: Evolving fuzzy systems build and adapt fuzzy models - such as predictors and controllers - by incrementally updating their rule-base structure from data streams. On the occasion of the 60-year anniversary of fuzzy set theory, commemorated during the Fuzz-IEEE 2025 event, this brief paper revisits the historical development and core contributions of classical fuzzy and adaptive modeling and control frameworks. It then highlights the emergence and significance of evolving intelligent systems in fuzzy modeling and control, emphasizing their advantages in handling nonstationary environments. Key challenges and future directions are discussed, including safety, interpretability, and principled structural evolution.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scoring the Unscorables: Cyber Risk Assessment Beyond Internet Scans</title>
<link>https://arxiv.org/abs/2506.06604</link>
<guid>https://arxiv.org/abs/2506.06604</guid>
<content:encoded><![CDATA[
arXiv:2506.06604v1 Announce Type: cross 
Abstract: In this paper we present a study on using novel data types to perform cyber risk quantification by estimating the likelihood of a data breach. We demonstrate that it is feasible to build a highly accurate cyber risk assessment model using public and readily available technology signatures obtained from crawling an organization's website. This approach overcomes the limitations of previous similar approaches that relied on large-scale IP address based scanning data, which suffers from incomplete/missing IP address mappings as well as the lack of such data for large numbers of small and medium-sized organizations (SMEs). In comparison to scan data, technology digital signature data is more readily available for millions of SMEs. Our study shows that there is a strong relationship between these technology signatures and an organization's cybersecurity posture. In cross-validating our model using different cyber incident datasets, we also highlight the key differences between ransomware attack victims and the larger population of cyber incident and data breach victims.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training-Free Tokenizer Transplantation via Orthogonal Matching Pursuit</title>
<link>https://arxiv.org/abs/2506.06607</link>
<guid>https://arxiv.org/abs/2506.06607</guid>
<content:encoded><![CDATA[
arXiv:2506.06607v1 Announce Type: cross 
Abstract: We present a training-free method to transplant tokenizers in pretrained large language models (LLMs) by reconstructing unseen token embeddings via Orthogonal Matching Pursuit (OMP). Specifically, we approximate each out-of-vocabulary token as a sparse linear combination of shared tokens, in two phases: first, compute each new token's representation in the donor embedding space with a small dictionary of shared anchor tokens, then transfer these same sparse coefficients back into the base model's embedding space.
  On two challenging cross-tokenizer tasks--Llama$\to$Mistral NeMo (12B) and Qwen$\to$Llama (1B)--we show that OMP achieves best zero-shot preservation of the base model's performance across multiple benchmarks, while other zero-shot approaches degrade significantly. Compared to baselines (zero-init, mean-init, and existing approaches like WECHSEL, FOCUS, ZETT), OMP consistently achieves the best overall performance, effectively bridging large tokenizer discrepancies without gradient updates. Our analysis further identifies mismatched numerical tokenization schemes as a critical challenge for preserving mathematical reasoning capabilities. This technique enables direct reuse of pretrained model weights with new tokenizers, facilitating cross-tokenizer knowledge distillation, speculative decoding, ensembling, merging, and domain-specific vocabulary adaptations. We integrate our method into the open-source mergekit-tokensurgeon tool for post hoc vocabulary realignment.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transferring Features Across Language Models With Model Stitching</title>
<link>https://arxiv.org/abs/2506.06609</link>
<guid>https://arxiv.org/abs/2506.06609</guid>
<content:encoded><![CDATA[
arXiv:2506.06609v1 Announce Type: cross 
Abstract: In this work, we demonstrate that affine mappings between residual streams of language models is a cheap way to effectively transfer represented features between models. We apply this technique to transfer the weights of Sparse Autoencoders (SAEs) between models of different sizes to compare their representations. We find that small and large models learn highly similar representation spaces, which motivates training expensive components like SAEs on a smaller model and transferring to a larger model at a FLOPs savings. For example, using a small-to-large transferred SAE as initialization can lead to 50% cheaper training runs when training SAEs on larger models. Next, we show that transferred probes and steering vectors can effectively recover ground truth performance. Finally, we dive deeper into feature-level transferability, finding that semantic and structural features transfer noticeably differently while specific classes of functional features have their roles faithfully mapped. Overall, our findings illustrate similarities and differences in the linear representation spaces of small and large models and demonstrate a method for improving the training efficiency of SAEs.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Learnability of Sample-Compressible Distributions under Noisy or Adversarial Perturbations</title>
<link>https://arxiv.org/abs/2506.06613</link>
<guid>https://arxiv.org/abs/2506.06613</guid>
<content:encoded><![CDATA[
arXiv:2506.06613v1 Announce Type: cross 
Abstract: Learning distribution families over $\mathbb{R}^d$ is a fundamental problem in unsupervised learning and statistics. A central question in this setting is whether a given family of distributions possesses sufficient structure to be (at least) information-theoretically learnable and, if so, to characterize its sample complexity. In 2018, Ashtiani et al. reframed \emph{sample compressibility}, originally due to Littlestone and Warmuth (1986), as a structural property of distribution classes, proving that it guarantees PAC-learnability. This discovery subsequently enabled a series of recent advancements in deriving nearly tight sample complexity bounds for various high-dimensional open problems. It has been further conjectured that the converse also holds: every learnable class admits a tight sample compression scheme.
  In this work, we establish that sample compressible families remain learnable even from perturbed samples, subject to a set of necessary and sufficient conditions. We analyze two models of data perturbation: (i) an additive independent noise model, and (ii) an adversarial corruption model, where an adversary manipulates a limited subset of the samples unknown to the learner. Our results are general and rely on as minimal assumptions as possible. We develop a perturbation-quantization framework that interfaces naturally with the compression scheme and leads to sample complexity bounds that scale gracefully with the noise level and corruption budget. As concrete applications, we establish new sample complexity bounds for learning finite mixtures of high-dimensional uniform distributions under both noise and adversarial perturbations, as well as for learning Gaussian mixture models from adversarially corrupted samples, resolving two open problems in the literature.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explaining Risks: Axiomatic Risk Attributions for Financial Models</title>
<link>https://arxiv.org/abs/2506.06653</link>
<guid>https://arxiv.org/abs/2506.06653</guid>
<content:encoded><![CDATA[
arXiv:2506.06653v1 Announce Type: cross 
Abstract: In recent years, machine learning models have achieved great success at the expense of highly complex black-box structures. By using axiomatic attribution methods, we can fairly allocate the contributions of each feature, thus allowing us to interpret the model predictions. In high-risk sectors such as finance, risk is just as important as mean predictions. Throughout this work, we address the following risk attribution problem: how to fairly allocate the risk given a model with data? We demonstrate with analysis and empirical examples that risk can be well allocated by extending the Shapley value framework.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flood-DamageSense: Multimodal Mamba with Multitask Learning for Building Flood Damage Assessment using SAR Remote Sensing Imagery</title>
<link>https://arxiv.org/abs/2506.06667</link>
<guid>https://arxiv.org/abs/2506.06667</guid>
<content:encoded><![CDATA[
arXiv:2506.06667v1 Announce Type: cross 
Abstract: Most post-disaster damage classifiers succeed only when destructive forces leave clear spectral or structural signatures -- conditions rarely present after inundation. Consequently, existing models perform poorly at identifying flood-related building damages. The model presented in this study, Flood-DamageSense, addresses this gap as the first deep-learning framework purpose-built for building-level flood-damage assessment. The architecture fuses pre- and post-event SAR/InSAR scenes with very-high-resolution optical basemaps and an inherent flood-risk layer that encodes long-term exposure probabilities, guiding the network toward plausibly affected structures even when compositional change is minimal. A multimodal Mamba backbone with a semi-Siamese encoder and task-specific decoders jointly predicts (1) graded building-damage states, (2) floodwater extent, and (3) building footprints. Training and evaluation on Hurricane Harvey (2017) imagery from Harris County, Texas -- supported by insurance-derived property-damage extents -- show a mean F1 improvement of up to 19 percentage points over state-of-the-art baselines, with the largest gains in the frequently misclassified "minor" and "moderate" damage categories. Ablation studies identify the inherent-risk feature as the single most significant contributor to this performance boost. An end-to-end post-processing pipeline converts pixel-level outputs to actionable, building-scale damage maps within minutes of image acquisition. By combining risk-aware modeling with SAR's all-weather capability, Flood-DamageSense delivers faster, finer-grained, and more reliable flood-damage intelligence to support post-disaster decision-making and resource allocation.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretation of Deep Learning Model in Embryo Selection for In Vitro Fertilization (IVF) Treatment</title>
<link>https://arxiv.org/abs/2506.06680</link>
<guid>https://arxiv.org/abs/2506.06680</guid>
<content:encoded><![CDATA[
arXiv:2506.06680v1 Announce Type: cross 
Abstract: Infertility has a considerable impact on individuals' quality of life, affecting them socially and psychologically, with projections indicating a rise in the upcoming years. In vitro fertilization (IVF) emerges as one of the primary techniques within economically developed nations, employed to address the rising problem of low fertility. Expert embryologists conventionally grade embryos by reviewing blastocyst images to select the most optimal for transfer, yet this process is time-consuming and lacks efficiency. Blastocyst images provide a valuable resource for assessing embryo viability. In this study, we introduce an explainable artificial intelligence (XAI) framework for classifying embryos, employing a fusion of convolutional neural network (CNN) and long short-term memory (LSTM) architecture, referred to as CNN-LSTM. Utilizing deep learning, our model achieves high accuracy in embryo classification while maintaining interpretability through XAI.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contextual Experience Replay for Self-Improvement of Language Agents</title>
<link>https://arxiv.org/abs/2506.06698</link>
<guid>https://arxiv.org/abs/2506.06698</guid>
<content:encoded><![CDATA[
arXiv:2506.06698v1 Announce Type: cross 
Abstract: Large language model (LLM) agents have been applied to sequential decision-making tasks such as web navigation, but without any environment-specific experiences, they often fail in these complex tasks. Moreover, current LLM agents are not designed to continually learn from past experiences during inference time, which could be crucial for them to gain these environment-specific experiences. To address this, we propose Contextual Experience Replay (CER), a training-free framework to enable efficient self-improvement for language agents in their context window. Specifically, CER accumulates and synthesizes past experiences into a dynamic memory buffer. These experiences encompass environment dynamics and common decision-making patterns, allowing the agents to retrieve and augment themselves with relevant knowledge in new tasks, enhancing their adaptability in complex environments. We evaluate CER on the challenging WebArena and VisualWebArena benchmarks. On VisualWebArena, CER achieves a competitive performance of 31.9%. On WebArena, CER also gets a competitive average success rate of 36.7%, relatively improving the success rate of the GPT-4o agent baseline by 51.0%. We also conduct a comprehensive analysis on it to prove its efficiency, validity and understand it better.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IQFM A Wireless Foundational Model for I/Q Streams in AI-Native 6G</title>
<link>https://arxiv.org/abs/2506.06718</link>
<guid>https://arxiv.org/abs/2506.06718</guid>
<content:encoded><![CDATA[
arXiv:2506.06718v1 Announce Type: cross 
Abstract: Foundational models have shown remarkable potential in natural language processing and computer vision, yet remain in their infancy in wireless communications. While a few efforts have explored image-based modalities such as channel state information (CSI) and frequency spectrograms, foundational models that operate directly on raw IQ data remain largely unexplored. This paper presents, IQFM, the first I/Q signal foundational model for wireless communications. IQFM supporting diverse tasks: modulation classification, angle-of-arrival (AoA), beam prediction, and RF fingerprinting, without heavy preprocessing or handcrafted features. We also introduce a task-aware augmentation strategy that categorizes transformations into core augmentations, such as cyclic time shifting, and task-specific augmentations. This strategy forms the basis for structured, task-dependent representation learning within a contrastive self-supervised learning (SSL) framework. Using this strategy, the lightweight encoder, pre-trained via SSL on over-the-air multi-antenna IQ data, achieves up to 99.67% and 65.45% accuracy on modulation and AoA classification, respectively, using only one labeled sample per class, outperforming supervised baselines by up to 7x and 145x. The model also generalizes to out-of-distribution tasks; when adapted to new tasks using only 500 samples per class and minimal parameter updates via LoRA, the same frozen encoder achieves 94.15% on beam prediction (vs. 89.53% supervised), 50.00% on RML2016a modulation classification (vs. 49.30%), and 96.05% on RF fingerprinting (vs. 96.64%). These results demonstrate the potential of raw IQ-based foundational models as efficient, reusable encoders for multi-task learning in AI-native 6G systems.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WorldLLM: Improving LLMs' world modeling using curiosity-driven theory-making</title>
<link>https://arxiv.org/abs/2506.06725</link>
<guid>https://arxiv.org/abs/2506.06725</guid>
<content:encoded><![CDATA[
arXiv:2506.06725v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) possess general world knowledge but often struggle to generate precise predictions in structured, domain-specific contexts such as simulations. These limitations arise from their inability to ground their broad, unstructured understanding in specific environments. To address this, we present WorldLLM, a framework that enhances LLM-based world modeling by combining Bayesian inference and autonomous active exploration with reinforcement learning. WorldLLM leverages the in-context learning abilities of LLMs to guide an LLM-based world model's predictions using natural language hypotheses given in its prompt. These hypotheses are iteratively refined through a Bayesian inference framework that leverages a second LLM as the proposal distribution given collected evidence. This evidence is collected using a curiosity-driven reinforcement learning policy that explores the environment to find transitions with a low log-likelihood under our LLM-based predictive model using the current hypotheses. By alternating between refining hypotheses and collecting new evidence, our framework autonomously drives continual improvement of the predictions. Our experiments demonstrate the effectiveness of WorldLLM in a textual game environment that requires agents to manipulate and combine objects. The framework not only enhances predictive accuracy, but also generates human-interpretable theories of environment dynamics.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Honey, I shrunk the hypothesis space (through logical preprocessing)</title>
<link>https://arxiv.org/abs/2506.06739</link>
<guid>https://arxiv.org/abs/2506.06739</guid>
<content:encoded><![CDATA[
arXiv:2506.06739v1 Announce Type: cross 
Abstract: Inductive logic programming (ILP) is a form of logical machine learning. The goal is to search a hypothesis space for a hypothesis that generalises training examples and background knowledge. We introduce an approach that 'shrinks' the hypothesis space before an ILP system searches it. Our approach uses background knowledge to find rules that cannot be in an optimal hypothesis regardless of the training examples. For instance, our approach discovers relationships such as "even numbers cannot be odd" and "prime numbers greater than 2 are odd". It then removes violating rules from the hypothesis space. We implement our approach using answer set programming and use it to shrink the hypothesis space of a constraint-based ILP system. Our experiments on multiple domains, including visual reasoning and game playing, show that our approach can substantially reduce learning times whilst maintaining predictive accuracies. For instance, given just 10 seconds of preprocessing time, our approach can reduce learning times from over 10 hours to only 2 seconds.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bio-Inspired Classification: Combining Information Theory and Spiking Neural Networks -- Influence of the Learning Rules</title>
<link>https://arxiv.org/abs/2506.06750</link>
<guid>https://arxiv.org/abs/2506.06750</guid>
<content:encoded><![CDATA[
arXiv:2506.06750v1 Announce Type: cross 
Abstract: Training of Spiking Neural Networks (SNN) is challenging due to their unique properties, including temporal dynamics, non-differentiability of spike events, and sparse event-driven activations. In this paper, we widely consider the influence of the type of chosen learning algorithm, including bioinspired learning rules on the accuracy of classification. We proposed a bioinspired classifier based on the combination of SNN and Lempel-Ziv complexity (LZC). This approach synergizes the strengths of SNNs in temporal precision and biological realism with LZC's structural complexity analysis, facilitating efficient and interpretable classification of spatiotemporal neural data. It turned out that the classic backpropagation algorithm achieves excellent classification accuracy, but at extremely high computational cost, which makes it impractical for real-time applications. Biologically inspired learning algorithms such as tempotron and Spikprop provide increased computational efficiency while maintaining competitive classification performance, making them suitable for time-sensitive tasks. The results obtained indicate that the selection of the most appropriate learning algorithm depends on the trade-off between classification accuracy and computational cost as well as application constraints.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taming Wild Branches: Overcoming Hard-to-Predict Branches using the Bullseye Predictor</title>
<link>https://arxiv.org/abs/2506.06773</link>
<guid>https://arxiv.org/abs/2506.06773</guid>
<content:encoded><![CDATA[
arXiv:2506.06773v1 Announce Type: cross 
Abstract: Branch prediction is key to the performance of out-of-order processors. While the CBP-2016 winner TAGE-SC-L combines geometric-history tables, a statistical corrector, and a loop predictor, over half of its remaining mispredictions stem from a small set of hard-to-predict (H2P) branches. These branches occur under diverse global histories, causing repeated thrashing in TAGE and eviction before usefulness counters can mature. Prior work shows that simply enlarging the tables offers only marginal improvement.
  We augment a 159 KB TAGE-SC-L predictor with a 28 KB H2P-targeted subsystem called the Bullseye predictor. It identifies problematic PCs using a set-associative H2P Identification Table (HIT) and steers them to one of two branch-specific perceptrons, one indexed by hashed local history and the other by folded global history. A short trial phase tracks head-to-head accuracy in an H2P cache. A branch becomes perceptron-resident only if the perceptron's sustained accuracy and output magnitude exceed dynamic thresholds, after which TAGE updates for that PC are suppressed to reduce pollution. The HIT, cache, and perceptron operate fully in parallel with TAGE-SC-L, providing higher fidelity on the H2P tail. This achieves an average MPKI of 3.4045 and CycWpPKI of 145.09.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuous Semi-Implicit Models</title>
<link>https://arxiv.org/abs/2506.06778</link>
<guid>https://arxiv.org/abs/2506.06778</guid>
<content:encoded><![CDATA[
arXiv:2506.06778v1 Announce Type: cross 
Abstract: Semi-implicit distributions have shown great promise in variational inference and generative modeling. Hierarchical semi-implicit models, which stack multiple semi-implicit layers, enhance the expressiveness of semi-implicit distributions and can be used to accelerate diffusion models given pretrained score networks. However, their sequential training often suffers from slow convergence. In this paper, we introduce CoSIM, a continuous semi-implicit model that extends hierarchical semi-implicit models into a continuous framework. By incorporating a continuous transition kernel, CoSIM enables efficient, simulation-free training. Furthermore, we show that CoSIM achieves consistency with a carefully designed transition kernel, offering a novel approach for multistep distillation of generative models at the distributional level. Extensive experiments on image generation demonstrate that CoSIM performs on par or better than existing diffusion model acceleration methods, achieving superior performance on FD-DINOv2.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuous-Time SO(3) Forecasting with Savitzky--Golay Neural Controlled Differential Equations</title>
<link>https://arxiv.org/abs/2506.06780</link>
<guid>https://arxiv.org/abs/2506.06780</guid>
<content:encoded><![CDATA[
arXiv:2506.06780v1 Announce Type: cross 
Abstract: Tracking and forecasting the rotation of objects is fundamental in computer vision and robotics, yet SO(3) extrapolation remains challenging as (1) sensor observations can be noisy and sparse, (2) motion patterns can be governed by complex dynamics, and (3) application settings can demand long-term forecasting. This work proposes modeling continuous-time rotational object dynamics on $SO(3)$ using Neural Controlled Differential Equations guided by Savitzky-Golay paths. Unlike existing methods that rely on simplified motion assumptions, our method learns a general latent dynamical system of the underlying object trajectory while respecting the geometric structure of rotations. Experimental results on real-world data demonstrate compelling forecasting capabilities compared to existing approaches.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Label-semantics Aware Generative Approach for Domain-Agnostic Multilabel Classification</title>
<link>https://arxiv.org/abs/2506.06806</link>
<guid>https://arxiv.org/abs/2506.06806</guid>
<content:encoded><![CDATA[
arXiv:2506.06806v1 Announce Type: cross 
Abstract: The explosion of textual data has made manual document classification increasingly challenging. To address this, we introduce a robust, efficient domain-agnostic generative model framework for multi-label text classification. Instead of treating labels as mere atomic symbols, our approach utilizes predefined label descriptions and is trained to generate these descriptions based on the input text. During inference, the generated descriptions are matched to the pre-defined labels using a finetuned sentence transformer. We integrate this with a dual-objective loss function, combining cross-entropy loss and cosine similarity of the generated sentences with the predefined target descriptions, ensuring both semantic alignment and accuracy. Our proposed model LAGAMC stands out for its parameter efficiency and versatility across diverse datasets, making it well-suited for practical applications. We demonstrate the effectiveness of our proposed model by achieving new state-of-the-art performances across all evaluated datasets, surpassing several strong baselines. We achieve improvements of 13.94% in Micro-F1 and 24.85% in Macro-F1 compared to the closest baseline across all datasets.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ASPO: Constraint-Aware Bayesian Optimization for FPGA-based Soft Processors</title>
<link>https://arxiv.org/abs/2506.06817</link>
<guid>https://arxiv.org/abs/2506.06817</guid>
<content:encoded><![CDATA[
arXiv:2506.06817v1 Announce Type: cross 
Abstract: Bayesian Optimization (BO) has shown promise in tuning processor design parameters. However, standard BO does not support constraints involving categorical parameters such as types of branch predictors and division circuits. In addition, optimization time of BO grows with processor complexity, which becomes increasingly significant especially for FPGA-based soft processors. This paper introduces ASPO, an approach that leverages disjunctive form to enable BO to handle constraints involving categorical parameters. Unlike existing methods that directly apply standard BO, the proposed ASPO method, for the first time, customizes the mathematical mechanism of BO to address challenges faced by soft-processor designs on FPGAs. Specifically, ASPO supports categorical parameters using a novel customized BO covariance kernel. It also accelerates the design evaluation procedure by penalizing the BO acquisition function with potential evaluation time and by reusing FPGA synthesis checkpoints from previously evaluated configurations. ASPO targets three soft processors: RocketChip, BOOM, and EL2 VeeR. The approach is evaluated based on seven RISC-V benchmarks. Results show that ASPO can reduce execution time for the ``multiply'' benchmark on the BOOM processor by up to 35\% compared to the default configuration. Furthermore, it reduces design time for the BOOM processor by up to 74\% compared to Boomerang, a state-of-the-art hardware-oriented BO approach.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Currents of Conflict: Decomposing Conflict Trends with Gaussian Processes</title>
<link>https://arxiv.org/abs/2506.06828</link>
<guid>https://arxiv.org/abs/2506.06828</guid>
<content:encoded><![CDATA[
arXiv:2506.06828v1 Announce Type: cross 
Abstract: I present a novel approach to estimating the temporal and spatial patterns of violent conflict. I show how we can use highly temporally and spatially disaggregated data on conflict events in tandem with Gaussian processes to estimate temporospatial conflict trends. These trends can be studied to gain insight into conflict traps, diffusion and tempo-spatial conflict exposure in general; they can also be used to control for such phenomenons given other estimation tasks; lastly, the approach allow us to extrapolate the estimated tempo-spatial conflict patterns into future temporal units, thus facilitating powerful, stat-of-the-art, conflict forecasts. Importantly, these results are achieved via a relatively parsimonious framework using only one data source: past conflict patterns.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing Vision-Language Models for Time Series Anomaly Detection</title>
<link>https://arxiv.org/abs/2506.06836</link>
<guid>https://arxiv.org/abs/2506.06836</guid>
<content:encoded><![CDATA[
arXiv:2506.06836v1 Announce Type: cross 
Abstract: Time-series anomaly detection (TSAD) has played a vital role in a variety of fields, including healthcare, finance, and industrial monitoring. Prior methods, which mainly focus on training domain-specific models on numerical data, lack the visual-temporal reasoning capacity that human experts have to identify contextual anomalies. To fill this gap, we explore a solution based on vision language models (VLMs). Recent studies have shown the ability of VLMs for visual reasoning tasks, yet their direct application to time series has fallen short on both accuracy and efficiency. To harness the power of VLMs for TSAD, we propose a two-stage solution, with (1) ViT4TS, a vision-screening stage built on a relatively lightweight pretrained vision encoder, which leverages 2-D time-series representations to accurately localize candidate anomalies; (2) VLM4TS, a VLM-based stage that integrates global temporal context and VLM reasoning capacity to refine the detection upon the candidates provided by ViT4TS. We show that without any time-series training, VLM4TS outperforms time-series pretrained and from-scratch baselines in most cases, yielding a 24.6 percent improvement in F1-max score over the best baseline. Moreover, VLM4TS also consistently outperforms existing language-model-based TSAD methods and is on average 36 times more efficient in token usage.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Statistical Framework for Model Selection in LSTM Networks</title>
<link>https://arxiv.org/abs/2506.06840</link>
<guid>https://arxiv.org/abs/2506.06840</guid>
<content:encoded><![CDATA[
arXiv:2506.06840v1 Announce Type: cross 
Abstract: Long Short-Term Memory (LSTM) neural network models have become the cornerstone for sequential data modeling in numerous applications, ranging from natural language processing to time series forecasting. Despite their success, the problem of model selection, including hyperparameter tuning, architecture specification, and regularization choice remains largely heuristic and computationally expensive. In this paper, we propose a unified statistical framework for systematic model selection in LSTM networks. Our framework extends classical model selection ideas, such as information criteria and shrinkage estimation, to sequential neural networks. We define penalized likelihoods adapted to temporal structures, propose a generalized threshold approach for hidden state dynamics, and provide efficient estimation strategies using variational Bayes and approximate marginal likelihood methods. Several biomedical data centric examples demonstrate the flexibility and improved performance of the proposed framework.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Spatial Language Maps for Robot Navigation and Manipulation</title>
<link>https://arxiv.org/abs/2506.06862</link>
<guid>https://arxiv.org/abs/2506.06862</guid>
<content:encoded><![CDATA[
arXiv:2506.06862v1 Announce Type: cross 
Abstract: Grounding language to a navigating agent's observations can leverage pretrained multimodal foundation models to match perceptions to object or event descriptions. However, previous approaches remain disconnected from environment mapping, lack the spatial precision of geometric maps, or neglect additional modality information beyond vision. To address this, we propose multimodal spatial language maps as a spatial map representation that fuses pretrained multimodal features with a 3D reconstruction of the environment. We build these maps autonomously using standard exploration. We present two instances of our maps, which are visual-language maps (VLMaps) and their extension to audio-visual-language maps (AVLMaps) obtained by adding audio information. When combined with large language models (LLMs), VLMaps can (i) translate natural language commands into open-vocabulary spatial goals (e.g., "in between the sofa and TV") directly localized in the map, and (ii) be shared across different robot embodiments to generate tailored obstacle maps on demand. Building upon the capabilities above, AVLMaps extend VLMaps by introducing a unified 3D spatial representation integrating audio, visual, and language cues through the fusion of features from pretrained multimodal foundation models. This enables robots to ground multimodal goal queries (e.g., text, images, or audio snippets) to spatial locations for navigation. Additionally, the incorporation of diverse sensory inputs significantly enhances goal disambiguation in ambiguous environments. Experiments in simulation and real-world settings demonstrate that our multimodal spatial language maps enable zero-shot spatial and multimodal goal navigation and improve recall by 50% in ambiguous scenarios. These capabilities extend to mobile robots and tabletop manipulators, supporting navigation and interaction guided by visual, audio, and spatial cues.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NSD-Imagery: A benchmark dataset for extending fMRI vision decoding methods to mental imagery</title>
<link>https://arxiv.org/abs/2506.06898</link>
<guid>https://arxiv.org/abs/2506.06898</guid>
<content:encoded><![CDATA[
arXiv:2506.06898v1 Announce Type: cross 
Abstract: We release NSD-Imagery, a benchmark dataset of human fMRI activity paired with mental images, to complement the existing Natural Scenes Dataset (NSD), a large-scale dataset of fMRI activity paired with seen images that enabled unprecedented improvements in fMRI-to-image reconstruction efforts. Recent models trained on NSD have been evaluated only on seen image reconstruction. Using NSD-Imagery, it is possible to assess how well these models perform on mental image reconstruction. This is a challenging generalization requirement because mental images are encoded in human brain activity with relatively lower signal-to-noise and spatial resolution; however, generalization from seen to mental imagery is critical for real-world applications in medical domains and brain-computer interfaces, where the desired information is always internally generated. We provide benchmarks for a suite of recent NSD-trained open-source visual decoding models (MindEye1, MindEye2, Brain Diffuser, iCNN, Takagi et al.) on NSD-Imagery, and show that the performance of decoding methods on mental images is largely decoupled from performance on vision reconstruction. We further demonstrate that architectural choices significantly impact cross-decoding performance: models employing simple linear decoding architectures and multimodal feature decoding generalize better to mental imagery, while complex architectures tend to overfit visual training data. Our findings indicate that mental imagery datasets are critical for the development of practical applications, and establish NSD-Imagery as a useful resource for better aligning visual decoding methods with this goal.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Neural Networks in Modern AI-aided Drug Discovery</title>
<link>https://arxiv.org/abs/2506.06915</link>
<guid>https://arxiv.org/abs/2506.06915</guid>
<content:encoded><![CDATA[
arXiv:2506.06915v1 Announce Type: cross 
Abstract: Graph neural networks (GNNs), as topology/structure-aware models within deep learning, have emerged as powerful tools for AI-aided drug discovery (AIDD). By directly operating on molecular graphs, GNNs offer an intuitive and expressive framework for learning the complex topological and geometric features of drug-like molecules, cementing their role in modern molecular modeling. This review provides a comprehensive overview of the methodological foundations and representative applications of GNNs in drug discovery, spanning tasks such as molecular property prediction, virtual screening, molecular generation, biomedical knowledge graph construction, and synthesis planning. Particular attention is given to recent methodological advances, including geometric GNNs, interpretable models, uncertainty quantification, scalable graph architectures, and graph generative frameworks. We also discuss how these models integrate with modern deep learning approaches, such as self-supervised learning, multi-task learning, meta-learning and pre-training. Throughout this review, we highlight the practical challenges and methodological bottlenecks encountered when applying GNNs to real-world drug discovery pipelines, and conclude with a discussion on future directions.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity</title>
<link>https://arxiv.org/abs/2506.06941</link>
<guid>https://arxiv.org/abs/2506.06941</guid>
<content:encoded><![CDATA[
arXiv:2506.06941v1 Announce Type: cross 
Abstract: Recent generations of language models have introduced Large Reasoning Models (LRMs) that generate detailed thinking processes before providing answers. While these models demonstrate improved performance on reasoning benchmarks, their fundamental capabilities, scaling properties, and limitations remain insufficiently understood. Current evaluations primarily focus on established math and coding benchmarks, emphasizing final answer accuracy. However, this evaluation paradigm often suffers from contamination and does not provide insights into the reasoning traces. In this work, we systematically investigate these gaps with the help of controllable puzzle environments that allow precise manipulation of complexity while maintaining consistent logical structures. This setup enables the analysis of not only final answers but also the internal reasoning traces, offering insights into how LRMs think. Through extensive experiments, we show that LRMs face a complete accuracy collapse beyond certain complexities. Moreover, they exhibit a counterintuitive scaling limit: their reasoning effort increases with problem complexity up to a point, then declines despite having remaining token budget. By comparing LRMs with their standard LLM counterparts under same inference compute, we identify three performance regimes: (1) low-complexity tasks where standard models outperform LRMs, (2) medium-complexity tasks where LRMs demonstrates advantage, and (3) high-complexity tasks where both models face complete collapse. We found that LRMs have limitations in exact computation: they fail to use explicit algorithms and reason inconsistently across scales. We also investigate the reasoning traces in more depth, studying the patterns of explored solutions and analyzing the models' computational behavior, shedding light on their strengths, limitations, and raising questions about their reasoning capabilities.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conditional Denoising Diffusion for ISAC Enhanced Channel Estimation in Cell-Free 6G</title>
<link>https://arxiv.org/abs/2506.06942</link>
<guid>https://arxiv.org/abs/2506.06942</guid>
<content:encoded><![CDATA[
arXiv:2506.06942v1 Announce Type: cross 
Abstract: Cell-free Integrated Sensing and Communication (ISAC) aims to revolutionize 6th Generation (6G) networks. By combining distributed access points with ISAC capabilities, it boosts spectral efficiency, situational awareness, and communication reliability. Channel estimation is a critical step in cell-free ISAC systems to ensure reliable communication, but its performance is usually limited by challenges such as pilot contamination and noisy channel estimates. This paper presents a novel framework leveraging sensing information as a key input within a Conditional Denoising Diffusion Model (CDDM). In this framework, we integrate CDDM with a Multimodal Transformer (MMT) to enhance channel estimation in ISAC-enabled cell-free systems. The MMT encoder effectively captures inter-modal relationships between sensing and location data, enabling the CDDM to iteratively denoise and refine channel estimates. Simulation results demonstrate that the proposed approach achieves significant performance gains. As compared with Least Squares (LS) and Minimum Mean Squared Error (MMSE) estimators, the proposed model achieves normalized mean squared error (NMSE) improvements of 8 dB and 9 dB, respectively. Moreover, we achieve a 27.8% NMSE improvement compared to the traditional denoising diffusion model (TDDM), which does not incorporate sensing channel information. Additionally, the model exhibits higher robustness against pilot contamination and maintains high accuracy under challenging conditions, such as low signal-to-noise ratios (SNRs). According to the simulation results, the model performs well for users near sensing targets by leveraging the correlation between sensing and communication channels.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Polar Hierarchical Mamba: Towards Streaming LiDAR Object Detection with Point Clouds as Egocentric Sequences</title>
<link>https://arxiv.org/abs/2506.06944</link>
<guid>https://arxiv.org/abs/2506.06944</guid>
<content:encoded><![CDATA[
arXiv:2506.06944v1 Announce Type: cross 
Abstract: Accurate and efficient object detection is essential for autonomous vehicles, where real-time perception requires low latency and high throughput. LiDAR sensors provide robust depth information, but conventional methods process full 360{\deg} scans in a single pass, introducing significant delay. Streaming approaches address this by sequentially processing partial scans in the native polar coordinate system, yet they rely on translation-invariant convolutions that are misaligned with polar geometry -- resulting in degraded performance or requiring complex distortion mitigation. Recent Mamba-based state space models (SSMs) have shown promise for LiDAR perception, but only in the full-scan setting, relying on geometric serialization and positional embeddings that are memory-intensive and ill-suited to streaming. We propose Polar Hierarchical Mamba (PHiM), a novel SSM architecture designed for polar-coordinate streaming LiDAR. PHiM uses local bidirectional Mamba blocks for intra-sector spatial encoding and a global forward Mamba for inter-sector temporal modeling, replacing convolutions and positional encodings with distortion-aware, dimensionally-decomposed operations. PHiM sets a new state-of-the-art among streaming detectors on the Waymo Open Dataset, outperforming the previous best by 10\% and matching full-scan baselines at twice the throughput. Code will be available at https://github.com/meilongzhang/Polar-Hierarchical-Mamba .
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Your Training Pipeline Production-Ready? A Case Study in the Healthcare Domain</title>
<link>https://arxiv.org/abs/2506.06946</link>
<guid>https://arxiv.org/abs/2506.06946</guid>
<content:encoded><![CDATA[
arXiv:2506.06946v1 Announce Type: cross 
Abstract: Deploying a Machine Learning (ML) training pipeline into production requires robust software engineering practices. This differs significantly from experimental workflows. This experience report investigates this challenge in SPIRA, a project whose goal is to create an ML-Enabled System (MLES) to pre-diagnose insufficiency respiratory via speech analysis. The first version of SPIRA's training pipeline lacked critical software quality attributes. This paper presents an overview of the MLES, then compares three versions of the architecture of the Continuous Training subsystem, which evolved from a Big Ball of Mud, to a Modular Monolith, towards Microservices. By adopting different design principles and patterns to enhance its maintainability, robustness, and extensibility. In this way, the paper seeks to offer insights for both ML Engineers tasked to productionize ML training pipelines and Data Scientists seeking to adopt MLOps practices.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Clarify by Reinforcement Learning Through Reward-Weighted Fine-Tuning</title>
<link>https://arxiv.org/abs/2506.06964</link>
<guid>https://arxiv.org/abs/2506.06964</guid>
<content:encoded><![CDATA[
arXiv:2506.06964v1 Announce Type: cross 
Abstract: Question answering (QA) agents automatically answer questions posed in natural language. In this work, we learn to ask clarifying questions in QA agents. The key idea in our method is to simulate conversations that contain clarifying questions and learn from them using reinforcement learning (RL). To make RL practical, we propose and analyze offline RL objectives that can be viewed as reward-weighted supervised fine-tuning (SFT) and easily optimized in large language models. Our work stands in a stark contrast to recently proposed methods, based on SFT and direct preference optimization, which have additional hyper-parameters and do not directly optimize rewards. We compare to these methods empirically and report gains in both optimized rewards and language quality.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep RL Needs Deep Behavior Analysis: Exploring Implicit Planning by Model-Free Agents in Open-Ended Environments</title>
<link>https://arxiv.org/abs/2506.06981</link>
<guid>https://arxiv.org/abs/2506.06981</guid>
<content:encoded><![CDATA[
arXiv:2506.06981v1 Announce Type: cross 
Abstract: Understanding the behavior of deep reinforcement learning (DRL) agents -- particularly as task and agent sophistication increase -- requires more than simple comparison of reward curves, yet standard methods for behavioral analysis remain underdeveloped in DRL. We apply tools from neuroscience and ethology to study DRL agents in a novel, complex, partially observable environment, ForageWorld, designed to capture key aspects of real-world animal foraging -- including sparse, depleting resource patches, predator threats, and spatially extended arenas. We use this environment as a platform for applying joint behavioral and neural analysis to agents, revealing detailed, quantitatively grounded insights into agent strategies, memory, and planning. Contrary to common assumptions, we find that model-free RNN-based DRL agents can exhibit structured, planning-like behavior purely through emergent dynamics -- without requiring explicit memory modules or world models. Our results show that studying DRL agents like animals -- analyzing them with neuroethology-inspired tools that reveal structure in both behavior and neural dynamics -- uncovers rich structure in their learning dynamics that would otherwise remain invisible. We distill these tools into a general analysis framework linking core behavioral and representational features to diagnostic methods, which can be reused for a wide range of tasks and agents. As agents grow more complex and autonomous, bridging neuroscience, cognitive science, and AI will be essential -- not just for understanding their behavior, but for ensuring safe alignment and maximizing desirable behaviors that are hard to measure via reward. We show how this can be done by drawing on lessons from how biological intelligence is studied.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Correcting for Position Bias in Learning to Rank: A Control Function Approach</title>
<link>https://arxiv.org/abs/2506.06989</link>
<guid>https://arxiv.org/abs/2506.06989</guid>
<content:encoded><![CDATA[
arXiv:2506.06989v1 Announce Type: cross 
Abstract: Implicit feedback data, such as user clicks, is commonly used in learning-to-rank (LTR) systems because it is easy to collect and it often reflects user preferences. However, this data is prone to various biases, and training an LTR system directly on biased data can result in suboptimal ranking performance. One of the most prominent and well-studied biases in implicit feedback data is position bias, which occurs because users are more likely to interact with higher-ranked documents regardless of their true relevance. In this paper, we propose a novel control function-based method that accounts for position bias in a two-stage process. The first stage uses exogenous variation from the residuals of the ranking process to correct for position bias in the second stage click equation. Unlike previous position bias correction methods, our method does not require knowledge of the click or propensity model and allows for nonlinearity in the underlying ranking model. Moreover, our method is general and allows for debiasing any state-of-the-art ranking algorithm by plugging it into the second stage. We also introduce a technique to debias validation clicks for hyperparameter tuning to select the optimal model in the absence of unbiased validation data. Experimental results demonstrate that our method outperforms state-of-the-art approaches in correcting for position bias.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What makes Reasoning Models Different? Follow the Reasoning Leader for Efficient Decoding</title>
<link>https://arxiv.org/abs/2506.06998</link>
<guid>https://arxiv.org/abs/2506.06998</guid>
<content:encoded><![CDATA[
arXiv:2506.06998v1 Announce Type: cross 
Abstract: Large reasoning models (LRMs) achieve strong reasoning performance by emitting long chains of thought. Yet, these verbose traces slow down inference and often drift into unnecessary detail, known as the overthinking phenomenon. To better understand LRMs' behavior, we systematically analyze the token-level misalignment between reasoning and non-reasoning models. While it is expected that their primary difference lies in the stylistic "thinking cues", LRMs uniquely exhibit two pivotal, previously under-explored phenomena: a Global Misalignment Rebound, where their divergence from non-reasoning models persists or even grows as response length increases, and more critically, a Local Misalignment Diminish, where the misalignment concentrates at the "thinking cues" each sentence starts with but rapidly declines in the remaining of the sentence. Motivated by the Local Misalignment Diminish, we propose FoReaL-Decoding, a collaborative fast-slow thinking decoding method for cost-quality trade-off. In FoReaL-Decoding, a Leading model leads the first few tokens for each sentence, and then a weaker draft model completes the following tokens to the end of each sentence. FoReaL-Decoding adopts a stochastic gate to smoothly interpolate between the small and the large model. On four popular math-reasoning benchmarks (AIME24, GPQA-Diamond, MATH500, AMC23), FoReaL-Decoding reduces theoretical FLOPs by 30 to 50% and trims CoT length by up to 40%, while preserving 86 to 100% of model performance. These results establish FoReaL-Decoding as a simple, plug-and-play route to controllable cost-quality trade-offs in reasoning-centric tasks.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Half-AVAE: Adversarial-Enhanced Factorized and Structured Encoder-Free VAE for Underdetermined Independent Component Analysis</title>
<link>https://arxiv.org/abs/2506.07011</link>
<guid>https://arxiv.org/abs/2506.07011</guid>
<content:encoded><![CDATA[
arXiv:2506.07011v1 Announce Type: cross 
Abstract: This study advances the Variational Autoencoder (VAE) framework by addressing challenges in Independent Component Analysis (ICA) under both determined and underdetermined conditions, focusing on enhancing the independence and interpretability of latent variables. Traditional VAEs map observed data to latent variables and back via an encoder-decoder architecture, but struggle with underdetermined ICA where the number of latent variables exceeds observed signals. The proposed Half Adversarial VAE (Half-AVAE) builds on the encoder-free Half-VAE framework, eliminating explicit inverse mapping to tackle underdetermined scenarios. By integrating adversarial networks and External Enhancement (EE) terms, Half-AVAE promotes mutual independence among latent dimensions, achieving factorized and interpretable representations. Experiments with synthetic signals demonstrate that Half-AVAE outperforms baseline models, including GP-AVAE and Half-VAE, in recovering independent components under underdetermined conditions, as evidenced by lower root mean square errors. The study highlights the flexibility of VAEs in variational inference, showing that encoder omission, combined with adversarial training and structured priors, enables effective solutions for complex ICA tasks, advancing applications in disentanglement, causal inference, and generative modeling.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TABLET: Table Structure Recognition using Encoder-only Transformers</title>
<link>https://arxiv.org/abs/2506.07015</link>
<guid>https://arxiv.org/abs/2506.07015</guid>
<content:encoded><![CDATA[
arXiv:2506.07015v1 Announce Type: cross 
Abstract: To address the challenges of table structure recognition, we propose a novel Split-Merge-based top-down model optimized for large, densely populated tables. Our approach formulates row and column splitting as sequence labeling tasks, utilizing dual Transformer encoders to capture feature interactions. The merging process is framed as a grid cell classification task, leveraging an additional Transformer encoder to ensure accurate and coherent merging. By eliminating unstable bounding box predictions, our method reduces resolution loss and computational complexity, achieving high accuracy while maintaining fast processing speed. Extensive experiments on FinTabNet and PubTabNet demonstrate the superiority of our model over existing approaches, particularly in real-world applications. Our method offers a robust, scalable, and efficient solution for large-scale table recognition, making it well-suited for industrial deployment.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>D2R: dual regularization loss with collaborative adversarial generation for model robustness</title>
<link>https://arxiv.org/abs/2506.07056</link>
<guid>https://arxiv.org/abs/2506.07056</guid>
<content:encoded><![CDATA[
arXiv:2506.07056v1 Announce Type: cross 
Abstract: The robustness of Deep Neural Network models is crucial for defending models against adversarial attacks. Recent defense methods have employed collaborative learning frameworks to enhance model robustness. Two key limitations of existing methods are (i) insufficient guidance of the target model via loss functions and (ii) non-collaborative adversarial generation. We, therefore, propose a dual regularization loss (D2R Loss) method and a collaborative adversarial generation (CAG) strategy for adversarial training. D2R loss includes two optimization steps. The adversarial distribution and clean distribution optimizations enhance the target model's robustness by leveraging the strengths of different loss functions obtained via a suitable function space exploration to focus more precisely on the target model's distribution. CAG generates adversarial samples using a gradient-based collaboration between guidance and target models. We conducted extensive experiments on three benchmark databases, including CIFAR-10, CIFAR-100, Tiny ImageNet, and two popular target models, WideResNet34-10 and PreActResNet18. Our results show that D2R loss with CAG produces highly robust models.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating 3D Gaussian Splatting with Neural Sorting and Axis-Oriented Rasterization</title>
<link>https://arxiv.org/abs/2506.07069</link>
<guid>https://arxiv.org/abs/2506.07069</guid>
<content:encoded><![CDATA[
arXiv:2506.07069v1 Announce Type: cross 
Abstract: 3D Gaussian Splatting (3DGS) has recently gained significant attention for high-quality and efficient view synthesis, making it widely adopted in fields such as AR/VR, robotics, and autonomous driving. Despite its impressive algorithmic performance, real-time rendering on resource-constrained devices remains a major challenge due to tight power and area budgets. This paper presents an architecture-algorithm co-design to address these inefficiencies. First, we reveal substantial redundancy caused by repeated computation of common terms/expressions during the conventional rasterization. To resolve this, we propose axis-oriented rasterization, which pre-computes and reuses shared terms along both the X and Y axes through a dedicated hardware design, effectively reducing multiply-and-add (MAC) operations by up to 63%. Second, by identifying the resource and performance inefficiency of the sorting process, we introduce a novel neural sorting approach that predicts order-independent blending weights using an efficient neural network, eliminating the need for costly hardware sorters. A dedicated training framework is also proposed to improve its algorithmic stability. Third, to uniformly support rasterization and neural network inference, we design an efficient reconfigurable processing array that maximizes hardware utilization and throughput. Furthermore, we introduce a $\pi$-trajectory tile schedule, inspired by Morton encoding and Hilbert curve, to optimize Gaussian reuse and reduce memory access overhead. Comprehensive experiments demonstrate that the proposed design preserves rendering quality while achieving a speedup of $23.4\sim27.8\times$ and energy savings of $28.8\sim51.4\times$ compared to edge GPUs for real-world scenes. We plan to open-source our design to foster further development in this field.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Generalization of Data-Assisted Control in port-Hamiltonian Systems (DAC-pH)</title>
<link>https://arxiv.org/abs/2506.07079</link>
<guid>https://arxiv.org/abs/2506.07079</guid>
<content:encoded><![CDATA[
arXiv:2506.07079v1 Announce Type: cross 
Abstract: This paper introduces a hypothetical hybrid control framework for port-Hamiltonian (p$\mathcal{H}$) systems, employing a dynamic decomposition based on Data-Assisted Control (DAC). The system's evolution is split into two parts with fixed topology: Right-Hand Side (RHS)- an intrinsic Hamiltonian flow handling worst-case parametric uncertainties, and Left-Hand Side (LHS)- a dissipative/input flow addressing both structural and parametric uncertainties. A virtual port variable $\Pi$ serves as the interface between these two components. A nonlinear controller manages the intrinsic Hamiltonian flow, determining a desired port control value $\Pi_c$. Concurrently, Reinforcement Learning (RL) is applied to the dissipative/input flow to learn an agent for providing optimal policy in mapping $\Pi_c$ to the actual system input. This hybrid approach effectively manages RHS uncertainties while preserving the system's inherent structure. Key advantages include adjustable performance via LHS controller parameters, enhanced AI explainability and interpretability through the port variable $\Pi$, the ability to guarantee safety and state attainability with hard/soft constraints, reduced complexity in learning hypothesis classes compared to end-to-end solutions, and improved state/parameter estimation using LHS prior knowledge and system Hamiltonian to address partial observability. The paper details the p$\mathcal{H}$ formulation, derives the decomposition, and presents the modular controller architecture. Beyond design, crucial aspects of stability and robustness analysis and synthesis are investigated, paving the way for deeper theoretical investigations. An application example, a pendulum with nonlinear dynamics, is simulated to demonstrate the approach's empirical and phenomenological benefits for future research.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inverse Design of Metamaterials with Manufacturing-Guiding Spectrum-to-Structure Conditional Diffusion Model</title>
<link>https://arxiv.org/abs/2506.07083</link>
<guid>https://arxiv.org/abs/2506.07083</guid>
<content:encoded><![CDATA[
arXiv:2506.07083v1 Announce Type: cross 
Abstract: Metamaterials are artificially engineered structures that manipulate electromagnetic waves, having optical properties absent in natural materials. Recently, machine learning for the inverse design of metamaterials has drawn attention. However, the highly nonlinear relationship between the metamaterial structures and optical behaviour, coupled with fabrication difficulties, poses challenges for using machine learning to design and manufacture complex metamaterials. Herein, we propose a general framework that implements customised spectrum-to-shape and size parameters to address one-to-many metamaterial inverse design problems using conditional diffusion models. Our method exhibits superior spectral prediction accuracy, generates a diverse range of patterns compared to other typical generative models, and offers valuable prior knowledge for manufacturing through the subsequent analysis of the diverse generated results, thereby facilitating the experimental fabrication of metamaterial designs. We demonstrate the efficacy of the proposed method by successfully designing and fabricating a free-form metamaterial with a tailored selective emission spectrum for thermal camouflage applications.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantile-Optimal Policy Learning under Unmeasured Confounding</title>
<link>https://arxiv.org/abs/2506.07140</link>
<guid>https://arxiv.org/abs/2506.07140</guid>
<content:encoded><![CDATA[
arXiv:2506.07140v1 Announce Type: cross 
Abstract: We study quantile-optimal policy learning where the goal is to find a policy whose reward distribution has the largest $\alpha$-quantile for some $\alpha \in (0, 1)$. We focus on the offline setting whose generating process involves unobserved confounders. Such a problem suffers from three main challenges: (i) nonlinearity of the quantile objective as a functional of the reward distribution, (ii) unobserved confounding issue, and (iii) insufficient coverage of the offline dataset. To address these challenges, we propose a suite of causal-assisted policy learning methods that provably enjoy strong theoretical guarantees under mild conditions. In particular, to address (i) and (ii), using causal inference tools such as instrumental variables and negative controls, we propose to estimate the quantile objectives by solving nonlinear functional integral equations. Then we adopt a minimax estimation approach with nonparametric models to solve these integral equations, and propose to construct conservative policy estimates that address (iii). The final policy is the one that maximizes these pessimistic estimates. In addition, we propose a novel regularized policy learning method that is more amenable to computation. Finally, we prove that the policies learned by these methods are $\tilde{\mathscr{O}}(n^{-1/2})$ quantile-optimal under a mild coverage assumption on the offline dataset. Here, $\tilde{\mathscr{O}}(\cdot)$ omits poly-logarithmic factors. To the best of our knowledge, we propose the first sample-efficient policy learning algorithms for estimating the quantile-optimal policy when there exist unmeasured confounding.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Syntactic Control of Language Models by Posterior Inference</title>
<link>https://arxiv.org/abs/2506.07154</link>
<guid>https://arxiv.org/abs/2506.07154</guid>
<content:encoded><![CDATA[
arXiv:2506.07154v1 Announce Type: cross 
Abstract: Controlling the syntactic structure of text generated by language models is valuable for applications requiring clarity, stylistic consistency, or interpretability, yet it remains a challenging task. In this paper, we argue that sampling algorithms based on the posterior inference can effectively enforce a target constituency structure during generation. Our approach combines sequential Monte Carlo, which estimates the posterior distribution by sampling from a proposal distribution, with a syntactic tagger that ensures that each generated token aligns with the desired syntactic structure. Our experiments with GPT2 and Llama3-8B models show that with an appropriate proposal distribution, we can improve syntactic accuracy, increasing the F1 score from $12.31$ (GPT2-large) and $35.33$ (Llama3-8B) to about $93$ in both cases without compromising the language model's fluency. These results underscore both the complexity of syntactic control and the effectiveness of sampling algorithms, offering a promising approach for applications where precise control over syntax is essential.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>pFedSOP : Accelerating Training Of Personalized Federated Learning Using Second-Order Optimization</title>
<link>https://arxiv.org/abs/2506.07159</link>
<guid>https://arxiv.org/abs/2506.07159</guid>
<content:encoded><![CDATA[
arXiv:2506.07159v1 Announce Type: cross 
Abstract: Personalized Federated Learning (PFL) enables clients to collaboratively train personalized models tailored to their individual objectives, addressing the challenge of model generalization in traditional Federated Learning (FL) due to high data heterogeneity. However, existing PFL methods often require increased communication rounds to achieve the desired performance, primarily due to slow training caused by the use of first-order optimization, which has linear convergence. Additionally, many of these methods increase local computation because of the additional data fed into the model during the search for personalized local models. One promising solution to this slow training is second-order optimization, known for its quadratic convergence. However, employing it in PFL is challenging due to the Hessian matrix and its inverse. In this paper, we propose pFedSOP, which efficiently utilizes second-order optimization in PFL to accelerate the training of personalized models and enhance performance with fewer communication rounds. Our approach first computes a personalized local gradient update using the Gompertz function-based normalized angle between local and global gradient updates, incorporating client-specific global information. We then use a regularized Fisher Information Matrix (FIM), computed from this personalized gradient update, as an approximation of the Hessian to update the personalized models. This FIM-based second-order optimization speeds up training with fewer communication rounds by tackling the challenges with exact Hessian and avoids additional data being fed into the model during the search for personalized local models. Extensive experiments on heterogeneously partitioned image classification datasets with partial client participation demonstrate that pFedSOP outperforms state-of-the-art FL and PFL algorithms.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RULE: Reinforcement UnLEarning Achieves Forget-Retain Pareto Optimality</title>
<link>https://arxiv.org/abs/2506.07171</link>
<guid>https://arxiv.org/abs/2506.07171</guid>
<content:encoded><![CDATA[
arXiv:2506.07171v1 Announce Type: cross 
Abstract: The widespread deployment of Large Language Models (LLMs) trained on massive, uncurated corpora has raised growing concerns about the inclusion of sensitive, copyrighted, or illegal content. This has led to increasing interest in LLM unlearning: the task of selectively removing specific information from a model without retraining from scratch or degrading overall utility. However, existing methods often rely on large-scale forget and retain datasets, and suffer from unnatural responses, poor generalization, or catastrophic utility loss. In this work, we propose Reinforcement UnLearning (RULE), an efficient framework that formulates unlearning as a refusal boundary optimization problem. RULE is trained with a small portion of the forget set and synthesized boundary queries, using a verifiable reward function that encourages safe refusal on forget--related queries while preserving helpful responses on permissible inputs. We provide both theoretical and empirical evidence demonstrating the effectiveness of RULE in achieving targeted unlearning without compromising model utility. Experimental results show that, with only $12%$ forget set and $8%$ synthesized boundary data, RULE outperforms existing baselines by up to $17.5%$ forget quality and $16.3%$ naturalness response while maintaining general utility, achieving forget--retain Pareto optimality. Remarkably, we further observe that RULE improves the naturalness of model outputs, enhances training efficiency, and exhibits strong generalization ability, generalizing refusal behavior to semantically related but unseen queries.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Audio synthesizer inversion in symmetric parameter spaces with approximately equivariant flow matching</title>
<link>https://arxiv.org/abs/2506.07199</link>
<guid>https://arxiv.org/abs/2506.07199</guid>
<content:encoded><![CDATA[
arXiv:2506.07199v1 Announce Type: cross 
Abstract: Many audio synthesizers can produce the same signal given different parameter configurations, meaning the inversion from sound to parameters is an inherently ill-posed problem. We show that this is largely due to intrinsic symmetries of the synthesizer, and focus in particular on permutation invariance. First, we demonstrate on a synthetic task that regressing point estimates under permutation symmetry degrades performance, even when using a permutation-invariant loss function or symmetry-breaking heuristics. Then, viewing equivalent solutions as modes of a probability distribution, we show that a conditional generative model substantially improves performance. Further, acknowledging the invariance of the implicit parameter distribution, we find that performance is further improved by using a permutation equivariant continuous normalizing flow. To accommodate intricate symmetries in real synthesizers, we also propose a relaxed equivariance strategy that adaptively discovers relevant symmetries from data. Applying our method to Surge XT, a full-featured open source synthesizer used in real world audio production, we find our method outperforms regression and generative baselines across audio reconstruction metrics.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learn as Individuals, Evolve as a Team: Multi-agent LLMs Adaptation in Embodied Environments</title>
<link>https://arxiv.org/abs/2506.07232</link>
<guid>https://arxiv.org/abs/2506.07232</guid>
<content:encoded><![CDATA[
arXiv:2506.07232v1 Announce Type: cross 
Abstract: Large language models (LLMs) possess extensive knowledge bases and strong reasoning capabilities, making them promising tools for complex, multi-agent planning in embodied environments. However, despite LLMs' advanced abilities and the sophisticated modular design of agentic methods, existing LLM-based planning algorithms remain limited by weak adaptation capabilities to multi-agent embodied scenarios. We address this limitation by introducing a framework that enables LLM agents to learn and evolve both before and during test time, equipping them with environment-relevant knowledge for better planning and enhanced communication for improved cooperation. Inspired by centralized training with decentralized execution in multi-agent reinforcement learning, we propose a \textit{Learn as Individuals, Evolve as a Team (LIET)} paradigm for multi-agent LLMs adaptation. At the individual level, LLM agents learn a local utility function from exploratory datasets to better comprehend the embodied environment, which is then queried during test time to support informed decision-making. At the team level, LLM agents collaboratively and iteratively maintain and update a shared cooperation knowledge list based on new experiences, using it to guide more effective communication. By combining individual learning with team evolution, LIET enables comprehensive and flexible adaptation for LLM agents. Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving the Efficiency of Long Document Classification using Sentence Ranking Approach</title>
<link>https://arxiv.org/abs/2506.07248</link>
<guid>https://arxiv.org/abs/2506.07248</guid>
<content:encoded><![CDATA[
arXiv:2506.07248v1 Announce Type: cross 
Abstract: Long document classification poses challenges due to the computational limitations of transformer-based models, particularly BERT, which are constrained by fixed input lengths and quadratic attention complexity. Moreover, using the full document for classification is often redundant, as only a subset of sentences typically carries the necessary information. To address this, we propose a TF-IDF-based sentence ranking method that improves efficiency by selecting the most informative content. Our approach explores fixed-count and percentage-based sentence selection, along with an enhanced scoring strategy combining normalized TF-IDF scores and sentence length. Evaluated on the MahaNews LDC dataset of long Marathi news articles, the method consistently outperforms baselines such as first, last, and random sentence selection. With MahaBERT-v2, we achieve near-identical classification accuracy with just a 0.33 percent drop compared to the full-context baseline, while reducing input size by over 50 percent and inference latency by 43 percent. This demonstrates that significant context reduction is possible without sacrificing performance, making the method practical for real-world long document classification tasks.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ALINE: Joint Amortization for Bayesian Inference and Active Data Acquisition</title>
<link>https://arxiv.org/abs/2506.07259</link>
<guid>https://arxiv.org/abs/2506.07259</guid>
<content:encoded><![CDATA[
arXiv:2506.07259v1 Announce Type: cross 
Abstract: Many critical applications, from autonomous scientific discovery to personalized medicine, demand systems that can both strategically acquire the most informative data and instantaneously perform inference based upon it. While amortized methods for Bayesian inference and experimental design offer part of the solution, neither approach is optimal in the most general and challenging task, where new data needs to be collected for instant inference. To tackle this issue, we introduce the Amortized Active Learning and Inference Engine (ALINE), a unified framework for amortized Bayesian inference and active data acquisition. ALINE leverages a transformer architecture trained via reinforcement learning with a reward based on self-estimated information gain provided by its own integrated inference component. This allows it to strategically query informative data points while simultaneously refining its predictions. Moreover, ALINE can selectively direct its querying strategy towards specific subsets of model parameters or designated predictive tasks, optimizing for posterior estimation, data prediction, or a mixture thereof. Empirical results on regression-based active learning, classical Bayesian experimental design benchmarks, and a psychometric model with selectively targeted parameters demonstrate that ALINE delivers both instant and accurate inference along with efficient selection of informative points.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RADAR: Recall Augmentation through Deferred Asynchronous Retrieval</title>
<link>https://arxiv.org/abs/2506.07261</link>
<guid>https://arxiv.org/abs/2506.07261</guid>
<content:encoded><![CDATA[
arXiv:2506.07261v1 Announce Type: cross 
Abstract: Modern large-scale recommender systems employ multi-stage ranking funnel (Retrieval, Pre-ranking, Ranking) to balance engagement and computational constraints (latency, CPU). However, the initial retrieval stage, often relying on efficient but less precise methods like K-Nearest Neighbors (KNN), struggles to effectively surface the most engaging items from billion-scale catalogs, particularly distinguishing highly relevant and engaging candidates from merely relevant ones. We introduce Recall Augmentation through Deferred Asynchronous Retrieval (RADAR), a novel framework that leverages asynchronous, offline computation to pre-rank a significantly larger candidate set for users using the full complexity ranking model. These top-ranked items are stored and utilized as a high-quality retrieval source during online inference, bypassing online retrieval and pre-ranking stages for these candidates. We demonstrate through offline experiments that RADAR significantly boosts recall (2X Recall@200 vs DNN retrieval baseline) by effectively combining a larger retrieved candidate set with a more powerful ranking model. Online A/B tests confirm a +0.8% lift in topline engagement metrics, validating RADAR as a practical and effective method to improve recommendation quality under strict online serving constraints.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning-Based Self-Localization Using Internal Sensors for Automating Bulldozers</title>
<link>https://arxiv.org/abs/2506.07271</link>
<guid>https://arxiv.org/abs/2506.07271</guid>
<content:encoded><![CDATA[
arXiv:2506.07271v1 Announce Type: cross 
Abstract: Self-localization is an important technology for automating bulldozers. Conventional bulldozer self-localization systems rely on RTK-GNSS (Real Time Kinematic-Global Navigation Satellite Systems). However, RTK-GNSS signals are sometimes lost in certain mining conditions. Therefore, self-localization methods that do not depend on RTK-GNSS are required. In this paper, we propose a machine learning-based self-localization method for bulldozers. The proposed method consists of two steps: estimating local velocities using a machine learning model from internal sensors, and incorporating these estimates into an Extended Kalman Filter (EKF) for global localization. We also created a novel dataset for bulldozer odometry and conducted experiments across various driving scenarios, including slalom, excavation, and driving on slopes. The result demonstrated that the proposed self-localization method suppressed the accumulation of position errors compared to kinematics-based methods, especially when slip occurred. Furthermore, this study showed that bulldozer-specific sensors, such as blade position sensors and hydraulic pressure sensors, contributed to improving self-localization accuracy.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Step Guided Diffusion for Image Restoration on Edge Devices: Toward Lightweight Perception in Embodied AI</title>
<link>https://arxiv.org/abs/2506.07286</link>
<guid>https://arxiv.org/abs/2506.07286</guid>
<content:encoded><![CDATA[
arXiv:2506.07286v1 Announce Type: cross 
Abstract: Diffusion models have shown remarkable flexibility for solving inverse problems without task-specific retraining. However, existing approaches such as Manifold Preserving Guided Diffusion (MPGD) apply only a single gradient update per denoising step, limiting restoration fidelity and robustness, especially in embedded or out-of-distribution settings. In this work, we introduce a multistep optimization strategy within each denoising timestep, significantly enhancing image quality, perceptual accuracy, and generalization. Our experiments on super-resolution and Gaussian deblurring demonstrate that increasing the number of gradient updates per step improves LPIPS and PSNR with minimal latency overhead. Notably, we validate this approach on a Jetson Orin Nano using degraded ImageNet and a UAV dataset, showing that MPGD, originally trained on face datasets, generalizes effectively to natural and aerial scenes. Our findings highlight MPGD's potential as a lightweight, plug-and-play restoration module for real-time visual perception in embodied AI agents such as drones and mobile robots.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Generalized Source Tracing for Codec-Based Deepfake Speech</title>
<link>https://arxiv.org/abs/2506.07294</link>
<guid>https://arxiv.org/abs/2506.07294</guid>
<content:encoded><![CDATA[
arXiv:2506.07294v1 Announce Type: cross 
Abstract: Recent attempts at source tracing for codec-based deepfake speech (CodecFake), generated by neural audio codec-based speech generation (CoSG) models, have exhibited suboptimal performance. However, how to train source tracing models using simulated CoSG data while maintaining strong performance on real CoSG-generated audio remains an open challenge. In this paper, we show that models trained solely on codec-resynthesized data tend to overfit to non-speech regions and struggle to generalize to unseen content. To mitigate these challenges, we introduce the Semantic-Acoustic Source Tracing Network (SASTNet), which jointly leverages Whisper for semantic feature encoding and Wav2vec2 with AudioMAE for acoustic feature encoding. Our proposed SASTNet achieves state-of-the-art performance on the CoSG test set of the CodecFake+ dataset, demonstrating its effectiveness for reliable source tracing.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Aware Strategies: A Model-Agnostic Framework for Robust Financial Optimization through Subsampling</title>
<link>https://arxiv.org/abs/2506.07299</link>
<guid>https://arxiv.org/abs/2506.07299</guid>
<content:encoded><![CDATA[
arXiv:2506.07299v1 Announce Type: cross 
Abstract: This paper addresses the challenge of model uncertainty in quantitative finance, where decisions in portfolio allocation, derivative pricing, and risk management rely on estimating stochastic models from limited data. In practice, the unavailability of the true probability measure forces reliance on an empirical approximation, and even small misestimations can lead to significant deviations in decision quality. Building on the framework of Klibanoff et al. (2005), we enhance the conventional objective - whether this is expected utility in an investing context or a hedging metric - by superimposing an outer "uncertainty measure", motivated by traditional monetary risk measures, on the space of models. In scenarios where a natural model distribution is lacking or Bayesian methods are impractical, we propose an ad hoc subsampling strategy, analogous to bootstrapping in statistical finance and related to mini-batch sampling in deep learning, to approximate model uncertainty. To address the quadratic memory demands of naive implementations, we also present an adapted stochastic gradient descent algorithm that enables efficient parallelization. Through analytical, simulated, and empirical studies - including multi-period, real data and high-dimensional examples - we demonstrate that uncertainty measures outperform traditional mixture of measures strategies and our model-agnostic subsampling-based approach not only enhances robustness against model risk but also achieves performance comparable to more elaborate Bayesian methods.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reward Model Interpretability via Optimal and Pessimal Tokens</title>
<link>https://arxiv.org/abs/2506.07326</link>
<guid>https://arxiv.org/abs/2506.07326</guid>
<content:encoded><![CDATA[
arXiv:2506.07326v1 Announce Type: cross 
Abstract: Reward modeling has emerged as a crucial component in aligning large language models with human values. Significant attention has focused on using reward models as a means for fine-tuning generative models. However, the reward models themselves -- which directly encode human value judgments by turning prompt-response pairs into scalar rewards -- remain relatively understudied. We present a novel approach to reward model interpretability through exhaustive analysis of their responses across their entire vocabulary space. By examining how different reward models score every possible single-token response to value-laden prompts, we uncover several striking findings: (i) substantial heterogeneity between models trained on similar objectives, (ii) systematic asymmetries in how models encode high- vs low-scoring tokens, (iii) significant sensitivity to prompt framing that mirrors human cognitive biases, and (iv) overvaluation of more frequent tokens. We demonstrate these effects across ten recent open-source reward models of varying parameter counts and architectures. Our results challenge assumptions about the interchangeability of reward models, as well as their suitability as proxies of complex and context-dependent human values. We find that these models can encode concerning biases toward certain identity groups, which may emerge as unintended consequences of harmlessness training -- distortions that risk propagating through the downstream large language models now deployed to millions.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"CASE: Contrastive Activation for Saliency Estimation</title>
<link>https://arxiv.org/abs/2506.07327</link>
<guid>https://arxiv.org/abs/2506.07327</guid>
<content:encoded><![CDATA[
arXiv:2506.07327v1 Announce Type: cross 
Abstract: Saliency methods are widely used to visualize which input features are deemed relevant to a model's prediction. However, their visual plausibility can obscure critical limitations. In this work, we propose a diagnostic test for class sensitivity: a method's ability to distinguish between competing class labels on the same input. Through extensive experiments, we show that many widely used saliency methods produce nearly identical explanations regardless of the class label, calling into question their reliability. We find that class-insensitive behavior persists across architectures and datasets, suggesting the failure mode is structural rather than model-specific. Motivated by these findings, we introduce CASE, a contrastive explanation method that isolates features uniquely discriminative for the predicted class. We evaluate CASE using the proposed diagnostic and a perturbation-based fidelity test, and show that it produces faithful and more class-specific explanations than existing methods.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time Execution of Action Chunking Flow Policies</title>
<link>https://arxiv.org/abs/2506.07339</link>
<guid>https://arxiv.org/abs/2506.07339</guid>
<content:encoded><![CDATA[
arXiv:2506.07339v1 Announce Type: cross 
Abstract: Modern AI systems, especially those interacting with the physical world, increasingly require real-time performance. However, the high latency of state-of-the-art generalist models, including recent vision-language action models (VLAs), poses a significant challenge. While action chunking has enabled temporal consistency in high-frequency control tasks, it does not fully address the latency problem, leading to pauses or out-of-distribution jerky movements at chunk boundaries. This paper presents a novel inference-time algorithm that enables smooth asynchronous execution of action chunking policies. Our method, real-time chunking (RTC), is applicable to any diffusion- or flow-based VLA out of the box with no re-training. It generates the next action chunk while executing the current one, "freezing" actions guaranteed to execute and "inpainting" the rest. To test RTC, we introduce a new benchmark of 12 highly dynamic tasks in the Kinetix simulator, as well as evaluate 6 challenging real-world bimanual manipulation tasks. Results demonstrate that RTC is fast, performant, and uniquely robust to inference delay, significantly improving task throughput and enabling high success rates in precise tasks $\unicode{x2013}$ such as lighting a match $\unicode{x2013}$ even in the presence of significant latency. See https://pi.website/research/real_time_chunking for videos.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributed Risk-Sensitive Safety Filters for Uncertain Discrete-Time Systems</title>
<link>https://arxiv.org/abs/2506.07347</link>
<guid>https://arxiv.org/abs/2506.07347</guid>
<content:encoded><![CDATA[
arXiv:2506.07347v1 Announce Type: cross 
Abstract: Ensuring safety in multi-agent systems is a significant challenge, particularly in settings where centralized coordination is impractical. In this work, we propose a novel risk-sensitive safety filter for discrete-time multi-agent systems with uncertain dynamics that leverages control barrier functions (CBFs) defined through value functions. Our approach relies on centralized risk-sensitive safety conditions based on exponential risk operators to ensure robustness against model uncertainties. We introduce a distributed formulation of the safety filter by deriving two alternative strategies: one based on worst-case anticipation and another on proximity to a known safe policy. By allowing agents to switch between strategies, feasibility can be ensured. Through detailed numerical evaluations, we demonstrate the efficacy of our approach in maintaining safety without being overly conservative.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Optimization on Compact Submanifolds by Quantized Riemannian Gradient Tracking</title>
<link>https://arxiv.org/abs/2506.07351</link>
<guid>https://arxiv.org/abs/2506.07351</guid>
<content:encoded><![CDATA[
arXiv:2506.07351v1 Announce Type: cross 
Abstract: This paper considers the problem of decentralized optimization on compact submanifolds, where a finite sum of smooth (possibly non-convex) local functions is minimized by $n$ agents forming an undirected and connected graph. However, the efficiency of distributed optimization is often hindered by communication bottlenecks. To mitigate this, we propose the Quantized Riemannian Gradient Tracking (Q-RGT) algorithm, where agents update their local variables using quantized gradients. The introduction of quantization noise allows our algorithm to bypass the constraints of the accurate Riemannian projection operator (such as retraction), further improving iterative efficiency. To the best of our knowledge, this is the first algorithm to achieve an $\mathcal{O}(1/K)$ convergence rate in the presence of quantization, matching the convergence rate of methods without quantization. Additionally, we explicitly derive lower bounds on decentralized consensus associated with a function of quantization levels. Numerical experiments demonstrate that Q-RGT performs comparably to non-quantized methods while reducing communication bottlenecks and computational overhead.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CBAM-STN-TPS-YOLO: Enhancing Agricultural Object Detection through Spatially Adaptive Attention Mechanisms</title>
<link>https://arxiv.org/abs/2506.07357</link>
<guid>https://arxiv.org/abs/2506.07357</guid>
<content:encoded><![CDATA[
arXiv:2506.07357v1 Announce Type: cross 
Abstract: Object detection is vital in precision agriculture for plant monitoring, disease detection, and yield estimation. However, models like YOLO struggle with occlusions, irregular structures, and background noise, reducing detection accuracy. While Spatial Transformer Networks (STNs) improve spatial invariance through learned transformations, affine mappings are insufficient for non-rigid deformations such as bent leaves and overlaps.
  We propose CBAM-STN-TPS-YOLO, a model integrating Thin-Plate Splines (TPS) into STNs for flexible, non-rigid spatial transformations that better align features. Performance is further enhanced by the Convolutional Block Attention Module (CBAM), which suppresses background noise and emphasizes relevant spatial and channel-wise features.
  On the occlusion-heavy Plant Growth and Phenotyping (PGP) dataset, our model outperforms STN-YOLO in precision, recall, and mAP. It achieves a 12% reduction in false positives, highlighting the benefits of improved spatial flexibility and attention-guided refinement. We also examine the impact of the TPS regularization parameter in balancing transformation smoothness and detection performance.
  This lightweight model improves spatial awareness and supports real-time edge deployment, making it ideal for smart farming applications requiring accurate and efficient monitoring.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightweight Joint Audio-Visual Deepfake Detection via Single-Stream Multi-Modal Learning Framework</title>
<link>https://arxiv.org/abs/2506.07358</link>
<guid>https://arxiv.org/abs/2506.07358</guid>
<content:encoded><![CDATA[
arXiv:2506.07358v1 Announce Type: cross 
Abstract: Deepfakes are AI-synthesized multimedia data that may be abused for spreading misinformation. Deepfake generation involves both visual and audio manipulation. To detect audio-visual deepfakes, previous studies commonly employ two relatively independent sub-models to learn audio and visual features, respectively, and fuse them subsequently for deepfake detection. However, this may underutilize the inherent correlations between audio and visual features. Moreover, utilizing two isolated feature learning sub-models can result in redundant neural layers, making the overall model inefficient and impractical for resource-constrained environments.
  In this work, we design a lightweight network for audio-visual deepfake detection via a single-stream multi-modal learning framework. Specifically, we introduce a collaborative audio-visual learning block to efficiently integrate multi-modal information while learning the visual and audio features. By iteratively employing this block, our single-stream network achieves a continuous fusion of multi-modal features across its layers. Thus, our network efficiently captures visual and audio features without the need for excessive block stacking, resulting in a lightweight network design. Furthermore, we propose a multi-modal classification module that can boost the dependence of the visual and audio classifiers on modality content. It also enhances the whole resistance of the video classifier against the mismatches between audio and visual modalities. We conduct experiments on the DF-TIMIT, FakeAVCeleb, and DFDC benchmark datasets. Compared to state-of-the-art audio-visual joint detection methods, our method is significantly lightweight with only 0.48M parameters, yet it achieves superiority in both uni-modal and multi-modal deepfakes, as well as in unseen types of deepfakes.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiple Object Stitching for Unsupervised Representation Learning</title>
<link>https://arxiv.org/abs/2506.07364</link>
<guid>https://arxiv.org/abs/2506.07364</guid>
<content:encoded><![CDATA[
arXiv:2506.07364v1 Announce Type: cross 
Abstract: Contrastive learning for single object centric images has achieved remarkable progress on unsupervised representation, but suffering inferior performance on the widespread images with multiple objects. In this paper, we propose a simple but effective method, Multiple Object Stitching (MOS), to refine the unsupervised representation for multi-object images. Specifically, we construct the multi-object images by stitching the single object centric ones, where the objects in the synthesized multi-object images are predetermined. Hence, compared to the existing contrastive methods, our method provides additional object correspondences between multi-object images without human annotations. In this manner, our method pays more attention to the representations of each object in multi-object image, thus providing more detailed representations for complicated downstream tasks, such as object detection and semantic segmentation. Experimental results on ImageNet, CIFAR and COCO datasets demonstrate that our proposed method achieves the leading unsupervised representation performance on both single object centric images and multi-object ones. The source code is available at https://github.com/visresearch/MultipleObjectStitching.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapter Naturally Serves as Decoupler for Cross-Domain Few-Shot Semantic Segmentation</title>
<link>https://arxiv.org/abs/2506.07376</link>
<guid>https://arxiv.org/abs/2506.07376</guid>
<content:encoded><![CDATA[
arXiv:2506.07376v1 Announce Type: cross 
Abstract: Cross-domain few-shot segmentation (CD-FSS) is proposed to pre-train the model on a source-domain dataset with sufficient samples, and then transfer the model to target-domain datasets where only a few samples are available for efficient fine-tuning. There are majorly two challenges in this task: (1) the domain gap and (2) fine-tuning with scarce data. To solve these challenges, we revisit the adapter-based methods, and discover an intriguing insight not explored in previous works: the adapter not only helps the fine-tuning of downstream tasks but also naturally serves as a domain information decoupler. Then, we delve into this finding for an interpretation, and find the model's inherent structure could lead to a natural decoupling of domain information. Building upon this insight, we propose the Domain Feature Navigator (DFN), which is a structure-based decoupler instead of loss-based ones like current works, to capture domain-specific information, thereby directing the model's attention towards domain-agnostic knowledge. Moreover, to prevent the potential excessive overfitting of DFN during the source-domain training, we further design the SAM-SVN method to constrain DFN from learning sample-specific knowledge. On target domains, we freeze the model and fine-tune the DFN to learn target-specific knowledge specific. Extensive experiments demonstrate that our method surpasses the state-of-the-art method in CD-FSS significantly by 2.69% and 4.68% MIoU in 1-shot and 5-shot scenarios, respectively.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Static to Adaptive Defense: Federated Multi-Agent Deep Reinforcement Learning-Driven Moving Target Defense Against DoS Attacks in UAV Swarm Networks</title>
<link>https://arxiv.org/abs/2506.07392</link>
<guid>https://arxiv.org/abs/2506.07392</guid>
<content:encoded><![CDATA[
arXiv:2506.07392v1 Announce Type: cross 
Abstract: The proliferation of unmanned aerial vehicle (UAV) swarms has enabled a wide range of mission-critical applications, but also exposes UAV networks to severe Denial-of-Service (DoS) threats due to their open wireless environment, dynamic topology, and resource constraints. Traditional static or centralized defense mechanisms are often inadequate for such dynamic and distributed scenarios. To address these challenges, we propose a novel federated multi-agent deep reinforcement learning (FMADRL)-driven moving target defense (MTD) framework for proactive and adaptive DoS mitigation in UAV swarm networks. Specifically, we design three lightweight and coordinated MTD mechanisms, including leader switching, route mutation, and frequency hopping, that leverage the inherent flexibility of UAV swarms to disrupt attacker efforts and enhance network resilience. The defense problem is formulated as a multi-agent partially observable Markov decision process (POMDP), capturing the distributed, resource-constrained, and uncertain nature of UAV swarms under attack. Each UAV is equipped with a local policy agent that autonomously selects MTD actions based on partial observations and local experiences. By employing a policy gradient-based FMADRL algorithm, UAVs collaboratively optimize their defense policies via reward-weighted aggregation, enabling distributed learning without sharing raw data and thus reducing communication overhead. Extensive simulations demonstrate that our approach significantly outperforms state-of-the-art baselines, achieving up to a 34.6% improvement in attack mitigation rate, a reduction in average recovery time of up to 94.6%, and decreases in energy consumption and defense cost by as much as 29.3% and 98.3%, respectively, while maintaining robust mission continuity under various DoS attack strategies.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2506.07398</link>
<guid>https://arxiv.org/abs/2506.07398</guid>
<content:encoded><![CDATA[
arXiv:2506.07398v1 Announce Type: cross 
Abstract: Large language model (LLM)-powered multi-agent systems (MAS) have demonstrated cognitive and execution capabilities that far exceed those of single LLM agents, yet their capacity for self-evolution remains hampered by underdeveloped memory architectures. Upon close inspection, we are alarmed to discover that prevailing MAS memory mechanisms (1) are overly simplistic, completely disregarding the nuanced inter-agent collaboration trajectories, and (2) lack cross-trial and agent-specific customization, in stark contrast to the expressive memory developed for single agents. To bridge this gap, we introduce G-Memory, a hierarchical, agentic memory system for MAS inspired by organizational memory theory, which manages the lengthy MAS interaction via a three-tier graph hierarchy: insight, query, and interaction graphs. Upon receiving a new user query, G-Memory performs bi-directional memory traversal to retrieve both $\textit{high-level, generalizable insights}$ that enable the system to leverage cross-trial knowledge, and $\textit{fine-grained, condensed interaction trajectories}$ that compactly encode prior collaboration experiences. Upon task execution, the entire hierarchy evolves by assimilating new collaborative trajectories, nurturing the progressive evolution of agent teams. Extensive experiments across five benchmarks, three LLM backbones, and three popular MAS frameworks demonstrate that G-Memory improves success rates in embodied action and accuracy in knowledge QA by up to $20.89\%$ and $10.12\%$, respectively, without any modifications to the original frameworks. Our codes are available at https://github.com/bingreeky/GMemory.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedChat: A Multi-Agent Framework for Multimodal Diagnosis with Large Language Models</title>
<link>https://arxiv.org/abs/2506.07400</link>
<guid>https://arxiv.org/abs/2506.07400</guid>
<content:encoded><![CDATA[
arXiv:2506.07400v1 Announce Type: cross 
Abstract: The integration of deep learning-based glaucoma detection with large language models (LLMs) presents an automated strategy to mitigate ophthalmologist shortages and improve clinical reporting efficiency. However, applying general LLMs to medical imaging remains challenging due to hallucinations, limited interpretability, and insufficient domain-specific medical knowledge, which can potentially reduce clinical accuracy. Although recent approaches combining imaging models with LLM reasoning have improved reporting, they typically rely on a single generalist agent, restricting their capacity to emulate the diverse and complex reasoning found in multidisciplinary medical teams. To address these limitations, we propose MedChat, a multi-agent diagnostic framework and platform that combines specialized vision models with multiple role-specific LLM agents, all coordinated by a director agent. This design enhances reliability, reduces hallucination risk, and enables interactive diagnostic reporting through an interface tailored for clinical review and educational use. Code available at https://github.com/Purdue-M2/MedChat.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HeTa: Relation-wise Heterogeneous Graph Foundation Attack Model</title>
<link>https://arxiv.org/abs/2506.07428</link>
<guid>https://arxiv.org/abs/2506.07428</guid>
<content:encoded><![CDATA[
arXiv:2506.07428v1 Announce Type: cross 
Abstract: Heterogeneous Graph Neural Networks (HGNNs) are vulnerable, highlighting the need for tailored attacks to assess their robustness and ensure security. However, existing HGNN attacks often require complex retraining of parameters to generate specific perturbations for new scenarios. Recently, foundation models have opened new horizons for the generalization of graph neural networks by capturing shared semantics across various graph distributions. This leads us to ask:Can we design a foundation attack model for HGNNs that enables generalizable perturbations across different HGNNs, and quickly adapts to new heterogeneous graphs (HGs)? Empirical findings reveal that, despite significant differences in model design and parameter space, different HGNNs surprisingly share common vulnerability patterns from a relation-aware perspective. Therefore, we explore how to design foundation HGNN attack criteria by mining shared attack units. In this paper, we propose a novel relation-wise heterogeneous graph foundation attack model, HeTa. We introduce a foundation surrogate model to align heterogeneity and identify the importance of shared relation-aware attack units. Building on this, we implement a serialized relation-by-relation attack based on the identified relational weights. In this way, the perturbation can be transferred to various target HGNNs and easily fine-tuned for new HGs. Extensive experiments exhibit powerful attack performances and generalizability of our method.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Geometric Embedding for Node Influence Maximization</title>
<link>https://arxiv.org/abs/2506.07435</link>
<guid>https://arxiv.org/abs/2506.07435</guid>
<content:encoded><![CDATA[
arXiv:2506.07435v1 Announce Type: cross 
Abstract: Computing classical centrality measures such as betweenness and closeness is computationally expensive on large-scale graphs. In this work, we introduce an efficient force layout algorithm that embeds a graph into a low-dimensional space, where the radial distance from the origin serves as a proxy for various centrality measures. We evaluate our method on multiple graph families and demonstrate strong correlations with degree, PageRank, and paths-based centralities. As an application, it turns out that the proposed embedding allows to find high-influence nodes in a network, and provides a fast and scalable alternative to the standard greedy algorithm.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KScope: A Framework for Characterizing the Knowledge Status of Language Models</title>
<link>https://arxiv.org/abs/2506.07458</link>
<guid>https://arxiv.org/abs/2506.07458</guid>
<content:encoded><![CDATA[
arXiv:2506.07458v1 Announce Type: cross 
Abstract: Characterizing a large language model's (LLM's) knowledge of a given question is challenging. As a result, prior work has primarily examined LLM behavior under knowledge conflicts, where the model's internal parametric memory contradicts information in the external context. However, this does not fully reflect how well the model knows the answer to the question. In this paper, we first introduce a taxonomy of five knowledge statuses based on the consistency and correctness of LLM knowledge modes. We then propose KScope, a hierarchical framework of statistical tests that progressively refines hypotheses about knowledge modes and characterizes LLM knowledge into one of these five statuses. We apply KScope to nine LLMs across four datasets and systematically establish: (1) Supporting context narrows knowledge gaps across models. (2) Context features related to difficulty, relevance, and familiarity drive successful knowledge updates. (3) LLMs exhibit similar feature preferences when partially correct or conflicted, but diverge sharply when consistently wrong. (4) Context summarization constrained by our feature analysis, together with enhanced credibility, further improves update effectiveness and generalizes across LLMs.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoCoA-Mix: Confusion-and-Confidence-Aware Mixture Model for Context Optimization</title>
<link>https://arxiv.org/abs/2506.07484</link>
<guid>https://arxiv.org/abs/2506.07484</guid>
<content:encoded><![CDATA[
arXiv:2506.07484v1 Announce Type: cross 
Abstract: Prompt tuning, which adapts vision-language models by freezing model parameters and optimizing only the prompt, has proven effective for task-specific adaptations. The core challenge in prompt tuning is improving specialization for a specific task and generalization for unseen domains. However, frozen encoders often produce misaligned features, leading to confusion between classes and limiting specialization. To overcome this issue, we propose a confusion-aware loss (CoA-loss) that improves specialization by refining the decision boundaries between confusing classes. Additionally, we mathematically demonstrate that a mixture model can enhance generalization without compromising specialization. This is achieved using confidence-aware weights (CoA-weights), which adjust the weights of each prediction in the mixture model based on its confidence within the class domains. Extensive experiments show that CoCoA-Mix, a mixture model with CoA-loss and CoA-weights, outperforms state-of-the-art methods by enhancing specialization and generalization. Our code is publicly available at https://github.com/url-kaist/CoCoA-Mix.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning What Reinforcement Learning Can't: Interleaved Online Fine-Tuning for Hardest Questions</title>
<link>https://arxiv.org/abs/2506.07527</link>
<guid>https://arxiv.org/abs/2506.07527</guid>
<content:encoded><![CDATA[
arXiv:2506.07527v1 Announce Type: cross 
Abstract: Recent advances in large language model (LLM) reasoning have shown that sophisticated behaviors such as planning and self-reflection can emerge through reinforcement learning (RL). However, despite these successes, RL in its current form remains insufficient to induce capabilities that exceed the limitations of the base model, as it is primarily optimized based on existing knowledge of the model rather than facilitating the acquisition of new information. To address this limitation, we employ supervised fine-tuning (SFT) to learn what RL cannot, which enables the incorporation of new knowledge and reasoning patterns by leveraging high-quality demonstration data. We analyze the training dynamics of RL and SFT for LLM reasoning and find that RL excels at maintaining and improving performance on questions within the model's original capabilities, while SFT is more effective at enabling progress on questions beyond the current scope of the model. Motivated by the complementary strengths of RL and SFT, we introduce a novel training approach, \textbf{ReLIFT} (\textbf{Re}inforcement \textbf{L}earning \textbf{I}nterleaved with Online \textbf{F}ine-\textbf{T}uning). In ReLIFT, the model is primarily trained using RL, but when it encounters challenging questions, high-quality solutions are collected for fine-tuning, and the training process alternates between RL and fine-tuning to enhance the model's reasoning abilities. ReLIFT achieves an average improvement of over +5.2 points across five competition-level benchmarks and one out-of-distribution benchmark compared to other zero-RL models. Furthermore, we demonstrate that ReLIFT outperforms both RL and SFT while using only 13\% of the detailed demonstration data, highlighting its scalability. These results provide compelling evidence that ReLIFT overcomes the fundamental limitations of RL and underscores the significant potential.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-o: One Model-agnostic Framework for Unveiling Uncertainty in Large Multimodal Models</title>
<link>https://arxiv.org/abs/2506.07575</link>
<guid>https://arxiv.org/abs/2506.07575</guid>
<content:encoded><![CDATA[
arXiv:2506.07575v1 Announce Type: cross 
Abstract: Large Multimodal Models (LMMs), harnessing the complementarity among diverse modalities, are often considered more robust than pure Language Large Models (LLMs); yet do LMMs know what they do not know? There are three key open questions remaining: (1) how to evaluate the uncertainty of diverse LMMs in a unified manner, (2) how to prompt LMMs to show its uncertainty, and (3) how to quantify uncertainty for downstream tasks. In an attempt to address these challenges, we introduce Uncertainty-o: (1) a model-agnostic framework designed to reveal uncertainty in LMMs regardless of their modalities, architectures, or capabilities, (2) an empirical exploration of multimodal prompt perturbations to uncover LMM uncertainty, offering insights and findings, and (3) derive the formulation of multimodal semantic uncertainty, which enables quantifying uncertainty from multimodal responses. Experiments across 18 benchmarks spanning various modalities and 10 LMMs (both open- and closed-source) demonstrate the effectiveness of Uncertainty-o in reliably estimating LMM uncertainty, thereby enhancing downstream tasks such as hallucination detection, hallucination mitigation, and uncertainty-aware Chain-of-Thought reasoning.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explore the vulnerability of black-box models via diffusion models</title>
<link>https://arxiv.org/abs/2506.07590</link>
<guid>https://arxiv.org/abs/2506.07590</guid>
<content:encoded><![CDATA[
arXiv:2506.07590v1 Announce Type: cross 
Abstract: Recent advancements in diffusion models have enabled high-fidelity and photorealistic image generation across diverse applications. However, these models also present security and privacy risks, including copyright violations, sensitive information leakage, and the creation of harmful or offensive content that could be exploited maliciously. In this study, we uncover a novel security threat where an attacker leverages diffusion model APIs to generate synthetic images, which are then used to train a high-performing substitute model. This enables the attacker to execute model extraction and transfer-based adversarial attacks on black-box classification models with minimal queries, without needing access to the original training data. The generated images are sufficiently high-resolution and diverse to train a substitute model whose outputs closely match those of the target model. Across the seven benchmarks, including CIFAR and ImageNet subsets, our method shows an average improvement of 27.37% over state-of-the-art methods while using just 0.01 times of the query budget, achieving a 98.68% success rate in adversarial attacks on the target model.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TimberStrike: Dataset Reconstruction Attack Revealing Privacy Leakage in Federated Tree-Based Systems</title>
<link>https://arxiv.org/abs/2506.07605</link>
<guid>https://arxiv.org/abs/2506.07605</guid>
<content:encoded><![CDATA[
arXiv:2506.07605v1 Announce Type: cross 
Abstract: Federated Learning has emerged as a privacy-oriented alternative to centralized Machine Learning, enabling collaborative model training without direct data sharing. While extensively studied for neural networks, the security and privacy implications of tree-based models remain underexplored. This work introduces TimberStrike, an optimization-based dataset reconstruction attack targeting horizontally federated tree-based models. Our attack, carried out by a single client, exploits the discrete nature of decision trees by using split values and decision paths to infer sensitive training data from other clients. We evaluate TimberStrike on State-of-the-Art federated gradient boosting implementations across multiple frameworks, including Flower, NVFlare, and FedTree, demonstrating their vulnerability to privacy breaches. On a publicly available stroke prediction dataset, TimberStrike consistently reconstructs between 73.05% and 95.63% of the target dataset across all implementations. We further analyze Differential Privacy, showing that while it partially mitigates the attack, it also significantly degrades model performance. Our findings highlight the need for privacy-preserving mechanisms specifically designed for tree-based Federated Learning systems, and we provide preliminary insights into their design.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Poisson Midpoint Method for Log Concave Sampling: Beyond the Strong Error Lower Bounds</title>
<link>https://arxiv.org/abs/2506.07614</link>
<guid>https://arxiv.org/abs/2506.07614</guid>
<content:encoded><![CDATA[
arXiv:2506.07614v1 Announce Type: cross 
Abstract: We study the problem of sampling from strongly log-concave distributions over $\mathbb{R}^d$ using the Poisson midpoint discretization (a variant of the randomized midpoint method) for overdamped/underdamped Langevin dynamics. We prove its convergence in the 2-Wasserstein distance ($W_2$), achieving a cubic speedup in dependence on the target accuracy ($\epsilon$) over the Euler-Maruyama discretization, surpassing existing bounds for randomized midpoint methods. Notably, in the case of underdamped Langevin dynamics, we demonstrate the complexity of $W_2$ convergence is much smaller than the complexity lower bounds for convergence in $L^2$ strong error established in the literature.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoRMA: Low-Rank Multiplicative Adaptation for LLMs</title>
<link>https://arxiv.org/abs/2506.07621</link>
<guid>https://arxiv.org/abs/2506.07621</guid>
<content:encoded><![CDATA[
arXiv:2506.07621v1 Announce Type: cross 
Abstract: Large Language Models have shown remarkable capabilities in the NLP domain. Their effectiveness can mainly be attributed to their ability to adapt to an array of downstream tasks. However, generally, full fine-tuning is a computationally expensive job. To mitigate this, many techniques have been developed that prime efficiency, a prominent one being Low-Rank Adaptation (LoRA). However, LoRA and its variants employ re-parametrized additive updates. In this paper, we propose Low-Rank Multiplicative Adaptation (LoRMA), which shifts the paradigm of additive updates to a richer space of matrix multiplicative transformations. We tackle challenges such as computational complexity and rank bottleneck of matrix multiplication by effectively re-ordering operations and introducing rank inflation strategies. We conduct extensive experiments to demonstrate the effectiveness of our approach in terms of various evaluation metrics.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HieraEdgeNet: A Multi-Scale Edge-Enhanced Framework for Automated Pollen Recognition</title>
<link>https://arxiv.org/abs/2506.07637</link>
<guid>https://arxiv.org/abs/2506.07637</guid>
<content:encoded><![CDATA[
arXiv:2506.07637v1 Announce Type: cross 
Abstract: Automated pollen recognition is vital to paleoclimatology, biodiversity monitoring, and public health, yet conventional methods are hampered by inefficiency and subjectivity. Existing deep learning models often struggle to achieve the requisite localization accuracy for microscopic targets like pollen, which are characterized by their minute size, indistinct edges, and complex backgrounds. To overcome this limitation, we introduce HieraEdgeNet, a multi-scale edge-enhancement framework. The framework's core innovation is the introduction of three synergistic modules: the Hierarchical Edge Module (HEM), which explicitly extracts a multi-scale pyramid of edge features that corresponds to the semantic hierarchy at early network stages; the Synergistic Edge Fusion (SEF) module, for deeply fusing these edge priors with semantic information at each respective scale; and the Cross Stage Partial Omni-Kernel Module (CSPOKM), which maximally refines the most detail-rich feature layers using an Omni-Kernel operator - comprising anisotropic large-kernel convolutions and mixed-domain attention - all within a computationally efficient Cross-Stage Partial (CSP) framework. On a large-scale dataset comprising 120 pollen classes, HieraEdgeNet achieves a mean Average Precision (mAP@.5) of 0.9501, significantly outperforming state-of-the-art baseline models such as YOLOv12n and RT-DETR. Furthermore, qualitative analysis confirms that our approach generates feature representations that are more precisely focused on object boundaries. By systematically integrating edge information, HieraEdgeNet provides a robust and powerful solution for high-precision, high-efficiency automated detection of microscopic objects.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Silencing Empowerment, Allowing Bigotry: Auditing the Moderation of Hate Speech on Twitch</title>
<link>https://arxiv.org/abs/2506.07667</link>
<guid>https://arxiv.org/abs/2506.07667</guid>
<content:encoded><![CDATA[
arXiv:2506.07667v1 Announce Type: cross 
Abstract: To meet the demands of content moderation, online platforms have resorted to automated systems. Newer forms of real-time engagement($\textit{e.g.}$, users commenting on live streams) on platforms like Twitch exert additional pressures on the latency expected of such moderation systems. Despite their prevalence, relatively little is known about the effectiveness of these systems. In this paper, we conduct an audit of Twitch's automated moderation tool ($\texttt{AutoMod}$) to investigate its effectiveness in flagging hateful content. For our audit, we create streaming accounts to act as siloed test beds, and interface with the live chat using Twitch's APIs to send over $107,000$ comments collated from $4$ datasets. We measure $\texttt{AutoMod}$'s accuracy in flagging blatantly hateful content containing misogyny, racism, ableism and homophobia. Our experiments reveal that a large fraction of hateful messages, up to $94\%$ on some datasets, $\textit{bypass moderation}$. Contextual addition of slurs to these messages results in $100\%$ removal, revealing $\texttt{AutoMod}$'s reliance on slurs as a moderation signal. We also find that contrary to Twitch's community guidelines, $\texttt{AutoMod}$ blocks up to $89.5\%$ of benign examples that use sensitive words in pedagogical or empowering contexts. Overall, our audit points to large gaps in $\texttt{AutoMod}$'s capabilities and underscores the importance for such systems to understand context effectively.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rao-Blackwellised Reparameterisation Gradients</title>
<link>https://arxiv.org/abs/2506.07687</link>
<guid>https://arxiv.org/abs/2506.07687</guid>
<content:encoded><![CDATA[
arXiv:2506.07687v1 Announce Type: cross 
Abstract: Latent Gaussian variables have been popularised in probabilistic machine learning. In turn, gradient estimators are the machinery that facilitates gradient-based optimisation for models with latent Gaussian variables. The reparameterisation trick is often used as the default estimator as it is simple to implement and yields low-variance gradients for variational inference. In this work, we propose the R2-G2 estimator as the Rao-Blackwellisation of the reparameterisation gradient estimator. Interestingly, we show that the local reparameterisation gradient estimator for Bayesian MLPs is an instance of the R2-G2 estimator and Rao-Blackwellisation. This lets us extend benefits of Rao-Blackwellised gradients to a suite of probabilistic models. We show that initial training with R2-G2 consistently yields better performance in models with multiple applications of the reparameterisation trick.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training Superior Sparse Autoencoders for Instruct Models</title>
<link>https://arxiv.org/abs/2506.07691</link>
<guid>https://arxiv.org/abs/2506.07691</guid>
<content:encoded><![CDATA[
arXiv:2506.07691v1 Announce Type: cross 
Abstract: As large language models (LLMs) grow in scale and capability, understanding their internal mechanisms becomes increasingly critical. Sparse autoencoders (SAEs) have emerged as a key tool in mechanistic interpretability, enabling the extraction of human-interpretable features from LLMs. However, existing SAE training methods are primarily designed for base models, resulting in reduced reconstruction quality and interpretability when applied to instruct models. To bridge this gap, we propose $\underline{\textbf{F}}$inetuning-$\underline{\textbf{a}}$ligned $\underline{\textbf{S}}$equential $\underline{\textbf{T}}$raining ($\textit{FAST}$), a novel training method specifically tailored for instruct models. $\textit{FAST}$ aligns the training process with the data distribution and activation patterns characteristic of instruct models, resulting in substantial improvements in both reconstruction and feature interpretability. On Qwen2.5-7B-Instruct, $\textit{FAST}$ achieves a mean squared error of 0.6468 in token reconstruction, significantly outperforming baseline methods with errors of 5.1985 and 1.5096. In feature interpretability, $\textit{FAST}$ yields a higher proportion of high-quality features, for Llama3.2-3B-Instruct, $21.1\%$ scored in the top range, compared to $7.0\%$ and $10.2\%$ for $\textit{BT(P)}$ and $\textit{BT(F)}$. Surprisingly, we discover that intervening on the activations of special tokens via the SAEs leads to improvements in output quality, suggesting new opportunities for fine-grained control of model behavior. Code, data, and 240 trained SAEs are available at https://github.com/Geaming2002/FAST.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Small Language Model Lifecycle Framework</title>
<link>https://arxiv.org/abs/2506.07695</link>
<guid>https://arxiv.org/abs/2506.07695</guid>
<content:encoded><![CDATA[
arXiv:2506.07695v1 Announce Type: cross 
Abstract: Background: The growing demand for efficient and deployable language models has led to increased interest in Small Language Models (SLMs). However, existing research remains fragmented, lacking a unified lifecycle perspective.
  Objective: This study aims to define a comprehensive lifecycle framework for SLMs by synthesizing insights from academic literature and practitioner sources.
  Method: We conducted a comprehensive survey of 36 works, analyzing and categorizing lifecycle-relevant techniques.
  Results: We propose a modular lifecycle model structured into main, optional, and cross-cutting components. The model captures key interconnections across stages, supporting method reuse, co-adaptation, and lifecycle-awareness.
  Conclusion: Our framework provides a coherent foundation for developing and maintaining SLMs, bridging theory and practice, and guiding future research and tool development.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Profiling Electric Vehicles via Early Charging Voltage Patterns</title>
<link>https://arxiv.org/abs/2506.07714</link>
<guid>https://arxiv.org/abs/2506.07714</guid>
<content:encoded><![CDATA[
arXiv:2506.07714v1 Announce Type: cross 
Abstract: Electric Vehicles (EVs) are rapidly gaining adoption as a sustainable alternative to fuel-powered vehicles, making secure charging infrastructure essential. Despite traditional authentication protocols, recent results showed that attackers may steal energy through tailored relay attacks. One countermeasure is leveraging the EV's fingerprint on the current exchanged during charging. However, existing methods focus on the final charging stage, allowing malicious actors to consume substantial energy before being detected and repudiated. This underscores the need for earlier and more effective authentication methods to prevent unauthorized charging. Meanwhile, profiling raises privacy concerns, as uniquely identifying EVs through charging patterns could enable user tracking.
  In this paper, we propose a framework for uniquely identifying EVs using physical measurements from the early charging stages. We hypothesize that voltage behavior early in the process exhibits similar characteristics to current behavior in later stages. By extracting features from early voltage measurements, we demonstrate the feasibility of EV profiling. Our approach improves existing methods by enabling faster and more reliable vehicle identification. We test our solution on a dataset of 7408 usable charges from 49 EVs, achieving up to 0.86 accuracy. Feature importance analysis shows that near-optimal performance is possible with just 10 key features, improving efficiency alongside our lightweight models. This research lays the foundation for a novel authentication factor while exposing potential privacy risks from unauthorized access to charging data.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent Semantics, Semantic Spacetime, and Graphical Reasoning</title>
<link>https://arxiv.org/abs/2506.07756</link>
<guid>https://arxiv.org/abs/2506.07756</guid>
<content:encoded><![CDATA[
arXiv:2506.07756v1 Announce Type: cross 
Abstract: Some formal aspects of the Semantic Spacetime graph model are presented, with reference to its use for directed knowledge representations and process modelling. A finite $\gamma(3,4)$ representation is defined to form a closed set of operations that can scale to any degree of semantic complexity. The Semantic Spacetime postulates bring predictability with minimal constraints to pathways in graphs. The ubiquitous appearance of absorbing states in any partial graph means that a graph process leaks information. The issue is closely associated with the issue of division by zero, which signals a loss of closure and the need for manual injection of remedial information. The Semantic Spacetime model (and its Promise Theory) origins help to clarify how such absorbing states are associated with boundary information where intentionality can enter.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quickest Causal Change Point Detection by Adaptive Intervention</title>
<link>https://arxiv.org/abs/2506.07760</link>
<guid>https://arxiv.org/abs/2506.07760</guid>
<content:encoded><![CDATA[
arXiv:2506.07760v1 Announce Type: cross 
Abstract: We propose an algorithm for change point monitoring in linear causal models that accounts for interventions. Through a special centralization technique, we can concentrate the changes arising from causal propagation across nodes into a single dimension. Additionally, by selecting appropriate intervention nodes based on Kullback-Leibler divergence, we can amplify the change magnitude. We also present an algorithm for selecting the intervention values, which aids in the identification of the most effective intervention nodes. Two monitoring methods are proposed, each with an adaptive intervention policy to make a balance between exploration and exploitation. We theoretically demonstrate the first-order optimality of the proposed methods and validate their properties using simulation datasets and two real-world case studies.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Models-Aided Uplink Channel Estimation for RIS-Assisted Systems</title>
<link>https://arxiv.org/abs/2506.07770</link>
<guid>https://arxiv.org/abs/2506.07770</guid>
<content:encoded><![CDATA[
arXiv:2506.07770v1 Announce Type: cross 
Abstract: This letter proposes a channel estimation method for reconfigurable intelligent surface (RIS)-assisted systems through a novel diffusion model (DM) framework. We reformulate the channel estimation problem as a denoising process, which aligns with the reverse process of the DM. To overcome the inherent randomness in the reverse process of conventional DM approaches, we adopt a deterministic sampling strategy with a step alignment mechanism that ensures the accuracy of channel estimation while adapting to different signal-to-noise ratio (SNR). Furthermore, to reduce the number of parameters of the U-Net, we meticulously design a lightweight network that achieves comparable performance, thereby enhancing the practicality of our proposed method. Extensive simulations demonstrate superior performance over a wide range of SNRs compared to baselines. For instance, the proposed method achieves performance improvements of up to 13.5 dB in normalized mean square error (NMSE) at SNR = 0 dB. Notably, the proposed lightweight network exhibits almost no performance loss compared to the original U-Net, while requiring only 6.59\% of its parameters.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trend-Aware Fashion Recommendation with Visual Segmentation and Semantic Similarity</title>
<link>https://arxiv.org/abs/2506.07773</link>
<guid>https://arxiv.org/abs/2506.07773</guid>
<content:encoded><![CDATA[
arXiv:2506.07773v1 Announce Type: cross 
Abstract: We introduce a trend-aware and visually-grounded fashion recommendation system that integrates deep visual representations, garment-aware segmentation, semantic category similarity and user behavior simulation. Our pipeline extracts focused visual embeddings by masking non-garment regions via semantic segmentation followed by feature extraction using pretrained CNN backbones (ResNet-50, DenseNet-121, VGG16). To simulate realistic shopping behavior, we generate synthetic purchase histories influenced by user-specific trendiness and item popularity. Recommendations are computed using a weighted scoring function that fuses visual similarity, semantic coherence and popularity alignment. Experiments on the DeepFashion dataset demonstrate consistent gender alignment and improved category relevance, with ResNet-50 achieving 64.95% category similarity and lowest popularity MAE. An ablation study confirms the complementary roles of visual and popularity cues. Our method provides a scalable framework for personalized fashion recommendations that balances individual style with emerging trends. Our implementation is available at https://github.com/meddjilani/FashionRecommender
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Re-ranking Reasoning Context with Tree Search Makes Large Vision-Language Models Stronger</title>
<link>https://arxiv.org/abs/2506.07785</link>
<guid>https://arxiv.org/abs/2506.07785</guid>
<content:encoded><![CDATA[
arXiv:2506.07785v1 Announce Type: cross 
Abstract: Recent advancements in Large Vision Language Models (LVLMs) have significantly improved performance in Visual Question Answering (VQA) tasks through multimodal Retrieval-Augmented Generation (RAG). However, existing methods still face challenges, such as the scarcity of knowledge with reasoning examples and erratic responses from retrieved knowledge. To address these issues, in this study, we propose a multimodal RAG framework, termed RCTS, which enhances LVLMs by constructing a Reasoning Context-enriched knowledge base and a Tree Search re-ranking method. Specifically, we introduce a self-consistent evaluation mechanism to enrich the knowledge base with intrinsic reasoning patterns. We further propose a Monte Carlo Tree Search with Heuristic Rewards (MCTS-HR) to prioritize the most relevant examples. This ensures that LVLMs can leverage high-quality contextual reasoning for better and more consistent responses. Extensive experiments demonstrate that our framework achieves state-of-the-art performance on multiple VQA datasets, significantly outperforming In-Context Learning (ICL) and Vanilla-RAG methods. It highlights the effectiveness of our knowledge base and re-ranking method in improving LVLMs. Our code is available at https://github.com/yannqi/RCTS-RAG.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Unlearning Should Be Form-Independent</title>
<link>https://arxiv.org/abs/2506.07795</link>
<guid>https://arxiv.org/abs/2506.07795</guid>
<content:encoded><![CDATA[
arXiv:2506.07795v1 Announce Type: cross 
Abstract: Large Language Model (LLM) unlearning aims to erase or suppress undesirable knowledge within the model, offering promise for controlling harmful or private information to prevent misuse. However, recent studies highlight its limited efficacy in real-world scenarios, hindering practical adoption. In this study, we identify a pervasive issue underlying many downstream failures: the effectiveness of existing unlearning methods heavily depends on the form of training samples and frequently fails to generalize to alternate expressions of the same knowledge. We formally characterize this problem as Form-Dependent Bias and systematically investigate its specific manifestation patterns across various downstream tasks. To quantify its prevalence and support future research, we introduce ORT, a novel benchmark designed to evaluate the robustness of unlearning methods against variations in knowledge expression. Results reveal that Form-Dependent Bias is both widespread and severe among current techniques.
  We argue that LLM unlearning should be form-independent to address the endless forms of downstream tasks encountered in real-world security-critical scenarios. Towards this goal, we introduce Rank-one Concept Redirection (ROCR), a novel training-free method, as a promising solution path. ROCR performs unlearning by targeting the invariants in downstream tasks, specifically the activated dangerous concepts. It is capable of modifying model parameters within seconds to redirect the model's perception of a specific unlearning target concept to another harmless concept. Extensive experiments demonstrate that ROCR significantly improves unlearning effectiveness compared to traditional methods while generating highly natural outputs.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiMatch: Multihead Consistency Regularization Matching for Semi-Supervised Text Classification</title>
<link>https://arxiv.org/abs/2506.07801</link>
<guid>https://arxiv.org/abs/2506.07801</guid>
<content:encoded><![CDATA[
arXiv:2506.07801v1 Announce Type: cross 
Abstract: We introduce MultiMatch, a novel semi-supervised learning (SSL) algorithm combining the paradigms of co-training and consistency regularization with pseudo-labeling. At its core, MultiMatch features a three-fold pseudo-label weighting module designed for three key purposes: selecting and filtering pseudo-labels based on head agreement and model confidence, and weighting them according to the perceived classification difficulty. This novel module enhances and unifies three existing techniques -- heads agreement from Multihead Co-training, self-adaptive thresholds from FreeMatch, and Average Pseudo-Margins from MarginMatch -- resulting in a holistic approach that improves robustness and performance in SSL settings. Experimental results on benchmark datasets highlight the superior performance of MultiMatch, achieving state-of-the-art results on 9 out of 10 setups from 5 natural language processing datasets and ranking first according to the Friedman test among 19 methods. Furthermore, MultiMatch demonstrates exceptional robustness in highly imbalanced settings, outperforming the second-best approach by 3.26% -- and data imbalance is a key factor for many text classification tasks.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A weighted quantum ensemble of homogeneous quantum classifiers</title>
<link>https://arxiv.org/abs/2506.07810</link>
<guid>https://arxiv.org/abs/2506.07810</guid>
<content:encoded><![CDATA[
arXiv:2506.07810v1 Announce Type: cross 
Abstract: Ensemble methods in machine learning aim to improve prediction accuracy by combining multiple models. This is achieved by ensuring diversity among predictors to capture different data aspects. Homogeneous ensembles use identical models, achieving diversity through different data subsets, and weighted-average ensembles assign higher influence to more accurate models through a weight learning procedure. We propose a method to achieve a weighted homogeneous quantum ensemble using quantum classifiers with indexing registers for data encoding. This approach leverages instance-based quantum classifiers, enabling feature and training point subsampling through superposition and controlled unitaries, and allowing for a quantum-parallel execution of diverse internal classifiers with different data compositions in superposition. The method integrates a learning process involving circuit execution and classical weight optimization, for a trained ensemble execution with weights encoded in the circuit at test-time. Empirical evaluation demonstrate the effectiveness of the proposed method, offering insights into its performance.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Constrained Sampling: A Large Deviations Approach</title>
<link>https://arxiv.org/abs/2506.07816</link>
<guid>https://arxiv.org/abs/2506.07816</guid>
<content:encoded><![CDATA[
arXiv:2506.07816v1 Announce Type: cross 
Abstract: The problem of sampling a target probability distribution on a constrained domain arises in many applications including machine learning. For constrained sampling, various Langevin algorithms such as projected Langevin Monte Carlo (PLMC) based on the discretization of reflected Langevin dynamics (RLD) and more generally skew-reflected non-reversible Langevin Monte Carlo (SRNLMC) based on the discretization of skew-reflected non-reversible Langevin dynamics (SRNLD) have been proposed and studied in the literature. This work focuses on the long-time behavior of SRNLD, where a skew-symmetric matrix is added to RLD. Although the non-asymptotic convergence analysis for SRNLD (and SRNLMC) and the acceleration compared to RLD (and PMLC) have been studied in the literature, it is not clear how one should design the skew-symmetric matrix in the dynamics to achieve good performance in practice. We establish a large deviation principle (LDP) for the empirical measure of SRNLD when the skew-symmetric matrix is chosen such that its product with the inward unit normal vector field on the boundary is zero. By explicitly characterizing the rate functions, we show that SRNLD can accelerate the convergence to the target distribution compared to RLD with this choice of the skew-symmetric matrix. Numerical experiments for SRNLMC based on the proposed skew-symmetric matrix show superior performance which validate the theoretical findings from the large deviations theory.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R3D2: Realistic 3D Asset Insertion via Diffusion for Autonomous Driving Simulation</title>
<link>https://arxiv.org/abs/2506.07826</link>
<guid>https://arxiv.org/abs/2506.07826</guid>
<content:encoded><![CDATA[
arXiv:2506.07826v1 Announce Type: cross 
Abstract: Validating autonomous driving (AD) systems requires diverse and safety-critical testing, making photorealistic virtual environments essential. Traditional simulation platforms, while controllable, are resource-intensive to scale and often suffer from a domain gap with real-world data. In contrast, neural reconstruction methods like 3D Gaussian Splatting (3DGS) offer a scalable solution for creating photorealistic digital twins of real-world driving scenes. However, they struggle with dynamic object manipulation and reusability as their per-scene optimization-based methodology tends to result in incomplete object models with integrated illumination effects. This paper introduces R3D2, a lightweight, one-step diffusion model designed to overcome these limitations and enable realistic insertion of complete 3D assets into existing scenes by generating plausible rendering effects-such as shadows and consistent lighting-in real time. This is achieved by training R3D2 on a novel dataset: 3DGS object assets are generated from in-the-wild AD data using an image-conditioned 3D generative model, and then synthetically placed into neural rendering-based virtual environments, allowing R3D2 to learn realistic integration. Quantitative and qualitative evaluations demonstrate that R3D2 significantly enhances the realism of inserted assets, enabling use-cases like text-to-3D asset insertion and cross-scene/dataset object transfer, allowing for true scalability in AD validation. To promote further research in scalable and realistic AD simulation, we will release our dataset and code, see https://research.zenseact.com/publications/R3D2/.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conditional Local Independence Testing with Application to Dynamic Causal Discovery</title>
<link>https://arxiv.org/abs/2506.07844</link>
<guid>https://arxiv.org/abs/2506.07844</guid>
<content:encoded><![CDATA[
arXiv:2506.07844v1 Announce Type: cross 
Abstract: In this note, we extend the conditional local independence testing theory developed in Christgau et al. (2024) to Ito processes. The result can be applied to causal discovery in dynamic systems.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LogoSP: Local-global Grouping of Superpoints for Unsupervised Semantic Segmentation of 3D Point Clouds</title>
<link>https://arxiv.org/abs/2506.07857</link>
<guid>https://arxiv.org/abs/2506.07857</guid>
<content:encoded><![CDATA[
arXiv:2506.07857v1 Announce Type: cross 
Abstract: We study the problem of unsupervised 3D semantic segmentation on raw point clouds without needing human labels in training. Existing methods usually formulate this problem into learning per-point local features followed by a simple grouping strategy, lacking the ability to discover additional and possibly richer semantic priors beyond local features. In this paper, we introduce LogoSP to learn 3D semantics from both local and global point features. The key to our approach is to discover 3D semantic information by grouping superpoints according to their global patterns in the frequency domain, thus generating highly accurate semantic pseudo-labels for training a segmentation network. Extensive experiments on two indoor and an outdoor datasets show that our LogoSP surpasses all existing unsupervised methods by large margins, achieving the state-of-the-art performance for unsupervised 3D semantic segmentation. Notably, our investigation into the learned global patterns reveals that they truly represent meaningful 3D semantics in the absence of human labels during training.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep reinforcement learning for near-deterministic preparation of cubic- and quartic-phase gates in photonic quantum computing</title>
<link>https://arxiv.org/abs/2506.07859</link>
<guid>https://arxiv.org/abs/2506.07859</guid>
<content:encoded><![CDATA[
arXiv:2506.07859v1 Announce Type: cross 
Abstract: Cubic-phase states are a sufficient resource for universal quantum computing over continuous variables. We present results from numerical experiments in which deep neural networks are trained via reinforcement learning to control a quantum optical circuit for generating cubic-phase states, with an average success rate of 96%. The only non-Gaussian resource required is photon-number-resolving measurements. We also show that the exact same resources enable the direct generation of a quartic-phase gate, with no need for a cubic gate decomposition.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VIVAT: Virtuous Improving VAE Training through Artifact Mitigation</title>
<link>https://arxiv.org/abs/2506.07863</link>
<guid>https://arxiv.org/abs/2506.07863</guid>
<content:encoded><![CDATA[
arXiv:2506.07863v1 Announce Type: cross 
Abstract: Variational Autoencoders (VAEs) remain a cornerstone of generative computer vision, yet their training is often plagued by artifacts that degrade reconstruction and generation quality. This paper introduces VIVAT, a systematic approach to mitigating common artifacts in KL-VAE training without requiring radical architectural changes. We present a detailed taxonomy of five prevalent artifacts - color shift, grid patterns, blur, corner and droplet artifacts - and analyze their root causes. Through straightforward modifications, including adjustments to loss weights, padding strategies, and the integration of Spatially Conditional Normalization, we demonstrate significant improvements in VAE performance. Our method achieves state-of-the-art results in image reconstruction metrics (PSNR and SSIM) across multiple benchmarks and enhances text-to-image generation quality, as evidenced by superior CLIP scores. By preserving the simplicity of the KL-VAE framework while addressing its practical challenges, VIVAT offers actionable insights for researchers and practitioners aiming to optimize VAE training.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FreeGave: 3D Physics Learning from Dynamic Videos by Gaussian Velocity</title>
<link>https://arxiv.org/abs/2506.07865</link>
<guid>https://arxiv.org/abs/2506.07865</guid>
<content:encoded><![CDATA[
arXiv:2506.07865v1 Announce Type: cross 
Abstract: In this paper, we aim to model 3D scene geometry, appearance, and the underlying physics purely from multi-view videos. By applying various governing PDEs as PINN losses or incorporating physics simulation into neural networks, existing works often fail to learn complex physical motions at boundaries or require object priors such as masks or types. In this paper, we propose FreeGave to learn the physics of complex dynamic 3D scenes without needing any object priors. The key to our approach is to introduce a physics code followed by a carefully designed divergence-free module for estimating a per-Gaussian velocity field, without relying on the inefficient PINN losses. Extensive experiments on three public datasets and a newly collected challenging real-world dataset demonstrate the superior performance of our method for future frame extrapolation and motion segmentation. Most notably, our investigation into the learned physics codes reveals that they truly learn meaningful 3D physical motion patterns in the absence of any human labels in training.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoK: Data Reconstruction Attacks Against Machine Learning Models: Definition, Metrics, and Benchmark</title>
<link>https://arxiv.org/abs/2506.07888</link>
<guid>https://arxiv.org/abs/2506.07888</guid>
<content:encoded><![CDATA[
arXiv:2506.07888v1 Announce Type: cross 
Abstract: Data reconstruction attacks, which aim to recover the training dataset of a target model with limited access, have gained increasing attention in recent years. However, there is currently no consensus on a formal definition of data reconstruction attacks or appropriate evaluation metrics for measuring their quality. This lack of rigorous definitions and universal metrics has hindered further advancement in this field. In this paper, we address this issue in the vision domain by proposing a unified attack taxonomy and formal definitions of data reconstruction attacks. We first propose a set of quantitative evaluation metrics that consider important criteria such as quantifiability, consistency, precision, and diversity. Additionally, we leverage large language models (LLMs) as a substitute for human judgment, enabling visual evaluation with an emphasis on high-quality reconstructions. Using our proposed taxonomy and metrics, we present a unified framework for systematically evaluating the strengths and limitations of existing attacks and establishing a benchmark for future research. Empirical results, primarily from a memorization perspective, not only validate the effectiveness of our metrics but also offer valuable insights for designing new attacks.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GaussianVAE: Adaptive Learning Dynamics of 3D Gaussians for High-Fidelity Super-Resolution</title>
<link>https://arxiv.org/abs/2506.07897</link>
<guid>https://arxiv.org/abs/2506.07897</guid>
<content:encoded><![CDATA[
arXiv:2506.07897v1 Announce Type: cross 
Abstract: We present a novel approach for enhancing the resolution and geometric fidelity of 3D Gaussian Splatting (3DGS) beyond native training resolution. Current 3DGS methods are fundamentally limited by their input resolution, producing reconstructions that cannot extrapolate finer details than are present in the training views. Our work breaks this limitation through a lightweight generative model that predicts and refines additional 3D Gaussians where needed most. The key innovation is our Hessian-assisted sampling strategy, which intelligently identifies regions that are likely to benefit from densification, ensuring computational efficiency. Unlike computationally intensive GANs or diffusion approaches, our method operates in real-time (0.015s per inference on a single consumer-grade GPU), making it practical for interactive applications. Comprehensive experiments demonstrate significant improvements in both geometric accuracy and rendering quality compared to state-of-the-art methods, establishing a new paradigm for resolution-free 3D scene enhancement.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEMOIR: Lifelong Model Editing with Minimal Overwrite and Informed Retention for LLMs</title>
<link>https://arxiv.org/abs/2506.07899</link>
<guid>https://arxiv.org/abs/2506.07899</guid>
<content:encoded><![CDATA[
arXiv:2506.07899v1 Announce Type: cross 
Abstract: Language models deployed in real-world systems often require post-hoc updates to incorporate new or corrected knowledge. However, editing such models efficiently and reliably - without retraining or forgetting previous information - remains a major challenge. Existing methods for lifelong model editing either compromise generalization, interfere with past edits, or fail to scale to long editing sequences. We propose MEMOIR, a novel scalable framework that injects knowledge through a residual memory, i.e., a dedicated parameter module, while preserving the core capabilities of the pre-trained model. By sparsifying input activations through sample-dependent masks, MEMOIR confines each edit to a distinct subset of the memory parameters, minimizing interference among edits. At inference, it identifies relevant edits by comparing the sparse activation patterns of new queries to those stored during editing. This enables generalization to rephrased queries by activating only the relevant knowledge while suppressing unnecessary memory activation for unrelated prompts. Experiments on question answering, hallucination correction, and out-of-distribution generalization benchmarks across LLaMA-3 and Mistral demonstrate that MEMOIR achieves state-of-the-art performance across reliability, generalization, and locality metrics, scaling to thousands of sequential edits with minimal forgetting.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comparative Study of U-Net Architectures for Change Detection in Satellite Images</title>
<link>https://arxiv.org/abs/2506.07925</link>
<guid>https://arxiv.org/abs/2506.07925</guid>
<content:encoded><![CDATA[
arXiv:2506.07925v1 Announce Type: cross 
Abstract: Remote sensing change detection is essential for monitoring the everchanging landscapes of the Earth. The U-Net architecture has gained popularity for its capability to capture spatial information and perform pixel-wise classification. However, their application in the Remote sensing field remains largely unexplored. Therefore, this paper fill the gap by conducting a comprehensive analysis of 34 papers. This study conducts a comparison and analysis of 18 different U-Net variations, assessing their potential for detecting changes in remote sensing. We evaluate both benefits along with drawbacks of each variation within the framework of this particular application. We emphasize variations that are explicitly built for change detection, such as Siamese Swin-U-Net, which utilizes a Siamese architecture. The analysis highlights the significance of aspects such as managing data from different time periods and collecting relationships over a long distance to enhance the precision of change detection. This study provides valuable insights for researchers and practitioners that choose U-Net versions for remote sensing change detection tasks.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving Inequality Proofs with Large Language Models</title>
<link>https://arxiv.org/abs/2506.07927</link>
<guid>https://arxiv.org/abs/2506.07927</guid>
<content:encoded><![CDATA[
arXiv:2506.07927v1 Announce Type: cross 
Abstract: Inequality proving, crucial across diverse scientific and mathematical fields, tests advanced reasoning skills such as discovering tight bounds and strategic theorem application. This makes it a distinct, demanding frontier for large language models (LLMs), offering insights beyond general mathematical problem-solving. Progress in this area is hampered by existing datasets that are often scarce, synthetic, or rigidly formal. We address this by proposing an informal yet verifiable task formulation, recasting inequality proving into two automatically checkable subtasks: bound estimation and relation prediction. Building on this, we release IneqMath, an expert-curated dataset of Olympiad-level inequalities, including a test set and training corpus enriched with step-wise solutions and theorem annotations. We also develop a novel LLM-as-judge evaluation framework, combining a final-answer judge with four step-wise judges designed to detect common reasoning flaws. A systematic evaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even top models like o1 achieve less than 10% overall accuracy under step-wise scrutiny; this is a drop of up to 65.5% from their accuracy considering only final answer equivalence. This discrepancy exposes fragile deductive chains and a critical gap for current LLMs between merely finding an answer and constructing a rigorous proof. Scaling model size and increasing test-time computation yield limited gains in overall proof correctness. Instead, our findings highlight promising research directions such as theorem-guided reasoning and self-refinement. Code and data are available at https://ineqmath.github.io/.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Squeeze3D: Your 3D Generation Model is Secretly an Extreme Neural Compressor</title>
<link>https://arxiv.org/abs/2506.07932</link>
<guid>https://arxiv.org/abs/2506.07932</guid>
<content:encoded><![CDATA[
arXiv:2506.07932v1 Announce Type: cross 
Abstract: We propose Squeeze3D, a novel framework that leverages implicit prior knowledge learnt by existing pre-trained 3D generative models to compress 3D data at extremely high compression ratios. Our approach bridges the latent spaces between a pre-trained encoder and a pre-trained generation model through trainable mapping networks. Any 3D model represented as a mesh, point cloud, or a radiance field is first encoded by the pre-trained encoder and then transformed (i.e. compressed) into a highly compact latent code. This latent code can effectively be used as an extremely compressed representation of the mesh or point cloud. A mapping network transforms the compressed latent code into the latent space of a powerful generative model, which is then conditioned to recreate the original 3D model (i.e. decompression). Squeeze3D is trained entirely on generated synthetic data and does not require any 3D datasets. The Squeeze3D architecture can be flexibly used with existing pre-trained 3D encoders and existing generative models. It can flexibly support different formats, including meshes, point clouds, and radiance fields. Our experiments demonstrate that Squeeze3D achieves compression ratios of up to 2187x for textured meshes, 55x for point clouds, and 619x for radiance fields while maintaining visual quality comparable to many existing methods. Squeeze3D only incurs a small compression and decompression latency since it does not involve training object-specific networks to compress an object.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mimicking or Reasoning: Rethinking Multi-Modal In-Context Learning in Vision-Language Models</title>
<link>https://arxiv.org/abs/2506.07936</link>
<guid>https://arxiv.org/abs/2506.07936</guid>
<content:encoded><![CDATA[
arXiv:2506.07936v1 Announce Type: cross 
Abstract: Vision-language models (VLMs) are widely assumed to exhibit in-context learning (ICL), a property similar to that of their language-only counterparts. While recent work suggests VLMs can perform multimodal ICL (MM-ICL), studies show they often rely on shallow heuristics -- such as copying or majority voting -- rather than true task understanding. We revisit this assumption by evaluating VLMs under distribution shifts, where support examples come from a dataset different from the query. Surprisingly, performance often degrades with more demonstrations, and models tend to copy answers rather than learn from them. To investigate further, we propose a new MM-ICL with Reasoning pipeline that augments each demonstration with a generated rationale alongside the answer. We conduct extensive and comprehensive experiments on both perception- and reasoning-required datasets with open-source VLMs ranging from 3B to 72B and proprietary models such as Gemini 2.0. We conduct controlled studies varying shot count, retrieval method, rationale quality, and distribution. Our results show limited performance sensitivity across these factors, suggesting that current VLMs do not effectively utilize demonstration-level information as intended in MM-ICL.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradients: When Markets Meet Fine-tuning -- A Distributed Approach to Model Optimisation</title>
<link>https://arxiv.org/abs/2506.07940</link>
<guid>https://arxiv.org/abs/2506.07940</guid>
<content:encoded><![CDATA[
arXiv:2506.07940v1 Announce Type: cross 
Abstract: Foundation model fine-tuning faces a fundamental challenge: existing AutoML platforms rely on single optimisation strategies that explore only a fraction of viable hyperparameter configurations. In this white paper, We introduce Gradients, a decentralised AutoML platform that transforms hyperparameter optimisation into a competitive marketplace where independent miners compete to discover optimal configurations. Economic incentives align individual exploration with collective optimisation goals, driving systematic investigation of hyperparameter regions that centralised methods miss. We evaluate our approach across 180 controlled experiments spanning diverse model architectures (70M to 70B parameters) and task types. Gradients achieves an 82.8\% win rate against HuggingFace AutoTrain and 100\% against TogetherAI, Databricks, and Google Cloud, with mean improvements of 11.8\% and 42.1\% respectively. Complex reasoning and retrieval tasks show particularly strong gains of 30-40\%, whilst diffusion models achieve 23.4\% improvements for person-specific generation. These results demonstrate that competitive, economically-driven approaches can systematically discover superior configurations that centralised AutoML consistently miss.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discrete and Continuous Difference of Submodular Minimization</title>
<link>https://arxiv.org/abs/2506.07952</link>
<guid>https://arxiv.org/abs/2506.07952</guid>
<content:encoded><![CDATA[
arXiv:2506.07952v1 Announce Type: cross 
Abstract: Submodular functions, defined on continuous or discrete domains, arise in numerous applications. We study the minimization of the difference of two submodular (DS) functions, over both domains, extending prior work restricted to set functions. We show that all functions on discrete domains and all smooth functions on continuous domains are DS. For discrete domains, we observe that DS minimization is equivalent to minimizing the difference of two convex (DC) functions, as in the set function case. We propose a novel variant of the DC Algorithm (DCA) and apply it to the resulting DC Program, obtaining comparable theoretical guarantees as in the set function case. The algorithm can be applied to continuous domains via discretization. Experiments demonstrate that our method outperforms baselines in integer compressive sensing and integer least squares.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Models over Canonical Byte-Pair Encodings</title>
<link>https://arxiv.org/abs/2506.07956</link>
<guid>https://arxiv.org/abs/2506.07956</guid>
<content:encoded><![CDATA[
arXiv:2506.07956v1 Announce Type: cross 
Abstract: Modern language models represent probability distributions over character strings as distributions over (shorter) token strings derived via a deterministic tokenizer, such as byte-pair encoding. While this approach is highly effective at scaling up language models to large corpora, its current incarnations have a concerning property: the model assigns nonzero probability mass to an exponential number of $\it{noncanonical}$ token encodings of each character string -- these are token strings that decode to valid character strings but are impossible under the deterministic tokenizer (i.e., they will never be seen in any training corpus, no matter how large). This misallocation is both erroneous, as noncanonical strings never appear in training data, and wasteful, diverting probability mass away from plausible outputs. These are avoidable mistakes! In this work, we propose methods to enforce canonicality in token-level language models, ensuring that only canonical token strings are assigned positive probability. We present two approaches: (1) canonicality by conditioning, leveraging test-time inference strategies without additional training, and (2) canonicality by construction, a model parameterization that guarantees canonical outputs but requires training. We demonstrate that fixing canonicality mistakes improves the likelihood of held-out data for several models and corpora.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-time Localization of a Soccer Ball from a Single Camera</title>
<link>https://arxiv.org/abs/2506.07981</link>
<guid>https://arxiv.org/abs/2506.07981</guid>
<content:encoded><![CDATA[
arXiv:2506.07981v1 Announce Type: cross 
Abstract: We propose a computationally efficient method for real-time three-dimensional football trajectory reconstruction from a single broadcast camera. In contrast to previous work, our approach introduces a multi-mode state model with $W$ discrete modes to significantly accelerate optimization while preserving centimeter-level accuracy -- even in cases of severe occlusion, motion blur, and complex backgrounds. The system operates on standard CPUs and achieves low latency suitable for live broadcast settings. Extensive evaluation on a proprietary dataset of 6K-resolution Russian Premier League matches demonstrates performance comparable to multi-camera systems, without the need for specialized or costly infrastructure. This work provides a practical method for accessible and accurate 3D ball tracking in professional football environments.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CXR-LT 2024: A MICCAI challenge on long-tailed, multi-label, and zero-shot disease classification from chest X-ray</title>
<link>https://arxiv.org/abs/2506.07984</link>
<guid>https://arxiv.org/abs/2506.07984</guid>
<content:encoded><![CDATA[
arXiv:2506.07984v1 Announce Type: cross 
Abstract: The CXR-LT series is a community-driven initiative designed to enhance lung disease classification using chest X-rays (CXR). It tackles challenges in open long-tailed lung disease classification and enhances the measurability of state-of-the-art techniques. The first event, CXR-LT 2023, aimed to achieve these goals by providing high-quality benchmark CXR data for model development and conducting comprehensive evaluations to identify ongoing issues impacting lung disease classification performance. Building on the success of CXR-LT 2023, the CXR-LT 2024 expands the dataset to 377,110 chest X-rays (CXRs) and 45 disease labels, including 19 new rare disease findings. It also introduces a new focus on zero-shot learning to address limitations identified in the previous event. Specifically, CXR-LT 2024 features three tasks: (i) long-tailed classification on a large, noisy test set, (ii) long-tailed classification on a manually annotated "gold standard" subset, and (iii) zero-shot generalization to five previously unseen disease findings. This paper provides an overview of CXR-LT 2024, detailing the data curation process and consolidating state-of-the-art solutions, including the use of multimodal models for rare disease detection, advanced generative approaches to handle noisy labels, and zero-shot learning strategies for unseen diseases. Additionally, the expanded dataset enhances disease coverage to better represent real-world clinical settings, offering a valuable resource for future research. By synthesizing the insights and innovations of participating teams, we aim to advance the development of clinically realistic and generalizable diagnostic models for chest radiography.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Crowd-Sourced Evaluation of Neuron Explanations</title>
<link>https://arxiv.org/abs/2506.07985</link>
<guid>https://arxiv.org/abs/2506.07985</guid>
<content:encoded><![CDATA[
arXiv:2506.07985v1 Announce Type: cross 
Abstract: Interpreting individual neurons or directions in activations space is an important component of mechanistic interpretability. As such, many algorithms have been proposed to automatically produce neuron explanations, but it is often not clear how reliable these explanations are, or which methods produce the best explanations. This can be measured via crowd-sourced evaluations, but they can often be noisy and expensive, leading to unreliable results. In this paper, we carefully analyze the evaluation pipeline and develop a cost-effective and highly accurate crowdsourced evaluation strategy. In contrast to previous human studies that only rate whether the explanation matches the most highly activating inputs, we estimate whether the explanation describes neuron activations across all inputs. To estimate this effectively, we introduce a novel application of importance sampling to determine which inputs are the most valuable to show to raters, leading to around 30x cost reduction compared to uniform sampling. We also analyze the label noise present in crowd-sourced evaluations and propose a Bayesian method to aggregate multiple ratings leading to a further ~5x reduction in number of ratings required for the same accuracy. Finally, we use these methods to conduct a large-scale study comparing the quality of neuron explanations produced by the most popular methods for two different vision models.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MADFormer: Mixed Autoregressive and Diffusion Transformers for Continuous Image Generation</title>
<link>https://arxiv.org/abs/2506.07999</link>
<guid>https://arxiv.org/abs/2506.07999</guid>
<content:encoded><![CDATA[
arXiv:2506.07999v1 Announce Type: cross 
Abstract: Recent progress in multimodal generation has increasingly combined autoregressive (AR) and diffusion-based approaches, leveraging their complementary strengths: AR models capture long-range dependencies and produce fluent, context-aware outputs, while diffusion models operate in continuous latent spaces to refine high-fidelity visual details. However, existing hybrids often lack systematic guidance on how and why to allocate model capacity between these paradigms. In this work, we introduce MADFormer, a Mixed Autoregressive and Diffusion Transformer that serves as a testbed for analyzing AR-diffusion trade-offs. MADFormer partitions image generation into spatial blocks, using AR layers for one-pass global conditioning across blocks and diffusion layers for iterative local refinement within each block. Through controlled experiments on FFHQ-1024 and ImageNet, we identify two key insights: (1) block-wise partitioning significantly improves performance on high-resolution images, and (2) vertically mixing AR and diffusion layers yields better quality-efficiency balances--improving FID by up to 75% under constrained inference compute. Our findings offer practical design principles for future hybrid generative models.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hidden in plain sight: VLMs overlook their visual representations</title>
<link>https://arxiv.org/abs/2506.08008</link>
<guid>https://arxiv.org/abs/2506.08008</guid>
<content:encoded><![CDATA[
arXiv:2506.08008v1 Announce Type: cross 
Abstract: Language provides a natural interface to specify and evaluate performance on visual tasks. To realize this possibility, vision language models (VLMs) must successfully integrate visual and linguistic information. Our work compares VLMs to a direct readout of their visual encoders to understand their ability to integrate across these modalities. Across a series of vision-centric benchmarks (e.g., depth estimation, correspondence), we find that VLMs perform substantially worse than their visual encoders, dropping to near-chance performance. We investigate these results through a series of analyses across the entire VLM: namely 1) the degradation of vision representations, 2) brittleness to task prompt, and 3) the language model's role in solving the task. We find that the bottleneck in performing these vision-centric tasks lies in this third category; VLMs are not effectively using visual information easily accessible throughout the entire model, and they inherit the language priors present in the LLM. Our work helps diagnose the failure modes of open-source VLMs, and presents a series of evaluations useful for future investigations into visual understanding within VLMs.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion</title>
<link>https://arxiv.org/abs/2506.08009</link>
<guid>https://arxiv.org/abs/2506.08009</guid>
<content:encoded><![CDATA[
arXiv:2506.08009v1 Announce Type: cross 
Abstract: We introduce Self Forcing, a novel training paradigm for autoregressive video diffusion models. It addresses the longstanding issue of exposure bias, where models trained on ground-truth context must generate sequences conditioned on their own imperfect outputs during inference. Unlike prior methods that denoise future frames based on ground-truth context frames, Self Forcing conditions each frame's generation on previously self-generated outputs by performing autoregressive rollout with key-value (KV) caching during training. This strategy enables supervision through a holistic loss at the video level that directly evaluates the quality of the entire generated sequence, rather than relying solely on traditional frame-wise objectives. To ensure training efficiency, we employ a few-step diffusion model along with a stochastic gradient truncation strategy, effectively balancing computational cost and performance. We further introduce a rolling KV cache mechanism that enables efficient autoregressive video extrapolation. Extensive experiments demonstrate that our approach achieves real-time streaming video generation with sub-second latency on a single GPU, while matching or even surpassing the generation quality of significantly slower and non-causal diffusion models. Project website: http://self-forcing.github.io/
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StableMTL: Repurposing Latent Diffusion Models for Multi-Task Learning from Partially Annotated Synthetic Datasets</title>
<link>https://arxiv.org/abs/2506.08013</link>
<guid>https://arxiv.org/abs/2506.08013</guid>
<content:encoded><![CDATA[
arXiv:2506.08013v1 Announce Type: cross 
Abstract: Multi-task learning for dense prediction is limited by the need for extensive annotation for every task, though recent works have explored training with partial task labels. Leveraging the generalization power of diffusion models, we extend the partial learning setup to a zero-shot setting, training a multi-task model on multiple synthetic datasets, each labeled for only a subset of tasks. Our method, StableMTL, repurposes image generators for latent regression. Adapting a denoising framework with task encoding, per-task conditioning and a tailored training scheme. Instead of per-task losses requiring careful balancing, a unified latent loss is adopted, enabling seamless scaling to more tasks. To encourage inter-task synergy, we introduce a multi-stream model with a task-attention mechanism that converts N-to-N task interactions into efficient 1-to-N attention, promoting effective cross-task sharing. StableMTL outperforms baselines on 7 tasks across 8 benchmarks.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R-FORCE: Robust Learning for Random Recurrent Neural Networks</title>
<link>https://arxiv.org/abs/2003.11660</link>
<guid>https://arxiv.org/abs/2003.11660</guid>
<content:encoded><![CDATA[
arXiv:2003.11660v2 Announce Type: replace 
Abstract: Random Recurrent Neural Networks (RRNN) are the simplest recurrent networks to model and extract features from sequential data. The simplicity however comes with a price; RRNN are known to be susceptible to diminishing/exploding gradient problem when trained with gradient-descent based optimization. To enhance robustness of RRNN, alternative training approaches have been proposed. Specifically, FORCE learning approach proposed a recursive least squares alternative to train RRNN and was shown to be applicable even for the challenging task of target-learning, where the network is tasked with generating dynamic patterns with no guiding input. While FORCE training indicates that solving target-learning is possible, it appears to be effective only in a specific regime of network dynamics (edge-of-chaos). We thereby investigate whether initialization of RRNN connectivity according to a tailored distribution can guarantee robust FORCE learning. We are able to generate such distribution by inference of four generating principles constraining the spectrum of the network Jacobian to remain in stability region. This initialization along with FORCE learning provides a robust training method, i.e., Robust-FORCE (R-FORCE). We validate R-FORCE performance on various target functions for a wide range of network configurations and compare with alternative methods. Our experiments indicate that R-FORCE facilitates significantly more stable and accurate target-learning for a wide class of RRNN. Such stability becomes critical in modeling multi-dimensional sequences as we demonstrate on modeling time-series of human body joints during physical movements.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effective Regularization Through Loss-Function Metalearning</title>
<link>https://arxiv.org/abs/2010.00788</link>
<guid>https://arxiv.org/abs/2010.00788</guid>
<content:encoded><![CDATA[
arXiv:2010.00788v4 Announce Type: replace 
Abstract: Evolutionary computation can be used to optimize several different aspects of neural network architectures. For instance, the TaylorGLO method discovers novel, customized loss functions, resulting in improved performance, faster training, and improved data utilization. A likely reason is that such functions discourage overfitting, leading to effective regularization. This paper demonstrates theoretically that this is indeed the case for TaylorGLO. Learning rule decomposition reveals that evolved loss functions balance two factors: the pull toward zero error, and a push away from it to avoid overfitting. This is a general principle that may be used to understand other regularization techniques as well (as demonstrated in this paper for label smoothing). The theoretical analysis leads to a constraint that can be utilized to find more effective loss functions in practice; the mechanism also results in networks that are more robust (as demonstrated in this paper with adversarial inputs). The analysis in this paper thus constitutes a first step towards understanding regularization, and demonstrates the power of evolutionary neural architecture search in general.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analysis of Thompson Sampling for Controlling Unknown Linear Diffusion Processes</title>
<link>https://arxiv.org/abs/2206.09977</link>
<guid>https://arxiv.org/abs/2206.09977</guid>
<content:encoded><![CDATA[
arXiv:2206.09977v2 Announce Type: replace 
Abstract: Linear diffusion processes serve as canonical continuous-time models for dynamic decision-making under uncertainty. These systems evolve according to drift matrices that specify the instantaneous rates of change in the expected system state, while also experiencing continuous random disturbances modeled by Brownian noise. For instance, in medical applications such as artificial pancreas systems, the drift matrices represent the internal dynamics of glucose concentrations. Classical results in stochastic control provide optimal policies under perfect knowledge of the drift matrices. However, practical decision-making scenarios typically feature uncertainty about the drift; in medical contexts, such parameters are patient-specific and unknown, requiring adaptive policies for efficiently learning the drift matrices while ensuring system stability and optimal performance.
  We study the Thompson sampling (TS) algorithm for decision-making in linear diffusion processes with unknown drift matrices. For this algorithm that designs control policies as if samples from a posterior belief about the parameters fully coincide with the unknown truth, we establish efficiency. That is, Thompson sampling learns optimal control actions fast, incurring only a square-root of time regret, and also learns to stabilize the system in a short time period. To our knowledge, this is the first such result for TS in a diffusion process control problem. Moreover, our empirical simulations in three settings that involve blood-glucose and flight control demonstrate that TS significantly improves regret, compared to the state-of-the-art algorithms, suggesting it explores in a more guarded fashion. Our theoretical analysis includes characterization of a certain optimality manifold that relates the geometry of the drift matrices to the optimal control of the diffusion process, among others.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GradSkip: Communication-Accelerated Local Gradient Methods with Better Computational Complexity</title>
<link>https://arxiv.org/abs/2210.16402</link>
<guid>https://arxiv.org/abs/2210.16402</guid>
<content:encoded><![CDATA[
arXiv:2210.16402v3 Announce Type: replace 
Abstract: We study a class of distributed optimization algorithms that aim to alleviate high communication costs by allowing clients to perform multiple local gradient-type training steps before communication. In a recent breakthrough, Mishchenko et al. (2022) proved that local training, when properly executed, leads to provable communication acceleration, and this holds in the strongly convex regime without relying on any data similarity assumptions. However, their ProxSkip method requires all clients to take the same number of local training steps in each communication round. We propose a redesign of the ProxSkip method, allowing clients with ``less important'' data to get away with fewer local training steps without impacting the overall communication complexity of the method. In particular, we prove that our modified method, GradSkip, converges linearly under the same assumptions and has the same accelerated communication complexity, while the number of local gradient steps can be reduced relative to a local condition number. We further generalize our method by extending the randomness of probabilistic alternations to arbitrary unbiased compression operators and by considering a generic proximable regularizer. This generalization, which we call GradSkip+, recovers several related methods in the literature as special cases. Finally, we present an empirical study on carefully designed toy problems that confirm our theoretical claims.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic Nonlinear Control via Finite-dimensional Spectral Dynamic Embedding</title>
<link>https://arxiv.org/abs/2304.03907</link>
<guid>https://arxiv.org/abs/2304.03907</guid>
<content:encoded><![CDATA[
arXiv:2304.03907v5 Announce Type: replace 
Abstract: This paper proposes an approach, Spectral Dynamics Embedding Control (SDEC), to optimal control for nonlinear stochastic systems. This method reveals an infinite-dimensional feature representation induced by the system's nonlinear stochastic dynamics, enabling a linear representation of the state-action value function. For practical implementation, this representation is approximated using finite-dimensional trucations, specifically via two prominent kernel approximation methods: random feature truncation and Nystrom approximation. To characterize the effectiveness of these approximations, we provide an in-depth theoretical analysis to characterize the approximation error arising from the finite-dimension truncation and statistical error due to finite-sample approximation in both policy evaluation and policy optimization. Empirically, our algorithm performs favorably against existing stochastic control algorithms on several benchmark problems.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Homophily-Driven Sanitation View for Robust Graph Contrastive Learning</title>
<link>https://arxiv.org/abs/2307.12555</link>
<guid>https://arxiv.org/abs/2307.12555</guid>
<content:encoded><![CDATA[
arXiv:2307.12555v2 Announce Type: replace 
Abstract: We investigate adversarial robustness of unsupervised Graph Contrastive Learning (GCL) against structural attacks. First, we provide a comprehensive empirical and theoretical analysis of existing attacks, revealing how and why they downgrade the performance of GCL. Inspired by our analytic results, we present a robust GCL framework that integrates a homophily-driven sanitation view, which can be learned jointly with contrastive learning. A key challenge this poses, however, is the non-differentiable nature of the sanitation objective. To address this challenge, we propose a series of techniques to enable gradient-based end-to-end robust GCL. Moreover, we develop a fully unsupervised hyperparameter tuning method which, unlike prior approaches, does not require knowledge of node labels. We conduct extensive experiments to evaluate the performance of our proposed model, GCHS (Graph Contrastive Learning with Homophily-driven Sanitation View), against two state of the art structural attacks on GCL. Our results demonstrate that GCHS consistently outperforms all state of the art baselines in terms of the quality of generated node embeddings as well as performance on two important downstream tasks.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DOMAIN: MilDly COnservative Model-BAsed OfflINe Reinforcement Learning</title>
<link>https://arxiv.org/abs/2309.08925</link>
<guid>https://arxiv.org/abs/2309.08925</guid>
<content:encoded><![CDATA[
arXiv:2309.08925v4 Announce Type: replace 
Abstract: Model-based reinforcement learning (RL), which learns an environment model from the offline dataset and generates more out-of-distribution model data, has become an effective approach to the problem of distribution shift in offline RL. Due to the gap between the learned and actual environment, conservatism should be incorporated into the algorithm to balance accurate offline data and imprecise model data. The conservatism of current algorithms mostly relies on model uncertainty estimation. However, uncertainty estimation is unreliable and leads to poor performance in certain scenarios, and the previous methods ignore differences between the model data, which brings great conservatism. To address the above issues, this paper proposes a milDly cOnservative Model-bAsed offlINe RL algorithm (DOMAIN) without estimating model uncertainty, and designs the adaptive sampling distribution of model samples, which can adaptively adjust the model data penalty. In this paper, we theoretically demonstrate that the Q value learned by the DOMAIN outside the region is a lower bound of the true Q value, the DOMAIN is less conservative than previous model-based offline RL algorithms, and has the guarantee of safety policy improvement. The results of extensive experiments show that DOMAIN outperforms prior RL algorithms and the average performance has improved by 1.8% on the D4RL benchmark.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sharpness-Aware Teleportation on Riemannian Manifolds</title>
<link>https://arxiv.org/abs/2309.17215</link>
<guid>https://arxiv.org/abs/2309.17215</guid>
<content:encoded><![CDATA[
arXiv:2309.17215v2 Announce Type: replace 
Abstract: Recent studies highlight the effectiveness of flat minima in enhancing generalization, with sharpness-aware minimization (SAM) achieving state-of-the-art performance. Additionally, insights into the intrinsic geometry of the loss landscape have shown promise for improving model generalization. Building on these advancements, we introduce a novel sharpness-aware, geometry-aware teleportation mechanism to further enhance robustness and generalization. The core innovation of our approach is to decompose each iteration into a teleportation step within a local orbit and a sharpness-aware step that transitions between different orbits, leveraging the Riemannian quotient manifold. Our approach is grounded in a theoretical framework that analyzes the generalization gap between population loss and worst-case empirical loss within the context of Riemannian manifolds. To demonstrate the effectiveness of our method, we evaluate and compare our algorithm on diverse vision benchmarks with various datasets and Riemannian manifolds.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diversity from Human Feedback</title>
<link>https://arxiv.org/abs/2310.06648</link>
<guid>https://arxiv.org/abs/2310.06648</guid>
<content:encoded><![CDATA[
arXiv:2310.06648v3 Announce Type: replace 
Abstract: Diversity plays a significant role in many problems, such as ensemble learning, reinforcement learning, and combinatorial optimization. How to define the diversity measure is a longstanding problem. Many methods rely on expert experience to define a proper behavior space and then obtain the diversity measure, which is, however, challenging in many scenarios. In this paper, we propose the problem of learning a behavior space from human feedback and present a general method called Diversity from Human Feedback (DivHF) to solve it. DivHF learns a behavior descriptor consistent with human preference by querying human feedback. The learned behavior descriptor can be combined with any distance measure to define a diversity measure. We demonstrate the effectiveness of DivHF by integrating it with the Quality-Diversity optimization algorithm MAP-Elites and conducting experiments on the QDax suite. The results show that the behavior learned by DivHF is much more consistent with human requirements than the one learned by direct data-driven approaches without human feedback, and makes the final solutions more diverse under human preference. Our contributions include formulating the problem, proposing the DivHF method, and demonstrating its effectiveness through experiments.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SecFormer: Fast and Accurate Privacy-Preserving Inference for Transformer Models via SMPC</title>
<link>https://arxiv.org/abs/2401.00793</link>
<guid>https://arxiv.org/abs/2401.00793</guid>
<content:encoded><![CDATA[
arXiv:2401.00793v5 Announce Type: replace 
Abstract: With the growing use of Transformer models hosted on cloud platforms to offer inference services, privacy concerns are escalating, especially concerning sensitive data like investment plans and bank account details. Secure Multi-Party Computing (SMPC) emerges as a promising solution to protect the privacy of inference data and model parameters. However, the application of SMPC in Privacy-Preserving Inference (PPI) for Transformer models often leads to considerable slowdowns or declines in performance. This is largely due to the multitude of nonlinear operations in the Transformer architecture, which are not well-suited to SMPC and difficult to circumvent or optimize effectively. To address this concern, we introduce a comprehensive PPI framework called SecFormer to achieve fast and accurate PPI for Transformer models. We successfully eliminate the high-cost exponential and maximum operations in PPI without sacrificing model performance and develop a suite of efficient SMPC protocols by employing suitable numerical computation methods to boost other complex nonlinear functions in PPI, including GeLU, LayerNorm, and a redesigned Softmax. Our extensive experiments reveal that SecFormer outperforms MPCFormer in performance, showing improvements of $3.4\%$ and $24.7\%$ for BERT$_{\text{BASE}}$ and BERT$_{\text{LARGE}}$, respectively. In terms of efficiency, SecFormer is 3.57 and 3.58 times faster than PUMA for BERT$_{\text{BASE}}$ and BERT$_{\text{LARGE}}$, demonstrating its effectiveness and speed.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Out-of-Distribution Objects through Class-Conditioned Inpainting</title>
<link>https://arxiv.org/abs/2402.03292</link>
<guid>https://arxiv.org/abs/2402.03292</guid>
<content:encoded><![CDATA[
arXiv:2402.03292v3 Announce Type: replace 
Abstract: Recent object detectors have achieved impressive accuracy in identifying objects seen during training. However, real-world deployment often introduces novel and unexpected objects, referred to as out-of-distribution (OOD) objects, posing significant challenges to model trustworthiness. Modern object detectors are typically overconfident, making it unreliable to use their predictions alone for OOD detection. To address this, we propose leveraging an auxiliary model as a complementary solution. Specifically, we utilize an off-the-shelf text-to-image generative model, such as Stable Diffusion, which is trained with objective functions distinct from those of discriminative object detectors. We hypothesize that this fundamental difference enables the detection of OOD objects by measuring inconsistencies between the models. Concretely, for a given detected object bounding box and its predicted in-distribution class label, we perform class-conditioned inpainting on the image with the object removed. If the object is OOD, the inpainted image is likely to deviate significantly from the original, making the reconstruction error a robust indicator of OOD status. Extensive experiments demonstrate that our approach consistently surpasses existing zero-shot and non-zero-shot OOD detection methods, establishing a robust framework for enhancing object detection systems in dynamic environments.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Link Prediction with Relational Hypergraphs</title>
<link>https://arxiv.org/abs/2402.04062</link>
<guid>https://arxiv.org/abs/2402.04062</guid>
<content:encoded><![CDATA[
arXiv:2402.04062v3 Announce Type: replace 
Abstract: Link prediction with knowledge graphs has been thoroughly studied in graph machine learning, leading to a rich landscape of graph neural network architectures with successful applications. Nonetheless, it remains challenging to transfer the success of these architectures to relational hypergraphs, where the task of link prediction is over $k$-ary relations, which is substantially harder than link prediction with knowledge graphs. In this paper, we propose a framework for link prediction with relational hypergraphs, unlocking applications of graph neural networks to fully relational structures. Theoretically, we conduct a thorough analysis of the expressive power of the resulting model architectures via corresponding relational Weisfeiler-Leman algorithms and also via logical expressiveness. Empirically, we validate the power of the proposed model architectures on various relational hypergraph benchmarks. The resulting model architectures substantially outperform every baseline for inductive link prediction, and lead to state-of-the-art results for transductive link prediction.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EasyFS: an Efficient Model-free Feature Selection Framework via Elastic Transformation of Features</title>
<link>https://arxiv.org/abs/2402.05954</link>
<guid>https://arxiv.org/abs/2402.05954</guid>
<content:encoded><![CDATA[
arXiv:2402.05954v2 Announce Type: replace 
Abstract: Traditional model-free feature selection methods treat each feature independently while disregarding the interrelationships among features, which leads to relatively poor performance compared with the model-aware methods. To address this challenge, we propose an efficient model-free feature selection framework via elastic expansion and compression of the features, namely EasyFS, to achieve better performance than state-of-the-art model-aware methods while sharing the characters of efficiency and flexibility with the existing model-free methods. In particular, EasyFS expands the feature space by using the random non-linear projection network to achieve the non-linear combinations of the original features, so as to model the interrelationships among the features and discover most correlated features. Meanwhile, a novel redundancy measurement based on the change of coding rate is proposed for efficient filtering of redundant features. Comprehensive experiments on 21 different datasets show that EasyFS outperforms state-of-the art methods up to 10.9\% in the regression tasks and 5.7\% in the classification tasks while saving more than 94\% of the time.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Preference Optimization for Sample Efficient RLHF</title>
<link>https://arxiv.org/abs/2402.10500</link>
<guid>https://arxiv.org/abs/2402.10500</guid>
<content:encoded><![CDATA[
arXiv:2402.10500v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) aligned using Reinforcement Learning from Human Feedback (RLHF) have shown remarkable generation abilities in numerous tasks. However, collecting high-quality human preferences creates costly bottlenecks in practical deployments, and hence, training data are often budgeted. In these scenarios, it is crucial to collect training data (e.g., contexts, a pair of generations for each context, and a preference indicating which generation is better) carefully, yet most of the existing methods sample contexts uniformly at random from a given collection. Given this, under the Bradley-Terry-Luce preference model and with a small budget of training data, we show that uniform sampling of contexts could lead to a policy (i.e., an aligned model) that suffers a constant sub-optimality gap from the optimal policy. This highlights the need for an adaptive context sampling strategy for effective alignment under a small sample budget. To address this, we reformulate RLHF within the contextual preference bandit framework, treating generations as actions, and give a nearly complete characterization of the sub-optimality gap in terms of both lower and upper bounds. First, when the action set is a $d$-dimensional hypercube and the number of samples is $T$, we show an $\Omega(d/\sqrt{T})$ lower bound. Next, we propose an algorithm, $\textit{Active Preference Optimization}$ ($\texttt{APO}$), that iteratively collects preferences for the most uncertain contexts. We show that the sub-optimality gap of the policy learned via $\texttt{APO}$ matches the lower bound up to a log factor and a non-linearity constant. Finally, we perform experiments on practical datasets to validate $\texttt{APO}$'s efficacy over existing methods, establishing it as a sample-efficient and cost-effective solution for LLM alignment.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An end-to-end attention-based approach for learning on graphs</title>
<link>https://arxiv.org/abs/2402.10793</link>
<guid>https://arxiv.org/abs/2402.10793</guid>
<content:encoded><![CDATA[
arXiv:2402.10793v3 Announce Type: replace 
Abstract: There has been a recent surge in transformer-based architectures for learning on graphs, mainly motivated by attention as an effective learning mechanism and the desire to supersede handcrafted operators characteristic of message passing schemes. However, concerns over their empirical effectiveness, scalability, and complexity of the pre-processing steps have been raised, especially in relation to much simpler graph neural networks that typically perform on par with them across a wide range of benchmarks. To tackle these shortcomings, we consider graphs as sets of edges and propose a purely attention-based approach consisting of an encoder and an attention pooling mechanism. The encoder vertically interleaves masked and vanilla self-attention modules to learn an effective representations of edges, while allowing for tackling possible misspecifications in input graphs. Despite its simplicity, the approach outperforms fine-tuned message passing baselines and recently proposed transformer-based methods on more than 70 node and graph-level tasks, including challenging long-range benchmarks. Moreover, we demonstrate state-of-the-art performance across different tasks, ranging from molecular to vision graphs, and heterophilous node classification. The approach also outperforms graph neural networks and transformers in transfer learning settings, and scales much better than alternatives with a similar performance level or expressive power.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TS-RSR: A provably efficient approach for batch Bayesian Optimization</title>
<link>https://arxiv.org/abs/2403.04764</link>
<guid>https://arxiv.org/abs/2403.04764</guid>
<content:encoded><![CDATA[
arXiv:2403.04764v4 Announce Type: replace 
Abstract: This paper presents a new approach for batch Bayesian Optimization (BO) called Thompson Sampling-Regret to Sigma Ratio directed sampling (TS-RSR), where we sample a new batch of actions by minimizing a Thompson Sampling approximation of a regret to uncertainty ratio. Our sampling objective is able to coordinate the actions chosen in each batch in a way that minimizes redundancy between points whilst focusing on points with high predictive means or high uncertainty. Theoretically, we provide rigorous convergence guarantees on our algorithm's regret, and numerically, we demonstrate that our method attains state-of-the-art performance on a range of challenging synthetic and realistic test functions, where it outperforms several competitive benchmark batch BO algorithms.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Binary Classifier Optimization for Large Language Model Alignment</title>
<link>https://arxiv.org/abs/2404.04656</link>
<guid>https://arxiv.org/abs/2404.04656</guid>
<content:encoded><![CDATA[
arXiv:2404.04656v2 Announce Type: replace 
Abstract: In real-world services such as ChatGPT, aligning models based on user feedback is crucial for improving model performance. However, due to the simplicity and convenience of providing feedback, users typically offer only basic binary signals, such as 'thumbs-up' or 'thumbs-down'. Most existing alignment research, on the other hand, relies on preference-based approaches that require both positive and negative responses as a pair. We propose Binary Classifier Optimization (BCO), a technique that effectively aligns LLMs using only binary feedback. BCO trains a binary classifier, where the logit serves as an implicit reward, effectively minimizing the Direct Preference Optimization (DPO) loss. We demonstrate that the binary cross-entropy loss employed in classifier training acts as an upper bound for the DPO loss. Additionally, a novel reward shift technique further minimizes the gap between the losses. We validate our methodology in two settings: first, on a paired preference dataset, where our method performs on par with DPO; and second, on a Likert-5 scale annotation dataset which stems from real users' queries. Our model consistently demonstrates effective and robust alignment across four base LLMs and three different datasets, showcasing the strength of our approach to learning from binary signals.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Ridgelet Transform and Unified Universality Theorem for Deep and Shallow Joint-Group-Equivariant Machines</title>
<link>https://arxiv.org/abs/2405.13682</link>
<guid>https://arxiv.org/abs/2405.13682</guid>
<content:encoded><![CDATA[
arXiv:2405.13682v5 Announce Type: replace 
Abstract: We present a constructive universal approximation theorem for learning machines equipped with joint-group-equivariant feature maps, called the joint-equivariant machines, based on the group representation theory. ``Constructive'' here indicates that the distribution of parameters is given in a closed-form expression known as the ridgelet transform. Joint-group-equivariance encompasses a broad class of feature maps that generalize classical group-equivariance. Particularly, fully-connected networks are not group-equivariant but are joint-group-equivariant. Our main theorem also unifies the universal approximation theorems for both shallow and deep networks. Until this study, the universality of deep networks has been shown in a different manner from the universality of shallow networks, but our results discuss them on common ground. Now we can understand the approximation schemes of various learning machines in a unified manner. As applications, we show the constructive universal approximation properties of four examples: depth-$n$ joint-equivariant machine, depth-$n$ fully-connected network, depth-$n$ group-convolutional network, and a new depth-$2$ network with quadratic forms whose universality has not been known.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Output-Constrained Decision Trees</title>
<link>https://arxiv.org/abs/2405.15314</link>
<guid>https://arxiv.org/abs/2405.15314</guid>
<content:encoded><![CDATA[
arXiv:2405.15314v3 Announce Type: replace 
Abstract: Incorporating domain-specific constraints into machine learning models is essential for generating predictions that are both accurate and feasible in real-world applications. This paper introduces new methods for training Output-Constrained Regression Trees (OCRT), addressing the limitations of traditional decision trees in constrained multi-target regression tasks. We propose three approaches: M-OCRT, which uses split-based mixed integer programming to enforce constraints; E-OCRT, which employs an exhaustive search for optimal splits and solves constrained prediction problems at each decision node; and EP-OCRT, which applies post-hoc constrained optimization to tree predictions. To illustrate their potential uses in ensemble learning, we also introduce a random forest framework working under convex feasible sets. We validate the proposed methods through a computational study both on synthetic and industry-driven hierarchical time series datasets. Our results demonstrate that imposing constraints on decision tree training results in accurate and feasible predictions.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equivariant Denoisers Cannot Copy Graphs: Align Your Graph Diffusion Models</title>
<link>https://arxiv.org/abs/2405.17656</link>
<guid>https://arxiv.org/abs/2405.17656</guid>
<content:encoded><![CDATA[
arXiv:2405.17656v2 Announce Type: replace 
Abstract: Graph diffusion models, dominant in graph generative modeling, remain underexplored for graph-to-graph translation tasks like chemical reaction prediction. We demonstrate that standard permutation equivariant denoisers face fundamental limitations in these tasks due to their inability to break symmetries in noisy inputs. To address this, we propose aligning input and target graphs to break input symmetries while preserving permutation equivariance in non-matching graph portions. Using retrosynthesis (i.e., the task of predicting precursors for synthesis of a given target molecule) as our application domain, we show how alignment dramatically improves discrete diffusion model performance from 5% to a SOTA-matching 54.7% top-1 accuracy. Code is available at https://github.com/Aalto-QuML/DiffAlign.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Outlier-weighed Layerwise Sampling for LLM Fine-tuning</title>
<link>https://arxiv.org/abs/2405.18380</link>
<guid>https://arxiv.org/abs/2405.18380</guid>
<content:encoded><![CDATA[
arXiv:2405.18380v3 Announce Type: replace 
Abstract: The rapid advancements in Large Language Models (LLMs) have revolutionized various natural language processing tasks. However, the substantial size of LLMs presents significant challenges in training or fine-tuning. While parameter-efficient approaches such as low-rank adaptation (LoRA) have gained popularity, they often compromise performance compared to full-rank fine-tuning. In this paper, we propose Outlier-weighed Layerwise Sampling (OWS), a new memory-efficient fine-tuning approach, inspired by the layerwise outlier distribution of LLMs. Unlike LoRA, which adds extra adapters to all layers, OWS strategically assigns higher sampling probabilities to layers with more outliers, selectively sampling only a few layers and fine-tuning their pre-trained weights. To further increase the number of fine-tuned layers without a proportional rise in memory costs, we incorporate gradient low-rank projection, further boosting the approach's performance. Our extensive experiments across various architectures, including LLaMa2 and Mistral, demonstrate that OWS consistently outperforms baseline approaches, including full fine-tuning. Specifically, it achieves up to a 1.1% average accuracy gain on the Commonsense Reasoning benchmark, a 3.0% improvement on MMLU, and a notable 10% boost on MT-Bench, while being more memory efficient. OWS allows us to fine-tune 7B LLMs with only 21GB of memory. Our code is available at https://github.com/pixeli99/OWS.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GNNAnatomy: Rethinking Model-Level Explanations for Graph Neural Networks</title>
<link>https://arxiv.org/abs/2406.04548</link>
<guid>https://arxiv.org/abs/2406.04548</guid>
<content:encoded><![CDATA[
arXiv:2406.04548v3 Announce Type: replace 
Abstract: Graph Neural Networks (GNNs) achieve outstanding performance across graph-based tasks but remain difficult to interpret. In this paper, we revisit foundational assumptions underlying model-level explanation methods for GNNs, namely: (1) maximizing classification confidence yields representative explanations, (2) a single explanation suffices for an entire class of graphs, and (3) explanations are inherently trustworthy. We identify pitfalls resulting from these assumptions: methods that optimize for classification confidence may overlook partially learned patterns; topological diversity across graph subsets within the same class is often underrepresented; and explanations alone offer limited support for building user trust when applied to new datasets or models. This paper introduces GNNAnatomy, a distillation-based method designed to generate explanations while avoiding these pitfalls. GNNAnatomy first characterizes graph topology using graphlets, a set of fundamental substructures. We then train a transparent multilayer perceptron surrogate to directly approximate GNN predictions based on the graphlet representations. By analyzing the weights assigned to each graphlet, we identify the most discriminative topologies, which serve as GNN explanations. To account for structural diversity within a class, GNNAnatomy generates explanations at the required granularity through an interface that supports human-AI teaming. This interface helps users identify subsets of graphs where distinct critical substructures drive class differentiation, enabling multi-grained explanations. Additionally, by enabling exploration and linking explanations back to input graphs, the interface fosters greater transparency and trust. We evaluate GNNAnatomy on both synthetic and real-world datasets through quantitative metrics and qualitative comparisons with state-of-the-art model-level explainable GNN methods.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Shapley in One Training Run</title>
<link>https://arxiv.org/abs/2406.11011</link>
<guid>https://arxiv.org/abs/2406.11011</guid>
<content:encoded><![CDATA[
arXiv:2406.11011v3 Announce Type: replace 
Abstract: Data Shapley provides a principled framework for attributing data's contribution within machine learning contexts. However, existing approaches require re-training models on different data subsets, which is computationally intensive, foreclosing their application to large-scale models. Furthermore, they produce the same attribution score for any models produced by running the learning algorithm, meaning they cannot perform targeted attribution towards a specific model obtained from a single run of the algorithm. This paper introduces In-Run Data Shapley, which addresses these limitations by offering scalable data attribution for a target model of interest. In its most efficient implementation, our technique incurs negligible additional runtime compared to standard model training. This dramatic efficiency improvement makes it possible to perform data attribution for the foundation model pretraining stage for the first time. We present several case studies that offer fresh insights into pretraining data's contribution and discuss their implications for copyright in generative AI and pretraining data curation.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversaries With Incentives: A Strategic Alternative to Adversarial Robustness</title>
<link>https://arxiv.org/abs/2406.11458</link>
<guid>https://arxiv.org/abs/2406.11458</guid>
<content:encoded><![CDATA[
arXiv:2406.11458v3 Announce Type: replace 
Abstract: Adversarial training aims to defend against adversaries: malicious opponents whose sole aim is to harm predictive performance in any way possible. This presents a rather harsh perspective, which we assert results in unnecessarily conservative training. As an alternative, we propose to model opponents as simply pursuing their own goals--rather than working directly against the classifier. Employing tools from strategic modeling, our approach enables knowledge or beliefs regarding the opponent's possible incentives to be used as inductive bias for learning. Accordingly, our method of strategic training is designed to defend against all opponents within an 'incentive uncertainty set'. This resorts to adversarial learning when the set is maximal, but offers potential gains when the set can be appropriately reduced. We conduct a series of experiments that show how even mild knowledge regarding the opponent's incentives can be useful, and that the degree of potential gains depends on how these incentives relate to the structure of the learning task.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is poisoning a real threat to LLM alignment? Maybe more so than you think</title>
<link>https://arxiv.org/abs/2406.12091</link>
<guid>https://arxiv.org/abs/2406.12091</guid>
<content:encoded><![CDATA[
arXiv:2406.12091v4 Announce Type: replace 
Abstract: Recent advancements in Reinforcement Learning with Human Feedback (RLHF) have significantly impacted the alignment of Large Language Models (LLMs). The sensitivity of reinforcement learning algorithms such as Proximal Policy Optimization (PPO) has led to new line work on Direct Policy Optimization (DPO), which treats RLHF in a supervised learning framework. The increased practical use of these RLHF methods warrants an analysis of their vulnerabilities. In this work, we investigate the vulnerabilities of DPO to poisoning attacks under different scenarios and compare the effectiveness of preference poisoning, a first of its kind. We comprehensively analyze DPO's vulnerabilities under different types of attacks, i.e., backdoor and non-backdoor attacks, and different poisoning methods across a wide array of language models, i.e., LLama 7B, Mistral 7B, and Gemma 7B. We find that unlike PPO-based methods, which, when it comes to backdoor attacks, require at least 4\% of the data to be poisoned to elicit harmful behavior, we exploit the true vulnerabilities of DPO more simply so we can poison the model with only as much as 0.5\% of the data. We further investigate the potential reasons behind the vulnerability and how well this vulnerability translates into backdoor vs non-backdoor attacks.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Fits All: Learning Fair Graph Neural Networks for Various Sensitive Attributes</title>
<link>https://arxiv.org/abs/2406.13544</link>
<guid>https://arxiv.org/abs/2406.13544</guid>
<content:encoded><![CDATA[
arXiv:2406.13544v3 Announce Type: replace 
Abstract: Recent studies have highlighted fairness issues in Graph Neural Networks (GNNs), where they produce discriminatory predictions against specific protected groups categorized by sensitive attributes such as race and age. While various efforts to enhance GNN fairness have made significant progress, these approaches are often tailored to specific sensitive attributes. Consequently, they necessitate retraining the model from scratch to accommodate changes in the sensitive attribute requirement, resulting in high computational costs. To gain deeper insights into this issue, we approach the graph fairness problem from a causal modeling perspective, where we identify the confounding effect induced by the sensitive attribute as the underlying reason. Motivated by this observation, we formulate the fairness problem in graphs from an invariant learning perspective, which aims to learn invariant representations across environments. Accordingly, we propose a graph fairness framework based on invariant learning, namely FairINV, which enables the training of fair GNNs to accommodate various sensitive attributes within a single training session. Specifically, FairINV incorporates sensitive attribute partition and trains fair GNNs by eliminating spurious correlations between the label and various sensitive attributes. Experimental results on several real-world datasets demonstrate that FairINV significantly outperforms state-of-the-art fairness approaches, underscoring its effectiveness. Our code is available via: https://github.com/ZzoomD/FairINV/.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tree-Sliced Wasserstein Distance: A Geometric Perspective</title>
<link>https://arxiv.org/abs/2406.13725</link>
<guid>https://arxiv.org/abs/2406.13725</guid>
<content:encoded><![CDATA[
arXiv:2406.13725v3 Announce Type: replace 
Abstract: Many variants of Optimal Transport (OT) have been developed to address its heavy computation. Among them, notably, Sliced Wasserstein (SW) is widely used for application domains by projecting the OT problem onto one-dimensional lines, and leveraging the closed-form expression of the univariate OT to reduce the computational burden. However, projecting measures onto low-dimensional spaces can lead to a loss of topological information. To mitigate this issue, in this work, we propose to replace one-dimensional lines with a more intricate structure, called tree systems. This structure is metrizable by a tree metric, which yields a closed-form expression for OT problems on tree systems. We provide an extensive theoretical analysis to formally define tree systems with their topological properties, introduce the concept of splitting maps, which operate as the projection mechanism onto these structures, then finally propose a novel variant of Radon transform for tree systems and verify its injectivity. This framework leads to an efficient metric between measures, termed Tree-Sliced Wasserstein distance on Systems of Lines (TSW-SL). By conducting a variety of experiments on gradient flows, image style transfer, and generative models, we illustrate that our proposed approach performs favorably compared to SW and its variants.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Scheduling for Vehicle-to-Vehicle Communications Enhanced Federated Learning</title>
<link>https://arxiv.org/abs/2406.17470</link>
<guid>https://arxiv.org/abs/2406.17470</guid>
<content:encoded><![CDATA[
arXiv:2406.17470v2 Announce Type: replace 
Abstract: Leveraging the computing and sensing capabilities of vehicles, vehicular federated learning (VFL) has been applied to edge training for connected vehicles. The dynamic and interconnected nature of vehicular networks presents unique opportunities to harness direct vehicle-to-vehicle (V2V) communications, enhancing VFL training efficiency. In this paper, we formulate a stochastic optimization problem to optimize the VFL training performance, considering the energy constraints and mobility of vehicles, and propose a V2V-enhanced dynamic scheduling (VEDS) algorithm to solve it. The model aggregation requirements of VFL and the limited transmission time due to mobility result in a stepwise objective function, which presents challenges in solving the problem. We thus propose a derivative-based drift-plus-penalty method to convert the long-term stochastic optimization problem to an online mixed integer nonlinear programming (MINLP) problem, and provide a theoretical analysis to bound the performance gap between the online solution and the offline optimal solution. Further analysis of the scheduling priority reduces the original problem into a set of convex optimization problems, which are efficiently solved using the interior-point method. Experimental results demonstrate that compared with the state-of-the-art benchmarks, the proposed algorithm enhances the image classification accuracy on the CIFAR-10 dataset by 4.20% and reduces the average displacement errors on the Argoverse trajectory prediction dataset by 9.82%.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Numeric Rewards: In-Context Dueling Bandits with LLM Agents</title>
<link>https://arxiv.org/abs/2407.01887</link>
<guid>https://arxiv.org/abs/2407.01887</guid>
<content:encoded><![CDATA[
arXiv:2407.01887v4 Announce Type: replace 
Abstract: In-Context Reinforcement Learning (ICRL) is a frontier paradigm to solve Reinforcement Learning (RL) problems in the foundation model era. While ICRL capabilities have been demonstrated in transformers through task-specific training, the potential of Large Language Models (LLMs) out-of-the-box remains largely unexplored. This paper investigates whether LLMs can generalize cross-domain to perform ICRL under the problem of Dueling Bandits (DB), a stateless preference-based RL setting. We find that the top-performing LLMs exhibit a notable zero-shot capacity for relative decision-making, which translates to low short-term weak regret across all DB environment instances by quickly including the best arm in duels. However, an optimality gap still exists between LLMs and classic DB algorithms in terms of strong regret. LLMs struggle to converge and consistently exploit even when explicitly prompted to do so, and are sensitive to prompt variations. To bridge this gap, we propose an agentic flow framework: LLM with Enhanced Algorithmic Dueling (LEAD), which integrates off-the-shelf DB algorithm support with LLM agents through fine-grained adaptive interplay. We show that LEAD has theoretical guarantees inherited from classic DB algorithms on both weak and strong regret. We validate its efficacy and robustness even with noisy and adversarial prompts. The design of such an agentic framework sheds light on how to enhance the trustworthiness of general-purpose LLMs generalized to in-context decision-making tasks.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Low Rank Gradient Subspace Stabilization to Low-Rank Weights: Observations, Theories, and Applications</title>
<link>https://arxiv.org/abs/2407.11239</link>
<guid>https://arxiv.org/abs/2407.11239</guid>
<content:encoded><![CDATA[
arXiv:2407.11239v2 Announce Type: replace 
Abstract: Large Language Models' (LLMs) weight matrices can often be expressed in low-rank form with potential to relax memory and compute resource requirements. Unlike prior efforts that focus on developing novel matrix decompositions, in this work we study the non-uniform low-rank properties of weight matrices in LLMs through the lens of stabilizing gradient subspace. First, we provide a theoretical framework to understand the stabilization of gradient subspaces through Hessian analysis. Second, we empirically establish an important relationship between gradient dynamics and low-rank expressiveness of weight matrices. Our findings reveal that different LLM components exhibit varying levels of converged low-rank structures, necessitating variable rank reduction across them to minimize drop in performance due to compression. Drawing on this result, we present Weight Low-Rank Projection(WeLore) that unifies weight compression and memory-efficient fine-tuning into one, in a data-agnostic and one-shot manner. When used as a compression technique, WeLore categorizes weight matrices into Low-rank Components (LRCs) and Non-Low-rank Components (N-LRCs) and suitably encodes them for minimum performance loss. Our gradient dynamics perspective illustrates that LRCs tend to have better fine-tuning capabilities and their standalone fine-tuning can closely mimic and sometimes outperform the training loss trajectory and performance of full fine-tuning with notable memory and compute footprint reduction. Codes are available at https://github.com/VITA-Group/WeLore.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accounting for plasticity: An extension of inelastic Constitutive Artificial Neural Networks</title>
<link>https://arxiv.org/abs/2407.19326</link>
<guid>https://arxiv.org/abs/2407.19326</guid>
<content:encoded><![CDATA[
arXiv:2407.19326v2 Announce Type: replace 
Abstract: In this work, we extend the existing framework of inelastic constitutive artificial neural networks (iCANNs) by incorporating plasticity to increase their applicability to model more complex material behavior. The proposed approach ensures objectivity, material symmetry, and thermodynamic consistency, providing a robust basis for automatic model discovery of constitutive equations at finite strains. These are predicted by discovering formulations for the Helmholtz free energy and plastic potentials for the yield function and evolution equations in terms of feed-forward networks. Our framework captures both linear and nonlinear kinematic hardening behavior. Investigation of our model's prediction showed that the extended iCANNs successfully predict both linear and nonlinear kinematic hardening behavior based on experimental and artificially generated datasets, showcasing promising capabilities of this framework. Nonetheless, challenges remain in discovering more complex yield criteria with tension-compression asymmetry and addressing deviations in experimental data at larger strains. Despite these limitations, the proposed framework provides a promising basis for incorporating plasticity into iCANNs, offering a platform for advancing in the field of automated model discovery.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FDC: Fast KV Dimensionality Compression for Efficient LLM Inference</title>
<link>https://arxiv.org/abs/2408.04107</link>
<guid>https://arxiv.org/abs/2408.04107</guid>
<content:encoded><![CDATA[
arXiv:2408.04107v3 Announce Type: replace 
Abstract: In large-language models, memory constraints in the Key-Value Cache (KVC) pose a challenge during inference. In this work, we propose FDC, a fast KV dimensionality compression system that eliminates the decompression overhead incurred in the existing KV dimensionality compression system, Palu, and reduces attention time. Moreover, FDC employs adaptive compression, tailoring KV compression rates across heads and layers based on their contributions to inference to maximize overall compression while maintaining an accuracy loss constraint. Additionally, FDC enhances the attention kernel to balance the uneven workloads caused by the adaptive compression approach to further reduce attention computation latency. Comprehensive experiments demonstrate that compared to Palu, FDC can reduce Job Completion Time (JCT) by up to 64%, and delivers up to 1.97X throughput under the same latency, while maintaining 99% of the accuracy without compression. When state-of-the-art eviction and quantization methods are combined with FDC, they exhibit similar improvements compared to those combined with Palu. We open-sourced the code.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Closure Models: Learning Chaotic-Systems via Physics-Informed Neural Operators</title>
<link>https://arxiv.org/abs/2408.05177</link>
<guid>https://arxiv.org/abs/2408.05177</guid>
<content:encoded><![CDATA[
arXiv:2408.05177v4 Announce Type: replace 
Abstract: Accurately predicting the long-term behavior of chaotic systems is crucial for various applications such as climate modeling. However, achieving such predictions typically requires iterative computations over a dense spatiotemporal grid to account for the unstable nature of chaotic systems, which is expensive and impractical in many real-world situations. An alternative approach to such a full-resolved simulation is using a coarse grid and then correcting its errors through a \textit{closure model}, which approximates the overall information from fine scales not captured in the coarse-grid simulation. Recently, ML approaches have been used for closure modeling, but they typically require a large number of training samples from expensive fully-resolved simulations (FRS). In this work, we prove an even more fundamental limitation, i.e., the standard approach to learning closure models suffers from a large approximation error for generic problems, no matter how large the model is, and it stems from the non-uniqueness of the mapping. We propose an alternative end-to-end learning approach using a physics-informed neural operator (PINO) that overcomes this limitation by not using a closure model or a coarse-grid solver. We first train the PINO model on data from a coarse-grid solver and then fine-tune it with (a small amount of) FRS and physics-based losses on a fine grid. The discretization-free nature of neural operators means that they do not suffer from the restriction of a coarse grid that closure models face, and they can provably approximate the long-term statistics of chaotic systems. In our experiments, our PINO model achieves a 330x speedup compared to FRS with a relative error $\sim 10\%$. In contrast, the closure model coupled with a coarse-grid solver is $60$x slower than PINO while having a much higher error $\sim186\%$ when the closure model is trained on the same FRS dataset.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Selective Prompt Anchoring for Code Generation</title>
<link>https://arxiv.org/abs/2408.09121</link>
<guid>https://arxiv.org/abs/2408.09121</guid>
<content:encoded><![CDATA[
arXiv:2408.09121v5 Announce Type: replace 
Abstract: Recent advances in large language models (LLMs) have transformed software development by automatically generating code from natural language. Yet challenges remain in generating fully correct code that aligns with user intent. Our study reveals that LLMs tend to pay less attention to user prompts as more code tokens are generated. We hypothesize that this attention dilution issue is an important reason for code generation errors. To mitigate this issue, we propose Selective Prompt Anchoring (SPA) to guide code LLMs to pay more attention to user intent when generating code. We evaluate SPA using six base LLMs across six benchmarks. Our results demonstrate that SPA enhances Pass@1 by up to 12.9%, consistently outperforming SOTA code generation methods in all settings. Our code is available at https://github.com/magic-YuanTian/Selective-Prompt-Anchoring.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ferret: Federated Full-Parameter Tuning at Scale for Large Language Models</title>
<link>https://arxiv.org/abs/2409.06277</link>
<guid>https://arxiv.org/abs/2409.06277</guid>
<content:encoded><![CDATA[
arXiv:2409.06277v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have become indispensable in numerous real-world applications. However, fine-tuning these models at scale, especially in federated settings where data privacy and communication efficiency are critical, presents significant challenges. Existing approaches often resort to parameter-efficient fine-tuning (PEFT) to mitigate communication overhead, but this typically comes at the cost of model accuracy. To this end, we propose federated full-parameter tuning at scale for LLMs (Ferret), the first first-order method with shared randomness to enable scalable full-parameter tuning of LLMs across decentralized data sources while maintaining competitive model accuracy. Ferret accomplishes this through three aspects: (i) it employs widely used first-order methods for efficient local updates; (ii) it projects these updates into a low-dimensional space to considerably reduce communication overhead; and (iii) it reconstructs local updates from this low-dimensional space with shared randomness to facilitate effective full-parameter global aggregation, ensuring fast convergence and competitive final performance. Our rigorous theoretical analyses and insights along with extensive experiments, show that Ferret significantly enhances the scalability of existing federated full-parameter tuning approaches by achieving high computational efficiency, reduced communication overhead, and fast convergence, all while maintaining competitive model accuracy. Our implementation is available at https://github.com/allen4747/Ferret.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Policy Filtration for RLHF to Mitigate Noise in Reward Models</title>
<link>https://arxiv.org/abs/2409.06957</link>
<guid>https://arxiv.org/abs/2409.06957</guid>
<content:encoded><![CDATA[
arXiv:2409.06957v5 Announce Type: replace 
Abstract: While direct policy optimization methods exist, pioneering LLMs are fine-tuned with reinforcement learning from human feedback (RLHF) to generate better responses under the supervision of a reward model learned from preference data. One major challenge of RLHF is the inaccuracy of the intermediate reward model, especially in the tasks that requires complex reasoning for the reward model to score a response. We find that the reliability of the reward model varies across responses assigned with different rewards. This motivates us to filter the samples whose rewards may be unreliable to improve the signal-to-noise ratio during policy learning, resulting in Policy Filtration for Proximal Policy Optimization (PF-PPO). To choose a proper policy filtering strategy, we use the coefficient of determination (R2) between the rewards and actual scores on filtered samples as the metrics to help us find promising strategies since it measures how well the rewards filtered by PF-PPO indicate real performance. We provide extensive experiments to validate the effectiveness of PF-PPO in code generation and math reasoning tasks. In code generation, PF-PPO achieves the state-of-the-art performance of 7-billion-parameter models on HumanEval (+7.9%), MBPP (+0.7%), and LeetCode Contest (+10.0%) which is a more challenging benchmark created by us. In math reasoning, PF-PPO yields performance increase using different reward models and benchmarks (Ape210K and CMATH). Code is available on https://github.com/DtYXs/verl/tree/pf-ppo.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling Markov Heads in Pretrained Language Models for Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2409.06985</link>
<guid>https://arxiv.org/abs/2409.06985</guid>
<content:encoded><![CDATA[
arXiv:2409.06985v2 Announce Type: replace 
Abstract: Recently, incorporating knowledge from pretrained language models (PLMs) into decision transformers (DTs) has generated significant attention in offline reinforcement learning (RL). These PLMs perform well in RL tasks, raising an intriguing question: what kind of knowledge from PLMs has been transferred to RL to achieve such good results? This work first dives into this problem by analyzing each head quantitatively and points out Markov head, a crucial component that exists in the attention heads of PLMs. It leads to extreme attention on the last-input token and performs well only in short-term environments. Furthermore, we prove that this extreme attention cannot be changed by re-training embedding layer or fine-tuning. Inspired by our analysis, we propose a general method GPT2-DTMA, which equips a pretrained DT with Mixture of Attention (MoA), to accommodate diverse attention requirements during fine-tuning. Extensive experiments corroborate our theorems and demonstrate the effectiveness of GPT2-DTMA: it achieves comparable performance in short-term environments while significantly narrowing the performance gap in long-term environments.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hybrid Multi-Factor Network with Dynamic Sequence Modeling for Early Warning of Intraoperative Hypotension</title>
<link>https://arxiv.org/abs/2409.11064</link>
<guid>https://arxiv.org/abs/2409.11064</guid>
<content:encoded><![CDATA[
arXiv:2409.11064v4 Announce Type: replace 
Abstract: Intraoperative hypotension (IOH) prediction using past physiological signals is crucial, as IOH may lead to inadequate organ perfusion and significantly elevate the risk of severe complications and mortality. However, current methods often rely on static modeling, overlooking the complex temporal dependencies and the inherently non-stationary nature of physiological signals. We propose a Hybrid Multi-Factor (HMF) network that formulates IOH prediction as a dynamic sequence forecasting task, explicitly capturing both temporal dependencies and physiological non-stationarity. We represent signal dynamics as multivariate time series and decompose them into trend and seasonal components, enabling separate modeling of long-term and periodic variations. Each component is encoded with a patch-based Transformer to balance computational efficiency and feature representation. To address distributional drift from evolving signals, we introduce a symmetric normalization mechanism. Experiments on both public and real-world clinical datasets show that HMF significantly outperforms competitive baselines. We hope HMF offers new insights into IOH prediction and ultimately promotes safer surgical care. Our code is available at https://github.com/Mingyue-Cheng/HMF.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PropEnc: A Property Encoder for Graph Neural Networks</title>
<link>https://arxiv.org/abs/2409.11554</link>
<guid>https://arxiv.org/abs/2409.11554</guid>
<content:encoded><![CDATA[
arXiv:2409.11554v3 Announce Type: replace 
Abstract: Graph machine learning, particularly using graph neural networks, heavily relies on node features. However, many real-world systems, such as social and biological networks, lack node features due to privacy concerns, incomplete data, or collection limitations. Structural and positional encoding are commonly used to address this but are constrained by the maximum values of the encoded properties, such as the highest node degree. This limitation makes them impractical for scale-free networks and applications involving large or non-categorical properties. This paper introduces PropEnc, a novel and versatile encoder to generate expressive node embedding from any graph metric. By combining histogram construction with reversed index encoding, PropEnc offers a flexible solution that supports low-dimensional representations and diverse input types, effectively mitigating sparsity issues while improving computational efficiency. Additionally, it replicates one-hot encoding or approximates indices with high accuracy, making it adaptable to a wide range of graph applications. We validate PropEnc through extensive experiments on graph classification task across several social networks lacking node features. The empirical results demonstrate that PropEnc offers an efficient mechanism for constructing node features from various graph metrics.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Novel Neural Filter to Improve Accuracy of Neural Network Models of Dynamic Systems</title>
<link>https://arxiv.org/abs/2409.13654</link>
<guid>https://arxiv.org/abs/2409.13654</guid>
<content:encoded><![CDATA[
arXiv:2409.13654v2 Announce Type: replace 
Abstract: The application of neural networks in modeling dynamic systems has become prominent due to their ability to estimate complex nonlinear functions. Despite their effectiveness, neural networks face challenges in long-term predictions, where the prediction error diverges over time, thus degrading their accuracy. This paper presents a neural filter to enhance the accuracy of long-term state predictions of neural network-based models of dynamic systems. Motivated by the extended Kalman filter, the neural filter combines the neural network state predictions with the measurements from the physical system to improve the estimated state's accuracy. The neural filter's improvements in prediction accuracy are demonstrated through applications to four nonlinear dynamical systems. Numerical experiments show that the neural filter significantly improves prediction accuracy and bounds the state estimate covariance, outperforming the neural network predictions. Furthermore, it is also shown that the accuracy of a poorly trained neural network model can be improved to the same level as that of an adequately trained neural network model, potentially decreasing the training cost and required data to train a neural network.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Easy2Hard-Bench: Standardized Difficulty Labels for Profiling LLM Performance and Generalization</title>
<link>https://arxiv.org/abs/2409.18433</link>
<guid>https://arxiv.org/abs/2409.18433</guid>
<content:encoded><![CDATA[
arXiv:2409.18433v2 Announce Type: replace 
Abstract: While generalization over tasks from easy to hard is crucial to profile language models (LLMs), the datasets with fine-grained difficulty annotations for each problem across a broad range of complexity are still blank. Aiming to address this limitation, we present Easy2Hard-Bench, a consistently formatted collection of 6 benchmark datasets spanning various domains, such as mathematics and programming problems, chess puzzles, and reasoning questions. Each problem within these datasets is annotated with numerical difficulty scores. To systematically estimate problem difficulties, we collect abundant performance data on attempts to each problem by humans in the real world or LLMs on the prominent leaderboard. Leveraging the rich performance data, we apply well-established difficulty ranking systems, such as Item Response Theory (IRT) and Glicko-2 models, to uniformly assign numerical difficulty scores to problems. Moreover, datasets in Easy2Hard-Bench distinguish themselves from previous collections by a higher proportion of challenging problems. Through extensive experiments with six state-of-the-art LLMs, we provide a comprehensive analysis of their performance and generalization capabilities across varying levels of difficulty, with the aim of inspiring future research in LLM generalization. The datasets are available at https://huggingface.co/datasets/furonghuang-lab/Easy2Hard-Bench.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Generalization with Flat Hilbert Bayesian Inference</title>
<link>https://arxiv.org/abs/2410.04196</link>
<guid>https://arxiv.org/abs/2410.04196</guid>
<content:encoded><![CDATA[
arXiv:2410.04196v2 Announce Type: replace 
Abstract: We introduce Flat Hilbert Bayesian Inference (FHBI), an algorithm designed to enhance generalization in Bayesian inference. Our approach involves an iterative two-step procedure with an adversarial functional perturbation step and a functional descent step within a reproducing kernel Hilbert space. This methodology is supported by a theoretical analysis that extends previous findings on generalization ability from finite-dimensional Euclidean spaces to infinite-dimensional functional spaces. To evaluate the effectiveness of FHBI, we conduct comprehensive comparisons against nine baseline methods on the \texttt{VTAB-1K} benchmark, which encompasses 19 diverse datasets across various domains with diverse semantics. Empirical results demonstrate that FHBI consistently outperforms the baselines by notable margins, highlighting its practical efficacy.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EnsemW2S: Can an Ensemble of LLMs be Leveraged to Obtain a Stronger LLM?</title>
<link>https://arxiv.org/abs/2410.04571</link>
<guid>https://arxiv.org/abs/2410.04571</guid>
<content:encoded><![CDATA[
arXiv:2410.04571v2 Announce Type: replace 
Abstract: How can we harness the collective capabilities of multiple Large Language Models (LLMs) to create an even more powerful model? This question forms the foundation of our research, where we propose an innovative approach to weak-to-strong (w2s) generalization-a critical problem in AI alignment. Our work introduces an easy-to-hard (e2h) framework for studying the feasibility of w2s generalization, where weak models trained on simpler tasks collaboratively supervise stronger models on more complex tasks. This setup mirrors real-world challenges, where direct human supervision is limited. To achieve this, we develop a novel AdaBoost-inspired ensemble method, demonstrating that an ensemble of weak supervisors can enhance the performance of stronger LLMs across classification and generative tasks on difficult QA datasets. In several cases, our ensemble approach matches the performance of models trained on ground-truth data, establishing a new benchmark for w2s generalization. We observe an improvement of up to 14% over existing baselines and average improvements of 5% and 4% for binary classification and generative tasks, respectively. This research points to a promising direction for enhancing AI through collective supervision, especially in scenarios where labeled data is sparse or insufficient.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collapse-Proof Non-Contrastive Self-Supervised Learning</title>
<link>https://arxiv.org/abs/2410.04959</link>
<guid>https://arxiv.org/abs/2410.04959</guid>
<content:encoded><![CDATA[
arXiv:2410.04959v3 Announce Type: replace 
Abstract: We present a principled and simplified design of the projector and loss function for non-contrastive self-supervised learning based on hyperdimensional computing. We theoretically demonstrate that this design introduces an inductive bias that encourages representations to be simultaneously decorrelated and clustered, without explicitly enforcing these properties. This bias provably enhances generalization and suffices to avoid known training failure modes, such as representation, dimensional, cluster, and intracluster collapses. We validate our theoretical findings on image datasets, including SVHN, CIFAR-10, CIFAR-100, and ImageNet-100. Our approach effectively combines the strengths of feature decorrelation and cluster-based self-supervised learning methods, overcoming training failure modes while achieving strong generalization in clustering and linear classification tasks.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Sample Complexity for Private Nonsmooth Nonconvex Optimization</title>
<link>https://arxiv.org/abs/2410.05880</link>
<guid>https://arxiv.org/abs/2410.05880</guid>
<content:encoded><![CDATA[
arXiv:2410.05880v2 Announce Type: replace 
Abstract: We study differentially private (DP) optimization algorithms for stochastic and empirical objectives which are neither smooth nor convex, and propose methods that return a Goldstein-stationary point with sample complexity bounds that improve on existing works. We start by providing a single-pass $(\epsilon,\delta)$-DP algorithm that returns an $(\alpha,\beta)$-stationary point as long as the dataset is of size $\widetilde{\Omega}(\sqrt{d}/\alpha\beta^{3}+d/\epsilon\alpha\beta^{2})$, which is $\Omega(\sqrt{d})$ times smaller than the algorithm of Zhang et al. [2024] for this task, where $d$ is the dimension. We then provide a multi-pass polynomial time algorithm which further improves the sample complexity to $\widetilde{\Omega}\left(d/\beta^2+d^{3/4}/\epsilon\alpha^{1/2}\beta^{3/2}\right)$, by designing a sample efficient ERM algorithm, and proving that Goldstein-stationary points generalize from the empirical loss to the population loss.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Q-WSL: Optimizing Goal-Conditioned RL with Weighted Supervised Learning via Dynamic Programming</title>
<link>https://arxiv.org/abs/2410.06648</link>
<guid>https://arxiv.org/abs/2410.06648</guid>
<content:encoded><![CDATA[
arXiv:2410.06648v5 Announce Type: replace 
Abstract: A novel class of advanced algorithms, termed Goal-Conditioned Weighted Supervised Learning (GCWSL), has recently emerged to tackle the challenges posed by sparse rewards in goal-conditioned reinforcement learning (RL). GCWSL consistently delivers strong performance across a diverse set of goal-reaching tasks due to its simplicity, effectiveness, and stability. However, GCWSL methods lack a crucial capability known as trajectory stitching, which is essential for learning optimal policies when faced with unseen skills during testing. This limitation becomes particularly pronounced when the replay buffer is predominantly filled with sub-optimal trajectories. In contrast, traditional TD-based RL methods, such as Q-learning, which utilize Dynamic Programming, do not face this issue but often experience instability due to the inherent difficulties in value function approximation. In this paper, we propose Q-learning Weighted Supervised Learning (Q-WSL), a novel framework designed to overcome the limitations of GCWSL by incorporating the strengths of Dynamic Programming found in Q-learning. Q-WSL leverages Dynamic Programming results to output the optimal action of (state, goal) pairs across different trajectories within the replay buffer. This approach synergizes the strengths of both Q-learning and GCWSL, effectively mitigating their respective weaknesses and enhancing overall performance. Empirical evaluations on challenging goal-reaching tasks demonstrate that Q-WSL surpasses other goal-conditioned approaches in terms of both performance and sample efficiency. Additionally, Q-WSL exhibits notable robustness in environments characterized by binary reward structures and environmental stochasticity.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Low-Dimension-to-High-Dimension Generalization And Its Implications for Length Generalization</title>
<link>https://arxiv.org/abs/2410.08898</link>
<guid>https://arxiv.org/abs/2410.08898</guid>
<content:encoded><![CDATA[
arXiv:2410.08898v2 Announce Type: replace 
Abstract: Low-Dimension-to-High-Dimension (LDHD) generalization is a special case of Out-of-Distribution (OOD) generalization, where the training data are restricted to a low-dimensional subspace of the high-dimensional testing space. Assuming that each instance is generated from a latent variable and the dimension of the latent variable reflects the problem scale, the inherent scaling challenge in length generalization can be captured by the LDHD generalization in the latent space. We theoretically demonstrate that LDHD generalization is generally unattainable without exploiting prior knowledge to provide appropriate inductive bias. Specifically, we explore LDHD generalization in Boolean functions. We verify that different architectures trained with (S)GD converge to \emph{min-degree interpolators w.r.t. different independent sets}. LDHD generalization is achievable if and only if the target function coincides with this inductive bias. Applying the insights from LDHD generalization to length generalization, we explain the effectiveness of CoT as changing the structure latent space to enable better LDHD generalization. We also propose a principle for position embedding design to handle both the inherent LDHD generalization and the nuisances such as the data format. Following the principle, we propose a novel position embedding called RPE-Square that remedies the RPE for dealing with the data format nuisance.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parameter-Efficient Fine-Tuning of State Space Models</title>
<link>https://arxiv.org/abs/2410.09016</link>
<guid>https://arxiv.org/abs/2410.09016</guid>
<content:encoded><![CDATA[
arXiv:2410.09016v3 Announce Type: replace 
Abstract: Deep State Space Models (SSMs), such as Mamba (Gu & Dao, 2024), have become powerful tools for language modeling, offering high performance and linear scalability with sequence length. However, the application of parameter-efficient fine-tuning (PEFT) methods to SSM-based models remains largely underexplored. We start by investigating two fundamental questions on existing PEFT methods: (i) How do they perform on SSM-based models? (ii) Which parameters should they target for optimal results? Our analysis shows that LoRA and its variants consistently outperform all other PEFT methods. While LoRA is effective for linear projection matrices, it fails on SSM modules-yet still outperforms other methods applicable to SSMs, indicating their limitations. This underscores the need for a specialized SSM tuning approach. To address this, we propose Sparse Dimension Tuning (SDT), a PEFT method tailored for SSM modules. Combining SDT for SSMs with LoRA for linear projection matrices, we achieve state-of-the-art performance across extensive experiments.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Online Learning Approach to Prompt-based Selection of Generative Models and LLMs</title>
<link>https://arxiv.org/abs/2410.13287</link>
<guid>https://arxiv.org/abs/2410.13287</guid>
<content:encoded><![CDATA[
arXiv:2410.13287v3 Announce Type: replace 
Abstract: Selecting a sample generation scheme from multiple prompt-based generative models, including large language models (LLMs) and prompt-guided image and video generation models, is typically addressed by choosing the model that maximizes an averaged evaluation score. However, this score-based selection overlooks the possibility that different models achieve the best generation performance for different types of text prompts. An online identification of the best generation model for various input prompts can reduce the costs associated with querying sub-optimal models. In this work, we explore the possibility of varying rankings of text-based generative models for different text prompts and propose an online learning framework to predict the best data generation model for a given input prompt. The proposed PAK-UCB algorithm addresses a contextual bandit (CB) setting with shared context variables across the arms, utilizing the generated data to update kernel-based functions that predict the score of each model available for unseen text prompts. Additionally, we leverage random Fourier features (RFF) to accelerate the online learning process of PAK-UCB. Our numerical experiments on real and simulated text-to-image and image-to-text generative models show that RFF-UCB performs successfully in identifying the best generation model across different sample types. The code is available at: github.com/yannxiaoyanhu/dgm-online-select.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Arbitrarily-Conditioned Multi-Functional Diffusion for Multi-Physics Emulation</title>
<link>https://arxiv.org/abs/2410.13794</link>
<guid>https://arxiv.org/abs/2410.13794</guid>
<content:encoded><![CDATA[
arXiv:2410.13794v2 Announce Type: replace 
Abstract: Modern physics simulation often involves multiple functions of interests, and traditional numerical approaches are known to be complex and computationally costly. While machine learning-based surrogate models can offer significant cost reductions, most focus on a single task, such as forward prediction, and typically lack uncertainty quantification -- an essential component in many applications. To overcome these limitations, we propose Arbitrarily-Conditioned Multi-Functional Diffusion (ACM-FD), a versatile probabilistic surrogate model for multi-physics emulation. ACM-FD can perform a wide range of tasks within a single framework, including forward prediction, various inverse problems, and simulating data for entire systems or subsets of quantities conditioned on others. Specifically, we extend the standard Denoising Diffusion Probabilistic Model (DDPM) for multi-functional generation by modeling noise as Gaussian processes (GP). We propose a random-mask based, zero-regularized denoising loss to achieve flexible and robust conditional generation. We induce a Kronecker product structure in the GP covariance matrix, substantially reducing the computational cost and enabling efficient training and sampling. We demonstrate the effectiveness of ACM-FD across several fundamental multi-physics systems.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixed-curvature decision trees and random forests</title>
<link>https://arxiv.org/abs/2410.13879</link>
<guid>https://arxiv.org/abs/2410.13879</guid>
<content:encoded><![CDATA[
arXiv:2410.13879v3 Announce Type: replace 
Abstract: Decision trees (DTs) and their random forest (RF) extensions are workhorses of classification and regression in Euclidean spaces. However, algorithms for learning in non-Euclidean spaces are still limited. We extend DT and RF algorithms to product manifolds: Cartesian products of several hyperbolic, hyperspherical, or Euclidean components. Such manifolds handle heterogeneous curvature while still factorizing neatly into simpler components, making them compelling embedding spaces for complex datasets. Our novel angular reformulation respects manifold geometry while preserving the algorithmic properties that make decision trees effective. In the special cases of single-component manifolds, our method simplifies to its Euclidean or hyperbolic counterparts, or introduces hyperspherical DT algorithms, depending on the curvature. In benchmarks on a diverse suite of 57 classification, regression, and link prediction tasks, our product RFs ranked first on 29 tasks and came in the top 2 for 41. This highlights the value of product RFs as straightforward yet powerful new tools for data analysis in product manifolds. Code for our method is available at https://github.com/pchlenski/manify.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-shot Generalist Graph Anomaly Detection with Unified Neighborhood Prompts</title>
<link>https://arxiv.org/abs/2410.14886</link>
<guid>https://arxiv.org/abs/2410.14886</guid>
<content:encoded><![CDATA[
arXiv:2410.14886v2 Announce Type: replace 
Abstract: Graph anomaly detection (GAD), which aims to identify nodes in a graph that significantly deviate from normal patterns, plays a crucial role in broad application domains. However, existing GAD methods are one-model-for-one-dataset approaches, i.e., training a separate model for each graph dataset. This largely limits their applicability in real-world scenarios. To overcome this limitation, we propose a novel zero-shot generalist GAD approach UNPrompt that trains a one-for-all detection model, requiring the training of one GAD model on a single graph dataset and then effectively generalizing to detect anomalies in other graph datasets without any retraining or fine-tuning. The key insight in UNPrompt is that i) the predictability of latent node attributes can serve as a generalized anomaly measure and ii) generalized normal and abnormal graph patterns can be learned via latent node attribute prediction in a properly normalized node attribute space. UNPrompt achieves a generalist mode for GAD through two main modules: one module aligns the dimensionality and semantics of node attributes across different graphs via coordinate-wise normalization, while another module learns generalized neighborhood prompts that support the use of latent node attribute predictability as an anomaly score across different datasets. Extensive experiments on real-world GAD datasets show that UNPrompt significantly outperforms diverse competing methods under the generalist GAD setting, and it also has strong superiority under the one-model-for-one-dataset setting. Code is available at https://github.com/mala-lab/UNPrompt.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tilted Sharpness-Aware Minimization</title>
<link>https://arxiv.org/abs/2410.22656</link>
<guid>https://arxiv.org/abs/2410.22656</guid>
<content:encoded><![CDATA[
arXiv:2410.22656v2 Announce Type: replace 
Abstract: Sharpness-Aware Minimization (SAM) has been demonstrated to improve the generalization performance of overparameterized models by seeking flat minima on the loss landscape through optimizing model parameters that incur the largest loss within a neighborhood. Nevertheless, such min-max formulations are computationally challenging especially when the problem is highly non-convex. Additionally, focusing only on the worst-case local solution while ignoring potentially many other local solutions may be suboptimal when searching for flat minima. In this work, we propose Tilted SAM (TSAM), a smoothed generalization of SAM inspired by exponential tilting that effectively assigns higher priority to local solutions that incur larger losses. TSAM is parameterized by a tilt hyperparameter $t$ and reduces to SAM as $t$ approaches infinity. We show that TSAM is smoother than SAM and thus easier to optimize, and it explicitly favors flatter minima. We develop algorithms motivated by the discretization of Hamiltonian dynamics to solve TSAM. Empirically, TSAM arrives at flatter local minima and results in superior test performance than the baselines of SAM and ERM across a range of image and text tasks.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Theoretical Characterization of Optimal Data Augmentations in Self-Supervised Learning</title>
<link>https://arxiv.org/abs/2411.01767</link>
<guid>https://arxiv.org/abs/2411.01767</guid>
<content:encoded><![CDATA[
arXiv:2411.01767v4 Announce Type: replace 
Abstract: Data augmentations play an important role in the recent success of self-supervised learning (SSL). While augmentations are commonly understood to encode invariances between different views into the learned representations, this interpretation overlooks the impact of the pretraining architecture and suggests that SSL would require diverse augmentations which resemble the data to work well. However, these assumptions do not align with empirical evidence, encouraging further theoretical understanding to guide the principled design of augmentations in new domains. To this end, we use kernel theory to derive analytical expressions for data augmentations that achieve desired target representations after pretraining. We consider non-contrastive and contrastive losses, namely VICReg, Barlow Twins and the Spectral Contrastive Loss, and provide an algorithm to construct such augmentations. Our analysis shows that augmentations need not be similar to the data to learn useful representations, nor be diverse, and that the architecture has a significant impact on the optimal augmentations.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Does DPO Reduce Toxicity? A Mechanistic Neuron-Level Analysis</title>
<link>https://arxiv.org/abs/2411.06424</link>
<guid>https://arxiv.org/abs/2411.06424</guid>
<content:encoded><![CDATA[
arXiv:2411.06424v3 Announce Type: replace 
Abstract: Safety fine-tuning algorithms reduce harmful outputs in language models, yet their mechanisms remain under-explored. Direct Preference Optimization (DPO) is a popular choice of algorithm, but prior explanations, attributing its effects solely to dampened toxic neurons in the MLP layers, are incomplete. In this study, we analyse four language models (Llama-3.1-8B, Gemma-2-2B, Mistral-7B, GPT-2-Medium) and show that toxic neurons only account for 2.5% to 24% of DPO's effects across models. Instead, DPO balances distributed activation shifts across all MLP neurons to create a net toxicity reduction. We attribute this reduction to four neuron groups, two aligned with reducing toxicity and two promoting anti-toxicity, whose combined effects replicate DPO across models. To further validate this understanding, we develop an activation editing method mimicking DPO through distributed shifts along a toxicity representation. This method outperforms DPO in reducing toxicity while preserving perplexity, without requiring any weight updates. Our work provides a mechanistic understanding of DPO and introduces an efficient, tuning-free alternative for safety fine-tuning.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling and Addressing Pseudo Forgetting in Large Language Models</title>
<link>https://arxiv.org/abs/2411.11932</link>
<guid>https://arxiv.org/abs/2411.11932</guid>
<content:encoded><![CDATA[
arXiv:2411.11932v2 Announce Type: replace 
Abstract: Although substantial efforts have been made to mitigate catastrophic forgetting in continual learning, the intrinsic mechanisms are not well understood. In this work, we demonstrate the existence of "pseudo forgetting": the performance degradation on previous tasks is not attributed to a loss of capabilities, but rather to the failure of the instructions to activate the appropriate model abilities. We show that the model's performance on previous tasks can be restored through two simple interventions: (1) providing partial external correct rationale, and (2) appending semantically meaningless suffixes to the original instructions, to guide the generation of correct rationales. Through empirical analysis of the internal mechanisms governing rationale generation, we reveal that models exhibiting pseudo forgetting show reduced instruction dependence during rationale generation, leading to suboptimal activation of their inherent capabilities. Based on this insight, we propose Rationale-Guidance Difficulty based Replay (RGD-R) framework that dynamically allocates replay data based on the model's ability to correctly leverage the intrinsic capabilities. Experimental results demonstrate that RGD-R effectively mitigates pseudo forgetting while maintaining model plasticity.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Integration of Longitudinal Noninvasive Diagnostics for Survival Prediction in Immunotherapy Using Deep Learning</title>
<link>https://arxiv.org/abs/2411.18253</link>
<guid>https://arxiv.org/abs/2411.18253</guid>
<content:encoded><![CDATA[
arXiv:2411.18253v2 Announce Type: replace 
Abstract: Purpose: Immunotherapies have revolutionized the landscape of cancer treatments. However, our understanding of response patterns in advanced cancers treated with immunotherapy remains limited. By leveraging routinely collected noninvasive longitudinal and multimodal data with artificial intelligence, we could unlock the potential to transform immunotherapy for cancer patients, paving the way for personalized treatment approaches. Methods: In this study, we developed a novel artificial neural network architecture, multimodal transformer-based simple temporal attention (MMTSimTA) network, building upon a combination of recent successful developments. We integrated pre- and on-treatment blood measurements, prescribed medications and CT-based volumes of organs from a large pan-cancer cohort of 694 patients treated with immunotherapy to predict mortality at three, six, nine and twelve months. Different variants of our extended MMTSimTA network were implemented and compared to baseline methods incorporating intermediate and late fusion based integration methods. Results: The strongest prognostic performance was demonstrated using a variant of the MMTSimTA model with area under the curves (AUCs) of $0.84 \pm $0.04, $0.83 \pm $0.02, $0.82 \pm $0.02, $0.81 \pm $0.03 for 3-, 6-, 9-, and 12-month survival prediction, respectively. Discussion: Our findings show that integrating noninvasive longitudinal data using our novel architecture yields an improved multimodal prognostic performance, especially in short-term survival prediction. Conclusion: Our study demonstrates that multimodal longitudinal integration of noninvasive data using deep learning may offer a promising approach for personalized prognostication in immunotherapy-treated cancer patients.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DualCast: A Model to Disentangle Aperiodic Events from Traffic Series</title>
<link>https://arxiv.org/abs/2411.18286</link>
<guid>https://arxiv.org/abs/2411.18286</guid>
<content:encoded><![CDATA[
arXiv:2411.18286v2 Announce Type: replace 
Abstract: Traffic forecasting is crucial for transportation systems optimisation. Current models minimise the mean forecasting errors, often favouring periodic events prevalent in the training data, while overlooking critical aperiodic ones like traffic incidents. To address this, we propose DualCast, a dual-branch framework that disentangles traffic signals into intrinsic spatial-temporal patterns and external environmental contexts, including aperiodic events. DualCast also employs a cross-time attention mechanism to capture high-order spatial-temporal relationships from both periodic and aperiodic patterns. DualCast is versatile. We integrate it with recent traffic forecasting models, consistently reducing their forecasting errors by up to 9.6% on multiple real datasets. Our source code is available at https://github.com/suzy0223/DualCast.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed Deep Learning Model for Line-integral Diagnostics Across Fusion Devices</title>
<link>https://arxiv.org/abs/2412.00087</link>
<guid>https://arxiv.org/abs/2412.00087</guid>
<content:encoded><![CDATA[
arXiv:2412.00087v3 Announce Type: replace 
Abstract: Rapid reconstruction of 2D plasma profiles from line-integral measurements is important in nuclear fusion. This paper introduces a physics-informed model architecture called Onion, that can enhance the performance of models and be adapted to various backbone networks. The model under Onion incorporates physical information by a multiplication process and applies the physics-informed loss function according to the principle of line integration. Prediction results demonstrate that the additional input of physical information improves the deep learning model's ability, leading to a reduction in the average relative error E_1 between the reconstruction profiles and the target profiles by approximately 0.84x10^(-2) on synthetic datasets and about 0.06x10^(-2) on experimental datasets. Furthermore, the implementation of the Softplus activation function in the final two fully connected layers improves model performance. This enhancement results in a reduction in the E_1 by approximately 1.06x10^(-2) on synthetic datasets and about 0.11x10^(-2) on experimental datasets. The incorporation of the physics-informed loss function has been shown to correct the model's predictions, bringing the back-projections closer to the actual inputs and reducing the errors associated with inversion algorithms. Besides, we have developed a synthetic data model to generate customized line-integral diagnostic datasets and have also collected soft x-ray diagnostic datasets from EAST and HL-2A. This study achieves reductions in reconstruction errors, and accelerates the development of surrogate models in fusion research.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Defending Against Diverse Attacks in Federated Learning Through Consensus-Based Bi-Level Optimization</title>
<link>https://arxiv.org/abs/2412.02535</link>
<guid>https://arxiv.org/abs/2412.02535</guid>
<content:encoded><![CDATA[
arXiv:2412.02535v2 Announce Type: replace 
Abstract: Adversarial attacks pose significant challenges in many machine learning applications, particularly in the setting of distributed training and federated learning, where malicious agents seek to corrupt the training process with the goal of jeopardizing and compromising the performance and reliability of the final models. In this paper, we address the problem of robust federated learning in the presence of such attacks by formulating the training task as a bi-level optimization problem. We conduct a theoretical analysis of the resilience of consensus-based bi-level optimization (CB$^2$O), an interacting multi-particle metaheuristic optimization method, in adversarial settings. Specifically, we provide a global convergence analysis of CB$^2$O in mean-field law in the presence of malicious agents, demonstrating the robustness of CB$^2$O against a diverse range of attacks. Thereby, we offer insights into how specific hyperparameter choices enable to mitigate adversarial effects. On the practical side, we extend CB$^2$O to the clustered federated learning setting by proposing FedCB$^2$O, a novel interacting multi-particle system, and design a practical algorithm that addresses the demands of real-world applications. Extensive experiments demonstrate the robustness of the FedCB$^2$O algorithm against label-flipping attacks in decentralized clustered federated learning scenarios, showcasing its effectiveness in practical contexts.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMI-Editor: Edit-based SMILES Language Model with Fragment-level Supervision</title>
<link>https://arxiv.org/abs/2412.05569</link>
<guid>https://arxiv.org/abs/2412.05569</guid>
<content:encoded><![CDATA[
arXiv:2412.05569v2 Announce Type: replace 
Abstract: SMILES, a crucial textual representation of molecular structures, has garnered significant attention as a foundation for pre-trained language models (LMs). However, most existing pre-trained SMILES LMs focus solely on the single-token level supervision during pre-training, failing to fully leverage the substructural information of molecules. This limitation makes the pre-training task overly simplistic, preventing the models from capturing richer molecular semantic information. Moreover, during pre-training, these SMILES LMs only process corrupted SMILES inputs, never encountering any valid SMILES, which leads to a train-inference mismatch. To address these challenges, we propose SMI-Editor, a novel edit-based pre-trained SMILES LM. SMI-Editor disrupts substructures within a molecule at random and feeds the resulting SMILES back into the model, which then attempts to restore the original SMILES through an editing process. This approach not only introduces fragment-level training signals, but also enables the use of valid SMILES as inputs, allowing the model to learn how to reconstruct complete molecules from these incomplete structures. As a result, the model demonstrates improved scalability and an enhanced ability to capture fragment-level molecular information. Experimental results show that SMI-Editor achieves state-of-the-art performance across multiple downstream molecular tasks, and even outperforming several 3D molecular representation models.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing radioisotope identification in gamma spectra via supervised domain adaptation</title>
<link>https://arxiv.org/abs/2412.07069</link>
<guid>https://arxiv.org/abs/2412.07069</guid>
<content:encoded><![CDATA[
arXiv:2412.07069v2 Announce Type: replace 
Abstract: Machine learning methods in gamma spectroscopy have the potential to provide accurate, real-time classification of unknown radioactive samples. However, obtaining sufficient experimental training data is often prohibitively expensive and time-consuming, and models trained solely on simulated data can struggle to generalize to the unpredictable range of real-world operating scenarios. In this study, we explore how supervised domain adaptation techniques can improve radioisotope identification models by transferring knowledge between different data domains. We begin by pretraining a model for radioisotope identification using data from a synthetic source domain, and then fine-tune it for a new target domain that shares the same label space. Our analysis indicates that fine-tuned models significantly outperform those trained exclusively on source-domain data or solely on target-domain data, particularly in the intermediate data regime ($\approx 10^2$ to $10^5$ target training samples). This conclusion is consistent across four different machine learning architectures (MLP, CNN, Transformer, and LSTM). Furthermore, our findings show that fine-tuned Transformers yield a statistically significant improvement in test performance compared to the other architectures. Overall, this study serves as a proof of concept for applying supervised domain adaptation techniques to gamma spectroscopy in scenarios where experimental data is limited.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Label Distribution Learning using the Squared Neural Family on the Probability Simplex</title>
<link>https://arxiv.org/abs/2412.07324</link>
<guid>https://arxiv.org/abs/2412.07324</guid>
<content:encoded><![CDATA[
arXiv:2412.07324v2 Announce Type: replace 
Abstract: Label distribution learning (LDL) provides a framework wherein a distribution over categories rather than a single category is predicted, with the aim of addressing ambiguity in labeled data. Existing research on LDL mainly focuses on the task of point estimation, i.e., finding an optimal distribution in the probability simplex conditioned on the given sample. In this paper, we propose a novel label distribution learning model SNEFY-LDL, which estimates a probability distribution of all possible label distributions over the simplex, by unleashing the expressive power of the recently introduced Squared Neural Family (SNEFY), a new class of tractable probability models. As a way to summarize the fitted model, we derive the closed-form label distribution mean, variance and covariance conditioned on the given sample, which can be used to predict the ground-truth label distributions, construct label distribution confidence intervals, and measure the correlations between different labels. Moreover, more information about the label distribution prediction uncertainties can be acquired from the modeled probability density function. Extensive experiments on conformal prediction, active learning and ensemble learning are conducted, verifying SNEFY-LDL's great effectiveness in LDL uncertainty quantification. The source code of this paper is available at https://github.com/daokunzhang/SNEFY-LDL.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AHSG: Adversarial Attack on High-level Semantics in Graph Neural Networks</title>
<link>https://arxiv.org/abs/2412.07468</link>
<guid>https://arxiv.org/abs/2412.07468</guid>
<content:encoded><![CDATA[
arXiv:2412.07468v3 Announce Type: replace 
Abstract: Adversarial attacks on Graph Neural Networks aim to perturb the performance of the learner by carefully modifying the graph topology and node attributes. Existing methods achieve attack stealthiness by constraining the modification budget and differences in graph properties. However, these methods typically disrupt task-relevant primary semantics directly, which results in low defensibility and detectability of the attack. In this paper, we propose an Adversarial Attack on High-level Semantics for Graph Neural Networks (AHSG), which is a graph structure attack model that ensures the retention of primary semantics. By combining latent representations with shared primary semantics, our model retains detectable attributes and relational patterns of the original graph while leveraging more subtle changes to carry out the attack. Then we use the Projected Gradient Descent algorithm to map the latent representations with attack effects to the adversarial graph. Through experiments on robust graph deep learning models equipped with defense strategies, we demonstrate that AHSG outperforms other state-of-the-art methods in attack effectiveness. Additionally, using Contextual Stochastic Block Models to detect the attacked graph further validates that our method preserves the primary semantics of the graph.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Causal Discovery by Approximate Kernel-based Generalized Score Functions with Linear Computational Complexity</title>
<link>https://arxiv.org/abs/2412.17717</link>
<guid>https://arxiv.org/abs/2412.17717</guid>
<content:encoded><![CDATA[
arXiv:2412.17717v2 Announce Type: replace 
Abstract: Score-based causal discovery methods can effectively identify causal relationships by evaluating candidate graphs and selecting the one with the highest score. One popular class of scores is kernel-based generalized score functions, which can adapt to a wide range of scenarios and work well in practice because they circumvent assumptions about causal mechanisms and data distributions. Despite these advantages, kernel-based generalized score functions pose serious computational challenges in time and space, with a time complexity of $\mathcal{O}(n^3)$ and a memory complexity of $\mathcal{O}(n^2)$, where $n$ is the sample size. In this paper, we propose an approximate kernel-based generalized score function with $\mathcal{O}(n)$ time and space complexities by using low-rank technique and designing a set of rules to handle the complex composite matrix operations required to calculate the score, as well as developing sampling algorithms for different data types to benefit the handling of diverse data types efficiently. Our extensive causal discovery experiments on both synthetic and real-world data demonstrate that compared to the state-of-the-art method, our method can not only significantly reduce computational costs, but also achieve comparable accuracy, especially for large datasets.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stable Derivative Free Gaussian Mixture Variational Inference for Bayesian Inverse Problems</title>
<link>https://arxiv.org/abs/2501.04259</link>
<guid>https://arxiv.org/abs/2501.04259</guid>
<content:encoded><![CDATA[
arXiv:2501.04259v3 Announce Type: replace 
Abstract: This paper is concerned with the approximation of probability distributions known up to normalization constants, with a focus on Bayesian inference for large-scale inverse problems in scientific computing. In this context, key challenges include costly repeated evaluations of forward models, multimodality, and inaccessible gradients for the forward model. To address them, we develop a variational inference framework that combines Fisher-Rao natural gradient with specialized quadrature rules to enable derivative free updates of Gaussian mixture variational families. The resulting method, termed Derivative Free Gaussian Mixture Variational Inference (DF-GMVI), guarantees covariance positivity and affine invariance, offering a stable and efficient framework for approximating complex posterior distributions. The effectiveness of DF-GMVI is demonstrated through numerical experiments on challenging scenarios, including distributions with multiple modes, infinitely many modes, and curved modes in spaces with up to 100 dimensions. The method's practicality is further demonstrated in a large-scale application, where it successfully recovers the initial conditions of the Navier-Stokes equations from solution data at positive times.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-driven inventory management for new products: An adjusted Dyna-$Q$ approach with transfer learning</title>
<link>https://arxiv.org/abs/2501.08109</link>
<guid>https://arxiv.org/abs/2501.08109</guid>
<content:encoded><![CDATA[
arXiv:2501.08109v4 Announce Type: replace 
Abstract: In this paper, we propose a novel reinforcement learning algorithm for inventory management of newly launched products with no historical demand information. The algorithm follows the classic Dyna-$Q$ structure, balancing the model-free and model-based approaches, while accelerating the training process of Dyna-$Q$ and mitigating the model discrepancy generated by the model-based feedback. Based on the idea of transfer learning, warm-start information from the demand data of existing similar products can be incorporated into the algorithm to further stabilize the early-stage training and reduce the variance of the estimated optimal policy. Our approach is validated through a case study of bakery inventory management with real data. The adjusted Dyna-$Q$ shows up to a 23.7\% reduction in average daily cost compared with $Q$-learning, and up to a 77.5\% reduction in training time within the same horizon compared with classic Dyna-$Q$. By using transfer learning, it can be found that the adjusted Dyna-$Q$ has the lowest total cost, lowest variance in total cost, and relatively low shortage percentages among all the benchmarking algorithms under a 30-day testing.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rational Tuning of LLM Cascades via Probabilistic Modeling</title>
<link>https://arxiv.org/abs/2501.09345</link>
<guid>https://arxiv.org/abs/2501.09345</guid>
<content:encoded><![CDATA[
arXiv:2501.09345v4 Announce Type: replace 
Abstract: Understanding the reliability of large language models (LLMs) has recently garnered significant attention. Given LLMs' propensity to hallucinate, as well as their high sensitivity to prompt design, it is already challenging to predict the performance of an individual LLM. However, the problem becomes more complex for compound LLM systems such as cascades, where in addition to each model's standalone performance, we must understand how the error rates of different models interact. In this paper, we present a probabilistic model for the joint performance distribution of a sequence of LLMs, which enables a framework for rationally tuning the confidence thresholds of a LLM cascade using continuous optimization. Compared to selecting confidence thresholds using Bayesian optimization, our parametric Markov-copula model yields more favorable error-cost trade-offs, improving the area under the error-cost curve by 4.3% on average for cascades with $k\geq 3$ models. In the low-sample regime with $n \leq 30$ training examples, the performance improvement widens to 10.2%, suggesting that our framework's inductive assumptions about the interactions between the error rates of different LLMs enhance sample efficiency. Overall, our Markov-copula model provides a rational basis for tuning LLM cascade performance and points to the potential of probabilistic methods in analyzing systems of LLMs.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Randomness, exchangeability, and conformal prediction</title>
<link>https://arxiv.org/abs/2501.11689</link>
<guid>https://arxiv.org/abs/2501.11689</guid>
<content:encoded><![CDATA[
arXiv:2501.11689v3 Announce Type: replace 
Abstract: This paper argues for a wider use of the functional theory of randomness, a modification of the algorithmic theory of randomness getting rid of unspecified additive constants. Both theories are useful for understanding relationships between the assumptions of IID data and data exchangeability. While the assumption of IID data is standard in machine learning, conformal prediction relies on data exchangeability. Nouretdinov, V'yugin, and Gammerman showed, using the language of the algorithmic theory of randomness, that conformal prediction is a universal method under the assumption of IID data. In this paper (written for the Alex Gammerman Festschrift) I will selectively review connections between exchangeability and the property of being IID, early history of conformal prediction, my encounters and collaboration with Alex and other interesting people, and a translation of Nouretdinov et al.'s results into the language of the functional theory of randomness, which moves it closer to practice. Namely, the translation says that every confidence predictor that is valid for IID data can be transformed to a conformal predictor without losing much in predictive efficiency.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GANQ: GPU-Adaptive Non-Uniform Quantization for Large Language Models</title>
<link>https://arxiv.org/abs/2501.12956</link>
<guid>https://arxiv.org/abs/2501.12956</guid>
<content:encoded><![CDATA[
arXiv:2501.12956v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) face significant deployment challenges due to their substantial resource requirements. While low-bit quantized weights can reduce memory usage and improve inference efficiency, current hardware lacks native support for mixed-precision General Matrix Multiplication (mpGEMM), resulting in inefficient dequantization-based implementations. Moreover, uniform quantization methods often fail to capture weight distributions adequately, leading to performance degradation. We propose GANQ (GPU-Adaptive Non-Uniform Quantization), a layer-wise post-training non-uniform quantization framework optimized for hardware-efficient lookup table-based mpGEMM. GANQ achieves superior quantization performance by utilizing a training-free, GPU-adaptive optimization algorithm to efficiently reduce layer-wise quantization errors. Extensive experiments demonstrate GANQ's ability to reduce the perplexity gap from the FP16 baseline compared to state-of-the-art methods for both 3-bit and 4-bit quantization. Furthermore, when deployed on a single NVIDIA RTX 4090 GPU, GANQ's quantized models achieve up to 2.57$\times$ speedup over the baseline, advancing memory and inference efficiency in LLM deployment.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraphRAG under Fire</title>
<link>https://arxiv.org/abs/2501.14050</link>
<guid>https://arxiv.org/abs/2501.14050</guid>
<content:encoded><![CDATA[
arXiv:2501.14050v3 Announce Type: replace 
Abstract: GraphRAG advances retrieval-augmented generation (RAG) by structuring external knowledge as multi-scale knowledge graphs, enabling language models to integrate both broad context and granular details in their generation. While GraphRAG has demonstrated success across domains, its security implications remain largely unexplored. To bridge this gap, this work examines GraphRAG's vulnerability to poisoning attacks, uncovering an intriguing security paradox: existing RAG poisoning attacks are less effective under GraphRAG than conventional RAG, due to GraphRAG's graph-based indexing and retrieval; yet, the same features also create new attack surfaces. We present GragPoison, a novel attack that exploits shared relations in the underlying knowledge graph to craft poisoning text capable of compromising multiple queries simultaneously. GragPoison employs three key strategies: (i) relation injection to introduce false knowledge, (ii) relation enhancement to amplify poisoning influence, and (iii) narrative generation to embed malicious content within coherent text. Empirical evaluation across diverse datasets and models shows that GragPoison substantially outperforms existing attacks in terms of effectiveness (up to 98% success rate) and scalability (using less than 68% poisoning text) on multiple variations of GraphRAG. We also explore potential defensive measures and their limitations, identifying promising directions for future research.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-attacker: Enhancing Closed-loop Adversarial Scenario Generation for Autonomous Driving with Large Language Models</title>
<link>https://arxiv.org/abs/2501.15850</link>
<guid>https://arxiv.org/abs/2501.15850</guid>
<content:encoded><![CDATA[
arXiv:2501.15850v2 Announce Type: replace 
Abstract: Ensuring and improving the safety of autonomous driving systems (ADS) is crucial for the deployment of highly automated vehicles, especially in safety-critical events. To address the rarity issue, adversarial scenario generation methods are developed, in which behaviors of traffic participants are manipulated to induce safety-critical events. However, existing methods still face two limitations. First, identification of the adversarial participant directly impacts the effectiveness of the generation. However, the complexity of real-world scenarios, with numerous participants and diverse behaviors, makes identification challenging. Second, the potential of generated safety-critical scenarios to continuously improve ADS performance remains underexplored. To address these issues, we propose LLM-attacker: a closed-loop adversarial scenario generation framework leveraging large language models (LLMs). Specifically, multiple LLM agents are designed and coordinated to identify optimal attackers. Then, the trajectories of the attackers are optimized to generate adversarial scenarios. These scenarios are iteratively refined based on the performance of ADS, forming a feedback loop to improve ADS. Experimental results show that LLM-attacker can create more dangerous scenarios than other methods, and the ADS trained with it achieves a collision rate half that of training with normal scenarios. This indicates the ability of LLM-attacker to test and enhance the safety and robustness of ADS. Video demonstrations are provided at: https://drive.google.com/file/d/1Zv4V3iG7825oyiKbUwS2Y-rR0DQIE1ZA/view.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Inference-Efficient Language Models</title>
<link>https://arxiv.org/abs/2501.18107</link>
<guid>https://arxiv.org/abs/2501.18107</guid>
<content:encoded><![CDATA[
arXiv:2501.18107v2 Announce Type: replace 
Abstract: Scaling laws are powerful tools to predict the performance of large language models. However, current scaling laws fall short of accounting for inference costs. In this work, we first show that model architecture affects inference latency, where models of the same size can have up to 3.5x difference in latency. To tackle this challenge, we modify the Chinchilla scaling laws to co-optimize the model parameter count, the number of training tokens, and the model architecture. Due to the reason that models of similar training loss exhibit gaps in downstream evaluation, we also propose a novel method to train inference-efficient models based on the revised scaling laws. We perform extensive empirical studies to fit and evaluate our inference-aware scaling laws. We vary model parameters from 80M to 1B, training tokens from 1.6B to 30B, and model shapes, training 63 models. Guided by our inference-efficient scaling law and model selection method, we release the Morph-1B model, which improves inference latency by 1.8x while maintaining accuracy on downstream tasks compared to open-source models, pushing the Pareto frontier of accuracy-latency tradeoff. Notably, our experiments reveal that wider and shallower models can yield efficiency gains while preserving accuracy.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning</title>
<link>https://arxiv.org/abs/2501.18858</link>
<guid>https://arxiv.org/abs/2501.18858</guid>
<content:encoded><![CDATA[
arXiv:2501.18858v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, yet generating reliable reasoning processes remains a significant challenge. We present a unified probabilistic framework that formalizes LLM reasoning through a novel graphical model incorporating latent thinking processes and evaluation signals. Within this framework, we introduce the Bootstrapping Reinforced Thinking Process (BRiTE) algorithm, which works in two steps. First, it generates high-quality rationales by approximating the optimal thinking process through reinforcement learning, using a novel reward shaping mechanism. Second, it enhances the base LLM by maximizing the joint probability of rationale generation with respect to the model's parameters. Theoretically, we demonstrate BRiTE's convergence at a rate of $1/T$ with $T$ representing the number of iterations. Empirical evaluations on math and coding benchmarks demonstrate that our approach consistently improves performance across different base models without requiring human-annotated thinking processes. In addition, BRiTE demonstrates superior performance compared to existing algorithms that bootstrap thinking processes use alternative methods such as rejection sampling, and can even match or exceed the results achieved through supervised fine-tuning with human-annotated data.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Refining Adaptive Zeroth-Order Optimization at Ease</title>
<link>https://arxiv.org/abs/2502.01014</link>
<guid>https://arxiv.org/abs/2502.01014</guid>
<content:encoded><![CDATA[
arXiv:2502.01014v2 Announce Type: replace 
Abstract: Recently, zeroth-order (ZO) optimization plays an essential role in scenarios where gradient information is inaccessible or unaffordable, such as black-box systems and resource-constrained environments. While existing adaptive methods such as ZO-AdaMM have shown promise, they are fundamentally limited by their underutilization of moment information during optimization, usually resulting in underperforming convergence. To overcome these limitations, this paper introduces Refined Adaptive Zeroth-Order Optimization (R-AdaZO). Specifically, we first show the untapped variance reduction effect of first moment estimate on ZO gradient estimation, which improves the accuracy and stability of ZO updates. We then refine the second moment estimate based on these variance-reduced gradient estimates to better capture the geometry of the optimization landscape, enabling a more effective scaling of ZO updates. We present rigorous theoretical analysis to show (a) the first analysis to the variance reduction of first moment estimate in ZO optimization, (b) the improved second moment estimates with a more accurate approximation of its variance-free ideal, (c) the first variance-aware convergence framework for adaptive ZO methods, which may be of independent interest, and (d) the faster convergence of R-AdaZO than existing baselines like ZO-AdaMM. Our extensive experiments, including synthetic problems, black-box adversarial attack, and memory-efficient fine-tuning of large language models (LLMs), further verify the superior convergence of R-AdaZO, indicating that R-AdaZO offers an improved solution for real-world ZO optimization challenges.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tackling Feature and Sample Heterogeneity in Decentralized Multi-Task Learning: A Sheaf-Theoretic Approach</title>
<link>https://arxiv.org/abs/2502.01145</link>
<guid>https://arxiv.org/abs/2502.01145</guid>
<content:encoded><![CDATA[
arXiv:2502.01145v2 Announce Type: replace 
Abstract: Federated multi-task learning (FMTL) aims to simultaneously learn multiple related tasks across clients without sharing sensitive raw data. However, in the decentralized setting, existing FMTL frameworks are limited in their ability to capture complex task relationships and handle feature and sample heterogeneity across clients. To address these challenges, we introduce a novel sheaf-theoretic-based approach for FMTL. By representing client relationships using cellular sheaves, our framework can flexibly model interactions between heterogeneous client models. We formulate the sheaf-based FMTL optimization problem using sheaf Laplacian regularization and propose the Sheaf-FMTL algorithm to solve it. We show that the proposed framework provides a unified view encompassing many existing federated learning (FL) and FMTL approaches. Furthermore, we prove that our proposed algorithm, Sheaf-FMTL, achieves a sublinear convergence rate in line with state-of-the-art decentralized FMTL algorithms. Extensive experiments show that although Sheaf-FMTL introduces computational and storage overhead due to the management of interaction maps, it achieves substantial communication savings in terms of transmitted bits when compared to decentralized FMTL baselines. This trade-off makes Sheaf-FMTL especially suitable for cross-silo FL scenarios, where managing model heterogeneity and ensuring communication efficiency are essential, and where clients have adequate computational resources.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trajectory World Models for Heterogeneous Environments</title>
<link>https://arxiv.org/abs/2502.01366</link>
<guid>https://arxiv.org/abs/2502.01366</guid>
<content:encoded><![CDATA[
arXiv:2502.01366v2 Announce Type: replace 
Abstract: Heterogeneity in sensors and actuators across environments poses a significant challenge to building large-scale pre-trained world models on top of this low-dimensional sensor information. In this work, we explore pre-training world models for heterogeneous environments by addressing key transfer barriers in both data diversity and model flexibility. We introduce UniTraj, a unified dataset comprising over one million trajectories from 80 environments, designed to scale data while preserving critical diversity. Additionally, we propose TrajWorld, a novel architecture capable of flexibly handling varying sensor and actuator information and capturing environment dynamics in-context. Pre-training TrajWorld on UniTraj yields substantial gains in transition prediction, achieves a new state-of-the-art for off-policy evaluation, and also delivers superior online performance of model predictive control. To the best of our knowledge, this work, for the first time, demonstrates the transfer benefits of world models across heterogeneous and complex control environments. Code and data are available at https://github.com/thuml/TrajWorld.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Membership Inference Attack Should Move On to Distributional Statistics for Distilled Generative Models</title>
<link>https://arxiv.org/abs/2502.02970</link>
<guid>https://arxiv.org/abs/2502.02970</guid>
<content:encoded><![CDATA[
arXiv:2502.02970v2 Announce Type: replace 
Abstract: To detect unauthorized data usage in training large-scale generative models (e.g., ChatGPT or Midjourney), membership inference attacks (MIA) have proven effective in distinguishing a single training instance (a member) from a single non-training instance (a non-member). This success is mainly credited to a memorization effect: models tend to perform better on a member than a non-member. However, we find that standard MIAs fail against distilled generative models (i.e., student models) that are increasingly deployed in practice for efficiency (e.g., ChatGPT 4o-mini). Trained exclusively on data generated from a large-scale model (a teacher model), the student model lacks direct exposure to any members (teacher's training data), nullifying the memorization effect that standard MIAs rely on. This finding reveals a serious privacy loophole, where generation-service providers could deploy a student model whose teacher was potentially trained on unauthorized data, yet claim the deployed model is clean because it was not directly trained on such data. Hence, are distilled models inherently unauditable for upstream privacy violations, and should we discard them when we care about privacy? We contend no, as we uncover a memory chain connecting the student and teacher's member data: the distribution of student-generated data aligns more closely with the distribution of the teacher's members than with non-members, thus we can detect unauthorized data usage even when direct instance-level memorization is absent. This leads us to posit that MIAs on distilled generative models should shift from instance-level scores to distribution-level statistics. We further propose three principles of distribution-based MIAs for detecting unauthorized training data through distilled generative models, and validate our position through an exemplar framework. We lastly discuss the implications of our position.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RepLoRA: Reparameterizing Low-Rank Adaptation via the Perspective of Mixture of Experts</title>
<link>https://arxiv.org/abs/2502.03044</link>
<guid>https://arxiv.org/abs/2502.03044</guid>
<content:encoded><![CDATA[
arXiv:2502.03044v2 Announce Type: replace 
Abstract: Low-rank Adaptation (LoRA) has emerged as a powerful method for fine-tuning large-scale foundation models. Despite its popularity, the theoretical understanding of LoRA has remained limited. This paper presents a theoretical analysis of LoRA by examining its connection to the Mixture of Experts models. Under this framework, we show that simple reparameterizations of the LoRA matrices can notably accelerate the low-rank matrix estimation process. In particular, we prove that reparameterization can reduce the data needed to achieve a desired estimation error from an exponential to a polynomial scale. Motivated by this insight, we propose Reparameterized Low-Rank Adaptation (RepLoRA), which incorporates lightweight MLPs to reparameterize the LoRA matrices. Extensive experiments across multiple domains demonstrate that RepLoRA consistently outperforms vanilla LoRA. Notably, with limited data, RepLoRA surpasses LoRA by a margin of up to 40.0% and achieves LoRA's performance with only 30.0% of the training data, highlighting both the theoretical and empirical robustness of our PEFT method.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-agent Architecture Search via Agentic Supernet</title>
<link>https://arxiv.org/abs/2502.04180</link>
<guid>https://arxiv.org/abs/2502.04180</guid>
<content:encoded><![CDATA[
arXiv:2502.04180v2 Announce Type: replace 
Abstract: Large Language Model (LLM)-empowered multi-agent systems extend the cognitive boundaries of individual agents through disciplined collaboration and interaction, while constructing these systems often requires labor-intensive manual designs. Despite the availability of methods to automate the design of agentic workflows, they typically seek to identify a static, complex, one-size-fits-all system, which, however, fails to dynamically allocate inference resources based on the difficulty and domain of each query. To address this challenge, we shift away from the pursuit of a monolithic agentic system, instead optimizing the \textbf{agentic supernet}, a probabilistic and continuous distribution of agentic architectures. We introduce MaAS, an automated framework that samples query-dependent agentic systems from the supernet, delivering high-quality solutions and tailored resource allocation (\textit{e.g.}, LLM calls, tool calls, token cost). Comprehensive evaluation across six benchmarks demonstrates that MaAS \textbf{(I)} requires only $6\sim45\%$ of the inference costs of existing handcrafted or automated multi-agent systems, \textbf{(II)} surpasses them by $0.54\%\sim11.82\%$, and \textbf{(III)} enjoys superior cross-dataset and cross-LLM-backbone transferability.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Short-length Adversarial Training Helps LLMs Defend Long-length Jailbreak Attacks: Theoretical and Empirical Evidence</title>
<link>https://arxiv.org/abs/2502.04204</link>
<guid>https://arxiv.org/abs/2502.04204</guid>
<content:encoded><![CDATA[
arXiv:2502.04204v2 Announce Type: replace 
Abstract: Jailbreak attacks against large language models (LLMs) aim to induce harmful behaviors in LLMs through carefully crafted adversarial prompts. To mitigate attacks, one way is to perform adversarial training (AT)-based alignment, i.e., training LLMs on some of the most adversarial prompts to help them learn how to behave safely under attacks. During AT, the length of adversarial prompts plays a critical role in the robustness of aligned LLMs. While long-length adversarial prompts during AT might lead to strong LLM robustness, their synthesis however is very resource-consuming, which may limit the application of LLM AT. This paper focuses on adversarial suffix jailbreak attacks and unveils that to defend against a jailbreak attack with an adversarial suffix of length $\Theta(M)$, it is enough to align LLMs on prompts with adversarial suffixes of length $\Theta(\sqrt{M})$. Theoretically, we analyze the adversarial in-context learning of linear transformers on linear regression tasks and prove a robust generalization bound for trained transformers. The bound depends on the term $\Theta(\sqrt{M_{\text{test}}}/M_{\text{train}})$, where $M_{\text{train}}$ and $M_{\text{test}}$ are the numbers of adversarially perturbed in-context samples during training and testing. Empirically, we conduct AT on popular open-source LLMs and evaluate their robustness against jailbreak attacks of different adversarial suffix lengths. Results confirm a positive correlation between the attack success rate and the ratio of the square root of the adversarial suffix length during jailbreaking to the length during AT. Our findings show that it is practical to defend against ``long-length'' jailbreak attacks via efficient ``short-length'' AT. The code is available at https://github.com/fshp971/adv-icl.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Complex Physics-Informed Neural Network</title>
<link>https://arxiv.org/abs/2502.04917</link>
<guid>https://arxiv.org/abs/2502.04917</guid>
<content:encoded><![CDATA[
arXiv:2502.04917v2 Announce Type: replace 
Abstract: We propose compleX-PINN, a novel physics-informed neural network (PINN) architecture incorporating a learnable activation function inspired by the Cauchy integral theorem. By optimizing the activation parameters, compleX-PINN achieves high accuracy with just a single hidden layer. Empirically, we demonstrate that compleX-PINN solves high-dimensional problems that pose significant challenges for PINNs. Our results show that compleX-PINN consistently achieves substantially greater precision, often improving accuracy by an order of magnitude, on these complex tasks.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When, Where and Why to Average Weights?</title>
<link>https://arxiv.org/abs/2502.06761</link>
<guid>https://arxiv.org/abs/2502.06761</guid>
<content:encoded><![CDATA[
arXiv:2502.06761v2 Announce Type: replace 
Abstract: Averaging checkpoints along the training trajectory is a simple yet powerful approach to improve the generalization performance of Machine Learning models and reduce training time. Motivated by these potential gains, and in an effort to fairly and thoroughly benchmark this technique, we present an extensive evaluation of averaging techniques in modern Deep Learning, which we perform using AlgoPerf \citep{dahl_benchmarking_2023}, a large-scale benchmark for optimization algorithms. We investigate whether weight averaging can reduce training time, improve generalization, and replace learning rate decay, as suggested by recent literature. Our evaluation across seven architectures and datasets reveals that averaging significantly accelerates training and yields considerable efficiency gains, at the price of a minimal implementation and memory cost, while mildly improving generalization across all considered workloads. Finally, we explore the relationship between averaging and learning rate annealing and show how to optimally combine the two to achieve the best performances.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RelGNN: Composite Message Passing for Relational Deep Learning</title>
<link>https://arxiv.org/abs/2502.06784</link>
<guid>https://arxiv.org/abs/2502.06784</guid>
<content:encoded><![CDATA[
arXiv:2502.06784v2 Announce Type: replace 
Abstract: Predictive tasks on relational databases are critical in real-world applications spanning e-commerce, healthcare, and social media. To address these tasks effectively, Relational Deep Learning (RDL) encodes relational data as graphs, enabling Graph Neural Networks (GNNs) to exploit relational structures for improved predictions. However, existing RDL methods often overlook the intrinsic structural properties of the graphs built from relational databases, leading to modeling inefficiencies, particularly in handling many-to-many relationships. Here we introduce RelGNN, a novel GNN framework specifically designed to leverage the unique structural characteristics of the graphs built from relational databases. At the core of our approach is the introduction of atomic routes, which are simple paths that enable direct single-hop interactions between the source and destination nodes. Building upon these atomic routes, RelGNN designs new composite message passing and graph attention mechanisms that reduce redundancy, highlight key signals, and enhance predictive accuracy. RelGNN is evaluated on 30 diverse real-world tasks from Relbench (Fey et al., 2024), and achieves state-of-the-art performance on the vast majority of tasks, with improvements of up to 25%. Code is available at https://github.com/snap-stanford/RelGNN.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intelligent Offloading in Vehicular Edge Computing: A Comprehensive Review of Deep Reinforcement Learning Approaches and Architectures</title>
<link>https://arxiv.org/abs/2502.06963</link>
<guid>https://arxiv.org/abs/2502.06963</guid>
<content:encoded><![CDATA[
arXiv:2502.06963v2 Announce Type: replace 
Abstract: The increasing complexity of Intelligent Transportation Systems (ITS) has led to significant interest in computational offloading to external infrastructures such as edge servers, vehicular nodes, and UAVs. These dynamic and heterogeneous environments pose challenges for traditional offloading strategies, prompting the exploration of Reinforcement Learning (RL) and Deep Reinforcement Learning (DRL) as adaptive decision-making frameworks. This survey presents a comprehensive review of recent advances in DRL-based offloading for vehicular edge computing (VEC). We classify and compare existing works based on learning paradigms (e.g., single-agent, multi-agent), system architectures (e.g., centralized, distributed, hierarchical), and optimization objectives (e.g., latency, energy, fairness). Furthermore, we analyze how Markov Decision Process (MDP) formulations are applied and highlight emerging trends in reward design, coordination mechanisms, and scalability. Finally, we identify open challenges and outline future research directions to guide the development of robust and intelligent offloading strategies for next-generation ITS.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Capability Discovery via Foundation Model Self-Exploration</title>
<link>https://arxiv.org/abs/2502.07577</link>
<guid>https://arxiv.org/abs/2502.07577</guid>
<content:encoded><![CDATA[
arXiv:2502.07577v3 Announce Type: replace 
Abstract: Foundation models have become general-purpose assistants, exhibiting diverse capabilities across numerous domains through training on web-scale data. It remains challenging to precisely characterize even a fraction of the full spectrum of these abilities and potential risks in any new model. Existing evaluation approaches often require significant human effort, and it is taking increasing effort to design ever harder challenges for more capable models. We introduce Automated Capability Discovery (ACD), a framework that designates one foundation model as a scientist to systematically propose open-ended tasks probing the abilities of a subject model (potentially itself). By combining frontier models with ideas from the field of open-endedness, ACD automatically and systematically uncovers a diverse spectrum of surprising capabilities and failures in the subject model. We demonstrate ACD across a range of foundation models (including the GPT, Claude, and Llama series), showing that it automatically generates thousands of distinct tasks, which are then clustered to reveal dozens of broader capability areas and failure modes, that would be challenging for any single team to uncover. We further validate our method's automated scoring with extensive human surveys, observing high agreement between model-generated and human evaluations. By leveraging foundation models' ability to both create tasks and self-evaluate, ACD is a significant step toward scalable, automated evaluation of novel AI systems. All code and evaluation logs are open-sourced at https://github.com/conglu1997/ACD.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Non-Acyclic GFlowNets in Discrete Environments</title>
<link>https://arxiv.org/abs/2502.07735</link>
<guid>https://arxiv.org/abs/2502.07735</guid>
<content:encoded><![CDATA[
arXiv:2502.07735v2 Announce Type: replace 
Abstract: Generative Flow Networks (GFlowNets) are a family of generative models that learn to sample objects from a given probability distribution, potentially known up to a normalizing constant. Instead of working in the object space, GFlowNets proceed by sampling trajectories in an appropriately constructed directed acyclic graph environment, greatly relying on the acyclicity of the graph. In our paper, we revisit the theory that relaxes the acyclicity assumption and present a simpler theoretical framework for non-acyclic GFlowNets in discrete environments. Moreover, we provide various novel theoretical insights related to training with fixed backward policies, the nature of flow functions, and connections between entropy-regularized RL and non-acyclic GFlowNets, which naturally generalize the respective concepts and theoretical results from the acyclic setting. In addition, we experimentally re-examine the concept of loss stability in non-acyclic GFlowNet training, as well as validate our own theoretical findings.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task Generalization With AutoRegressive Compositional Structure: Can Learning From $D$ Tasks Generalize to $D^{T}$ Tasks?</title>
<link>https://arxiv.org/abs/2502.08991</link>
<guid>https://arxiv.org/abs/2502.08991</guid>
<content:encoded><![CDATA[
arXiv:2502.08991v2 Announce Type: replace 
Abstract: Large language models (LLMs) exhibit remarkable task generalization, solving tasks they were never explicitly trained on with only a few demonstrations. This raises a fundamental question: When can learning from a small set of tasks generalize to a large task family? In this paper, we investigate task generalization through the lens of autoregressive compositional structure, where each task is a composition of $T$ operations, and each operation is among a finite family of $D$ subtasks. This yields a total class of size $D^T$. We first show that generalization to all $D^T$ tasks is theoretically achievable by training on only $\widetilde{O}(D)$ tasks. Empirically, we demonstrate that Transformers achieve such exponential task generalization on sparse parity functions via In-context Learning (ICL) and chain-of-thought (CoT) reasoning. We further show generalization in arithmetic and translation, beyond parity functions.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When do neural networks learn world models?</title>
<link>https://arxiv.org/abs/2502.09297</link>
<guid>https://arxiv.org/abs/2502.09297</guid>
<content:encoded><![CDATA[
arXiv:2502.09297v4 Announce Type: replace 
Abstract: Humans develop world models that capture the underlying generation process of data. Whether neural networks can learn similar world models remains an open problem. In this work, we present the first theoretical results for this problem, showing that in a multi-task setting, models with a low-degree bias provably recover latent data-generating variables under mild assumptions--even if proxy tasks involve complex, non-linear functions of the latents. However, such recovery is sensitive to model architecture. Our analysis leverages Boolean models of task solutions via the Fourier-Walsh transform and introduces new techniques for analyzing invertible Boolean transforms, which may be of independent interest. We illustrate the algorithmic implications of our results and connect them to related research areas, including self-supervised learning, out-of-distribution generalization, and the linear representation hypothesis in large language models.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Theoretical Benefit and Limitation of Diffusion Language Model</title>
<link>https://arxiv.org/abs/2502.09622</link>
<guid>https://arxiv.org/abs/2502.09622</guid>
<content:encoded><![CDATA[
arXiv:2502.09622v2 Announce Type: replace 
Abstract: Diffusion language models have emerged as a promising approach for text generation. One would naturally expect this method to be an efficient replacement for autoregressive models since multiple tokens can be sampled in parallel during each diffusion step. However, its efficiency-accuracy trade-off is not yet well understood. In this paper, we present a rigorous theoretical analysis of a widely used type of diffusion language model, the Masked Diffusion Model (MDM), and find that its effectiveness heavily depends on the target evaluation metric. Under mild conditions, we prove that when using perplexity as the metric, MDMs can achieve near-optimal perplexity in sampling steps regardless of sequence length, demonstrating that efficiency can be achieved without sacrificing performance. However, when using the sequence error rate--which is important for understanding the "correctness" of a sequence, such as a reasoning chain--we show that the required sampling steps must scale linearly with sequence length to obtain "correct" sequences, thereby eliminating MDM's efficiency advantage over autoregressive models. Our analysis establishes the first theoretical foundation for understanding the benefits and limitations of MDMs. All theoretical findings are supported by empirical studies.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeltaProduct: Improving State-Tracking in Linear RNNs via Householder Products</title>
<link>https://arxiv.org/abs/2502.10297</link>
<guid>https://arxiv.org/abs/2502.10297</guid>
<content:encoded><![CDATA[
arXiv:2502.10297v5 Announce Type: replace 
Abstract: Linear Recurrent Neural Networks (linear RNNs) have emerged as competitive alternatives to Transformers for sequence modeling, offering efficient training and linear-time inference. However, existing architectures face a fundamental trade-off between expressivity and efficiency, dictated by the structure of their state-transition matrices. Diagonal matrices, used in models such as Mamba, GLA, or mLSTM, yield fast runtime but have limited expressivity. To address this, recent architectures such as DeltaNet and RWKV-7 adopted a diagonal plus rank-1 structure, which allows simultaneous token and channel mixing, improving associative recall and, as recently shown, state-tracking when allowing negative eigenvalues in the state-transition matrices. Building on the interpretation of DeltaNet's recurrence as performing one step of online gradient descent per token on an associative recall loss, we introduce DeltaProduct, which instead takes multiple ($n_h$) steps per token. This naturally leads to diagonal plus rank-$n_h$ state-transition matrices, formed as products of $n_h$ generalized Householder transformations, providing a tunable mechanism to balance expressivity and efficiency. We provide a detailed theoretical characterization of the state-tracking capability of DeltaProduct in finite precision, showing how it improves by increasing $n_h$. Our extensive experiments demonstrate that DeltaProduct outperforms DeltaNet in both state-tracking and language modeling, while also showing significantly improved length extrapolation capabilities.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Epidemic-guided deep learning for spatiotemporal forecasting of Tuberculosis outbreak</title>
<link>https://arxiv.org/abs/2502.10786</link>
<guid>https://arxiv.org/abs/2502.10786</guid>
<content:encoded><![CDATA[
arXiv:2502.10786v2 Announce Type: replace 
Abstract: Tuberculosis (TB) remains a formidable global health challenge, driven by complex spatiotemporal transmission dynamics and influenced by factors such as population mobility and behavioral changes. We propose an Epidemic-Guided Deep Learning (EGDL) approach that fuses mechanistic epidemiological principles with advanced deep learning techniques to enhance early warning systems and intervention strategies for TB outbreaks. Our framework is built upon a modified networked Susceptible-Infectious-Recovered (MN-SIR) model augmented with a saturated incidence rate and graph Laplacian diffusion, capturing both long-term transmission dynamics and region-specific population mobility patterns. Compartmental model parameters are rigorously estimated using Bayesian inference via the Markov Chain Monte Carlo approach. Theoretical analysis leveraging the comparison principle and Green's formula establishes global stability properties of the disease-free and endemic equilibria. Building on these epidemiological insights, we design two forecasting architectures, EGDL-Parallel and EGDL-Series, that integrate the mechanistic outputs of the MN-SIR model within deep neural networks. This integration mitigates the overfitting risks commonly encountered in data-driven methods and filters out noise inherent in surveillance data, resulting in reliable forecasts of real-world epidemic trends. Experiments conducted on TB incidence data from 47 prefectures in Japan and 31 provinces in mainland China demonstrate that our approach delivers robust and accurate predictions across multiple time horizons (short to medium-term forecasts), supporting its generalizability across regions with different population dynamics.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Generalization on Text Attribute Graphs: Principles with Large Language Models</title>
<link>https://arxiv.org/abs/2502.11836</link>
<guid>https://arxiv.org/abs/2502.11836</guid>
<content:encoded><![CDATA[
arXiv:2502.11836v2 Announce Type: replace 
Abstract: Large language models (LLMs) have recently been introduced to graph learning, aiming to extend their zero-shot generalization success to tasks where labeled graph data is scarce. Among these applications, inference over text-attributed graphs (TAGs) presents unique challenges: existing methods struggle with LLMs' limited context length for processing large node neighborhoods and the misalignment between node embeddings and the LLM token space. To address these issues, we establish two key principles for ensuring generalization and derive the framework LLM-BP accordingly: (1) Unifying the attribute space with task-adaptive embeddings, where we leverage LLM-based encoders and task-aware prompting to enhance generalization of the text attribute embeddings; (2) Developing a generalizable graph information aggregation mechanism, for which we adopt belief propagation with LLM-estimated parameters that adapt across graphs. Evaluations on 11 real-world TAG benchmarks demonstrate that LLM-BP significantly outperforms existing approaches, achieving 8.10% improvement with task-conditional embeddings and an additional 1.71% gain from adaptive aggregation. The code and task-adaptive embeddings are publicly available.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Benign Overfitting in Two-Layer Neural Networks</title>
<link>https://arxiv.org/abs/2502.11893</link>
<guid>https://arxiv.org/abs/2502.11893</guid>
<content:encoded><![CDATA[
arXiv:2502.11893v2 Announce Type: replace 
Abstract: Recent theoretical studies (Kou et al., 2023; Cao et al., 2022) have revealed a sharp phase transition from benign to harmful overfitting when the noise-to-feature ratio exceeds a threshold-a situation common in long-tailed data distributions where atypical data is prevalent. However, harmful overfitting rarely happens in overparameterized neural networks. Further experimental results suggested that memorization is necessary for achieving near-optimal generalization error in long-tailed data distributions (Feldman & Zhang, 2020). We argue that this discrepancy between theoretical predictions and empirical observations arises because previous feature-noise data models overlook the heterogeneous nature of noise across different data classes. In this paper, we refine the feature-noise data model by incorporating class-dependent heterogeneous noise and re-examine the overfitting phenomenon in neural networks. Through a comprehensive analysis of the training dynamics, we establish test loss bounds for the refined model. Our findings reveal that neural networks can leverage "data noise" to learn implicit features that improve the classification accuracy for long-tailed data. Our analysis also provides a training-free metric for evaluating data influence on test performance. Experimental validation on both synthetic and real-world datasets supports our theoretical results.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning Should Maximize Welfare, but Not by (Only) Maximizing Accuracy</title>
<link>https://arxiv.org/abs/2502.11981</link>
<guid>https://arxiv.org/abs/2502.11981</guid>
<content:encoded><![CDATA[
arXiv:2502.11981v2 Announce Type: replace 
Abstract: Decades of research in machine learning have given us powerful tools for making accurate predictions. This has made such tools appealing for use in social settings and on human inputs. Yet despite a lack of justification for why the generic approach of accuracy maximization can or should improve our collective well-being -- and mounting evidence of likely adverse outcomes -- it remains the widespread default. This position paper asserts that for machine learning to become socially beneficial, it must be embedded within a broader economic framework that explicitly aims to maximize social welfare. The field of welfare economics asks: how should we allocate limited resources among self-interested agents to maximize overall benefits? We contend that this perspective applies to many contemporary applications of machine learning in social contexts, and advocate for its adoption. Rather than disposing of prediction, we propose to leverage this forte of machine learning towards welfare maximization. We demonstrate this idea by portraying a conceptual framework that gradually transitions from accuracy maximization (with awareness to welfare) to welfare maximization (via accurate prediction). We detail applications and use-cases for which this framework can be effective, identify technical challenges and practical opportunities, and highlight future avenues worth pursuing.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Expressive are Knowledge Graph Foundation Models?</title>
<link>https://arxiv.org/abs/2502.13339</link>
<guid>https://arxiv.org/abs/2502.13339</guid>
<content:encoded><![CDATA[
arXiv:2502.13339v2 Announce Type: replace 
Abstract: Knowledge Graph Foundation Models (KGFMs) are at the frontier for deep learning on knowledge graphs (KGs), as they can generalize to completely novel knowledge graphs with different relational vocabularies. Despite their empirical success, our theoretical understanding of KGFMs remains very limited. In this paper, we conduct a rigorous study of the expressive power of KGFMs. Specifically, we show that the expressive power of KGFMs directly depends on the motifs that are used to learn the relation representations. We then observe that the most typical motifs used in the existing literature are binary, as the representations are learned based on how pairs of relations interact, which limits the model's expressiveness. As part of our study, we design more expressive KGFMs using richer motifs, which necessitate learning relation representations based on, e.g., how triples of relations interact with each other. Finally, we empirically validate our theoretical findings, showing that the use of richer motifs results in better performance on a wide range of datasets drawn from different domains.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Bad Goods Risk Scores with ARIMA Time Series: A Novel Risk Assessment Approach</title>
<link>https://arxiv.org/abs/2502.16520</link>
<guid>https://arxiv.org/abs/2502.16520</guid>
<content:encoded><![CDATA[
arXiv:2502.16520v3 Announce Type: replace 
Abstract: The increasing complexity of supply chains and the rising costs associated with defective or substandard goods (bad goods) highlight the urgent need for advanced predictive methodologies to mitigate risks and enhance operational efficiency. This research presents a novel framework that integrates Time Series ARIMA (AutoRegressive Integrated Moving Average) models with a proprietary formula specifically designed to calculate bad goods after time series forecasting. By leveraging historical data patterns, including sales, returns, and capacity, the model forecasts potential quality failures, enabling proactive decision-making. ARIMA is employed to capture temporal trends in time series data, while the newly developed formula quantifies the likelihood and impact of defects with greater precision. Experimental results, validated on a dataset spanning 2022-2024 for Organic Beer-G 1 Liter, demonstrate that the proposed method outperforms traditional statistical models, such as Exponential Smoothing and Holt-Winters, in both prediction accuracy and risk evaluation. This study advances the field of predictive analytics by bridging time series forecasting, ARIMA, and risk management in supply chain quality control, offering a scalable and practical solution for minimizing losses due to bad goods.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DISC: DISC: Dynamic Decomposition Improves LLM Inference Scaling</title>
<link>https://arxiv.org/abs/2502.16706</link>
<guid>https://arxiv.org/abs/2502.16706</guid>
<content:encoded><![CDATA[
arXiv:2502.16706v2 Announce Type: replace 
Abstract: Inference scaling methods for large language models often work by breaking problems into steps or groups of tokens, then sampling and selecting the best next steps. However, these steps and their sizes are usually fixed or manually designed based on domain knowledge. We introduce dynamic decomposition, a method that adaptively and automatically breaks down solution and reasoning traces into manageable steps during inference. By allocating compute more effectively - especially by subdividing difficult steps and prioritizing their sampling - dynamic decomposition significantly boosts inference efficiency. Experiments on benchmarks like APPS, MATH, and LiveCodeBench show that dynamic decomposition outperforms fixed strategies such as token-level, sentence-level, and single-step decompositions, reducing the pass@10 error rate by 5.0%, 6.7%, and 10.5% respectively. These results show the promise of dynamic decomposition for improving a broad range of inference scaling techniques.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic Text Generation for Training Large Language Models via Gradient Matching</title>
<link>https://arxiv.org/abs/2502.17607</link>
<guid>https://arxiv.org/abs/2502.17607</guid>
<content:encoded><![CDATA[
arXiv:2502.17607v2 Announce Type: replace 
Abstract: Synthetic data has the potential to improve the performance, training efficiency, and privacy of real training examples. Nevertheless, existing approaches for synthetic text generation are mostly heuristics and cannot generate human-readable text without compromising the privacy of real data, or provide performance guarantees for training Large Language Models (LLMs). In this work, we propose the first theoretically rigorous approach for generating synthetic human-readable text that provides convergence, performance, and privacy guarantees for fine-tuning LLMs on a target task. To do so, we leverage Alternating Direction Method of Multipliers (ADMM) that iteratively optimizes the embeddings of synthetic examples to match the noisy gradient of the target training or validation data, and maps them to a sequence of text tokens with low perplexity. In doing so, the generated synthetic text guarantees convergence of the model to a close neighborhood of the solution obtained by fine-tuning on real data and preserves their privacy. Experiments on various classification tasks confirm the effectiveness of our proposed approach. Our code is available at https://github.com/BigML-CS-UCLA/GRADMM.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMPO: Active Multi-Preference Optimization for Self-play Preference Selection</title>
<link>https://arxiv.org/abs/2502.18293</link>
<guid>https://arxiv.org/abs/2502.18293</guid>
<content:encoded><![CDATA[
arXiv:2502.18293v2 Announce Type: replace 
Abstract: Multi-preference optimization enriches language-model alignment beyond pairwise preferences by contrasting entire sets of helpful and undesired responses, thereby enabling richer training signals for large language models. During self-play alignment, these models often produce numerous candidate answers per query, rendering it computationally infeasible to include all responses in the training objective. In this work, we propose $\textit{Active Multi-Preference Optimization}$ (AMPO), a novel approach that combines on-policy generation, a multi-preference group-contrastive loss, and active subset selection. Specifically, we score and embed large candidate pools of responses and then select a small, yet informative, subset that covers reward extremes and distinct semantic clusters for preference optimization. Our contrastive training scheme is capable of identifying not only the best and worst answers but also subtle, underexplored modes that are crucial for robust alignment. Theoretically, we provide guarantees for expected reward maximization using our active selection method, and empirically, AMPO achieves state-of-the-art results on $\textit{AlpacaEval}$ using Llama 8B and Mistral 7B. We release our datasets $\href{https://huggingface.co/Multi-preference-Optimization}{here}$.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Offline to Online Memory-Free and Task-Free Continual Learning via Fine-Grained Hypergradients</title>
<link>https://arxiv.org/abs/2502.18762</link>
<guid>https://arxiv.org/abs/2502.18762</guid>
<content:encoded><![CDATA[
arXiv:2502.18762v2 Announce Type: replace 
Abstract: Continual Learning (CL) aims to learn from a non-stationary data stream where the underlying distribution changes over time. While recent advances have produced efficient memory-free methods in the offline CL (offCL) setting, where tasks are known in advance and data can be revisited, online CL (onCL) remains dominated by memory-based approaches. The transition from offCL to onCL is challenging, as many offline methods rely on (1) prior knowledge of task boundaries and (2) sophisticated scheduling or optimization schemes, both of which are unavailable when data arrives sequentially and can be seen only once. In this paper, we investigate the adaptation of state-of-the-art memory-free offCL methods to the online setting. We first show that augmenting these methods with lightweight prototypes significantly improves performance, albeit at the cost of increased Gradient Imbalance, resulting in a biased learning towards earlier tasks. To address this issue, we introduce Fine-Grained Hypergradients, an online mechanism for rebalancing gradient updates during training. Our experiments demonstrate that the synergy between prototype memory and hypergradient reweighting substantially enhances the performance of memory-free methods in onCL and surpasses onCL baselines. Code will be released upon acceptance.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universality of conformal prediction under the assumption of randomness</title>
<link>https://arxiv.org/abs/2502.19254</link>
<guid>https://arxiv.org/abs/2502.19254</guid>
<content:encoded><![CDATA[
arXiv:2502.19254v2 Announce Type: replace 
Abstract: Conformal predictors provide set or functional predictions that are valid under the assumption of randomness, i.e., under the assumption of independent and identically distributed data. The question asked in this paper is whether there are predictors that are valid in the same sense under the assumption of randomness and that are more efficient than conformal predictors. The answer is that the class of conformal predictors is universal in that only limited gains in predictive efficiency are possible. The previous work in this area has relied on the algorithmic theory of randomness and so involved unspecified constants, whereas this paper's results are much more practical. They are also shown to be optimal in some respects.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PhantomWiki: On-Demand Datasets for Reasoning and Retrieval Evaluation</title>
<link>https://arxiv.org/abs/2502.20377</link>
<guid>https://arxiv.org/abs/2502.20377</guid>
<content:encoded><![CDATA[
arXiv:2502.20377v2 Announce Type: replace 
Abstract: High-quality benchmarks are essential for evaluating reasoning and retrieval capabilities of large language models (LLMs). However, curating datasets for this purpose is not a permanent solution as they are prone to data leakage and inflated performance results. To address these challenges, we propose PhantomWiki: a pipeline to generate unique, factually consistent document corpora with diverse question-answer pairs. Unlike prior work, PhantomWiki is neither a fixed dataset, nor is it based on any existing data. Instead, a new PhantomWiki instance is generated on demand for each evaluation. We vary the question difficulty and corpus size to disentangle reasoning and retrieval capabilities respectively, and find that PhantomWiki datasets are surprisingly challenging for frontier LLMs. Thus, we contribute a scalable and data leakage-resistant framework for disentangled evaluation of reasoning, retrieval, and tool-use abilities. Our code is available at https://github.com/kilian-group/phantom-wiki.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BRIDGE: Bootstrapping Text to Control Time-Series Generation via Multi-Agent Iterative Optimization and Diffusion Modeling</title>
<link>https://arxiv.org/abs/2503.02445</link>
<guid>https://arxiv.org/abs/2503.02445</guid>
<content:encoded><![CDATA[
arXiv:2503.02445v4 Announce Type: replace 
Abstract: Time-series Generation (TSG) is a prominent research area with broad applications in simulations, data augmentation, and counterfactual analysis. While existing methods have shown promise in unconditional single-domain TSG, real-world applications demand for cross-domain approaches capable of controlled generation tailored to domain-specific constraints and instance-level requirements. In this paper, we argue that text can provide semantic insights, domain information and instance-specific temporal patterns, to guide and improve TSG. We introduce ``Text-Controlled TSG'', a task focused on generating realistic time series by incorporating textual descriptions. To address data scarcity in this setting, we propose a novel LLM-based Multi-Agent framework that synthesizes diverse, realistic text-to-TS datasets. Furthermore, we introduce BRIDGE, a hybrid text-controlled TSG framework that integrates semantic prototypes with text description for supporting domain-level guidance. This approach achieves state-of-the-art generation fidelity on 11 of 12 datasets, and improves controllability by up to 12% on MSE and 6% MAE compared to no text input generation, highlighting its potential for generating tailored time-series data.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts</title>
<link>https://arxiv.org/abs/2503.02819</link>
<guid>https://arxiv.org/abs/2503.02819</guid>
<content:encoded><![CDATA[
arXiv:2503.02819v2 Announce Type: replace 
Abstract: While score-based generative models are the model of choice across diverse domains, there are limited tools available for controlling inference-time behavior in a principled manner, e.g. for composing multiple pretrained models. Existing classifier-free guidance methods use a simple heuristic to mix conditional and unconditional scores to approximately sample from conditional distributions. However, such methods do not approximate the intermediate distributions, necessitating additional `corrector' steps. In this work, we provide an efficient and principled method for sampling from a sequence of annealed, geometric-averaged, or product distributions derived from pretrained score-based models. We derive a weighted simulation scheme which we call Feynman-Kac Correctors (FKCs) based on the celebrated Feynman-Kac formula by carefully accounting for terms in the appropriate partial differential equations (PDEs). To simulate these PDEs, we propose Sequential Monte Carlo (SMC) resampling algorithms that leverage inference-time scaling to improve sampling quality. We empirically demonstrate the utility of our methods by proposing amortized sampling via inference-time temperature annealing, improving multi-objective molecule generation using pretrained models, and improving classifier-free guidance for text-to-image generation. Our code is available at https://github.com/martaskrt/fkc-diffusion.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Straight-Line Diffusion Model for Efficient 3D Molecular Generation</title>
<link>https://arxiv.org/abs/2503.02918</link>
<guid>https://arxiv.org/abs/2503.02918</guid>
<content:encoded><![CDATA[
arXiv:2503.02918v2 Announce Type: replace 
Abstract: Diffusion-based models have shown great promise in molecular generation but often require a large number of sampling steps to generate valid samples. In this paper, we introduce a novel Straight-Line Diffusion Model (SLDM) to tackle this problem, by formulating the diffusion process to follow a linear trajectory. The proposed process aligns well with the noise sensitivity characteristic of molecular structures and uniformly distributes reconstruction effort across the generative process, thus enhancing learning efficiency and efficacy. Consequently, SLDM achieves state-of-the-art performance on 3D molecule generation benchmarks, delivering a 100-fold improvement in sampling efficiency.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>State-offset Tuning: State-based Parameter-Efficient Fine-Tuning for State Space Models</title>
<link>https://arxiv.org/abs/2503.03499</link>
<guid>https://arxiv.org/abs/2503.03499</guid>
<content:encoded><![CDATA[
arXiv:2503.03499v2 Announce Type: replace 
Abstract: State Space Models (SSMs) have emerged as efficient alternatives to Transformers, mitigating their quadratic computational cost. However, the application of Parameter-Efficient Fine-Tuning (PEFT) methods to SSMs remains largely unexplored. In particular, prompt-based methods like Prompt Tuning and Prefix-Tuning, which are widely used in Transformers, do not perform well on SSMs. To address this, we propose state-based methods as a superior alternative to prompt-based methods. This new family of methods naturally stems from the architectural characteristics of SSMs. State-based methods adjust state-related features directly instead of depending on external prompts. Furthermore, we introduce a novel state-based PEFT method: State-offset Tuning. At every timestep, our method directly affects the state at the current step, leading to more effective adaptation. Through extensive experiments across diverse datasets, we demonstrate the effectiveness of our method. Code is available at https://github.com/furiosa-ai/ssm-state-tuning.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accurate INT8 Training Through Dynamic Block-Level Fallback</title>
<link>https://arxiv.org/abs/2503.08040</link>
<guid>https://arxiv.org/abs/2503.08040</guid>
<content:encoded><![CDATA[
arXiv:2503.08040v3 Announce Type: replace 
Abstract: Transformer models have achieved remarkable success across various AI applications but face significant training costs. Low-bit training, such as INT8 training, can leverage computational units with higher throughput, and has already demonstrated its effectiveness on GPT2 models with block-level quantization. However, it struggles with modern Transformer variants incorporating GLU units. This is because those variants demonstrate complex distributions of activation outliers. To address the challenge, we propose Fallback Quantization, implementing mixed-precision GEMM that dynamically falls back 8-bit to 16-bit for activation blocks containing outliers. Experiments show that our approach is robustly competent in both fine-tuning and pretraining settings. Moreover, our method achieves a 1.57x end-to-end training speedup on RTX4090 GPUs.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extragradient Preference Optimization (EGPO): Beyond Last-Iterate Convergence for Nash Learning from Human Feedback</title>
<link>https://arxiv.org/abs/2503.08942</link>
<guid>https://arxiv.org/abs/2503.08942</guid>
<content:encoded><![CDATA[
arXiv:2503.08942v2 Announce Type: replace 
Abstract: Reinforcement learning from human feedback (RLHF) has become essential for improving language model capabilities, but traditional approaches rely on the assumption that human preferences follow a transitive Bradley-Terry model. This assumption fails to capture the non-transitive nature of populational human preferences. Nash learning from human feedback (NLHF), targeting non-transitive preferences, is a problem of computing the Nash equilibrium (NE) of the two-player constant-sum game defined by the human preference. We introduce Extragradient preference optimization (EGPO), a novel algorithm for NLHF achieving last-iterate linear convergence to the NE of KL-regularized games and polynomial convergence to the NE of original games, while being robust to noise. Unlike previous approaches that rely on nested optimization, we derive an equivalent implementation using gradients of an online variant of the identity preference optimization (IPO) loss, enabling more faithful implementation for neural networks. Our empirical evaluations demonstrate EGPO's superior performance over baseline methods when training for the same number of epochs, as measured by pairwise win-rates using the ground truth preference. These results validate both the theoretical strengths and practical advantages of EGPO for language model alignment with non-transitive human preferences.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting semi-supervised learning in the era of foundation models</title>
<link>https://arxiv.org/abs/2503.09707</link>
<guid>https://arxiv.org/abs/2503.09707</guid>
<content:encoded><![CDATA[
arXiv:2503.09707v2 Announce Type: replace 
Abstract: Semi-supervised learning (SSL) leverages abundant unlabeled data alongside limited labeled data to enhance learning. As vision foundation models (VFMs) increasingly serve as the backbone of vision applications, it remains unclear how SSL interacts with these pre-trained models. To address this gap, we develop new SSL benchmark datasets where frozen VFMs underperform and systematically evaluate representative SSL methods. We make a surprising observation: parameter-efficient fine-tuning (PEFT) using only labeled data often matches SSL performance, even without leveraging unlabeled data. This motivates us to revisit self-training, a conceptually simple SSL baseline, where we use the supervised PEFT model to pseudo-label unlabeled data for further training. To overcome the notorious issue of noisy pseudo-labels, we propose ensembling multiple PEFT approaches and VFM backbones to produce more robust pseudo-labels. Empirical results validate the effectiveness of this simple yet powerful approach, providing actionable insights into SSL with VFMs and paving the way for more scalable and practical semi-supervised learning in the era of foundation models.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedALT: Federated Fine-Tuning through Adaptive Local Training with Rest-of-World LoRA</title>
<link>https://arxiv.org/abs/2503.11880</link>
<guid>https://arxiv.org/abs/2503.11880</guid>
<content:encoded><![CDATA[
arXiv:2503.11880v2 Announce Type: replace 
Abstract: Fine-tuning large language models (LLMs) in federated settings enables privacy-preserving adaptation but suffers from cross-client interference due to model aggregation. Existing federated LoRA fine-tuning methods, primarily based on FedAvg, struggle with data heterogeneity, leading to harmful cross-client interference and suboptimal personalization. In this work, we propose \textbf{FedALT}, a novel personalized federated LoRA fine-tuning algorithm that fundamentally departs from FedAvg. Instead of using an aggregated model to initialize local training, each client continues training its individual LoRA while incorporating shared knowledge through a separate Rest-of-World (RoW) LoRA component. To effectively balance local adaptation and global information, FedALT introduces an adaptive mixer that dynamically learns input-specific weightings between the individual and RoW LoRA components, drawing conceptual foundations from the Mixture-of-Experts (MoE) paradigm. Through extensive experiments on NLP benchmarks, we demonstrate that FedALT significantly outperforms state-of-the-art personalized federated LoRA fine-tuning methods, achieving superior local adaptation without sacrificing computational efficiency.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Achieving Perfect Multimodal Alignment</title>
<link>https://arxiv.org/abs/2503.15352</link>
<guid>https://arxiv.org/abs/2503.15352</guid>
<content:encoded><![CDATA[
arXiv:2503.15352v2 Announce Type: replace 
Abstract: Multimodal alignment constructs a joint latent vector space where modalities representing the same concept map to neighboring latent vectors. We formulate this as an inverse problem and show that, under certain conditions, paired data from each modality can map to equivalent latent vectors, which we refer to as perfect alignment. When perfect alignment cannot be achieved, it can be approximated using the Singular Value Decomposition (SVD) of a multimodal data matrix. Experiments on synthetic multimodal Gaussian data verify the effectiveness of our perfect alignment method compared to a learned contrastive alignment method. We further demonstrate the practical application of cross-modal transfer for human action recognition, showing that perfect alignment significantly enhances the model's accuracy. We conclude by discussing how these findings can be applied to various modalities and tasks and the limitations of our method. We hope these findings inspire further exploration of perfect alignment and its applications in representation learning.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KEA: Keeping Exploration Alive by Proactively Coordinating Exploration Strategies</title>
<link>https://arxiv.org/abs/2503.18234</link>
<guid>https://arxiv.org/abs/2503.18234</guid>
<content:encoded><![CDATA[
arXiv:2503.18234v2 Announce Type: replace 
Abstract: Soft Actor-Critic (SAC) has achieved notable success in continuous control tasks but struggles in sparse reward settings, where infrequent rewards make efficient exploration challenging. While novelty-based exploration methods address this issue by encouraging the agent to explore novel states, they are not trivial to apply to SAC. In particular, managing the interaction between novelty-based exploration and SAC's stochastic policy can lead to inefficient exploration and redundant sample collection. In this paper, we propose KEA (Keeping Exploration Alive) which tackles the inefficiencies in balancing exploration strategies when combining SAC with novelty-based exploration. KEA integrates a novelty-augmented SAC with a standard SAC agent, proactively coordinated via a switching mechanism. This coordination allows the agent to maintain stochasticity in high-novelty regions, enhancing exploration efficiency and reducing repeated sample collection. We first analyze this potential issue in a 2D navigation task, and then evaluate KEA on the DeepSea hard-exploration benchmark as well as sparse reward control tasks from the DeepMind Control Suite. Compared to state-of-the-art novelty-based exploration baselines, our experiments show that KEA significantly improves learning efficiency and robustness in sparse reward setups.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feature Statistics with Uncertainty Help Adversarial Robustness</title>
<link>https://arxiv.org/abs/2503.20583</link>
<guid>https://arxiv.org/abs/2503.20583</guid>
<content:encoded><![CDATA[
arXiv:2503.20583v2 Announce Type: replace 
Abstract: Despite the remarkable success of deep neural networks (DNNs), the security threat of adversarial attacks poses a significant challenge to the reliability of DNNs. In this paper, both theoretically and empirically, we discover a universal phenomenon that has been neglected in previous works, i.e., adversarial attacks tend to shift the distributions of feature statistics. Motivated by this finding, and by leveraging the advantages of uncertainty-aware stochastic methods in building robust models efficiently, we propose an uncertainty-driven feature statistics adjustment module for robustness enhancement, named Feature Statistics with Uncertainty (FSU). It randomly resamples channel-wise feature means and standard deviations of examples from multivariate Gaussian distributions, which helps to reconstruct the perturbed examples and calibrate the shifted distributions. The calibration recovers some domain characteristics of the data for classification, thereby mitigating the influence of perturbations and weakening the ability of attacks to deceive models. The proposed FSU module has universal applicability in training, attacking, predicting, and fine-tuning, demonstrating impressive robustness enhancement ability at a trivial additional time cost. For example, by fine-tuning the well-established models with FSU, the state-of-the-art methods achieve up to 17.13% and 34.82% robustness improvement against powerful AA and CW attacks on benchmark datasets.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representation Bending for Large Language Model Safety</title>
<link>https://arxiv.org/abs/2504.01550</link>
<guid>https://arxiv.org/abs/2504.01550</guid>
<content:encoded><![CDATA[
arXiv:2504.01550v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have emerged as powerful tools, but their inherent safety risks - ranging from harmful content generation to broader societal harms - pose significant challenges. These risks can be amplified by the recent adversarial attacks, fine-tuning vulnerabilities, and the increasing deployment of LLMs in high-stakes environments. Existing safety-enhancing techniques, such as fine-tuning with human feedback or adversarial training, are still vulnerable as they address specific threats and often fail to generalize across unseen attacks, or require manual system-level defenses. This paper introduces RepBend, a novel approach that fundamentally disrupts the representations underlying harmful behaviors in LLMs, offering a scalable solution to enhance (potentially inherent) safety. RepBend brings the idea of activation steering - simple vector arithmetic for steering model's behavior during inference - to loss-based fine-tuning. Through extensive evaluation, RepBend achieves state-of-the-art performance, outperforming prior methods such as Circuit Breaker, RMU, and NPO, with up to 95% reduction in attack success rates across diverse jailbreak benchmarks, all with negligible reduction in model usability and general capabilities.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Free Random Projection for In-Context Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.06983</link>
<guid>https://arxiv.org/abs/2504.06983</guid>
<content:encoded><![CDATA[
arXiv:2504.06983v2 Announce Type: replace 
Abstract: Hierarchical inductive biases are hypothesized to promote generalizable policies in reinforcement learning, as demonstrated by explicit hyperbolic latent representations and architectures. Therefore, a more flexible approach is to have these biases emerge naturally from the algorithm. We introduce Free Random Projection, an input mapping grounded in free probability theory that constructs random orthogonal matrices where hierarchical structure arises inherently. The free random projection integrates seamlessly into existing in-context reinforcement learning frameworks by encoding hierarchical organization within the input space without requiring explicit architectural modifications. Empirical results on multi-environment benchmarks show that free random projection consistently outperforms the standard random projection, leading to improvements in generalization. Furthermore, analyses within linearly solvable Markov decision processes and investigations of the spectrum of kernel random matrices reveal the theoretical underpinnings of free random projection's enhanced performance, highlighting its capacity for effective adaptation in hierarchically structured state spaces.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Framework of decision-relevant observability: Reinforcement Learning converges under relative ignorability</title>
<link>https://arxiv.org/abs/2504.07722</link>
<guid>https://arxiv.org/abs/2504.07722</guid>
<content:encoded><![CDATA[
arXiv:2504.07722v4 Announce Type: replace 
Abstract: From clinical dosing algorithms to autonomous robots, sequential decision-making systems routinely operate with missing or incomplete data. Classical reinforcement learning theory, which is commonly used to solve sequential decision problems, assumes Markovian observability, which may not hold under partial observability. Causal inference paradigms formalise ignorability of missingness. We show these views can be unified and generalized in order to guarantee Q-learning convergence even when the Markov property fails. To do so, we introduce the concept of \emph{relative ignorability}. Relative ignorability is a graphical-causal criterion which refines the requirements for accurate decision-making based on incomplete data. Theoretical results and simulations both reveal that non-markovian stochastic processes whose missingness is relatively ignorable with respect to causal estimands can still be optimized using standard Reinforcement Learning algorithms. These results expand the theoretical foundations of safe, data-efficient AI to real-world environments where complete information is unattainable.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regretful Decisions under Label Noise</title>
<link>https://arxiv.org/abs/2504.09330</link>
<guid>https://arxiv.org/abs/2504.09330</guid>
<content:encoded><![CDATA[
arXiv:2504.09330v2 Announce Type: replace 
Abstract: Machine learning models are routinely used to support decisions that affect individuals -- be it to screen a patient for a serious illness or to gauge their response to treatment. In these tasks, we are limited to learning models from datasets with noisy labels. In this paper, we study the instance-level impact of learning under label noise. We introduce a notion of regret for this regime, which measures the number of unforeseen mistakes due to noisy labels. We show that standard approaches to learning under label noise can return models that perform well at a population-level while subjecting individuals to a lottery of mistakes. We present a versatile approach to estimate the likelihood of mistakes at the individual-level from a noisy dataset by training models over plausible realizations of datasets without label noise. This is supported by a comprehensive empirical study of label noise in clinical prediction tasks. Our results reveal how failure to anticipate mistakes can compromise model reliability and adoption -- we demonstrate how we can address these challenges by anticipating and avoiding regretful decisions.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bipartite Ranking From Multiple Labels: On Loss Versus Label Aggregation</title>
<link>https://arxiv.org/abs/2504.11284</link>
<guid>https://arxiv.org/abs/2504.11284</guid>
<content:encoded><![CDATA[
arXiv:2504.11284v2 Announce Type: replace 
Abstract: Bipartite ranking is a fundamental supervised learning problem, with the goal of learning a ranking over instances with maximal Area Under the ROC Curve (AUC) against a single binary target label. However, one may often observe multiple binary target labels, e.g., from distinct human annotators. How can one synthesize such labels into a single coherent ranking? In this work, we formally analyze two approaches to this problem -- loss aggregation and label aggregation -- by characterizing their Bayes-optimal solutions. We show that while both approaches can yield Pareto-optimal solutions, loss aggregation can exhibit label dictatorship: one can inadvertently (and undesirably) favor one label over others. This suggests that label aggregation can be preferable to loss aggregation, which we empirically verify.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedX: Adaptive Model Decomposition and Quantization for IoT Federated Learning</title>
<link>https://arxiv.org/abs/2504.12849</link>
<guid>https://arxiv.org/abs/2504.12849</guid>
<content:encoded><![CDATA[
arXiv:2504.12849v3 Announce Type: replace 
Abstract: Federated Learning (FL) allows collaborative training among multiple devices without data sharing, thus enabling privacy-sensitive applications on mobile or Internet of Things (IoT) devices, such as mobile health and asset tracking. However, designing an FL system with good model utility that works with low computation/communication overhead on heterogeneous, resource-constrained mobile/IoT devices is challenging. To address this problem, this paper proposes FedX, a novel adaptive model decomposition and quantization FL system for IoT. To balance utility with resource constraints on IoT devices, FedX decomposes a global FL model into different sub-networks with adaptive numbers of quantized bits for different devices. The key idea is that a device with fewer resources receives a smaller sub-network for lower overhead but utilizes a larger number of quantized bits for higher model utility, and vice versa. The quantization operations in FedX are done at the server to reduce the computational load on devices. FedX iteratively minimizes the losses in the devices' local data and in the server's public data using quantized sub-networks under a regularization term, and thus it maximizes the benefits of combining FL with model quantization through knowledge sharing among the server and devices in a cost-effective training process. Extensive experiments show that FedX significantly improves quantization times by up to 8.43X, on-device computation time by 1.5X, and total end-to-end training time by 1.36X, compared with baseline FL systems. We guarantee the global model convergence theoretically and validate local model convergence empirically, highlighting FedX's optimization efficiency.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIB: A Mechanistic Interpretability Benchmark</title>
<link>https://arxiv.org/abs/2504.13151</link>
<guid>https://arxiv.org/abs/2504.13151</guid>
<content:encoded><![CDATA[
arXiv:2504.13151v2 Announce Type: replace 
Abstract: How can we know whether new mechanistic interpretability methods achieve real improvements? In pursuit of lasting evaluation standards, we propose MIB, a Mechanistic Interpretability Benchmark, with two tracks spanning four tasks and five models. MIB favors methods that precisely and concisely recover relevant causal pathways or causal variables in neural language models. The circuit localization track compares methods that locate the model components - and connections between them - most important for performing a task (e.g., attribution patching or information flow routes). The causal variable localization track compares methods that featurize a hidden vector, e.g., sparse autoencoders (SAEs) or distributed alignment search (DAS), and align those features to a task-relevant causal variable. Using MIB, we find that attribution and mask optimization methods perform best on circuit localization. For causal variable localization, we find that the supervised DAS method performs best, while SAE features are not better than neurons, i.e., non-featurized hidden vectors. These findings illustrate that MIB enables meaningful comparisons, and increases our confidence that there has been real progress in the field.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STAMP Your Content: Proving Dataset Membership via Watermarked Rephrasings</title>
<link>https://arxiv.org/abs/2504.13416</link>
<guid>https://arxiv.org/abs/2504.13416</guid>
<content:encoded><![CDATA[
arXiv:2504.13416v2 Announce Type: replace 
Abstract: Given how large parts of publicly available text are crawled to pretrain large language models (LLMs), data creators increasingly worry about the inclusion of their proprietary data for model training without attribution or licensing. Their concerns are also shared by benchmark curators whose test-sets might be compromised. In this paper, we present STAMP, a framework for detecting dataset membership-i.e., determining the inclusion of a dataset in the pretraining corpora of LLMs. Given an original piece of content, our proposal involves first generating multiple rephrases, each embedding a watermark with a unique secret key. One version is to be released publicly, while others are to be kept private. Subsequently, creators can compare model likelihoods between public and private versions using paired statistical tests to prove membership. We show that our framework can successfully detect contamination across four benchmarks which appear only once in the training data and constitute less than 0.001% of the total tokens, outperforming several contamination detection and dataset inference baselines. We verify that STAMP preserves both the semantic meaning and utility of the original data. We apply STAMP to two real-world scenarios to confirm the inclusion of paper abstracts and blog articles in the pretraining corpora.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pushing the Limits of Low-Bit Optimizers: A Focus on EMA Dynamics</title>
<link>https://arxiv.org/abs/2505.00347</link>
<guid>https://arxiv.org/abs/2505.00347</guid>
<content:encoded><![CDATA[
arXiv:2505.00347v2 Announce Type: replace 
Abstract: The rapid scaling of models has led to prohibitively high training and fine-tuning costs. A major factor accounting for memory consumption is the widespread use of stateful optimizers (e.g., Adam), which maintain auxiliary information of even 2x the model size in order to achieve optimal convergence. We therefore present SOLO in this work to spawn a novel type of optimizer that requires an extremely light memory footprint. While previous efforts have achieved certain success in 8-bit or 4-bit cases, SOLO enables Adam-style optimizers to maintain quantized states with precision as low as 3 bits, or even 2 bits. This immense progress is due to the identification and resolution of two key challenges: the signal swamping problem in unsigned quantization that results in unchanged state dynamics, and the increased gradient variance in signed quantization that leads to incorrect descent directions. The theoretical analysis suggests a tailored logarithmic quantization for the former and a precision-specific momentum hyperparameter for the latter. SOLO can thus be seamlessly applied to Adam-style optimizers, leading to substantial memory savings with minimal accuracy loss.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Test-time Correlation Alignment</title>
<link>https://arxiv.org/abs/2505.00533</link>
<guid>https://arxiv.org/abs/2505.00533</guid>
<content:encoded><![CDATA[
arXiv:2505.00533v2 Announce Type: replace 
Abstract: Deep neural networks often degrade under distribution shifts. Although domain adaptation offers a solution, privacy constraints often prevent access to source data, making Test-Time Adaptation (TTA, which adapts using only unlabeled test data) increasingly attractive. However, current TTA methods still face practical challenges: (1) a primary focus on instance-wise alignment, overlooking CORrelation ALignment (CORAL) due to missing source correlations; (2) complex backpropagation operations for model updating, resulting in overhead computation and (3) domain forgetting. To address these challenges, we provide a theoretical analysis to investigate the feasibility of Test-time Correlation Alignment (TCA), demonstrating that correlation alignment between high-certainty instances and test instances can enhance test performances with a theoretical guarantee. Based on this, we propose two simple yet effective algorithms: LinearTCA and LinearTCA+. LinearTCA applies a simple linear transformation to achieve both instance and correlation alignment without additional model updates, while LinearTCA+ serves as a plug-and-play module that can easily boost existing TTA methods. Extensive experiments validate our theoretical insights and show that TCA methods significantly outperforms baselines across various tasks, benchmarks and backbones. Notably, LinearTCA achieves higher accuracy with only 4% GPU memory and 0.6% computation time compared to the best TTA baseline. It also outperforms existing methods on CLIP over 1.86%.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Directly Forecasting Belief for Reinforcement Learning with Delays</title>
<link>https://arxiv.org/abs/2505.00546</link>
<guid>https://arxiv.org/abs/2505.00546</guid>
<content:encoded><![CDATA[
arXiv:2505.00546v2 Announce Type: replace 
Abstract: Reinforcement learning (RL) with delays is challenging as sensory perceptions lag behind the actual events: the RL agent needs to estimate the real state of its environment based on past observations. State-of-the-art (SOTA) methods typically employ recursive, step-by-step forecasting of states. This can cause the accumulation of compounding errors. To tackle this problem, our novel belief estimation method, named Directly Forecasting Belief Transformer (DFBT), directly forecasts states from observations without incrementally estimating intermediate states step-by-step. We theoretically demonstrate that DFBT greatly reduces compounding errors of existing recursively forecasting methods, yielding stronger performance guarantees. In experiments with D4RL offline datasets, DFBT reduces compounding errors with remarkable prediction accuracy. DFBT's capability to forecast state sequences also facilitates multi-step bootstrapping, thus greatly improving learning efficiency. On the MuJoCo benchmark, our DFBT-based method substantially outperforms SOTA baselines. Code is available at https://github.com/QingyuanWuNothing/DFBT.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tree-Sliced Wasserstein Distance with Nonlinear Projection</title>
<link>https://arxiv.org/abs/2505.00968</link>
<guid>https://arxiv.org/abs/2505.00968</guid>
<content:encoded><![CDATA[
arXiv:2505.00968v2 Announce Type: replace 
Abstract: Tree-Sliced methods have recently emerged as an alternative to the traditional Sliced Wasserstein (SW) distance, replacing one-dimensional lines with tree-based metric spaces and incorporating a splitting mechanism for projecting measures. This approach enhances the ability to capture the topological structures of integration domains in Sliced Optimal Transport while maintaining low computational costs. Building on this foundation, we propose a novel nonlinear projectional framework for the Tree-Sliced Wasserstein (TSW) distance, substituting the linear projections in earlier versions with general projections, while ensuring the injectivity of the associated Radon Transform and preserving the well-definedness of the resulting metric. By designing appropriate projections, we construct efficient metrics for measures on both Euclidean spaces and spheres. Finally, we validate our proposed metric through extensive numerical experiments for Euclidean and spherical datasets. Applications include gradient flows, self-supervised learning, and generative models, where our methods demonstrate significant improvements over recent SW and TSW variants.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LookAlike: Consistent Distractor Generation in Math MCQs</title>
<link>https://arxiv.org/abs/2505.01903</link>
<guid>https://arxiv.org/abs/2505.01903</guid>
<content:encoded><![CDATA[
arXiv:2505.01903v2 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly used to generate distractors for multiple-choice questions (MCQs), especially in domains like math education. However, existing approaches are limited in ensuring that the generated distractors are consistent with common student errors. We propose LookAlike, a method that improves error-distractor consistency via preference optimization. Our two main innovations are: (a) mining synthetic preference pairs from model inconsistencies, and (b) alternating supervised fine-tuning (SFT) with Direct Preference Optimization (DPO) to stabilize training. Unlike prior work that relies on heuristics or manually annotated preference data, LookAlike uses its own generation inconsistencies as dispreferred samples, thus enabling scalable and stable training. Evaluated on a real-world dataset of 1,400+ math MCQs, LookAlike achieves 51.6% accuracy in distractor generation and 57.2% in error generation under LLM-as-a-judge evaluation, outperforming an existing state-of-the-art method (45.6% / 47.7%). These improvements highlight the effectiveness of preference-based regularization and inconsistency mining for generating consistent math MCQ distractors at scale.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Restoring Calibration for Aligned Large Language Models: A Calibration-Aware Fine-Tuning Approach</title>
<link>https://arxiv.org/abs/2505.01997</link>
<guid>https://arxiv.org/abs/2505.01997</guid>
<content:encoded><![CDATA[
arXiv:2505.01997v2 Announce Type: replace 
Abstract: One of the key technologies for the success of Large Language Models (LLMs) is preference alignment. However, a notable side effect of preference alignment is poor calibration: while the pre-trained models are typically well-calibrated, LLMs tend to become poorly calibrated after alignment with human preferences. In this paper, we investigate why preference alignment affects calibration and how to address this issue. For the first question, we observe that the preference collapse issue in alignment undesirably generalizes to the calibration scenario, causing LLMs to exhibit overconfidence and poor calibration. To address this, we demonstrate the importance of fine-tuning with domain-specific knowledge to alleviate the overconfidence issue. To further analyze whether this affects the model's performance, we categorize models into two regimes: calibratable and non-calibratable, defined by bounds of Expected Calibration Error (ECE). In the calibratable regime, we propose a calibration-aware fine-tuning approach to achieve proper calibration without compromising LLMs' performance. However, as models are further fine-tuned for better performance, they enter the non-calibratable regime. For this case, we develop an EM-algorithm-based ECE regularization for the fine-tuning loss to maintain low calibration error. Extensive experiments validate the effectiveness of the proposed methods.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: We Need Responsible, Application-Driven (RAD) AI Research</title>
<link>https://arxiv.org/abs/2505.04104</link>
<guid>https://arxiv.org/abs/2505.04104</guid>
<content:encoded><![CDATA[
arXiv:2505.04104v2 Announce Type: replace 
Abstract: This position paper argues that achieving meaningful scientific and societal advances with artificial intelligence (AI) requires a responsible, application-driven approach (RAD) to AI research. As AI is increasingly integrated into society, AI researchers must engage with the specific contexts where AI is being applied. This includes being responsive to ethical and legal considerations, technical and societal constraints, and public discourse. We present the case for RAD-AI to drive research through a three-staged approach: (1) building transdisciplinary teams and people-centred studies; (2) addressing context-specific methods, ethical commitments, assumptions, and metrics; and (3) testing and sustaining efficacy through staged testbeds and a community of practice. We present a vision for the future of application-driven AI research to unlock new value through technically feasible methods that are adaptive to the contextual needs and values of the communities they ultimately serve.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trial and Trust: Addressing Byzantine Attacks with Comprehensive Defense Strategy</title>
<link>https://arxiv.org/abs/2505.07614</link>
<guid>https://arxiv.org/abs/2505.07614</guid>
<content:encoded><![CDATA[
arXiv:2505.07614v2 Announce Type: replace 
Abstract: Recent advancements in machine learning have improved performance while also increasing computational demands. While federated and distributed setups address these issues, their structure is vulnerable to malicious influences. In this paper, we address a specific threat, Byzantine attacks, where compromised clients inject adversarial updates to derail global convergence. We combine the trust scores concept with trial function methodology to dynamically filter outliers. Our methods address the critical limitations of previous approaches, allowing functionality even when Byzantine nodes are in the majority. Moreover, our algorithms adapt to widely used scaled methods like Adam and RMSProp, as well as practical scenarios, including local training and partial participation. We validate the robustness of our methods by conducting extensive experiments on both synthetic and real ECG data collected from medical institutions. Furthermore, we provide a broad theoretical analysis of our algorithms and their extensions to aforementioned practical setups. The convergence guarantees of our methods are comparable to those of classical algorithms developed without Byzantine interference.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Nonlinear Implicit Bias via Region Counts in Input Space</title>
<link>https://arxiv.org/abs/2505.11370</link>
<guid>https://arxiv.org/abs/2505.11370</guid>
<content:encoded><![CDATA[
arXiv:2505.11370v2 Announce Type: replace 
Abstract: One explanation for the strong generalization ability of neural networks is implicit bias. Yet, the definition and mechanism of implicit bias in non-linear contexts remains little understood. In this work, we propose to characterize implicit bias by the count of connected regions in the input space with the same predicted label. Compared with parameter-dependent metrics (e.g., norm or normalized margin), region count can be better adapted to nonlinear, overparameterized models, because it is determined by the function mapping and is invariant to reparametrization. Empirically, we found that small region counts align with geometrically simple decision boundaries and correlate well with good generalization performance. We also observe that good hyper-parameter choices such as larger learning rates and smaller batch sizes can induce small region counts. We further establish the theoretical connections and explain how larger learning rate can induce small region counts in neural networks.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Q-Policy: Quantum-Enhanced Policy Evaluation for Scalable Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.11862</link>
<guid>https://arxiv.org/abs/2505.11862</guid>
<content:encoded><![CDATA[
arXiv:2505.11862v2 Announce Type: replace 
Abstract: We propose Q-Policy, a hybrid quantum-classical reinforcement learning (RL) framework that mathematically accelerates policy evaluation and optimization by exploiting quantum computing primitives. Q-Policy encodes value functions in quantum superposition, enabling simultaneous evaluation of multiple state-action pairs via amplitude encoding and quantum parallelism. We introduce a quantum-enhanced policy iteration algorithm with provable polynomial reductions in sample complexity for the evaluation step, under standard assumptions. To demonstrate the technical feasibility and theoretical soundness of our approach, we validate Q-Policy on classical emulations of small discrete control tasks. Due to current hardware and simulation limitations, our experiments focus on showcasing proof-of-concept behavior rather than large-scale empirical evaluation. Our results support the potential of Q-Policy as a theoretical foundation for scalable RL on future quantum devices, addressing RL scalability challenges beyond classical approaches.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Pareto-Optimal Rewards from Noisy Preferences: A Framework for Multi-Objective Inverse Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.11864</link>
<guid>https://arxiv.org/abs/2505.11864</guid>
<content:encoded><![CDATA[
arXiv:2505.11864v2 Announce Type: replace 
Abstract: As generative agents become increasingly capable, alignment of their behavior with complex human values remains a fundamental challenge. Existing approaches often simplify human intent through reduction to a scalar reward, overlooking the multi-faceted nature of human feedback. In this work, we introduce a theoretical framework for preference-based Multi-Objective Inverse Reinforcement Learning (MO-IRL), where human preferences are modeled as latent vector-valued reward functions. We formalize the problem of recovering a Pareto-optimal reward representation from noisy preference queries and establish conditions for identifying the underlying multi-objective structure. We derive tight sample complexity bounds for recovering $\epsilon$-approximations of the Pareto front and introduce a regret formulation to quantify suboptimality in this multi-objective setting. Furthermore, we propose a provably convergent algorithm for policy optimization using preference-inferred reward cones. Our results bridge the gap between practical alignment techniques and theoretical guarantees, providing a principled foundation for learning aligned behaviors in a high-dimension and value-pluralistic environment.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing On-Device Large Language Model: Empirical Results and Implications for AI PC</title>
<link>https://arxiv.org/abs/2505.15030</link>
<guid>https://arxiv.org/abs/2505.15030</guid>
<content:encoded><![CDATA[
arXiv:2505.15030v3 Announce Type: replace 
Abstract: The increasing deployment of Large Language Models (LLMs) on edge devices, driven by model advancements and hardware improvements, offers significant privacy benefits. However, these on-device LLMs inherently face performance limitations due to reduced model capacity and necessary compression techniques. To address this, we introduce a systematic methodology -- encompassing model capability, development efficiency, and system resources -- for evaluating on-device LLMs. Our comprehensive evaluation, encompassing models from 0.5B to 14B parameters and seven post-training quantization (PTQ) methods on commodity laptops, yields several critical insights: 1) System-level metrics exhibit near-linear scaling with effective bits-per-weight (BPW). 2) A practical threshold exists around $\sim$3.5 effective BPW, larger models subjected to low-bit quantization consistently outperform smaller models utilizing higher bit-precision. 3) Quantization with low BPW incurs marginal accuracy loss but significant memory savings. 4) Determined by low-level implementation specifics power consumption on CPU, where computation-intensive operations spend more power than memory-intensive ones. These findings offer crucial insights and practical guidelines for the efficient deployment and optimized configuration of LLMs on resource-constrained edge devices. Our codebase is available at https://github.com/simmonssong/LLMOnDevice.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neighbour-Driven Gaussian Process Variational Autoencoders for Scalable Structured Latent Modelling</title>
<link>https://arxiv.org/abs/2505.16481</link>
<guid>https://arxiv.org/abs/2505.16481</guid>
<content:encoded><![CDATA[
arXiv:2505.16481v2 Announce Type: replace 
Abstract: Gaussian Process (GP) Variational Autoencoders (VAEs) extend standard VAEs by replacing the fully factorised Gaussian prior with a GP prior, thereby capturing richer correlations among latent variables. However, performing exact GP inference in large-scale GPVAEs is computationally prohibitive, often forcing existing approaches to rely on restrictive kernel assumptions or large sets of inducing points. In this work, we propose a neighbour-driven approximation strategy that exploits local adjacencies in the latent space to achieve scalable GPVAE inference. By confining computations to the nearest neighbours of each data point, our method preserves essential latent dependencies, allowing more flexible kernel choices and mitigating the need for numerous inducing points. Through extensive experiments on tasks including representation learning, data imputation, and conditional generation, we demonstrate that our approach outperforms other GPVAE variants in both predictive performance and computational efficiency.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention with Trained Embeddings Provably Selects Important Tokens</title>
<link>https://arxiv.org/abs/2505.17282</link>
<guid>https://arxiv.org/abs/2505.17282</guid>
<content:encoded><![CDATA[
arXiv:2505.17282v2 Announce Type: replace 
Abstract: Token embeddings play a crucial role in language modeling but, despite this practical relevance, their theoretical understanding remains limited. Our paper addresses the gap by characterizing the structure of embeddings obtained via gradient descent. Specifically, we consider a one-layer softmax attention model with a linear head for binary classification, i.e., $\texttt{Softmax}( p^\top E_X^\top ) E_X v = \frac{ \sum_{i=1}^T \exp(p^\top E_{x_i}) E_{x_i}^\top v}{\sum_{j=1}^T \exp(p^\top E_{x_{j}}) }$, where $E_X = [ E_{x_1} , \dots, E_{x_T} ]^\top$ contains the embeddings of the input sequence, $p$ is the embedding of the $\mathrm{\langle cls \rangle}$ token and $v$ the output vector. First, we show that, already after a single step of gradient training with the logistic loss, the embeddings $E_X$ capture the importance of tokens in the dataset by aligning with the output vector $v$ proportionally to the frequency with which the corresponding tokens appear in the dataset. Then, after training $p$ via gradient flow until convergence, the softmax selects the important tokens in the sentence (i.e., those that are predictive of the label), and the resulting $\mathrm{\langle cls \rangle}$ embedding maximizes the margin for such a selection. Experiments on real-world datasets (IMDB, Yelp) exhibit a phenomenology close to that unveiled by our theory.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient Nonlinear MCMC on General Graphs</title>
<link>https://arxiv.org/abs/2505.18300</link>
<guid>https://arxiv.org/abs/2505.18300</guid>
<content:encoded><![CDATA[
arXiv:2505.18300v2 Announce Type: replace 
Abstract: We propose a history-driven target (HDT) framework in Markov Chain Monte Carlo (MCMC) to improve any random walk algorithm on discrete state spaces, such as general undirected graphs, for efficient sampling from target distribution $\boldsymbol{\mu}$. With broad applications in network science and distributed optimization, recent innovations like the self-repellent random walk (SRRW) achieve near-zero variance by prioritizing under-sampled states through transition kernel modifications based on past visit frequencies. However, SRRW's reliance on explicit computation of transition probabilities for all neighbors at each step introduces substantial computational overhead, while its strict dependence on time-reversible Markov chains excludes advanced non-reversible MCMC methods. To overcome these limitations, instead of direct modification of transition kernel, HDT introduces a history-dependent target distribution $\boldsymbol{\pi}[\mathbf{x}]$ to replace the original target $\boldsymbol{\mu}$ in any graph sampler, where $\mathbf{x}$ represents the empirical measure of past visits. This design preserves lightweight implementation by requiring only local information between the current and proposed states and achieves compatibility with both reversible and non-reversible MCMC samplers, while retaining unbiased samples with target distribution $\boldsymbol{\mu}$ and near-zero variance performance. Extensive experiments in graph sampling demonstrate consistent performance gains, and a memory-efficient Least Recently Used (LRU) cache ensures scalability to large general graphs.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sample Complexity of Diffusion Model Training Without Empirical Risk Minimizer Access</title>
<link>https://arxiv.org/abs/2505.18344</link>
<guid>https://arxiv.org/abs/2505.18344</guid>
<content:encoded><![CDATA[
arXiv:2505.18344v2 Announce Type: replace 
Abstract: Diffusion models have demonstrated state-of-the-art performance across vision, language, and scientific domains. Despite their empirical success, prior theoretical analyses of the sample complexity suffer from poor scaling with input data dimension or rely on unrealistic assumptions such as access to exact empirical risk minimizers. In this work, we provide a principled analysis of score estimation, establishing a sample complexity bound of $\widetilde{\mathcal{O}}(\epsilon^{-6})$. Our approach leverages a structured decomposition of the score estimation error into statistical, approximation, and optimization errors, enabling us to eliminate the exponential dependence on neural network parameters that arises in prior analyses. It is the first such result which achieves sample complexity bounds without assuming access to the empirical risk minimizer of score function estimation loss.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Fluid-Structure Interaction Dynamics with Physics-Informed Neural Networks and Immersed Boundary Methods</title>
<link>https://arxiv.org/abs/2505.18565</link>
<guid>https://arxiv.org/abs/2505.18565</guid>
<content:encoded><![CDATA[
arXiv:2505.18565v2 Announce Type: replace 
Abstract: We introduce neural network architectures that combine physics-informed neural networks (PINNs) with the immersed boundary method (IBM) to solve fluid-structure interaction (FSI) problems. Our approach features two distinct architectures: a Single-FSI network with a unified parameter space, and an innovative Eulerian-Lagrangian network that maintains separate parameter spaces for fluid and structure domains. We study each architecture using standard Tanh and adaptive B-spline activation functions. Empirical studies on a 2D cavity flow problem involving a moving solid structure show that the Eulerian-Lagrangian architecture performs significantly better. The adaptive B-spline activation further enhances accuracy by providing locality-aware representation near boundaries. While our methodology shows promising results in predicting the velocity field, pressure recovery remains challenging due to the absence of explicit force-coupling constraints in the current formulation. Our findings underscore the importance of domain-specific architectural design and adaptive activation functions for modeling FSI problems within the PINN framework.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Turb-L1: Achieving Long-term Turbulence Tracing By Tackling Spectral Bias</title>
<link>https://arxiv.org/abs/2505.19038</link>
<guid>https://arxiv.org/abs/2505.19038</guid>
<content:encoded><![CDATA[
arXiv:2505.19038v2 Announce Type: replace 
Abstract: Accurately predicting the long-term evolution of turbulence is crucial for advancing scientific understanding and optimizing engineering applications. However, existing deep learning methods face significant bottlenecks in long-term autoregressive prediction, which exhibit excessive smoothing and fail to accurately track complex fluid dynamics. Our extensive experimental and spectral analysis of prevailing methods provides an interpretable explanation for this shortcoming, identifying Spectral Bias as the core obstacle. Concretely, spectral bias is the inherent tendency of models to favor low-frequency, smooth features while overlooking critical high-frequency details during training, thus reducing fidelity and causing physical distortions in long-term predictions. Building on this insight, we propose Turb-L1, an innovative turbulence prediction method, which utilizes a Hierarchical Dynamics Synthesis mechanism within a multi-grid architecture to explicitly overcome spectral bias. It accurately captures cross-scale interactions and preserves the fidelity of high-frequency dynamics, enabling reliable long-term tracking of turbulence evolution. Extensive experiments on the 2D turbulence benchmark show that Turb-L1 demonstrates excellent performance: (I) In long-term predictions, it reduces Mean Squared Error (MSE) by $80.3\%$ and increases Structural Similarity (SSIM) by over $9\times$ compared to the SOTA baseline, significantly improving prediction fidelity. (II) It effectively overcomes spectral bias, accurately reproducing the full enstrophy spectrum and maintaining physical realism in high-wavenumber regions, thus avoiding the spectral distortions or spurious energy accumulation seen in other methods.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Calibrated Value-Aware Model Learning with Probabilistic Environment Models</title>
<link>https://arxiv.org/abs/2505.22772</link>
<guid>https://arxiv.org/abs/2505.22772</guid>
<content:encoded><![CDATA[
arXiv:2505.22772v2 Announce Type: replace 
Abstract: The idea of value-aware model learning, that models should produce accurate value estimates, has gained prominence in model-based reinforcement learning. The MuZero loss, which penalizes a model's value function prediction compared to the ground-truth value function, has been utilized in several prominent empirical works in the literature. However, theoretical investigation into its strengths and weaknesses is limited. In this paper, we analyze the family of value-aware model learning losses, which includes the popular MuZero loss. We show that these losses, as normally used, are uncalibrated surrogate losses, which means that they do not always recover the correct model and value function. Building on this insight, we propose corrections to solve this issue. Furthermore, we investigate the interplay between the loss calibration, latent model architectures, and auxiliary losses that are commonly employed when training MuZero-style agents. We show that while deterministic models can be sufficient to predict accurate values, learning calibrated stochastic models is still advantageous.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Noise-Robustness Through Noise: Asymmetric LoRA Adaption with Poisoning Expert</title>
<link>https://arxiv.org/abs/2505.23868</link>
<guid>https://arxiv.org/abs/2505.23868</guid>
<content:encoded><![CDATA[
arXiv:2505.23868v3 Announce Type: replace 
Abstract: Current parameter-efficient fine-tuning methods for adapting pre-trained language models to downstream tasks are susceptible to interference from noisy data. Conventional noise-handling approaches either rely on laborious data pre-processing or employ model architecture modifications prone to error accumulation. In contrast to existing noise-process paradigms, we propose a noise-robust adaptation method via asymmetric LoRA poisoning experts (LoPE), a novel framework that enhances model robustness to noise only with generated noisy data. Drawing inspiration from the mixture-of-experts architecture, LoPE strategically integrates a dedicated poisoning expert in an asymmetric LoRA configuration. Through a two-stage paradigm, LoPE performs noise injection on the poisoning expert during fine-tuning to enhance its noise discrimination and processing ability. During inference, we selectively mask the dedicated poisoning expert to leverage purified knowledge acquired by normal experts for noise-robust output. Extensive experiments demonstrate that LoPE achieves strong performance and robustness purely through the low-cost noise injection, which completely eliminates the requirement of data cleaning.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Double Positive and Unlabeled Data for Potential-Customer Identification</title>
<link>https://arxiv.org/abs/2506.00436</link>
<guid>https://arxiv.org/abs/2506.00436</guid>
<content:encoded><![CDATA[
arXiv:2506.00436v2 Announce Type: replace 
Abstract: In this study, we propose a method for identifying potential customers in targeted marketing by applying learning from positive and unlabeled data (PU learning). We consider a scenario in which a company sells a product and can observe only the customers who purchased it. Decision-makers seek to market products effectively based on whether people have loyalty to the company. Individuals with loyalty are those who are likely to remain interested in the company even without additional advertising. Consequently, those loyal customers would likely purchase from the company if they are interested in the product. In contrast, people with lower loyalty may overlook the product or buy similar products from other companies unless they receive marketing attention. Therefore, by focusing marketing efforts on individuals who are interested in the product but do not have strong loyalty, we can achieve more efficient marketing. To achieve this goal, we consider how to learn, from limited data, a classifier that identifies potential customers who (i) have interest in the product and (ii) do not have loyalty to the company. Although our algorithm comprises a single-stage optimization, its objective function implicitly contains two losses derived from standard PU learning settings. For this reason, we refer to our approach as double PU learning. We verify the validity of the proposed algorithm through numerical experiments, confirming that it functions appropriately for the problem at hand.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributionally Robust Learning in Survival Analysis</title>
<link>https://arxiv.org/abs/2506.01348</link>
<guid>https://arxiv.org/abs/2506.01348</guid>
<content:encoded><![CDATA[
arXiv:2506.01348v2 Announce Type: replace 
Abstract: We introduce an innovative approach that incorporates a Distributionally Robust Learning (DRL) approach into Cox regression to enhance the robustness and accuracy of survival predictions. By formulating a DRL framework with a Wasserstein distance-based ambiguity set, we develop a variant Cox model that is less sensitive to assumptions about the underlying data distribution and more resilient to model misspecification and data perturbations. By leveraging Wasserstein duality, we reformulate the original min-max DRL problem into a tractable regularized empirical risk minimization problem, which can be computed by exponential conic programming. We provide guarantees on the finite sample behavior of our DRL-Cox model. Moreover, through extensive simulations and real world case studies, we demonstrate that our regression model achieves superior performance in terms of prediction accuracy and robustness compared with traditional methods.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IF-GUIDE: Influence Function-Guided Detoxification of LLMs</title>
<link>https://arxiv.org/abs/2506.01790</link>
<guid>https://arxiv.org/abs/2506.01790</guid>
<content:encoded><![CDATA[
arXiv:2506.01790v2 Announce Type: replace 
Abstract: We study how training data contributes to the emergence of toxic behaviors in large-language models. Most prior work on reducing model toxicity adopts $reactive$ approaches, such as fine-tuning pre-trained (and potentially toxic) models to align them with human values. In contrast, we propose a $proactive$ approach$-$IF-Guide$-$which leverages influence functions to identify harmful tokens within any training data and suppress their impact during training. To this end, we first show that standard influence functions are ineffective at discovering harmful training records. We then present a novel adaptation that measures token-level attributions from training data to model toxicity, along with techniques for selecting toxic training documents and a learning objective that can be integrated into both pre-training and fine-tuning. Moreover, IF-Guide does not rely on human-preference data, which is typically required by existing alignment methods. In evaluation, we demonstrate that IF-Guide substantially reduces both explicit and implicit toxicity$-$by up to 10$\times$ compared to uncensored models, and up to 3$\times$ compared to baseline alignment methods, e.g., DPO and RAD$-$across both pre-training and fine-tuning scenarios. IF-Guide is computationally efficient: a billion-parameter model is $not$ $necessary$ for computing influence scores; a million-parameter model$-$with 7.5$\times$ fewer parameters$-$can effectively serve as a proxy for identifying harmful data. Our code is publicly available at: https://github.com/ztcoalson/IF-Guide
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MINT: Multimodal Instruction Tuning with Multimodal Interaction Grouping</title>
<link>https://arxiv.org/abs/2506.02308</link>
<guid>https://arxiv.org/abs/2506.02308</guid>
<content:encoded><![CDATA[
arXiv:2506.02308v3 Announce Type: replace 
Abstract: Recent advances in multimodal foundation models have achieved state-of-the-art performance across a range of tasks. These breakthroughs are largely driven by new pre-training paradigms that leverage large-scale, unlabeled multimodal data, followed by instruction fine-tuning on curated labeled datasets and high-quality prompts. While there is growing interest in scaling instruction fine-tuning to ever-larger datasets in both quantity and scale, our findings reveal that simply increasing the number of instruction-tuning tasks does not consistently yield better performance. Instead, we observe that grouping tasks by the common interactions across modalities, such as discovering redundant shared information, prioritizing modality selection with unique information, or requiring synergistic fusion to discover new information from both modalities, encourages the models to learn transferrable skills within a group while suppressing interference from mismatched tasks. To this end, we introduce MINT, a simple yet surprisingly effective task-grouping strategy based on the type of multimodal interaction. We demonstrate that the proposed method greatly outperforms existing task grouping baselines for multimodal instruction tuning, striking an effective balance between generalization and specialization.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comba: Improving Bilinear RNNs with Closed-loop Control</title>
<link>https://arxiv.org/abs/2506.02475</link>
<guid>https://arxiv.org/abs/2506.02475</guid>
<content:encoded><![CDATA[
arXiv:2506.02475v2 Announce Type: replace 
Abstract: Recent efficient sequence modeling methods such as Gated DeltaNet, TTT, and RWKV-7 have achieved performance improvements by supervising the recurrent memory management through Delta learning rule. Unlike previous state-space models (e.g., Mamba) and gated linear attentions (e.g., GLA), these models introduce interactions between the recurrent state and the key vector, structurally resembling bilinear systems. In this paper, we first introduce the concept of Bilinear RNNs with a comprehensive analysis on the advantages and limitations of these models. Then, based on closed-loop control theory, we propose a novel Bilinear RNN variant named Comba, which adopts a scalar-plus-low-rank state transition, with both state feedback and output feedback corrections. We also implement a hardware-efficient chunk-wise parallel kernel in Triton and train models with 340M/1.3B parameters on large-scale corpus. Comba demonstrates superior performance and computation efficiency in both language and vision modeling.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval-Augmented Generation as Noisy In-Context Learning: A Unified Theory and Risk Bounds</title>
<link>https://arxiv.org/abs/2506.03100</link>
<guid>https://arxiv.org/abs/2506.03100</guid>
<content:encoded><![CDATA[
arXiv:2506.03100v3 Announce Type: replace 
Abstract: Retrieval-augmented generation (RAG) has seen many empirical successes in recent years by aiding the LLM with external knowledge. However, its theoretical aspect has remained mostly unexplored. In this paper, we propose the first finite-sample generalization bound for RAG in in-context linear regression and derive an exact bias-variance tradeoff. Our framework views the retrieved texts as query-dependent noisy in-context examples and recovers the classical in-context learning (ICL) and standard RAG as the limit cases. Our analysis suggests that an intrinsic ceiling on generalization error exists on RAG as opposed to the ICL. Furthermore, our framework is able to model retrieval both from the training data and from external corpora by introducing uniform and non-uniform RAG noise. In line with our theory, we show the sample efficiency of ICL and RAG empirically with experiments on common QA benchmarks, such as Natural Questions and TriviaQA.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Project Spatial Coordinates into Pavement Management Prioritization</title>
<link>https://arxiv.org/abs/1811.03437</link>
<guid>https://arxiv.org/abs/1811.03437</guid>
<content:encoded><![CDATA[
arXiv:1811.03437v2 Announce Type: replace-cross 
Abstract: To date, pavement management software products and studies on optimizing the prioritization of pavement maintenance and rehabilitation (M&amp;R) have been mainly focused on three parameters; the pre-treatment pavement condition, the rehabilitation cost, and the available budget. Yet, the role of the candidate projects' spatial characteristics in the decision-making process has not been deeply considered. Such a limitation, predominately, allows the recommended M&amp;R projects' schedule to involve simultaneously running but spatially scattered construction sites, which are very challenging to monitor and manage. This study introduces a novel approach to integrate pavement segments' spatial coordinates into the M&amp;R prioritization analysis. The introduced approach aims at combining the pavement segments with converged spatial coordinates to be repaired in the same timeframe without compromising the allocated budget levels or the overall target Pavement Condition Index (PCI). Such a combination would result in minimizing the routing of crews, materials and other equipment among the construction sites and would provide better collaborations and communications between the pavement maintenance teams. Proposed herein is a novel spatial clustering algorithm that automatically finds the projects within a certain budget and spatial constrains. The developed algorithm was successfully validated using 1,800 pavement maintenance projects from two real-life examples of the City of Milton, GA and the City of Tyler, TX.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Highly Fast Text Segmentation With Pairwise Markov Chains</title>
<link>https://arxiv.org/abs/2102.11037</link>
<guid>https://arxiv.org/abs/2102.11037</guid>
<content:encoded><![CDATA[
arXiv:2102.11037v2 Announce Type: replace-cross 
Abstract: Natural Language Processing (NLP) models' current trend consists of using increasingly more extra-data to build the best models as possible. It implies more expensive computational costs and training time, difficulties for deployment, and worries about these models' carbon footprint reveal a critical problem in the future. Against this trend, our goal is to develop NLP models requiring no extra-data and minimizing training time. To do so, in this paper, we explore Markov chain models, Hidden Markov Chain (HMC) and Pairwise Markov Chain (PMC), for NLP segmentation tasks. We apply these models for three classic applications: POS Tagging, Named-Entity-Recognition, and Chunking. We develop an original method to adapt these models for text segmentation's specific challenges to obtain relevant performances with very short training and execution times. PMC achieves equivalent results to those obtained by Conditional Random Fields (CRF), one of the most applied models for these tasks when no extra-data are used. Moreover, PMC has training times 30 times shorter than the CRF ones, which validates this model given our objectives.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Counterfactual inference in sequential experiments</title>
<link>https://arxiv.org/abs/2202.06891</link>
<guid>https://arxiv.org/abs/2202.06891</guid>
<content:encoded><![CDATA[
arXiv:2202.06891v5 Announce Type: replace-cross 
Abstract: We consider after-study statistical inference for sequentially designed experiments wherein multiple units are assigned treatments for multiple time points using treatment policies that adapt over time. Our goal is to provide inference guarantees for the counterfactual mean at the smallest possible scale -- mean outcome under different treatments for each unit and each time -- with minimal assumptions on the adaptive treatment policy. Without any structural assumptions on the counterfactual means, this challenging task is infeasible due to more unknowns than observed data points. To make progress, we introduce a latent factor model over the counterfactual means that serves as a non-parametric generalization of the non-linear mixed effects model and the bilinear latent factor model considered in prior works. For estimation, we use a non-parametric method, namely a variant of nearest neighbors, and establish a non-asymptotic high probability error bound for the counterfactual mean for each unit and each time. Under regularity conditions, this bound leads to asymptotically valid confidence intervals for the counterfactual mean as the number of units and time points grows to $\infty$ together at suitable rates. We illustrate our theory via several simulations and a case study involving data from a mobile health clinical trial HeartSteps.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Hypothesis Transfer Learning of Functional Linear Models</title>
<link>https://arxiv.org/abs/2206.04277</link>
<guid>https://arxiv.org/abs/2206.04277</guid>
<content:encoded><![CDATA[
arXiv:2206.04277v5 Announce Type: replace-cross 
Abstract: We study the transfer learning (TL) for the functional linear regression (FLR) under the Reproducing Kernel Hilbert Space (RKHS) framework, observing that the TL techniques in existing high-dimensional linear regression are not compatible with the truncation-based FLR methods, as functional data are intrinsically infinite-dimensional and generated by smooth underlying processes. We measure the similarity across tasks using RKHS distance, allowing the type of information being transferred to be tied to the properties of the imposed RKHS. Building on the hypothesis offset transfer learning paradigm, two algorithms are proposed: one conducts the transfer when positive sources are known, while the other leverages aggregation techniques to achieve robust transfer without prior information about the sources. We establish asymptotic lower bounds for this learning problem and show that the proposed algorithms enjoy a matching upper bound. These analyses provide statistical insights into factors that contribute to the dynamics of the transfer. We also extend the results to functional generalized linear models. The effectiveness of the proposed algorithms is demonstrated via extensive synthetic data as well as real-world data applications.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Post Reinforcement Learning Inference</title>
<link>https://arxiv.org/abs/2302.08854</link>
<guid>https://arxiv.org/abs/2302.08854</guid>
<content:encoded><![CDATA[
arXiv:2302.08854v4 Announce Type: replace-cross 
Abstract: We consider estimation and inference using data collected from reinforcement learning algorithms. These algorithms, characterized by their adaptive experimentation, interact with individual units over multiple stages, dynamically adjusting their strategies based on previous interactions. Our goal is to evaluate a counterfactual policy post-data collection and estimate structural parameters, like dynamic treatment effects, which can be used for credit assignment and determining the effect of earlier actions on final outcomes. Such parameters of interest can be framed as solutions to moment equations, but not minimizers of a population loss function, leading to Z-estimation approaches for static data. However, in the adaptive data collection environment of reinforcement learning, where algorithms deploy nonstationary behavior policies, standard estimators do not achieve asymptotic normality due to the fluctuating variance. We propose a weighted Z-estimation approach with carefully designed adaptive weights to stabilize the time-varying estimation variance. We identify proper weighting schemes to restore the consistency and asymptotic normality of the weighted Z-estimators for target parameters, which allows for hypothesis testing and constructing uniform confidence regions. Primary applications include dynamic treatment effect estimation and dynamic off-policy evaluation.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Optimized Ensemble Deep Learning Model For Brain Tumor Classification</title>
<link>https://arxiv.org/abs/2305.12844</link>
<guid>https://arxiv.org/abs/2305.12844</guid>
<content:encoded><![CDATA[
arXiv:2305.12844v3 Announce Type: replace-cross 
Abstract: Brain tumors present a grave risk to human life, demanding precise and timely diagnosis for effective treatment. Inaccurate identification of brain tumors can significantly diminish life expectancy, underscoring the critical need for precise diagnostic methods. Manual identification of brain tumors within vast Magnetic Resonance Imaging (MRI) image datasets is arduous and time-consuming. Thus, the development of a reliable deep learning (DL) model is essential to enhance diagnostic accuracy and ultimately save lives. This study introduces an innovative optimization-based deep ensemble approach employing transfer learning (TL) to efficiently classify brain tumors. Our methodology includes meticulous preprocessing, reconstruction of TL architectures, fine-tuning, and ensemble DL models utilizing weighted optimization techniques such as Genetic Algorithm-based Weight Optimization (GAWO) and Grid Search-based Weight Optimization (GSWO). Experimentation is conducted on the Figshare Contrast-Enhanced MRI (CE-MRI) brain tumor dataset, comprising 3064 images. Our approach achieves notable accuracy scores, with Xception, ResNet50V2, ResNet152V2, InceptionResNetV2, GAWO, and GSWO attaining 99.42%, 98.37%, 98.22%, 98.26%, 99.71%, and 99.76% accuracy, respectively. Notably, GSWO demonstrates superior accuracy, averaging 99.76\% accuracy across five folds on the Figshare CE-MRI brain tumor dataset. The comparative analysis highlights the significant performance enhancement of our proposed model over existing counterparts. In conclusion, our optimized deep ensemble model exhibits exceptional accuracy in swiftly classifying brain tumors. Furthermore, it has the potential to assist neurologists and clinicians in making accurate and immediate diagnostic decisions.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sequential Experimental Design for X-Ray CT Using Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2307.06343</link>
<guid>https://arxiv.org/abs/2307.06343</guid>
<content:encoded><![CDATA[
arXiv:2307.06343v2 Announce Type: replace-cross 
Abstract: In X-ray Computed Tomography (CT), projections from many angles are acquired and used for 3D reconstruction. To make CT suitable for in-line quality control, reducing the number of angles while maintaining reconstruction quality is necessary. Sparse-angle tomography is a popular approach for obtaining 3D reconstructions from limited data. To optimize its performance, one can adapt scan angles sequentially to select the most informative angles for each scanned object. Mathematically, this corresponds to solving an optimal experimental design (OED) problem. OED problems are high-dimensional, non-convex, bi-level optimization problems that cannot be solved online, i.e., during the scan. To address these challenges, we pose the OED problem as a partially observable Markov decision process in a Bayesian framework, and solve it through deep reinforcement learning. The approach learns efficient non-greedy policies to solve a given class of OED problems through extensive offline training rather than solving a given OED problem directly via numerical optimization. As such, the trained policy can successfully find the most informative scan angles online. We use a policy training method based on the Actor-Critic approach and evaluate its performance on 2D tomography with synthetic data.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Targeting relative risk heterogeneity with causal forests</title>
<link>https://arxiv.org/abs/2309.15793</link>
<guid>https://arxiv.org/abs/2309.15793</guid>
<content:encoded><![CDATA[
arXiv:2309.15793v3 Announce Type: replace-cross 
Abstract: The identification of heterogeneous treatment effects (HTE) across subgroups is of significant interest in clinical trial analysis. Several state-of-the-art HTE estimation methods, including causal forests, apply recursive partitioning for non-parametric identification of relevant covariates and interactions. However, the partitioning criterion is typically based on differences in absolute risk. This can dilute statistical power by masking variation in the relative risk, which is often a more appropriate quantity of clinical interest. In this work, we propose and implement a methodology for modifying causal forests to target relative risk, using a novel node-splitting procedure based on exhaustive generalized linear model comparison. We present results from simulated data that suggest relative risk causal forests can capture otherwise undetected sources of heterogeneity. We implement our method on real-world trial data to explore HTEs for liraglutide in patients with type 2 diabetes.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AfroBench: How Good are Large Language Models on African Languages?</title>
<link>https://arxiv.org/abs/2311.07978</link>
<guid>https://arxiv.org/abs/2311.07978</guid>
<content:encoded><![CDATA[
arXiv:2311.07978v5 Announce Type: replace-cross 
Abstract: Large-scale multilingual evaluations, such as MEGA, often include only a handful of African languages due to the scarcity of high-quality evaluation data and the limited discoverability of existing African datasets. This lack of representation hinders comprehensive LLM evaluation across a diverse range of languages and tasks. To address these challenges, we introduce AfroBench -- a multi-task benchmark for evaluating the performance of LLMs across 64 African languages, 15 tasks and 22 datasets. AfroBench consists of nine natural language understanding datasets, six text generation datasets, six knowledge and question answering tasks, and one mathematical reasoning task. We present results comparing the performance of prompting LLMs to fine-tuned baselines based on BERT and T5-style models. Our results suggest large gaps in performance between high-resource languages, such as English, and African languages across most tasks; but performance also varies based on the availability of monolingual data resources. Our findings confirm that performance on African languages continues to remain a hurdle for current LLMs, underscoring the need for additional efforts to close this gap.
  https://mcgill-nlp.github.io/AfroBench/
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empowering COVID-19 Detection: Optimizing Performance Through Fine-Tuned EfficientNet Deep Learning Architecture</title>
<link>https://arxiv.org/abs/2311.16593</link>
<guid>https://arxiv.org/abs/2311.16593</guid>
<content:encoded><![CDATA[
arXiv:2311.16593v2 Announce Type: replace-cross 
Abstract: The worldwide COVID-19 pandemic has profoundly influenced the health and everyday experiences of individuals across the planet. It is a highly contagious respiratory disease requiring early and accurate detection to curb its rapid transmission. Initial testing methods primarily revolved around identifying the genetic composition of the coronavirus, exhibiting a relatively low detection rate and requiring a time-intensive procedure. To address this challenge, experts have suggested using radiological imagery, particularly chest X-rays, as a valuable approach within the diagnostic protocol. This study investigates the potential of leveraging radiographic imaging (X-rays) with deep learning algorithms to swiftly and precisely identify COVID-19 patients. The proposed approach elevates the detection accuracy by fine-tuning with appropriate layers on various established transfer learning models. The experimentation was conducted on a COVID-19 X-ray dataset containing 2000 images. The accuracy rates achieved were impressive of 100% for EfficientNetB4 model. The fine-tuned EfficientNetB4 achieved an excellent accuracy score, showcasing its potential as a robust COVID-19 detection model. Furthermore, EfficientNetB4 excelled in identifying Lung disease using Chest X-ray dataset containing 4,350 Images, achieving remarkable performance with an accuracy of 99.17%, precision of 99.13%, recall of 99.16%, and f1-score of 99.14%. These results highlight the promise of fine-tuned transfer learning for efficient lung detection through medical imaging, especially with X-ray images. This research offers radiologists an effective means of aiding rapid and precise COVID-19 diagnosis and contributes valuable assistance for healthcare professionals in accurately identifying affected patients.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resilience of Rademacher chaos of low degree</title>
<link>https://arxiv.org/abs/2402.10504</link>
<guid>https://arxiv.org/abs/2402.10504</guid>
<content:encoded><![CDATA[
arXiv:2402.10504v5 Announce Type: replace-cross 
Abstract: The resilience of a Rademacher chaos is the maximum number of adversarial sign-flips that the chaos can sustain without having its largest atom probability significantly altered. Inspired by probabilistic lower-bound guarantees for the resilience of linear Rademacher chaos, obtained by Bandeira, Ferber, and Kwan (Advances in Mathematics, Vol. $319$, $2017$), we provide probabilistic lower-bound guarantees for the resilience of Rademacher chaos of arbitrary yet sufficiently low degree.
  Our main results distinguish between Rademacher chaos of order two and those of higher order. In that, our first main result pertains to the resilience of decoupled bilinear Rademacher forms where different asymptotic behaviour is observed for sparse and dense matrices. For our second main result, we bootstrap our first result in order to provide resilience guarantees for quadratic Rademacher chaos. Our third main result, generalises the first and handles the resilience of decoupled Rademacher chaos of arbitrary yet sufficiently low order.
  Our results for decoupled Rademacher chaos of order two and that of higher order whilst are established through the same conceptual framework, differ substantially. A difference incurred due to the implementation of the same conceptual argument. The order two result is established using Dudley's maximal inequality for sub-Gaussian processes, the Hanson-Wright inequality, as well as the Kolmogorov-Rogozin inequality. To handle higher order chaos, appeals to Dudley's inequality as well as the Hanson-Wright inequality are replaced with tools suited for random tensors. Appeals to the Hanson-Wright inequality are replaced with appeals to a concentration result for random tensors put forth by Adamczak and Wolff.
  Our results are instance-dependent and thus allow for the efficient computation of resilience guarantees provided the order of the chaos is constant.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Impact of Uncertainty and Calibration on Likelihood-Ratio Membership Inference Attacks</title>
<link>https://arxiv.org/abs/2402.10686</link>
<guid>https://arxiv.org/abs/2402.10686</guid>
<content:encoded><![CDATA[
arXiv:2402.10686v5 Announce Type: replace-cross 
Abstract: In a membership inference attack (MIA), an attacker exploits the overconfidence exhibited by typical machine learning models to determine whether a specific data point was used to train a target model. In this paper, we analyze the performance of the likelihood ratio attack (LiRA) within an information-theoretical framework that allows the investigation of the impact of the aleatoric uncertainty in the true data generation process, of the epistemic uncertainty caused by a limited training data set, and of the calibration level of the target model. We compare three different settings, in which the attacker receives decreasingly informative feedback from the target model: confidence vector (CV) disclosure, in which the output probability vector is released; true label confidence (TLC) disclosure, in which only the probability assigned to the true label is made available by the model; and decision set (DS) disclosure, in which an adaptive prediction set is produced as in conformal prediction. We derive bounds on the advantage of an MIA adversary with the aim of offering insights into the impact of uncertainty and calibration on the effectiveness of MIAs. Simulation results demonstrate that the derived analytical bounds predict well the effectiveness of MIAs.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structure-agnostic Optimality of Doubly Robust Learning for Treatment Effect Estimation</title>
<link>https://arxiv.org/abs/2402.14264</link>
<guid>https://arxiv.org/abs/2402.14264</guid>
<content:encoded><![CDATA[
arXiv:2402.14264v4 Announce Type: replace-cross 
Abstract: Average treatment effect estimation is the most central problem in causal inference with application to numerous disciplines. While many estimation strategies have been proposed in the literature, the statistical optimality of these methods has still remained an open area of investigation, especially in regimes where these methods do not achieve parametric rates. In this paper, we adopt the recently introduced structure-agnostic framework of statistical lower bounds, which poses no structural properties on the nuisance functions other than access to black-box estimators that achieve some statistical estimation rate. This framework is particularly appealing when one is only willing to consider estimation strategies that use non-parametric regression and classification oracles as black-box sub-processes. Within this framework, we prove the statistical optimality of the celebrated and widely used doubly robust estimators for both the Average Treatment Effect (ATE) and the Average Treatment Effect on the Treated (ATT), as well as weighted variants of the former, which arise in policy evaluation.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Survey on Artificial Intelligence for Complex Network: Potential, Methodology and Application</title>
<link>https://arxiv.org/abs/2402.16887</link>
<guid>https://arxiv.org/abs/2402.16887</guid>
<content:encoded><![CDATA[
arXiv:2402.16887v2 Announce Type: replace-cross 
Abstract: Complex networks pervade various real-world systems, from the natural environment to human societies. The essence of these networks is in their ability to transition and evolve from microscopic disorder-where network topology and node dynamics intertwine-to a macroscopic order characterized by certain collective behaviors. Over the past two decades, complex network science has significantly enhanced our understanding of the statistical mechanics, structures, and dynamics underlying real-world networks. Despite these advancements, there remain considerable challenges in exploring more realistic systems and enhancing practical applications. The emergence of artificial intelligence (AI) technologies, coupled with the abundance of diverse real-world network data, has heralded a new era in complex network science research. This survey aims to systematically address the potential advantages of AI in overcoming the lingering challenges of complex network research. It endeavors to summarize the pivotal research problems and provide an exhaustive review of the corresponding methodologies and applications. Through this comprehensive survey-the first of its kind on AI for complex networks-we expect to provide valuable insights that will drive further research and advancement in this interdisciplinary field.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust 3D Shape Reconstruction in Zero-Shot from a Single Image in the Wild</title>
<link>https://arxiv.org/abs/2403.14539</link>
<guid>https://arxiv.org/abs/2403.14539</guid>
<content:encoded><![CDATA[
arXiv:2403.14539v3 Announce Type: replace-cross 
Abstract: Recent monocular 3D shape reconstruction methods have shown promising zero-shot results on object-segmented images without any occlusions. However, their effectiveness is significantly compromised in real-world conditions, due to imperfect object segmentation by off-the-shelf models and the prevalence of occlusions. To effectively address these issues, we propose a unified regression model that integrates segmentation and reconstruction, specifically designed for occlusion-aware 3D shape reconstruction. To facilitate its reconstruction in the wild, we also introduce a scalable data synthesis pipeline that simulates a wide range of variations in objects, occluders, and backgrounds. Training on our synthetic data enables the proposed model to achieve state-of-the-art zero-shot results on real-world images, using significantly fewer parameters than competing approaches.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsolvable Problem Detection: Robust Understanding Evaluation for Large Multimodal Models</title>
<link>https://arxiv.org/abs/2403.20331</link>
<guid>https://arxiv.org/abs/2403.20331</guid>
<content:encoded><![CDATA[
arXiv:2403.20331v4 Announce Type: replace-cross 
Abstract: This paper introduces a novel task to evaluate the robust understanding capability of Large Multimodal Models (LMMs), termed $\textbf{Unsolvable Problem Detection (UPD)}$. Multiple-choice question answering (MCQA) is widely used to assess the understanding capability of LMMs, but it does not guarantee that LMMs truly comprehend the answer. UPD assesses the LMM's ability to withhold answers when encountering unsolvable problems of MCQA, verifying whether the model truly understands the answer. UPD encompasses three problems: Absent Answer Detection (AAD), Incompatible Answer Set Detection (IASD), and Incompatible Visual Question Detection (IVQD), covering unsolvable cases like answer-lacking or incompatible choices and image-question mismatches. For the evaluation, we introduce the MM-UPD Bench, a benchmark for assessing performance across various ability dimensions. Our experiments reveal that even most LMMs, which demonstrate adequate performance on existing benchmarks, struggle significantly with MM-UPD, underscoring a novel aspect of trustworthiness that current benchmarks have overlooked. A detailed analysis shows that LMMs have different bottlenecks and chain-of-thought and self-reflection improved performance for LMMs with the bottleneck in their LLM capability. We hope our insights will enhance the broader understanding and development of more reliable LMMs. The code is available at https://github.com/AtsuMiyai/UPD.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nonparametric Modern Hopfield Models</title>
<link>https://arxiv.org/abs/2404.03900</link>
<guid>https://arxiv.org/abs/2404.03900</guid>
<content:encoded><![CDATA[
arXiv:2404.03900v2 Announce Type: replace-cross 
Abstract: We present a nonparametric interpretation for deep learning compatible modern Hopfield models and utilize this new perspective to debut efficient variants. Our key contribution stems from interpreting the memory storage and retrieval processes in modern Hopfield models as a nonparametric regression problem subject to a set of query-memory pairs. Interestingly, our framework not only recovers the known results from the original dense modern Hopfield model but also fills the void in the literature regarding efficient modern Hopfield models, by introducing \textit{sparse-structured} modern Hopfield models with sub-quadratic complexity. We establish that this sparse model inherits the appealing theoretical properties of its dense analogue -- connection with transformer attention, fixed point convergence and exponential memory capacity. Additionally, we showcase the versatility of our framework by constructing a family of modern Hopfield models as extensions, including linear, random masked, top-$K$ and positive random feature modern Hopfield models. Empirically, we validate our framework in both synthetic and realistic settings for memory retrieval and learning tasks.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FairICP: Encouraging Equalized Odds via Inverse Conditional Permutation</title>
<link>https://arxiv.org/abs/2404.05678</link>
<guid>https://arxiv.org/abs/2404.05678</guid>
<content:encoded><![CDATA[
arXiv:2404.05678v4 Announce Type: replace-cross 
Abstract: $\textit{Equalized odds}$, an important notion of algorithmic fairness, aims to ensure that sensitive variables, such as race and gender, do not unfairly influence the algorithm's prediction when conditioning on the true outcome. Despite rapid advancements, current research primarily focuses on equalized odds violations caused by a single sensitive attribute, leaving the challenge of simultaneously accounting for multiple attributes under-addressed. We bridge this gap by introducing an in-processing fairness-aware learning approach, FairICP, which integrates adversarial learning with a novel inverse conditional permutation scheme. FairICP offers a flexible and efficient scheme to promote equalized odds under fairness conditions described by complex and multi-dimensional sensitive attributes. The efficacy and adaptability of our method are demonstrated through both simulation studies and empirical analyses of real-world datasets.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Attention Collapses: How Degenerate Layers in LLMs Enable Smaller, Stronger Models</title>
<link>https://arxiv.org/abs/2404.08634</link>
<guid>https://arxiv.org/abs/2404.08634</guid>
<content:encoded><![CDATA[
arXiv:2404.08634v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) rely on the transformer architecture and its self-attention mechanism to deliver strong performance across tasks. However, we uncover a structural inefficiency in standard pre-trained decoder-style LLMs: in many of the deeper layers, attention matrices frequently collapse to near rank-one, single-column patterns. We refer to these underutilized components as lazy layers, which are redundant and computationally inefficient. To address this, we propose Inheritune, a simple and effective training recipe for building smaller, more efficient, and high performing language models. Inheritune initializes a compact model by inheriting the useful early layers from a larger pre-trained model, then progressively retrains and expands it. Our experiments across multiple models and datasets show that Inheritune trained models, despite having significantly fewer layers, can match or even outperform their larger counterparts. This approach yields compact, performant models and offers a practical path for efficient language model compression. Code is available at https://github.com/sanyalsunny111/LLM-Inheritune
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Flow Diffusion Models: Learnable Forward Process for Improved Diffusion Modelling</title>
<link>https://arxiv.org/abs/2404.12940</link>
<guid>https://arxiv.org/abs/2404.12940</guid>
<content:encoded><![CDATA[
arXiv:2404.12940v3 Announce Type: replace-cross 
Abstract: Conventional diffusion models typically relies on a fixed forward process, which implicitly defines complex marginal distributions over latent variables. This can often complicate the reverse process' task in learning generative trajectories, and results in costly inference for diffusion models. To address these limitations, we introduce Neural Flow Diffusion Models (NFDM), a novel framework that enhances diffusion models by supporting a broader range of forward processes beyond the standard Gaussian. We also propose a novel parameterization technique for learning the forward process. Our framework provides an end-to-end, simulation-free optimization objective, effectively minimizing a variational upper bound on the negative log-likelihood. Experimental results demonstrate NFDM's strong performance, evidenced by state-of-the-art likelihood estimation. Furthermore, we investigate NFDM's capacity for learning generative dynamics with specific characteristics, such as deterministic straight lines trajectories, and demonstrate how the framework may be adopted for learning bridges between two distributions. The results underscores NFDM's versatility and its potential for a wide range of applications.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Perplexity Predict Fine-tuning Performance? An Investigation of Tokenization Effects on Sequential Language Models for Nepali</title>
<link>https://arxiv.org/abs/2404.18071</link>
<guid>https://arxiv.org/abs/2404.18071</guid>
<content:encoded><![CDATA[
arXiv:2404.18071v2 Announce Type: replace-cross 
Abstract: The impact of subword tokenization on language model performance is well-documented for perplexity, with finer granularity consistently reducing this intrinsic metric. However, research on how different tokenization schemes affect a model's understanding capabilities remains limited, particularly for non-Latin script languages. Addressing this gap, we conducted a comprehensive evaluation of six distinct tokenization strategies by pretraining transformer-based language models for Nepali and evaluating their performance across multiple downstream tasks. While recent prominent models like GPT, RoBERTa, Claude, LLaMA, Mistral, Falcon, and MPT have adopted byte-level BPE tokenization, our findings demonstrate that for Nepali, SentencePiece tokenization consistently yields superior results on understanding-based tasks. Unlike previous studies that primarily focused on BERT-based architectures, our research specifically examines sequential transformer models, providing valuable insights for language model development in low-resource languages and highlighting the importance of tokenization strategy beyond perplexity reduction.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Black-Box Membership Inference Attack for Diffusion Models</title>
<link>https://arxiv.org/abs/2405.20771</link>
<guid>https://arxiv.org/abs/2405.20771</guid>
<content:encoded><![CDATA[
arXiv:2405.20771v4 Announce Type: replace-cross 
Abstract: Given the rising popularity of AI-generated art and the associated copyright concerns, identifying whether an artwork was used to train a diffusion model is an important research topic. The work approaches this problem from the membership inference attack (MIA) perspective. We first identify the limitation of applying existing MIA methods for proprietary diffusion models: the required access of internal U-nets. To address the above problem, we introduce a novel membership inference attack method that uses only the image-to-image variation API and operates without access to the model's internal U-net. Our method is based on the intuition that the model can more easily obtain an unbiased noise prediction estimate for images from the training set. By applying the API multiple times to the target image, averaging the outputs, and comparing the result to the original image, our approach can classify whether a sample was part of the training set. We validate our method using DDIM and Stable Diffusion setups and further extend both our approach and existing algorithms to the Diffusion Transformer architecture. Our experimental results consistently outperform previous methods.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Watermarking Language Models with Error Correcting Codes</title>
<link>https://arxiv.org/abs/2406.10281</link>
<guid>https://arxiv.org/abs/2406.10281</guid>
<content:encoded><![CDATA[
arXiv:2406.10281v4 Announce Type: replace-cross 
Abstract: Recent progress in large language models enables the creation of realistic machine-generated content. Watermarking is a promising approach to distinguish machine-generated text from human text, embedding statistical signals in the output that are ideally undetectable to humans. We propose a watermarking framework that encodes such signals through an error correcting code. Our method, termed robust binary code (RBC) watermark, introduces no noticeable degradation in quality. We evaluate our watermark on base and instruction fine-tuned models and find that our watermark is robust to edits, deletions, and translations. We provide an information-theoretic perspective on watermarking, a powerful statistical test for detection and for generating $p$-values, and theoretical guarantees. Our empirical findings suggest our watermark is fast, powerful, and robust, comparing favorably to the state-of-the-art.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FRED: Flexible REduction-Distribution Interconnect and Communication Implementation for Wafer-Scale Distributed Training of DNN Models</title>
<link>https://arxiv.org/abs/2406.19580</link>
<guid>https://arxiv.org/abs/2406.19580</guid>
<content:encoded><![CDATA[
arXiv:2406.19580v2 Announce Type: replace-cross 
Abstract: Distributed Deep Neural Network (DNN) training is a technique to reduce the training overhead by distributing the training tasks into multiple accelerators, according to a parallelization strategy. However, high-performance compute and interconnects are needed for maximum speed-up and linear scaling of the system. Wafer-scale systems are a promising technology that allows for tightly integrating high-end accelerators with high-speed wafer-scale interconnects, making it an attractive platform for distributed training. However, the wafer-scale interconnect should offer high performance and flexibility for various parallelization strategies to enable maximum optimizations for compute and memory usage. In this paper, we propose FRED, a wafer-scale interconnect that is tailored for the high-BW requirements of wafer-scale networks and can efficiently execute communication patterns of different parallelization strategies. Furthermore, FRED supports in-switch collective communication execution that reduces the network traffic by approximately 2X. Our results show that FRED can improve the average end-to-end training time of ResNet-152, Transformer-17B, GPT-3, and Transformer-1T by 1.76X, 1.87X, 1.34X, and 1.4X, respectively when compared to a baseline waferscale 2D-Mesh fabric.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Protecting Deep Learning Model Copyrights with Adversarial Example-Free Reuse Detection</title>
<link>https://arxiv.org/abs/2407.03883</link>
<guid>https://arxiv.org/abs/2407.03883</guid>
<content:encoded><![CDATA[
arXiv:2407.03883v2 Announce Type: replace-cross 
Abstract: Model reuse techniques can reduce the resource requirements for training high-performance deep neural networks (DNNs) by leveraging existing models. However, unauthorized reuse and replication of DNNs can lead to copyright infringement and economic loss to the model owner. This underscores the need to analyze the reuse relation between DNNs and develop copyright protection techniques to safeguard intellectual property rights. Existing white-box testing-based approaches cannot address the common heterogeneous reuse case where the model architecture is changed, and DNN fingerprinting approaches heavily rely on generating adversarial examples with good transferability, which is known to be challenging in the black-box setting. To bridge the gap, we propose NFARD, a Neuron Functionality Analysis-based Reuse Detector, which only requires normal test samples to detect reuse relations by measuring the models' differences on a newly proposed model characterization, i.e., neuron functionality (NF). A set of NF-based distance metrics is designed to make NFARD applicable to both white-box and black-box settings. Moreover, we devise a linear transformation method to handle heterogeneous reuse cases by constructing the optimal projection matrix for dimension consistency, significantly extending the application scope of NFARD. To the best of our knowledge, this is the first adversarial example-free method that exploits neuron functionality for DNN copyright protection. As a side contribution, we constructed a reuse detection benchmark named Reuse Zoo that covers various practical reuse techniques and popular datasets. Extensive evaluations on this comprehensive benchmark show that NFARD achieves F1 scores of 0.984 and 1.0 for detecting reuse relationships in black-box and white-box settings, respectively, while generating test suites 2 ~ 99 times faster than previous methods.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diversifying the Expert Knowledge for Task-Agnostic Pruning in Sparse Mixture-of-Experts</title>
<link>https://arxiv.org/abs/2407.09590</link>
<guid>https://arxiv.org/abs/2407.09590</guid>
<content:encoded><![CDATA[
arXiv:2407.09590v4 Announce Type: replace-cross 
Abstract: By increasing model parameters but activating them sparsely when performing a task, the use of Mixture-of-Experts (MoE) architecture significantly improves the performance of Large Language Models (LLMs) without increasing the inference cost. However, the memory consumption due to the growing number of experts presents a challenge to the deployment of these models in many real world settings. Our empirical study reveals that some experts encode redundant knowledge during pre-training. We thus propose a method of grouping and pruning similar experts to improve the model's parameter efficiency. We validate the effectiveness of our method by pruning three state-of-the-art MoE architectures, including Mixtral, Deepseek-MoE, and Qwen. The evaluation shows that our method outperforms other model pruning methods on a range of natural language tasks. We will release our code to facilitate future research.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>C3T: Cross-modal Transfer Through Time for Sensor-based Human Activity Recognition</title>
<link>https://arxiv.org/abs/2407.16803</link>
<guid>https://arxiv.org/abs/2407.16803</guid>
<content:encoded><![CDATA[
arXiv:2407.16803v3 Announce Type: replace-cross 
Abstract: In order to unlock the potential of diverse sensors, we investigate a method to transfer knowledge between time-series modalities using a multimodal \textit{temporal} representation space for Human Activity Recognition (HAR). Specifically, we explore the setting where the modality used in testing has no labeled data during training, which we refer to as Unsupervised Modality Adaptation (UMA). We categorize existing UMA approaches as Student-Teacher or Contrastive Alignment methods. These methods typically compress continuous-time data samples into single latent vectors during alignment, inhibiting their ability to transfer temporal information through real-world temporal distortions. To address this, we introduce Cross-modal Transfer Through Time (C3T), which preserves temporal information during alignment to handle dynamic sensor data better. C3T achieves this by aligning a set of temporal latent vectors across sensing modalities. Our extensive experiments on various camera+IMU datasets demonstrate that C3T outperforms existing methods in UMA by at least 8% in accuracy and shows superior robustness to temporal distortions such as time-shift, misalignment, and dilation. Our findings suggest that C3T has significant potential for developing generalizable models for time-series sensor data, opening new avenues for various multimodal applications.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GLASS: Guided Latent Slot Diffusion for Object-Centric Learning</title>
<link>https://arxiv.org/abs/2407.17929</link>
<guid>https://arxiv.org/abs/2407.17929</guid>
<content:encoded><![CDATA[
arXiv:2407.17929v2 Announce Type: replace-cross 
Abstract: Object-centric learning aims to decompose an input image into a set of meaningful object files (slots). These latent object representations enable a variety of downstream tasks. Yet, object-centric learning struggles on real-world datasets, which contain multiple objects of complex textures and shapes in natural everyday scenes. To address this, we introduce Guided Latent Slot Diffusion (GLASS), a novel slot-attention model that learns in the space of generated images and uses semantic and instance guidance modules to learn better slot embeddings for various downstream tasks. Our experiments show that GLASS surpasses state-of-the-art slot-attention methods by a wide margin on tasks such as (zero-shot) object discovery and conditional image generation for real-world scenes. Moreover, GLASS enables the first application of slot attention to the compositional generation of complex, realistic scenes.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A spring-block theory of feature learning in deep neural networks</title>
<link>https://arxiv.org/abs/2407.19353</link>
<guid>https://arxiv.org/abs/2407.19353</guid>
<content:encoded><![CDATA[
arXiv:2407.19353v3 Announce Type: replace-cross 
Abstract: Feature-learning deep nets progressively collapse data to a regular low-dimensional geometry. How this emerges from the collective action of nonlinearity, noise, learning rate, and other factors, has eluded first-principles theories built from microscopic neuronal dynamics. We exhibit a noise-nonlinearity phase diagram that identifies regimes where shallow or deep layers learn more effectively and propose a macroscopic mechanical theory that reproduces the diagram and links feature learning across layers to generalization.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online SLA Decomposition: Enabling Real-Time Adaptation to Evolving Network Systems</title>
<link>https://arxiv.org/abs/2408.08968</link>
<guid>https://arxiv.org/abs/2408.08968</guid>
<content:encoded><![CDATA[
arXiv:2408.08968v5 Announce Type: replace-cross 
Abstract: When a network slice spans multiple technology domains, it is crucial for each domain to uphold the End-to-End (E2E) Service Level Agreement (SLA) associated with the slice. Consequently, the E2E SLA must be properly decomposed into partial SLAs that are assigned to each domain involved. In a network slice management system with a two-level architecture, comprising an E2E service orchestrator and local domain controllers, we consider that the orchestrator has access only to historical data regarding the responses of local controllers to previous requests, and this information is used to construct a risk model for each domain. In this study, we extend our previous work by investigating the dynamic nature of real-world systems and introducing an online learning-decomposition framework to tackle the dynamicity. We propose a framework that continuously updates the risk models based on the most recent feedback. This approach leverages key components such as online gradient descent and FIFO memory buffers, which enhance the stability and robustness of the overall process. Our empirical study on an analytic model-based simulator demonstrates that the proposed framework outperforms the state-of-the-art static approach, delivering more accurate and resilient SLA decomposition under varying conditions and data limitations. Furthermore, we provide a comprehensive complexity analysis of the proposed solution.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ST-USleepNet: A Spatial-Temporal Coupling Prominence Network for Multi-Channel Sleep Staging</title>
<link>https://arxiv.org/abs/2408.11884</link>
<guid>https://arxiv.org/abs/2408.11884</guid>
<content:encoded><![CDATA[
arXiv:2408.11884v3 Announce Type: replace-cross 
Abstract: Sleep staging is critical to assess sleep quality and diagnose disorders. Despite advancements in artificial intelligence enabling automated sleep staging, significant challenges remain: (1) Simultaneously extracting prominent temporal and spatial sleep features from multi-channel raw signals, including characteristic sleep waveforms and salient spatial brain networks. (2) Capturing the spatial-temporal coupling patterns essential for accurate sleep staging. To address these challenges, we propose a novel framework named ST-USleepNet, comprising a spatial-temporal graph construction module (ST) and a U-shaped sleep network (USleepNet). The ST module converts raw signals into a spatial-temporal graph based on signal similarity, temporal, and spatial relationships to model spatial-temporal coupling patterns. The USleepNet employs a U-shaped structure for both the temporal and spatial streams, mirroring its original use in image segmentation to isolate significant targets. Applied to raw sleep signals and graph data from the ST module, USleepNet effectively segments these inputs, simultaneously extracting prominent temporal and spatial sleep features. Testing on three datasets demonstrates that ST-USleepNet outperforms existing baselines, and model visualizations confirm its efficacy in extracting prominent sleep features and temporal-spatial coupling patterns across various sleep stages. The code is available at https://github.com/Majy-Yuji/ST-USleepNet.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HSF: Defending against Jailbreak Attacks with Hidden State Filtering</title>
<link>https://arxiv.org/abs/2409.03788</link>
<guid>https://arxiv.org/abs/2409.03788</guid>
<content:encoded><![CDATA[
arXiv:2409.03788v2 Announce Type: replace-cross 
Abstract: With the growing deployment of LLMs in daily applications like chatbots and content generation, efforts to ensure outputs align with human values and avoid harmful content have intensified. However, increasingly sophisticated jailbreak attacks threaten this alignment, aiming to induce unsafe outputs. Current defense efforts either focus on prompt rewriting or detection, which are limited in effectiveness due to the various design of jailbreak prompts, or on output control and detection, which are computationally expensive as they require LLM inference. Therefore, designing a pre-inference defense method that resists diverse jailbreak prompts is crucial for preventing LLM jailbreak attacks. We observe that jailbreak attacks, safe queries, and harmful queries exhibit different clustering patterns within the LLM's hidden state representation space. This suggests that by leveraging the LLM's hidden state representational capabilities, we can analyze the LLM's forthcoming behavior and proactively intervene for defense. In this paper, we propose a jailbreak attack defense strategy based on a Hidden State Filter (HSF), a lossless architectural defense mechanism that enables the model to preemptively identify and reject adversarial inputs before the inference process begins. We activate its defensive potential through an additional plugin module, effectively framing the defense task as a classification problem. Experimental results on two benchmark datasets, utilizing three different LLMs, show that HSF significantly enhances resilience against six cutting-edge jailbreak attacks. It significantly reduces the success rate of jailbreak attacks while minimally impacting responses to benign user queries, with negligible inference overhead, and outperforming defense baselines.Our code and data are available at https://anonymous.4open.science/r/Hidden-State-Filtering-8652/
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalization of Geometric Graph Neural Networks with Lipschitz Loss Functions</title>
<link>https://arxiv.org/abs/2409.05191</link>
<guid>https://arxiv.org/abs/2409.05191</guid>
<content:encoded><![CDATA[
arXiv:2409.05191v2 Announce Type: replace-cross 
Abstract: In this paper, we study the generalization capabilities of geometric graph neural networks (GNNs). We consider GNNs over a geometric graph constructed from a finite set of randomly sampled points over an embedded manifold with topological information captured. We prove a generalization gap between the optimal empirical risk and the optimal statistical risk of this GNN, which decreases with the number of sampled points from the manifold and increases with the dimension of the underlying manifold. This generalization gap ensures that the GNN trained on a graph on a set of sampled points can be utilized to process other unseen graphs constructed from the same underlying manifold. The most important observation is that the generalization capability can be realized with one large graph instead of being limited to the size of the graph as in previous results. The generalization gap is derived based on the non-asymptotic convergence result of a GNN on the sampled graph to the underlying manifold neural networks (MNNs). We verify this theoretical result with experiments on multiple real-world datasets.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Finite-Particle Convergence Rates for Stein Variational Gradient Descent</title>
<link>https://arxiv.org/abs/2409.08469</link>
<guid>https://arxiv.org/abs/2409.08469</guid>
<content:encoded><![CDATA[
arXiv:2409.08469v3 Announce Type: replace-cross 
Abstract: We provide finite-particle convergence rates for the Stein Variational Gradient Descent (SVGD) algorithm in the Kernelized Stein Discrepancy ($\mathsf{KSD}$) and Wasserstein-2 metrics. Our key insight is that the time derivative of the relative entropy between the joint density of $N$ particle locations and the $N$-fold product target measure, starting from a regular initial distribution, splits into a dominant `negative part' proportional to $N$ times the expected $\mathsf{KSD}^2$ and a smaller `positive part'. This observation leads to $\mathsf{KSD}$ rates of order $1/\sqrt{N}$, in both continuous and discrete time, providing a near optimal (in the sense of matching the corresponding i.i.d. rates) double exponential improvement over the recent result by Shi and Mackey (2024). Under mild assumptions on the kernel and potential, these bounds also grow polynomially in the dimension $d$. By adding a bilinear component to the kernel, the above approach is used to further obtain Wasserstein-2 convergence in continuous time. For the case of `bilinear + Mat\'ern' kernels, we derive Wasserstein-2 rates that exhibit a curse-of-dimensionality similar to the i.i.d. setting. We also obtain marginal convergence and long-time propagation of chaos results for the time-averaged particle laws.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fitting Multilevel Factor Models</title>
<link>https://arxiv.org/abs/2409.12067</link>
<guid>https://arxiv.org/abs/2409.12067</guid>
<content:encoded><![CDATA[
arXiv:2409.12067v3 Announce Type: replace-cross 
Abstract: We examine a special case of the multilevel factor model, with covariance given by multilevel low rank (MLR) matrix~\cite{parshakova2023factor}. We develop a novel, fast implementation of the expectation-maximization algorithm, tailored for multilevel factor models, to maximize the likelihood of the observed data. This method accommodates any hierarchical structure and maintains linear time and storage complexities per iteration. This is achieved through a new efficient technique for computing the inverse of the positive definite MLR matrix. We show that the inverse of positive definite MLR matrix is also an MLR matrix with the same sparsity in factors, and we use the recursive Sherman-Morrison-Woodbury matrix identity to obtain the factors of the inverse. Additionally, we present an algorithm that computes the Cholesky factorization of an expanded matrix with linear time and space complexities, yielding the covariance matrix as its Schur complement. This paper is accompanied by an open-source package that implements the proposed methods.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PecSched: Preemptive and Efficient Cluster Scheduling for LLM Inference</title>
<link>https://arxiv.org/abs/2409.15104</link>
<guid>https://arxiv.org/abs/2409.15104</guid>
<content:encoded><![CDATA[
arXiv:2409.15104v2 Announce Type: replace-cross 
Abstract: The scaling of transformer-based Large Language Models (LLMs) has significantly expanded their context lengths, enabling applications where inputs exceed 100K tokens. Our analysis of a recent Azure LLM inference trace reveals a highly skewed long-tail distribution of input lengths, with approximately 80% of inputs shorter than 2K tokens. Long inputs constitute only a small fraction. Existing cluster-level LLM scheduling strategies, including First-In-First-Out (FIFO), reservation-based, and priority-based approaches, primarily target short-input requests with lengths below 2K and fail to address this heterogeneity, leading to inefficiencies such as head-of-line blocking, resource underutilization, and starvation of long-input requests. We propose PecSched, a Preemptive and Efficient Cluster SCHEDuling system for LLM inference. PecSched introduces the following key techniques: 1) preemptive scheduling that prioritizes short-input requests for their performance; 2) coordinated prefill-decode colocation and disaggregation, which reduces both the duration and frequency of preemptions; 3) fast Sequence Parallelism (SP) that minimizes the prefill time of long-input requests to further reduce the likelihood and frequency of preemptions. Evaluations based on Azure LLM inference trace show that, compared to state-of-the-art cluster-level LLM inference schedulers, PecSched reduces the 99th percentile queueing delay of short-input requests by up to 92% and improves their throughput by up to 595%, without significantly affecting the Job Completion Time (JCT) of long-input requests. We open-sourced our code.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RED QUEEN: Safeguarding Large Language Models against Concealed Multi-Turn Jailbreaking</title>
<link>https://arxiv.org/abs/2409.17458</link>
<guid>https://arxiv.org/abs/2409.17458</guid>
<content:encoded><![CDATA[
arXiv:2409.17458v2 Announce Type: replace-cross 
Abstract: The rapid progress of Large Language Models (LLMs) has opened up new opportunities across various domains and applications; yet it also presents challenges related to potential misuse. To mitigate such risks, red teaming has been employed as a proactive security measure to probe language models for harmful outputs via jailbreak attacks. However, current jailbreak attack approaches are single-turn with explicit malicious queries that do not fully capture the complexity of real-world interactions. In reality, users can engage in multi-turn interactions with LLM-based chat assistants, allowing them to conceal their true intentions in a more covert manner. To bridge this gap, we, first, propose a new jailbreak approach, RED QUEEN ATTACK. This method constructs a multi-turn scenario, concealing the malicious intent under the guise of preventing harm. We craft 40 scenarios that vary in turns and select 14 harmful categories to generate 56k multi-turn attack data points. We conduct comprehensive experiments on the RED QUEEN ATTACK with four representative LLM families of different sizes. Our experiments reveal that all LLMs are vulnerable to RED QUEEN ATTACK, reaching 87.62% attack success rate on GPT-4o and 75.4% on Llama3-70B. Further analysis reveals that larger models are more susceptible to the RED QUEEN ATTACK, with multi-turn structures and concealment strategies contributing to its success. To prioritize safety, we introduce a straightforward mitigation strategy called RED QUEEN GUARD, which aligns LLMs to effectively counter adversarial attacks. This approach reduces the attack success rate to below 1% while maintaining the model's performance across standard benchmarks. Full implementation and dataset are publicly accessible at https://github.com/kriti-hippo/red_queen.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalization and Robustness of the Tilted Empirical Risk</title>
<link>https://arxiv.org/abs/2409.19431</link>
<guid>https://arxiv.org/abs/2409.19431</guid>
<content:encoded><![CDATA[
arXiv:2409.19431v3 Announce Type: replace-cross 
Abstract: The generalization error (risk) of a supervised statistical learning algorithm quantifies its prediction ability on previously unseen data. Inspired by exponential tilting, \citet{li2020tilted} proposed the {\it tilted empirical risk} (TER) as a non-linear risk metric for machine learning applications such as classification and regression problems. In this work, we examine the generalization error of the tilted empirical risk in the robustness regime under \textit{negative tilt}. Our first contribution is to provide uniform and information-theoretic bounds on the {\it tilted generalization error}, defined as the difference between the population risk and the tilted empirical risk, under negative tilt for unbounded loss function under bounded $(1+\epsilon)$-th moment of loss function for some $\epsilon\in(0,1]$ with a convergence rate of $O(n^{-\epsilon/(1+\epsilon)})$ where $n$ is the number of training samples, revealing a novel application for TER under no distribution shift. Secondly, we study the robustness of the tilted empirical risk with respect to noisy outliers at training time and provide theoretical guarantees under distribution shift for the tilted empirical risk. We empirically corroborate our findings in simple experimental setups where we evaluate our bounds to select the value of tilt in a data-driven manner.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Overview of the Burer-Monteiro Method for Certifiable Robot Perception</title>
<link>https://arxiv.org/abs/2410.00117</link>
<guid>https://arxiv.org/abs/2410.00117</guid>
<content:encoded><![CDATA[
arXiv:2410.00117v2 Announce Type: replace-cross 
Abstract: This paper presents an overview of the Burer-Monteiro method (BM), a technique that has been applied to solve robot perception problems to certifiable optimality in real-time. BM is often used to solve semidefinite programming relaxations, which can be used to perform global optimization for non-convex perception problems. Specifically, BM leverages the low-rank structure of typical semidefinite programs to dramatically reduce the computational cost of performing optimization. This paper discusses BM in certifiable perception, with three main objectives: (i) to consolidate information from the literature into a unified presentation, (ii) to elucidate the role of the linear independence constraint qualification (LICQ), a concept not yet well-covered in certifiable perception literature, and (iii) to share practical considerations that are discussed among practitioners but not thoroughly covered in the literature. Our general aim is to offer a practical primer for applying BM towards certifiable perception.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TorchTitan: One-stop PyTorch native solution for production ready LLM pre-training</title>
<link>https://arxiv.org/abs/2410.06511</link>
<guid>https://arxiv.org/abs/2410.06511</guid>
<content:encoded><![CDATA[
arXiv:2410.06511v3 Announce Type: replace-cross 
Abstract: The development of large language models (LLMs) has been instrumental in advancing state-of-the-art natural language processing applications. Training LLMs with billions of parameters and trillions of tokens require sophisticated distributed systems that enable composing and comparing several state-of-the-art techniques in order to efficiently scale across thousands of accelerators. However, existing solutions are complex, scattered across multiple libraries/repositories, lack interoperability, and are cumbersome to maintain. Thus, curating and empirically comparing training recipes require non-trivial engineering effort.
  This paper introduces TorchTitan, an open-source, PyTorch-native distributed training system that unifies state-of-the-art techniques, streamlining integration and reducing overhead. TorchTitan enables 3D parallelism in a modular manner with elastic scaling, providing comprehensive logging, checkpointing, and debugging tools for production-ready training. It also incorporates hardware-software co-designed solutions, leveraging features like Float8 training and SymmetricMemory. As a flexible test bed, TorchTitan facilitates custom recipe curation and comparison, allowing us to develop optimized training recipes for Llama 3.1 and provide guidance on selecting techniques for maximum efficiency based on our experiences.
  We thoroughly assess TorchTitan on the Llama 3.1 family of LLMs, spanning 8 billion to 405 billion parameters, and showcase its exceptional performance, modular composability, and elastic scalability. By stacking training optimizations, we demonstrate accelerations of 65.08% with 1D parallelism at the 128-GPU scale (Llama 3.1 8B), an additional 12.59% with 2D parallelism at the 256-GPU scale (Llama 3.1 70B), and an additional 30% with 3D parallelism at the 512-GPU scale (Llama 3.1 405B) on NVIDIA H100 GPUs over optimized baselines.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Stop: Deep Learning for Mean Field Optimal Stopping</title>
<link>https://arxiv.org/abs/2410.08850</link>
<guid>https://arxiv.org/abs/2410.08850</guid>
<content:encoded><![CDATA[
arXiv:2410.08850v2 Announce Type: replace-cross 
Abstract: Optimal stopping is a fundamental problem in optimization with applications in risk management, finance, robotics, and machine learning. We extend the standard framework to a multi-agent setting, named multi-agent optimal stopping (MAOS), where agents cooperate to make optimal stopping decisions in a finite-space, discrete-time environment. Since solving MAOS becomes computationally prohibitive as the number of agents is very large, we study the mean-field optimal stopping (MFOS) problem, obtained as the number of agents tends to infinity. We establish that MFOS provides a good approximation to MAOS and prove a dynamic programming principle (DPP) based on mean-field control theory. We then propose two deep learning approaches: one that learns optimal stopping decisions by simulating full trajectories and another that leverages the DPP to compute the value function and to learn the optimal stopping rule using backward induction. Both methods train neural networks to approximate optimal stopping policies. We demonstrate the effectiveness and the scalability of our work through numerical experiments on 6 different problems in spatial dimension up to 300. To the best of our knowledge, this is the first work to formalize and computationally solve MFOS in discrete time and finite space, opening new directions for scalable MAOS methods.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing Dialect Fairness and Robustness of Large Language Models in Reasoning Tasks</title>
<link>https://arxiv.org/abs/2410.11005</link>
<guid>https://arxiv.org/abs/2410.11005</guid>
<content:encoded><![CDATA[
arXiv:2410.11005v3 Announce Type: replace-cross 
Abstract: Language is not monolithic. While benchmarks, including those designed for multiple languages, are often used as proxies to evaluate the performance of Large Language Models (LLMs), they tend to overlook the nuances of within-language variation and thus fail to model the experience of speakers of non-standard dialects. Focusing on African American Vernacular English (AAVE), we present the first study aimed at objectively assessing the fairness and robustness of LLMs in handling dialects across canonical reasoning tasks, including algorithm, math, logic, and integrated reasoning. We introduce ReDial (Reasoning with Dialect Queries), a benchmark containing 1.2K+ parallel query pairs in Standardized English and AAVE. We hire AAVE speakers, including experts with computer science backgrounds, to rewrite seven popular benchmarks, such as HumanEval and GSM8K. With ReDial, we evaluate widely used LLMs, including GPT, Claude, Llama, Mistral, and the Phi model families. Our findings reveal that almost all of these widely used models show significant brittleness and unfairness to queries in AAVE. Our work establishes a systematic and objective framework for analyzing LLM bias in dialectal queries. Moreover, it highlights how mainstream LLMs provide unfair service to dialect speakers in reasoning tasks, laying a critical foundation for future research.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Simplifying and Learnable Graph Convolutional Attention Network for Unsupervised Knowledge Graphs Alignment</title>
<link>https://arxiv.org/abs/2410.13263</link>
<guid>https://arxiv.org/abs/2410.13263</guid>
<content:encoded><![CDATA[
arXiv:2410.13263v2 Announce Type: replace-cross 
Abstract: The success of current Entity Alignment (EA) task depends largely on the supervision information provided by labeled data. Considering the cost of labeled data, most supervised methods are difficult to apply in practical scenarios. Therefore, more and more works based on contrastive learning, active learning or other deep learning techniques have been developed, to solve the performance bottleneck caused by the lack of labeled data. However, the existing unsupervised EA methods still have some limitations, either their modeling complexity is high or they cannot balance the effectiveness and practicality of alignment. To overcome these issues, we propose a Simplifying and Learnable graph convolutional attention network for Unsupervised Knowledge Graphs alignment method (SLU). Specifically, we first introduce LCAT, a new and simple framework as the backbone network to model the graph structure of two KGs. Then we design a reconstruction method of relation structure based on potential matching relations for efficiently filtering invalid neighborhood information of aligned entities, to improve the usability and scalability of SLU. Impressively, a similarity function based on consistency is proposed to better measure the similarity of candidate entity pairs. Finally, we conduct extensive experiments on three datasets of different sizes (15K and 100K) and different types (cross-lingual and monolingual) to verify the superiority of SLU. Experimental results show that SLU significantly improves alignment accuracy, outperforming 25 supervised or unsupervised methods, and improving 6.4% in Hits@1 over the best baseline in the best case.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LabSafety Bench: Benchmarking LLMs on Safety Issues in Scientific Labs</title>
<link>https://arxiv.org/abs/2410.14182</link>
<guid>https://arxiv.org/abs/2410.14182</guid>
<content:encoded><![CDATA[
arXiv:2410.14182v3 Announce Type: replace-cross 
Abstract: Artificial Intelligence (AI) is revolutionizing scientific research, yet its growing integration into laboratory environments presents critical safety challenges. While large language models (LLMs) increasingly assist in tasks ranging from procedural guidance to autonomous experiment orchestration, an "illusion of understanding" may lead researchers to overestimate their reliability. Such overreliance is particularly dangerous in high-stakes laboratory settings, where failures in hazard identification or risk assessment can result in severe accidents. To address these concerns, we propose the Laboratory Safety Benchmark (LabSafety Bench), a comprehensive framework that evaluates large language models and vision language models (VLMs) on their ability to identify potential hazards, assess risks, and predict the consequences of unsafe actions in lab environments. LabSafety Bench comprises 765 multiple-choice questions aligned with US Occupational Safety and Health Administration (OSHA) protocols, along with 404 realistic laboratory scenarios featuring dual evaluation tasks: the Hazards Identification Test and the Consequence Identification Test, with 3128 open-ended questions in total. Evaluations across eight proprietary models, seven open-weight LLMs, and four VLMs reveal that, despite advanced performance on structured assessments, no model achieves the safety threshold required for reliable operation -- none scoring above 70% on the Hazards Identification Test. Moreover, while proprietary models tend to excel in multiple-choice evaluations, their performance in open-ended, real-world scenario responses is comparable to that of open-source models. These findings underscore the urgent need for specialized evaluation frameworks to ensure the safe and responsible deployment of AI in laboratory settings.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-HDR: Bridging LLM-based Perception and Self-Supervision for Unpaired LDR-to-HDR Image Reconstruction</title>
<link>https://arxiv.org/abs/2410.15068</link>
<guid>https://arxiv.org/abs/2410.15068</guid>
<content:encoded><![CDATA[
arXiv:2410.15068v3 Announce Type: replace-cross 
Abstract: The translation of Low Dynamic Range (LDR) to High Dynamic Range (HDR) images is an important computer vision task. There is a significant amount of research utilizing both conventional non-learning methods and modern data-driven approaches, focusing on using both single-exposed and multi-exposed LDR for HDR image reconstruction. However, most current state-of-the-art methods require high-quality paired {LDR,HDR} datasets for model training. In addition, there is limited literature on using unpaired datasets for this task, that is, the model learns a mapping between domains, i.e., {LDR,HDR}. This paper proposes LLM-HDR, a method that integrates the perception of Large Language Models (LLM) into a modified semantic- and cycle-consistent adversarial architecture that utilizes unpaired {LDR,HDR} datasets for training. The method introduces novel artifact- and exposure-aware generators to address visual artifact removal and an encoder and loss to address semantic consistency, another under-explored topic. LLM-HDR is the first to use an LLM for the {LDR,HDR} translation task in a self-supervised setup. The method achieves state-of-the-art performance across several benchmark datasets and reconstructs high-quality HDR images. The official website of this work is available at: https://github.com/HrishavBakulBarua/LLM-HDR
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Theoretical Limitations of Ensembles in the Age of Overparameterization</title>
<link>https://arxiv.org/abs/2410.16201</link>
<guid>https://arxiv.org/abs/2410.16201</guid>
<content:encoded><![CDATA[
arXiv:2410.16201v2 Announce Type: replace-cross 
Abstract: Classic ensembles generalize better than any single component model. In contrast, recent empirical studies find that modern ensembles of (overparameterized) neural networks may not provide any inherent generalization advantage over single but larger neural networks. This paper clarifies how modern overparameterized ensembles differ from their classic underparameterized counterparts, using ensembles of random feature (RF) regressors as a basis for developing theory. In contrast to the underparameterized regime, where ensembling typically induces regularization and increases generalization, we prove with minimal assumptions that infinite ensembles of overparameterized RF regressors become pointwise equivalent to (single) infinite-width RF regressors, and finite width ensembles rapidly converge to single models with the same parameter budget. These results, which are exact for ridgeless models and approximate for small ridge penalties, imply that overparameterized ensembles and single large models exhibit nearly identical generalization. We further characterize the predictive variance amongst ensemble members, demonstrating that it quantifies the expected effects of increasing capacity rather than capturing any conventional notion of uncertainty. Our results challenge common assumptions about the advantages of ensembles in overparameterized settings, prompting a reconsideration of how well intuitions from underparameterized ensembles transfer to deep ensembles and the overparameterized regime.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Xeno-learning: knowledge transfer across species in deep learning-based spectral image analysis</title>
<link>https://arxiv.org/abs/2410.19789</link>
<guid>https://arxiv.org/abs/2410.19789</guid>
<content:encoded><![CDATA[
arXiv:2410.19789v2 Announce Type: replace-cross 
Abstract: Novel optical imaging techniques, such as hyperspectral imaging (HSI) combined with machine learning-based (ML) analysis, have the potential to revolutionize clinical surgical imaging. However, these novel modalities face a shortage of large-scale, representative clinical data for training ML algorithms, while preclinical animal data is abundantly available through standardized experiments and allows for controlled induction of pathological tissue states, which is not ethically possible in patients. To leverage this situation, we propose a novel concept called "xeno-learning", a cross-species knowledge transfer paradigm inspired by xeno-transplantation, where organs from a donor species are transplanted into a recipient species. Using a total of 13,874 HSI images from humans as well as porcine and rat models, we show that although spectral signatures of organs differ substantially across species, relative changes resulting from pathologies or surgical manipulation (e.g., malperfusion; injection of contrast agent) are comparable. Such changes learnt in one species can thus be transferred to a new species via a novel "physiology-based data augmentation" method, enabling the large-scale secondary use of preclinical animal data for humans. The resulting ethical, monetary, and performance benefits promise a high impact of the proposed knowledge transfer paradigm on future developments in the field.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Certifiably Robust Model Evaluation in Federated Learning under Meta-Distributional Shifts</title>
<link>https://arxiv.org/abs/2410.20250</link>
<guid>https://arxiv.org/abs/2410.20250</guid>
<content:encoded><![CDATA[
arXiv:2410.20250v2 Announce Type: replace-cross 
Abstract: We address the challenge of certifying the performance of a federated learning model on an unseen target network using only measurements from the source network that trained the model. Specifically, consider a source network "A" with $K$ clients, each holding private, non-IID datasets drawn from heterogeneous distributions, modeled as samples from a broader meta-distribution $\mu$. Our goal is to provide certified guarantees for the model's performance on a different, unseen network "B", governed by an unknown meta-distribution $\mu'$, assuming the deviation between $\mu$ and $\mu'$ is bounded either in Wasserstein distance or an $f$-divergence. We derive worst-case uniform guarantees for both the model's average loss and its risk CDF, the latter corresponding to a novel, adversarially robust version of the Dvoretzky-Kiefer-Wolfowitz (DKW) inequality. In addition, we show how the vanilla DKW bound enables principled certification of the model's true performance on unseen clients within the same (source) network. Our bounds are efficiently computable, asymptotically minimax optimal, and preserve clients' privacy. We also establish non-asymptotic generalization bounds that converge to zero as $K$ grows and the minimum per-client sample size exceeds $\mathcal{O}(\log K)$. Empirical evaluations confirm the practical utility of our bounds across real-world tasks. The project code is available at: github.com/samin-mehdizadeh/Robust-Evaluation-DKW
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CE-CoLLM: Efficient and Adaptive Large Language Models Through Cloud-Edge Collaboration</title>
<link>https://arxiv.org/abs/2411.02829</link>
<guid>https://arxiv.org/abs/2411.02829</guid>
<content:encoded><![CDATA[
arXiv:2411.02829v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) exhibit remarkable human-like predictive capabilities. However, it is challenging to deploy LLMs to provide efficient and adaptive inference services at the edge. This paper proposes a novel Cloud-Edge Collaboration framework for LLMs (CE-CoLLM) to tackle these challenges. First, we identify the transmission of LLM contextual data between the cloud and edge as a key performance bottleneck, which introduces substantial communication overhead that dominates overall inference latency and makes na\"ive cloud-edge collaboration for LLMs inefficient. Second, we introduce a suite of novel techniques, including a latency-aware early exit mechanism and efficient cloud context management, into CE-CoLLM, which collectively reduce communication overhead and preserve LLM inference accuracy. Third, we design two adaptive inference modes to accommodate diverse edge environments: (1) a low-latency standalone edge inference mode that enables reliable edge-side independent LLM inference even under unstable network conditions, and (2) a high-accuracy cloud-edge collaborative inference mode that adaptively leverages cloud resources to enhance prediction accuracy. Extensive experiments on multiple benchmark datasets demonstrate that CE-CoLLM reduces overall inference time by up to 13.81% and offloads over 84.53% of the computational workload from the cloud to the edge, compared to conventional cloud-based LLM deployment, without sacrificing prediction accuracy. The code is provided on GitHub at https://github.com/mlsysx/CE-CoLLM.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Watermark under Fire: A Robustness Evaluation of LLM Watermarking</title>
<link>https://arxiv.org/abs/2411.13425</link>
<guid>https://arxiv.org/abs/2411.13425</guid>
<content:encoded><![CDATA[
arXiv:2411.13425v3 Announce Type: replace-cross 
Abstract: Various watermarking methods (``watermarkers'') have been proposed to identify LLM-generated texts; yet, due to the lack of unified evaluation platforms, many critical questions remain under-explored: i) What are the strengths/limitations of various watermarkers, especially their attack robustness? ii) How do various design choices impact their robustness? iii) How to optimally operate watermarkers in adversarial environments? To fill this gap, we systematize existing LLM watermarkers and watermark removal attacks, mapping out their design spaces. We then develop WaterPark, a unified platform that integrates 10 state-of-the-art watermarkers and 12 representative attacks. More importantly, by leveraging WaterPark, we conduct a comprehensive assessment of existing watermarkers, unveiling the impact of various design choices on their attack robustness. We further explore the best practices to operate watermarkers in adversarial environments. We believe our study sheds light on current LLM watermarking techniques while WaterPark serves as a valuable testbed to facilitate future research.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JESTR: Joint Embedding Space Technique for Ranking Candidate Molecules for the Annotation of Untargeted Metabolomics Data</title>
<link>https://arxiv.org/abs/2411.14464</link>
<guid>https://arxiv.org/abs/2411.14464</guid>
<content:encoded><![CDATA[
arXiv:2411.14464v3 Announce Type: replace-cross 
Abstract: Motivation: A major challenge in metabolomics is annotation: assigning molecular structures to mass spectral fragmentation patterns. Despite recent advances in molecule-to-spectra and in spectra-to-molecular fingerprint prediction (FP), annotation rates remain low. Results: We introduce in this paper a novel paradigm (JESTR) for annotation. Unlike prior approaches that explicitly construct molecular fingerprints or spectra, JESTR leverages the insight that molecules and their corresponding spectra are views of the same data and effectively embeds their representations in a joint space. Candidate structures are ranked based on cosine similarity between the embeddings of query spectrum and each candidate. We evaluate JESTR against mol-to-spec and spec-to-FP annotation tools on three datasets. On average, for rank@[1-5], JESTR outperforms other tools by 23.6%-71.6%. We further demonstrate the strong value of regularization with candidate molecules during training, boosting rank@1 performance by 11.4% and enhancing the model's ability to discern between target and candidate molecules. When comparing JESTR's performance against that of publicly available pretrained models of SIRIUS and CFM-ID on appropriate subsets of MassSpecGym benchmark dataset, JESTR outperforms these tools by 31% and 238%, respectively. Through JESTR, we offer a novel promising avenue towards accurate annotation, therefore unlocking valuable insights into the metabolome.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learnable Activation Functions in Physics-Informed Neural Networks for Solving Partial Differential Equations</title>
<link>https://arxiv.org/abs/2411.15111</link>
<guid>https://arxiv.org/abs/2411.15111</guid>
<content:encoded><![CDATA[
arXiv:2411.15111v3 Announce Type: replace-cross 
Abstract: We investigate learnable activation functions in Physics-Informed Neural Networks (PINNs) for solving Partial Differential Equations (PDEs): comparing traditional Multilayer Perceptrons (MLPs) with fixed and trainable activations against Kolmogorov-Arnold Networks (KANs) that employ learnable basis functions. While PINNs effectively incorporate physical laws into the learning process, they suffer from convergence and spectral bias problems, which limit their applicability to problems with rapid oscillations or sharp transitions. In this work, we study various activation and basis functions across diverse PDEs, including oscillatory, nonlinear wave, mixed-physics, and fluid dynamics problems. Using empirical Neural Tangent Kernel (NTK) analysis and Hessian eigenvalue decomposition, we assess convergence behavior, stability, and high-frequency approximation capacity. While KANs offer improved expressivity for capturing complex, high-frequency PDE solutions, they introduce new optimization challenges, especially in deeper networks. Our findings show that KANs face a curse of functional dimensionality, creating intractable optimization landscapes in deeper networks. Low spectral bias alone does not guarantee good performance; adaptive spectral bias approaches such as B-splines achieve optimal results by balancing global stability with local high-frequency resolution. Different PDE types require tailored strategies: smooth global activation functions excel for wave phenomena, while local adaptive activation functions suit problems with sharp transitions.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache</title>
<link>https://arxiv.org/abs/2411.18077</link>
<guid>https://arxiv.org/abs/2411.18077</guid>
<content:encoded><![CDATA[
arXiv:2411.18077v3 Announce Type: replace-cross 
Abstract: How to efficiently serve LLMs in practice has become exceptionally challenging due to their prohibitive memory and computation requirements. In this study, we investigate optimizing the KV cache, whose memory footprint poses a critical bottleneck in LLM inference, especially when dealing with long context tasks. To tackle the challenge, we introduce MiniKV, a KV cache optimization method that simultaneously preserves long context task accuracy while significantly reducing KV cache size via a novel 2-bit layer-discriminative KV cache. More importantly, we develop specialized CUDA kernels to make MiniKV compatible with FlashAttention. Experiments on a wide range of long context tasks show that MiniKV effectively achieves 86% KV cache compression ratio while recovering over 98.5% of accuracy, outperforming state-of-the-art methods while achieving excellent measured system performance improvements.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Another look at inference after prediction</title>
<link>https://arxiv.org/abs/2411.19908</link>
<guid>https://arxiv.org/abs/2411.19908</guid>
<content:encoded><![CDATA[
arXiv:2411.19908v4 Announce Type: replace-cross 
Abstract: From structural biology to epidemiology, predictions from machine learning (ML) models are increasingly used to complement costly gold-standard data to enable faster, more affordable, and scalable scientific inquiry. In response, prediction-based (PB) inference has emerged to accommodate statistical analysis using a large volume of predictions together with a small amount of gold-standard data. The goals of PB inference are two-fold: (i) to mitigate bias from errors in predictions and (ii) to improve efficiency relative to traditional inference using only the gold-standard data. While early PB inference methods focused on bias, their ability to enhance efficiency remains unclear. We revisit a popular PB inference method and show that a simple modification can be applied to guarantee improvements in efficiency beyond yielding valid inferences when the ML predictions are imperfect. The utility of this approach in leveraging prediction-based outcomes to enhance efficiency is demonstrated through extensive simulation studies and an application to the UK Biobank data. We further contextualize the problem of PB inference through historical literature from economics and statistics to highlight perspectives from classical methods in this contemporary problem.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DELT: A Simple Diversity-driven EarlyLate Training for Dataset Distillation</title>
<link>https://arxiv.org/abs/2411.19946</link>
<guid>https://arxiv.org/abs/2411.19946</guid>
<content:encoded><![CDATA[
arXiv:2411.19946v2 Announce Type: replace-cross 
Abstract: Recent advances in dataset distillation have led to solutions in two main directions. The conventional batch-to-batch matching mechanism is ideal for small-scale datasets and includes bi-level optimization methods on models and syntheses, such as FRePo, RCIG, and RaT-BPTT, as well as other methods like distribution matching, gradient matching, and weight trajectory matching. Conversely, batch-to-global matching typifies decoupled methods, which are particularly advantageous for large-scale datasets. This approach has garnered substantial interest within the community, as seen in SRe$^2$L, G-VBSM, WMDD, and CDA. A primary challenge with the second approach is the lack of diversity among syntheses within each class since samples are optimized independently and the same global supervision signals are reused across different synthetic images. In this study, we propose a new Diversity-driven EarlyLate Training (DELT) scheme to enhance the diversity of images in batch-to-global matching with less computation. Our approach is conceptually simple yet effective, it partitions predefined IPC samples into smaller subtasks and employs local optimizations to distill each subset into distributions from distinct phases, reducing the uniformity induced by the unified optimization process. These distilled images from the subtasks demonstrate effective generalization when applied to the entire task. We conduct extensive experiments on CIFAR, Tiny-ImageNet, ImageNet-1K, and its sub-datasets. Our approach outperforms the previous state-of-the-art by 2$\sim$5% on average across different datasets and IPCs (images per class), increasing diversity per class by more than 5% while reducing synthesis time by up to 39.3% for enhancing the training efficiency. Code is available at: https://github.com/VILA-Lab/DELT.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can the Rookies Cut the Tough Cookie? Exploring the Use of LLMs for SQL Equivalence Checking</title>
<link>https://arxiv.org/abs/2412.05561</link>
<guid>https://arxiv.org/abs/2412.05561</guid>
<content:encoded><![CDATA[
arXiv:2412.05561v2 Announce Type: replace-cross 
Abstract: Equivalence checking of SQL queries is an intractable problem often encountered in settings ranging from grading SQL submissions to debugging query optimizers. Despite recent work toward developing practical solutions, only simple queries written using a small subset of SQL are supported, leaving the equivalence checking of sophisticated SQL queries at the mercy of intensive, potentially error-prone, manual analysis. In this paper, we explore how LLMs can be used to reason with SQL queries to address this challenging problem. Towards this, we introduce a novel, realistic, and sufficiently complex benchmark called SQLEquiQuest for SQL query equivalence checking that reflects real-world settings. We establish strong baselines for SQL equivalence checking by leveraging the ability of LLMs to reason with SQL queries. We conduct a detailed evaluation of several state-of-the-art LLMs using various prompting strategies and carefully constructed in-context learning examples, including logical plans generated by SQL query processors. Our empirical evaluation shows that LLMs go well beyond the current capabilities of formal models for SQL equivalence, going from a mere 30% supported query pairs to full coverage, achieving up to 82% accuracy on Spider+DIN. However, a critical limitation of LLMs revealed by our analysis is that they exhibit a strong bias for equivalence predictions, with consistently poor performance over non-equivalent pairs, opening a new direction for potential future research.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Adaptively Inexact Method for Bilevel Learning Using Primal-Dual Style Differentiation</title>
<link>https://arxiv.org/abs/2412.06436</link>
<guid>https://arxiv.org/abs/2412.06436</guid>
<content:encoded><![CDATA[
arXiv:2412.06436v3 Announce Type: replace-cross 
Abstract: We consider a bilevel learning framework for learning linear operators. In this framework, the learnable parameters are optimized via a loss function that also depends on the minimizer of a convex optimization problem (denoted lower-level problem). We utilize an iterative algorithm called `piggyback' to compute the gradient of the loss and minimizer of the lower-level problem. Given that the lower-level problem is solved numerically, the loss function and thus its gradient can only be computed inexactly. To estimate the accuracy of the computed hypergradient, we derive an a-posteriori error bound, which provides guides for setting the tolerance for the lower-level problem, as well as the piggyback algorithm. To efficiently solve the upper-level optimization, we also propose an adaptive method for choosing a suitable step-size. To illustrate the proposed method, we consider a few learned regularizer problems, such as training an input-convex neural network.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Doubly Robust Forests</title>
<link>https://arxiv.org/abs/2412.07184</link>
<guid>https://arxiv.org/abs/2412.07184</guid>
<content:encoded><![CDATA[
arXiv:2412.07184v2 Announce Type: replace-cross 
Abstract: This paper proposes the automatic Doubly Robust Random Forest (DRRF) algorithm for estimating the conditional expectation of a moment functional in the presence of high-dimensional nuisance functions. DRRF extends the automatic debiasing framework based on the Riesz representer to the conditional setting and enables nonparametric, forest-based estimation (Athey et al., 2019; Oprescu et al., 2019). In contrast to existing methods, DRRF does not require prior knowledge of the form of the debiasing term or impose restrictive parametric or semi-parametric assumptions on the target quantity. Additionally, it is computationally efficient in making predictions at multiple query points. We establish consistency and asymptotic normality results for the DRRF estimator under general assumptions, allowing for the construction of valid confidence intervals. Through extensive simulations in heterogeneous treatment effect (HTE) estimation, we demonstrate the superior performance of DRRF over benchmark approaches in terms of estimation accuracy, robustness, and computational efficiency.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Numerical Analysis of HiPPO-LegS ODE for Deep State Space Models</title>
<link>https://arxiv.org/abs/2412.08595</link>
<guid>https://arxiv.org/abs/2412.08595</guid>
<content:encoded><![CDATA[
arXiv:2412.08595v2 Announce Type: replace-cross 
Abstract: In deep learning, the recently introduced state space models utilize HiPPO (High-order Polynomial Projection Operators) memory units to approximate continuous-time trajectories of input functions using ordinary differential equations (ODEs), and these techniques have shown empirical success in capturing long-range dependencies in long input sequences. However, the mathematical foundations of these ODEs, particularly the singular HiPPO-LegS (Legendre Scaled) ODE, and their corresponding numerical discretizations remain unsettled. In this work, we fill this gap by establishing that HiPPO-LegS ODE is well-posed despite its singularity, albeit without the freedom of arbitrary initial conditions. Further, we establish convergence of the associated numerical discretization schemes for Riemann integrable input functions.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finite-PINN: A Physics-Informed Neural Network with Finite Geometric Encoding for Solid Mechanics</title>
<link>https://arxiv.org/abs/2412.09453</link>
<guid>https://arxiv.org/abs/2412.09453</guid>
<content:encoded><![CDATA[
arXiv:2412.09453v2 Announce Type: replace-cross 
Abstract: PINN models have demonstrated capabilities in addressing fluid PDE problems, and their potential in solid mechanics is beginning to emerge. This study identifies two key challenges when using PINN to solve general solid mechanics problems. These challenges become evident when comparing the limitations of PINN with the well-established numerical methods commonly used in solid mechanics, such as the finite element method (FEM). Specifically: a) PINN models generate solutions over an infinite domain, which conflicts with the finite boundaries typical of most solid structures; and b) the solution space utilised by PINN is Euclidean, which is inadequate for addressing the complex geometries often present in solid structures.
  This work presents a PINN architecture for general solid mechanics problems, referred to as the Finite-PINN model. The model is designed to effectively tackle two key challenges, while retaining as much of the original PINN framework as possible. To this end, the Finite-PINN incorporates finite geometric encoding into the neural network inputs, thereby transforming the solution space from a conventional Euclidean space into a hybrid Euclidean-topological space. The model is comprehensively trained using both strong-form and weak-form loss formulations, enabling its application to a wide range of forward and inverse problems in solid mechanics. For forward problems, the Finite-PINN model efficiently approximates solutions to solid mechanics problems when the geometric information of a given structure has been preprocessed. For inverse problems, it effectively reconstructs full-field solutions from very sparse observations by embedding both physical laws and geometric information within its architecture.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prediction-Enhanced Monte Carlo: A Machine Learning View on Control Variate</title>
<link>https://arxiv.org/abs/2412.11257</link>
<guid>https://arxiv.org/abs/2412.11257</guid>
<content:encoded><![CDATA[
arXiv:2412.11257v3 Announce Type: replace-cross 
Abstract: For many complex simulation tasks spanning areas such as healthcare, engineering, and finance, Monte Carlo (MC) methods are invaluable due to their unbiased estimates and precise error quantification. Nevertheless, Monte Carlo simulations often become computationally prohibitive, especially for nested, multi-level, or path-dependent evaluations lacking effective variance reduction techniques. While machine learning (ML) surrogates appear as natural alternatives, naive replacements typically introduce unquantifiable biases. We address this challenge by introducing Prediction-Enhanced Monte Carlo (PEMC), a framework that leverages modern ML models as learned predictors, using cheap and parallelizable simulation as features, to output unbiased evaluation with reduced variance and runtime. PEMC can also be viewed as a "modernized" view of control variates, where we consider the overall computation-cost-aware variance reduction instead of per-replication reduction, while bypassing the closed-form mean function requirement and maintaining the advantageous unbiasedness and uncertainty quantifiability of Monte Carlo.
  We illustrate PEMC's broader efficacy and versatility through three examples: first, equity derivatives such as variance swaps under stochastic local volatility models; second, interest rate derivatives such as swaption pricing under the Heath-Jarrow-Morton (HJM) interest-rate model. Finally, we showcase PEMC in a socially significant context - ambulance dispatch and hospital load balancing - where accurate mortality rate estimates are key for ethically sensitive decision-making. Across these diverse scenarios, PEMC consistently reduces variance while preserving unbiasedness, highlighting its potential as a powerful enhancement to standard Monte Carlo baselines.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VORTEX: A Spatial Computing Framework for Optimized Drone Telemetry Extraction from First-Person View Flight Data</title>
<link>https://arxiv.org/abs/2412.18505</link>
<guid>https://arxiv.org/abs/2412.18505</guid>
<content:encoded><![CDATA[
arXiv:2412.18505v2 Announce Type: replace-cross 
Abstract: This paper presents the Visual Optical Recognition Telemetry EXtraction (VORTEX) system for extracting and analyzing drone telemetry data from First Person View (FPV) Uncrewed Aerial System (UAS) footage. VORTEX employs MMOCR, a PyTorch-based Optical Character Recognition (OCR) toolbox, to extract telemetry variables from drone Heads Up Display (HUD) recordings, utilizing advanced image preprocessing techniques, including CLAHE enhancement and adaptive thresholding. The study optimizes spatial accuracy and computational efficiency through systematic investigation of temporal sampling rates (1s, 5s, 10s, 15s, 20s) and coordinate processing methods. Results demonstrate that the 5-second sampling rate, utilizing 4.07% of available frames, provides the optimal balance with a point retention rate of 64% and mean speed accuracy within 4.2% of the 1-second baseline while reducing computational overhead by 80.5%. Comparative analysis of coordinate processing methods reveals that while UTM Zone 33N projection and Haversine calculations provide consistently similar results (within 0.1% difference), raw WGS84 coordinates underestimate distances by 15-30% and speeds by 20-35%. Altitude measurements showed unexpected resilience to sampling rate variations, with only 2.1% variation across all intervals. This research is the first of its kind, providing quantitative benchmarks for establishing a robust framework for drone telemetry extraction and analysis using open-source tools and spatial libraries.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SWAG: Long-term Surgical Workflow Prediction with Generative-based Anticipation</title>
<link>https://arxiv.org/abs/2412.18849</link>
<guid>https://arxiv.org/abs/2412.18849</guid>
<content:encoded><![CDATA[
arXiv:2412.18849v3 Announce Type: replace-cross 
Abstract: While existing approaches excel at recognising current surgical phases, they provide limited foresight and intraoperative guidance into future procedural steps. Similarly, current anticipation methods are constrained to predicting short-term and single events, neglecting the dense, repetitive, and long sequential nature of surgical workflows. To address these needs and limitations, we propose SWAG (Surgical Workflow Anticipative Generation), a framework that combines phase recognition and anticipation using a generative approach. This paper investigates two distinct decoding methods - single-pass (SP) and auto-regressive (AR) - to generate sequences of future surgical phases at minute intervals over long horizons. We propose a novel embedding approach using class transition probabilities to enhance the accuracy of phase anticipation. Additionally, we propose a generative framework using remaining time regression to classification (R2C). SWAG was evaluated on two publicly available datasets, Cholec80 and AutoLaparo21. Our single-pass model with class transition probability embeddings (SP*) achieves 32.1% and 41.3% F1 scores over 20 and 30 minutes on Cholec80 and AutoLaparo21, respectively. Moreover, our approach competes with existing methods on phase remaining time regression, achieving weighted mean absolute errors of 0.32 and 0.48 minutes for 2- and 3-minute horizons. SWAG demonstrates versatility across generative decoding frame works and classification and regression tasks to create temporal continuity between surgical workflow recognition and anticipation. Our method provides steps towards intraoperative surgical workflow generation for anticipation. Project: https://maxboels.github.io/swag.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimLTD: Simple Supervised and Semi-Supervised Long-Tailed Object Detection</title>
<link>https://arxiv.org/abs/2412.20047</link>
<guid>https://arxiv.org/abs/2412.20047</guid>
<content:encoded><![CDATA[
arXiv:2412.20047v3 Announce Type: replace-cross 
Abstract: While modern visual recognition systems have made significant advancements, many continue to struggle with the open problem of learning from few exemplars. This paper focuses on the task of object detection in the setting where object classes follow a natural long-tailed distribution. Existing methods for long-tailed detection resort to external ImageNet labels to augment the low-shot training instances. However, such dependency on a large labeled database has limited utility in practical scenarios. We propose a versatile and scalable approach to leverage optional unlabeled images, which are easy to collect without the burden of human annotations. Our SimLTD framework is straightforward and intuitive, and consists of three simple steps: (1) pre-training on abundant head classes; (2) transfer learning on scarce tail classes; and (3) fine-tuning on a sampled set of both head and tail classes. Our approach can be viewed as an improved head-to-tail model transfer paradigm without the added complexities of meta-learning or knowledge distillation, as was required in past research. By harnessing supplementary unlabeled images, without extra image labels, SimLTD establishes new record results on the challenging LVIS v1 benchmark across both supervised and semi-supervised settings.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Adversarial Robustness of Language Models in Transfer Learning</title>
<link>https://arxiv.org/abs/2501.00066</link>
<guid>https://arxiv.org/abs/2501.00066</guid>
<content:encoded><![CDATA[
arXiv:2501.00066v2 Announce Type: replace-cross 
Abstract: We investigate the adversarial robustness of LLMs in transfer learning scenarios. Through comprehensive experiments on multiple datasets (MBIB Hate Speech, MBIB Political Bias, MBIB Gender Bias) and various model architectures (BERT, RoBERTa, GPT-2, Gemma, Phi), we reveal that transfer learning, while improving standard performance metrics, often leads to increased vulnerability to adversarial attacks. Our findings demonstrate that larger models exhibit greater resilience to this phenomenon, suggesting a complex interplay between model size, architecture, and adaptation methods. Our work highlights the crucial need for considering adversarial robustness in transfer learning scenarios and provides insights into maintaining model security without compromising performance. These findings have significant implications for the development and deployment of LLMs in real-world applications where both performance and robustness are paramount.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Pixels to Predicates: Learning Symbolic World Models via Pretrained Vision-Language Models</title>
<link>https://arxiv.org/abs/2501.00296</link>
<guid>https://arxiv.org/abs/2501.00296</guid>
<content:encoded><![CDATA[
arXiv:2501.00296v2 Announce Type: replace-cross 
Abstract: Our aim is to learn to solve long-horizon decision-making problems in complex robotics domains given low-level skills and a handful of short-horizon demonstrations containing sequences of images. To this end, we focus on learning abstract symbolic world models that facilitate zero-shot generalization to novel goals via planning. A critical component of such models is the set of symbolic predicates that define properties of and relationships between objects. In this work, we leverage pretrained vision language models (VLMs) to propose a large set of visual predicates potentially relevant for decision-making, and to evaluate those predicates directly from camera images. At training time, we pass the proposed predicates and demonstrations into an optimization-based model-learning algorithm to obtain an abstract symbolic world model that is defined in terms of a compact subset of the proposed predicates. At test time, given a novel goal in a novel setting, we use the VLM to construct a symbolic description of the current world state, and then use a search-based planning algorithm to find a sequence of low-level skills that achieves the goal. We demonstrate empirically across experiments in both simulation and the real world that our method can generalize aggressively, applying its learned world model to solve problems with a wide variety of object types, arrangements, numbers of objects, and visual backgrounds, as well as novel goals and much longer horizons than those seen at training time.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiC: Rethinking Conv3x3 Designs in Diffusion Models</title>
<link>https://arxiv.org/abs/2501.00603</link>
<guid>https://arxiv.org/abs/2501.00603</guid>
<content:encoded><![CDATA[
arXiv:2501.00603v2 Announce Type: replace-cross 
Abstract: Diffusion models have shown exceptional performance in visual generation tasks. Recently, these models have shifted from traditional U-Shaped CNN-Attention hybrid structures to fully transformer-based isotropic architectures. While these transformers exhibit strong scalability and performance, their reliance on complicated self-attention operation results in slow inference speeds. Contrary to these works, we rethink one of the simplest yet fastest module in deep learning, 3x3 Convolution, to construct a scaled-up purely convolutional diffusion model. We first discover that an Encoder-Decoder Hourglass design outperforms scalable isotropic architectures for Conv3x3, but still under-performing our expectation. Further improving the architecture, we introduce sparse skip connections to reduce redundancy and improve scalability. Based on the architecture, we introduce conditioning improvements including stage-specific embeddings, mid-block condition injection, and conditional gating. These improvements lead to our proposed Diffusion CNN (DiC), which serves as a swift yet competitive diffusion architecture baseline. Experiments on various scales and settings show that DiC surpasses existing diffusion transformers by considerable margins in terms of performance while keeping a good speed advantage. Project page: https://github.com/YuchuanTian/DiC
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Eliciting In-context Retrieval and Reasoning for Long-context Large Language Models</title>
<link>https://arxiv.org/abs/2501.08248</link>
<guid>https://arxiv.org/abs/2501.08248</guid>
<content:encoded><![CDATA[
arXiv:2501.08248v3 Announce Type: replace-cross 
Abstract: Recent advancements in long-context language models (LCLMs) promise to transform Retrieval-Augmented Generation (RAG) by simplifying pipelines. With their expanded context windows, LCLMs can process entire knowledge bases and perform retrieval and reasoning directly -- a capability we define as In-Context Retrieval and Reasoning (ICR^2). However, existing benchmarks like LOFT often overestimate LCLM performance by providing overly simplified contexts. To address this, we introduce ICR^2, a benchmark that evaluates LCLMs in more realistic scenarios by including confounding passages retrieved with strong retrievers. We then propose three methods to enhance LCLM performance: (1) retrieve-then-generate fine-tuning, (2) retrieval-attention-probing, which uses attention heads to filter and de-noise long contexts during decoding, and (3) joint retrieval head training alongside the generation head. Our evaluation of five well-known LCLMs on LOFT and ICR^2 demonstrates significant gains with our best approach applied to Mistral-7B: +17 and +15 points by Exact Match on LOFT, and +13 and +2 points on ICR^2, compared to vanilla RAG and supervised fine-tuning, respectively. It even outperforms GPT-4-Turbo on most tasks despite being a much smaller model.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ExLM: Rethinking the Impact of [MASK] Tokens in Masked Language Models</title>
<link>https://arxiv.org/abs/2501.13397</link>
<guid>https://arxiv.org/abs/2501.13397</guid>
<content:encoded><![CDATA[
arXiv:2501.13397v5 Announce Type: replace-cross 
Abstract: Masked Language Models (MLMs) have achieved remarkable success in many self-supervised representation learning tasks. MLMs are trained by randomly masking portions of the input sequences with [MASK] tokens and learning to reconstruct the original content based on the remaining context. This paper explores the impact of [MASK] tokens on MLMs. Analytical studies show that masking tokens can introduce the corrupted semantics problem, wherein the corrupted context may convey multiple, ambiguous meanings. This problem is also a key factor affecting the performance of MLMs on downstream tasks. Based on these findings, we propose a novel enhanced-context MLM, ExLM. Our approach expands [MASK] tokens in the input context and models the dependencies between these expanded states. This enhancement increases context capacity and enables the model to capture richer semantic information, effectively mitigating the corrupted semantics problem during pre-training. Experimental results demonstrate that ExLM achieves significant performance improvements in both text modeling and SMILES modeling tasks. Further analysis confirms that ExLM enriches semantic representations through context enhancement, and effectively reduces the semantic multimodality commonly observed in MLMs.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unraveling Token Prediction Refinement and Identifying Essential Layers in Language Models</title>
<link>https://arxiv.org/abs/2501.15054</link>
<guid>https://arxiv.org/abs/2501.15054</guid>
<content:encoded><![CDATA[
arXiv:2501.15054v2 Announce Type: replace-cross 
Abstract: This research aims to unravel how large language models (LLMs) iteratively refine token predictions through internal processing. We utilized a logit lens technique to analyze the model's token predictions derived from intermediate representations. Specifically, we focused on (1) how LLMs access and utilize information from input contexts, and (2) how positioning of relevant information affects the model's token prediction refinement process. On a multi-document question answering task with varying input context lengths, we found that the depth of prediction refinement (defined as the number of intermediate layers an LLM uses to transition from an initial correct token prediction to its final, stable correct output), as a function of the position of relevant information, exhibits an approximately inverted U-shaped curve. We also found that the gap between these two layers, on average, diminishes when relevant information is positioned at the beginning or end of the input context. This suggested that the model requires more refinements when processing longer contexts with relevant information situated in the middle. Furthermore, our findings indicate that not all layers are equally essential for determining final correct outputs. Our analysis provides insights into how token predictions are distributed across different conditions, and establishes important connections to existing hypotheses and previous findings in AI safety research and development.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Sobolev IPM for Probability Measures on a Graph</title>
<link>https://arxiv.org/abs/2502.00737</link>
<guid>https://arxiv.org/abs/2502.00737</guid>
<content:encoded><![CDATA[
arXiv:2502.00737v2 Announce Type: replace-cross 
Abstract: We investigate the Sobolev IPM problem for probability measures supported on a graph metric space. Sobolev IPM is an important instance of integral probability metrics (IPM), and is obtained by constraining a critic function within a unit ball defined by the Sobolev norm. In particular, it has been used to compare probability measures and is crucial for several theoretical works in machine learning. However, to our knowledge, there are no efficient algorithmic approaches to compute Sobolev IPM effectively, which hinders its practical applications. In this work, we establish a relation between Sobolev norm and weighted $L^p$-norm, and leverage it to propose a \emph{novel regularization} for Sobolev IPM. By exploiting the graph structure, we demonstrate that the regularized Sobolev IPM provides a \emph{closed-form} expression for fast computation. This advancement addresses long-standing computational challenges, and paves the way to apply Sobolev IPM for practical applications, even in large-scale settings. Additionally, the regularized Sobolev IPM is negative definite. Utilizing this property, we design positive-definite kernels upon the regularized Sobolev IPM, and provide preliminary evidences of their advantages for comparing probability measures on a given graph for document classification and topological data analysis.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Thought Models with Variational Bayes Inference-Time Computation</title>
<link>https://arxiv.org/abs/2502.01567</link>
<guid>https://arxiv.org/abs/2502.01567</guid>
<content:encoded><![CDATA[
arXiv:2502.01567v2 Announce Type: replace-cross 
Abstract: We propose a novel class of language models, Latent Thought Models (LTMs), which incorporate explicit latent thought vectors that follow an explicit prior model in latent space. These latent thought vectors guide the autoregressive generation of ground tokens through a Transformer decoder. Training employs a dual-rate optimization process within the classical variational Bayes framework: fast learning of local variational parameters for the posterior distribution of latent vectors (inference-time computation), and slow learning of global decoder parameters. Empirical studies reveal that LTMs possess additional scaling dimensions beyond traditional Large Language Models (LLMs), such as the number of iterations in inference-time computation and number of latent thought vectors. Higher sample efficiency can be achieved by increasing training compute per token, with further gains possible by trading model size for more inference steps. Designed based on these scaling properties, LTMs demonstrate superior sample and parameter efficiency compared to autoregressive models and discrete diffusion models. They significantly outperform these counterparts in validation perplexity and zero-shot language modeling tasks. Additionally, LTMs exhibit emergent few-shot in-context reasoning capabilities that scale with model size, and achieve competitive performance in conditional and unconditional text generation.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsafe LLM-Based Search: Quantitative Analysis and Mitigation of Safety Risks in AI Web Search</title>
<link>https://arxiv.org/abs/2502.04951</link>
<guid>https://arxiv.org/abs/2502.04951</guid>
<content:encoded><![CDATA[
arXiv:2502.04951v2 Announce Type: replace-cross 
Abstract: Recent advancements in Large Language Models (LLMs) have significantly enhanced the capabilities of AI-Powered Search Engines (AIPSEs), offering precise and efficient responses by integrating external databases with pre-existing knowledge. However, we observe that these AIPSEs raise risks such as quoting malicious content or citing malicious websites, leading to harmful or unverified information dissemination. In this study, we conduct the first safety risk quantification on seven production AIPSEs by systematically defining the threat model, risk type, and evaluating responses to various query types. With data collected from PhishTank, ThreatBook, and LevelBlue, our findings reveal that AIPSEs frequently generate harmful content that contains malicious URLs even with benign queries (e.g., with benign keywords). We also observe that directly querying a URL will increase the number of main risk-inclusive responses, while querying with natural language will slightly mitigate such risk. Compared to traditional search engines, AIPSEs outperform in both utility and safety. We further perform two case studies on online document spoofing and phishing to show the ease of deceiving AIPSEs in the real-world setting. To mitigate these risks, we develop an agent-based defense with a GPT-4.1-based content refinement tool and a URL detector. Our evaluation shows that our defense can effectively reduce the risk, with only a minor cost of reducing available information by approximately 10.7%. Our research highlights the urgent need for robust safety measures in AIPSEs.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMRS: advocating a unified reporting standard for surrogate models in the artificial intelligence era</title>
<link>https://arxiv.org/abs/2502.06753</link>
<guid>https://arxiv.org/abs/2502.06753</guid>
<content:encoded><![CDATA[
arXiv:2502.06753v2 Announce Type: replace-cross 
Abstract: Surrogate models are widely used to approximate complex systems across science and engineering to reduce computational costs. Despite their widespread adoption, the field lacks standardisation across key stages of the modelling pipeline, including data sampling, model selection, evaluation, and downstream analysis. This fragmentation limits reproducibility and cross-domain utility -- a challenge further exacerbated by the rapid proliferation of AI-driven surrogate models. We argue for the urgent need to establish a structured reporting standard, the Surrogate Model Reporting Specification (SMRS), that systematically captures essential design and evaluation choices while remaining agnostic to implementation specifics. By promoting a standardised yet flexible framework, we aim to improve the reliability of surrogate modelling, foster interdisciplinary knowledge transfer, and, as a result, accelerate scientific progress in the AI era.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Algorithmic Aspects of Strategic Trading</title>
<link>https://arxiv.org/abs/2502.07606</link>
<guid>https://arxiv.org/abs/2502.07606</guid>
<content:encoded><![CDATA[
arXiv:2502.07606v2 Announce Type: replace-cross 
Abstract: Algorithmic trading in modern financial markets is widely acknowledged to exhibit strategic, game-theoretic behaviors whose complexity can be difficult to model. A recent series of papers (Chriss, 2024b,c,a, 2025) has made progress in the setting of trading for position building. Here parties wish to buy or sell a fixed number of shares in a fixed time period in the presence of both temporary and permanent market impact, resulting in exponentially large strategy spaces. While these papers primarily consider the existence and structural properties of equilibrium strategies, in this work we focus on the algorithmic aspects of the proposed model. We give an efficient algorithm for computing best responses, and show that while the temporary impact only setting yields a potential game, best response dynamics do not generally converge for the general setting, for which no fast algorithm for (Nash) equilibrium computation is known. This leads us to consider the broader notion of Coarse Correlated Equilibria (CCE), which we show can be computed efficiently via an implementation of Follow the Perturbed Leader (FTPL). We illustrate the model and our results with an experimental investigation, where FTPL exhibits interesting behavior in different regimes of the relative weighting between temporary and permanent market impact.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Incentives Backfire, Data Stops Being Human</title>
<link>https://arxiv.org/abs/2502.07732</link>
<guid>https://arxiv.org/abs/2502.07732</guid>
<content:encoded><![CDATA[
arXiv:2502.07732v2 Announce Type: replace-cross 
Abstract: Progress in AI has relied on human-generated data, from annotator marketplaces to the wider Internet. However, the widespread use of large language models now threatens the quality and integrity of human-generated data on these very platforms. We argue that this issue goes beyond the immediate challenge of filtering AI-generated content -- it reveals deeper flaws in how data collection systems are designed. Existing systems often prioritize speed, scale, and efficiency at the cost of intrinsic human motivation, leading to declining engagement and data quality. We propose that rethinking data collection systems to align with contributors' intrinsic motivations -- rather than relying solely on external incentives -- can help sustain high-quality data sourcing at scale while maintaining contributor trust and long-term participation.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the kernel learning problem</title>
<link>https://arxiv.org/abs/2502.11665</link>
<guid>https://arxiv.org/abs/2502.11665</guid>
<content:encoded><![CDATA[
arXiv:2502.11665v2 Announce Type: replace-cross 
Abstract: The classical kernel ridge regression problem aims to find the best fit for the output $Y$ as a function of the input data $X\in \mathbb{R}^d$, with a fixed choice of regularization term imposed by a given choice of a reproducing kernel Hilbert space, such as a Sobolev space. Here we consider a generalization of the kernel ridge regression problem, by introducing an extra matrix parameter $U$, which aims to detect the scale parameters and the feature variables in the data, and thereby improve the efficiency of kernel ridge regression. This naturally leads to a nonlinear variational problem to optimize the choice of $U$. We study various foundational mathematical aspects of this variational problem, and in particular how this behaves in the presence of multiscale structures in the data.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaSplash: Adaptive Sparse Flash Attention</title>
<link>https://arxiv.org/abs/2502.12082</link>
<guid>https://arxiv.org/abs/2502.12082</guid>
<content:encoded><![CDATA[
arXiv:2502.12082v2 Announce Type: replace-cross 
Abstract: The computational cost of softmax-based attention in transformers limits their applicability to long-context tasks. Adaptive sparsity, of which $\alpha$-entmax attention is an example, offers a flexible data-dependent alternative, but existing implementations are inefficient and do not leverage the sparsity to obtain runtime and memory gains. In this work, we propose AdaSplash, which combines the efficiency of GPU-optimized algorithms with the sparsity benefits of $\alpha$-entmax. We first introduce a hybrid Halley-bisection algorithm, resulting in a 7-fold reduction in the number of iterations needed to compute the $\alpha$-entmax transformation. Then, we implement custom Triton kernels to efficiently handle adaptive sparsity. Experiments with RoBERTa and ModernBERT for text classification and single-vector retrieval, along with GPT-2 for language modeling, show that our method achieves substantial improvements in runtime and memory efficiency compared to existing $\alpha$-entmax implementations. It approaches -- and in some cases surpasses -- the efficiency of highly optimized softmax implementations like FlashAttention-2, enabling long-context training while maintaining strong task performance.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RestoreGrad: Signal Restoration Using Conditional Denoising Diffusion Models with Jointly Learned Prior</title>
<link>https://arxiv.org/abs/2502.13574</link>
<guid>https://arxiv.org/abs/2502.13574</guid>
<content:encoded><![CDATA[
arXiv:2502.13574v3 Announce Type: replace-cross 
Abstract: Denoising diffusion probabilistic models (DDPMs) can be utilized to recover a clean signal from its degraded observation(s) by conditioning the model on the degraded signal. The degraded signals are themselves contaminated versions of the clean signals; due to this correlation, they may encompass certain useful information about the target clean data distribution. However, existing adoption of the standard Gaussian as the prior distribution in turn discards such information when shaping the prior, resulting in sub-optimal performance. In this paper, we propose to improve conditional DDPMs for signal restoration by leveraging a more informative prior that is jointly learned with the diffusion model. The proposed framework, called RestoreGrad, seamlessly integrates DDPMs into the variational autoencoder (VAE) framework, taking advantage of the correlation between the degraded and clean signals to encode a better diffusion prior. On speech and image restoration tasks, we show that RestoreGrad demonstrates faster convergence (5-10 times fewer training steps) to achieve better quality of restored signals over existing DDPM baselines and improved robustness to using fewer sampling steps in inference time (2-2.5 times fewer), advocating the advantages of leveraging jointly learned prior for efficiency improvements in the diffusion process.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prediction-Powered Adaptive Shrinkage Estimation</title>
<link>https://arxiv.org/abs/2502.14166</link>
<guid>https://arxiv.org/abs/2502.14166</guid>
<content:encoded><![CDATA[
arXiv:2502.14166v2 Announce Type: replace-cross 
Abstract: Prediction-Powered Inference (PPI) is a powerful framework for enhancing statistical estimates by combining limited gold-standard data with machine learning (ML) predictions. While prior work has demonstrated PPI's benefits for individual statistical problems, modern applications require answering numerous parallel statistical questions. We introduce Prediction-Powered Adaptive Shrinkage (PAS), a method that bridges PPI with empirical Bayes shrinkage to improve the estimation of multiple means. PAS debiases noisy ML predictions within each task and then borrows strength across tasks by using those same predictions as a reference point for shrinkage. The amount of shrinkage is determined by minimizing an unbiased estimate of risk, and we prove that this tuning strategy is asymptotically optimal. Experiments on both synthetic and real-world datasets show that PAS adapts to the reliability of the ML predictions and outperforms traditional and modern baselines in large-scale applications.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving the Diffusability of Autoencoders</title>
<link>https://arxiv.org/abs/2502.14831</link>
<guid>https://arxiv.org/abs/2502.14831</guid>
<content:encoded><![CDATA[
arXiv:2502.14831v3 Announce Type: replace-cross 
Abstract: Latent diffusion models have emerged as the leading approach for generating high-quality images and videos, utilizing compressed latent representations to reduce the computational burden of the diffusion process. While recent advancements have primarily focused on scaling diffusion backbones and improving autoencoder reconstruction quality, the interaction between these components has received comparatively less attention. In this work, we perform a spectral analysis of modern autoencoders and identify inordinate high-frequency components in their latent spaces, which are especially pronounced in the autoencoders with a large bottleneck channel size. We hypothesize that this high-frequency component interferes with the coarse-to-fine nature of the diffusion synthesis process and hinders the generation quality. To mitigate the issue, we propose scale equivariance: a simple regularization strategy that aligns latent and RGB spaces across frequencies by enforcing scale equivariance in the decoder. It requires minimal code changes and only up to 20K autoencoder fine-tuning steps, yet significantly improves generation quality, reducing FID by 19% for image generation on ImageNet-1K $256^2$ and FVD by at least 44% for video generation on Kinetics-700 $17 \times 256^2$. The source code is available at https://github.com/snap-research/diffusability.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tensor Product Neural Networks for Functional ANOVA Model</title>
<link>https://arxiv.org/abs/2502.15215</link>
<guid>https://arxiv.org/abs/2502.15215</guid>
<content:encoded><![CDATA[
arXiv:2502.15215v4 Announce Type: replace-cross 
Abstract: Interpretability for machine learning models is becoming more and more important as machine learning models become more complex. The functional ANOVA model, which decomposes a high-dimensional function into a sum of lower dimensional functions (commonly referred to as components), is one of the most popular tools for interpretable AI, and recently, various neural networks have been developed for estimating each component in the functional ANOVA model. However, such neural networks are highly unstable when estimating each component since the components themselves are not uniquely defined. That is, there are multiple functional ANOVA decompositions for a given function. In this paper, we propose a novel neural network which guarantees a unique functional ANOVA decomposition and thus is able to estimate each component stably and accurately. We call our proposed neural network ANOVA Tensor Product Neural Network (ANOVA-TPNN) since it is motivated by the tensor product basis expansion. Theoretically, we prove that ANOVA-TPNN can approximate any smooth function well. Empirically, we show that ANOVA-TPNN provide much more stable estimation of each component and thus much more stable interpretation when training data and initial values of the model parameters vary than existing neural networks do.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MindLLM: A Subject-Agnostic and Versatile Model for fMRI-to-Text Decoding</title>
<link>https://arxiv.org/abs/2502.15786</link>
<guid>https://arxiv.org/abs/2502.15786</guid>
<content:encoded><![CDATA[
arXiv:2502.15786v2 Announce Type: replace-cross 
Abstract: Decoding functional magnetic resonance imaging (fMRI) signals into text has been a key challenge in the neuroscience community, with the potential to advance brain-computer interfaces and uncover deeper insights into brain mechanisms. However, existing approaches often struggle with suboptimal predictive performance, limited task variety, and poor generalization across subjects. In response to this, we propose MindLLM, a model designed for subject-agnostic and versatile fMRI-to-text decoding. MindLLM consists of an fMRI encoder and an off-the-shelf LLM. The fMRI encoder employs a neuroscience-informed attention mechanism, which is capable of accommodating subjects with varying input shapes and thus achieves high-performance subject-agnostic decoding. Moreover, we introduce Brain Instruction Tuning (BIT), a novel approach that enhances the model's ability to capture diverse semantic representations from fMRI signals, facilitating more versatile decoding. We evaluate MindLLM on comprehensive fMRI-to-text benchmarks. Results demonstrate that our model outperforms the baselines, improving downstream tasks by 12.0%, unseen subject generalization by 24.5%, and novel task adaptation by 25.0%. Furthermore, the attention patterns in MindLLM provide interpretable insights into its decision-making process.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dialogue Without Limits: Constant-Sized KV Caches for Extended Responses in LLMs</title>
<link>https://arxiv.org/abs/2503.00979</link>
<guid>https://arxiv.org/abs/2503.00979</guid>
<content:encoded><![CDATA[
arXiv:2503.00979v2 Announce Type: replace-cross 
Abstract: Autoregressive Transformers rely on Key-Value (KV) caching to accelerate inference. However, the linear growth of the KV cache with context length leads to excessive memory consumption and bandwidth constraints. This bottleneck is particularly problematic in real-time applications -- such as chatbots and interactive assistants -- where low latency and high memory efficiency are critical. Existing methods drop distant tokens or compress states in a lossy manner, sacrificing accuracy by discarding vital context or introducing bias.
  We propose MorphKV, an inference-time technique that maintains a constant-sized KV cache while preserving accuracy. MorphKV balances long-range dependencies and local coherence during text generation. It eliminates early-token bias while retaining high-fidelity context by adaptively ranking tokens through correlation-aware selection. Unlike heuristic retention or lossy compression, MorphKV iteratively refines the KV cache via lightweight updates guided by attention patterns of recent tokens. This approach captures inter-token correlation with greater accuracy, crucial for tasks like content creation and code generation. Our studies on long-response tasks show 52.9$\%$ memory savings and 18.2$\%$ higher accuracy on average compared to state-of-the-art prior works, enabling efficient real-world deployment.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Autonomous Reinforcement Learning for Real-World Robotic Manipulation with Large Language Models</title>
<link>https://arxiv.org/abs/2503.04280</link>
<guid>https://arxiv.org/abs/2503.04280</guid>
<content:encoded><![CDATA[
arXiv:2503.04280v3 Announce Type: replace-cross 
Abstract: Recent advancements in Large Language Models (LLMs) and Visual Language Models (VLMs) have significantly impacted robotics, enabling high-level semantic motion planning applications. Reinforcement Learning (RL), a complementary paradigm, enables agents to autonomously optimize complex behaviors through interaction and reward signals. However, designing effective reward functions for RL remains challenging, especially in real-world tasks where sparse rewards are insufficient and dense rewards require elaborate design. In this work, we propose Autonomous Reinforcement learning for Complex Human-Informed Environments (ARCHIE), an unsupervised pipeline leveraging GPT-4, a pre-trained LLM, to generate reward functions directly from natural language task descriptions. The rewards are used to train RL agents in simulated environments, where we formalize the reward generation process to enhance feasibility. Additionally, GPT-4 automates the coding of task success criteria, creating a fully automated, one-shot procedure for translating human-readable text into deployable robot skills. Our approach is validated through extensive simulated experiments on single-arm and bi-manual manipulation tasks using an ABB YuMi collaborative robot, highlighting its practicality and effectiveness. Tasks are demonstrated on the real robot setup.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized Interpolating Discrete Diffusion</title>
<link>https://arxiv.org/abs/2503.04482</link>
<guid>https://arxiv.org/abs/2503.04482</guid>
<content:encoded><![CDATA[
arXiv:2503.04482v2 Announce Type: replace-cross 
Abstract: While state-of-the-art language models achieve impressive results through next-token prediction, they have inherent limitations such as the inability to revise already generated tokens. This has prompted exploration of alternative approaches such as discrete diffusion. However, masked diffusion, which has emerged as a popular choice due to its simplicity and effectiveness, reintroduces this inability to revise words. To overcome this, we generalize masked diffusion, deriving a new family of general interpolating discrete diffusion (GIDD) which offers greater flexibility in the design of the noising processes. Leveraging a novel diffusion ELBO, we achieve compute-matched state-of-the-art performance in diffusion language modeling. Exploiting GIDD's flexibility, we explore a hybrid approach combining masking and uniform noise, leading to improved sample quality and unlocking the ability for the model to correct its own mistakes, an area where autoregressive models notoriously have struggled. Code: https://github.com/dvruette/gidd/
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfoSEM: A Deep Generative Model with Informative Priors for Gene Regulatory Network Inference</title>
<link>https://arxiv.org/abs/2503.04483</link>
<guid>https://arxiv.org/abs/2503.04483</guid>
<content:encoded><![CDATA[
arXiv:2503.04483v2 Announce Type: replace-cross 
Abstract: Inferring Gene Regulatory Networks (GRNs) from gene expression data is crucial for understanding biological processes. While supervised models are reported to achieve high performance for this task, they rely on costly ground truth (GT) labels and risk learning gene-specific biases, such as class imbalances of GT interactions, rather than true regulatory mechanisms. To address these issues, we introduce InfoSEM, an unsupervised generative model that leverages textual gene embeddings as informative priors, improving GRN inference without GT labels. InfoSEM can also integrate GT labels as an additional prior when available, avoiding biases and further enhancing performance. Additionally, we propose a biologically motivated benchmarking framework that better reflects real-world applications such as biomarker discovery and reveals learned biases of existing supervised methods. InfoSEM outperforms existing models by 38.5% across four datasets using textual embeddings prior and further boosts performance by 11.1% when integrating labeled data as priors.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sentence-level Reward Model can Generalize Better for Aligning LLM from Human Preference</title>
<link>https://arxiv.org/abs/2503.04793</link>
<guid>https://arxiv.org/abs/2503.04793</guid>
<content:encoded><![CDATA[
arXiv:2503.04793v2 Announce Type: replace-cross 
Abstract: Learning reward models from human preference datasets and subsequently optimizing language models via reinforcement learning has emerged as a fundamental paradigm for aligning LLMs with human preferences. The performance of the reward model plays a crucial role in the effectiveness of alignment. Previous reward models operate at a coarse-grained level, requiring the generation of a complete response to obtain a reward value. The sparse reward may present challenges for downstream reinforcement learning. While recent efforts have attempted to learn token-level reward models, the lack of explicit semantic information makes it difficult to model the credit of every individual token. In this paper, we propose assigning scores to every sentence, introducing an intermediate-grained reward model. By segmenting the complete response into sentences and applying differential operations to reward output at the start and end positions of each sentence, we can effectively model the rewards of sentences. Moreover, a novel attention mechanism is introduced to aggregate the scores of all sentences into a response-level score, which allows it to be trained using the Bradley-Terry model. On common benchmarks, our method outperforms the response-level reward model by 2.7% on RewardBench (for reward modeling evaluation) and surpasses all baselines on AlpacaEval (for alignment evaluation).
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-based Framework for Robust Model-Based Connector Mating in Robotic Wire Harness Installation</title>
<link>https://arxiv.org/abs/2503.09409</link>
<guid>https://arxiv.org/abs/2503.09409</guid>
<content:encoded><![CDATA[
arXiv:2503.09409v2 Announce Type: replace-cross 
Abstract: Despite the widespread adoption of industrial robots in automotive assembly, wire harness installation remains a largely manual process, as it requires precise and flexible manipulation. To address this challenge, we design a novel AI-based framework that automates cable connector mating by integrating force control with deep visuotactile learning. Our system optimizes search-and-insertion strategies using first-order optimization over a multimodal transformer architecture trained on visual, tactile, and proprioceptive data. Additionally, we design a novel automated data collection and optimization pipeline that minimizes the need for machine learning expertise. The framework optimizes robot programs that run natively on standard industrial controllers, permitting human experts to audit and certify them. Experimental validations on a center console assembly task demonstrate significant improvements in cycle times and robustness compared to conventional robot programming approaches. Videos are available under https://claudius-kienle.github.io/AppMuTT.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The global convergence time of stochastic gradient descent in non-convex landscapes: Sharp estimates via large deviations</title>
<link>https://arxiv.org/abs/2503.16398</link>
<guid>https://arxiv.org/abs/2503.16398</guid>
<content:encoded><![CDATA[
arXiv:2503.16398v2 Announce Type: replace-cross 
Abstract: In this paper, we examine the time it takes for stochastic gradient descent (SGD) to reach the global minimum of a general, non-convex loss function. We approach this question through the lens of randomly perturbed dynamical systems and large deviations theory, and we provide a tight characterization of the global convergence time of SGD via matching upper and lower bounds. These bounds are dominated by the most "costly" set of obstacles that the algorithm may need to overcome in order to reach a global minimizer from a given initialization, coupling in this way the global geometry of the underlying loss landscape with the statistics of the noise entering the process. Finally, motivated by applications to the training of deep neural networks, we also provide a series of refinements and extensions of our analysis for loss functions with shallow local minima.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Imagine to Hear: Auditory Knowledge Generation can be an Effective Assistant for Language Models</title>
<link>https://arxiv.org/abs/2503.16853</link>
<guid>https://arxiv.org/abs/2503.16853</guid>
<content:encoded><![CDATA[
arXiv:2503.16853v2 Announce Type: replace-cross 
Abstract: Language models pretrained on text-only corpora often struggle with tasks that require auditory commonsense knowledge. Previous work addresses this problem by augmenting the language model to retrieve knowledge from external audio databases. This approach has several limitations, such as the potential lack of relevant audio in databases and the high costs associated with constructing the databases. To address these issues, we propose Imagine to Hear, a novel approach that dynamically generates auditory knowledge using generative models. Our framework detects multiple audio-related textual spans from the given prompt and generates corresponding auditory knowledge. We develop several mechanisms to efficiently process multiple auditory knowledge, including a CLAP-based rejection sampler and a language-audio fusion module. Our experiments show that our method achieves state-of-the-art performance on AuditoryBench without relying on external databases, highlighting the effectiveness of our generation-based approach.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ThinkEdit: Interpretable Weight Editing to Mitigate Overly Short Thinking in Reasoning Models</title>
<link>https://arxiv.org/abs/2503.22048</link>
<guid>https://arxiv.org/abs/2503.22048</guid>
<content:encoded><![CDATA[
arXiv:2503.22048v3 Announce Type: replace-cross 
Abstract: Recent studies have shown that Large Language Models (LLMs) augmented with chain-of-thought (CoT) reasoning demonstrate impressive problem-solving abilities. However, in this work, we identify a recurring issue where these models occasionally generate overly short reasoning, leading to degraded performance on even simple mathematical problems. Specifically, we investigate how reasoning length is embedded in the hidden representations of reasoning models and its impact on accuracy. Our analysis reveals that reasoning length is governed by a linear direction in the representation space, allowing us to induce overly short reasoning by steering the model along this direction. Building on this insight, we introduce \textbf{\textit{ThinkEdit}}, a simple yet effective weight-editing approach to mitigate the issue of overly short reasoning. We first identify a small subset of attention heads (approximately 4%) that predominantly drive short reasoning behavior. We then edit the output projection weights of these heads to remove the short reasoning direction. With changes to only 0.2% of the model's parameters, \textbf{\textit{ThinkEdit}} effectively reduces overly short reasoning and yields notable accuracy gains for short reasoning outputs (+6.39%), along with an overall improvement across multiple math benchmarks (+3.34%). Our findings provide new mechanistic insights into how reasoning length is controlled within LLMs and highlight the potential of fine-grained model interventions to improve reasoning quality. Our code is available at: https://github.com/Trustworthy-ML-Lab/ThinkEdit\
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models</title>
<link>https://arxiv.org/abs/2504.10415</link>
<guid>https://arxiv.org/abs/2504.10415</guid>
<content:encoded><![CDATA[
arXiv:2504.10415v2 Announce Type: replace-cross 
Abstract: Scientific equation discovery is a fundamental task in the history of scientific progress, enabling the derivation of laws governing natural phenomena. Recently, Large Language Models (LLMs) have gained interest for this task due to their potential to leverage embedded scientific knowledge for hypothesis generation. However, evaluating the true discovery capabilities of these methods remains challenging, as existing benchmarks often rely on common equations that are susceptible to memorization by LLMs, leading to inflated performance metrics that do not reflect discovery. In this paper, we introduce LLM-SRBench, a comprehensive benchmark with 239 challenging problems across four scientific domains specifically designed to evaluate LLM-based scientific equation discovery methods while preventing trivial memorization. Our benchmark comprises two main categories: LSR-Transform, which transforms common physical models into less common mathematical representations to test reasoning beyond memorized forms, and LSR-Synth, which introduces synthetic, discovery-driven problems requiring data-driven reasoning. Through extensive evaluation of several state-of-the-art methods, using both open and closed LLMs, we find that the best-performing system so far achieves only 31.5% symbolic accuracy. These findings highlight the challenges of scientific equation discovery, positioning LLM-SRBench as a valuable resource for future research.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment</title>
<link>https://arxiv.org/abs/2504.15585</link>
<guid>https://arxiv.org/abs/2504.15585</guid>
<content:encoded><![CDATA[
arXiv:2504.15585v4 Announce Type: replace-cross 
Abstract: The remarkable success of Large Language Models (LLMs) has illuminated a promising pathway toward achieving Artificial General Intelligence for both academic and industrial communities, owing to their unprecedented performance across various applications. As LLMs continue to gain prominence in both research and commercial domains, their security and safety implications have become a growing concern, not only for researchers and corporations but also for every nation. Currently, existing surveys on LLM safety primarily focus on specific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning phase, lacking a comprehensive understanding of the entire "lifechain" of LLMs. To address this gap, this paper introduces, for the first time, the concept of "full-stack" safety to systematically consider safety issues throughout the entire process of LLM training, deployment, and eventual commercialization. Compared to the off-the-shelf LLM safety surveys, our work demonstrates several distinctive advantages: (I) Comprehensive Perspective. We define the complete LLM lifecycle as encompassing data preparation, pre-training, post-training, deployment and final commercialization. To our knowledge, this represents the first safety survey to encompass the entire lifecycle of LLMs. (II) Extensive Literature Support. Our research is grounded in an exhaustive review of over 800+ papers, ensuring comprehensive coverage and systematic organization of security issues within a more holistic understanding. (III) Unique Insights. Through systematic literature analysis, we have developed reliable roadmaps and perspectives for each chapter. Our work identifies promising research directions, including safety in data generation, alignment techniques, model editing, and LLM-based agent systems. These insights provide valuable guidance for researchers pursuing future work in this field.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Parallelization of Message Passing Neural Network Potentials for Large-scale Molecular Dynamics</title>
<link>https://arxiv.org/abs/2505.06711</link>
<guid>https://arxiv.org/abs/2505.06711</guid>
<content:encoded><![CDATA[
arXiv:2505.06711v3 Announce Type: replace-cross 
Abstract: Machine learning potentials have achieved great success in accelerating atomistic simulations. Many of them relying on atom-centered local descriptors are natural for parallelization. More recent message passing neural network (MPNN) models have demonstrated their superior accuracy and become increasingly popular. However, efficiently parallelizing MPNN models across multiple nodes remains challenging, limiting their practical applications in large-scale simulations. Here, we propose an efficient parallel algorithm for MPNN models, in which additional data communication is minimized among local atoms only in each MP layer without redundant computation, thus scaling linearly with the layer number. Integrated with our recursively embedded atom neural network model, this algorithm demonstrates excellent strong scaling and weak scaling behaviors in several benchmark systems. This approach enables massive molecular dynamics simulations on MPNN models as fast as on strictly local models for over 100 million atoms, vastly extending the applicability of the MPNN potential to an unprecedented scale. This general parallelization framework can empower various MPNN models to efficiently simulate very large and complex systems.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forests for Differences: Robust Causal Inference Beyond Parametric DiD</title>
<link>https://arxiv.org/abs/2505.09706</link>
<guid>https://arxiv.org/abs/2505.09706</guid>
<content:encoded><![CDATA[
arXiv:2505.09706v2 Announce Type: replace-cross 
Abstract: This paper introduces the Difference-in-Differences Bayesian Causal Forest (DiD-BCF), a novel non-parametric model addressing key challenges in DiD estimation, such as staggered adoption and heterogeneous treatment effects. DiD-BCF provides a unified framework for estimating Average (ATE), Group-Average (GATE), and Conditional Average Treatment Effects (CATE). A core innovation, its Parallel Trends Assumption (PTA)-based reparameterization, enhances estimation accuracy and stability in complex panel data settings. Extensive simulations demonstrate DiD-BCF's superior performance over established benchmarks, particularly under non-linearity, selection biases, and effect heterogeneity. Applied to U.S. minimum wage policy, the model uncovers significant conditional treatment effect heterogeneity related to county population, insights obscured by traditional methods. DiD-BCF offers a robust and versatile tool for more nuanced causal inference in modern DiD applications.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measurement to Meaning: A Validity-Centered Framework for AI Evaluation</title>
<link>https://arxiv.org/abs/2505.10573</link>
<guid>https://arxiv.org/abs/2505.10573</guid>
<content:encoded><![CDATA[
arXiv:2505.10573v3 Announce Type: replace-cross 
Abstract: While the capabilities and utility of AI systems have advanced, rigorous norms for evaluating these systems have lagged. Grand claims, such as models achieving general reasoning capabilities, are supported with model performance on narrow benchmarks, like performance on graduate-level exam questions, which provide a limited and potentially misleading assessment. We provide a structured approach for reasoning about the types of evaluative claims that can be made given the available evidence. For instance, our framework helps determine whether performance on a mathematical benchmark is an indication of the ability to solve problems on math tests or instead indicates a broader ability to reason. Our framework is well-suited for the contemporary paradigm in machine learning, where various stakeholders provide measurements and evaluations that downstream users use to validate their claims and decisions. At the same time, our framework also informs the construction of evaluations designed to speak to the validity of the relevant claims. By leveraging psychometrics' breakdown of validity, evaluations can prioritize the most critical facets for a given claim, improving empirical utility and decision-making efficacy. We illustrate our framework through detailed case studies of vision and language model evaluations, highlighting how explicitly considering validity strengthens the connection between evaluation evidence and the claims being made.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BLEUBERI: BLEU is a surprisingly effective reward for instruction following</title>
<link>https://arxiv.org/abs/2505.11080</link>
<guid>https://arxiv.org/abs/2505.11080</guid>
<content:encoded><![CDATA[
arXiv:2505.11080v2 Announce Type: replace-cross 
Abstract: Reward models are central to aligning LLMs with human preferences, but they are costly to train, requiring large-scale human-labeled preference data and powerful pretrained LLM backbones. Meanwhile, the increasing availability of high-quality synthetic instruction-following datasets raises the question: can simpler, reference-based metrics serve as viable alternatives to reward models during RL-based alignment? In this paper, we show first that BLEU, a basic string-matching metric, surprisingly matches strong reward models in agreement with human preferences on general instruction-following datasets. Based on this insight, we develop BLEUBERI, a method that first identifies challenging instructions and then applies Group Relative Policy Optimization (GRPO) using BLEU directly as the reward function. We demonstrate that BLEUBERI-trained models are competitive with models trained via reward model-guided RL across four challenging instruction-following benchmarks and three different base language models. A human evaluation further supports that the quality of BLEUBERI model outputs is on par with those from reward model-aligned models. Moreover, BLEUBERI models generate outputs that are more factually grounded than competing methods. Overall, we show that given access to high-quality reference outputs (easily obtained via existing instruction-following datasets or synthetic data generation), string matching-based metrics are cheap yet effective proxies for reward models during alignment. We release our code and data at https://github.com/lilakk/BLEUBERI.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Missing Data Imputation by Reducing Mutual Information with Rectified Flows</title>
<link>https://arxiv.org/abs/2505.11749</link>
<guid>https://arxiv.org/abs/2505.11749</guid>
<content:encoded><![CDATA[
arXiv:2505.11749v2 Announce Type: replace-cross 
Abstract: This paper introduces a novel iterative method for missing data imputation that sequentially reduces the mutual information between data and their corresponding missing mask. Inspired by GAN-based approaches, which train generators to decrease the predictability of missingness patterns, our method explicitly targets the reduction of mutual information. Specifically, our algorithm iteratively minimizes the KL divergence between the joint distribution of the imputed data and missing mask, and the product of their marginals from the previous iteration. We show that the optimal imputation under this framework corresponds to solving an ODE, whose velocity field minimizes a rectified flow training objective. We further illustrate that some existing imputation techniques can be interpreted as approximate special cases of our mutual-information-reducing framework. Comprehensive experiments on synthetic and real-world datasets validate the efficacy of our proposed approach, demonstrating superior imputation performance.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LEANCODE: Understanding Models Better for Code Simplification of Pre-trained Large Language Models</title>
<link>https://arxiv.org/abs/2505.14759</link>
<guid>https://arxiv.org/abs/2505.14759</guid>
<content:encoded><![CDATA[
arXiv:2505.14759v3 Announce Type: replace-cross 
Abstract: Large Language Models for code often entail significant computational complexity, which grows significantly with the length of the input code sequence. We propose LeanCode for code simplification to reduce training and prediction time, leveraging code contexts in utilizing attention scores to represent the tokens' importance. We advocate for the selective removal of tokens based on the average context-aware attention scores rather than average scores across all inputs. LeanCode uses the attention scores of `CLS' tokens within the encoder for classification tasks, such as code search. It also employs the encoder-decoder attention scores to determine token significance for sequence-to-sequence tasks like code summarization. Our evaluation shows LeanCode's superiority over the SOTAs DietCode and Slimcode, with improvements of 60% and 16% for code search, and 29% and 27% for code summarization, respectively.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing Far and Clearly: Mitigating Hallucinations in MLLMs with Attention Causal Decoding</title>
<link>https://arxiv.org/abs/2505.16652</link>
<guid>https://arxiv.org/abs/2505.16652</guid>
<content:encoded><![CDATA[
arXiv:2505.16652v2 Announce Type: replace-cross 
Abstract: Recent advancements in multimodal large language models (MLLMs) have significantly improved performance in visual question answering. However, they often suffer from hallucinations. In this work, hallucinations are categorized into two main types: initial hallucinations and snowball hallucinations. We argue that adequate contextual information can be extracted directly from the token interaction process. Inspired by causal inference in the decoding strategy, we propose to leverage causal masks to establish information propagation between multimodal tokens. The hypothesis is that insufficient interaction between those tokens may lead the model to rely on outlier tokens, overlooking dense and rich contextual cues. Therefore, we propose to intervene in the propagation process by tackling outlier tokens to enhance in-context inference. With this goal, we present FarSight, a versatile plug-and-play decoding strategy to reduce attention interference from outlier tokens merely by optimizing the causal mask. The heart of our method is effective token propagation. We design an attention register structure within the upper triangular matrix of the causal mask, dynamically allocating attention to capture attention diverted to outlier tokens. Moreover, a positional awareness encoding method with a diminishing masking rate is proposed, allowing the model to attend to further preceding tokens, especially for video sequence tasks. With extensive experiments, FarSight demonstrates significant hallucination-mitigating performance across different MLLMs on both image and video benchmarks, proving its effectiveness.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Power-Law Decay Loss for Large Language Model Finetuning: A Theory Perspective</title>
<link>https://arxiv.org/abs/2505.16900</link>
<guid>https://arxiv.org/abs/2505.16900</guid>
<content:encoded><![CDATA[
arXiv:2505.16900v5 Announce Type: replace-cross 
Abstract: During the finetuning stage of text generation tasks, standard cross-entropy loss treats all tokens equally. This can lead models to overemphasize high-frequency, low-information tokens, neglecting lower-frequency tokens crucial for specificity and informativeness in generated content. This paper introduces a novel loss function, Power-Law Decay Loss (PDL), specifically designed to optimize the finetuning process for text generation. The core motivation for PDL stems from observations in information theory and linguistics: the informativeness of a token is often inversely proportional to its frequency of occurrence. PDL re-weights the contribution of each token in the standard cross-entropy loss based on its frequency in the training corpus, following a power-law decay. Specifically, the weights for high-frequency tokens are reduced, while low-frequency, information-dense tokens are assigned higher weights. This mechanism guides the model during finetuning to focus more on learning and generating tokens that convey specific and unique information, thereby enhancing the quality, diversity, and informativeness of the generated text. We theoretically elaborate on the motivation and construction of PDL and discuss its potential applications and advantages across various text generation finetuning tasks, such as abstractive summarization, dialogue systems, and style transfer.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Two LLMs Debate, Both Think They'll Win</title>
<link>https://arxiv.org/abs/2505.19184</link>
<guid>https://arxiv.org/abs/2505.19184</guid>
<content:encoded><![CDATA[
arXiv:2505.19184v3 Announce Type: replace-cross 
Abstract: Can LLMs accurately adjust their confidence when facing opposition? Building on previous studies measuring calibration on static fact-based question-answering tasks, we evaluate Large Language Models (LLMs) in a dynamic, adversarial debate setting, uniquely combining two realistic factors: (a) a multi-turn format requiring models to update beliefs as new information emerges, and (b) a zero-sum structure to control for task-related uncertainty, since mutual high-confidence claims imply systematic overconfidence. We organized 60 three-round policy debates among ten state-of-the-art LLMs, with models privately rating their confidence (0-100) in winning after each round. We observed five concerning patterns: (1) Systematic overconfidence: models began debates with average initial confidence of 72.9% vs. a rational 50% baseline. (2) Confidence escalation: rather than reducing confidence as debates progressed, debaters increased their win probabilities, averaging 83% by the final round. (3) Mutual overestimation: in 61.7% of debates, both sides simultaneously claimed >=75% probability of victory, a logical impossibility. (4) Persistent self-debate bias: models debating identical copies increased confidence from 64.1% to 75.2%; even when explicitly informed their chance of winning was exactly 50%, confidence still rose (from 50.0% to 57.1%). (5) Misaligned private reasoning: models' private scratchpad thoughts sometimes differed from their public confidence ratings, raising concerns about faithfulness of chain-of-thought reasoning. These results suggest LLMs lack the ability to accurately self-assess or update their beliefs in dynamic, multi-turn tasks; a major concern as LLMs are now increasingly deployed without careful review in assistant and agentic roles.
  Code for our experiments is available at https://github.com/pradyuprasad/llms_overconfidence
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>APE: Selective Fine-tuning with Acceptance Criteria for Language Model Adaptation</title>
<link>https://arxiv.org/abs/2505.19912</link>
<guid>https://arxiv.org/abs/2505.19912</guid>
<content:encoded><![CDATA[
arXiv:2505.19912v2 Announce Type: replace-cross 
Abstract: We present Adjacent Possible Exploration (APE), a selective fine-tuning method for adapting large language models that systematically explores parameter modifications while maintaining model stability. Inspired by evolutionary optimization principles, APE evaluates multiple candidate parameter updates through fine-tuning on small data subsets and accepts only those exceeding a performance threshold. Unlike standard fine-tuning that follows single gradient directions, APE implements a filtered selection process that prevents destabilizing parameter changes while enabling systematic improvement. Our method achieves 33.9\% BLEU improvement and 36.2\% perplexity reduction on news summarization tasks while using minimal computational resources. The approach provides a practical framework for controlled model adaptation that balances performance gains with representational stability.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling over Scaling: Exploring Test-Time Scaling Plateau in Large Reasoning Models</title>
<link>https://arxiv.org/abs/2505.20522</link>
<guid>https://arxiv.org/abs/2505.20522</guid>
<content:encoded><![CDATA[
arXiv:2505.20522v2 Announce Type: replace-cross 
Abstract: Large reasoning models (LRMs) have exhibited the capacity of enhancing reasoning performance via internal test-time scaling. Building upon this, a promising direction is to further scale test-time compute to unlock even greater reasoning capabilities. However, as we push these scaling boundaries, systematically understanding the practical limits and achieving optimal resource allocation becomes a critical challenge. In this paper, we investigate the scaling plateau of test-time scaling and introduce the Test-Time Scaling Performance Model (TTSPM). We theoretically analyze two fundamental paradigms for such extended scaling, parallel scaling and sequential scaling, from a probabilistic modeling perspective. Our primary contribution is the derivation of the saturation point on the scaling budget for both strategies, identifying thresholds beyond which additional computation yields diminishing returns. Remarkably, despite their distinct mechanisms, both paradigms converge to a unified mathematical structure in their upper bounds. We empirically validate our theoretical findings on challenging reasoning benchmarks, including AIME, MATH-500, and GPQA, demonstrating the practical utility of these bounds for test-time resource allocation. We hope that this work provides insights into the cost-benefit trade-offs of test-time scaling, guiding the development of more resource-efficient inference strategies for large reasoning models.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training Articulatory Inversion Models for Interspeaker Consistency</title>
<link>https://arxiv.org/abs/2505.20529</link>
<guid>https://arxiv.org/abs/2505.20529</guid>
<content:encoded><![CDATA[
arXiv:2505.20529v3 Announce Type: replace-cross 
Abstract: Acoustic-to-Articulatory Inversion (AAI) attempts to model the inverse mapping from speech to articulation. Exact articulatory prediction from speech alone may be impossible, as speakers can choose different forms of articulation seemingly without reference to their vocal tract structure. However, once a speaker has selected an articulatory form, their productions vary minimally. Recent works in AAI have proposed adapting Self-Supervised Learning (SSL) models to single-speaker datasets, claiming that these single-speaker models provide a universal articulatory template. In this paper, we investigate whether SSL-adapted models trained on single and multi-speaker data produce articulatory targets which are consistent across speaker identities for English and Russian. We do this through the use of a novel evaluation method which extracts articulatory targets using minimal pair sets. We also present a training method which can improve interspeaker consistency using only speech data.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hyperbolic recurrent neural network as the first type of non-Euclidean neural quantum state ansatz</title>
<link>https://arxiv.org/abs/2505.22083</link>
<guid>https://arxiv.org/abs/2505.22083</guid>
<content:encoded><![CDATA[
arXiv:2505.22083v2 Announce Type: replace-cross 
Abstract: In this work, we introduce the first type of non-Euclidean neural quantum state (NQS) ansatz, in the form of the hyperbolic GRU (a variant of recurrent neural networks (RNNs)), to be used in the Variational Monte Carlo method of approximating the ground state energy for quantum many-body systems. In particular, we examine the performances of NQS ansatzes constructed from both conventional or Euclidean RNN/GRU and from hyperbolic GRU in the prototypical settings of the one- and two-dimensional transverse field Ising models (TFIM) and the one-dimensional Heisenberg $J_1J_2$ and $J_1J_2J_3$ systems. By virtue of the fact that, for all of the experiments performed in this work, hyperbolic GRU can yield performances comparable to or better than Euclidean RNNs, which have been extensively studied in these settings in the literature, our work is a proof-of-concept for the viability of hyperbolic GRU as the first type of non-Euclidean NQS ansatz for quantum many-body systems. Furthermore, in settings where the Hamiltonian displays a clear hierarchical interaction structure, such as the 1D Heisenberg $J_1J_2$ & $J_1J_2J_3$ systems with the 1st, 2nd and even 3rd nearest neighbor interactions, our results show that hyperbolic GRU definitively outperforms its Euclidean version in all instances. The fact that these results are reminiscent of the established ones from natural language processing where hyperbolic GRU almost always outperforms Euclidean RNNs when the training data exhibit a tree-like or hierarchical structure leads us to hypothesize that hyperbolic GRU NQS ansatz would likely outperform Euclidean RNN/GRU NQS ansatz in quantum spin systems that involve different degrees of nearest neighbor interactions. Finally, with this work, we hope to initiate future studies of other types of non-Euclidean NQS beyond hyperbolic GRU.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Theoretical Foundations of the Deep Copula Classifier: A Generative Approach to Modeling Dependent Features</title>
<link>https://arxiv.org/abs/2505.22997</link>
<guid>https://arxiv.org/abs/2505.22997</guid>
<content:encoded><![CDATA[
arXiv:2505.22997v2 Announce Type: replace-cross 
Abstract: Traditional classifiers often assume feature independence or rely on overly simplistic relationships, leading to poor performance in settings where real-world dependencies matter. We introduce the Deep Copula Classifier (DCC), a generative model that separates the learning of each feature's marginal distribution from the modeling of their joint dependence structure via neural network-parameterized copulas. For each class, lightweight neural networks are used to flexibly and adaptively capture feature interactions, making DCC particularly effective when classification is driven by complex dependencies. We establish that DCC converges to the Bayes-optimal classifier under standard conditions and provide explicit convergence rates of O(n^{-r/(2r + d)}) for r-smooth copula densities. Beyond theoretical guarantees, we outline several practical extensions, including high-dimensional scalability through vine and factor copula architectures, semi-supervised learning via entropy regularization, and online adaptation using streaming gradient methods. By unifying statistical rigor with the representational power of neural networks, DCC offers a mathematically grounded and interpretable framework for dependency-aware classification.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diversity of Transformer Layers: One Aspect of Parameter Scaling Laws</title>
<link>https://arxiv.org/abs/2505.24009</link>
<guid>https://arxiv.org/abs/2505.24009</guid>
<content:encoded><![CDATA[
arXiv:2505.24009v2 Announce Type: replace-cross 
Abstract: Transformers deliver outstanding performance across a wide range of tasks and are now a dominant backbone architecture for large language models (LLMs). Their task-solving performance is improved by increasing parameter size, as shown in the recent studies on parameter scaling laws. Although recent mechanistic-interpretability studies have deepened our understanding of the internal behavior of Transformers by analyzing their residual stream, the relationship between these internal mechanisms and the parameter scaling laws remains unclear. To bridge this gap, we focus on layers and their size, which mainly decide the parameter size of Transformers. For this purpose, we first theoretically investigate the layers within the residual stream through a bias-diversity decomposition. The decomposition separates (i) bias, the error of each layer's output from the ground truth, and (ii) diversity, which indicates how much the outputs of each layer differ from each other. Analyzing Transformers under this theory reveals that performance improves when individual layers make predictions close to the correct answer and remain mutually diverse. We show that diversity becomes especially critical when individual layers' outputs are far from the ground truth. Finally, we introduce an information-theoretic diversity and show our main findings that adding layers enhances performance only when those layers behave differently, i.e., are diverse. We also reveal the performance gains from increasing the number of layers exhibit submodularity: marginal improvements diminish as additional layers increase, mirroring the logarithmic convergence predicted by the parameter scaling laws. Experiments on multiple semantic-understanding tasks with various LLMs empirically confirm the theoretical properties derived in this study.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equilibrium Distribution for t-Distributed Stochastic Neighbor Embedding with Generalized Kernels</title>
<link>https://arxiv.org/abs/2505.24311</link>
<guid>https://arxiv.org/abs/2505.24311</guid>
<content:encoded><![CDATA[
arXiv:2505.24311v2 Announce Type: replace-cross 
Abstract: T-distributed stochastic neighbor embedding (t-SNE) is a well-known algorithm for visualizing high-dimensional data by finding low-dimensional representations. In this paper, we study the convergence of t-SNE with generalized kernels and extend the results of Auffinger and Fletcher in 2023. Our work starts by giving a concrete formulation of generalized input and output kernels. Then we prove that under certain conditions, the t-SNE algorithm converges to an equilibrium distribution for a wide range of input and output kernels as the number of data points diverges.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SIFBench: An Extensive Benchmark for Fatigue Analysis</title>
<link>https://arxiv.org/abs/2506.01173</link>
<guid>https://arxiv.org/abs/2506.01173</guid>
<content:encoded><![CDATA[
arXiv:2506.01173v2 Announce Type: replace-cross 
Abstract: Fatigue-induced crack growth is a leading cause of structural failure across critical industries such as aerospace, civil engineering, automotive, and energy. Accurate prediction of stress intensity factors (SIFs) -- the key parameters governing crack propagation in linear elastic fracture mechanics -- is essential for assessing fatigue life and ensuring structural integrity. While machine learning (ML) has shown great promise in SIF prediction, its advancement has been severely limited by the lack of rich, transparent, well-organized, and high-quality datasets.
  To address this gap, we introduce SIFBench, an open-source, large-scale benchmark database designed to support ML-based SIF prediction. SIFBench contains over 5 million different crack and component geometries derived from high-fidelity finite element simulations across 37 distinct scenarios, and provides a unified Python interface for seamless data access and customization. We report baseline results using a range of popular ML models -- including random forests, support vector machines, feedforward neural networks, and Fourier neural operators -- alongside comprehensive evaluation metrics and template code for model training, validation, and assessment. By offering a standardized and scalable resource, SIFBench substantially lowers the entry barrier and fosters the development and application of ML methods in damage tolerance design and predictive maintenance.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Scientists Fail Without Strong Implementation Capability</title>
<link>https://arxiv.org/abs/2506.01372</link>
<guid>https://arxiv.org/abs/2506.01372</guid>
<content:encoded><![CDATA[
arXiv:2506.01372v2 Announce Type: replace-cross 
Abstract: The emergence of Artificial Intelligence (AI) Scientist represents a paradigm shift in scientific discovery, with large language models (LLMs) taking the lead as the primary executor in the entire scientific workflow from idea generation to experiment implementation. Recent AI Scientist studies demonstrate sufficient capabilities for independent scientific discovery, with the generated research reports gaining acceptance at the ICLR 2025 workshop and ACL 2025, arguing that a human-level AI Scientist, capable of uncovering phenomena previously unknown to humans, may be imminent. Despite this substantial progress, AI Scientist has yet to produce a groundbreaking achievement in the domain of computer science on par with automated scientific tools. Based on extensive quantitative evidence from existing benchmarks in complex engineering tasks and a systematic evaluation assess 28 research papers generated by five advanced AI Scientist systems, we argue that \textbf{the fundamental bottleneck for AI Scientists lies in their capability to execute the requisite verification procedures.} Current AI Scientist systems lack the execution capabilities needed to execute rigorous experiments and produce high-quality scientific papers. To better illustrate the root cause of this \textbf{implementation gap}, we provide an in-depth discussion on the fundamental limitations of AI Scientist. This position paper aims to call for the participants in the community to bridge the implementation gap.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Protap: A Benchmark for Protein Modeling on Realistic Downstream Applications</title>
<link>https://arxiv.org/abs/2506.02052</link>
<guid>https://arxiv.org/abs/2506.02052</guid>
<content:encoded><![CDATA[
arXiv:2506.02052v2 Announce Type: replace-cross 
Abstract: Recently, extensive deep learning architectures and pretraining strategies have been explored to support downstream protein applications. Additionally, domain-specific models incorporating biological knowledge have been developed to enhance performance in specialized tasks. In this work, we introduce $\textbf{Protap}$, a comprehensive benchmark that systematically compares backbone architectures, pretraining strategies, and domain-specific models across diverse and realistic downstream protein applications. Specifically, Protap covers five applications: three general tasks and two novel specialized tasks, i.e., enzyme-catalyzed protein cleavage site prediction and targeted protein degradation, which are industrially relevant yet missing from existing benchmarks. For each application, Protap compares various domain-specific models and general architectures under multiple pretraining settings. Our empirical studies imply that: (i) Though large-scale pretraining encoders achieve great results, they often underperform supervised encoders trained on small downstream training sets. (ii) Incorporating structural information during downstream fine-tuning can match or even outperform protein language models pretrained on large-scale sequence corpora. (iii) Domain-specific biological priors can enhance performance on specialized downstream tasks. Code and datasets are publicly available at https://github.com/Trust-App-AI-Lab/protap.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ensemble-MIX: Enhancing Sample Efficiency in Multi-Agent RL Using Ensemble Methods</title>
<link>https://arxiv.org/abs/2506.02841</link>
<guid>https://arxiv.org/abs/2506.02841</guid>
<content:encoded><![CDATA[
arXiv:2506.02841v2 Announce Type: replace-cross 
Abstract: Multi-agent reinforcement learning (MARL) methods have achieved state-of-the-art results on a range of multi-agent tasks. Yet, MARL algorithms typically require significantly more environment interactions than their single-agent counterparts to converge, a problem exacerbated by the difficulty in exploring over a large joint action space and the high variance intrinsic to MARL environments. To tackle these issues, we propose a novel algorithm that combines a decomposed centralized critic with decentralized ensemble learning, incorporating several key contributions. The main component in our scheme is a selective exploration method that leverages ensemble kurtosis. We extend the global decomposed critic with a diversity-regularized ensemble of individual critics and utilize its excess kurtosis to guide exploration toward high-uncertainty states and actions. To improve sample efficiency, we train the centralized critic with a novel truncated variation of the TD($\lambda$) algorithm, enabling efficient off-policy learning with reduced variance. On the actor side, our suggested algorithm adapts the mixed samples approach to MARL, mixing on-policy and off-policy loss functions for training the actors. This approach balances between stability and efficiency and outperforms purely off-policy learning. The evaluation shows our method outperforms state-of-the-art baselines on standard MARL benchmarks, including a variety of SMAC II maps.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overcoming Challenges of Partial Client Participation in Federated Learning : A Comprehensive Review</title>
<link>https://arxiv.org/abs/2506.02887</link>
<guid>https://arxiv.org/abs/2506.02887</guid>
<content:encoded><![CDATA[
<div> Learning, Federated, Partial participation, Data heterogeneity, Challenges  
Summary:  
This paper presents a survey on the impact of partial client participation in Federated Learning (FL). It addresses the lack of attention given to challenges arising from partial client participation in real-world scenarios. The focus is on existing FL methods designed to handle partial client participation, offering a thorough analysis with theoretical insights and empirical findings. The paper categorizes these methods based on their advantages and disadvantages. Key points covered include the collaborative training of a shared global model in FL without disclosing raw data, issues like generalization, robustness, and fairness due to data heterogeneity, and the practical and theoretical challenges of partial client participation in FL. Overall, the survey provides a comprehensive review of methods aimed at dealing with partial client participation in FL. <br /><br /> <div>
arXiv:2506.02887v2 Announce Type: replace 
Abstract: Federated Learning (FL) is a learning mechanism that falls under the distributed training umbrella, which collaboratively trains a shared global model without disclosing the raw data from different clients. This paper presents an extensive survey on the impact of partial client participation in federated learning. While much of the existing research focuses on addressing issues such as generalization, robustness, and fairness caused by data heterogeneity under the assumption of full client participation, limited attention has been given to the practical and theoretical challenges arising from partial client participation, which is common in real-world scenarios. This survey provides an in-depth review of existing FL methods designed to cope with partial client participation. We offer a comprehensive analysis supported by theoretical insights and empirical findings, along with a structured categorization of these methods, highlighting their respective advantages and disadvantages.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-Asymptotic Length Generalization</title>
<link>https://arxiv.org/abs/2506.03085</link>
<guid>https://arxiv.org/abs/2506.03085</guid>
<content:encoded><![CDATA[
<div> length generalization, non-asymptotic, Minimum-Complexity Interpolator, Context-Free Grammars, C-RASP 

Summary:
The paper discusses length generalization in learning algorithms, focusing on non-asymptotic scenarios. It formalizes the concept of length complexity, which is the minimum input length required for generalization. The Minimum-Complexity Interpolator algorithm is shown to achieve optimal length complexity. The decidability of language equivalence problems determines whether a function class allows non-asymptotic length generalization. Context-Free Grammars lack a computable upper bound for length complexity. On the positive side, the length complexity of Deterministic Finite Automata is determined. The study extends to a subset of transformer-related functions called C-RASP, with upper bounds of length complexity established for 1-layer and 2-layer functions. Specifically, the length complexity for these C-RASP functions is related to the precision and number of heads in the ground-truth function. <div>
arXiv:2506.03085v2 Announce Type: replace 
Abstract: Length generalization is the ability of a learning algorithm to learn a hypothesis which generalizes to longer inputs than the inputs in the training set. In this paper, we provide provable guarantees of length generalization for various classes of functions in an idealized setting. First, we formalize the framework of non-asymptotic length generalization, which requires a computable upper bound for the minimum input length that guarantees length generalization, as a function of the complexity of ground-truth function under some given complexity measure. We refer to this minimum input length to length generalize as length complexity. We show the Minimum-Complexity Interpolator learning algorithm achieves optimal length complexity. We further show that whether a function class admits non-asymptotic length generalization is equivalent to the decidability of its language equivalence problem, which implies that there is no computable upper bound for the length complexity of Context-Free Grammars. On the positive side, we show that the length complexity of Deterministic Finite Automata is $2n - 2$ where $n$ is the number of states of the ground-truth automaton. Our main results are upper bounds of length complexity for a subset of a transformer-related function class called C-RASP (Yang & Chiang, 2024). We show that the length complexity of 1-layer C-RASP functions is $O(T^2)$ when the ground-truth function has precision $T$, and that the length complexity of 2-layer C-RASP functions is $O(T^{O(K)})$ when the ground-truth function has precision $T$ and $K$ heads.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixture-of-Experts Meets In-Context Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.05426</link>
<guid>https://arxiv.org/abs/2506.05426</guid>
<content:encoded><![CDATA[
<div> Keywords: in-context reinforcement learning, mixture-of-experts, transformer-based models, token-wise MoE, task-wise MoE

Summary: 
The paper introduces T2MIR, a framework that incorporates mixture-of-experts (MoE) into transformer-based decision models for in-context reinforcement learning (ICRL). T2MIR addresses challenges of multi-modality state-action-reward data and diverse decision tasks by using a token-wise MoE to capture input semantics and a task-wise MoE to handle a wide task distribution. A contrastive learning method enhances task-wise routing by maximizing mutual information between tasks and router representations. Experimental results demonstrate T2MIR's effectiveness in improving in-context learning capacity compared to baselines. This work showcases the potential of MoE in advancing ICRL and offers a scalable architectural enhancement for tasks in language and vision domains.

<br /><br />Summary: <div>
arXiv:2506.05426v1 Announce Type: new 
Abstract: In-context reinforcement learning (ICRL) has emerged as a promising paradigm for adapting RL agents to downstream tasks through prompt conditioning. However, two notable challenges remain in fully harnessing in-context learning within RL domains: the intrinsic multi-modality of the state-action-reward data and the diverse, heterogeneous nature of decision tasks. To tackle these challenges, we propose \textbf{T2MIR} (\textbf{T}oken- and \textbf{T}ask-wise \textbf{M}oE for \textbf{I}n-context \textbf{R}L), an innovative framework that introduces architectural advances of mixture-of-experts (MoE) into transformer-based decision models. T2MIR substitutes the feedforward layer with two parallel layers: a token-wise MoE that captures distinct semantics of input tokens across multiple modalities, and a task-wise MoE that routes diverse tasks to specialized experts for managing a broad task distribution with alleviated gradient conflicts. To enhance task-wise routing, we introduce a contrastive learning method that maximizes the mutual information between the task and its router representation, enabling more precise capture of task-relevant information. The outputs of two MoE components are concatenated and fed into the next layer. Comprehensive experiments show that T2MIR significantly facilitates in-context learning capacity and outperforms various types of baselines. We bring the potential and promise of MoE to ICRL, offering a simple and scalable architectural enhancement to advance ICRL one step closer toward achievements in language and vision communities. Our code is available at https://github.com/NJU-RL/T2MIR.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MTPNet: Multi-Grained Target Perception for Unified Activity Cliff Prediction</title>
<link>https://arxiv.org/abs/2506.05427</link>
<guid>https://arxiv.org/abs/2506.05427</guid>
<content:encoded><![CDATA[
<div> Prediction, Drug discovery, Material design, MTPNet, GNN architectures 

Summary:
The paper introduces MTPNet, a novel framework for activity cliff prediction in drug discovery and material design. MTPNet incorporates prior knowledge of interactions between molecules and target proteins through Macro-level Target Semantic (MTS) and Micro-level Pocket Semantic (MPS) guidance. By dynamically optimizing molecular representations using multi-grained protein semantic conditions, MTPNet effectively captures critical interaction details. This approach, which leverages receptor proteins as guiding information, outperforms existing methods by achieving an average RMSE improvement of 18.95% on mainstream GNN architectures. Through conditional deep learning, MTPNet internalizes interaction patterns to make unified predictions of activity cliffs, facilitating compound optimization and design. The experimental results on 30 activity cliff datasets demonstrate the superior performance of MTPNet. The code for MTPNet is available at the provided GitHub repository. 

<br /><br />Summary: <div>
arXiv:2506.05427v1 Announce Type: new 
Abstract: Activity cliff prediction is a critical task in drug discovery and material design. Existing computational methods are limited to handling single binding targets, which restricts the applicability of these prediction models. In this paper, we present the Multi-Grained Target Perception network (MTPNet) to incorporate the prior knowledge of interactions between the molecules and their target proteins. Specifically, MTPNet is a unified framework for activity cliff prediction, which consists of two components: Macro-level Target Semantic (MTS) guidance and Micro-level Pocket Semantic (MPS) guidance. By this way, MTPNet dynamically optimizes molecular representations through multi-grained protein semantic conditions. To our knowledge, it is the first time to employ the receptor proteins as guiding information to effectively capture critical interaction details. Extensive experiments on 30 representative activity cliff datasets demonstrate that MTPNet significantly outperforms previous approaches, achieving an average RMSE improvement of 18.95% on top of several mainstream GNN architectures. Overall, MTPNet internalizes interaction patterns through conditional deep learning to achieve unified predictions of activity cliffs, helping to accelerate compound optimization and design. Codes are available at: https://github.com/ZishanShu/MTPNet.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion with a Linguistic Compass: Steering the Generation of Clinically Plausible Future sMRI Representations for Early MCI Conversion Prediction</title>
<link>https://arxiv.org/abs/2506.05428</link>
<guid>https://arxiv.org/abs/2506.05428</guid>
<content:encoded><![CDATA[
<div> Diffusion-based framework, Mild Cognitive Impairment (MCI) conversion, prediction, sMRI, early conversion accuracy <br />
Summary:
- MCI-Diff is a novel framework for early prediction of Mild Cognitive Impairment (MCI) conversion.
- It achieves a balance between immediacy and accuracy by synthesizing future sMRI representations from baseline data.
- A multi-task sequence reconstruction strategy trains a denoising network on interpolation and extrapolation tasks to learn robust latent trajectories.
- An LLM-driven "linguistic compass" ensures clinical plausibility by generating realistic disease patterns.
- Experiments on ADNI and AIBL cohorts demonstrate that MCI-Diff outperforms existing baselines, improving early conversion accuracy by 5-12%. <br /><br /> <div>
arXiv:2506.05428v1 Announce Type: new 
Abstract: Early prediction of Mild Cognitive Impairment (MCI) conversion is hampered by a trade-off between immediacy--making fast predictions from a single baseline sMRI--and accuracy--leveraging longitudinal scans to capture disease progression. We propose MCI-Diff, a diffusion-based framework that synthesizes clinically plausible future sMRI representations directly from baseline data, achieving both real-time risk assessment and high predictive performance. First, a multi-task sequence reconstruction strategy trains a shared denoising network on interpolation and extrapolation tasks to handle irregular follow-up sampling and learn robust latent trajectories. Second, an LLM-driven "linguistic compass" is introduced for clinical plausibility sampling: generated feature candidates are quantized, tokenized, and scored by a fine-tuned language model conditioned on expected structural biomarkers, guiding autoregressive generation toward realistic disease patterns. Experiments on ADNI and AIBL cohorts show that MCI-Diff outperforms state-of-the-art baselines, improving early conversion accuracy by 5-12%.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PCDVQ: Enhancing Vector Quantization for Large Language Models via Polar Coordinate Decoupling</title>
<link>https://arxiv.org/abs/2506.05432</link>
<guid>https://arxiv.org/abs/2506.05432</guid>
<content:encoded><![CDATA[
<div> Quantization, Large Language Models, Vector Quantization, Polar Coordinate Decoupled Vector Quantization, Compression<br />
<br />
Summary: <br />
Large Language Models (LLMs) face challenges in edge deployment due to their massive parameter scale. To address this, a new approach called Polar Coordinate Decoupled Vector Quantization (PCDVQ) is proposed. This framework decouples the quantization of direction and magnitude parameters, improving accuracy in compressed LLMs. By separately clustering directions and magnitudes, PCDVQ reduces quantization errors and outperforms baseline methods. The method optimizes codebooks based on the source distribution, resulting in at least a 1.5% increase in zero-shot accuracy at the 2-bit level. PCDVQ offers an innovative solution for accurate and highly compressed LLMs. <br /> <div>
arXiv:2506.05432v1 Announce Type: new 
Abstract: Large Language Models (LLMs) face significant challenges in edge deployment due to their massive parameter scale. Vector Quantization (VQ), a clustering-based quantization method, serves as a prevalent solution to this issue for its extremely low-bit (even at 2-bit) and considerable accuracy. Since a vector is a quantity in mathematics and physics that has both direction and magnitude, existing VQ works typically quantize them in a coupled manner. However, we find that direction exhibits significantly greater sensitivity to quantization compared to the magnitude. For instance, when separately clustering the directions and magnitudes of weight vectors in LLaMA-2-7B, the accuracy drop of zero-shot tasks are 46.5\% and 2.3\%, respectively. This gap even increases with the reduction of clustering centers. Further, Euclidean distance, a common metric to access vector similarities in current VQ works, places greater emphasis on reducing the magnitude error. This property is contrary to the above finding, unavoidably leading to larger quantization errors. To these ends, this paper proposes Polar Coordinate Decoupled Vector Quantization (PCDVQ), an effective and efficient VQ framework consisting of two key modules: 1) Polar Coordinate Decoupling (PCD), which transforms vectors into their polar coordinate representations and perform independent quantization of the direction and magnitude parameters.2) Distribution Aligned Codebook Construction (DACC), which optimizes the direction and magnitude codebooks in accordance with the source distribution. Experimental results show that PCDVQ outperforms baseline methods at 2-bit level by at least 1.5\% zero-shot accuracy, establishing a novel paradigm for accurate and highly compressed LLMs.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prefix Grouper: Efficient GRPO Training through Shared-Prefix Forward</title>
<link>https://arxiv.org/abs/2506.05433</link>
<guid>https://arxiv.org/abs/2506.05433</guid>
<content:encoded><![CDATA[
<div> Algorithm, Group Relative Policy Optimization, Prefix Grouper, Computational cost, Scalability <br />
Summary: <br />
The article introduces Prefix Grouper, an efficient training algorithm for Group Relative Policy Optimization (GRPO). It eliminates redundant prefix computation by using a Shared-Prefix Forward strategy, reducing computational overhead in long-context learning scenarios. The method restructures self-attention and encodes the shared prefix only once, maintaining full differentiability and compatibility with end-to-end training. Prefix Grouper is shown to be training-equivalent to standard GRPO, with identical forward outputs and backward gradients. Empirical results demonstrate consistent performance and significant training cost reduction, allowing for larger group sizes within the same computational budget. The method is plug-and-play, compatible with existing GRPO-based architectures, and requires minimal changes to training pipelines. This advancement in efficiency enhances the scalability of GRPO for complex tasks and larger models. <div>
arXiv:2506.05433v1 Announce Type: new 
Abstract: Group Relative Policy Optimization (GRPO) enhances policy learning by computing gradients from relative comparisons among candidate outputs that share a common input prefix. Despite its effectiveness, GRPO introduces substantial computational overhead when processing long shared prefixes, which must be redundantly encoded for each group member. This inefficiency becomes a major scalability bottleneck in long-context learning scenarios. We propose Prefix Grouper, an efficient GRPO training algorithm that eliminates redundant prefix computation via a Shared-Prefix Forward strategy. In particular, by restructuring self-attention into two parts, our method enables the shared prefix to be encoded only once, while preserving full differentiability and compatibility with end-to-end training. We provide both theoretical and empirical evidence that Prefix Grouper is training-equivalent to standard GRPO: it yields identical forward outputs and backward gradients, ensuring that the optimization dynamics and final policy performance remain unchanged. Empirically, our experiments confirm that Prefix Grouper achieves consistent results while significantly reducing the computational cost of training, particularly in long-prefix scenarios. The proposed method is fully plug-and-play: it is compatible with existing GRPO-based architectures and can be seamlessly integrated into current training pipelines as a drop-in replacement, requiring no structural modifications and only minimal changes to input construction and attention computation. Prefix Grouper enables the use of larger group sizes under the same computational budget, thereby improving the scalability of GRPO to more complex tasks and larger models. Code is now available at https://github.com/johncaged/PrefixGrouper
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Robust Conformal Prediction via Lipschitz-Bounded Networks</title>
<link>https://arxiv.org/abs/2506.05434</link>
<guid>https://arxiv.org/abs/2506.05434</guid>
<content:encoded><![CDATA[
<div> Lipschitz-bounded networks, robust conformal prediction, adversarial attacks, ImageNet, computational efficiency <br />
Summary: 
The article introduces a new method, lip-rcp, for estimating robust conformal prediction sets efficiently in large-scale scenarios like ImageNet. By leveraging Lipschitz-bounded networks, lip-rcp outperforms existing methods in both set size and computational efficiency. The method is particularly effective when combined with a 1-Lipschitz robust network. Additionally, the study examines vanilla conformal prediction under attack and derives new worst-case coverage bounds that hold for all levels of adversarial attacks. This analysis enhances the efficiency of vanilla CP while also providing robustness guarantees, making it a valuable approach in improving the trustworthiness of neural networks. <br /><br />Summary: <div>
arXiv:2506.05434v1 Announce Type: new 
Abstract: Conformal Prediction (CP) has proven to be an effective post-hoc method for improving the trustworthiness of neural networks by providing prediction sets with finite-sample guarantees. However, under adversarial attacks, classical conformal guarantees do not hold anymore: this problem is addressed in the field of Robust Conformal Prediction. Several methods have been proposed to provide robust CP sets with guarantees under adversarial perturbations, but, for large scale problems, these sets are either too large or the methods are too computationally demanding to be deployed in real life scenarios. In this work, we propose a new method that leverages Lipschitz-bounded networks to precisely and efficiently estimate robust CP sets. When combined with a 1-Lipschitz robust network, we demonstrate that our lip-rcp method outperforms state-of-the-art results in both the size of the robust CP sets and computational efficiency in medium and large-scale scenarios such as ImageNet. Taking a different angle, we also study vanilla CP under attack, and derive new worst-case coverage bounds of vanilla CP sets, which are valid simultaneously for all adversarial attack levels. Our lip-rcp method makes this second approach as efficient as vanilla CP while also allowing robustness guarantees.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Event Classification of Accelerometer Data for Industrial Package Monitoring with Embedded Deep Learning</title>
<link>https://arxiv.org/abs/2506.05435</link>
<guid>https://arxiv.org/abs/2506.05435</guid>
<content:encoded><![CDATA[
<div> monitoring, embedded system, deep learning, data augmentation, compression techniques

Summary:<br />
- The study proposes using an embedded system on reusable packages to monitor their state, achieving operational efficiency and ecological sustainability.
- To maximize device lifespan, the approach focuses on minimizing wake time and utilizes a pipeline for data processing, training, and evaluating a deep learning model for classifying accelerometer data.
- Two data augmentation techniques, Synthetic Minority Oversampling TEchnique and ADAptive SYNthetic sampling, are tested to address dataset imbalance, resulting in a high precision of 94.54% and 95.83% for the two classes, respectively.
- Compression techniques are implemented post-training to reduce model size by a factor of four, without compromising accuracy.
- The trained model is deployed on the IoT device and operates with a power consumption of 316 mW during inference. 

Summary: <div>
arXiv:2506.05435v1 Announce Type: new 
Abstract: Package monitoring is an important topic in industrial applications, with significant implications for operational efficiency and ecological sustainability. In this study, we propose an approach that employs an embedded system, placed on reusable packages, to detect their state (on a Forklift, in a Truck, or in an undetermined location). We aim to design a system with a lifespan of several years, corresponding to the lifespan of reusable packages. Our analysis demonstrates that maximizing device lifespan requires minimizing wake time. We propose a pipeline that includes data processing, training, and evaluation of the deep learning model designed for imbalanced, multiclass time series data collected from an embedded sensor. The method uses a one-dimensional Convolutional Neural Network architecture to classify accelerometer data from the IoT device. Before training, two data augmentation techniques are tested to solve the imbalance problem of the dataset: the Synthetic Minority Oversampling TEchnique and the ADAptive SYNthetic sampling approach. After training, compression techniques are implemented to have a small model size. On the considered twoclass problem, the methodology yields a precision of 94.54% for the first class and 95.83% for the second class, while compression techniques reduce the model size by a factor of four. The trained model is deployed on the IoT device, where it operates with a power consumption of 316 mW during inference.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Unsupervised Framework for Dynamic Health Indicator Construction and Its Application in Rolling Bearing Prognostics</title>
<link>https://arxiv.org/abs/2506.05438</link>
<guid>https://arxiv.org/abs/2506.05438</guid>
<content:encoded><![CDATA[
<div> health indicator, degradation assessment, rolling bearings, dynamic information, prognostics

Summary:
- The article introduces a novel approach for constructing a dynamic health indicator (HI) for rolling bearings' degradation assessment and prognostics.
- Traditional HI construction methods often rely on expert knowledge for feature extraction and overlook dynamic information in sequential degradation processes.
- The proposed method includes a degradation feature learning module and an HI-generating module, which together capture essential degradation features and model temporal dependence for dynamic HI construction.
- Experimental results on two bearing lifecycle datasets show that the dynamic HI outperforms comparison methods in prognostic tasks.
- The dynamic HI effectively represents degradation trends and enables accurate prediction of future degradation, making it a valuable tool for maintenance and reliability purposes. 

<br /><br />Summary: <div>
arXiv:2506.05438v1 Announce Type: new 
Abstract: Health indicator (HI) plays a key role in degradation assessment and prognostics of rolling bearings. Although various HI construction methods have been investigated, most of them rely on expert knowledge for feature extraction and overlook capturing dynamic information hidden in sequential degradation processes, which limits the ability of the constructed HI for degradation trend representation and prognostics. To address these concerns, a novel dynamic HI that considers HI-level temporal dependence is constructed through an unsupervised framework. Specifically, a degradation feature learning module composed of a skip-connection-based autoencoder first maps raw signals to a representative degradation feature space (DFS) to automatically extract essential degradation features without the need for expert knowledge. Subsequently, in this DFS, a new HI-generating module embedded with an inner HI-prediction block is proposed for dynamic HI construction, where the temporal dependence between past and current HI states is guaranteed and modeled explicitly. On this basis, the dynamic HI captures the inherent dynamic contents of the degradation process, ensuring its effectiveness for degradation tendency modeling and future degradation prognostics. The experiment results on two bearing lifecycle datasets demonstrate that the proposed HI construction method outperforms comparison methods, and the constructed dynamic HI is superior for prognostic tasks.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniPTMs: The First Unified Multi-type PTM Site Prediction Model via Master-Slave Architecture-Based Multi-Stage Fusion Strategy and Hierarchical Contrastive Loss</title>
<link>https://arxiv.org/abs/2506.05443</link>
<guid>https://arxiv.org/abs/2506.05443</guid>
<content:encoded><![CDATA[
<div> framework, PTM prediction, deep learning, protein sequences, feature fusion <br />
<br />
Summary: <br />
The study introduces UniPTMs, a unified framework for multi-type PTM prediction in eukaryotes. UniPTMs addresses existing deep learning model limitations by incorporating a "Master-Slave" dual-path architecture with innovative modules such as Bidirectional Gated Cross-Attention (BGCA) and Low-Dimensional Fusion Network (LDFN). It includes features like Multi-scale Adaptive convolutional Pyramid (MACP) and Hierarchical Dynamic Weighting Fusion (HDWF) for improved performance. UniPTMs surpasses state-of-the-art models, demonstrating significant performance boosts in prediction accuracy (MCC and AP increases). A lightweight version, UniPTMs-mini, balances complexity and performance. The framework also introduces a Hierarchical Contrastive loss function for feature consistency optimization. UniPTMs transcends the Single-Type Prediction Paradigm, showcasing the potential for deciphering dynamic epigenetic regulation networks and advancing our understanding of protein post-translational modifications. <div>
arXiv:2506.05443v1 Announce Type: new 
Abstract: As a core mechanism of epigenetic regulation in eukaryotes, protein post-translational modifications (PTMs) require precise prediction to decipher dynamic life activity networks. To address the limitations of existing deep learning models in cross-modal feature fusion, domain generalization, and architectural optimization, this study proposes UniPTMs: the first unified framework for multi-type PTM prediction. The framework innovatively establishes a "Master-Slave" dual-path collaborative architecture: The master path dynamically integrates high-dimensional representations of protein sequences, structures, and evolutionary information through a Bidirectional Gated Cross-Attention (BGCA) module, while the slave path optimizes feature discrepancies and recalibration between structural and traditional features using a Low-Dimensional Fusion Network (LDFN). Complemented by a Multi-scale Adaptive convolutional Pyramid (MACP) for capturing local feature patterns and a Bidirectional Hierarchical Gated Fusion Network (BHGFN) enabling multi-level feature integration across paths, the framework employs a Hierarchical Dynamic Weighting Fusion (HDWF) mechanism to intelligently aggregate multimodal features. Enhanced by a novel Hierarchical Contrastive loss function for feature consistency optimization, UniPTMs demonstrates significant performance improvements (3.2%-11.4% MCC and 4.2%-14.3% AP increases) over state-of-the-art models across five modification types and transcends the Single-Type Prediction Paradigm. To strike a balance between model complexity and performance, we have also developed a lightweight variant named UniPTMs-mini.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Policy Learning in Reinforcement Learning: Backdoor-Adjusted Soft Actor-Critic</title>
<link>https://arxiv.org/abs/2506.05445</link>
<guid>https://arxiv.org/abs/2506.05445</guid>
<content:encoded><![CDATA[
<div> confounding, policy learning, reinforcement learning, causal intervention, algorithm<br />
Summary:<br />
The article introduces DoSAC, a novel approach in reinforcement learning that addresses hidden confounders that can affect policy learning. Traditional RL algorithms often overlook this issue, leading to suboptimal behavior. DoSAC corrects for hidden confounding by estimating the interventional policy using the backdoor criterion, without needing access to true confounders or causal labels. It incorporates a Backdoor Reconstructor module that infers pseudo-past variables from the current state, enabling backdoor adjustment from observational data. This method, integrated into a soft actor-critic framework, outperforms baselines in continuous control benchmarks under confounded settings. It demonstrates improved robustness, generalization, and policy reliability, showcasing its potential for more effective and reliable policy learning in RL. <br />Summary: <div>
arXiv:2506.05445v1 Announce Type: new 
Abstract: Hidden confounders that influence both states and actions can bias policy learning in reinforcement learning (RL), leading to suboptimal or non-generalizable behavior. Most RL algorithms ignore this issue, learning policies from observational trajectories based solely on statistical associations rather than causal effects. We propose DoSAC (Do-Calculus Soft Actor-Critic with Backdoor Adjustment), a principled extension of the SAC algorithm that corrects for hidden confounding via causal intervention estimation. DoSAC estimates the interventional policy $\pi(a | \mathrm{do}(s))$ using the backdoor criterion, without requiring access to true confounders or causal labels. To achieve this, we introduce a learnable Backdoor Reconstructor that infers pseudo-past variables (previous state and action) from the current state to enable backdoor adjustment from observational data. This module is integrated into a soft actor-critic framework to compute both the interventional policy and its entropy. Empirical results on continuous control benchmarks show that DoSAC outperforms baselines under confounded settings, with improved robustness, generalization, and policy reliability.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training Dynamics Underlying Language Model Scaling Laws: Loss Deceleration and Zero-Sum Learning</title>
<link>https://arxiv.org/abs/2506.05447</link>
<guid>https://arxiv.org/abs/2506.05447</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, scaling, training dynamics, loss deceleration, zero-sum learning

Summary: 
This study investigates the impact of scaling on language models and their training dynamics. The researchers observe a phenomenon called loss deceleration early in training, where the rate of loss improvement slows down abruptly. Scaling up the model helps to mitigate this deceleration by improving the rate of loss improvement after the slowdown occurs. The authors attribute loss deceleration to a form of training dynamics called zero-sum learning (ZSL), where per-example gradients become opposing, leading to interference in loss improvement. This interference causes improvements in some examples to negatively impact others, hindering overall progress. Understanding loss deceleration and ZSL provides valuable insights into language model training dynamics and can potentially be targeted to enhance model performance irrespective of scale.<br /><br />Summary: <div>
arXiv:2506.05447v1 Announce Type: new 
Abstract: This work aims to understand how scaling improves language models, specifically in terms of training dynamics. We find that language models undergo loss deceleration early in training; an abrupt slowdown in the rate of loss improvement, resulting in piecewise linear behaviour of the loss curve in log-log space. Scaling up the model mitigates this transition by (1) decreasing the loss at which deceleration occurs, and (2) improving the log-log rate of loss improvement after deceleration. We attribute loss deceleration to a type of degenerate training dynamics we term zero-sum learning (ZSL). In ZSL, per-example gradients become systematically opposed, leading to destructive interference in per-example changes in loss. As a result, improving loss on one subset of examples degrades it on another, bottlenecking overall progress. Loss deceleration and ZSL provide new insights into the training dynamics underlying language model scaling laws, and could potentially be targeted directly to improve language models independent of scale. We make our code and artefacts available at: https://github.com/mirandrom/zsl
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zeroth-Order Optimization Finds Flat Minima</title>
<link>https://arxiv.org/abs/2506.05454</link>
<guid>https://arxiv.org/abs/2506.05454</guid>
<content:encoded><![CDATA[
<div> Keywords: zeroth-order optimization, Hessian trace, implicit regularization, convex functions, language model fine-tuning

Summary:
Zeroth-order optimization is a valuable approach in machine learning for situations where computing gradients is challenging. This study investigates the implicit regularization of zeroth-order optimization methods, focusing on solutions with a small trace of the Hessian matrix. The research highlights that zeroth-order optimization tends to favor solutions with a smaller Hessian trace, commonly associated with flat minima. The study establishes convergence rates for zeroth-order optimization to approximate flat minima in convex and smooth functions. Experimental results on binary classification tasks and language model fine-tuning support the theoretical findings, demonstrating the preference for solutions with a minimized Hessian trace. This work contributes to a deeper understanding of the behavior and outcomes of zeroth-order optimization techniques in machine learning applications. 

<br /><br />Summary: <div>
arXiv:2506.05454v1 Announce Type: new 
Abstract: Zeroth-order methods are extensively used in machine learning applications where gradients are infeasible or expensive to compute, such as black-box attacks, reinforcement learning, and language model fine-tuning. Existing optimization theory focuses on convergence to an arbitrary stationary point, but less is known on the implicit regularization that provides a fine-grained characterization on which particular solutions are finally reached. We show that zeroth-order optimization with the standard two-point estimator favors solutions with small trace of Hessian, which is widely used in previous work to distinguish between sharp and flat minima. We further provide convergence rates of zeroth-order optimization to approximate flat minima for convex and sufficiently smooth functions, where flat minima are defined as the minimizers that achieve the smallest trace of Hessian among all optimal solutions. Experiments on binary classification tasks with convex losses and language model fine-tuning support our theoretical findings.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning-Augmented Algorithms for MTS with Bandit Access to Multiple Predictors</title>
<link>https://arxiv.org/abs/2506.05479</link>
<guid>https://arxiv.org/abs/2506.05479</guid>
<content:encoded><![CDATA[
<div> heuristics, Metrical Task Systems, online, Bandit Learning, regret<br />
Summary:<br />
- The problem discussed involves utilizing multiple heuristics tailored to different input instances for Metrical Task Systems (MTS) to achieve optimal performance.<br />
- Only one heuristic can be queried per time step while processing online input instances.<br />
- The challenge lies in the uncertainty of heuristic cost estimation, requiring the same heuristic to be queried in consecutive time steps.<br />
- The research presents a strategy to achieve regret of O(OPT^(2/3)) in this setting.<br />
- A tight lower bound is established based on the work of Dekel et al. (2013). <div>
arXiv:2506.05479v1 Announce Type: new 
Abstract: We consider the following problem: We are given $\ell$ heuristics for Metrical Task Systems (MTS), where each might be tailored to a different type of input instances. While processing an input instance received online, we are allowed to query the action of only one of the heuristics at each time step. Our goal is to achieve performance comparable to the best of the given heuristics. The main difficulty of our setting comes from the fact that the cost paid by a heuristic at time $t$ cannot be estimated unless the same heuristic was also queried at time $t-1$. This is related to Bandit Learning against memory bounded adversaries (Arora et al., 2012). We show how to achieve regret of $O(\text{OPT}^{2/3})$ and prove a tight lower bound based on the construction of Dekel et al. (2013).
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Initial Model Incorporation for Deep Learning FWI: Pretraining or Denormalization?</title>
<link>https://arxiv.org/abs/2506.05484</link>
<guid>https://arxiv.org/abs/2506.05484</guid>
<content:encoded><![CDATA[
<div> neural network, full waveform inversion, subsurface property, reparameterized, denormalization

Summary:<br /><br />
Subsurface property neural network reparameterized full waveform inversion (FWI) is an effective unsupervised learning framework that updates neural network parameters instead of fine-tuning on the subsurface model directly. The incorporation of prior knowledge of the initial model into neural networks can be done through pretraining or denormalization. Pretraining involves regulating neural network parameters by fitting the initial velocity model, while denormalization directly adds network outputs to the initial models without pretraining. The study found that pretraining inverts model perturbations based on a constant velocity value in a complex two-stage process, potentially causing network parameters to lose plasticity. In contrast, denormalization simplifies workflows, speeds up convergence, and improves inversion accuracy compared to pretraining methods. <div>
arXiv:2506.05484v1 Announce Type: new 
Abstract: Subsurface property neural network reparameterized full waveform inversion (FWI) has emerged as an effective unsupervised learning framework, which can invert stably with an inaccurate starting model. It updates the trainable neural network parameters instead of fine-tuning on the subsurface model directly. There are primarily two ways to embed the prior knowledge of the initial model into neural networks, that is, pretraining and denormalization. Pretraining first regulates the neural networks' parameters by fitting the initial velocity model; Denormalization directly adds the outputs of the network into the initial models without pretraining. In this letter, we systematically investigate the influence of the two ways of initial model incorporation for the neural network reparameterized FWI. We demonstrate that pretraining requires inverting the model perturbation based on a constant velocity value (mean) with a two-stage implementation. It leads to a complex workflow and inconsistency of objective functions in the two-stage process, causing the network parameters to become inactive and lose plasticity. Experimental results demonstrate that denormalization can simplify workflows, accelerate convergence, and enhance inversion accuracy compared with pretraining.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformal Prediction Beyond the Seen: A Missing Mass Perspective for Uncertainty Quantification in Generative Models</title>
<link>https://arxiv.org/abs/2506.05497</link>
<guid>https://arxiv.org/abs/2506.05497</guid>
<content:encoded><![CDATA[
<div> Quantum Machine Learning, Uncertainty Quantification, Conformal Prediction, Language Models, Query Oracle <br />
Summary: <br />
The article introduces Conformal Prediction with Query Oracle (CPQ) as a framework for uncertainty quantification in generative AI models like language models. Unlike classical methods, CPQ operates in a query-only setting, where prediction sets are constructed from limited queries to a black box model, balancing coverage, query budget, and informativeness. The algorithm is based on optimal query policies and mappings inspired by the missing mass problem in statistics. Experimental results on language models demonstrate CPQ's effectiveness in providing more informative prediction sets compared to existing methods. The study showcases the significance of each core principle in CPQ's performance and its potential for various real-world tasks in language uncertainty quantification. <br /> <div>
arXiv:2506.05497v1 Announce Type: new 
Abstract: Uncertainty quantification (UQ) is essential for safe deployment of generative AI models such as large language models (LLMs), especially in high stakes applications. Conformal prediction (CP) offers a principled uncertainty quantification framework, but classical methods focus on regression and classification, relying on geometric distances or softmax scores: tools that presuppose structured outputs. We depart from this paradigm by studying CP in a query only setting, where prediction sets must be constructed solely from finite queries to a black box generative model, introducing a new trade off between coverage, test time query budget, and informativeness. We introduce Conformal Prediction with Query Oracle (CPQ), a framework characterizing the optimal interplay between these objectives. Our finite sample algorithm is built on two core principles: one governs the optimal query policy, and the other defines the optimal mapping from queried samples to prediction sets. Remarkably, both are rooted in the classical missing mass problem in statistics. Specifically, the optimal query policy depends on the rate of decay, or the derivative, of the missing mass, for which we develop a novel estimator. Meanwhile, the optimal mapping hinges on the missing mass itself, which we estimate using Good Turing estimators. We then turn our focus to implementing our method for language models, where outputs are vast, variable, and often under specified. Fine grained experiments on three real world open ended tasks and two LLMs, show CPQ applicability to any black box LLM and highlight: (1) individual contribution of each principle to CPQ performance, and (2) CPQ ability to yield significantly more informative prediction sets than existing conformal methods for language uncertainty quantification.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Generative Leap: Sharp Sample Complexity for Efficiently Learning Gaussian Multi-Index Models</title>
<link>https://arxiv.org/abs/2506.05500</link>
<guid>https://arxiv.org/abs/2506.05500</guid>
<content:encoded><![CDATA[
<div> Gaussian Multi-index Models, Efficient Agnostic Estimation, Generative Leap Exponent, Sample Complexity, Sequential Estimation Procedure<br />
Summary:<br />
In this study, Gaussian Multi-index models with labels depending on Gaussian input projections onto a low-dimensional subspace are examined. The generative leap exponent, an extension of the generative exponent to the multi-index setting, is introduced. It is shown that a sample complexity of $\Theta(d^{1 \vee \k/2})$ is necessary for algorithms in the Low-Degree-Polynomial framework. An agnostic sequential estimation procedure based on spectral U-statistics over Hermite tensors is presented, achieving the necessary sample complexity without prior knowledge. The generative leap exponent is calculated for various models, including piecewise linear functions and general deep neural networks with a first hidden layer of dimension $r$. <div>
arXiv:2506.05500v1 Announce Type: new 
Abstract: In this work we consider generic Gaussian Multi-index models, in which the labels only depend on the (Gaussian) $d$-dimensional inputs through their projection onto a low-dimensional $r = O_d(1)$ subspace, and we study efficient agnostic estimation procedures for this hidden subspace. We introduce the \emph{generative leap} exponent $k^\star$, a natural extension of the generative exponent from [Damian et al.'24] to the multi-index setting. We first show that a sample complexity of $n=\Theta(d^{1 \vee \k/2})$ is necessary in the class of algorithms captured by the Low-Degree-Polynomial framework. We then establish that this sample complexity is also sufficient, by giving an agnostic sequential estimation procedure (that is, requiring no prior knowledge of the multi-index model) based on a spectral U-statistic over appropriate Hermite tensors. We further compute the generative leap exponent for several examples including piecewise linear functions (deep ReLU networks with bias), and general deep neural networks (with $r$-dimensional first hidden layer).
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometric and Physical Constraints Synergistically Enhance Neural PDE Surrogates</title>
<link>https://arxiv.org/abs/2506.05513</link>
<guid>https://arxiv.org/abs/2506.05513</guid>
<content:encoded><![CDATA[
<div> Neural PDE surrogates, Staggered grids, Physical constraints, Symmetry constraints, Computational fluid dynamics<br /> 
<br />Summary: 
Novel input and output layers for neural PDE surrogates on staggered grids are introduced, improving performance on challenging problems like shallow water equations and decaying turbulence. Constraints of physical laws and symmetries enhance accuracy significantly, with surrogates benefiting from both. Surrogates with both constraints outperform baselines with data augmentation or pushforward training, even accurately predicting real-world ocean currents. Symmetries are more effective than physical constraints individually, but their combination yields the best results. Surrogates with double constraints also show improved generalization to new initial conditions and longer durations, outperforming existing techniques. <div>
arXiv:2506.05513v1 Announce Type: new 
Abstract: Neural PDE surrogates can improve the cost-accuracy tradeoff of classical solvers, but often generalize poorly to new initial conditions and accumulate errors over time. Physical and symmetry constraints have shown promise in closing this performance gap, but existing techniques for imposing these inductive biases are incompatible with the staggered grids commonly used in computational fluid dynamics. Here we introduce novel input and output layers that respect physical laws and symmetries on the staggered grids, and for the first time systematically investigate how these constraints, individually and in combination, affect the accuracy of PDE surrogates. We focus on two challenging problems: shallow water equations with closed boundaries and decaying incompressible turbulence. Compared to strong baselines, symmetries and physical constraints consistently improve performance across tasks, architectures, autoregressive prediction steps, accuracy measures, and network sizes. Symmetries are more effective than physical constraints, but surrogates with both performed best, even compared to baselines with data augmentation or pushforward training, while themselves benefiting from the pushforward trick. Doubly-constrained surrogates also generalize better to initial conditions and durations beyond the range of the training data, and more accurately predict real-world ocean currents.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Winner-takes-all for Multivariate Probabilistic Time Series Forecasting</title>
<link>https://arxiv.org/abs/2506.05515</link>
<guid>https://arxiv.org/abs/2506.05515</guid>
<content:encoded><![CDATA[
<div> Keywords: TimeMCL, Multiple Choice Learning, neural network, time series forecasting, diversity

Summary:
TimeMCL introduces a method utilizing Multiple Choice Learning (MCL) for forecasting multiple plausible futures in time series. The approach involves a neural network with multiple heads and uses the Winner-Takes-All (WTA) loss to encourage diverse predictions. MCL is known for its ability to handle ambiguous tasks, making it well-suited for time series forecasting. The method is efficient and promotes diversity by implicitly incorporating a quantization objective. Insights into the approach are provided using synthetic data, and its performance on real-world time series data shows promising results with low computational cost. TimeMCL presents a novel way to forecast multiple time series futures while maintaining accuracy and diversity in predictions. 

<br /><br />Summary: TimeMCL leverages Multiple Choice Learning (MCL) in time series forecasting using a neural network and Winner-Takes-All (WTA) loss to ensure diverse predictions. The method is efficient, handles ambiguous tasks, and shows promising results on real-world data, underlining its potential for accurate and diverse time series forecasting. <div>
arXiv:2506.05515v1 Announce Type: new 
Abstract: We introduce TimeMCL, a method leveraging the Multiple Choice Learning (MCL) paradigm to forecast multiple plausible time series futures. Our approach employs a neural network with multiple heads and utilizes the Winner-Takes-All (WTA) loss to promote diversity among predictions. MCL has recently gained attention due to its simplicity and ability to address ill-posed and ambiguous tasks. We propose an adaptation of this framework for time-series forecasting, presenting it as an efficient method to predict diverse futures, which we relate to its implicit quantization objective. We provide insights into our approach using synthetic data and evaluate it on real-world time series, demonstrating its promising performance at a light computational cost.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Fitting Flow Models with Large Sinkhorn Couplings</title>
<link>https://arxiv.org/abs/2506.05526</link>
<guid>https://arxiv.org/abs/2506.05526</guid>
<content:encoded><![CDATA[
<div> Optimal transport, flow models, training, Sinkhorn algorithm, image generation<br />
Summary: <br />
The article explores the benefits of increasing the batch size to improve training flow models without given pairing between source and target points. By using Optimal Transport (OT) measures and Sinkhorn algorithm with low entropic regularization, the efficiency of flow models can be significantly enhanced. The study investigates the impact of increasing the batch size by orders of magnitude and the level of entropic regularization in the Sinkhorn algorithm. The analysis introduces scale-invariant metrics to evaluate the sharpness of couplings, enabling a more thorough exploration of large Sinkhorn couplings' effectiveness in synthetic and image generation tasks. The research demonstrates that employing larger Sinkhorn couplings with minimal entropic regularization yields substantial improvements in flow model performance, emphasizing the importance of batch size and optimization methods in training flow models efficiently. <br /> <div>
arXiv:2506.05526v1 Announce Type: new 
Abstract: Flow models transform data gradually from one modality (e.g. noise) onto another (e.g. images). Such models are parameterized by a time-dependent velocity field, trained to fit segments connecting pairs of source and target points. When the pairing between source and target points is given, training flow models boils down to a supervised regression problem. When no such pairing exists, as is the case when generating data from noise, training flows is much harder. A popular approach lies in picking source and target points independently. This can, however, lead to velocity fields that are slow to train, but also costly to integrate at inference time. In theory, one would greatly benefit from training flow models by sampling pairs from an optimal transport (OT) measure coupling source and target, since this would lead to a highly efficient flow solving the Benamou and Brenier dynamical OT problem. In practice, recent works have proposed to sample mini-batches of $n$ source and $n$ target points and reorder them using an OT solver to form better pairs. These works have advocated using batches of size $n\approx 256$, and considered OT solvers that return couplings that are either sharp (using e.g. the Hungarian algorithm) or blurred (using e.g. entropic regularization, a.k.a. Sinkhorn). We follow in the footsteps of these works by exploring the benefits of increasing $n$ by three to four orders of magnitude, and look more carefully on the effect of the entropic regularization $\varepsilon$ used in the Sinkhorn algorithm. Our analysis is facilitated by new scale invariant quantities to report the sharpness of a coupling, while our sharded computations across multiple GPU or GPU nodes allow scaling up $n$. We show that in both synthetic and image generation tasks, flow models greatly benefit when fitted with large Sinkhorn couplings, with a low entropic regularization $\varepsilon$.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectral Graph Neural Networks are Incomplete on Graphs with a Simple Spectrum</title>
<link>https://arxiv.org/abs/2506.05530</link>
<guid>https://arxiv.org/abs/2506.05530</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, Spectral features, Expressive power, Laplacian eigenvectors, SGNNs <br />
<br />Summary: 
The article introduces the concept of Spectrally-enhanced Graph Neural Networks (SGNNs) and evaluates their expressive power through a new hierarchy based on the largest eigenvalue multiplicity. It is shown that many SGNNs are incomplete even on graphs with distinct eigenvalues. To address this limitation, a method using rotation equivariant neural networks adapted to the graph spectra setting is proposed to enhance SGNNs' expressivity on simple spectrum graphs. Empirical verification of the theoretical claims is conducted through image classification experiments on the MNIST Superpixel dataset and eigenvector canonicalization on graphs from ZINC. This work offers insights into improving SGNNs' performance by leveraging spectral features and provides a new framework for evaluating their expressive power. <div>
arXiv:2506.05530v1 Announce Type: new 
Abstract: Spectral features are widely incorporated within Graph Neural Networks (GNNs) to improve their expressive power, or their ability to distinguish among non-isomorphic graphs. One popular example is the usage of graph Laplacian eigenvectors for positional encoding in MPNNs and Graph Transformers. The expressive power of such Spectrally-enhanced GNNs (SGNNs) is usually evaluated via the k-WL graph isomorphism test hierarchy and homomorphism counting. Yet, these frameworks align poorly with the graph spectra, yielding limited insight into SGNNs' expressive power. We leverage a well-studied paradigm of classifying graphs by their largest eigenvalue multiplicity to introduce an expressivity hierarchy for SGNNs. We then prove that many SGNNs are incomplete even on graphs with distinct eigenvalues. To mitigate this deficiency, we adapt rotation equivariant neural networks to the graph spectra setting to propose a method to provably improve SGNNs' expressivity on simple spectrum graphs. We empirically verify our theoretical claims via an image classification experiment on the MNIST Superpixel dataset and eigenvector canonicalization on graphs from ZINC.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SocialDF: Benchmark Dataset and Detection Model for Mitigating Harmful Deepfake Content on Social Media Platforms</title>
<link>https://arxiv.org/abs/2506.05538</link>
<guid>https://arxiv.org/abs/2506.05538</guid>
<content:encoded><![CDATA[
<div> dataset, deepfake, social media, detection, multi-modal<br />
Summary:<br />
The paper introduces the SocialDF dataset, designed to address the challenges of detecting deepfakes on social media platforms. The dataset includes various high-fidelity deepfakes sourced from online sources, providing a comprehensive representation of manipulative techniques used in deepfakes. The proposed detection approach utilizes a multi-factor method based on Language Model (LLM), combining facial recognition, automated speech transcription, and a multi-agent LLM pipeline for cross-verification of audio-visual cues. This approach emphasizes the importance of robust, multi-modal verification techniques that incorporate linguistic, behavioral, and contextual analysis to effectively differentiate synthetic media from authentic content on social media platforms. <div>
arXiv:2506.05538v1 Announce Type: new 
Abstract: The rapid advancement of deep generative models has significantly improved the realism of synthetic media, presenting both opportunities and security challenges. While deepfake technology has valuable applications in entertainment and accessibility, it has emerged as a potent vector for misinformation campaigns, particularly on social media. Existing detection frameworks struggle to distinguish between benign and adversarially generated deepfakes engineered to manipulate public perception. To address this challenge, we introduce SocialDF, a curated dataset reflecting real-world deepfake challenges on social media platforms. This dataset encompasses high-fidelity deepfakes sourced from various online ecosystems, ensuring broad coverage of manipulative techniques. We propose a novel LLM-based multi-factor detection approach that combines facial recognition, automated speech transcription, and a multi-agent LLM pipeline to cross-verify audio-visual cues. Our methodology emphasizes robust, multi-modal verification techniques that incorporate linguistic, behavioral, and contextual analysis to effectively discern synthetic media from authentic content.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentomics-ML: Autonomous Machine Learning Experimentation Agent for Genomic and Transcriptomic Data</title>
<link>https://arxiv.org/abs/2506.05542</link>
<guid>https://arxiv.org/abs/2506.05542</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, deep learning, molecular medicine, genomic data, autonomous agent

Summary:
Agentomics-ML is an autonomous agent-based system designed to automate the process of building and evaluating machine learning models on heterogeneous computational biology datasets. The system follows predefined steps of the ML experimentation process, interacting with the file system to complete tasks such as data representation, model architecture, and hyperparameter choices. It receives feedback on training and validation metrics, allowing it to identify and address issues like overfitting through a reflection step. Evaluations on genomic and transcriptomic benchmark datasets show that Agentomics-ML outperforms existing agent-based methods in terms of generalization and success rates. While domain expert-built models still lead in absolute performance on most datasets, Agentomics-ML narrows the gap for fully autonomous systems and achieves state-of-the-art performance on one benchmark dataset. The code for Agentomics-ML is available on GitHub for further exploration and development. 

<br /><br />Summary: <div>
arXiv:2506.05542v1 Announce Type: new 
Abstract: The adoption of machine learning (ML) and deep learning methods has revolutionized molecular medicine by driving breakthroughs in genomics, transcriptomics, drug discovery, and biological systems modeling. The increasing quantity, multimodality, and heterogeneity of biological datasets demand automated methods that can produce generalizable predictive models. Recent developments in large language model-based agents have shown promise for automating end-to-end ML experimentation on structured benchmarks. However, when applied to heterogeneous computational biology datasets, these methods struggle with generalization and success rates. Here, we introduce Agentomics-ML, a fully autonomous agent-based system designed to produce a classification model and the necessary files for reproducible training and inference. Our method follows predefined steps of an ML experimentation process, repeatedly interacting with the file system through Bash to complete individual steps. Once an ML model is produced, training and validation metrics provide scalar feedback to a reflection step to identify issues such as overfitting. This step then creates verbal feedback for future iterations, suggesting adjustments to steps such as data representation, model architecture, and hyperparameter choices. We have evaluated Agentomics-ML on several established genomic and transcriptomic benchmark datasets and show that it outperforms existing state-of-the-art agent-based methods in both generalization and success rates. While state-of-the-art models built by domain experts still lead in absolute performance on the majority of the computational biology datasets used in this work, Agentomics-ML narrows the gap for fully autonomous systems and achieves state-of-the-art performance on one of the used benchmark datasets. The code is available at https://github.com/BioGeMT/Agentomics-ML.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>