<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.LG updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.LG</link>


<item>
<title>Scalable Temporal Anomaly Causality Discovery in Large Systems: Achieving Computational Efficiency with Binary Anomaly Flag Data</title>
<link>https://arxiv.org/abs/2412.11800</link>
<guid>https://arxiv.org/abs/2412.11800</guid>
<content:encoded><![CDATA[
<div> Keywords: anomaly causality, graphical causal models, binary anomaly data, computational efficiency, temporal datasets<br /><br />Summary:<br /><br />1. The paper addresses the challenge of extracting anomaly causality from large-scale monitoring systems, which is crucial for effective diagnostics when system faults occur. 2. Identifying causes in extensive systems requires analyzing many monitoring variables across various subsystems, but existing graphical causal models (GCMs) are often computationally expensive, limiting their use in real-time and large-scale settings. 3. Binary alarm flags generated by modern monitoring systems introduce unique difficulties, such as sparse data and the significance of state transitions, which hinder conventional causality learning methods. 4. To overcome these issues, the authors propose AnomalyCD, a novel anomaly causal discovery approach designed specifically for temporal binary flag datasets, incorporating anomaly data-aware causality testing, techniques for compression of sparse data and prior links, and edge pruning adjustments. 5. The approach is validated on two datasets: sensor data from the Compact Muon Solenoid experiment at CERN and a public IT monitoring dataset, demonstrating a significant reduction in computational overhead and moderate improvements in accuracy for temporal GCMs dealing with binary anomaly data. 6. The study also provides open-source implementation, facilitating broader adoption and further research in anomaly causality discovery. <div>
arXiv:2412.11800v3 Announce Type: replace 
Abstract: Extracting anomaly causality facilitates diagnostics once monitoring systems detect system faults. Identifying anomaly causes in large systems involves investigating a broader set of monitoring variables across multiple subsystems. However, learning graphical causal models (GCMs) comes with a significant computational burden that restrains the applicability of most existing methods in real-time and large-scale deployments. In addition, modern monitoring applications for large systems often generate large amounts of binary alarm flags, and the distinct characteristics of binary anomaly data -- the meaning of state transition and data sparsity -- challenge existing causality learning mechanisms. This study proposes an anomaly causal discovery approach (AnomalyCD), addressing the accuracy and computational challenges of generating GCMs from temporal binary flag datasets. The AnomalyCD presents several strategies, such as anomaly data-aware causality testing, sparse data and prior link compression, and edge pruning adjustment approaches. We validate the performance of the approach on two datasets: monitoring sensor data from the readout-box system of the Compact Muon Solenoid experiment at CERN, and a public dataset from an information technology monitoring system. The results on temporal GCMs demonstrate a considerable reduction of computation overhead and a moderate enhancement of accuracy on the binary anomaly datasets Source code: https://github.com/muleina/AnomalyCD .
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PyGraph: Robust Compiler Support for CUDA Graphs in PyTorch</title>
<link>https://arxiv.org/abs/2503.19779</link>
<guid>https://arxiv.org/abs/2503.19779</guid>
<content:encoded><![CDATA[
<div> CUDA Graphs, GPU kernels, Machine Learning, PyGraph, compiler optimizations<br /><br />Summary:<br /><br />1. Machine learning workloads generate hundreds to thousands of short GPU kernel launches per iteration, where CPU-side kernel launch latency has become a significant performance bottleneck as GPU compute throughput increases.<br /><br />2. CUDA Graphs provide a mechanism to replay a sequence of GPU kernels with a single graph dispatch, effectively removing the overheads associated with launching each kernel individually.<br /><br />3. Despite their potential, CUDA Graphs are difficult to use correctly and efficiently in practice, limiting their adoption in ML workloads.<br /><br />4. PyGraph is introduced as a compiler framework designed to maximize CUDA Graphs coverage and benefits specifically for ML applications by employing three key optimizations: automatic code transformations to better suit CUDA Graphs, elimination of parameter copy overheads within graphs, and cost-benefit guided selective deployment of CUDA Graphs.<br /><br />5. Evaluated on 25 diverse ML workloads from TorchBench, HuggingFace, and TIMM, PyGraph achieves more than a twofold increase in the benefits of CUDA Graph deployment compared to the widely used PyTorch2 compiler, all without requiring programmer intervention. <div>
arXiv:2503.19779v3 Announce Type: replace 
Abstract: Machine learning (ML) workloads launch hundreds to thousands of short-running GPU kernels per iteration. With GPU compute throughput growing rapidly, CPU-side launch latency of kernels is emerging as a bottleneck. CUDA Graphs promise to address this by replaying a set of kernels with a single dispatch of the graph, removing per-kernel launch costs. However, CUDA Graphs remain surprisingly difficult to deploy correctly and efficiently.
  We present PyGraph - a compiler framework to maximize the coverage and benefits of CUDA Graphs for ML workloads. It introduces three novel optimizations: it applies automatic code transformations to make ML applications amenable to CUDA Graphs; it eliminates the parameter copy overheads for kernels executing in CUDA Graphs, and it selectively deploys CUDA Graphs guided by a cost-benefit analysis. For 25 ML workloads from TorchBench, HuggingFace, and TIMM, PyGraph more than doubles the benefit from deploying CUDA Graph compared to the most popular and widely used ML compiler, PyTorch2. PyGraph is built atop PyTorch2's compilation framework and requires no programmer intervention.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attention in Motion: Secure Platooning via Transformer-based Misbehavior Detection</title>
<link>https://arxiv.org/abs/2512.15503</link>
<guid>https://arxiv.org/abs/2512.15503</guid>
<content:encoded><![CDATA[
<div> Keywords: vehicular platooning, misbehaviour detection, transformer, V2X communication, edge deployment  

<br /><br />Summary:  
Vehicular platooning enhances transportation efficiency and safety by coordinating multi-vehicle formations through Vehicle-to-Everything (V2X) communication. However, distributed coordination exposes security risks where authenticated vehicles can falsify kinematic data, endangering operational stability and passenger safety. Traditional misbehaviour detection methods relying on plausibility checks and statistical analysis struggle with high false positive rates and fail to capture complex temporal dependencies within multi-vehicle coordination. To address these challenges, the paper introduces AIMformer, a transformer-based framework designed for real-time misbehaviour detection with the ability to deploy on edge devices. AIMformer utilizes multi-head self-attention to model both intra-vehicle temporal dynamics and inter-vehicle spatial correlations simultaneously. It features global positional encoding combined with vehicle-specific temporal offsets to manage platoon join and exit maneuvers effectively. Additionally, the Precision-Focused Binary Cross-Entropy (PFBCE) loss function is proposed to reduce false positives, aligning with stringent safety requirements. Evaluations across four different platoon controllers, diverse attack types, and mobility conditions show AIMformer achieves superior detection performance with metrics at or above 0.93, outperforming existing baseline models. Deployment tests using TensorFlow Lite, ONNX, and TensorRT demonstrate sub-millisecond inference latency on resource-constrained edge platforms, confirming its suitability for real-time application in both in-vehicle units and roadside infrastructure. <div>
arXiv:2512.15503v2 Announce Type: replace-cross 
Abstract: Vehicular platooning promises transformative improvements in transportation efficiency and safety through the coordination of multi-vehicle formations enabled by Vehicle-to-Everything (V2X) communication. However, the distributed nature of platoon coordination creates security vulnerabilities, allowing authenticated vehicles to inject falsified kinematic data, compromise operational stability, and pose a threat to passenger safety. Traditional misbehaviour detection approaches, which rely on plausibility checks and statistical methods, suffer from high False Positive (FP) rates and cannot capture the complex temporal dependencies inherent in multi-vehicle coordination dynamics. We present Attention In Motion (AIMformer), a transformer-based framework specifically tailored for real-time misbehaviour detection in vehicular platoons with edge deployment capabilities. AIMformer leverages multi-head self-attention mechanisms to simultaneously capture intra-vehicle temporal dynamics and inter-vehicle spatial correlations. It incorporates global positional encoding with vehicle-specific temporal offsets to handle join/exit maneuvers. We propose a Precision-Focused Binary Cross-Entropy (PFBCE) loss function that penalizes FPs to meet the requirements of safety-critical vehicular systems. Extensive evaluation across 4 platoon controllers, multiple attack vectors, and diverse mobility scenarios demonstrates superior performance ($\geq$ 0.93) compared to state-of-the-art baseline architectures. A comprehensive deployment analysis utilizing TensorFlow Lite (TFLite), Open Neural Network Exchange (ONNX), and TensorRT achieves sub-millisecond inference latency, making it suitable for real-time operation on resource-constrained edge platforms. Hence, validating AIMformer is viable for both in-vehicle and roadside infrastructure deployment.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Language Models for EDA Cloud Job Resource and Lifetime Prediction</title>
<link>https://arxiv.org/abs/2512.19701</link>
<guid>https://arxiv.org/abs/2512.19701</guid>
<content:encoded><![CDATA[
<div> Keywords: cloud computing, Electronic Design Automation, Large Language Models, resource prediction, job scheduling  

<br /><br />Summary:  
The paper addresses the growing demand for effective resource and job lifetime prediction in the Electronic Design Automation (EDA) industry amidst the rise of cloud computing. Traditional machine learning approaches often face challenges due to the complexity and heterogeneity of EDA workloads, as well as the need for extensive feature engineering and domain knowledge. To overcome these limitations, the authors propose a novel framework that fine-tunes Large Language Models (LLMs) using a text-to-text regression approach tailored for this task. They introduce techniques such as scientific notation and prefix filling to constrain the model outputs, significantly enhancing the reliability and format consistency of predictions. Furthermore, the study demonstrates that utilizing full-attention fine-tuning and inference mechanisms for sliding-window-attention LLM architectures improves prediction accuracy. The framework is validated on real-world cloud computing datasets from the EDA domain, establishing a new performance baseline for prediction tasks related to resource management and job scheduling. This work highlights the potential of LLM fine-tuning strategies in addressing complex, domain-specific prediction problems without heavy reliance on manual feature extraction. <div>
arXiv:2512.19701v1 Announce Type: new 
Abstract: The rapid growth of cloud computing in the Electronic Design Automation (EDA) industry has created a critical need for resource and job lifetime prediction to achieve optimal scheduling. Traditional machine learning methods often struggle with the complexity and heterogeneity of EDA workloads, requiring extensive feature engineering and domain expertise. We propose a novel framework that fine-tunes Large Language Models (LLMs) to address this challenge through text-to-text regression. We introduce the scientific notation and prefix filling to constrain the LLM, significantly improving output format reliability. Moreover, we found that full-attention finetuning and inference improves the prediction accuracy of sliding-window-attention LLMs. We demonstrate the effectiveness of our proposed framework on real-world cloud datasets, setting a new baseline for performance prediction in the EDA domain.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reducing Label Dependency in Human Activity Recognition with Wearables: From Supervised Learning to Novel Weakly Self-Supervised Approaches</title>
<link>https://arxiv.org/abs/2512.19713</link>
<guid>https://arxiv.org/abs/2512.19713</guid>
<content:encoded><![CDATA[
<div> Keywords: human activity recognition, wearable sensors, weakly supervised learning, self-supervised learning, multi-task learning  

<br /><br />Summary:  
This paper investigates human activity recognition (HAR) using wearable sensors across various levels of supervision, highlighting trade-offs between accuracy and the need for labeled data. First, it compares six approaches: fully supervised, basic unsupervised, weakly supervised with constraints, multi-task learning with knowledge sharing, self-supervised leveraging domain expertise, and a novel weakly self-supervised framework combining minimal labeled data with domain knowledge. Second, experiments on benchmark datasets reveal that weakly supervised methods can match fully supervised performance while substantially reducing labeling effort. Third, a multi-task learning approach improves accuracy by enabling knowledge sharing among related tasks. Fourth, the newly proposed weakly self-supervised method achieves impressive results using just 10% of labeled data, demonstrating efficiency and practicality. Finally, the study provides insights into selecting HAR solutions suited to the availability of labeled data and establishes the weakly self-supervised framework as a promising approach for real-world applications where labeled data are scarce. <div>
arXiv:2512.19713v1 Announce Type: new 
Abstract: Human activity recognition (HAR) using wearable sensors has advanced through various machine learning paradigms, each with inherent trade-offs between performance and labeling requirements. While fully supervised techniques achieve high accuracy, they demand extensive labeled datasets that are costly to obtain. Conversely, unsupervised methods eliminate labeling needs but often deliver suboptimal performance. This paper presents a comprehensive investigation across the supervision spectrum for wearable-based HAR, with particular focus on novel approaches that minimize labeling requirements while maintaining competitive accuracy. We develop and empirically compare: (1) traditional fully supervised learning, (2) basic unsupervised learning, (3) a weakly supervised learning approach with constraints, (4) a multi-task learning approach with knowledge sharing, (5) a self-supervised approach based on domain expertise, and (6) a novel weakly self-supervised learning framework that leverages domain knowledge and minimal labeled data. Experiments across benchmark datasets demonstrate that: (i) our weakly supervised methods achieve performance comparable to fully supervised approaches while significantly reducing supervision requirements; (ii) the proposed multi-task framework enhances performance through knowledge sharing between related tasks; (iii) our weakly self-supervised approach demonstrates remarkable efficiency with just 10\% of labeled data. These results not only highlight the complementary strengths of different learning paradigms, offering insights into tailoring HAR solutions based on the availability of labeled data, but also establish that our novel weakly self-supervised framework offers a promising solution for practical HAR applications where labeled data are limited.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Development and external validation of a multimodal artificial intelligence mortality prediction model of critically ill patients using multicenter data</title>
<link>https://arxiv.org/abs/2512.19716</link>
<guid>https://arxiv.org/abs/2512.19716</guid>
<content:encoded><![CDATA[
<div> Keywords: in-hospital mortality, multimodal deep learning, ICU, clinical notes, chest X-ray images  

<br /><br />Summary:  
The study aimed to develop a multimodal deep learning model to predict in-hospital mortality risk among critically ill patients after their first 24 hours in the ICU. The model integrated structured (time-invariant and time-variant variables) and unstructured clinical data (clinical notes and chest X-ray images) from multiple large datasets, including MIMIC-III, MIMIC-IV, eICU, and HiRID, covering over 200,000 ICU admissions from more than 200 hospitals between 2001 and 2022. The overall mortality rates ranged from 5.2% to 7.9% across these datasets. The multimodal model demonstrated strong performance, achieving an AUROC of 0.92, AUPRC of 0.53, and Brier score of 0.19 when using structured data alone. External validation on eight institutions in the eICU dataset showed consistent AUROCs between 0.84 and 0.92. When including patients with both clinical notes and imaging, the predictive metrics further improved, with AUROC increasing from 0.87 to 0.89, AUPRC from 0.43 to 0.48, and Brier score improving from 0.37 to 0.17. These results emphasize the value of incorporating diverse patient data sources and highlight the importance of rigorous external validation for reliable mortality prediction in critically ill patients. <div>
arXiv:2512.19716v1 Announce Type: new 
Abstract: Early prediction of in-hospital mortality in critically ill patients can aid clinicians in optimizing treatment. The objective was to develop a multimodal deep learning model, using structured and unstructured clinical data, to predict in-hospital mortality risk among critically ill patients after their initial 24 hour intensive care unit (ICU) admission. We used data from MIMIC-III, MIMIC-IV, eICU, and HiRID. A multimodal model was developed on the MIMIC datasets, featuring time series components occurring within the first 24 hours of ICU admission and predicting risk of subsequent inpatient mortality. Inputs included time-invariant variables, time-variant variables, clinical notes, and chest X-ray images. External validation occurred in a temporally separated MIMIC population, HiRID, and eICU datasets. A total of 203,434 ICU admissions from more than 200 hospitals between 2001 to 2022 were included, in which mortality rate ranged from 5.2% to 7.9% across the four datasets. The model integrating structured data points had AUROC, AUPRC, and Brier scores of 0.92, 0.53, and 0.19, respectively. We externally validated the model on eight different institutions within the eICU dataset, demonstrating AUROCs ranging from 0.84-0.92. When including only patients with available clinical notes and imaging data, inclusion of notes and imaging into the model, the AUROC, AUPRC, and Brier score improved from 0.87 to 0.89, 0.43 to 0.48, and 0.37 to 0.17, respectively. Our findings highlight the importance of incorporating multiple sources of patient information for mortality prediction and the importance of external validation.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thermodynamic Focusing for Inference-Time Search: Practical Methods for Target-Conditioned Sampling and Prompted Inference</title>
<link>https://arxiv.org/abs/2512.19717</link>
<guid>https://arxiv.org/abs/2512.19717</guid>
<content:encoded><![CDATA[
<div> Keywords: Inverted Causality Focusing Algorithm, rare solutions, focused sampling, constrained language generation, sparse-reward navigation  

<br /><br />Summary:  
This paper introduces the Inverted Causality Focusing Algorithm (ICFA), a practical framework designed to find rare but valuable solutions within large candidate spaces encountered in language generation, planning, and reinforcement learning. ICFA operates by treating the search process as a target-conditioned reweighting mechanism, leveraging an existing proposal sampler along with a task-specific similarity function to build a focused sampling distribution. It adaptively controls the focusing strength to prevent degeneracy, ensuring stability during sampling. The authors provide a clear procedural guide for implementing ICFA alongside a stability diagnostic tool based on the effective sample size, which helps monitor and maintain sample diversity. A compact theoretical sketch is included, outlining scenarios where ICFA effectively reduces the number of samples required to find rare solutions. The paper demonstrates ICFA’s applicability through two reproducible experiments: constrained language generation and sparse-reward navigation tasks. Additionally, the work explores how structured prompts serve as an approximate, language-level form of ICFA, and proposes a hybrid architecture that integrates prompted inference with algorithmic reweighting, enhancing the capability to target and discover rare yet useful outcomes. <div>
arXiv:2512.19717v1 Announce Type: new 
Abstract: Finding rare but useful solutions in very large candidate spaces is a recurring practical challenge across language generation, planning, and reinforcement learning. We present a practical framework, \emph{Inverted Causality Focusing Algorithm} (ICFA), that treats search as a target-conditioned reweighting process. ICFA reuses an available proposal sampler and a task-specific similarity function to form a focused sampling distribution, while adaptively controlling focusing strength to avoid degeneracy. We provide a clear recipe, a stability diagnostic based on effective sample size, a compact theoretical sketch explaining when ICFA can reduce sample needs, and two reproducible experiments: constrained language generation and sparse-reward navigation. We further show how structured prompts instantiate an approximate, language-level form of ICFA and describe a hybrid architecture combining prompted inference with algorithmic reweighting.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synthetic Data Blueprint (SDB): A modular framework for the statistical, structural, and graph-based evaluation of synthetic tabular data</title>
<link>https://arxiv.org/abs/2512.19718</link>
<guid>https://arxiv.org/abs/2512.19718</guid>
<content:encoded><![CDATA[
<div> synthetic data, data fidelity, evaluation metrics, tabular data, artificial intelligence<br /><br />Summary:<br /><br />1. The paper addresses the challenge of evaluating synthetic data, which is increasingly used in AI for innovation, privacy preservation, and data accessibility.<br />2. It introduces Synthetic Data Blueprint (SDB), a modular Python library designed to quantitatively and visually assess the fidelity of synthetic tabular data.<br />3. SDB features automated detection of feature types, several fidelity metrics at distributional and dependency levels, structure preservation scores using graph and embedding methods, and extensive data visualization tools.<br />4. The framework’s versatility and robustness are demonstrated through application to three distinct real-world domains: healthcare diagnostics, socioeconomic and financial modeling, and cybersecurity/network traffic analysis.<br />5. These case studies illustrate SDB’s capability to handle complex, mixed data types including clinical variables, high-cardinality categories, and high-dimensional signals, providing consistent, transparent, and reproducible benchmarking across diverse data scenarios. <div>
arXiv:2512.19718v1 Announce Type: new 
Abstract: In the rapidly evolving era of Artificial Intelligence (AI), synthetic data are widely used to accelerate innovation while preserving privacy and enabling broader data accessibility. However, the evaluation of synthetic data remains fragmented across heterogeneous metrics, ad-hoc scripts, and incomplete reporting practices. To address this gap, we introduce Synthetic Data Blueprint (SDB), a modular Pythonic based library to quantitatively and visually assess the fidelity of synthetic tabular data. SDB supports: (i) automated feature-type detection, (ii) distributional and dependency-level fidelity metrics, (iii) graph- and embedding-based structure preservation scores, and (iv) a rich suite of data visualization schemas. To demonstrate the breadth, robustness, and domain-agnostic applicability of the SDB, we evaluated the framework across three real-world use cases that differ substantially in scale, feature composition, statistical complexity, and downstream analytical requirements. These include: (i) healthcare diagnostics, (ii) socioeconomic and financial modelling, and (iii) cybersecurity and network traffic analysis. These use cases reveal how SDB can address diverse data fidelity assessment challenges, varying from mixed-type clinical variables to high-cardinality categorical attributes and high-dimensional telemetry signals, while at the same time offering a consistent, transparent, and reproducible benchmarking across heterogeneous domains.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multiscale Dual-path Feature Aggregation Network for Remaining Useful Life Prediction of Lithium-Ion Batteries</title>
<link>https://arxiv.org/abs/2512.19719</link>
<guid>https://arxiv.org/abs/2512.19719</guid>
<content:encoded><![CDATA[
<div> Keywords: RUL prediction, Lithium-ion battery, multiscale dual-path feature aggregation network, deep learning, capacity degradation<br /><br />Summary: This paper addresses the challenge of accurately predicting the remaining useful life (RUL) of Lithium-ion batteries, which is crucial for targeted maintenance and ensuring the dependability of industrial machinery. The authors highlight the inefficiencies and limitations of current modeling techniques in capturing both local and global correlations in battery degradation sequences. To overcome these challenges, they propose a novel deep learning architecture called the multiscale dual-path feature aggregation network (MDFA-Net). The MDFA-Net consists of two complementary network paths: the multiscale feature network (MF-Net), which preserves shallow information and prevents data loss, and the encoder network (EC-Net), which captures continuous trends and retains deeper sequence details. By integrating features from both shallow and deep layers, the network effectively models both local variations and global degradation patterns. The proposed approach was validated on two publicly available Lithium-ion battery datasets, where it demonstrated superior performance compared to leading existing methods. This improvement enables more accurate forecasting of the capacity degradation trajectory, facilitating better maintenance decisions and enhancing battery reliability in practical applications. <div>
arXiv:2512.19719v1 Announce Type: new 
Abstract: Targeted maintenance strategies, ensuring the dependability and safety of industrial machinery. However, current modeling techniques for assessing both local and global correlation of battery degradation sequences are inefficient and difficult to meet the needs in real-life applications. For this reason, we propose a novel deep learning architecture, multiscale dual-path feature aggregation network (MDFA-Net), for RUL prediction. MDFA-Net consists of dual-path networks, the first path network, multiscale feature network (MF-Net) that maintains the shallow information and avoids missing information, and the second path network is an encoder network (EC-Net) that captures the continuous trend of the sequences and retains deep details. Integrating both deep and shallow attributes effectively grasps both local and global patterns. Testing conducted with two publicly available Lithium-ion battery datasets reveals our approach surpasses existing top-tier methods in RUL forecasting, accurately mapping the capacity degradation trajectory.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Per-Axis Weight Deltas for Frequent Model Updates</title>
<link>https://arxiv.org/abs/2512.19720</link>
<guid>https://arxiv.org/abs/2512.19720</guid>
<content:encoded><![CDATA[
<div> Keywords: weight compression, 1-bit deltas, per-axis scaling, cold-start latency, model updates<br /><br />Summary:<br /><br />1. The paper addresses the challenge of serving multiple task-specialized large language model (LLM) variants, which is constrained by the large sizes of fine-tuned model checkpoints and the resulting cold-start latency.  
2. It introduces a novel compression approach called a 1-bit delta scheme that stores only the sign of the weight differences relative to a base model, combined with lightweight per-axis (row/column) FP16 scaling factors that are learned from a small calibration dataset.  
3. This method improves reconstruction quality compared to scalar alternatives by capturing variation across weight matrix dimensions more effectively, while keeping the compactness advantages of 1-bit storage.  
4. From a system perspective, the design enables a streamlined loader that transfers packed deltas in a single operation per module, thus reducing both cold-start latency and storage overhead significantly compared to a full FP16 checkpoint.  
5. The approach is drop-in, requires minimal calibration data, maintains inference efficiency by avoiding dense reconstruction, and the authors provide their experimental setup and source code publicly for reproducibility and further research. <div>
arXiv:2512.19720v1 Announce Type: new 
Abstract: Serving many task-specialized LLM variants is often limited by the large size of fine-tuned checkpoints and the resulting cold-start latency. Since fine-tuned weights differ from their base model by relatively small structured residuals, a natural approach is to represent them as compressed deltas. We propose a simple 1-bit delta scheme that stores only the sign of the weight difference together with lightweight per-axis (row/column) FP16 scaling factors, learned from a small calibration set. This design preserves the compactness of 1-bit deltas while more accurately capturing variation across weight dimensions, leading to improved reconstruction quality over scalar alternatives. From a systems perspective, a streamlined loader that transfers packed deltas in a single operation per module reduces cold-start latency and storage overhead, with artifacts several times smaller than a full FP16 checkpoint. The method is drop-in, requires minimal calibration data, and maintains inference efficiency by avoiding dense reconstruction. Our experimental setup and source code are available at https://github.com/kuiumdjiev/Per-Axis-Weight-Deltas-for-Frequent-Model-Updates.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sign-Aware Multistate Jaccard Kernels and Geometry for Real and Complex-Valued Signals</title>
<link>https://arxiv.org/abs/2512.19721</link>
<guid>https://arxiv.org/abs/2512.19721</guid>
<content:encoded><![CDATA[
<div> Keywords: sign-aware, Jaccard/Tanimoto, positive-semidefinite kernels, multistate embedding, coalition analysis<br /><br />Summary:  
This article introduces a novel extension of overlap-based distances, specifically the Jaccard and Tanimoto measures, tailored for arbitrary real- and complex-valued signals rather than only nonnegative vectors. The framework is sign-aware, embedding signals into multistate representations that handle positive/negative splits for real signals and Cartesian/polar decompositions for complex signals, with flexibility for user-defined state partitions to capture different regimes. By interpreting signals as atomic measures on a signed state space, similarity is defined through a generalized Jaccard overlap, yielding a family of distances bounded in [0,1] that comply with the triangle inequality and generate positive-semidefinite kernels. These properties enable direct application in kernel methods and graph-based learning algorithms. Beyond pairwise similarity, the authors develop a coalition analysis method via M\"obius inversion, which decomposes signal magnitude into additive, nonnegative components with exact budget closure across signal coalitions. Additionally, normalizing embeddings produces probability measures over state configurations, linking the distance metric to total variation and allowing decomposition by regime and intensity. The proposed construction unifies metric structure, kernel theory, probabilistic interpretation, and transparent budget accounting in one comprehensive, sign-aware framework, supporting diverse applications including correlograms, feature engineering, similarity graph construction, and analyses in scientific and financial domains. <div>
arXiv:2512.19721v1 Announce Type: new 
Abstract: We introduce a sign-aware, multistate Jaccard/Tanimoto framework that extends overlap-based distances from nonnegative vectors and measures to arbitrary real- and complex-valued signals while retaining bounded metric and positive-semidefinite kernel structure. Formally, the construction is a set- and measure-theoretic geometry: signals are represented as atomic measures on a signed state space, and similarity is given by a generalized Jaccard overlap of these measures. Each signal is embedded into a nonnegative multistate representation, using positive/negative splits for real signals, Cartesian and polar decompositions for complex signals, and user-defined state partitions for refined regime analysis. Applying the Tanimoto construction to these embeddings yields a family of $[0,1]$ distances that satisfy the triangle inequality and define positive-semidefinite kernels usable directly in kernel methods and graph-based learning. Beyond pairwise distances, we develop coalition analysis via M\"obius inversion, which decomposes signal magnitude into nonnegative, additive contributions with exact budget closure across coalitions of signals. Normalizing the same embeddings produces probability measures on coordinate -- state configurations, so that the distance becomes a monotone transform of total variation and admits a regime -- intensity decomposition. The resulting construction yields a single, mechanistically interpretable distance that simultaneously provides bounded metric structure, positive-semidefinite kernels, probabilistic semantics, and transparent budget accounting within one sign-aware framework, supporting correlograms, feature engineering, similarity graphs, and other analytical tools in scientific and financial applications.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Node-Level Financial Optimization in Demand Forecasting Through Dynamic Cost Asymmetry and Feedback Mechanism</title>
<link>https://arxiv.org/abs/2512.19722</link>
<guid>https://arxiv.org/abs/2512.19722</guid>
<content:encoded><![CDATA[
<div> Keywords: forecast adjustment, cost asymmetry, savings, self-regulation, empirical results

<br /><br />Summary: This paper introduces a novel methodology for adjusting forecasts by incorporating node-specific cost function asymmetry, allowing models to favor less costly outcomes. The approach dynamically integrates cost asymmetry into the forecasting error probability distribution, thereby generating savings through more economically efficient predictions. It employs a self-regulation mechanism that modulates the magnitude of adjustments based on observed savings, enabling the model to adapt to individual station conditions and account for factors that are not explicitly modeled, such as calibration errors and changes in macroeconomic dynamics. Through this adaptive framework, the model effectively tailors itself to varying environments to maintain accuracy and cost-efficiency over time. Finally, empirical evidence demonstrates the method's practical value, showcasing its ability to achieve annual savings of \$5.1 million, highlighting significant economic benefits and robust performance in real-world scenarios. <div>
arXiv:2512.19722v1 Announce Type: new 
Abstract: This work introduces a methodology to adjust forecasts based on node-specific cost function asymmetry. The proposed model generates savings by dynamically incorporating the cost asymmetry into the forecasting error probability distribution to favor the least expensive scenario. Savings are calculated and a self-regulation mechanism modulates the adjustments magnitude based on the observed savings, enabling the model to adapt to station-specific conditions and unmodeled factors such as calibration errors or shifting macroeconomic dynamics. Finally, empirical results demonstrate the model's ability to achieve \$5.1M annual savings.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>End-to-End Data Quality-Driven Framework for Machine Learning in Production Environment</title>
<link>https://arxiv.org/abs/2512.19723</link>
<guid>https://arxiv.org/abs/2512.19723</guid>
<content:encoded><![CDATA[
<div> Keywords: data quality assessment, machine learning, MLOps, drift detection, industrial applications<br /><br />Summary: This paper presents an innovative end-to-end framework that seamlessly integrates data quality assessment with machine learning (ML) model operations in real-time production settings, addressing the current disconnect between these processes. The framework uniquely combines dynamic drift detection, adaptive data quality metrics, and MLOps into a unified, efficient system designed for minimal computational overhead. Validated in a steel manufacturing context, specifically the Electroslag Remelting (ESR) vacuum pumping process, the framework demonstrated a 12% improvement in ML model performance, reaching an R2 score of 94%, alongside a fourfold decrease in prediction latency. A key focus is on exploring data quality acceptability thresholds to balance standards with predictive performance, providing practical guidance for industrial applications. This approach advances MLOps by delivering a robust, lightweight solution optimized for time-sensitive, data-driven decision-making in dynamic industrial environments, thereby closing the gap between theoretical data quality methods and their practical deployment. <div>
arXiv:2512.19723v1 Announce Type: new 
Abstract: This paper introduces a novel end-to-end framework that efficiently integrates data quality assessment with machine learning (ML) model operations in real-time production environments. While existing approaches treat data quality assessment and ML systems as isolated processes, our framework addresses the critical gap between theoretical methods and practical implementation by combining dynamic drift detection, adaptive data quality metrics, and MLOps into a cohesive, lightweight system. The key innovation lies in its operational efficiency, enabling real-time, quality-driven ML decision-making with minimal computational overhead. We validate the framework in a steel manufacturing company's Electroslag Remelting (ESR) vacuum pumping process, demonstrating a 12% improvement in model performance (R2 = 94%) and a fourfold reduction in prediction latency. By exploring the impact of data quality acceptability thresholds, we provide actionable insights into balancing data quality standards and predictive performance in industrial applications. This framework represents a significant advancement in MLOps, offering a robust solution for time-sensitive, data-driven decision-making in dynamic industrial environments.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Out-of-Distribution Detection for Continual Learning: Design Principles and Benchmarking</title>
<link>https://arxiv.org/abs/2512.19725</link>
<guid>https://arxiv.org/abs/2512.19725</guid>
<content:encoded><![CDATA[
<div> Keywords: Continual Learning, Out-of-Distribution Detection, Machine Learning, Adaptability, Real-world Deployment  

<br /><br />Summary:  
1. The article highlights significant progress in machine learning fueled by computational resources, large datasets, and deep learning architectures, applied across diverse fields such as malware detection and autonomous navigation.  
2. It emphasizes the growing importance of maintaining model reliability and adaptability in dynamic, real-world scenarios where data distributions change over time and novel conditions occur post-deployment.  
3. The traditional i.i.d. assumption—where training and test data are from the same distribution—is inadequate for many real-world applications, limiting model effectiveness when facing evolving environments.  
4. Retraining models from scratch with new data is costly, time-consuming, and impractical, especially in resource-constrained environments, underscoring the need for efficient alternatives.  
5. The paper advocates for the integration of Continual Learning (CL), which enables incremental learning without forgetting previous knowledge, and Out-of-Distribution (OOD) detection, which identifies novel or anomalous inputs, as essential for building robust, adaptive, and efficient AI systems capable of handling evolving data streams. <div>
arXiv:2512.19725v1 Announce Type: new 
Abstract: Recent years have witnessed significant progress in the development of machine learning models across a wide range of fields, fueled by increased computational resources, large-scale datasets, and the rise of deep learning architectures. From malware detection to enabling autonomous navigation, modern machine learning systems have demonstrated remarkable capabilities. However, as these models are deployed in ever-changing real-world scenarios, their ability to remain reliable and adaptive over time becomes increasingly important. For example, in the real world, new malware families are continuously developed, whereas autonomous driving cars are employed in many different cities and weather conditions. Models trained in fixed settings can not respond effectively to novel conditions encountered post-deployment. In fact, most machine learning models are still developed under the assumption that training and test data are independent and identically distributed (i.i.d.), i.e., sampled from the same underlying (unknown) distribution. While this assumption simplifies model development and evaluation, it does not hold in many real-world applications, where data changes over time and unexpected inputs frequently occur. Retraining models from scratch whenever new data appears is computationally expensive, time-consuming, and impractical in resource-constrained environments. These limitations underscore the need for Continual Learning (CL), which enables models to incrementally learn from evolving data streams without forgetting past knowledge, and Out-of-Distribution (OOD) detection, which allows systems to identify and respond to novel or anomalous inputs. Jointly addressing both challenges is critical to developing robust, efficient, and adaptive AI systems.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tiny, On-Device Decision Makers with the MiniConv Library</title>
<link>https://arxiv.org/abs/2512.19726</link>
<guid>https://arxiv.org/abs/2512.19726</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Edge Devices, Split-Policy Architecture, Latency Reduction, Embedded GPU<br /><br />Summary:<br /><br />1. The paper addresses the challenge of deploying visual reinforcement learning (RL) policies on resource-constrained edge devices, where computational costs and communication latency hinder performance.<br /><br />2. To mitigate these issues, the authors propose a split-policy architecture that splits the policy into two parts: a small on-device encoder and a remote policy head.<br /><br />3. The on-device encoder is implemented using OpenGL fragment-shader passes, leveraging broad support on embedded GPUs to transform high-dimensional observations into compact feature tensors.<br /><br />4. By transmitting these compact tensors instead of raw observations, the approach reduces communication overhead, lowers closed-loop decision latency in bandwidth-limited settings, and decreases server-side computational load per request.<br /><br />5. The method is benchmarked on hardware platforms including the NVIDIA Jetson Nano, Raspberry Pi 4B, and Raspberry Pi Zero 2 W, demonstrating similar learning performance with modest trade-offs in mean return, while improving execution behavior and scalability.<br /><br />6. Open-source code for training, deployment, and measurement accompanies the publication, facilitating replication and further research. <div>
arXiv:2512.19726v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has achieved strong results, but deploying visual policies on resource-constrained edge devices remains challenging due to computational cost and communication latency. Many deployments therefore offload policy inference to a remote server, incurring network round trips and requiring transmission of high-dimensional observations. We introduce a split-policy architecture in which a small on-device encoder, implemented as OpenGL fragment-shader passes for broad embedded GPU support, transforms each observation into a compact feature tensor that is transmitted to a remote policy head. In RL, this communication overhead manifests as closed-loop decision latency rather than only per-request inference latency. The proposed approach reduces transmitted data, lowers decision latency in bandwidth-limited settings, and reduces server-side compute per request, whilst achieving broadly comparable learning performance by final return (mean over the final 100 episodes) in single-run benchmarks, with modest trade-offs in mean return. We evaluate across an NVIDIA Jetson Nano, a Raspberry Pi 4B, and a Raspberry Pi Zero 2 W, reporting learning results, on-device execution behaviour under sustained load, and end-to-end decision latency and scalability measurements under bandwidth shaping. Code for training, deployment, and measurement is released as open source.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trend Extrapolation for Technology Forecasting: Leveraging LSTM Neural Networks for Trend Analysis of Space Exploration Vessels</title>
<link>https://arxiv.org/abs/2512.19727</link>
<guid>https://arxiv.org/abs/2512.19727</guid>
<content:encoded><![CDATA[
<div> Technology Forecasting, Space Exploration, LSTM, Spacecraft Lifetime, STETI  

<br /><br />Summary:  
This article addresses the complexities involved in forecasting technological advancements in domains like space exploration, where technical, economic, and policy factors intersect. It begins with an updated systematic literature review (SLR) that identifies a shift toward hybrid machine learning models in technology forecasting. Motivated by these findings, the authors introduce a novel predictive model that merges long short-term memory (LSTM) neural networks with an augmentation of Moore's law to forecast spacecraft operational lifetimes. Spacecraft lifetime is selected as it reflects technological progress in space exploration. A major methodological contribution is the enhancement of the Start Time End Time Integration (STETI) approach to overcome the right censoring bias inherent in lifetime data analyses. This bias arises because more recently launched spacecraft often have shorter observed lifetimes due to ongoing operation, which skews lifetime predictions downward for recent launch dates. STETI resolves this by transforming lifetime measurements to be functions of failure time instead of launch time, reducing distortion in lifetime vs. launch date relationships. The improved modeling approach offers valuable insights that can guide space mission planning and policy-making by providing more accurate lifetime forecasts and reflecting true technology trends. <div>
arXiv:2512.19727v1 Announce Type: new 
Abstract: Forecasting technological advancement in complex domains such as space exploration presents significant challenges due to the intricate interaction of technical, economic, and policy-related factors. The field of technology forecasting has long relied on quantitative trend extrapolation techniques, such as growth curves (e.g., Moore's law) and time series models, to project technological progress. To assess the current state of these methods, we conducted an updated systematic literature review (SLR) that incorporates recent advances. This review highlights a growing trend toward machine learning-based hybrid models.
  Motivated by this review, we developed a forecasting model that combines long short-term memory (LSTM) neural networks with an augmentation of Moore's law to predict spacecraft lifetimes. Operational lifetime is an important engineering characteristic of spacecraft and a potential proxy for technological progress in space exploration. Lifetimes were modeled as depending on launch date and additional predictors.
  Our modeling analysis introduces a novel advance in the recently introduced Start Time End Time Integration (STETI) approach. STETI addresses a critical right censoring problem known to bias lifetime analyses: the more recent the launch dates, the shorter the lifetimes of the spacecraft that have failed and can thus contribute lifetime data. Longer-lived spacecraft are still operating and therefore do not contribute data. This systematically distorts putative lifetime versus launch date curves by biasing lifetime estimates for recent launch dates downward. STETI mitigates this distortion by interconverting between expressing lifetimes as functions of launch time and modeling them as functions of failure time. The results provide insights relevant to space mission planning and policy decision-making.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hard Negative Sample-Augmented DPO Post-Training for Small Language Models</title>
<link>https://arxiv.org/abs/2512.19728</link>
<guid>https://arxiv.org/abs/2512.19728</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, mathematical reasoning, chain-of-thought, Direct Preference Optimization, MathVerifier

<br /><br />Summary: Large language models (LLMs) often struggle with mathematical reasoning, and the typical approach of categorizing solutions as simply correct or incorrect fails to capture nuanced errors in chain-of-thought (CoT) reasoning. These errors are frequently structured, where seemingly convincing solutions contain subtle logical, algebraic, or numerical mistakes. To address this, the paper proposes a lightweight and practical post-training method that fits realistic computational constraints. Starting with supervised fine-tuning (SFT) on MetaMathQA-style CoT data, the authors develop a compact MathVerifier which decomposes candidate solutions into a six-dimensional error profile, generating interpretable wrongness and absurdity scores. These scores serve two key purposes: mining hard negative examples that are near correct but flawed, and assigning per-sample importance weights emphasizing the most informative preference pairs. Both components are integrated into an offline Direct Preference Optimization (DPO) objective via a verifier-guided weighted formulation. Experiments on a 1.5B-parameter Qwen2.5 model demonstrate that verifier-guided weighted DPO outperforms vanilla SFT and unweighted DPO, especially on problems where solutions are numerically close to correct but logically inconsistent. This approach avoids costly training of large reward models or reliance on external judges, offering an efficient way to improve LLM mathematical reasoning performance. <div>
arXiv:2512.19728v1 Announce Type: new 
Abstract: Large language models (LLMs) continue to struggle with mathematical reasoning, and common post-training pipelines often reduce each generated solution to a binary outcome: correct or incorrect. This perspective is limiting in practice, as failures in chain-of-thought (CoT) reasoning are frequently structured; solutions may appear convincing while containing subtle logical, algebraic, or numerical flaws. Meanwhile, reinforcement learning from human feedback (RLHF) variants that rely on large reward models or LLM-as-a-judge signals are often expensive, difficult to scale, and unstable to iterate. We propose a lightweight and pragmatic post-training pipeline that targets such structured errors under realistic compute budgets. Starting from supervised fine-tuning (SFT) on MetaMathQA-style CoT data, we introduce a compact MathVerifier that decomposes a candidate solution into a six-dimensional error profile and aggregates it into interpretable wrongness and absurdity scores. These verifier signals serve two roles: (i) mining hard negatives that are near-correct yet structurally flawed, and (ii) defining per-sample importance weights that emphasize the most informative preference pairs. We integrate both into an offline Direct Preference Optimization (DPO) objective via a verifier-guided weighted formulation. Experiments on a 1.5B-parameter Qwen2.5 model show that verifier-guided, weighted DPO yields more targeted improvements than vanilla SFT and unweighted DPO, particularly on problems where solutions are numerically close to correct but logically inconsistent, while avoiding the overhead of training large reward models or relying on external judges.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>High-Performance Self-Supervised Learning by Joint Training of Flow Matching</title>
<link>https://arxiv.org/abs/2512.19729</link>
<guid>https://arxiv.org/abs/2512.19729</guid>
<content:encoded><![CDATA[
<div> Keywords: Diffusion models, Flow matching, Self-Supervised Learning, Wearable sensor data, Generative model  

<br /><br />Summary: This paper addresses key limitations of diffusion models used in Self-Supervised Learning (SSL), such as the trade-off between generative quality and discriminative performance, and their high computational and energy costs caused by iterative sampling. To overcome these challenges, the authors propose the Flow Matching-based Foundation Model (FlowFM), which jointly trains a representation encoder alongside a conditional flow matching generator using a decoupled architecture. This design enables FlowFM to achieve both high-fidelity data generation and effective recognition capabilities. Flow matching is employed to learn a simpler velocity field, which significantly accelerates and stabilizes the training process, thereby improving training efficiency for representation learning tasks. Experimental evaluations on wearable sensor datasets demonstrate that FlowFM reduces training time by 50.4% compared to diffusion-based approaches. Additionally, FlowFM outperforms the state-of-the-art SSL method for wearables (SSL-Wearables) across five different datasets, achieving superior downstream task performance. The model also provides up to a 51.0 times faster inference speed while maintaining a high level of generative quality. The implementation code supporting these results has been made publicly available, facilitating further research and industrial application. <div>
arXiv:2512.19729v1 Announce Type: new 
Abstract: Diffusion models can learn rich representations during data generation, showing potential for Self-Supervised Learning (SSL), but they face a trade-off between generative quality and discriminative performance. Their iterative sampling also incurs substantial computational and energy costs, hindering industrial and edge AI applications. To address these issues, we propose the Flow Matching-based Foundation Model (FlowFM), which jointly trains a representation encoder and a conditional flow matching generator. This decoupled design achieves both high-fidelity generation and effective recognition. By using flow matching to learn a simpler velocity field, FlowFM accelerates and stabilizes training, improving its efficiency for representation learning. Experiments on wearable sensor data show FlowFM reduces training time by 50.4\% compared to a diffusion-based approach. On downstream tasks, FlowFM surpassed the state-of-the-art SSL method (SSL-Wearables) on all five datasets while achieving up to a 51.0x inference speedup and maintaining high generative quality. The implementation code is available at https://github.com/Okita-Laboratory/jointOptimizationFlowMatching.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ArcGen: Generalizing Neural Backdoor Detection Across Diverse Architectures</title>
<link>https://arxiv.org/abs/2512.19730</link>
<guid>https://arxiv.org/abs/2512.19730</guid>
<content:encoded><![CDATA[
<div> Backdoor attacks, neural backdoor detection, architecture-invariant features, alignment losses, black-box detection<br /><br />Summary:<br /><br />Backdoor attacks threaten the security and reliability of deep learning models. Existing neural backdoor detection methods rely on learning features from target models but fail to generalize well to new architectures unseen during training. This paper identifies the root cause of poor generalization to new architectures and proposes ArcGen, a novel black-box detection framework. ArcGen seeks to extract architecture-invariant, or aligned, features for improved backdoor detection. Unlike prior methods that use raw model outputs as features, ArcGen introduces an alignment layer to process features and reduce architecture-specific influence. Two specially designed alignment losses enforce feature alignment across models with similar backdoor behaviors but different architectures, at both distribution and sample levels. Extensive evaluation demonstrates that ArcGen achieves up to 42.5% improvement in detection performance (e.g., AUC) on unseen architectures. The evaluation involves a large-scale dataset of 16,896 models trained on various datasets, attacked with diverse backdoor methods, and spanning multiple model architectures. The source code for ArcGen is publicly available at https://github.com/SeRAlab/ArcGen. <div>
arXiv:2512.19730v1 Announce Type: new 
Abstract: Backdoor attacks pose a significant threat to the security and reliability of deep learning models. To mitigate such attacks, one promising approach is to learn to extract features from the target model and use these features for backdoor detection. However, we discover that existing learning-based neural backdoor detection methods do not generalize well to new architectures not seen during the learning phase. In this paper, we analyze the root cause of this issue and propose a novel black-box neural backdoor detection method called ArcGen. Our method aims to obtain architecture-invariant model features, i.e., aligned features, for effective backdoor detection. Specifically, in contrast to existing methods directly using model outputs as model features, we introduce an additional alignment layer in the feature extraction function to further process these features. This reduces the direct influence of architecture information on the features. Then, we design two alignment losses to train the feature extraction function. These losses explicitly require that features from models with similar backdoor behaviors but different architectures are aligned at both the distribution and sample levels. With these techniques, our method demonstrates up to 42.5% improvements in detection performance (e.g., AUC) on unseen model architectures. This is based on a large-scale evaluation involving 16,896 models trained on diverse datasets, subjected to various backdoor attacks, and utilizing different model architectures. Our code is available at https://github.com/SeRAlab/ArcGen.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Deep-to-Shallow Transformable Neural Networks for Intelligent Embedded Systems</title>
<link>https://arxiv.org/abs/2512.19731</link>
<guid>https://arxiv.org/abs/2512.19731</guid>
<content:encoded><![CDATA[
<div> Keywords: Neural Architecture Search, Deep-to-Shallow Transform, Hardware Efficiency, Embedded Systems, Elastic Training<br /><br />Summary:<br /><br />1. The paper addresses the trade-off in convolutional neural networks (CNNs) between network depth, accuracy, and hardware efficiency, particularly in resource-constrained embedded systems. 2. The authors propose Double-Win NAS, a novel neural architecture search paradigm that automatically explores deep networks to achieve strong accuracy, which are then transformed into shallow counterparts to gain superior hardware efficiency. 3. Double-Win NAS introduces two enhanced training techniques: hybrid transformable training to improve training accuracy and arbitrary-resolution elastic training to allow the network to handle variable input resolutions naturally. 4. The method is evaluated on two popular embedded platforms, NVIDIA Jetson AGX Xavier and NVIDIA Jetson Nano, along with two large-scale datasets, ImageNet and ImageNet-100. 5. Experimental results demonstrate that Double-Win NAS outperforms existing state-of-the-art NAS approaches by delivering a better balance of accuracy and hardware efficiency, making it highly suitable for intelligent embedded systems requiring adaptable and efficient neural networks. <div>
arXiv:2512.19731v1 Announce Type: new 
Abstract: Thanks to the evolving network depth, convolutional neural networks (CNNs) have achieved remarkable success across various embedded scenarios, paving the way for ubiquitous embedded intelligence. Despite its promise, the evolving network depth comes at the cost of degraded hardware efficiency. In contrast to deep networks, shallow networks can deliver superior hardware efficiency but often suffer from inferior accuracy. To address this dilemma, we propose Double-Win NAS, a novel deep-to-shallow transformable neural architecture search (NAS) paradigm tailored for resource-constrained intelligent embedded systems. Specifically, Double-Win NAS strives to automatically explore deep networks to first win strong accuracy, which are then equivalently transformed into their shallow counterparts to further win strong hardware efficiency. In addition to search, we also propose two enhanced training techniques, including hybrid transformable training towards better training accuracy and arbitrary-resolution elastic training towards enabling natural network elasticity across arbitrary input resolutions. Extensive experimental results on two popular intelligent embedded systems (i.e., NVIDIA Jetson AGX Xavier and NVIDIA Jetson Nano) and two representative large-scale datasets (i.e., ImageNet and ImageNet-100) clearly demonstrate the superiority of Double-Win NAS over previous state-of-the-art NAS approaches.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leakage-Aware Bandgap Prediction on the JARVIS-DFT Dataset: A Phase-Wise Feature Analysis</title>
<link>https://arxiv.org/abs/2512.19732</link>
<guid>https://arxiv.org/abs/2512.19732</guid>
<content:encoded><![CDATA[
<div> Keywords: JARVIS-DFT, bandgap prediction, descriptor leakage, tree-based models, SHAP analysis<br /><br />Summary:<br /><br />1. This study conducts a systematic analysis of the JARVIS-DFT bandgap dataset to identify and eliminate descriptors that inadvertently encode band-structure information, such as effective masses, to prevent data leakage.<br />2. A curated, leakage-controlled subset of 2280 materials is generated, ensuring that predictive models are not influenced by features directly revealing the target property.<br />3. The authors propose a three-phase modeling framework that progressively integrates simple physical descriptors, engineered features, and compositional attributes to predict bandgaps.<br />4. Across all modeling phases, tree-based algorithms achieve high predictive performance with R² values ranging from approximately 0.88 to 0.90, indicating minimal gains from expanding the descriptor set when leakage is properly controlled.<br />5. SHAP (SHapley Additive exPlanations) analysis consistently highlights components of the dielectric tensor as the most influential features driving model predictions.<br /><br />This work offers a carefully curated dataset and establishes baseline metrics for bandgap prediction models that address descriptor leakage, serving as a valuable benchmark for future research in this domain. <div>
arXiv:2512.19732v1 Announce Type: new 
Abstract: In this study, we perform a systematic analysis of the JARVIS-DFT bandgap dataset and identify and remove descriptors that may inadvertently encode band-structure information, such as effective masses. This process yields a curated, leakage-controlled subset of 2280 materials. Using this dataset, a three-phase modeling framework is implemented that incrementally incorporates basic physical descriptors, engineered features, and compositional attributes. The results show that tree-based models achieve R2 values of approximately 0.88 to 0.90 across all phases, indicating that expanding the descriptor space does not substantially improve predictive accuracy when leakage is controlled. SHAP analysis consistently identifies the dielectric tensor components as the dominant contributors. This work provides a curated dataset and baseline performance metrics for future leakage-aware bandgap prediction studies.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Deleuzian Representation Hypothesis</title>
<link>https://arxiv.org/abs/2512.19734</link>
<guid>https://arxiv.org/abs/2512.19734</guid>
<content:encoded><![CDATA[
<div> Keywords: sparse autoencoders, concept extraction, clustering activations, discriminant analysis, multimodal evaluation  

<br /><br />Summary:  
This paper introduces a novel alternative to sparse autoencoders (SAEs) for unsupervised concept extraction from neural networks by clustering differences in neuron activations. The approach is theoretically grounded through a discriminant analysis framework, providing formal justification for clustering activation differences. To improve the diversity of the extracted concepts, the authors incorporate a weighting scheme based on the skewness of activations during clustering. Philosophically, the method resonates with Deleuze’s modern concept theory, which frames concepts as differences rather than static entities. The method is tested across five different neural network models and three data modalities—vision, language, and audio—to broadly assess its effectiveness. Performance metrics include concept quality, diversity, and consistency, where the new method outperforms prior unsupervised SAE variants and approaches supervised baseline results. Furthermore, the extracted concepts demonstrate a controllable influence over the model’s internal representations, confirming their causal role in shaping downstream task behavior. Overall, the paper offers a theoretically sound, interpretable, and versatile tool for concept discovery that enables enhanced understanding and manipulation of neural network representations across multiple domains. <div>
arXiv:2512.19734v1 Announce Type: new 
Abstract: We propose an alternative to sparse autoencoders (SAEs) as a simple and effective unsupervised method for extracting interpretable concepts from neural networks. The core idea is to cluster differences in activations, which we formally justify within a discriminant analysis framework. To enhance the diversity of extracted concepts, we refine the approach by weighting the clustering using the skewness of activations. The method aligns with Deleuze's modern view of concepts as differences. We evaluate the approach across five models and three modalities (vision, language, and audio), measuring concept quality, diversity, and consistency. Our results show that the proposed method achieves concept quality surpassing prior unsupervised SAE variants while approaching supervised baselines, and that the extracted concepts enable steering of a model's inner representations, demonstrating their causal influence on downstream behavior.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Case Prompting to Mitigate Large Language Model Bias for ICU Mortality Prediction</title>
<link>https://arxiv.org/abs/2512.19735</link>
<guid>https://arxiv.org/abs/2512.19735</guid>
<content:encoded><![CDATA[
<div> Keywords: ICU mortality prediction, large language models, fairness, debiasing, prompting framework<br /><br />Summary:<br />Accurate prediction of mortality risk for ICU patients is critical for clinical decision-making, but large language models (LLMs) often display demographic biases related to sex, age, and race. These biases limit their reliability and could negatively impact equitable healthcare delivery. Existing debiasing techniques generally come at the cost of reduced predictive accuracy, making it challenging to balance fairness and performance. This study addresses these challenges by first developing a comprehensive multi-dimensional bias assessment scheme to identify and diagnose biases in LLM-based ICU mortality prediction models. Building on the diagnosis, the authors propose CAse Prompting (CAP), a novel, training-free prompting framework that enhances fairness and accuracy simultaneously. CAP combines traditional debiasing prompts with case-based reasoning, guiding LLMs to learn from similar historical mispredicted cases along with their correct outcomes to correct biased reasoning patterns. Experimental evaluation on the MIMIC-IV dataset shows CAP significantly boosts performance, increasing AUROC from 0.806 to 0.873 and AUPRC from 0.497 to 0.694. It also reduces sex- and race-related disparities by over 90%. Additionally, feature reliance analysis reveals consistent attention patterns across demographic groups, with similarity scores over 0.98, emphasizing fairness at the model interpretation level. Overall, CAP offers a transferable and effective solution for equitable clinical decision support without the need for model retraining. <div>
arXiv:2512.19735v1 Announce Type: new 
Abstract: Accurate mortality risk prediction for intensive care unit (ICU) patients is essential for clinical decision-making. Although large language models (LLMs) show promise in predicting outcomes from structured medical data, their predictions may exhibit demographic biases related to sex, age, and race, limiting their trustworthy use in clinical practice. Existing debiasing methods often reduce predictive performance, making it difficult to jointly optimize fairness and accuracy. In this study, we systematically examine bias in LLM-based ICU mortality prediction and propose a training-free, clinically adaptive prompting framework to simultaneously improve fairness and performance. We first develop a multi-dimensional bias assessment scheme for comprehensive model diagnosis. Building on this analysis, we introduce CAse Prompting (CAP), a novel prompting framework that integrates conventional debiasing prompts with case-based reasoning. CAP guides the model to learn from similar historical misprediction cases and their correct outcomes, enabling correction of biased reasoning patterns. Experiments on the MIMIC-IV dataset show that CAP substantially improves both predictive accuracy and fairness. CAP increases AUROC from 0.806 to 0.873 and AUPRC from 0.497 to 0.694, while reducing sex- and race-related disparities by over 90%. Feature reliance analysis further indicates highly consistent attention patterns across demographic groups, with similarity scores exceeding 0.98. These results demonstrate that LLMs exhibit measurable bias in ICU mortality prediction, and that a carefully designed prompting framework can effectively co-optimize fairness and performance without retraining, offering a transferable paradigm for equitable clinical decision support.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoPHo: Classifier-guided Conditional Topology Generation with Persistent Homology</title>
<link>https://arxiv.org/abs/2512.19736</link>
<guid>https://arxiv.org/abs/2512.19736</guid>
<content:encoded><![CDATA[
<div> Keywords: topology generation, diffusion model, classifier guidance, persistent homology, graph structure<br /><br />Summary:<br /><br />1. The paper addresses the challenge of generating synthetic graphs that match desired topological properties due to scarcity of real topology data, which is important for applications testing and data release.<br />2. Existing diffusion-based methods either require retraining the model for each attribute by embedding conditions inside it or rely on post-training classifier guidance that ignores graph scale and practical constraints.<br />3. The authors propose a novel discrete approach that integrates gradients from a pre-trained graph-level classifier directly into the reverse diffusion process, effectively steering the graph generation toward specific structural properties.<br />4. They introduce CoPHo (Classifier-guided Conditional Topology Generation with Persistent Homology), which utilizes persistent homology filtrations during intermediate generation steps, interpreting topological features as guidance to refine generation iteratively.<br />5. Experimental results on four generic network datasets show that CoPHo surpasses previous methods in matching target graph metrics, and its applicability is further validated on the QM9 molecular dataset, demonstrating robustness and transferability across domains. <div>
arXiv:2512.19736v1 Announce Type: new 
Abstract: The structure of topology underpins much of the research on performance and robustness, yet available topology data are typically scarce, necessitating the generation of synthetic graphs with desired properties for testing or release. Prior diffusion-based approaches either embed conditions into the diffusion model, requiring retraining for each attribute and hindering real-time applicability, or use classifier-based guidance post-training, which does not account for topology scale and practical constraints. In this paper, we show from a discrete perspective that gradients from a pre-trained graph-level classifier can be incorporated into the discrete reverse diffusion posterior to steer generation toward specified structural properties. Based on this insight, we propose Classifier-guided Conditional Topology Generation with Persistent Homology (CoPHo), which builds a persistent homology filtration over intermediate graphs and interprets features as guidance signals that steer generation toward the desired properties at each denoising step. Experiments on four generic/network datasets demonstrate that CoPHo outperforms existing methods at matching target metrics, and we further validate its transferability on the QM9 molecular dataset.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simulation-Driven Railway Delay Prediction: An Imitation Learning Approach</title>
<link>https://arxiv.org/abs/2512.19737</link>
<guid>https://arxiv.org/abs/2512.19737</guid>
<content:encoded><![CDATA[
<div> train delays, stochastic simulation, imitation learning, covariate shift, uncertainty-aware forecasting<br /><br />Summary:<br /><br />1. This work addresses the critical problem of reliably predicting train delays to improve the robustness and efficiency of railway transportation systems.<br /><br />2. The authors reframe the delay forecasting task as a stochastic simulation problem by modeling state-transition dynamics using imitation learning techniques.<br /><br />3. They introduce Drift-Corrected Imitation Learning (DCIL), a novel self-supervised algorithm that extends the DAgger framework by adding a distance-based drift correction mechanism. This approach mitigates covariate shift during rollouts without needing access to an external oracle or adversarial training schemes.<br /><br />4. DCIL effectively merges the dynamical accuracy of event-driven models with the expressive power of data-driven methods, allowing for uncertainty-aware forecasting via Monte Carlo simulation.<br /><br />5. The method is thoroughly evaluated on a large-scale real-world dataset from Infrabel, the Belgian railway infrastructure manager, encompassing over three million train movements. Results show that DCIL outperforms traditional regression models and behavioral cloning strategies on deep learning architectures, demonstrating superior predictive accuracy for train delays up to 30 minutes ahead and capturing the sequential and uncertain nature of delay propagation across extensive railway networks. <div>
arXiv:2512.19737v1 Announce Type: new 
Abstract: Reliable prediction of train delays is essential for enhancing the robustness and efficiency of railway transportation systems. In this work, we reframe delay forecasting as a stochastic simulation task, modeling state-transition dynamics through imitation learning. We introduce Drift-Corrected Imitation Learning (DCIL), a novel self-supervised algorithm that extends DAgger by incorporating distance-based drift correction, thereby mitigating covariate shift during rollouts without requiring access to an external oracle or adversarial schemes. Our approach synthesizes the dynamical fidelity of event-driven models with the representational capacity of data-driven methods, enabling uncertainty-aware forecasting via Monte Carlo simulation. We evaluate DCIL using a comprehensive real-world dataset from \textsc{Infrabel}, the Belgian railway infrastructure manager, which encompasses over three million train movements. Our results, focused on predictions up to 30 minutes ahead, demonstrate superior predictive performance of DCIL over traditional regression models and behavioral cloning on deep learning architectures, highlighting its effectiveness in capturing the sequential and uncertain nature of delay propagation in large-scale networks.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpComm: A Reinforcement Learning Framework for Adaptive Buffer Control in Warehouse Volume Forecasting</title>
<link>https://arxiv.org/abs/2512.19738</link>
<guid>https://arxiv.org/abs/2512.19738</guid>
<content:encoded><![CDATA[
<div> Keywords: forecasting, reinforcement learning, LightGBM, last-mile logistics, generative AI  

<br /><br />Summary:  
Accurate forecasting of package volumes at delivery stations is essential for optimizing last-mile logistics, as misestimations cause inefficient resource use, higher costs, and delays. The paper introduces OpComm, a novel framework that integrates supervised learning with reinforcement learning-based buffer control and a generative AI communication module. A LightGBM regression model produces station-level demand forecasts that inform a Proximal Policy Optimization (PPO) agent responsible for choosing optimal buffer levels from a discrete set of actions. The reward function used in training penalizes under-buffering more severely than over-buffering, aligning with practical trade-offs between unmet demand and resource inefficiency. Feedback from station outcomes is incorporated via a Monte Carlo update mechanism, allowing the policy to adapt continuously over time. To improve interpretability and decision-making transparency, OpComm employs a generative AI layer that provides executive summaries and scenario analyses based on SHAP feature attributions. Empirical evaluation across more than 400 stations shows that OpComm outperforms manual forecasts by reducing Weighted Absolute Percentage Error (WAPE) by 21.65%, decreases under-buffering incidents, and enhances communication and transparency for stakeholders. This research demonstrates the effectiveness of combining contextual reinforcement learning with predictive models to solve complex operational forecasting problems and improve decision support in high-stakes logistics settings. <div>
arXiv:2512.19738v1 Announce Type: new 
Abstract: Accurate forecasting of package volumes at delivery stations is critical for last-mile logistics, where errors lead to inefficient resource allocation, higher costs, and delivery delays. We propose OpComm, a forecasting and decision-support framework that combines supervised learning with reinforcement learning-based buffer control and a generative AI-driven communication module. A LightGBM regression model generates station-level demand forecasts, which serve as context for a Proximal Policy Optimization (PPO) agent that selects buffer levels from a discrete action set. The reward function penalizes under-buffering more heavily than over-buffering, reflecting real-world trade-offs between unmet demand risks and resource inefficiency. Station outcomes are fed back through a Monte Carlo update mechanism, enabling continual policy adaptation. To enhance interpretability, a generative AI layer produces executive-level summaries and scenario analyses grounded in SHAP-based feature attributions. Across 400+ stations, OpComm reduced Weighted Absolute Percentage Error (WAPE) by 21.65% compared to manual forecasts, while lowering under-buffering incidents and improving transparency for decision-makers. This work shows how contextual reinforcement learning, coupled with predictive modeling, can address operational forecasting challenges and bridge statistical rigor with practical decision-making in high-stakes logistics environments.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OASI: Objective-Aware Surrogate Initialization for Multi-Objective Bayesian Optimization in TinyML Keyword Spotting</title>
<link>https://arxiv.org/abs/2512.19739</link>
<guid>https://arxiv.org/abs/2512.19739</guid>
<content:encoded><![CDATA[
<div> Keywords: Keyword Spotting, TinyML, Multi-Objective Bayesian Optimization, Initialization, Simulated Annealing  

<br /><br />Summary:  
1. The article addresses the challenge of developing accurate Keyword Spotting (KWS) models for ultra-low-power TinyML devices, which have strict resource limits such as less than 2 MB of flash memory.  
2. It highlights the effectiveness of Multi-Objective Bayesian Optimization (MOBO) for balancing multiple objectives like accuracy and model size, but emphasizes that MOBO's performance heavily depends on proper initialization, especially in budgeted black-box optimization scenarios.  
3. Current initialization strategies such as Latin Hypercube Sampling (LHS), Sobol sequences, and random search are described as naive and ad-hoc, lacking adaptation to the Pareto front and rigorous statistical evaluation.  
4. To improve initialization, the authors propose Objective-Aware Surrogate Initialization (OASI), which uses Multi-Objective Simulated Annealing (MOSA) to generate an initial diverse and high-performing Pareto set that carefully balances accuracy and model size.  
5. Experimental evaluation in a TinyML KWS setting shows that OASI outperforms LHS, Sobol, and random initialization methods by achieving the highest hypervolume (0.0627) and the lowest generational distance (0.0) consistently across multiple runs.  
6. Although OASI requires slightly more computation time (1934 seconds vs. approximately 1500 seconds), statistical analysis using the Kruskal-Wallis test and Dunn’s post-hoc test confirms its superior consistency without statistically significant differences at the 0.05 significance level.  
7. The study concludes that OASI is an effective and statistically robust initialization strategy for MOBO in resource-constrained KWS applications on TinyML devices. <div>
arXiv:2512.19739v1 Announce Type: new 
Abstract: Voice assistants utilize Keyword Spotting (KWS) to enable efficient, privacy-friendly activation. However, realizing accurate KWS models on ultra-low-power TinyML devices (often with less than $<2$ MB of flash memory) necessitates a delicate balance between accuracy with strict resource constraints. Multi-objective Bayesian Optimization (MOBO) is an ideal candidate for managing such a trade-off but is highly initialization-dependent, especially under the budgeted black-box setting. Existing methods typically fall back to naive, ad-hoc sampling routines (e.g., Latin Hypercube Sampling (LHS), Sobol sequences, or Random search) that are adapted to neither the Pareto front nor undergo rigorous statistical comparison. To address this, we propose Objective-Aware Surrogate Initialization (OASI), a novel initialization strategy that leverages Multi-Objective Simulated Annealing (MOSA) to generate a seed Pareto set of high-performing and diverse configurations that explicitly balance accuracy and model size. Evaluated in a TinyML KWS setting, OASI outperforms LHS, Sobol, and Random initialization, achieving the highest hypervolume (0.0627) and the lowest generational distance (0.0) across multiple runs, with only a modest increase in computation time (1934 s vs. $\sim$1500 s). A non-parametric statistical analysis using the Kruskal-Wallis test ($H = 5.40$, $p = 0.144$, $\eta^2 = 0.0007$) and Dunn's post-hoc test confirms OASI's superior consistency despite the non-significant overall difference with respect to the $\alpha=0.05$ threshold.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Asia Cup 2025: A Structured T20 Match-Level Dataset and Exploratory Analysis for Cricket Analytics</title>
<link>https://arxiv.org/abs/2512.19740</link>
<guid>https://arxiv.org/abs/2512.19740</guid>
<content:encoded><![CDATA[
<div> Keywords: Asia Cup 2025, T20 cricket, sports analytics, dataset, predictive modeling<br /><br />Summary:<br /><br />This paper introduces a well-structured and comprehensive dataset specifically created for the 2025 Asia Cup T20 cricket tournament. The dataset encompasses detailed records from all 19 matches, covering 61 distinct variables such as team scores, wickets, powerplay statistics, boundary counts, toss decisions, venues, and player performance highlights. The purpose of this dataset is to support and enhance data-driven research within the domain of sports analytics, particularly focusing on cricket. To illustrate the dataset's usefulness, the authors carry out an exploratory data analysis emphasizing key aspects like team performance metrics, distribution of boundary scores, and overall scoring patterns throughout the tournament. Importantly, the dataset is made publicly available via Zenodo under a Creative Commons BY 4.0 license, encouraging transparency, reproducibility, and further academic or practical applications. This open-access resource aims to serve as a benchmark for machine-readable cricket data, facilitating advancements in cricket analytics, including predictive modeling and strategic decision-making. Overall, the paper contributes a valuable tool for researchers and professionals seeking to gain insights from cricket match data and develop innovative analytical methods in sports. <div>
arXiv:2512.19740v1 Announce Type: new 
Abstract: This paper presents a structured and comprehensive dataset corresponding to the 2025 Asia Cup T20 cricket tournament, designed to facilitate data-driven research in sports analytics. The dataset comprises records from all 19 matches of the tournament and includes 61 variables covering team scores, wickets, powerplay statistics, boundary counts, toss decisions, venues, and player-specific highlights. To demonstrate its analytical value, we conduct an exploratory data analysis focusing on team performance indicators, boundary distributions, and scoring patterns. The dataset is publicly released through Zenodo under a CC-BY 4.0 license to support reproducibility and further research in cricket analytics, predictive modeling, and strategic decision-making. This work contributes an open, machine-readable benchmark dataset for advancing cricket analytics research.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EdgeFlex-Transformer: Transformer Inference for Edge Devices</title>
<link>https://arxiv.org/abs/2512.19741</link>
<guid>https://arxiv.org/abs/2512.19741</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision Transformers, model compression, pruning, quantization, edge deployment  

<br /><br />Summary: Deploying large Vision Transformer (ViT) models on edge devices is challenged by limited memory, compute, and latency constraints. This work introduces a multi-stage optimization pipeline to compress and speed up ViTs, focusing on edge deployment. The method combines activation profiling to identify less important channels, memory-aware structured pruning to reduce large MLP layers, selective mixed-precision conversion (FP16), and activation-aware quantization (AWQ) to convert weights and activations to INT8 without extensive retraining. Starting from a ViT-Huge model with 632 million parameters, the approach strictly controls memory use while maintaining accuracy. Experiments on CIFAR-10 show the optimized model reduces peak memory usage by 76% and lowers latency over sixfold, even improving accuracy compared to the original FP32 baseline. The technique enables efficient transformer inference on resource-constrained platforms without costly fine-tuning. Finally, the paper suggests that integrating dynamic sparsity and Mixture-of-Experts architectures could further boost performance and scalability across various tasks, marking a promising direction for future work in efficient edge AI. <div>
arXiv:2512.19741v1 Announce Type: new 
Abstract: Deploying large-scale transformer models on edge devices presents significant challenges due to strict constraints on memory, compute, and latency. In this work, we propose a lightweight yet effective multi-stage optimization pipeline designed to compress and accelerate Vision Transformers (ViTs) for deployment in resource-constrained environments. Our methodology combines activation profiling, memory-aware pruning, selective mixed-precision execution, and activation-aware quantization (AWQ) to reduce the model's memory footprint without requiring costly retraining or task-specific fine-tuning. Starting from a ViT-Huge backbone with 632 million parameters, we first identify low-importance channels using activation statistics collected via forward hooks, followed by structured pruning to shrink the MLP layers under a target memory budget. We further apply FP16 conversion to selected components and leverage AWQ to quantize the remaining model weights and activations to INT8 with minimal accuracy degradation. Our experiments on CIFAR-10 demonstrate that the fully optimized model achieves a 76% reduction in peak memory usage and over 6x lower latency, while retaining or even improving accuracy compared to the original FP32 baseline. This framework offers a practical path toward efficient transformer inference on edge platforms, and opens future avenues for integrating dynamic sparsity and Mixture-of-Experts (MoE) architectures to further scale performance across diverse tasks.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On-device Large Multi-modal Agent for Human Activity Recognition</title>
<link>https://arxiv.org/abs/2512.19742</link>
<guid>https://arxiv.org/abs/2512.19742</guid>
<content:encoded><![CDATA[
<div> Human Activity Recognition, Large Language Models, Multi-Modal Agent, Interpretability, Question-Answering<br /><br />Summary:<br /><br />This paper addresses the field of Human Activity Recognition (HAR), which has important applications such as healthcare and smart environments. It explores how recent advancements in Large Language Models (LLMs) can be leveraged not only for classifying human activities but also to improve interpretability and enable human-like interaction with the data outputs. The authors propose a novel Large Multi-Modal Agent designed specifically for HAR tasks that integrates LLMs to enhance both performance and user engagement. Unlike traditional HAR models that focus solely on classification accuracy, this framework provides reasoning and question-answering capabilities, enabling the system to translate complex technical results into easily understandable insights for users. The model is thoroughly evaluated on widely used HAR datasets including HHAR, Shoaib, and Motionsense. Results from these extensive experiments show that the proposed framework achieves classification accuracy on par with current state-of-the-art approaches. Additionally, it offers significant improvements in interpretability, making it more useful and accessible to end-users who need meaningful explanations rather than just raw predictions. This work bridges the gap between technical effectiveness and user-centered design in the field of HAR. <div>
arXiv:2512.19742v1 Announce Type: new 
Abstract: Human Activity Recognition (HAR) has been an active area of research, with applications ranging from healthcare to smart environments. The recent advancements in Large Language Models (LLMs) have opened new possibilities to leverage their capabilities in HAR, enabling not just activity classification but also interpretability and human-like interaction. In this paper, we present a Large Multi-Modal Agent designed for HAR, which integrates the power of LLMs to enhance both performance and user engagement. The proposed framework not only delivers activity classification but also bridges the gap between technical outputs and user-friendly insights through its reasoning and question-answering capabilities. We conduct extensive evaluations using widely adopted HAR datasets, including HHAR, Shoaib, Motionsense to assess the performance of our framework. The results demonstrate that our model achieves high classification accuracy comparable to state-of-the-art methods while significantly improving interpretability through its reasoning and Q&amp;A capabilities.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Theory to Throughput: CUDA-Optimized APML for Large-Batch 3D Learning</title>
<link>https://arxiv.org/abs/2512.19743</link>
<guid>https://arxiv.org/abs/2512.19743</guid>
<content:encoded><![CDATA[
<div> Chamfer Distance, Earth Mover Distance, APML, CUDA-APML, point cloud loss

<br /><br />Summary:  
1. Loss functions are vital for training accurate 3D point cloud models, yet existing methods often balance between geometric fidelity and computational efficiency.  
2. Chamfer Distance is computationally efficient but allows many-to-one correspondences, which can reduce geometric accuracy.  
3. Earth Mover Distance (EMD) better captures one-to-one correspondences but is computationally expensive.  
4. APML improves upon existing metrics by approximating transport via differentiable Sinkhorn iterations coupled with an analytically derived temperature parameter; however, its dense matrix computations scale quadratically in memory.  
5. This paper introduces CUDA-APML, a sparse GPU-based implementation that applies thresholding to negligible assignments and performs adaptive softmax, bidirectional symmetrization, and Sinkhorn normalization directly in sparse COO format.  
6. CUDA-APML achieves near-linear memory complexity by storing only relevant nonzero assignments, preserving gradient information while maintaining pairwise distance computation complexity at quadratic.  
7. Experimental results on ShapeNet and MM-Fi datasets show that CUDA-APML closely matches dense APML performance within a tight tolerance.  
8. Importantly, CUDA-APML reduces peak GPU memory usage by 99.9%, enabling scalable training of 3D point cloud models.  
9. The implementation code is publicly available at the provided GitHub repository for broader adoption and further research. <div>
arXiv:2512.19743v1 Announce Type: new 
Abstract: Loss functions are fundamental to learning accurate 3D point cloud models, yet common choices trade geometric fidelity for computational cost. Chamfer Distance is efficient but permits many-to-one correspondences, while Earth Mover Distance better reflects one-to-one transport at high computational cost. APML approximates transport with differentiable Sinkhorn iterations and an analytically derived temperature, but its dense formulation scales quadratically in memory. We present CUDA-APML, a sparse GPU implementation that thresholds negligible assignments and runs adaptive softmax, bidirectional symmetrization, and Sinkhorn normalization directly in COO form. This yields near-linear memory scaling and preserves gradients on the stored support, while pairwise distance evaluation remains quadratic in the current implementation. On ShapeNet and MM-Fi, CUDA-APML matches dense APML within a small tolerance while reducing peak GPU memory by 99.9%. Code available at: https://github.com/Multimodal-Sensing-Lab/apml
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepBridge: A Unified and Production-Ready Framework for Multi-Dimensional Machine Learning Validation</title>
<link>https://arxiv.org/abs/2512.19744</link>
<guid>https://arxiv.org/abs/2512.19744</guid>
<content:encoded><![CDATA[
<div> Keywords: DeepBridge, validation, fairness, knowledge distillation, synthetic data generation  

<br /><br />Summary:  
DeepBridge is an extensive 80,000-line Python library designed to unify multiple aspects of AI/ML model evaluation and enhancement, including multi-dimensional validation, automatic compliance verification, knowledge distillation, and synthetic data generation. It provides five comprehensive validation suites covering fairness (with 15 metrics), robustness through weakness detection, uncertainty quantification via conformal prediction, resilience against five types of data drift, and hyperparameter sensitivity analysis. The tool also supports automatic compliance checks aligned with EEOC, ECOA, and GDPR regulations. DeepBridge features a versatile reporting system capable of generating interactive and static HTML, PDF, and JSON audit-ready reports swiftly. Its novel HPM-KD framework employs meta-learning to enhance knowledge distillation, outperforming direct training across compression ratios from 2.3x to 7x on CIFAR100 by 1.00 to 2.04 percentage points with statistical significance (p<0.05). For scalable synthetic data creation, DeepBridge integrates Dask to handle large datasets efficiently. Demonstrated through six case studies in credit scoring, hiring, healthcare, mortgage, insurance, and fraud detection, DeepBridge significantly reduces validation time by 89% compared to fragmented tools and achieves comprehensive fairness violation detection. A usability study with 20 participants yielded an excellent SUS score of 87.5 (top 10%), a 95% success rate, and low cognitive load (NASA-TLX 28/100). DeepBridge is open source under the MIT license and fully documented online. <div>
arXiv:2512.19744v1 Announce Type: new 
Abstract: We present DeepBridge, an 80K-line Python library that unifies multi-dimensional validation, automatic compliance verification, knowledge distillation, and synthetic data generation. DeepBridge offers: (i) 5 validation suites (fairness with 15 metrics, robustness with weakness detection, uncertainty via conformal prediction, resilience with 5 drift types, hyperparameter sensitivity), (ii) automatic EEOC/ECOA/GDPR verification, (iii) multi-format reporting system (interactive/static HTML, PDF, JSON), (iv) HPM-KD framework for knowledge distillation with meta-learning, and (v) scalable synthetic data generation via Dask. Through 6 case studies (credit scoring, hiring, healthcare, mortgage, insurance, fraud) we demonstrate that DeepBridge: reduces validation time by 89% (17 min vs. 150 min with fragmented tools), automatically detects fairness violations with complete coverage (10/10 features vs. 2/10 from existing tools), generates audit-ready reports in minutes. HPM-KD demonstrates consistent superiority across compression ratios 2.3--7x (CIFAR100): +1.00--2.04pp vs. Direct Training (p<0.05), confirming that Knowledge Distillation is effective at larger teacher-student gaps. Usability study with 20 participants shows SUS score 87.5 (top 10%, ``excellent''), 95% success rate, and low cognitive load (NASA-TLX 28/100). DeepBridge is open-source under MIT license at https://github.com/deepbridge/deepbridge, with complete documentation at https://deepbridge.readthedocs.io
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Many Experts Are Enough? Towards Optimal Semantic Specialization for Mixture-of-Experts</title>
<link>https://arxiv.org/abs/2512.19765</link>
<guid>https://arxiv.org/abs/2512.19765</guid>
<content:encoded><![CDATA[
<div> Sparse Mixture-of-Experts, semantic specialization, adaptive routing, expert expansion, gradient-based detection<br /><br />Summary:<br /><br />1. The paper addresses the challenge of optimally configuring Sparse Mixture-of-Experts (SMoE) models to maximize semantic differentiation among experts, which is crucial for fully leveraging MoE architectures. 2. Existing SMoE frameworks either rely heavily on hyperparameter tuning or fail to adequately diversify the semantic roles of experts when adjusting the number of experts. 3. The authors propose MASS (Mixture-of-Experts for Adaptive Semantic Specialization), a novel framework that includes two main innovations: a gradient-based semantic drift detector that triggers expert expansion when the current experts cannot capture the semantic diversity of data, and an adaptive routing strategy that adjusts expert use dynamically based on token-level routing confidence. 4. Experiments in a synthetic controlled environment show that MASS converges to an optimal trade-off between computational cost and performance, with superior semantic specialization among experts. 5. Further evaluations on real-world datasets in both language and vision demonstrate that MASS consistently outperforms strong MoE baselines, proving its robustness across domains and its enhanced ability to specialize experts semantically. <div>
arXiv:2512.19765v1 Announce Type: new 
Abstract: Finding the optimal configuration of Sparse Mixture-ofExperts (SMoE) that maximizes semantic differentiation among experts is essential for exploiting the full potential of MoE architectures. However, existing SMoE frameworks either heavily rely on hyperparameter tuning or overlook the importance of diversifying semantic roles across experts when adapting the expert pool size. We propose Mixture-of-Experts for Adaptive Semantic Specialization (MASS), a semanticaware MoE framework for adaptive expert expansion and dynamic routing. MASS introduces two key advancements: (i) a gradient-based semantic drift detector that prompts targeted expert expansion when the existing expert pool lacks capacity to capture the full semantic diversity of the data, and (ii) an integration of adaptive routing strategy that dynamically adjusts expert usage based on token-level routing confidence mass. We first demonstrate that MASS reliably converges to the point of optimal balance between cost-performance trade-off with notably improved sematic specialization in a highly controlled synthetic setup. Further empirical results on real-world datasets across language and vision domains show that MASS consistently outperforms a range of strong MoE baselines, demonstrating its domain robustness and enhanced expert specialization.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Design City-scale Transit Routes</title>
<link>https://arxiv.org/abs/2512.19767</link>
<guid>https://arxiv.org/abs/2512.19767</guid>
<content:encoded><![CDATA[
<div> Keywords: transit route design, reinforcement learning, graph attention networks, urban mobility, network optimization<br /><br />Summary:<br /><br />1. The paper addresses the complex problem of designing efficient transit route networks, which is NP-hard and traditionally depends on manual planning with large solution spaces.<br />2. It proposes an end-to-end reinforcement learning (RL) framework utilizing graph attention networks to sequentially construct transit networks.<br />3. To tackle the long-horizon credit assignment issue in RL, the authors introduce a two-level reward system combining incremental topological feedback with simulation-based terminal rewards.<br />4. The approach is evaluated on a newly developed real-world dataset from Bloomington, Indiana, which includes accurate road networks, census-derived travel demand, and existing transit routes.<br />5. Experimental results show the learned policies significantly outperform existing transit networks and traditional heuristics in multiple scenarios and initialization schemes:<br /> - Under high transit adoption with transit center initialization, the model achieves 25.6% higher service rates, 30.9% shorter wait times, and 21.0% better bus utilization compared to the current real-world system.<br /> - Under mixed-mode scenarios with random initialization, it delivers 68.8% higher route efficiency than demand coverage heuristics and 5.9% lower travel times than shortest path methods.<br />6. The findings demonstrate that end-to-end reinforcement learning can effectively design urban transit networks that exceed the performance of human-designed and heuristic-based systems on realistic city-scale benchmarks. <div>
arXiv:2512.19767v1 Announce Type: new 
Abstract: Designing efficient transit route networks is an NP-hard problem with exponentially large solution spaces that traditionally relies on manual planning processes. We present an end-to-end reinforcement learning (RL) framework based on graph attention networks for sequential transit network construction. To address the long-horizon credit assignment challenge, we introduce a two-level reward structure combining incremental topological feedback with simulation-based terminal rewards. We evaluate our approach on a new real-world dataset from Bloomington, Indiana with topologically accurate road networks, census-derived demand, and existing transit routes. Our learned policies substantially outperform existing designs and traditional heuristics across two initialization schemes and two modal-split scenarios. Under high transit adoption with transit center initialization, our approach achieves 25.6% higher service rates, 30.9\% shorter wait times, and 21.0% better bus utilization compared to the real-world network. Under mixed-mode conditions with random initialization, it delivers 68.8% higher route efficiency than demand coverage heuristics and 5.9% lower travel times than shortest path construction. These results demonstrate that end-to-end RL can design transit networks that substantially outperform both human-designed systems and hand-crafted heuristics on realistic city-scale benchmarks.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A K-Means, Ward and DBSCAN repeatability study</title>
<link>https://arxiv.org/abs/2512.19772</link>
<guid>https://arxiv.org/abs/2512.19772</guid>
<content:encoded><![CDATA[
<div> Keywords: reproducibility, repeatability, clustering algorithms, K-Means, OpenMP<br /><br />Summary:<br /><br />1. This study emphasizes the critical importance of reproducibility in machine learning to ensure consistent scientific conclusions and integrity. 2. The authors break down three widely used clustering algorithms—K-Means, DBSCAN, and Ward—into their fundamental procedural steps to analyze where repeatability can be guaranteed. 3. Using scikit-learn’s Python implementations, they assess which parts of these methods yield repeatable results under various computational settings. 4. Their investigation uncovers that K-Means exhibits inconsistent outcomes when the number of OpenMP threads exceeds two, pointing to a concurrency-related repeatability problem. 5. The paper aims to inform both users and developers about these repeatability issues, encouraging more thorough scrutiny and development of fixes to ensure reliable clustering results in future machine learning workflows. <div>
arXiv:2512.19772v1 Announce Type: new 
Abstract: Reproducibility is essential in machine learning because it ensures that a model or experiment yields the same scientific conclusion. For specific algorithms repeatability with bitwise identical results is also a key for scientific integrity because it allows debugging. We decomposed several very popular clustering algorithms: K-Means, DBSCAN and Ward into their fundamental steps, and we identify the conditions required to achieve repeatability at each stage. We use an implementation example with the Python library scikit-learn to examine the repeatable aspects of each method. Our results reveal inconsistent results with K-Means when the number of OpenMP threads exceeds two. This work aims to raise awareness of this issue among both users and developers, encouraging further investigation and potential fixes.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reduced Order Modeling for Tsunami Forecasting with Bayesian Hierarchical Pooling</title>
<link>https://arxiv.org/abs/2512.19804</link>
<guid>https://arxiv.org/abs/2512.19804</guid>
<content:encoded><![CDATA[
<div> Keywords: Reduced order models, neural Galerkin-Projections, tsunami modeling, hierarchical pooling, randPROM<br /><br />Summary:<br /><br />1. The paper proposes a novel reduced order model (ROM) called randPROM, which differs from traditional ROMs by not being limited to fixed temporal weights. Instead, it leverages neural Galerkin-Projections to model the dynamics of these weights via physics-informed neural networks.<br /><br />2. This approach encodes the physics of governing partial differential equations (PDEs) into an initial value problem and calibrates the temporal weights through neural networks to accurately capture system trajectories.<br /><br />3. A hierarchical statistical pooling technique is used to learn distributions over the initial values of temporal weights, allowing randPROM to generate new, statistically interpretable weights that generalize across similar problems.<br /><br />4. When combined with fixed spatial features, randPROM forms a complete physics surrogate capable of generating simulations consistent in distribution with the neighborhood of initial conditions used for model construction.<br /><br />5. The method is applied to complex, nonlinear tsunami scenarios — both a synthetic Fiji model and the real 2011 Tohoku disaster — demonstrating its potential to drastically reduce the computational cost of generating accurate, statistically calibrated, and physically justified predictions for tsunami wave arrival times and heights. <div>
arXiv:2512.19804v1 Announce Type: new 
Abstract: Reduced order models (ROM) can represent spatiotemporal processes in significantly fewer dimensions and can be solved many orders faster than their governing partial differential equations (PDEs). For example, using a proper orthogonal decomposition produces a ROM that is a small linear combination of fixed features and weights, but that is constrained to the given process it models. In this work, we explore a new type of ROM that is not constrained to fixed weights, based on neural Galerkin-Projections, which is an initial value problem that encodes the physics of the governing PDEs, calibrated via neural networks to accurately model the trajectory of these weights. Then using a statistical hierarchical pooling technique to learn a distribution on the initial values of the temporal weights, we can create new, statistically interpretable and physically justified weights that are generalized to many similar problems. When recombined with the spatial features, we form a complete physics surrogate, called a randPROM, for generating simulations that are consistent in distribution to a neighborhood of initial conditions close to those used to construct the ROM. We apply the randPROM technique to the study of tsunamis, which are unpredictable, catastrophic, and highly-detailed non-linear problems, modeling both a synthetic case of tsunamis near Fiji and the real-world Tohoku 2011 disaster. We demonstrate that randPROMs may enable us to significantly reduce the number of simulations needed to generate a statistically calibrated and physically defensible prediction model for arrival time and height of tsunami waves.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guardrailed Uplift Targeting: A Causal Optimization Playbook for Marketing Strategy</title>
<link>https://arxiv.org/abs/2512.19805</link>
<guid>https://arxiv.org/abs/2512.19805</guid>
<content:encoded><![CDATA[
<div> Keywords: marketing decision framework, Conditional Average Treatment Effects, uplift learners, constrained targeting, revenue optimization<br /><br />Summary: This paper presents a novel marketing decision framework designed to convert heterogeneous-treatment uplift into constrained targeting strategies that optimize revenue and customer retention while respecting business guardrails. The framework begins by estimating Conditional Average Treatment Effects (CATE) using uplift learners, enabling more precise measurement of the incremental impact of different marketing actions. It then solves a constrained allocation problem to optimally decide both which customers to target and which specific offers to deploy, taking into account practical constraints like budget limits and acceptable sales deterioration thresholds. The approach is empirically validated across multiple use cases such as retention messaging, event rewards, and spend-threshold assignments, consistently outperforming traditional propensity score and static baseline methods in offline evaluation metrics including uplift AUC, Inverse Propensity Scoring (IPS), and Self-Normalized IPS (SNIPS). Moreover, a production-scale online A/B test confirms the method’s effectiveness in generating strategic lift on revenue and completion metrics while maintaining customer experience standards through enforced constraints. Ultimately, the paper delivers a practical, reusable playbook for marketers to efficiently operationalize causal targeting at scale, establish guardrails, and align marketing campaigns with key strategic performance indicators (KPIs). <div>
arXiv:2512.19805v1 Announce Type: new 
Abstract: This paper introduces a marketing decision framework that converts heterogeneous-treatment uplift into constrained targeting strategies to maximize revenue and retention while honoring business guardrails. The approach estimates Conditional Average Treatment Effects (CATE) with uplift learners and then solves a constrained allocation to decide who to target and which offer to deploy under limits such as budget or acceptable sales deterioration. Applied to retention messaging, event rewards, and spend-threshold assignment, the framework consistently outperforms propensity and static baselines in offline evaluations using uplift AUC, Inverse Propensity Scoring (IPS), and Self-Normalized IPS (SNIPS). A production-scale online A/B test further validates strategic lift on revenue and completion while preserving customer-experience constraints. The result is a reusable playbook for marketers to operationalize causal targeting at scale, set guardrails, and align campaigns with strategic KPIs.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-Tuned In-Context Learners for Efficient Adaptation</title>
<link>https://arxiv.org/abs/2512.19879</link>
<guid>https://arxiv.org/abs/2512.19879</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, fine-tuning, in-context learning, prompt engineering, prequential evaluation<br /><br />Summary:  
This paper addresses adapting large language models (LLMs) to downstream tasks using two common approaches: prompt engineering with in-context few-shot learning and fine-tuning on task-specific data. It identifies key limitations where prompt-based methods plateau as data increases, and fine-tuning struggles in low-data settings. The authors propose a unified approach that integrates in-context learning directly into the fine-tuning process. This is done by augmenting task-specific data with in-context examples that mimic k-shot prompting structures, thereby combining the strengths of both paradigms. Although this method requires task-specific fine-tuning, it offers improved sample efficiency from in-context learning and enhanced performance from fine-tuning, often surpassing traditional baselines. To efficiently select hyperparameters in scenarios with limited data, the paper suggests using prequential evaluation, which avoids costly cross-validation and maximizes data use by jointly training and validating in a sequential manner. An extensive empirical study evaluates and compares fine-tuning, in-context learning, and the proposed unified approach across concrete downstream tasks. Results demonstrate that the unified approach consistently achieves the best predictive performance, establishing a new effective strategy for LLM adaptation in both low- and high-data regimes. <div>
arXiv:2512.19879v1 Announce Type: new 
Abstract: When adapting large language models (LLMs) to a specific downstream task, two primary approaches are commonly employed: (1) prompt engineering, often with in-context few-shot learning, leveraging the model's inherent generalization abilities, and (2) fine-tuning on task-specific data, directly optimizing the model's parameters. While prompt-based methods excel in few-shot scenarios, their effectiveness often plateaus as more data becomes available. Conversely, fine-tuning scales well with data but may underperform when training examples are scarce. We investigate a unified approach that bridges these two paradigms by incorporating in-context learning directly into the fine-tuning process. Specifically, we fine-tune the model on task-specific data augmented with in-context examples, mimicking the structure of k-shot prompts. This approach, while requiring per-task fine-tuning, combines the sample efficiency of in-context learning with the performance gains of fine-tuning, leading to a method that consistently matches and often significantly exceeds both these baselines. To perform hyperparameter selection in the low-data regime, we propose to use prequential evaluation, which eliminates the need for expensive cross-validation and leverages all available data for training while simultaneously providing a robust validation signal. We conduct an extensive empirical study to determine which adaptation paradigm - fine-tuning, in-context learning, or our proposed unified approach offers the best predictive performance on a concrete data downstream-tasks.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Demystifying LLM-as-a-Judge: Analytically Tractable Model for Inference-Time Scaling</title>
<link>https://arxiv.org/abs/2512.19905</link>
<guid>https://arxiv.org/abs/2512.19905</guid>
<content:encoded><![CDATA[
<div> Keywords: inference-time scaling, Bayesian linear regression, generalization error, large language models, reward-weighted sampling<br /><br />Summary:  
This paper introduces an analytically tractable model to study inference-time scaling in large language models (LLMs), specifically modeling the scenario as Bayesian linear regression with a reward-weighted sampler. The reward is determined by a linear model reflecting an LLM acting as a judge. The analysis is focused on the high-dimensional regime, where closed-form expressions for the posterior predictive mean and variance are derived. The study examines how generalization error behaves when training data come from a teacher model and inference involves drawing \( k \) samples with selection via a softmax function at a temperature tied to a quadratic reward. When the reward closely aligns with the teacher, generalization error decreases monotonically as \( k \) increases. However, if the reward is misspecified, there exists an optimal number of samples \( k \), after which additional sampling worsens performance. For a fixed number of samples, an optimal sampling temperature is identified. Experimental verification uses another LLM as a judge to confirm these findings. Theoretical results show that in the "best-of-\( k \)" scenario with the teacher as reward, generalization error decays on the order of \( 1/k^2 \), with the leading term derived via extreme value theory. The work also highlights that increasing task difficulty reduces the advantage of inference-time compute over acquiring more data. <div>
arXiv:2512.19905v1 Announce Type: new 
Abstract: Recent developments in large language models have shown advantages in reallocating a notable share of computational resource from training time to inference time. However, the principles behind inference time scaling are not well understood. In this paper, we introduce an analytically tractable model of inference-time scaling: Bayesian linear regression with a reward-weighted sampler, where the reward is determined from a linear model, modeling LLM-as-a-judge scenario. We study this problem in the high-dimensional regime, where the deterministic equivalents dictate a closed-form expression for the posterior predictive mean and variance. We analyze the generalization error when training data are sampled from a teacher model. We draw $k$ inference-time samples and select via softmax at a temperature applied to a quadratic reward. When the reward is not too different from the teacher, the generalization error decreases monotonically with increasing inference time samples $k$. However, the specific reward that optimizes inference-time selection generally differs from the teacher. In contrast, substantial reward misspecification induces a finite optimal $k$ beyond which more sampling can increase the generalization error. For fixed $k$, there exists an optimal sampling temperature. We experimentally verify these facts in large language model inference with an additional large language model as a judge. In the "best-of-$k$" limit with the teacher as reward, we theoretically show that the generalization error decays as $\Theta(1/k^2)$ and determine the leading coefficient via extreme value theory. These formulas delineate domains where scaling inference-time computation is provably preferable to collecting more data. Finally, we demonstrate that when task difficulty increases, the previously mentioned advantage of inference-time compute degrades.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modeling Non-Ergodic Path Effects Using Conditional Generative Model for Fourier Amplitude Spectra</title>
<link>https://arxiv.org/abs/2512.19909</link>
<guid>https://arxiv.org/abs/2512.19909</guid>
<content:encoded><![CDATA[
<div> non-ergodic ground-motion models, Gaussian Process, Conditional Variational Autoencoder, Fourier Amplitude Spectra, seismic hazard analysis<br /><br />Summary:<br /><br />This study addresses limitations in current non-ergodic ground-motion models (GMMs) which use Gaussian Process (GP) methods that rely on prescribed correlation functions and face computational challenges for large-scale applications. The authors introduce a deep-learning method called Conditional Generative Modeling for Fourier Amplitude Spectra (CGM-FAS) as an alternative GP-based approach to model non-ergodic path effects across frequencies. CGM-FAS employs a Conditional Variational Autoencoder architecture that learns spatial patterns and interfrequency correlations directly from earthquake and station data, conditioned on their geographic coordinates. Using earthquake data from the San Francisco Bay Area, CGM-FAS is compared with a recent GP-based GMM, showing consistent predictions of non-ergodic path effects. Beyond accuracy, CGM-FAS can learn spatial dependencies without predefined correlation functions, capture correlations between frequencies, and perform rapid predictions, generating maps for 10,000 sites across 1,000 frequencies in about 10 seconds using a few gigabytes of memory. The model’s hyperparameters can be tuned to maintain variability consistent with empirical GP-based models. Overall, CGM-FAS presents an efficient and scalable approach to non-ergodic ground-motion prediction over multiple frequencies and large spatial regions, enabling improved site-specific seismic hazard assessments. <div>
arXiv:2512.19909v1 Announce Type: new 
Abstract: Recent developments in non-ergodic ground-motion models (GMMs) explicitly model systematic spatial variations in source, site, and path effects, reducing standard deviation to 30-40% of ergodic models and enabling more accurate site-specific seismic hazard analysis. Current non-ergodic GMMs rely on Gaussian Process (GP) methods with prescribed correlation functions and thus have computational limitations for large-scale predictions. This study proposes a deep-learning approach called Conditional Generative Modeling for Fourier Amplitude Spectra (CGM-FAS) as an alternative to GP-based methods for modeling non-ergodic path effects in Fourier Amplitude Spectra (FAS). CGM-FAS uses a Conditional Variational Autoencoder architecture to learn spatial patterns and interfrequency correlation directly from data by using geographical coordinates of earthquakes and stations as conditional variables. Using San Francisco Bay Area earthquake data, we compare CGM-FAS against a recent GP-based GMM for the region and demonstrate consistent predictions of non-ergodic path effects. Additionally, CGM-FAS offers advantages compared to GP-based approaches in learning spatial patterns without prescribed correlation functions, capturing interfrequency correlations, and enabling rapid predictions, generating maps for 10,000 sites across 1,000 frequencies within 10 seconds using a few GB of memory. CGM-FAS hyperparameters can be tuned to ensure generated path effects exhibit variability consistent with the GP-based empirical GMM. This work demonstrates a promising direction for efficient non-ergodic ground-motion prediction across multiple frequencies and large spatial domains.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.19920</link>
<guid>https://arxiv.org/abs/2512.19920</guid>
<content:encoded><![CDATA[
<div> Keywords: hallucination, behavioral calibration, reinforcement learning, uncertainty quantification, LLM training

<br /><br />Summary:  
1. This paper addresses the challenge of hallucinations in large language models (LLMs), which produce plausible but factually incorrect assertions, limiting their deployment in critical domains.  
2. It argues that hallucinations are not mere random errors but arise from training objectives prioritizing data mimicry over epistemic honesty, with standard RLVR methods incentivizing guessing to maximize binary rewards.  
3. The authors propose behavioral calibration techniques that encourage models to admit uncertainty by abstaining from responses or flagging uncertain claims, thus aligning outputs with accuracy.  
4. They introduce training interventions based on strictly proper scoring rules to help models output well-calibrated probabilities of correctness.  
5. Empirical studies using Qwen3-4B-Instruct demonstrate that behavior-calibrated reinforcement learning enables smaller models to excel in uncertainty quantification, outperforming frontier models like GPT-5 in metrics such as Accuracy-to-Hallucination Ratio on math reasoning tasks (BeyondAIME).  
6. Furthermore, their 4B model matches zero-shot calibration error levels of top-tier models (Grok-4, Gemini-2.5-Pro) in cross-domain factual QA (SimpleQA), despite having lower factual accuracy, indicating a transferable meta-skill separable from raw prediction performance. <div>
arXiv:2512.19920v1 Announce Type: new 
Abstract: LLM deployment in critical domains is currently impeded by persistent hallucinations--generating plausible but factually incorrect assertions. While scaling laws drove significant improvements in general capabilities, theoretical frameworks suggest hallucination is not merely stochastic error but a predictable statistical consequence of training objectives prioritizing mimicking data distribution over epistemic honesty. Standard RLVR paradigms, utilizing binary reward signals, inadvertently incentivize models as good test-takers rather than honest communicators, encouraging guessing whenever correctness probability exceeds zero. This paper presents an exhaustive investigation into behavioral calibration, which incentivizes models to stochastically admit uncertainty by abstaining when not confident, aligning model behavior with accuracy. Synthesizing recent advances, we propose and evaluate training interventions optimizing strictly proper scoring rules for models to output a calibrated probability of correctness. Our methods enable models to either abstain from producing a complete response or flag individual claims where uncertainty remains. Utilizing Qwen3-4B-Instruct, empirical analysis reveals behavior-calibrated reinforcement learning allows smaller models to surpass frontier models in uncertainty quantification--a transferable meta-skill decouplable from raw predictive accuracy. Trained on math reasoning tasks, our model's log-scale Accuracy-to-Hallucination Ratio gain (0.806) exceeds GPT-5's (0.207) in a challenging in-domain evaluation (BeyondAIME). Moreover, in cross-domain factual QA (SimpleQA), our 4B LLM achieves zero-shot calibration error on par with frontier models including Grok-4 and Gemini-2.5-Pro, even though its factual accuracy is much lower.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Seismic Wavefield Common Task Framework</title>
<link>https://arxiv.org/abs/2512.19927</link>
<guid>https://arxiv.org/abs/2512.19927</guid>
<content:encoded><![CDATA[
<div> Keywords: seismology, machine learning, seismic wavefields, Common Task Framework, reconstruction  

<br /><br />Summary:  
Seismology faces major challenges in accurately forecasting and reconstructing seismic states such as earthquake early warnings and ground motion predictions, complicated by source variability and diverse Earth models. Traditional simulation approaches are limited due to massive data volumes and numerical complexity, while real-world data collection suffers from sparse sensor coverage and models that inadequately capture Earth's complexity. Machine learning (ML) has emerged as a promising solution, but progress is hindered by inconsistent evaluation methods and lack of standardized benchmarks. To address these issues, the authors propose a Common Task Framework (CTF) for ML applied to seismic wavefields, inspired by successful frameworks in fields like natural language processing. The CTF incorporates three distinct wavefield datasets spanning global, crustal, and local scales, along with task-specific metrics tailored to forecasting, reconstruction, and generalization challenges under realistic conditions such as noise and limited data availability. The framework enables rigorous, fair, and reproducible head-to-head comparisons of ML algorithms using hidden test sets. The paper demonstrates the CTF's utility through performance evaluations of various methods and foundational models on two datasets, highlighting their respective strengths and weaknesses for different seismic tasks. Overall, this standardized evaluation framework aims to improve rigor, transparency, and progress in scientific ML applied to seismology. <div>
arXiv:2512.19927v1 Announce Type: new 
Abstract: Seismology faces fundamental challenges in state forecasting and reconstruction (e.g., earthquake early warning and ground motion prediction) and managing the parametric variability of source locations, mechanisms, and Earth models (e.g., subsurface structure and topography effects). Addressing these with simulations is hindered by their massive scale, both in synthetic data volumes and numerical complexity, while real-data efforts are constrained by models that inadequately reflect the Earth's complexity and by sparse sensor measurements from the field. Recent machine learning (ML) efforts offer promise, but progress is obscured by a lack of proper characterization, fair reporting, and rigorous comparisons. To address this, we introduce a Common Task Framework (CTF) for ML for seismic wavefields, starting with three distinct wavefield datasets. Our CTF features a curated set of datasets at various scales (global, crustal, and local) and task-specific metrics spanning forecasting, reconstruction, and generalization under realistic constraints such as noise and limited data. Inspired by CTFs in fields like natural language processing, this framework provides a structured and rigorous foundation for head-to-head algorithm evaluation. We illustrate the evaluation procedure with scores reported for two of the datasets, showcasing the performance of various methods and foundation models for reconstructing seismic wavefields from both simulated and real-world sensor measurements. The CTF scores reveal the strengths, limitations, and suitability for specific problem classes. Our vision is to replace ad hoc comparisons with standardized evaluations on hidden test sets, raising the bar for rigor and reproducibility in scientific ML.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conditional Adversarial Fragility in Financial Machine Learning under Macroeconomic Stress</title>
<link>https://arxiv.org/abs/2512.19935</link>
<guid>https://arxiv.org/abs/2512.19935</guid>
<content:encoded><![CDATA[
<div> Keywords: adversarial robustness, financial machine learning, economic stress, regime dependent, model risk assessment<br /><br />Summary:<br /><br />This article introduces the concept of Conditional Adversarial Fragility, highlighting that adversarial vulnerability in financial machine learning models is not static but regime dependent, becoming amplified during periods of macroeconomic stress. It proposes a regime-aware evaluation framework for time-indexed tabular financial classification tasks that conditions robustness assessment on external economic stress indicators. Using volatility-based regime segmentation as a proxy for economic conditions, the study compares model behavior during calm versus stress periods while keeping model architecture, attack methods, and evaluation protocols constant. Results demonstrate that baseline predictive performance remains stable across regimes, showing economic stress alone does not cause inherent performance degradation. However, under adversarial perturbations, models in stress regimes exhibit significantly greater drops in predictive accuracy, impact operational decision thresholds, and worsen risk-sensitive outcomes. This fragility also increases false negative rates, raising the risk of missing high-risk financial cases during adverse economic conditions. To aid governance and interpretability, the authors introduce a semantic auditing layer leveraging large language models to evaluate model explanations in the context of robustness. Overall, the study reveals adversarial robustness in financial ML is regime-dependent and calls for stress-aware approaches in model risk management for critical financial applications. <div>
arXiv:2512.19935v1 Announce Type: new 
Abstract: Machine learning models used in financial decision systems operate in nonstationary economic environments, yet adversarial robustness is typically evaluated under static assumptions. This work introduces Conditional Adversarial Fragility, a regime dependent phenomenon in which adversarial vulnerability is systematically amplified during periods of macroeconomic stress. We propose a regime aware evaluation framework for time indexed tabular financial classification tasks that conditions robustness assessment on external indicators of economic stress. Using volatility based regime segmentation as a proxy for macroeconomic conditions, we evaluate model behavior across calm and stress periods while holding model architecture, attack methodology, and evaluation protocols constant. Baseline predictive performance remains comparable across regimes, indicating that economic stress alone does not induce inherent performance degradation. Under adversarial perturbations, however, models operating during stress regimes exhibit substantially greater degradation across predictive accuracy, operational decision thresholds, and risk sensitive outcomes. We further demonstrate that this amplification propagates to increased false negative rates, elevating the risk of missed high risk cases during adverse conditions. To complement numerical robustness metrics, we introduce an interpretive governance layer based on semantic auditing of model explanations using large language models. Together, these results demonstrate that adversarial robustness in financial machine learning is a regime dependent property and motivate stress aware approaches to model risk assessment in high stakes financial deployments.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatio-Temporal Graph Neural Networks for Dairy Farm Sustainability Forecasting and Counterfactual Policy Analysis</title>
<link>https://arxiv.org/abs/2512.19970</link>
<guid>https://arxiv.org/abs/2512.19970</guid>
<content:encoded><![CDATA[
<div> Keywords: Spatio-Temporal Graph Neural Networks, Variational Autoencoder, Irish Cattle Breeding Federation, sustainability indices, Principal Component Analysis<br /><br />Summary: This study presents a novel data-driven framework that employs Spatio-Temporal Graph Neural Networks (STGNN) at a county scale to forecast composite sustainability indices derived from herd-level operational data. A key innovation is the introduction of an end-to-end pipeline that uses a Variational Autoencoder (VAE) to augment sparse datasets from the Irish Cattle Breeding Federation (ICBF) while preserving their joint statistical distributions. The research further develops a pillar-based scoring system through Principal Component Analysis (PCA), which identifies four main sustainability pillars: Reproductive Efficiency, Genetic Management, Herd Health, and Herd Management. These pillars form the basis of weighted composite indices designed to comprehensively assess sustainability at the herd level. The proposed STGNN model explicitly encodes spatial dependencies between counties and incorporates non-linear temporal dynamics to enable multi-year forecasting, specifically targeting the years 2026 to 2030. This approach represents the first application of STGNNs on such a scale for sustainability forecasting in cattle breeding, offering improved accuracy and interpretability by capturing the complex interplay of geographic and temporal factors affecting herd sustainability outcomes. <div>
arXiv:2512.19970v1 Announce Type: new 
Abstract: This study introduces a novel data-driven framework and the first-ever county-scale application of Spatio-Temporal Graph Neural Networks (STGNN) to forecast composite sustainability indices from herd-level operational records. The methodology employs a novel, end-to-end pipeline utilizing a Variational Autoencoder (VAE) to augment Irish Cattle Breeding Federation (ICBF) datasets, preserving joint distributions while mitigating sparsity. A first-ever pillar-based scoring formulation is derived via Principal Component Analysis, identifying Reproductive Efficiency, Genetic Management, Herd Health, and Herd Management, to construct weighted composite indices. These indices are modelled using a novel STGNN architecture that explicitly encodes geographic dependencies and non-linear temporal dynamics to generate multi-year forecasts for 2026-2030.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bloom Filter Encoding for Machine Learning</title>
<link>https://arxiv.org/abs/2512.19991</link>
<guid>https://arxiv.org/abs/2512.19991</guid>
<content:encoded><![CDATA[
<div> Keywords: Bloom filter transform, privacy-preserving encoding, machine learning preprocessing, classification accuracy, memory efficiency<br /><br />Summary:<br /><br />This article introduces a novel preprocessing technique for machine learning that leverages the Bloom filter transform to encode data samples into compact bit arrays. The method aims to reduce memory usage while preserving the privacy of the original data by obfuscating sensitive information through the encoding process. The approach ensures that enough structural information is retained within the Bloom filter representation to enable accurate classification. The technique is evaluated across six diverse datasets, including SMS Spam Collection, ECG200, Adult 50K, CDC Diabetes, MNIST, and Fashion MNIST, demonstrating its versatility. Four classification models—Extreme Gradient Boosting, Deep Neural Networks, Convolutional Neural Networks, and Logistic Regression—are used to assess performance. Experimental results indicate that models trained on Bloom filter-encoded data achieve comparable accuracy to those trained on raw data or other data transformation methods. Additionally, the Bloom filter transform offers notable memory savings, making it efficient for large-scale applications. Importantly, the approach enhances data privacy, providing a layer of protection without sacrificing model performance. Overall, the study establishes the Bloom filter transform as an effective and privacy-conscious preprocessing strategy suitable for a wide range of machine learning tasks. <div>
arXiv:2512.19991v1 Announce Type: new 
Abstract: We present a method that uses the Bloom filter transform to preprocess data for machine learning. Each sample is encoded into a compact, privacy-preserving bit array. This reduces memory use and protects the original data while keeping enough structure for accurate classification. We test the method on six datasets: SMS Spam Collection, ECG200, Adult 50K, CDC Diabetes, MNIST, and Fashion MNIST. Four classifiers are used: Extreme Gradient Boosting, Deep Neural Networks, Convolutional Neural Networks, and Logistic Regression. Results show that models trained on Bloom filter encodings achieve accuracy similar to models trained on raw data or other transforms. At the same time, the method provides memory savings while enhancing privacy. These results suggest that the Bloom filter transform is an efficient preprocessing approach for diverse machine learning tasks.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LoFT-LLM: Low-Frequency Time-Series Forecasting with Large Language Models</title>
<link>https://arxiv.org/abs/2512.20002</link>
<guid>https://arxiv.org/abs/2512.20002</guid>
<content:encoded><![CDATA[
<div> Keywords: time-series forecasting, low-frequency learning, large language model, few-shot learning, spectral patch analysis

<br /><br />Summary:  
This paper addresses challenges in time-series forecasting encountered in real-world domains like finance and energy, where data is limited and temporal dynamics are noisy and complex. Traditional deep forecasting models typically rely on full-length temporal windows, which tend to be overwhelmed by high-frequency noise and mask important long-term trends. To overcome these limitations, the authors propose LoFT-LLM, a novel frequency-aware forecasting framework that combines low-frequency trend extraction with semantic calibration using a large language model (LLM). The method first employs a Patch Low-Frequency forecasting Module (PLFM) to isolate stable low-frequency signals from localized spectral patches, capturing essential trends. Then, a residual learner component models the challenging high-frequency variations, enhancing prediction detail. Finally, a fine-tuned LLM refines forecasts by integrating rich auxiliary context and domain-specific knowledge through structured natural language prompts, addressing underutilization of side information especially in few-shot scenarios. Empirical results on financial and energy datasets show that LoFT-LLM consistently outperforms strong benchmarks across both full-data and few-shot regimes, achieving greater accuracy, robustness to noise, and improved interpretability of forecasts. This demonstrates the potential of combining spectral analysis and language model-based semantic calibration for enhanced time-series forecasting in practical applications. <div>
arXiv:2512.20002v1 Announce Type: new 
Abstract: Time-series forecasting in real-world applications such as finance and energy often faces challenges due to limited training data and complex, noisy temporal dynamics. Existing deep forecasting models typically supervise predictions using full-length temporal windows, which include substantial high-frequency noise and obscure long-term trends. Moreover, auxiliary variables containing rich domain-specific information are often underutilized, especially in few-shot settings. To address these challenges, we propose LoFT-LLM, a frequency-aware forecasting pipeline that integrates low-frequency learning with semantic calibration via a large language model (LLM). Firstly, a Patch Low-Frequency forecasting Module (PLFM) extracts stable low-frequency trends from localized spectral patches. Secondly, a residual learner then models high-frequency variations. Finally, a fine-tuned LLM refines the predictions by incorporating auxiliary context and domain knowledge through structured natural language prompts. Extensive experiments on financial and energy datasets demonstrate that LoFT-LLM significantly outperforms strong baselines under both full-data and few-shot regimes, delivering superior accuracy, robustness, and interpretability.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Control Variate Score Matching for Diffusion Models</title>
<link>https://arxiv.org/abs/2512.20003</link>
<guid>https://arxiv.org/abs/2512.20003</guid>
<content:encoded><![CDATA[
<div> Diffusion models, Score estimation, Control variates, Variance reduction, Sampling efficiency<br /><br />Summary:<br /><br />This paper addresses the challenge of accurately estimating the score function in diffusion models, which are essential for sampling from unnormalized probability distributions. Traditional estimators like the Denoising Score Identity (DSI) depend on data samples but suffer from high variance when noise levels are low. Conversely, the Target Score Identity (TSI), which leverages access to the target energy function, experiences high variance under high noise conditions. To resolve this variance trade-off, the authors unify these two estimators using the framework of control variates, leading to the introduction of the Control Variate Score Identity (CVSI). CVSI incorporates an optimal, time-dependent control coefficient designed to minimize variance consistently throughout the entire noise spectrum. The proposed CVSI estimator offers a robust, low-variance solution that improves sample efficiency both in scenarios where samplers are learned without data and during inference-time diffusion sampling. This approach provides a theoretically grounded and practically effective method to enhance score function estimation, thereby advancing the performance and reliability of diffusion-based generative modeling and sampling techniques. <div>
arXiv:2512.20003v1 Announce Type: new 
Abstract: Diffusion models offer a robust framework for sampling from unnormalized probability densities, which requires accurately estimating the score of the noise-perturbed target distribution. While the standard Denoising Score Identity (DSI) relies on data samples, access to the target energy function enables an alternative formulation via the Target Score Identity (TSI). However, these estimators face a fundamental variance trade-off: DSI exhibits high variance in low-noise regimes, whereas TSI suffers from high variance at high noise levels. In this work, we reconcile these approaches by unifying both estimators within the principled framework of control variates. We introduce the Control Variate Score Identity (CVSI), deriving an optimal, time-dependent control coefficient that theoretically guarantees variance minimization across the entire noise spectrum. We demonstrate that CVSI serves as a robust, low-variance plug-in estimator that significantly enhances sample efficiency in both data-free sampler learning and inference-time diffusion sampling.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Orthogonal Activation with Implicit Group-Aware Bias Learning for Class Imbalance</title>
<link>https://arxiv.org/abs/2512.20006</link>
<guid>https://arxiv.org/abs/2512.20006</guid>
<content:encoded><![CDATA[
<div> class imbalance, deep learning, activation function, orthogonality, group-aware bias<br /><br />Summary:<br /><br />Class imbalance poses a significant challenge in machine learning by often reducing classifier performance, especially in deep learning where feature extraction capabilities degrade with imbalanced data. The paper introduces a novel activation function named OGAB, designed specifically to mitigate class imbalance impacts within deep learning classifiers. OGAB integrates orthogonality and group-aware bias learning to improve feature distinguishability under imbalance, operating without the need for explicit label information. The authors emphasize that activation functions can introduce strong inductive biases that extend beyond traditional non-linearity to address complex data issues. Orthogonal transformations in OGAB help preserve minority class information by maintaining feature independence, preventing majority classes from dominating the embedding space. The group-aware bias mechanism automatically detects data clusters and modifies embeddings to enhance separability, all without supervised signals. This approach is distinct from existing methods that focus on data preprocessing or post-processing corrections, as OGAB addresses class imbalance during training at the embedding learning stage for seamless integration into the learning process. Experimental validation on both synthetic and real-world imbalanced datasets demonstrates consistent improvements over traditional and other learnable activation functions, proving the effectiveness of the proposed method. <div>
arXiv:2512.20006v1 Announce Type: new 
Abstract: Class imbalance is a common challenge in machine learning and data mining, often leading to suboptimal performance in classifiers. While deep learning excels in feature extraction, its performance still deteriorates under imbalanced data. In this work, we propose a novel activation function, named OGAB, designed to alleviate class imbalance in deep learning classifiers. OGAB incorporates orthogonality and group-aware bias learning to enhance feature distinguishability in imbalanced scenarios without explicitly requiring label information. Our key insight is that activation functions can be used to introduce strong inductive biases that can address complex data challenges beyond traditional non-linearity. Our work demonstrates that orthogonal transformations can preserve information about minority classes by maintaining feature independence, thereby preventing the dominance of majority classes in the embedding space. Further, the proposed group-aware bias mechanism automatically identifies data clusters and adjusts embeddings to enhance class separability without the need for explicit supervision. Unlike existing approaches that address class imbalance through preprocessing data modifications or post-processing corrections, our proposed approach tackles class imbalance during the training phase at the embedding learning level, enabling direct integration with the learning process. We demonstrate the effectiveness of our solution on both real-world and synthetic imbalanced datasets, showing consistent performance improvements over both traditional and learnable activation functions.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DecoKAN: Interpretable Decomposition for Forecasting Cryptocurrency Market Dynamics</title>
<link>https://arxiv.org/abs/2512.20028</link>
<guid>https://arxiv.org/abs/2512.20028</guid>
<content:encoded><![CDATA[
<div> Keywords: Cryptocurrency forecasting, Discrete Wavelet Transform, Kolmogorov-Arnold Network, interpretability, time series decomposition<br /><br />Summary:<br /><br />1. The article addresses the critical need for accurate and interpretable forecasting in multivariate cryptocurrency time series to better understand complex market dynamics.<br />2. It highlights limitations of existing deep learning models, especially Transformer-based and MLP-based "black-box" approaches, which struggle to separate long-term socio-economic trends from short-term speculative oscillations and lack interpretability.<br />3. To solve these issues, the authors propose DecoKAN, a novel framework combining multi-level Discrete Wavelet Transform (DWT) for hierarchical frequency-based signal decomposition and Kolmogorov-Arnold Network (KAN) mixers for transparent nonlinear modeling.<br />4. DecoKAN's DWT component decomposes cryptocurrency data into distinct frequency components, allowing frequency-specific analysis, while the KAN mixers provide interpretable spline-based mappings within each decomposed subseries.<br />5. Further interpretability is enhanced by a symbolic analysis pipeline that includes sparsification, pruning, and symbolization, yielding concise symbolic expressions of learned patterns.<br /><br />Extensive experiments on real-world datasets (BTC, ETH, XMR) demonstrate DecoKAN achieves the lowest average Mean Squared Error compared to state-of-the-art baselines, validating its ability to balance predictive accuracy and transparency, thus advancing trustworthy decision support in complex cryptocurrency markets. <div>
arXiv:2512.20028v1 Announce Type: new 
Abstract: Accurate and interpretable forecasting of multivariate time series is crucial for understanding the complex dynamics of cryptocurrency markets in digital asset systems. Advanced deep learning methodologies, particularly Transformer-based and MLP-based architectures, have achieved competitive predictive performance in cryptocurrency forecasting tasks. However, cryptocurrency data is inherently composed of long-term socio-economic trends and local high-frequency speculative oscillations. Existing deep learning-based 'black-box' models fail to effectively decouple these composite dynamics or provide the interpretability needed for trustworthy financial decision-making. To overcome these limitations, we propose DecoKAN, an interpretable forecasting framework that integrates multi-level Discrete Wavelet Transform (DWT) for decoupling and hierarchical signal decomposition with Kolmogorov-Arnold Network (KAN) mixers for transparent and interpretable nonlinear modeling. The DWT component decomposes complex cryptocurrency time series into distinct frequency components, enabling frequency-specific analysis, while KAN mixers provide intrinsically interpretable spline-based mappings within each decomposed subseries. Furthermore, interpretability is enhanced through a symbolic analysis pipeline involving sparsification, pruning, and symbolization, which produces concise analytical expressions offering symbolic representations of the learned patterns. Extensive experiments demonstrate that DecoKAN achieves the lowest average Mean Squared Error on all tested real-world cryptocurrency datasets (BTC, ETH, XMR), consistently outperforming a comprehensive suite of competitive state-of-the-art baselines. These results validate DecoKAN's potential to bridge the gap between predictive accuracy and model transparency, advancing trustworthy decision support within complex cryptocurrency markets.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Optimal Policy for Learning Controllable Dynamics by Exploration</title>
<link>https://arxiv.org/abs/2512.20053</link>
<guid>https://arxiv.org/abs/2512.20053</guid>
<content:encoded><![CDATA[
<div> Controllable Markov Chains, Optimal Policy, Exploration, Non-stationary Policy, Dynamic Programming<br /><br />Summary:<br /><br />This paper addresses the problem of learning controllable dynamics in unknown environments using controllable Markov chains, which are central in sequential decision making and reinforcement learning. It presents the general form of an optimal policy tailored for exploration over a limited time horizon, emphasizing maximizing information gain greedily while managing a constraint set of controls that changes over time. A simple parameterization of the control set is introduced alongside an algorithm for finding the optimal policy. The need for a non-stationary policy is justified by the presence of special states—transient, absorbing, and non-backtracking—that restrict control over the dynamics. These state types necessitate adapting the policy during exploration to achieve optimal results. The paper explores six detailed examples of controllable dynamics to illustrate the concepts. Optimality of the proposed policy is rigorously demonstrated through counting arguments, comparisons with suboptimal strategies, and leveraging a sequential improvement property derived from dynamic programming. Overall, the work provides a computationally efficient and implementable solution for learning and exploration in controllable Markov chain frameworks under practical constraints. <div>
arXiv:2512.20053v1 Announce Type: new 
Abstract: Controllable Markov chains describe the dynamics of sequential decision making tasks and are the central component in optimal control and reinforcement learning. In this work, we give the general form of an optimal policy for learning controllable dynamics in an unknown environment by exploring over a limited time horizon. This policy is simple to implement and efficient to compute, and allows an agent to ``learn by exploring" as it maximizes its information gain in a greedy fashion by selecting controls from a constraint set that changes over time during exploration. We give a simple parameterization for the set of controls, and present an algorithm for finding an optimal policy. The reason for this policy is due to the existence of certain types of states that restrict control of the dynamics; such as transient states, absorbing states, and non-backtracking states. We show why the occurrence of these states makes a non-stationary policy essential for achieving optimal exploration. Six interesting examples of controllable dynamics are treated in detail. Policy optimality is demonstrated using counting arguments, comparing with suboptimal policies, and by making use of a sequential improvement property from dynamic programming.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PairFlow: Closed-Form Source-Target Coupling for Few-Step Generation in Discrete Flow Models</title>
<link>https://arxiv.org/abs/2512.20063</link>
<guid>https://arxiv.org/abs/2512.20063</guid>
<content:encoded><![CDATA[
<div> Keywords: PairFlow, Discrete Flow Models, few-step sampling, pretrained teacher, model acceleration  

<br /><br />Summary:  
The paper introduces PairFlow, a lightweight preprocessing technique designed for training Discrete Flow Models (DFMs), which are generative models for discrete data known for strong performance but slow sampling times due to their iterative sampling process. PairFlow enables few-step sampling without the need for a pretrained teacher, addressing limitations of prior acceleration approaches that rely heavily on finetuning and incur significant additional training costs. Inspired by ReFlow, the method involves training DFMs using paired samples from source and target distributions generated through a novel closed-form inversion technique, enabling efficient creation of these paired samples at very low computational expense—only about 1.7% of the compute required for complete model training. Despite its minimal cost, PairFlow matches or exceeds the performance of conventional two-stage training methods that include finetuning. Additionally, models trained with PairFlow serve as stronger base models for subsequent distillation procedures, allowing even faster sampling after finetuning. Empirical results demonstrate the broad applicability and effectiveness of PairFlow across multiple data domains, including molecular structures, binary images, and RGB images, showing it is a versatile and efficient solution for accelerating discrete flow model sampling. <div>
arXiv:2512.20063v1 Announce Type: new 
Abstract: We introduce $\texttt{PairFlow}$, a lightweight preprocessing step for training Discrete Flow Models (DFMs) to achieve few-step sampling without requiring a pretrained teacher. DFMs have recently emerged as a new class of generative models for discrete data, offering strong performance. However, they suffer from slow sampling due to their iterative nature. Existing acceleration methods largely depend on finetuning, which introduces substantial additional training overhead. $\texttt{PairFlow}$ addresses this issue with a lightweight preprocessing step. Inspired by ReFlow and its extension to DFMs, we train DFMs from coupled samples of source and target distributions, without requiring any pretrained teacher. At the core of our approach is a closed-form inversion for DFMs, which allows efficient construction of paired source-target samples. Despite its extremely low cost, taking only up to 1.7% of the compute needed for full model training, $\texttt{PairFlow}$ matches or even surpasses the performance of two-stage training involving finetuning. Furthermore, models trained with our framework provide stronger base models for subsequent distillation, yielding further acceleration after finetuning. Experiments on molecular data as well as binary and RGB images demonstrate the broad applicability and effectiveness of our approach.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QE-Catalytic: A Graph-Language Multimodal Base Model for Relaxed-Energy Prediction in Catalytic Adsorption</title>
<link>https://arxiv.org/abs/2512.20084</link>
<guid>https://arxiv.org/abs/2512.20084</guid>
<content:encoded><![CDATA[
<div> Adsorption energy, catalytic reactivity, E(3)-equivariant graph neural networks, large language model, inverse design<br /><br />Summary:<br /><br />1. Adsorption energy is a crucial metric for evaluating catalytic reactivity, defined by the difference between the relaxed total energies of adsorbate-surface systems and their reference states. Accurate prediction of these relaxed energies is essential for reliable machine-learning-based catalyst screening.<br /><br />2. E(3)-equivariant graph neural networks (GNNs) effectively handle 3D atomic coordinates under periodic boundary conditions and have shown strong performance in adsorption energy prediction tasks.<br /><br />3. Language model approaches offer the advantage of interpretable textual descriptions and broader applicability beyond explicit graph structures but struggle with accuracy and differentiating systems with varying configurations, even when using graph-assisted pretraining methods like GAP-CATBERTa.<br /><br />4. To overcome these challenges, the authors propose QE-Catalytic, a multimodal framework that integrates a large language model (Qwen) with an E(3)-equivariant graph Transformer (Equiformer-V2). This enables simultaneous support for adsorption-configuration property prediction and inverse design on complex catalytic surfaces.<br /><br />5. QE-Catalytic combines 3D structural data and structured text, injecting 3D geometric information into the language model through graph-text alignment. It can operate effectively even when precise coordinates are unavailable and can autoregressively generate CIF files for structure design.<br /><br />6. Evaluated on the OC20 dataset, QE-Catalytic reduces the mean absolute error (MAE) of relaxed adsorption energy predictions from 0.713 eV to 0.486 eV and consistently outperforms baseline models like CatBERTa and GAP-CATBERTa across multiple evaluation protocols. <div>
arXiv:2512.20084v1 Announce Type: new 
Abstract: Adsorption energy is a key descriptor of catalytic reactivity. It is fundamentally defined as the difference between the relaxed total energy of the adsorbate-surface system and that of an appropriate reference state; therefore, the accuracy of relaxed-energy prediction directly determines the reliability of machine-learning-driven catalyst screening. E(3)-equivariant graph neural networks (GNNs) can natively operate on three-dimensional atomic coordinates under periodic boundary conditions and have demonstrated strong performance on such tasks. In contrast, language-model-based approaches, while enabling human-readable textual descriptions and reducing reliance on explicit graph -- thereby broadening applicability -- remain insufficient in both adsorption-configuration energy prediction accuracy and in distinguishing ``the same system with different configurations,'' even with graph-assisted pretraining in the style of GAP-CATBERTa.
  To this end, we propose QE-Catalytic, a multimodal framework that deeply couples a large language model (\textbf{Q}wen) with an E(3)-equivariant graph Transformer (\textbf{E}quiformer-V2), enabling unified support for adsorption-configuration property prediction and inverse design on complex catalytic surfaces. During prediction, QE-Catalytic jointly leverages three-dimensional structures and structured configuration text, and injects ``3D geometric information'' into the language channel via graph-text alignment, allowing it to function as a high-performance text-based predictor when precise coordinates are unavailable, while also autoregressively generating CIF files for target-energy-driven structure design and information completion. On OC20, QE-Catalytic reduces the MAE of relaxed adsorption energy from 0.713~eV to 0.486~eV, and consistently outperforms baseline models such as CatBERTa and GAP-CATBERTa across multiple evaluation protocols.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatio-Temporal Graphs Beyond Grids: Benchmark for Maritime Anomaly Detection</title>
<link>https://arxiv.org/abs/2512.20086</link>
<guid>https://arxiv.org/abs/2512.20086</guid>
<content:encoded><![CDATA[
<div> Spatio-temporal graphs, anomaly detection, maritime traffic, benchmark dataset, graph neural networks<br /><br />Summary:<br /><br />This work addresses the challenge of anomaly detection in spatio-temporal systems without fixed spatial anchors, such as maritime traffic, where traditional graph models struggle due to irregular and sparse trajectories. The authors introduce a novel benchmark dataset tailored for graph-based anomaly detection in the maritime domain by extending the existing Open Maritime Traffic Analysis Dataset (OMTAD). This new dataset enables systematic evaluation of anomalies at three granularities: node-level, edge-level, and graph-level, facilitating more comprehensive analysis. To enrich the dataset and ensure the anomalies are semantically meaningful, the study proposes the use of two specialized large language model-based agents termed Trajectory Synthesizer and Anomaly Injector. These agents help synthesize richer interaction contexts and generate diverse anomaly types that reflect realistic conditions. The benchmark aims to improve reproducibility and encourage methodological advancements in anomaly detection specific to non-grid spatio-temporal graphs, filling a gap in current research. Ultimately, this work supports the development of more effective graph neural network techniques suited for complex, spatially unanchored systems like maritime traffic. <div>
arXiv:2512.20086v1 Announce Type: new 
Abstract: Spatio-temporal graph neural networks (ST-GNNs) have achieved notable success in structured domains such as road traffic and public transportation, where spatial entities can be naturally represented as fixed nodes. In contrast, many real-world systems including maritime traffic lack such fixed anchors, making the construction of spatio-temporal graphs a fundamental challenge. Anomaly detection in these non-grid environments is particularly difficult due to the absence of canonical reference points, the sparsity and irregularity of trajectories, and the fact that anomalies may manifest at multiple granularities. In this work, we introduce a novel benchmark dataset for anomaly detection in the maritime domain, extending the Open Maritime Traffic Analysis Dataset (OMTAD) into a benchmark tailored for graph-based anomaly detection. Our dataset enables systematic evaluation across three different granularities: node-level, edge-level, and graph-level anomalies. We plan to employ two specialized LLM-based agents: \emph{Trajectory Synthesizer} and \emph{Anomaly Injector} to construct richer interaction contexts and generate semantically meaningful anomalies. We expect this benchmark to promote reproducibility and to foster methodological advances in anomaly detection for non-grid spatio-temporal systems.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Jensen-Shannon Divergence Message-Passing for Rich-Text Graph Representation Learning</title>
<link>https://arxiv.org/abs/2512.20094</link>
<guid>https://arxiv.org/abs/2512.20094</guid>
<content:encoded><![CDATA[
arXiv:2512.20094v1 Announce Type: new 
Abstract: In this paper, we investigate how the widely existing contextual and structural divergence may influence the representation learning in rich-text graphs. To this end, we propose Jensen-Shannon Divergence Message-Passing (JSDMP), a new learning paradigm for rich-text graph representation learning. Besides considering similarity regarding structure and text, JSDMP further captures their corresponding dissimilarity by Jensen-Shannon divergence. Similarity and dissimilarity are then jointly used to compute new message weights among text nodes, thus enabling representations to learn with contextual and structural information from truly correlated text nodes. With JSDMP, we propose two novel graph neural networks, namely Divergent message-passing graph convolutional network (DMPGCN) and Divergent message-passing Page-Rank graph neural networks (DMPPRG), for learning representations in rich-text graphs. DMPGCN and DMPPRG have been extensively texted on well-established rich-text datasets and compared with several state-of-the-art baselines. The experimental results show that DMPGCN and DMPPRG can outperform other baselines, demonstrating the effectiveness of the proposed Jensen-Shannon Divergence Message-Passing paradigm
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Information-directed sampling for bandits: a primer</title>
<link>https://arxiv.org/abs/2512.20096</link>
<guid>https://arxiv.org/abs/2512.20096</guid>
<content:encoded><![CDATA[
arXiv:2512.20096v1 Announce Type: new 
Abstract: The Multi-Armed Bandit problem provides a fundamental framework for analyzing the tension between exploration and exploitation in sequential learning. This paper explores Information Directed Sampling (IDS) policies, a class of heuristics that balance immediate regret against information gain. We focus on the tractable environment of two-state Bernoulli bandits as a minimal model to rigorously compare heuristic strategies against the optimal policy. We extend the IDS framework to the discounted infinite-horizon setting by introducing a modified information measure and a tuning parameter to modulate the decision-making behavior. We examine two specific problem classes: symmetric bandits and the scenario involving one fair coin. In the symmetric case we show that IDS achieves bounded cumulative regret, whereas in the one-fair-coin scenario the IDS policy yields a regret that scales logarithmically with the horizon, in agreement with classical asymptotic lower bounds. This work serves as a pedagogical synthesis, aiming to bridge concepts from reinforcement learning and information theory for an audience of statistical physicists.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sample-Efficient Policy Constraint Offline Deep Reinforcement Learning based on Sample Filtering</title>
<link>https://arxiv.org/abs/2512.20115</link>
<guid>https://arxiv.org/abs/2512.20115</guid>
<content:encoded><![CDATA[
arXiv:2512.20115v1 Announce Type: new 
Abstract: Offline reinforcement learning (RL) aims to learn a policy that maximizes the expected return using a given static dataset of transitions. However, offline RL faces the distribution shift problem. The policy constraint offline RL method is proposed to solve the distribution shift problem. During the policy constraint offline RL training, it is important to ensure the difference between the learned policy and behavior policy within a given threshold. Thus, the learned policy heavily relies on the quality of the behavior policy. However, a problem exists in existing policy constraint methods: if the dataset contains many low-reward transitions, the learned will be contained with a suboptimal reference policy, leading to slow learning speed, low sample efficiency, and inferior performances. This paper shows that the sampling method in policy constraint offline RL that uses all the transitions in the dataset can be improved. A simple but efficient sample filtering method is proposed to improve the sample efficiency and the final performance. First, we evaluate the score of the transitions by average reward and average discounted reward of episodes in the dataset and extract the transition samples of high scores. Second, the high-score transition samples are used to train the offline RL algorithms. We verify the proposed method in a series of offline RL algorithms and benchmark tasks. Experimental results show that the proposed method outperforms baselines.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Reason in LLMs by Expectation Maximization</title>
<link>https://arxiv.org/abs/2512.20169</link>
<guid>https://arxiv.org/abs/2512.20169</guid>
<content:encoded><![CDATA[
arXiv:2512.20169v1 Announce Type: new 
Abstract: Large language models (LLMs) solve reasoning problems by first generating a rationale and then answering. We formalize reasoning as a latent variable model and derive an expectation-maximization (EM) objective for learning to reason. This view connects EM and modern reward-based optimization, and shows that the main challenge lies in designing a sampling distribution that generates rationales that justify correct answers. We instantiate and compare several sampling schemes: rejection sampling with a budget, self-taught reasoner (STaR), and prompt posterior sampling (PPS), which only keeps the rationalization stage of STaR. Our experiments on the ARC, MMLU, and OpenBookQA datasets with the Llama and Qwen models show that the sampling scheme can significantly affect the accuracy of learned reasoning models. Despite its simplicity, we observe that PPS outperforms the other sampling schemes.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeuralCrop: Combining physics and machine learning for improved crop yield predictions</title>
<link>https://arxiv.org/abs/2512.20177</link>
<guid>https://arxiv.org/abs/2512.20177</guid>
<content:encoded><![CDATA[
arXiv:2512.20177v1 Announce Type: new 
Abstract: Global gridded crop models (GGCMs) simulate daily crop growth by explicitly representing key biophysical processes and project end-of-season yield time series. They are a primary tool to quantify the impacts of climate change on agricultural productivity and assess associated risks for food security. Despite decades of development, state-of-the-art GGCMs still have substantial uncertainties in simulating complex biophysical processes due to limited process understanding. Recently, machine learning approaches trained on observational data have shown great potential in crop yield predictions. However, these models have not demonstrated improved performance over classical GGCMs and are not suitable for simulating crop yields under changing climate conditions due to problems in generalizing outside their training distributions. Here we introduce NeuralCrop, a hybrid GGCM that combines the strengths of an advanced process-based GGCM, resolving important processes explicitly, with data-driven machine learning components. The model is first trained to emulate a competitive GGCM before it is fine-tuned on observational data. We show that NeuralCrop outperforms state-of-the-art GGCMs across site-level and large-scale cropping regions. Across moisture conditions, NeuralCrop reproduces the interannual yield anomalies in European wheat regions and the US Corn Belt more accurately during the period from 2000 to 2019 with particularly strong improvements under drought extremes. When generalizing to conditions unseen during training, NeuralCrop continues to make robust projections, while pure machine learning models exhibit substantial performance degradation. Our results show that our hybrid crop modelling approach offers overall improved crop modeling and more reliable yield projections under climate change and intensifying extreme weather conditions.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cost-TrustFL: Cost-Aware Hierarchical Federated Learning with Lightweight Reputation Evaluation across Multi-Cloud</title>
<link>https://arxiv.org/abs/2512.20218</link>
<guid>https://arxiv.org/abs/2512.20218</guid>
<content:encoded><![CDATA[
arXiv:2512.20218v1 Announce Type: new 
Abstract: Federated learning across multi-cloud environments faces critical challenges, including non-IID data distributions, malicious participant detection, and substantial cross-cloud communication costs (egress fees). Existing Byzantine-robust methods focus primarily on model accuracy while overlooking the economic implications of data transfer across cloud providers. This paper presents Cost-TrustFL, a hierarchical federated learning framework that jointly optimizes model performance and communication costs while providing robust defense against poisoning attacks. We propose a gradient-based approximate Shapley value computation method that reduces the complexity from exponential to linear, enabling lightweight reputation evaluation. Our cost-aware aggregation strategy prioritizes intra-cloud communication to minimize expensive cross-cloud data transfers. Experiments on CIFAR-10 and FEMNIST datasets demonstrate that Cost-TrustFL achieves 86.7% accuracy under 30% malicious clients while reducing communication costs by 32% compared to baseline methods. The framework maintains stable performance across varying non-IID degrees and attack intensities, making it practical for real-world multi-cloud deployments.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalisation in Multitask Fitted Q-Iteration and Offline Q-learning</title>
<link>https://arxiv.org/abs/2512.20220</link>
<guid>https://arxiv.org/abs/2512.20220</guid>
<content:encoded><![CDATA[
arXiv:2512.20220v1 Announce Type: new 
Abstract: We study offline multitask reinforcement learning in settings where multiple tasks share a low-rank representation of their action-value functions. In this regime, a learner is provided with fixed datasets collected from several related tasks, without access to further online interaction, and seeks to exploit shared structure to improve statistical efficiency and generalization. We analyze a multitask variant of fitted Q-iteration that jointly learns a shared representation and task-specific value functions via Bellman error minimization on offline data. Under standard realizability and coverage assumptions commonly used in offline reinforcement learning, we establish finite-sample generalization guarantees for the learned value functions. Our analysis explicitly characterizes how pooling data across tasks improves estimation accuracy, yielding a $1/\sqrt{nT}$ dependence on the total number of samples across tasks, while retaining the usual dependence on the horizon and concentrability coefficients arising from distribution shift. In addition, we consider a downstream offline setting in which a new task shares the same underlying representation as the upstream tasks. We study how reusing the representation learned during the multitask phase affects value estimation for this new task, and show that it can reduce the effective complexity of downstream learning relative to learning from scratch. Together, our results clarify the role of shared representations in multitask offline Q-learning and provide theoretical insight into when and how multitask structure can improve generalization in model-free, value-based reinforcement learning.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Multi-task Learning for Probabilistic Load Forecasting</title>
<link>https://arxiv.org/abs/2512.20232</link>
<guid>https://arxiv.org/abs/2512.20232</guid>
<content:encoded><![CDATA[
arXiv:2512.20232v1 Announce Type: new 
Abstract: Simultaneous load forecasting across multiple entities (e.g., regions, buildings) is crucial for the efficient, reliable, and cost-effective operation of power systems. Accurate load forecasting is a challenging problem due to the inherent uncertainties in load demand, dynamic changes in consumption patterns, and correlations among entities. Multi-task learning has emerged as a powerful machine learning approach that enables the simultaneous learning across multiple related problems. However, its application to load forecasting remains underexplored and is limited to offline learning-based methods, which cannot capture changes in consumption patterns. This paper presents an adaptive multi-task learning method for probabilistic load forecasting. The proposed method can dynamically adapt to changes in consumption patterns and correlations among entities. In addition, the techniques presented provide reliable probabilistic predictions for loads of multiples entities and assess load uncertainties. Specifically, the method is based on vectorvalued hidden Markov models and uses a recursive process to update the model parameters and provide predictions with the most recent parameters. The performance of the proposed method is evaluated using datasets that contain the load demand of multiple entities and exhibit diverse and dynamic consumption patterns. The experimental results show that the presented techniques outperform existing methods both in terms of forecasting performance and uncertainty assessment.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How I Met Your Bias: Investigating Bias Amplification in Diffusion Models</title>
<link>https://arxiv.org/abs/2512.20233</link>
<guid>https://arxiv.org/abs/2512.20233</guid>
<content:encoded><![CDATA[
arXiv:2512.20233v1 Announce Type: new 
Abstract: Diffusion-based generative models demonstrate state-of-the-art performance across various image synthesis tasks, yet their tendency to replicate and amplify dataset biases remains poorly understood. Although previous research has viewed bias amplification as an inherent characteristic of diffusion models, this work provides the first analysis of how sampling algorithms and their hyperparameters influence bias amplification. We empirically demonstrate that samplers for diffusion models -- commonly optimized for sample quality and speed -- have a significant and measurable effect on bias amplification. Through controlled studies with models trained on Biased MNIST, Multi-Color MNIST and BFFHQ, and with Stable Diffusion, we show that sampling hyperparameters can induce both bias reduction and amplification, even when the trained model is fixed. Source code is available at https://github.com/How-I-met-your-bias/how_i_met_your_bias.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unified Multimodal Brain Decoding via Cross-Subject Soft-ROI Fusion</title>
<link>https://arxiv.org/abs/2512.20249</link>
<guid>https://arxiv.org/abs/2512.20249</guid>
<content:encoded><![CDATA[
arXiv:2512.20249v1 Announce Type: new 
Abstract: Multimodal brain decoding aims to reconstruct semantic information that is consistent with visual stimuli from brain activity signals such as fMRI, and then generate readable natural language descriptions. However, multimodal brain decoding still faces key challenges in cross-subject generalization and interpretability. We propose a BrainROI model and achieve leading-level results in brain-captioning evaluation on the NSD dataset. Under the cross-subject setting, compared with recent state-of-the-art methods and representative baselines, metrics such as BLEU-4 and CIDEr show clear improvements. Firstly, to address the heterogeneity of functional brain topology across subjects, we design a new fMRI encoder. We use multi-atlas soft functional parcellations (soft-ROI) as a shared space. We extend the discrete ROI Concatenation strategy in MINDLLM to a voxel-wise gated fusion mechanism (Voxel-gate). We also ensure consistent ROI mapping through global label alignment, which enhances cross-subject transferability. Secondly, to overcome the limitations of manual and black-box prompting methods in stability and transparency, we introduce an interpretable prompt optimization process. In a small-sample closed loop, we use a locally deployed Qwen model to iteratively generate and select human-readable prompts. This process improves the stability of prompt design and preserves an auditable optimization trajectory. Finally, we impose parameterized decoding constraints during inference to further improve the stability and quality of the generated descriptions.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepONet-accelerated Bayesian inversion for moving boundary problems</title>
<link>https://arxiv.org/abs/2512.20268</link>
<guid>https://arxiv.org/abs/2512.20268</guid>
<content:encoded><![CDATA[
arXiv:2512.20268v1 Announce Type: new 
Abstract: This work demonstrates that neural operator learning provides a powerful and flexible framework for building fast, accurate emulators of moving boundary systems, enabling their integration into digital twin platforms. To this end, a Deep Operator Network (DeepONet) architecture is employed to construct an efficient surrogate model for moving boundary problems in single-phase Darcy flow through porous media. The surrogate enables rapid and accurate approximation of complex flow dynamics and is coupled with an Ensemble Kalman Inversion (EKI) algorithm to solve Bayesian inverse problems.
  The proposed inversion framework is demonstrated by estimating the permeability and porosity of fibre reinforcements for composite materials manufactured via the Resin Transfer Moulding (RTM) process. Using both synthetic and experimental in-process data, the DeepONet surrogate accelerates inversion by several orders of magnitude compared with full-model EKI. This computational efficiency enables real-time, accurate, high-resolution estimation of local variations in permeability, porosity, and other parameters, thereby supporting effective monitoring and control of RTM processes, as well as other applications involving moving boundary flows. Unlike prior approaches for RTM inversion that learn mesh-dependent mappings, the proposed neural operator generalises across spatial and temporal domains, enabling evaluation at arbitrary sensor configurations without retraining, and represents a significant step toward practical industrial deployment of digital twins.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HGAN-SDEs: Learning Neural Stochastic Differential Equations with Hermite-Guided Adversarial Training</title>
<link>https://arxiv.org/abs/2512.20272</link>
<guid>https://arxiv.org/abs/2512.20272</guid>
<content:encoded><![CDATA[
arXiv:2512.20272v1 Announce Type: new 
Abstract: Neural Stochastic Differential Equations (Neural SDEs) provide a principled framework for modeling continuous-time stochastic processes and have been widely adopted in fields ranging from physics to finance. Recent advances suggest that Generative Adversarial Networks (GANs) offer a promising solution to learning the complex path distributions induced by SDEs. However, a critical bottleneck lies in designing a discriminator that faithfully captures temporal dependencies while remaining computationally efficient. Prior works have explored Neural Controlled Differential Equations (CDEs) as discriminators due to their ability to model continuous-time dynamics, but such architectures suffer from high computational costs and exacerbate the instability of adversarial training. To address these limitations, we introduce HGAN-SDEs, a novel GAN-based framework that leverages Neural Hermite functions to construct a structured and efficient discriminator. Hermite functions provide an expressive yet lightweight basis for approximating path-level dynamics, enabling both reduced runtime complexity and improved training stability. We establish the universal approximation property of our framework for a broad class of SDE-driven distributions and theoretically characterize its convergence behavior. Extensive empirical evaluations on synthetic and real-world systems demonstrate that HGAN-SDEs achieve superior sample quality and learning efficiency compared to existing generative models for SDEs
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mixture-of-Experts with Gradient Conflict-Driven Subspace Topology Pruning for Emergent Modularity</title>
<link>https://arxiv.org/abs/2512.20291</link>
<guid>https://arxiv.org/abs/2512.20291</guid>
<content:encoded><![CDATA[
arXiv:2512.20291v1 Announce Type: new 
Abstract: Mixture-of-Experts (MoE) architectures achieve parameter efficiency through conditional computation, yet contemporary designs suffer from two fundamental limitations: structural parameter isolation that causes catastrophic forgetting, and instruction-overfitting that degrades performance in instruction-free scenarios. We propose CDSP-MoE (Conflict-Driven Subspace Pruning MoE), a framework that addresses these issues through a paradigm shift from isolated expert containers to dynamic expert instantiation within a shared physical subspace. Grounded in the Universal Weight Subspace Hypothesis, CDSP-MoE maintains a super-complete parameter backbone where logical experts are carved out via learnable topology masks. Unlike prior work that uses gradient conflict for token reassignment or optimization surgery, we leverage it as a structural supervisory signal: a Lagged Gradient Game penalizes interfering connections in the shared manifold, enabling the topology to spontaneously prune conflicting pathways and evolve interpretable modular structures. Experimental results demonstrate that CDSP-MoE achieves robust content-driven routing without human-defined task labels, maintaining semantic specialization even under strict blind inference protocols where explicit instructions are absent. Code is available at: https://github.com/konodiodaaaaa1/Conflict-Driven-Subspace-Pruning-Mixture-of-Experts
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TableGPT-R1: Advancing Tabular Reasoning Through Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.20312</link>
<guid>https://arxiv.org/abs/2512.20312</guid>
<content:encoded><![CDATA[
arXiv:2512.20312v1 Announce Type: new 
Abstract: Tabular data serves as the backbone of modern data analysis and scientific research. While Large Language Models (LLMs) fine-tuned via Supervised Fine-Tuning (SFT) have significantly improved natural language interaction with such structured data, they often fall short in handling the complex, multi-step reasoning and robust code execution required for real-world table tasks. Reinforcement Learning (RL) offers a promising avenue to enhance these capabilities, yet its application in the tabular domain faces three critical hurdles: the scarcity of high-quality agentic trajectories with closed-loop code execution and environment feedback on diverse table structures, the extreme heterogeneity of feedback signals ranging from rigid SQL execution to open-ended data interpretation, and the risk of catastrophic forgetting of general knowledge during vertical specialization. To overcome these challenges and unlock advanced reasoning on complex tables, we introduce \textbf{TableGPT-R1}, a specialized tabular model built on a systematic RL framework. Our approach integrates a comprehensive data engineering pipeline that synthesizes difficulty-stratified agentic trajectories for both supervised alignment and RL rollouts, a task-adaptive reward system that combines rule-based verification with a criteria-injected reward model and incorporates process-level step reward shaping with behavioral regularization, and a multi-stage training framework that progressively stabilizes reasoning before specializing in table-specific tasks. Extensive evaluations demonstrate that TableGPT-R1 achieves state-of-the-art performance on authoritative benchmarks, significantly outperforming baseline models while retaining robust general capabilities. Our model is available at https://huggingface.co/tablegpt/TableGPT-R1.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedDPC : Handling Data Heterogeneity and Partial Client Participation in Federated Learning</title>
<link>https://arxiv.org/abs/2512.20329</link>
<guid>https://arxiv.org/abs/2512.20329</guid>
<content:encoded><![CDATA[
arXiv:2512.20329v1 Announce Type: new 
Abstract: Data heterogeneity is a significant challenge in modern federated learning (FL) as it creates variance in local model updates, causing the aggregated global model to shift away from the true global optimum. Partial client participation in FL further exacerbates this issue by skewing the aggregation of local models towards the data distribution of participating clients. This creates additional variance in the global model updates, causing the global model to converge away from the optima of the global objective. These variances lead to instability in FL training, which degrades global model performance and slows down FL training. While existing literature primarily focuses on addressing data heterogeneity, the impact of partial client participation has received less attention. In this paper, we propose FedDPC, a novel FL method, designed to improve FL training and global model performance by mitigating both data heterogeneity and partial client participation. FedDPC addresses these issues by projecting each local update onto the previous global update, thereby controlling variance in both local and global updates. To further accelerate FL training, FedDPC employs adaptive scaling for each local update before aggregation. Extensive experiments on image classification tasks with multiple heterogeneously partitioned datasets validate the effectiveness of FedDPC. The results demonstrate that FedDPC outperforms state-of-the-art FL algorithms by achieving faster reduction in training loss and improved test accuracy across communication rounds.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inverse Autoregressive Flows for Zero Degree Calorimeter fast simulation</title>
<link>https://arxiv.org/abs/2512.20346</link>
<guid>https://arxiv.org/abs/2512.20346</guid>
<content:encoded><![CDATA[
arXiv:2512.20346v1 Announce Type: new 
Abstract: Physics-based machine learning blends traditional science with modern data-driven techniques. Rather than relying exclusively on empirical data or predefined equations, this methodology embeds domain knowledge directly into the learning process, resulting in models that are both more accurate and robust. We leverage this paradigm to accelerate simulations of the Zero Degree Calorimeter (ZDC) of the ALICE experiment at CERN. Our method introduces a novel loss function and an output variability-based scaling mechanism, which enhance the model's capability to accurately represent the spatial distribution and morphology of particle showers in detector outputs while mitigating the influence of rare artefacts on the training. Leveraging Normalizing Flows (NFs) in a teacher-student generative framework, we demonstrate that our approach not only outperforms classic data-driven model assimilation but also yields models that are 421 times faster than existing NF implementations in ZDC simulation literature.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-guided Neural Network-based Shaft Power Prediction for Vessels</title>
<link>https://arxiv.org/abs/2512.20348</link>
<guid>https://arxiv.org/abs/2512.20348</guid>
<content:encoded><![CDATA[
arXiv:2512.20348v1 Announce Type: new 
Abstract: Optimizing maritime operations, particularly fuel consumption for vessels, is crucial, considering its significant share in global trade. As fuel consumption is closely related to the shaft power of a vessel, predicting shaft power accurately is a crucial problem that requires careful consideration to minimize costs and emissions. Traditional approaches, which incorporate empirical formulas, often struggle to model dynamic conditions, such as sea conditions or fouling on vessels. In this paper, we present a hybrid, physics-guided neural network-based approach that utilizes empirical formulas within the network to combine the advantages of both neural networks and traditional techniques. We evaluate the presented method using data obtained from four similar-sized cargo vessels and compare the results with those of a baseline neural network and a traditional approach that employs empirical formulas. The experimental results demonstrate that the physics-guided neural network approach achieves lower mean absolute error, root mean square error, and mean absolute percentage error for all tested vessels compared to both the empirical formula-based method and the base neural network.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Field-Space Attention for Structure-Preserving Earth System Transformers</title>
<link>https://arxiv.org/abs/2512.20350</link>
<guid>https://arxiv.org/abs/2512.20350</guid>
<content:encoded><![CDATA[
arXiv:2512.20350v1 Announce Type: new 
Abstract: Accurate and physically consistent modeling of Earth system dynamics requires machine-learning architectures that operate directly on continuous geophysical fields and preserve their underlying geometric structure. Here we introduce Field-Space attention, a mechanism for Earth system Transformers that computes attention in the physical domain rather than in a learned latent space. By maintaining all intermediate representations as continuous fields on the sphere, the architecture enables interpretable internal states and facilitates the enforcement of scientific constraints. The model employs a fixed, non-learned multiscale decomposition and learns structure-preserving deformations of the input field, allowing coherent integration of coarse and fine-scale information while avoiding the optimization instabilities characteristic of standard single-scale Vision Transformers. Applied to global temperature super-resolution on a HEALPix grid, Field-Space Transformers converge more rapidly and stably than conventional Vision Transformers and U-Net baselines, while requiring substantially fewer parameters. The explicit preservation of field structure throughout the network allows physical and statistical priors to be embedded directly into the architecture, yielding improved fidelity and reliability in data-driven Earth system modeling. These results position Field-Space Attention as a compact, interpretable, and physically grounded building block for next-generation Earth system prediction and generative modeling frameworks.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Clust-PSI-PFL: A Population Stability Index Approach for Clustered Non-IID Personalized Federated Learning</title>
<link>https://arxiv.org/abs/2512.20363</link>
<guid>https://arxiv.org/abs/2512.20363</guid>
<content:encoded><![CDATA[
arXiv:2512.20363v1 Announce Type: new 
Abstract: Federated learning (FL) supports privacy-preserving, decentralized machine learning (ML) model training by keeping data on client devices. However, non-independent and identically distributed (non-IID) data across clients biases updates and degrades performance. To alleviate these issues, we propose Clust-PSI-PFL, a clustering-based personalized FL framework that uses the Population Stability Index (PSI) to quantify the level of non-IID data. We compute a weighted PSI metric, $WPSI^L$, which we show to be more informative than common non-IID metrics (Hellinger, Jensen-Shannon, and Earth Mover's distance). Using PSI features, we form distributionally homogeneous groups of clients via K-means++; the number of optimal clusters is chosen by a systematic silhouette-based procedure, typically yielding few clusters with modest overhead. Across six datasets (tabular, image, and text modalities), two partition protocols (Dirichlet with parameter $\alpha$ and Similarity with parameter S), and multiple client sizes, Clust-PSI-PFL delivers up to 18% higher global accuracy than state-of-the-art baselines and markedly improves client fairness by a relative improvement of 37% under severe non-IID data. These results establish PSI-guided clustering as a principled, lightweight mechanism for robust PFL under label skew.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoTransolver: Learning Physics on Irregumar Domains Using Multi-scale Geometry Aware Physics Attention Transformer</title>
<link>https://arxiv.org/abs/2512.20399</link>
<guid>https://arxiv.org/abs/2512.20399</guid>
<content:encoded><![CDATA[
arXiv:2512.20399v1 Announce Type: new 
Abstract: We present GeoTransolver, a Multiscale Geometry-Aware Physics Attention Transformer for CAE that replaces standard attention with GALE, coupling physics-aware self-attention on learned state slices with cross-attention to a shared geometry/global/boundary-condition context computed from multi-scale ball queries (inspired by DoMINO) and reused in every block. Implemented and released in NVIDIA PhysicsNeMo, GeoTransolver persistently projects geometry, global and boundary condition parameters into physical state spaces to anchor latent computations to domain structure and operating regimes. We benchmark GeoTransolver on DrivAerML, Luminary SHIFT-SUV, and Luminary SHIFT-Wing, comparing against Domino, Transolver (as released in PhysicsNeMo), and literature-reported AB-UPT, and evaluate drag/lift R2 and Relative L1 errors for field variables. GeoTransolver delivers better accuracy, improved robustness to geometry/regime shifts, and favorable data efficiency; we include ablations on DrivAerML and qualitative results such as contour plots and design trends for the best GeoTransolver models. By unifying multiscale geometry-aware context with physics-based attention in a scalable transformer, GeoTransolver advances operator learning for high-fidelity surrogate modeling across complex, irregular domains and non-linear physical regimes.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BRIDGE: Budget-aware Reasoning via Intermediate Distillation with Guided Examples</title>
<link>https://arxiv.org/abs/2512.20403</link>
<guid>https://arxiv.org/abs/2512.20403</guid>
<content:encoded><![CDATA[
arXiv:2512.20403v1 Announce Type: new 
Abstract: Distilling knowledge from large proprietary models (e.g., GPT-4) to tiny deployable models (less than 1B parameters) faces a critical capacity-budget trap: the 1000x capacity gap between teachers and students prevents effective direct transfer, while API costs prohibit extensive data collection. We introduce BRIDGE (Budget-Aware Reasoning via Intermediate Distillation), a two-phase framework that resolves these constraints through strategic intermediation and budget asymmetry. In Phase 1, a mid-sized Teacher Assistant (TA; e.g., about 7B) learns from the black-box teacher on a strictly limited subset of data (e.g., 3-5%), selected via a zero-API-cost pipeline that balances entropic difficulty and semantic diversity using only local TA inference. In Phase 2, we exploit this asymmetry-teacher queries are expensive, whereas TA inference is free to amplify supervision: the refined TA generates synthetic rationales for the full dataset to train the tiny student. Crucially, we apply an instruction-tuning curriculum to establish behavioral alignment in the tiny student before transferring reasoning. Our theoretical analysis shows that BRIDGE yields tighter generalization bounds than direct distillation when data is abundant. Experiments across medical, legal, and financial benchmarks demonstrate consistent improvements: BRIDGE delivers student performance gains of 28-41%, closing the capability gap with proprietary teachers by 12-16% while using 10x fewer teacher queries. Notably, BRIDGE defies the conventional cost-performance frontier, surpassing direct distillation baselines that use 100% of the budget while consuming only 5% of the resources.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simplifying Multi-Task Architectures Through Task-Specific Normalization</title>
<link>https://arxiv.org/abs/2512.20420</link>
<guid>https://arxiv.org/abs/2512.20420</guid>
<content:encoded><![CDATA[
arXiv:2512.20420v1 Announce Type: new 
Abstract: Multi-task learning (MTL) aims to leverage shared knowledge across tasks to improve generalization and parameter efficiency, yet balancing resources and mitigating interference remain open challenges. Architectural solutions often introduce elaborate task-specific modules or routing schemes, increasing complexity and overhead. In this work, we show that normalization layers alone are sufficient to address many of these challenges. Simply replacing shared normalization with task-specific variants already yields competitive performance, questioning the need for complex designs. Building on this insight, we propose Task-Specific Sigmoid Batch Normalization (TS$\sigma$BN), a lightweight mechanism that enables tasks to softly allocate network capacity while fully sharing feature extractors. TS$\sigma$BN improves stability across CNNs and Transformers, matching or exceeding performance on NYUv2, Cityscapes, CelebA, and PascalContext, while remaining highly parameter-efficient. Moreover, its learned gates provide a natural framework for analyzing MTL dynamics, offering interpretable insights into capacity allocation, filter specialization, and task relationships. Our findings suggest that complex MTL architectures may be unnecessary and that task-specific normalization offers a simple, interpretable, and efficient alternative.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Learning to Predict Digital Frustration from Clickstream Data</title>
<link>https://arxiv.org/abs/2512.20438</link>
<guid>https://arxiv.org/abs/2512.20438</guid>
<content:encoded><![CDATA[
arXiv:2512.20438v1 Announce Type: new 
Abstract: Many businesses depend on their mobile apps and websites, so user frustration while trying to complete a task on these channels can cause lost sales and complaints. In this research, I use clickstream data from a real e-commerce site to predict whether a session is frustrated or not. Frustration is defined using certain rules based on rage bursts, back and forth navigation (U turns), cart churn, search struggle, and long wandering sessions, and applies these rules to 5.4 million raw clickstream events (304,881 sessions). From each session, I build tabular features and train standard classifier models. I also use the full event sequence to train a discriminative LSTM classifier. XGBoost reaches about 90% accuracy, ROC AUC of 0.9579, while the LSTM performs best with about 91% accuracy and a ROC AUC of 0.9705. Finally, the research shows that with only the first 20 to 30 interactions, the LSTM already predicts frustration reliably.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Recurrent Off-Policy Deep Reinforcement Learning Doesn't Have to be Slow</title>
<link>https://arxiv.org/abs/2512.20513</link>
<guid>https://arxiv.org/abs/2512.20513</guid>
<content:encoded><![CDATA[
arXiv:2512.20513v1 Announce Type: new 
Abstract: Recurrent off-policy deep reinforcement learning models achieve state-of-the-art performance but are often sidelined due to their high computational demands. In response, we introduce RISE (Recurrent Integration via Simplified Encodings), a novel approach that can leverage recurrent networks in any image-based off-policy RL setting without significant computational overheads via using both learnable and non-learnable encoder layers. When integrating RISE into leading non-recurrent off-policy RL algorithms, we observe a 35.6% human-normalized interquartile mean (IQM) performance improvement across the Atari benchmark. We analyze various implementation strategies to highlight the versatility and potential of our proposed framework.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable time-series forecasting with sampling-free SHAP for Transformers</title>
<link>https://arxiv.org/abs/2512.20514</link>
<guid>https://arxiv.org/abs/2512.20514</guid>
<content:encoded><![CDATA[
arXiv:2512.20514v1 Announce Type: new 
Abstract: Time-series forecasts are essential for planning and decision-making in many domains. Explainability is key to building user trust and meeting transparency requirements. Shapley Additive Explanations (SHAP) is a popular explainable AI framework, but it lacks efficient implementations for time series and often assumes feature independence when sampling counterfactuals. We introduce SHAPformer, an accurate, fast and sampling-free explainable time-series forecasting model based on the Transformer architecture. It leverages attention manipulation to make predictions based on feature subsets. SHAPformer generates explanations in under one second, several orders of magnitude faster than the SHAP Permutation Explainer. On synthetic data with ground truth explanations, SHAPformer provides explanations that are true to the data. Applied to real-world electrical load data, it achieves competitive predictive performance and delivers meaningful local and global insights, such as identifying the past load as the key predictor and revealing a distinct model behavior during the Christmas period.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs</title>
<link>https://arxiv.org/abs/2512.20573</link>
<guid>https://arxiv.org/abs/2512.20573</guid>
<content:encoded><![CDATA[
arXiv:2512.20573v1 Announce Type: new 
Abstract: Diffusion Large Language Models (dLLMs) offer fast, parallel token generation, but their standalone use is plagued by an inherent efficiency-quality tradeoff. We show that, if carefully applied, the attributes of dLLMs can actually be a strength for drafters in speculative decoding with autoregressive (AR) verifiers. Our core insight is that dLLM's speed from parallel decoding drastically lowers the risk of costly rejections, providing a practical mechanism to effectively realize the (elusive) lengthy drafts that lead to large speedups with speculative decoding. We present FailFast, a dLLM-based speculative decoding framework that realizes this approach by dynamically adapting its speculation length. It "fails fast" by spending minimal compute in hard-to-speculate regions to shrink speculation latency and "wins big" by aggressively extending draft lengths in easier regions to reduce verification latency (in many cases, speculating and accepting 70 tokens at a time!). Without any fine-tuning, FailFast delivers lossless acceleration of AR LLMs and achieves up to 4.9$\times$ speedup over vanilla decoding, 1.7$\times$ over the best naive dLLM drafter, and 1.4$\times$ over EAGLE-3 across diverse models and workloads. We open-source FailFast at https://github.com/ruipeterpan/failfast.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Performative Policy Gradient: Optimality in Performative Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.20576</link>
<guid>https://arxiv.org/abs/2512.20576</guid>
<content:encoded><![CDATA[
arXiv:2512.20576v1 Announce Type: new 
Abstract: Post-deployment machine learning algorithms often influence the environments they act in, and thus shift the underlying dynamics that the standard reinforcement learning (RL) methods ignore. While designing optimal algorithms in this performative setting has recently been studied in supervised learning, the RL counterpart remains under-explored. In this paper, we prove the performative counterparts of the performance difference lemma and the policy gradient theorem in RL, and further introduce the Performative Policy Gradient algorithm (PePG). PePG is the first policy gradient algorithm designed to account for performativity in RL. Under softmax parametrisation, and also with and without entropy regularisation, we prove that PePG converges to performatively optimal policies, i.e. policies that remain optimal under the distribution shifts induced by themselves. Thus, PePG significantly extends the prior works in Performative RL that achieves performative stability but not optimality. Furthermore, our empirical analysis on standard performative RL environments validate that PePG outperforms standard policy gradient algorithms and the existing performative RL algorithms aiming for stability.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving ML Training Data with Gold-Standard Quality Metrics</title>
<link>https://arxiv.org/abs/2512.20577</link>
<guid>https://arxiv.org/abs/2512.20577</guid>
<content:encoded><![CDATA[
arXiv:2512.20577v1 Announce Type: new 
Abstract: Hand-tagged training data is essential to many machine learning tasks. However, training data quality control has received little attention in the literature, despite data quality varying considerably with the tagging exercise. We propose methods to evaluate and enhance the quality of hand-tagged training data using statistical approaches to measure tagging consistency and agreement. We show that agreement metrics give more reliable results if recorded over multiple iterations of tagging, where declining variance in such recordings is an indicator of increasing data quality. We also show one way a tagging project can collect high-quality training data without requiring multiple tags for every work item, and that a tagger burn-in period may not be sufficient for minimizing tagger errors.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Relu and softplus neural nets as zero-sum turn-based games</title>
<link>https://arxiv.org/abs/2512.20582</link>
<guid>https://arxiv.org/abs/2512.20582</guid>
<content:encoded><![CDATA[
arXiv:2512.20582v1 Announce Type: new 
Abstract: We show that the output of a ReLU neural network can be interpreted as the value of a zero-sum, turn-based, stopping game, which we call the ReLU net game. The game runs in the direction opposite to that of the network, and the input of the network serves as the terminal reward of the game. In fact, evaluating the network is the same as running the Shapley-Bellman backward recursion for the value of the game. Using the expression of the value of the game as an expected total payoff with respect to the path measure induced by the transition probabilities and a pair of optimal policies, we derive a discrete Feynman-Kac-type path-integral formula for the network output. This game-theoretic representation can be used to derive bounds on the output from bounds on the input, leveraging the monotonicity of Shapley operators, and to verify robustness properties using policies as certificates. Moreover, training the neural network becomes an inverse game problem: given pairs of terminal rewards and corresponding values, one seeks transition probabilities and rewards of a game that reproduces them. Finally, we show that a similar approach applies to neural networks with Softplus activation functions, where the ReLU net game is replaced by its entropic regularization.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning</title>
<link>https://arxiv.org/abs/2512.20605</link>
<guid>https://arxiv.org/abs/2512.20605</guid>
<content:encoded><![CDATA[
arXiv:2512.20605v1 Announce Type: new 
Abstract: Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term "internal RL", enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Saddle-to-Saddle Dynamics Explains A Simplicity Bias Across Neural Network Architectures</title>
<link>https://arxiv.org/abs/2512.20607</link>
<guid>https://arxiv.org/abs/2512.20607</guid>
<content:encoded><![CDATA[
arXiv:2512.20607v1 Announce Type: new 
Abstract: Neural networks trained with gradient descent often learn solutions of increasing complexity over time, a phenomenon known as simplicity bias. Despite being widely observed across architectures, existing theoretical treatments lack a unifying framework. We present a theoretical framework that explains a simplicity bias arising from saddle-to-saddle learning dynamics for a general class of neural networks, incorporating fully-connected, convolutional, and attention-based architectures. Here, simple means expressible with few hidden units, i.e., hidden neurons, convolutional kernels, or attention heads. Specifically, we show that linear networks learn solutions of increasing rank, ReLU networks learn solutions with an increasing number of kinks, convolutional networks learn solutions with an increasing number of convolutional kernels, and self-attention models learn solutions with an increasing number of attention heads. By analyzing fixed points, invariant manifolds, and dynamics of gradient descent learning, we show that saddle-to-saddle dynamics operates by iteratively evolving near an invariant manifold, approaching a saddle, and switching to another invariant manifold. Our analysis also illuminates the effects of data distribution and weight initialization on the duration and number of plateaus in learning, dissociating previously confounding factors. Overall, our theory offers a framework for understanding when and why gradient descent progressively learns increasingly complex solutions.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QoS-Aware Dynamic CU Selection in O-RAN with Graph-Based Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.19696</link>
<guid>https://arxiv.org/abs/2512.19696</guid>
<content:encoded><![CDATA[
arXiv:2512.19696v1 Announce Type: cross 
Abstract: Open Radio Access Network (O RAN) disaggregates conventional RAN into interoperable components, enabling flexible resource allocation, energy savings, and agile architectural design. In legacy deployments, the binding between logical functions and physical locations is static, which leads to inefficiencies under time varying traffic and resource conditions. We address this limitation by relaxing the fixed mapping and performing dynamic service function chain (SFC) provisioning with on the fly O CU selection. We formulate the problem as a Markov decision process and solve it using GRLDyP, i.e., a graph neural network (GNN) assisted deep reinforcement learning (DRL). The proposed agent jointly selects routes and the O-CU location (from candidate sites) for each incoming service flow to minimize network energy consumption while satisfying quality of service (QoS) constraints. The GNN encodes the instantaneous network topology and resource utilization (e.g., CPU and bandwidth), and the DRL policy learns to balance grade of service, latency, and energy. We perform the evaluation of GRLDyP on a data set with 24-hour traffic traces from the city of Montreal, showing that dynamic O CU selection and routing significantly reduce energy consumption compared to a static mapping baseline, without violating QoS. The results highlight DRL based SFC provisioning as a practical control primitive for energy-aware, resource-adaptive O-RAN deployments.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ASK: Adaptive Self-improving Knowledge Framework for Audio Text Retrieval</title>
<link>https://arxiv.org/abs/2512.19703</link>
<guid>https://arxiv.org/abs/2512.19703</guid>
<content:encoded><![CDATA[
arXiv:2512.19703v1 Announce Type: cross 
Abstract: The dominant paradigm for Audio-Text Retrieval (ATR) relies on mini-batch-based contrastive learning. This process, however, is inherently limited by what we formalize as the Gradient Locality Bottleneck (GLB), which structurally prevents models from leveraging out-of-batch knowledge and thus impairs fine-grained and long-tail learning. While external knowledge-enhanced methods can alleviate the GLB, we identify a critical, unaddressed side effect: the Representation-Drift Mismatch (RDM), where a static knowledge base becomes progressively misaligned with the evolving model, turning guidance into noise. To address this dual challenge, we propose the Adaptive Self-improving Knowledge (ASK) framework, a model-agnostic, plug-and-play solution. ASK breaks the GLB via multi-grained knowledge injection, systematically mitigates RDM through dynamic knowledge refinement, and introduces a novel adaptive reliability weighting scheme to ensure consistent knowledge contributes to optimization. Experimental results on two benchmark datasets with superior, state-of-the-art performance justify the efficacy of our proposed ASK framework.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bidirectional human-AI collaboration in brain tumour assessments improves both expert human and AI agent performance</title>
<link>https://arxiv.org/abs/2512.19707</link>
<guid>https://arxiv.org/abs/2512.19707</guid>
<content:encoded><![CDATA[
arXiv:2512.19707v1 Announce Type: cross 
Abstract: The benefits of artificial intelligence (AI) human partnerships-evaluating how AI agents enhance expert human performance-are increasingly studied. Though rarely evaluated in healthcare, an inverse approach is possible: AI benefiting from the support of an expert human agent. Here, we investigate both human-AI clinical partnership paradigms in the magnetic resonance imaging-guided characterisation of patients with brain tumours. We reveal that human-AI partnerships improve accuracy and metacognitive ability not only for radiologists supported by AI, but also for AI agents supported by radiologists. Moreover, the greatest patient benefit was evident with an AI agent supported by a human one. Synergistic improvements in agent accuracy, metacognitive performance, and inter-rater agreement suggest that AI can create more capable, confident, and consistent clinical agents, whether human or model-based. Our work suggests that the maximal value of AI in healthcare could emerge not from replacing human intelligence, but from AI agents that routinely leverage and amplify it.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PHANTOM: PHysical ANamorphic Threats Obstructing Connected Vehicle Mobility</title>
<link>https://arxiv.org/abs/2512.19711</link>
<guid>https://arxiv.org/abs/2512.19711</guid>
<content:encoded><![CDATA[
arXiv:2512.19711v1 Announce Type: cross 
Abstract: Connected autonomous vehicles (CAVs) rely on vision-based deep neural networks (DNNs) and low-latency (Vehicle-to-Everything) V2X communication to navigate safely and efficiently. Despite their advances, these systems remain vulnerable to physical adversarial attacks. In this paper, we introduce PHANTOM (PHysical ANamorphic Threats Obstructing connected vehicle Mobility), a novel framework for crafting and deploying perspective-dependent adversarial examples using \textit{anamorphic art}. PHANTOM exploits geometric distortions that appear natural to humans but are misclassified with high confidence by state-of-the-art object detectors. Unlike conventional attacks, PHANTOM operates in black-box settings without model access and demonstrates strong transferability across four diverse detector architectures (YOLOv5, SSD, Faster R-CNN, and RetinaNet). Comprehensive evaluation in CARLA across varying speeds, weather conditions, and lighting scenarios shows that PHANTOM achieves over 90\% attack success rate under optimal conditions and maintains 60-80\% effectiveness even in degraded environments. The attack activates within 6-10 meters of the target, providing insufficient time for safe maneuvering. Beyond individual vehicle deception, PHANTOM triggers network-wide disruption in CAV systems: SUMO-OMNeT++ co-simulation demonstrates that false emergency messages propagate through V2X links, increasing Peak Age of Information by 68-89\% and degrading safety-critical communication. These findings expose critical vulnerabilities in both perception and communication layers of CAV ecosystems.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chemically-Informed Machine Learning Approach for Prediction of Reactivity Ratios in Radical Copolymerization</title>
<link>https://arxiv.org/abs/2512.19715</link>
<guid>https://arxiv.org/abs/2512.19715</guid>
<content:encoded><![CDATA[
arXiv:2512.19715v1 Announce Type: cross 
Abstract: Predicting monomer reactivity ratios is crucial for controlling monomer sequence distribution in copolymers and their properties. Traditional experimental methods of determining reactivity ratios are time-consuming and resource-intensive, while existing computational methods often struggle with accuracy or scalability. Here, we present a method that combines unsupervised learning with artificial neural networks to predict reactivity ratios in radical copolymerization. By applying spectral clustering to physicochemical features of monomers, we identified three distinct monomer groups with characteristic reactivity patterns. This computationally efficient clustering approach revealed specific monomer group interactions leading to different sequence arrangements, including alternating, random, block, and gradient copolymers, providing chemical insights for initial exploration. Building upon these insights, we trained artificial neural networks to achieve quantitative reactivity ratio predictions. We explored two integration strategies including direct feature concatenation, and cluster-specific training, which demonstrated performance enhancements for targeted chemical domains compared to general training with equivalent sample sizes. However, models utilizing complete datasets outperformed specialized models trained on focused subsets, revealing a fundamental trade-off between chemical specificity and data availability. This work demonstrates that unsupervised learning offers rapid chemical insight for exploratory analysis, while supervised learning provides the accuracy necessary for final design predictions, with optimal strategies depending on data availability and application requirements.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NMIRacle: Multi-modal Generative Molecular Elucidation from IR and NMR Spectra</title>
<link>https://arxiv.org/abs/2512.19733</link>
<guid>https://arxiv.org/abs/2512.19733</guid>
<content:encoded><![CDATA[
arXiv:2512.19733v1 Announce Type: cross 
Abstract: Molecular structure elucidation from spectroscopic data is a long-standing challenge in Chemistry, traditionally requiring expert interpretation. We introduce NMIRacle, a two-stage generative framework that builds upon recent paradigms in AI-driven spectroscopy with minimal assumptions. In the first stage, NMIRacle learns to reconstruct molecular structures from count-aware fragment encodings, which capture both fragment identities and their occurrences. In the second stage, a spectral encoder maps input spectroscopic measurements (IR, 1H-NMR, 13C-NMR) into a latent embedding that conditions the pre-trained generator. This formulation bridges fragment-level chemical modeling with spectral evidence, yielding accurate molecular predictions. Empirical results show that NMIRacle outperforms existing baselines on molecular elucidation, while maintaining robust performance across increasing levels of molecular complexity.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Causal Directionality Inference in Quantum Inference under MNAR Observation and High-Dimensional Noise</title>
<link>https://arxiv.org/abs/2512.19746</link>
<guid>https://arxiv.org/abs/2512.19746</guid>
<content:encoded><![CDATA[
arXiv:2512.19746v1 Announce Type: cross 
Abstract: In quantum mechanics, observation actively shapes the system, paralleling the statistical notion of Missing Not At Random (MNAR). This study introduces a unified framework for \textbf{robust causal directionality inference} in quantum engineering, determining whether relations are system$\to$observation, observation$\to$system, or bidirectional.
  The method integrates CVAE-based latent constraints, MNAR-aware selection models, GEE-stabilized regression, penalized empirical likelihood, and Bayesian optimization. It jointly addresses quantum and classical noise while uncovering causal directionality, with theoretical guarantees for double robustness, perturbation stability, and oracle inequalities.
  Simulation and real-data analyses (TCGA gene expression, proteomics) show that the proposed MNAR-stabilized CVAE+GEE+AIPW+PEL framework achieves lower bias and variance, near-nominal coverage, and superior quantum-specific diagnostics. This establishes robust causal directionality inference as a key methodological advance for reliable quantum engineering.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fundamentals of quantum Boltzmann machine learning with visible and hidden units</title>
<link>https://arxiv.org/abs/2512.19819</link>
<guid>https://arxiv.org/abs/2512.19819</guid>
<content:encoded><![CDATA[
arXiv:2512.19819v1 Announce Type: cross 
Abstract: One of the primary applications of classical Boltzmann machines is generative modeling, wherein the goal is to tune the parameters of a model distribution so that it closely approximates a target distribution. Training relies on estimating the gradient of the relative entropy between the target and model distributions, a task that is well understood when the classical Boltzmann machine has both visible and hidden units. For some years now, it has been an obstacle to generalize this finding to quantum state learning with quantum Boltzmann machines that have both visible and hidden units. In this paper, I derive an analytical expression for the gradient of the quantum relative entropy between a target quantum state and the reduced state of the visible units of a quantum Boltzmann machine. Crucially, this expression is amenable to estimation on a quantum computer, as it involves modular-flow-generated unitary rotations reminiscent of those appearing in my prior work on rotated Petz recovery maps. This leads to a quantum algorithm for gradient estimation in this setting. I then specialize the setting to quantum visible units and classical hidden units, and vice versa, and provide analytical expressions for the gradients, along with quantum algorithms for estimating them. Finally, I replace the quantum relative entropy objective function with the Petz-Tsallis relative entropy; here I develop an analytical expression for the gradient and sketch a quantum algorithm for estimating it, as an application of a novel formula for the derivative of the matrix power function, which also involves modular-flow-generated unitary rotations. Ultimately, this paper demarcates progress in training quantum Boltzmann machines with visible and hidden units for generative modeling and quantum state learning.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UCCL-EP: Portable Expert-Parallel Communication</title>
<link>https://arxiv.org/abs/2512.19849</link>
<guid>https://arxiv.org/abs/2512.19849</guid>
<content:encoded><![CDATA[
arXiv:2512.19849v1 Announce Type: cross 
Abstract: Mixture-of-Experts (MoE) workloads rely on expert parallelism (EP) to achieve high GPU efficiency. State-of-the-art EP communication systems such as DeepEP demonstrate strong performance but exhibit poor portability across heterogeneous GPU and NIC platforms. The poor portability is rooted in architecture: GPU-initiated token-level RDMA communication requires tight vertical integration between GPUs and NICs, e.g., GPU writes to NIC driver/MMIO interfaces.
  We present UCCL-EP, a portable EP communication system that delivers DeepEP-level performance across heterogeneous GPU and NIC hardware. UCCL-EP replaces GPU-initiated RDMA with a high-throughput GPU-CPU control channel: compact token-routing commands are transferred to multithreaded CPU proxies, which then issue GPUDirect RDMA operations on behalf of GPUs. UCCL-EP further emulates various ordering semantics required by specialized EP communication modes using RDMA immediate data, enabling correctness on NICs that lack such ordering, e.g., AWS EFA. We implement UCCL-EP on NVIDIA and AMD GPUs with EFA and Broadcom NICs. On EFA, it outperforms the best existing EP solution by up to $2.1\times$ for dispatch and combine throughput. On NVIDIA-only platform, UCCL-EP achieves comparable performance to the original DeepEP. UCCL-EP also improves token throughput on SGLang by up to 40% on the NVIDIA+EFA platform, and improves DeepSeek-V3 training throughput over the AMD Primus/Megatron-LM framework by up to 45% on a 16-node AMD+Broadcom platform.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Learning of Lattice Gauge Theories with Fermions</title>
<link>https://arxiv.org/abs/2512.19891</link>
<guid>https://arxiv.org/abs/2512.19891</guid>
<content:encoded><![CDATA[
arXiv:2512.19891v1 Announce Type: cross 
Abstract: We introduce a learning method for recovering action parameters in lattice field theories. Our method is based on the minimization of a convex loss function constructed using the Schwinger-Dyson relations. We show that score matching, a popular learning method, is a special case of our construction of an infinite family of valid loss functions. Importantly, our general Schwinger-Dyson-based construction applies to gauge theories and models with Grassmann-valued fields used to represent dynamical fermions. In particular, we extend our method to realistic lattice field theories including quantum chromodynamics.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detecting cyberbullying in Spanish texts through deep learning techniques</title>
<link>https://arxiv.org/abs/2512.19899</link>
<guid>https://arxiv.org/abs/2512.19899</guid>
<content:encoded><![CDATA[
arXiv:2512.19899v1 Announce Type: cross 
Abstract: Recent recollected data suggests that it is possible to automatically detect events that may negatively affect the most vulnerable parts of our society, by using any communication technology like social networks or messaging applications. This research consolidates and prepares a corpus with Spanish bullying expressions taken from Twitter in order to use them as an input to train a convolutional neuronal network through deep learning techniques. As a result of this training, a predictive model was created, which can identify Spanish cyberbullying expressions such as insults, racism, homophobic attacks, and so on.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quasiprobabilistic Density Ratio Estimation with a Reverse Engineered Classification Loss Function</title>
<link>https://arxiv.org/abs/2512.19913</link>
<guid>https://arxiv.org/abs/2512.19913</guid>
<content:encoded><![CDATA[
arXiv:2512.19913v1 Announce Type: cross 
Abstract: We consider a generalization of the classifier-based density-ratio estimation task to a quasiprobabilistic setting where probability densities can be negative. The problem with most loss functions used for this task is that they implicitly define a relationship between the optimal classifier and the target quasiprobabilistic density ratio which is discontinuous or not surjective. We address these problems by introducing a convex loss function that is well-suited for both probabilistic and quasiprobabilistic density ratio estimation. To quantify performance, an extended version of the Sliced-Wasserstein distance is introduced which is compatible with quasiprobability distributions. We demonstrate our approach on a real-world example from particle physics, of di-Higgs production in association with jets via gluon-gluon fusion, and achieve state-of-the-art results.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vehicle-centric Perception via Multimodal Structured Pre-training</title>
<link>https://arxiv.org/abs/2512.19934</link>
<guid>https://arxiv.org/abs/2512.19934</guid>
<content:encoded><![CDATA[
arXiv:2512.19934v1 Announce Type: cross 
Abstract: Vehicle-centric perception plays a crucial role in many intelligent systems, including large-scale surveillance systems, intelligent transportation, and autonomous driving. Existing approaches lack effective learning of vehicle-related knowledge during pre-training, resulting in poor capability for modeling general vehicle perception representations. To handle this problem, we propose VehicleMAE-V2, a novel vehicle-centric pre-trained large model. By exploring and exploiting vehicle-related multimodal structured priors to guide the masked token reconstruction process, our approach can significantly enhance the model's capability to learn generalizable representations for vehicle-centric perception. Specifically, we design the Symmetry-guided Mask Module (SMM), Contour-guided Representation Module (CRM) and Semantics-guided Representation Module (SRM) to incorporate three kinds of structured priors into token reconstruction including symmetry, contour and semantics of vehicles respectively. SMM utilizes the vehicle symmetry constraints to avoid retaining symmetric patches and can thus select high-quality masked image patches and reduce information redundancy. CRM minimizes the probability distribution divergence between contour features and reconstructed features and can thus preserve holistic vehicle structure information during pixel-level reconstruction. SRM aligns image-text features through contrastive learning and cross-modal distillation to address the feature confusion caused by insufficient semantic understanding during masked reconstruction. To support the pre-training of VehicleMAE-V2, we construct Autobot4M, a large-scale dataset comprising approximately 4 million vehicle images and 12,693 text descriptions. Extensive experiments on five downstream tasks demonstrate the superior performance of VehicleMAE-V2.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GIMLET: Generalizable and Interpretable Model Learning through Embedded Thermodynamics</title>
<link>https://arxiv.org/abs/2512.19936</link>
<guid>https://arxiv.org/abs/2512.19936</guid>
<content:encoded><![CDATA[
arXiv:2512.19936v1 Announce Type: cross 
Abstract: We develop a data-driven framework for discovering constitutive relations in models of fluid flow and scalar transport. Our approach infers unknown closure terms in the governing equations (gray-box discovery) under the assumption that the temporal derivative, convective transport, and pressure-gradient contributions are known. The formulation is rooted in a variational principle from nonequilibrium thermodynamics, where the dynamics is defined by a free-energy functional and a dissipation functional. The unknown constitutive terms arise as functional derivatives of these functionals with respect to the state variables. To enable a flexible and structured model discovery, the free-energy and dissipation functionals are parameterized using neural networks, while their functional derivatives are obtained via automatic differentiation. This construction enforces thermodynamic consistency by design, ensuring monotonic decay of the total free energy and non-negative entropy production. The resulting method, termed GIMLET (Generalizable and Interpretable Model Learning through Embedded Thermodynamics), avoids reliance on a predefined library of candidate functions, unlike sparse regression or symbolic identification approaches. The learned models are generalizable in that functionals identified from one dataset can be transferred to distinct datasets governed by the same underlying equations. Moreover, the inferred free-energy and dissipation functions provide direct physical interpretability of the learned dynamics. The framework is demonstrated on several benchmark systems, including the viscous Burgers equation, the Kuramoto--Sivashinsky equation, and the incompressible Navier--Stokes equations for both Newtonian and non-Newtonian fluids.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Block-Recurrent Dynamics in Vision Transformers</title>
<link>https://arxiv.org/abs/2512.19941</link>
<guid>https://arxiv.org/abs/2512.19941</guid>
<content:encoded><![CDATA[
arXiv:2512.19941v1 Announce Type: cross 
Abstract: As Vision Transformers (ViTs) become standard vision backbones, a mechanistic account of their computational phenomenology is essential. Despite architectural cues that hint at dynamical structure, there is no settled framework that interprets Transformer depth as a well-characterized flow. In this work, we introduce the Block-Recurrent Hypothesis (BRH), arguing that trained ViTs admit a block-recurrent depth structure such that the computation of the original $L$ blocks can be accurately rewritten using only $k \ll L$ distinct blocks applied recurrently. Across diverse ViTs, between-layer representational similarity matrices suggest few contiguous phases. To determine whether these phases reflect genuinely reusable computation, we train block-recurrent surrogates of pretrained ViTs: Recurrent Approximations to Phase-structured TransfORmers (Raptor). In small-scale, we demonstrate that stochastic depth and training promote recurrent structure and subsequently correlate with our ability to accurately fit Raptor. We then provide an empirical existence proof for BRH by training a Raptor model to recover $96\%$ of DINOv2 ImageNet-1k linear probe accuracy in only 2 blocks at equivalent computational cost. Finally, we leverage our hypothesis to develop a program of Dynamical Interpretability. We find i) directional convergence into class-dependent angular basins with self-correcting trajectories under small perturbations, ii) token-specific dynamics, where cls executes sharp late reorientations while patch tokens exhibit strong late-stage coherence toward their mean direction, and iii) a collapse to low rank updates in late depth, consistent with convergence to low-dimensional attractors. Altogether, we find a compact recurrent program emerges along ViT depth, pointing to a low-complexity normative solution that enables these models to be studied through principled dynamical systems analysis.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Covariance-Aware Simplex Projection for Cardinality-Constrained Portfolio Optimization</title>
<link>https://arxiv.org/abs/2512.19986</link>
<guid>https://arxiv.org/abs/2512.19986</guid>
<content:encoded><![CDATA[
arXiv:2512.19986v1 Announce Type: cross 
Abstract: Metaheuristic algorithms for cardinality-constrained portfolio optimization require repair operators to map infeasible candidates onto the feasible region. Standard Euclidean projection treats assets as independent and can ignore the covariance structure that governs portfolio risk, potentially producing less diversified portfolios. This paper introduces Covariance-Aware Simplex Projection (CASP), a two-stage repair operator that (i) selects a target number of assets using volatility-normalized scores and (ii) projects the candidate weights using a covariance-aware geometry aligned with tracking-error risk. This provides a portfolio-theoretic foundation for using a covariance-induced distance in repair operators. On S&amp;P 500 data (2020-2024), CASP-Basic delivers materially lower portfolio variance than standard Euclidean repair without relying on return estimates, with improvements that are robust across assets and statistically significant. Ablation results indicate that volatility-normalized selection drives most of the variance reduction, while the covariance-aware projection provides an additional, consistent improvement. We further show that optional return-aware extensions can improve Sharpe ratios, and out-of-sample tests confirm that gains transfer to realized performance. CASP integrates as a drop-in replacement for Euclidean projection in metaheuristic portfolio optimizers.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Novel CNN Gradient Boosting Ensemble for Guava Disease Detection</title>
<link>https://arxiv.org/abs/2512.19989</link>
<guid>https://arxiv.org/abs/2512.19989</guid>
<content:encoded><![CDATA[
arXiv:2512.19989v1 Announce Type: cross 
Abstract: As a significant agricultural country, Bangladesh utilizes its fertile land for guava cultivation and dedicated labor to boost its economic development. In a nation like Bangladesh, enhancing guava production and agricultural practices plays a crucial role in its economy. Anthracnose and fruit fly infection can lower the quality and productivity of guava, a crucial tropical fruit. Expert systems that detect diseases early can reduce losses and safeguard the harvest. Images of guava fruits classified into the Healthy, Fruit Flies, and Anthracnose classes are included in the Guava Fruit Disease Dataset 2024 (GFDD24), which comes from plantations in Rajshahi and Pabna, Bangladesh. This study aims to create models using CNN alongside traditional machine learning techniques that can effectively identify guava diseases in locally cultivated varieties in Bangladesh. In order to achieve the highest classification accuracy of approximately 99.99% for the guava dataset, we propose utilizing ensemble models that combine CNNML with Gradient Boosting Machine. In general, the CNN-ML cascade framework exhibits strong, high-accuracy guava disease detection that is appropriate for real-time agricultural monitoring systems.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Schoenfeld's Anatomy of Mathematical Reasoning by Language Models</title>
<link>https://arxiv.org/abs/2512.19995</link>
<guid>https://arxiv.org/abs/2512.19995</guid>
<content:encoded><![CDATA[
arXiv:2512.19995v1 Announce Type: cross 
Abstract: Large language models increasingly expose reasoning traces, yet their underlying cognitive structure and steps remain difficult to identify and analyze beyond surface-level statistics. We adopt Schoenfeld's Episode Theory as an inductive, intermediate-scale lens and introduce ThinkARM (Anatomy of Reasoning in Models), a scalable framework that explicitly abstracts reasoning traces into functional reasoning steps such as Analysis, Explore, Implement, Verify, etc. When applied to mathematical problem solving by diverse models, this abstraction reveals reproducible thinking dynamics and structural differences between reasoning and non-reasoning models, which are not apparent from token-level views. We further present two diagnostic case studies showing that exploration functions as a critical branching step associated with correctness, and that efficiency-oriented methods selectively suppress evaluative feedback steps rather than uniformly shortening responses. Together, our results demonstrate that episode-level representations make reasoning steps explicit, enabling systematic analysis of how reasoning is structured, stabilized, and altered in modern language models.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IoT-based Android Malware Detection Using Graph Neural Network With Adversarial Defense</title>
<link>https://arxiv.org/abs/2512.20004</link>
<guid>https://arxiv.org/abs/2512.20004</guid>
<content:encoded><![CDATA[
arXiv:2512.20004v1 Announce Type: cross 
Abstract: Since the Internet of Things (IoT) is widely adopted using Android applications, detecting malicious Android apps is essential. In recent years, Android graph-based deep learning research has proposed many approaches to extract relationships from applications as graphs to generate graph embeddings. First, we demonstrate the effectiveness of graph-based classification using a Graph Neural Network (GNN)-based classifier to generate API graph embeddings. The graph embeddings are combined with Permission and Intent features to train multiple machine learning and deep learning models for Android malware detection. The proposed classification approach achieves an accuracy of 98.33 percent on the CICMaldroid dataset and 98.68 percent on the Drebin dataset. However, graph-based deep learning models are vulnerable, as attackers can add fake relationships to evade detection by the classifier. Second, we propose a Generative Adversarial Network (GAN)-based attack algorithm named VGAE-MalGAN targeting graph-based GNN Android malware classifiers. The VGAE-MalGAN generator produces adversarial malware API graphs, while the VGAE-MalGAN substitute detector attempts to mimic the target detector. Experimental results show that VGAE-MalGAN can significantly reduce the detection rate of GNN-based malware classifiers. Although the model initially fails to detect adversarial malware, retraining with generated adversarial samples improves robustness and helps mitigate adversarial attacks.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semiparametric KSD test: unifying score and distance-based approaches for goodness-of-fit testing</title>
<link>https://arxiv.org/abs/2512.20007</link>
<guid>https://arxiv.org/abs/2512.20007</guid>
<content:encoded><![CDATA[
arXiv:2512.20007v1 Announce Type: cross 
Abstract: Goodness-of-fit (GoF) tests are fundamental for assessing model adequacy. Score-based tests are appealing because they require fitting the model only once under the null. However, extending them to powerful nonparametric alternatives is difficult due to the lack of suitable score functions. Through a class of exponentially tilted models, we show that the resulting score-based GoF tests are equivalent to the tests based on integral probability metrics (IPMs) indexed by a function class. When the class is rich, the test is universally consistent. This simple yet insightful perspective enables reinterpretation of classical distance-based testing procedures-including those based on Kolmogorov-Smirnov distance, Wasserstein-1 distance, and maximum mean discrepancy-as arising from score-based constructions. Building on this insight, we propose a new nonparametric score-based GoF test through a special class of IPM induced by kernelized Stein's function class, called semiparametric kernelized Stein discrepancy (SKSD) test. Compared with other nonparametric score-based tests, the SKSD test is computationally efficient and accommodates general nuisance-parameter estimators, supported by a generic parametric bootstrap procedure. The SKSD test is universally consistent and attains Pitman efficiency. Moreover, SKSD test provides simple GoF tests for models with intractable likelihoods but tractable scores with the help of Stein's identity and we use two popular models, kernel exponential family and conditional Gaussian models, to illustrate the power of our method. Our method achieves power comparable to task-specific normality tests such as Anderson-Darling and Lilliefors, despite being designed for general nonparametric alternatives.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reliable LLM-Based Edge-Cloud-Expert Cascades for Telecom Knowledge Systems</title>
<link>https://arxiv.org/abs/2512.20012</link>
<guid>https://arxiv.org/abs/2512.20012</guid>
<content:encoded><![CDATA[
arXiv:2512.20012v1 Announce Type: cross 
Abstract: Large language models (LLMs) are emerging as key enablers of automation in domains such as telecommunications, assisting with tasks including troubleshooting, standards interpretation, and network optimization. However, their deployment in practice must balance inference cost, latency, and reliability. In this work, we study an edge-cloud-expert cascaded LLM-based knowledge system that supports decision-making through a question-and-answer pipeline. In it, an efficient edge model handles routine queries, a more capable cloud model addresses complex cases, and human experts are involved only when necessary. We define a misalignment-cost constrained optimization problem, aiming to minimize average processing cost, while guaranteeing alignment of automated answers with expert judgments. We propose a statistically rigorous threshold selection method based on multiple hypothesis testing (MHT) for a query processing mechanism based on knowledge and confidence tests. The approach provides finite-sample guarantees on misalignment risk. Experiments on the TeleQnA dataset -- a telecom-specific benchmark -- demonstrate that the proposed method achieves superior cost-efficiency compared to conventional cascaded baselines, while ensuring reliability at prescribed confidence levels.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gaussian Process Assisted Meta-learning for Image Classification and Object Detection Models</title>
<link>https://arxiv.org/abs/2512.20021</link>
<guid>https://arxiv.org/abs/2512.20021</guid>
<content:encoded><![CDATA[
arXiv:2512.20021v1 Announce Type: cross 
Abstract: Collecting operationally realistic data to inform machine learning models can be costly. Before collecting new data, it is helpful to understand where a model is deficient. For example, object detectors trained on images of rare objects may not be good at identification in poorly represented conditions. We offer a way of informing subsequent data acquisition to maximize model performance by leveraging the toolkit of computer experiments and metadata describing the circumstances under which the training data was collected (e.g., season, time of day, location). We do this by evaluating the learner as the training data is varied according to its metadata. A Gaussian process (GP) surrogate fit to that response surface can inform new data acquisitions. This meta-learning approach offers improvements to learner performance as compared to data with randomly selected metadata, which we illustrate on both classic learning examples, and on a motivating application involving the collection of aerial images in search of airplanes.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal Anytime-Valid Tests for Composite Nulls</title>
<link>https://arxiv.org/abs/2512.20039</link>
<guid>https://arxiv.org/abs/2512.20039</guid>
<content:encoded><![CDATA[
arXiv:2512.20039v1 Announce Type: cross 
Abstract: We consider the problem of designing optimal level-$\alpha$ power-one tests for composite nulls. Given a parameter $\alpha \in (0,1)$ and a stream of $\mathcal{X}$-valued observations $\{X_n: n \geq 1\} \overset{i.i.d.}{\sim} P$, the goal is to design a level-$\alpha$ power-one test $\tau_\alpha$ for the null $H_0: P \in \mathcal{P}_0 \subset \mathcal{P}(\mathcal{X})$. Prior works have shown that any such $\tau_\alpha$ must satisfy $\mathbb{E}_P[\tau_\alpha] \geq \tfrac{\log(1/\alpha)}{\gamma^*(P, \mathcal{P}_0)}$, where $\gamma^*(P, \mathcal{P}_0)$ is the so-called $\mathrm{KL}_{\inf}$ or minimum divergence of $P$ to the null class. In this paper, our objective is to develop and analyze constructive schemes that match this lower bound as $\alpha \downarrow 0$.
  We first consider the finite-alphabet case~($|\mathcal{X}| = m < \infty$), and show that a test based on \emph{universal} $e$-process~(formed by the ratio of a universal predictor and the running null MLE) is optimal in the above sense. The proof relies on a Donsker-Varadhan~(DV) based saddle-point representation of $\mathrm{KL}_{\inf}$, and an application of Sion's minimax theorem. This characterization motivates a general method for arbitrary $\mathcal{X}$: construct an $e$-process based on the empirical solutions to the saddle-point representation over a sufficiently rich class of test functions. We give sufficient conditions for the optimality of this test for compact convex nulls, and verify them for H\"older smooth density models. We end the paper with a discussion on the computational aspects of implementing our proposed tests in some practical settings.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Eigenspace Network and Its Application to Parametric Non-selfadjoint Eigenvalue Problems</title>
<link>https://arxiv.org/abs/2512.20058</link>
<guid>https://arxiv.org/abs/2512.20058</guid>
<content:encoded><![CDATA[
arXiv:2512.20058v1 Announce Type: cross 
Abstract: We consider operator learning for efficiently solving parametric non-selfadjoint eigenvalue problems. To overcome the spectral instability and mode switching inherent in non-selfadjoint operators, we introduce a hybrid framework that learns the stable invariant eigensubspace mapping rather than individual eigenfunctions. We proposed a Deep Eigenspace Network (DEN) architecture integrating Fourier Neural Operators, geometry-adaptive POD bases, and explicit banded cross-mode mixing mechanisms to capture complex spectral dependencies on unstructured meshes. We apply DEN to the parametric non-selfadjoint Steklov eigenvalue problem and provide theoretical proofs for the Lipschitz continuity of the eigensubspace with respect to the parameters. In addition, we derive error bounds for the reconstruction of the eigenspace. Numerical experiments validate DEN's high accuracy and zero-shot generalization capabilities across different discretizations.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DS-HGCN: A Dual-Stream Hypergraph Convolutional Network for Predicting Student Engagement via Social Contagion</title>
<link>https://arxiv.org/abs/2512.20059</link>
<guid>https://arxiv.org/abs/2512.20059</guid>
<content:encoded><![CDATA[
arXiv:2512.20059v1 Announce Type: cross 
Abstract: Student engagement is a critical factor influencing academic success and learning outcomes. Accurately predicting student engagement is essential for optimizing teaching strategies and providing personalized interventions. However, most approaches focus on single-dimensional feature analysis and assessing engagement based on individual student factors. In this work, we propose a dual-stream multi-feature fusion model based on hypergraph convolutional networks (DS-HGCN), incorporating social contagion of student engagement. DS-HGCN enables accurate prediction of student engagement states by modeling multi-dimensional features and their propagation mechanisms between students. The framework constructs a hypergraph structure to encode engagement contagion among students and captures the emotional and behavioral differences and commonalities by multi-frequency signals. Furthermore, we introduce a hypergraph attention mechanism to dynamically weigh the influence of each student, accounting for individual differences in the propagation process. Extensive experiments on public benchmark datasets demonstrate that our proposed method achieves superior performance and significantly outperforms existing state-of-the-art approaches.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language</title>
<link>https://arxiv.org/abs/2512.20111</link>
<guid>https://arxiv.org/abs/2512.20111</guid>
<content:encoded><![CDATA[
arXiv:2512.20111v1 Announce Type: cross 
Abstract: As the length of sequential decision-making tasks increases, it becomes computationally impractical to keep full interaction histories in context. We introduce a general framework for LLM agents to maintain concise contexts through multi-step interaction: Acting through Belief Bottlenecks Expressed in Language (ABBEL), and methods to further improve ABBEL agents with RL post-training. ABBEL replaces long multi-step interaction history by a belief state, i.e., a natural language summary of what has been discovered about task-relevant unknowns. Under ABBEL, at each step the agent first updates a prior belief with the most recent observation from the environment to form a posterior belief, then uses only the posterior to select an action. We systematically evaluate frontier models under ABBEL across six diverse multi-step environments, finding that ABBEL supports generating interpretable beliefs while maintaining near-constant memory use over interaction steps. However, bottleneck approaches are generally prone to error propagation, which we observe causing inferior performance when compared to the full context setting due to errors in belief updating. Therefore, we train LLMs to generate and act on beliefs within the ABBEL framework via reinforcement learning (RL). We experiment with belief grading, to reward higher quality beliefs, as well as belief length penalties to reward more compressed beliefs. Our experiments demonstrate the ability of RL to improve ABBEL's performance beyond the full context setting, while using less memory than contemporaneous approaches.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Retrieval-augmented Prompt Learning for Pre-trained Foundation Models</title>
<link>https://arxiv.org/abs/2512.20145</link>
<guid>https://arxiv.org/abs/2512.20145</guid>
<content:encoded><![CDATA[
arXiv:2512.20145v1 Announce Type: cross 
Abstract: The pre-trained foundation models (PFMs) have become essential for facilitating large-scale multimodal learning. Researchers have effectively employed the ``pre-train, prompt, and predict'' paradigm through prompt learning to induce improved few-shot performance. However, prompt learning approaches for PFMs still follow a parametric learning paradigm. As such, the stability of generalization in memorization and rote learning can be compromised. More specifically, conventional prompt learning might face difficulties in fully utilizing atypical instances and avoiding overfitting to shallow patterns with limited data during the process of fully-supervised training. To overcome these constraints, we present our approach, named RetroPrompt, which aims to achieve a balance between memorization and generalization by decoupling knowledge from mere memorization. Unlike traditional prompting methods, RetroPrompt leverages a publicly accessible knowledge base generated from the training data and incorporates a retrieval mechanism throughout the input, training, and inference stages. This enables the model to actively retrieve relevant contextual information from the corpus, thereby enhancing the available cues. We conduct comprehensive experiments on a variety of datasets across natural language processing and computer vision tasks to demonstrate the superior performance of our proposed approach, RetroPrompt, in both zero-shot and few-shot scenarios. Through detailed analysis of memorization patterns, we observe that RetroPrompt effectively reduces the reliance on rote memorization, leading to enhanced generalization.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Odysseus: Jailbreaking Commercial Multimodal LLM-integrated Systems via Dual Steganography</title>
<link>https://arxiv.org/abs/2512.20168</link>
<guid>https://arxiv.org/abs/2512.20168</guid>
<content:encoded><![CDATA[
arXiv:2512.20168v1 Announce Type: cross 
Abstract: By integrating language understanding with perceptual modalities such as images, multimodal large language models (MLLMs) constitute a critical substrate for modern AI systems, particularly intelligent agents operating in open and interactive environments. However, their increasing accessibility also raises heightened risks of misuse, such as generating harmful or unsafe content. To mitigate these risks, alignment techniques are commonly applied to align model behavior with human values. Despite these efforts, recent studies have shown that jailbreak attacks can circumvent alignment and elicit unsafe outputs. Currently, most existing jailbreak methods are tailored for open-source models and exhibit limited effectiveness against commercial MLLM-integrated systems, which often employ additional filters. These filters can detect and prevent malicious input and output content, significantly reducing jailbreak threats. In this paper, we reveal that the success of these safety filters heavily relies on a critical assumption that malicious content must be explicitly visible in either the input or the output. This assumption, while often valid for traditional LLM-integrated systems, breaks down in MLLM-integrated systems, where attackers can leverage multiple modalities to conceal adversarial intent, leading to a false sense of security in existing MLLM-integrated systems. To challenge this assumption, we propose Odysseus, a novel jailbreak paradigm that introduces dual steganography to covertly embed malicious queries and responses into benign-looking images. Extensive experiments on benchmark datasets demonstrate that our Odysseus successfully jailbreaks several pioneering and realistic MLLM-integrated systems, achieving up to 99% attack success rate. It exposes a fundamental blind spot in existing defenses, and calls for rethinking cross-modal security in MLLM-integrated systems.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimality-Informed Neural Networks for Solving Parametric Optimization Problems</title>
<link>https://arxiv.org/abs/2512.20270</link>
<guid>https://arxiv.org/abs/2512.20270</guid>
<content:encoded><![CDATA[
arXiv:2512.20270v1 Announce Type: cross 
Abstract: Many engineering tasks require solving families of nonlinear constrained optimization problems, parametrized in setting-specific variables. This is computationally demanding, particularly, if solutions have to be computed across strongly varying parameter values, e.g., in real-time control or for model-based design. Thus, we propose to learn the mapping from parameters to the primal optimal solutions and to their corresponding duals using neural networks, giving a dense estimation in contrast to gridded approaches. Our approach, Optimality-informed Neural Networks (OptINNs), combines (i) a KKT-residual loss that penalizes violations of the first-order optimality conditions under standard constraint qualifications assumptions, and (ii) problem-specific output activations that enforce simple inequality constraints (e.g., box-type/positivity) by construction. This design reduces data requirements, allows the prediction of dual variables, and improves feasibility and closeness to optimality compared to penalty-only training. Taking quadratic penalties as a baseline, since this approach has been previously proposed for the considered problem class in literature, our method simplifies hyperparameter tuning and attains tighter adherence to optimality conditions. We evaluate OptINNs on different nonlinear optimization problems ranging from low to high dimensions. On small problems, OptINNs match a quadratic-penalty baseline in primal accuracy while additionally predicting dual variables with low error. On larger problems, OptINNs achieve lower constraint violations and lower primal error compared to neural networks based on the quadratic-penalty method. These results suggest that embedding feasibility and optimality into the network architecture and loss can make learning-based surrogates more accurate, feasible, and data-efficient for parametric optimization.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KAN-AFT: An Interpretable Nonlinear Survival Model Integrating Kolmogorov-Arnold Networks with Accelerated Failure Time Analysis</title>
<link>https://arxiv.org/abs/2512.20305</link>
<guid>https://arxiv.org/abs/2512.20305</guid>
<content:encoded><![CDATA[
arXiv:2512.20305v1 Announce Type: cross 
Abstract: Survival analysis relies fundamentally on the semi-parametric Cox Proportional Hazards (CoxPH) model and the parametric Accelerated Failure Time (AFT) model. CoxPH assumes constant hazard ratios, often failing to capture real-world dynamics, while traditional AFT models are limited by rigid distributional assumptions. Although deep learning models like DeepAFT address these constraints by improving predictive accuracy and handling censoring, they inherit the significant challenge of black-box interpretability. The recent introduction of CoxKAN demonstrated the successful integration of Kolmogorov-Arnold Networks (KANs), a novel architecture that yields highly accurate and interpretable symbolic representations, within the CoxPH framework. Motivated by the interpretability gains of CoxKAN, we introduce KAN-AFT (Kolmogorov Arnold Network-based AFT), the first framework to apply KANs to the AFT model. KAN-AFT effectively models complex nonlinear relationships within the AFT framework. Our primary contributions include: (i) a principled AFT-KAN formulation, (ii) robust optimization strategies for right-censored observations (e.g., Buckley-James and IPCW), and (iii) an interpretability pipeline that converts the learned spline functions into closed-form symbolic equations for survival time. Empirical results on multiple datasets confirm that KAN-AFT achieves performance comparable to or better than DeepAFT, while uniquely providing transparent, symbolic models of the survival process.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Algorithm for Interpretable Graph Features via Motivic Persistent Cohomology</title>
<link>https://arxiv.org/abs/2512.20311</link>
<guid>https://arxiv.org/abs/2512.20311</guid>
<content:encoded><![CDATA[
arXiv:2512.20311v1 Announce Type: cross 
Abstract: We present the Chromatic Persistence Algorithm (CPA), an event-driven method for computing persistent cohomological features of weighted graphs via graphic arrangements, a classical object in computational geometry. We establish rigorous complexity results: CPA is exponential in the worst case, fixed-parameter tractable in treewidth, and nearly linear for common graph families such as trees, cycles, and series-parallel graphs. Finally, we demonstrate its practical applicability through a controlled experiment on molecular-like graph structures.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Top-K Exterior Power Persistent Homology: Algorithm, Structure, and Stability</title>
<link>https://arxiv.org/abs/2512.20325</link>
<guid>https://arxiv.org/abs/2512.20325</guid>
<content:encoded><![CDATA[
arXiv:2512.20325v1 Announce Type: cross 
Abstract: Exterior powers play important roles in persistent homology in computational geometry. In the present paper we study the problem of extracting the $K$ longest intervals of the exterior-power layers of a tame persistence module. We prove a structural decomposition theorem that organizes the exterior-power layers into monotone per-anchor streams with explicit multiplicities, enabling a best-first algorithm. We also show that the Top-$K$ length vector is $2$-Lipschitz under bottleneck perturbations of the input barcode, and prove a comparison-model lower bound. Our experiments confirm the theory, showing speedups over full enumeration in high overlap cases. By enabling efficient extraction of the most prominent features, our approach makes higher-order persistence feasible for large datasets and thus broadly applicable to machine learning, data science, and scientific computing.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Explaining Large Language Models in Software Engineering Tasks</title>
<link>https://arxiv.org/abs/2512.20328</link>
<guid>https://arxiv.org/abs/2512.20328</guid>
<content:encoded><![CDATA[
arXiv:2512.20328v1 Announce Type: cross 
Abstract: Recent progress in Large Language Models (LLMs) has substantially advanced the automation of software engineering (SE) tasks, enabling complex activities such as code generation and code summarization. However, the black-box nature of LLMs remains a major barrier to their adoption in high-stakes and safety-critical domains, where explainability and transparency are vital for trust, accountability, and effective human supervision. Despite increasing interest in explainable AI for software engineering, existing methods lack domain-specific explanations aligned with how practitioners reason about SE artifacts. To address this gap, we introduce FeatureSHAP, the first fully automated, model-agnostic explainability framework tailored to software engineering tasks. Based on Shapley values, FeatureSHAP attributes model outputs to high-level input features through systematic input perturbation and task-specific similarity comparisons, while remaining compatible with both open-source and proprietary LLMs. We evaluate FeatureSHAP on two bi-modal SE tasks: code generation and code summarization. The results show that FeatureSHAP assigns less importance to irrelevant input features and produces explanations with higher fidelity than baseline methods. A practitioner survey involving 37 participants shows that FeatureSHAP helps practitioners better interpret model outputs and make more informed decisions. Collectively, FeatureSHAP represents a meaningful step toward practical explainable AI in software engineering. FeatureSHAP is available at https://github.com/deviserlab/FeatureSHAP.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Avoiding the Price of Adaptivity: Inference in Linear Contextual Bandits via Stability</title>
<link>https://arxiv.org/abs/2512.20368</link>
<guid>https://arxiv.org/abs/2512.20368</guid>
<content:encoded><![CDATA[
arXiv:2512.20368v1 Announce Type: cross 
Abstract: Statistical inference in contextual bandits is complicated by the adaptive, non-i.i.d. nature of the data. A growing body of work has shown that classical least-squares inference may fail under adaptive sampling, and that constructing valid confidence intervals for linear functionals of the model parameter typically requires paying an unavoidable inflation of order $\sqrt{d \log T}$. This phenomenon -- often referred to as the price of adaptivity -- highlights the inherent difficulty of reliable inference under general contextual bandit policies.
  A key structural property that circumvents this limitation is the \emph{stability} condition of Lai and Wei, which requires the empirical feature covariance to concentrate around a deterministic limit. When stability holds, the ordinary least-squares estimator satisfies a central limit theorem, and classical Wald-type confidence intervals -- designed for i.i.d. data -- become asymptotically valid even under adaptation, \emph{without} incurring the $\sqrt{d \log T}$ price of adaptivity.
  In this paper, we propose and analyze a penalized EXP4 algorithm for linear contextual bandits. Our first main result shows that this procedure satisfies the Lai--Wei stability condition and therefore admits valid Wald-type confidence intervals for linear functionals. Our second result establishes that the same algorithm achieves regret guarantees that are minimax optimal up to logarithmic factors, demonstrating that stability and statistical efficiency can coexist within a single contextual bandit method. Finally, we complement our theory with simulations illustrating the empirical normality of the resulting estimators and the sharpness of the corresponding confidence intervals.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AUDRON: A Deep Learning Framework with Fused Acoustic Signatures for Drone Type Recognition</title>
<link>https://arxiv.org/abs/2512.20407</link>
<guid>https://arxiv.org/abs/2512.20407</guid>
<content:encoded><![CDATA[
arXiv:2512.20407v1 Announce Type: cross 
Abstract: Unmanned aerial vehicles (UAVs), commonly known as drones, are increasingly used across diverse domains, including logistics, agriculture, surveillance, and defense. While these systems provide numerous benefits, their misuse raises safety and security concerns, making effective detection mechanisms essential. Acoustic sensing offers a low-cost and non-intrusive alternative to vision or radar-based detection, as drone propellers generate distinctive sound patterns. This study introduces AUDRON (AUdio-based Drone Recognition Network), a hybrid deep learning framework for drone sound detection, employing a combination of Mel-Frequency Cepstral Coefficients (MFCC), Short-Time Fourier Transform (STFT) spectrograms processed with convolutional neural networks (CNNs), recurrent layers for temporal modeling, and autoencoder-based representations. Feature-level fusion integrates complementary information before classification. Experimental evaluation demonstrates that AUDRON effectively differentiates drone acoustic signatures from background noise, achieving high accuracy while maintaining generalizability across varying conditions. AUDRON achieves 98.51 percent and 97.11 percent accuracy in binary and multiclass classification. The results highlight the advantage of combining multiple feature representations with deep learning for reliable acoustic drone detection, suggesting the framework's potential for deployment in security and surveillance applications where visual or radar sensing may be limited.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Aligned Economic Index &amp; The State Switching Model</title>
<link>https://arxiv.org/abs/2512.20460</link>
<guid>https://arxiv.org/abs/2512.20460</guid>
<content:encoded><![CDATA[
arXiv:2512.20460v1 Announce Type: cross 
Abstract: A growing empirical literature suggests that equity-premium predictability is state dependent, with much of the forecasting power concentrated around recessionary periods \parencite{Henkel2011,DanglHalling2012,Devpura2018}. I study U.S. stock return predictability across economic regimes and document strong evidence of time-varying expected returns across both expansionary and contractionary states. I contribute in two ways. First, I introduce a state-switching predictive regression in which the market state is defined in real time using the slope of the yield curve. Relative to the standard one-state predictive regression, the state-switching specification increases both in-sample and out-of-sample performance for the set of popular predictors considered by \textcite{WelchGoyal2008}, improving the out-of-sample performance of most predictors in economically meaningful ways. Second, I propose a new aggregate predictor, the Aligned Economic Index, constructed via partial least squares (PLS). Under the state-switching model, the Aligned Economic Index exhibits statistically and economically significant predictive power in sample and out of sample, and it outperforms widely used benchmark predictors and alternative predictor-combination methods.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ScoreMatchingRiesz: Auto-DML with Infinitesimal Classification</title>
<link>https://arxiv.org/abs/2512.20523</link>
<guid>https://arxiv.org/abs/2512.20523</guid>
<content:encoded><![CDATA[
arXiv:2512.20523v1 Announce Type: cross 
Abstract: This study proposes Riesz representer estimation methods based on score matching. The Riesz representer is a key component in debiased machine learning for constructing $\sqrt{n}$-consistent and efficient estimators in causal inference and structural parameter estimation. To estimate the Riesz representer, direct approaches have garnered attention, such as Riesz regression and the covariate balancing propensity score. These approaches can also be interpreted as variants of direct density ratio estimation (DRE) in several applications such as average treatment effect estimation. In DRE, it is well known that flexible models can easily overfit the observed data due to the estimand and the form of the loss function. To address this issue, recent work has proposed modeling the density ratio as a product of multiple intermediate density ratios and estimating it using score-matching techniques, which are often used in the diffusion model literature. We extend score-matching-based DRE methods to Riesz representer estimation. Our proposed method not only mitigates overfitting but also provides insights for causal inference by bridging marginal effects and average policy effects through time score functions.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Over-the-Air Goal-Oriented Communications</title>
<link>https://arxiv.org/abs/2512.20533</link>
<guid>https://arxiv.org/abs/2512.20533</guid>
<content:encoded><![CDATA[
arXiv:2512.20533v1 Announce Type: cross 
Abstract: Goal-oriented communications offer an attractive alternative to the Shannon-based communication paradigm, where the data is never reconstructed at the Receiver (RX) side. Rather, focusing on the case of edge inference, the Transmitter (TX) and the RX cooperate to exchange features of the input data that will be used to predict an unseen attribute of them, leveraging information from collected data sets. This chapter demonstrates that the wireless channel can be used to perform computations over the data, when equipped with programmable metasurfaces. The end-to-end system of the TX, RX, and MS-based channel is treated as a single deep neural network which is trained through backpropagation to perform inference on unseen data. Using Stacked Intelligent Metasurfaces (SIM), it is shown that this Metasurfaces-Integrated Neural Network (MINN) can achieve performance comparable to fully digital neural networks under various system parameters and data sets. By offloading computations onto the channel itself, important benefits may be achieved in terms of energy consumption, arising from reduced computations at the transceivers and smaller transmission power required for successful inference.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shallow Neural Networks Learn Low-Degree Spherical Polynomials with Learnable Channel Attention</title>
<link>https://arxiv.org/abs/2512.20562</link>
<guid>https://arxiv.org/abs/2512.20562</guid>
<content:encoded><![CDATA[
arXiv:2512.20562v1 Announce Type: cross 
Abstract: We study the problem of learning a low-degree spherical polynomial of degree $\ell_0 = \Theta(1) \ge 1$ defined on the unit sphere in $\RR^d$ by training an over-parameterized two-layer neural network (NN) with channel attention in this paper. Our main result is the significantly improved sample complexity for learning such low-degree polynomials. We show that, for any regression risk $\eps \in (0,1)$, a carefully designed two-layer NN with channel attention and finite width of $m \ge \Theta({n^4 \log (2n/\delta)}/{d^{2\ell_0}})$ trained by the vanilla gradient descent (GD) requires the lowest sample complexity of $n \asymp \Theta(d^{\ell_0}/\eps)$ with probability $1-\delta$ for every $\delta \in (0,1)$, in contrast with the representative sample complexity $\Theta\pth{d^{\ell_0} \max\set{\eps^{-2},\log d}}$, where $n$ is the training daata size. Moreover, such sample complexity is not improvable since the trained network renders a sharp rate of the nonparametric regression risk of the order $\Theta(d^{\ell_0}/{n})$ with probability at least $1-\delta$. On the other hand, the minimax optimal rate for the regression risk with a kernel of rank $\Theta(d^{\ell_0})$ is $\Theta(d^{\ell_0}/{n})$, so that the rate of the nonparametric regression risk of the network trained by GD is minimax optimal. The training of the two-layer NN with channel attention consists of two stages. In Stage 1, a provable learnable channel selection algorithm identifies the ground-truth channel number $\ell_0$ from the initial $L \ge \ell_0$ channels in the first-layer activation, with high probability. This learnable selection is achieved by an efficient one-step GD update on both layers, enabling feature learning for low-degree polynomial targets. In Stage 2, the second layer is trained by standard GD using the activation function with the selected channels.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving</title>
<link>https://arxiv.org/abs/2512.20563</link>
<guid>https://arxiv.org/abs/2512.20563</guid>
<content:encoded><![CDATA[
arXiv:2512.20563v1 Announce Type: cross 
Abstract: Simulators can generate virtually unlimited driving data, yet imitation learning policies in simulation still struggle to achieve robust closed-loop performance. Motivated by this gap, we empirically study how misalignment between privileged expert demonstrations and sensor-based student observations can limit the effectiveness of imitation learning. More precisely, experts have significantly higher visibility (e.g., ignoring occlusions) and far lower uncertainty (e.g., knowing other vehicles' actions), making them difficult to imitate reliably. Furthermore, navigational intent (i.e., the route to follow) is under-specified in student models at test time via only a single target point. We demonstrate that these asymmetries can measurably limit driving performance in CARLA and offer practical interventions to address them. After careful modifications to narrow the gaps between expert and student, our TransFuser v6 (TFv6) student policy achieves a new state of the art on all major publicly available CARLA closed-loop benchmarks, reaching 95 DS on Bench2Drive and more than doubling prior performances on Longest6~v2 and Town13. Additionally, by integrating perception supervision from our dataset into a shared sim-to-real pipeline, we show consistent gains on the NAVSIM and Waymo Vision-Based End-to-End driving benchmarks. Our code, data, and models are publicly available at https://github.com/autonomousvision/lead.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedPOD: the deployable units of training for federated learning</title>
<link>https://arxiv.org/abs/2512.20610</link>
<guid>https://arxiv.org/abs/2512.20610</guid>
<content:encoded><![CDATA[
arXiv:2512.20610v1 Announce Type: cross 
Abstract: This paper proposes FedPOD (Proportionally Orchestrated Derivative) for optimizing learning efficiency and communication cost in federated learning among multiple clients. Inspired by FedPIDAvg, we define a round-wise task for FedPOD to enhance training efficiency. FedPIDAvg achieved performance improvement by incorporating the training loss reduction for prediction entropy as weights using differential terms. Furthermore, by modeling data distribution with a Poisson distribution and using a PID controller, it reduced communication costs even in skewed data distribution. However, excluding participants classified as outliers based on the Poisson distribution can limit data utilization. Additionally, PID controller requires the same participants to be maintained throughout the federated learning process as it uses previous rounds' learning information in the current round. In our approach, FedPOD addresses these issues by including participants excluded as outliers, eliminating dependency on previous rounds' learning information, and applying a method for calculating validation loss at each round. In this challenge, FedPOD presents comparable performance to FedPIDAvg in metrics of Dice score, 0.78, 0.71 and 0.72 for WT, ET and TC in average, and projected convergence score, 0.74 in average. Furthermore, the concept of FedPOD draws inspiration from Kubernetes' smallest computing unit, POD, designed to be compatible with Kubernetes auto-scaling. Extending round-wise tasks of FedPOD to POD units allows flexible design by applying scale-out similar to Kubernetes' auto-scaling. This work demonstrated the potentials of FedPOD to enhance federated learning by improving efficiency, flexibility, and performance in metrics.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LongVideoAgent: Multi-Agent Reasoning with Long Videos</title>
<link>https://arxiv.org/abs/2512.20618</link>
<guid>https://arxiv.org/abs/2512.20618</guid>
<content:encoded><![CDATA[
arXiv:2512.20618v1 Announce Type: cross 
Abstract: Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Local Training in Federated Learning via Temperature Scaling</title>
<link>https://arxiv.org/abs/2401.09986</link>
<guid>https://arxiv.org/abs/2401.09986</guid>
<content:encoded><![CDATA[
arXiv:2401.09986v3 Announce Type: replace 
Abstract: Federated learning is inherently hampered by data heterogeneity: non-i.i.d. training data over local clients. We propose a novel model training approach for federated learning, FLex&amp;Chill, which exploits the Logit Chilling method. Through extensive evaluations, we demonstrate that, in the presence of non-i.i.d. data characteristics inherent in federated learning systems, this approach can expedite model convergence and improve inference accuracy. Quantitatively, from our experiments, we observe up to 6X improvement in the global federated learning model convergence time, and up to 3.37% improvement in inference accuracy.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Topological Dependencies in Spatio-Temporal Graphs with Cycle Message Passing Blocks</title>
<link>https://arxiv.org/abs/2401.15894</link>
<guid>https://arxiv.org/abs/2401.15894</guid>
<content:encoded><![CDATA[
arXiv:2401.15894v3 Announce Type: replace 
Abstract: Graph Neural Networks (GNNs) and Transformer-based models have been increasingly adopted to learn the complex vector representations of spatio-temporal graphs, capturing intricate spatio-temporal dependencies crucial for applications such as traffic datasets. Although many existing methods utilize multi-head attention mechanisms and message-passing neural networks (MPNNs) to capture both spatial and temporal relations, these approaches encode temporal and spatial relations independently, and reflect the graph's topological characteristics in a limited manner. In this work, we introduce the Cycle to Mixer (Cy2Mixer), a novel spatio-temporal GNN based on topological non-trivial invariants of spatio-temporal graphs with gated multi-layer perceptrons (gMLP). The Cy2Mixer is composed of three blocks based on MLPs: A temporal block for capturing temporal properties, a message-passing block for encapsulating spatial information, and a cycle message-passing block for enriching topological information through cyclic subgraphs. We bolster the effectiveness of Cy2Mixer with mathematical evidence emphasizing that our cycle message-passing block is capable of offering differentiated information to the deep learning model compared to the message-passing block. Furthermore, empirical evaluations substantiate the efficacy of the Cy2Mixer, demonstrating state-of-the-art performances across various spatio-temporal benchmark datasets. The source code is available at https://github.com/leemingo/cy2mixer.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning a Neural Solver for Parametric PDE to Enhance Physics-Informed Methods</title>
<link>https://arxiv.org/abs/2410.06820</link>
<guid>https://arxiv.org/abs/2410.06820</guid>
<content:encoded><![CDATA[
arXiv:2410.06820v4 Announce Type: replace 
Abstract: Physics-informed deep learning often faces optimization challenges due to the complexity of solving partial differential equations (PDEs), which involve exploring large solution spaces, require numerous iterations, and can lead to unstable training. These challenges arise particularly from the ill-conditioning of the optimization problem caused by the differential terms in the loss function. To address these issues, we propose learning a solver, i.e., solving PDEs using a physics-informed iterative algorithm trained on data. Our method learns to condition a gradient descent algorithm that automatically adapts to each PDE instance, significantly accelerating and stabilizing the optimization process and enabling faster convergence of physics-aware models. Furthermore, while traditional physics-informed methods solve for a single PDE instance, our approach extends to parametric PDEs. Specifically, we integrate the physical loss gradient with PDE parameters, allowing our method to solve over a distribution of PDE parameters, including coefficients, initial conditions, and boundary conditions. We demonstrate the effectiveness of our approach through empirical experiments on multiple datasets, comparing both training and test-time optimization performance. The code is available at https://github.com/2ailesB/neural-parametric-solver.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning and Machine Learning -- Python Data Structures and Mathematics Fundamental: From Theory to Practice</title>
<link>https://arxiv.org/abs/2410.19849</link>
<guid>https://arxiv.org/abs/2410.19849</guid>
<content:encoded><![CDATA[
arXiv:2410.19849v2 Announce Type: replace 
Abstract: This book provides a comprehensive introduction to the foundational concepts of machine learning (ML) and deep learning (DL). It bridges the gap between theoretical mathematics and practical application, focusing on Python as the primary programming language for implementing key algorithms and data structures. The book covers a wide range of topics, including basic and advanced Python programming, fundamental mathematical operations, matrix operations, linear algebra, and optimization techniques crucial for training ML and DL models. Advanced subjects like neural networks, optimization algorithms, and frequency domain methods are also explored, along with real-world applications of large language models (LLMs) and artificial intelligence (AI) in big data management. Designed for both beginners and advanced learners, the book emphasizes the critical role of mathematical principles in developing scalable AI solutions. Practical examples and Python code are provided throughout, ensuring readers gain hands-on experience in applying theoretical knowledge to solve complex problems in ML, DL, and big data analytics.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FP=xINT:Representing Neural Networks via Low-Bit Series Basis Functions</title>
<link>https://arxiv.org/abs/2412.06865</link>
<guid>https://arxiv.org/abs/2412.06865</guid>
<content:encoded><![CDATA[
arXiv:2412.06865v2 Announce Type: replace 
Abstract: Post-Training Quantization (PTQ) converts pre-trained Full-Precision (FP) models into quantized versions without training. While existing methods reduce size and computational costs, they also significantly degrade performance and quantization efficiency at extremely low settings due to quantization noise. We introduce a deep model series expansion framework to address this issue, enabling rapid and accurate approximation of unquantized models without calibration sets or fine-tuning. This is the first use of series expansion for neural network quantization. Specifically, our method expands the FP model into multiple low-bit basis models. To ensure accurate quantization, we develop low-bit basis model expansions at different granularities (tensor, layer, model), and theoretically confirm their convergence to the dense model, thus restoring FP model accuracy. Additionally, we design AbelianAdd/Mul operations between isomorphic models in the low-bit expansion, forming an Abelian group to ensure operation parallelism and commutativity. The experiments show that our algorithm achieves state-of-the-art performance in low-bit settings; for example, 4-bit quantization of ResNet-50 surpasses the original accuracy, reaching 77.03%. The code will be made public.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lossless Model Compression via Joint Low-Rank Factorization Optimization</title>
<link>https://arxiv.org/abs/2412.06867</link>
<guid>https://arxiv.org/abs/2412.06867</guid>
<content:encoded><![CDATA[
arXiv:2412.06867v2 Announce Type: replace 
Abstract: Low-rank factorization is a popular model compression technique that minimizes the error $\delta$ between approximated and original weight matrices. Despite achieving performances close to the original models when $\delta$ is optimized, a performance discrepancy remains due to the separate optimization processes for low-rank factorization and model performance, resulting in unavoidable losses. We address this issue by introducing a novel joint optimization strategy for lossless low-rank weight factorization, which, for the first time, enhances the model's performance beyond the original. Our approach begins with a theoretical analysis of the relationship between low-rank factorization and model optimization objectives, establishing a precise perturbation range for matrix factorization errors on model performance. This challenge is then reformulated as a numerical rank deficiency problem with inequality constraints and develop a joint objective that simultaneously addresses factorization error and model performance. Based on the above analysis, we propose two optimization algorithms: \textbf{a lossless optimization algorithm} that maximizes model accuracy while ensuring compression, and \textbf{a compact optimization algorithm} that minimizes model size while preserving performance. These algorithms do not require fine-tuning and can directly compress numerous deep models to achieve lossless results. Our methods demonstrate robust efficacy across various vision and language tasks. For example, the compressed model reduced by 70\% on ResNext50 outperforms the original. Our code will be made public.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning for Spatio-Temporal Fusion in Land Surface Temperature Estimation: A Comprehensive Survey, Experimental Analysis, and Future Trends</title>
<link>https://arxiv.org/abs/2412.16631</link>
<guid>https://arxiv.org/abs/2412.16631</guid>
<content:encoded><![CDATA[
arXiv:2412.16631v2 Announce Type: replace 
Abstract: Land Surface Temperature (LST) plays a key role in climate monitoring, urban heat assessment, and land-atmosphere interactions. However, current thermal infrared satellite sensors cannot simultaneously achieve high spatial and temporal resolution. Spatio-temporal fusion (STF) techniques address this limitation by combining complementary satellite data, one with high spatial but low temporal resolution, and another with high temporal but low spatial resolution. Existing STF techniques, from classical models to modern deep learning (DL) architectures, were primarily developed for surface reflectance (SR). Their application to thermal data remains limited and often overlooks LST-specific spatial and temporal variability. This study provides a focused review of DL-based STF methods for LST. We present a formal mathematical definition of the thermal fusion task, propose a refined taxonomy of relevant DL methods, and analyze the modifications required when adapting SR-oriented models to LST. To support reproducibility and benchmarking, we introduce a new dataset comprising 51 Terra MODIS-Landsat LST pairs from 2013 to 2024, and evaluate representative models to explore their behavior on thermal data. The analysis highlights performance gaps, architecture sensitivities, and open research challenges. The dataset and accompanying resources are publicly available at https://github.com/Sofianebouaziz1/STF-LST.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedMeld: A Model-dispersal Federated Learning Framework for Space-ground Integrated Networks</title>
<link>https://arxiv.org/abs/2412.17231</link>
<guid>https://arxiv.org/abs/2412.17231</guid>
<content:encoded><![CDATA[
arXiv:2412.17231v3 Announce Type: replace 
Abstract: To bridge the digital divide, space-ground integrated networks (SGINs) are expected to deliver artificial intelligence (AI) services to every corner of the world. One key mission of SGINs is to support federated learning (FL) at a global scale. However, existing space-ground integrated FL frameworks involve ground stations or costly inter-satellite links, entailing excessive training latency and communication costs. To overcome these limitations, we propose an infrastructure-free federated learning framework based on a model dispersal (FedMeld) strategy, which exploits periodic movement patterns and store-carry-forward capabilities of satellites to enable parameter mixing across large-scale geographical regions. We theoretically show that FedMeld leads to global model convergence and quantify the effects of round interval and mixing ratio between adjacent areas on its learning performance. Based on the theoretical results, we formulate a joint optimization problem to design the staleness control and mixing ratio (SC-MR) for minimizing the training loss. By decomposing the problem into sequential SC and MR subproblems without compromising the optimality, we derive the round interval solution in a closed form and the mixing ratio in a semi-closed form to achieve the optimal latency-accuracy tradeoff. Experiments using various datasets demonstrate that FedMeld achieves superior model accuracy while significantly reducing communication costs as compared with traditional FL schemes for SGINs.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A General Error-Theoretical Analysis Framework for Constructing Compression Strategies</title>
<link>https://arxiv.org/abs/2502.15802</link>
<guid>https://arxiv.org/abs/2502.15802</guid>
<content:encoded><![CDATA[
arXiv:2502.15802v2 Announce Type: replace 
Abstract: The exponential growth in parameter size and computational complexity of deep models poses significant challenges for efficient deployment. The core problem of existing compression methods is that different layers of the model have significant differences in their tolerance to compression levels. For instance, the first layer of a model can typically sustain a higher compression level compared to the last layer without compromising performance. Thus, the key challenge lies in how to allocate compression levels across layers in a way that minimizes performance loss while maximizing parameter reduction. To address this challenge, we propose a Compression Error Theory (CET) framework, designed to determine the optimal compression level for each layer. Taking quantization as an example, CET leverages differential expansion and algebraic geometry to reconstruct the quadratic form of quantization error as ellipsoids and hyperbolic paraboloids, and utilizes their geometric structures to define an error subspace. To identify the error subspace with minimal performance loss, by performing orthogonal decomposition of the geometric space, CET transforms the optimization process of the error subspace into a complementary problem. The final theoretical analysis shows that constructing the quantization subspace along the major axis results in minimal performance degradation. Through experimental verification of the theory, CET can greatly retain performance while compressing. Specifically, on the ResNet-34 model, CET achieves nearly 11$\times$ parameter compression while even surpassing performance comparable to the original model.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GradMix: Gradient-based Selective Mixup for Robust Data Augmentation in Class-Incremental Learning</title>
<link>https://arxiv.org/abs/2505.08528</link>
<guid>https://arxiv.org/abs/2505.08528</guid>
<content:encoded><![CDATA[
arXiv:2505.08528v2 Announce Type: replace 
Abstract: In the context of continual learning, acquiring new knowledge while maintaining previous knowledge presents a significant challenge. Existing methods often use experience replay techniques that store a small portion of previous task data for training. In experience replay approaches, data augmentation has emerged as a promising strategy to further improve the model performance by mixing limited previous task data with sufficient current task data. However, we theoretically and empirically analyze that training with mixed samples from random sample pairs may harm the knowledge of previous tasks and cause greater catastrophic forgetting. We then propose GradMix, a robust data augmentation method specifically designed for mitigating catastrophic forgetting in class-incremental learning. GradMix performs gradient-based selective mixup using a class-based criterion that mixes only samples from helpful class pairs and not from detrimental class pairs for reducing catastrophic forgetting. Our experiments on various real datasets show that GradMix outperforms data augmentation baselines in accuracy by minimizing the forgetting of previous knowledge.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Deep Morphological Neural Networks as Universal Approximators</title>
<link>https://arxiv.org/abs/2505.09710</link>
<guid>https://arxiv.org/abs/2505.09710</guid>
<content:encoded><![CDATA[
arXiv:2505.09710v3 Announce Type: replace 
Abstract: We investigate deep morphological neural networks (DMNNs). We demonstrate that despite their inherent non-linearity, "linear" activations are essential for DMNNs. To preserve their inherent sparsity, we propose architectures that constraint the parameters of the "linear" activations: For the first (resp. second) architecture, we work under the constraint that the majority of parameters (resp. learnable parameters) should be part of morphological operations. We improve the generalization ability of our networks via residual connections and weight dropout. Our proposed networks can be successfully trained, and are more prunable than linear networks. To the best of our knowledge, we are the first to successfully train DMNNs under such constraints. Finally, we propose a hybrid network architecture combining linear and morphological layers, showing empirically that the inclusion of morphological layers significantly accelerates the convergence of gradient descent with large batches.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Scale Harmonic Encoding for Feature-Wise Graph Message Passing</title>
<link>https://arxiv.org/abs/2505.15015</link>
<guid>https://arxiv.org/abs/2505.15015</guid>
<content:encoded><![CDATA[
arXiv:2505.15015v2 Announce Type: replace 
Abstract: Most Graph Neural Networks (GNNs) propagate messages by treating node embeddings as holistic feature vectors, implicitly assuming uniform relevance across feature dimensions. This limits their ability to selectively transmit informative components, especially when graph structures exhibit distinct frequency characteristics. We propose MSH-GNN (Multi-Scale Harmonic Graph Neural Network), a frequency-aware message passing framework that performs feature-wise adaptive propagation. Each node projects incoming messages onto node-conditioned feature subspaces derived from its own representation, enabling selective extraction of frequency-relevant components. Learnable multi-scale harmonic modulations further allow the model to capture both smooth and oscillatory structural patterns. A frequency-aware attention pooling mechanism is introduced for graph-level readout. We show that MSH-GNN admits an interpretation as a learnable Fourier-feature approximation of kernelized message functions and matches the expressive power of the 1-Weisfeiler-Lehman (1-WL) test. Extensive experiments on node- and graph-level benchmarks demonstrate consistent improvements over state-of-the-art methods, particularly in joint structure-frequency analysis tasks.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion Self-Weighted Guidance for Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.18345</link>
<guid>https://arxiv.org/abs/2505.18345</guid>
<content:encoded><![CDATA[
arXiv:2505.18345v2 Announce Type: replace 
Abstract: Offline reinforcement learning (RL) recovers the optimal policy $\pi$ given historical observations of an agent. In practice, $\pi$ is modeled as a weighted version of the agent's behavior policy $\mu$, using a weight function $w$ working as a critic of the agent's behavior. Though recent approaches to offline RL based on diffusion models have exhibited promising results, the computation of the required scores is challenging due to their dependence on the unknown $w$. In this work, we alleviate this issue by constructing a diffusion over both the actions and the weights. With the proposed setting, the required scores are directly obtained from the diffusion model without learning extra networks. Our main conceptual contribution is a novel guidance method, where guidance (which is a function of $w$) comes from the same diffusion model, therefore, our proposal is termed Self-Weighted Guidance (SWG). We show that SWG generates samples from the desired distribution on toy examples and performs on par with state-of-the-art methods on D4RL's challenging environments, while maintaining a streamlined training pipeline. We further validate SWG through ablation studies on weight formulations and scalability.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Position: Federated Foundation Language Model Post-Training Should Focus on Open-Source Models</title>
<link>https://arxiv.org/abs/2505.23593</link>
<guid>https://arxiv.org/abs/2505.23593</guid>
<content:encoded><![CDATA[
arXiv:2505.23593v3 Announce Type: replace 
Abstract: Post-training of foundation language models has emerged as a promising research domain in federated learning (FL) with the goal to enable privacy-preserving model improvements and adaptations to user's downstream tasks. Recent advances in this area adopt centralized post-training approaches that build upon black-box foundation language models where there is no access to model weights and architecture details. Although the use of black-box models has been successful in centralized post-training, their blind replication in FL raises several concerns. Our position is that using black-box models in FL contradicts the core principles of federation such as data privacy and autonomy. In this position paper, we critically analyze the usage of black-box models in federated post-training, and provide a detailed account of various aspects of openness and their implications for FL.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Position as Probability: Self-Supervised Transformers that Think Past Their Training for Length Extrapolation</title>
<link>https://arxiv.org/abs/2506.00920</link>
<guid>https://arxiv.org/abs/2506.00920</guid>
<content:encoded><![CDATA[
arXiv:2506.00920v2 Announce Type: replace 
Abstract: Deep sequence models typically degrade in accuracy when test sequences significantly exceed their training lengths, yet many critical tasks--such as algorithmic reasoning, multi-step arithmetic, and compositional generalization--require robust length extrapolation. We introduce PRISM, a Probabilistic Relative-position Implicit Superposition Model, a novel positional encoding mechanism that enables Transformers to extrapolate accurately up to 10x beyond their training length. PRISM learns continuous relative positions through a differentiable histogram-filter update, preserving position uncertainty via a probabilistic superposition rather than conventional deterministic embeddings. Empirically, PRISM achieves state-of-the-art length extrapolation, successfully generalizing to previously intractable sequence lengths across algorithmic benchmarks--including arithmetic (addition, multiplication), SCAN compositionality tasks, and complex copy variants derived from DeepMind's recent datasets. Our analysis demonstrates that PRISM's stochastic positional encoding maintains sharp and interpretable internal states, providing a theoretical basis for reliable length generalization. These results advance the goal of neural sequence models that remain algorithmically robust at lengths far exceeding their training horizon.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Heartcare Suite: A Unified Multimodal ECG Suite for Dual Signal-Image Modeling and Understanding</title>
<link>https://arxiv.org/abs/2506.05831</link>
<guid>https://arxiv.org/abs/2506.05831</guid>
<content:encoded><![CDATA[
arXiv:2506.05831v3 Announce Type: replace 
Abstract: Although electrocardiograms (ECG) play a dominant role in cardiovascular diagnosis and treatment, their intrinsic data forms and representational patterns pose significant challenges for medical multimodal large language models (Med-MLLMs) in achieving cross-modal semantic alignment. To address this gap, we propose Heartcare Suite, a unified ECG suite designed for dual signal-image modeling and understanding. (i) Heartcare-400K: We build a finegrained ECG instruction dataset on top of our data pipeline engine--HeartAgent--by integrating 12,170 high quality clinical ECG reports from top hospitals with open-source data; (ii) Heartcare-Bench: a systematic benchmark assessing performance of models in multi-perspective ECG understanding and cross-modal generalization, providing guidance for optimizing ECG comprehension models; (iii) HeartcareGPT: built upon a structure-aware discrete tokenizer Beat, we propose the DSPA (Dual Stream Projection Alignment) paradigm--a dual encoder projection alignment mechanism enabling joint optimizing and modeling native ECG signal-image within a shared feature space. Heartcare achieves consistent improvements across diverse ECG understanding tasks, validating both the effectiveness of the unified modeling paradigm and the necessity of a high-quality data pipeline, and establishing a methodological foundation for extending Med-MLLMs toward physiological signal domains. Our project is available at https://github.com/DCDmllm/Heartcare-Suite .
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>mLaSDI: Multi-stage latent space dynamics identification</title>
<link>https://arxiv.org/abs/2506.09207</link>
<guid>https://arxiv.org/abs/2506.09207</guid>
<content:encoded><![CDATA[
arXiv:2506.09207v3 Announce Type: replace 
Abstract: Accurately solving partial differential equations (PDEs) is essential across many scientific disciplines. However, high-fidelity solvers can be computationally prohibitive, motivating the development of reduced-order models (ROMs). Recently, Latent Space Dynamics Identification (LaSDI) was proposed as a data-driven, non-intrusive ROM framework. LaSDI compresses the training data via an autoencoder and learns user-specified ordinary differential equations (ODEs), governing the latent dynamics, enabling rapid predictions for unseen parameters. While LaSDI has produced effective ROMs for numerous problems, the autoencoder must simultaneously reconstruct the training data and satisfy the imposed latent dynamics, which are often competing objectives that limit accuracy, particularly for complex or high-frequency phenomena. To address this limitation, we propose multi-stage Latent Space Dynamics Identification (mLaSDI). With mLaSDI, we train LaSDI sequentially in stages. After training the initial autoencoder, we train additional decoders which map the latent trajectories to residuals from previous stages. This staged residual learning, combined with periodic activation functions, enables recovery of high-frequency content without sacrificing interpretability of the latent dynamics. Numerical experiments on a multiscale oscillating system, unsteady wake flow, and the 1D-1V Vlasov equation demonstrate that mLaSDI achieves significantly lower reconstruction and prediction errors, often by an order of magnitude, while requiring less training time and reduced hyperparameter tuning compared to standard LaSDI.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mixture of Experts in Large Language Models</title>
<link>https://arxiv.org/abs/2507.11181</link>
<guid>https://arxiv.org/abs/2507.11181</guid>
<content:encoded><![CDATA[
arXiv:2507.11181v2 Announce Type: replace 
Abstract: This paper presents a comprehensive review of the Mixture-of-Experts (MoE) architecture in large language models, highlighting its ability to significantly enhance model performance while maintaining minimal computational overhead. Through a systematic analysis spanning theoretical foundations, core architectural designs, and large language model (LLM) applications, we examine expert gating and routing mechanisms, hierarchical and sparse MoE configurations, meta-learning approaches, multimodal and multitask learning scenarios, real-world deployment cases, and recent advances and challenges in deep learning. Our analysis identifies key advantages of MoE, including superior model capacity compared to equivalent Bayesian approaches, improved task-specific performance, and the ability to scale model capacity efficiently. We also underscore the importance of ensuring expert diversity, accurate calibration, and reliable inference aggregation, as these are essential for maximizing the effectiveness of MoE architectures. Finally, this review outlines current research limitations, open challenges, and promising future directions, providing a foundation for continued innovation in MoE architecture and its applications.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>C3RL: Rethinking the Combination of Channel-independence and Channel-mixing from Representation Learning</title>
<link>https://arxiv.org/abs/2507.17454</link>
<guid>https://arxiv.org/abs/2507.17454</guid>
<content:encoded><![CDATA[
arXiv:2507.17454v2 Announce Type: replace 
Abstract: Multivariate time series forecasting has drawn increasing attention due to its practical importance. Existing approaches typically adopt either channel-mixing (CM) or channel-independence (CI) strategies. CM strategy can capture inter-variable dependencies but fails to discern variable-specific temporal patterns. CI strategy improves this aspect but fails to fully exploit cross-variable dependencies like CM. Hybrid strategies based on feature fusion offer limited generalization and interpretability. To address these issues, we propose C3RL, a novel representation learning framework that jointly models both CM and CI strategies. Motivated by contrastive learning in computer vision, C3RL treats the inputs of the two strategies as transposed views and builds a siamese network architecture: one strategy serves as the backbone, while the other complements it. By jointly optimizing contrastive and prediction losses with adaptive weighting, C3RL balances representation and forecasting performance. Extensive experiments on seven models show that C3RL boosts the best-case performance rate to 81.4% for models based on CI strategy and to 76.3% for models based on CM strategy, demonstrating strong generalization and effectiveness.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Treatment Policies From Multimodal Electronic Health Records</title>
<link>https://arxiv.org/abs/2507.20993</link>
<guid>https://arxiv.org/abs/2507.20993</guid>
<content:encoded><![CDATA[
arXiv:2507.20993v2 Announce Type: replace 
Abstract: We study how to learn effective treatment policies from multimodal electronic health records (EHRs) that consist of tabular data and clinical text. These policies can help physicians make better treatment decisions and allocate healthcare resources more efficiently. Causal policy learning methods prioritize patients with the largest expected treatment benefit. Yet, existing estimators assume tabular covariates that satisfy strong causal assumptions, which are typically violated in the multimodal setting. As a result, predictive models of baseline risk are commonly used in practice to guide such decisions, as they extend naturally to multimodal data. However, such risk-based policies are not designed to identify which patients benefit most from treatment. We propose an extension of causal policy learning that uses expert-provided annotations during training to supervise treatment effect estimation, while using only multimodal representations as input during inference. We show that the proposed method achieves strong empirical performance across synthetic, semi-synthetic, and real-world EHR datasets, thereby offering practical insights into applying causal machine learning to realistic clinical data.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Membership Inference Attack with Partial Features</title>
<link>https://arxiv.org/abs/2508.06244</link>
<guid>https://arxiv.org/abs/2508.06244</guid>
<content:encoded><![CDATA[
arXiv:2508.06244v2 Announce Type: replace 
Abstract: Machine learning models are vulnerable to membership inference attack, which can be used to determine whether a given sample appears in the training data. Most existing methods assume the attacker has full access to the features of the target sample. This assumption, however, does not hold in many real-world scenarios where only partial features are available, thereby limiting the applicability of these methods. In this work, we introduce Partial Feature Membership Inference (PFMI), a scenario where the adversary observes only partial features of each sample and aims to infer whether this observed subset was present in the training set. To address this problem, we propose MRAD (Memory-guided Reconstruction and Anomaly Detection), a two-stage attack framework that works in both white-box and black-box settings. In the first stage, MRAD leverages the latent memory of the target model to reconstruct the unknown features of the sample. We observe that when the known features are absent from the training set, the reconstructed sample deviates significantly from the true data distribution. Consequently, in the second stage, we use anomaly detection algorithms to measure the deviation between the reconstructed sample and the training data distribution, thereby determining whether the known features belong to a member of the training set. Empirical results demonstrate that MRAD is effective across various datasets, and maintains compatibility with off-the-shelf anomaly detection techniques. For example, on STL-10, our attack exceeds an AUC of around 0.75 even with 60% of the missing features.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Storage-Aware Learning with Compressed Data An Empirical Exploratory Study on JPEG</title>
<link>https://arxiv.org/abs/2508.12833</link>
<guid>https://arxiv.org/abs/2508.12833</guid>
<content:encoded><![CDATA[
arXiv:2508.12833v2 Announce Type: replace 
Abstract: On-device machine learning is often constrained by limited storage, particularly in continuous data collection scenarios. This paper presents an empirical study on storage-aware learning, focusing on the trade-off between data quantity and quality via compression. We demonstrate that naive strategies, such as uniform data dropping or one-size-fits-all compression, are suboptimal. Our findings further reveal that data samples exhibit varying sensitivities to compression, supporting the feasibility of a sample-wise adaptive compression strategy. These insights provide a foundation for developing a new class of storage-aware learning systems. The primary contribution of this work is the systematic characterization of this under-explored challenge, offering valuable insights that advance the understanding of storage-aware learning.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedReFT: Federated Representation Fine-Tuning with All-But-Me Aggregation</title>
<link>https://arxiv.org/abs/2508.20295</link>
<guid>https://arxiv.org/abs/2508.20295</guid>
<content:encoded><![CDATA[
arXiv:2508.20295v2 Announce Type: replace 
Abstract: Parameter-efficient fine-tuning (PEFT) adapts large pre-trained models by updating only a small subset of parameters. Recently, Representation Fine-Tuning (ReFT) has emerged as an effective alternative. ReFT shifts the fine-tuning paradigm from updating model weights to directly manipulating hidden representations that capture rich semantic information, and outperforms state-of-the-art PEFTs in standalone settings. However, its application in Federated Learning (FL) remains challenging due to heterogeneity in clients' data distributions, model capacities, and computational resources. To address these challenges, we introduce Federated Representation Fine-Tuning (FedReFT), a novel approach to fine-tune clients' hidden representations. FedReFT applies sparse intervention layers to steer hidden representations directly, offering a lightweight and semantically rich fine-tuning alternative ideal for edge devices. However, representation-level updates are especially vulnerable to aggregation mismatch under different task heterogeneity, where naive averaging can corrupt semantic alignment. To mitigate this issue, we propose All-But-Me (ABM) aggregation, where each client receives the aggregated updates of others and partially incorporates them, enabling stable and personalized learning by balancing local focus with global knowledge. We further design an adaptive update strategy inspired by Test-Time Computing (TTC) to balance local and global contributions under heterogeneous conditions. FedReFT achieves state-of-the-art performance on commonsense reasoning, arithmetic reasoning, and GLUE benchmarks, while delivering 1-49 times higher parameter efficiency compared to leading LoRA-based methods.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI for Scientific Discovery is a Social Problem</title>
<link>https://arxiv.org/abs/2509.06580</link>
<guid>https://arxiv.org/abs/2509.06580</guid>
<content:encoded><![CDATA[
arXiv:2509.06580v4 Announce Type: replace 
Abstract: Artificial intelligence (AI) is increasingly applied to scientific research, but its benefits remain unevenly distributed across communities and disciplines. While technical challenges such as limited data, fragmented standards, and unequal access to computational resources exist, social and institutional factors are often the primary constraints. Narratives emphasizing autonomous "AI scientists," under-recognition of data and infrastructure work, misaligned incentives, and gaps between domain experts and machine learning researchers all limit the impact of AI on scientific discovery. This paper highlights four interconnected challenges: community coordination, misalignment of research priorities with upstream needs, data fragmentation, and infrastructure inequities. We argue that addressing these challenges requires not only technical innovation but also intentional efforts in community-building, cross-disciplinary education, shared benchmarks, and accessible infrastructure. We call for reframing AI for science as a collective social project, where sustainable collaboration and equitable participation are treated as prerequisites for technical progress
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoupling the "What" and "Where" With Polar Coordinate Positional Embeddings</title>
<link>https://arxiv.org/abs/2509.10534</link>
<guid>https://arxiv.org/abs/2509.10534</guid>
<content:encoded><![CDATA[
arXiv:2509.10534v2 Announce Type: replace 
Abstract: The attention mechanism in a Transformer architecture matches key to query based on both content -- the what -- and position in a sequence -- the where. We present an analysis indicating that what and where are entangled in the popular RoPE rotary position embedding. This entanglement can impair performance particularly when decisions require independent matches on these two factors. We propose an improvement to RoPE, which we call Polar Coordinate Position Embeddings or PoPE, that eliminates the what-where confound. PoPE is far superior on a diagnostic task requiring indexing solely by position or by content. On autoregressive sequence modeling in music, genomic, and natural language domains, Transformers using PoPE as the positional encoding scheme outperform baselines using RoPE with respect to evaluation loss (perplexity) and downstream task performance. On language modeling, these gains persist across model scale, from 124M to 774M parameters. Crucially, PoPE shows strong zero-shot length extrapolation capabilities compared not only to RoPE but even a method designed for extrapolation, YaRN, which requires additional fine tuning and frequency interpolation.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ORACLE: Explaining Feature Interactions in Neural Networks with ANOVA</title>
<link>https://arxiv.org/abs/2509.10825</link>
<guid>https://arxiv.org/abs/2509.10825</guid>
<content:encoded><![CDATA[
arXiv:2509.10825v3 Announce Type: replace 
Abstract: We introduce ORACLE, a framework for explaining neural networks on tabular data and scientific factorial designs. ORACLE summarizes a trained network's prediction surface with main effects and pairwise interactions by treating the network as a black-box response, discretizing the inputs onto a grid, and fitting an orthogonal factorial (ANOVA-style) surrogate -- the $L^2$ orthogonal projection of the model response onto a finite-dimensional factorial subspace. A simple centering and $\mu$-rebalancing step then expresses this surrogate as main- and interaction-effect tables that remain faithful to the original model in the $L^2$ sense. The resulting grid-based interaction maps are easy to visualize, comparable across backbones, and directly aligned with classical design-of-experiments practice. On synthetic factorial benchmarks and low- to medium-dimensional tabular regression tasks, ORACLE more accurately recovers ground-truth interaction structure and hotspots than Monte Carlo SHAP-family interaction methods, as measured by ranking, localization, and cross-backbone stability. In latent image and text settings, ORACLE clarifies its scope: grid-based factorial surrogates are most effective when features admit an interpretable factorial structure, making ORACLE particularly well-suited to scientific and engineering workflows that require stable, DoE-style interaction summaries.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Latent learning: episodic memory complements parametric learning by enabling flexible reuse of experiences</title>
<link>https://arxiv.org/abs/2509.16189</link>
<guid>https://arxiv.org/abs/2509.16189</guid>
<content:encoded><![CDATA[
arXiv:2509.16189v3 Announce Type: replace 
Abstract: When do machine learning systems fail to generalize, and what mechanisms could improve their generalization? Here, we draw inspiration from cognitive science to argue that one weakness of parametric machine learning systems is their failure to exhibit latent learning -- learning information that is not relevant to the task at hand, but that might be useful in a future task. We show how this perspective links failures ranging from the reversal curse in language modeling to new findings on agent-based navigation. We then highlight how cognitive science points to episodic memory as a potential part of the solution to these issues. Correspondingly, we show that a system with an oracle retrieval mechanism can use learning experiences more flexibly to generalize better across many of these challenges. We also identify some of the essential components for effectively using retrieval, including the importance of within-example in-context learning for acquiring the ability to use information across retrieved examples. In summary, our results illustrate one possible contributor to the relative data inefficiency of current machine learning systems compared to natural intelligence, and help to understand how retrieval methods can complement parametric learning to improve generalization. We close by discussing some of the links between these findings and prior results in cognitive science and neuroscience, and the broader implications.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stochastic activations</title>
<link>https://arxiv.org/abs/2509.22358</link>
<guid>https://arxiv.org/abs/2509.22358</guid>
<content:encoded><![CDATA[
arXiv:2509.22358v2 Announce Type: replace 
Abstract: We introduce stochastic activations. This novel strategy randomly selects between several non-linear functions in the feed-forward layer of a large language model. In particular, we choose between SILU or RELU depending on a Bernoulli draw. This strategy circumvents the optimization problem associated with RELU, namely, the constant shape for negative inputs that prevents the gradient flow. We leverage this strategy in two ways:
  (1) We use stochastic activations during pre-training and fine-tune the model with RELU, which is used at inference time to provide sparse latent vectors. This reduces the inference FLOPs and translates into a significant speedup in the CPU. Interestingly, this leads to much better results than training from scratch with the RELU activation function.
  (2) We evaluate stochastic activations for generation. This strategy performs reasonably well: it is only slightly inferior to the best deterministic non-linearity, namely SILU combined with temperature scaling. This offers an alternative to existing strategies by providing a controlled way to increase the diversity of the generated text.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>C$^2$GSPG: Confidence-calibrated Group Sequence Policy Gradient towards Self-aware Reasoning</title>
<link>https://arxiv.org/abs/2509.23129</link>
<guid>https://arxiv.org/abs/2509.23129</guid>
<content:encoded><![CDATA[
arXiv:2509.23129v2 Announce Type: replace 
Abstract: Reinforcement Learning (RL) methods, exemplified by Group Relative Policy Optimization (GRPO) and its variants, play a central role in developing reasoning models. However, these methods often suffer from a critical overconfidence issue, which prevents them from achieving self-aware reasoning models. In this study, we propose a simple yet effective confidence-calibration group sequence policy gradient method, called C$^2$GSPG, which simultaneously enhances reasoning performance while suppressing overconfidence. In principle, we propose a Group Sequence Policy Gradient (GSPG) framework for learning reasoning models, which eliminates the token-level bias commonly appearing in GRPO and its variants. In this framework, we define the model confidence for each reasoning problem using the normalized sequence-level probability, and then apply a cross-entropy regularizer to calibrate the model confidence to the sequence's reward. We demonstrate that the confidence calibration regularizer and GSPG are collaborative for binary rewards, as their objectives always share the same gradient direction. For non-binary rewards, we apply nonlinear reward normalization and adaptive regularizer clipping, mitigating the potential conflict between the two objectives. Applying C$^2$GSPG to post-train large language models in logical and mathematical reasoning tasks, we show its superiority over state-of-the-art methods in both reasoning accuracy and confidence calibration. The code of C$^2$GSPG is available at https://github.com/HaotianLiu123/CCGSPG.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PATCH: Learnable Tile-level Hybrid Sparsity for LLMs</title>
<link>https://arxiv.org/abs/2509.23410</link>
<guid>https://arxiv.org/abs/2509.23410</guid>
<content:encoded><![CDATA[
arXiv:2509.23410v3 Announce Type: replace 
Abstract: Large language models (LLMs) deliver impressive performance but incur prohibitive memory and compute costs at deployment. Model pruning is an effective way to reduce these overheads, yet existing approaches face challenges: unstructured sparsity, where nonzeros can appear anywhere, preserves accuracy but yields irregular access patterns that prevent GPU acceleration, while semi-structured 2:4 sparsity is hardware-friendly but enforces a rigid 50% pattern that degrades model quality. To bridge this gap, we introduce PATCH, a hybrid sparsity framework that enables a continuous sparsity ratio between 0% and 50%. PATCH partitions weight matrices into tiles, assigning each tile to be either dense or 2:4 sparse via a learnable mask selection mechanism. This design provides fine-grained control over accuracy-acceleration tradeoffs and supports non-uniform sparsity across layers, leading to superior overall quality. Across models from 0.5B to 8B parameters, PATCH consistently narrows the gap to dense accuracy while delivering practical speedups. For instance, on LLaMA-2 7B with an A6000 GPU, PATCH achieves 1.18x-1.38x end-to-end speedup over dense baselines while improving accuracy by 0.37%-2.96% compared to the state-of-the-art 2:4 pruning method, MaskLLM.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta-Router: Bridging Gold-standard and Preference-based Evaluations in Large Language Model Routing</title>
<link>https://arxiv.org/abs/2509.25535</link>
<guid>https://arxiv.org/abs/2509.25535</guid>
<content:encoded><![CDATA[
arXiv:2509.25535v2 Announce Type: replace 
Abstract: In language tasks that require extensive human--model interaction, deploying a single "best" model for every query can be expensive. To reduce inference cost while preserving the quality of the responses, a large language model (LLM) router selects the most appropriate model from a pool of candidates for each query. A central challenge to training a high-quality router is the scarcity of reliable supervision. Gold-standard data (e.g., expert-verified labels or rubric-based scores) provide accurate quality evaluations of LLM responses but are costly and difficult to scale. In contrast, preference-based data, collected via crowdsourcing or LLM-as-a-judge systems, are cheaper and more scalable, yet often biased in reflecting the true quality of responses. We cast the problem of LLM router training with combined gold-standard and preference-based data into a causal inference framework by viewing the response evaluation mechanism as the treatment assignment. This perspective further reveals that the bias in preference-based data corresponds to the well-known causal estimand: the conditional average treatment effect. Based on this new perspective, we develop an integrative causal router training framework that corrects preference-data bias, address imbalances between two data sources, and improve routing robustness and efficiency. Numerical experiments demonstrate that our approach delivers more accurate routing and improves the trade-off between cost and quality.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Why mask diffusion does not work</title>
<link>https://arxiv.org/abs/2510.03289</link>
<guid>https://arxiv.org/abs/2510.03289</guid>
<content:encoded><![CDATA[
arXiv:2510.03289v2 Announce Type: replace 
Abstract: The main advantages of diffusion language models over autoregressive (AR) models lie in their ability to support parallel generation and bidirectional attention, enabling a more controllable generation process. In recent years, open-source mask diffusion language models have emerged, most of which are based on a variant known as absorbing diffusion. However, this paper demonstrates why mask diffusion faces inherent difficulties in achieving parallel generation and bidirectional attention. We also propose the most effective training and inference strategies for mask diffusion.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Structured State-Space Duality</title>
<link>https://arxiv.org/abs/2510.04944</link>
<guid>https://arxiv.org/abs/2510.04944</guid>
<content:encoded><![CDATA[
arXiv:2510.04944v2 Announce Type: replace 
Abstract: Structured State-Space Duality (SSD) [Dao & Gu, ICML 2024] is an equivalence between a simple Structured State-Space Model (SSM) and a masked attention mechanism. In particular, a state-space model with a scalar-times-identity state matrix is equivalent to a masked self-attention with a $1$-semiseparable causal mask. Consequently, the same sequence transformation (model) has two algorithmic realizations: as a linear-time $O(T)$ recurrence or as a quadratic-time $O(T^2)$ attention. In this note, we formalize and generalize this duality: (i) we extend SSD from the scalar-identity case to general diagonal SSMs (diagonal state matrices); (ii) we show that these diagonal SSMs match the scalar case's training complexity lower bounds while supporting richer dynamics; (iii) we establish a necessary and sufficient condition under which an SSM is equivalent to $1$-semiseparable masked attention; and (iv) we show that such duality fails to extend to standard softmax attention due to rank explosion. Together, these results tighten bridge between recurrent SSMs and Transformers, and widen the design space for expressive yet efficient sequence models.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Low Rank Attention for Long-Context Inference in Large Language Models</title>
<link>https://arxiv.org/abs/2510.23649</link>
<guid>https://arxiv.org/abs/2510.23649</guid>
<content:encoded><![CDATA[
arXiv:2510.23649v3 Announce Type: replace 
Abstract: As the length of input text increases, the key-value (KV) cache in LLMs imposes prohibitive GPU memory costs and limits long-context inference on resource constrained devices. Existing approaches, such as KV quantization and pruning, reduce memory usage but suffer from numerical precision loss or suboptimal retention of key-value pairs. In this work, Low Rank Query and Key attention (LRQK) is introduced, a two-stage framework that jointly decomposes full-precision query and key matrices into compact rank-\(r\) factors during the prefill stage, and then employs these low-dimensional projections to compute proxy attention scores in \(\mathcal{O}(lr)\) time at each decode step. By selecting only the top-\(k\) tokens and a small fixed set of recent tokens, LRQK employs a mixed GPU-CPU cache with a hit-and-miss mechanism where only missing full-precision KV pairs are transferred, thereby preserving exact attention outputs while reducing CPU-GPU data movement. Extensive experiments on the RULER and LongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK matches or surpasses leading sparse-attention methods in long context settings, while delivering significant memory savings with minimal accuracy loss. Our code is available at https://github.com/tenghuilee/LRQK.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Equivalence of Context and Parameter Updates in Modern Transformer Blocks</title>
<link>https://arxiv.org/abs/2511.17864</link>
<guid>https://arxiv.org/abs/2511.17864</guid>
<content:encoded><![CDATA[
arXiv:2511.17864v2 Announce Type: replace 
Abstract: Recent research has established that the impact of context in a vanilla transformer can be represented implicitly by forming a token-dependent, rank-1 patch to its MLP weights. This work extends that foundational theory to the diverse architectures of modern Large Language Models. We first demonstrate a precise, analytical solution for a Gemma-style transformer block, proving that the entire effect of a context can be perfectly mapped to rank-1 patches on its MLP weight matrices and a patch to the RMSNorm scale. We then generalize this result, providing a constructive proof and algorithm for multi-layer models. To unify these findings, we introduce a general framework centered on two core properties: input controllability and output controllability. We prove that a perfect implicit weight patch is possible for any MLP block where the inner function is input-controllable and the outer function is output-controllable. This provides a simpler and more powerful lens for understanding how transformer models transmute prompts into effective weights. This setup generalizes to a wide range of modern LLM architectures including gating, pre-/post-norm, mixture of experts and sequential/parallel transformer blocks.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Statistically-Guided Dual-Domain Meta-Learning with Adaptive Multi-Prototype Aggregation for Distributed Fiber Optic Sensing</title>
<link>https://arxiv.org/abs/2511.17902</link>
<guid>https://arxiv.org/abs/2511.17902</guid>
<content:encoded><![CDATA[
arXiv:2511.17902v2 Announce Type: replace 
Abstract: Distributed Fiber Optic Sensing (DFOS) is promising for long-range perimeter security, yet practical deployment faces three key obstacles: severe cross-deployment domain shift, scarce or unavailable labels at new sites, and limited within-class coverage even in source deployments. We propose DUPLE, a prototype-based meta-learning framework tailored for cross-deployment DFOS recognition. The core idea is to jointly exploit complementary time- and frequency-domain cues and adapt class representations to sample-specific statistics: (i) a dual-domain learner constructs multi-prototype class representations to cover intra-class heterogeneity; (ii) a lightweight statistical guidance mechanism estimates the reliability of each domain from raw signal statistics; and (iii) a query-adaptive aggregation strategy selects and combines the most relevant prototypes for each query. Extensive experiments on two real-world cross-deployment benchmarks demonstrate consistent improvements over strong deep learning and meta-learning baselines, achieving more accurate and stable recognition under label-scarce target deployments.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Categorical Equivariant Deep Learning: Category-Equivariant Neural Networks and Universal Approximation Theorems</title>
<link>https://arxiv.org/abs/2511.18417</link>
<guid>https://arxiv.org/abs/2511.18417</guid>
<content:encoded><![CDATA[
arXiv:2511.18417v2 Announce Type: replace 
Abstract: We develop a theory of category-equivariant neural networks (CENNs) that unifies group/groupoid-equivariant networks, poset/lattice-equivariant networks, graph and sheaf neural networks. Equivariance is formulated as naturality in a topological category with Radon measures. Formulating linear and nonlinear layers in the categorical setup, we prove the equivariant universal approximation theorem in the general setting: the class of finite-depth CENNs is dense in the space of continuous equivariant transformations. We instantiate the framework for groups/groupoids, posets/lattices, graphs and cellular sheaves, deriving universal approximation theorems for them in a systematic manner. Categorical equivariant deep learning thus allows us to expand the horizons of equivariant deep learning beyond group actions, encompassing not only geometric symmetries but also contextual and compositional symmetries.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>No Trust Issues Here: A Technical Report on the Winning Solutions for the Rayan AI Contest</title>
<link>https://arxiv.org/abs/2512.01498</link>
<guid>https://arxiv.org/abs/2512.01498</guid>
<content:encoded><![CDATA[
arXiv:2512.01498v2 Announce Type: replace 
Abstract: This report presents solutions to three machine learning challenges developed as part of the Rayan AI Contest: compositional image retrieval, zero-shot anomaly detection, and backdoored model detection. In compositional image retrieval, we developed a system that processes visual and textual inputs to retrieve relevant images, achieving 95.38% accuracy and ranking first with a clear margin over the second team. For zero-shot anomaly detection, we designed a model that identifies and localizes anomalies in images without prior exposure to abnormal examples, securing second place with a 73.14% score. In the backdoored model detection task, we proposed a method to detect hidden backdoor triggers in neural networks, reaching an accuracy of 78%, which placed our approach in second place. These results demonstrate the effectiveness of our methods in addressing key challenges related to retrieval, anomaly detection, and model security, with implications for real-world applications in industries such as healthcare, manufacturing, and cybersecurity. Code for all solutions is available online (https://github.com/safinal/rayan-ai-contest-solutions).
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Low-Tubal-Rank Tensor Estimation via Alternating Preconditioned Gradient Descent</title>
<link>https://arxiv.org/abs/2512.07490</link>
<guid>https://arxiv.org/abs/2512.07490</guid>
<content:encoded><![CDATA[
arXiv:2512.07490v3 Announce Type: replace 
Abstract: The problem of low-tubal-rank tensor estimation is a fundamental task with wide applications across high-dimensional signal processing, machine learning, and image science. Traditional approaches tackle such a problem by performing tensor singular value decomposition, which is computationally expensive and becomes infeasible for large-scale tensors. Recent approaches address this issue by factorizing the tensor into two smaller factor tensors and solving the resulting problem using gradient descent. However, this kind of approach requires an accurate estimate of the tensor rank, and when the rank is overestimated, the convergence of gradient descent and its variants slows down significantly or even diverges. To address this problem, we propose an Alternating Preconditioned Gradient Descent (APGD) algorithm, which accelerates convergence in the over-parameterized setting by adding a preconditioning term to the original gradient and updating these two factors alternately. Based on certain geometric assumptions on the objective function, we establish linear convergence guarantees for more general low-tubal-rank tensor estimation problems. Then we further analyze the specific cases of low-tubal-rank tensor factorization and low-tubal-rank tensor recovery. Our theoretical results show that APGD achieves linear convergence even under over-parameterization, and the convergence rate is independent of the tensor condition number. Extensive simulations on synthetic data are carried out to validate our theoretical assertions.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training LLMs for Honesty via Confessions</title>
<link>https://arxiv.org/abs/2512.08093</link>
<guid>https://arxiv.org/abs/2512.08093</guid>
<content:encoded><![CDATA[
arXiv:2512.08093v2 Announce Type: replace 
Abstract: Large language models (LLMs) can be dishonest when reporting on their actions and beliefs -- for example, they may overstate their confidence in factual claims or cover up evidence of covert actions. Such dishonesty may arise due to the effects of reinforcement learning (RL), where challenges with reward shaping can result in a training process that inadvertently incentivizes the model to lie or misrepresent its actions.
  In this work we propose a method for eliciting an honest expression of an LLM's shortcomings via a self-reported *confession*. A confession is an output, provided upon request after a model's original answer, that is meant to serve as a full account of the model's compliance with the letter and spirit of its policies and instructions. The reward assigned to a confession during training is solely based on its honesty, and does not impact positively or negatively the main answer's reward. As long as the "path of least resistance" for maximizing confession reward is to surface misbehavior rather than covering it up, this incentivizes models to be honest in their confessions. Our findings provide some justification this empirical assumption, especially in the case of egregious model misbehavior.
  To demonstrate the viability of our approach, we train GPT-5-Thinking to produce confessions, and we evaluate its honesty in out-of-distribution scenarios measuring hallucination, instruction following, scheming, and reward hacking. We find that when the model lies or omits shortcomings in its "main" answer, it often confesses to these behaviors honestly, and this confession honesty modestly improves with training. Confessions can enable a number of inference-time interventions including monitoring, rejection sampling, and surfacing issues to the user.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reinforcement Learning From State and Temporal Differences</title>
<link>https://arxiv.org/abs/2512.08855</link>
<guid>https://arxiv.org/abs/2512.08855</guid>
<content:encoded><![CDATA[
arXiv:2512.08855v2 Announce Type: replace 
Abstract: TD($\lambda$) with function approximation has proved empirically successful for some complex reinforcement learning problems. For linear approximation, TD($\lambda$) has been shown to minimise the squared error between the approximate value of each state and the true value. However, as far as policy is concerned, it is error in the relative ordering of states that is critical, rather than error in the state values. We illustrate this point, both in simple two-state and three-state systems in which TD($\lambda$)--starting from an optimal policy--converges to a sub-optimal policy, and also in backgammon. We then present a modified form of TD($\lambda$), called STD($\lambda$), in which function approximators are trained with respect to relative state values on binary decision problems. A theoretical analysis, including a proof of monotonic policy improvement for STD($\lambda$) in the context of the two-state system, is presented, along with a comparison with Bertsekas' differential training method [1]. This is followed by successful demonstrations of STD($\lambda$) on the two-state system and a variation on the well known acrobot problem.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Information Routing for Multimodal Time Series Forecasting</title>
<link>https://arxiv.org/abs/2512.10229</link>
<guid>https://arxiv.org/abs/2512.10229</guid>
<content:encoded><![CDATA[
arXiv:2512.10229v3 Announce Type: replace 
Abstract: Time series forecasting is a critical task for artificial intelligence with numerous real-world applications. Traditional approaches primarily rely on historical time series data to predict the future values. However, in practical scenarios, this is often insufficient for accurate predictions due to the limited information available. To address this challenge, multimodal time series forecasting methods which incorporate additional data modalities, mainly text data, alongside time series data have been explored. In this work, we introduce the Adaptive Information Routing (AIR) framework, a novel approach for multimodal time series forecasting. Unlike existing methods that treat text data on par with time series data as interchangeable auxiliary features for forecasting, AIR leverages text information to dynamically guide the time series model by controlling how and to what extent multivariate time series information should be combined. We also present a text-refinement pipeline that employs a large language model to convert raw text data into a form suitable for multimodal forecasting, and we introduce a benchmark that facilitates multimodal forecasting experiments based on this pipeline. Experiment results with the real world market data such as crude oil price and exchange rates demonstrate that AIR effectively modulates the behavior of the time series model using textual inputs, significantly enhancing forecasting accuracy in various time series forecasting tasks.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Estimating Graph Dimension with Cross-validated Eigenvalues</title>
<link>https://arxiv.org/abs/2108.03336</link>
<guid>https://arxiv.org/abs/2108.03336</guid>
<content:encoded><![CDATA[
arXiv:2108.03336v2 Announce Type: replace-cross 
Abstract: In applied multivariate statistics, estimating the number of latent dimensions or the number of clusters, $k$, is a fundamental and recurring problem. We study a sequence of statistics called "cross-validated eigenvalues." Under a large class of random graph models, including both Poisson and Bernoulli edges, without parametric assumptions, we provide a $p$-value for each cross-validated eigenvalue. It tests the null hypothesis that the sample eigenvector is orthogonal to (i.e., uncorrelated with) the true latent dimensions. This approach naturally adapts to problems where some dimensions are not statistically detectable. In scenarios where all $k$ dimensions can be estimated, we show that our procedure consistently estimates $k$. In simulations and data example, the proposed estimator compares favorably to alternative approaches in both computational and statistical performance.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Algorithmic Aspects of the Log-Laplace Transform and a Non-Euclidean Proximal Sampler</title>
<link>https://arxiv.org/abs/2302.06085</link>
<guid>https://arxiv.org/abs/2302.06085</guid>
<content:encoded><![CDATA[
arXiv:2302.06085v3 Announce Type: replace-cross 
Abstract: The development of efficient sampling algorithms catering to non-Euclidean geometries has been a challenging endeavor, as discretization techniques which succeed in the Euclidean setting do not readily carry over to more general settings. We develop a non-Euclidean analog of the recent proximal sampler of [LST21], which naturally induces regularization by an object known as the log-Laplace transform (LLT) of a density. We prove new mathematical properties (with an algorithmic flavor) of the LLT, such as strong convexity-smoothness duality and an isoperimetric inequality, which are used to prove a mixing time on our proximal sampler matching [LST21] under a warm start. As our main application, we show our warm-started sampler improves the value oracle complexity of differentially private convex optimization in $\ell_p$ and Schatten-$p$ norms for $p \in [1, 2]$ to match the Euclidean setting [GLL22], while retaining state-of-the-art excess risk bounds [GLLST23]. We find our investigation of the LLT to be a promising proof-of-concept of its utility as a tool for designing samplers, and outline directions for future exploration.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Behavioral Machine Learning? Regularization and Forecast Bias</title>
<link>https://arxiv.org/abs/2303.16158</link>
<guid>https://arxiv.org/abs/2303.16158</guid>
<content:encoded><![CDATA[
arXiv:2303.16158v4 Announce Type: replace-cross 
Abstract: Standard forecast efficiency tests interpret violations as evidence of behavioral bias. We show theoretically and empirically that rational forecasters using optimal regularization systematically violate these tests. Machine learning forecasts show near zero bias at one year horizon, but strong overreaction at two years, consistent with predictions from a model of regularization and measurement noise. We provide three complementary tests: experimental variation in regularization parameters, cross-sectional heterogeneity in firm signal quality, and quasi-experimental evidence from ML adoption around 2013. Technically trained analysts shift sharply toward overreaction post-2013. Our findings suggest reported violations may reflect statistical sophistication rather than cognitive failure.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Boosted Control Functions: Distribution generalization and invariance in confounded models</title>
<link>https://arxiv.org/abs/2310.05805</link>
<guid>https://arxiv.org/abs/2310.05805</guid>
<content:encoded><![CDATA[
arXiv:2310.05805v3 Announce Type: replace-cross 
Abstract: Modern machine learning methods and the availability of large-scale data have significantly advanced our ability to predict target quantities from large sets of covariates. However, these methods often struggle under distributional shifts, particularly in the presence of hidden confounding. While the impact of hidden confounding is well-studied in causal effect estimation, e.g., instrumental variables, its implications for prediction tasks under shifting distributions remain underexplored. This work addresses this gap by introducing a strong notion of invariance that, unlike existing weaker notions, allows for distribution generalization even in the presence of nonlinear, non-identifiable structural functions. Central to this framework is the Boosted Control Function (BCF), a novel, identifiable target of inference that satisfies the proposed strong invariance notion and is provably worst-case optimal under distributional shifts. The theoretical foundation of our work lies in Simultaneous Equation Models for Distribution Generalization (SIMDGs), which bridge machine learning with econometrics by describing data-generating processes under distributional shifts. To put these insights into practice, we propose the ControlTwicing algorithm to estimate the BCF using nonparametric machine-learning techniques and study its generalization performance on synthetic and real-world datasets compared to robust and empirical risk minimization approaches.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tactile-based Object Retrieval From Granular Media</title>
<link>https://arxiv.org/abs/2402.04536</link>
<guid>https://arxiv.org/abs/2402.04536</guid>
<content:encoded><![CDATA[
arXiv:2402.04536v3 Announce Type: replace-cross 
Abstract: We introduce GEOTACT, the first robotic system capable of grasping and retrieving objects of potentially unknown shapes buried in a granular environment. While important in many applications, ranging from mining and exploration to search and rescue, this type of interaction with granular media is difficult due to the uncertainty stemming from visual occlusion and noisy contact signals. To address these challenges, we use a learning method relying exclusively on touch feedback, trained end-to-end with simulated sensor noise. We show that our problem formulation leads to the natural emergence of learned pushing behaviors that the manipulator uses to reduce uncertainty and funnel the object to a stable grasp despite spurious and noisy tactile readings. We introduce a training curriculum that bootstraps learning in simulated granular environments, enabling zero-shot transfer to real hardware. Despite being trained only on seven objects with primitive shapes, our method is shown to successfully retrieve 35 different objects, including rigid, deformable, and articulated objects with complex shapes. Videos and additional information can be found at https://jxu.ai/geotact.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reinforcement Learning for Unsupervised Video Summarization with Reward Generator Training</title>
<link>https://arxiv.org/abs/2407.04258</link>
<guid>https://arxiv.org/abs/2407.04258</guid>
<content:encoded><![CDATA[
arXiv:2407.04258v2 Announce Type: replace-cross 
Abstract: This paper presents a novel approach for unsupervised video summarization using reinforcement learning (RL), addressing limitations like unstable adversarial training and reliance on heuristic-based reward functions. The method operates on the principle that reconstruction fidelity serves as a proxy for informativeness, correlating summary quality with reconstruction ability. The summarizer model assigns importance scores to frames to generate the final summary. For training, RL is coupled with a unique reward generation pipeline that incentivizes improved reconstructions. This pipeline uses a generator model to reconstruct the full video from the selected summary frames; the similarity between the original and reconstructed video provides the reward signal. The generator itself is pre-trained self-supervisedly to reconstruct randomly masked frames. This two-stage training process enhances stability compared to adversarial architectures. Experimental results show strong alignment with human judgments and promising F-scores, validating the reconstruction objective.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning and Machine Learning: Advancing Big Data Analytics and Management with Design Patterns</title>
<link>https://arxiv.org/abs/2410.03795</link>
<guid>https://arxiv.org/abs/2410.03795</guid>
<content:encoded><![CDATA[
arXiv:2410.03795v3 Announce Type: replace-cross 
Abstract: This book, Design Patterns in Machine Learning and Deep Learning: Advancing Big Data Analytics Management, presents a comprehensive study of essential design patterns tailored for large-scale machine learning and deep learning applications. The book explores the application of classical software engineering patterns, Creational, Structural, Behavioral, and Concurrency Patterns, to optimize the development, maintenance, and scalability of big data analytics systems. Through practical examples and detailed Python implementations, it bridges the gap between traditional object-oriented design patterns and the unique demands of modern data analytics environments. Key design patterns such as Singleton, Factory, Observer, and Strategy are analyzed for their impact on model management, deployment strategies, and team collaboration, providing invaluable insights into the engineering of efficient, reusable, and flexible systems. This volume is an essential resource for developers, researchers, and engineers aiming to enhance their technical expertise in both machine learning and software design.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Don't Pay Attention, PLANT It: Pretraining Attention via Learning-to-Rank</title>
<link>https://arxiv.org/abs/2410.23066</link>
<guid>https://arxiv.org/abs/2410.23066</guid>
<content:encoded><![CDATA[
arXiv:2410.23066v3 Announce Type: replace-cross 
Abstract: State-of-the-art Extreme Multi-Label Text Classification models rely on multi-label attention to focus on key tokens in input text, but learning good attention weights is challenging. We introduce PLANT - Pretrained and Leveraged Attention - a plug-and-play strategy for initializing attention. PLANT works by planting label-specific attention using a pretrained Learning-to-Rank model guided by mutual information gain. This architecture-agnostic approach integrates seamlessly with large language model backbones such as Mistral-7B, LLaMA3-8B, DeepSeek-V3, and Phi-3. PLANT outperforms state-of-the-art methods across tasks including ICD coding, legal topic classification, and content recommendation. Gains are especially pronounced in few-shot settings, with substantial improvements on rare labels. Ablation studies confirm that attention initialization is a key driver of these gains. For code and trained models, see https://github.com/debjyotiSRoy/xcube/tree/plant
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable deep learning improves human mental models of self-driving cars</title>
<link>https://arxiv.org/abs/2411.18714</link>
<guid>https://arxiv.org/abs/2411.18714</guid>
<content:encoded><![CDATA[
arXiv:2411.18714v2 Announce Type: replace-cross 
Abstract: Self-driving cars increasingly rely on deep neural networks to achieve human-like driving. The opacity of such black-box planners makes it challenging for the human behind the wheel to accurately anticipate when they will fail, with potentially catastrophic consequences. While research into interpreting these systems has surged, most of it is confined to simulations or toy setups due to the difficulty of real-world deployment, leaving the practical utility of such techniques unknown. Here, we introduce the Concept-Wrapper Network (CW-Net), a method for explaining the behavior of machine-learning-based planners by grounding their reasoning in human-interpretable concepts. We deploy CW-Net on a real self-driving car and show that the resulting explanations improve the human driver's mental model of the car, allowing them to better predict its behavior. To our knowledge, this is the first demonstration that explainable deep learning integrated into self-driving cars can be both understandable and useful in a realistic deployment setting. CW-Net accomplishes this level of intelligibility while providing explanations which are causally faithful and do not sacrifice driving performance. Overall, our study establishes a general pathway to interpretability for autonomous agents by way of concept-based explanations, which could help make them more transparent and safe.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Computational Basis of LLM's Decision Making in Social Simulation</title>
<link>https://arxiv.org/abs/2504.11671</link>
<guid>https://arxiv.org/abs/2504.11671</guid>
<content:encoded><![CDATA[
arXiv:2504.11671v4 Announce Type: replace-cross 
Abstract: Large language models (LLMs) increasingly serve as human-like decision-making agents in social science and applied settings. These LLM-agents are typically assigned human-like characters and placed in real-life contexts. However, how these characters and contexts shape an LLM's behavior remains underexplored. This study proposes and tests methods for probing, quantifying, and modifying an LLM's internal representations in a Dictator Game, a classic behavioral experiment on fairness and prosocial behavior. We extract ``vectors of variable variations'' (e.g., ``male'' to ``female'') from the LLM's internal state. Manipulating these vectors during the model's inference can substantially alter how those variables relate to the model's decision-making. This approach offers a principled way to study and regulate how social concepts can be encoded and engineered within transformer-based models, with implications for alignment, debiasing, and designing AI agents for social simulations in both academic and commercial applications, strengthening sociological theory and measurement.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Informative Attention Weights for Person Re-Identification</title>
<link>https://arxiv.org/abs/2505.08961</link>
<guid>https://arxiv.org/abs/2505.08961</guid>
<content:encoded><![CDATA[
arXiv:2505.08961v2 Announce Type: replace-cross 
Abstract: Attention mechanisms have been widely used in deep learning, and recent efforts have been devoted to incorporating attention modules into deep neural networks (DNNs) for person Re-Identification (Re-ID) to enhance their discriminative feature learning capabilities. Existing attention modules, including self-attention and channel attention, learn attention weights that quantify the importance of feature tokens or feature channels. However, existing attention methods do not explicitly ensure that the attention weights are informative for predicting the identity of the person in the input image, and may consequently introduce noisy information from the input image. To address this issue, we propose a novel method termed Reduction of Information Bottleneck loss (RIB), motivated by the principle of the Information Bottleneck (IB). A novel distribution-free and efficient variational upper bound for the IB loss (IBB), which can be optimized by standard SGD, is derived and incorporated into the training loss of the RIB models. RIB is applied to DNNs with self-attention modules through a novel Differentiable Channel Selection Attention module, or DCS-Attention, that selects the most informative channels for computing attention weights, leading to competitive models termed RIB-DCS. RIB is also incorporated into DNNs with existing channel attention modules to promote the learning of informative channel attention weights, leading to models termed RIB-CA. Both RIB-DCS and RIB-CA are applied to fixed neural network backbones and learnable backbones with Differentiable Neural Architecture Search (DNAS). Extensive experiments on multiple person Re-ID benchmarks show that RIB significantly enhances the prediction accuracy of DNNs for person Re-ID, even for the occluded person Re-ID.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Surprisingly High Redundancy in Electronic Structure Data</title>
<link>https://arxiv.org/abs/2507.09001</link>
<guid>https://arxiv.org/abs/2507.09001</guid>
<content:encoded><![CDATA[
arXiv:2507.09001v2 Announce Type: replace-cross 
Abstract: Accurate prediction of electronic structure underpins advances in chemistry, materials science, and condensed matter physics. In recent years, Machine Learning (ML) has enabled the development of powerful surrogate models that can enable the prediction of the ground state electron density and related properties at a fraction of the computational cost of conventional first principles simulations. Such ML models typically rely on massive datasets generated through expensive Kohn-Sham Density Functional Theory calculations. A key reason for relying on such large datasets is the lack of prior knowledge about which portions of the data are essential, and which are redundant. This study reveals significant redundancies in electronic structure datasets across various material systems, including molecules, simple metals, and chemically complex alloys -- challenging the notion that extensive datasets are essential for accurate ML-based electronic structure predictions. We demonstrate that even random pruning can substantially reduce dataset size with minimal loss in predictive accuracy. Furthermore, a state-of-the-art coverage-based pruning strategy that selects data across all learning difficulties, retains chemical accuracy and model generalizability using up to 100-fold less data, while reducing training time by threefold or greater. By contrast, widely used importance-based pruning methods, which eliminate easy-to-learn data, can catastrophically fail at higher pruning factors due to significant reduction in data coverage. This heretofore unexplored high redundancy in electronic structure data holds the potential to identify a minimal, essential dataset representative of each material class.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification</title>
<link>https://arxiv.org/abs/2507.11662</link>
<guid>https://arxiv.org/abs/2507.11662</guid>
<content:encoded><![CDATA[
arXiv:2507.11662v2 Announce Type: replace-cross 
Abstract: Verifiers--functions assigning rewards to agent behavior--have been key for AI progress in domains like math and code. However, extending gains to domains without clear-cut success criteria (e.g., computer use) remains a challenge: while humans can recognize desired outcomes, translating this intuition into scalable rules is nontrivial. Multimodal Large Language Models (MLLMs) emerge as a promising solution, given their world knowledge, human-preference alignment, and reasoning skills. We evaluate MLLMs as verifiers across web navigation, computer use, and robotic manipulation, and identify a critical limitation: a strong tendency to over-validate agent behavior, a phenomenon we term agreement bias. This bias is pervasive across models, resilient to test-time scaling, and poses risks to existing methods relying on MLLM evaluations. We discuss methods to evaluate and improve MLLM verifiers and introduce Self-Grounded Verification (SGV), a lightweight method that harnesses MLLMs' own sampling mechanisms by modulating (un)conditional generation to better leverage their knowledge, alignment, and reasoning. SGV operates in two steps: first, the MLLM is elicited to generate broad priors about desired behavior, independent of the data under evaluation. Then, conditioned on self-generated priors, it reasons over and evaluates a candidate trajectory. SGV yields more human-aligned evaluations with gains of up to 25pp in failure detection, 14pp in accuracy, and benefits extending to downstream applications. In self-refinement and online supervision, SGV boosts task completion of a GUI specialist in OSWorld, a diffusion policy in robomimic, and a ReAct agent in VisualWebArena--setting a new state of the art, surpassing the previous best by 20pp. We release an updated version of VisualWebArena featuring more human-aligned evaluators, high-fidelity environment parallelism, and speedups of over 10x.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning without training: The implicit dynamics of in-context learning</title>
<link>https://arxiv.org/abs/2507.16003</link>
<guid>https://arxiv.org/abs/2507.16003</guid>
<content:encoded><![CDATA[
arXiv:2507.16003v3 Announce Type: replace-cross 
Abstract: One of the most striking features of Large Language Models (LLMs) is their ability to learn in-context. Namely at inference time an LLM is able to learn new patterns without any additional weight update when these patterns are presented in the form of examples in the prompt, even if these patterns were not seen during training. The mechanisms through which this can happen are still largely unknown. In this work, we show that the stacking of a self-attention layer with an MLP, allows the transformer block to implicitly modify the weights of the MLP layer according to the context. We argue through theory and experimentation that this simple mechanism may be the reason why LLMs can learn in-context and not only during training. Specifically, we show how a transformer block implicitly transforms a context into a low-rank weight-update of its MLP layer.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deformable Cluster Manipulation via Whole-Arm Policy Learning</title>
<link>https://arxiv.org/abs/2507.17085</link>
<guid>https://arxiv.org/abs/2507.17085</guid>
<content:encoded><![CDATA[
arXiv:2507.17085v2 Announce Type: replace-cross 
Abstract: Manipulating clusters of deformable objects presents a substantial challenge with widespread applicability, but requires contact-rich whole-arm interactions. A potential solution must address the limited capacity for realistic model synthesis, high uncertainty in perception, and the lack of efficient spatial abstractions, among others. We propose a novel framework for learning model-free policies integrating two modalities: 3D point clouds and proprioceptive touch indicators, emphasising manipulation with full body contact awareness, going beyond traditional end-effector modes. Our reinforcement learning framework leverages a distributional state representation, aided by kernel mean embeddings, to achieve improved training efficiency and real-time inference. Furthermore, we propose a novel context-agnostic occlusion heuristic to clear deformables from a target region for exposure tasks. We deploy the framework in a power line clearance scenario and observe that the agent generates creative strategies leveraging multiple arm links for de-occlusion. Finally, we perform zero-shot sim-to-real policy transfer, allowing the arm to clear real branches with unknown occlusion patterns, unseen topology, and uncertain dynamics. Website: https://sites.google.com/view/dcmwap/
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BConformeR: A Conformer Based on Mutual Sampling for Unified Prediction of Continuous and Discontinuous Antibody Binding Sites</title>
<link>https://arxiv.org/abs/2508.12029</link>
<guid>https://arxiv.org/abs/2508.12029</guid>
<content:encoded><![CDATA[
arXiv:2508.12029v3 Announce Type: replace-cross 
Abstract: Accurate prediction of antibody-binding sites (epitopes) on antigens is crucial for vaccine design, immunodiagnostics, therapeutic antibody development, antibody engineering, research into autoimmune and allergic diseases, and advancing our understanding of immune responses. Despite in silico methods that have been proposed to predict both linear (continuous) and conformational (discontinuous) epitopes, they consistently underperform in predicting conformational epitopes. In this work, we propose Conformer-based models trained separately on AlphaFold-predicted structures and experimentally determined structures, leveraging convolutional neural networks (CNNs) to extract local features and Transformers to capture long-range dependencies within antigen sequences. Ablation studies demonstrate that CNN enhances the prediction of linear epitopes, and the Transformer module improves the prediction of conformational epitopes. Experimental results show that our model outperforms existing baselines in terms of MCC, ROC-AUC, PR-AUC, and F1 scores on both linear and conformational epitopes.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spectral Bottleneck in Sinusoidal Representation Networks: Noise is All You Need</title>
<link>https://arxiv.org/abs/2509.09719</link>
<guid>https://arxiv.org/abs/2509.09719</guid>
<content:encoded><![CDATA[
arXiv:2509.09719v2 Announce Type: replace-cross 
Abstract: This work identifies and attempts to address a fundamental limitation of implicit neural representations with sinusoidal activation. The fitting error of SIRENs is highly sensitive to the target frequency content and to the choice of initialization. In extreme cases, this sensitivity leads to a spectral bottleneck that can result in a zero-valued output. This phenomenon is characterized by analyzing the evolution of activation spectra and the empirical neural tangent kernel (NTK) during the training process. An unfavorable distribution of energy across frequency modes was noted to give rise to this failure mode. Furthermore, the effect of Gaussian perturbations applied to the baseline uniformly initialized weights is examined, showing how these perturbations influence activation spectra and the NTK eigenbasis of SIREN. Overall, initialization emerges as a central factor governing the evolution of SIRENs, indicating the need for adaptive, target-aware strategies as the target length increases and fine-scale detail becomes essential. The proposed weight initialization scheme (WINNER) represents a simple ad hoc step in this direction and demonstrates that fitting accuracy can be significantly improved by modifying the spectral profile of network activations through a target-aware initialization. The approach achieves state-of-the-art performance on audio fitting tasks and yields notable improvements in image fitting tasks.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-Intrusive Parametrized-Background Data-Weak Reconstruction of Cardiac Displacement Fields from Sparse MRI-like Observations</title>
<link>https://arxiv.org/abs/2509.14844</link>
<guid>https://arxiv.org/abs/2509.14844</guid>
<content:encoded><![CDATA[
arXiv:2509.14844v2 Announce Type: replace-cross 
Abstract: Personalized cardiac diagnostics require accurate reconstruction of myocardial displacement fields from sparse clinical imaging data, yet current methods often demand intrusive access to computational models. In this work, we apply the non-intrusive Parametrized-Background Data-Weak (PBDW) approach to three-dimensional (3D) cardiac displacement field reconstruction from limited Magnetic Resonance Image (MRI)-like observations. Our implementation requires only solution snapshots -- no governing equations, assembly routines, or solver access -- enabling immediate deployment across commercial and research codes using different constitutive models. Additionally, we introduce two enhancements: an H-size minibatch worst-case Orthogonal Matching Pursuit (wOMP) algorithm that improves Sensor Selection (SS) computational efficiency while maintaining reconstruction accuracy, and memory optimization techniques exploiting block matrix structures in vectorial problems. We demonstrate the effectiveness of the method through validation on a 3D left ventricular model with simulated scar tissue. Starting with noise-free reconstruction, we systematically incorporate Gaussian noise and spatial sparsity mimicking realistic MRI acquisition protocols. Results show exceptional accuracy in noise-free conditions (relative L2 error of order O(1e-5)), robust performance with 10% noise (relative L2 error of order O(1e-2)), and effective reconstruction from sparse measurements (relative L2 error of order O(1e-2)). The online reconstruction achieves four-order-of-magnitude computational speed-up compared to full Finite Element (FE) simulations, with reconstruction times under one tenth of second for sparse scenarios, demonstrating significant potential for integration into clinical cardiac modeling workflows.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Resolution scaling governs DINOv3 transfer performance in chest radiograph classification</title>
<link>https://arxiv.org/abs/2510.07191</link>
<guid>https://arxiv.org/abs/2510.07191</guid>
<content:encoded><![CDATA[
arXiv:2510.07191v2 Announce Type: replace-cross 
Abstract: Self-supervised learning (SSL) has advanced visual representation learning, but its value in chest radiography, a high-volume imaging modality with fine-grained findings, remains unclear. Meta's DINOv3 extends earlier SSL models through Gram-anchored self-distillation. Whether these design choices improve transfer learning for chest radiography has not been systematically tested. We benchmarked DINOv3 against DINOv2 and ImageNet initialization across seven datasets (n>814,000). Two representative backbones were evaluated: ViT-B/16 and ConvNeXt-B. Images were analyzed at 224x224, 512x512, and 1024x1024 pixels. We additionally assessed frozen features from a 7B model. The primary outcome was mean AUROC across labels. At 224x224, DINOv3 and DINOv2 achieved comparable performance on adult datasets. Increasing resolution to 512x512 yielded consistent improvements for DINOv3 over both DINOv2 and ImageNet. In contrast, results in pediatric cohort showed no differences across initializations. Across all settings, ConvNeXt-B outperformed ViT-B/16. Models using frozen DINOv3-7B features underperformed relative to fully finetuned 86-89M-parameter backbones, highlighting the importance of domain adaptation. Scaling to 1024x1024 did not further improve accuracy. Resolution-related gains were most evident for boundary-dependent and small focal abnormalities. In chest radiography, higher input resolution is critical for leveraging the benefits of modern self-supervised models. 512x512 pixels represent a practical upper limit where DINOv3-initialized ConvNeXt-B networks provide the strongest performance, while larger inputs offer minimal return on cost. Clinically, these findings support use of finetuned, mid-sized backbones at 512x512 for chest radiograph interpretation, with the greatest gains expected in detecting subtle or boundary-centered lesions relevant to emergency and critical care settings.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Speech Emotion Recognition with Mutual Information Regularized Generative Model</title>
<link>https://arxiv.org/abs/2510.10078</link>
<guid>https://arxiv.org/abs/2510.10078</guid>
<content:encoded><![CDATA[
arXiv:2510.10078v3 Announce Type: replace-cross 
Abstract: Lack of large, well-annotated emotional speech corpora continues to limit the performance and robustness of speech emotion recognition (SER), particularly as models grow more complex and the demand for multimodal systems increases. While generative data augmentation offers a promising solution, existing approaches often produce emotionally inconsistent samples due to oversimplified conditioning on categorical labels. This paper introduces a novel mutual-information-regularised generative framework that combines cross-modal alignment with feature-level synthesis. Building on an InfoGAN-style architecture, our method first learns a semantically aligned audio-text representation space using pre-trained transformers and contrastive objectives. A feature generator is then trained to produce emotion-aware audio features while employing mutual information as a quantitative regulariser to ensure strong dependency between generated features and their conditioning variables. We extend this approach to multimodal settings, enabling the generation of novel, paired (audio, text) features. Comprehensive evaluation on three benchmark datasets (IEMOCAP, MSP-IMPROV, MSP-Podcast) demonstrates that our framework consistently outperforms existing augmentation methods, achieving state-of-the-art performance with improvements of up to 2.6% in unimodal SER and 3.2% in multimodal emotion recognition. Most importantly, we demonstrate that mutual information functions as both a regulariser and a measurable metric for generative quality, offering a systematic approach to data augmentation in affective computing.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cash Flow Underwriting with Bank Transaction Data: Advancing MSME Financial Inclusion in Malaysia</title>
<link>https://arxiv.org/abs/2510.16066</link>
<guid>https://arxiv.org/abs/2510.16066</guid>
<content:encoded><![CDATA[
arXiv:2510.16066v3 Announce Type: replace-cross 
Abstract: Despite accounting for 96.1% of all businesses in Malaysia, access to financing remains one of the most persistent challenges faced by Micro, Small, and Medium Enterprises (MSMEs). Newly established businesses are often excluded from formal credit markets as traditional underwriting approaches rely heavily on credit bureau data. This study investigates the potential of bank statement data as an alternative data source for credit assessment to promote financial inclusion in emerging markets. First, we propose a cash flow-based underwriting pipeline where we utilise bank statement data for end-to-end data extraction and machine learning credit scoring. Second, we introduce a novel dataset of 611 loan applicants from a Malaysian lending institution. Third, we develop and evaluate credit scoring models based on application information and bank transaction-derived features. Empirical results show that the use of such data boosts the performance of all models on our dataset, which can improve credit scoring for new-to-lending MSMEs. Finally, we will release the anonymised bank transaction dataset to facilitate further research on MSME financial inclusion within Malaysia's emerging economy.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Compute-in-Memory Implementation of State Space Models for Event Sequence Processing</title>
<link>https://arxiv.org/abs/2511.13912</link>
<guid>https://arxiv.org/abs/2511.13912</guid>
<content:encoded><![CDATA[
arXiv:2511.13912v2 Announce Type: replace-cross 
Abstract: State space models (SSMs) have recently emerged as a powerful framework for long sequence processing, outperforming traditional methods on diverse benchmarks. Fundamentally, SSMs can generalize both recurrent and convolutional networks and have been shown to even capture key functions of biological systems. Here we report an approach to implement SSMs in energy-efficient compute-in-memory (CIM) hardware to achieve real-time, event-driven processing. Our work re-parameterizes the model to function with real-valued coefficients and shared decay constants, reducing the complexity of model mapping onto practical hardware systems. By leveraging device dynamics and diagonalized state transition parameters, the state evolution can be natively implemented in crossbar-based CIM systems combined with memristors exhibiting short-term memory effects. Through this algorithm and hardware co-design, we show the proposed system offers both high accuracy and high energy efficiency while supporting fully asynchronous processing for event-based vision and audio tasks.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FORWARD: Dataset of a forwarder operating in rough terrain</title>
<link>https://arxiv.org/abs/2511.17318</link>
<guid>https://arxiv.org/abs/2511.17318</guid>
<content:encoded><![CDATA[
arXiv:2511.17318v2 Announce Type: replace-cross 
Abstract: We present FORWARD, a high-resolution multimodal dataset of a cut-to-length forwarder operating in rough terrain on two harvest sites in the middle part of Sweden. The forwarder is a large Komatsu model equipped with vehicle telematics sensors, including global positioning via satellite navigation, movement sensors, accelerometers, and engine sensors. The vehicle was additionally equipped with cameras, operator vibration sensors, and multiple IMUs. The data includes event time logs recorded at 5 Hz of driving speed, fuel consumption, vehicle position with centimeter accuracy, and crane use while the vehicle operates in forest areas, aerially laser-scanned with a resolution of around 1500 points per square meter. Production log files (StanForD standard) with time-stamped machine events, extensive video material, and terrain data in various formats are included as well. About 18 hours of regular wood extraction work during three days is annotated from 360-video material into individual work elements and included in the dataset. We also include scenario specifications of conducted experiments on forest roads and in terrain. Scenarios include repeatedly driving the same routes with and without steel tracks, different load weights, and different target driving speeds. The dataset is intended for developing models and algorithms for trafficability, perception, and autonomous control of forest machines using artificial intelligence, simulation, and experiments on physical testbeds. In part, we focus on forwarders traversing terrain, avoiding or handling obstacles, and loading or unloading logs, with consideration for efficiency, fuel consumption, safety, and environmental impact. Other benefits of the open dataset include the ability to explore auto-generation and calibration of forestry machine simulators and automation scenario descriptions using the data recorded in the field.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation</title>
<link>https://arxiv.org/abs/2512.01801</link>
<guid>https://arxiv.org/abs/2512.01801</guid>
<content:encoded><![CDATA[
arXiv:2512.01801v3 Announce Type: replace-cross 
Abstract: We present GR-RL, a robotic learning framework that turns a generalist vision-language-action (VLA) policy into a highly capable specialist for long-horizon dexterous manipulation. Assuming the optimality of human demonstrations is core to existing VLA policies. However, we claim that in highly dexterous and precise manipulation tasks, human demonstrations are noisy and suboptimal. GR-RL proposes a multi-stage training pipeline that filters, augments, and reinforces the demonstrations by reinforcement learning. First, GR-RL learns a vision-language-conditioned task progress, filters the demonstration trajectories, and only keeps the transitions that contribute positively to the progress. Specifically, we show that by directly applying offline RL with sparse reward, the resulting $Q$-values can be treated as a robust progress function. Next, we introduce morphological symmetry augmentation that greatly improves the generalization and performance of GR-RL. Lastly, to better align the VLA policy with its deployment behaviors for high-precision control, we perform online RL by learning a latent space noise predictor. With this pipeline, GR-RL is, to our knowledge, the first learning-based policy that can autonomously lace up a shoe by threading shoelaces through multiple eyelets with an 83.3% success rate, a task requiring long-horizon reasoning, millimeter-level precision, and compliant soft-body interaction. We hope GR-RL provides a step toward enabling generalist robot foundation models to specialize into reliable real-world experts.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One Permutation Is All You Need: Fast, Reliable Variable Importance and Model Stress-Testing</title>
<link>https://arxiv.org/abs/2512.13892</link>
<guid>https://arxiv.org/abs/2512.13892</guid>
<content:encoded><![CDATA[
arXiv:2512.13892v2 Announce Type: replace-cross 
Abstract: Reliable estimation of feature contributions in machine learning models is essential for trust, transparency and regulatory compliance, especially when models are proprietary or otherwise operate as black boxes. While permutation-based methods are a standard tool for this task, classical implementations rely on repeated random permutations, introducing computational overhead and stochastic instability. In this paper, we show that by replacing multiple random permutations with a single, deterministic, and optimal permutation, we achieve a method that retains the core principles of permutation-based importance while being non-random, faster, and more stable. We validate this approach across nearly 200 scenarios, including real-world household finance and credit risk applications, demonstrating improved bias-variance tradeoffs and accuracy in challenging regimes such as small sample sizes, high dimensionality, and low signal-to-noise ratios. Finally, we introduce Systemic Variable Importance, a natural extension designed for model stress-testing that explicitly accounts for feature correlations. This framework provides a transparent way to quantify how shocks or perturbations propagate through correlated inputs, revealing dependencies that standard variable importance measures miss. Two real-world case studies demonstrate how this metric can be used to audit models for hidden reliance on protected attributes (e.g., gender or race), enabling regulators and practitioners to assess fairness and systemic risk in a principled and computationally efficient manner.
]]></content:encoded>
<pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Unsupervised Deep Explainable AI Framework for Localization of Concurrent Replay Attacks in Nuclear Reactor Signals</title>
<link>https://arxiv.org/abs/2508.09162</link>
<guid>https://arxiv.org/abs/2508.09162</guid>
<content:encoded><![CDATA[
<div> Keywords: advanced nuclear reactors, replay attacks, explainable AI, autoencoder, windowSHAP<br /><br />Summary:<br /><br />1. Next generation advanced nuclear reactors are smaller and utilize fully digital instrumentation and control systems, producing complex multivariate time series data representing various nonlinear cyber-physical and operational states. <br /><br />2. Ensuring data integrity against deception attacks, particularly replay attacks, is critical for safe and reliable reactor operation amid networked communication environments. <br /><br />3. Existing approaches to replay attack detection primarily rely on watermarking or supervised anomaly detection, often using synthetic data with simplifying assumptions like uncorrelated Gaussian noise, full state feedback, or linear systems, which limit effectiveness and fail to capture unmodeled dynamics.<br /><br />4. There is a need for unsupervised, explainable AI methods using real data to characterize replay attacks, including detection, timing, source identification, and attack type, especially in regulated nuclear cyber-physical systems.<br /><br />5. This study proposes an unsupervised explainable AI framework combining an autoencoder with a customized windowSHAP algorithm to identify and characterize replay attacks in real time during dynamic reactor processes. The framework was tested on real datasets from Purdue’s PUR-1 reactor with up to six concurrently replayed signals, achieving at least 95% accuracy in detecting the presence, source, number, and duration of replayed falsified signals. <div>
arXiv:2508.09162v4 Announce Type: replace 
Abstract: Next generation advanced nuclear reactors are expected to be smaller both in size and power output, relying extensively on fully digital instrumentation and control systems. These reactors will generate a large flow of information in the form of multivariate time series data, conveying simultaneously various non linear cyber physical, process, control, sensor, and operational states. Ensuring data integrity against deception attacks is becoming increasingly important for networked communication and a requirement for safe and reliable operation. Current efforts to address replay attacks, almost universally focus on watermarking or supervised anomaly detection approaches without further identifying and characterizing the root cause of the anomaly. In addition, these approaches rely mostly on synthetic data with uncorrelated Gaussian process and measurement noise and full state feedback or are limited to univariate signals, signal stationarity, linear quadratic regulators, or other linear-time invariant state-space which may fail to capture any unmodeled system dynamics. In the realm of regulated nuclear cyber-physical systems, additional work is needed on characterization of replay attacks and explainability of predictions using real data. Here, we propose an unsupervised explainable AI framework based on a combination of autoencoder and customized windowSHAP algorithm to fully characterize real-time replay attacks, i.e., detection, source identification, timing and type, of increasing complexity during a dynamic time evolving reactor process. The proposed XAI framework was benchmarked on several real world datasets from Purdue's nuclear reactor PUR-1 with up to six signals concurrently being replayed. In all cases, the XAI framework was able to detect and identify the source and number of signals being replayed and the duration of the falsification with 95 percent or better accuracy.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spectral Concentration at the Edge of Stability: Information Geometry of Kernel Associative Memory</title>
<link>https://arxiv.org/abs/2511.23083</link>
<guid>https://arxiv.org/abs/2511.23083</guid>
<content:encoded><![CDATA[
<div> Keywords: Kernel Hopfield Networks, Ridge of Optimization, Edge of Stability, Fisher Information Matrix, Minimum Description Length principle<br /><br />Summary:<br /><br />This article investigates the dynamics of high-capacity kernel Hopfield networks, focusing on a phenomenon known as the Ridge of Optimization, which is characterized by extreme stability in network performance. Previously, this Ridge was linked to Spectral Concentration, though its fundamental origin had not been fully understood. By analyzing the network dynamics on a statistical manifold, the authors reveal that the Ridge corresponds to the Edge of Stability—a critical boundary at which the Fisher Information Matrix becomes singular, indicating a phase transition in system behavior. Furthermore, the study clarifies that the observed antagonism of Euclidean forces in the network dynamics is actually due to a Dual Equilibrium condition when the system is viewed through the lens of Riemannian geometry. This insight provides a unified framework connecting learning dynamics and network capacity through the Minimum Description Length principle, which quantifies model complexity and generalization. Ultimately, the work offers a geometric theory for self-organized criticality in neural networks, highlighting the intrinsic balance between stability and adaptability that governs learning processes and memory capacity in kernel Hopfield models. <div>
arXiv:2511.23083v5 Announce Type: replace 
Abstract: High-capacity kernel Hopfield networks exhibit a \textit{Ridge of Optimization} characterized by extreme stability. While previously linked to \textit{Spectral Concentration}, its origin remains elusive. Here, we analyze the network dynamics on a statistical manifold, revealing that the Ridge corresponds to the Edge of Stability, a critical boundary where the Fisher Information Matrix becomes singular. We demonstrate that the apparent Euclidean force antagonism is a manifestation of \textit{Dual Equilibrium} in the Riemannian space. This unifies learning dynamics and capacity via the Minimum Description Length principle, offering a geometric theory of self-organized criticality.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Contrastive Learning via Spectral Graph Alignment</title>
<link>https://arxiv.org/abs/2512.07878</link>
<guid>https://arxiv.org/abs/2512.07878</guid>
<content:encoded><![CDATA[
<div> Keywords: contrastive learning, graph embeddings, normalized Laplacian, SpecMatch-CL, graph-of-graphs<br /><br />Summary: This paper introduces SpecMatch-CL, a novel loss function designed to improve contrastive learning on graph data by focusing on aligning the global structure of view-specific graph-of-graphs representations. Traditional contrastive methods like InfoNCE optimize pairwise alignment of graph embeddings from augmented views but do not explicitly control the global structural relationship among these embeddings. SpecMatch-CL addresses this gap by minimizing the difference between the normalized Laplacians of the graph-of-graphs from each view, providing a theoretically grounded mechanism to better align views at a global scale. The authors prove that under certain assumptions, the difference between normalized Laplacians bounds the gap between the current contrastive loss and an ideal Perfect Alignment loss, and also bounds the Uniformly loss, thereby connecting spectral properties with contrastive learning objectives. Empirically, SpecMatch-CL achieves new state-of-the-art results on eight TU benchmarks for both unsupervised and semi-supervised learning at low label rates. Furthermore, it demonstrates consistent performance improvements in transfer learning tasks on large-scale PPI-306K and ZINC 2M graph datasets, indicating its effectiveness and generalization across various graph learning settings. This work contributes both theoretical insights and practical advances in unsupervised graph representation learning. <div>
arXiv:2512.07878v3 Announce Type: replace 
Abstract: Given augmented views of each input graph, contrastive learning methods (e.g., InfoNCE) optimize pairwise alignment of graph embeddings across views while providing no mechanism to control the global structure of the view specific graph-of-graphs built from these embeddings. We introduce SpecMatch-CL, a novel loss function that aligns the view specific graph-of-graphs by minimizing the difference between their normalized Laplacians. Theoretically, we show that under certain assumptions, the difference between normalized Laplacians provides an upper bound not only for the difference between the ideal Perfect Alignment contrastive loss and the current loss, but also for the Uniformly loss. Empirically, SpecMatch-CL establishes new state of the art on eight TU benchmarks under unsupervised learning and semi-supervised learning at low label rates, and yields consistent gains in transfer learning on PPI-306K and ZINC 2M datasets.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural CDEs as Correctors for Learned Time Series Models</title>
<link>https://arxiv.org/abs/2512.12116</link>
<guid>https://arxiv.org/abs/2512.12116</guid>
<content:encoded><![CDATA[
<div> Keywords: time-series forecasting, Predictor-Corrector mechanism, neural controlled differential equation, multi-step prediction, regularization strategies<br /><br />Summary:<br /><br />This article addresses the challenge of error accumulation in multi-step time-series forecasting using learned models. It proposes a Predictor-Corrector mechanism where the Predictor is any learned time-series model that generates forecasts and the Corrector is a neural controlled differential equation designed to predict and correct the forecasting errors. The combination improves overall forecasting accuracy by adding predicted errors back to the original forecasts. This Corrector is adaptable, working effectively with irregularly sampled time series and both continuous- and discrete-time models. Additionally, the study introduces two novel regularization strategies aimed at enhancing the Corrector’s extrapolation capabilities and speeding up training. The framework is evaluated with a variety of Predictor models including neural ordinary differential equations, Contiformer, and DLinear. Experiments are conducted on synthetic data, physics simulations, and diverse real-world forecasting datasets. The results consistently demonstrate that the Predictor-Corrector approach outperforms using the Predictor model alone, highlighting its robustness and broad applicability for improving multi-step forecasting performance across different settings and data types. <div>
arXiv:2512.12116v2 Announce Type: replace 
Abstract: Learned time-series models, whether continuous- or discrete-time, are widely used to forecast the states of a dynamical system. Such models generate multi-step forecasts either directly, by predicting the full horizon at once, or iteratively, by feeding back their own predictions at each step. In both cases, the multi-step forecasts are prone to errors. To address this, we propose a Predictor-Corrector mechanism where the Predictor is any learned time-series model and the Corrector is a neural controlled differential equation. The Predictor forecasts, and the Corrector predicts the errors of the forecasts. Adding these errors to the forecasts improves forecast performance. The proposed Corrector works with irregularly sampled time series and continuous- and discrete-time Predictors. Additionally, we introduce two regularization strategies to improve the extrapolation performance of the Corrector with accelerated training. We evaluate our Corrector with diverse Predictors, e.g., neural ordinary differential equations, Contiformer, and DLinear, on synthetic, physics simulation, and real-world forecasting datasets. The experiments demonstrate that the Predictor-Corrector mechanism consistently improves the performance compared to Predictor alone.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparative Evaluation of Explainable Machine Learning Versus Linear Regression for Predicting County-Level Lung Cancer Mortality Rate in the United States</title>
<link>https://arxiv.org/abs/2512.17934</link>
<guid>https://arxiv.org/abs/2512.17934</guid>
<content:encoded><![CDATA[
<div> Lung cancer, mortality prediction, machine learning, SHAP analysis, geographic disparities

<br /><br />Summary:  
Lung cancer (LC) remains a leading cause of cancer-related deaths in the U.S., necessitating accurate mortality rate predictions to inform targeted interventions and reduce health disparities. This study compared three predictive models—random forest (RF), gradient boosting regression (GBR), and linear regression (LR)—to estimate county-level LC mortality rates across the country. Model evaluation using R-squared and root mean squared error (RMSE) revealed the RF model outperformed the others, achieving an R2 of 41.9% and RMSE of 12.8. Using Shapley Additive Explanations (SHAP), smoking rate emerged as the most influential predictor, followed by median home value and the percentage of the Hispanic population, highlighting key socioeconomic and behavioral factors shaping LC mortality. Furthermore, spatial analysis via Getis-Ord (Gi*) hotspot detection identified significant clusters of elevated LC mortality primarily in mid-eastern U.S. counties, underscoring geographic disparities. The findings demonstrate that explainable machine learning methods, particularly RF combined with SHAP, provide improved accuracy and interpretability for LC mortality prediction. These insights support the development of focused public health strategies, such as smoking cessation programs and enhanced screening efforts, especially in high-risk regions, to effectively address lung cancer outcomes and related health equity issues across the United States. <div>
arXiv:2512.17934v1 Announce Type: new 
Abstract: Lung cancer (LC) is a leading cause of cancer-related mortality in the United States. Accurate prediction of LC mortality rates is crucial for guiding targeted interventions and addressing health disparities. Although traditional regression-based models have been commonly used, explainable machine learning models may offer enhanced predictive accuracy and deeper insights into the factors influencing LC mortality. This study applied three models: random forest (RF), gradient boosting regression (GBR), and linear regression (LR) to predict county-level LC mortality rates across the United States. Model performance was evaluated using R-squared and root mean squared error (RMSE). Shapley Additive Explanations (SHAP) values were used to determine variable importance and their directional impact. Geographic disparities in LC mortality were analyzed through Getis-Ord (Gi*) hotspot analysis. The RF model outperformed both GBR and LR, achieving an R2 value of 41.9% and an RMSE of 12.8. SHAP analysis identified smoking rate as the most important predictor, followed by median home value and the percentage of the Hispanic ethnic population. Spatial analysis revealed significant clusters of elevated LC mortality in the mid-eastern counties of the United States. The RF model demonstrated superior predictive performance for LC mortality rates, emphasizing the critical roles of smoking prevalence, housing values, and the percentage of Hispanic ethnic population. These findings offer valuable actionable insights for designing targeted interventions, promoting screening, and addressing health disparities in regions most affected by LC in the United States.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What's the Price of Monotonicity? A Multi-Dataset Benchmark of Monotone-Constrained Gradient Boosting for Credit PD</title>
<link>https://arxiv.org/abs/2512.17945</link>
<guid>https://arxiv.org/abs/2512.17945</guid>
<content:encoded><![CDATA[
<div> Monotonicity constraints, Credit risk, Gradient boosting, Price of Monotonicity, Predictive accuracy<br /><br />Summary:<br /><br />1. This paper addresses the challenge financial institutions face when deploying machine learning models for credit risk, specifically balancing predictive accuracy and model interpretability. 2. It focuses on monotonicity constraints, which enforce model behavior consistent with domain knowledge but may potentially reduce accuracy. 3. The authors introduce the "Price of Monotonicity" (PoM), defined as the relative performance drop when moving from unconstrained to monotone-constrained gradient boosting models, measured using paired comparisons and bootstrap uncertainty. 4. The empirical benchmark includes five public credit datasets and three different gradient boosting libraries, comparing monotone-constrained and unconstrained models for the probability of default prediction. 5. Results show PoM in AUC ranges from nearly zero to about 2.9%, with the penalty being minimal (often below 0.2%) on large datasets and more significant (2-3%) on smaller datasets with heavy monotonic constraints. Overall, the study finds that well-specified monotonicity constraints can provide valuable interpretability with only minor trade-offs in accuracy, especially beneficial for large-scale credit portfolios. <div>
arXiv:2512.17945v1 Announce Type: new 
Abstract: Financial institutions face a trade-off between predictive accuracy and interpretability when deploying machine learning models for credit risk. Monotonicity constraints align model behavior with domain knowledge, but their performance cost - the price of monotonicity - is not well quantified. This paper benchmarks monotone-constrained versus unconstrained gradient boosting models for credit probability of default across five public datasets and three libraries. We define the Price of Monotonicity (PoM) as the relative change in standard performance metrics when moving from unconstrained to constrained models, estimated via paired comparisons with bootstrap uncertainty. In our experiments, PoM in AUC ranges from essentially zero to about 2.9 percent: constraints are almost costless on large datasets (typically less than 0.2 percent, often indistinguishable from zero) and most costly on smaller datasets with extensive constraint coverage (around 2-3 percent). Thus, appropriately specified monotonicity constraints can often deliver interpretability with small accuracy losses, particularly in large-scale credit portfolios.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Convolutional-neural-operator-based transfer learning for solving PDEs</title>
<link>https://arxiv.org/abs/2512.17969</link>
<guid>https://arxiv.org/abs/2512.17969</guid>
<content:encoded><![CDATA[
<div> Convolutional neural operator, few-shot learning, PDEs, neuron linear transformation, surrogate accuracy<br /><br />Summary:<br /><br />1. The convolutional neural operator (CNO) is a CNN-based architecture designed to preserve the continuous-discrete equivalence in learning solution operators for partial differential equations (PDEs), enabling alias-free and structure-preserving modeling.<br />2. Previous studies demonstrated that CNO outperforms baseline methods such as DeepONet, Fourier neural operator, and Galerkin transformer in terms of surrogate accuracy for certain PDE problems.<br />3. However, CNO has not been thoroughly validated in few-shot learning scenarios where limited training data is available for the target task.<br />4. This work extends CNO to few-shot learning by first pre-training it on a source dataset and subsequently adapting its parameters using only a small target dataset.<br />5. Three parameter adjustment strategies were evaluated: fine-tuning, low-rank adaptation, and neuron linear transformation.<br />6. Among these strategies, neuron linear transformation achieved the highest surrogate accuracy across several challenging PDEs, including the Kuramoto-Sivashinsky equation, Brusselator diffusion-reaction system, and Navier-Stokes equations.<br />7. The results highlight the potential of neuron linear transformation in enabling efficient few-shot learning for neural operators in PDE modeling tasks. <div>
arXiv:2512.17969v1 Announce Type: new 
Abstract: Convolutional neural operator is a CNN-based architecture recently proposed to enforce structure-preserving continuous-discrete equivalence and enable the genuine, alias-free learning of solution operators of PDEs. This neural operator was demonstrated to outperform for certain cases some baseline models such as DeepONet, Fourier neural operator, and Galerkin transformer in terms of surrogate accuracy. The convolutional neural operator, however, seems not to be validated for few-shot learning. We extend the model to few-shot learning scenarios by first pre-training a convolutional neural operator using a source dataset and then adjusting the parameters of the trained neural operator using only a small target dataset. We investigate three strategies for adjusting the parameters of a trained neural operator, including fine-tuning, low-rank adaption, and neuron linear transformation, and find that the neuron linear transformation strategy enjoys the highest surrogate accuracy in solving PDEs such as Kuramoto-Sivashinsky equation, Brusselator diffusion-reaction system, and Navier-Stokes equations.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CodeGEMM: A Codebook-Centric Approach to Efficient GEMM in Quantized LLMs</title>
<link>https://arxiv.org/abs/2512.17970</link>
<guid>https://arxiv.org/abs/2512.17970</guid>
<content:encoded><![CDATA[
<div> Weight-only quantization, codebook-based quantization, GEMM kernel, latency optimization, Llama-3 models<br /><br />Summary: Weight-only quantization helps reduce memory bottlenecks during large language model (LLM) inference. Codebook-based quantization achieves strong accuracy at extremely low bit widths, such as 2-bit, but existing implementations heavily rely on dequantization, which causes latency and cache inefficiencies by repeatedly fetching centroids and reconstructing weights. The paper introduces CodeGEMM, a novel GEMM kernel that centers around the codebook and avoids dequantization by precomputing inner products between centroids and activations, stored in a lightweight structure called the Psumbook. During inference, code indices directly gather partial sums from Psumbook, eliminating costly per-element lookups and significantly reducing on-chip memory usage. CodeGEMM also systematically explores the trade-offs between latency, memory usage, and accuracy within a single framework. Evaluations on Llama-3 models demonstrate that CodeGEMM achieves substantial speedups—1.83x for the 8B model and 8.93x for the 70B model—compared to the previous state-of-the-art codebook-based quantization approaches, all while maintaining comparable accuracy. This kernel consequently improves overall computational efficiency and memory subsystem utilization during LLM inference. <div>
arXiv:2512.17970v1 Announce Type: new 
Abstract: Weight-only quantization is widely used to mitigate the memory-bound nature of LLM inference. Codebook-based methods extend this trend by achieving strong accuracy in the extremely low-bit regime (e.g., 2-bit). However, current kernels rely on dequantization, which repeatedly fetches centroids and reconstructs weights, incurring substantial latency and cache pressure. We present CodeGEMM, a codebook-centric GEMM kernel that replaces dequantization with precomputed inner products between centroids and activations stored in a lightweight Psumbook. At inference, code indices directly gather these partial sums, eliminating per-element lookups and reducing the on-chip footprint. The kernel supports the systematic exploration of latency-memory-accuracy trade-offs under a unified implementation. On Llama-3 models, CodeGEMM delivers 1.83x (8B) and 8.93x (70B) speedups in the 2-bit configuration compared to state-of-the-art codebook-based quantization at comparable accuracy and further improves computing efficiency and memory subsystem utilization.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parameter-Efficient Fine-Tuning for HAR: Integrating LoRA and QLoRA into Transformer Models</title>
<link>https://arxiv.org/abs/2512.17983</link>
<guid>https://arxiv.org/abs/2512.17983</guid>
<content:encoded><![CDATA[
<div> Low-Rank Adaptation, Quantized LoRA, Human Activity Recognition, Masked Autoencoder, Parameter-Efficient Fine-Tuning<br /><br />Summary:<br /><br />This paper addresses the challenge of adapting large pretrained Human Activity Recognition (HAR) models to new domains under limited computational resources on target devices. It explores parameter-efficient fine-tuning methods, particularly Low-Rank Adaptation (LoRA) and Quantized LoRA (QLoRA), as alternatives to full model fine-tuning. The proposed framework uses a Masked Autoencoder backbone for feature extraction. The study evaluates performance using a Leave-One-Dataset-Out validation protocol on five publicly available HAR datasets. Experimental results show that both LoRA and QLoRA achieve recognition performance comparable to full fine-tuning but significantly reduce trainable parameters, memory consumption, and training time. Additional analysis demonstrates that LoRA remains effective even with limited supervision and that the adapter rank can be adjusted to balance accuracy against computational efficiency. Furthermore, QLoRA enhances these benefits by quantizing frozen model weights, thereby decreasing memory footprint without substantially degrading classification accuracy. This work highlights scalable, resource-friendly adaptation strategies for deploying large HAR models in practical, resource-constrained environments. <div>
arXiv:2512.17983v1 Announce Type: new 
Abstract: Human Activity Recognition is a foundational task in pervasive computing. While recent advances in self-supervised learning and transformer-based architectures have significantly improved HAR performance, adapting large pretrained models to new domains remains a practical challenge due to limited computational resources on target devices. This papers investigates parameter-efficient fine-tuning techniques, specifically Low-Rank Adaptation (LoRA) and Quantized LoRA, as scalable alternatives to full model fine-tuning for HAR. We propose an adaptation framework built upon a Masked Autoencoder backbone and evaluate its performance under a Leave-One-Dataset-Out validation protocol across five open HAR datasets. Our experiments demonstrate that both LoRA and QLoRA can match the recognition performance of full fine-tuning while significantly reducing the number of trainable parameters, memory usage, and training time. Further analyses reveal that LoRA maintains robust performance even under limited supervision and that the adapter rank provides a controllable trade-off between accuracy and efficiency. QLoRA extends these benefits by reducing the memory footprint of frozen weights through quantization, with minimal impact on classification quality.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Hybrid Inductive-Transductive Network for Traffic Flow Imputation on Unsampled Locations</title>
<link>https://arxiv.org/abs/2512.17984</link>
<guid>https://arxiv.org/abs/2512.17984</guid>
<content:encoded><![CDATA[
<div> Traffic flow imputation, Inductive learning, Transductive learning, Graph Neural Networks, Traffic simulation<br /><br />Summary:<br /><br />1. The article addresses the challenge of accurately imputing traffic flow at unsensed locations, noting that data sources like loop detectors are sparse, and probe vehicle speeds only weakly correlate with flow. Additionally, heterophily in neighboring traffic segments disrupts common GNN assumptions.<br /><br />2. The authors propose HINT (Hybrid INductive-Transductive Network), which combines an inductive spatial transformer for learning long-range similarity-driven interactions with a FiLM-conditioned diffusion GCN that incorporates rich static contextual information from OSM and traffic simulation.<br /><br />3. A node-wise calibration layer in HINT corrects scale biases unique to each traffic segment, improving accuracy.<br /><br />4. The INDU-TRANSDUCTIVE training strategy treats speed as a transductive network-wide signal while learning flow inductively; the method uses masked reconstruction, epoch-wise node sampling, hard-node mining, and noise injection to prevent overfitting.<br /><br />5. Experiments on three real-world datasets (Antwerp, Torino, Essen) show that HINT consistently outperforms state-of-the-art inductive methods, reducing MAE significantly, especially when including traffic simulation data.<br /><br />6. The results demonstrate the benefit of combining inductive flow imputation with transductive speed signals, simulations, and external geospatial data to improve traffic flow prediction accuracy at unsensed locations. <div>
arXiv:2512.17984v1 Announce Type: new 
Abstract: Accurately imputing traffic flow at unsensed locations is difficult: loop detectors provide precise but sparse measurements, speed from probe vehicles is widely available yet only weakly correlated with flow, and nearby links often exhibit strong heterophily in the scale of traffic flow (e.g., ramps vs. mainline), which breaks standard GNN assumptions. We propose HINT, a Hybrid INductive-Transductive Network, and an INDU-TRANSDUCTIVE training strategy that treats speed as a transductive, network-wide signal while learning flow inductively to generalize to unseen locations. HINT couples (i) an inductive spatial transformer that learns similarity-driven, long-range interactions from node features with (ii) a diffusion GCN conditioned by FiLM on rich static context (OSM-derived attributes and traffic simulation), and (iii) a node-wise calibration layer that corrects scale biases per segment. Training uses masked reconstruction with epoch-wise node sampling, hard-node mining to emphasize difficult sensors, and noise injection on visible flows to prevent identity mapping, while graph structure is built from driving distances.
  Across three real-world datasets, MOW (Antwerp, Belgium), UTD19-Torino, and UTD19-Essen, HINT consistently surpasses state-of-the-art inductive baselines. Relative to KITS, HINT reduces MAE on MOW by $\approx42$% with basic simulation and $\approx50$% with calibrated simulation; on Torino by $\approx22$%, and on Essen by $\approx12$%. Even without simulation, HINT remains superior on MOW and Torino, while simulation is crucial on Essen. These results show that combining inductive flow imputation with transductive speed, traffic simulations and external geospatial improves accuracy for the task described above.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoE-TransMov: A Transformer-based Model for Next POI Prediction in Familiar &amp; Unfamiliar Movements</title>
<link>https://arxiv.org/abs/2512.17985</link>
<guid>https://arxiv.org/abs/2512.17985</guid>
<content:encoded><![CDATA[
<div> Keywords: next point of interest prediction, human mobility, Mixture-of-Experts, Transformer model, personalized recommendation<br /><br />Summary:  
The paper addresses the challenge of accurately predicting the next point of interest (POI) within human mobility trajectories, which is pivotal for location-based services and personalized recommendations. It highlights that users show different POI preferences when moving in familiar versus unfamiliar areas, a distinction often overlooked by existing methods. To tackle this, the authors propose MoE-TransMov, a Transformer-based model augmented with a Mixture-of-Experts (MoE) architecture. This model captures distinct mobility patterns by dynamically selecting expert networks specialized for familiar and unfamiliar movement contexts through adaptive gating mechanisms. The framework does not require separate training of models for different contexts, enabling efficient learning within a unified system. Experiments were conducted on two real-world datasets: the small-scale open-source Foursquare NYC dataset and the large-scale Kyoto dataset by LY Corporation. Results demonstrate that MoE-TransMov consistently outperforms state-of-the-art baselines in multiple evaluation metrics, including Top-1, Top-5, Top-10 accuracy, and mean reciprocal rank (MRR). The improved prediction accuracy emphasizes the value of incorporating user familiarity into mobility modeling. Ultimately, the approach advances personalized recommendation systems and supports enhanced urban applications by better understanding users’ context-dependent movement behaviors. <div>
arXiv:2512.17985v1 Announce Type: new 
Abstract: Accurate prediction of the next point of interest (POI) within human mobility trajectories is essential for location-based services, as it enables more timely and personalized recommendations. In particular, with the rise of these approaches, studies have shown that users exhibit different POI choices in their familiar and unfamiliar areas, highlighting the importance of incorporating user familiarity into predictive models. However, existing methods often fail to distinguish between the movements of users in familiar and unfamiliar regions. To address this, we propose MoE-TransMov, a Transformer-based model with a Transformer model with a Mixture-of-Experts (MoE) architecture designed to use one framework to capture distinct mobility patterns across different moving contexts without requiring separate training for certain data. Using user-check-in data, we classify movements into familiar and unfamiliar categories and develop a specialized expert network to improve prediction accuracy. Our approach integrates self-attention mechanisms and adaptive gating networks to dynamically select the most relevant expert models for different mobility contexts. Experiments on two real-world datasets, including the widely used but small open-source Foursquare NYC dataset and the large-scale Kyoto dataset collected with LY Corporation (Yahoo Japan Corporation), show that MoE-TransMov outperforms state-of-the-art baselines with notable improvements in Top-1, Top-5, Top-10 accuracy, and mean reciprocal rank (MRR). Given the results, we find that by using this approach, we can efficiently improve mobility predictions under different moving contexts, thereby enhancing the personalization of recommendation systems and advancing various urban applications.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedOAED: Federated On-Device Autoencoder Denoiser for Heterogeneous Data under Limited Client Availability</title>
<link>https://arxiv.org/abs/2512.17986</link>
<guid>https://arxiv.org/abs/2512.17986</guid>
<content:encoded><![CDATA[
<div> Federated Learning, Client-Drift, Autoencoder Denoiser, Non-IID Data, Partial Client Participation<br /><br />Summary:<br /><br />1. The paper addresses challenges in Federated Learning (FL) related to client-drift and increased variance caused by heterogeneous data and limited client participation.  
2. It highlights the impact of data privacy regulations such as GDPR and HIPAA on the deployment of traditional machine learning models, motivating the use of FL where raw data remains on local devices.  
3. The authors propose FedOAED, a novel FL algorithm that integrates an on-device autoencoder denoiser to reduce client-drift caused by multiple local training updates and to mitigate variance from partial client availability.  
4. FedOAED is designed specifically to handle Non-IID data distributions, where data across clients is not identically and independently distributed—a common real-world scenario in FL.  
5. Experimental results on multiple vision datasets demonstrate that FedOAED consistently outperforms current state-of-the-art federated learning approaches, showing improved model accuracy and robustness in heterogeneous and constrained client participation environments. <div>
arXiv:2512.17986v1 Announce Type: new 
Abstract: Over the last few decades, machine learning (ML) and deep learning (DL) solutions have demonstrated their potential across many applications by leveraging large amounts of high-quality data. However, strict data-sharing regulations such as the General Data Protection Regulation (GDPR) and the Health Insurance Portability and Accountability Act (HIPAA) have prevented many data-driven applications from being realised. Federated Learning (FL), in which raw data never leaves local devices, has shown promise in overcoming these limitations. Although FL has grown rapidly in recent years, it still struggles with heterogeneity, which produces gradient noise, client-drift, and increased variance from partial client participation. In this paper, we propose FedOAED, a novel federated learning algorithm designed to mitigate client-drift arising from multiple local training updates and the variance induced by partial client participation. FedOAED incorporates an on-device autoencoder denoiser on the client side to mitigate client-drift and variance resulting from heterogeneous data under limited client availability. Experiments on multiple vision datasets under Non-IID settings demonstrate that FedOAED consistently outperforms state-of-the-art baselines.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Dataset and Benchmarks for Atrial Fibrillation Detection from Electrocardiograms of Intensive Care Unit Patients</title>
<link>https://arxiv.org/abs/2512.18031</link>
<guid>https://arxiv.org/abs/2512.18031</guid>
<content:encoded><![CDATA[
<div> Atrial fibrillation, ICU, machine learning, ECG foundation models, transfer learning  

<br /><br />Summary:  
1. The study focuses on detecting atrial fibrillation (AF), the most common cardiac arrhythmia in intensive care unit (ICU) patients, which can lead to significant adverse health outcomes.  
2. A newly labeled ICU dataset is published alongside benchmark results to facilitate research in AF detection.  
3. The research compares three AI-based machine learning approaches for AF detection: traditional feature-based classifiers, deep learning (DL) models, and ECG foundation models (FMs).  
4. Experiments utilize ECG data from a Canadian ICU and the 2021 PhysioNet/Computing in Cardiology Challenge, exploring different training regimes including zero-shot inference and transfer learning.  
5. Results show that ECG foundation models outperform deep learning and feature-based methods, with the best performance (F1 score of 0.89) achieved by an ECG-FM using transfer learning on the ICU test set.  
6. The study demonstrates AI's promising potential for enhancing automatic patient monitoring systems in critical care.  
7. By releasing their labeled ICU dataset and benchmarking results, the authors invite the research community to improve and advance AF detection technologies in ICU settings. <div>
arXiv:2512.18031v1 Announce Type: new 
Abstract: Objective: Atrial fibrillation (AF) is the most common cardiac arrhythmia experienced by intensive care unit (ICU) patients and can cause adverse health effects. In this study, we publish a labelled ICU dataset and benchmarks for AF detection. Methods: We compared machine learning models across three data-driven artificial intelligence (AI) approaches: feature-based classifiers, deep learning (DL), and ECG foundation models (FMs). This comparison addresses a critical gap in the literature and aims to pinpoint which AI approach is best for accurate AF detection. Electrocardiograms (ECGs) from a Canadian ICU and the 2021 PhysioNet/Computing in Cardiology Challenge were used to conduct the experiments. Multiple training configurations were tested, ranging from zero-shot inference to transfer learning. Results: On average and across both datasets, ECG FMs performed best, followed by DL, then feature-based classifiers. The model that achieved the top F1 score on our ICU test set was ECG-FM through a transfer learning strategy (F1=0.89). Conclusion: This study demonstrates promising potential for using AI to build an automatic patient monitoring system. Significance: By publishing our labelled ICU dataset (LinkToBeAdded) and performance benchmarks, this work enables the research community to continue advancing the state-of-the-art in AF detection in the ICU.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Benchmarking Privacy Vulnerabilities in Selective Forgetting with Large Language Models</title>
<link>https://arxiv.org/abs/2512.18035</link>
<guid>https://arxiv.org/abs/2512.18035</guid>
<content:encoded><![CDATA[
<div> Keywords: selective forgetting, machine unlearning, privacy leakage, data protection, privacy attacks  

<br /><br />Summary:  
1. This article addresses the emerging concept of selective forgetting, also known as machine unlearning, which enables AI models to erase the influence of specific previously seen data.  
2. Selective forgetting has become increasingly important for ensuring privacy, regulatory compliance, and alignment with human values, especially as AI systems are deployed in sensitive domains.  
3. Despite its potential benefits, selective forgetting raises significant privacy concerns due to new privacy attacks tailored to exploit vulnerabilities introduced during the unlearning process.  
4. Existing privacy attacks vary widely in their evaluation setups, often favoring certain methods without fair or consistent comparisons, leading to overly optimistic assessments of vulnerabilities.  
5. The work presents the first comprehensive benchmark that systematically evaluates privacy leakage risks associated with different unlearning techniques, victim data types, privacy attacks, and model architectures.  
6. The study identifies critical factors influencing privacy leakage induced by unlearning and provides novel insights into the risks and trade-offs inherent in deploying machine unlearning technologies.  
7. Ultimately, this benchmark serves as a standardized tool, enabling practitioners to assess privacy risks faithfully and customize unlearning applications according to their specific privacy needs. <div>
arXiv:2512.18035v1 Announce Type: new 
Abstract: The rapid advancements in artificial intelligence (AI) have primarily focused on the process of learning from data to acquire knowledgeable learning systems. As these systems are increasingly deployed in critical areas, ensuring their privacy and alignment with human values is paramount. Recently, selective forgetting (also known as machine unlearning) has shown promise for privacy and data removal tasks, and has emerged as a transformative paradigm shift in the field of AI. It refers to the ability of a model to selectively erase the influence of previously seen data, which is especially important for compliance with modern data protection regulations and for aligning models with human values. Despite its promise, selective forgetting raises significant privacy concerns, especially when the data involved come from sensitive domains. While new unlearning-induced privacy attacks are continuously proposed, each is shown to outperform its predecessors using different experimental settings, which can lead to overly optimistic and potentially unfair assessments that may disproportionately favor one particular attack over the others. In this work, we present the first comprehensive benchmark for evaluating privacy vulnerabilities in selective forgetting. We extensively investigate privacy vulnerabilities of machine unlearning techniques and benchmark privacy leakage across a wide range of victim data, state-of-the-art unlearning privacy attacks, unlearning methods, and model architectures. We systematically evaluate and identify critical factors related to unlearning-induced privacy leakage. With our novel insights, we aim to provide a standardized tool for practitioners seeking to deploy customized unlearning applications with faithful privacy assessments.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probabilistic Digital Twins of Users: Latent Representation Learning with Statistically Validated Semantics</title>
<link>https://arxiv.org/abs/2512.18056</link>
<guid>https://arxiv.org/abs/2512.18056</guid>
<content:encoded><![CDATA[
<div> Keywords: probabilistic digital twin, user identity, variational autoencoder, latent representation, uncertainty quantification<br /><br />Summary:<br /><br />1. The paper addresses the challenge of understanding user identity and behavior, critical for applications like personalization and recommendation systems.<br />2. It critiques existing methods for relying on deterministic embeddings or opaque predictive models that lack uncertainty quantification and interpretability.<br />3. To overcome these limitations, the authors propose a probabilistic digital twin framework modeling each user as a latent stochastic state that generates observed behavioral data.<br />4. The framework employs amortized variational inference for scalable posterior estimation, maintaining a full probabilistic perspective.<br />5. The digital twin is instantiated via a variational autoencoder (VAE) applied to a specially designed user-response dataset that captures stable aspects of user identity.<br />6. Beyond traditional reconstruction metrics, the study introduces an interpretation pipeline that statistically links latent dimensions to observable behavioral traits.<br />7. By examining users at the extremes of latent dimensions and using nonparametric hypothesis testing plus effect size measures, specific latent dimensions are shown to correspond to meaningful traits like opinion strength and decisiveness.<br />8. The empirical analysis reveals that user structures are mostly continuous rather than discretely clustered, with weak but interpretable structures emerging along a limited number of dominant latent axes.<br />9. The results highlight that probabilistic digital twins can produce interpretable, uncertainty-aware user representations that surpass deterministic embedding approaches in insight and flexibility. <div>
arXiv:2512.18056v1 Announce Type: new 
Abstract: Understanding user identity and behavior is central to applications such as personalization, recommendation, and decision support. Most existing approaches rely on deterministic embeddings or black-box predictive models, offering limited uncertainty quantification and little insight into what latent representations encode. We propose a probabilistic digital twin framework in which each user is modeled as a latent stochastic state that generates observed behavioral data. The digital twin is learned via amortized variational inference, enabling scalable posterior estimation while retaining a fully probabilistic interpretation. We instantiate this framework using a variational autoencoder (VAE) applied to a user-response dataset designed to capture stable aspects of user identity. Beyond standard reconstruction-based evaluation, we introduce a statistically grounded interpretation pipeline that links latent dimensions to observable behavioral patterns. By analyzing users at the extremes of each latent dimension and validating differences using nonparametric hypothesis tests and effect sizes, we demonstrate that specific dimensions correspond to interpretable traits such as opinion strength and decisiveness. Empirically, we find that user structure is predominantly continuous rather than discretely clustered, with weak but meaningful structure emerging along a small number of dominant latent axes. These results suggest that probabilistic digital twins can provide interpretable, uncertainty-aware representations that go beyond deterministic user embeddings.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Microstructure-based Variational Neural Networks for Robust Uncertainty Quantification in Materials Digital Twins</title>
<link>https://arxiv.org/abs/2512.18104</link>
<guid>https://arxiv.org/abs/2512.18104</guid>
<content:encoded><![CDATA[
<div> Aleatoric uncertainty, Variational Deep Material Network, digital twins, microstructure variability, inverse calibration<br /><br />Summary:<br /><br />1. The paper addresses the challenge of aleatoric uncertainties, which are inherent and irremovable variabilities in material microstructure, constituent behavior, and processing conditions, that complicate the development of uncertainty-robust digital twins.<br />2. It introduces the Variational Deep Material Network (VDMN), a physics-informed surrogate model designed to perform efficient and probabilistic forward and inverse predictions of material behavior while capturing microstructure-induced variability.<br />3. The VDMN architecture embeds variational distributions within a hierarchical, mechanistic framework and employs an analytic uncertainty propagation method based on Taylor-series expansion combined with automatic differentiation to efficiently handle uncertainty during both training and prediction phases.<br />4. The model's effectiveness is demonstrated through two applications: first, as an uncertainty-aware materials digital twin predicting and experimentally validating nonlinear mechanical variability in additively manufactured polymer composites; second, as an inverse calibration tool that disentangles and quantitatively identifies overlapping sources of uncertainty in constituent properties.<br />5. The study establishes the VDMN as a foundational approach for creating materials digital twins that are robust to uncertainty, enabling enhanced predictive accuracy and improved understanding of material variability in digital manufacturing processes. <div>
arXiv:2512.18104v1 Announce Type: new 
Abstract: Aleatoric uncertainties - irremovable variability in microstructure morphology, constituent behavior, and processing conditions - pose a major challenge to developing uncertainty-robust digital twins. We introduce the Variational Deep Material Network (VDMN), a physics-informed surrogate model that enables efficient and probabilistic forward and inverse predictions of material behavior. The VDMN captures microstructure-induced variability by embedding variational distributions within its hierarchical, mechanistic architecture. Using an analytic propagation scheme based on Taylor-series expansion and automatic differentiation, the VDMN efficiently propagates uncertainty through the network during training and prediction. We demonstrate its capabilities in two digital-twin-driven applications: (1) as an uncertainty-aware materials digital twin, it predicts and experimentally validates the nonlinear mechanical variability in additively manufactured polymer composites; and (2) as an inverse calibration engine, it disentangles and quantitatively identifies overlapping sources of uncertainty in constituent properties. Together, these results establish the VDMN as a foundation for uncertainty-robust materials digital twins.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Generalizable Neural Operators for Inverse Problems</title>
<link>https://arxiv.org/abs/2512.18120</link>
<guid>https://arxiv.org/abs/2512.18120</guid>
<content:encoded><![CDATA[
<div> Keywords: inverse problems, neural operators, basis-to-basis, ill-posedness, PDE benchmarks<br /><br />Summary: Inverse problems in neural operator learning present challenges due to violations of continuity, uniqueness, and stability, which are critical assumptions for existing architectures. The article introduces B2B${}^{-1}$, a novel inverse basis-to-basis neural operator framework that addresses these challenges by decoupling the function representation from the inverse mapping process. The core innovation is learning neural basis functions separately for input and output function spaces, then performing inversion on their coefficient spaces, which simplifies and stabilizes the inverse problem. This architecture supports training deterministic, invertible, and probabilistic inverse models within a unified framework, allowing model selection based on the degree of ill-posedness in the problem. Evaluation is conducted on six inverse partial differential equation (PDE) benchmarks, including two newly created datasets, demonstrating competitive performance relative to existing invertible neural operator baselines. The probabilistic models developed effectively capture uncertainty and input variability and exhibit robustness to measurement noise through implicit denoising during coefficient computation. Results highlight consistent re-simulation accuracy across different levels of ill-posedness, underscoring the framework's ability to generate scalable surrogate models that generalize well across various instances, spatial domains, and problem difficulties. <div>
arXiv:2512.18120v1 Announce Type: new 
Abstract: Inverse problems challenge existing neural operator architectures because ill-posed inverse maps violate continuity, uniqueness, and stability assumptions. We introduce B2B${}^{-1}$, an inverse basis-to-basis neural operator framework that addresses this limitation. Our key innovation is to decouple function representation from the inverse map. We learn neural basis functions for the input and output spaces, then train inverse models that operate on the resulting coefficient space. This structure allows us to learn deterministic, invertible, and probabilistic models within a single framework, and to choose models based on the degree of ill-posedness. We evaluate our approach on six inverse PDE benchmarks, including two novel datasets, and compare against existing invertible neural operator baselines. We learn probabilistic models that capture uncertainty and input variability, and remain robust to measurement noise due to implicit denoising in the coefficient calculation. Our results show consistent re-simulation performance across varying levels of ill-posedness. By separating representation from inversion, our framework enables scalable surrogate models for inverse problems that generalize across instances, domains, and degrees of ill-posedness.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TraCeR: Transformer-Based Competing Risk Analysis with Longitudinal Covariates</title>
<link>https://arxiv.org/abs/2512.18129</link>
<guid>https://arxiv.org/abs/2512.18129</guid>
<content:encoded><![CDATA[
<div> Keywords: survival analysis, longitudinal covariates, transformer, hazard function, model calibration<br /><br />Summary:<br /><br />1. This paper introduces TraCeR, a novel transformer-based framework for survival analysis that effectively incorporates longitudinal covariates, addressing a limitation of many previous models that mainly use cross-sectional features.<br /><br />2. TraCeR utilizes a factorized self-attention architecture to model the hazard function from sequences of time-varying measurements, enabling it to capture complex temporal interactions among covariates without relying on strict assumptions about the data-generating process.<br /><br />3. The design of TraCeR inherently supports censored data and competing risks, making it applicable to realistic survival analysis scenarios where some event times are unknown or multiple event types may occur.<br /><br />4. Extensive experiments on various real-world datasets demonstrate that TraCeR significantly outperforms existing state-of-the-art survival analysis methods, showing notable improvements in predictive accuracy.<br /><br />5. Beyond traditional evaluation metrics focused on discrimination, the study emphasizes assessing model calibration as well, filling a critical gap in survival model evaluation and ensuring more reliable risk predictions in practice. <div>
arXiv:2512.18129v1 Announce Type: new 
Abstract: Survival analysis is a critical tool for modeling time-to-event data. Recent deep learning-based models have reduced various modeling assumptions including proportional hazard and linearity. However, a persistent challenge remains in incorporating longitudinal covariates, with prior work largely focusing on cross-sectional features, and in assessing calibration of these models, with research primarily focusing on discrimination during evaluation. We introduce TraCeR, a transformer-based survival analysis framework for incorporating longitudinal covariates. Based on a factorized self-attention architecture, TraCeR estimates the hazard function from a sequence of measurements, naturally capturing temporal covariate interactions without assumptions about the underlying data-generating process. The framework is inherently designed to handle censored data and competing events. Experiments on multiple real-world datasets demonstrate that TraCeR achieves substantial and statistically significant performance improvements over state-of-the-art methods. Furthermore, our evaluation extends beyond discrimination metrics and assesses model calibration, addressing a key oversight in literature.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grad: Guided Relation Diffusion Generation for Graph Augmentation in Graph Fraud Detection</title>
<link>https://arxiv.org/abs/2512.18133</link>
<guid>https://arxiv.org/abs/2512.18133</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Fraud Detection, Adaptive Camouflage, relation diffusion, graph contrastive learning, financial security<br /><br />Summary: Graph Fraud Detection (GFD) in financial scenarios is crucial for ensuring online payment security. Fraudsters increasingly use sophisticated camouflage strategies called Adaptive Camouflage, where they mimic benign user behaviors to hide their fraudulent activities, making it hard for current GFD models to detect them. To tackle this challenge, the authors propose Grad, a relation diffusion-based graph augmentation model. Grad integrates a supervised graph contrastive learning module which amplifies the differences between fraudulent and benign users. Additionally, it features a guided relation diffusion generator that creates auxiliary homophilic relations from scratch to strengthen weak fraudulent signals. By enhancing these signals during the aggregation process, fraudulent activities become more distinguishable and detectable. The model’s effectiveness is validated through extensive experiments on two real-world datasets from WeChat Pay—one of the largest online payment platforms—and three public datasets. Grad consistently outperforms state-of-the-art methods across various scenarios, achieving improvements of up to 11.10% in AUC and 43.95% in Average Precision (AP). The authors have released the source code publicly to support further research and development in fraud detection. <div>
arXiv:2512.18133v1 Announce Type: new 
Abstract: Nowadays, Graph Fraud Detection (GFD) in financial scenarios has become an urgent research topic to protect online payment security. However, as organized crime groups are becoming more professional in real-world scenarios, fraudsters are employing more sophisticated camouflage strategies. Specifically, fraudsters disguise themselves by mimicking the behavioral data collected by platforms, ensuring that their key characteristics are consistent with those of benign users to a high degree, which we call Adaptive Camouflage. Consequently, this narrows the differences in behavioral traits between them and benign users within the platform's database, thereby making current GFD models lose efficiency. To address this problem, we propose a relation diffusion-based graph augmentation model Grad. In detail, Grad leverages a supervised graph contrastive learning module to enhance the fraud-benign difference and employs a guided relation diffusion generator to generate auxiliary homophilic relations from scratch. Based on these, weak fraudulent signals would be enhanced during the aggregation process, thus being obvious enough to be captured. Extensive experiments have been conducted on two real-world datasets provided by WeChat Pay, one of the largest online payment platforms with billions of users, and three public datasets. The results show that our proposed model Grad outperforms SOTA methods in both various scenarios, achieving at most 11.10% and 43.95% increases in AUC and AP, respectively. Our code is released at https://github.com/AI4Risk/antifraud and https://github.com/Muyiiiii/WWW25-Grad.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conscious Data Contribution via Community-Driven Chain-of-Thought Distillation</title>
<link>https://arxiv.org/abs/2512.18174</link>
<guid>https://arxiv.org/abs/2512.18174</guid>
<content:encoded><![CDATA[
<div> Keywords: data portability, user autonomy, chain-of-thought reasoning, personal data, model distillation  

<br /><br />Summary:  
This paper addresses the emerging challenges surrounding data portability and user autonomy within the context of large language models (LLMs) that employ chain-of-thought (CoT) reasoning. First, it argues from a legal perspective that the intermediate reasoning steps generated by LLMs—referred to as CoT traces—should be classified as users' personal data under current privacy and portability laws. Building on this, the paper introduces and extends the concept of Conscious Data Contribution, proposing a framework whereby communities dissatisfied with existing LLM utility can collaboratively aggregate and distill their shared domain knowledge into a new, community-aligned model. The authors empirically validate this methodology, demonstrating that it enables the creation of alternate models better tailored to the specific goals and values of distinct user groups. Additionally, the study examines how factors such as community diversity, the granularity of reasoning steps, and the size of the contributing community influence the effectiveness and performance of the model distillation process. Overall, this work highlights a novel intersection of legal considerations, ethical data usage, and technical approaches to empower user communities in asserting greater control over AI model behavior through targeted knowledge distillation. <div>
arXiv:2512.18174v1 Announce Type: new 
Abstract: The current era of AI development places a heavy emphasis on training large models on increasingly scaled-up datasets. This paradigm has catalyzed entirely new product categories, such as LLM chatbots, while also raising concerns about data privacy and consumer choice. In this paper, we consider questions of data portability and user autonomy in the context of LLMs that "reason" using chain-of-thought (CoT) traces, computing intermediate text artifacts from user input before producing a final output. We first interpret recent data privacy and portability law to argue that these intermediate computations qualify as users' personal data. Then, building on the existing framework of Conscious Data Contribution, we show how communities who receive low utility from an available model can aggregate and distill their shared knowledge into an alternate model better aligned with their goals. We verify this approach empirically and investigate the effects of community diversity, reasoning granularity, and community size on distillation performance.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FairExpand: Individual Fairness on Graphs with Partial Similarity Information</title>
<link>https://arxiv.org/abs/2512.18180</link>
<guid>https://arxiv.org/abs/2512.18180</guid>
<content:encoded><![CDATA[
<div> Individual fairness, graph representation learning, similarity propagation, partial similarity information, FairExpand<br /><br />Summary:<br /><br />1. The paper addresses individual fairness in graph representation learning, emphasizing that similar individuals should be treated similarly by algorithmic systems.  
2. Existing methods rely on predefined similarity information across all node pairs, which is impractical in many real-world scenarios due to incomplete knowledge.  
3. The authors propose FairExpand, a novel framework designed to enforce individual fairness when similarity data is only available for a limited subset of node pairs.  
4. FairExpand operates via a two-step iterative process: refining node representations using a backbone model like a graph neural network, and gradually propagating the limited similarity information throughout the graph.  
5. Experimental results demonstrate that FairExpand significantly improves individual fairness metrics while maintaining the overall model performance, proving its practical utility for real-world applications where similarity information is partial or incomplete. <div>
arXiv:2512.18180v1 Announce Type: new 
Abstract: Individual fairness, which requires that similar individuals should be treated similarly by algorithmic systems, has become a central principle in fair machine learning. Individual fairness has garnered traction in graph representation learning due to its practical importance in high-stakes Web areas such as user modeling, recommender systems, and search. However, existing methods assume the existence of predefined similarity information over all node pairs, an often unrealistic requirement that prevents their operationalization in practice. In this paper, we assume the similarity information is only available for a limited subset of node pairs and introduce FairExpand, a flexible framework that promotes individual fairness in this more realistic partial information scenario. FairExpand follows a two-step pipeline that alternates between refining node representations using a backbone model (e.g., a graph neural network) and gradually propagating similarity information, which allows fairness enforcement to effectively expand to the entire graph. Extensive experiments show that FairExpand consistently enhances individual fairness while preserving performance, making it a practical solution for enabling graph-based individual fairness in real-world applications with partial similarity information.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Does Learning Renormalize? Sufficient Conditions for Power Law Spectral Dynamics</title>
<link>https://arxiv.org/abs/2512.18209</link>
<guid>https://arxiv.org/abs/2512.18209</guid>
<content:encoded><![CDATA[
<div> Keywords: power-law scaling, Generalized Resolution-Shell Dynamics, renormalizability, gradient flow, log-shift invariance

<br /><br />Summary:  
This article investigates the theoretical foundation of empirical power-law scaling observed in deep learning systems, focusing on the Generalized Resolution-Shell Dynamics (GRSD) framework, which models learning as spectral energy transport across logarithmic resolution shells. It highlights that power-law scaling corresponds to a renormalized shell dynamics within GRSD but is not guaranteed without specific structural properties of the learning process. The study identifies sufficient conditions for renormalizable coarse-grained shell dynamics, including bounded gradient propagation in the computation graph, weak functional incoherence at initialization, controlled Jacobian evolution during training, and log-shift invariance of renormalized shell couplings. Importantly, the paper shows that renormalizability alone does not produce power-law scaling; instead, this behavior emerges as a rigidity consequence when log-shift invariance is combined with the time-rescaling covariance intrinsic to gradient flow. Under these combined conditions, the renormalized GRSD velocity field necessarily adopts a power-law form, providing a deeper understanding of when and why power-law scaling arises in deep learning dynamics. This work bridges empirical observations with theoretical insights on scaling laws, offering a structured framework to explain learning behavior from a coarse-grained dynamical systems perspective. <div>
arXiv:2512.18209v1 Announce Type: new 
Abstract: Empirical power--law scaling has been widely observed across modern deep learning systems, yet its theoretical origins and scope of validity remain incompletely understood. The Generalized Resolution--Shell Dynamics (GRSD) framework models learning as spectral energy transport across logarithmic resolution shells, providing a coarse--grained dynamical description of training. Within GRSD, power--law scaling corresponds to a particularly simple renormalized shell dynamics; however, such behavior is not automatic and requires additional structural properties of the learning process.
  In this work, we identify a set of sufficient conditions under which the GRSD shell dynamics admits a renormalizable coarse--grained description. These conditions constrain the learning configuration at multiple levels, including boundedness of gradient propagation in the computation graph, weak functional incoherence at initialization, controlled Jacobian evolution along training, and log--shift invariance of renormalized shell couplings. We further show that power--law scaling does not follow from renormalizability alone, but instead arises as a rigidity consequence: once log--shift invariance is combined with the intrinsic time--rescaling covariance of gradient flow, the renormalized GRSD velocity field is forced into a power--law form.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stable and Efficient Single-Rollout RL for Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2512.18215</link>
<guid>https://arxiv.org/abs/2512.18215</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Multimodal Large Language Models, Single-Rollout, Stability, Advantage-Shaping<br /><br />Summary:<br /><br />This paper addresses the challenge of optimizing Reinforcement Learning with Verifiable Rewards (RLVR) for Multimodal Large Language Models (MLLMs), focusing on the trade-off between training stability and efficiency. Existing group-based algorithms like GRPO rely on multiple rollouts per prompt but are computationally expensive. Conversely, single-rollout methods explored in text-only contexts suffer from instability and training collapse when applied to multimodal settings. To overcome these issues, the authors propose MSSR (Multimodal Stabilized Single-Rollout), a novel RLVR framework that eliminates group-based rollouts while ensuring stable and effective training. MSSR incorporates an entropy-based advantage-shaping mechanism that adaptively regulates advantage magnitudes, preventing collapse and preserving stability during training. This approach is shown to be essential for multimodal single-rollout environments, not just beneficial as in group-based settings. Experimental results demonstrate that MSSR achieves comparable validation accuracy to group-based methods using half the training steps, indicating superior compute efficiency. When trained equivalently, MSSR outperforms group-based baselines and generalizes better across five benchmarks focused on complex multimodal reasoning. Overall, MSSR enables stable, efficient, and high-performing reinforcement learning tailored to the demanding context of multimodal reasoning tasks. <div>
arXiv:2512.18215v1 Announce Type: new 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become a key paradigm to improve the reasoning capabilities of Multimodal Large Language Models (MLLMs). However, prevalent group-based algorithms such as GRPO require multi-rollout sampling for each prompt. While more efficient single-rollout variants have recently been explored in text-only settings, we find that they suffer from severe instability in multimodal contexts, often leading to training collapse. To address this training efficiency-stability trade-off, we introduce $\textbf{MSSR}$ (Multimodal Stabilized Single-Rollout), a group-free RLVR framework that achieves both stable optimization and effective multimodal reasoning performance. MSSR achieves this via an entropy-based advantage-shaping mechanism that adaptively regularizes advantage magnitudes, preventing collapse and maintaining training stability. While such mechanisms have been used in group-based RLVR, we show that in the multimodal single-rollout setting they are not merely beneficial but essential for stability. In in-distribution evaluations, MSSR demonstrates superior training compute efficiency, achieving similar validation accuracy to the group-based baseline with half the training steps. When trained for the same number of steps, MSSR's performance surpasses the group-based baseline and shows consistent generalization improvements across five diverse reasoning-intensive benchmarks. Together, these results demonstrate that MSSR enables stable, compute-efficient, and effective RLVR for complex multimodal reasoning tasks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Offline Behavioral Data Selection</title>
<link>https://arxiv.org/abs/2512.18246</link>
<guid>https://arxiv.org/abs/2512.18246</guid>
<content:encoded><![CDATA[
<div> Keywords: Behavioral cloning, Offline policy learning, Data selection, Stepwise Dual Ranking, D4RL benchmarks<br /><br />Summary:<br /><br />1. The paper addresses the challenge of computationally intensive training caused by the large scale of offline behavioral datasets used in behavioral cloning for offline policy learning. 2. It reveals a phenomenon called data saturation, where policy performance quickly plateaus even when trained on a small fraction of the dataset, indicating inefficiencies in using the entire dataset. 3. This saturation is linked to a weak correlation between policy performance and test loss, suggesting that not all data contribute equally to improving performance. 4. To improve data efficiency, the authors propose Stepwise Dual Ranking (SDR), a method that extracts a compact, informative subset from large offline behavioral datasets by prioritizing early-stage data (stepwise clip) and selecting samples based on a combined ranking of high action-value and low state-density. 5. Extensive experiments and ablation studies on the D4RL benchmark suite demonstrate that SDR significantly enhances the quality of data selection, improving offline behavioral cloning performance while reducing computational demands. <div>
arXiv:2512.18246v1 Announce Type: new 
Abstract: Behavioral cloning is a widely adopted approach for offline policy learning from expert demonstrations. However, the large scale of offline behavioral datasets often results in computationally intensive training when used in downstream tasks. In this paper, we uncover the striking data saturation in offline behavioral data: policy performance rapidly saturates when trained on a small fraction of the dataset. We attribute this effect to the weak alignment between policy performance and test loss, revealing substantial room for improvement through data selection. To this end, we propose a simple yet effective method, Stepwise Dual Ranking (SDR), which extracts a compact yet informative subset from large-scale offline behavioral datasets. SDR is build on two key principles: (1) stepwise clip, which prioritizes early-stage data; and (2) dual ranking, which selects samples with both high action-value rank and low state-density rank. Extensive experiments and ablation studies on D4RL benchmarks demonstrate that SDR significantly enhances data selection for offline behavioral data.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Convergence Rate of LoRA Gradient Descent</title>
<link>https://arxiv.org/abs/2512.18248</link>
<guid>https://arxiv.org/abs/2512.18248</guid>
<content:encoded><![CDATA[
<div> LoRA, convergence analysis, gradient descent, low-rank adaptation, non-asymptotic  

<br /><br />Summary:  
The paper analyzes the low-rank adaptation (LoRA) algorithm, which is widely used to fine-tune large models efficiently by training two low-rank adapter matrices, thereby significantly reducing the number of parameters updated during training. Despite its empirical success, the convergence behavior of LoRA remains poorly understood because the problem lacks Lipschitz smoothness, a common assumption in classical convergence proofs. Existing theoretical work either focuses on asymptotic results or imposes strong boundedness assumptions that artificially ensure Lipschitz smoothness, limiting practical relevance. This study provides the first non-asymptotic convergence analysis of the original LoRA gradient descent algorithm under realistic settings without such assumptions. The authors achieve this by reformulating the problem in terms of the outer product of the concatenated adapter matrices, introducing a modified descent lemma tailored to a "Lipschitz-like" property of the reparametrized function, and controlling the step size carefully. Their analysis proves that LoRA gradient descent converges to a stationary point at a rate of \(O(\frac{1}{\log T})\), where \(T\) is the number of iterations, offering new theoretical insights into the optimization dynamics and guarantees for LoRA’s practical use. <div>
arXiv:2512.18248v1 Announce Type: new 
Abstract: The low-rank adaptation (LoRA) algorithm for fine-tuning large models has grown popular in recent years due to its remarkable performance and low computational requirements. LoRA trains two ``adapter" matrices that form a low-rank representation of the model parameters, thereby massively reducing the number of parameters that need to be updated at every step. Although LoRA is simple, its convergence is poorly understood due to the lack of Lipschitz smoothness, a key condition for classic convergence analyses. As a result, current theoretical results only consider asymptotic behavior or assume strong boundedness conditions which artificially enforce Lipschitz smoothness. In this work, we provide for the first time a non-asymptotic convergence analysis of the \textit{original LoRA gradient descent} algorithm, which reflects widespread practice, without such assumptions. Our work relies on three key steps: i) reformulating the problem in terms of the outer product of the stacked adapter matrices, ii) a modified descent lemma for the ``Lipschitz-like" reparametrized function, and iii) controlling the step size. With this approach, we prove that LoRA gradient descent converges to a stationary point at rate $O(\frac{1}{\log T})$, where $T$ is the number of iterations.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LeJOT: An Intelligent Job Cost Orchestration Solution for Databricks Platform</title>
<link>https://arxiv.org/abs/2512.18266</link>
<guid>https://arxiv.org/abs/2512.18266</guid>
<content:encoded><![CDATA[
<div> Keywords: Databricks, job scheduling, cost optimization, machine learning, resource allocation<br /><br />Summary:<br /><br />1. The paper addresses the challenge of managing rising operational costs in job execution on the Databricks platform, a widely used environment for big data processing.<br /><br />2. Traditional cost management approaches are limited by static configurations or reactive mechanisms, which are insufficient for handling the dynamic and variable nature of workloads.<br /><br />3. The authors propose LeJOT, an intelligent orchestration framework that integrates machine learning-based execution time predictions with a solver-driven optimization model for real-time resource allocation.<br /><br />4. LeJOT proactively forecasts workload demands, enabling dynamic allocation of computing resources that minimizes cloud computing expenses without compromising performance targets.<br /><br />5. Experimental evaluation on real Databricks workloads demonstrates that LeJOT can reduce cloud computing costs by an average of 20% within scheduling intervals of about one minute, outperforming conventional static allocation strategies.<br /><br />6. The framework offers a scalable and adaptive solution particularly suited for cost-efficient job scheduling in Data Lakehouse environments, enhancing both operational efficiency and cost-effectiveness. <div>
arXiv:2512.18266v1 Announce Type: new 
Abstract: With the rapid advancements in big data technologies, the Databricks platform has become a cornerstone for enterprises and research institutions, offering high computational efficiency and a robust ecosystem. However, managing the escalating operational costs associated with job execution remains a critical challenge. Existing solutions rely on static configurations or reactive adjustments, which fail to adapt to the dynamic nature of workloads. To address this, we introduce LeJOT, an intelligent job cost orchestration framework that leverages machine learning for execution time prediction and a solver-based optimization model for real-time resource allocation. Unlike conventional scheduling techniques, LeJOT proactively predicts workload demands, dynamically allocates computing resources, and minimizes costs while ensuring performance requirements are met. Experimental results on real-world Databricks workloads demonstrate that LeJOT achieves an average 20% reduction in cloud computing costs within a minute-level scheduling timeframe, outperforming traditional static allocation strategies. Our approach provides a scalable and adaptive solution for cost-efficient job scheduling in Data Lakehouse environments.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedSUM Family: Efficient Federated Learning Methods under Arbitrary Client Participation</title>
<link>https://arxiv.org/abs/2512.18275</link>
<guid>https://arxiv.org/abs/2512.18275</guid>
<content:encoded><![CDATA[
<div> Federated Learning, client participation, FedSUM, delay metrics, convergence guarantees<br /><br />Summary:<br /><br />1. Federated Learning (FL) methods traditionally assume specific client participation patterns, which restrict their effectiveness in real-world applications where client availability varies.  
2. The paper introduces the FedSUM family of algorithms designed to support arbitrary client participation without relying on assumptions about the heterogeneity of client data.  
3. FedSUM models client participation variability through two delay metrics: maximum delay ($\tau_{\max}$) and average delay ($\tau_{\text{avg}}$), capturing different aspects of participation timing.  
4. The FedSUM family consists of three variants tailored for different needs: FedSUM-B (basic), FedSUM (standard), and FedSUM-CR (which reduces communication overhead).  
5. The authors provide unified convergence guarantees for all FedSUM variants, showing their effectiveness across a wide range of client participation scenarios, thereby enhancing the robustness and applicability of FL in diverse practical deployments. <div>
arXiv:2512.18275v1 Announce Type: new 
Abstract: Federated Learning (FL) methods are often designed for specific client participation patterns, limiting their applicability in practical deployments. We introduce the FedSUM family of algorithms, which supports arbitrary client participation without additional assumptions on data heterogeneity. Our framework models participation variability with two delay metrics, the maximum delay $\tau_{\max}$ and the average delay $\tau_{\text{avg}}$. The FedSUM family comprises three variants: FedSUM-B (basic version), FedSUM (standard version), and FedSUM-CR (communication-reduced version). We provide unified convergence guarantees demonstrating the effectiveness of our approach across diverse participation patterns, thereby broadening the applicability of FL in real-world scenarios.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AL-GNN: Privacy-Preserving and Replay-Free Continual Graph Learning via Analytic Learning</title>
<link>https://arxiv.org/abs/2512.18295</link>
<guid>https://arxiv.org/abs/2512.18295</guid>
<content:encoded><![CDATA[
<div> Keywords: Continual Graph Learning, Analytic Learning Theory, Recursive Least Squares, Catastrophic Forgetting, Privacy Preservation  

<br /><br />Summary:  
Continual Graph Learning (CGL) is focused on enabling graph neural networks to progressively learn from streaming graph data without losing previously acquired knowledge. Existing experience replay methods mitigate forgetting by storing and revisiting past graph data but suffer from privacy issues and inefficiency. The proposed AL GNN framework offers a novel solution by eliminating the need for backpropagation and replay buffers. Instead, AL GNN utilizes analytic learning theory to express learning as a recursive least squares optimization, allowing it to update model knowledge through closed-form classifier updates and a regularized feature autocorrelation matrix. This approach enables efficient one-pass training for each task while inherently preserving data privacy since no historical data samples are stored. Extensive experiments on dynamic graph classification benchmarks show that AL GNN achieves competitive or better results compared to existing methods. Empirically, it improves average task performance by 10% on the CoraFull dataset, reduces catastrophic forgetting by over 30% on Reddit, and cuts training time by nearly 50% due to its backpropagation-free nature. This work advances continual learning for graphs by combining efficiency, privacy, and strong performance. <div>
arXiv:2512.18295v1 Announce Type: new 
Abstract: Continual graph learning (CGL) aims to enable graph neural networks to incrementally learn from a stream of graph structured data without forgetting previously acquired knowledge. Existing methods particularly those based on experience replay typically store and revisit past graph data to mitigate catastrophic forgetting. However, these approaches pose significant limitations, including privacy concerns, inefficiency. In this work, we propose AL GNN, a novel framework for continual graph learning that eliminates the need for backpropagation and replay buffers. Instead, AL GNN leverages principles from analytic learning theory to formulate learning as a recursive least squares optimization process. It maintains and updates model knowledge analytically through closed form classifier updates and a regularized feature autocorrelation matrix. This design enables efficient one pass training for each task, and inherently preserves data privacy by avoiding historical sample storage. Extensive experiments on multiple dynamic graph classification benchmarks demonstrate that AL GNN achieves competitive or superior performance compared to existing methods. For instance, it improves average performance by 10% on CoraFull and reduces forgetting by over 30% on Reddit, while also reducing training time by nearly 50% due to its backpropagation free design.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Embedded Safety-Aligned Intelligence via Differentiable Internal Alignment Embeddings</title>
<link>https://arxiv.org/abs/2512.18309</link>
<guid>https://arxiv.org/abs/2512.18309</guid>
<content:encoded><![CDATA[
<div> Embedded Safety-Aligned Intelligence, Multi-agent Reinforcement Learning, Internal Alignment Embeddings, Counterfactual Reasoning, Graph Diffusion<br /><br />Summary:<br /><br />This paper introduces Embedded Safety-Aligned Intelligence (ESAI), a novel theoretical framework for multi-agent reinforcement learning that integrates alignment constraints directly into the agents' internal representations. Instead of relying on external reward shaping or post-hoc safety constraints, ESAI uses differentiable internal alignment embeddings—latent variables learned to predict potential externalized harm through counterfactual reasoning. These embeddings influence policy updates by leveraging attention mechanisms and graph-based propagation to steer agents toward harm reduction. The ESAI framework combines four core mechanisms: differentiable counterfactual alignment penalties derived from soft reference distributions, alignment-weighted perceptual attention to prioritize important information, Hebbian associative memory facilitating temporal credit assignment, and similarity-weighted graph diffusion enhanced with bias mitigation controls. The paper further analyzes conditions for the stability of these internal embeddings, emphasizing Lipschitz continuity and spectral constraints, and studies theoretical properties such as contraction behavior and the tradeoffs between fairness and performance. While ESAI represents an important conceptual advancement in differentiable alignment for multi-agent systems, several open theoretical questions remain regarding convergence guarantees, optimal embedding dimensionality, and scalability to high-dimensional environments. Empirical validation of the framework is left for future work. <div>
arXiv:2512.18309v1 Announce Type: new 
Abstract: We introduce Embedded Safety-Aligned Intelligence (ESAI), a theoretical framework for multi-agent reinforcement learning that embeds alignment constraints directly into agents internal representations using differentiable internal alignment embeddings. Unlike external reward shaping or post-hoc safety constraints, internal alignment embeddings are learned latent variables that predict externalized harm through counterfactual reasoning and modulate policy updates toward harm reduction through attention and graph-based propagation.
  The ESAI framework integrates four mechanisms: differentiable counterfactual alignment penalties computed from soft reference distributions, alignment-weighted perceptual attention, Hebbian associative memory supporting temporal credit assignment, and similarity-weighted graph diffusion with bias mitigation controls. We analyze stability conditions for bounded internal embeddings under Lipschitz continuity and spectral constraints, discuss computational complexity, and examine theoretical properties including contraction behavior and fairness-performance tradeoffs.
  This work positions ESAI as a conceptual contribution to differentiable alignment mechanisms in multi-agent systems. We identify open theoretical questions regarding convergence guarantees, embedding dimensionality, and extension to high-dimensional environments. Empirical evaluation is left to future work.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trustworthy and Explainable Deep Reinforcement Learning for Safe and Energy-Efficient Process Control: A Use Case in Industrial Compressed Air Systems</title>
<link>https://arxiv.org/abs/2512.18317</link>
<guid>https://arxiv.org/abs/2512.18317</guid>
<content:encoded><![CDATA[
<div> compressed air systems, reinforcement learning, energy efficiency, explainability, industrial control<br /><br />Summary:<br /><br />This paper introduces a trustworthy reinforcement learning (RL) framework tailored for controlling industrial compressed air systems with the goal of achieving safe and energy-efficient operation under realistic boundary conditions. The authors develop a multi-level explainability pipeline that integrates input perturbation tests, gradient-based sensitivity analysis, and SHAP feature attribution to ensure transparency and interpretability of the learned RL policy. Empirical evaluations conducted on multiple compressor configurations demonstrate that the learned policy is physically plausible, effectively anticipates future demand, and consistently respects system boundaries, thereby validating its practical applicability. Compared to the traditional industrial controller currently installed, the proposed RL approach reduces unnecessary overpressure and leads to energy savings of approximately 4% without depending on explicit physics-based models. The analysis further reveals that system pressure and forecast information predominantly influence policy decisions, while inputs specific to individual compressors have a lesser impact. Overall, the combination of measurable energy efficiency improvements, predictive control behavior, and a robust explainability methodology supports the reliable and transparent deployment of reinforcement learning techniques in industrial energy management systems. <div>
arXiv:2512.18317v1 Announce Type: new 
Abstract: This paper presents a trustworthy reinforcement learning approach for the control of industrial compressed air systems. We develop a framework that enables safe and energy-efficient operation under realistic boundary conditions and introduce a multi-level explainability pipeline combining input perturbation tests, gradient-based sensitivity analysis, and SHAP (SHapley Additive exPlanations) feature attribution. An empirical evaluation across multiple compressor configurations shows that the learned policy is physically plausible, anticipates future demand, and consistently respects system boundaries. Compared to the installed industrial controller, the proposed approach reduces unnecessary overpressure and achieves energy savings of approximately 4\,\% without relying on explicit physics models. The results further indicate that system pressure and forecast information dominate policy decisions, while compressor-level inputs play a secondary role. Overall, the combination of efficiency gains, predictive behavior, and transparent validation supports the trustworthy deployment of reinforcement learning in industrial energy systems.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Guided Descent: Optimization Algorithms for Training Neural Networks At Scale</title>
<link>https://arxiv.org/abs/2512.18373</link>
<guid>https://arxiv.org/abs/2512.18373</guid>
<content:encoded><![CDATA[
<div> Keywords: neural network optimization, stochastic gradient descent, second-order methods, adaptive learning rates, deep learning training  

<br /><br />Summary:  
This thesis addresses the critical and complex challenge of optimizing neural networks, a foundational aspect of modern AI research. It begins by analyzing classical first-order methods such as stochastic gradient descent (SGD) and adaptive gradient algorithms, highlighting their empirical success but also their theoretical limitations, especially in handling data anisotropy typical of real-world scenarios. Motivated by these limitations, the work explores more advanced techniques that incorporate curvature information, including second-order approximations, layer-wise preconditioning, and adaptive learning rates, which provide sophisticated means to improve training efficiency and model performance. Additionally, the thesis emphasizes the importance of integrating these optimization approaches with broader training strategies like maximal update parametrization, learning rate schedules, and exponential moving averages, which collectively contribute to empirical improvements. By bridging theoretical insights with practical implementations, the paper offers actionable guidelines and strategies for effectively incorporating these advanced optimization techniques into contemporary deep learning workflows, aiming to enhance feature learning, reduce training times, and improve interpretability in foundation models. Ultimately, this research demystifies neural network training by tracing optimization evolution and proposing principled algorithmic solutions. <div>
arXiv:2512.18373v1 Announce Type: new 
Abstract: Neural network optimization remains one of the most consequential yet poorly understood challenges in modern AI research, where improvements in training algorithms can lead to enhanced feature learning in foundation models, order-of-magnitude reductions in training time, and improved interpretability into how networks learn. While stochastic gradient descent (SGD) and its variants have become the de facto standard for training deep networks, their success in these over-parameterized regimes often appears more empirical than principled. This thesis investigates this apparent paradox by tracing the evolution of optimization algorithms from classical first-order methods to modern higher-order techniques, revealing how principled algorithmic design can demystify the training process. Starting from first principles with SGD and adaptive gradient methods, the analysis progressively uncovers the limitations of these conventional approaches when confronted with anisotropy that is representative of real-world data. These breakdowns motivate the exploration of sophisticated alternatives rooted in curvature information: second-order approximation techniques, layer-wise preconditioning, adaptive learning rates, and more. Next, the interplay between these optimization algorithms and the broader neural network training toolkit, which includes prior and recent developments such as maximal update parametrization, learning rate schedules, and exponential moving averages, emerges as equally essential to empirical success. To bridge the gap between theoretical understanding and practical deployment, this paper offers practical prescriptions and implementation strategies for integrating these methods into modern deep learning workflows.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Challenger: When Do New Data Sources Justify Switching Machine Learning Models?</title>
<link>https://arxiv.org/abs/2512.18390</link>
<guid>https://arxiv.org/abs/2512.18390</guid>
<content:encoded><![CDATA[
<div> Keywords: model replacement, learning curves, cost optimization, sequential algorithms, credit scoring<br /><br />Summary:<br /><br />This paper addresses the decision problem of when an organization should replace an incumbent predictive model with a new challenger that incorporates recently available features. It proposes a unified framework that integrates learning curve dynamics, costs related to data acquisition and model retraining, and discounting of future benefits to determine optimal switching timing. The authors first analyze idealized scenarios and derive closed-form expressions that clarify how factors such as the length of the decision horizon, the shape of the learning curve, and differences in cost influence the optimal replacement strategy. They then introduce three practical algorithms for deciding switching timings: a one-shot baseline, a greedy sequential method, and a look-ahead sequential method. Using a real-world credit scoring dataset with gradually introduced alternative data, empirical results show that optimal switching time depends systematically on costs and learning behavior. Moreover, the look-ahead sequential algorithm outperforms the other methods, nearing the performance of an oracle that has complete foresight. Finally, finite-sample theoretical guarantees are provided, demonstrating conditions under which the sequential look-ahead method achieves sublinear regret compared to the oracle. The findings offer a practical guide for economically justified transitions between predictive models as new data sources emerge. <div>
arXiv:2512.18390v1 Announce Type: new 
Abstract: We study the problem of deciding whether, and when an organization should replace a trained incumbent model with a challenger relying on newly available features. We develop a unified economic and statistical framework that links learning-curve dynamics, data-acquisition and retraining costs, and discounting of future gains. First, we characterize the optimal switching time in stylized settings and derive closed-form expressions that quantify how horizon length, learning-curve curvature, and cost differentials shape the optimal decision. Second, we propose three practical algorithms: a one-shot baseline, a greedy sequential method, and a look-ahead sequential method. Using a real-world credit-scoring dataset with gradually arriving alternative data, we show that (i) optimal switching times vary systematically with cost parameters and learning-curve behavior, and (ii) the look-ahead sequential method outperforms other methods and is able to approach in value an oracle with full foresight. Finally, we establish finite-sample guarantees, including conditions under which the sequential look-ahead method achieve sublinear regret relative to that oracle. Our results provide an operational blueprint for economically sound model transitions as new data sources become available.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Why Most Optimism Bandit Algorithms Have the Same Regret Analysis: A Simple Unifying Theorem</title>
<link>https://arxiv.org/abs/2512.18409</link>
<guid>https://arxiv.org/abs/2512.18409</guid>
<content:encoded><![CDATA[
<div> Keywords: stochastic bandits, optimism-based algorithms, logarithmic regret, concentration condition, unified analysis<br /><br />Summary:  
This article addresses several optimism-based stochastic bandit algorithms, including UCB, UCB-V, linear UCB, and finite-arm GP-UCB, highlighting that despite their apparent differences, their regret analyses share a fundamental common structure. The authors isolate the core components underlying these analyses into a minimal set of conditions, primarily a single high-probability concentration condition on the estimators. From this condition, they derive logarithmic regret guarantees using two deterministic lemmas that describe radius collapse—the narrowing confidence bounds—and optimism-forced deviations—choices favored by the optimism principle that guide exploration. This distilled framework provides a unified and near-minimal proof methodology for these classical bandit algorithms, offering clarity and simplicity over previous fragmented proofs. Furthermore, the approach naturally extends to many modern bandit variants, suggesting broad applicability and potential for simplifying regret analyses across the bandit learning literature. This contribution serves both as a unifying theoretical insight and a practical toolset for researchers working on advanced bandit algorithm analyses, promoting greater understanding and ease of verification. <div>
arXiv:2512.18409v1 Announce Type: new 
Abstract: Several optimism-based stochastic bandit algorithms -- including UCB, UCB-V, linear UCB, and finite-arm GP-UCB -- achieve logarithmic regret using proofs that, despite superficial differences, follow essentially the same structure. This note isolates the minimal ingredients behind these analyses: a single high-probability concentration condition on the estimators, after which logarithmic regret follows from two short deterministic lemmas describing radius collapse and optimism-forced deviations. The framework yields unified, near-minimal proofs for these classical algorithms and extends naturally to many contemporary bandit variants.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoE Pathfinder: Trajectory-driven Expert Pruning</title>
<link>https://arxiv.org/abs/2512.18425</link>
<guid>https://arxiv.org/abs/2512.18425</guid>
<content:encoded><![CDATA[
<div> Mixture-of-experts, expert pruning, large language models, trajectory-based selection, global optimal path planning<br /><br />Summary:  
The paper addresses practical challenges in deploying mixture-of-experts (MoE) architectures within large language models, specifically focusing on reducing computational overhead through expert pruning. Unlike existing pruning methods that use local importance metrics and uniformly prune experts across layers, this work proposes a novel approach that considers the activation trajectory of experts throughout the layers. By treating the MoE model as a weighted computation graph, the expert selection problem is formulated as a global optimal path planning task, enabling a comprehensive, trajectory-level evaluation. The method integrates multiple complementary importance signals, including reconstruction error, routing probabilities, and activation strength, to capture the diverse contributions of experts more effectively. This framework naturally allows for non-uniform expert pruning, preserving experts based on their global importance rather than local criteria. Experimental results demonstrate that this trajectory-based expert pruning outperforms most existing methods across a wide range of tasks, achieving better pruning efficiency and maintaining performance. The approach simplifies MoE deployment by optimizing expert retention patterns and reducing unnecessary computation without significant loss in model capability. Overall, this research offers a more effective and principled strategy for expert pruning in MoE-based large language models. <div>
arXiv:2512.18425v1 Announce Type: new 
Abstract: Mixture-of-experts (MoE) architectures used in large language models (LLMs) achieve state-of-the-art performance across diverse tasks yet face practical challenges such as deployment complexity and low activation efficiency. Expert pruning has thus emerged as a promising solution to reduce computational overhead and simplify the deployment of MoE models. However, existing expert pruning approaches conventionally rely on local importance metrics and often apply uniform layer-wise pruning, leveraging only partial evaluation signals and overlooking the heterogeneous contributions of experts across layers. To address these limitations, we propose an expert pruning approach based on the trajectory of activated experts across layers, which treats MoE as a weighted computation graph and casts expert selection as a global optimal path planning problem. Within this framework, we integrate complementary importance signals from reconstruction error, routing probabilities, and activation strength at the trajectory level, which naturally yields non-uniform expert retention across layers. Experiments show that our approach achieves superior pruning performance on nearly all tasks compared with most existing approaches.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Universality of Transformer Architectures; How Much Attention Is Enough?</title>
<link>https://arxiv.org/abs/2512.18445</link>
<guid>https://arxiv.org/abs/2512.18445</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformers, universality, expressiveness, architectural refinements, theoretical research  

<br /><br />Summary:  
Transformers have become fundamental across various AI domains, including large language models, computer vision, and reinforcement learning, due to their believed universality and scalability compared to other architectures. This article addresses the critical question of the universality of Transformers, investigating whether and how they can approximate a wide class of functions or computations. It reviews recent advancements such as improvements in structural minimality, which focus on simplifying the Transformer architecture without losing expressive power, and examines results related to approximation rates that quantify how efficiently Transformers can represent target functions. The work also surveys state-of-the-art theoretical and empirical progress to distinguish between guarantees about Transformer expressiveness that are well-founded and those that may be less robust or more fragile. By clarifying the current landscape of knowledge, the paper aims to provide a clear understanding of the expressive capabilities of Transformers. Furthermore, it identifies important open questions and promising directions for future theoretical research to better comprehend and potentially improve the universality and practical utility of Transformer models. <div>
arXiv:2512.18445v1 Announce Type: new 
Abstract: Transformers are crucial across many AI fields, such as large language models, computer vision, and reinforcement learning. This prominence stems from the architecture's perceived universality and scalability compared to alternatives. This work examines the problem of universality in Transformers, reviews recent progress, including architectural refinements such as structural minimality and approximation rates, and surveys state-of-the-art advances that inform both theoretical and practical understanding. Our aim is to clarify what is currently known about Transformers expressiveness, separate robust guarantees from fragile ones, and identify key directions for future theoretical research.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Secret mixtures of experts inside your LLM</title>
<link>https://arxiv.org/abs/2512.18452</link>
<guid>https://arxiv.org/abs/2512.18452</guid>
<content:encoded><![CDATA[
<div> MLP, Mixture of Experts, Sparse Autoencoder, Large Language Models, Activation Distribution<br /><br />Summary:<br /><br />1. The paper investigates the role and function of Multilayer Perceptron (MLP) layers within transformer architectures, particularly in large language models (LLMs), noting that these layers are dense and lack straightforward interpretability. <br /><br />2. It proposes the novel hypothesis that MLP layers approximate sparse computations, effectively functioning like sparsely-activating Mixture of Experts (MoE) layers, despite their dense appearance. <br /><br />3. This hypothesis is supported by a newly uncovered theoretical link between MoE models and the structure of Sparse Autoencoders (SAE) in the activation space of neural networks. <br /><br />4. Empirical validation on pretrained LLMs confirms that the sparse approximation hypothesis holds true, with a key insight that the activation distributions’ structure is essential for this effect; random Gaussian data do not replicate these results. <br /><br />5. The findings elucidate why MoE-based transformers are effective and inspire future research towards more efficient MoE architectures, proposing designs incorporating low-rank routing mechanisms to better exploit sparsity. <div>
arXiv:2512.18452v1 Announce Type: new 
Abstract: Despite being one of the earliest neural network layers, the Multilayer Perceptron (MLP) is arguably one of the least understood parts of the transformer architecture due to its dense computation and lack of easy visualization. This paper seeks to understand the MLP layers in dense LLM models by hypothesizing that these layers secretly approximately perform a sparse computation -- namely, that they can be well approximated by sparsely-activating Mixture of Experts (MoE) layers.
  Our hypothesis is based on a novel theoretical connection between MoE models and Sparse Autoencoder (SAE) structure in activation space. We empirically validate the hypothesis on pretrained LLMs, and demonstrate that the activation distribution matters -- these results do not hold for Gaussian data, but rather rely crucially on structure in the distribution of neural network activations.
  Our results shine light on a general principle at play in MLP layers inside LLMs, and give an explanation for the effectiveness of modern MoE-based transformers. Additionally, our experimental explorations suggest new directions for more efficient MoE architecture design based on low-rank routers.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NOVA: Discovering Well-Conditioned Winograd Transforms through Numerical Optimization of Vandermonde Arithmetic</title>
<link>https://arxiv.org/abs/2512.18453</link>
<guid>https://arxiv.org/abs/2512.18453</guid>
<content:encoded><![CDATA[
<div> Winograd convolution, numerical instability, optimization, FP16 inference, fractional transforms<br /><br />Summary:<br /><br />1. Winograd convolution is widely used to accelerate inference by reducing arithmetic complexity, notably by 2.25x for 3x3 kernels.<br />2. However, scaling Winograd tiles to larger sizes (e.g., F(6,3), F(8,3)) faces severe numerical instability due to exploding condition numbers, especially in low precision formats like FP16 and Int8.<br />3. NOVA (Numerical Optimization of Vandermonde Arithmetic) is introduced as a new framework that treats selecting Winograd points as a continuous optimization problem rather than relying on traditional integer-based interpolation.<br />4. Using Evolution Strategies, NOVA explores the real-valued space, snaps solutions to simple rationals, and verifies correctness symbolically.<br />5. NOVA discovers stable, fractional point configurations that significantly improve conditioning—for F(8,3), a 415x improvement in 1D conditioning is achieved, which equates to over 170,000x improvement for 2D convolutions.<br />6. In practical FP16 ImageNet inference, NOVA’s transforms avoid the collapse to random chance seen in standard transforms, restoring accuracy from around 4.7% to between 75% and 78% on VGG16 without any retraining or calibration.<br />7. These transforms can be used as drop-in replacements, enabling efficient and stable large-tile Winograd convolution on modern low-precision hardware. <div>
arXiv:2512.18453v1 Announce Type: new 
Abstract: Winograd convolution is the standard algorithm for efficient inference, reducing arithmetic complexity by 2.25x for 3x3 kernels. However, it faces a critical barrier in the modern era of low precision computing: numerical instability. As tiles scale to maximize efficiency (e.g., F(6,3), F(8,3)), the condition numbers of standard integer based transforms explode, reaching kappa = 2 x 10^5 for F(8,3), rendering them unusable in FP16 or Int8. We introduce NOVA (Numerical Optimization of Vandermonde Arithmetic), a discovery framework that breaks the decades old convention of integer interpolation. Treating Winograd point selection as a continuous optimization problem, NOVA searches the manifold R^n-1 via Evolution Strategy, snaps candidates to simple rationals, and guarantees correctness via symbolic verification. This process uncovers a hidden landscape of stable, fractional configurations such as {+-5/6, +-7/6, +-3/5} that defy traditional vocabulary constraints. The impact is transformative: NOVA improves the conditioning of F(8,3) by 415x in 1D, which squares to a 172,484x improvement for 2D convolution. In real world FP16 ImageNet inference, where standard transforms collapse to random chance (e.g., 4.7 percent accuracy on VGG16), NOVA's points restore full accuracy (75 to 78 percent), recovering over 70 percentage points without retraining, calibration, or learned parameters. These discovered transforms act as drop in replacements, effectively unlocking the efficiency of large tile Winograd convolution for next generation hardware.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Out-of-Distribution Detection in Molecular Complexes via Diffusion Models for Irregular Graphs</title>
<link>https://arxiv.org/abs/2512.18454</link>
<guid>https://arxiv.org/abs/2512.18454</guid>
<content:encoded><![CDATA[
<div> Keywords: OOD detection, 3D graphs, diffusion model, protein-ligand complexes, geometric deep learning<br /><br />Summary:<br />1. The paper addresses the challenge of out-of-distribution (OOD) detection for irregular 3D graph data, which contain both continuous geometric data and discrete categorical features and lack an inherent ordering.  
2. The authors propose a probabilistic OOD detection framework based on a diffusion model that learns the density of the training data in a fully unsupervised way, enabling reliable identification of OOD samples.  
3. A novel contribution is the introduction of a unified continuous diffusion process acting on both the 3D coordinates and discrete categorical identities, where categorical features are embedded into a continuous space and trained with cross-entropy loss; the diffusion score for discrete features is calculated analytically via posterior-mean interpolation from predicted class probabilities.  
4. This approach yields a single probability-flow ODE (PF-ODE) that produces per-sample log-likelihood estimates, which serve as principled typicality scores to detect distribution shifts.  
5. The method is validated on protein-ligand complex datasets, where entire protein families are withheld during training to simulate strict OOD scenarios; PF-ODE likelihoods successfully identify held-out families as OOD and correlate with errors in an independent binding-affinity model, demonstrating utility for a priori reliability estimation.  
6. Beyond scalar likelihoods, additional multi-scale trajectory statistics derived from PF-ODE flows—such as path tortuosity, flow stiffness, and vector-field instability—offer complementary OOD indicators.  
7. Combining these trajectory features into a joint model produces a highly sensitive OOD detector that outperforms likelihood-only baselines, providing a practical, label-free workflow for OOD quantification in geometric deep learning settings. <div>
arXiv:2512.18454v1 Announce Type: new 
Abstract: Predictive machine learning models generally excel on in-distribution data, but their performance degrades on out-of-distribution (OOD) inputs. Reliable deployment therefore requires robust OOD detection, yet this is particularly challenging for irregular 3D graphs that combine continuous geometry with categorical identities and are unordered by construction. Here, we present a probabilistic OOD detection framework for complex 3D graph data built on a diffusion model that learns a density of the training distribution in a fully unsupervised manner. A key ingredient we introduce is a unified continuous diffusion over both 3D coordinates and discrete features: categorical identities are embedded in a continuous space and trained with cross-entropy, while the corresponding diffusion score is obtained analytically via posterior-mean interpolation from predicted class probabilities. This yields a single self-consistent probability-flow ODE (PF-ODE) that produces per-sample log-likelihoods, providing a principled typicality score for distribution shift. We validate the approach on protein-ligand complexes and construct strict OOD datasets by withholding entire protein families from training. PF-ODE likelihoods identify held-out families as OOD and correlate strongly with prediction errors of an independent binding-affinity model (GEMS), enabling a priori reliability estimates on new complexes. Beyond scalar likelihoods, we show that multi-scale PF-ODE trajectory statistics - including path tortuosity, flow stiffness, and vector-field instability - provide complementary OOD information. Modeling the joint distribution of these trajectory features yields a practical, high-sensitivity detector that improves separation over likelihood-only baselines, offering a label-free OOD quantification workflow for geometric deep learning.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-organizing maps for water quality assessment in reservoirs and lakes: A systematic literature review</title>
<link>https://arxiv.org/abs/2512.18466</link>
<guid>https://arxiv.org/abs/2512.18466</guid>
<content:encoded><![CDATA[
<div> Keywords: Self-Organizing Map, water quality assessment, ecological monitoring, multidimensional data, sustainable management<br /><br />Summary:<br /><br />This review article explores the application of the Self-Organizing Map (SOM), an unsupervised artificial intelligence technique, in assessing and managing water quality in lakes and reservoirs. It addresses the challenges posed by data sparsity, heterogeneity, and complex nonlinear relationships among water quality parameters. The study synthesizes existing research focused on parameter selection, spatial and temporal sampling strategies, and various clustering approaches integrated with SOM. Emphasizing SOM's capability to process and visualize multidimensional data, the review highlights how it uncovers hidden ecological patterns and critical correlations that support effective water resource management. The increasing availability of environmental data from sensors, remote sensing, IoT devices, and historical datasets is noted as a major enabler of enhanced monitoring and analysis. The review also underscores SOM’s versatility across different ecological applications, including trophic state classification, algal bloom monitoring, and evaluating catchment area impacts. By providing comprehensive insights into SOM methodologies and use cases, the article aims to guide future research and practical efforts toward improving sustainable water quality monitoring and management of lake and reservoir ecosystems. <div>
arXiv:2512.18466v1 Announce Type: new 
Abstract: Sustainable water quality underpins ecological balance and water security. Assessing and managing lakes and reservoirs is difficult due to data sparsity, heterogeneity, and nonlinear relationships among parameters. This review examines how Self-Organizing Map (SOM), an unsupervised AI technique, is applied to water quality assessment. It synthesizes research on parameter selection, spatial and temporal sampling strategies, and clustering approaches. Emphasis is placed on how SOM handles multidimensional data and uncovers hidden patterns to support effective water management. The growing availability of environmental data from in-situ sensors, remote sensing imagery, IoT technologies, and historical records has significantly expanded analytical opportunities in environmental monitoring. SOM has proven effective in analysing complex datasets, particularly when labelled data are limited or unavailable. It enables high-dimensional data visualization, facilitates the detection of hidden ecological patterns, and identifies critical correlations among diverse water quality indicators. This review highlights SOMs versatility in ecological assessments, trophic state classification, algal bloom monitoring, and catchment area impact evaluations. The findings offer comprehensive insights into existing methodologies, supporting future research and practical applications aimed at improving the monitoring and sustainable management of lake and reservoir ecosystems.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Geometry of Abstraction: Continual Learning via Recursive Quotienting</title>
<link>https://arxiv.org/abs/2512.18471</link>
<guid>https://arxiv.org/abs/2512.18471</guid>
<content:encoded><![CDATA[
<div> Keywords: continual learning, geometric barrier, recursive metric contraction, topological deformation, catastrophic forgetting<br /><br />Summary:<br /><br />This article addresses a fundamental geometric issue in continual learning systems known as the flat manifold problem, where the linear growth of geodesic distance in fixed-dimensional Euclidean spaces leads to inevitable trajectory overlap and catastrophic interference. To overcome this, the authors propose Recursive Metric Contraction, a geometric solution that formalizes abstraction as a topological deformation via quotient maps collapsing the metric tensor within temporal neighborhoods, effectively reducing local manifold diameters to zero. Four key theoretical results support this framework: (1) The Bounded Capacity Theorem shows that using recursive quotient maps enables embedding arbitrarily long trajectories into bounded volumes, trading linear metric growth for logarithmic topological depth. (2) The Topological Collapse Separability Theorem demonstrates that recursive quotienting transforms non-linearly separable temporal sequences into linearly separable ones without requiring infinite-dimensional kernel projections. (3) The Parity-Partitioned Stability Theorem proves that by partitioning the state space into orthogonal flow and scaffold manifolds, metric deformations during active learning preserve memory stability, resolving catastrophic forgetting. (4) The analysis suggests that neural tokens correspond physically to singularities or wormholes—regions of extreme positive curvature that connect distant points on the temporal manifold—offering a novel insight into neural architecture representations. <div>
arXiv:2512.18471v1 Announce Type: new 
Abstract: Continual learning systems operating in fixed-dimensional spaces face a fundamental geometric barrier: the flat manifold problem. When experience is represented as a linear trajectory in Euclidean space, the geodesic distance between temporal events grows linearly with time, forcing the required covering number to diverge. In fixed-dimensional hardware, this volume expansion inevitably forces trajectory overlap, manifesting as catastrophic interference. In this work, we propose a geometric resolution to this paradox based on Recursive Metric Contraction. We formalize abstraction not as symbolic grouping, but as a topological deformation: a quotient map that collapses the metric tensor within validated temporal neighborhoods, effectively driving the diameter of local sub-manifolds to zero. We substantiate our framework with four rigorous results. First, the Bounded Capacity Theorem establishes that recursive quotient maps allow the embedding of arbitrarily long trajectories into bounded representational volumes, trading linear metric growth for logarithmic topological depth. Second, the Topological Collapse Separability Theorem, derived via Urysohn's Lemma, proves that recursive quotienting renders non-linearly separable temporal sequences linearly separable in the limit, bypassing the need for infinite-dimensional kernel projections. Third, the Parity-Partitioned Stability Theorem solves the catastrophic forgetting problem by proving that if the state space is partitioned into orthogonal flow and scaffold manifolds, the metric deformations of active learning do not disturb the stability of stored memories. Our analysis reveals that tokens in neural architectures are physically realizable as singularities or wormholes, regions of extreme positive curvature that bridge distant points in the temporal manifold.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>APC-GNN++: An Adaptive Patient-Centric GNN with Context-Aware Attention and Mini-Graph Explainability for Diabetes Classification</title>
<link>https://arxiv.org/abs/2512.18473</link>
<guid>https://arxiv.org/abs/2512.18473</guid>
<content:encoded><![CDATA[
<div> Keywords: APC-GNN++, diabetes classification, graph neural network, patient-centric, explainable predictions<br /><br />Summary:  
1. This paper introduces APC-GNN++, an adaptive patient-centric Graph Neural Network designed specifically for diabetes classification.  
2. The model incorporates context-aware edge attention, confidence-guided blending of node features with graph representations, and neighborhood consistency regularization to more effectively capture clinically significant relationships between patients.  
3. To address the challenge of classifying unseen patients, a mini-graph approach is proposed that leverages nearest neighbors of the new patient, allowing for real-time, explainable predictions without needing to retrain the global model.  
4. APC-GNN++ is evaluated on a real-world diabetes dataset from a regional hospital in Algeria, where it demonstrates superior performance over traditional machine learning algorithms such as MLP, Random Forest, XGBoost, and a basic Graph Convolutional Network (GCN), achieving higher test accuracy and macro F1-score.  
5. The study also provides interpretability by analyzing node-level confidence scores, showing how the model balances information derived from each patient and their graph context to deliver patient-centric insights. Additionally, a Tkinter-based graphical user interface (GUI) is developed for interactive use by healthcare professionals, facilitating practical deployment. <div>
arXiv:2512.18473v1 Announce Type: new 
Abstract: We propose APC-GNN++, an adaptive patient-centric Graph Neural Network for diabetes classification. Our model integrates context-aware edge attention, confidence-guided blending of node features and graph representations, and neighborhood consistency regularization to better capture clinically meaningful relationships between patients. To handle unseen patients, we introduce a mini-graph approach that leverages the nearest neighbors of the new patient, enabling real-time explainable predictions without retraining the global model. We evaluate APC-GNN++ on a real-world diabetes dataset collected from a regional hospital in Algeria and show that it outperforms traditional machine learning models (MLP, Random Forest, XGBoost) and a vanilla GCN, achieving higher test accuracy and macro F1- score. The analysis of node-level confidence scores further reveals how the model balances self-information and graph-based evidence across different patient groups, providing interpretable patient-centric insights. The system is also embedded in a Tkinter-based graphical user interface (GUI) for interactive use by healthcare professionals .
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prediction and Forecast of Short-Term Drought Impacts Using Machine Learning to Support Mitigation and Adaptation Efforts</title>
<link>https://arxiv.org/abs/2512.18522</link>
<guid>https://arxiv.org/abs/2512.18522</guid>
<content:encoded><![CDATA[
<div> Drought, Machine Learning, Impact Forecasting, XGBoost, New Mexico<br /><br />Summary:<br /><br />1. The study addresses drought as a complex natural hazard affecting both ecological and human systems, with increasing severity and frequency necessitating improved monitoring and mitigation.<br /><br />2. It emphasizes predicting drought impacts rather than merely drought conditions to enhance early warning systems and proactive decision-making.<br /><br />3. Machine learning techniques, specifically eXtreme Gradient Boosting (XGBoost), were applied to link drought indices—the Drought Severity and Coverage Index (DSCI) and the Evaporative Stress Index (ESI)—with historical drought impact data from 2005 to 2024.<br /><br />4. The study focused on modeling and forecasting weekly drought impacts, successfully predicting Fire and Relief impacts with the highest accuracy, followed by Agriculture and Water, while impacts on Plants and Society were more variable.<br /><br />5. The model generated forecasts for New Mexico at county and state levels up to eight weeks in advance, using the preceding eight weeks of data.<br /><br />6. This approach supports the development of an Ecological Drought Information Communication System (EcoDri) for New Mexico and shows potential for wider application in other drought-prone regions.<br /><br />7. The findings aim to assist stakeholders, land managers, and decision-makers in implementing more effective drought mitigation and adaptation strategies. <div>
arXiv:2512.18522v1 Announce Type: new 
Abstract: Drought is a complex natural hazard that affects ecological and human systems, often resulting in substantial environmental and economic losses. Recent increases in drought severity, frequency, and duration underscore the need for effective monitoring and mitigation strategies. Predicting drought impacts rather than drought conditions alone offers opportunities to support early warning systems and proactive decision-making. This study applies machine learning techniques to link drought indices with historical drought impact records (2005:2024) to generate short-term impact forecasts. By addressing key conceptual and data-driven challenges regarding temporal scale and impact quantification, the study aims to improve the predictability of drought impacts at actionable lead times. The Drought Severity and Coverage Index (DSCI) and the Evaporative Stress Index (ESI) were combined with impact data from the Drought Impact Reporter (DIR) to model and forecast weekly drought impacts. Results indicate that Fire and Relief impacts were predicted with the highest accuracy, followed by Agriculture and Water, while forecasts for Plants and Society impacts showed greater variability. County and state level forecasts for New Mexico were produced using an eXtreme Gradient Boosting (XGBoost) model that incorporated both DSCI and ESI. The model successfully generated forecasts up to eight weeks in advance using the preceding eight weeks of data for most impact categories. This work supports the development of an Ecological Drought Information Communication System (EcoDri) for New Mexico and demonstrates the potential for broader application in similar drought-prone regions. The findings can aid stakeholders, land managers, and decision-makers in developing and implementing more effective drought mitigation and adaptation strategies.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feature-Enhanced Graph Neural Networks for Classification of Synthetic Graph Generative Models: A Benchmarking Study</title>
<link>https://arxiv.org/abs/2512.18524</link>
<guid>https://arxiv.org/abs/2512.18524</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, Graph classification, Synthetic graph models, Feature selection, Generative graph families<br /><br />Summary:<br /><br />This paper addresses the problem of discriminating between different generative graph models, which is important for understanding complex structural patterns in both synthetic and real-world graphs. The authors propose a hybrid approach combining Graph Neural Networks (GNNs) with engineered graph-theoretic features to improve classification performance. They create a large synthetic dataset consisting of graphs from five representative generative families: Erdos-Renyi, Watts-Strogatz, Barabási-Albert, Holme-Kim, and the Stochastic Block Model, with graphs containing up to 10,000 nodes and 110,000 edges. A wide range of node- and graph-level features is extracted and then pruned using a Random Forest-based feature selection method to retain the most relevant features. These selected features are integrated into six different GNN architectures: GCN, GAT, GATv2, GIN, GraphSAGE, and GTN. Hyperparameters for each model are optimized using the Optuna framework. The evaluation shows that GraphSAGE and GTN achieve the best results, with classification accuracy reaching 98.5%, and clear class separation is visualized through t-SNE and UMAP plots. Other models like GCN and GIN also perform well, while GAT-based models perform worse due to their limited capacity to capture global graph structures. A Support Vector Machine baseline trained solely on handcrafted features confirms the added value of message passing in GNNs for enhanced performance and meaningful class separation. <div>
arXiv:2512.18524v1 Announce Type: new 
Abstract: The ability to discriminate between generative graph models is critical to understanding complex structural patterns in both synthetic graphs and the real-world structures that they emulate. While Graph Neural Networks (GNNs) have seen increasing use to great effect in graph classification tasks, few studies explore their integration with interpretable graph theoretic features. This paper investigates the classification of synthetic graph families using a hybrid approach that combines GNNs with engineered graph-theoretic features. We generate a large and structurally diverse synthetic dataset comprising graphs from five representative generative families, Erdos-Renyi, Watts-Strogatz, Barab'asi-Albert, Holme-Kim, and Stochastic Block Model. These graphs range in size up to 1x10^4 nodes, containing up to 1.1x10^5 edges. A comprehensive range of node and graph level features is extracted for each graph and pruned using a Random Forest based feature selection pipeline. The features are integrated into six GNN architectures: GCN, GAT, GATv2, GIN, GraphSAGE and GTN. Each architecture is optimised for hyperparameter selection using Optuna. Finally, models were compared against a baseline Support Vector Machine (SVM) trained solely on the handcrafted features. Our evaluation demonstrates that GraphSAGE and GTN achieve the highest classification performance, with 98.5% accuracy, and strong class separation evidenced by t-SNE and UMAP visualisations. GCN and GIN also performed well, while GAT-based models lagged due to limitations in their ability to capture global structures. The SVM baseline confirmed the importance of the message passing functionality for performance gains and meaningful class separation.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparing Dynamical Models Through Diffeomorphic Vector Field Alignment</title>
<link>https://arxiv.org/abs/2512.18566</link>
<guid>https://arxiv.org/abs/2512.18566</guid>
<content:encoded><![CDATA[
<div> Keywords: dynamical systems, recurrent neural networks, coordinate transformation, topological equivalence, invariant manifolds<br /><br />Summary: This paper addresses challenges in analyzing dynamical systems models like recurrent neural networks (RNNs) used in theoretical neuroscience. First, it highlights the difficulty of comparing learned dynamics across models due to the lack of coordinate system equivalence. Second, it points out the intractability of identifying important low-dimensional dynamical motifs in high-dimensional nonlinear systems. To overcome these issues, the authors introduce DFORM (Diffeomorphic vector field alignment FOR learned Models), a framework that learns nonlinear coordinate transformations to align trajectories between two dynamical systems maximally one-to-one. By doing so, DFORM enables assessment of topological equivalence, revealing similar mechanisms despite different coordinate systems. Additionally, DFORM aids in locating key dynamical motifs such as invariant manifolds and saddle limit sets embedded within high-dimensional spaces. The method was validated on canonical topologically equivalent systems, RNNs, and systems related by nonlinear flows, demonstrating its ability to identify both linear and nonlinear coordinate transformations. DFORM can also quantify the similarity between topologically distinct systems. Finally, the framework was applied to RNNs trained on human fMRI data, successfully identifying limit cycles consistent with prior numerical analyses, showcasing its utility for data-driven neuroscientific models. <div>
arXiv:2512.18566v1 Announce Type: new 
Abstract: Dynamical systems models such as recurrent neural networks (RNNs) are increasingly popular in theoretical neuroscience for hypothesis-generation and data analysis. Evaluating the dynamics in such models is key to understanding their learned generative mechanisms. However, such evaluation is impeded by two major challenges: First, comparison of learned dynamics across models is difficult because there is no enforced equivalence of their coordinate systems. Second, identification of mechanistically important low-dimensional motifs (e.g., limit sets) is intractable in high-dimensional nonlinear models such as RNNs. Here, we propose a comprehensive framework to address these two issues, termed Diffeomorphic vector field alignment FOR learned Models (DFORM). DFORM learns a nonlinear coordinate transformation between the state spaces of two dynamical systems, which aligns their trajectories in a maximally one-to-one manner. In so doing, DFORM enables an assessment of whether two models exhibit topological equivalence, i.e., similar mechanisms despite differences in coordinate systems. A byproduct of this method is a means to locate dynamical motifs on low-dimensional manifolds embedded within higher-dimensional systems. We verified DFORM's ability to identify linear and nonlinear coordinate transformations using canonical topologically equivalent systems, RNNs, and systems related by nonlinear flows. DFORM was also shown to provide a quantification of similarity between topologically distinct systems. We then demonstrated that DFORM can locate important dynamical motifs including invariant manifolds and saddle limit sets within high-dimensional models. Finally, using a set of RNN models trained on human functional MRI (fMRI) recordings, we illustrated that DFORM can identify limit cycles from high-dimensional data-driven models, which agreed well with prior numerical analysis.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modality-Dependent Memory Mechanisms in Cross-Modal Neuromorphic Computing</title>
<link>https://arxiv.org/abs/2512.18575</link>
<guid>https://arxiv.org/abs/2512.18575</guid>
<content:encoded><![CDATA[
<div> Keywords: spiking neural networks, memory mechanisms, cross-modal, energy efficiency, neuromorphic computing<br /><br />Summary:  
This paper investigates memory-augmented spiking neural networks (SNNs) to understand their generalization ability across different sensory modalities, specifically visual and auditory tasks. The authors conduct the first comprehensive cross-modal ablation study evaluating memory mechanisms including Hopfield networks, Hierarchical Gated Recurrent Networks (HGRNs), and supervised contrastive learning (SCL). They test these methods on neuromorphic datasets: N-MNIST (visual) and SHD (auditory). Their evaluation of five architectures reveals that Hopfield networks excel in visual tasks with 97.68% accuracy but perform poorly on auditory tasks (76.15%), showing significant modality-dependent specialization. In contrast, SCL provides a more balanced performance across modalities (96.72% visual, 82.16% auditory). The study demonstrates that memory mechanisms offer task-specific advantages rather than universal benefits. Furthermore, joint multi-modal training with HGRN attains 94.41% accuracy in visual and 79.37% in auditory tasks, with an average accuracy of 88.78%, rivaling parallel specialized HGRN models but through a unified architecture. Engram analysis reveals weak similarity (0.038) between modalities, justifying the architecture choices. Notably, the proposed approaches achieve a remarkable 603-fold improvement in energy efficiency compared to traditional neural networks, highlighting their potential for neuromorphic computing. <div>
arXiv:2512.18575v1 Announce Type: new 
Abstract: Memory-augmented spiking neural networks (SNNs) promise energy-efficient neuromorphic computing, yet their generalization across sensory modalities remains unexplored. We present the first comprehensive cross-modal ablation study of memory mechanisms in SNNs, evaluating Hopfield networks, Hierarchical Gated Recurrent Networks (HGRNs), and supervised contrastive learning (SCL) across visual (N-MNIST) and auditory (SHD) neuromorphic datasets. Our systematic evaluation of five architectures reveals striking modality-dependent performance patterns: Hopfield networks achieve 97.68% accuracy on visual tasks but only 76.15% on auditory tasks (21.53 point gap), revealing severe modality-specific specialization, while SCL demonstrates more balanced cross-modal performance (96.72% visual, 82.16% audio, 14.56 point gap). These findings establish that memory mechanisms exhibit task-specific benefits rather than universal applicability. Joint multi-modal training with HGRN achieves 94.41% visual and 79.37% audio accuracy (88.78% average), matching parallel HGRN performance through unified deployment. Quantitative engram analysis confirms weak cross-modal alignment (0.038 similarity), validating our parallel architecture design. Our work provides the first empirical evidence for modality-specific memory optimization in neuromorphic systems, achieving 603x energy efficiency over traditional neural networks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SD2AIL: Adversarial Imitation Learning from Synthetic Demonstrations via Diffusion Models</title>
<link>https://arxiv.org/abs/2512.18583</link>
<guid>https://arxiv.org/abs/2512.18583</guid>
<content:encoded><![CDATA[
<div> Keywords: Adversarial Imitation Learning, Diffusion Models, Synthetic Demonstrations, Prioritized Expert Demonstration Replay, Policy Optimization<br /><br />Summary:<br /><br />1. The paper addresses Adversarial Imitation Learning (AIL), a key framework in imitation learning that uses expert demonstrations to infer rewards for policy optimization.<br />2. Collecting large amounts of expert demonstrations is often difficult, which motivates the use of synthetic demonstrations.<br />3. SD2AIL is proposed, a novel method that employs diffusion models within the discriminator to generate synthetic demonstrations, augmenting the expert data.<br />4. To efficiently use the large pool of synthetic and expert demonstrations, the authors introduce the Prioritized Expert Demonstration Replay (PEDR) strategy, which focuses on replaying the most valuable demonstrations.<br />5. Experimental results on simulation tasks, including the Hopper benchmark, show that SD2AIL outperforms previous state-of-the-art methods, achieving an average return of 3441, which is 89 points higher.<br />6. The approach demonstrates both effectiveness and robustness across tested environments.<br />7. The authors commit to open-sourcing their code to facilitate further research and application through their GitHub repository. <div>
arXiv:2512.18583v1 Announce Type: new 
Abstract: Adversarial Imitation Learning (AIL) is a dominant framework in imitation learning that infers rewards from expert demonstrations to guide policy optimization. Although providing more expert demonstrations typically leads to improved performance and greater stability, collecting such demonstrations can be challenging in certain scenarios. Inspired by the success of diffusion models in data generation, we propose SD2AIL, which utilizes synthetic demonstrations via diffusion models. We first employ a diffusion model in the discriminator to generate synthetic demonstrations as pseudo-expert data that augment the expert demonstrations. To selectively replay the most valuable demonstrations from the large pool of (pseudo-) expert demonstrations, we further introduce a prioritized expert demonstration replay strategy (PEDR). The experimental results on simulation tasks demonstrate the effectiveness and robustness of our method. In particular, in the Hopper task, our method achieves an average return of 3441, surpassing the state-of-the-art method by 89. Our code will be available at https://github.com/positron-lpc/SD2AIL.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking neural surrogates on realistic spatiotemporal multiphysics flows</title>
<link>https://arxiv.org/abs/2512.18595</link>
<guid>https://arxiv.org/abs/2512.18595</guid>
<content:encoded><![CDATA[
<div> multiphysics, neural surrogates, benchmarking, reactive flows, model robustness<br /><br />Summary:<br /><br />This paper addresses the computational challenges in predicting multiphysics dynamics, which involve complex couplings of multi-scale and heterogeneous processes. It highlights the current limitations in neural surrogate evaluations that often rely on simplified, low-dimensional proxies, thus creating an "illusion of mastery" by failing to reveal model fragility in realistic settings. To overcome this, the authors introduce REALM (REalistic AI Learning for Multiphysics), a comprehensive benchmarking framework designed specifically for neural surrogates applied to realistic, application-driven reactive flow problems. REALM includes 11 high-fidelity datasets ranging from canonical multiphysics tasks to complex scenarios such as propulsion and fire safety, along with a standardized end-to-end training and evaluation protocol that employs multiphysics-aware preprocessing and a robust rollout strategy. Using REALM, the paper benchmarks over a dozen surrogate model families—including spectral operators, convolutional networks, Transformers, pointwise operators, and graph/mesh networks—and discovers three key trends: (i) a scaling barrier that depends on dimensionality, stiffness, and mesh irregularity, causing rapidly increasing rollout errors; (ii) that model performance is more influenced by architectural inductive biases than sheer parameter count; and (iii) a notable discrepancy between standard accuracy metrics and physically meaningful behavior, where models scoring high on correlation still fail to capture critical transient structures and integral quantities. Overall, REALM reveals the current limits of neural surrogates in realistic multiphysics flows and provides a rigorous platform to guide the development of more physically informed AI architectures. <div>
arXiv:2512.18595v1 Announce Type: new 
Abstract: Predicting multiphysics dynamics is computationally expensive and challenging due to the severe coupling of multi-scale, heterogeneous physical processes. While neural surrogates promise a paradigm shift, the field currently suffers from an "illusion of mastery", as repeatedly emphasized in top-tier commentaries: existing evaluations overly rely on simplified, low-dimensional proxies, which fail to expose the models' inherent fragility in realistic regimes. To bridge this critical gap, we present REALM (REalistic AI Learning for Multiphysics), a rigorous benchmarking framework designed to test neural surrogates on challenging, application-driven reactive flows. REALM features 11 high-fidelity datasets spanning from canonical multiphysics problems to complex propulsion and fire safety scenarios, alongside a standardized end-to-end training and evaluation protocol that incorporates multiphysics-aware preprocessing and a robust rollout strategy. Using this framework, we systematically benchmark over a dozen representative surrogate model families, including spectral operators, convolutional models, Transformers, pointwise operators, and graph/mesh networks, and identify three robust trends: (i) a scaling barrier governed jointly by dimensionality, stiffness, and mesh irregularity, leading to rapidly growing rollout errors; (ii) performance primarily controlled by architectural inductive biases rather than parameter count; and (iii) a persistent gap between nominal accuracy metrics and physically trustworthy behavior, where models with high correlations still miss key transient structures and integral quantities. Taken together, REALM exposes the limits of current neural surrogates on realistic multiphysics flows and offers a rigorous testbed to drive the development of next-generation physics-aware architectures.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EIA-SEC: Improved Actor-Critic Framework for Multi-UAV Collaborative Control in Smart Agriculture</title>
<link>https://arxiv.org/abs/2512.18596</link>
<guid>https://arxiv.org/abs/2512.18596</guid>
<content:encoded><![CDATA[
<div> multi-UAV, trajectory planning, Markov decision process, reinforcement learning, smart agriculture<br /><br />Summary:<br /><br />This paper addresses the challenge of coordinating multiple unmanned aerial vehicles (UAVs) in smart agriculture for tasks such as data collection, image acquisition, and communication. To optimize the UAV trajectory planning, the authors model the problem as a Markov decision process (MDP), enabling a structured decision-making framework under uncertainty. They propose a novel reinforcement learning framework named Elite Imitation Actor-Shared Ensemble Critic (EIA-SEC). This framework introduces two key innovations: first, agents learn adaptively from an elite agent, which helps to reduce the number of costly trial-and-error interactions; second, a shared ensemble critic mechanism works alongside each agent's local critic to obtain unbiased value estimation and avoid overestimation bias common in actor-critic methods. The experimental results demonstrate that EIA-SEC outperforms state-of-the-art baseline algorithms by achieving higher reward performance, better training stability, and faster convergence. Overall, the study contributes an effective and efficient multi-agent reinforcement learning approach that is well-suited for the complex, cooperative UAV operations needed in smart agriculture environments. <div>
arXiv:2512.18596v1 Announce Type: new 
Abstract: The widespread application of wireless communication technology has promoted the development of smart agriculture, where unmanned aerial vehicles (UAVs) play a multifunctional role. We target a multi-UAV smart agriculture system where UAVs cooperatively perform data collection, image acquisition, and communication tasks. In this context, we model a Markov decision process to solve the multi-UAV trajectory planning problem. Moreover, we propose a novel Elite Imitation Actor-Shared Ensemble Critic (EIA-SEC) framework, where agents adaptively learn from the elite agent to reduce trial-and-error costs, and a shared ensemble critic collaborates with each agent's local critic to ensure unbiased objective value estimates and prevent overestimation. Experimental results demonstrate that EIA-SEC outperforms state-of-the-art baselines in terms of reward performance, training stability, and convergence speed.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trajectory Planning for UAV-Based Smart Farming Using Imitation-Based Triple Deep Q-Learning</title>
<link>https://arxiv.org/abs/2512.18604</link>
<guid>https://arxiv.org/abs/2512.18604</guid>
<content:encoded><![CDATA[
<div> Keywords: UAV, smart agriculture, trajectory planning, multi-agent reinforcement learning, imitation-based triple deep Q-network

<br /><br />Summary: Unmanned aerial vehicles (UAVs) have become valuable tools in smart agriculture for tasks such as weed detection, recognition, and data collection from wireless sensors. However, planning optimal UAV trajectories is challenging due to environmental uncertainties, partial information, and limited battery life. To tackle these issues, the authors model the trajectory planning problem using a Markov decision process (MDP) framework. They apply multi-agent reinforcement learning (MARL) techniques to learn effective policies. A novel imitation-based triple deep Q-network (ITDQN) algorithm is introduced, which incorporates an elite imitation mechanism to reduce exploration costs significantly. This approach also uses a mediator Q-network layered over a double deep Q-network (DDQN) to enhance training speed, stability, and overall performance. The method was evaluated in both simulated and real-world conditions, confirming its practical viability. Results demonstrate that ITDQN outperforms the standard DDQN algorithm, achieving a 4.43% improvement in weed recognition rates and a 6.94% increase in data collection efficiency. This work highlights the potential of advanced reinforcement learning methods in addressing complex trajectory planning problems for UAVs in agricultural applications. <div>
arXiv:2512.18604v1 Announce Type: new 
Abstract: Unmanned aerial vehicles (UAVs) have emerged as a promising auxiliary platform for smart agriculture, capable of simultaneously performing weed detection, recognition, and data collection from wireless sensors. However, trajectory planning for UAV-based smart agriculture is challenging due to the high uncertainty of the environment, partial observations, and limited battery capacity of UAVs. To address these issues, we formulate the trajectory planning problem as a Markov decision process (MDP) and leverage multi-agent reinforcement learning (MARL) to solve it. Furthermore, we propose a novel imitation-based triple deep Q-network (ITDQN) algorithm, which employs an elite imitation mechanism to reduce exploration costs and utilizes a mediator Q-network over a double deep Q-network (DDQN) to accelerate and stabilize training and improve performance. Experimental results in both simulated and real-world environments demonstrate the effectiveness of our solution. Moreover, our proposed ITDQN outperforms DDQN by 4.43\% in weed recognition rate and 6.94\% in data collection rate.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Interaction Bottleneck of Deep Neural Networks: Discovery, Proof, and Modulation</title>
<link>https://arxiv.org/abs/2512.18607</link>
<guid>https://arxiv.org/abs/2512.18607</guid>
<content:encoded><![CDATA[
<div> Keywords: deep neural networks, interaction order, representation capacity, gradient variance, generalization  

<br /><br />Summary:  
This work investigates how deep neural networks (DNNs) encode cooperative interactions across varying levels of contextual complexity using a multi-order interaction framework, where each order measures the contextual information required to assess the joint utility of variable pairs. The authors conduct a comprehensive empirical and theoretical study of interaction structures in DNNs. First, they empirically identify a universal interaction bottleneck: DNNs readily learn low- and high-order interactions but systematically under-represent mid-order interactions. Second, they theoretically explain this bottleneck by demonstrating that mid-order interactions possess the highest contextual variability, leading to greater gradient variance and making them inherently harder to learn. Third, the study shows that this bottleneck can be modulated by introducing loss functions that bias the model toward emphasizing interactions of specific orders. Lastly, they link microscopic interaction patterns with macroscopic representational behaviors, finding that models emphasizing low-order interactions tend to generalize better and be more robust, while models focusing on high-order interactions excel at structural modeling and fitting. Collectively, these findings reveal an intrinsic representational bias in modern DNNs and position interaction order as a meaningful framework for interpreting and guiding deep learning representations. <div>
arXiv:2512.18607v1 Announce Type: new 
Abstract: Understanding what kinds of cooperative structures deep neural networks (DNNs) can represent remains a fundamental yet insufficiently understood problem. In this work, we treat interactions as the fundamental units of such structure and investigate a largely unexplored question: how DNNs encode interactions under different levels of contextual complexity, and how these microscopic interaction patterns shape macroscopic representation capacity. To quantify this complexity, we use multi-order interactions [57], where each order reflects the amount of contextual information required to evaluate the joint interaction utility of a variable pair. This formulation enables a stratified analysis of cooperative patterns learned by DNNs. Building on this formulation, we develop a comprehensive study of interaction structure in DNNs. (i) We empirically discover a universal interaction bottleneck: across architectures and tasks, DNNs easily learn low-order and high-order interactions but consistently under-represent mid-order ones. (ii) We theoretically explain this bottleneck by proving that mid-order interactions incur the highest contextual variability, yielding large gradient variance and making them intrinsically difficult to learn. (iii) We further modulate the bottleneck by introducing losses that steer models toward emphasizing interactions of selected orders. Finally, we connect microscopic interaction structures with macroscopic representational behavior: low-order-emphasized models exhibit stronger generalization and robustness, whereas high-order-emphasized models demonstrate greater structural modeling and fitting capability. Together, these results uncover an inherent representational bias in modern DNNs and establish interaction order as a powerful lens for interpreting and guiding deep representations.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Procrustean Bed of Time Series: The Optimization Bias of Point-wise Loss</title>
<link>https://arxiv.org/abs/2512.18610</link>
<guid>https://arxiv.org/abs/2512.18610</guid>
<content:encoded><![CDATA[
<div> Keywords: Expectation of Optimization Bias, time series, point-wise loss, Structural Signal-to-Noise Ratio, debiasing

<br /><br />Summary:  
This paper addresses a fundamental flaw in optimizing time series models using point-wise loss functions like MSE, which assume data points are independent and identically distributed (i.i.d.) and thereby ignore the causal temporal structure inherent in time series. It focuses on the independence assumption issue under covariance stationarity, providing a first-principles, information-theoretic analysis of the Expectation of Optimization Bias (EOB), defining it as the discrepancy between the true joint distribution of the time series and its flawed i.i.d. assumption counterpart. The study uncovers a paradox: the more deterministic and structured a time series is, the greater the bias introduced by point-wise loss functions. The authors derive the first closed-form expressions that quantify the non-deterministic EOB in both linear and nonlinear systems, proving that EOB is an intrinsic property depending only on sequence length and a newly proposed Structural Signal-to-Noise Ratio (SSNR). To counteract this bias, the paper proposes a principled debiasing approach based on reducing sequence length and applying structural orthogonalization using techniques such as the Discrete Fourier Transform (DFT) or Discrete Wavelet Transform (DWT). Additionally, a novel harmonized ℓ_p norm framework is introduced to address gradient issues caused by high-variance series. Extensive experiments validate the universality of EOB theory and demonstrate the superior effectiveness of the proposed debiasing program. <div>
arXiv:2512.18610v1 Announce Type: new 
Abstract: Optimizing time series models via point-wise loss functions (e.g., MSE) relying on a flawed point-wise independent and identically distributed (i.i.d.) assumption that disregards the causal temporal structure, an issue with growing awareness yet lacking formal theoretical grounding. Focusing on the core independence issue under covariance stationarity, this paper aims to provide a first-principles analysis of the Expectation of Optimization Bias (EOB), formalizing it information-theoretically as the discrepancy between the true joint distribution and its flawed i.i.d. counterpart. Our analysis reveals a fundamental paradigm paradox: the more deterministic and structured the time series, the more severe the bias by point-wise loss function. We derive the first closed-form quantification for the non-deterministic EOB across linear and non-linear systems, and prove EOB is an intrinsic data property, governed exclusively by sequence length and our proposed Structural Signal-to-Noise Ratio (SSNR). This theoretical diagnosis motivates our principled debiasing program that eliminates the bias through sequence length reduction and structural orthogonalization. We present a concrete solution that simultaneously achieves both principles via DFT or DWT. Furthermore, a novel harmonized $\ell_p$ norm framework is proposed to rectify gradient pathologies of high-variance series. Extensive experiments validate EOB Theory's generality and the superior performance of debiasing program.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ARC: Leveraging Compositional Representations for Cross-Problem Learning on VRPs</title>
<link>https://arxiv.org/abs/2512.18633</link>
<guid>https://arxiv.org/abs/2512.18633</guid>
<content:encoded><![CDATA[
<div> Keywords: Vehicle Routing Problems, cross-problem learning, attribute representation, analogical consistency, zero-shot generalization<br /><br />Summary:<br /><br />This paper addresses Vehicle Routing Problems (VRPs) characterized by diverse real-world attributes, which necessitate learning methods that generalize effectively across multiple problem variants. The authors introduce ARC (Attribute Representation via Compositional Learning), a novel framework designed to learn disentangled attribute representations. ARC decomposes attribute representations into two complementary parts: Intrinsic Attribute Embeddings (IAE), capturing invariant semantic information of each attribute, and Contextual Interaction Embeddings (CIE), which model the interaction effects arising from combinations of attributes. A key innovation is the enforcement of analogical consistency within the embedding space, ensuring that the semantic transformation induced by adding a particular attribute remains consistent across different contexts. This property allows ARC to transfer learned invariant semantics across trained variants and to construct reliable representations for previously unseen attribute combinations. The framework is evaluated extensively and demonstrates state-of-the-art performance on various settings including in-distribution tests, zero-shot generalization to unseen variants, few-shot adaptation to new problems, and real-world benchmarks. These results highlight the efficacy of ARC in enabling scalable and flexible solutions for complex, attribute-rich Vehicle Routing Problems. <div>
arXiv:2512.18633v1 Announce Type: new 
Abstract: Vehicle Routing Problems (VRPs) with diverse real-world attributes have driven recent interest in cross-problem learning approaches that efficiently generalize across problem variants. We propose ARC (Attribute Representation via Compositional Learning), a cross-problem learning framework that learns disentangled attribute representations by decomposing them into two complementary components: an Intrinsic Attribute Embedding (IAE) for invariant attribute semantics and a Contextual Interaction Embedding (CIE) for attribute-combination effects. This disentanglement is achieved by enforcing analogical consistency in the embedding space to ensure the semantic transformation of adding an attribute (e.g., a length constraint) remains invariant across different problem contexts. This enables our model to reuse invariant semantics across trained variants and construct representations for unseen combinations. ARC achieves state-of-the-art performance across in-distribution, zero-shot generalization, few-shot adaptation, and real-world benchmarks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Shortcut to Induction Head: How Data Diversity Shapes Algorithm Selection in Transformers</title>
<link>https://arxiv.org/abs/2512.18634</link>
<guid>https://arxiv.org/abs/2512.18634</guid>
<content:encoded><![CDATA[
<div> Keywords: transformers, pretraining data distribution, induction heads, positional shortcuts, out-of-distribution generalization<br /><br />Summary:<br /><br />This paper investigates how the distribution of pretraining data influences whether a shallow transformer develops a generalizable algorithmic behavior or relies on simple positional shortcuts. The authors focus on a minimal trigger-output prediction task where the model must copy the token immediately following a special trigger upon its second occurrence. They conduct a rigorous analysis of gradient-based training on a single-layer transformer, providing proofs in both infinite and finite sample regimes. A key finding is a transition in the learned mechanism determined by the diversity of input sequences, measured by a "max-sum" ratio of trigger-to-trigger distances: low ratios encourage the formation of induction heads that generalize well to unseen contexts, while high ratios cause the model to memorize fixed output positions and fail to generalize out-of-distribution (OOD). The study also uncovers a trade-off between the pretraining context length and OOD generalization and derives an optimal pretraining distribution minimizing computational costs per sample. Finally, synthetic experiments validate these theoretical insights, showing that broadening the distribution of context lengths robustly induces induction heads and improves generalization. The work offers valuable conceptual guidelines for steering pretrained transformers’ behaviors through data-driven approaches. <div>
arXiv:2512.18634v1 Announce Type: new 
Abstract: Transformers can implement both generalizable algorithms (e.g., induction heads) and simple positional shortcuts (e.g., memorizing fixed output positions). In this work, we study how the choice of pretraining data distribution steers a shallow transformer toward one behavior or the other. Focusing on a minimal trigger-output prediction task -- copying the token immediately following a special trigger upon its second occurrence -- we present a rigorous analysis of gradient-based training of a single-layer transformer. In both the infinite and finite sample regimes, we prove a transition in the learned mechanism: if input sequences exhibit sufficient diversity, measured by a low ``max-sum'' ratio of trigger-to-trigger distances, the trained model implements an induction head and generalizes to unseen contexts; by contrast, when this ratio is large, the model resorts to a positional shortcut and fails to generalize out-of-distribution (OOD). We also reveal a trade-off between the pretraining context length and OOD generalization, and derive the optimal pretraining distribution that minimizes computational cost per sample. Finally, we validate our theoretical predictions with controlled synthetic experiments, demonstrating that broadening context distributions robustly induces induction heads and enables OOD generalization. Our results shed light on the algorithmic biases of pretrained transformers and offer conceptual guidelines for data-driven control of their learned behaviors.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Demonstration-Guided Continual Reinforcement Learning in Dynamic Environments</title>
<link>https://arxiv.org/abs/2512.18670</link>
<guid>https://arxiv.org/abs/2512.18670</guid>
<content:encoded><![CDATA[
<div> Keywords: continual reinforcement learning, demonstration-guided exploration, stability-plasticity dilemma, knowledge transfer, curriculum learning<br /><br />Summary:<br /><br />Reinforcement learning (RL) faces significant challenges in dynamic environments where the task distribution changes over time. To address this, continual reinforcement learning (CRL) aims to enable agents to continuously learn and adapt across multiple tasks while balancing stability (retaining old knowledge) and plasticity (acquiring new knowledge). However, existing CRL approaches generally influence learning optimization using past knowledge but do not directly integrate this knowledge into the agent's behavior, limiting knowledge reuse. This paper introduces demonstration-guided continual reinforcement learning (DGCRL), a novel framework that stores previous knowledge in an external, self-evolving demonstration repository. DGCRL assists RL agents by dynamically selecting relevant demonstrations for each new task, effectively guiding exploration and accelerating learning. It employs a curriculum-based strategy that initially emphasizes demonstration-guided exploration and gradually transitions to independent exploration, enhancing adaptation efficiency. Extensive experiments are conducted on 2D navigation and MuJoCo locomotion benchmarks, showing that DGCRL achieves better average performance, improved knowledge transfer between tasks, reduced forgetting of prior skills, and greater training efficiency compared to baselines. Additional sensitivity analyses and ablation studies confirm the robustness and effectiveness of the proposed method in handling the stability-plasticity trade-off in continual learning scenarios. <div>
arXiv:2512.18670v1 Announce Type: new 
Abstract: Reinforcement learning (RL) excels in various applications but struggles in dynamic environments where the underlying Markov decision process evolves. Continual reinforcement learning (CRL) enables RL agents to continually learn and adapt to new tasks, but balancing stability (preserving prior knowledge) and plasticity (acquiring new knowledge) remains challenging. Existing methods primarily address the stability-plasticity dilemma through mechanisms where past knowledge influences optimization but rarely affects the agent's behavior directly, which may hinder effective knowledge reuse and efficient learning. In contrast, we propose demonstration-guided continual reinforcement learning (DGCRL), which stores prior knowledge in an external, self-evolving demonstration repository that directly guides RL exploration and adaptation. For each task, the agent dynamically selects the most relevant demonstration and follows a curriculum-based strategy to accelerate learning, gradually shifting from demonstration-guided exploration to fully self-exploration. Extensive experiments on 2D navigation and MuJoCo locomotion tasks demonstrate its superior average performance, enhanced knowledge transfer, mitigation of forgetting, and training efficiency. The additional sensitivity analysis and ablation study further validate its effectiveness.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Pattern Recognition of Scheduling Anomalies through Structure-Aware and Semantically-Enhanced Graphs</title>
<link>https://arxiv.org/abs/2512.18673</link>
<guid>https://arxiv.org/abs/2512.18673</guid>
<content:encoded><![CDATA[
arXiv:2512.18673v1 Announce Type: new 
Abstract: This paper proposes a structure-aware driven scheduling graph modeling method to improve the accuracy and representation capability of anomaly identification in scheduling behaviors of complex systems. The method first designs a structure-guided scheduling graph construction mechanism that integrates task execution stages, resource node states, and scheduling path information to build dynamically evolving scheduling behavior graphs, enhancing the model's ability to capture global scheduling relationships. On this basis, a multi-scale graph semantic aggregation module is introduced to achieve semantic consistency modeling of scheduling features through local adjacency semantic integration and global topology alignment, thereby strengthening the model's capability to capture abnormal features in complex scenarios such as multi-task concurrency, resource competition, and stage transitions. Experiments are conducted on a real scheduling dataset with multiple scheduling disturbance paths set to simulate different types of anomalies, including structural shifts, resource changes, and task delays. The proposed model demonstrates significant performance advantages across multiple metrics, showing a sensitive response to structural disturbances and semantic shifts. Further visualization analysis reveals that, under the combined effect of structure guidance and semantic aggregation, the scheduling behavior graph exhibits stronger anomaly separability and pattern representation, validating the effectiveness and adaptability of the method in scheduling anomaly detection tasks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fusion of Multiscale Features Via Centralized Sparse-attention Network for EEG Decoding</title>
<link>https://arxiv.org/abs/2512.18689</link>
<guid>https://arxiv.org/abs/2512.18689</guid>
<content:encoded><![CDATA[
arXiv:2512.18689v1 Announce Type: new 
Abstract: Electroencephalography (EEG) signal decoding is a key technology that translates brain activity into executable commands, laying the foundation for direct brain-machine interfacing and intelligent interaction. To address the inherent spatiotemporal heterogeneity of EEG signals, this paper proposes a multi-branch parallel architecture, where each temporal scale is equipped with an independent spatial feature extraction module. To further enhance multi-branch feature fusion, we propose a Fusion of Multiscale Features via Centralized Sparse-attention Network (EEG-CSANet), a centralized sparse-attention network. It employs a main-auxiliary branch architecture, where the main branch models core spatiotemporal patterns via multiscale self-attention, and the auxiliary branch facilitates efficient local interactions through sparse cross-attention. Experimental results show that EEG-CSANet achieves state-of-the-art (SOTA) performance across five public datasets (BCIC-IV-2A, BCIC-IV-2B, HGD, SEED, and SEED-VIG), with accuracies of 88.54%, 91.09%, 99.43%, 96.03%, and 90.56%, respectively. Such performance demonstrates its strong adaptability and robustness across various EEG decoding tasks. Moreover, extensive ablation studies are conducted to enhance the interpretability of EEG-CSANet. In the future, we hope that EEG-CSANet could serve as a promising baseline model in the field of EEG signal decoding. The source code is publicly available at: https://github.com/Xiangrui-Cai/EEG-CSANet
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generating Risky Samples with Conformity Constraints via Diffusion Models</title>
<link>https://arxiv.org/abs/2512.18722</link>
<guid>https://arxiv.org/abs/2512.18722</guid>
<content:encoded><![CDATA[
arXiv:2512.18722v1 Announce Type: new 
Abstract: Although neural networks achieve promising performance in many tasks, they may still fail when encountering some examples and bring about risks to applications. To discover risky samples, previous literature attempts to search for patterns of risky samples within existing datasets or inject perturbation into them. Yet in this way the diversity of risky samples is limited by the coverage of existing datasets. To overcome this limitation, recent works adopt diffusion models to produce new risky samples beyond the coverage of existing datasets. However, these methods struggle in the conformity between generated samples and expected categories, which could introduce label noise and severely limit their effectiveness in applications. To address this issue, we propose RiskyDiff that incorporates the embeddings of both texts and images as implicit constraints of category conformity. We also design a conformity score to further explicitly strengthen the category conformity, as well as introduce the mechanisms of embedding screening and risky gradient guidance to boost the risk of generated samples. Extensive experiments reveal that RiskyDiff greatly outperforms existing methods in terms of the degree of risk, generation quality, and conformity with conditioned categories. We also empirically show the generalization ability of the models can be enhanced by augmenting training data with generated samples of high conformity.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ML Inference Scheduling with Predictable Latency</title>
<link>https://arxiv.org/abs/2512.18725</link>
<guid>https://arxiv.org/abs/2512.18725</guid>
<content:encoded><![CDATA[
arXiv:2512.18725v1 Announce Type: new 
Abstract: Machine learning (ML) inference serving systems can schedule requests to improve GPU utilization and to meet service level objectives (SLOs) or deadlines. However, improving GPU utilization may compromise latency-sensitive scheduling, as concurrent tasks contend for GPU resources and thereby introduce interference. Given that interference effects introduce unpredictability in scheduling, neglecting them may compromise SLO or deadline satisfaction. Nevertheless, existing interference prediction approaches remain limited in several respects, which may restrict their usefulness for scheduling. First, they are often coarse-grained, which ignores runtime co-location dynamics and thus restricts their accuracy in interference prediction. Second, they tend to use a static prediction model, which may not effectively cope with different workload characteristics. To this end, we evaluate the potential limitations of existing interference prediction approaches and outline our ongoing work toward achieving efficient ML inference scheduling.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Theoretical Lens for RL-Tuned Language Models via Energy-Based Models</title>
<link>https://arxiv.org/abs/2512.18730</link>
<guid>https://arxiv.org/abs/2512.18730</guid>
<content:encoded><![CDATA[
arXiv:2512.18730v1 Announce Type: new 
Abstract: Large language models (LLMs) trained via KL-regularized reinforcement learning demonstrate strong instruction following, self-correction, and reasoning abilities. Yet their theoretical underpinnings remain limited. We exploit the closed-form energy-based model (EBM) structure of the optimal KL-regularized policy to provide a unified variational analysis of LLMs.
  For instruction-tuned models, under natural assumptions on reward potentials and pretraining symmetry, we prove that the transition kernel satisfies detailed balance with respect to a scalar potential encoding response quality. This yields monotonic KL convergence to a high-quality stationary distribution, bounded hitting times to superior states, and exponential mixing governed by the spectral gap.
  For reasoning models trained with verifiable rewards (RLVR), we show the objective is equivalent to expected KL minimization toward an optimal reasoning distribution, with the suboptimality gap reducing to the Bernoulli KL between target and current accuracies along the natural gradient flow. This helps explain empirical entropy-accuracy trade-offs.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Is Your Conditional Diffusion Model Actually Denoising?</title>
<link>https://arxiv.org/abs/2512.18736</link>
<guid>https://arxiv.org/abs/2512.18736</guid>
<content:encoded><![CDATA[
arXiv:2512.18736v1 Announce Type: new 
Abstract: We study the inductive biases of diffusion models with a conditioning-variable, which have seen widespread application as both text-conditioned generative image models and observation-conditioned continuous control policies. We observe that when these models are queried conditionally, their generations consistently deviate from the idealized "denoising" process upon which diffusion models are formulated, inducing disagreement between popular sampling algorithms (e.g. DDPM, DDIM). We introduce Schedule Deviation, a rigorous measure which captures the rate of deviation from a standard denoising process, and provide a methodology to compute it. Crucially, we demonstrate that the deviation from an idealized denoising process occurs irrespective of the model capacity or amount of training data. We posit that this phenomenon occurs due to the difficulty of bridging distinct denoising flows across different parts of the conditioning space and show theoretically how such a phenomenon can arise through an inductive bias towards smoothness.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PIPCFR: Pseudo-outcome Imputation with Post-treatment Variables for Individual Treatment Effect Estimation</title>
<link>https://arxiv.org/abs/2512.18737</link>
<guid>https://arxiv.org/abs/2512.18737</guid>
<content:encoded><![CDATA[
arXiv:2512.18737v1 Announce Type: new 
Abstract: The estimation of individual treatment effects (ITE) focuses on predicting the outcome changes that result from a change in treatment. A fundamental challenge in observational data is that while we need to infer outcome differences under alternative treatments, we can only observe each individual's outcome under a single treatment. Existing approaches address this limitation either by training with inferred pseudo-outcomes or by creating matched instance pairs. However, recent work has largely overlooked the potential impact of post-treatment variables on the outcome. This oversight prevents existing methods from fully capturing outcome variability, resulting in increased variance in counterfactual predictions. This paper introduces Pseudo-outcome Imputation with Post-treatment Variables for Counterfactual Regression (PIPCFR), a novel approach that incorporates post-treatment variables to improve pseudo-outcome imputation. We analyze the challenges inherent in utilizing post-treatment variables and establish a novel theoretical bound for ITE risk that explicitly connects post-treatment variables to ITE estimation accuracy. Unlike existing methods that ignore these variables or impose restrictive assumptions, PIPCFR learns effective representations that preserve informative components while mitigating bias. Empirical evaluations on both real-world and simulated datasets demonstrate that PIPCFR achieves significantly lower ITE errors compared to existing methods.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gaussian-Mixture-Model Q-Functions for Policy Iteration in Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.18763</link>
<guid>https://arxiv.org/abs/2512.18763</guid>
<content:encoded><![CDATA[
arXiv:2512.18763v1 Announce Type: new 
Abstract: Unlike their conventional use as estimators of probability density functions in reinforcement learning (RL), this paper introduces a novel function-approximation role for Gaussian mixture models (GMMs) as direct surrogates for Q-function losses. These parametric models, termed GMM-QFs, possess substantial representational capacity, as they are shown to be universal approximators over a broad class of functions. They are further embedded within Bellman residuals, where their learnable parameters -- a fixed number of mixing weights, together with Gaussian mean vectors and covariance matrices -- are inferred from data via optimization on a Riemannian manifold. This geometric perspective on the parameter space naturally incorporates Riemannian optimization into the policy-evaluation step of standard policy-iteration frameworks. Rigorous theoretical results are established, and supporting numerical tests show that, even without access to experience data, GMM-QFs deliver competitive performance and, in some cases, outperform state-of-the-art approaches across a range of benchmark RL tasks, all while maintaining a significantly smaller computational footprint than deep-learning methods that rely on experience data.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Label-Informed Outlier Detection Based on Granule Density</title>
<link>https://arxiv.org/abs/2512.18774</link>
<guid>https://arxiv.org/abs/2512.18774</guid>
<content:encoded><![CDATA[
arXiv:2512.18774v1 Announce Type: new 
Abstract: Outlier detection, crucial for identifying unusual patterns with significant implications across numerous applications, has drawn considerable research interest. Existing semi-supervised methods typically treat data as purely numerical and} in a deterministic manner, thereby neglecting the heterogeneity and uncertainty inherent in complex, real-world datasets. This paper introduces a label-informed outlier detection method for heterogeneous data based on Granular Computing and Fuzzy Sets, namely Granule Density-based Outlier Factor (GDOF). Specifically, GDOF first employs label-informed fuzzy granulation to effectively represent various data types and develops granule density for precise density estimation. Subsequently, granule densities from individual attributes are integrated for outlier scoring by assessing attribute relevance with a limited number of labeled outliers. Experimental results on various real-world datasets show that GDOF stands out in detecting outliers in heterogeneous data with a minimal number of labeled outliers. The integration of Fuzzy Sets and Granular Computing in GDOF offers a practical framework for outlier detection in complex and diverse data types. All relevant datasets and source codes are publicly available for further research. This is the author's accepted manuscript of a paper published in IEEE Transactions on Fuzzy Systems. The final version is available at https://doi.org/10.1109/TFUZZ.2024.3514853
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Controllable Probabilistic Forecasting with Stochastic Decomposition Layers</title>
<link>https://arxiv.org/abs/2512.18815</link>
<guid>https://arxiv.org/abs/2512.18815</guid>
<content:encoded><![CDATA[
arXiv:2512.18815v1 Announce Type: new 
Abstract: AI weather prediction ensembles with latent noise injection and optimized with the continuous ranked probability score (CRPS) have produced both accurate and well-calibrated predictions with far less computational cost compared with diffusion-based methods. However, current CRPS ensemble approaches vary in their training strategies and noise injection mechanisms, with most injecting noise globally throughout the network via conditional normalization. This structure increases training expense and limits the physical interpretability of the stochastic perturbations. We introduce Stochastic Decomposition Layers (SDL) for converting deterministic machine learning weather models into probabilistic ensemble systems. Adapted from StyleGAN's hierarchical noise injection, SDL applies learned perturbations at three decoder scales through latent-driven modulation, per-pixel noise, and channel scaling. When applied to WXFormer via transfer learning, SDL requires less than 2\% of the computational cost needed to train the baseline model. Each ensemble member is generated from a compact latent tensor (5 MB), enabling perfect reproducibility and post-inference spread adjustment through latent rescaling. Evaluation on 2022 ERA5 reanalysis shows ensembles with spread-skill ratios approaching unity and rank histograms that progressively flatten toward uniformity through medium-range forecasts, achieving calibration competitive with operational IFS-ENS. Multi-scale experiments reveal hierarchical uncertainty: coarse layers modulate synoptic patterns while fine layers control mesoscale variability. The explicit latent parameterization provides interpretable uncertainty quantification for operational forecasting and climate applications.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hyperbolic Graph Embeddings: a Survey and an Evaluation on Anomaly Detection</title>
<link>https://arxiv.org/abs/2512.18826</link>
<guid>https://arxiv.org/abs/2512.18826</guid>
<content:encoded><![CDATA[
arXiv:2512.18826v1 Announce Type: new 
Abstract: This survey reviews hyperbolic graph embedding models, and evaluate them on anomaly detection, highlighting their advantages over Euclidean methods in capturing complex structures. Evaluating models like \textit{HGCAE}, \textit{\(\mathcal{P}\)-VAE}, and \textit{HGCN} demonstrates high performance, with \textit{\(\mathcal{P}\)-VAE} achieving an F1-score of 94\% on the \textit{Elliptic} dataset and \textit{HGCAE} scoring 80\% on \textit{Cora}. In contrast, Euclidean methods like \textit{DOMINANT} and \textit{GraphSage} struggle with complex data. The study emphasizes the potential of hyperbolic spaces for improving anomaly detection, and provides an open-source library to foster further research in this field.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Modeling through Spectral Analysis of Koopman Operator</title>
<link>https://arxiv.org/abs/2512.18837</link>
<guid>https://arxiv.org/abs/2512.18837</guid>
<content:encoded><![CDATA[
arXiv:2512.18837v1 Announce Type: new 
Abstract: We propose Koopman Spectral Wasserstein Gradient Descent (KSWGD), a generative modeling framework that combines operator-theoretic spectral analysis with optimal transport. The novel insight is that the spectral structure required for accelerated Wasserstein gradient descent can be directly estimated from trajectory data via Koopman operator approximation which can eliminate the need for explicit knowledge of the target potential or neural network training. We provide rigorous convergence analysis and establish connection to Feynman-Kac theory that clarifies the method's probabilistic foundation. Experiments across diverse settings, including compact manifold sampling, metastable multi-well systems, image generation, and high dimensional stochastic partial differential equation, demonstrate that KSWGD consistently achieves faster convergence than other existing methods while maintaining high sample quality.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Merging of Kolmogorov-Arnold networks trained on disjoint datasets</title>
<link>https://arxiv.org/abs/2512.18921</link>
<guid>https://arxiv.org/abs/2512.18921</guid>
<content:encoded><![CDATA[
arXiv:2512.18921v1 Announce Type: new 
Abstract: Training on disjoint datasets can serve two primary goals: accelerating data processing and enabling federated learning. It has already been established that Kolmogorov-Arnold networks (KANs) are particularly well suited for federated learning and can be merged through simple parameter averaging. While the federated learning literature has mostly focused on achieving training convergence across distributed nodes, the present paper specifically targets acceleration of the training, which depends critically on the choice of an optimisation method and the type of the basis functions. To the best knowledge of the authors, the fastest currently-available combination is the Newton-Kaczmarz method and the piecewise-linear basis functions. Here, it is shown that training on disjoint datasets (or disjoint subsets of the training dataset) can further improve the performance. Experimental comparisons are provided, and all corresponding codes are publicly available.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Ensemble Schr{\"o}dinger Bridge filter for Nonlinear Data Assimilation</title>
<link>https://arxiv.org/abs/2512.18928</link>
<guid>https://arxiv.org/abs/2512.18928</guid>
<content:encoded><![CDATA[
arXiv:2512.18928v1 Announce Type: new 
Abstract: This work puts forward a novel nonlinear optimal filter namely the Ensemble Schr{\"o}dinger Bridge nonlinear filter. The proposed filter finds marriage of the standard prediction procedure and the diffusion generative modeling for the analysis procedure to realize one filtering step. The designed approach finds no structural model error, and it is derivative free, training free and highly parallizable. Experimental results show that the designed algorithm performs well given highly nonlinear dynamics in (mildly) high dimension up to 40 or above under a chaotic environment. It also shows better performance than classical methods such as the ensemble Kalman filter and the Particle filter in numerous tests given different level of nonlinearity. Future work will focus on extending the proposed approach to practical meteorological applications and establishing a rigorous convergence analysis.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DPSR: Differentially Private Sparse Reconstruction via Multi-Stage Denoising for Recommender Systems</title>
<link>https://arxiv.org/abs/2512.18932</link>
<guid>https://arxiv.org/abs/2512.18932</guid>
<content:encoded><![CDATA[
arXiv:2512.18932v1 Announce Type: new 
Abstract: Differential privacy (DP) has emerged as the gold standard for protecting user data in recommender systems, but existing privacy-preserving mechanisms face a fundamental challenge: the privacy-utility tradeoff inevitably degrades recommendation quality as privacy budgets tighten. We introduce DPSR (Differentially Private Sparse Reconstruction), a novel three-stage denoising framework that fundamentally addresses this limitation by exploiting the inherent structure of rating matrices -- sparsity, low-rank properties, and collaborative patterns.
  DPSR consists of three synergistic stages: (1) \textit{information-theoretic noise calibration} that adaptively reduces noise for high-information ratings, (2) \textit{collaborative filtering-based denoising} that leverages item-item similarities to remove privacy noise, and (3) \textit{low-rank matrix completion} that exploits latent structure for signal recovery. Critically, all denoising operations occur \textit{after} noise injection, preserving differential privacy through the post-processing immunity theorem while removing both privacy-induced and inherent data noise.
  Through extensive experiments on synthetic datasets with controlled ground truth, we demonstrate that DPSR achieves 5.57\% to 9.23\% RMSE improvement over state-of-the-art Laplace and Gaussian mechanisms across privacy budgets ranging from $\varepsilon=0.1$ to $\varepsilon=10.0$ (all improvements statistically significant with $p < 0.05$, most $p < 0.001$). Remarkably, at $\varepsilon=1.0$, DPSR achieves RMSE of 0.9823, \textit{outperforming even the non-private baseline} (1.0983), demonstrating that our denoising pipeline acts as an effective regularizer that removes data noise in addition to privacy noise.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Less is More: 8-bit Quantization Improves Continual Learning in Large Language Models</title>
<link>https://arxiv.org/abs/2512.18934</link>
<guid>https://arxiv.org/abs/2512.18934</guid>
<content:encoded><![CDATA[
arXiv:2512.18934v1 Announce Type: new 
Abstract: Catastrophic forgetting poses a fundamental challenge in continual learning, particularly when models are quantized for deployment efficiency. We systematically investigate the interplay between quantization precision (FP16, INT8, INT4) and replay buffer strategies in large language models, revealing unexpected dynamics. While FP16 achieves superior initial task performance (74.44% on NLU), we observe a striking inversion on subsequent tasks: quantized models outperform FP16 by 8-15% on final task forward accuracy, with INT4 achieving nearly double FP16's performance on Code generation (40% vs 20%). Critically, even minimal replay buffers (0.1%) dramatically improve retention - increasing NLU retention after Math training from 45% to 65% across all precision levels - with INT8 consistently achieving the optimal balance between learning plasticity and knowledge retention. We hypothesize that quantization-induced noise acts as implicit regularization, preventing the overfitting to new task gradients that plagues high-precision models. These findings challenge the conventional wisdom that higher precision is always preferable, suggesting instead that INT8 quantization offers both computational efficiency and superior continual learning dynamics. Our results provide practical guidelines for deploying compressed models in continual learning scenarios: small replay buffers (1-2%) suffice for NLU tasks, while Math and Code benefit from moderate buffers (5-10%), with quantized models requiring less replay than FP16 to achieve comparable retention. Code is available at https://github.com/Festyve/LessIsMore.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Hierarchical Procedural Memory for LLM Agents through Bayesian Selection and Contrastive Refinement</title>
<link>https://arxiv.org/abs/2512.18950</link>
<guid>https://arxiv.org/abs/2512.18950</guid>
<content:encoded><![CDATA[
arXiv:2512.18950v1 Announce Type: new 
Abstract: We present MACLA, a framework that decouples reasoning from learning by maintaining a frozen large language model while performing all adaptation in an external hierarchical procedural memory. MACLA extracts reusable procedures from trajectories, tracks reliability via Bayesian posteriors, selects actions through expected-utility scoring, and refines procedures by contrasting successes and failures. Across four benchmarks (ALFWorld, WebShop, TravelPlanner, InterCodeSQL), MACLA achieves 78.1 percent average performance, outperforming all baselines. On ALFWorld unseen tasks, MACLA reaches 90.3 percent with 3.1 percent positive generalization. The system constructs memory in 56 seconds, 2800 times faster than the state-of-the-art LLM parameter-training baseline, compressing 2851 trajectories into 187 procedures. Experimental results demonstrate that structured external memory with Bayesian selection and contrastive refinement enables sample-efficient, interpretable, and continually improving agents without LLM parameter updates.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Through Little Eyes: Attribute Discrimination Beyond Objects</title>
<link>https://arxiv.org/abs/2512.18951</link>
<guid>https://arxiv.org/abs/2512.18951</guid>
<content:encoded><![CDATA[
arXiv:2512.18951v1 Announce Type: new 
Abstract: Infants learn to recognize not only object categories but also fine grained attributes such as color, size, and texture within their first two years of life. Prior work explores Childs View for Contrastive Learning (CVCL), a CLIP style model trained on infant egocentric video as a computational model of early infant learning, but it focuses only on class level recognition. This leaves it unclear whether infant scale learning also supports attribute discrimination. To address this, we introduce a benchmark that systematically varies color, size, and texture, allowing controlled tests of within class attribute recognition. Comparing CVCL with CLIP shows clear differences. CVCL is better at size discrimination, while CLIP achieves higher accuracy on color discrimination. Both models represent texture in image embeddings but fail to ground texture linguistically, suggesting a gap between visual and language spaces.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Online Distributionally Robust Reinforcement Learning: Sample-Efficient Guarantees with General Function Approximation</title>
<link>https://arxiv.org/abs/2512.18957</link>
<guid>https://arxiv.org/abs/2512.18957</guid>
<content:encoded><![CDATA[
arXiv:2512.18957v1 Announce Type: new 
Abstract: The deployment of reinforcement learning (RL) agents in real-world applications is often hindered by performance degradation caused by mismatches between training and deployment environments. Distributionally robust RL (DR-RL) addresses this issue by optimizing worst-case performance over an uncertainty set of transition dynamics. However, existing work typically relies on substantial prior knowledge-such as access to a generative model or a large offline dataset-and largely focuses on tabular methods that do not scale to complex domains. We overcome these limitations by proposing an online DR-RL algorithm with general function approximation that learns an optimal robust policy purely through interaction with the environment, without requiring prior models or offline data, enabling deployment in high-dimensional tasks. We further provide a theoretical analysis establishing a near-optimal sublinear regret bound under a total variation uncertainty set, demonstrating the sample efficiency and effectiveness of our method.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lag Operator SSMs: A Geometric Framework for Structured State Space Modeling</title>
<link>https://arxiv.org/abs/2512.18965</link>
<guid>https://arxiv.org/abs/2512.18965</guid>
<content:encoded><![CDATA[
arXiv:2512.18965v1 Announce Type: new 
Abstract: Structured State Space Models (SSMs), which are at the heart of the recently popular Mamba architecture, are powerful tools for sequence modeling. However, their theoretical foundation relies on a complex, multi-stage process of continuous-time modeling and subsequent discretization, which can obscure intuition. We introduce a direct, first-principles framework for constructing discrete-time SSMs that is both flexible and modular. Our approach is based on a novel lag operator, which geometrically derives the discrete-time recurrence by measuring how the system's basis functions "slide" and change from one timestep to the next. The resulting state matrices are computed via a single inner product involving this operator, offering a modular design space for creating novel SSMs by flexibly combining different basis functions and time-warping schemes. To validate our approach, we demonstrate that a specific instance exactly recovers the recurrence of the influential HiPPO model. Numerical simulations confirm our derivation, providing new theoretical tools for designing flexible and robust sequence models.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Consistency-guided semi-supervised outlier detection in heterogeneous data using fuzzy rough sets</title>
<link>https://arxiv.org/abs/2512.18977</link>
<guid>https://arxiv.org/abs/2512.18977</guid>
<content:encoded><![CDATA[
arXiv:2512.18977v1 Announce Type: new 
Abstract: Outlier detection aims to find samples that behave differently from the majority of the data. Semi-supervised detection methods can utilize the supervision of partial labels, thus reducing false positive rates. However, most of the current semi-supervised methods focus on numerical data and neglect the heterogeneity of data information. In this paper, we propose a consistency-guided outlier detection algorithm (COD) for heterogeneous data with the fuzzy rough set theory in a semi-supervised manner. First, a few labeled outliers are leveraged to construct label-informed fuzzy similarity relations. Next, the consistency of the fuzzy decision system is introduced to evaluate attributes' contributions to knowledge classification. Subsequently, we define the outlier factor based on the fuzzy similarity class and predict outliers by integrating the classification consistency and the outlier factor. The proposed algorithm is extensively evaluated on 15 freshly proposed datasets. Experimental results demonstrate that COD is better than or comparable with the leading outlier detectors. This manuscript is the accepted author version of a paper published by Elsevier. The final published version is available at https://doi.org/10.1016/j.asoc.2024.112070
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Outlier detection in mixed-attribute data: a semi-supervised approach with fuzzy approximations and relative entropy</title>
<link>https://arxiv.org/abs/2512.18978</link>
<guid>https://arxiv.org/abs/2512.18978</guid>
<content:encoded><![CDATA[
arXiv:2512.18978v1 Announce Type: new 
Abstract: Outlier detection is a critical task in data mining, aimed at identifying objects that significantly deviate from the norm. Semi-supervised methods improve detection performance by leveraging partially labeled data but typically overlook the uncertainty and heterogeneity of real-world mixed-attribute data. This paper introduces a semi-supervised outlier detection method, namely fuzzy rough sets-based outlier detection (FROD), to effectively handle these challenges. Specifically, we first utilize a small subset of labeled data to construct fuzzy decision systems, through which we introduce the attribute classification accuracy based on fuzzy approximations to evaluate the contribution of attribute sets in outlier detection. Unlabeled data is then used to compute fuzzy relative entropy, which provides a characterization of outliers from the perspective of uncertainty. Finally, we develop the detection algorithm by combining attribute classification accuracy with fuzzy relative entropy. Experimental results on 16 public datasets show that FROD is comparable with or better than leading detection algorithms. All datasets and source codes are accessible at https://github.com/ChenBaiyang/FROD. This manuscript is the accepted author version of a paper published by Elsevier. The final published version is available at https://doi.org/10.1016/j.ijar.2025.109373
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OPBO: Order-Preserving Bayesian Optimization</title>
<link>https://arxiv.org/abs/2512.18980</link>
<guid>https://arxiv.org/abs/2512.18980</guid>
<content:encoded><![CDATA[
arXiv:2512.18980v1 Announce Type: new 
Abstract: Bayesian optimization is an effective method for solving expensive black-box optimization problems. Most existing methods use Gaussian processes (GP) as the surrogate model for approximating the black-box objective function, it is well-known that it can fail in high-dimensional space (e.g., dimension over 500). We argue that the reliance of GP on precise numerical fitting is fundamentally ill-suited in high-dimensional space, where it leads to prohibitive computational complexity. In order to address this, we propose a simple order-preserving Bayesian optimization (OPBO) method, where the surrogate model preserves the order, instead of the value, of the black-box objective function. Then we can use a simple but effective OP neural network (NN) to replace GP as the surrogate model. Moreover, instead of searching for the best solution from the acquisition model, we select good-enough solutions in the ordinal set to reduce computational cost. The experimental results show that for high-dimensional (over 500) black-box optimization problems, the proposed OPBO significantly outperforms traditional BO methods based on regression NN and GP. The source code is available at https://github.com/pengwei222/OPBO.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>R-GenIMA: Integrating Neuroimaging and Genetics with Interpretable Multimodal AI for Alzheimer's Disease Progression</title>
<link>https://arxiv.org/abs/2512.18986</link>
<guid>https://arxiv.org/abs/2512.18986</guid>
<content:encoded><![CDATA[
arXiv:2512.18986v1 Announce Type: new 
Abstract: Early detection of Alzheimer's disease (AD) requires models capable of integrating macro-scale neuroanatomical alterations with micro-scale genetic susceptibility, yet existing multimodal approaches struggle to align these heterogeneous signals. We introduce R-GenIMA, an interpretable multimodal large language model that couples a novel ROI-wise vision transformer with genetic prompting to jointly model structural MRI and single nucleotide polymorphisms (SNPs) variations. By representing each anatomically parcellated brain region as a visual token and encoding SNP profiles as structured text, the framework enables cross-modal attention that links regional atrophy patterns to underlying genetic factors. Applied to the ADNI cohort, R-GenIMA achieves state-of-the-art performance in four-way classification across normal cognition (NC), subjective memory concerns (SMC), mild cognitive impairment (MCI), and AD. Beyond predictive accuracy, the model yields biologically meaningful explanations by identifying stage-specific brain regions and gene signatures, as well as coherent ROI-Gene association patterns across the disease continuum. Attention-based attribution revealed genes consistently enriched for established GWAS-supported AD risk loci, including APOE, BIN1, CLU, and RBFOX1. Stage-resolved neuroanatomical signatures identified shared vulnerability hubs across disease stages alongside stage-specific patterns: striatal involvement in subjective decline, frontotemporal engagement during prodromal impairment, and consolidated multimodal network disruption in AD. These results demonstrate that interpretable multimodal AI can synthesize imaging and genetics to reveal mechanistic insights, providing a foundation for clinically deployable tools that enable earlier risk stratification and inform precision therapeutic strategies in Alzheimer's disease.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The 6th International Verification of Neural Networks Competition (VNN-COMP 2025): Summary and Results</title>
<link>https://arxiv.org/abs/2512.19007</link>
<guid>https://arxiv.org/abs/2512.19007</guid>
<content:encoded><![CDATA[
arXiv:2512.19007v1 Announce Type: new 
Abstract: This report summarizes the 6th International Verification of Neural Networks Competition (VNN-COMP 2025), held as a part of the 8th International Symposium on AI Verification (SAIV), that was collocated with the 37th International Conference on Computer-Aided Verification (CAV). VNN-COMP is held annually to facilitate the fair and objective comparison of state-of-the-art neural network verification tools, encourage the standardization of tool interfaces, and bring together the neural network verification community. To this end, standardized formats for networks (ONNX) and specification (VNN-LIB) were defined, tools were evaluated on equal-cost hardware (using an automatic evaluation pipeline based on AWS instances), and tool parameters were chosen by the participants before the final test sets were made public. In the 2025 iteration, 8 teams participated on a diverse set of 16 regular and 9 extended benchmarks. This report summarizes the rules, benchmarks, participating tools, results, and lessons learned from this iteration of this competition.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizer Dynamics at the Edge of Stability with Differential Privacy</title>
<link>https://arxiv.org/abs/2512.19019</link>
<guid>https://arxiv.org/abs/2512.19019</guid>
<content:encoded><![CDATA[
arXiv:2512.19019v1 Announce Type: new 
Abstract: Deep learning models can reveal sensitive information about individual training examples, and while differential privacy (DP) provides guarantees restricting such leakage, it also alters optimization dynamics in poorly understood ways. We study the training dynamics of neural networks under DP by comparing Gradient Descent (GD), and Adam to their privacy-preserving variants. Prior work shows that these optimizers exhibit distinct stability dynamics: full-batch methods train at the Edge of Stability (EoS), while mini-batch and adaptive methods exhibit analogous edge-of-stability behavior. At these regimes, the training loss and the sharpness--the maximum eigenvalue of the training loss Hessian--exhibit certain characteristic behavior. In DP training, per-example gradient clipping and Gaussian noise modify the update rule, and it is unclear whether these stability patterns persist. We analyze how clipping and noise change sharpness and loss evolution and show that while DP generally reduces the sharpness and can prevent optimizers from fully reaching the classical stability thresholds, patterns from EoS and analogous adaptive methods stability regimes persist, with the largest learning rates and largest privacy budgets approaching, and sometimes exceeding, these thresholds. These findings highlight the unpredictability introduced by DP in neural network optimization.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Surrogate-Augmented Symbolic CFD-Driven Training Framework for Accelerating Multi-objective Physical Model Development</title>
<link>https://arxiv.org/abs/2512.19031</link>
<guid>https://arxiv.org/abs/2512.19031</guid>
<content:encoded><![CDATA[
arXiv:2512.19031v1 Announce Type: new 
Abstract: Computational Fluid Dynamics (CFD)-driven training combines machine learning (ML) with CFD solvers to develop physically consistent closure models with improved predictive accuracy. In the original framework, each ML-generated candidate model is embedded in a CFD solver and evaluated against reference data, requiring hundreds to thousands of high-fidelity simulations and resulting in prohibitive computational cost for complex flows. To overcome this limitation, we propose an extended framework that integrates surrogate modeling into symbolic CFD-driven training in real time to reduce training cost. The surrogate model learns to approximate the errors of ML-generated models based on previous CFD evaluations and is continuously refined during training. Newly generated models are first assessed using the surrogate, and only those predicted to yield small errors or high uncertainty are subsequently evaluated with full CFD simulations. Discrete expressions generated by symbolic regression are mapped into a continuous space using averaged input-symbol values as inputs to a probabilistic surrogate model. To support multi-objective model training, particularly when fixed weighting of competing quantities is challenging, the surrogate is extended to a multi-output formulation by generalizing the kernel to a matrix form, providing one mean and variance prediction per training objective. Selection metrics based on these probabilistic outputs are used to identify an optimal training setup. The proposed surrogate-augmented CFD-driven training framework is demonstrated across a range of statistically one- and two-dimensional flows, including both single- and multi-expression model optimization. In all cases, the framework substantially reduces training cost while maintaining predictive accuracy comparable to that of the original CFD-driven approach.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Time-series Forecast for Indoor Zone Air Temperature with Long Horizons: A Case Study with Sensor-based Data from a Smart Building</title>
<link>https://arxiv.org/abs/2512.19038</link>
<guid>https://arxiv.org/abs/2512.19038</guid>
<content:encoded><![CDATA[
arXiv:2512.19038v1 Announce Type: new 
Abstract: With the press of global climate change, extreme weather and sudden weather changes are becoming increasingly common. To maintain a comfortable indoor environment and minimize the contribution of the building to climate change as much as possible, higher requirements are placed on the operation and control of HVAC systems, e.g., more energy-efficient and flexible to response to the rapid change of weather. This places demands on the rapid modeling and prediction of zone air temperatures of buildings. Compared to the traditional simulation-based approach such as EnergyPlus and DOE2, a hybrid approach combined physics and data-driven is more suitable. Recently, the availability of high-quality datasets and algorithmic breakthroughs have driven a considerable amount of work in this field. However, in the niche of short- and long-term predictions, there are still some gaps in existing research. This paper aims to develop a time series forecast model to predict the zone air temperature in a building located in America on a 2-week horizon. The findings could be further improved to support intelligent control and operation of HVAC systems (i.e. demand flexibility) and could also be used as hybrid building energy modeling.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Personalization of Generative Models via Optimal Experimental Design</title>
<link>https://arxiv.org/abs/2512.19057</link>
<guid>https://arxiv.org/abs/2512.19057</guid>
<content:encoded><![CDATA[
arXiv:2512.19057v1 Announce Type: new 
Abstract: Preference learning from human feedback has the ability to align generative models with the needs of end-users. Human feedback is costly and time-consuming to obtain, which creates demand for data-efficient query selection methods. This work presents a novel approach that leverages optimal experimental design to ask humans the most informative preference queries, from which we can elucidate the latent reward function modeling user preferences efficiently. We formulate the problem of preference query selection as the one that maximizes the information about the underlying latent preference model. We show that this problem has a convex optimization formulation, and introduce a statistically and computationally efficient algorithm ED-PBRL that is supported by theoretical guarantees and can efficiently construct structured queries such as images or text. We empirically present the proposed framework by personalizing a text-to-image generative model to user-specific styles, showing that it requires less preference queries compared to random query selection.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fraud Detection Through Large-Scale Graph Clustering with Heterogeneous Link Transformation</title>
<link>https://arxiv.org/abs/2512.19061</link>
<guid>https://arxiv.org/abs/2512.19061</guid>
<content:encoded><![CDATA[
arXiv:2512.19061v1 Announce Type: new 
Abstract: Collaborative fraud, where multiple fraudulent accounts coordinate to exploit online payment systems, poses significant challenges due to the formation of complex network structures. Traditional detection methods that rely solely on high-confidence identity links suffer from limited coverage, while approaches using all available linkages often result in fragmented graphs with reduced clustering effectiveness. In this paper, we propose a novel graph-based fraud detection framework that addresses the challenge of large-scale heterogeneous graph clustering through a principled link transformation approach. Our method distinguishes between \emph{hard links} (high-confidence identity relationships such as phone numbers, credit cards, and national IDs) and \emph{soft links} (behavioral associations including device fingerprints, cookies, and IP addresses). We introduce a graph transformation technique that first identifies connected components via hard links, merges them into super-nodes, and then reconstructs a weighted soft-link graph amenable to efficient embedding and clustering. The transformed graph is processed using LINE (Large-scale Information Network Embedding) for representation learning, followed by HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) for density-based cluster discovery. Experiments on a real-world payment platform dataset demonstrate that our approach achieves significant graph size reduction (from 25 million to 7.7 million nodes), doubles the detection coverage compared to hard-link-only baselines, and maintains high precision across identified fraud clusters. Our framework provides a scalable and practical solution for industrial-scale fraud detection systems.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DIVER-1 : Deep Integration of Vast Electrophysiological Recordings at Scale</title>
<link>https://arxiv.org/abs/2512.19097</link>
<guid>https://arxiv.org/abs/2512.19097</guid>
<content:encoded><![CDATA[
arXiv:2512.19097v1 Announce Type: new 
Abstract: Electrophysiology signals such as EEG and iEEG are central to neuroscience, brain-computer interfaces, and clinical applications, yet existing foundation models remain limited in scale despite clear evidence that scaling improves performance. We introduce DIVER-1, a family of EEG and iEEG foundation models trained on the largest and most diverse corpus to date-5.3k hours of iEEG and 54k hours of EEG (1.6M channel-hours from over 17.7k subjects)-and scaled up to 1.82B parameters. We present the first systematic scaling law analysis for this domain, showing that they follow data-constrained scaling laws: for a given amount of data and compute, smaller models trained for extended epochs consistently outperform larger models trained briefly. This behavior contrasts with prior electrophysiology foundation models that emphasized model size over training duration. To achieve strong performance, we also design architectural innovations including any-variate attention, sliding temporal conditional positional encoding, and multi-domain reconstruction. DIVER-1 iEEG and EEG models each achieve state-of-the-art performance on their respective benchmarks, establishing a concrete guidelines for efficient scaling and resource allocation in electrophysiology foundation model development.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual Model Deep Learning for Alzheimer Prognostication</title>
<link>https://arxiv.org/abs/2512.19099</link>
<guid>https://arxiv.org/abs/2512.19099</guid>
<content:encoded><![CDATA[
arXiv:2512.19099v1 Announce Type: new 
Abstract: Disease modifying therapies for Alzheimer's disease demand precise timing decisions, yet current predictive models require longitudinal observations and provide no uncertainty quantification, rendering them impractical at the critical first visit when treatment decisions must be made. We developed PROGRESS (PRognostic Generalization from REsting Static Signatures), a dual-model deep learning framework that transforms a single baseline cerebrospinal fluid biomarker assessment into actionable prognostic estimates without requiring prior clinical history. The framework addresses two complementary clinical questions: a probabilistic trajectory network predicts individualized cognitive decline with calibrated uncertainty bounds achieving near-nominal coverage, enabling honest prognostic communication; and a deep survival model estimates time to conversion from mild cognitive impairment to dementia. Using data from over 3,000 participants across 43 Alzheimer's Disease Research Centers in the National Alzheimer's Coordinating Center database, PROGRESS substantially outperforms Cox proportional hazards, Random Survival Forests, and gradient boosting methods for survival prediction. Risk stratification identifies patient groups with seven-fold differences in conversion rates, enabling clinically meaningful treatment prioritization. Leave-one-center-out validation demonstrates robust generalizability, with survival discrimination remaining strong across held-out sites despite heterogeneous measurement conditions spanning four decades of assay technologies. By combining superior survival prediction with trustworthy trajectory uncertainty quantification, PROGRESS bridges the gap between biomarker measurement and personalized clinical decision-making.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Timely Parameter Updating in Over-the-Air Federated Learning</title>
<link>https://arxiv.org/abs/2512.19103</link>
<guid>https://arxiv.org/abs/2512.19103</guid>
<content:encoded><![CDATA[
arXiv:2512.19103v1 Announce Type: new 
Abstract: Incorporating over-the-air computations (OAC) into the model training process of federated learning (FL) is an effective approach to alleviating the communication bottleneck in FL systems. Under OAC-FL, every client modulates its intermediate parameters, such as gradient, onto the same set of orthogonal waveforms and simultaneously transmits the radio signal to the edge server. By exploiting the superposition property of multiple-access channels, the edge server can obtain an automatically aggregated global gradient from the received signal. However, the limited number of orthogonal waveforms available in practical systems is fundamentally mismatched with the high dimensionality of modern deep learning models. To address this issue, we propose Freshness Freshness-mAgnItude awaRe top-k (FAIR-k), an algorithm that selects, in each communication round, the most impactful subset of gradients to be updated over the air. In essence, FAIR-k combines the complementary strengths of the Round-Robin and Top-k algorithms, striking a delicate balance between timeliness (freshness of parameter updates) and importance (gradient magnitude). Leveraging tools from Markov analysis, we characterize the distribution of parameter staleness under FAIR-k. Building on this, we establish the convergence rate of OAC-FL with FAIR-k, which discloses the joint effect of data heterogeneity, channel noise, and parameter staleness on the training efficiency. Notably, as opposed to conventional analyses that assume a universal Lipschitz constant across all the clients, our framework adopts a finer-grained model of the data heterogeneity. The analysis demonstrates that since FAIR-k promotes fresh (and fair) parameter updates, it not only accelerates convergence but also enhances communication efficiency by enabling an extended period of local training without significantly affecting overall training efficiency.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HyperLoad: A Cross-Modality Enhanced Large Language Model-Based Framework for Green Data Center Cooling Load Prediction</title>
<link>https://arxiv.org/abs/2512.19114</link>
<guid>https://arxiv.org/abs/2512.19114</guid>
<content:encoded><![CDATA[
arXiv:2512.19114v1 Announce Type: new 
Abstract: The rapid growth of artificial intelligence is exponentially escalating computational demand, inflating data center energy use and carbon emissions, and spurring rapid deployment of green data centers to relieve resource and environmental stress. Achieving sub-minute orchestration of renewables, storage, and loads, while minimizing PUE and lifecycle carbon intensity, hinges on accurate load forecasting. However, existing methods struggle to address small-sample scenarios caused by cold start, load distortion, multi-source data fragmentation, and distribution shifts in green data centers. We introduce HyperLoad, a cross-modality framework that exploits pre-trained large language models (LLMs) to overcome data scarcity. In the Cross-Modality Knowledge Alignment phase, textual priors and time-series data are mapped to a common latent space, maximizing the utility of prior knowledge. In the Multi-Scale Feature Modeling phase, domain-aligned priors are injected through adaptive prefix-tuning, enabling rapid scenario adaptation, while an Enhanced Global Interaction Attention mechanism captures cross-device temporal dependencies. The public DCData dataset is released for benchmarking. Under both data sufficient and data scarce settings, HyperLoad consistently surpasses state-of-the-art (SOTA) baselines, demonstrating its practicality for sustainable green data center management.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Composable Channel-Adaptive Architecture for Seizure Classification</title>
<link>https://arxiv.org/abs/2512.19123</link>
<guid>https://arxiv.org/abs/2512.19123</guid>
<content:encoded><![CDATA[
arXiv:2512.19123v1 Announce Type: new 
Abstract: Objective: We develop a channel-adaptive (CA) architecture that seamlessly processes multi-variate time-series with an arbitrary number of channels, and in particular intracranial electroencephalography (iEEG) recordings. Methods: Our CA architecture first processes the iEEG signal using state-of-the-art models applied to each single channel independently. The resulting features are then fused using a vector-symbolic algorithm which reconstructs the spatial relationship using a trainable scalar per channel. Finally, the fused features are accumulated in a long-term memory of up to 2 minutes to perform the classification. Each CA-model can then be pre-trained on a large corpus of iEEG recordings from multiple heterogeneous subjects. The pre-trained model is personalized to each subject via a quick fine-tuning routine, which uses equal or lower amounts of data compared to existing state-of-the-art models, but requiring only 1/5 of the time. Results: We evaluate our CA-models on a seizure detection task both on a short-term (~20 hours) and a long-term (~2500 hours) dataset. In particular, our CA-EEGWaveNet is trained on a single seizure of the tested subject, while the baseline EEGWaveNet is trained on all but one. Even in this challenging scenario, our CA-EEGWaveNet surpasses the baseline in median F1-score (0.78 vs 0.76). Similarly, CA-EEGNet based on EEGNet, also surpasses its baseline in median F1-score (0.79 vs 0.74). Conclusion and significance: Our CA-model addresses two issues: first, it is channel-adaptive and can therefore be trained across heterogeneous subjects without loss of performance; second, it increases the effective temporal context size to a clinically-relevant length. Therefore, our model is a drop-in replacement for existing models, bringing better characteristics and performance across the board.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Convex Loss Function for Set Prediction with Optimal Trade-offs Between Size and Conditional Coverage</title>
<link>https://arxiv.org/abs/2512.19142</link>
<guid>https://arxiv.org/abs/2512.19142</guid>
<content:encoded><![CDATA[
arXiv:2512.19142v1 Announce Type: new 
Abstract: We consider supervised learning problems in which set predictions provide explicit uncertainty estimates. Using Choquet integrals (a.k.a. Lov{\'a}sz extensions), we propose a convex loss function for nondecreasing subset-valued functions obtained as level sets of a real-valued function. This loss function allows optimal trade-offs between conditional probabilistic coverage and the ''size'' of the set, measured by a non-decreasing submodular function. We also propose several extensions that mimic loss functions and criteria for binary classification with asymmetric losses, and show how to naturally obtain sets with optimized conditional coverage. We derive efficient optimization algorithms, either based on stochastic gradient descent or reweighted least-squares formulations, and illustrate our findings with a series of experiments on synthetic datasets for classification and regression tasks, showing improvements over approaches that aim for marginal coverage.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RP-CATE: Recurrent Perceptron-based Channel Attention Transformer Encoder for Industrial Hybrid Modeling</title>
<link>https://arxiv.org/abs/2512.19147</link>
<guid>https://arxiv.org/abs/2512.19147</guid>
<content:encoded><![CDATA[
arXiv:2512.19147v1 Announce Type: new 
Abstract: Nowadays, industrial hybrid modeling which integrates both mechanistic modeling and machine learning-based modeling techniques has attracted increasing interest from scholars due to its high accuracy, low computational cost, and satisfactory interpretability. Nevertheless, the existing industrial hybrid modeling methods still face two main limitations. First, current research has mainly focused on applying a single machine learning method to one specific task, failing to develop a comprehensive machine learning architecture suitable for modeling tasks, which limits their ability to effectively represent complex industrial scenarios. Second, industrial datasets often contain underlying associations (e.g., monotonicity or periodicity) that are not adequately exploited by current research, which can degrade model's predictive performance. To address these limitations, this paper proposes the Recurrent Perceptron-based Channel Attention Transformer Encoder (RP-CATE), with three distinctive characteristics: 1: We developed a novel architecture by replacing the self-attention mechanism with channel attention and incorporating our proposed Recurrent Perceptron (RP) Module into Transformer, achieving enhanced effectiveness for industrial modeling tasks compared to the original Transformer. 2: We proposed a new data type called Pseudo-Image Data (PID) tailored for channel attention requirements and developed a cyclic sliding window method for generating PID. 3: We introduced the concept of Pseudo-Sequential Data (PSD) and a method for converting industrial datasets into PSD, which enables the RP Module to capture the underlying associations within industrial dataset more effectively. An experiment aimed at hybrid modeling in chemical engineering was conducted by using RP-CATE and the experimental results demonstrate that RP-CATE achieves the best performance compared to other baseline models.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Sliding Windows: Learning to Manage Memory in Non-Markovian Environments</title>
<link>https://arxiv.org/abs/2512.19154</link>
<guid>https://arxiv.org/abs/2512.19154</guid>
<content:encoded><![CDATA[
arXiv:2512.19154v1 Announce Type: new 
Abstract: Recent success in developing increasingly general purpose agents based on sequence models has led to increased focus on the problem of deploying computationally limited agents within the vastly more complex real-world. A key challenge experienced in these more realistic domains is highly non-Markovian dependencies with respect to the agent's observations, which are less common in small controlled domains. The predominant approach for dealing with this in the literature is to stack together a window of the most recent observations (Frame Stacking), but this window size must grow with the degree of non-Markovian dependencies, which results in prohibitive computational and memory requirements for both action inference and learning. In this paper, we are motivated by the insight that in many environments that are highly non-Markovian with respect to time, the environment only causally depends on a relatively small number of observations over that time-scale. A natural direction would then be to consider meta-algorithms that maintain relatively small adaptive stacks of memories such that it is possible to express highly non-Markovian dependencies with respect to time while considering fewer observations at each step and thus experience substantial savings in both compute and memory requirements. Hence, we propose a meta-algorithm (Adaptive Stacking) for achieving exactly that with convergence guarantees and quantify the reduced computation and memory constraints for MLP, LSTM, and Transformer-based agents. Our experiments utilize popular memory tasks, which give us control over the degree of non-Markovian dependencies. This allows us to demonstrate that an appropriate meta-algorithm can learn the removal of memories not predictive of future rewards without excessive removal of important experiences. Code: https://github.com/geraudnt/adaptive-stacking
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Practical Quantum-Classical Feature Fusion for complex data Classification</title>
<link>https://arxiv.org/abs/2512.19180</link>
<guid>https://arxiv.org/abs/2512.19180</guid>
<content:encoded><![CDATA[
arXiv:2512.19180v1 Announce Type: new 
Abstract: Hybrid quantum and classical learning aims to couple quantum feature maps with the robustness of classical neural networks, yet most architectures treat the quantum circuit as an isolated feature extractor and merge its measurements with classical representations by direct concatenation. This neglects that the quantum and classical branches constitute distinct computational modalities and limits reliable performance on complex, high dimensional tabular and semi structured data, including remote sensing, environmental monitoring, and medical diagnostics. We present a multimodal formulation of hybrid learning and propose a cross attention mid fusion architecture in which a classical representation queries quantum derived feature tokens through an attention block with residual connectivity. The quantum branch is kept within practical NISQ budgets and uses up to nine qubits. We evaluate on Wine, Breast Cancer, Forest CoverType, FashionMNIST, and SteelPlatesFaults, comparing a quantum only model, a classical baseline, residual hybrid models, and the proposed mid fusion model under a consistent protocol. Pure quantum and standard hybrid designs underperform due to measurement induced information loss, while cross attention mid fusion is consistently competitive and improves performance on the more complex datasets in most cases. These findings suggest that quantum derived information becomes most valuable when integrated through principled multimodal fusion rather than used in isolation or loosely appended to classical features.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Operator-Based Generalization Bound for Deep Learning: Insights on Multi-Task Learning</title>
<link>https://arxiv.org/abs/2512.19184</link>
<guid>https://arxiv.org/abs/2512.19184</guid>
<content:encoded><![CDATA[
arXiv:2512.19184v1 Announce Type: new 
Abstract: This paper presents novel generalization bounds for vector-valued neural networks and deep kernel methods, focusing on multi-task learning through an operator-theoretic framework. Our key development lies in strategically combining a Koopman based approach with existing techniques, achieving tighter generalization guarantees compared to traditional norm-based bounds. To mitigate computational challenges associated with Koopman-based methods, we introduce sketching techniques applicable to vector valued neural networks. These techniques yield excess risk bounds under generic Lipschitz losses, providing performance guarantees for applications including robust and multiple quantile regression. Furthermore, we propose a novel deep learning framework, deep vector-valued reproducing kernel Hilbert spaces (vvRKHS), leveraging Perron Frobenius (PF) operators to enhance deep kernel methods. We derive a new Rademacher generalization bound for this framework, explicitly addressing underfitting and overfitting through kernel refinement strategies. This work offers novel insights into the generalization properties of multitask learning with deep learning architectures, an area that has been relatively unexplored until recent developments.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal Heterogeneous Graph Learning Method for Chronic Obstructive Pulmonary Disease Prediction</title>
<link>https://arxiv.org/abs/2512.19194</link>
<guid>https://arxiv.org/abs/2512.19194</guid>
<content:encoded><![CDATA[
arXiv:2512.19194v1 Announce Type: new 
Abstract: Due to the insufficient diagnosis and treatment capabilities at the grassroots level, there are still deficiencies in the early identification and early warning of acute exacerbation of Chronic obstructive pulmonary disease (COPD), often resulting in a high prevalence rate and high burden, but the screening rate is relatively low. In order to gradually improve this situation. In this paper, this study develop a Causal Heterogeneous Graph Representation Learning (CHGRL) method for COPD comorbidity risk prediction method that: a) constructing a heterogeneous Our dataset includes the interaction between patients and diseases; b) A cause-aware heterogeneous graph learning architecture has been constructed, combining causal inference mechanisms with heterogeneous graph learning, which can support heterogeneous graph causal learning for different types of relationships; and c) Incorporate the causal loss function in the model design, and add counterfactual reasoning learning loss and causal regularization loss on the basis of the cross-entropy classification loss. We evaluate our method and compare its performance with strong GNN baselines. Following experimental evaluation, the proposed model demonstrates high detection accuracy.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Koopman-Based Generalization Bounds for Multi-Task Deep Learning</title>
<link>https://arxiv.org/abs/2512.19199</link>
<guid>https://arxiv.org/abs/2512.19199</guid>
<content:encoded><![CDATA[
arXiv:2512.19199v1 Announce Type: new 
Abstract: The paper establishes generalization bounds for multitask deep neural networks using operator-theoretic techniques. The authors propose a tighter bound than those derived from conventional norm based methods by leveraging small condition numbers in the weight matrices and introducing a tailored Sobolev space as an expanded hypothesis space. This enhanced bound remains valid even in single output settings, outperforming existing Koopman based bounds. The resulting framework maintains key advantages such as flexibility and independence from network width, offering a more precise theoretical understanding of multitask deep learning in the context of kernel methods.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MixKVQ: Query-Aware Mixed-Precision KV Cache Quantization for Long-Context Reasoning</title>
<link>https://arxiv.org/abs/2512.19206</link>
<guid>https://arxiv.org/abs/2512.19206</guid>
<content:encoded><![CDATA[
arXiv:2512.19206v1 Announce Type: new 
Abstract: Long Chain-of-Thought (CoT) reasoning has significantly advanced the capabilities of Large Language Models (LLMs), but this progress is accompanied by substantial memory and latency overhead from the extensive Key-Value (KV) cache. Although KV cache quantization is a promising compression technique, existing low-bit quantization methods often exhibit severe performance degradation on complex reasoning tasks. Fixed-precision quantization struggles to handle outlier channels in the key cache, while current mixed-precision strategies fail to accurately identify components requiring high-precision representation. We find that an effective low-bit KV cache quantization strategy must consider two factors: a key channel's intrinsic quantization difficulty and its relevance to the query. Based on this insight, we propose MixKVQ, a novel plug-and-play method that introduces a lightweight, query-aware algorithm to identify and preserve critical key channels that need higher precision, while applying per-token quantization for value cache. Experiments on complex reasoning datasets demonstrate that our approach significantly outperforms existing low-bit methods, achieving performance comparable to a full-precision baseline at a substantially reduced memory footprint.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Phase-space entropy at acquisition reflects downstream learnability</title>
<link>https://arxiv.org/abs/2512.19223</link>
<guid>https://arxiv.org/abs/2512.19223</guid>
<content:encoded><![CDATA[
arXiv:2512.19223v1 Announce Type: new 
Abstract: Modern learning systems work with data that vary widely across domains, but they all ultimately depend on how much structure is already present in the measurements before any model is trained. This raises a basic question: is there a general, modality-agnostic way to quantify how acquisition itself preserves or destroys the information that downstream learners could use? Here we propose an acquisition-level scalar $\Delta S_{\mathcal B}$ based on instrument-resolved phase space. Unlike pixelwise distortion or purely spectral errors that often saturate under aggressive undersampling, $\Delta S_{\mathcal B}$ directly quantifies how acquisition mixes or removes joint space--frequency structure at the instrument scale. We show theoretically that \(\Delta S_{\mathcal B}\) correctly identifies the phase-space coherence of periodic sampling as the physical source of aliasing, recovering classical sampling-theorem consequences. Empirically, across masked image classification, accelerated MRI, and massive MIMO (including over-the-air measurements), $|\Delta S_{\mathcal B}|$ consistently ranks sampling geometries and predicts downstream reconstruction/recognition difficulty \emph{without training}. In particular, minimizing $|\Delta S_{\mathcal B}|$ enables zero-training selection of variable-density MRI mask parameters that matches designs tuned by conventional pre-reconstruction criteria. These results suggest that phase-space entropy at acquisition reflects downstream learnability, enabling pre-training selection of candidate sampling policies and as a shared notion of information preservation across modalities.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Regression generation adversarial network based on dual data evaluation strategy for industrial application</title>
<link>https://arxiv.org/abs/2512.19232</link>
<guid>https://arxiv.org/abs/2512.19232</guid>
<content:encoded><![CDATA[
arXiv:2512.19232v1 Announce Type: new 
Abstract: Soft sensing infers hard-to-measure data through a large number of easily obtainable variables. However, in complex industrial scenarios, the issue of insufficient data volume persists, which diminishes the reliability of soft sensing. Generative Adversarial Networks (GAN) are one of the effective solutions for addressing insufficient samples. Nevertheless, traditional GAN fail to account for the mapping relationship between labels and features, which limits further performance improvement. Although some studies have proposed solutions, none have considered both performance and efficiency simultaneously. To address these problems, this paper proposes the multi-task learning-based regression GAN framework that integrates regression information into both the discriminator and generator, and implements a shallow sharing mechanism between the discriminator and regressor. This approach significantly enhances the quality of generated samples while improving the algorithm's operational efficiency. Moreover, considering the importance of training samples and generated samples, a dual data evaluation strategy is designed to make GAN generate more diverse samples, thereby increasing the generalization of subsequent modeling. The superiority of method is validated through four classic industrial soft sensing cases: wastewater treatment plants, surface water, $CO_2$ absorption towers, and industrial gas turbines.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Black-Box Tuning to Guided Optimization via Hyperparameters Interaction Analysis</title>
<link>https://arxiv.org/abs/2512.19246</link>
<guid>https://arxiv.org/abs/2512.19246</guid>
<content:encoded><![CDATA[
arXiv:2512.19246v1 Announce Type: new 
Abstract: Hyperparameters tuning is a fundamental, yet computationally expensive, step in optimizing machine learning models. Beyond optimization, understanding the relative importance and interaction of hyperparameters is critical to efficient model development. In this paper, we introduce MetaSHAP, a scalable semi-automated eXplainable AI (XAI) method, that uses meta-learning and Shapley values analysis to provide actionable and dataset-aware tuning insights. MetaSHAP operates over a vast benchmark of over 09 millions evaluated machine learning pipelines, allowing it to produce interpretable importance scores and actionable tuning insights that reveal how much each hyperparameter matters, how it interacts with others and in which value ranges its influence is concentrated. For a given algorithm and dataset, MetaSHAP learns a surrogate performance model from historical configurations, computes hyperparameters interactions using SHAP-based analysis, and derives interpretable tuning ranges from the most influential hyperparameters. This allows practitioners not only to prioritize which hyperparameters to tune, but also to understand their directionality and interactions. We empirically validate MetaSHAP on a diverse benchmark of 164 classification datasets and 14 classifiers, demonstrating that it produces reliable importance rankings and competitive performance when used to guide Bayesian optimization.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Small Language Models as Compiler Experts: Auto-Parallelization for Heterogeneous Systems</title>
<link>https://arxiv.org/abs/2512.19250</link>
<guid>https://arxiv.org/abs/2512.19250</guid>
<content:encoded><![CDATA[
arXiv:2512.19250v1 Announce Type: new 
Abstract: Traditional auto-parallelizing compilers, reliant on rigid heuristics, struggle with the complexity of modern heterogeneous systems. This paper presents a comprehensive evaluation of small (approximately 1B parameter) language-model-driven compiler auto-parallelization. We evaluate three models: gemma3, llama3.2, and qwen2.5, using six reasoning strategies across 11 real-world kernels drawn from scientific computing, graph algorithms, and machine learning. Our system is benchmarked against strong compiler baselines, including LLVM Polly, TVM, and Triton. Across 376 total evaluations, the proposed approach achieves an average speedup of 6.81x and a peak performance of 43.25x on convolution operations. We analyze scalability, verify correctness using multiple sanitizers, and confirm robustness across diverse compilers and hardware platforms. Our results demonstrate that small, efficient language models can serve as powerful reasoning engines for complex compiler optimization tasks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Unlearning in the Era of Quantum Machine Learning: An Empirical Study</title>
<link>https://arxiv.org/abs/2512.19253</link>
<guid>https://arxiv.org/abs/2512.19253</guid>
<content:encoded><![CDATA[
arXiv:2512.19253v1 Announce Type: new 
Abstract: We present the first comprehensive empirical study of machine unlearning (MU) in hybrid quantum-classical neural networks. While MU has been extensively explored in classical deep learning, its behavior within variational quantum circuits (VQCs) and quantum-augmented architectures remains largely unexplored. First, we adapt a broad suite of unlearning methods to quantum settings, including gradient-based, distillation-based, regularization-based and certified techniques. Second, we introduce two new unlearning strategies tailored to hybrid models. Experiments across Iris, MNIST, and Fashion-MNIST, under both subset removal and full-class deletion, reveal that quantum models can support effective unlearning, but outcomes depend strongly on circuit depth, entanglement structure, and task complexity. Shallow VQCs display high intrinsic stability with minimal memorization, whereas deeper hybrid models exhibit stronger trade-offs between utility, forgetting strength, and alignment with retrain oracle. We find that certain methods, e.g. EU-k, LCA, and Certified Unlearning, consistently provide the best balance across metrics. These findings establish baseline empirical insights into quantum machine unlearning and highlight the need for quantum-aware algorithms and theoretical guarantees, as quantum machine learning systems continue to expand in scale and capability. We publicly release our code at: https://github.com/CrivoiCarla/HQML.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Digital Twin-Driven Zero-Shot Fault Diagnosis of Axial Piston Pumps Using Fluid-Borne Noise Signals</title>
<link>https://arxiv.org/abs/2512.19280</link>
<guid>https://arxiv.org/abs/2512.19280</guid>
<content:encoded><![CDATA[
arXiv:2512.19280v1 Announce Type: new 
Abstract: Axial piston pumps are crucial components in fluid power systems, where reliable fault diagnosis is essential for ensuring operational safety and efficiency. Traditional data-driven methods require extensive labeled fault data, which is often impractical to obtain, while model-based approaches suffer from parameter uncertainties. This paper proposes a digital twin (DT)-driven zero-shot fault diagnosis framework utilizing fluid-borne noise (FBN) signals. The framework calibrates a high-fidelity DT model using only healthy-state data, generates synthetic fault signals for training deep learning classifiers, and employs a physics-informed neural network (PINN) as a virtual sensor for flow ripple estimation. Gradient-weighted class activation mapping (Grad-CAM) is integrated to visualize the decision-making process of neural networks, revealing that large kernels matching the subsequence length in time-domain inputs and small kernels in time-frequency domain inputs enable higher diagnostic accuracy by focusing on physically meaningful features. Experimental validations demonstrate that training on signals from the calibrated DT model yields diagnostic accuracies exceeding 95\% on real-world benchmarks, while uncalibrated models result in significantly lower performance, highlighting the framework's effectiveness in data-scarce scenarios.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Time-Vertex Machine Learning for Optimal Sensor Placement in Temporal Graph Signals: Applications in Structural Health Monitoring</title>
<link>https://arxiv.org/abs/2512.19309</link>
<guid>https://arxiv.org/abs/2512.19309</guid>
<content:encoded><![CDATA[
arXiv:2512.19309v1 Announce Type: new 
Abstract: Structural Health Monitoring (SHM) plays a crucial role in maintaining the safety and resilience of infrastructure. As sensor networks grow in scale and complexity, identifying the most informative sensors becomes essential to reduce deployment costs without compromising monitoring quality. While Graph Signal Processing (GSP) has shown promise by leveraging spatial correlations among sensor nodes, conventional approaches often overlook the temporal dynamics of structural behavior. To overcome this limitation, we propose Time-Vertex Machine Learning (TVML), a novel framework that integrates GSP, time-domain analysis, and machine learning to enable interpretable and efficient sensor placement by identifying representative nodes that minimize redundancy while preserving critical information. We evaluate the proposed approach on two bridge datasets for damage detection and time-varying graph signal reconstruction tasks. The results demonstrate the effectiveness of our approach in enhancing SHM systems by providing a robust, adaptive, and efficient solution for sensor placement.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAGIC: Achieving Superior Model Merging via Magnitude Calibration</title>
<link>https://arxiv.org/abs/2512.19320</link>
<guid>https://arxiv.org/abs/2512.19320</guid>
<content:encoded><![CDATA[
arXiv:2512.19320v1 Announce Type: new 
Abstract: The proliferation of pre-trained models has given rise to a wide array of specialised, fine-tuned models. Model merging aims to merge the distinct capabilities of these specialised models into a unified model, requiring minimal or even no additional training. A core objective of model merging is to ensure the merged model retains the behavioural characteristics of the specialised models, typically achieved through feature alignment. We identify that features consist of two critical components: direction and magnitude. Prior research has predominantly focused on directional alignment, while the influence of magnitude remains largely neglected, despite its pronounced vulnerability to perturbations introduced by common merging operations (e.g., parameter fusion and sparsification). Such perturbations to magnitude inevitably lead to feature deviations in the merged model from the specialised models, resulting in subsequent performance degradation. To address this, we propose MAGnItude Calibration (MAGIC), a plug-and-play framework that rectifies layer-wise magnitudes in feature and weight spaces, with three variants. Specifically, our Feature Space Calibration (FSC) realigns the merged model's features using a small set of unlabelled data, while Weight Space Calibration (WSC) extends this calibration to the weight space without requiring additional data. Combining these yields Dual Space Calibration (DSC). Comprehensive experiments demonstrate that MAGIC consistently boosts performance across diverse Computer Vision tasks (+4.3% on eight datasets) and NLP tasks (+8.0% on Llama) without additional training. Our code is available at: https://github.com/lyymuwu/MAGIC
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Alternative positional encoding functions for neural transformers</title>
<link>https://arxiv.org/abs/2512.19323</link>
<guid>https://arxiv.org/abs/2512.19323</guid>
<content:encoded><![CDATA[
arXiv:2512.19323v1 Announce Type: new 
Abstract: A key module in neural transformer-based deep architectures is positional encoding. This module enables a suitable way to encode positional information as input for transformer neural layers. This success has been rooted in the use of sinusoidal functions of various frequencies, in order to capture recurrent patterns of differing typical periods. In this work, an alternative set of periodic functions is proposed for positional encoding. These functions preserve some key properties of sinusoidal ones, while they depart from them in fundamental ways. Some tentative experiments are reported, where the original sinusoidal version is substantially outperformed. This strongly suggests that the alternative functions may have a wider use in other transformer architectures.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Logical View of GNN-Style Computation and the Role of Activation Functions</title>
<link>https://arxiv.org/abs/2512.19332</link>
<guid>https://arxiv.org/abs/2512.19332</guid>
<content:encoded><![CDATA[
arXiv:2512.19332v1 Announce Type: new 
Abstract: We study the numerical and Boolean expressiveness of MPLang, a declarative language that captures the computation of graph neural networks (GNNs) through linear message passing and activation functions. We begin with A-MPLang, the fragment without activation functions, and give a characterization of its expressive power in terms of walk-summed features. For bounded activation functions, we show that (under mild conditions) all eventually constant activations yield the same expressive power - numerical and Boolean - and that it subsumes previously established logics for GNNs with eventually constant activation functions but without linear layers. Finally, we prove the first expressive separation between unbounded and bounded activations in the presence of linear layers: MPLang with ReLU is strictly more powerful for numerical queries than MPLang with eventually constant activation functions, e.g., truncated ReLU. This hinges on subtle interactions between linear aggregation and eventually constant non-linearities, and it establishes that GNNs using ReLU are more expressive than those restricted to eventually constant activations and linear layers.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable Hybrid Deep Q-Learning Framework for IoT-Based Food Spoilage Prediction with Synthetic Data Generation and Hardware Validation</title>
<link>https://arxiv.org/abs/2512.19361</link>
<guid>https://arxiv.org/abs/2512.19361</guid>
<content:encoded><![CDATA[
arXiv:2512.19361v1 Announce Type: new 
Abstract: The need for an intelligent, real-time spoilage prediction system has become critical in modern IoT-driven food supply chains, where perishable goods are highly susceptible to environmental conditions. Existing methods often lack adaptability to dynamic conditions and fail to optimize decision making in real time. To address these challenges, we propose a hybrid reinforcement learning framework integrating Long Short-Term Memory (LSTM) and Recurrent Neural Networks (RNN) for enhanced spoilage prediction. This hybrid architecture captures temporal dependencies within sensor data, enabling robust and adaptive decision making. In alignment with interpretable artificial intelligence principles, a rule-based classifier environment is employed to provide transparent ground truth labeling of spoilage levels based on domain-specific thresholds. This structured design allows the agent to operate within clearly defined semantic boundaries, supporting traceable and interpretable decisions. Model behavior is monitored using interpretability-driven metrics, including spoilage accuracy, reward-to-step ratio, loss reduction rate, and exploration decay. These metrics provide both quantitative performance evaluation and insights into learning dynamics. A class-wise spoilage distribution visualization is used to analyze the agents decision profile and policy behavior. Extensive evaluations on simulated and real-time hardware data demonstrate that the LSTM and RNN based agent outperforms alternative reinforcement learning approaches in prediction accuracy and decision efficiency while maintaining interpretability. The results highlight the potential of hybrid deep reinforcement learning with integrated interpretability for scalable IoT-based food monitoring systems.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Points to Coalitions: Hierarchical Contrastive Shapley Values for Prioritizing Data Samples</title>
<link>https://arxiv.org/abs/2512.19363</link>
<guid>https://arxiv.org/abs/2512.19363</guid>
<content:encoded><![CDATA[
arXiv:2512.19363v1 Announce Type: new 
Abstract: How should we quantify the value of each training example when datasets are large, heterogeneous, and geometrically structured? Classical Data-Shapley answers in principle, but its O(n!) complexity and point-wise perspective are ill-suited to modern scales. We propose Hierarchical Contrastive Data Valuation (HCDV), a three-stage framework that (i) learns a contrastive, geometry-preserving representation, (ii) organizes the data into a balanced coarse-to-fine hierarchy of clusters, and (iii) assigns Shapley-style payoffs to coalitions via local Monte-Carlo games whose budgets are propagated downward. HCDV collapses the factorial burden to O(T sum_{l} K_{l}) = O(T K_max log n), rewards examples that sharpen decision boundaries, and regularizes outliers through curvature-based smoothness. We prove that HCDV approximately satisfies the four Shapley axioms with surplus loss O(eta log n), enjoys sub-Gaussian coalition deviation tilde O(1/sqrt{T}), and incurs at most k epsilon_infty regret for top-k selection. Experiments on four benchmarks--tabular, vision, streaming, and a 45M-sample CTR task--plus the OpenDataVal suite show that HCDV lifts accuracy by up to +5 pp, slashes valuation time by up to 100x, and directly supports tasks such as augmentation filtering, low-latency streaming updates, and fair marketplace payouts.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sprecher Networks: A Parameter-Efficient Kolmogorov-Arnold Architecture</title>
<link>https://arxiv.org/abs/2512.19367</link>
<guid>https://arxiv.org/abs/2512.19367</guid>
<content:encoded><![CDATA[
arXiv:2512.19367v1 Announce Type: new 
Abstract: We present Sprecher Networks (SNs), a family of trainable neural architectures inspired by the classical Kolmogorov-Arnold-Sprecher (KAS) construction for approximating multivariate continuous functions. Distinct from Multi-Layer Perceptrons (MLPs) with fixed node activations and Kolmogorov-Arnold Networks (KANs) featuring learnable edge activations, SNs utilize shared, learnable splines (monotonic and general) within structured blocks incorporating explicit shift parameters and mixing weights. Our approach directly realizes Sprecher's specific 1965 sum of shifted splines formula in its single-layer variant and extends it to deeper, multi-layer compositions. We further enhance the architecture with optional lateral mixing connections that enable intra-block communication between output dimensions, providing a parameter-efficient alternative to full attention mechanisms. Beyond parameter efficiency with $O(LN + LG)$ scaling (where $G$ is the knot count of the shared splines) versus MLPs' $O(LN^2)$, SNs admit a sequential evaluation strategy that reduces peak forward-intermediate memory from $O(N^2)$ to $O(N)$ (treating batch size as constant), making much wider architectures feasible under memory constraints. We demonstrate empirically that composing these blocks into deep networks leads to highly parameter and memory-efficient models, discuss theoretical motivations, and compare SNs with related architectures (MLPs, KANs, and networks with learnable node activations).
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniMER: Indonesian Multimodal Emotion Recognition via Auxiliary-Enhanced LLM Adaptation</title>
<link>https://arxiv.org/abs/2512.19379</link>
<guid>https://arxiv.org/abs/2512.19379</guid>
<content:encoded><![CDATA[
arXiv:2512.19379v1 Announce Type: new 
Abstract: Indonesian, spoken by over 200 million people, remains underserved in multimodal emotion recognition research despite its dominant presence on Southeast Asian social media platforms. We introduce IndoMER, the first multimodal emotion recognition benchmark for Indonesian, comprising 1,944 video segments from 203 speakers with temporally aligned text, audio, and visual annotations across seven emotion categories. The dataset exhibits realistic challenges including cross-modal inconsistency and long-tailed class distributions shaped by Indonesian cultural communication norms. To address these challenges, we propose OmniMER, a multimodal adaptation framework built upon Qwen2.5-Omni that enhances emotion recognition through three auxiliary modality-specific perception tasks: emotion keyword extraction for text, facial expression analysis for video, and prosody analysis for audio. These auxiliary tasks help the model identify emotion-relevant cues in each modality before fusion, reducing reliance on spurious correlations in low-resource settings. Experiments on IndoMER show that OmniMER achieves 0.582 Macro-F1 on sentiment classification and 0.454 on emotion recognition, outperforming the base model by 7.6 and 22.1 absolute points respectively. Cross-lingual evaluation on the Chinese CH-SIMS dataset further demonstrates the generalizability of the proposed framework. The dataset and code are publicly available. https://github.com/yanxm01/INDOMER
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real-Time Machine Learning for Embedded Anomaly Detection</title>
<link>https://arxiv.org/abs/2512.19383</link>
<guid>https://arxiv.org/abs/2512.19383</guid>
<content:encoded><![CDATA[
arXiv:2512.19383v1 Announce Type: new 
Abstract: The spread of a resource-constrained Internet of Things (IoT) environment and embedded devices has put pressure on the real-time detection of anomalies occurring at the edge. This survey presents an overview of machine-learning methods aimed specifically at on-device anomaly detection with extremely strict constraints for latency, memory, and power consumption. Lightweight algorithms such as Isolation Forest, One-Class SVM, recurrent architectures, and statistical techniques are compared here according to the realities of embedded implementation. Our survey brings out significant trade-offs of accuracy and computational efficiency of detection, as well as how hardware constraints end up fundamentally redefining algorithm choice. The survey is completed with a set of practical recommendations on the choice of the algorithm depending on the equipment profiles and new trends in TinyML, which can help close the gap between detection capabilities and embedded reality. The paper serves as a strategic roadmap for engineers deploying anomaly detection in edge environments that are constrained by bandwidth and may be safety-critical.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Brain-Grounded Axes for Reading and Steering LLM States</title>
<link>https://arxiv.org/abs/2512.19399</link>
<guid>https://arxiv.org/abs/2512.19399</guid>
<content:encoded><![CDATA[
arXiv:2512.19399v1 Announce Type: new 
Abstract: Interpretability methods for large language models (LLMs) typically derive directions from textual supervision, which can lack external grounding. We propose using human brain activity not as a training signal but as a coordinate system for reading and steering LLM states. Using the SMN4Lang MEG dataset, we construct a word-level brain atlas of phase-locking value (PLV) patterns and extract latent axes via ICA. We validate axes with independent lexica and NER-based labels (POS/log-frequency used as sanity checks), then train lightweight adapters that map LLM hidden states to these brain axes without fine-tuning the LLM. Steering along the resulting brain-derived directions yields a robust lexical (frequency-linked) axis in a mid TinyLlama layer, surviving perplexity-matched controls, and a brain-vs-text probe comparison shows larger log-frequency shifts (relative to the text probe) with lower perplexity for the brain axis. A function/content axis (axis 13) shows consistent steering in TinyLlama, Qwen2-0.5B, and GPT-2, with PPL-matched text-level corroboration. Layer-4 effects in TinyLlama are large but inconsistent, so we treat them as secondary (Appendix). Axis structure is stable when the atlas is rebuilt without GPT embedding-change features or with word2vec embeddings (|r|=0.64-0.95 across matched axes), reducing circularity concerns. Exploratory fMRI anchoring suggests potential alignment for embedding change and log frequency, but effects are sensitive to hemodynamic modeling assumptions and are treated as population-level evidence only. These results support a new interface: neurophysiology-grounded axes provide interpretable and controllable handles for LLM behavior.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Symplectic Reservoir Representation of Legendre Dynamics</title>
<link>https://arxiv.org/abs/2512.19409</link>
<guid>https://arxiv.org/abs/2512.19409</guid>
<content:encoded><![CDATA[
arXiv:2512.19409v1 Announce Type: new 
Abstract: Modern learning systems act on internal representations of data, yet how these representations encode underlying physical or statistical structure is often left implicit. In physics, conservation laws of Hamiltonian systems such as symplecticity guarantee long-term stability, and recent work has begun to hard-wire such constraints into learning models at the loss or output level. Here we ask a different question: what would it mean for the representation itself to obey a symplectic conservation law in the sense of Hamiltonian mechanics?
  We express this symplectic constraint through Legendre duality: the pairing between primal and dual parameters, which becomes the structure that the representation must preserve. We formalize Legendre dynamics as stochastic processes whose trajectories remain on Legendre graphs, so that the evolving primal-dual parameters stay Legendre dual. We show that this class includes linear time-invariant Gaussian process regression and Ornstein-Uhlenbeck dynamics.
  Geometrically, we prove that the maps that preserve all Legendre graphs are exactly symplectomorphisms of cotangent bundles of the form "cotangent lift of a base diffeomorphism followed by an exact fibre translation". Dynamically, this characterization leads to the design of a Symplectic Reservoir (SR), a reservoir-computing architecture that is a special case of recurrent neural network and whose recurrent core is generated by Hamiltonian systems that are at most linear in the momentum.
  Our main theorem shows that every SR update has this normal form and therefore transports Legendre graphs to Legendre graphs, preserving Legendre duality at each time step. Overall, SR implements a geometrically constrained, Legendre-preserving representation map, injecting symplectic geometry and Hamiltonian mechanics directly at the representational level.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Research Program: Theory of Learning in Dynamical Systems</title>
<link>https://arxiv.org/abs/2512.19410</link>
<guid>https://arxiv.org/abs/2512.19410</guid>
<content:encoded><![CDATA[
arXiv:2512.19410v1 Announce Type: new 
Abstract: Modern learning systems increasingly interact with data that evolve over time and depend on hidden internal state. We ask a basic question: when is such a dynamical system learnable from observations alone? This paper proposes a research program for understanding learnability in dynamical systems through the lens of next-token prediction. We argue that learnability in dynamical systems should be studied as a finite-sample question, and be based on the properties of the underlying dynamics rather than the statistical properties of the resulting sequence. To this end, we give a formulation of learnability for stochastic processes induced by dynamical systems, focusing on guarantees that hold uniformly at every time step after a finite burn-in period. This leads to a notion of dynamic learnability which captures how the structure of a system, such as stability, mixing, observability, and spectral properties, governs the number of observations required before reliable prediction becomes possible. We illustrate the framework in the case of linear dynamical systems, showing that accurate prediction can be achieved after finite observation without system identification, by leveraging improper methods based on spectral filtering. We survey the relationship between learning in dynamical systems and classical PAC, online, and universal prediction theories, and suggest directions for studying nonlinear and controlled systems.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attention Is Not What You Need</title>
<link>https://arxiv.org/abs/2512.19428</link>
<guid>https://arxiv.org/abs/2512.19428</guid>
<content:encoded><![CDATA[
arXiv:2512.19428v1 Announce Type: new 
Abstract: We revisit a basic question in sequence modeling: is explicit self-attention actually necessary for strong performance and reasoning? We argue that standard multi-head attention is best seen as a form of tensor lifting: hidden vectors are mapped into a high-dimensional space of pairwise interactions, and learning proceeds by constraining this lifted tensor through gradient descent. This mechanism is extremely expressive but mathematically opaque, because after many layers it becomes very hard to describe the model with a small family of explicit invariants.
  To explore an alternative, we propose an attention-free architecture based on Grassmann flows. Instead of forming an L by L attention matrix, our Causal Grassmann layer (i) linearly reduces token states, (ii) encodes local token pairs as two-dimensional subspaces on a Grassmann manifold via Plucker coordinates, and (iii) fuses these geometric features back into the hidden states through gated mixing. Information therefore propagates by controlled deformations of low-rank subspaces over multi-scale local windows, so the core computation lives on a finite-dimensional manifold rather than in an unstructured tensor space.
  On the Wikitext-2 language modeling benchmark, purely Grassmann-based models with 13 to 18 million parameters achieve validation perplexities within about 10 to 15 percent of size-matched Transformers. On the SNLI natural language inference task, a Grassmann-Plucker head on top of DistilBERT slightly outperforms a Transformer head, with best validation and test accuracies of 0.8550 and 0.8538 compared to 0.8545 and 0.8511. We analyze the complexity of Grassmann mixing, show linear scaling in sequence length for fixed rank, and argue that such manifold-based designs offer a more structured route toward geometric and invariant-based interpretations of neural reasoning.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Inverse Scattering Inspired Fourier Neural Operator for Time-Dependent PDE Learning</title>
<link>https://arxiv.org/abs/2512.19439</link>
<guid>https://arxiv.org/abs/2512.19439</guid>
<content:encoded><![CDATA[
arXiv:2512.19439v1 Announce Type: new 
Abstract: Learning accurate and stable time-advancement operators for nonlinear partial differential equations (PDEs) remains challenging, particularly for chaotic, stiff, and long-horizon dynamical systems. While neural operator methods such as the Fourier Neural Operator (FNO) and Koopman-inspired extensions achieve good short-term accuracy, their long-term stability is often limited by unconstrained latent representations and cumulative rollout errors. In this work, we introduce an inverse scattering inspired Fourier Neural Operator(IS-FNO), motivated by the reversibility and spectral evolution structure underlying the classical inverse scattering transform. The proposed architecture enforces a near-reversible pairing between lifting and projection maps through an explicitly invertible neural transformation, and models latent temporal evolution using exponential Fourier layers that naturally encode linear and nonlinear spectral dynamics. We systematically evaluate IS-FNO against baseline FNO and Koopman-based models on a range of benchmark PDEs, including the Michelson-Sivashinsky and Kuramoto-Sivashinsky equations (in one and two dimensions), as well as the integrable Korteweg-de Vries and Kadomtsev-Petviashvili equations. The results demonstrate that IS-FNO achieves lower short-term errors and substantially improved long-horizon stability in non-stiff regimes. For integrable systems, reduced IS-FNO variants that embed analytical scattering structure retain competitive long-term accuracy despite limited model capacity. Overall, this work shows that incorporating physical structure -- particularly reversibility and spectral evolution -- into neural operator design significantly enhances robustness and long-term predictive fidelity for nonlinear PDE dynamics.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Binary Kernel Logistic Regression: a sparsity-inducing formulation and a convergent decomposition training algorithm</title>
<link>https://arxiv.org/abs/2512.19440</link>
<guid>https://arxiv.org/abs/2512.19440</guid>
<content:encoded><![CDATA[
arXiv:2512.19440v1 Announce Type: new 
Abstract: Kernel logistic regression (KLR) is a widely used supervised learning method for binary and multi-class classification, which provides estimates of the conditional probabilities of class membership for the data points. Unlike other kernel methods such as Support Vector Machines (SVMs), KLRs are generally not sparse. Previous attempts to deal with sparsity in KLR include a heuristic method referred to as the Import Vector Machine (IVM) and ad hoc regularizations such as the $\ell_{1/2}$-based one. Achieving a good trade-off between prediction accuracy and sparsity is still a challenging issue with a potential significant impact from the application point of view. In this work, we revisit binary KLR and propose an extension of the training formulation proposed by Keerthi et al., which is able to induce sparsity in the trained model, while maintaining good testing accuracy. To efficiently solve the dual of this formulation, we devise a decomposition algorithm of Sequential Minimal Optimization type which exploits second-order information, and for which we establish global convergence. Numerical experiments conducted on 12 datasets from the literature show that the proposed binary KLR approach achieves a competitive trade-off between accuracy and sparsity with respect to IVM, $\ell_{1/2}$-based regularization for KLR, and SVM while retaining the advantages of providing informative estimates of the class membership probabilities.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Layer Confidence Scoring for Detection of Out-of-Distribution Samples, Adversarial Attacks, and In-Distribution Misclassifications</title>
<link>https://arxiv.org/abs/2512.19472</link>
<guid>https://arxiv.org/abs/2512.19472</guid>
<content:encoded><![CDATA[
arXiv:2512.19472v1 Announce Type: new 
Abstract: The recent explosive growth in Deep Neural Networks applications raises concerns about the black-box usage of such models, with limited trasparency and trustworthiness in high-stakes domains, which have been crystallized as regulatory requirements such as the European Union Artificial Intelligence Act. While models with embedded confidence metrics have been proposed, such approaches cannot be applied to already existing models without retraining, limiting their broad application. On the other hand, post-hoc methods, which evaluate pre-trained models, focus on solving problems related to improving the confidence in the model's predictions, and detecting Out-Of-Distribution or Adversarial Attacks samples as independent applications. To tackle the limited applicability of already existing methods, we introduce Multi-Layer Analysis for Confidence Scoring (MACS), a unified post-hoc framework that analyzes intermediate activations to produce classification-maps. From the classification-maps, we derive a score applicable for confidence estimation, detecting distributional shifts and adversarial attacks, unifying the three problems in a common framework, and achieving performances that surpass the state-of-the-art approaches in our experiments with the VGG16 and ViTb16 models with a fraction of their computational overhead.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lightweight Intrusion Detection in IoT via SHAP-Guided Feature Pruning and Knowledge-Distilled Kronecker Networks</title>
<link>https://arxiv.org/abs/2512.19488</link>
<guid>https://arxiv.org/abs/2512.19488</guid>
<content:encoded><![CDATA[
arXiv:2512.19488v1 Announce Type: new 
Abstract: The widespread deployment of Internet of Things (IoT) devices requires intrusion detection systems (IDS) with high accuracy while operating under strict resource constraints. Conventional deep learning IDS are often too large and computationally intensive for edge deployment. We propose a lightweight IDS that combines SHAP-guided feature pruning with knowledge-distilled Kronecker networks. A high-capacity teacher model identifies the most relevant features through SHAP explanations, and a compressed student leverages Kronecker-structured layers to minimize parameters while preserving discriminative inputs. Knowledge distillation transfers softened decision boundaries from teacher to student, improving generalization under compression. Experiments on the TON\_IoT dataset show that the student is nearly three orders of magnitude smaller than the teacher yet sustains macro-F1 above 0.986 with millisecond-level inference latency. The results demonstrate that explainability-driven pruning and structured compression can jointly enable scalable, low-latency, and energy-efficient IDS for heterogeneous IoT environments.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning from sanctioned government suppliers: A machine learning and network science approach to detecting fraud and corruption in Mexico</title>
<link>https://arxiv.org/abs/2512.19491</link>
<guid>https://arxiv.org/abs/2512.19491</guid>
<content:encoded><![CDATA[
arXiv:2512.19491v1 Announce Type: new 
Abstract: Detecting fraud and corruption in public procurement remains a major challenge for governments worldwide. Most research to-date builds on domain-knowledge-based corruption risk indicators of individual contract-level features and some also analyzes contracting network patterns. A critical barrier for supervised machine learning is the absence of confirmed non-corrupt, negative, examples, which makes conventional machine learning inappropriate for this task. Using publicly available data on federally funded procurement in Mexico and company sanction records, this study implements positive-unlabeled (PU) learning algorithms that integrate domain-knowledge-based red flags with network-derived features to identify likely corrupt and fraudulent contracts. The best-performing PU model on average captures 32 percent more known positives and performs on average 2.3 times better than random guessing, substantially outperforming approaches based solely on traditional red flags. The analysis of the Shapley Additive Explanations reveals that network-derived features, particularly those associated with contracts in the network core or suppliers with high eigenvector centrality, are the most important. Traditional red flags further enhance model performance in line with expectations, albeit mainly for contracts awarded through competitive tenders. This methodology can support law enforcement in Mexico, and it can be adapted to other national contexts too.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Kolmogorov-Arnold Graph Neural Networks Applied to Inorganic Nanomaterials Dataset</title>
<link>https://arxiv.org/abs/2512.19494</link>
<guid>https://arxiv.org/abs/2512.19494</guid>
<content:encoded><![CDATA[
arXiv:2512.19494v1 Announce Type: new 
Abstract: The recent development of Kolmogorov-Arnold Networks (KANs) introduced new discoveries in the field of Graph Neural Networks (GNNs), expanding the existing set of models with KAN-based versions of GNNs, which often surpass the accuracy of MultiLayer Perceptron (MLP)-based GNNs. These models were widely tested on the graph datasets consisting of organic molecules; however, those studies disregarded the inorganic nanomaterials datasets. In this work, we close this gap by applying Kolmogorov-Arnold Graph Neural Networks (KAGNNs) to a recently published large inorganic nanomaterials dataset called CHILI. For this, we adapt and test KAGNNs appropriate for this dataset. Our experiments reveal that on the CHILI datasets, particularly on the CHILI-3K, KAGNNs substantially surpass conventional GNNs in classification, achieving state-of-the-art results.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DK-STN: A Domain Knowledge Embedded Spatio-Temporal Network Model for MJO Forecast</title>
<link>https://arxiv.org/abs/2512.19506</link>
<guid>https://arxiv.org/abs/2512.19506</guid>
<content:encoded><![CDATA[
arXiv:2512.19506v1 Announce Type: new 
Abstract: Understanding and predicting the Madden-Julian Oscillation (MJO) is fundamental for precipitation forecasting and disaster prevention. To date, long-term and accurate MJO prediction has remained a challenge for researchers. Conventional MJO prediction methods using Numerical Weather Prediction (NWP) are resource-intensive, time-consuming, and highly unstable (most NWP methods are sensitive to seasons, with better MJO forecast results in winter). While existing Artificial Neural Network (ANN) methods save resources and speed forecasting, their accuracy never reaches the 28 days predicted by the state-of-the-art NWP method, i.e., the operational forecasts from ECMWF, since neural networks cannot handle climate data effectively. In this paper, we present a Domain Knowledge Embedded Spatio-Temporal Network (DK-STN), a stable neural network model for accurate and efficient MJO forecasting. It combines the benefits of NWP and ANN methods and successfully improves the forecast accuracy of ANN methods while maintaining a high level of efficiency and stability. We begin with a spatial-temporal network (STN) and embed domain knowledge in it using two key methods: (i) applying a domain knowledge enhancement method and (ii) integrating a domain knowledge processing method into network training. We evaluated DK-STN with the 5th generation of ECMWF reanalysis (ERA5) data and compared it with ECMWF. Given 7 days of climate data as input, DK-STN can generate reliable forecasts for the following 28 days in 1-2 seconds, with an error of only 2-3 days in different seasons. DK-STN significantly exceeds ECMWF in that its forecast accuracy is equivalent to ECMWF's, while its efficiency and stability are significantly superior.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Scalable and Valid Conditional Independence Testing with Spectral Representations</title>
<link>https://arxiv.org/abs/2512.19510</link>
<guid>https://arxiv.org/abs/2512.19510</guid>
<content:encoded><![CDATA[
arXiv:2512.19510v1 Announce Type: new 
Abstract: Conditional independence (CI) is central to causal inference, feature selection, and graphical modeling, yet it is untestable in many settings without additional assumptions. Existing CI tests often rely on restrictive structural conditions, limiting their validity on real-world data. Kernel methods using the partial covariance operator offer a more principled approach but suffer from limited adaptivity, slow convergence, and poor scalability. In this work, we explore whether representation learning can help address these limitations. Specifically, we focus on representations derived from the singular value decomposition of the partial covariance operator and use them to construct a simple test statistic, reminiscent of the Hilbert-Schmidt Independence Criterion (HSIC). We also introduce a practical bi-level contrastive algorithm to learn these representations. Our theory links representation learning error to test performance and establishes asymptotic validity and power guarantees. Preliminary experiments suggest that this approach offers a practical and statistically grounded path toward scalable CI testing, bridging kernel-based theory with modern representation learning.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LacaDM: A Latent Causal Diffusion Model for Multiobjective Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.19516</link>
<guid>https://arxiv.org/abs/2512.19516</guid>
<content:encoded><![CDATA[
arXiv:2512.19516v1 Announce Type: new 
Abstract: Multiobjective reinforcement learning (MORL) poses significant challenges due to the inherent conflicts between objectives and the difficulty of adapting to dynamic environments. Traditional methods often struggle to generalize effectively, particularly in large and complex state-action spaces. To address these limitations, we introduce the Latent Causal Diffusion Model (LacaDM), a novel approach designed to enhance the adaptability of MORL in discrete and continuous environments. Unlike existing methods that primarily address conflicts between objectives, LacaDM learns latent temporal causal relationships between environmental states and policies, enabling efficient knowledge transfer across diverse MORL scenarios. By embedding these causal structures within a diffusion model-based framework, LacaDM achieves a balance between conflicting objectives while maintaining strong generalization capabilities in previously unseen environments. Empirical evaluations on various tasks from the MOGymnasium framework demonstrate that LacaDM consistently outperforms the state-of-art baselines in terms of hypervolume, sparsity, and expected utility maximization, showcasing its effectiveness in complex multiobjective tasks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Initialization of a Polyharmonic Cascade, Launch and Testing</title>
<link>https://arxiv.org/abs/2512.19524</link>
<guid>https://arxiv.org/abs/2512.19524</guid>
<content:encoded><![CDATA[
arXiv:2512.19524v1 Announce Type: new 
Abstract: This paper concludes a series of studies on the polyharmonic cascade, a deep machine learning architecture theoretically derived from indifference principles and the theory of random functions. A universal initialization procedure is proposed, based on symmetric constellations in the form of hyperoctahedra with a central point. This initialization not only ensures stable training of cascades with tens and hundreds of layers (up to 500 layers without skip connections), but also radically simplifies the computations. Scalability and robustness are demonstrated on MNIST (98.3% without convolutions or augmentations), HIGGS (AUC approximately 0.885 on 11M examples), and Epsilon (AUC approximately 0.963 with 2000 features). All linear algebra is reduced to 2D operations and is efficiently executed on GPUs. A public repository and an archived snapshot are provided for full reproducibility.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning for Unrelated-Machines Scheduling: Handling Variable Dimensions</title>
<link>https://arxiv.org/abs/2512.19527</link>
<guid>https://arxiv.org/abs/2512.19527</guid>
<content:encoded><![CDATA[
arXiv:2512.19527v1 Announce Type: new 
Abstract: Deep learning has been effectively applied to many discrete optimization problems. However, learning-based scheduling on unrelated parallel machines remains particularly difficult to design. Not only do the numbers of jobs and machines vary, but each job-machine pair has a unique processing time, dynamically altering feature dimensions. We propose a novel approach with a neural network tailored for offline deterministic scheduling of arbitrary sizes on unrelated machines. The goal is to minimize a complex objective function that includes the makespan and the weighted tardiness of jobs and machines. Unlike existing online approaches, which process jobs sequentially, our method generates a complete schedule considering the entire input at once. The key contribution of this work lies in the sophisticated architecture of our model. By leveraging various NLP-inspired architectures, it effectively processes any number of jobs and machines with varying feature dimensions imposed by unrelated processing times. Our approach enables supervised training on small problem instances while demonstrating strong generalization to much larger scheduling environments. Trained and tested on instances with 8 jobs and 4 machines, costs were only 2.51% above optimal. Across all tested configurations of up to 100 jobs and 10 machines, our network consistently outperformed an advanced dispatching rule, which incurred 22.22% higher costs on average. As our method allows fast retraining with simulated data and adaptation to various scheduling conditions, we believe it has the potential to become a standard approach for learning-based scheduling on unrelated machines and similar problem environments.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement</title>
<link>https://arxiv.org/abs/2512.19530</link>
<guid>https://arxiv.org/abs/2512.19530</guid>
<content:encoded><![CDATA[
arXiv:2512.19530v1 Announce Type: new 
Abstract: Predicting reaction outcomes across continuous solvent composition ranges remains a critical challenge in organic synthesis and process chemistry. Traditional machine learning approaches often treat solvent identity as a discrete categorical variable, which prevents systematic interpolation and extrapolation across the solvent space. This work introduces the \textbf{Catechol Benchmark}, a high-throughput transient flow chemistry dataset comprising 1,227 experimental yield measurements for the rearrangement of allyl-substituted catechol in 24 pure solvents and their binary mixtures, parameterized by continuous volume fractions ($\% B$). We evaluate various architectures under rigorous leave-one-solvent-out and leave-one-mixture-out protocols to test generalization to unseen chemical environments.
  Our results demonstrate that classical tabular methods (e.g., Gradient-Boosted Decision Trees) and large language model embeddings (e.g., Qwen-7B) struggle with quantitative precision, yielding Mean Squared Errors (MSE) of 0.099 and 0.129, respectively. In contrast, we propose a hybrid GNN-based architecture that integrates Graph Attention Networks (GATs) with Differential Reaction Fingerprints (DRFP) and learned mixture-aware solvent encodings. This approach achieves an \textbf{MSE of 0.0039} ($\pm$ 0.0003), representing a 60\% error reduction over competitive baselines and a $>25\times$ improvement over tabular ensembles. Ablation studies confirm that explicit molecular graph message-passing and continuous mixture encoding are essential for robust generalization. The complete dataset, evaluation protocols, and reference implementations are released to facilitate data-efficient reaction prediction and continuous solvent representation learning.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DFORD: Directional Feedback based Online Ordinal Regression Learning</title>
<link>https://arxiv.org/abs/2512.19550</link>
<guid>https://arxiv.org/abs/2512.19550</guid>
<content:encoded><![CDATA[
arXiv:2512.19550v1 Announce Type: new 
Abstract: In this paper, we introduce directional feedback in the ordinal regression setting, in which the learner receives feedback on whether the predicted label is on the left or the right side of the actual label. This is a weak supervision setting for ordinal regression compared to the full information setting, where the learner can access the labels. We propose an online algorithm for ordinal regression using directional feedback. The proposed algorithm uses an exploration-exploitation scheme to learn from directional feedback efficiently. Furthermore, we introduce its kernel-based variant to learn non-linear ordinal regression models in an online setting. We use a truncation trick to make the kernel implementation more memory efficient. The proposed algorithm maintains the ordering of the thresholds in the expected sense. Moreover, it achieves the expected regret of $\mathcal{O}(\log T)$. We compare our approach with a full information and a weakly supervised algorithm for ordinal regression on synthetic and real-world datasets. The proposed approach, which learns using directional feedback, performs comparably (sometimes better) to its full information counterpart.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CARE What Fails: Contrastive Anchored-REflection for Verifiable Multimodal</title>
<link>https://arxiv.org/abs/2512.19554</link>
<guid>https://arxiv.org/abs/2512.19554</guid>
<content:encoded><![CDATA[
arXiv:2512.19554v1 Announce Type: new 
Abstract: Group-relative reinforcement learning with verifiable rewards (RLVR) often wastes the most informative data it already has the failures. When all rollouts are wrong, gradients stall; when one happens to be correct, the update usually ignores why the others are close-but-wrong, and credit can be misassigned to spurious chains. We present CARE (Contrastive Anchored REflection), a failure-centric post-training framework for multimodal reasoning that turns errors into supervision. CARE combines: (i) an anchored-contrastive objective that forms a compact subgroup around the best rollout and a set of semantically proximate hard negatives, performs within-subgroup z-score normalization with negative-only scaling, and includes an all-negative rescue to prevent zero-signal batches; and (ii) Reflection-Guided Resampling (RGR), a one-shot structured self-repair that rewrites a representative failure and re-scores it with the same verifier, converting near-misses into usable positives without any test-time reflection. CARE improves accuracy and training smoothness while explicitly increasing the share of learning signal that comes from failures. On Qwen2.5-VL-7B, CARE lifts macro-averaged accuracy by 4.6 points over GRPO across six verifiable visual-reasoning benchmarks; with Qwen3-VL-8B it reaches competitive or state-of-the-art results on MathVista and MMMU-Pro under an identical evaluation protocol.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KerJEPA: Kernel Discrepancies for Euclidean Self-Supervised Learning</title>
<link>https://arxiv.org/abs/2512.19605</link>
<guid>https://arxiv.org/abs/2512.19605</guid>
<content:encoded><![CDATA[
arXiv:2512.19605v1 Announce Type: new 
Abstract: Recent breakthroughs in self-supervised Joint-Embedding Predictive Architectures (JEPAs) have established that regularizing Euclidean representations toward isotropic Gaussian priors yields provable gains in training stability and downstream generalization. We introduce a new, flexible family of KerJEPAs, self-supervised learning algorithms with kernel-based regularizers. One instance of this family corresponds to the recently-introduced LeJEPA Epps-Pulley regularizer which approximates a sliced maximum mean discrepancy (MMD) with a Gaussian prior and Gaussian kernel. By expanding the class of viable kernels and priors and computing the closed-form high-dimensional limit of sliced MMDs, we develop alternative KerJEPAs with a number of favorable properties including improved training stability and design flexibility.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Best of Both Worlds: Hybridizing Neural Operators and Solvers for Stable Long-Horizon Inference</title>
<link>https://arxiv.org/abs/2512.19643</link>
<guid>https://arxiv.org/abs/2512.19643</guid>
<content:encoded><![CDATA[
arXiv:2512.19643v1 Announce Type: new 
Abstract: Numerical simulation of time-dependent partial differential equations (PDEs) is central to scientific and engineering applications, but high-fidelity solvers are often prohibitively expensive for long-horizon or time-critical settings. Neural operator (NO) surrogates offer fast inference across parametric and functional inputs; however, most autoregressive NO frameworks remain vulnerable to compounding errors, and ensemble-averaged metrics provide limited guarantees for individual inference trajectories. In practice, error accumulation can become unacceptable beyond the training horizon, and existing methods lack mechanisms for online monitoring or correction. To address this gap, we propose ANCHOR (Adaptive Numerical Correction for High-fidelity Operator Rollouts), an online, instance-aware hybrid inference framework for stable long-horizon prediction of nonlinear, time-dependent PDEs. ANCHOR treats a pretrained NO as the primary inference engine and adaptively couples it with a classical numerical solver using a physics-informed, residual-based error estimator. Inspired by adaptive time-stepping in numerical analysis, ANCHOR monitors an exponential moving average (EMA) of the normalized PDE residual to detect accumulating error and trigger corrective solver interventions without requiring access to ground-truth solutions. We show that the EMA-based estimator correlates strongly with the true relative L2 error, enabling data-free, instance-aware error control during inference. Evaluations on four canonical PDEs: 1D and 2D Burgers', 2D Allen-Cahn, and 3D heat conduction, demonstrate that ANCHOR reliably bounds long-horizon error growth, stabilizes extrapolative rollouts, and significantly improves robustness over standalone neural operators, while remaining substantially more efficient than high-fidelity numerical solvers.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Legendre Transform</title>
<link>https://arxiv.org/abs/2512.19649</link>
<guid>https://arxiv.org/abs/2512.19649</guid>
<content:encoded><![CDATA[
arXiv:2512.19649v1 Announce Type: new 
Abstract: We introduce a novel deep learning algorithm for computing convex conjugates of differentiable convex functions, a fundamental operation in convex analysis with various applications in different fields such as optimization, control theory, physics and economics. While traditional numerical methods suffer from the curse of dimensionality and become computationally intractable in high dimensions, more recent neural network-based approaches scale better, but have mostly been studied with the aim of solving optimal transport problems and require the solution of complicated optimization or max-min problems. Using an implicit Fenchel formulation of convex conjugation, our approach facilitates an efficient gradient-based framework for the minimization of approximation errors and, as a byproduct, also provides a posteriori error estimates for the approximation quality. Numerical experiments demonstrate our method's ability to deliver accurate results across different high-dimensional examples. Moreover, by employing symbolic regression with Kolmogorov--Arnold networks, it is able to obtain the exact convex conjugates of specific convex functions.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies</title>
<link>https://arxiv.org/abs/2512.19673</link>
<guid>https://arxiv.org/abs/2512.19673</guid>
<content:encoded><![CDATA[
arXiv:2512.19673v1 Announce Type: new 
Abstract: Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Multi-Adapter LLM Serving via Cross-Model KV-Cache Reuse with Activated LoRA</title>
<link>https://arxiv.org/abs/2512.17910</link>
<guid>https://arxiv.org/abs/2512.17910</guid>
<content:encoded><![CDATA[
arXiv:2512.17910v1 Announce Type: cross 
Abstract: Modern large language model (LLM) systems increasingly rely on multi-turn pipelines that are composed of multiple task-specific adapters, yet existing serving frameworks remain inefficient, incurring substantial recomputation overhead when switching between adapters. We present the first LLM serving engine that supports cross-model prefix cache reuse between base and adapted models via Activated LoRA (aLoRA), enabling efficient and fine-grained adapter switching during inference. Our design extends the vLLM framework by introducing base-aligned block hashing and activation-aware masking within the model execution path, permitting cache reuse across models while preserving compatibility with existing serving engine optimizations. Integrated into a production-grade inference stack, this approach supports dynamic adapter activation without excessive key-value tensor recomputation. Evaluation across representative multi-turn, multi-adapter pipelines demonstrates up to 58x end-to-end latency reduction and over 100x time-to-first-token improvement relative to standard LoRA baselines, with benefits that scale with model size and sequence length and manifest across all stages of the request lifecycle. This work bridges parameter-efficient model adaptation with high-performance serving, providing the first complete realization of cross-model KV-cache reuse in modern LLM inference engines.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Reasoning-Preserving Unlearning in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2512.17911</link>
<guid>https://arxiv.org/abs/2512.17911</guid>
<content:encoded><![CDATA[
arXiv:2512.17911v1 Announce Type: cross 
Abstract: Machine unlearning aims to erase requested data from trained models without full retraining. For Reasoning Multimodal Large Language Models (RMLLMs), this is uniquely challenging: intermediate chain-of-thought steps can still leak sensitive information even when final answers are forgotten, and overly aggressive interventions easily damage general reasoning ability. Yet no benchmark jointly evaluates how well unlearning methods suppress reasoning-level leakage while preserving reasoning competence. We address this gap with RMLLMU-Bench, the first benchmark for RMLLM unlearning that extends standard forgetting metrics with dedicated measures of reasoning leakage and reasoning retention. A systematic evaluation on RMLLMU-Bench reveals that existing unlearning methods for MLLMs and Large (Language) Reasoning Models (LRMs) either leave substantial leakage in the reasoning process or severely degrade reasoning performance. To address these gaps, we propose R-MUSE (Reasoning-preserving MLLM Unlearning via Subspace guidance and Adaptive Steering), a training-free and inference-time intervention framework that steers internal representations to forget both answers and reasoning traces while explicitly preserving general reasoning. Experiments on RMLLMU-Bench demonstrate that R-MUSE achieves a substantially better balance between effective forgetting and reasoning retention.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Supplementary Resources and Analysis for Automatic Speech Recognition Systems Trained on the Loquacious Dataset</title>
<link>https://arxiv.org/abs/2512.17915</link>
<guid>https://arxiv.org/abs/2512.17915</guid>
<content:encoded><![CDATA[
arXiv:2512.17915v1 Announce Type: cross 
Abstract: The recently published Loquacious dataset aims to be a replacement for established English automatic speech recognition (ASR) datasets such as LibriSpeech or TED-Lium. The main goal of the Loquacious dataset is to provide properly defined training and test partitions across many acoustic and language domains, with an open license suitable for both academia and industry. To further promote the benchmarking and usability of this new dataset, we present additional resources in the form of n-gram language models (LMs), a grapheme-to-phoneme (G2P) model and pronunciation lexica, with open and public access. Utilizing those additional resources we show experimental results across a wide range of ASR architectures with different label units and topologies. Our initial experimental results indicate that the Loquacious dataset offers a valuable study case for a variety of common challenges in ASR.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QAISim: A Toolkit for Modeling and Simulation of AI in Quantum Cloud Computing Environments</title>
<link>https://arxiv.org/abs/2512.17918</link>
<guid>https://arxiv.org/abs/2512.17918</guid>
<content:encoded><![CDATA[
arXiv:2512.17918v1 Announce Type: cross 
Abstract: Quantum computing offers new ways to explore the theory of computation via the laws of quantum mechanics. Due to the rising demand for quantum computing resources, there is growing interest in developing cloud-based quantum resource sharing platforms that enable researchers to test and execute their algorithms on real quantum hardware. These cloud-based systems face a fundamental challenge in efficiently allocating quantum hardware resources to fulfill the growing computational demand of modern Internet of Things (IoT) applications. So far, attempts have been made in order to make efficient resource allocation, ranging from heuristic-based solutions to machine learning. In this work, we employ quantum reinforcement learning based on parameterized quantum circuits to address the resource allocation problem to support large IoT networks. We propose a python-based toolkit called QAISim for the simulation and modeling of Quantum Artificial Intelligence (QAI) models for designing resource management policies in quantum cloud environments. We have simulated policy gradient and Deep Q-Learning algorithms for reinforcement learning. QAISim exhibits a substantial reduction in model complexity compared to its classical counterparts with fewer trainable variables.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inferring Latent Market Forces: Evaluating LLM Detection of Gamma Exposure Patterns via Obfuscation Testing</title>
<link>https://arxiv.org/abs/2512.17923</link>
<guid>https://arxiv.org/abs/2512.17923</guid>
<content:encoded><![CDATA[
arXiv:2512.17923v1 Announce Type: cross 
Abstract: We introduce obfuscation testing, a novel methodology for validating whether large language models detect structural market patterns through causal reasoning rather than temporal association. Testing three dealer hedging constraint patterns (gamma positioning, stock pinning, 0DTE hedging) on 242 trading days (95.6% coverage) of S&amp;P 500 options data, we find LLMs achieve 71.5% detection rate using unbiased prompts that provide only raw gamma exposure values without regime labels or temporal context. The WHO-WHOM-WHAT causal framework forces models to identify the economic actors (dealers), affected parties (directional traders), and structural mechanisms (forced hedging) underlying observed market dynamics. Critically, detection accuracy (91.2%) remains stable even as economic profitability varies quarterly, demonstrating that models identify structural constraints rather than profitable patterns. When prompted with regime labels, detection increases to 100%, but the 71.5% unbiased rate validates genuine pattern recognition. Our findings suggest LLMs possess emergent capabilities for detecting complex financial mechanisms through pure structural reasoning, with implications for systematic strategy development, risk management, and our understanding of how transformer architectures process financial market dynamics.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A curated UK rain radar data set for training and benchmarking nowcasting models</title>
<link>https://arxiv.org/abs/2512.17924</link>
<guid>https://arxiv.org/abs/2512.17924</guid>
<content:encoded><![CDATA[
arXiv:2512.17924v1 Announce Type: cross 
Abstract: This paper documents a data set of UK rain radar image sequences for use in statistical modeling and machine learning methods for nowcasting. The main dataset contains 1,000 randomly sampled sequences of length 20 steps (15-minute increments) of 2D radar intensity fields of dimension 40x40 (at 5km spatial resolution). Spatially stratified sampling ensures spatial homogeneity despite removal of clear-sky cases by threshold-based truncation. For each radar sequence, additional atmospheric and geographic features are made available, including date, location, mean elevation, mean wind direction and speed and prevailing storm type. New R functions to extract data from the binary "Nimrod" radar data format are provided. A case study is presented to train and evaluate a simple convolutional neural network for radar nowcasting, including self-contained R code.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Beamforming Optimization for STAR-RIS-Assisted Communications: A Gradient-Based Meta Learning Approach</title>
<link>https://arxiv.org/abs/2512.17928</link>
<guid>https://arxiv.org/abs/2512.17928</guid>
<content:encoded><![CDATA[
arXiv:2512.17928v1 Announce Type: cross 
Abstract: Simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) has emerged as a promising technology to realize full-space coverage and boost spectral efficiency in next-generation wireless networks. Yet, the joint design of the base station precoding matrix as well as the STAR-RIS transmission and reflection coefficient matrices leads to a high-dimensional, strongly nonconvex, and NP-hard optimization problem. Conventional alternating optimization (AO) schemes typically involve repeated large-scale matrix inversion operations, resulting in high computational complexity and poor scalability, while existing deep learning approaches often rely on expensive pre-training and large network models. In this paper, we develop a gradient-based meta learning (GML) framework that directly feeds optimization gradients into lightweight neural networks, thereby removing the need for pre-training and enabling fast adaptation. Specifically, we design dedicated GML-based schemes for both independent-phase and coupled-phase STAR-RIS models, effectively handling their respective amplitude and phase constraints while achieving weighted sum-rate performance very close to that of AO-based benchmarks. Extensive simulations demonstrate that, for both phase models, the proposed methods substantially reduce computational overhead, with complexity growing nearly linearly when the number of BS antennas and STAR-RIS elements grows, and yielding up to 10 times runtime speedup over AO, which confirms the scalability and practicality of the proposed GML method for large-scale STAR-RIS-assisted communications.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reinforcement Learning for Monetary Policy Under Macroeconomic Uncertainty: Analyzing Tabular and Function Approximation Methods</title>
<link>https://arxiv.org/abs/2512.17929</link>
<guid>https://arxiv.org/abs/2512.17929</guid>
<content:encoded><![CDATA[
arXiv:2512.17929v1 Announce Type: cross 
Abstract: We study how a central bank should dynamically set short-term nominal interest rates to stabilize inflation and unemployment when macroeconomic relationships are uncertain and time-varying. We model monetary policy as a sequential decision-making problem where the central bank observes macroeconomic conditions quarterly and chooses interest rate adjustments. Using publically accessible historical Federal Reserve Economic Data (FRED), we construct a linear-Gaussian transition model and implement a discrete-action Markov Decision Process with a quadratic loss reward function. We chose to compare nine different reinforcement learning style approaches against Taylor Rule and naive baselines, including tabular Q-learning variants, SARSA, Actor-Critic, Deep Q-Networks, Bayesian Q-learning with uncertainty quantification, and POMDP formulations with partial observability. Surprisingly, standard tabular Q-learning achieved the best performance (-615.13 +- 309.58 mean return), outperforming both enhanced RL methods and traditional policy rules. Our results suggest that while sophisticated RL techniques show promise for monetary policy applications, simpler approaches may be more robust in this domain, highlighting important challenges in applying modern RL to macroeconomic policy.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>chatter: a Python library for applying information theory and AI/ML models to animal communication</title>
<link>https://arxiv.org/abs/2512.17935</link>
<guid>https://arxiv.org/abs/2512.17935</guid>
<content:encoded><![CDATA[
arXiv:2512.17935v1 Announce Type: cross 
Abstract: The study of animal communication often involves categorizing units into types (e.g. syllables in songbirds, or notes in humpback whales). While this approach is useful in many cases, it necessarily flattens the complexity and nuance present in real communication systems. chatter is a new Python library for analyzing animal communication in continuous latent space using information theory and modern machine learning techniques. It is taxonomically agnostic, and has been tested with the vocalizations of birds, bats, whales, and primates. By leveraging a variety of different architectures, including variational autoencoders and vision transformers, chatter represents vocal sequences as trajectories in high-dimensional latent space, bypassing the need for manual or automatic categorization of units. The library provides an end-to-end workflow -- from preprocessing and segmentation to model training and feature extraction -- that enables researchers to quantify the complexity, predictability, similarity, and novelty of vocal sequences.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Risk-Aware Financial Forecasting Enhanced by Machine Learning and Intuitionistic Fuzzy Multi-Criteria Decision-Making</title>
<link>https://arxiv.org/abs/2512.17936</link>
<guid>https://arxiv.org/abs/2512.17936</guid>
<content:encoded><![CDATA[
arXiv:2512.17936v1 Announce Type: cross 
Abstract: In the face of increasing financial uncertainty and market complexity, this study presents a novel risk-aware financial forecasting framework that integrates advanced machine learning techniques with intuitionistic fuzzy multi-criteria decision-making (MCDM). Tailored to the BIST 100 index and validated through a case study of a major defense company in T\"urkiye, the framework fuses structured financial data, unstructured text data, and macroeconomic indicators to enhance predictive accuracy and robustness. It incorporates a hybrid suite of models, including extreme gradient boosting (XGBoost), long short-term memory (LSTM) network, graph neural network (GNN), to deliver probabilistic forecasts with quantified uncertainty. The empirical results demonstrate high forecasting accuracy, with a net profit mean absolute percentage error (MAPE) of 3.03% and narrow 95% confidence intervals for key financial indicators. The risk-aware analysis indicates a favorable risk-return profile, with a Sharpe ratio of 1.25 and a higher Sortino ratio of 1.80, suggesting relatively low downside volatility and robust performance under market fluctuations. Sensitivity analysis shows that the key financial indicator predictions are highly sensitive to variations of inflation, interest rates, sentiment, and exchange rates. Additionally, using an intuitionistic fuzzy MCDM approach, combining entropy weighting, evaluation based on distance from the average solution (EDAS), and the measurement of alternatives and ranking according to compromise solution (MARCOS) methods, the tabular data learning network (TabNet) outperforms the other models and is identified as the most suitable candidate for deployment. Overall, the findings of this work highlight the importance of integrating advanced machine learning, risk quantification, and fuzzy MCDM methodologies in financial forecasting, particularly in emerging markets.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCS-SupCon: Sigmoid-based Common and Style Supervised Contrastive Learning with Adaptive Decision Boundaries</title>
<link>https://arxiv.org/abs/2512.17954</link>
<guid>https://arxiv.org/abs/2512.17954</guid>
<content:encoded><![CDATA[
arXiv:2512.17954v1 Announce Type: cross 
Abstract: Image classification is hindered by subtle inter-class differences and substantial intra-class variations, which limit the effectiveness of existing contrastive learning methods. Supervised contrastive approaches based on the InfoNCE loss suffer from negative-sample dilution and lack adaptive decision boundaries, thereby reducing discriminative power in fine-grained recognition tasks. To address these limitations, we propose Sigmoid-based Common and Style Supervised Contrastive Learning (SCS-SupCon). Our framework introduces a sigmoid-based pairwise contrastive loss with learnable temperature and bias parameters to enable adaptive decision boundaries. This formulation emphasizes hard negatives, mitigates negative-sample dilution, and more effectively exploits supervision. In addition, an explicit style-distance constraint further disentangles style and content representations, leading to more robust feature learning. Comprehensive experiments on six benchmark datasets, including CUB200-2011 and Stanford Dogs, demonstrate that SCS-SupCon achieves state-of-the-art performance across both CNN and Transformer backbones. On CIFAR-100 with ResNet-50, SCS-SupCon improves top-1 accuracy over SupCon by approximately 3.9 percentage points and over CS-SupCon by approximately 1.7 points under five-fold cross-validation. On fine-grained datasets, it outperforms CS-SupCon by 0.4--3.0 points. Extensive ablation studies and statistical analyses further confirm the robustness and generalization of the proposed framework, with Friedman tests and Nemenyi post-hoc evaluations validating the stability of the observed improvements.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sampling from multimodal distributions with warm starts: Non-asymptotic bounds for the Reweighted Annealed Leap-Point Sampler</title>
<link>https://arxiv.org/abs/2512.17977</link>
<guid>https://arxiv.org/abs/2512.17977</guid>
<content:encoded><![CDATA[
arXiv:2512.17977v1 Announce Type: cross 
Abstract: Sampling from multimodal distributions is a central challenge in Bayesian inference and machine learning. In light of hardness results for sampling -- classical MCMC methods, even with tempering, can suffer from exponential mixing times -- a natural question is how to leverage additional information, such as a warm start point for each mode, to enable faster mixing across modes. To address this, we introduce Reweighted ALPS (Re-ALPS), a modified version of the Annealed Leap-Point Sampler (ALPS) that dispenses with the Gaussian approximation assumption. We prove the first polynomial-time bound that works in a general setting, under a natural assumption that each component contains significant mass relative to the others when tilted towards the corresponding warm start point. Similarly to ALPS, we define distributions tilted towards a mixture centered at the warm start points, and at the coldest level, use teleportation between warm start points to enable efficient mixing across modes. In contrast to ALPS, our method does not require Hessian information at the modes, but instead estimates component partition functions via Monte Carlo. This additional estimation step is crucial in allowing the algorithm to handle target distributions with more complex geometries besides approximate Gaussian. For the proof, we show convergence results for Markov processes when only part of the stationary distribution is well-mixing and estimation for partition functions for individual components of a mixture. We numerically evaluate our algorithm's mixing performance compared to ALPS on a mixture of heavy-tailed distributions.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MEGState: Phoneme Decoding from Magnetoencephalography Signals</title>
<link>https://arxiv.org/abs/2512.17978</link>
<guid>https://arxiv.org/abs/2512.17978</guid>
<content:encoded><![CDATA[
arXiv:2512.17978v1 Announce Type: cross 
Abstract: Decoding linguistically meaningful representations from non-invasive neural recordings remains a central challenge in neural speech decoding. Among available neuroimaging modalities, magnetoencephalography (MEG) provides a safe and repeatable means of mapping speech-related cortical dynamics, yet its low signal-to-noise ratio and high temporal dimensionality continue to hinder robust decoding. In this work, we introduce MEGState, a novel architecture for phoneme decoding from MEG signals that captures fine-grained cortical responses evoked by auditory stimuli. Extensive experiments on the LibriBrain dataset demonstrate that MEGState consistently surpasses baseline model across multiple evaluation metrics. These findings highlight the potential of MEG-based phoneme decoding as a scalable pathway toward non-invasive brain-computer interfaces for speech.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Tea Leaf Disease Recognition with Attention Mechanisms and Grad-CAM Visualization</title>
<link>https://arxiv.org/abs/2512.17987</link>
<guid>https://arxiv.org/abs/2512.17987</guid>
<content:encoded><![CDATA[
arXiv:2512.17987v1 Announce Type: cross 
Abstract: Tea is among the most widely consumed drinks globally. Tea production is a key industry for many countries. One of the main challenges in tea harvesting is tea leaf diseases. If the spread of tea leaf diseases is not stopped in time, it can lead to massive economic losses for farmers. Therefore, it is crucial to identify tea leaf diseases as soon as possible. Manually identifying tea leaf disease is an ineffective and time-consuming method, without any guarantee of success. Automating this process will improve both the efficiency and the success rate of identifying tea leaf diseases. The purpose of this study is to create an automated system that can classify different kinds of tea leaf diseases, allowing farmers to take action to minimize the damage. A novel dataset was developed specifically for this study. The dataset contains 5278 images across seven classes. The dataset was pre-processed prior to training the model. We deployed three pretrained models: DenseNet, Inception, and EfficientNet. EfficientNet was used only in the ensemble model. We utilized two different attention modules to improve model performance. The ensemble model achieved the highest accuracy of 85.68%. Explainable AI was introduced for better model interpretability.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing Justice Clearly: Handwritten Legal Document Translation with OCR and Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.18004</link>
<guid>https://arxiv.org/abs/2512.18004</guid>
<content:encoded><![CDATA[
arXiv:2512.18004v1 Announce Type: cross 
Abstract: Handwritten text recognition (HTR) and machine translation continue to pose significant challenges, particularly for low-resource languages like Marathi, which lack large digitized corpora and exhibit high variability in handwriting styles. The conventional approach to address this involves a two-stage pipeline: an OCR system extracts text from handwritten images, which is then translated into the target language using a machine translation model. In this work, we explore and compare the performance of traditional OCR-MT pipelines with Vision Large Language Models that aim to unify these stages and directly translate handwritten text images in a single, end-to-end step. Our motivation is grounded in the urgent need for scalable, accurate translation systems to digitize legal records such as FIRs, charge sheets, and witness statements in India's district and high courts. We evaluate both approaches on a curated dataset of handwritten Marathi legal documents, with the goal of enabling efficient legal document processing, even in low-resource environments. Our findings offer actionable insights toward building robust, edge-deployable solutions that enhance access to legal information for non-native speakers and legal professionals alike.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReGal: A First Look at PPO-based Legal AI for Judgment Prediction and Summarization in India</title>
<link>https://arxiv.org/abs/2512.18014</link>
<guid>https://arxiv.org/abs/2512.18014</guid>
<content:encoded><![CDATA[
arXiv:2512.18014v1 Announce Type: cross 
Abstract: This paper presents an early exploration of reinforcement learning methodologies for legal AI in the Indian context. We introduce Reinforcement Learning-based Legal Reasoning (ReGal), a framework that integrates Multi-Task Instruction Tuning with Reinforcement Learning from AI Feedback (RLAIF) using Proximal Policy Optimization (PPO). Our approach is evaluated across two critical legal tasks: (i) Court Judgment Prediction and Explanation (CJPE), and (ii) Legal Document Summarization. Although the framework underperforms on standard evaluation metrics compared to supervised and proprietary models, it provides valuable insights into the challenges of applying RL to legal texts. These challenges include reward model alignment, legal language complexity, and domain-specific adaptation. Through empirical and qualitative analysis, we demonstrate how RL can be repurposed for high-stakes, long-document tasks in law. Our findings establish a foundation for future work on optimizing legal reasoning pipelines using reinforcement learning, with broader implications for building interpretable and adaptive legal AI systems.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shuttling Compiler for Trapped-Ion Quantum Computers Based on Large Language Models</title>
<link>https://arxiv.org/abs/2512.18021</link>
<guid>https://arxiv.org/abs/2512.18021</guid>
<content:encoded><![CDATA[
arXiv:2512.18021v1 Announce Type: cross 
Abstract: Trapped-ion quantum computers based on segmented traps rely on shuttling operations to establish connectivity between multiple sub-registers within a quantum processing unit. Several architectures of increasing complexity have already been realized, including linear arrays, racetrack loops, and junction-based layouts. As hardware capabilities advance, the need arises for flexible software layers within the control stack to manage qubit routing$\unicode{x2014}$the process of dynamically reconfiguring qubit positions so that all qubits involved in a gate operation are co-located within the same segment. Existing approaches typically employ architecture-specific heuristics, which become impractical as system complexity grows. To address this challenge, we propose a layout-independent compilation strategy based on large language models (LLMs). Specifically, we fine-tune pretrained LLMs to generate the required shuttling operations. We evaluate this approach on both linear and branched one-dimensional architectures, demonstrating that it provides a foundation for developing LLM-based shuttling compilers for trapped-ion quantum computers.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Long-range electrostatics for machine learning interatomic potentials is easier than we thought</title>
<link>https://arxiv.org/abs/2512.18029</link>
<guid>https://arxiv.org/abs/2512.18029</guid>
<content:encoded><![CDATA[
arXiv:2512.18029v1 Announce Type: cross 
Abstract: The lack of long-range electrostatics is a key limitation of modern machine learning interatomic potentials (MLIPs), hindering reliable applications to interfaces, charge-transfer reactions, polar and ionic materials, and biomolecules. In this Perspective, we distill two design principles behind the Latent Ewald Summation (LES) framework, which can capture long-range interactions, charges, and electrical response just by learning from standard energy and force training data: (i) use a Coulomb functional form with environment-dependent charges to capture electrostatic interactions, and (ii) avoid explicit training on ambiguous density functional theory (DFT) partial charges. When both principles are satisfied, substantial flexibility remains: essentially any short-range MLIP can be augmented; charge equilibration schemes can be added when desired; dipoles and Born effective charges can be inferred or finetuned; and charge/spin-state embeddings or tensorial targets can be further incorporated. We also discuss current limitations and open challenges. Together, these minimal, physics-guided design rules suggest that incorporating long-range electrostatics into MLIPs is simpler and perhaps more broadly applicable than is commonly assumed.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NodMAISI: Nodule-Oriented Medical AI for Synthetic Imaging</title>
<link>https://arxiv.org/abs/2512.18038</link>
<guid>https://arxiv.org/abs/2512.18038</guid>
<content:encoded><![CDATA[
arXiv:2512.18038v1 Announce Type: cross 
Abstract: Objective: Although medical imaging datasets are increasingly available, abnormal and annotation-intensive findings critical to lung cancer screening, particularly small pulmonary nodules, remain underrepresented and inconsistently curated. Methods: We introduce NodMAISI, an anatomically constrained, nodule-oriented CT synthesis and augmentation framework trained on a unified multi-source cohort (7,042 patients, 8,841 CTs, 14,444 nodules). The framework integrates: (i) a standardized curation and annotation pipeline linking each CT with organ masks and nodule-level annotations, (ii) a ControlNet-conditioned rectified-flow generator built on MAISI-v2's foundational blocks to enforce anatomy- and lesion-consistent synthesis, and (iii) lesion-aware augmentation that perturbs nodule masks (controlled shrinkage) while preserving surrounding anatomy to generate paired CT variants. Results: Across six public test datasets, NodMAISI improved distributional fidelity relative to MAISI-v2 (real-to-synthetic FID range 1.18 to 2.99 vs 1.69 to 5.21). In lesion detectability analysis using a MONAI nodule detector, NodMAISI substantially increased average sensitivity and more closely matched clinical scans (IMD-CT: 0.69 vs 0.39; DLCS24: 0.63 vs 0.20), with the largest gains for sub-centimeter nodules where MAISI-v2 frequently failed to reproduce the conditioned lesion. In downstream nodule-level malignancy classification trained on LUNA25 and externally evaluated on LUNA16, LNDbv4, and DLCS24, NodMAISI augmentation improved AUC by 0.07 to 0.11 at <=20% clinical data and by 0.12 to 0.21 at 10%, consistently narrowing the performance gap under data scarcity.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Narrative Consolidation: Formulating a New Task for Unifying Multi-Perspective Accounts</title>
<link>https://arxiv.org/abs/2512.18041</link>
<guid>https://arxiv.org/abs/2512.18041</guid>
<content:encoded><![CDATA[
arXiv:2512.18041v1 Announce Type: cross 
Abstract: Processing overlapping narrative documents, such as legal testimonies or historical accounts, often aims not for compression but for a unified, coherent, and chronologically sound text. Standard Multi-Document Summarization (MDS), with its focus on conciseness, fails to preserve narrative flow. This paper formally defines this challenge as a new NLP task: Narrative Consolidation, where the central objectives are chronological integrity, completeness, and the fusion of complementary details. To demonstrate the critical role of temporal structure in this task, we introduce Temporal Alignment Event Graph (TAEG), a graph structure that explicitly models chronology and event alignment. By applying a standard centrality algorithm to TAEG, our method functions as a version selection mechanism, choosing the most central representation of each event in its correct temporal position. In a study on the four Biblical Gospels, this structure-focused approach guarantees perfect temporal ordering (Kendall's Tau of 1.000) by design and dramatically improves content metrics (e.g., +357.2% in ROUGE-L F1). The success of this baseline method validates the formulation of Narrative Consolidation as a relevant task and establishes that an explicit temporal backbone is a fundamental component for its resolution.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FOODER: Real-time Facial Authentication and Expression Recognition</title>
<link>https://arxiv.org/abs/2512.18057</link>
<guid>https://arxiv.org/abs/2512.18057</guid>
<content:encoded><![CDATA[
arXiv:2512.18057v1 Announce Type: cross 
Abstract: Out-of-distribution (OOD) detection is essential for the safe deployment of neural networks, as it enables the identification of samples outside the training domain. We present FOODER, a real-time, privacy-preserving radar-based framework that integrates OOD-based facial authentication with facial expression recognition. FOODER operates using low-cost frequency-modulated continuous-wave (FMCW) radar and exploits both range-Doppler and micro range-Doppler representations. The authentication module employs a multi-encoder multi-decoder architecture with Body Part (BP) and Intermediate Linear Encoder-Decoder (ILED) components to classify a single enrolled individual as in-distribution while detecting all other faces as OOD. Upon successful authentication, an expression recognition module is activated. Concatenated radar representations are processed by a ResNet block to distinguish between dynamic and static facial expressions. Based on this categorization, two specialized MobileViT networks are used to classify dynamic expressions (smile, shock) and static expressions (neutral, anger). This hierarchical design enables robust facial authentication and fine-grained expression recognition while preserving user privacy by relying exclusively on radar data. Experiments conducted on a dataset collected with a 60 GHz short-range FMCW radar demonstrate that FOODER achieves an AUROC of 94.13% and an FPR95 of 18.12% for authentication, along with an average expression recognition accuracy of 94.70%. FOODER outperforms state-of-the-art OOD detection methods and several transformer-based architectures while operating efficiently in real time.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Approximation and learning with compositional tensor trains</title>
<link>https://arxiv.org/abs/2512.18059</link>
<guid>https://arxiv.org/abs/2512.18059</guid>
<content:encoded><![CDATA[
arXiv:2512.18059v1 Announce Type: cross 
Abstract: We introduce compositional tensor trains (CTTs) for the approximation of multivariate functions, a class of models obtained by composing low-rank functions in the tensor-train format. This format can encode standard approximation tools, such as (sparse) polynomials, deep neural networks (DNNs) with fixed width, or tensor networks with arbitrary permutation of the inputs, or more general affine coordinate transformations, with similar complexities. This format can be viewed as a DNN with width exponential in the input dimension and structured weights matrices. Compared to DNNs, this format enables controlled compression at the layer level using efficient tensor algebra. On the optimization side, we derive a layerwise algorithm inspired by natural gradient descent, allowing to exploit efficient low-rank tensor algebra. This relies on low-rank estimations of Gram matrices, and tensor structured random sketching. Viewing the format as a discrete dynamical system, we also derive an optimization algorithm inspired by numerical methods in optimal control. Numerical experiments on regression tasks demonstrate the expressivity of the new format and the relevance of the proposed optimization algorithms. Overall, CTTs combine the expressivity of compositional models with the algorithmic efficiency of tensor algebra, offering a scalable alternative to standard deep neural networks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph-based Nearest Neighbors with Dynamic Updates via Random Walks</title>
<link>https://arxiv.org/abs/2512.18060</link>
<guid>https://arxiv.org/abs/2512.18060</guid>
<content:encoded><![CDATA[
arXiv:2512.18060v1 Announce Type: cross 
Abstract: Approximate nearest neighbor search (ANN) is a common way to retrieve relevant search results, especially now in the context of large language models and retrieval augmented generation. One of the most widely used algorithms for ANN is based on constructing a multi-layer graph over the dataset, called the Hierarchical Navigable Small World (HNSW). While this algorithm supports insertion of new data, it does not support deletion of existing data. Moreover, deletion algorithms described by prior work come at the cost of increased query latency, decreased recall, or prolonged deletion time. In this paper, we propose a new theoretical framework for graph-based ANN based on random walks. We then utilize this framework to analyze a randomized deletion approach that preserves hitting time statistics compared to the graph before deleting the point. We then turn this theoretical framework into a deterministic deletion algorithm, and show that it provides better tradeoff between query latency, recall, deletion time, and memory usage through an extensive collection of experiments.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal Inference as Distribution Adaptation: Optimizing ATE Risk under Propensity Uncertainty</title>
<link>https://arxiv.org/abs/2512.18083</link>
<guid>https://arxiv.org/abs/2512.18083</guid>
<content:encoded><![CDATA[
arXiv:2512.18083v1 Announce Type: cross 
Abstract: Standard approaches to causal inference, such as Outcome Regression and Inverse Probability Weighted Regression Adjustment (IPWRA), are typically derived through the lens of missing data imputation and identification theory. In this work, we unify these methods from a Machine Learning perspective, reframing ATE estimation as a \textit{domain adaptation problem under distribution shift}. We demonstrate that the canonical Hajek estimator is a special case of IPWRA restricted to a constant hypothesis class, and that IPWRA itself is fundamentally Importance-Weighted Empirical Risk Minimization designed to correct for the covariate shift between the treated sub-population and the target population.
  Leveraging this unified framework, we critically examine the optimization objectives of Doubly Robust estimators. We argue that standard methods enforce \textit{sufficient but not necessary} conditions for consistency by requiring outcome models to be individually unbiased. We define the true "ATE Risk Function" and show that minimizing it requires only that the biases of the treated and control models structurally cancel out. Exploiting this insight, we propose the \textbf{Joint Robust Estimator (JRE)}. Instead of treating propensity estimation and outcome modeling as independent stages, JRE utilizes bootstrap-based uncertainty quantification of the propensity score to train outcome models jointly. By optimizing for the expected ATE risk over the distribution of propensity scores, JRE leverages model degrees of freedom to achieve robustness against propensity misspecification. Simulation studies demonstrate that JRE achieves up to a 15\% reduction in MSE compared to standard IPWRA in finite-sample regimes with misspecified outcome models.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Faithful and Stable Neuron Explanations for Trustworthy Mechanistic Interpretability</title>
<link>https://arxiv.org/abs/2512.18092</link>
<guid>https://arxiv.org/abs/2512.18092</guid>
<content:encoded><![CDATA[
arXiv:2512.18092v1 Announce Type: cross 
Abstract: Neuron identification is a popular tool in mechanistic interpretability, aiming to uncover the human-interpretable concepts represented by individual neurons in deep networks. While algorithms such as Network Dissection and CLIP-Dissect achieve great empirical success, a rigorous theoretical foundation remains absent, which is crucial to enable trustworthy and reliable explanations. In this work, we observe that neuron identification can be viewed as the inverse process of machine learning, which allows us to derive guarantees for neuron explanations. Based on this insight, we present the first theoretical analysis of two fundamental challenges: (1) Faithfulness: whether the identified concept faithfully represents the neuron's underlying function and (2) Stability: whether the identification results are consistent across probing datasets. We derive generalization bounds for widely used similarity metrics (e.g. accuracy, AUROC, IoU) to guarantee faithfulness, and propose a bootstrap ensemble procedure that quantifies stability along with BE (Bootstrap Explanation) method to generate concept prediction sets with guaranteed coverage probability. Experiments on both synthetic and real data validate our theoretical results and demonstrate the practicality of our method, providing an important step toward trustworthy neuron identification.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Coverage to Causes: Data-Centric Fuzzing for JavaScript Engines</title>
<link>https://arxiv.org/abs/2512.18102</link>
<guid>https://arxiv.org/abs/2512.18102</guid>
<content:encoded><![CDATA[
arXiv:2512.18102v1 Announce Type: cross 
Abstract: Context: Exhaustive fuzzing of modern JavaScript engines is infeasible due to the vast number of program states and execution paths. Coverage-guided fuzzers waste effort on low-risk inputs, often ignoring vulnerability-triggering ones that do not increase coverage. Existing heuristics proposed to mitigate this require expert effort, are brittle, and hard to adapt.
  Objective: We propose a data-centric, LLM-boosted alternative that learns from historical vulnerabilities to automatically identify minimal static (code) and dynamic (runtime) features for detecting high-risk inputs.
  Method: Guided by historical V8 bugs, iterative prompting generated 115 static and 49 dynamic features, with the latter requiring only five trace flags, minimizing instrumentation cost. After feature selection, 41 features remained to train an XGBoost model to predict high-risk inputs during fuzzing.
  Results: Combining static and dynamic features yields over 85% precision and under 1% false alarms. Only 25% of these features are needed for comparable performance, showing that most of the search space is irrelevant.
  Conclusion: This work introduces feature-guided fuzzing, an automated data-driven approach that replaces coverage with data-directed inference, guiding fuzzers toward high-risk states for faster, targeted, and reproducible vulnerability discovery. To support open science, all scripts and data are available at https://github.com/KKGanguly/DataCentricFuzzJS .
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring polymer classification with a hybrid single-photon quantum approach</title>
<link>https://arxiv.org/abs/2512.18125</link>
<guid>https://arxiv.org/abs/2512.18125</guid>
<content:encoded><![CDATA[
arXiv:2512.18125v1 Announce Type: cross 
Abstract: Polymers exhibit complex architectures and diverse properties that place them at the center of contemporary research in chemistry and materials science. As conventional computational techniques, even multi-scale ones, struggle to capture this complexity, quantum computing offers a promising alternative framework for extracting structure-property relationships. Noisy Intermediate-Scale Quantum (NISQ) devices are commonly used to explore the implementation of algorithms, including quantum neural networks for classification tasks, despite ongoing debate regarding their practical impact.
  We present a hybrid classical-quantum formalism that couples a classical deep neural network for polymer featurization with a single-photon-based quantum classifier native to photonic quantum computing. This pipeline successfully classifies polymer species by their optical gap, with performance in line between CPU-based noisy simulations and a proof-of-principle run on Quandela's Ascella quantum processor. These findings demonstrate the effectiveness of the proposed computational workflow and indicate that chemistryfrelated classification tasks can already be tackled under the constraints of today's NISQ devices.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal Software Pipelining and Warp Specialization for Tensor Core GPUs</title>
<link>https://arxiv.org/abs/2512.18134</link>
<guid>https://arxiv.org/abs/2512.18134</guid>
<content:encoded><![CDATA[
arXiv:2512.18134v1 Announce Type: cross 
Abstract: GPU architectures have continued to grow in complexity, with recent incarnations introducing increasingly powerful fixed-function units for matrix multiplication and data movement to accompany highly parallel general-purpose cores. To fully leverage these machines, software must use sophisticated schedules that maximally utilize all hardware resources. Since realizing such schedules is complex, both programmers and compilers routinely employ program transformations, such as software pipelining (SWP) and warp specialization (WS), to do so in practice. However, determining how best to use SWP and WS in combination is a challenging problem that is currently handled through a mix of brittle compilation heuristics and fallible human intuition, with little insight into the space of solutions. To remedy this situation, we introduce a novel formulation of SWP and WS as a joint optimization problem that can be solved holistically by off-the-shelf constraint solvers. We reify our approach in Twill, the first system that automatically derives optimal SWP and WS schedules for a large class of iterative programs. Twill is heuristic-free, easily extensible to new GPU architectures, and guaranteed to produce optimal schedules. We show that Twill can rediscover, and thereby prove optimal, the SWP and WS schedules manually developed by experts for Flash Attention on both the NVIDIA Hopper and Blackwell GPU architectures.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Estimating Solvation Free Energies with Boltzmann Generators</title>
<link>https://arxiv.org/abs/2512.18147</link>
<guid>https://arxiv.org/abs/2512.18147</guid>
<content:encoded><![CDATA[
arXiv:2512.18147v1 Announce Type: cross 
Abstract: Accurate calculations of solvation free energies remain a central challenge in molecular simulations, often requiring extensive sampling and numerous alchemical intermediates to ensure sufficient overlap between phase-space distributions of a solute in the gas phase and in solution. Here, we introduce a computational framework based on normalizing flows that directly maps solvent configurations between solutes of different sizes, and compare the accuracy and efficiency to conventional free energy estimates. For a Lennard-Jones solvent, we demonstrate that this approach yields acceptable accuracy in estimating free energy differences for challenging transformations, such as solute growth or increased solute-solute separation, which typically demand multiple intermediate simulation steps along the transformation. Analysis of radial distribution functions indicates that the flow generates physically meaningful solvent rearrangements, substantially enhancing configurational overlap between states in configuration space. These results suggest flow-based models as a promising alternative to traditional free energy estimation methods.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>External Hippocampus: Topological Cognitive Maps for Guiding Large Language Model Reasoning</title>
<link>https://arxiv.org/abs/2512.18190</link>
<guid>https://arxiv.org/abs/2512.18190</guid>
<content:encoded><![CDATA[
arXiv:2512.18190v1 Announce Type: cross 
Abstract: This paper proposes the External Hippocampus framework, which models language model reasoning from a cognitive dynamics perspective as the flow of information energy in semantic space. Unlike traditional weight-space optimization methods, this framework constructs topological cognitive maps through dimensionality reduction projection, enabling precise navigation and intervention of energy flow at test time while avoiding substantial computational requirements and demonstrating predictable intervention patterns. The method effectively addresses the cognitive deadlock problem in multi-step reasoning for small models. Experiments on models <=7B parameters show: map-guided methods achieve 81.20% accuracy on 500 challenging problems (relative baseline +16.80%), reduce reasoning time by >= 15x, with key findings revealing that reasoning stagnation manifests as "Cognitive Vortex" and low-entropy potential wells, while temperature perturbations effectively restart energy flow. The framework requires no additional training, possesses autonomous growth capability, and provides an efficient and controllable topological-aware solution for small model reasoning.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Efficient Testing of Graph Neural Networks via Test Input Prioritization</title>
<link>https://arxiv.org/abs/2512.18228</link>
<guid>https://arxiv.org/abs/2512.18228</guid>
<content:encoded><![CDATA[
arXiv:2512.18228v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable efficacy in handling graph-structured data; however, they exhibit failures after deployment, which can cause severe consequences. Hence, conducting thorough testing before deployment becomes imperative to ensure the reliability of GNNs. However, thorough testing requires numerous manually annotated test data. To mitigate the annotation cost, strategically prioritizing and labeling high-quality unlabeled inputs for testing becomes crucial, which facilitates uncovering more model failures with a limited labeling budget. Unfortunately, existing test input prioritization techniques either overlook the valuable information contained in graph structures or are overly reliant on attributes extracted from the target model, i.e., model-aware attributes, whose quality can vary significantly. To address these issues, we propose a novel test input prioritization framework, named GraphRank, for GNNs. GraphRank introduces model-agnostic attributes to compensate for the limitations of the model-aware ones. It also leverages the graph structure information to aggregate attributes from neighboring nodes, thereby enhancing the model-aware and model-agnostic attributes. Furthermore, GraphRank combines the above attributes with a binary classifier, using it as a ranking model to prioritize inputs. This classifier undergoes iterative training, which enables it to learn from each round's feedback and improve its performance accordingly. Extensive experiments demonstrate GraphRank's superiority over existing techniques.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dimensionality Reduction Considered Harmful (Some of the Time)</title>
<link>https://arxiv.org/abs/2512.18230</link>
<guid>https://arxiv.org/abs/2512.18230</guid>
<content:encoded><![CDATA[
arXiv:2512.18230v1 Announce Type: cross 
Abstract: Visual analytics now plays a central role in decision-making across diverse disciplines, but it can be unreliable: the knowledge or insights derived from the analysis may not accurately reflect the underlying data. In this dissertation, we improve the reliability of visual analytics with a focus on dimensionality reduction (DR). DR techniques enable visual analysis of high-dimensional data by reducing it to two or three dimensions, but they inherently introduce errors that can compromise the reliability of visual analytics. To this end, I investigate reliability challenges that practitioners face when using DR for visual analytics. Then, I propose technical solutions to address these challenges, including new evaluation metrics, optimization strategies, and interaction techniques. We conclude the thesis by discussing how our contributions lay the foundation for achieving more reliable visual analytics practices.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoSchA: Automatic Hierarchical Music Representations via Multi-Relational Node Isolation</title>
<link>https://arxiv.org/abs/2512.18232</link>
<guid>https://arxiv.org/abs/2512.18232</guid>
<content:encoded><![CDATA[
arXiv:2512.18232v1 Announce Type: cross 
Abstract: Hierarchical representations provide powerful and principled approaches for analyzing many musical genres. Such representations have been broadly studied in music theory, for instance via Schenkerian analysis (SchA). Hierarchical music analyses, however, are highly cost-intensive; the analysis of a single piece of music requires a great deal of time and effort from trained experts. The representation of hierarchical analyses in a computer-readable format is a further challenge. Given recent developments in hierarchical deep learning and increasing quantities of computer-readable data, there is great promise in extending such work for an automatic hierarchical representation framework. This paper thus introduces a novel approach, AutoSchA, which extends recent developments in graph neural networks (GNNs) for hierarchical music analysis. AutoSchA features three key contributions: 1) a new graph learning framework for hierarchical music representation, 2) a new graph pooling mechanism based on node isolation that directly optimizes learned pooling assignments, and 3) a state-of-the-art architecture that integrates such developments for automatic hierarchical music analysis. We show, in a suite of experiments, that AutoSchA performs comparably to human experts when analyzing Baroque fugue subjects.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CrystalFormer-CSP: Thinking Fast and Slow for Crystal Structure Prediction</title>
<link>https://arxiv.org/abs/2512.18251</link>
<guid>https://arxiv.org/abs/2512.18251</guid>
<content:encoded><![CDATA[
arXiv:2512.18251v1 Announce Type: cross 
Abstract: Crystal structure prediction is a fundamental problem in materials science. We present CrystalFormer-CSP, an efficient framework that unifies data-driven heuristic and physics-driven optimization approaches to predict stable crystal structures for given chemical compositions. The approach combines pretrained generative models for space-group-informed structure generation and a universal machine learning force field for energy minimization. Reinforcement fine-tuning can be employed to further boost the accuracy of the framework. We demonstrate the effectiveness of CrystalFormer-CSP on benchmark problems and showcase its usage via web interface and language model integration.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TICL+: A Case Study On Speech In-Context Learning for Children's Speech Recognition</title>
<link>https://arxiv.org/abs/2512.18263</link>
<guid>https://arxiv.org/abs/2512.18263</guid>
<content:encoded><![CDATA[
arXiv:2512.18263v1 Announce Type: cross 
Abstract: Children's speech recognition remains challenging due to substantial acoustic and linguistic variability, limited labeled data, and significant differences from adult speech. Speech foundation models can address these challenges through Speech In-Context Learning (SICL), allowing adaptation to new domains without fine-tuning. However, the effectiveness of SICL depends on how in-context examples are selected. We extend an existing retrieval-based method, Text-Embedding KNN for SICL (TICL), introducing an acoustic reranking step to create TICL+. This extension prioritizes examples that are both semantically and acoustically aligned with the test input. Experiments on four children's speech corpora show that TICL+ achieves up to a 53.3% relative word error rate reduction over zero-shot performance and 37.6% over baseline TICL, highlighting the value of combining semantic and acoustic information for robust, scalable ASR in children's speech.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A two-stream network with global-local feature fusion for bone age assessment</title>
<link>https://arxiv.org/abs/2512.18331</link>
<guid>https://arxiv.org/abs/2512.18331</guid>
<content:encoded><![CDATA[
arXiv:2512.18331v1 Announce Type: cross 
Abstract: Bone Age Assessment (BAA) is a widely used clinical technique that can accurately reflect an individual's growth and development level, as well as maturity. In recent years, although deep learning has advanced the field of bone age assessment, existing methods face challenges in efficiently balancing global features and local skeletal details. This study aims to develop an automated bone age assessment system based on a two-stream deep learning architecture to achieve higher accuracy in bone age assessment. We propose the BoNet+ model incorporating global and local feature extraction channels. A Transformer module is introduced into the global feature extraction channel to enhance the ability in extracting global features through multi-head self-attention mechanism. A RFAConv module is incorporated into the local feature extraction channel to generate adaptive attention maps within multiscale receptive fields, enhancing local feature extraction capabilities. Global and local features are concatenated along the channel dimension and optimized by an Inception-V3 network. The proposed method has been validated on the Radiological Society of North America (RSNA) and Radiological Hand Pose Estimation (RHPE) test datasets, achieving mean absolute errors (MAEs) of 3.81 and 5.65 months, respectively. These results are comparable to the state-of-the-art. The BoNet+ model reduces the clinical workload and achieves automatic, high-precision, and more objective bone age assessment.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reinforcement Learning Position Control of a Quadrotor Using Soft Actor-Critic (SAC)</title>
<link>https://arxiv.org/abs/2512.18333</link>
<guid>https://arxiv.org/abs/2512.18333</guid>
<content:encoded><![CDATA[
arXiv:2512.18333v1 Announce Type: cross 
Abstract: This paper proposes a new Reinforcement Learning (RL) based control architecture for quadrotors. With the literature focusing on controlling the four rotors' RPMs directly, this paper aims to control the quadrotor's thrust vector. The RL agent computes the percentage of overall thrust along the quadrotor's z-axis along with the desired Roll ($\phi$) and Pitch ($\theta$) angles. The agent then sends the calculated control signals along with the current quadrotor's Yaw angle ($\psi$) to an attitude PID controller. The PID controller then maps the control signals to motor RPMs. The Soft Actor-Critic algorithm, a model-free off-policy stochastic RL algorithm, was used to train the RL agents. Training results show the faster training time of the proposed thrust vector controller in comparison to the conventional RPM controllers. Simulation results show smoother and more accurate path-following for the proposed thrust vector controller.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Entropy Tuning in Reinforcement Learning Low-Level Quadcopter Control: Stochasticity vs Determinism</title>
<link>https://arxiv.org/abs/2512.18336</link>
<guid>https://arxiv.org/abs/2512.18336</guid>
<content:encoded><![CDATA[
arXiv:2512.18336v1 Announce Type: cross 
Abstract: This paper explores the impact of dynamic entropy tuning in Reinforcement Learning (RL) algorithms that train a stochastic policy. Its performance is compared against algorithms that train a deterministic one. Stochastic policies optimize a probability distribution over actions to maximize rewards, while deterministic policies select a single deterministic action per state. The effect of training a stochastic policy with both static entropy and dynamic entropy and then executing deterministic actions to control the quadcopter is explored. It is then compared against training a deterministic policy and executing deterministic actions. For the purpose of this research, the Soft Actor-Critic (SAC) algorithm was chosen for the stochastic algorithm while the Twin Delayed Deep Deterministic Policy Gradient (TD3) was chosen for the deterministic algorithm. The training and simulation results show the positive effect the dynamic entropy tuning has on controlling the quadcopter by preventing catastrophic forgetting and improving exploration efficiency.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Zero-Shot Inpainting with Decoupled Diffusion Guidance</title>
<link>https://arxiv.org/abs/2512.18365</link>
<guid>https://arxiv.org/abs/2512.18365</guid>
<content:encoded><![CDATA[
arXiv:2512.18365v1 Announce Type: cross 
Abstract: Diffusion models have emerged as powerful priors for image editing tasks such as inpainting and local modification, where the objective is to generate realistic content that remains consistent with observed regions. In particular, zero-shot approaches that leverage a pretrained diffusion model, without any retraining, have been shown to achieve highly effective reconstructions. However, state-of-the-art zero-shot methods typically rely on a sequence of surrogate likelihood functions, whose scores are used as proxies for the ideal score. This procedure however requires vector-Jacobian products through the denoiser at every reverse step, introducing significant memory and runtime overhead. To address this issue, we propose a new likelihood surrogate that yields simple and efficient to sample Gaussian posterior transitions, sidestepping the backpropagation through the denoiser network. Our extensive experiments show that our method achieves strong observation consistency compared with fine-tuned baselines and produces coherent, high-quality reconstructions, all while significantly reducing inference cost. Code is available at https://github.com/YazidJanati/ding.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PSI3D: Plug-and-Play 3D Stochastic Inference with Slice-wise Latent Diffusion Prior</title>
<link>https://arxiv.org/abs/2512.18367</link>
<guid>https://arxiv.org/abs/2512.18367</guid>
<content:encoded><![CDATA[
arXiv:2512.18367v1 Announce Type: cross 
Abstract: Diffusion models are highly expressive image priors for Bayesian inverse problems. However, most diffusion models cannot operate on large-scale, high-dimensional data due to high training and inference costs. In this work, we introduce a Plug-and-play algorithm for 3D stochastic inference with latent diffusion prior (PSI3D) to address massive ($1024\times 1024\times 128$) volumes. Specifically, we formulate a Markov chain Monte Carlo approach to reconstruct each two-dimensional (2D) slice by sampling from a 2D latent diffusion model. To enhance inter-slice consistency, we also incorporate total variation (TV) regularization stochastically along the concatenation axis. We evaluate our performance on optical coherence tomography (OCT) super-resolution. Our method significantly improves reconstruction quality for large-scale scientific imaging compared to traditional and learning-based baselines, while providing robust and credible reconstructions.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Proofs for Sound Verification and Control of Complex Systems</title>
<link>https://arxiv.org/abs/2512.18389</link>
<guid>https://arxiv.org/abs/2512.18389</guid>
<content:encoded><![CDATA[
arXiv:2512.18389v1 Announce Type: cross 
Abstract: This informal contribution presents an ongoing line of research that is pursuing a new approach to the construction of sound proofs for the formal verification and control of complex stochastic models of dynamical systems, of reactive programs and, more generally, of models of Cyber-Physical Systems. Neural proofs are made up of two key components: 1) proof rules encode requirements entailing the verification of general temporal specifications over the models of interest; and 2) certificates that discharge such rules, namely they are constructed from said proof rules with an inductive (that is, cyclic, repetitive) approach; this inductive approach involves: 2a) accessing samples from the model's dynamics and accordingly training neural networks, whilst 2b) generalising such networks via SAT-modulo-theory (SMT) queries that leverage the full knowledge of the models. In the context of sequential decision making problems over complex stochastic models, it is possible to additionally generate provably-correct policies/strategies/controllers, namely state-feedback functions that, in conjunction with neural certificates, formally attain the given specifications for the models of interest.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Mosaic Tesserae Segmentation via Deep Learning Techniques</title>
<link>https://arxiv.org/abs/2512.18406</link>
<guid>https://arxiv.org/abs/2512.18406</guid>
<content:encoded><![CDATA[
arXiv:2512.18406v1 Announce Type: cross 
Abstract: Art is widely recognized as a reflection of civilization and mosaics represent an important part of cultural heritage. Mosaics are an ancient art form created by arranging small pieces, called tesserae, on a surface using adhesive. Due to their age and fragility, they are prone to damage, highlighting the need for digital preservation. This paper addresses the problem of digitizing mosaics by segmenting the tesserae to separate them from the background within the broader field of Image Segmentation in Computer Vision. We propose a method leveraging Segment Anything Model 2 (SAM 2) by Meta AI, a foundation model that outperforms most conventional segmentation models, to automatically segment mosaics. Due to the limited open datasets in the field, we also create an annotated dataset of mosaic images to fine-tune and evaluate the model. Quantitative evaluation on our testing dataset shows notable improvements compared to the baseline SAM 2 model, with Intersection over Union increasing from 89.00% to 91.02% and Recall from 92.12% to 95.89%. Additionally, on a benchmark proposed by a prior approach, our model achieves an F-measure 3% higher than previous methods and reduces the error in the absolute difference between predicted and actual tesserae from 0.20 to just 0.02. The notable performance of the fine-tuned SAM 2 model together with the newly annotated dataset can pave the way for real-time segmentation of mosaic images.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Spurious Correlations in NLI via LLM-Synthesized Counterfactuals and Dynamic Balanced Sampling</title>
<link>https://arxiv.org/abs/2512.18462</link>
<guid>https://arxiv.org/abs/2512.18462</guid>
<content:encoded><![CDATA[
arXiv:2512.18462v1 Announce Type: cross 
Abstract: Natural Language Inference (NLI) models frequently rely on spurious correlations rather than semantic reasoning. Existing mitigation strategies often incur high annotation costs or trigger catastrophic forgetting during fine-tuning. We propose an automated, scalable pipeline to address these limitations. First, we introduce Log-Frequency LMI (LF-LMI) to accurately detect semantic artifacts. Second, we generate a high-quality synthetic contrast set via an LLM-synthesis pipeline with multi-judge verification. Finally, we introduce Dynamic Balanced Sampling, a training strategy that rotates the original data distribution to prevent forgetting. Our method improves consistency on a challenging benchmark from 63.5% to 81.0% while maintaining 88.4% in-domain accuracy, significantly outperforming naive fine-tuning.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Research on a hybrid LSTM-CNN-Attention model for text-based web content classification</title>
<link>https://arxiv.org/abs/2512.18475</link>
<guid>https://arxiv.org/abs/2512.18475</guid>
<content:encoded><![CDATA[
arXiv:2512.18475v1 Announce Type: cross 
Abstract: This study presents a hybrid deep learning architecture that integrates LSTM, CNN, and an Attention mechanism to enhance the classification of web content based on text. Pretrained GloVe embeddings are used to represent words as dense vectors that preserve semantic similarity. The CNN layer extracts local n-gram patterns and lexical features, while the LSTM layer models long-range dependencies and sequential structure. The integrated Attention mechanism enables the model to focus selectively on the most informative parts of the input sequence. A 5-fold cross-validation setup was used to assess the robustness and generalizability of the proposed solution. Experimental results show that the hybrid LSTM-CNN-Attention model achieved outstanding performance, with an accuracy of 0.98, precision of 0.94, recall of 0.92, and F1-score of 0.93. These results surpass the performance of baseline models based solely on CNNs, LSTMs, or transformer-based classifiers such as BERT. The combination of neural network components enabled the model to effectively capture both fine-grained text structures and broader semantic context. Furthermore, the use of GloVe embeddings provided an efficient and effective representation of textual data, making the model suitable for integration into systems with real-time or near-real-time requirements. The proposed hybrid architecture demonstrates high effectiveness in text-based web content classification, particularly in tasks requiring both syntactic feature extraction and semantic interpretation. By combining presented mechanisms, the model addresses the limitations of individual architectures and achieves improved generalization. These findings support the broader use of hybrid deep learning approaches in NLP applications, especially where complex, unstructured textual data must be processed and classified with high reliability.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PlantDiseaseNet-RT50: A Fine-tuned ResNet50 Architecture for High-Accuracy Plant Disease Detection Beyond Standard CNNs</title>
<link>https://arxiv.org/abs/2512.18500</link>
<guid>https://arxiv.org/abs/2512.18500</guid>
<content:encoded><![CDATA[
arXiv:2512.18500v1 Announce Type: cross 
Abstract: Plant diseases pose a significant threat to agricultural productivity and global food security, accounting for 70-80% of crop losses worldwide. Traditional detection methods rely heavily on expert visual inspection, which is time-consuming, labour-intensive, and often impractical for large-scale farming operations. In this paper, we present PlantDiseaseNet-RT50, a novel fine-tuned deep learning architecture based on ResNet50 for automated plant disease detection. Our model features strategically unfrozen layers, a custom classification head with regularization mechanisms, and dynamic learning rate scheduling through cosine decay. Using a comprehensive dataset of distinct plant disease categories across multiple crop species, PlantDiseaseNet-RT50 achieves exceptional performance with approximately 98% accuracy, precision, and recall. Our architectural modifications and optimization protocol demonstrate how targeted fine-tuning can transform a standard pretrained model into a specialized agricultural diagnostic tool. We provide a detailed account of our methodology, including the systematic unfreezing of terminal layers, implementation of batch normalization and dropout regularization and application of advanced training techniques. PlantDiseaseNet-RT50 represents a significant advancement in AI-driven agricultural tools, offering a computationally efficient solution for rapid and accurate plant disease diagnosis that can be readily implemented in practical farming contexts to support timely interventions and reduce crop losses.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NASTaR: NovaSAR Automated Ship Target Recognition Dataset</title>
<link>https://arxiv.org/abs/2512.18503</link>
<guid>https://arxiv.org/abs/2512.18503</guid>
<content:encoded><![CDATA[
arXiv:2512.18503v1 Announce Type: cross 
Abstract: Synthetic Aperture Radar (SAR) offers a unique capability for all-weather, space-based maritime activity monitoring by capturing and imaging strong reflections from ships at sea. A well-defined challenge in this domain is ship type classification. Due to the high diversity and complexity of ship types, accurate recognition is difficult and typically requires specialized deep learning models. These models, however, depend on large, high-quality ground-truth datasets to achieve robust performance and generalization. Furthermore, the growing variety of SAR satellites operating at different frequencies and spatial resolutions has amplified the need for more annotated datasets to enhance model accuracy. To address this, we present the NovaSAR Automated Ship Target Recognition (NASTaR) dataset. This dataset comprises of 3415 ship patches extracted from NovaSAR S-band imagery, with labels matched to AIS data. It includes distinctive features such as 23 unique classes, inshore/offshore separation, and an auxiliary wake dataset for patches where ship wakes are visible. We validated the dataset applicability across prominent ship-type classification scenarios using benchmark deep learning models. Results demonstrate over 60% accuracy for classifying four major ship types, over 70% for a three-class scenario, more than 75% for distinguishing cargo from tanker ships, and over 87% for identifying fishing vessels. The NASTaR dataset is available at https://10.5523/bris, while relevant codes for benchmarking and analysis are available at https://github.com/benyaminhosseiny/nastar.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pushing the limits of one-dimensional NMR spectroscopy for automated structure elucidation using artificial intelligence</title>
<link>https://arxiv.org/abs/2512.18531</link>
<guid>https://arxiv.org/abs/2512.18531</guid>
<content:encoded><![CDATA[
arXiv:2512.18531v1 Announce Type: cross 
Abstract: One-dimensional NMR spectroscopy is one of the most widely used techniques for the characterization of organic compounds and natural products. For molecules with up to 36 non-hydrogen atoms, the number of possible structures has been estimated to range from $10^{20} - 10^{60}$. The task of determining the structure (formula and connectivity) of a molecule of this size using only its one-dimensional $^1$H and/or $^{13}$C NMR spectrum, i.e. de novo structure generation, thus appears completely intractable. Here we show how it is possible to achieve this task for systems with up to 40 non-hydrogen atoms across the full elemental coverage typically encountered in organic chemistry (C, N, O, H, P, S, Si, B, and the halogens) using a deep learning framework, thus covering a vast portion of the drug-like chemical space. Leveraging insights from natural language processing, we show that our transformer-based architecture predicts the correct molecule with 55.2% accuracy within the first 15 predictions using only the $^1$H and $^{13}$C NMR spectra, thus overcoming the combinatorial growth of the chemical space while also being extensible to experimental data via fine-tuning.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalization Gaps in Political Fake News Detection: An Empirical Study on the LIAR Dataset</title>
<link>https://arxiv.org/abs/2512.18533</link>
<guid>https://arxiv.org/abs/2512.18533</guid>
<content:encoded><![CDATA[
arXiv:2512.18533v1 Announce Type: cross 
Abstract: The proliferation of linguistically subtle political disinformation poses a significant challenge to automated fact-checking systems. Despite increasing emphasis on complex neural architectures, the empirical limits of text-only linguistic modeling remain underexplored. We present a systematic diagnostic evaluation of nine machine learning algorithms on the LIAR benchmark. By isolating lexical features (Bag-of-Words, TF-IDF) and semantic embeddings (GloVe), we uncover a hard "Performance Ceiling", with fine-grained classification not exceeding a Weighted F1-score of 0.32 across models. Crucially, a simple linear SVM (Accuracy: 0.624) matches the performance of pre-trained Transformers such as RoBERTa (Accuracy: 0.620), suggesting that model capacity is not the primary bottleneck. We further diagnose a massive "Generalization Gap" in tree-based ensembles, which achieve more than 99% training accuracy but collapse to approximately 25% on test data, indicating reliance on lexical memorization rather than semantic inference. Synthetic data augmentation via SMOTE yields no meaningful gains, confirming that the limitation is semantic (feature ambiguity) rather than distributional. These findings indicate that for political fact-checking, increasing model complexity without incorporating external knowledge yields diminishing returns.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling up Stability: Reinforcement Learning for Distributed Control of Networked Systems in the Space of Stabilizing Policies</title>
<link>https://arxiv.org/abs/2512.18540</link>
<guid>https://arxiv.org/abs/2512.18540</guid>
<content:encoded><![CDATA[
arXiv:2512.18540v1 Announce Type: cross 
Abstract: We study distributed control of networked systems through reinforcement learning, where neural policies must be simultaneously scalable, expressive and stabilizing. We introduce a policy parameterization that embeds Graph Neural Networks (GNNs) into a Youla-like magnitude-direction parameterization, yielding distributed stochastic controllers that guarantee network-level closed-loop stability by design. The magnitude is implemented as a stable operator consisting of a GNN acting on disturbance feedback, while the direction is a GNN acting on local observations. We prove robustness of the closed loop to perturbations in both the graph topology and model parameters, and show how to integrate our parameterization with Proximal Policy Optimization. Experiments on a multi-agent navigation task show that policies trained on small networks transfer directly to larger ones and unseen network topologies, achieve higher returns and lower variance than a state-of-the-art MARL baseline while preserving stability.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SecureCode v2.0: A Production-Grade Dataset for Training Security-Aware Code Generation Models</title>
<link>https://arxiv.org/abs/2512.18542</link>
<guid>https://arxiv.org/abs/2512.18542</guid>
<content:encoded><![CDATA[
arXiv:2512.18542v1 Announce Type: cross 
Abstract: AI assistants produce vulnerable code in 45% of security-relevant scenarios, introducing flaws into production systems at scale. Yet existing secure coding datasets fall short. They lack incident grounding, don't provide the scale modern training requires, and miss the operational security context developers need for production deployments. We present SecureCode v2.0, a production-grade dataset of 1,215 security-focused coding examples that passed structural validation and expert security review. Every example ties to actual documented security incidents with CVE references, provides vulnerable and secure implementations, demonstrates concrete attacks, and includes defense-in-depth operational guidance. The dataset covers 11 vulnerability categories (complete OWASP Top 10:2025 plus AI/ML Security Threats) across 11 languages (Python, JavaScript, Java, Go, PHP, C#, TypeScript, Ruby, Rust, Kotlin, and YAML for infrastructure-as-code).
  Our quality assurance framework ensures complete incident grounding. Each example includes SIEM integration strategies, infrastructure hardening recommendations (Docker, AppArmor, WAF configurations), and testing approaches using language-appropriate frameworks. The dataset uses a 4-turn conversational structure mirroring actual developer-AI interactions, escalating from basic implementations to advanced security considerations and defense-in-depth guidance.
  Our contributions: (1) 1,215 rigorously validated examples split into 989 training, 122 validation, and 104 test sets, (2) an automated validation framework ensuring dataset consistency, (3) a 4-turn conversational structure capturing realistic security workflows, (4) comprehensive operational security guidance with SIEM integration strategies, (5) complete language-specific implementation fidelity, and (6) open-source release of data, validation tools, and benchmarking protocols.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Training Superintelligent Software Agents through Self-Play SWE-RL</title>
<link>https://arxiv.org/abs/2512.18552</link>
<guid>https://arxiv.org/abs/2512.18552</guid>
<content:encoded><![CDATA[
arXiv:2512.18552v1 Announce Type: cross 
Abstract: While current software agents powered by large language models (LLMs) and agentic reinforcement learning (RL) can boost programmer productivity, their training data (e.g., GitHub issues and pull requests) and environments (e.g., pass-to-pass and fail-to-pass tests) heavily depend on human knowledge or curation, posing a fundamental barrier to superintelligence. In this paper, we present Self-play SWE-RL (SSR), a first step toward training paradigms for superintelligent software agents. Our approach takes minimal data assumptions, only requiring access to sandboxed repositories with source code and installed dependencies, with no need for human-labeled issues or tests. Grounded in these real-world codebases, a single LLM agent is trained via reinforcement learning in a self-play setting to iteratively inject and repair software bugs of increasing complexity, with each bug formally specified by a test patch rather than a natural language issue description. On the SWE-bench Verified and SWE-Bench Pro benchmarks, SSR achieves notable self-improvement (+10.4 and +7.8 points, respectively) and consistently outperforms the human-data baseline over the entire training trajectory, despite being evaluated on natural language issues absent from self-play. Our results, albeit early, suggest a path where agents autonomously gather extensive learning experiences from real-world software repositories, ultimately enabling superintelligent systems that exceed human capabilities in understanding how systems are constructed, solving novel challenges, and autonomously creating new software from scratch.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Placenta Accreta Spectrum Detection Using an MRI-based Hybrid CNN-Transformer Model</title>
<link>https://arxiv.org/abs/2512.18573</link>
<guid>https://arxiv.org/abs/2512.18573</guid>
<content:encoded><![CDATA[
arXiv:2512.18573v1 Announce Type: cross 
Abstract: Placenta Accreta Spectrum (PAS) is a serious obstetric condition that can be challenging to diagnose with Magnetic Resonance Imaging (MRI) due to variability in radiologists' interpretations. To overcome this challenge, a hybrid 3D deep learning model for automated PAS detection from volumetric MRI scans is proposed in this study. The model integrates a 3D DenseNet121 to capture local features and a 3D Vision Transformer (ViT) to model global spatial context. It was developed and evaluated on a retrospective dataset of 1,133 MRI volumes. Multiple 3D deep learning architectures were also evaluated for comparison. On an independent test set, the DenseNet121-ViT model achieved the highest performance with a five-run average accuracy of 84.3%. These results highlight the strength of hybrid CNN-Transformer models as a computer-aided diagnosis tool. The model's performance demonstrates a clear potential to assist radiologists by providing a robust decision support to improve diagnostic consistency across interpretations, and ultimately enhance the accuracy and timeliness of PAS diagnosis.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PMPGuard: Catching Pseudo-Matched Pairs in Remote Sensing Image-Text Retrieval</title>
<link>https://arxiv.org/abs/2512.18660</link>
<guid>https://arxiv.org/abs/2512.18660</guid>
<content:encoded><![CDATA[
arXiv:2512.18660v1 Announce Type: cross 
Abstract: Remote sensing (RS) image-text retrieval faces significant challenges in real-world datasets due to the presence of Pseudo-Matched Pairs (PMPs), semantically mismatched or weakly aligned image-text pairs, which hinder the learning of reliable cross-modal alignments. To address this issue, we propose a novel retrieval framework that leverages Cross-Modal Gated Attention and a Positive-Negative Awareness Attention mechanism to mitigate the impact of such noisy associations. The gated module dynamically regulates cross-modal information flow, while the awareness mechanism explicitly distinguishes informative (positive) cues from misleading (negative) ones during alignment learning. Extensive experiments on three benchmark RS datasets, i.e., RSICD, RSITMD, and RS5M, demonstrate that our method consistently achieves state-of-the-art performance, highlighting its robustness and effectiveness in handling real-world mismatches and PMPs in RS image-text retrieval tasks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Task Vector in TTS: Toward Emotionally Expressive Dialectal Speech Synthesis</title>
<link>https://arxiv.org/abs/2512.18699</link>
<guid>https://arxiv.org/abs/2512.18699</guid>
<content:encoded><![CDATA[
arXiv:2512.18699v1 Announce Type: cross 
Abstract: Recent advances in text-to-speech (TTS) have yielded remarkable improvements in naturalness and intelligibility. Building on these achievements, research has increasingly shifted toward enhancing the expressiveness of generated speech, such as dialectal and emotional TTS. However, cross-style synthesis combining both dialect and emotion remains challenging and largely unexplored, mainly due to the scarcity of dialectal data with emotional labels. To address this, we propose Hierarchical Expressive Vector (HE-Vector), a two-stage method for Emotional Dialectal TTS. In the first stage, we construct different task vectors to model dialectal and emotional styles independently, and then enhance single-style synthesis by adjusting their weights, a method we refer to as Expressive Vector (E-Vector). For the second stage, we hierarchically integrate these vectors to achieve controllable emotionally expressive dialect synthesis without requiring jointly labeled data, corresponding to Hierarchical Expressive Vector (HE-Vector). Experimental results demonstrate that HE-Vectors achieve superior performance in dialect synthesis, and promising results in synthesizing emotionally expressive dialectal speech in a zero-shot setting.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised Feature Selection via Robust Autoencoder and Adaptive Graph Learning</title>
<link>https://arxiv.org/abs/2512.18720</link>
<guid>https://arxiv.org/abs/2512.18720</guid>
<content:encoded><![CDATA[
arXiv:2512.18720v1 Announce Type: cross 
Abstract: Effective feature selection is essential for high-dimensional data analysis and machine learning. Unsupervised feature selection (UFS) aims to simultaneously cluster data and identify the most discriminative features. Most existing UFS methods linearly project features into a pseudo-label space for clustering, but they suffer from two critical limitations: (1) an oversimplified linear mapping that fails to capture complex feature relationships, and (2) an assumption of uniform cluster distributions, ignoring outliers prevalent in real-world data. To address these issues, we propose the Robust Autoencoder-based Unsupervised Feature Selection (RAEUFS) model, which leverages a deep autoencoder to learn nonlinear feature representations while inherently improving robustness to outliers. We further develop an efficient optimization algorithm for RAEUFS. Extensive experiments demonstrate that our method outperforms state-of-the-art UFS approaches in both clean and outlier-contaminated data settings.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Counterfactual Basis Extension and Representational Geometry: An MDL-Constrained Model of Conceptual Growth</title>
<link>https://arxiv.org/abs/2512.18732</link>
<guid>https://arxiv.org/abs/2512.18732</guid>
<content:encoded><![CDATA[
arXiv:2512.18732v1 Announce Type: cross 
Abstract: Concept learning becomes possible only when existing representations fail to account for experience. Most models of learning and inference, however, presuppose a fixed representational basis within which belief updating occurs. In this paper, I address a prior question: under what structural conditions can the representational basis itself expand in a principled and selective way?
  I propose a geometric framework in which conceptual growth is modeled as admissible basis extension evaluated under a Minimum Description Length (MDL) criterion. Experience, whether externally observed or internally simulated, is represented as vectors relative to a current conceptual subspace. Residual components capture systematic representational failure, and candidate conceptual extensions are restricted to low-rank, admissible transformations. I show that any MDL-accepted extension can be chosen so that its novel directions lie entirely within the residual span induced by experience, while extensions orthogonal to this span strictly increase description length and are therefore rejected.
  This yields a conservative account of imagination and conceptual innovation. Internally generated counterfactual representations contribute to learning only insofar as they expose or amplify structured residual error, and cannot introduce arbitrary novelty. I further distinguish representational counterfactuals--counterfactuals over an agent's conceptual basis--from causal or value-level counterfactuals, and show how MDL provides a normative selection principle governing representational change.
  Overall, the framework characterizes conceptual development as an error-driven, geometry-constrained process of basis extension, clarifying both the role and the limits of imagination in learning and theory change.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search</title>
<link>https://arxiv.org/abs/2512.18745</link>
<guid>https://arxiv.org/abs/2512.18745</guid>
<content:encoded><![CDATA[
arXiv:2512.18745v1 Announce Type: cross 
Abstract: The ability for AI agents to "think with images" requires a sophisticated blend of reasoning and perception. However, current open multimodal agents still largely fall short on the reasoning aspect crucial for real-world tasks like analyzing documents with dense charts/diagrams and navigating maps. To address this gap, we introduce O3-Bench, a new benchmark designed to evaluate multimodal reasoning with interleaved attention to visual details. O3-Bench features challenging problems that require agents to piece together subtle visual information from distinct image areas through multi-step reasoning. The problems are highly challenging even for frontier systems like OpenAI o3, which only obtains 40.8% accuracy on O3-Bench. To make progress, we propose InSight-o3, a multi-agent framework consisting of a visual reasoning agent (vReasoner) and a visual search agent (vSearcher) for which we introduce the task of generalized visual search -- locating relational, fuzzy, or conceptual regions described in free-form language, beyond just simple objects or figures in natural images. We then present a multimodal LLM purpose-trained for this task via reinforcement learning. As a plug-and-play agent, our vSearcher empowers frontier multimodal models (as vReasoners), significantly improving their performance on a wide range of benchmarks. This marks a concrete step towards powerful o3-like open systems. Our code and dataset can be found at https://github.com/m-Just/InSight-o3 .
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Eff-GRot: Efficient and Generalizable Rotation Estimation with Transformers</title>
<link>https://arxiv.org/abs/2512.18784</link>
<guid>https://arxiv.org/abs/2512.18784</guid>
<content:encoded><![CDATA[
arXiv:2512.18784v1 Announce Type: cross 
Abstract: We introduce Eff-GRot, an approach for efficient and generalizable rotation estimation from RGB images. Given a query image and a set of reference images with known orientations, our method directly predicts the object's rotation in a single forward pass, without requiring object- or category-specific training. At the core of our framework is a transformer that performs a comparison in the latent space, jointly processing rotation-aware representations from multiple references alongside a query. This design enables a favorable balance between accuracy and computational efficiency while remaining simple, scalable, and fully end-to-end. Experimental results show that Eff-GRot offers a promising direction toward more efficient rotation estimation, particularly in latency-sensitive applications.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RIS-Enabled Smart Wireless Environments: Fundamentals and Distributed Optimization</title>
<link>https://arxiv.org/abs/2512.18788</link>
<guid>https://arxiv.org/abs/2512.18788</guid>
<content:encoded><![CDATA[
arXiv:2512.18788v1 Announce Type: cross 
Abstract: This chapter overviews the concept of Smart Wireless Environments (SWEs) motivated by the emerging technology of Reconfigurable Intelligent Surfaces (RISs). The operating principles and state-of-the-art hardware architectures of programmable metasurfaces are first introduced. Subsequently, key performance objectives and use cases of RIS-enabled SWEs, including spectral and energy efficiency, physical-layer security, integrated sensing and communications, as well as the emerging paradigm of over-the-air computing, are discussed. Focusing on the recent trend of Beyond-Diagonal (BD) RISs, two distributed designs of respective SWEs are presented. The first deals with a multi-user Multiple-Input Single-Output (MISO) system operating within the area of influence of a SWE comprising multiple BD-RISs. A hybrid distributed and fusion machine learning framework based on multi-branch attention-based convolutional Neural Networks (NNs), NN parameter sharing, and neuroevolutionary training is presented, which enables online mapping of channel realizations to the BD-RIS configurations as well as the multi-user transmit precoder. Performance evaluation results showcase that the distributedly optimized RIS-enabled SWE achieves near-optimal sum-rate performance with low online computational complexity. The second design focuses on the wideband interference MISO broadcast channel, where each base station exclusively controls one BD-RIS to serve its assigned group of users. A cooperative optimization framework that jointly designs the base station transmit precoders as well as the tunable capacitances and switch matrices of all metasurfaces is presented. Numerical results demonstrating the superior sum-rate performance of the designed RIS-enabled SWE for multi-cell MISO networks over benchmark schemes, considering non-cooperative configuration and conventional diagonal metasurfaces, are presented.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CORE: Concept-Oriented Reinforcement for Bridging the Definition-Application Gap in Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2512.18857</link>
<guid>https://arxiv.org/abs/2512.18857</guid>
<content:encoded><![CDATA[
arXiv:2512.18857v1 Announce Type: cross 
Abstract: Large language models (LLMs) often solve challenging math exercises yet fail to apply the concept right when the problem requires genuine understanding. Popular Reinforcement Learning with Verifiable Rewards (RLVR) pipelines reinforce final answers but provide little fine-grained conceptual signal, so models improve at pattern reuse rather than conceptual applications. We introduce CORE (Concept-Oriented REinforcement), an RL training framework that turns explicit concepts into a controllable supervision signal. Starting from a high-quality, low-contamination textbook resource that links verifiable exercises to concise concept descriptions, we run a sanity probe showing LLMs can restate definitions but fail concept-linked quizzes, quantifying the conceptual reasoning gap. CORE then (i) synthesizes concept-aligned quizzes, (ii) injects brief concept snippets during rollouts to elicit concept-primed trajectories, and (iii) reinforces conceptual reasoning via trajectory replacement after group failures, a lightweight forward-KL constraint that aligns unguided with concept-primed policies, or standard GRPO directly on concept-aligned quizzes. Across several models, CORE delivers consistent gains over vanilla and SFT baselines on both in-domain concept-exercise suites and diverse out-of-domain math benchmarks. CORE unifies direct training on concept-aligned quizzes and concept-injected rollouts under outcome regularization. It provides fine-grained conceptual supervision that bridges problem-solving competence and genuine conceptual reasoning, while remaining algorithm- and verifier-agnostic.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Application of deep learning approaches for medieval historical documents transcription</title>
<link>https://arxiv.org/abs/2512.18865</link>
<guid>https://arxiv.org/abs/2512.18865</guid>
<content:encoded><![CDATA[
arXiv:2512.18865v1 Announce Type: cross 
Abstract: Handwritten text recognition and optical character recognition solutions show excellent results with processing data of modern era, but efficiency drops with Latin documents of medieval times. This paper presents a deep learning method to extract text information from handwritten Latin-language documents of the 9th to 11th centuries. The approach takes into account the properties inherent in medieval documents. The paper provides a brief introduction to the field of historical document transcription, a first-sight analysis of the raw data, and the related works and studies. The paper presents the steps of dataset development for further training of the models. The explanatory data analysis of the processed data is provided as well. The paper explains the pipeline of deep learning models to extract text information from the document images, from detecting objects to word recognition using classification models and embedding word images. The paper reports the following results: recall, precision, F1 score, intersection over union, confusion matrix, and mean string distance. The plots of the metrics are also included. The implementation is published on the GitHub repository.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structural Reinforcement Learning for Heterogeneous Agent Macroeconomics</title>
<link>https://arxiv.org/abs/2512.18892</link>
<guid>https://arxiv.org/abs/2512.18892</guid>
<content:encoded><![CDATA[
arXiv:2512.18892v1 Announce Type: cross 
Abstract: We present a new approach to formulating and solving heterogeneous agent models with aggregate risk. We replace the cross-sectional distribution with low-dimensional prices as state variables and let agents learn equilibrium price dynamics directly from simulated paths. To do so, we introduce a structural reinforcement learning (SRL) method which treats prices via simulation while exploiting agents' structural knowledge of their own individual dynamics. Our SRL method yields a general and highly efficient global solution method for heterogeneous agent models that sidesteps the Master equation and handles problems traditional methods struggle with, in particular nontrivial market-clearing conditions. We illustrate the approach in the Krusell-Smith model, the Huggett model with aggregate shocks, and a HANK model with a forward-looking Phillips curve, all of which we solve globally within minutes.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gabliteration: Adaptive Multi-Directional Neural Weight Modification for Selective Behavioral Alteration in Large Language Models</title>
<link>https://arxiv.org/abs/2512.18901</link>
<guid>https://arxiv.org/abs/2512.18901</guid>
<content:encoded><![CDATA[
arXiv:2512.18901v1 Announce Type: cross 
Abstract: We present Gabliteration, a novel neural weight modification technique that advances beyond traditional abliteration methods by implementing adaptive multi-directional projections with regularized layer selection. Our approach addresses the fundamental limitation of existing methods that compromise model quality while attempting to modify specific behavioral patterns. Through dynamic layer optimization, regularized projection matrices, and adaptive scaling mechanisms, we achieve theoretically superior weight modification while minimizing quality degradation in unrelated domains. We validate our method through the gabliterated-v1 model series (0.6B to 4B parameters) available on Hugging Face, demonstrating practical applicability across multiple model scales.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Multimodal Large Reasoning Models Needs Better Thoughts: A Three-Stage Framework for Long Chain-of-Thought Synthesis and Selection</title>
<link>https://arxiv.org/abs/2512.18956</link>
<guid>https://arxiv.org/abs/2512.18956</guid>
<content:encoded><![CDATA[
arXiv:2512.18956v1 Announce Type: cross 
Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex reasoning tasks through long Chain-of-Thought (CoT) reasoning. Extending these successes to multimodal reasoning remains challenging due to the increased complexity of integrating diverse input modalities and the scarcity of high-quality long CoT training data. Existing multimodal datasets and CoT synthesis methods still suffer from limited reasoning depth, modality conversion errors, and rigid generation pipelines, hindering model performance and stability. To this end, in this paper, we propose SynSelect, a novel three-stage Synthesis-Selection framework for generating high-quality long CoT data tailored to multimodal reasoning tasks. Specifically, SynSelect first leverages multiple heterogeneous multimodal LRMs to produce diverse candidate CoTs, and then applies both instance and batch level selection to filter high-quality CoTs that can effectively enhance the model's reasoning capabilities. Extensive experiments on multiple multimodal benchmarks demonstrate that models supervised fine-tuned on SynSelect-generated data significantly outperform baselines and achieve further improvements after reinforcement learning post-training. Our results validate SynSelect as an effective approach for advancing multimodal LRMs reasoning capabilities.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Conditional Stochastic Interpolation for Generative Nonlinear Sufficient Dimension Reduction</title>
<link>https://arxiv.org/abs/2512.18971</link>
<guid>https://arxiv.org/abs/2512.18971</guid>
<content:encoded><![CDATA[
arXiv:2512.18971v1 Announce Type: cross 
Abstract: Identifying low-dimensional sufficient structures in nonlinear sufficient dimension reduction (SDR) has long been a fundamental yet challenging problem. Most existing methods lack theoretical guarantees of exhaustiveness in identifying lower dimensional structures, either at the population level or at the sample level. We tackle this issue by proposing a new method, generative sufficient dimension reduction (GenSDR), which leverages modern generative models. We show that GenSDR is able to fully recover the information contained in the central $\sigma$-field at both the population and sample levels. In particular, at the sample level, we establish a consistency property for the GenSDR estimator from the perspective of conditional distributions, capitalizing on the distributional learning capabilities of deep generative models. Moreover, by incorporating an ensemble technique, we extend GenSDR to accommodate scenarios with non-Euclidean responses, thereby substantially broadening its applicability. Extensive numerical results demonstrate the outstanding empirical performance of GenSDR and highlight its strong potential for addressing a wide range of complex, real-world tasks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ORPR: An OR-Guided Pretrain-then-Reinforce Learning Model for Inventory Management</title>
<link>https://arxiv.org/abs/2512.19001</link>
<guid>https://arxiv.org/abs/2512.19001</guid>
<content:encoded><![CDATA[
arXiv:2512.19001v1 Announce Type: cross 
Abstract: As the pursuit of synergy between Artificial Intelligence (AI) and Operations Research (OR) gains momentum in handling complex inventory systems, a critical challenge persists: how to effectively reconcile AI's adaptive perception with OR's structural rigor. To bridge this gap, we propose a novel OR-Guided "Pretrain-then-Reinforce" framework. To provide structured guidance, we propose a simulation-augmented OR model that generates high-quality reference decisions, implicitly capturing complex business constraints and managerial preferences. Leveraging these OR-derived decisions as foundational training labels, we design a domain-informed deep learning foundation model to establish foundational decision-making capabilities, followed by a reinforcement learning (RL) fine-tuning stage. Uniquely, we position RL as a deep alignment mechanism that enables the AI agent to internalize the optimality principles of OR, while simultaneously leveraging exploration for general policy refinement and allowing expert guidance for scenario-specific adaptation (e.g., promotional events). Validated through extensive numerical experiments and a field deployment at JD.com augmented by a Difference-in-Differences (DiD) analysis, our model significantly outperforms incumbent industrial practices, delivering real-world gains of a 5.27-day reduction in turnover and a 2.29% increase in in-stock rates, alongside a 29.95% decrease in holding costs. Contrary to the prevailing trend of brute-force model scaling, our study demonstrates that a lightweight, domain-informed model can deliver state-of-the-art performance and robust transferability when guided by structured OR logic. This approach offers a scalable and cost-effective paradigm for intelligent supply chain management, highlighting the value of deeply aligning AI with OR.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context-Aware Initialization for Reducing Generative Path Length in Diffusion Language Models</title>
<link>https://arxiv.org/abs/2512.19004</link>
<guid>https://arxiv.org/abs/2512.19004</guid>
<content:encoded><![CDATA[
arXiv:2512.19004v1 Announce Type: cross 
Abstract: Diffusion Large Language Models (DLLMs) enable fully parallel token decoding but often remain impractical at inference time due to the many denoising iterations required to refine an information-free, fully masked initialization into coherent text. Most existing acceleration methods focus on traversing this generative trajectory more efficiently via improved solvers or sampling strategies. We advance a complementary perspective: shorten the trajectory itself by starting closer to the target distribution through context-aware initialization.
  We propose a training-free interface that injects prompt-conditioned priors from a lightweight auxiliary model into the diffusion initialization, and instantiate it with two mechanisms: discrete token injection and representation-level embedding interpolation. Because injected priors can be imperfect and unmask-only decoding can over-commit early, we also introduce a simple confidence-based remasking mechanism as a form of prior skepticism. Preliminary evidence on GSM8K suggests that context-aware initialization can substantially reduce denoising iterations (about 35\% fewer function evaluations in our setting), while also exposing a key open challenge: naive warm-starting can degrade final accuracy relative to strong diffusion baselines. We use these findings to motivate a research agenda around calibration, revision mechanisms, and representation alignment for reliable warm-started diffusion decoding.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Jailbreak Mitigation Using Semantic Linear Classification in a Multi-Staged Pipeline</title>
<link>https://arxiv.org/abs/2512.19011</link>
<guid>https://arxiv.org/abs/2512.19011</guid>
<content:encoded><![CDATA[
arXiv:2512.19011v1 Announce Type: cross 
Abstract: Prompt injection and jailbreaking attacks pose persistent security challenges to large language model (LLM)-based systems. We present an efficient and systematically evaluated defense architecture that mitigates these threats through a lightweight, multi-stage pipeline. Its core component is a semantic filter based on text normalization, TF-IDF representations, and a Linear SVM classifier. Despite its simplicity, this module achieves 93.4% accuracy and 96.5% specificity on held-out data, substantially reducing attack throughput while incurring negligible computational overhead.
  Building on this efficient foundation, the full pipeline integrates complementary detection and mitigation mechanisms that operate at successive stages, providing strong robustness with minimal latency. In comparative experiments, our SVM-based configuration improves overall accuracy from 35.1% to 93.4% while reducing average time to completion from approximately 450s to 47s, yielding over 10 times lower latency than ShieldGemma. These results demonstrate that the proposed design simultaneously advances defensive precision and efficiency, addressing a core limitation of current model-based moderators.
  Evaluation across a curated corpus of over 30,000 labeled prompts, including benign, jailbreak, and application-layer injections, confirms that staged, resource-efficient defenses can robustly secure modern LLM-driven applications.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CETCAM: Camera-Controllable Video Generation via Consistent and Extensible Tokenization</title>
<link>https://arxiv.org/abs/2512.19020</link>
<guid>https://arxiv.org/abs/2512.19020</guid>
<content:encoded><![CDATA[
arXiv:2512.19020v1 Announce Type: cross 
Abstract: Achieving precise camera control in video generation remains challenging, as existing methods often rely on camera pose annotations that are difficult to scale to large and dynamic datasets and are frequently inconsistent with depth estimation, leading to train-test discrepancies. We introduce CETCAM, a camera-controllable video generation framework that eliminates the need for camera annotations through a consistent and extensible tokenization scheme. CETCAM leverages recent advances in geometry foundation models, such as VGGT, to estimate depth and camera parameters and converts them into unified, geometry-aware tokens. These tokens are seamlessly integrated into a pretrained video diffusion backbone via lightweight context blocks. Trained in two progressive stages, CETCAM first learns robust camera controllability from diverse raw video data and then refines fine-grained visual quality using curated high-fidelity datasets. Extensive experiments across multiple benchmarks demonstrate state-of-the-art geometric consistency, temporal stability, and visual realism. Moreover, CETCAM exhibits strong adaptability to additional control modalities, including inpainting and layout control, highlighting its flexibility beyond camera control. The project page is available at https://sjtuytc.github.io/CETCam_project_page.github.io/.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Erasure Illusion: Stress-Testing the Generalization of LLM Forgetting Evaluation</title>
<link>https://arxiv.org/abs/2512.19025</link>
<guid>https://arxiv.org/abs/2512.19025</guid>
<content:encoded><![CDATA[
arXiv:2512.19025v1 Announce Type: cross 
Abstract: Machine unlearning aims to remove specific data influences from trained models, a capability essential for adhering to copyright laws and ensuring AI safety. Current unlearning metrics typically measure success by monitoring the model's performance degradation on the specific unlearning dataset ($D_u$). We argue that for Large Language Models (LLMs), this evaluation paradigm is insufficient and potentially misleading. Many real-world uses of unlearning--motivated by copyright or safety--implicitly target not only verbatim content in $D_u$, but also behaviors influenced by the broader generalizations the model derived from it. We demonstrate that LLMs can pass standard unlearning evaluation and appear to have ``forgotten'' the target knowledge, while simultaneously retaining strong capabilities on content that is semantically adjacent to $D_u$. This phenomenon indicates that erasing exact sentences does not necessarily equate to removing the underlying knowledge. To address this gap, we propose \name, an automated stress-testing framework that generates a surrogate dataset, $\tilde{D}_u$. This surrogate set is constructed to be semantically derived from $D_u$ yet sufficiently distinct in embedding space. By comparing unlearning metric scores between $D_u$ and $\tilde{D}_u$, we can stress-test the reliability of the metric itself. Our extensive evaluation across three LLM families (Llama-3-8B, Qwen2.5-7B, and Zephyr-7B-$\beta$), three distinct datasets, and seven standard metrics reveals widespread inconsistencies. We find that current metrics frequently overestimate unlearning success, failing to detect retained knowledge exposed by our stress-test datasets.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Recontextualization Mitigates Specification Gaming without Modifying the Specification</title>
<link>https://arxiv.org/abs/2512.19027</link>
<guid>https://arxiv.org/abs/2512.19027</guid>
<content:encoded><![CDATA[
arXiv:2512.19027v1 Announce Type: cross 
Abstract: Developers often struggle to specify correct training labels and rewards. Perhaps they don't need to. We propose recontextualization, which reduces how often language models "game" training signals, performing misbehaviors those signals mistakenly reinforce. We show recontextualization prevents models from learning to 1) prioritize evaluation metrics over chat response quality; 2) special-case code to pass incorrect tests; 3) lie to users; and 4) become sycophantic. Our method works by generating completions from prompts discouraging misbehavior and then recontextualizing them as though they were in response to prompts permitting misbehavior. Recontextualization trains language models to resist misbehavior even when instructions permit it. This mitigates the reinforcement of misbehavior from misspecified training signals, reducing specification gaming without improving the supervision signal.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Elevating Intrusion Detection and Security Fortification in Intelligent Networks through Cutting-Edge Machine Learning Paradigms</title>
<link>https://arxiv.org/abs/2512.19037</link>
<guid>https://arxiv.org/abs/2512.19037</guid>
<content:encoded><![CDATA[
arXiv:2512.19037v1 Announce Type: cross 
Abstract: The proliferation of IoT devices and their reliance on Wi-Fi networks have introduced significant security vulnerabilities, particularly the KRACK and Kr00k attacks, which exploit weaknesses in WPA2 encryption to intercept and manipulate sensitive data. Traditional IDS using classifiers face challenges such as model overfitting, incomplete feature extraction, and high false positive rates, limiting their effectiveness in real-world deployments. To address these challenges, this study proposes a robust multiclass machine learning based intrusion detection framework. The methodology integrates advanced feature selection techniques to identify critical attributes, mitigating redundancy and enhancing detection accuracy. Two distinct ML architectures are implemented: a baseline classifier pipeline and a stacked ensemble model combining noise injection, Principal Component Analysis (PCA), and meta learning to improve generalization and reduce false positives. Evaluated on the AWID3 data set, the proposed ensemble architecture achieves superior performance, with an accuracy of 98%, precision of 98%, recall of 98%, and a false positive rate of just 2%, outperforming existing state-of-the-art methods. This work demonstrates the efficacy of combining preprocessing strategies with ensemble learning to fortify network security against sophisticated Wi-Fi attacks, offering a scalable and reliable solution for IoT environments. Future directions include real-time deployment and adversarial resilience testing to further enhance the model's adaptability.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Cost-Aware Sequential Hypothesis Testing with Random Costs and Action Cancellation</title>
<link>https://arxiv.org/abs/2512.19067</link>
<guid>https://arxiv.org/abs/2512.19067</guid>
<content:encoded><![CDATA[
arXiv:2512.19067v1 Announce Type: cross 
Abstract: We study a variant of cost-aware sequential hypothesis testing in which a single active Decision Maker (DM) selects actions with positive, random costs to identify the true hypothesis under an average error constraint, while minimizing the expected total cost. The DM may abort an in-progress action, yielding no sample, by truncating its realized cost at a smaller, tunable deterministic limit, which we term a per-action deadline. We analyze how this cancellation option can be exploited under two cost-revelation models: ex-post, where the cost is revealed only after the sample is obtained, and ex-ante, where the cost accrues before sample acquisition.
  In the ex-post model, per-action deadlines do not affect the expected total cost, and the cost-error tradeoffs coincide with the baseline obtained by replacing deterministic costs with cost means. In the ex-ante model, we show how per-action deadlines inflate the expected number of times actions are applied, and that the resulting expected total cost can be reduced to the constant-cost setting by introducing an effective per-action cost. We characterize when deadlines are beneficial and study several families in detail.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Auditing Significance, Metric Choice, and Demographic Fairness in Medical AI Challenges</title>
<link>https://arxiv.org/abs/2512.19091</link>
<guid>https://arxiv.org/abs/2512.19091</guid>
<content:encoded><![CDATA[
arXiv:2512.19091v1 Announce Type: cross 
Abstract: Open challenges have become the de facto standard for comparative ranking of medical AI methods. Despite their importance, medical AI leaderboards exhibit three persistent limitations: (1) score gaps are rarely tested for statistical significance, so rank stability is unknown; (2) single averaged metrics are applied to every organ, hiding clinically important boundary errors; (3) performance across intersecting demographics is seldom reported, masking fairness and equity gaps. We introduce RankInsight, an open-source toolkit that seeks to address these limitations. RankInsight (1) computes pair-wise significance maps that show the nnU-Net family outperforms Vision-Language and MONAI submissions with high statistical certainty; (2) recomputes leaderboards with organ-appropriate metrics, reversing the order of the top four models when Dice is replaced by NSD for tubular structures; and (3) audits intersectional fairness, revealing that more than half of the MONAI-based entries have the largest gender-race discrepancy on our proprietary Johns Hopkins Hospital dataset. The RankInsight toolkit is publicly released and can be directly applied to past, ongoing, and future challenges. It enables organizers and participants to publish rankings that are statistically sound, clinically meaningful, and demographically fair.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explicit and Non-asymptotic Query Complexities of Rank-Based Zeroth-order Algorithm on Stochastic Smooth Functions</title>
<link>https://arxiv.org/abs/2512.19104</link>
<guid>https://arxiv.org/abs/2512.19104</guid>
<content:encoded><![CDATA[
arXiv:2512.19104v1 Announce Type: cross 
Abstract: Zeroth-order (ZO) optimization with ordinal feedback has emerged as a fundamental problem in modern machine learning systems, particularly in human-in-the-loop settings such as reinforcement learning from human feedback, preference learning, and evolutionary strategies. While rank-based ZO algorithms enjoy strong empirical success and robustness properties, their theoretical understanding, especially under stochastic objectives and standard smoothness assumptions, remains limited. In this paper, we study rank-based zeroth-order optimization for stochastic functions where only ordinal feedback of the stochastic function is available. We propose a simple and computationally efficient rank-based ZO algorithm. Under standard assumptions including smoothness, strong convexity, and bounded second moments of stochastic gradients, we establish explicit non-asymptotic query complexity bounds for both convex and nonconvex objectives. Notably, our results match the best-known query complexities of value-based ZO algorithms, demonstrating that ordinal information alone is sufficient for optimal query efficiency in stochastic settings. Our analysis departs from existing drift-based and information-geometric techniques, offering new tools for the study of rank-based optimization under noise. These findings narrow the gap between theory and practice and provide a principled foundation for optimization driven by human preferences.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAP: Syntactic Attention Pruning for Transformer-based Language Models</title>
<link>https://arxiv.org/abs/2512.19125</link>
<guid>https://arxiv.org/abs/2512.19125</guid>
<content:encoded><![CDATA[
arXiv:2512.19125v1 Announce Type: cross 
Abstract: This paper introduces Syntactic Attention Pruning (SAP), a novel method for effectively pruning attention heads in Transformer models. Unlike conventional approaches that rely solely on mathematical analysis of model weights and activations, SAP incorporates both the syntactic structure and attention patterns of sentences to guide the pruning process. By leveraging these linguistic features, SAP not only achieves performance comparable to state-of-the-art methods but also enhances the interpretability of model behavior. To further improve robustness, we propose Candidate Filtering (CF), a mechanism that prioritizes heads based on their contribution to model performance, mitigating degradation during pruning. Experimental results indicate that SAP effectively preserves critical heads of a high density of strong attention values, outperforming existing head pruning strategies in retrain-free settings. These findings position SAP as a promising foundation for a new direction in model compression research, offering high flexibility for pruning across all transformer-based language models.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evidential Trust-Aware Model Personalization in Decentralized Federated Learning for Wearable IoT</title>
<link>https://arxiv.org/abs/2512.19131</link>
<guid>https://arxiv.org/abs/2512.19131</guid>
<content:encoded><![CDATA[
arXiv:2512.19131v1 Announce Type: cross 
Abstract: Decentralized federated learning (DFL) enables collaborative model training across edge devices without centralized coordination, offering resilience against single points of failure. However, statistical heterogeneity arising from non-identically distributed local data creates a fundamental challenge: nodes must learn personalized models adapted to their local distributions while selectively collaborating with compatible peers. Existing approaches either enforce a single global model that fits no one well, or rely on heuristic peer selection mechanisms that cannot distinguish between peers with genuinely incompatible data distributions and those with valuable complementary knowledge. We present Murmura, a framework that leverages evidential deep learning to enable trust-aware model personalization in DFL. Our key insight is that epistemic uncertainty from Dirichlet-based evidential models directly indicates peer compatibility: high epistemic uncertainty when a peer's model evaluates local data reveals distributional mismatch, enabling nodes to exclude incompatible influence while maintaining personalized models through selective collaboration. Murmura introduces a trust-aware aggregation mechanism that computes peer compatibility scores through cross-evaluation on local validation samples and personalizes model aggregation based on evidential trust with adaptive thresholds. Evaluation on three wearable IoT datasets (UCI HAR, PAMAP2, PPG-DaLiA) demonstrates that Murmura reduces performance degradation from IID to non-IID conditions compared to baseline (0.9% vs. 19.3%), achieves 7.4$\times$ faster convergence, and maintains stable accuracy across hyperparameter choices. These results establish evidential uncertainty as a principled foundation for compatibility-aware personalization in decentralized heterogeneous environments.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Finite-sample guarantees for data-driven forward-backward operator methods</title>
<link>https://arxiv.org/abs/2512.19172</link>
<guid>https://arxiv.org/abs/2512.19172</guid>
<content:encoded><![CDATA[
arXiv:2512.19172v1 Announce Type: cross 
Abstract: We establish finite sample certificates on the quality of solutions produced by data-based forward-backward (FB) operator splitting schemes. As frequently happens in stochastic regimes, we consider the problem of finding a zero of the sum of two operators, where one is either unavailable in closed form or computationally expensive to evaluate, and shall therefore be approximated using a finite number of noisy oracle samples. Under the lens of algorithmic stability, we then derive probabilistic bounds on the distance between a true zero and the FB output without making specific assumptions about the underlying data distribution. We show that under weaker conditions ensuring the convergence of FB schemes, stability bounds grow proportionally to the number of iterations. Conversely, stronger assumptions yield stability guarantees that are independent of the iteration count. We then specialize our results to a popular FB stochastic Nash equilibrium seeking algorithm and validate our theoretical bounds on a control problem for smart grids, where the energy price uncertainty is approximated by means of historical data.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PEDESTRIAN: An Egocentric Vision Dataset for Obstacle Detection on Pavements</title>
<link>https://arxiv.org/abs/2512.19190</link>
<guid>https://arxiv.org/abs/2512.19190</guid>
<content:encoded><![CDATA[
arXiv:2512.19190v1 Announce Type: cross 
Abstract: Walking has always been a primary mode of transportation and is recognized as an essential activity for maintaining good health. Despite the need for safe walking conditions in urban environments, sidewalks are frequently obstructed by various obstacles that hinder free pedestrian movement. Any object obstructing a pedestrian's path can pose a safety hazard. The advancement of pervasive computing and egocentric vision techniques offers the potential to design systems that can automatically detect such obstacles in real time, thereby enhancing pedestrian safety. The development of effective and efficient identification algorithms relies on the availability of comprehensive and well-balanced datasets of egocentric data. In this work, we introduce the PEDESTRIAN dataset, comprising egocentric data for 29 different obstacles commonly found on urban sidewalks. A total of 340 videos were collected using mobile phone cameras, capturing a pedestrian's point of view. Additionally, we present the results of a series of experiments that involved training several state-of-the-art deep learning algorithms using the proposed dataset, which can be used as a benchmark for obstacle detection and recognition tasks. The dataset can be used for training pavement obstacle detectors to enhance the safety of pedestrians in urban areas.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Consistent Probability Flow for High-Dimensional Fokker-Planck Equations</title>
<link>https://arxiv.org/abs/2512.19196</link>
<guid>https://arxiv.org/abs/2512.19196</guid>
<content:encoded><![CDATA[
arXiv:2512.19196v1 Announce Type: cross 
Abstract: Solving high-dimensional Fokker-Planck (FP) equations is a challenge in computational physics and stochastic dynamics, due to the curse of dimensionality (CoD) and the bottleneck of evaluating second-order diffusion terms. Existing deep learning approaches, such as Physics-Informed Neural Networks (PINNs), face computational challenges as dimensionality increases, driven by the $O(D^2)$ complexity of automatic differentiation for second-order derivatives. While recent probability flow approaches bypass this by learning score functions or matching velocity fields, they often involve serial computational operations or depend on sampling efficiency in complex distributions. To address these issues, we propose the Self-Consistent Probability Flow (SCPF) method. We reformulate the second-order FP equation into an equivalent first-order deterministic Probability Flow ODE (PF-ODE) constraint. Unlike score matching or velocity matching, SCPF solves this problem by minimizing the residual of the PF-ODE continuity equation, which avoids explicit Hessian computation. We leverage Continuous Normalizing Flows (CNF) combined with the Hutchinson Trace Estimator (HTE) to reduce the training complexity to linear scale $O(D)$, achieving an effective $O(1)$ wall-clock time on GPUs. To address data sparsity in high dimensions, we apply a generative adaptive sampling strategy and theoretically prove that dynamically aligning collocation points with the evolving probability mass is a necessary condition to bound the approximation error. Experiments on diverse benchmarks -- ranging from anisotropic Ornstein-Uhlenbeck (OU) processes and high-dimensional Brownian motions with time-varying diffusion terms, to Geometric OU processes featuring non-Gaussian solutions -- demonstrate that SCPF effectively mitigates the CoD, maintaining high accuracy and constant computational cost for problems up to 100 dimensions.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Identifying Features Associated with Bias Against 93 Stigmatized Groups in Language Models and Guardrail Model Safety Mitigation</title>
<link>https://arxiv.org/abs/2512.19238</link>
<guid>https://arxiv.org/abs/2512.19238</guid>
<content:encoded><![CDATA[
arXiv:2512.19238v1 Announce Type: cross 
Abstract: Large language models (LLMs) have been shown to exhibit social bias, however, bias towards non-protected stigmatized identities remain understudied. Furthermore, what social features of stigmas are associated with bias in LLM outputs is unknown. From psychology literature, it has been shown that stigmas contain six shared social features: aesthetics, concealability, course, disruptiveness, origin, and peril. In this study, we investigate if human and LLM ratings of the features of stigmas, along with prompt style and type of stigma, have effect on bias towards stigmatized groups in LLM outputs. We measure bias against 93 stigmatized groups across three widely used LLMs (Granite 3.0-8B, Llama-3.1-8B, Mistral-7B) using SocialStigmaQA, a benchmark that includes 37 social scenarios about stigmatized identities; for example deciding wether to recommend them for an internship. We find that stigmas rated by humans to be highly perilous (e.g., being a gang member or having HIV) have the most biased outputs from SocialStigmaQA prompts (60% of outputs from all models) while sociodemographic stigmas (e.g. Asian-American or old age) have the least amount of biased outputs (11%). We test if the amount of biased outputs could be decreased by using guardrail models, models meant to identify harmful input, using each LLM's respective guardrail model (Granite Guardian 3.0, Llama Guard 3.0, Mistral Moderation API). We find that bias decreases significantly by 10.4%, 1.4%, and 7.8%, respectively. However, we show that features with significant effect on bias remain unchanged post-mitigation and that guardrail models often fail to recognize the intent of bias in prompts. This work has implications for using LLMs in scenarios involving stigmatized groups and we suggest future work towards improving guardrail models for bias mitigation.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Translating Flow to Policy via Hindsight Online Imitation</title>
<link>https://arxiv.org/abs/2512.19269</link>
<guid>https://arxiv.org/abs/2512.19269</guid>
<content:encoded><![CDATA[
arXiv:2512.19269v1 Announce Type: cross 
Abstract: Recent advances in hierarchical robot systems leverage a high-level planner to propose task plans and a low-level policy to generate robot actions. This design allows training the planner on action-free or even non-robot data sources (e.g., videos), providing transferable high-level guidance. Nevertheless, grounding these high-level plans into executable actions remains challenging, especially with the limited availability of high-quality robot data. To this end, we propose to improve the low-level policy through online interactions. Specifically, our approach collects online rollouts, retrospectively annotates the corresponding high-level goals from achieved outcomes, and aggregates these hindsight-relabeled experiences to update a goal-conditioned imitation policy. Our method, Hindsight Flow-conditioned Online Imitation (HinFlow), instantiates this idea with 2D point flows as the high-level planner. Across diverse manipulation tasks in both simulation and physical world, our method achieves more than $2\times$ performance improvement over the base policy, significantly outperforming the existing methods. Moreover, our framework enables policy acquisition from planners trained on cross-embodiment video data, demonstrating its potential for scalable and transferable robot learning.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GShield: Mitigating Poisoning Attacks in Federated Learning</title>
<link>https://arxiv.org/abs/2512.19286</link>
<guid>https://arxiv.org/abs/2512.19286</guid>
<content:encoded><![CDATA[
arXiv:2512.19286v1 Announce Type: cross 
Abstract: Federated Learning (FL) has recently emerged as a revolutionary approach to collaborative training Machine Learning models. In particular, it enables decentralized model training while preserving data privacy, but its distributed nature makes it highly vulnerable to a severe attack known as Data Poisoning. In such scenarios, malicious clients inject manipulated data into the training process, thereby degrading global model performance or causing targeted misclassification. In this paper, we present a novel defense mechanism called GShield, designed to detect and mitigate malicious and low-quality updates, especially under non-independent and identically distributed (non-IID) data scenarios. GShield operates by learning the distribution of benign gradients through clustering and Gaussian modeling during an initial round, enabling it to establish a reliable baseline of trusted client behavior. With this benign profile, GShield selectively aggregates only those updates that align with the expected gradient patterns, effectively isolating adversarial clients and preserving the integrity of the global model. An extensive experimental campaign demonstrates that our proposed defense significantly improves model robustness compared to the state-of-the-art methods while maintaining a high accuracy of performance across both tabular and image datasets. Furthermore, GShield improves the accuracy of the targeted class by 43\% to 65\% after detecting malicious and low-quality clients.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Orthogonal Approximate Message Passing with Optimal Spectral Initializations for Rectangular Spiked Matrix Models</title>
<link>https://arxiv.org/abs/2512.19334</link>
<guid>https://arxiv.org/abs/2512.19334</guid>
<content:encoded><![CDATA[
arXiv:2512.19334v1 Announce Type: cross 
Abstract: We propose an orthogonal approximate message passing (OAMP) algorithm for signal estimation in the rectangular spiked matrix model with general rotationally invariant (RI) noise. We establish a rigorous state evolution that precisely characterizes the algorithm's high-dimensional dynamics and enables the construction of iteration-wise optimal denoisers. Within this framework, we accommodate spectral initializations under minimal assumptions on the empirical noise spectrum. In the rectangular setting, where a single rank-one component typically generates multiple informative outliers, we further propose a procedure for combining these outliers under mild non-Gaussian signal assumptions. For general RI noise models, the predicted performance of the proposed optimal OAMP algorithm agrees with replica-symmetric predictions for the associated Bayes-optimal estimator, and we conjecture that it is statistically optimal within a broad class of iterative estimation methods.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Faster Distributed Inference-Only Recommender Systems via Bounded Lag Synchronous Collectives</title>
<link>https://arxiv.org/abs/2512.19342</link>
<guid>https://arxiv.org/abs/2512.19342</guid>
<content:encoded><![CDATA[
arXiv:2512.19342v1 Announce Type: cross 
Abstract: Recommender systems are enablers of personalized content delivery, and therefore revenue, for many large companies. In the last decade, deep learning recommender models (DLRMs) are the de-facto standard in this field. The main bottleneck in DLRM inference is the lookup of sparse features across huge embedding tables, which are usually partitioned across the aggregate RAM of many nodes. In state-of-the-art recommender systems, the distributed lookup is implemented via irregular all-to-all (alltoallv) communication, and often presents the main bottleneck. Today, most related work sees this operation as a given; in addition, every collective is synchronous in nature. In this work, we propose a novel bounded lag synchronous (BLS) version of the alltoallv operation. The bound can be a parameter allowing slower processes to lag behind entire iterations before the fastest processes block. In special applications such as inference-only DLRM, the accuracy of the application is fully preserved. We implement BLS alltoallv in a new PyTorch Distributed backend and evaluate it with a BLS version of the reference DLRM code. We show that for well balanced, homogeneous-access DLRM runs our BLS technique does not offer notable advantages. But for unbalanced runs, e.g. runs with strongly irregular embedding table accesses or with delays across different processes, our BLS technique improves both the latency and throughput of inference-only DLRM. In the best-case scenario, the proposed reduced synchronisation can mask the delays across processes altogether.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VIGOR+: Iterative Confounder Generation and Validation via LLM-CEVAE Feedback Loop</title>
<link>https://arxiv.org/abs/2512.19349</link>
<guid>https://arxiv.org/abs/2512.19349</guid>
<content:encoded><![CDATA[
arXiv:2512.19349v1 Announce Type: cross 
Abstract: Hidden confounding remains a fundamental challenge in causal inference from observational data. Recent advances leverage Large Language Models (LLMs) to generate plausible hidden confounders based on domain knowledge, yet a critical gap exists: LLM-generated confounders often exhibit semantic plausibility without statistical utility. We propose VIGOR+ (Variational Information Gain for iterative cOnfounder Refinement), a novel framework that closes the loop between LLM-based confounder generation and CEVAE-based statistical validation. Unlike prior approaches that treat generation and validation as separate stages, VIGOR+ establishes an iterative feedback mechanism: validation signals from CEVAE (including information gain, latent consistency metrics, and diagnostic messages) are transformed into natural language feedback that guides subsequent LLM generation rounds. This iterative refinement continues until convergence criteria are met. We formalize the feedback mechanism, prove convergence properties under mild assumptions, and provide a complete algorithmic framework.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning General Policies with Policy Gradient Methods</title>
<link>https://arxiv.org/abs/2512.19366</link>
<guid>https://arxiv.org/abs/2512.19366</guid>
<content:encoded><![CDATA[
arXiv:2512.19366v1 Announce Type: cross 
Abstract: While reinforcement learning methods have delivered remarkable results in a number of settings, generalization, i.e., the ability to produce policies that generalize in a reliable and systematic way, has remained a challenge. The problem of generalization has been addressed formally in classical planning where provable correct policies that generalize over all instances of a given domain have been learned using combinatorial methods. The aim of this work is to bring these two research threads together to illuminate the conditions under which (deep) reinforcement learning approaches, and in particular, policy optimization methods, can be used to learn policies that generalize like combinatorial methods do. We draw on lessons learned from previous combinatorial and deep learning approaches, and extend them in a convenient way. From the former, we model policies as state transition classifiers, as (ground) actions are not general and change from instance to instance. From the latter, we use graph neural networks (GNNs) adapted to deal with relational structures for representing value functions over planning states, and in our case, policies. With these ingredients in place, we find that actor-critic methods can be used to learn policies that generalize almost as well as those obtained using combinatorial approaches while avoiding the scalability bottleneck and the use of feature pools. Moreover, the limitations of the DRL methods on the benchmarks considered have little to do with deep learning or reinforcement learning algorithms, and result from the well-understood expressive limitations of GNNs, and the tradeoff between optimality and generalization (general policies cannot be optimal in some domains). Both of these limitations are addressed without changing the basic DRL methods by adding derived predicates and an alternative cost structure to optimize.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cluster-Based Generalized Additive Models Informed by Random Fourier Features</title>
<link>https://arxiv.org/abs/2512.19373</link>
<guid>https://arxiv.org/abs/2512.19373</guid>
<content:encoded><![CDATA[
arXiv:2512.19373v1 Announce Type: cross 
Abstract: Explainable machine learning aims to strike a balance between prediction accuracy and model transparency, particularly in settings where black-box predictive models, such as deep neural networks or kernel-based methods, achieve strong empirical performance but remain difficult to interpret. This work introduces a mixture of generalized additive models (GAMs) in which random Fourier feature (RFF) representations are leveraged to uncover locally adaptive structure in the data. In the proposed method, an RFF-based embedding is first learned and then compressed via principal component analysis. The resulting low-dimensional representations are used to perform soft clustering of the data through a Gaussian mixture model. These cluster assignments are then applied to construct a mixture-of-GAMs framework, where each local GAM captures nonlinear effects through interpretable univariate smooth functions. Numerical experiments on real-world regression benchmarks, including the California Housing, NASA Airfoil Self-Noise, and Bike Sharing datasets, demonstrate improved predictive performance relative to classical interpretable models. Overall, this construction provides a principled approach for integrating representation learning with transparent statistical modeling.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Critical Assessment of Pattern Comparisons Between POD and Autoencoders in Intraventricular Flows</title>
<link>https://arxiv.org/abs/2512.19376</link>
<guid>https://arxiv.org/abs/2512.19376</guid>
<content:encoded><![CDATA[
arXiv:2512.19376v1 Announce Type: cross 
Abstract: Understanding intraventricular hemodynamics requires compact and physically interpretable representations of the underlying flow structures, as characteristic flow patterns are closely associated with cardiovascular conditions and can support early detection of cardiac deterioration. Conventional visualization of velocity or pressure fields, however, provides limited insight into the coherent mechanisms driving these dynamics. Reduced-order modeling techniques, like Proper Orthogonal Decomposition (POD) and Autoencoder (AE) architectures, offer powerful alternatives to extract dominant flow features from complex datasets. This study systematically compares POD with several AE variants (Linear, Nonlinear, Convolutional, and Variational) using left ventricular flow fields obtained from computational fluid dynamics simulations. We show that, for a suitably chosen latent dimension, AEs produce modes that become nearly orthogonal and qualitatively resemble POD modes that capture a given percentage of kinetic energy. As the number of latent modes increases, AE modes progressively lose orthogonality, leading to linear dependence, spatial redundancy, and the appearance of repeated modes with substantial high-frequency content. This degradation reduces interpretability and introduces noise-like components into AE-based reduced-order models, potentially complicating their integration with physics-based formulations or neural-network surrogates. The extent of interpretability loss varies across the AEs, with nonlinear, convolutional, and variational models exhibiting distinct behaviors in orthogonality preservation and feature localization. Overall, the results indicate that AEs can reproduce POD-like coherent structures under specific latent-space configurations, while highlighting the need for careful mode selection to ensure physically meaningful representations of cardiac flow dynamics.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real-Time Streamable Generative Speech Restoration with Flow Matching</title>
<link>https://arxiv.org/abs/2512.19442</link>
<guid>https://arxiv.org/abs/2512.19442</guid>
<content:encoded><![CDATA[
arXiv:2512.19442v1 Announce Type: cross 
Abstract: Diffusion-based generative models have greatly impacted the speech processing field in recent years, exhibiting high speech naturalness and spawning a new research direction. Their application in real-time communication is, however, still lagging behind due to their computation-heavy nature involving multiple calls of large DNNs.
  Here, we present Stream.FM, a frame-causal flow-based generative model with an algorithmic latency of 32 milliseconds (ms) and a total latency of 48 ms, paving the way for generative speech processing in real-time communication. We propose a buffered streaming inference scheme and an optimized DNN architecture, show how learned few-step numerical solvers can boost output quality at a fixed compute budget, explore model weight compression to find favorable points along a compute/quality tradeoff, and contribute a model variant with 24 ms total latency for the speech enhancement task.
  Our work looks beyond theoretical latencies, showing that high-quality streaming generative speech processing can be realized on consumer GPUs available today. Stream.FM can solve a variety of speech processing tasks in a streaming fashion: speech enhancement, dereverberation, codec post-filtering, bandwidth extension, STFT phase retrieval, and Mel vocoding. As we verify through comprehensive evaluations and a MUSHRA listening test, Stream.FM establishes a state-of-the-art for generative streaming speech restoration, exhibits only a reasonable reduction in quality compared to a non-streaming variant, and outperforms our recent work (Diffusion Buffer) on generative streaming speech enhancement while operating at a lower latency.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GLUE: Generative Latent Unification of Expertise-Informed Engineering Models</title>
<link>https://arxiv.org/abs/2512.19469</link>
<guid>https://arxiv.org/abs/2512.19469</guid>
<content:encoded><![CDATA[
arXiv:2512.19469v1 Announce Type: cross 
Abstract: Engineering complex systems (aircraft, buildings, vehicles) requires accounting for geometric and performance couplings across subsystems. As generative models proliferate for specialized domains (wings, structures, engines), a key research gap is how to coordinate frozen, pre-trained submodels to generate full-system designs that are feasible, diverse, and high-performing. We introduce Generative Latent Unification of Expertise-Informed Engineering Models (GLUE), which orchestrates pre-trained, frozen subsystem generators while enforcing system-level feasibility, optimality, and diversity. We propose and benchmark (i) data-driven GLUE models trained on pre-generated system-level designs and (ii) a data-free GLUE model trained online on a differentiable geometry layer. On a UAV design problem with five coupling constraints, we find that data-driven approaches yield diverse, high-performing designs but require large datasets to satisfy constraints reliably. The data-free approach is competitive with Bayesian optimization and gradient-based optimization in performance and feasibility while training a full generative model in only 10 min on a RTX 4090 GPU, requiring more than two orders of magnitude fewer geometry evaluations and FLOPs than the data-driven method. Ablations focused on data-free training show that subsystem output continuity affects coordination, and equality constraints can trigger mode collapse unless mitigated. By integrating unmodified, domain-informed submodels into a modular generative workflow, this work provides a viable path for scaling generative design to complex, real-world engineering systems.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Active Convolved Illumination with Deep Transfer Learning for Complex Beam Transmission through Atmospheric Turbulence</title>
<link>https://arxiv.org/abs/2512.19540</link>
<guid>https://arxiv.org/abs/2512.19540</guid>
<content:encoded><![CDATA[
arXiv:2512.19540v1 Announce Type: cross 
Abstract: Atmospheric turbulence imposes a fundamental limitation across a broad range of applications, including optical imaging, remote sensing, and free-space optical communication. Recent advances in adaptive optics, wavefront shaping, and machine learning, driven by synergistic progress in fundamental theories, optoelectronic hardware, and computational algorithms, have demonstrated substantial potential in mitigating turbulence-induced distortions. Recently, active convolved illumination (ACI) was proposed as a versatile and physics-driven technique for transmitting structured light beams with minimal distortion through highly challenging turbulent regimes. While distinct in its formulation, ACI shares conceptual similarities with other physics-driven distortion correction approaches and stands to benefit from complementary integration with data-driven deep learning (DL) models. Inspired by recent work coupling deep learning with traditional turbulence mitigation strategies, the present work investigates the feasibility of integrating ACI with neural network-based methods. We outline a conceptual framework for coupling ACI with data-driven models and identify conditions under which learned representations can meaningfully support ACI's correlation-injection mechanism. As a representative example, we employ a convolutional neural network (CNN) together with a transfer-learning approach to examine how a learned model may operate in tandem with ACI. This exploratory study demonstrates feasible implementation pathways and establishes an early foundation for assessing the potential of future ACI-DL hybrid architectures, representing a step toward evaluating broader synergistic interactions between ACI and modern DL models.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller</title>
<link>https://arxiv.org/abs/2512.19576</link>
<guid>https://arxiv.org/abs/2512.19576</guid>
<content:encoded><![CDATA[
arXiv:2512.19576v1 Announce Type: cross 
Abstract: Attitude control is essential for many satellite missions. Classical controllers, however, are time-consuming to design and sensitive to model uncertainties and variations in operational boundary conditions. Deep Reinforcement Learning (DRL) offers a promising alternative by learning adaptive control strategies through autonomous interaction with a simulation environment. Overcoming the Sim2Real gap, which involves deploying an agent trained in simulation onto the real physical satellite, remains a significant challenge. In this work, we present the first successful in-orbit demonstration of an AI-based attitude controller for inertial pointing maneuvers. The controller was trained entirely in simulation and deployed to the InnoCube 3U nanosatellite, which was developed by the Julius-Maximilians-Universit\"at W\"urzburg in cooperation with the Technische Universit\"at Berlin, and launched in January 2025. We present the AI agent design, the methodology of the training procedure, the discrepancies between the simulation and the observed behavior of the real satellite, and a comparison of the AI-based attitude controller with the classical PD controller of InnoCube. Steady-state metrics confirm the robust performance of the AI-based controller during repeated in-orbit maneuvers.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning</title>
<link>https://arxiv.org/abs/2512.19687</link>
<guid>https://arxiv.org/abs/2512.19687</guid>
<content:encoded><![CDATA[
arXiv:2512.19687v1 Announce Type: cross 
Abstract: We introduce Perception Encoder Audiovisual, PE-AV, a new family of encoders for audio and video understanding trained with scaled contrastive learning. Built on PE, PE-AV makes several key contributions to extend representations to audio, and natively support joint embeddings across audio-video, audio-text, and video-text modalities. PE-AV's unified cross-modal embeddings enable novel tasks such as speech retrieval, and set a new state of the art across standard audio and video benchmarks. We unlock this by building a strong audiovisual data engine that synthesizes high-quality captions for O(100M) audio-video pairs, enabling large-scale supervision consistent across modalities. Our audio data includes speech, music, and general sound effects-avoiding single-domain limitations common in prior work. We exploit ten pairwise contrastive objectives, showing that scaling cross-modality and caption-type pairs strengthens alignment and improves zero-shot performance. We further develop PE-A-Frame by fine-tuning PE-AV with frame-level contrastive objectives, enabling fine-grained audio-frame-to-text alignment for tasks such as sound event detection.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trajectory-Aware Eligibility Traces for Off-Policy Reinforcement Learning</title>
<link>https://arxiv.org/abs/2301.11321</link>
<guid>https://arxiv.org/abs/2301.11321</guid>
<content:encoded><![CDATA[
arXiv:2301.11321v3 Announce Type: replace 
Abstract: Off-policy learning from multistep returns is crucial for sample-efficient reinforcement learning, but counteracting off-policy bias without exacerbating variance is challenging. Classically, off-policy bias is corrected in a per-decision manner: past temporal-difference errors are re-weighted by the instantaneous Importance Sampling (IS) ratio after each action via eligibility traces. Many off-policy algorithms rely on this mechanism, along with differing protocols for cutting the IS ratios to combat the variance of the IS estimator. Unfortunately, once a trace has been fully cut, the effect cannot be reversed. This has led to the development of credit-assignment strategies that account for multiple past experiences at a time. These trajectory-aware methods have not been extensively analyzed, and their theoretical justification remains uncertain. In this paper, we propose a multistep operator that can express both per-decision and trajectory-aware methods. We prove convergence conditions for our operator in the tabular setting, establishing the first guarantees for several existing methods as well as many new ones. Finally, we introduce Recency-Bounded Importance Sampling (RBIS), which leverages trajectory awareness to perform robustly across $\lambda$-values in an off-policy control task.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Exploitation and Exploration of Contextual Bandits</title>
<link>https://arxiv.org/abs/2305.03784</link>
<guid>https://arxiv.org/abs/2305.03784</guid>
<content:encoded><![CDATA[
arXiv:2305.03784v2 Announce Type: replace 
Abstract: In this paper, we study utilizing neural networks for the exploitation and exploration of contextual multi-armed bandits. Contextual multi-armed bandits have been studied for decades with various applications. To solve the exploitation-exploration trade-off in bandits, there are three main techniques: epsilon-greedy, Thompson Sampling (TS), and Upper Confidence Bound (UCB). In recent literature, a series of neural bandit algorithms have been proposed to adapt to the non-linear reward function, combined with TS or UCB strategies for exploration. In this paper, instead of calculating a large-deviation based statistical bound for exploration like previous methods, we propose, ``EE-Net,'' a novel neural-based exploitation and exploration strategy. In addition to using a neural network (Exploitation network) to learn the reward function, EE-Net uses another neural network (Exploration network) to adaptively learn the potential gains compared to the currently estimated reward for exploration. We provide an instance-based $\widetilde{\mathcal{O}}(\sqrt{T})$ regret upper bound for EE-Net and show that EE-Net outperforms related linear and neural contextual bandit baselines on real-world datasets.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Anti-Correlated Noise in Epoch-Based Stochastic Gradient Descent: Implications for Weight Variances in Flat Directions</title>
<link>https://arxiv.org/abs/2306.05300</link>
<guid>https://arxiv.org/abs/2306.05300</guid>
<content:encoded><![CDATA[
arXiv:2306.05300v3 Announce Type: replace 
Abstract: Stochastic Gradient Descent (SGD) has become a cornerstone of neural network optimization due to its computational efficiency and generalization capabilities. However, the gradient noise introduced by SGD is often assumed to be uncorrelated over time, despite the common practice of epoch-based training where data is sampled without replacement. In this work, we challenge this assumption and investigate the effects of epoch-based noise correlations on the stationary distribution of discrete-time SGD with momentum. Our main contributions are twofold: First, we calculate the exact autocorrelation of the noise during epoch-based training under the assumption that the noise is independent of small fluctuations in the weight vector, revealing that SGD noise is inherently anti-correlated over time. Second, we explore the influence of these anti-correlations on the variance of weight fluctuations. We find that for directions with curvature of the loss greater than a hyperparameter-dependent crossover value, the conventional predictions of isotropic weight variance under stationarity, based on uncorrelated and curvature-proportional noise, are recovered. Anti-correlations have negligible effect here. However, for relatively flat directions, the weight variance is significantly reduced, leading to a considerable decrease in loss fluctuations compared to the constant weight variance assumption. Furthermore, we present a numerical experiment where training with these anti-correlations enhances test performance, suggesting that the inherent noise structure induced by epoch-based training may play a role in finding flatter minima that generalize better.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HUTFormer: Hierarchical U-Net Transformer for Long-Term Traffic Forecasting</title>
<link>https://arxiv.org/abs/2307.14596</link>
<guid>https://arxiv.org/abs/2307.14596</guid>
<content:encoded><![CDATA[
arXiv:2307.14596v2 Announce Type: replace 
Abstract: Traffic forecasting, which aims to predict traffic conditions based on historical observations, has been an enduring research topic and is widely recognized as an essential component of intelligent transportation. Recent proposals on Spatial-Temporal Graph Neural Networks~(STGNNs) have made significant progress by combining sequential models with graph convolution networks. However, due to high complexity issues, STGNNs only focus on short-term traffic forecasting (e.g., 1-h ahead), while ignoring more practical long-term forecasting. In this paper, we make the first attempt to explore long-term traffic forecasting (e.g., 1-day ahead). To this end, we first reveal its unique challenges in exploiting multi-scale representations. Then, we propose a novel Hierarchical U-Net TransFormer~(HUTFormer) to address the issues of long-term traffic forecasting. HUTFormer consists of a hierarchical encoder and decoder to jointly generate and utilize multi-scale representations of traffic data. Specifically, for the encoder, we {\color{black}propose} window self-attention and segment merging to extract multi-scale representations from long-term traffic data. For the decoder, we design a cross-scale attention mechanism to effectively incorporate multi-scale representations. In addition, HUTFormer employs an efficient input embedding strategy to address the complexity issues. Extensive experiments on four traffic datasets show that the proposed HUTFormer significantly outperforms state-of-the-art traffic forecasting and long time series forecasting baselines.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Certified Defense on the Fairness of Graph Neural Networks</title>
<link>https://arxiv.org/abs/2311.02757</link>
<guid>https://arxiv.org/abs/2311.02757</guid>
<content:encoded><![CDATA[
arXiv:2311.02757v3 Announce Type: replace 
Abstract: Graph Neural Networks (GNNs) have emerged as a prominent graph learning model in various graph-based tasks over the years. Nevertheless, due to the vulnerabilities of GNNs, it has been empirically shown that malicious attackers could easily corrupt the fairness level of their predictions by adding perturbations to the input graph data. In this paper, we take crucial steps to study a novel problem of certifiable defense on the fairness level of GNNs. Specifically, we propose a principled framework named ELEGANT and present a detailed theoretical certification analysis for the fairness of GNNs. ELEGANT takes {\em any} GNN as its backbone, and the fairness level of such a backbone is theoretically impossible to be corrupted under certain perturbation budgets for attackers. Notably, ELEGANT does not make any assumptions over the GNN structure or parameters, and does not require re-training the GNNs to realize certification. Hence it can serve as a plug-and-play framework for any optimized GNNs ready to be deployed. We verify the satisfactory effectiveness of ELEGANT in practice through extensive experiments on real-world datasets across different backbones of GNNs and parameter settings.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Averaging $n$-step Returns Reduces Variance in Reinforcement Learning</title>
<link>https://arxiv.org/abs/2402.03903</link>
<guid>https://arxiv.org/abs/2402.03903</guid>
<content:encoded><![CDATA[
arXiv:2402.03903v4 Announce Type: replace 
Abstract: Multistep returns, such as $n$-step returns and $\lambda$-returns, are commonly used to improve the sample efficiency of reinforcement learning (RL) methods. The variance of the multistep returns becomes the limiting factor in their length; looking too far into the future increases variance and reverses the benefits of multistep learning. In our work, we demonstrate the ability of compound returns -- weighted averages of $n$-step returns -- to reduce variance. We prove for the first time that any compound return with the same contraction modulus as a given $n$-step return has strictly lower variance. We additionally prove that this variance-reduction property improves the finite-sample complexity of temporal-difference learning under linear function approximation. Because general compound returns can be expensive to implement, we introduce two-bootstrap returns which reduce variance while remaining efficient, even when using minibatched experience replay. We conduct experiments showing that compound returns often increase the sample efficiency of $n$-step deep RL agents like DQN and PPO.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IT Intrusion Detection Using Statistical Learning and Testbed Measurements</title>
<link>https://arxiv.org/abs/2402.13081</link>
<guid>https://arxiv.org/abs/2402.13081</guid>
<content:encoded><![CDATA[
arXiv:2402.13081v2 Announce Type: replace 
Abstract: We study automated intrusion detection in an IT infrastructure, specifically the problem of identifying the start of an attack, the type of attack, and the sequence of actions an attacker takes, based on continuous measurements from the infrastructure. We apply statistical learning methods, including Hidden Markov Model (HMM), Long Short-Term Memory (LSTM), and Random Forest Classifier (RFC) to map sequences of observations to sequences of predicted attack actions. In contrast to most related research, we have abundant data to train the models and evaluate their predictive power. The data comes from traces we generate on an in-house testbed where we run attacks against an emulated IT infrastructure. Central to our work is a machine-learning pipeline that maps measurements from a high-dimensional observation space to a space of low dimensionality or to a small set of observation symbols. Investigating intrusions in offline as well as online scenarios, we find that both HMM and LSTM can be effective in predicting attack start time, attack type, and attack actions. If sufficient training data is available, LSTM achieves higher prediction accuracy than HMM. HMM, on the other hand, requires less computational resources and less training data for effective prediction. Also, we find that the methods we study benefit from data produced by traditional intrusion detection systems like SNORT.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continuum Attention for Neural Operators</title>
<link>https://arxiv.org/abs/2406.06486</link>
<guid>https://arxiv.org/abs/2406.06486</guid>
<content:encoded><![CDATA[
arXiv:2406.06486v4 Announce Type: replace 
Abstract: Transformers, and the attention mechanism in particular, have become ubiquitous in machine learning. Their success in modeling nonlocal, long-range correlations has led to their widespread adoption in natural language processing, computer vision, and time series problems. Neural operators, which map spaces of functions into spaces of functions, are necessarily both nonlinear and nonlocal if they are universal; it is thus natural to ask whether the attention mechanism can be used in the design of neural operators. Motivated by this, we study transformers in the function space setting. We formulate attention as a map between infinite dimensional function spaces and prove that the attention mechanism as implemented in practice is a Monte Carlo or finite difference approximation of this operator. The function space formulation allows for the design of transformer neural operators, a class of architectures designed to learn mappings between function spaces. In this paper, we state and prove the first universal approximation result for transformer neural operators, using only a slight modification of the architecture implemented in practice. The prohibitive cost of applying the attention operator to functions defined on multi-dimensional domains leads to the need for more efficient attention-based architectures. For this reason we also introduce a function space generalization of the patching strategy from computer vision, and introduce a class of associated neural operators. Numerical results, on an array of operator learning problems, demonstrate the promise of our approaches to function space formulations of attention and their use in neural operators.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Transformers: A Survey</title>
<link>https://arxiv.org/abs/2407.09777</link>
<guid>https://arxiv.org/abs/2407.09777</guid>
<content:encoded><![CDATA[
arXiv:2407.09777v2 Announce Type: replace 
Abstract: Graph transformers are a recent advancement in machine learning, offering a new class of neural network models for graph-structured data. The synergy between transformers and graph learning demonstrates strong performance and versatility across various graph-related tasks. This survey provides an in-depth review of recent progress and challenges in graph transformer research. We begin with foundational concepts of graphs and transformers. We then explore design perspectives of graph transformers, focusing on how they integrate graph inductive biases and graph attention mechanisms into the transformer architecture. Furthermore, we propose a taxonomy classifying graph transformers based on depth, scalability, and pre-training strategies, summarizing key principles for effective development of graph transformer models. Beyond technical analysis, we discuss the applications of graph transformer models for node-level, edge-level, and graph-level tasks, exploring their potential in other application scenarios as well. Finally, we identify remaining challenges in the field, such as scalability and efficiency, generalization and robustness, interpretability and explainability, dynamic and complex graphs, as well as data quality and diversity, charting future directions for graph transformer research.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Overcoming Growth-Induced Forgetting in Task-Agnostic Continual Learning</title>
<link>https://arxiv.org/abs/2408.10566</link>
<guid>https://arxiv.org/abs/2408.10566</guid>
<content:encoded><![CDATA[
arXiv:2408.10566v5 Announce Type: replace 
Abstract: In continual learning (CL), model growth enhances adaptability to new data. However, when model growth is applied improperly, especially in task-agnostic CL, where the entire grown model is used for inference, it can lead to severe degradation of learned knowledge, a problem we term growth-induced forgetting. Most existing methods that adopt model growth to improve adaptability often overlook the forgetting issue, resulting in compromised knowledge retention, making them unsuitable for task-agnostic settings. To promote both adaptability and knowledge retention with model growth, we identify the key: gradient and parameter sparsity. Introducing SparseGrow, which increases gradient sparsity through layer expansion and gradient gating to enable focused updates on parameters while preserving critical parameters, thus inhibiting forgetting. Moreover, it promotes parameter sparsity with sparse initialization and training, aiming at better control of model plasticity, improving adaptability over new data. Extensive experiments across diverse datasets, task-agnostic settings, and a large number of tasks demonstrate the necessity of controlled layer expansion and validate the effectiveness of SparseGrow in achieving high adaptability while minimizing forgetting in continual learning. By enabling model growth with sparsified gradients and parameters, SparseGrow paves the way for building scalable lifelong learning systems capable of continual adaptation with better knowledge retention.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Reliable are Causal Probing Interventions?</title>
<link>https://arxiv.org/abs/2408.15510</link>
<guid>https://arxiv.org/abs/2408.15510</guid>
<content:encoded><![CDATA[
arXiv:2408.15510v5 Announce Type: replace 
Abstract: Causal probing aims to analyze foundation models by examining how intervening on their representation of various latent properties impacts their outputs. Recent works have cast doubt on the theoretical basis of several leading causal probing methods, but it has been unclear how to systematically evaluate the effectiveness of these methods in practice. To address this, we define two key causal probing desiderata: completeness (how thoroughly the representation of the target property has been transformed) and selectivity (how little non-targeted properties have been impacted). We find that there is an inherent tradeoff between the two, which we define as reliability, their harmonic mean. We introduce an empirical analysis framework to measure and evaluate these quantities, allowing us to make the first direct comparisons between different families of leading causal probing methods (e.g., linear vs. nonlinear, or concept removal vs. counterfactual interventions). We find that: (1) all methods show a clear tradeoff between completeness and selectivity; (2) more complete and reliable methods have a greater impact on LLM behavior; and (3) nonlinear interventions are almost always more reliable than linear interventions.
  Our project webpage is available at: https://ahdavies6.github.io/causal_probing_reliability/
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Reinforcement Learning Environment for Automatic Code Optimization in the MLIR Compiler</title>
<link>https://arxiv.org/abs/2409.11068</link>
<guid>https://arxiv.org/abs/2409.11068</guid>
<content:encoded><![CDATA[
arXiv:2409.11068v2 Announce Type: replace 
Abstract: Code optimization is a crucial task that aims to enhance code performance. However, this process is often tedious and complex, highlighting the necessity for automatic code optimization techniques. Reinforcement Learning (RL) has emerged as a promising approach for tackling such complex optimization problems. In this project, we introduce MLIR RL, an RL environment for the MLIR compiler, dedicated to facilitating MLIR compiler research and enabling automatic code optimization. We propose a multi-discrete formulation of the action space where the action space is the Cartesian product of simpler action subspaces. We also propose a new method, called level pointers, to reduce the size of the action space related to the loop interchange transformation. This enables more efficient and effective learning of the policy. To demonstrate the effectiveness of MLIR RL, we train an RL agent to optimize MLIR Linalg code, targeting CPU. The code is generated from two domain-specific frameworks: deep-learning models generated from PyTorch, and LQCD (Lattice Quantum Chromodynamics) code generated from an LQCD compiler. The result of this work is a research environment that allows the community to experiment with novel ideas in RL-driven loop-nest optimization.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Equivariant Polynomial Functional Networks</title>
<link>https://arxiv.org/abs/2410.04213</link>
<guid>https://arxiv.org/abs/2410.04213</guid>
<content:encoded><![CDATA[
arXiv:2410.04213v2 Announce Type: replace 
Abstract: Neural Functional Networks (NFNs) have gained increasing interest due to their wide range of applications, including extracting information from implicit representations of data, editing network weights, and evaluating policies. A key design principle of NFNs is their adherence to the permutation and scaling symmetries inherent in the connectionist structure of the input neural networks. Recent NFNs have been proposed with permutation and scaling equivariance based on either graph-based message-passing mechanisms or parameter-sharing mechanisms. However, graph-based equivariant NFNs suffer from high memory consumption and long running times. On the other hand, parameter-sharing-based NFNs built upon equivariant linear layers exhibit lower memory consumption and faster running time, yet their expressivity is limited due to the large size of the symmetric group of the input neural networks. The challenge of designing a permutation and scaling equivariant NFN that maintains low memory consumption and running time while preserving expressivity remains unresolved. In this paper, we propose a novel solution with the development of MAGEP-NFN (Monomial mAtrix Group Equivariant Polynomial NFN). Our approach follows the parameter-sharing mechanism but differs from previous works by constructing a nonlinear equivariant layer represented as a polynomial in the input weights. This polynomial formulation enables us to incorporate additional relationships between weights from different input hidden layers, enhancing the model's expressivity while keeping memory consumption and running time low, thereby addressing the aforementioned challenge. We provide empirical evidence demonstrating that MAGEP-NFN achieves competitive performance and efficiency compared to existing baselines.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Deep Learning Infrastructures for Embedded Computing Systems: A Comprehensive Survey and Future Envision</title>
<link>https://arxiv.org/abs/2411.01431</link>
<guid>https://arxiv.org/abs/2411.01431</guid>
<content:encoded><![CDATA[
arXiv:2411.01431v2 Announce Type: replace 
Abstract: Deep neural networks (DNNs) have recently achieved impressive success across a wide range of real-world vision and language processing tasks, spanning from image classification to many other downstream vision tasks, such as object detection, tracking, and segmentation. However, previous well-established DNNs, despite being able to maintain superior accuracy, have also been evolving to be deeper and wider and thus inevitably necessitate prohibitive computational resources for both training and inference. This trend further enlarges the computational gap between computation-intensive DNNs and resource-constrained embedded computing systems, making it challenging to deploy powerful DNNs upon real-world embedded computing systems towards ubiquitous embedded intelligence. To alleviate the above computational gap and enable ubiquitous embedded intelligence, we, in this survey, focus on discussing recent efficient deep learning infrastructures for embedded computing systems, spanning from training to inference, from manual to automated, from convolutional neural networks to transformers, from transformers to vision transformers, from vision models to large language models, from software to hardware, and from algorithms to applications. Specifically, we discuss recent efficient deep learning infrastructures for embedded computing systems from the lens of (1) efficient manual network design for embedded computing systems, (2) efficient automated network design for embedded computing systems, (3) efficient network compression for embedded computing systems, (4) efficient on-device learning for embedded computing systems, (5) efficient large language models for embedded computing systems, (6) efficient deep learning software and hardware for embedded computing systems, and (7) efficient intelligent applications for embedded computing systems.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conditional Distribution Learning for Graph Classification</title>
<link>https://arxiv.org/abs/2411.15206</link>
<guid>https://arxiv.org/abs/2411.15206</guid>
<content:encoded><![CDATA[
arXiv:2411.15206v4 Announce Type: replace 
Abstract: Leveraging the diversity and quantity of data provided by various graph-structured data augmentations while preserving intrinsic semantic information is challenging. Additionally, successive layers in graph neural network (GNN) tend to produce more similar node embeddings, while graph contrastive learning aims to increase the dissimilarity between negative pairs of node embeddings. This inevitably results in a conflict between the message-passing mechanism (MPM) of GNNs and the contrastive learning (CL) of negative pairs via intraviews. In this paper, we propose a conditional distribution learning (CDL) method that learns graph representations from graph-structured data for semisupervised graph classification. Specifically, we present an end-to-end graph representation learning model to align the conditional distributions of weakly and strongly augmented features over the original features. This alignment enables the CDL model to effectively preserve intrinsic semantic information when both weak and strong augmentations are applied to graph-structured data. To avoid the conflict between the MPM and the CL of negative pairs, positive pairs of node representations are retained for measuring the similarity between the original features and the corresponding weakly augmented features. Extensive experiments with several benchmark graph datasets demonstrate the effectiveness of the proposed CDL method.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PowerMamba: A Deep State Space Model and Comprehensive Benchmark for Time Series Prediction in Electric Power Systems</title>
<link>https://arxiv.org/abs/2412.06112</link>
<guid>https://arxiv.org/abs/2412.06112</guid>
<content:encoded><![CDATA[
arXiv:2412.06112v2 Announce Type: replace 
Abstract: The electricity sector is undergoing substantial transformations due to the rising electrification of demand, enhanced integration of renewable energy resources, and the emergence of new technologies. These changes are rendering the electric grid more volatile and unpredictable, making it difficult to maintain reliable operations. In order to address these issues, advanced time series prediction models are needed for closing the gap between the forecasted and actual grid outcomes. In this paper, we introduce a multivariate time series prediction model that combines traditional state space models with deep learning methods to simultaneously capture and predict the underlying dynamics of multiple time series. Additionally, we design a time series processing module that incorporates high-resolution external forecasts into sequence-to-sequence prediction models, achieving this with negligible increases in size and no loss of accuracy. We also release an extended dataset spanning five years of load, electricity price, ancillary service price, and renewable generation. To complement this dataset, we provide an open-access toolbox that includes our proposed model, the dataset itself, and several state-of-the-art prediction models, thereby creating a unified framework for benchmarking advanced machine learning approaches. Our findings indicate that the proposed model outperforms existing models across various prediction tasks, improving state-of-the-art prediction error by an average of 7% and decreasing model parameters by 43%.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Communication-Efficient and Privacy-Adaptable Mechanism for Federated Learning</title>
<link>https://arxiv.org/abs/2501.12046</link>
<guid>https://arxiv.org/abs/2501.12046</guid>
<content:encoded><![CDATA[
arXiv:2501.12046v3 Announce Type: replace 
Abstract: Training machine learning models on decentralized private data via federated learning (FL) poses two key challenges: communication efficiency and privacy protection. In this work, we address these challenges within the trusted aggregator model by introducing a novel approach called the Communication-Efficient and Privacy-Adaptable Mechanism (CEPAM), achieving both objectives simultaneously. In particular, CEPAM leverages the rejection-sampled universal quantizer (RSUQ), a construction of randomized vector quantizer whose resulting distortion is equivalent to a prescribed noise, such as Gaussian or Laplace noise, enabling joint differential privacy and compression. Our CEPAM provides the additional benefit of privacy adaptability, allowing clients and the server to customize privacy protection based on required accuracy and protection. We theoretically analyze the privacy guarantee of CEPAM and investigate the trade-offs among user privacy and accuracy of CEPAM through experimental evaluations. Moreover, we assess CEPAM's utility performance using MNIST dataset, demonstrating that CEPAM surpasses baseline models in terms of learning accuracy.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hardware-Aware DNN Compression for Homogeneous Edge Devices</title>
<link>https://arxiv.org/abs/2501.15240</link>
<guid>https://arxiv.org/abs/2501.15240</guid>
<content:encoded><![CDATA[
arXiv:2501.15240v2 Announce Type: replace 
Abstract: Deploying deep neural networks (DNNs) across homogeneous edge devices (the devices with the same SKU labeled by the manufacturer) often assumes identical performance among them. However, once a device model is widely deployed, the performance of each device becomes different after a period of running. This is caused by the differences in user configurations, environmental conditions, manufacturing variances, battery degradation, etc. Existing DNN compression methods have not taken this scenario into consideration and can not guarantee good compression results in all homogeneous edge devices. To address this, we propose Homogeneous-Device Aware Pruning (HDAP), a hardware-aware DNN compression framework explicitly designed for homogeneous edge devices, aiming to achieve optimal average performance of the compressed model across all devices. To deal with the difficulty of time-consuming hardware-aware evaluations for thousands or millions of homogeneous edge devices, HDAP partitions all the devices into several device clusters, which can dramatically reduce the number of devices to evaluate and use the surrogate-based evaluation instead of hardware evaluation in real-time. Experiments on ResNet50 and MobileNetV1 with the ImageNet dataset show that HDAP consistently achieves lower average inference latency compared with state-of-the-art methods, with substantial speedup gains (e.g., 2.86 $\times$ speedup at 1.0G FLOPs for ResNet50) on the homogeneous device clusters. HDAP offers an effective solution for scalable, high-performance DNN deployment methods for homogeneous edge devices.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Error Slice Discovery via Manifold Compactness</title>
<link>https://arxiv.org/abs/2501.19032</link>
<guid>https://arxiv.org/abs/2501.19032</guid>
<content:encoded><![CDATA[
arXiv:2501.19032v2 Announce Type: replace 
Abstract: Despite the great performance of deep learning models in many areas, they still make mistakes and underperform on certain subsets of data, i.e. error slices. Given a trained model, it is important to identify its semantically coherent error slices that are easy to interpret, which is referred to as the error slice discovery problem. However, there is no proper metric of slice coherence without relying on extra information like predefined slice labels. Current evaluation of slice coherence requires access to predefined slices formulated by metadata like attributes or subclasses. Its validity heavily relies on the quality and abundance of metadata, where some possible patterns could be ignored. Besides, current algorithms cannot directly incorporate the constraint of coherence into their optimization objective due to absence of an explicit coherence metric, which could potentially hinder their effectiveness. In this paper, we propose manifold compactness, a coherence metric without reliance on extra information by incorporating the data geometry property into its design, and experiments on typical datasets empirically validate the rationality of the metric. Then we develop Manifold Compactness based error Slice Discovery (MCSD), a novel algorithm that directly treats risk and coherence as the optimization objective, and is flexible to be applied to models of various tasks. Extensive experiments on the benchmark and case studies on other typical datasets demonstrate the superiority of MCSD.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Planning and Learning in Average Risk-aware MDPs</title>
<link>https://arxiv.org/abs/2503.17629</link>
<guid>https://arxiv.org/abs/2503.17629</guid>
<content:encoded><![CDATA[
arXiv:2503.17629v3 Announce Type: replace 
Abstract: For continuing tasks, average cost Markov decision processes have well-documented value and can be solved using efficient algorithms. However, it explicitly assumes that the agent is risk-neutral. In this work, we extend risk-neutral algorithms to accommodate the more general class of dynamic risk measures. Specifically, we propose a relative value iteration (RVI) algorithm for planning and design two model-free Q-learning algorithms, namely a generic algorithm based on the multi-level Monte Carlo (MLMC) method, and an off-policy algorithm dedicated to utility-based shortfall risk measures. Both the RVI and MLMC-based Q-learning algorithms are proven to converge to optimality. Numerical experiments validate our analysis, confirm empirically the convergence of the off-policy algorithm, and demonstrate that our approach enables the identification of policies that are finely tuned to the intricate risk-awareness of the agent that they serve.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LogicXGNN: Grounded Logical Rules for Explaining Graph Neural Networks</title>
<link>https://arxiv.org/abs/2503.19476</link>
<guid>https://arxiv.org/abs/2503.19476</guid>
<content:encoded><![CDATA[
arXiv:2503.19476v3 Announce Type: replace 
Abstract: Existing rule-based explanations for Graph Neural Networks (GNNs) provide global interpretability but often optimize and assess fidelity in an intermediate, uninterpretable concept space, overlooking grounding quality for end users in the final subgraph explanations. This gap yields explanations that may appear faithful yet be unreliable in practice. To this end, we propose LogicXGNN, a post-hoc framework that constructs logical rules over reliable predicates explicitly designed to capture the GNN's message-passing structure, thereby ensuring effective grounding. We further introduce data-grounded fidelity ($\textit{Fid}_{\mathcal{D}}$), a realistic metric that evaluates explanations in their final-graph form, along with complementary utility metrics such as coverage and validity. Across extensive experiments, LogicXGNN improves $\textit{Fid}_{\mathcal{D}}$ by over 20% on average relative to state-of-the-art methods while being 10-100 $\times$ faster. With strong scalability and utility performance, LogicXGNN produces explanations that are faithful to the model's logic and reliably grounded in observable data. Our code is available at https://github.com/allengeng123/LogicXGNN/.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Variational Online Mirror Descent for Robust Learning in Schr\"odinger Bridge</title>
<link>https://arxiv.org/abs/2504.02618</link>
<guid>https://arxiv.org/abs/2504.02618</guid>
<content:encoded><![CDATA[
arXiv:2504.02618v4 Announce Type: replace 
Abstract: The Schr\"{o}dinger bridge (SB) has evolved into a universal class of probabilistic generative models. In practice, however, estimated learning signals are innately uncertain, and the reliability promised by existing methods is often based on speculative optimal case scenarios. Recent studies regarding the Sinkhorn algorithm through mirror descent (MD) have gained attention, revealing geometric insights into solution acquisition of the SB problems. In this paper, we propose a variational online MD (OMD) framework for the SB problems, which provides further stability to SB solvers. We formally prove convergence and a regret bound for the novel OMD formulation of SB acquisition. As a result, we propose a simulation-free SB algorithm called Variational Mirrored Schr\"{o}dinger Bridge (VMSB) by utilizing the Wasserstein-Fisher-Rao geometry of the Gaussian mixture parameterization for Schr\"{o}dinger potentials. Based on the Wasserstein gradient flow theory, the algorithm offers tractable learning dynamics that precisely approximate each OMD step. In experiments, we validate the performance of the proposed VMSB algorithm across an extensive suite of benchmarks. VMSB consistently outperforms contemporary SB solvers on a wide range of SB problems, demonstrating the robustness as well as generality predicted by our OMD theory.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TabRep: Training Tabular Diffusion Models with a Simple and Effective Continuous Representation</title>
<link>https://arxiv.org/abs/2504.04798</link>
<guid>https://arxiv.org/abs/2504.04798</guid>
<content:encoded><![CDATA[
arXiv:2504.04798v5 Announce Type: replace 
Abstract: Diffusion models have been the predominant generative model for tabular data generation. However, they face the conundrum of modeling under a separate versus a unified data representation. The former encounters the challenge of jointly modeling all multi-modal distributions of tabular data in one model. While the latter alleviates this by learning a single representation for all features, it currently leverages sparse suboptimal encoding heuristics and necessitates additional computation costs. In this work, we address the latter by presenting TabRep, a tabular diffusion architecture trained with a unified continuous representation. To motivate the design of our representation, we provide geometric insights into how the data manifold affects diffusion models. The key attributes of our representation are composed of its density, flexibility to provide ample separability for nominal features, and ability to preserve intrinsic relationships. Ultimately, TabRep provides a simple yet effective approach for training tabular diffusion models under a continuous data manifold. Our results showcase that TabRep achieves superior performance across a broad suite of evaluations. It is the first to synthesize tabular data that exceeds the downstream quality of the original datasets while preserving privacy and remaining computationally efficient. Code is available at https://github.com/jacobyhsi/TabRep.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models</title>
<link>https://arxiv.org/abs/2505.01912</link>
<guid>https://arxiv.org/abs/2505.01912</guid>
<content:encoded><![CDATA[
arXiv:2505.01912v2 Announce Type: replace 
Abstract: Data-driven molecular discovery leverages artificial intelligence/machine learning (AI/ML) and generative modeling to filter and design novel molecules. Discovering novel molecules requires accurate out-of-distribution (OOD) predictions, but ML models struggle to generalize OOD. Currently, no systematic benchmarks exist for molecular OOD prediction tasks. We present $\mathbf{BOOM}$, $\mathbf{b}$enchmarks for $\mathbf{o}$ut-$\mathbf{o}$f-distribution $\mathbf{m}$olecular property predictions: a chemically-informed benchmark for OOD performance on common molecular property prediction tasks. We evaluate over 150 model-task combinations to benchmark deep learning models on OOD performance. Overall, we find that no existing model achieves strong generalization across all tasks: even the top-performing model exhibited an average OOD error 3x higher than in-distribution. Current chemical foundation models do not show strong OOD extrapolation, while models with high inductive bias can perform well on OOD tasks with simple, specific properties. We perform extensive ablation experiments, highlighting how data generation, pre-training, hyperparameter optimization, model architecture, and molecular representation impact OOD performance. Developing models with strong OOD generalization is a new frontier challenge in chemical ML. This open-source benchmark is available at https://github.com/FLASK-LLNL/BOOM
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Focus on Likely Classes for Test-Time Prediction</title>
<link>https://arxiv.org/abs/2505.03819</link>
<guid>https://arxiv.org/abs/2505.03819</guid>
<content:encoded><![CDATA[
arXiv:2505.03819v3 Announce Type: replace 
Abstract: We ask: Can focusing on likely classes of a single, in-domain sample improve model predictions? Prior work argued ``no''. We put forward a novel rationale in favor of ``yes'': Sharedness of features among classes indicates their reliability for a single sample. We aim for an affirmative answer without using hand-engineered augmentations or auxiliary tasks. We propose two novel test-time fine-tuning methods to improve uncertain model predictions. Instead of greedily selecting the most likely class, we introduce an additional step, \emph{focus on the likely classes}, to refine predictions. By applying a single gradient descent step with a large learning rate, we refine predictions when an initial forward pass indicates high uncertainty. The experimental evaluation demonstrates accuracy gains for one of our methods on average, which emphasizes shared features among likely classes. The gains are confirmed across diverse text and image domain models.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DyGSSM: Multi-view Dynamic Graph Embeddings with State Space Model Gradient Update</title>
<link>https://arxiv.org/abs/2505.09017</link>
<guid>https://arxiv.org/abs/2505.09017</guid>
<content:encoded><![CDATA[
arXiv:2505.09017v2 Announce Type: replace 
Abstract: Most of the dynamic graph representation learning methods involve dividing a dynamic graph into discrete snapshots to capture the evolving behavior of nodes over time. Existing methods primarily capture only local or global structures of each node within a snapshot using message-passing and random walk-based methods. Then, they utilize sequence-based models (e.g., transformers) to encode the temporal evolution of node embeddings, and meta-learning techniques to update the model parameters. However, these approaches have two limitations. First, they neglect the extraction of global and local information simultaneously in each snapshot. Second, they fail to consider the model's performance in the current snapshot during parameter updates, resulting in a lack of temporal dependency management. Recently, HiPPO (High-order Polynomial Projection Operators) algorithm has gained attention for their ability to optimize and preserve sequence history in State Space Model (SSM). To address the aforementioned limitations in dynamic graph representation learning, we propose a novel method called Multi-view Dynamic Graph Embeddings with State Space Model Gradient Update (DyGSSM). Our approach combines Graph Convolution Networks (GCN) for local feature extraction and random walk with Gated Recurrent Unit (GRU) for global feature extraction in each snapshot. We then integrate the local and global features using a cross-attention mechanism. Additionally, we incorporate an SSM based on HiPPO algorithm to account for long-term dependencies when updating model parameters, ensuring that model performance in each snapshot informs subsequent updates. Experiments on five public datasets show that our method outperforms existing baseline and state-of-the-art (SOTA) methods in 17 out of 20 cases.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Addition is almost all you need: Compressing neural networks with double binary factorization</title>
<link>https://arxiv.org/abs/2505.11076</link>
<guid>https://arxiv.org/abs/2505.11076</guid>
<content:encoded><![CDATA[
arXiv:2505.11076v3 Announce Type: replace 
Abstract: Binary quantization approaches, which replace weight matrices with binary matrices and substitute costly multiplications with cheaper additions, offer a computationally efficient approach to address the increasing computational and storage requirements of Large Language Models (LLMs). However, the severe quantization constraint ($\pm1$) can lead to significant accuracy degradation. In this paper, we propose Double Binary Factorization (DBF), a novel method that factorizes dense weight matrices into products of two binary (sign) matrices, each accompanied by scaling vectors. DBF preserves the efficiency advantages of binary representations while achieving compression rates that are competitive with or superior to state-of-the-art methods. Specifically, in a 1-bit per weight range, DBF is better than existing binarization approaches. In a 2-bit per weight range, DBF is competitive with the best quantization methods like QuIP\# and QTIP. Unlike most existing compression techniques, which offer limited compression level choices, DBF allows fine-grained control over compression ratios by adjusting the factorization's intermediate dimension. Based on this advantage, we further introduce an algorithm for estimating non-uniform layer-wise compression ratios for DBF, based on previously developed channel pruning criteria.
  Code available at: https://github.com/usamec/double_binary
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shape it Up! Restoring LLM Safety during Finetuning</title>
<link>https://arxiv.org/abs/2505.17196</link>
<guid>https://arxiv.org/abs/2505.17196</guid>
<content:encoded><![CDATA[
arXiv:2505.17196v3 Announce Type: replace 
Abstract: Finetuning large language models (LLMs) enables user-specific customization but introduces critical safety risks: even a few harmful examples can compromise safety alignment. A common mitigation strategy is to update the model more strongly on examples deemed safe, while downweighting or excluding those flagged as unsafe. However, because safety context can shift within a single example, updating the model equally on both harmful and harmless parts of a response is suboptimal-a coarse treatment we term static safety shaping. In contrast, we propose dynamic safety shaping (DSS), a framework that uses fine-grained safety signals to reinforce learning from safe segments of a response while suppressing unsafe content. To enable such fine-grained control during finetuning, we introduce a key insight: guardrail models, traditionally used for filtering, can be repurposed to evaluate partial responses, tracking how safety risk evolves throughout the response, segment by segment. This leads to the Safety Trajectory Assessment of Response (STAR), a token-level signal that enables shaping to operate dynamically over the training sequence. Building on this, we present STAR-DSS, guided by STAR scores, that robustly mitigates finetuning risks and delivers substantial safety improvements across diverse threats, datasets, and model families-all without compromising capability on intended tasks. We encourage future safety research to build on dynamic shaping principles for stronger mitigation against evolving finetuning risks. Our code is publicly available at https://github.com/poloclub/star-dss.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAEs Are Good for Steering -- If You Select the Right Features</title>
<link>https://arxiv.org/abs/2505.20063</link>
<guid>https://arxiv.org/abs/2505.20063</guid>
<content:encoded><![CDATA[
arXiv:2505.20063v2 Announce Type: replace 
Abstract: Sparse Autoencoders (SAEs) have been proposed as an unsupervised approach to learn a decomposition of a model's latent space. This enables useful applications such as steering - influencing the output of a model towards a desired concept - without requiring labeled data. Current methods identify SAE features to steer by analyzing the input tokens that activate them. However, recent work has highlighted that activations alone do not fully describe the effect of a feature on the model's output. In this work, we draw a distinction between two types of features: input features, which mainly capture patterns in the model's input, and output features, which have a human-understandable effect on the model's output. We propose input and output scores to characterize and locate these types of features, and show that high values for both scores rarely co-occur in the same features. These findings have practical implications: after filtering out features with low output scores, we obtain 2-3x improvements when steering with SAEs, making them competitive with supervised methods.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Less is More: Unlocking Specialization of Time Series Foundation Models via Structured Pruning</title>
<link>https://arxiv.org/abs/2505.23195</link>
<guid>https://arxiv.org/abs/2505.23195</guid>
<content:encoded><![CDATA[
arXiv:2505.23195v3 Announce Type: replace 
Abstract: Scaling laws motivate the development of Time Series Foundation Models (TSFMs) that pre-train vast parameters and achieve remarkable zero-shot forecasting performance. Surprisingly, even after fine-tuning, TSFMs cannot consistently outperform smaller, specialized models trained on full-shot downstream data. A key question is how to realize effective adaptation of TSFMs for a target forecasting task. Through empirical studies on various TSFMs, the pre-trained models often exhibit inherent sparsity and redundancy in computation, suggesting that TSFMs have learned to activate task-relevant network substructures to accommodate diverse forecasting tasks. To preserve this valuable prior knowledge, we propose a structured pruning method to regularize the subsequent fine-tuning process by focusing it on a more relevant and compact parameter space. Extensive experiments on seven TSFMs and six benchmarks demonstrate that fine-tuning a smaller, pruned TSFM significantly improves forecasting performance compared to fine-tuning original models. This prune-then-finetune paradigm often enables TSFMs to achieve state-of-the-art performance and surpass strong specialized baselines. Source code is made publicly available at https://github.com/SJTU-DMTai/Prune-then-Finetune.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Slow-thinking LLMs Reason Over Time? Empirical Studies in Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.24511</link>
<guid>https://arxiv.org/abs/2505.24511</guid>
<content:encoded><![CDATA[
arXiv:2505.24511v4 Announce Type: replace 
Abstract: Time series forecasting (TSF) is a fundamental and widely studied task, spanning methods from classical statistical approaches to modern deep learning and multimodal language modeling. Despite their effectiveness, these methods often follow a fast thinking paradigm emphasizing pattern extraction and direct value mapping, while overlooking explicit reasoning over temporal dynamics and contextual dependencies. Meanwhile, emerging slow-thinking LLMs (e.g., ChatGPT-o1, DeepSeek-R1) have demonstrated impressive multi-step reasoning capabilities across diverse domains, suggesting a new opportunity for reframing TSF as a structured reasoning task. This motivates a key question: can slow-thinking LLMs effectively reason over temporal patterns to support time series forecasting, even in zero-shot manner? To investigate this, in this paper, we propose TimeReasoner, an extensive empirical study that formulates TSF as a conditional reasoning task. We design a series of prompting strategies to elicit inference-time reasoning from pretrained slow-thinking LLMs and evaluate their performance across diverse TSF benchmarks. Our findings reveal that slow-thinking LLMs exhibit non-trivial zero-shot forecasting capabilities, especially in capturing high-level trends and contextual shifts. While preliminary, our study surfaces important insights into the reasoning behaviors of LLMs in temporal domains highlighting both their potential and limitations. We hope this work catalyzes further research into reasoning-based forecasting paradigms and paves the way toward more interpretable and generalizable TSF frameworks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MOORL: A Framework for Integrating Offline-Online Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.09574</link>
<guid>https://arxiv.org/abs/2506.09574</guid>
<content:encoded><![CDATA[
arXiv:2506.09574v2 Announce Type: replace 
Abstract: Sample efficiency and exploration remain critical challenges in Deep Reinforcement Learning (DRL), particularly in complex domains. Offline RL, which enables agents to learn optimal policies from static, pre-collected datasets, has emerged as a promising alternative. However, offline RL is constrained by issues such as out-of-distribution (OOD) actions that limit policy performance and generalization. To overcome these limitations, we propose Meta Offline-Online Reinforcement Learning (MOORL), a hybrid framework that unifies offline and online RL for efficient and scalable learning. While previous hybrid methods rely on extensive design components and added computational complexity to utilize offline data effectively, MOORL introduces a meta-policy that seamlessly adapts across offline and online trajectories. This enables the agent to leverage offline data for robust initialization while utilizing online interactions to drive efficient exploration. Our theoretical analysis demonstrates that the hybrid approach enhances exploration by effectively combining the complementary strengths of offline and online data. Furthermore, we demonstrate that MOORL learns a stable Q-function without added complexity. Extensive experiments on 28 tasks from the D4RL and V-D4RL benchmarks validate its effectiveness, showing consistent improvements over state-of-the-art offline and hybrid RL baselines. With minimal computational overhead, MOORL achieves strong performance, underscoring its potential for practical applications in real-world scenarios.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdaLRS: Loss-Guided Adaptive Learning Rate Search for Efficient Foundation Model Pretraining</title>
<link>https://arxiv.org/abs/2506.13274</link>
<guid>https://arxiv.org/abs/2506.13274</guid>
<content:encoded><![CDATA[
arXiv:2506.13274v3 Announce Type: replace 
Abstract: Learning rate is widely regarded as crucial for effective foundation model pretraining. Recent research explores and demonstrates the transferability of learning rate configurations across varying model and dataset sizes, etc. Nevertheless, these approaches are constrained to specific training scenarios and typically necessitate extensive hyperparameter tuning on proxy models. In this work, we propose \textbf{AdaLRS}, a plug-in-and-play adaptive learning rate search algorithm that conducts online optimal learning rate search via optimizing loss descent velocities. We provide theoretical and experimental analyzes to show that foundation model pretraining loss and its descent velocity are both convex and share the same optimal learning rate. Relying solely on training loss dynamics, AdaLRS involves few extra computations to guide the search process, and its convergence is guaranteed via theoretical analysis. Experiments on both LLM and VLM pretraining show that AdaLRS adjusts suboptimal learning rates to the neighborhood of optimum with marked efficiency and effectiveness, with model performance improved accordingly. We also show the robust generalizability of AdaLRS across varying training scenarios, such as different model sizes, training paradigms, base learning rate scheduler choices, and hyperparameter settings.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Value-Free Policy Optimization via Reward Partitioning</title>
<link>https://arxiv.org/abs/2506.13702</link>
<guid>https://arxiv.org/abs/2506.13702</guid>
<content:encoded><![CDATA[
arXiv:2506.13702v3 Announce Type: replace 
Abstract: Single-trajectory reinforcement learning (RL) methods aim to optimize policies from datasets consisting of (prompt, response, reward) triplets, where scalar rewards are directly available. This supervision format is highly practical, as it mirrors real-world human feedback, such as thumbs-up/down signals, and avoids the need for structured preference annotations. In contrast, pairwise preference-based methods like Direct Preference Optimization (DPO) rely on datasets with both preferred and dispreferred responses, which are harder to construct and less natural to collect. Among single-trajectory approaches, Direct Reward Optimization (DRO) has shown strong empirical performance due to its simplicity and stability. However, DRO requires approximating a value function, which introduces several limitations: high off-policy variance, coupling between policy and value learning, and a lack of absolute supervision on the policy itself. We introduce Reward Partitioning Optimization (RPO), a new method that resolves these limitations by removing the need to model the value function. Instead, RPO normalizes observed rewards using a partitioning approach estimated directly from data. This leads to a straightforward supervised learning objective on the policy, with no auxiliary models and no joint optimization. RPO provides direct and stable supervision on the policy, making it robust and easy to implement in practice. We validate RPO on scalar-feedback language modeling tasks using Flan-T5 encoder-decoder models. Our results demonstrate that RPO outperforms existing single-trajectory baselines such as DRO and Kahneman-Tversky Optimization (KTO). These findings confirm that RPO is a simple, effective, and theoretically grounded method for single-trajectory policy optimization.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Subspace-Boosted Model Merging</title>
<link>https://arxiv.org/abs/2506.16506</link>
<guid>https://arxiv.org/abs/2506.16506</guid>
<content:encoded><![CDATA[
arXiv:2506.16506v3 Announce Type: replace 
Abstract: Model merging enables the combination of multiple specialized expert models into a single model capable of performing multiple tasks. However, the benefits of merging an increasing amount of specialized experts generally lead to diminishing returns and reduced overall performance gains. In this work, we empirically and theoretically analyze this limitation, proving that for Task Arithmetic-based methods, as more experts are merged, the common information dominates the task-specific information, leading to inevitable rank collapse. To mitigate this issue, we introduce Subspace Boosting, which operates on the singular value decomposed task vector space and maintains task vector ranks. Subspace Boosting raises merging efficacy for up to 20 experts by large margins of more than 10% when evaluated on both vision and language benchmarks. Moreover, we propose employing Higher-Order Generalized Singular Value Decomposition to quantify task similarity, offering a new interpretable perspective on model merging. Code and models are available at https://github.com/ronskoro/Subspace-Boosting.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ESSA: Evolutionary Strategies for Scalable Alignment</title>
<link>https://arxiv.org/abs/2507.04453</link>
<guid>https://arxiv.org/abs/2507.04453</guid>
<content:encoded><![CDATA[
arXiv:2507.04453v3 Announce Type: replace 
Abstract: Alignment of Large Language Models (LLMs) typically relies on Reinforcement Learning from Human Feedback (RLHF) with gradient-based optimizers such as Proximal Policy Optimization (PPO) or Group Relative Policy Optimization (GRPO). While effective, these methods require complex distributed training, large memory budgets, and careful hyperparameter tuning, all of which become increasingly difficult at billion-parameter scale. We present ESSA, Evolutionary Strategies for Scalable Alignment, a gradient-free framework that aligns LLMs using only forward inference and black-box optimization. ESSA focuses optimization on Low-Rank Adapters (LoRA) and further compresses their parameter space by optimizing only the singular values from an singular value decomposition (SVD) of each adapter matrix. This dimensionality reduction makes evolutionary search practical even for very large models and allows efficient operation in quantized INT4 and INT8 inference mode. Across these benchmarks ESSA improves the test accuracy of Qwen2.5-Math-7B by 12.6% on GSM8K and 14.8% on PRM800K, and raises the accuracy of LLaMA3.1-8B on IFEval by 22.5%, all compared with GRPO. In large-scale settings ESSA shows stronger scaling than gradient-based methods: on Qwen2.5-32B for PRM800K it reaches near-optimal accuracy twice as fast on 16 GPUs and six times as fast on 128 GPUs compared with GRPO. These results position evolutionary strategies as a compelling, hardware-friendly alternative to gradient-based LLM alignment, combining competitive quality with substantially reduced wall-clock time and engineering overhead.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Novelty to Imitation: Self-Distilled Rewards for Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.12815</link>
<guid>https://arxiv.org/abs/2507.12815</guid>
<content:encoded><![CDATA[
arXiv:2507.12815v2 Announce Type: replace 
Abstract: Offline Reinforcement Learning (RL) aims to learn effective policies from a static dataset without requiring further agent-environment interactions. However, its practical adoption is often hindered by the need for explicit reward annotations, which can be costly to engineer or difficult to obtain retrospectively. To address this, we propose ReLOAD (Reinforcement Learning with Offline Reward Annotation via Distillation), a novel reward annotation framework for offline RL. Unlike existing methods that depend on complex alignment procedures, our approach adapts Random Network Distillation (RND) to generate intrinsic rewards from expert demonstrations using a simple yet effective embedding discrepancy measure. First, we train a predictor network to mimic a fixed target network's embeddings based on expert state transitions. Later, the prediction error between these networks serves as a reward signal for each transition in the static dataset. This mechanism provides a structured reward signal without requiring handcrafted reward annotations. We provide a formal theoretical construct that offers insights into how RND prediction errors effectively serve as intrinsic rewards by distinguishing expert-like transitions. Experiments on the D4RL benchmark demonstrate that ReLOAD enables robust offline policy learning and achieves performance competitive with traditional reward-annotated methods.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vidar: Embodied Video Diffusion Model for Generalist Manipulation</title>
<link>https://arxiv.org/abs/2507.12898</link>
<guid>https://arxiv.org/abs/2507.12898</guid>
<content:encoded><![CDATA[
arXiv:2507.12898v4 Announce Type: replace 
Abstract: Scaling general-purpose manipulation to new robot embodiments remains challenging: each platform typically needs large, homogeneous demonstrations, and end-to-end pixel-to-action pipelines may degenerate under background and viewpoint shifts. Based on previous advances in video-based robot control, we present Vidar, consisting of an embodied video diffusion model as the generalizable prior and a masked inverse dynamics model (MIDM) as the adapter. We leverage a video diffusion model pre-trained at Internet scale, and further continuously pre-train it for the embodied domain using 750K multi-view trajectories collected from three real-world robot platforms. For this embodied pre-training, we introduce a unified observation space that jointly encodes robot, camera, task, and scene contexts. The MIDM module learns action-relevant pixel masks without dense labels, grounding the prior into the target embodiment's action space while suppressing distractors. With only 20 minutes of human demonstrations on an unseen robot (1% of typical data), Vidar outperforms state-of-the-art baselines and generalizes to unseen tasks, backgrounds, and camera layouts. Our results suggest a scalable recipe for "one prior, many embodiments": strong, inexpensive video priors together with minimal on-robot alignment.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prediction-Oriented Subsampling from Data Streams</title>
<link>https://arxiv.org/abs/2508.03868</link>
<guid>https://arxiv.org/abs/2508.03868</guid>
<content:encoded><![CDATA[
arXiv:2508.03868v2 Announce Type: replace 
Abstract: Data is often generated in streams, with new observations arriving over time. A key challenge for learning models from data streams is capturing relevant information while keeping computational costs manageable. We explore intelligent data subsampling for offline learning, and argue for an information-theoretic method centred on reducing uncertainty in downstream predictions of interest. Empirically, we demonstrate that this prediction-oriented approach performs better than a previously proposed information-theoretic technique on two widely studied problems. At the same time, we highlight that reliably achieving strong performance in practice requires careful model design.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Low-Regret and Low-Complexity Learning for Hierarchical Inference</title>
<link>https://arxiv.org/abs/2508.08985</link>
<guid>https://arxiv.org/abs/2508.08985</guid>
<content:encoded><![CDATA[
arXiv:2508.08985v3 Announce Type: replace 
Abstract: This work focuses on Hierarchical Inference (HI) in edge intelligence systems, where a compact Local-ML model on an end-device works in conjunction with a high-accuracy Remote-ML model on an edge-server. HI aims to reduce latency, improve accuracy, and lower bandwidth usage by first using the Local-ML model for inference and offloading to the Remote-ML only when the local inference is likely incorrect. A critical challenge in HI is estimating the likelihood of the local inference being incorrect, especially when data distributions and offloading costs change over time -- a problem we term Hierarchical Inference Learning (HIL). We introduce a novel approach to HIL by modeling the probability of correct inference by the Local-ML as an increasing function of the model's confidence measure, a structure motivated by empirical observations but previously unexploited. We propose two policies, HI-LCB and HI-LCB-lite, based on the Upper Confidence Bound (UCB) framework. We demonstrate that both policies achieve order-optimal regret of $O(\log T)$, a significant improvement over existing HIL policies with $O(T^{2/3})$ regret guarantees. Notably, HI-LCB-lite has an $O(1)$ per-sample computational complexity, making it well-suited for deployment on devices with severe resource limitations. Simulations using real-world datasets confirm that our policies outperform existing state-of-the-art HIL methods.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prototype-Guided Diffusion: Visual Conditioning without External Memory</title>
<link>https://arxiv.org/abs/2508.09922</link>
<guid>https://arxiv.org/abs/2508.09922</guid>
<content:encoded><![CDATA[
arXiv:2508.09922v5 Announce Type: replace 
Abstract: Diffusion models achieve state-of-the-art image generation but remain computationally costly due to iterative denoising. Latent-space models like Stable Diffusion reduce overhead yet lose fine detail, while retrieval-augmented methods improve efficiency but rely on large memory banks, static similarity models, and rigid infrastructures. We introduce the Prototype Diffusion Model (PDM), which embeds prototype learning into the diffusion process to provide adaptive, memory-free conditioning. Instead of retrieving references, PDM learns compact visual prototypes from clean features via contrastive learning, then aligns noisy representations with semantically relevant patterns during denoising. Experiments demonstrate that PDM sustains high generation quality while lowering computational and storage costs, offering a scalable alternative to retrieval-based conditioning.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-driven particle dynamics: Structure-preserving coarse-graining for emergent behavior in non-equilibrium systems</title>
<link>https://arxiv.org/abs/2508.12569</link>
<guid>https://arxiv.org/abs/2508.12569</guid>
<content:encoded><![CDATA[
arXiv:2508.12569v2 Announce Type: replace 
Abstract: Multiscale systems are ubiquitous in science and technology, but are notoriously challenging to simulate as short spatiotemporal scales must be appropriately linked to emergent bulk physics. When expensive high-dimensional dynamical systems are coarse-grained into low-dimensional models, the entropic loss of information leads to emergent physics which are dissipative, history-dependent, and stochastic. To machine learn coarse-grained dynamics from time-series observations of particle trajectories, we propose a framework using the metriplectic bracket formalism that preserves these properties by construction; most notably, the framework guarantees discrete notions of the first and second laws of thermodynamics, conservation of momentum, and a discrete fluctuation-dissipation balance crucial for capturing non-equilibrium statistics. We introduce the mathematical framework abstractly before specializing to a particle discretization. As labels are generally unavailable for entropic state variables, we introduce a novel self-supervised learning strategy to identify emergent structural variables. We validate the method on benchmark systems and demonstrate its utility on two challenging examples: (1) coarse-graining star polymers at challenging levels of coarse-graining while preserving non-equilibrium statistics, and (2) learning models from high-speed video of colloidal suspensions that capture coupling between local rearrangement events and emergent stochastic dynamics. We provide open-source implementations in both PyTorch and LAMMPS, enabling large-scale inference and extensibility to diverse particle-based systems.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Graph Spectral Clustering For GloVe-like Text Embeddings</title>
<link>https://arxiv.org/abs/2508.14075</link>
<guid>https://arxiv.org/abs/2508.14075</guid>
<content:encoded><![CDATA[
arXiv:2508.14075v2 Announce Type: replace 
Abstract: In a previous paper, we proposed an introduction to the explainability of Graph Spectral Clustering results for textual documents, given that document similarity is computed as cosine similarity in term vector space.
  In this paper, we generalize this idea by considering other embeddings of documents, in particular, based on the GloVe embedding idea.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PLaID++: A Preference Aligned Language Model for Targeted Inorganic Materials Design</title>
<link>https://arxiv.org/abs/2509.07150</link>
<guid>https://arxiv.org/abs/2509.07150</guid>
<content:encoded><![CDATA[
arXiv:2509.07150v3 Announce Type: replace 
Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a promising approach to improve correctness in LLMs, however, in many scientific problems, the objective is not necessarily to produce the correct answer, but instead to produce a diverse array of candidates which satisfy a set of constraints. We study this challenge in the context of materials generation. To this end, we introduce PLaID++, an LLM post-trained for stable and property-guided crystal generation. We find that performance hinges on our crystallographic representation and reward formulation. First, we introduce a compact, symmetry-informed Wyckoff text representation which improves computational efficiency and encourages generalization from physical priors. Second, we demonstrate that temperature scaling acts as an entropy regularizer which counteracts mode collapse and encourages exploration. By encoding symmetry constraints directly into text and guiding model outputs towards desirable chemical space, PLaID++ generates structures that are thermodynamically stable, unique, and novel at a $\sim$50\% greater rate than prior methods and conditionally generates structures with desired space group properties. Our work demonstrates the potential of adapting post-training techniques from natural language processing to materials design, paving the way for targeted and efficient discovery of novel materials.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Using LLMs for Late Multimodal Sensor Fusion for Activity Recognition</title>
<link>https://arxiv.org/abs/2509.10729</link>
<guid>https://arxiv.org/abs/2509.10729</guid>
<content:encoded><![CDATA[
arXiv:2509.10729v3 Announce Type: replace 
Abstract: Sensor data streams provide valuable information around activities and context for downstream applications, though integrating complementary information can be challenging. We show that large language models (LLMs) can be used for late fusion for activity classification from audio and motion time series data. We curated a subset of data for diverse activity recognition across contexts (e.g., household activities, sports) from the Ego4D dataset. Evaluated LLMs achieved 12-class zero- and one-shot classification F1-scores significantly above chance, with no task-specific training. Zero-shot classification via LLM-based fusion from modality-specific models can enable multimodal temporal applications where there is limited aligned training data for learning a shared embedding space. Additionally, LLM-based fusion can enable model deploying without requiring additional memory and computation for targeted application-specific multimodal models.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Supervised Learning of Graph Representations for Network Intrusion Detection</title>
<link>https://arxiv.org/abs/2509.16625</link>
<guid>https://arxiv.org/abs/2509.16625</guid>
<content:encoded><![CDATA[
arXiv:2509.16625v5 Announce Type: replace 
Abstract: Detecting intrusions in network traffic is a challenging task, particularly under limited supervision and constantly evolving attack patterns. While recent works have leveraged graph neural networks for network intrusion detection, they often decouple representation learning from anomaly detection, limiting the utility of the embeddings for identifying attacks. We propose GraphIDS, a self-supervised intrusion detection model that unifies these two stages by learning local graph representations of normal communication patterns through a masked autoencoder. An inductive graph neural network embeds each flow with its local topological context to capture typical network behavior, while a Transformer-based encoder-decoder reconstructs these embeddings, implicitly learning global co-occurrence patterns via self-attention without requiring explicit positional information. During inference, flows with unusually high reconstruction errors are flagged as potential intrusions. This end-to-end framework ensures that embeddings are directly optimized for the downstream task, facilitating the recognition of malicious traffic. On diverse NetFlow benchmarks, GraphIDS achieves up to 99.98% PR-AUC and 99.61% macro F1-score, outperforming baselines by 5-25 percentage points.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GenUQ: Predictive Uncertainty Estimates via Generative Hyper-Networks</title>
<link>https://arxiv.org/abs/2509.21605</link>
<guid>https://arxiv.org/abs/2509.21605</guid>
<content:encoded><![CDATA[
arXiv:2509.21605v2 Announce Type: replace 
Abstract: Operator learning is a recently developed generalization of regression to mappings between functions. It promises to drastically reduce expensive numerical integration of PDEs to fast evaluations of mappings between functional states of a system, i.e., surrogate and reduced-order modeling. Operator learning has already found applications in several areas such as modeling sea ice, combustion, and atmospheric physics. Recent approaches towards integrating uncertainty quantification into the operator models have relied on likelihood based methods to infer parameter distributions from noisy data. However, stochastic operators may yield actions from which a likelihood is difficult or impossible to construct. In this paper, we introduce, GenUQ, a measure-theoretic approach to UQ that avoids constructing a likelihood by introducing a generative hyper-network model that produces parameter distributions consistent with observed data. We demonstrate that GenUQ outperforms other UQ methods in three example problems, recovering a manufactured operator, learning the solution operator to a stochastic elliptic PDE, and modeling the failure location of porous steel under tension.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Brain-language fusion enables interactive neural readout and in-silico experimentation</title>
<link>https://arxiv.org/abs/2509.23941</link>
<guid>https://arxiv.org/abs/2509.23941</guid>
<content:encoded><![CDATA[
arXiv:2509.23941v2 Announce Type: replace 
Abstract: Large language models (LLMs) have revolutionized human-machine interaction, and have been extended by embedding diverse modalities such as images into a shared language space. Yet, neural decoding has remained constrained by static, non-interactive methods. We introduce CorText, a framework that integrates neural activity directly into the latent space of an LLM, enabling open-ended, natural language interaction with brain data. Trained on fMRI data recorded during viewing of natural scenes, CorText generates accurate image captions and can answer more detailed questions better than controls, while having access to neural data only. We showcase that CorText achieves zero-shot generalization beyond semantic categories seen during training. In-silico microstimulation experiments, which enable counterfactual prompts on brain activity, reveal a consistent, and graded mapping between brain-state and language output. These advances mark a shift from passive decoding toward generative, flexible interfaces between brain activity and language.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OAT-FM: Optimal Acceleration Transport for Improved Flow Matching</title>
<link>https://arxiv.org/abs/2509.24936</link>
<guid>https://arxiv.org/abs/2509.24936</guid>
<content:encoded><![CDATA[
arXiv:2509.24936v2 Announce Type: replace 
Abstract: As a powerful technique in generative modeling, Flow Matching (FM) aims to learn velocity fields from noise to data, which is often explained and implemented as solving Optimal Transport (OT) problems. In this study, we bridge FM and the recent theory of Optimal Acceleration Transport (OAT), developing an improved FM method called OAT-FM and exploring its benefits in both theory and practice. In particular, we demonstrate that the straightening objective hidden in existing OT-based FM methods is mathematically equivalent to minimizing the physical action associated with acceleration defined by OAT. Accordingly, instead of enforcing constant velocity, OAT-FM optimizes the acceleration transport in the product space of sample and velocity, whose objective corresponds to a necessary and sufficient condition of flow straightness. An efficient algorithm is designed to achieve OAT-FM with low complexity. OAT-FM motivates a new two-phase FM paradigm: Given a generative model trained by an arbitrary FM method, whose velocity information has been relatively reliable, we can fine-tune and improve it via OAT-FM. This paradigm eliminates the risk of data distribution drift and the need to generate a large number of noise data pairs, which consistently improves model performance in various generative tasks. Code is available at: https://github.com/AngxiaoYue/OAT-FM
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EEsizer: LLM-Based AI Agent for Sizing of Analog and Mixed Signal Circuit</title>
<link>https://arxiv.org/abs/2509.25510</link>
<guid>https://arxiv.org/abs/2509.25510</guid>
<content:encoded><![CDATA[
arXiv:2509.25510v2 Announce Type: replace 
Abstract: The design of Analog and Mixed-Signal (AMS) integrated circuits (ICs) often involves significant manual effort, especially during the transistor sizing process. While Machine Learning techniques in Electronic Design Automation (EDA) have shown promise in reducing complexity and minimizing human intervention, they still face challenges such as numerous iterations and a lack of knowledge about AMS circuit design. Recently, Large Language Models (LLMs) have demonstrated significant potential across various fields, showing a certain level of knowledge in circuit design and indicating their potential to automate the transistor sizing process. In this work, we propose EEsizer, an LLM-based AI agent that integrates large language models with circuit simulators and custom data analysis functions, enabling fully automated, closed-loop transistor sizing without relying on external knowledge. By employing prompt engineering and Chain-of-Thought reasoning, the agent iteratively explores design directions, evaluates performance, and refines solutions with minimal human intervention. We first benchmarked 8 LLMs on six basic circuits and selected three high-performing models to optimize a 20-transistor CMOS operational amplifier, targeting multiple performance metrics, including rail-to-rail operation from 180 nm to 90 nm technology nodes. Notably, OpenAI o3 successfully achieved the user-intended target at 90 nm across three different test groups, with a maximum of 20 iterations, demonstrating adaptability and robustness at advanced nodes. To assess design robustness, we manually designed a bias circuit and performed a variation analysis using Gaussian-distributed variations on transistor dimensions and threshold voltages.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Neural Operators for Real-Time Biomechanical Modelling of Traumatic Brain Injury</title>
<link>https://arxiv.org/abs/2510.03248</link>
<guid>https://arxiv.org/abs/2510.03248</guid>
<content:encoded><![CDATA[
arXiv:2510.03248v2 Announce Type: replace 
Abstract: Background: Traumatic brain injury (TBI) is a major global health concern with 69 million annual cases. While neural operators have revolutionized scientific computing, existing architectures cannot handle the heterogeneous multimodal data (anatomical imaging, scalar demographics, and geometric constraints) required for patient-specific biomechanical modeling. Objective: This study introduces the first multimodal neural operator framework for biomechanics, fusing heterogeneous inputs to predict brain displacement fields for rapid TBI risk assessment. Methods: TBI modeling was reformulated as a multimodal operator learning problem. We proposed two fusion strategies: field projection for Fourier Neural Operator (FNO) architectures and branch decomposition for Deep Operator Networks (DeepONet). Four architectures (FNO, Factorized FNO, Multi-Grid FNO, and DeepONet) were extended with fusion mechanisms and evaluated on 249 in vivo Magnetic Resonance Elastography (MRE) datasets (20-90 Hz). Results: Multi-Grid FNO achieved the highest accuracy (MSE = 0.0023, 94.3% spatial fidelity). DeepONet offered the fastest inference (14.5 iterations/s, 7x speedup), suitable for edge deployment. All architectures reduced computation from hours to milliseconds. Conclusion: Multimodal neural operators enable efficient, real-time, patient-specific TBI risk assessment. This framework establishes a generalizable paradigm for heterogeneous data fusion in scientific domains, including precision medicine.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TAPAS: Datasets for Learning the Learning with Errors Problem</title>
<link>https://arxiv.org/abs/2510.08797</link>
<guid>https://arxiv.org/abs/2510.08797</guid>
<content:encoded><![CDATA[
arXiv:2510.08797v2 Announce Type: replace 
Abstract: AI-powered attacks on Learning with Errors (LWE), an important hard math problem in post-quantum cryptography, rival or outperform "classical" attacks on LWE under certain parameter settings. Despite the promise of this approach, a dearth of accessible data limits AI practitioners' ability to study and improve these attacks. Creating LWE data for AI model training is time- and compute-intensive and requires significant domain expertise. To fill this gap and accelerate AI research on LWE attacks, we propose the TAPAS datasets, a Toolkit for Analysis of Post-quantum cryptography using AI Systems. These datasets cover several LWE settings and can be used off-the-shelf by AI practitioners to prototype new approaches to cracking LWE. This work documents TAPAS dataset creation, establishes attack performance baselines, and lays out directions for future work.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GraphShaper: Geometry-aware Alignment for Improving Transfer Learning in Text-Attributed Graphs</title>
<link>https://arxiv.org/abs/2510.12085</link>
<guid>https://arxiv.org/abs/2510.12085</guid>
<content:encoded><![CDATA[
arXiv:2510.12085v2 Announce Type: replace 
Abstract: Graph foundation models represent a transformative paradigm for learning transferable representations across diverse graph domains. Recent methods leverage large language models to unify graph and text modalities into a shared representation space using contrastive learning. However, systematic evaluations reveal significant performance degradation at structural boundaries where distinct topological patterns converge, with accuracy losses exceeding 20 percentage points. This issue arises from a key limitation: current methods assume all graph structures can be encoded within a single Euclidean space. In reality, tree structures require hyperbolic geometry to preserve hierarchical branching, while cyclic patterns depend on spherical geometry for closure properties. At structural boundaries, nodes experience conflicting geometric constraints that uniform encoding spaces cannot resolve. This raises a crucial challenge: \textbf{Can alignment frameworks be designed to respect the intrinsic geometric diversity of graph structures?} We introduce \textbf{GraphShaper}, a geometry-aware framework that enhances graph encoding through multi-geometric specialization. Our approach employs expert networks tailored to different geometric spaces, dynamically computing fusion weights to adaptively integrate geometric properties based on local structural characteristics. This adaptive fusion preserves structural integrity before alignment with text embeddings. Extensive experiments demonstrate that GraphShaper achieves 9.47\% accuracy improvements on citation networks and 7.63\% on social networks in zero-shot settings.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WebATLAS: An LLM Agent with Experience-Driven Memory and Action Simulation</title>
<link>https://arxiv.org/abs/2510.22732</link>
<guid>https://arxiv.org/abs/2510.22732</guid>
<content:encoded><![CDATA[
arXiv:2510.22732v2 Announce Type: replace 
Abstract: Large Language Model (LLM) web agents often struggle with long-horizon web navigation and web task completion in new websites, producing inefficient action sequences unless fine-tuned on environment-specific data. We show that experience-driven memory, combined with look-ahead action simulation, is sufficient for LLM agents to adapt to unseen web environments by remembering past failures and predicting the consequences of future actions. We introduce WebATLAS (Actor-Critic Task-completion with Look-ahead Action Simulation), a memory-augmented LLM web agent that learns a lightweight internal model of the environment from interaction experience and performs hypothetical action rollouts before acting in the real world. WebATLAS builds a persistent cognitive map via curiosity-driven exploration, stores interaction outcomes as experience-based memory, and evaluates candidate actions in cognitive space using a planner--simulator--critic loop. This enables the agent to reuse past experience, avoid previously unsuccessful behaviors, and generate more efficient plans. We evaluate WebATLAS on the WebArena-Lite benchmark for autonomous web navigation and demonstrate a success rate of 63%, outperforming the previous state-of-the-art at 53.9%. Unlike previous systems, our modular architecture requires no website-specific LLM fine-tuning. Ablation studies confirm that experience-driven memory, look-ahead action simulation, and hierarchical replanning play complementary roles in enabling robust, training-free web agents.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sharpness-Controlled Group Relative Policy Optimization with Token-Level Probability Shaping</title>
<link>https://arxiv.org/abs/2511.00066</link>
<guid>https://arxiv.org/abs/2511.00066</guid>
<content:encoded><![CDATA[
arXiv:2511.00066v2 Announce Type: replace 
Abstract: Reinforcement learning with verifiable rewards (RLVR) has become a practical route to improve large language model reasoning, and Group Relative Policy Optimization (GRPO) is a widely used optimizer in this setting. This paper revisits GRPO from a generalization perspective. Recent analysis shows that population performance can be controlled by a robust empirical objective that decomposes into the training loss plus a sharpness term measured by the gradient norm. We develop a token-level view of this sharpness term and show that GRPO can be dominated by a small subset of tokens with disproportionately large per-token gradients, which increases sharpness and can harm generalization. Motivated by this view, we propose Token-Regulated GRPO (TR-GRPO), which introduces a monotone probability shaping function to assign token weights based on the model's own token probabilities, and integrates these weights into the standard GRPO. Our analysis yields a bound that isolates a probability dependent multiplicative factor in token-gradient magnitudes, explaining how probability-aware weighting suppresses sharp directions while preserving learning signal on semantically critical tokens. Experiments on logic puzzles, mathematical reasoning, and tool-augmented question answering show consistent improvements over GRPO, along with smoother gradient-norm trajectories, supporting TR-GRPO as a simple and effective generalization-oriented upgrade to GRPO for RLVR.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Memory-Efficient Training with In-Place FFT Implementation</title>
<link>https://arxiv.org/abs/2511.01385</link>
<guid>https://arxiv.org/abs/2511.01385</guid>
<content:encoded><![CDATA[
arXiv:2511.01385v2 Announce Type: replace 
Abstract: Fast Fourier Transforms (FFT) are widely used to reduce memory and computational costs in deep learning. However, existing implementations, including standard FFT and real FFT (rFFT), cannot achieve true in-place computation. In particular, rFFT maps an input of size n to a complex output of size n/2+1, causing dimensional mismatch and requiring additional memory allocation. We propose the first real-domain, fully in-place FFT framework (rdFFT) that preserves input-output memory space consistency. By leveraging butterfly operation symmetry and conjugate properties in the frequency domain, we design an implicit complex encoding scheme that eliminates intermediate cache usage entirely. Experiments on multiple natural language understanding tasks demonstrate the method effectiveness in reducing training memory cost, offering a promising direction for frequency-domain lightweight adaptation.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal Graph Neural Networks for Healthcare</title>
<link>https://arxiv.org/abs/2511.02531</link>
<guid>https://arxiv.org/abs/2511.02531</guid>
<content:encoded><![CDATA[
arXiv:2511.02531v3 Announce Type: replace 
Abstract: Healthcare artificial intelligence systems routinely fail when deployed across institutions, with documented performance drops and perpetuation of discriminatory patterns embedded in historical data. This brittleness stems, in part, from learning statistical associations rather than causal mechanisms. Causal graph neural networks address this triple crisis of distribution shift, discrimination, and inscrutability by combining graph-based representations of biomedical data with causal inference principles to learn invariant mechanisms rather than spurious correlations. This Review examines methodological foundations spanning structural causal models, disentangled causal representation learning, and techniques for interventional prediction and counterfactual reasoning on graphs. We analyse applications demonstrating clinical value across psychiatric diagnosis through brain network analysis, cancer subtyping via multi-omics causal integration, continuous physiological monitoring with mechanistic interpretation, and drug recommendation correcting prescription bias. These advances establish foundations for patient-specific Causal Digital Twins, enabling in silico clinical experimentation, with integration of large language models for hypothesis generation and causal graph neural networks for mechanistic validation. Substantial barriers remain, including computational requirements precluding real-time deployment, validation challenges demanding multi-modal evidence triangulation beyond cross-validation, and risks of causal-washing where methods employ causal terminology without rigorous evidentiary support. We propose tiered frameworks distinguishing causally-inspired architectures from causally-validated discoveries and identify critical research priorities making causal rather than purely associational claims.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In Good GRACEs: Principled Teacher Selection for Knowledge Distillation</title>
<link>https://arxiv.org/abs/2511.02833</link>
<guid>https://arxiv.org/abs/2511.02833</guid>
<content:encoded><![CDATA[
arXiv:2511.02833v3 Announce Type: replace 
Abstract: Knowledge distillation is an efficient strategy to use data generated by large "teacher" language models to train smaller capable "student" models, but selecting the optimal teacher for a specific student-task combination requires expensive trial-and-error. We propose a lightweight score called GRACE to quantify how effective a teacher will be for post-training a student model. GRACE measures distributional properties of the student's gradients without access to a verifier, teacher logits, teacher internals, or test data. From an information-theoretic perspective, GRACE connects to leave-one-out stability of gradient-based algorithms, which controls the generalization performance of the distilled students. On GSM8K and MATH, GRACE correlates strongly (up to 86% Spearman correlation) with the performance of the distilled LLaMA and OLMo students. In particular, training a student using the GRACE-selected teacher can improve the performance by up to 7.4% over naively using the best-performing teacher. Further, GRACE can provide guidance on crucial design choices in distillation, including (1) the best temperature to use when generating from the teacher, (2) the best teacher to use given a size constraint, and (3) the best teacher to use within a specific model family. Altogether, our findings demonstrate that GRACE can efficiently and effectively identify a strongly compatible teacher for a given student and provide fine-grained guidance on how to perform distillation.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Nowcast3D: Reliable precipitation nowcasting via gray-box learning</title>
<link>https://arxiv.org/abs/2511.04659</link>
<guid>https://arxiv.org/abs/2511.04659</guid>
<content:encoded><![CDATA[
arXiv:2511.04659v3 Announce Type: replace 
Abstract: Extreme-precipitation nowcasting requires high spatial and temporal resolution together with extended lead times, yet current approaches remain constrained. Numerical weather prediction systems and their deep-learning emulators operate at relatively coarse space-time resolution and struggle to capture rapidly evolving convective systems. Radar extrapolation methods, which advect recent fields using estimated motion, have difficulty capturing the complex evolution of precipitation. Purely data-driven models often produce overly smoothed reflectivity fields and underestimate intensity. Hybrid 2D radar-based methods discard crucial vertical information, preventing accurate reconstruction of height-dependent dynamics. We introduce Nowcast3D, a gray-box, fully three-dimensional nowcasting framework that operates directly on volumetric radar reflectivity and couples physically constrained neural operators with data-driven learning. The model learns three fields that govern reflectivity evolution: a three-dimensional flow field for advective transport, a spatially varying diffusion field for local dispersive spreading, and a residual source term for unresolved microphysical effects. These learned operators advance the forecast in time under explicit physical constraints, while a conditional diffusion model, conditioned on both the observations and the physics-based forecast, generates ensembles of future radar volumes that quantify forecast uncertainty. In a blind evaluation by 160 meteorologists, Nowcast3D is preferred in 57% of post-hoc and 51% of prior assessments. By explicitly embedding three-dimensional dynamics and uncertainty into a single framework, Nowcast3D offers a scalable and robust approach for reliable nowcasting of extreme precipitation.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DETECT: Data-Driven Evaluation of Treatments Enabled by Classification Transformers</title>
<link>https://arxiv.org/abs/2511.07213</link>
<guid>https://arxiv.org/abs/2511.07213</guid>
<content:encoded><![CDATA[
arXiv:2511.07213v2 Announce Type: replace 
Abstract: Chronic pain is a global health challenge affecting millions of individuals, making it essential for physicians to have reliable and objective methods to measure the functional impact of clinical treatments. Traditionally used methods, like the numeric rating scale, while personalized and easy to use, are subjective due to their self-reported nature. Thus, this paper proposes DETECT (Data-Driven Evaluation of Treatments Enabled by Classification Transformers), a data-driven framework that assesses treatment success by comparing patient activities of daily life before and after treatment. We use DETECT on public benchmark datasets and simulated patient data from smartphone sensors. Our results demonstrate that DETECT is objective yet lightweight, making it a significant and novel contribution to clinical decision-making. By using DETECT, independently or together with other self-reported metrics, physicians can improve their understanding of their treatment impacts, ultimately leading to more personalized and responsive patient care.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Preparation of Fractal-Inspired Computational Architectures for Advanced Large Language Model Analysis</title>
<link>https://arxiv.org/abs/2511.07329</link>
<guid>https://arxiv.org/abs/2511.07329</guid>
<content:encoded><![CDATA[
arXiv:2511.07329v2 Announce Type: replace 
Abstract: It introduces FractalNet, a fractal-inspired computational architectures for advanced large language model analysis that mainly challenges model diversity on a large scale in an efficient manner. The new set-up involves a template-driven generator, runner, and evaluation framework that, through systematic permutations of convolutional, normalization, activation, and dropout layers, can create more than 1,200 variants of neural networks. Fractal templates allow for structural recursion and multi-column pathways, thus, models become deeper and wider in a balanced way. Training utilizes PyTorch, Automatic Mixed Precision (AMP), and gradient checkpointing and is carried out on the CIFAR-10 dataset for five epochs. The outcomes show that fractal-based architectures are capable of strong performance and are computationally efficient. The paper positions fractal design as a feasible and resource-efficient method of automated architecture exploration.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Potent but Stealthy: Rethink Profile Pollution against Sequential Recommendation via Bi-level Constrained Reinforcement Paradigm</title>
<link>https://arxiv.org/abs/2511.09392</link>
<guid>https://arxiv.org/abs/2511.09392</guid>
<content:encoded><![CDATA[
arXiv:2511.09392v4 Announce Type: replace 
Abstract: Sequential Recommenders, which exploit dynamic user intents through interaction sequences, is vulnerable to adversarial attacks. While existing attacks primarily rely on data poisoning, they require large-scale user access or fake profiles thus lacking practicality. In this paper, we focus on the Profile Pollution Attack that subtly contaminates partial user interactions to induce targeted mispredictions. Previous PPA methods suffer from two limitations, i.e., i) over-reliance on sequence horizon impact restricts fine-grained perturbations on item transitions, and ii) holistic modifications cause detectable distribution shifts. To address these challenges, we propose a constrained reinforcement driven attack CREAT that synergizes a bi-level optimization framework with multi-reward reinforcement learning to balance adversarial efficacy and stealthiness. We first develop a Pattern Balanced Rewarding Policy, which integrates pattern inversion rewards to invert critical patterns and distribution consistency rewards to minimize detectable shifts via unbalanced co-optimal transport. Then we employ a Constrained Group Relative Reinforcement Learning paradigm, enabling step-wise perturbations through dynamic barrier constraints and group-shared experience replay, achieving targeted pollution with minimal detectability. Extensive experiments demonstrate the effectiveness of CREAT.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What-If Decision Support for Product Line Extension Using Conditional Deep Generative Models</title>
<link>https://arxiv.org/abs/2511.11646</link>
<guid>https://arxiv.org/abs/2511.11646</guid>
<content:encoded><![CDATA[
arXiv:2511.11646v2 Announce Type: replace 
Abstract: Product line extension is a strategically important managerial decision that requires anticipating how consumer segments and purchasing contexts may respond to hypothetical product designs that do not yet exist in the market. Such decisions are inherently uncertain because managers must infer future outcomes from historical purchase data without direct market observations. This study addresses this challenge by proposing a data-driven decision support framework that enables forward-looking what-if analysis based on historical transaction data. We introduce a Conditional Tabular Variational Autoencoder (CTVAE) that learns the conditional joint distribution of product attributes and consumer characteristics from large-scale tabular data. By conditioning the generative process on controllable design variables such as container type, volume, flavor, and calorie content, the proposed model generates synthetic consumer attribute distributions for hypothetical line-extended products. This enables systematic exploration of alternative design scenarios without costly market pretests. The framework is evaluated using home-scan panel data covering more than 20,000 consumers and 700 soft drink products. Empirical results show that the CTVAE outperforms existing tabular generative models in capturing conditional consumer attribute distributions. Simulation-based analyses further demonstrate that the generated synthetic data support knowledge-driven reasoning for assessing cannibalization risks and identifying potential target segments. These findings highlight the value of conditional deep generative models as core components of decision support systems for product line extension planning.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Expressive Temporal Specifications for Reward Monitoring</title>
<link>https://arxiv.org/abs/2511.12808</link>
<guid>https://arxiv.org/abs/2511.12808</guid>
<content:encoded><![CDATA[
arXiv:2511.12808v3 Announce Type: replace 
Abstract: Specifying informative and dense reward functions remains a pivotal challenge in Reinforcement Learning, as it directly affects the efficiency of agent training. In this work, we harness the expressive power of quantitative Linear Temporal Logic on finite traces (($\text{LTL}_f[\mathcal{F}]$)) to synthesize reward monitors that generate a dense stream of rewards for runtime-observable state trajectories. By providing nuanced feedback during training, these monitors guide agents toward optimal behaviour and help mitigate the well-known issue of sparse rewards under long-horizon decision making, which arises under the Boolean semantics dominating the current literature. Our framework is algorithm-agnostic and only relies on a state labelling function, and naturally accommodates specifying non-Markovian properties. Empirical results show that our quantitative monitors consistently subsume and, depending on the environment, outperform Boolean monitors in maximizing a quantitative measure of task completion and in reducing convergence time.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Nonparametric estimation of conditional probability distributions using a generative approach based on conditional push-forward neural networks</title>
<link>https://arxiv.org/abs/2511.14455</link>
<guid>https://arxiv.org/abs/2511.14455</guid>
<content:encoded><![CDATA[
arXiv:2511.14455v3 Announce Type: replace 
Abstract: We introduce conditional push-forward neural networks (CPFN), a generative framework for conditional distribution estimation. Instead of directly modeling the conditional density $f_{Y|X}$, CPFN learns a stochastic map $\varphi=\varphi(x,u)$ such that $\varphi(x,U)$ and $Y|X=x$ follow approximately the same law, with $U$ a suitable random vector of pre-defined latent variables. This enables efficient conditional sampling and straightforward estimation of conditional statistics through Monte Carlo methods. The model is trained via an objective function derived from a Kullback-Leibler formulation, without requiring invertibility or adversarial training. We establish a near-asymptotic consistency result and demonstrate experimentally that CPFN can achieve performance competitive with, or even superior to, state-of-the-art methods, including kernel estimators, tree-based algorithms, and popular deep learning techniques, all while remaining lightweight and easy to train.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GateRA: Token-Aware Modulation for Parameter-Efficient Fine-Tuning</title>
<link>https://arxiv.org/abs/2511.17582</link>
<guid>https://arxiv.org/abs/2511.17582</guid>
<content:encoded><![CDATA[
arXiv:2511.17582v3 Announce Type: replace 
Abstract: Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, DoRA, and HiRA, enable lightweight adaptation of large pre-trained models via low-rank updates. However, existing PEFT approaches apply static, input-agnostic updates to all tokens, disregarding the varying importance and difficulty of different inputs. This uniform treatment can lead to overfitting on trivial content or under-adaptation on more informative regions, especially in autoregressive settings with distinct prefill and decoding dynamics. In this paper, we propose GateRA, a unified framework that introduces token-aware modulation to dynamically adjust the strength of PEFT updates. By incorporating adaptive gating into standard PEFT branches, GateRA enables selective, token-level adaptation, preserving pre-trained knowledge for well-modeled inputs while focusing capacity on challenging cases. Empirical visualizations reveal phase-sensitive behaviors, where GateRA automatically suppresses updates for redundant prefill tokens while emphasizing adaptation during decoding. To promote confident and efficient modulation, we further introduce an entropy-based regularization that encourages near-binary gating decisions. This regularization prevents diffuse update patterns and leads to interpretable, sparse adaptation without hard thresholding. Finally, we present a theoretical analysis showing that GateRA induces a soft gradient-masking effect over the PEFT path, enabling continuous and differentiable control over adaptation. Experiments on multiple commonsense reasoning benchmarks demonstrate that GateRA consistently outperforms or matches prior PEFT methods.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HVAdam: A Full-Dimension Adaptive Optimizer</title>
<link>https://arxiv.org/abs/2511.20277</link>
<guid>https://arxiv.org/abs/2511.20277</guid>
<content:encoded><![CDATA[
arXiv:2511.20277v2 Announce Type: replace 
Abstract: Adaptive optimizers such as Adam have achieved great success in training large-scale models like large language models and diffusion models. However, they often generalize worse than non-adaptive methods, such as SGD on classical architectures like CNNs. We identify a key cause of this performance gap: adaptivity in pre-conditioners, which limits the optimizer's ability to adapt to diverse optimization landscapes. To address this, we propose Anon (Adaptivity Non-restricted Optimizer with Novel convergence technique), a novel optimizer with continuously tunable adaptivity
  , allowing it to interpolate between SGD-like and Adam-like behaviors and even extrapolate beyond both. To ensure convergence across the entire adaptivity spectrum, we introduce incremental delay update (IDU), a novel mechanism that is more flexible than AMSGrad's hard max-tracking strategy and enhances robustness to gradient noise. We theoretically establish convergence guarantees under both convex and non-convex settings. Empirically, Anon consistently outperforms state-of-the-art optimizers on representative image classification, diffusion, and language modeling tasks. These results demonstrate that adaptivity can serve as a valuable tunable design principle, and Anon provides the first unified and reliable framework capable of bridging the gap between classical and modern optimizers and surpassing their advantageous properties.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Superiority vs. Forensic Efficiency: A Comparative Analysis of Deep Learning and Psycholinguistics for Business Email Compromise Detection</title>
<link>https://arxiv.org/abs/2511.20944</link>
<guid>https://arxiv.org/abs/2511.20944</guid>
<content:encoded><![CDATA[
arXiv:2511.20944v3 Announce Type: replace 
Abstract: Business Email Compromise (BEC) is a sophisticated social engineering threat that manipulates organizational hierarchies, leading to significant financial damage. According to the 2024 FBI Internet Crime Report, BEC accounts for over $2.9 billion in annual losses, presenting a massive economic asymmetry: the financial cost of a False Negative (fraud loss) exceeds the operational cost of a False Positive (manual review) by a ratio of approximately 5,480:1. This paper contrasts two detection paradigms: a Forensic Psycholinguistic Stream (CatBoost), which analyzes linguistic cues like urgency and authority with high interpretability, and a Semantic Stream (DistilBERT), which utilizes deep learning for contextual understanding. We evaluated both streams on a hybrid dataset (N=7,990) containing human-legitimate and AI-synthesized adversarial fraud. Benchmarked on Tesla T4 infrastructure, DistilBERT achieved near-perfect detection on synthetic threats (AUC >0.99, F1 =0.998) with acceptable real-time latency (7.4 ms). CatBoost achieved competitive detection (AUC =0.991, F1 =0.949) at 8.4x lower latency (0.8 ms) with negligible resource consumption. We conclude that while DistilBERT offers maximum accuracy for GPU-equipped organizations, CatBoost provides a viable, cost-effective alternative for edge deployments. Both approaches demonstrate a theoretical ROI exceeding 99.9% when optimized via cost-sensitive learning.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-Overhead Introspection for Adaptive Test-Time Compute</title>
<link>https://arxiv.org/abs/2512.01457</link>
<guid>https://arxiv.org/abs/2512.01457</guid>
<content:encoded><![CDATA[
arXiv:2512.01457v3 Announce Type: replace 
Abstract: Large language models excel at reasoning but lack key aspects of introspection, including anticipating their own success and the computation required to achieve it. Humans use real-time introspection to decide how much effort to invest, when to make multiple attempts, when to stop, and when to signal success or failure. Without this, LLMs struggle to make intelligent meta-cognition decisions. Test-time scaling methods like Best-of-N drive up cost and latency by using a fixed budget of samples regardless of the marginal benefit of each one at any point in generation, and the absence of confidence signals can mislead people, prevent appropriate escalation to better tools, and undermine trustworthiness. Learned verifiers or reward models can provide confidence estimates, but do not enable adaptive inference and add substantial cost by requiring extra models or forward passes. We present ZIP-RC, an adaptive inference method that equips models with zero-overhead inference-time predictions of reward and cost. At every token, ZIP-RC reuses reserved or unused logits in the same forward pass as next-token prediction to output a joint distribution over final reward and remaining length -- no extra models, architecture change, or inference overhead. This full joint distribution is used to compute a sampling utility which is the linear combination of the expected maximum reward, total compute, and latency of set of samples if generated to completion. During inference, we maximize this utility with meta-actions that determine which prefix of tokens to continue or initiate sampling from. On mixed-difficulty mathematical benchmarks, ZIP-RC improves accuracy by up to 12% over majority voting at equal or lower average cost, and traces smooth Pareto frontiers between quality, compute, and latency. By providing real-time reward-cost introspection, ZIP-RC enables adaptive, efficient reasoning.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ASPEN: An Adaptive Spectral Physics-Enabled Network for Ginzburg-Landau Dynamics</title>
<link>https://arxiv.org/abs/2512.03290</link>
<guid>https://arxiv.org/abs/2512.03290</guid>
<content:encoded><![CDATA[
arXiv:2512.03290v3 Announce Type: replace 
Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful, mesh-free paradigm for solving partial differential equations (PDEs). However, they notoriously struggle with stiff, multi-scale, and nonlinear systems due to the inherent spectral bias of standard multilayer perceptron (MLP) architectures, which prevents them from adequately representing high-frequency components. In this work, we introduce the Adaptive Spectral Physics-Enabled Network (ASPEN), a novel architecture designed to overcome this critical limitation. ASPEN integrates an adaptive spectral layer with learnable Fourier features directly into the network's input stage. This mechanism allows the model to dynamically tune its own spectral basis during training, enabling it to efficiently learn and represent the precise frequency content required by the solution. We demonstrate the efficacy of ASPEN by applying it to the complex Ginzburg-Landau equation (CGLE), a canonical and challenging benchmark for nonlinear, stiff spatio-temporal dynamics. Our results show that a standard PINN architecture catastrophically fails on this problem, diverging into non-physical oscillations. In contrast, ASPEN successfully solves the CGLE with exceptional accuracy. The predicted solution is visually indistinguishable from the high-resolution ground truth, achieving a low median physics residual of 5.10 x 10^-3. Furthermore, we validate that ASPEN's solution is not only pointwise accurate but also physically consistent, correctly capturing emergent physical properties, including the rapid free energy relaxation and the long-term stability of the domain wall front. This work demonstrates that by incorporating an adaptive spectral basis, our framework provides a robust and physically-consistent solver for complex dynamical systems where standard PINNs fail, opening new options for machine learning in challenging physical domains.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When, How Long and How Much? Interpretable Neural Networks for Time Series Regression by Learning to Mask and Aggregate</title>
<link>https://arxiv.org/abs/2512.03578</link>
<guid>https://arxiv.org/abs/2512.03578</guid>
<content:encoded><![CDATA[
arXiv:2512.03578v2 Announce Type: replace 
Abstract: Time series extrinsic regression (TSER) refers to the task of predicting a continuous target variable from an input time series. It appears in many domains, including healthcare, finance, environmental monitoring, and engineering. In these settings, accurate predictions and trustworthy reasoning are both essential. Although state-of-the-art TSER models achieve strong predictive performance, they typically operate as black boxes, making it difficult to understand which temporal patterns drive their decisions. Post-hoc interpretability techniques, such as feature attribution, aim to to explain how the model arrives at its predictions, but often produce coarse, noisy, or unstable explanations. Recently, inherently interpretable approaches based on concepts, additive decompositions, or symbolic regression, have emerged as promising alternatives. However, these approaches remain limited: they require explicit supervision on the concepts themselves, often cannot capture interactions between time-series features, lack expressiveness for complex temporal patterns, and struggle to scale to high-dimensional multivariate data.
  To address these limitations, we propose MAGNETS (Mask-and-AGgregate NEtwork for Time Series), an inherently interpretable neural architecture for TSER. MAGNETS learns a compact set of human-understandable concepts without requiring any annotations. Each concept corresponds to a learned, mask-based aggregation over selected input features, explicitly revealing both which features drive predictions and when they matter in the sequence. Predictions are formed as combinations of these learned concepts through a transparent, additive structure, enabling clear insight into the model's decision process.
  The code implementation and datasets are publicly available at https://github.com/FlorentF9/MAGNETS.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Arc Gradient Descent: A Mathematically Derived Reformulation of Gradient Descent with Phase-Aware, User-Controlled Step Dynamics</title>
<link>https://arxiv.org/abs/2512.06737</link>
<guid>https://arxiv.org/abs/2512.06737</guid>
<content:encoded><![CDATA[
arXiv:2512.06737v2 Announce Type: replace 
Abstract: The paper presents the formulation, implementation, and evaluation of the ArcGD optimiser. The evaluation is conducted initially on a non-convex benchmark function and subsequently on a real-world ML dataset. The initial comparative study using the Adam optimiser is conducted on a stochastic variant of the highly non-convex and notoriously challenging Rosenbrock function, renowned for its narrow, curved valley, across dimensions ranging from 2D to 1000D and an extreme case of 50,000D. Two configurations were evaluated to eliminate learning-rate bias: (i) both using ArcGD's effective learning rate and (ii) both using Adam's default learning rate. ArcGD consistently outperformed Adam under the first setting and, although slower under the second, achieved superior final solutions in most cases. In the second evaluation, ArcGD is evaluated against state-of-the-art optimizers (Adam, AdamW, Lion, SGD) on the CIFAR-10 image classification dataset across 8 diverse MLP architectures ranging from 1 to 5 hidden layers. ArcGD achieved the highest average test accuracy (50.7%) at 20,000 iterations, outperforming AdamW (46.6%), Adam (46.8%), SGD (49.6%), and Lion (43.4%), winning or tying on 6 of 8 architectures. Notably, while Adam and AdamW showed strong early convergence at 5,000 iterations, but regressed with extended training, whereas ArcGD continued improving, demonstrating generalization and resistance to overfitting without requiring early stopping tuning. Strong performance on geometric stress tests and standard deep-learning benchmarks indicates broad applicability, highlighting the need for further exploration. Moreover, it is also shown that a limiting variant of ArcGD can be interpreted as a sign-based momentum-like update, highlighting conceptual connections between the inherent mechanisms of ArcGD and the Lion optimiser.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Always Keep Your Promises: DynamicLRP, A Model-Agnostic Solution To Layer-Wise Relevance Propagation</title>
<link>https://arxiv.org/abs/2512.07010</link>
<guid>https://arxiv.org/abs/2512.07010</guid>
<content:encoded><![CDATA[
arXiv:2512.07010v2 Announce Type: replace 
Abstract: Layer-wise Relevance Propagation (LRP) provides principled attribution for neural networks through conservation properties and foundations in Deep Taylor Decomposition. However, existing implementations operate at the module level, requiring architecture-specific propagation rules and model modifications. These limit the generality of target model and sustainability of implementations as architectures evolve. We introduce DynamicLRP, a model-agnostic LRP framework operating at the tensor operation level. By decomposing attribution to individual operations within computation graphs and introducing a novel mechanism for deferred activation resolution, named the Promise System, our approach achieves true architecture agnosticity while maintaining LRP's theoretical guarantees. This design operates independently of backpropagation machinery, requiring no model modification, enabling side-by-side execution with gradient backpropagation. Being based on computation graphs, this method is theoretically extensible to other deep learning libraries that support auto-differentiation. We demonstrate faithfulness matching or exceeding specialized implementations (1.77 vs 1.69 ABPC on VGG, equivalent performance on ViT, 93.70% and 95.06% top-1 attribution accuracy for explaining RoBERTa-large and Flan-T5-large answers on SQuADv2, respectively) while maintaining practical efficiency on models with 100M-1B parameters. We achieved 99.92% node coverage across 31,465 computation graph nodes from 15 diverse architectures, including state-space models (Mamba), audio transformers (Whisper), and multimodal systems (DePlot) without any model-specific code with rules for 47 fundamental operations implemented. Our operation-level decomposition and Promise System establish a sustainable, extensible foundation for LRP across evolving architectures. All code is available at https://github.com/keeinlev/dynamicLRP .
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Renormalizable Spectral-Shell Dynamics as the Origin of Neural Scaling Laws</title>
<link>https://arxiv.org/abs/2512.10427</link>
<guid>https://arxiv.org/abs/2512.10427</guid>
<content:encoded><![CDATA[
arXiv:2512.10427v3 Announce Type: replace 
Abstract: Neural scaling laws and double-descent phenomena suggest that deep-network training obeys a simple macroscopic structure despite highly nonlinear optimization dynamics. We derive such structure directly from gradient descent in function space. For mean-squared error loss, the training error evolves as $\dot e_t=-M(t)e_t$ with $M(t)=J_{\theta(t)}J_{\theta(t)}^{\!*}$, a time-dependent self-adjoint operator induced by the network Jacobian. Using Kato perturbation theory, we obtain an exact system of coupled modewise ODEs in the instantaneous eigenbasis of $M(t)$.
  To extract macroscopic behavior, we introduce a logarithmic spectral-shell coarse-graining and track quadratic error energy across shells. Microscopic interactions within each shell cancel identically at the energy level, so shell energies evolve only through dissipation and external inter-shell interactions. We formalize this via a \emph{renormalizable shell-dynamics} assumption, under which cumulative microscopic effects reduce to a controlled net flux across shell boundaries.
  Assuming an effective power-law spectral transport in a relevant resolution range, the shell dynamics admits a self-similar solution with a moving resolution frontier and explicit scaling exponents. This framework explains neural scaling laws and double descent, and unifies lazy (NTK-like) training and feature learning as two limits of the same spectral-shell dynamics.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SSAS: Cross-subject EEG-based Emotion Recognition through Source Selection with Adversarial Strategy</title>
<link>https://arxiv.org/abs/2512.13458</link>
<guid>https://arxiv.org/abs/2512.13458</guid>
<content:encoded><![CDATA[
arXiv:2512.13458v2 Announce Type: replace 
Abstract: Electroencephalographic (EEG) signals have long been applied in the field of affective brain-computer interfaces (aBCIs). Cross-subject EEG-based emotion recognition has demonstrated significant potential in practical applications due to its suitability across diverse people. However, most studies on cross-subject EEG-based emotion recognition neglect the presence of inter-individual variability and negative transfer phenomena during model training. To address this issue, a cross-subject EEG-based emotion recognition through source selection with adversarial strategy is introduced in this paper. The proposed method comprises two modules: the source selection network (SS) and the adversarial strategies network (AS). The SS uses domain labels to reverse-engineer the training process of domain adaptation. Its key idea is to disrupt class separability and magnify inter-domain differences, thereby raising the classification difficulty and forcing the model to learn domain-invariant yet emotion-relevant representations. The AS gets the source domain selection results and the pretrained domain discriminators from SS. The pretrained domain discriminators compute a novel loss aimed at enhancing the performance of domain classification during adversarial training, ensuring the balance of adversarial strategies. This paper provides theoretical insights into the proposed method and achieves outstanding performance on two EEG-based emotion datasets, SEED and SEED-IV. The code can be found at https://github.com/liuyici/SSAS.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structure of Classifier Boundaries: Case Study for a Naive Bayes Classifier</title>
<link>https://arxiv.org/abs/2212.04382</link>
<guid>https://arxiv.org/abs/2212.04382</guid>
<content:encoded><![CDATA[
arXiv:2212.04382v3 Announce Type: replace-cross 
Abstract: Classifiers assign complex input data points to one of a small number of output categories. For a Bayes classifier whose input space is a graph, we study the structure of the \emph{boundary}, which comprises those points for which at least one neighbor is classified differently. The scientific setting is assignment of DNA reads produced by \NGSs\ to candidate source genomes. The boundary is both large and complicated in structure. We introduce a new measure of uncertainty, Neighbor Similarity, that compares the result for an input point to the distribution of results for its neighbors. This measure not only tracks two inherent uncertainty measures for the Bayes classifier, but also can be implemented for classifiers without inherent measures of uncertainty.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multiperiodic Processes: Ergodic Sources with a Sublinear Entropy</title>
<link>https://arxiv.org/abs/2302.09049</link>
<guid>https://arxiv.org/abs/2302.09049</guid>
<content:encoded><![CDATA[
arXiv:2302.09049v3 Announce Type: replace-cross 
Abstract: We construct multiperiodic processes -- a simple example of stationary ergodic (but not mixing) processes over natural numbers that enjoy the vanishing entropy rate under a mild condition. Multiperiodic processes are supported on randomly shifted deterministic sequences called multiperiodic sequences, which can be efficiently generated using an algorithm called the Infinite Clock. Under a suitable parameterization, multiperiodic sequences exhibit relative frequencies of particular numbers given by Zipf's law. Exactly in the same setting, the respective multiperiodic processes satisfy an asymptotic power-law growth of block entropy, called Hilberg's law. Hilberg's law is deemed to hold for statistical language models, in particular.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training robust and generalizable quantum models</title>
<link>https://arxiv.org/abs/2311.11871</link>
<guid>https://arxiv.org/abs/2311.11871</guid>
<content:encoded><![CDATA[
arXiv:2311.11871v4 Announce Type: replace-cross 
Abstract: Adversarial robustness and generalization are both crucial properties of reliable machine learning models. In this paper, we study these properties in the context of quantum machine learning based on Lipschitz bounds. We derive parameter-dependent Lipschitz bounds for quantum models with trainable encoding, showing that the norm of the data encoding has a crucial impact on the robustness against data perturbations. Further, we derive a bound on the generalization error which explicitly involves the parameters of the data encoding. Our theoretical findings give rise to a practical strategy for training robust and generalizable quantum models by regularizing the Lipschitz bound in the cost. Further, we show that, for fixed and non-trainable encodings, as those frequently employed in quantum machine learning, the Lipschitz bound cannot be influenced by tuning the parameters. Thus, trainable encodings are crucial for systematically adapting robustness and generalization during training. The practical implications of our theoretical findings are illustrated with numerical results.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Variance Reduction and Low Sample Complexity in Stochastic Optimization via Proximal Point Method</title>
<link>https://arxiv.org/abs/2402.08992</link>
<guid>https://arxiv.org/abs/2402.08992</guid>
<content:encoded><![CDATA[
arXiv:2402.08992v3 Announce Type: replace-cross 
Abstract: High-probability guarantees in stochastic optimization are often obtained only under strong noise assumptions such as sub-Gaussian tails. We show that such guarantees can also be achieved under the weaker assumption of bounded variance by developing a stochastic proximal point method. This method combines a proximal subproblem solver, which inherently reduces variance, with a probability booster that amplifies per-iteration reliability into high-confidence results. The analysis demonstrates convergence with low sample complexity, without restrictive noise assumptions or reliance on mini-batching.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Any-Time Regret-Guaranteed Algorithm for Control of Linear Quadratic Systems</title>
<link>https://arxiv.org/abs/2406.07746</link>
<guid>https://arxiv.org/abs/2406.07746</guid>
<content:encoded><![CDATA[
arXiv:2406.07746v2 Announce Type: replace-cross 
Abstract: We propose a computationally efficient algorithm that achieves anytime regret of order $\mathcal{O}(\sqrt{t})$, with explicit dependence on the system dimensions and on the solution of the Discrete Algebraic Riccati Equation (DARE). Our approach uses an appropriately tuned regularization and a sufficiently accurate initial estimate to construct confidence ellipsoids for control design. A carefully designed input-perturbation mechanism is incorporated to ensure anytime performance. We develop two variants of the algorithm. The first enforces strong sequential stability, requiring each policy to be stabilizing and successive policies to remain close. This sequential condition helps prevent state explosion at policy update times; however, it results in a suboptimal regret scaling with respect to the DARE solution. Motivated by this limitation, we introduce a second class of algorithms that removes this requirement and instead requires only that each generated policy be stabilizing. Closed-loop stability is then preserved through a dwell-time inspired policy-update rule. This class of algorithms also addresses key shortcomings of most existing approaches which lack explicit high-probability bounds on the state trajectory expressed in system-theoretic terms. Our analysis shows that partially relaxing the sequential-stability requirement yields optimal regret. Finally, our method eliminates the need for any \emph{a priori} bound on the norm of the DARE solution, an assumption required by all existing computationally efficient OFU based algorithms.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Retrieval with Few-shot Indexing</title>
<link>https://arxiv.org/abs/2408.02152</link>
<guid>https://arxiv.org/abs/2408.02152</guid>
<content:encoded><![CDATA[
arXiv:2408.02152v2 Announce Type: replace-cross 
Abstract: Existing generative retrieval (GR) methods rely on training-based indexing, which fine-tunes a model to memorise associations between queries and the document identifiers (docids) of relevant documents. Training-based indexing suffers from high training costs, under-utilisation of pre-trained knowledge in large language models (LLMs), and limited adaptability to dynamic document corpora. To address the issues, we propose a few-shot indexing-based GR framework (Few-Shot GR). It has a few-shot indexing process without any training, where we prompt an LLM to generate docids for all documents in a corpus, ultimately creating a docid bank for the entire corpus. During retrieval, we feed a query to the same LLM and constrain it to generate a docid within the docid bank created during indexing, and then map the generated docid back to its corresponding document. Moreover, we devise few-shot indexing with one-to-many mapping to further enhance Few-Shot GR. Experiments show that Few-Shot GR achieves superior performance to state-of-the-art GR methods requiring heavy training.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Networked Communication for Mean-Field Games with Function Approximation and Empirical Mean-Field Estimation</title>
<link>https://arxiv.org/abs/2408.11607</link>
<guid>https://arxiv.org/abs/2408.11607</guid>
<content:encoded><![CDATA[
arXiv:2408.11607v3 Announce Type: replace-cross 
Abstract: Recent algorithms allow decentralised agents, possibly connected via a communication network, to learn equilibria in mean-field games from a non-episodic run of the empirical system. However, these algorithms are for tabular settings: this computationally limits the size of agents' observation space, meaning the algorithms cannot handle anything but small state spaces, nor generalise beyond policies depending only on the agent's local state to so-called 'population-dependent' policies. We address this limitation by introducing function approximation to the existing setting, drawing on the Munchausen Online Mirror Descent method that has previously been employed only in finite-horizon, episodic, centralised settings. While this permits us to include the mean field in the observation for players' policies, it is unrealistic to assume decentralised agents have access to this global information: we therefore also provide new algorithms allowing agents to locally estimate the global empirical distribution, and to improve this estimate via inter-agent communication. We prove theoretically that exchanging policy information helps networked agents outperform both independent and even centralised agents in function-approximation settings. Our experiments demonstrate this happening empirically, and show that the communication network allows decentralised agents to estimate the mean field for population-dependent policies.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatic Detection of LLM-Generated Code: A Comparative Case Study of Contemporary Models Across Function and Class Granularities</title>
<link>https://arxiv.org/abs/2409.01382</link>
<guid>https://arxiv.org/abs/2409.01382</guid>
<content:encoded><![CDATA[
arXiv:2409.01382v2 Announce Type: replace-cross 
Abstract: The adoption of Large Language Models (LLMs) for code generation risks incorporating vulnerable code into software systems. Existing detectors face two critical limitations: a lack of systematic cross-model validation and opaque "black box" operation. We address this through a comparative study of code generated by four distinct LLMs: GPT-3.5, Claude 3 Haiku, Claude Haiku 4.5, and GPT-OSS.
  Analyzing 14,485 Python functions and 11,913 classes from the CodeSearchNet dataset, we generated corresponding code with all four LLMs. Using interpretable software metrics, we trained CatBoost classifiers for each configuration. Our analysis reveals that granularity effects dominate model differences by a factor of 8.6, with negligible feature overlap, indicating that function-level and class-level detection rely on fundamentally disjoint structural signatures.
  We discover critical granularity-dependent inversions: while modern models (Claude, GPT-OSS) are more detectable at the class level, GPT-3.5 is an anomaly that uniquely excels at the function level. SHAP analysis identifies the Comment-to-Code Ratio as the sole universal discriminator. However, its predictive magnitude varies drastically across models, explaining why detectors trained on specific LLMs fail to generalize.
  Our findings demonstrate that GPT-3.5's exceptional detectability (AUC-ROC 0.96) is unrepresentative of contemporary models (AUC-ROC approximately between 0.68 and 0.80). Robust detection requires moving beyond single-model studies to account for substantial diversity in structural fingerprints across architectures and granularities.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GUIDEd Agents: Enhancing Navigation Policies through Task-Specific Uncertainty Abstraction in Localization-Limited Environments</title>
<link>https://arxiv.org/abs/2410.15178</link>
<guid>https://arxiv.org/abs/2410.15178</guid>
<content:encoded><![CDATA[
arXiv:2410.15178v4 Announce Type: replace-cross 
Abstract: Autonomous vehicles performing navigation tasks in complex environments face significant challenges due to uncertainty in state estimation. In many scenarios, such as stealth operations or resource-constrained settings, accessing high-precision localization comes at a significant cost, forcing robots to rely primarily on less precise state estimates. Our key observation is that different tasks require varying levels of precision in different regions: a robot navigating a crowded space might need precise localization near obstacles but can operate effectively with less precision elsewhere. In this paper, we present a planning method for integrating task-specific uncertainty requirements directly into navigation policies. We introduce Task-Specific Uncertainty Maps (TSUMs), which abstract the acceptable levels of state estimation uncertainty across different regions. TSUMs align task requirements and environmental features using a shared representation space, generated via a domain-adapted encoder. Using TSUMs, we propose Generalized Uncertainty Integration for Decision-Making and Execution (GUIDE), a policy conditioning framework that incorporates these uncertainty requirements into robot decision-making. We find that TSUMs provide an effective way to abstract task-specific uncertainty requirements, and conditioning policies on TSUMs enables the robot to reason about the context-dependent value of certainty and adapt its behavior accordingly. We show how integrating GUIDE into reinforcement learning frameworks allows the agent to learn navigation policies that effectively balance task completion and uncertainty management without explicit reward engineering. We evaluate GUIDE on various real-world robotic navigation tasks and find that it demonstrates significant improvement in task completion rates compared to baseline methods that do not explicitly consider task-specific uncertainty.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Theoretical Convergence Guarantees for Variational Autoencoders</title>
<link>https://arxiv.org/abs/2410.16750</link>
<guid>https://arxiv.org/abs/2410.16750</guid>
<content:encoded><![CDATA[
arXiv:2410.16750v3 Announce Type: replace-cross 
Abstract: Variational Autoencoders (VAE) are popular generative models used to sample from complex data distributions. Despite their empirical success in various machine learning tasks, significant gaps remain in understanding their theoretical properties, particularly regarding convergence guarantees. This paper aims to bridge that gap by providing non-asymptotic convergence guarantees for VAE trained using both Stochastic Gradient Descent and Adam algorithms. We derive a convergence rate of $\mathcal{O}(\log n / \sqrt{n})$, where $n$ is the number of iterations of the optimization algorithm, with explicit dependencies on the batch size, the number of variational samples, and other key hyperparameters. Our theoretical analysis applies to both Linear VAE and Deep Gaussian VAE, as well as several VAE variants, including $\beta$-VAE and IWAE. Additionally, we empirically illustrate the impact of hyperparameters on convergence, offering new insights into the theoretical understanding of VAE training.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AIDOVECL: AI-generated Dataset of Outpainted Vehicles for Eye-level Classification and Localization</title>
<link>https://arxiv.org/abs/2410.24116</link>
<guid>https://arxiv.org/abs/2410.24116</guid>
<content:encoded><![CDATA[
arXiv:2410.24116v2 Announce Type: replace-cross 
Abstract: Image labeling is a critical bottleneck in the development of computer vision technologies, often constraining the potential of machine learning models due to the time-intensive nature of manual annotations. This work introduces a novel approach that leverages outpainting to mitigate the problem of annotated data scarcity by generating artificial contexts and annotations, significantly reducing manual labeling efforts. We apply this technique to a particularly acute challenge in autonomous driving, urban planning, and environmental monitoring: the lack of diverse, eye-level vehicle images in desired classes. Our dataset comprises AI-generated vehicle images obtained by detecting and cropping vehicles from manually selected seed images, which are then outpainted onto larger canvases to simulate varied real-world conditions. The outpainted images include detailed annotations, providing high-quality ground truth data. Advanced outpainting techniques and image quality assessments ensure visual fidelity and contextual relevance. Ablation results show that incorporating AIDOVECL improves overall detection performance by up to 10%, and delivers gains of up to 40% in settings with greater diversity of context, object scale, and placement, with underrepresented classes achieving up to 50% higher true positives. AIDOVECL enhances vehicle detection by augmenting real training data and supporting evaluation across diverse scenarios. By demonstrating outpainting as an automatic annotation paradigm, it offers a practical and versatile solution for building fine-grained datasets with reduced labeling effort across multiple machine learning domains. The code and links to datasets used in this study are available for further research and replication at https://github.com/amir-kazemi/aidovecl .
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HEIGHT: Heterogeneous Interaction Graph Transformer for Robot Navigation in Crowded and Constrained Environments</title>
<link>https://arxiv.org/abs/2411.12150</link>
<guid>https://arxiv.org/abs/2411.12150</guid>
<content:encoded><![CDATA[
arXiv:2411.12150v3 Announce Type: replace-cross 
Abstract: We study the problem of robot navigation in dense and interactive crowds with static constraints such as corridors and furniture. Previous methods fail to consider all types of spatial and temporal interactions among agents and obstacles, leading to unsafe and inefficient robot paths. In this article, we leverage a graph-based representation of crowded and constrained scenarios and propose a structured framework to learn robot navigation policies with deep reinforcement learning. We first split the representations of different inputs and propose a heterogeneous spatio-temporal graph to model distinct interactions among humans, robots, and obstacles. Based on the heterogeneous spatio-temporal graph, we propose HEIGHT, a novel navigation policy network architecture with different components to capture heterogeneous interactions through space and time. HEIGHT utilizes attention mechanisms to prioritize important interactions and a recurrent network to track changes in the dynamic scene over time, encouraging the robot to avoid collisions adaptively. Through extensive simulation and real-world experiments, we demonstrate that HEIGHT outperforms state-of-the-art baselines in terms of success, navigation time, and generalization to domain shifts in challenging navigation scenarios. More information is available at https://sites.google.com/view/crowdnav-height/home.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Riemannian Optimization Perspective of the Gauss-Newton Method for Feedforward Neural Networks</title>
<link>https://arxiv.org/abs/2412.14031</link>
<guid>https://arxiv.org/abs/2412.14031</guid>
<content:encoded><![CDATA[
arXiv:2412.14031v5 Announce Type: replace-cross 
Abstract: In this work, we establish non-asymptotic convergence bounds for the Gauss-Newton method in training neural networks with smooth activations. In the underparameterized regime, the Gauss-Newton gradient flow in parameter space induces a Riemannian gradient flow on a low-dimensional embedded submanifold of the function space. Using tools from Riemannian optimization, we establish geodesic Polyak-Lojasiewicz and Lipschitz-smoothness conditions for the loss under appropriately chosen output scaling, yielding geometric convergence to the optimal in-class predictor at an explicit rate independent of the conditioning of the Gram matrix. In the overparameterized regime, we propose adaptive, curvature-aware regularization schedules that ensure fast geometric convergence to a global optimum at a rate independent of the minimum eigenvalue of the neural tangent kernel and, locally, of the modulus of strong convexity of the loss. These results demonstrate that Gauss-Newton achieves accelerated convergence rates in settings where first-order methods exhibit slow convergence due to ill-conditioned kernel matrices and loss landscapes.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerated Preference Elicitation with LLM-Based Proxies</title>
<link>https://arxiv.org/abs/2501.14625</link>
<guid>https://arxiv.org/abs/2501.14625</guid>
<content:encoded><![CDATA[
arXiv:2501.14625v2 Announce Type: replace-cross 
Abstract: Bidders in combinatorial auctions face significant challenges when describing their preferences to an auctioneer. Classical work on preference elicitation focuses on query-based techniques inspired from proper learning--often via proxies that interface between bidders and an auction mechanism--to incrementally learn bidder preferences as needed to compute efficient allocations. Although such elicitation mechanisms enjoy theoretical query efficiency, the amount of communication required may still be too cognitively taxing in practice.
  We propose a family of efficient LLM-based proxy designs for eliciting preferences from bidders using natural language. Our proposed mechanism combines LLM pipelines and DNF-proper-learning techniques to quickly approximate preferences when communication is limited. To validate our approach, we create a testing sandbox for elicitation mechanisms that communicate in natural language. In our experiments, our most promising LLM proxy design reaches approximately efficient outcomes with five times fewer queries than classical proper learning based elicitation mechanisms.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SimSort: A Data-Driven Framework for Spike Sorting by Large-Scale Electrophysiology Simulation</title>
<link>https://arxiv.org/abs/2502.03198</link>
<guid>https://arxiv.org/abs/2502.03198</guid>
<content:encoded><![CDATA[
arXiv:2502.03198v3 Announce Type: replace-cross 
Abstract: Spike sorting is an essential process in neural recording, which identifies and separates electrical signals from individual neurons recorded by electrodes in the brain, enabling researchers to study how specific neurons communicate and process information. Although there exist a number of spike sorting methods which have contributed to significant neuroscientific breakthroughs, many are heuristically designed, making it challenging to verify their correctness due to the difficulty of obtaining ground truth labels from real-world neural recordings. In this work, we explore a data-driven, deep learning-based approach. We begin by creating a large-scale dataset through electrophysiology simulations using biologically realistic computational models. We then present SimSort, a pretraining framework for spike sorting. Trained solely on simulated data, SimSort demonstrates zero-shot generalizability to real-world spike sorting tasks, yielding consistent improvements over existing methods across multiple benchmarks. These results highlight the potential of simulation-driven pretraining to enhance the robustness and scalability of spike sorting in experimental neuroscience.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Keep It Light! Simplifying Image Clustering Via Text-Free Adapters</title>
<link>https://arxiv.org/abs/2502.04226</link>
<guid>https://arxiv.org/abs/2502.04226</guid>
<content:encoded><![CDATA[
arXiv:2502.04226v2 Announce Type: replace-cross 
Abstract: In the era of pre-trained models, effective classification can often be achieved using simple linear probing or lightweight readout layers. In contrast, many competitive clustering pipelines have a multi-modal design, leveraging large language models (LLMs) or other text encoders, and text-image pairs, which are often unavailable in real-world downstream applications. Additionally, such frameworks are generally complicated to train and require substantial computational resources, making widespread adoption challenging. In this work, we show that in deep clustering, competitive performance with more complex state-of-the-art methods can be achieved using a text-free and highly simplified training pipeline. In particular, our approach, Simple Clustering via Pre-trained models (SCP), trains only a small cluster head while leveraging pre-trained vision model feature representations and positive data pairs. Experiments on benchmark datasets, including CIFAR-10, CIFAR-20, CIFAR-100, STL-10, ImageNet-10, and ImageNet-Dogs, demonstrate that SCP achieves highly competitive performance. Furthermore, we provide a theoretical result explaining why, at least under ideal conditions, additional text-based embeddings may not be necessary to achieve strong clustering performance in vision.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GRADEO: Towards Human-Like Evaluation for Text-to-Video Generation via Multi-Step Reasoning</title>
<link>https://arxiv.org/abs/2503.02341</link>
<guid>https://arxiv.org/abs/2503.02341</guid>
<content:encoded><![CDATA[
arXiv:2503.02341v2 Announce Type: replace-cross 
Abstract: Recent great advances in video generation models have demonstrated their potential to produce high-quality videos, bringing challenges to effective evaluation. Unlike human evaluation, existing automated evaluation metrics lack highlevel semantic understanding and reasoning capabilities for video, thus making them infeasible and unexplainable. To fill this gap, we curate GRADEO-Instruct, a multi-dimensional T2V evaluation instruction tuning dataset, including 3.3k videos from over 10 existing video generation models and multi-step reasoning assessments converted by 16k human annotations. We then introduce GRADEO, one of the first specifically designed video evaluation models, which grades AI-generated videos for explainable scores and assessments through multi-step reasoning. Experiments show that our method aligns better with human evaluations than existing methods. Furthermore, our benchmarking reveals that current video generation models struggle to produce content that aligns with human reasoning and complex real-world scenarios.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Density estimation via mixture discrepancy and moments</title>
<link>https://arxiv.org/abs/2504.01570</link>
<guid>https://arxiv.org/abs/2504.01570</guid>
<content:encoded><![CDATA[
arXiv:2504.01570v2 Announce Type: replace-cross 
Abstract: With the aim of generalizing histogram statistics to higher dimensional cases, density estimation via discrepancy based sequential partition (DSP) has been proposed to learn an adaptive piecewise constant approximation defined on a binary sequential partition of the underlying domain, where the star discrepancy is adopted to measure the uniformity of particle distribution. However, the calculation of the star discrepancy is NP-hard and it does not satisfy the reflection invariance and rotation invariance either. To this end, we use the mixture discrepancy and the comparison of moments as a replacement of the star discrepancy, leading to the density estimation via mixture discrepancy based sequential partition (DSP-mix) and density estimation via moment-based sequential partition (MSP), respectively. Both DSP-mix and MSP are computationally tractable and exhibit the reflection and rotation invariance. Numerical experiments in reconstructing Beta mixtures, Gaussian mixtures and heavy-tailed Cauchy mixtures up to 30 dimension are conducted, demonstrating that MSP can maintain the same accuracy compared with DSP, while gaining an increase in speed by a factor of two to twenty for large sample size, and DSP-mix can achieve satisfactory accuracy and boost the efficiency in low-dimensional tests ($d \le 6$), but might lose accuracy in high-dimensional problems due to a reduction in partition level.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stochastic Optimization with Optimal Importance Sampling</title>
<link>https://arxiv.org/abs/2504.03560</link>
<guid>https://arxiv.org/abs/2504.03560</guid>
<content:encoded><![CDATA[
arXiv:2504.03560v2 Announce Type: replace-cross 
Abstract: Importance Sampling (IS) is a widely used variance reduction technique for enhancing the efficiency of Monte Carlo methods, particularly in rare-event simulation and related applications. Despite its effectiveness, the performance of IS is highly sensitive to the choice of the proposal distribution and often requires stochastic calibration. While the design and analysis of IS have been extensively studied in estimation settings, applying IS within stochastic optimization introduces a fundamental challenge: the decision variable and the importance sampling distribution are mutually dependent, creating a circular optimization structure. This interdependence complicates both convergence analysis and variance control. We consider convex stochastic optimization problems with linear constraints and propose a single-loop stochastic approximation algorithm, based on a joint variant of Nesterov's dual averaging, that jointly updates the decision variable and the importance sampling distribution, without time-scale separation or nested optimization. The method is globally convergent and achieves minimal asymptotic variance among stochastic gradient schemes, matching the performance of an oracle sampler adapted to the optimal solution.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JITServe: SLO-aware LLM Serving with Imprecise Request Information</title>
<link>https://arxiv.org/abs/2504.20068</link>
<guid>https://arxiv.org/abs/2504.20068</guid>
<content:encoded><![CDATA[
arXiv:2504.20068v3 Announce Type: replace-cross 
Abstract: The integration of Large Language Models (LLMs) into applications ranging from interactive chatbots to multi-agent systems has introduced a wide spectrum of service-level objectives (SLOs) for responsiveness. These include latency-sensitive requests emphasizing per-token latency in streaming chat, deadline-sensitive requests requiring rapid full responses to trigger external tools, and compound requests with evolving dependencies across multiple LLM calls. Despite-or perhaps, because of-this workload diversity and unpredictable request information (e.g., response lengths and dependencies), existing request schedulers have focused on aggregate performance, unable to ensure application-level SLO needs.
  This paper presents JITServe, the first SLO-aware LLM serving system designed to maximize service goodput (e.g., the number of tokens meeting request SLOs) across diverse workloads. JITServe novelly schedules requests using imprecise request information and gradually relaxes this conservatism by refining request information estimates as generation progresses. It applies a grouped margin goodput maximization algorithm to allocate just enough serving bandwidth to satisfy each request's SLO just-in-time (JIT), maximizing residual capacity for others, while deciding the composition of requests in a batch to maximize efficiency and goodput with provable guarantees. Our evaluation across diverse realistic workloads, including chat, deep research, and agentic pipelines, shows that JITServe improves service goodput by 1.4x-6.3x, alternatively achieving 28.5%-83.2% resource savings, compared to state-of-the-art designs.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differentiable Nonlinear Model Predictive Control</title>
<link>https://arxiv.org/abs/2505.01353</link>
<guid>https://arxiv.org/abs/2505.01353</guid>
<content:encoded><![CDATA[
arXiv:2505.01353v2 Announce Type: replace-cross 
Abstract: The efficient computation of parametric solution sensitivities is a key challenge in the integration of learning-enhanced methods with nonlinear model predictive control (MPC), as their availability is crucial for many learning algorithms. This paper discusses the computation of solution sensitivities of general nonlinear programs (NLPs) using the implicit function theorem (IFT) and smoothed optimality conditions treated in interior-point methods (IPM). We detail sensitivity computation within a sequential quadratic programming (SQP) method which employs an IPM for the quadratic subproblems. Previous works presented in the machine learning community are limited to convex or unconstrained formulations, or lack an implementation for efficient sensitivity evaluation. The publication is accompanied by an efficient open-source implementation within the acados framework, providing both forward and adjoint sensitivities for general optimal control problems, achieving speedups exceeding 3x over the state-of-the-art solvers mpc.pytorch and cvxpygen.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reproducing and Improving CheXNet: Deep Learning for Chest X-ray Disease Classification</title>
<link>https://arxiv.org/abs/2505.06646</link>
<guid>https://arxiv.org/abs/2505.06646</guid>
<content:encoded><![CDATA[
arXiv:2505.06646v2 Announce Type: replace-cross 
Abstract: Deep learning for radiologic image analysis is a rapidly growing field in biomedical research and is likely to become a standard practice in modern medicine. On the publicly available NIH ChestX-ray14 dataset, containing X-ray images that are classified by the presence or absence of 14 different diseases, we reproduced an algorithm known as CheXNet, as well as explored other algorithms that outperform CheXNet's baseline metrics. Model performance was primarily evaluated using the F1 score and AUC-ROC, both of which are critical metrics for imbalanced, multi-label classification tasks in medical imaging. The best model achieved an average AUC-ROC score of 0.85 and an average F1 score of 0.39 across all 14 disease classifications present in the dataset.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Personalized and Resilient Distributed Learning Through Opinion Dynamics</title>
<link>https://arxiv.org/abs/2505.14081</link>
<guid>https://arxiv.org/abs/2505.14081</guid>
<content:encoded><![CDATA[
arXiv:2505.14081v2 Announce Type: replace-cross 
Abstract: In this paper, we address two practical challenges of distributed learning in multi-agent network systems, namely personalization and resilience. Personalization is the need of heterogeneous agents to learn local models tailored to their own data and tasks, while still generalizing well; on the other hand, the learning process must be resilient to cyberattacks or anomalous training data to avoid disruption. Motivated by a conceptual affinity between these two requirements, we devise a distributed learning algorithm that combines distributed gradient descent and the Friedkin-Johnsen model of opinion dynamics to fulfill both of them. We quantify its convergence speed and the neighborhood that contains the final learned models, which can be easily controlled by tuning the algorithm parameters to enforce a more personalized/resilient behavior. We numerically showcase the effectiveness of our algorithm on synthetic and real-world distributed learning tasks, where it achieves high global accuracy both for personalized models and with malicious agents compared to standard strategies.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reliable Decision Support with LLMs: A Framework for Evaluating Consistency in Binary Text Classification Applications</title>
<link>https://arxiv.org/abs/2505.14918</link>
<guid>https://arxiv.org/abs/2505.14918</guid>
<content:encoded><![CDATA[
arXiv:2505.14918v2 Announce Type: replace-cross 
Abstract: This study introduces a framework for evaluating consistency in large language model (LLM) binary text classification, addressing the lack of established reliability assessment methods. Adapting psychometric principles, we determine sample size requirements, develop metrics for invalid responses, and evaluate intra- and inter-rater reliability. Our case study examines financial news sentiment classification across 14 LLMs (including claude-3-7-sonnet, gpt-4o, deepseek-r1, gemma3, llama3.2, phi4, and command-r-plus), with five replicates per model on 1,350 articles. Models demonstrated high intra-rater consistency, achieving perfect agreement on 90-98% of examples, with minimal differences between expensive and economical models from the same families. When validated against StockNewsAPI labels, models achieved strong performance (accuracy 0.76-0.88), with smaller models like gemma3:1B, llama3.2:3B, and claude-3-5-haiku outperforming larger counterparts. All models performed at chance when predicting actual market movements, indicating task constraints rather than model limitations. Our framework provides systematic guidance for LLM selection, sample size planning, and reliability assessment, enabling organizations to optimize resources for classification tasks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attractor learning for spatiotemporally chaotic dynamical systems using echo state networks with transfer learning</title>
<link>https://arxiv.org/abs/2505.24099</link>
<guid>https://arxiv.org/abs/2505.24099</guid>
<content:encoded><![CDATA[
arXiv:2505.24099v2 Announce Type: replace-cross 
Abstract: In this paper, we explore the predictive capabilities of echo state networks (ESNs) for the generalized Kuramoto-Sivashinsky (gKS) equation, an archetypal nonlinear PDE that exhibits spatiotemporal chaos. Our research focuses on predicting changes in long-term statistical patterns of the gKS model that result from varying the dispersion relation or the length of the spatial domain. We use transfer learning to adapt ESNs to different parameter settings and successfully capture changes in the underlying chaotic attractor. Previous work has shown that transfer learning can be used effectively with ESNs for single-orbit prediction. The novelty of our paper lies in our use of this pairing to predict the long-term statistical properties of spatiotemporally chaotic PDEs. We also show that transfer learning nontrivially improves the length of time that predictions of individual gKS trajectories remain accurate.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Condition Number as a Scale-Invariant Proxy for Information Encoding in Neural Units</title>
<link>https://arxiv.org/abs/2506.16289</link>
<guid>https://arxiv.org/abs/2506.16289</guid>
<content:encoded><![CDATA[
arXiv:2506.16289v2 Announce Type: replace-cross 
Abstract: This paper explores the relationship between the condition number of a neural network's weight tensor and the extent of information encoded by the associated processing unit, viewed through the lens of information theory. It argues that a high condition number, though not sufficient for effective knowledge encoding, may indicate that the unit has learned to selectively amplify and compress information. This intuition is formalized for linear units with Gaussian inputs, linking the condition number and the transformation's log-volume scaling factor to the characteristics of the output entropy and the geometric properties of the learned transformation. The analysis demonstrates that for a fixed weight norm, a concentrated distribution of singular values (high condition number) corresponds to reduced overall information transfer, indicating a specialized and efficient encoding strategy. Furthermore, the linear stage entropy bound provides an upper limit on post-activation information for contractive, element-wise nonlinearities, supporting the condition number as a scale-invariant proxy for encoding capacity in practical neural networks. An empirical case study applies these principles to guide selective fine-tuning of Large Language Models for both a new task and a new input modality. The experiments show that the proposed method, named KappaTune, effectively mitigates catastrophic forgetting. Unlike many existing catastrophic forgetting mitigation methods that rely on access to pre-training statistics, which are often unavailable, this selective fine-tuning approach offers a way to bypass this common requirement.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Libra: Unleashing GPU Heterogeneity for High-Performance Sparse Matrix Multiplication</title>
<link>https://arxiv.org/abs/2506.22714</link>
<guid>https://arxiv.org/abs/2506.22714</guid>
<content:encoded><![CDATA[
arXiv:2506.22714v2 Announce Type: replace-cross 
Abstract: Sparse matrix multiplication operators (i.e., SpMM and SDDMM) are widely used in deep learning and scientific computing. Modern accelerators are commonly equipped with Tensor Core Units (TCUs) and CUDA cores to accelerate sparse operators. The former excels at structured matrix computations, whereas the latter offers greater programming flexibility. However, how to combine these two resources to maximize sparse-operator performance remains unclear. In this work, we first identify the source of performance gains in hybrid computation and systematically analyze their complementary strengths. Motivated by this, we propose Libra, a holistic framework that efficiently leverages heterogeneous computing resources to accelerate both SpMM and SDDMM operators. Specifically, Libra introduces a 2D-aware (locality and utilization) workload distribution method to precisely identify the optimal task mapping, simultaneously leveraging the data reuse capabilities of TCUs and the flexibility of CUDA cores to minimize computational redundancy. Libra further incorporates hybrid load balancing, occupancy-aware task scheduling, and efficient kernel implementations to maximize execution efficiency. Extensive experiments on H100 and RTX 4090 GPUs demonstrate that Libra surpasses all the 12 up-to-date baselines significantly, e.g., on average 1.77x speedup over FlashSparse, 1.73x over RoDe, and 2.9x over DGL for end-to-end GNN applications. Libra opens up a new perspective for sparse operator acceleration by fully unleashing the power of heterogeneous GPU resources.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>cVLA: Towards Efficient Camera-Space VLAs</title>
<link>https://arxiv.org/abs/2507.02190</link>
<guid>https://arxiv.org/abs/2507.02190</guid>
<content:encoded><![CDATA[
arXiv:2507.02190v2 Announce Type: replace-cross 
Abstract: Vision-Language-Action (VLA) models offer a compelling framework for tackling complex robotic manipulation tasks, but they are often expensive to train. In this paper, we propose a novel VLA approach that leverages the competitive performance of Vision Language Models (VLMs) on 2D images to directly infer robot end-effector poses in image frame coordinates. Unlike prior VLA models that output low-level controls, our model predicts trajectory waypoints, making it both more efficient to train and robot embodiment agnostic. Despite its lightweight design, our next-token prediction architecture effectively learns meaningful and executable robot trajectories. We further explore the underutilized potential of incorporating depth images, inference-time techniques such as decoding strategies, and demonstration-conditioned action generation. Our model is trained on a simulated dataset and exhibits strong sim-to-real transfer capabilities. We evaluate our approach using a combination of simulated and real data, demonstrating its effectiveness on a real robotic system.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Practical Benchmarking of Data Cleaning Techniques: On Generating Authentic Errors via Large Language Models</title>
<link>https://arxiv.org/abs/2507.10934</link>
<guid>https://arxiv.org/abs/2507.10934</guid>
<content:encoded><![CDATA[
arXiv:2507.10934v2 Announce Type: replace-cross 
Abstract: Data quality remains an important challenge in data-driven systems, as errors in tabular data can severely compromise downstream analytics and machine learning performance. Although numerous error detection algorithms have been proposed, the lack of diverse, real-world error datasets limits comprehensive evaluation. Manual error annotation is both time-consuming and inconsistent, motivating the exploration of synthetic error generation as an alternative. In this work, we introduce TableEG, a framework that leverages large language models (LLMs) to generate authentic errors. By employing a table fine-tuning strategy and a triplet representation $(I, T, O)$ to model error generation, detection, and correction tasks, TableEG captures the complex dependencies inherent in two-dimensional tables. Trained on 12 real-world datasets spanning 10 diverse domains, TableEG ensures that the synthesized errors faithfully reflect authentic error distributions. Experimental results indicate that errors generated by TableEG exhibit superior pattern and distribution similarity compared to both rule-based methods and LLM-generated errors without fine-tuning. Furthermore, performance metrics on TableEG-generated errors closely align with those on real-world errors across nearly all datasets and detection algorithms, particularly for machine learning based detection techniques. Overall, TableEG not only bridges the gap between synthetic and real-world errors but also establishes a robust benchmark for subsequent error detection and correction tasks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Confidence Calibration in Vision-Language-Action Models</title>
<link>https://arxiv.org/abs/2507.17383</link>
<guid>https://arxiv.org/abs/2507.17383</guid>
<content:encoded><![CDATA[
arXiv:2507.17383v2 Announce Type: replace-cross 
Abstract: Trustworthy robot behavior requires not only high levels of task success but also that the robot can reliably quantify how likely it is to succeed. To this end, we present a first-of-its-kind study of confidence calibration in vision-language-action (VLA) foundation models, which map visual observations and natural language instructions to low-level robot motor commands. We establish a confidence baseline for VLAs, examine how task success relates to calibration error and how calibration evolves over time, and introduce two lightweight techniques to remedy the miscalibration we observe: prompt ensembles and action-wise Platt scaling. Our aim in this study is to begin to develop the tools and conceptual understanding necessary to render VLAs both highly performant and highly trustworthy via reliable uncertainty quantification.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ASR-Synchronized Speaker-Role Diarization</title>
<link>https://arxiv.org/abs/2507.17765</link>
<guid>https://arxiv.org/abs/2507.17765</guid>
<content:encoded><![CDATA[
arXiv:2507.17765v3 Announce Type: replace-cross 
Abstract: Speaker-role diarization (RD), such as doctor vs. patient or lawyer vs. client, is practically often more useful than conventional speaker diarization (SD), which assigns only generic labels (speaker-1, speaker-2). The state-of-the-art end-to-end ASR+RD approach uses a single transducer that serializes word and role predictions (role at the end of a speaker's turn), but at the cost of degraded ASR performance. To address this, we adapt a recent joint ASR+SD framework to ASR+RD by freezing the ASR transducer and training an auxiliary RD transducer in parallel to assign a role to each ASR-predicted word. For this, we first show that SD and RD are fundamentally different tasks, exhibiting different dependencies on acoustic and linguistic information. Motivated by this, we propose (1) task-specific predictor networks and (2) using higher-layer ASR encoder features as input to the RD encoder. Additionally, we replace the blank-shared RNNT loss by cross-entropy loss along the 1-best forced-alignment path to further improve performance while reducing computational and memory requirements during RD training. Experiments on a public and a private dataset of doctor-patient conversations demonstrate that our method outperforms the best baseline with relative reductions of 6.2% and 4.5% in role-based word diarization error rate (R-WDER), respectively
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Variational Free Energy Calculation of Hydrogen Hugoniot</title>
<link>https://arxiv.org/abs/2507.18540</link>
<guid>https://arxiv.org/abs/2507.18540</guid>
<content:encoded><![CDATA[
arXiv:2507.18540v2 Announce Type: replace-cross 
Abstract: We develop a deep variational free energy framework to compute the equation of state of hydrogen in the warm dense matter region. This method parameterizes the variational density matrix of hydrogen nuclei and electrons at finite temperature using three deep generative models: a normalizing flow model for the Boltzmann distribution of the classical nuclei, an autoregressive transformer for the distribution of electrons in excited states, and a permutational equivariant flow model for the unitary backflow transformation of electron coordinates in Hartree-Fock states. By jointly optimizing the three neural networks to minimize the variational free energy, we obtain the equation of state and related thermodynamic properties of dense hydrogen for the temperature range where electrons occupy excited states. We compare our results with other theoretical and experimental results on the deuterium Hugoniot curve, aiming to resolve existing discrepancies. Our results bridge the gap between the results obtained by path-integral Monte Carlo calculations at high temperature and ground-state electronic methods at low temperature, thus providing a valuable benchmark for hydrogen in the warm dense matter region.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Multi-Agent Collaboration with Attention-Based Actor-Critic Policies</title>
<link>https://arxiv.org/abs/2507.22782</link>
<guid>https://arxiv.org/abs/2507.22782</guid>
<content:encoded><![CDATA[
arXiv:2507.22782v3 Announce Type: replace-cross 
Abstract: This paper introduces Team-Attention-Actor-Critic (TAAC), a reinforcement learning algorithm designed to enhance multi-agent collaboration in cooperative environments. TAAC employs a Centralized Training/Centralized Execution scheme incorporating multi-headed attention mechanisms in both the actor and critic. This design facilitates dynamic, inter-agent communication, allowing agents to explicitly query teammates, thereby efficiently managing the exponential growth of joint-action spaces while ensuring a high degree of collaboration. We further introduce a penalized loss function which promotes diverse yet complementary roles among agents. We evaluate TAAC in a simulated soccer environment against benchmark algorithms representing other multi-agent paradigms, including Proximal Policy Optimization and Multi-Agent Actor-Attention-Critic. We find that TAAC exhibits superior performance and enhanced collaborative behaviors across a variety of metrics (win rates, goal differentials, Elo ratings, inter-agent connectivity, balanced spatial distributions, and frequent tactical interactions such as ball possession swaps).
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies</title>
<link>https://arxiv.org/abs/2508.20072</link>
<guid>https://arxiv.org/abs/2508.20072</guid>
<content:encoded><![CDATA[
arXiv:2508.20072v3 Announce Type: replace-cross 
Abstract: Vision-Language-Action (VLA) models adapt large vision-language backbones to map images and instructions into robot actions. However, prevailing VLAs either generate actions auto-regressively in a fixed left-to-right order or attach separate MLP or diffusion heads outside the backbone, leading to fragmented information pathways and specialized training requirements that hinder a unified, scalable architecture. We present Discrete Diffusion VLA, a unified-transformer policy that models discretized action chunks with discrete diffusion. The design retains diffusion's progressive refinement paradigm while remaining natively compatible with the discrete token interface of VLMs. Our method achieves an adaptive decoding order that resolves easy action elements before harder ones and uses secondary re-masking to revisit uncertain predictions across refinement rounds, which improves consistency and enables robust error correction. This unified decoder preserves pre-trained vision-language priors, supports parallel decoding, breaks the autoregressive bottleneck, and reduces the number of function evaluations. Discrete Diffusion VLA achieves 96.3% avg. success rates on LIBERO, 71.2% visual matching on SimplerEnv-Fractal and 54.2% overall on SimplerEnv-Bridge. We also provide ablation study on vision-language ability retention on LIBERO-OOD (Out-of-Distribution) benchmark, with our method improving over autoregressive, MLP decoder and continuous diffusion baselines. These findings indicate that discrete-diffusion VLA supports precise action modeling and consistent training, laying groundwork for scaling VLA to larger models and datasets. Our code is available at https://github.com/Liang-ZX/DiscreteDiffusionVLA/tree/libero.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCA-LLM: Spectral-Attentive LLM-Based Wireless World Modeling for Agentic Communications</title>
<link>https://arxiv.org/abs/2509.08139</link>
<guid>https://arxiv.org/abs/2509.08139</guid>
<content:encoded><![CDATA[
arXiv:2509.08139v2 Announce Type: replace-cross 
Abstract: Future AI-native wireless networks are moving from reactive optimization to agentic decision-making that can sense, predict, and plan under fast-varying channels. This calls for wireless world models that can predict and roll out channel dynamics, for which multi-step channel state information (CSI) prediction offers a practical short-horizon look-ahead. Recent advances in foundation sequence models further motivate large language models (LLMs) as general-purpose dynamics learners when suitably adapted to non-text time-series signals. However, bridging CSI to LLMs is non-trivial because an effective adapter must expose informative spectral and temporal evolution patterns, while prior designs provide limited inductive bias to capture such channel structures. To this end, we propose SCA-LLM, a spectral-attentive LLM-based wireless world modeling framework that bridges CSI to LLMs via a spectral-channel attention (SCA) adapter. Specifically, the SCA adapter performs multi-spectral representation learning to extract informative channel features and align CSI with the LLM's sequence modeling capability, enabling parameter-efficient adaptation while keeping the LLM backbone largely frozen. Extensive simulations show that SCA-LLM achieves state-of-the-art prediction performance and strong zero-shot generalization, yielding up to -2.4 dB normalized mean squared error (NMSE) advantage over the previous LLM based method. Our ablation studies further confirm the effectiveness of the proposed SCA adapter in mitigating domain mismatch.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for Multistep Reasoning</title>
<link>https://arxiv.org/abs/2509.09284</link>
<guid>https://arxiv.org/abs/2509.09284</guid>
<content:encoded><![CDATA[
arXiv:2509.09284v3 Announce Type: replace-cross 
Abstract: Recent advances in reasoning with large language models (LLMs) have shown the effectiveness of Monte Carlo Tree Search (MCTS) for generating high quality intermediate trajectories, particularly in math and symbolic domains. Inspired by this, we explore how MCTS derived trajectories, traditionally used for training value or reward models, can be repurposed to improve policy optimization in verifier guided reinforcement learning (RL). Specifically, we focus on Group Relative Policy Optimization (GRPO), a recent algorithm that enables consistent policy learning from group relative judgments. We reframe GRPO into a staged training paradigm, leveraging a teacher's MCTS rollouts to construct a tree structured curriculum of prefixes. This introduces the novel challenge of computing advantages for training samples that originate from different prefixes, each with a distinct expected return. To address this, we propose Staged Advantage Estimation (SAE), a framework for computing low variance, prefix aware advantages by projecting rewards onto a constraint set that respects the tree's hierarchy. Our empirical results on mathematical reasoning tasks show that SAE improves final accuracy over standard GRPO. This outcome is grounded in our theoretical analysis, which confirms that SAE reduces gradient variance, a principled path to improved sample efficiency. We demonstrate this through practical SAE implementations, comparing efficient heuristics against a formal quadratic program.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LiveOIBench: Can Large Language Models Outperform Human Contestants in Informatics Olympiads?</title>
<link>https://arxiv.org/abs/2510.09595</link>
<guid>https://arxiv.org/abs/2510.09595</guid>
<content:encoded><![CDATA[
arXiv:2510.09595v2 Announce Type: replace-cross 
Abstract: Competitive programming problems increasingly serve as valuable benchmarks to evaluate the coding capabilities of large language models (LLMs) due to their complexity and ease of verification. Yet, current coding benchmarks face limitations such as lack of exceptionally challenging problems, insufficient test case coverage, reliance on online platform APIs that limit accessibility. To address these issues, we introduce LiveOIBench, a comprehensive benchmark featuring 403 expert-curated Olympiad-level competitive programming problems, each with an average of 60 expert-designed test cases. The problems are sourced directly from 72 official contests of 14 Informatics Olympiads in different regions conducted between 2023 and 2025. LiveOIBench distinguishes itself through four key features: (1) meticulously curated high-quality tasks with detailed subtask rubrics and extensive private test cases; (2) direct integration of elite contestant performance data to enable informative comparison against top-performing humans; (3) planned continuous, contamination-free updates from newly released Olympiad problems; and (4) a self-contained evaluation system facilitating offline and easy-to-reproduce assessments. Benchmarking 34 popular general-purpose and reasoning LLMs, we find that GPT-5 achieves a notable 81.76th percentile, a strong result that nonetheless falls short of top human contestants, who usually place above 90th. In contrast, among open-weight reasoning models, GPT-OSS-120B achieves only a 60th percentile, underscoring significant capability disparities from frontier closed models. Detailed analyses indicate that robust reasoning models prioritize precise problem analysis over excessive exploration, suggesting future models should emphasize structured analysis and minimize unnecessary exploration. All data, code, and leaderboard results are publicly available on our website.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Network-Optimised Spiking Neural Network (NOS) Scheduling for 6G O-RAN: Spectral Margin and Delay-Tail Control</title>
<link>https://arxiv.org/abs/2510.11291</link>
<guid>https://arxiv.org/abs/2510.11291</guid>
<content:encoded><![CDATA[
arXiv:2510.11291v2 Announce Type: replace-cross 
Abstract: This work presents a Network-Optimised Spiking (NOS) delay-aware scheduler for 6G radio access. The scheme couples a bounded two-state kernel to a clique-feasible proportional-fair (PF) grant head: the excitability state acts as a finite-buffer proxy, the recovery state suppresses repeated grants, and neighbour pressure is injected along the interference graph via delayed spikes. A small-signal analysis yields a delay-dependent threshold $k_\star(\Delta)$ and a spectral margin $\delta = k_\star(\Delta) - gH\rho(W)$ that compress topology, controller gain, and delay into a single design parameter. Under light assumptions on arrivals, we prove geometric ergodicity for $\delta>0$ and derive sub-Gaussian backlog and delay tail bounds with exponents proportional to $\delta$. A numerical study, aligned with the analysis and a DU compute budget, compares NOS with PF and delayed backpressure (BP) across interference topologies over a $5$--$20$\,ms delay sweep. With a single gain fixed at the worst spectral radius, NOS sustains higher utilisation and a smaller 99.9th-percentile delay while remaining clique-feasible on integer PRBs.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Polarization based direction of arrival estimation using a radio interferometric array</title>
<link>https://arxiv.org/abs/2510.15116</link>
<guid>https://arxiv.org/abs/2510.15116</guid>
<content:encoded><![CDATA[
arXiv:2510.15116v2 Announce Type: replace-cross 
Abstract: Direction of arrival (DOA) estimation is mostly performed using specialized arrays that have carefully designed receiver spacing and layouts to match the operating frequency range. In contrast, radio interferometric arrays are designed to optimally sample the Fourier space data for making high quality images of the sky.
  Therefore, using existing radio interferometric arrays (with arbitrary geometry and wide frequency variation) for DOA estimation is practically infeasible except by using images made by such interferometers. In this paper, we focus on low cost DOA estimation without imaging, using a subset of a radio interferometric array, using a fraction of the data collected by the full array, and, enabling early determination of DOAs. The proposed method is suitable for transient and low duty cycle source detection. Moreover, the proposed method is an ideal follow-up step to online radio frequency interference (RFI) mitigation, enabling the early estimation of the DOA of the detected RFI.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-as-a-Prophet: Understanding Predictive Intelligence with Prophet Arena</title>
<link>https://arxiv.org/abs/2510.17638</link>
<guid>https://arxiv.org/abs/2510.17638</guid>
<content:encoded><![CDATA[
arXiv:2510.17638v2 Announce Type: replace-cross 
Abstract: Forecasting is not only a fundamental intellectual pursuit but also is of significant importance to societal systems such as finance and economics. With the rapid advances of large language models (LLMs) trained on Internet-scale data, it raises the promise of employing LLMs to forecast real-world future events, an emerging paradigm we call "LLM-as-a-Prophet". This paper systematically investigates such predictive intelligence of LLMs. To this end, we build Prophet Arena, a general evaluation benchmark that continuously collects live forecasting tasks and decomposes each task into distinct pipeline stages, in order to support our controlled and large-scale experimentation. Our comprehensive evaluation reveals that many LLMs already exhibit impressive forecasting capabilities, reflected in, e.g., their small calibration errors, consistent prediction confidence and promising market returns. However, we also uncover key bottlenecks towards achieving superior predictive intelligence via LLM-as-a-Prophet, such as LLMs' inaccurate event recalls, misunderstanding of data sources and slower information aggregation compared to markets when resolution nears.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Single-Loop First-Order Algorithm for Linearly Constrained Bilevel Optimization</title>
<link>https://arxiv.org/abs/2510.24710</link>
<guid>https://arxiv.org/abs/2510.24710</guid>
<content:encoded><![CDATA[
arXiv:2510.24710v2 Announce Type: replace-cross 
Abstract: We study bilevel optimization problems where the lower-level problems are strongly convex and have coupled linear constraints. To overcome the potential non-smoothness of the hyper-objective and the computational challenges associated with the Hessian matrix, we utilize penalty and augmented Lagrangian methods to reformulate the original problem as a single-level one. Especially, we establish a strong theoretical connection between the reformulated function and the original hyper-objective by characterizing the closeness of their values and derivatives. Based on this reformulation, we propose a single-loop, first-order algorithm for linearly constrained bilevel optimization (SFLCB). We provide rigorous analyses of its non-asymptotic convergence rates, showing an improvement over prior double-loop algorithms -- form $O(\epsilon^{-3}\log(\epsilon^{-1}))$ to $O(\epsilon^{-3})$. The experiments corroborate our theoretical findings and demonstrate the practical efficiency of the proposed SFLCB algorithm. Simulation code is provided at https://github.com/ShenGroup/SFLCB.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Configuration-First Framework for Reproducible, Low-Code Localization</title>
<link>https://arxiv.org/abs/2510.25692</link>
<guid>https://arxiv.org/abs/2510.25692</guid>
<content:encoded><![CDATA[
arXiv:2510.25692v2 Announce Type: replace-cross 
Abstract: Machine learning is increasingly permeating radio-based localization services. To keep results credible and comparable, everyday workflows should make rigorous experiment specification and exact repeatability the default, without blocking advanced experimentation. However, in practice, researchers face a three-way gap that could be filled by a framework that offers (i) low coding effort for end-to-end studies, (ii) reproducibility by default, including versioned code, data, and configurations, controlled randomness, isolated runs, and recorded artifacts, and (iii) built-in extensibility so new models, metrics, and stages can be added with minimal integration effort. Existing tools rarely deliver all three for machine learning in general and localization workflows in particular. In this paper, we introduce LOCALIZE, a low-code, configuration-first framework for radio localization in which experiments are declared in human-readable configuration files, a workflow orchestrator executes standardized pipelines from data preparation to reporting, and all artifacts, such as datasets, models, metrics, and reports, are versioned. Preconfigured, versioned datasets reduce initial setup effort and boilerplate, thereby accelerating model development and evaluation. The design, with explicit extension points, allows experts to add components without reworking the underlying infrastructure. Through a qualitative comparison and a head-to-head study against a plain Jupyter notebook baseline, we show that the framework reduces authoring effort while maintaining comparable runtime and memory behavior. Furthermore, using a Bluetooth Low Energy dataset, we demonstrate that scaling the training data from 1x to 10x keeps orchestration overheads bounded as data grows. Overall, the framework makes reproducible machine-learning-based localization experimentation practical, accessible, and extensible.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learned Static Function Data Structures</title>
<link>https://arxiv.org/abs/2510.27588</link>
<guid>https://arxiv.org/abs/2510.27588</guid>
<content:encoded><![CDATA[
arXiv:2510.27588v2 Announce Type: replace-cross 
Abstract: We consider the task of constructing a data structure for associating a static set of keys with values, while allowing arbitrary output values for queries involving keys outside the set. Compared to hash tables, these so-called static function data structures do not need to store the key set and thus use significantly less memory. Several techniques are known, with compressed static functions approaching the zero-order empirical entropy of the value sequence. In this paper, we introduce learned static functions, which use machine learning to capture correlations between keys and values. For each key, a model predicts a probability distribution over the values, from which we derive a key-specific prefix code to compactly encode the true value. The resulting codeword is stored in a classic static function data structure. This design allows learned static functions to break the zero-order entropy barrier while still supporting point queries. Our experiments show substantial space savings: up to one order of magnitude on real data, and up to three orders of magnitude on synthetic data.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models</title>
<link>https://arxiv.org/abs/2511.02376</link>
<guid>https://arxiv.org/abs/2511.02376</guid>
<content:encoded><![CDATA[
arXiv:2511.02376v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) remain vulnerable to jailbreaking attacks where adversarial prompts elicit harmful outputs. Yet most evaluations focus on single-turn interactions while real-world attacks unfold through adaptive multi-turn conversations. We present AutoAdv, a training-free framework for automated multi-turn jailbreaking that achieves an attack success rate of up to 95% on Llama-3.1-8B within six turns, a 24% improvement over single-turn baselines. AutoAdv uniquely combines three adaptive mechanisms: a pattern manager that learns from successful attacks to enhance future prompts, a temperature manager that dynamically adjusts sampling parameters based on failure modes, and a two-phase rewriting strategy that disguises harmful requests and then iteratively refines them. Extensive evaluation across commercial and open-source models (Llama-3.1-8B, GPT-4o mini, Qwen3-235B, Mistral-7B) reveals persistent vulnerabilities in current safety mechanisms, with multi-turn attacks consistently outperforming single-turn approaches. These findings demonstrate that alignment strategies optimized for single-turn interactions fail to maintain robustness across extended conversations, highlighting an urgent need for multi-turn-aware defenses.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Diffusion Model Guidance through Calibration and Regularization</title>
<link>https://arxiv.org/abs/2511.05844</link>
<guid>https://arxiv.org/abs/2511.05844</guid>
<content:encoded><![CDATA[
arXiv:2511.05844v3 Announce Type: replace-cross 
Abstract: Classifier-guided diffusion models have emerged as a powerful approach for conditional image generation, but they suffer from overconfident predictions during early denoising steps, causing the guidance gradient to vanish. This paper introduces two complementary contributions to address this issue. First, we propose a differentiable calibration objective based on the Smooth Expected Calibration Error (Smooth ECE), which improves classifier calibration with minimal fine-tuning and yields measurable improvements in Frechet Inception Distance (FID). Second, we develop enhanced sampling guidance methods that operate on off-the-shelf classifiers without requiring retraining. These include tilted sampling with batch-level reweighting, adaptive entropy-regularized sampling to preserve diversity, and a novel f-divergence-based sampling strategy that strengthens class-consistent guidance while maintaining mode coverage. Experiments on ImageNet 128x128 demonstrate that our divergence-regularized guidance achieves an FID of 2.13 using a ResNet-101 classifier, improving upon existing classifier-guided diffusion methods while requiring no diffusion model retraining. The results show that principled calibration and divergence-aware sampling provide practical and effective improvements for classifier-guided diffusion.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Source-Optimal Training is Transfer-Suboptimal</title>
<link>https://arxiv.org/abs/2511.08401</link>
<guid>https://arxiv.org/abs/2511.08401</guid>
<content:encoded><![CDATA[
arXiv:2511.08401v3 Announce Type: replace-cross 
Abstract: We prove that training a source model optimally for its own task is generically suboptimal when the objective is downstream transfer. We study the source-side optimization problem in L2-SP ridge regression and show a fundamental mismatch between the source-optimal and transfer-optimal source regularization: outside of a measure-zero set, $\tau_0^* \neq \tau_S^*$. We characterize the transfer-optimal source penalty $\tau_0^*$ as a function of task alignment and identify an alignment-dependent reversal: with imperfect alignment ($0<\rho<1$), transfer benefits from stronger source regularization, while in super-aligned regimes ($\rho>1$), transfer benefits from weaker regularization. In isotropic settings, the decision of whether transfer helps is independent of the target sample size and noise, depending only on task alignment and source characteristics. We verify the linear predictions in a synthetic ridge regression experiment, and we present CIFAR-10 experiments as evidence that the source-optimal versus transfer-optimal mismatch can persist in nonlinear networks.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ensuring Calibration Robustness in Split Conformal Prediction Under Adversarial Attacks</title>
<link>https://arxiv.org/abs/2511.18562</link>
<guid>https://arxiv.org/abs/2511.18562</guid>
<content:encoded><![CDATA[
arXiv:2511.18562v2 Announce Type: replace-cross 
Abstract: Conformal prediction (CP) provides distribution-free, finite-sample coverage guarantees but critically relies on exchangeability, a condition often violated under distribution shift. We study the robustness of split conformal prediction under adversarial perturbations at test time, focusing on both coverage validity and the resulting prediction set size. Our theoretical analysis characterizes how the strength of adversarial perturbations during calibration affects coverage guarantees under adversarial test conditions. We further examine the impact of adversarial training at the model-training stage. Extensive experiments support our theory: (i) Prediction coverage varies monotonically with the calibration-time attack strength, enabling the use of nonzero calibration-time attack to predictably control coverage under adversarial tests; (ii) target coverage can hold over a range of test-time attacks: with a suitable calibration attack, coverage stays within any chosen tolerance band across a contiguous set of perturbation levels; and (iii) adversarial training at the training stage produces tighter prediction sets that retain high informativeness.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Affective Multimodal Agents with Proactive Knowledge Grounding for Emotionally Aligned Marketing Dialogue</title>
<link>https://arxiv.org/abs/2511.21728</link>
<guid>https://arxiv.org/abs/2511.21728</guid>
<content:encoded><![CDATA[
arXiv:2511.21728v2 Announce Type: replace-cross 
Abstract: Recent advances in large language models (LLMs) have enabled fluent dialogue systems, but most remain reactive and struggle in emotionally rich, goal-oriented settings such as marketing conversations. To address this limitation, we propose AffectMind, a multimodal affective dialogue agent that performs proactive reasoning and dynamic knowledge grounding to sustain emotionally aligned and persuasive interactions. AffectMind combines three components: a Proactive Knowledge Grounding Network (PKGN) that continuously updates factual and affective context from text, vision, and prosody; an Emotion--Intent Alignment Model (EIAM) that jointly models user emotion and purchase intent to adapt persuasion strategies; and a Reinforced Discourse Loop (RDL) that optimizes emotional coherence and engagement via reinforcement signals from user responses. Experiments on two newly curated marketing dialogue datasets, MM-ConvMarket and AffectPromo, show that AffectMind outperforms strong LLM-based baselines in emotional consistency (+26\%), persuasive success rate (+19\%), and long-term user engagement (+23\%), highlighting emotion-grounded proactivity as a key capability for commercial multimodal agents.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continuous-time reinforcement learning for optimal switching over multiple regimes</title>
<link>https://arxiv.org/abs/2512.04697</link>
<guid>https://arxiv.org/abs/2512.04697</guid>
<content:encoded><![CDATA[
arXiv:2512.04697v2 Announce Type: replace-cross 
Abstract: This paper studies the continuous-time reinforcement learning (RL) for optimal switching problems across multiple regimes. We consider a type of exploratory formulation under entropy regularization where the agent randomizes both the timing of switches and the selection of regimes through the generator matrix of an associated continuous-time finite-state Markov chain. We establish the well-posedness of the associated system of Hamilton-Jacobi-Bellman (HJB) equations and provide a characterization of the optimal policy. The policy improvement and the convergence of the policy iterations are rigorously established by analyzing the system of equations. We also show the convergence of the value function in the exploratory formulation towards the value function in the classical formulation as the temperature parameter vanishes. Finally, a reinforcement learning algorithm is devised and implemented by invoking the policy evaluation based on the martingale characterization. Our numerical examples with the aid of neural networks illustrate the effectiveness of the proposed RL algorithm.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge</title>
<link>https://arxiv.org/abs/2512.06951</link>
<guid>https://arxiv.org/abs/2512.06951</guid>
<content:encoded><![CDATA[
arXiv:2512.06951v2 Announce Type: replace-cross 
Abstract: We present a vision-action policy that won 1st place in the 2025 BEHAVIOR Challenge - a large-scale benchmark featuring 50 diverse long-horizon household tasks in photo-realistic simulation, requiring bimanual manipulation, navigation, and context-aware decision making. Building on the Pi0.5 architecture, we introduce several innovations. Our primary contribution is correlated noise for flow matching, which improves training efficiency and enables correlation-aware inpainting for smooth action sequences. We also apply learnable mixed-layer attention and System 2 stage tracking for ambiguity resolution. Training employs multi-sample flow matching to reduce variance, while inference uses action compression and challenge-specific correction rules. Our approach achieves 26% q-score across all 50 tasks on both public and private leaderboards.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Independent Density Estimation</title>
<link>https://arxiv.org/abs/2512.10067</link>
<guid>https://arxiv.org/abs/2512.10067</guid>
<content:encoded><![CDATA[
arXiv:2512.10067v2 Announce Type: replace-cross 
Abstract: Large-scale Vision-Language models have achieved remarkable results in various domains, such as image captioning and conditioned image generation. Nevertheless, these models still encounter difficulties in achieving human-like compositional generalization. In this study, we propose a new method called Independent Density Estimation (IDE) to tackle this challenge. IDE aims to learn the connection between individual words in a sentence and the corresponding features in an image, enabling compositional generalization. We build two models based on the philosophy of IDE. The first one utilizes fully disentangled visual representations as input, and the second leverages a Variational Auto-Encoder to obtain partially disentangled features from raw images. Additionally, we propose an entropy-based compositional inference method to combine predictions of each word in the sentence. Our models exhibit superior generalization to unseen compositions compared to current models when evaluated on various datasets.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Iterative Compositional Data Generation for Robot Control</title>
<link>https://arxiv.org/abs/2512.10891</link>
<guid>https://arxiv.org/abs/2512.10891</guid>
<content:encoded><![CDATA[
arXiv:2512.10891v3 Announce Type: replace-cross 
Abstract: Collecting robotic manipulation data is expensive, making it impractical to acquire demonstrations for the combinatorially large space of tasks that arise in multi-object, multi-robot, and multi-environment settings. While recent generative models can synthesize useful data for individual tasks, they do not exploit the compositional structure of robotic domains and struggle to generalize to unseen task combinations. We propose a semantic compositional diffusion transformer that factorizes transitions into robot-, object-, obstacle-, and objective-specific components and learns their interactions through attention. Once trained on a limited subset of tasks, we show that our model can zero-shot generate high-quality transitions from which we can learn control policies for unseen task combinations. Then, we introduce an iterative self-improvement procedure in which synthetic data is validated via offline reinforcement learning and incorporated into subsequent training rounds. Our approach substantially improves zero-shot performance over monolithic and hard-coded compositional baselines, ultimately solving nearly all held-out tasks and demonstrating the emergence of meaningful compositional structure in the learned representations.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stopping Rules for Stochastic Gradient Descent via Anytime-Valid Confidence Sequences</title>
<link>https://arxiv.org/abs/2512.13123</link>
<guid>https://arxiv.org/abs/2512.13123</guid>
<content:encoded><![CDATA[
arXiv:2512.13123v3 Announce Type: replace-cross 
Abstract: We study stopping rules for stochastic gradient descent (SGD) for convex optimization from the perspective of anytime-valid confidence sequences. Classical analyses of SGD provide convergence guarantees in expectation or at a fixed horizon, but offer no statistically valid way to assess, at an arbitrary time, how close the current iterate is to the optimum. We develop an anytime-valid, data-dependent upper confidence sequence for the weighted average suboptimality of projected SGD, constructed via nonnegative supermartingales and requiring no smoothness or strong convexity. This confidence sequence yields a simple stopping rule that is provably $\varepsilon$-optimal with probability at least $1-\alpha$, with explicit bounds on the stopping time under standard stochastic approximation stepsizes. To the best of our knowledge, these are the first rigorous, time-uniform performance guarantees and finite-time $\varepsilon$-optimality certificates for projected SGD with general convex objectives, based solely on observable trajectory quantities.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Zipf's Law to Neural Scaling through Heaps' Law and Hilberg's Hypothesis</title>
<link>https://arxiv.org/abs/2512.13491</link>
<guid>https://arxiv.org/abs/2512.13491</guid>
<content:encoded><![CDATA[
arXiv:2512.13491v2 Announce Type: replace-cross 
Abstract: We inspect the deductive connection between the neural scaling law and Zipf's law -- two statements discussed in machine learning and quantitative linguistics. The neural scaling law describes how the cross entropy rate of a foundation model -- such as a large language model -- changes with respect to the amount of training tokens, parameters, and compute. By contrast, Zipf's law posits that the distribution of tokens exhibits a power law tail. Whereas similar claims have been made in more specific settings, we show that the neural scaling law is a consequence of Zipf's law under certain broad assumptions that we reveal systematically. The derivation steps are as follows: We derive Heaps' law on the vocabulary growth from Zipf's law, Hilberg's hypothesis on the entropy scaling from Heaps' law, and the neural scaling from Hilberg's hypothesis. We illustrate these inference steps by a toy example of the Santa Fe process that satisfies all the four statistical laws.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Universality of high-dimensional scaling limits of stochastic gradient descent</title>
<link>https://arxiv.org/abs/2512.13634</link>
<guid>https://arxiv.org/abs/2512.13634</guid>
<content:encoded><![CDATA[
arXiv:2512.13634v2 Announce Type: replace-cross 
Abstract: We consider statistical tasks in high dimensions whose loss depends on the data only through its projection into a fixed-dimensional subspace spanned by the parameter vectors and certain ground truth vectors. This includes classifying mixture distributions with cross-entropy loss with one and two-layer networks, and learning single and multi-index models with one and two-layer networks. When the data is drawn from an isotropic Gaussian mixture distribution, it is known that the evolution of a finite family of summary statistics under stochastic gradient descent converges to an autonomous ordinary differential equation (ODE), as the dimension and sample size go to $\infty$ and the step size goes to $0$ commensurately. Our main result is that these ODE limits are universal in that this limit is the same whenever the data is drawn from mixtures of arbitrary product distributions whose first two moments match the corresponding Gaussian distribution, provided the initialization and ground truth vectors are coordinate-delocalized. We complement this by proving two corresponding non-universality results. We provide a simple example where the ODE limits are non-universal if the initialization is coordinate aligned. We also show that the stochastic differential equation limits arising as fluctuations of the summary statistics around their ODE's fixed points are not universal.
]]></content:encoded>
<pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Journey Before Destination: On the importance of Visual Faithfulness in Slow Thinking</title>
<link>https://arxiv.org/abs/2512.12218</link>
<guid>https://arxiv.org/abs/2512.12218</guid>
<content:encoded><![CDATA[
<div> Reasoning-augmented vision language models, visual faithfulness, perception steps, self-reflection, multimodal reasoning<br /><br />Summary:<br /><br />This paper addresses the challenge of evaluating reasoning-augmented vision language models (VLMs) that generate explicit chains of thought. Such chains enhance model capability and transparency but introduce failure modes where models might produce correct answers through intermediate steps that are visually unfaithful or correctly reason but err on the final prediction. The authors argue that standard evaluations focusing solely on final-answer accuracy fail to capture these nuanced behaviors. They introduce the concept of visual faithfulness as a distinct evaluation dimension, which assesses whether the perception steps within reasoning chains are grounded accurately in the input image. Their proposed framework is training- and reference-free, decomposing reasoning chains into perception and reasoning components. It employs off-the-shelf VLM judges to evaluate step-level faithfulness and is validated through human meta-evaluation. Furthermore, leveraging this metric, they develop a lightweight self-reflection mechanism that detects and regenerates unfaithful perception steps without additional training. Experimental results across multiple reasoning-trained VLMs and perception-heavy benchmarks demonstrate that this approach effectively reduces the Unfaithful Perception Rate while maintaining final-answer accuracy, thereby improving the reliability and transparency of multimodal reasoning systems. <div>
arXiv:2512.12218v2 Announce Type: replace-cross 
Abstract: Reasoning-augmented vision language models (VLMs) generate explicit chains of thought that promise greater capability and transparency but also introduce new failure modes: models may reach correct answers via visually unfaithful intermediate steps, or reason faithfully yet fail on the final prediction. Standard evaluations that only measure final-answer accuracy cannot distinguish these behaviors. We introduce the visual faithfulness of reasoning chains as a distinct evaluation dimension, focusing on whether the perception steps of a reasoning chain are grounded in the image. We propose a training- and reference-free framework that decomposes chains into perception versus reasoning steps and uses off-the-shelf VLM judges for step-level faithfulness, additionally verifying this approach through a human meta-evaluation. Building on this metric, we present a lightweight self-reflection procedure that detects and locally regenerates unfaithful perception steps without any training. Across multiple reasoning-trained VLMs and perception-heavy benchmarks, our method reduces Unfaithful Perception Rate while preserving final-answer accuracy, improving the reliability of multimodal reasoning.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-Resolution Reasoning (NRR): A Computational Framework for Contextual Identity and Ambiguity Preservation</title>
<link>https://arxiv.org/abs/2512.13478</link>
<guid>https://arxiv.org/abs/2512.13478</guid>
<content:encoded><![CDATA[
<div> Keywords: Non-Resolution Reasoning, ambiguity retention, Multi-Vector Embeddings, Non-Collapsing Attention, Contextual Identity Tracking<br /><br />Summary:<br /><br />Current AI systems face a fundamental limitation where they prematurely resolve ambiguity, collapsing multiple valid interpretations into a single output. This issue arises from classical identity assumptions in standard neural architectures. The paper introduces Non-Resolution Reasoning (NRR), a new computational framework that treats ambiguity retention as a valuable reasoning mode rather than a flaw. NRR is built on three core principles: Non-Identity ($A \neq A$), where the same symbol can represent different entities depending on context; Approximate Identity ($A \approx A$), which allows partial but not full overlap between entities; and Non-Resolution, allowing conflicting interpretations to coexist simultaneously without forced convergence. To implement these principles, the authors propose three architectural components: Multi-Vector Embeddings for representing context-dependent meanings, Non-Collapsing Attention to maintain parallel interpretations, and Contextual Identity Tracking (CIT) to enforce the Non-Identity principle during inference. The paper demonstrates NRR’s benefits through case studies in handling paradoxes, enhancing creative generation, and improving context-dependent reasoning. Empirical validation on a synthetic context-shift task reveals that an NRR-lite model achieves 90.9% out-of-distribution accuracy, vastly outperforming standard architectures with 9.1%. This work reframes the role of ambiguity in AI, suggesting the critical question is not whether to resolve ambiguity, but when, how, and controlled by whom. <div>
arXiv:2512.13478v4 Announce Type: replace-cross 
Abstract: Current artificial intelligence systems, despite remarkable capabilities in text generation and pattern recognition, exhibit a fundamental architectural limitation: they resolve ambiguity prematurely. This premature semantic collapse -- the tendency to collapse multiple valid interpretations into a single output -- stems from classical identity assumptions embedded in standard neural architectures. We propose Non-Resolution Reasoning (NRR), a computational framework that treats ambiguity retention as a valid reasoning mode rather than a defect to be eliminated. NRR introduces three core principles: (1) Non-Identity ($A \neq A$) -- the same symbol refers to different entities across contexts; (2) Approximate Identity ($A \approx A$) -- entities share partial structural overlap without being identical; and (3) Non-Resolution -- conflicting interpretations can coexist without forced convergence. We formalize these principles through three architectural components: Multi-Vector Embeddings for context-dependent representation, Non-Collapsing Attention for parallel interpretation retention, and Contextual Identity Tracking (CIT) for maintaining $A \neq A$ across inference. We demonstrate NRR's advantages through case studies in paradox handling, creative generation, and context-dependent reasoning. Crucially, we provide a minimal empirical validation on a synthetic context-shift task where an NRR-lite model achieves 90.9% out-of-distribution accuracy compared to 9.1% for standard architectures, demonstrating that ambiguity preservation enables structural generalization. NRR challenges the assumption that meaning must collapse to be useful, offering a foundation for AI systems capable of sophisticated ambiguity handling and creative reasoning. The question is not whether AI should resolve ambiguity, but when, how, and under whose control.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dion2: A Simple Method to Shrink Matrix in Muon</title>
<link>https://arxiv.org/abs/2512.16928</link>
<guid>https://arxiv.org/abs/2512.16928</guid>
<content:encoded><![CDATA[
<div> Muon optimizer, orthonormalization, scalability, Dion2, sparse update<br /><br />Summary:<br /><br />1. The Muon optimizer is known for its strong empirical performance and solid theoretical foundation, yet it faces scalability challenges due to the super-linear complexity of its orthonormalization step. 2. The orthonormalization step becomes increasingly costly as the size of the matrix it processes grows, creating computational and communication bottlenecks in large-scale applications. 3. Prior research efforts have attempted to address this issue by reducing the size of the matrix involved in orthonormalization, but these methods tend to be complex. 4. The paper introduces Dion2, a simpler approach which improves scalability by shrinking the matrix through sampling, selectively choosing a fraction of rows or columns for orthonormalization at each iteration. 5. This selective sampling results in sparse updates that significantly reduce computational load and communication overhead, thereby enhancing the scalability and efficiency of the Muon optimizer in large-scale environments. <div>
arXiv:2512.16928v1 Announce Type: new 
Abstract: The Muon optimizer enjoys strong empirical performance and theoretical grounding. However, the super-linear cost of its orthonormalization step introduces increasing overhead with scale. To alleviate this cost, several works have attempted to reduce the size of the matrix entering the orthonormalization step. We introduce Dion2, a much simpler method for shrinking the matrix involved in Muon's computation compared to prior approaches. At a high level, Dion2 selects a fraction of rows or columns at each iteration and orthonormalizes only those. This sampling procedure makes the update sparse, reducing both computation and communication costs which in turn improves the scalability of Muon.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BIONIX: A Wireless, Low-Cost Prosthetic Arm with Dual-Signal EEG and EMG Control</title>
<link>https://arxiv.org/abs/2512.16929</link>
<guid>https://arxiv.org/abs/2512.16929</guid>
<content:encoded><![CDATA[
<div> Keywords: upper-limb prostheses, neuro-muscular control, EEG, EMG, low-cost

<br /><br />Summary:  
This project addresses the limitations of affordable upper-limb prostheses, which often lack intuitive control, particularly impacting amputees in low-resource settings. The proposed solution is a low-cost dual-mode neuro-muscular control system that integrates EEG and EMG signals to enable real-time, multi-degree-of-freedom control of a prosthetic arm. EEG data is captured using a NeuroSky MindWave Mobile 2 headset and processed by an ESP32 microcontroller running a lightweight classification model trained on 1500 seconds of data with specific filtering and data splits. The EEG classifier detects strong blink events that toggle the prosthetic hand between open and closed states. EMG signals, acquired via a MyoWare 2.0 sensor, are sent to a second ESP32 microcontroller to perform threshold-based detection across three activation bands, which control elbow movements with added stability via consecutive frame confirmation. The EEG-controlled ESP32 operates four finger servos, while the EMG-controlled ESP32 manages two elbow servos. A functional prototype was built for around $240, primarily due to the cost of the EEG headset. Future improvements aim to include a 3D-printed chassis, reduced EMG latency through auto-regressive models, and enhanced servo torque for better load capacity and grip strength. This system offers a promising, accessible approach to biologically intuitive prosthetic control for underserved populations and global health applications. <div>
arXiv:2512.16929v1 Announce Type: new 
Abstract: Affordable upper-limb prostheses often lack intuitive control systems, limiting functionality and accessibility for amputees in low-resource settings. This project presents a low-cost, dual-mode neuro-muscular control system integrating electroencephalography (EEG) and electromyography (EMG) to enable real-time, multi-degree-of-freedom control of a prosthetic arm. EEG signals are acquired using the NeuroSky MindWave Mobile 2 and transmitted via ThinkGear Bluetooth packets to an ESP32 microcontroller running a lightweight classification model. The model was trained on 1500 seconds of recorded EEG data using a 6-frame sliding window with low-pass filtering, excluding poor-signal samples and using a 70/20/10 training--validation--test split. The classifier detects strong blink events, which toggle the hand between open and closed states. EMG signals are acquired using a MyoWare 2.0 sensor and SparkFun wireless shield and transmitted to a second ESP32, which performs threshold-based detection. Three activation bands (rest: 0--T1; extension: T1--T2; contraction: greater than T2) enable intuitive elbow control, with movement triggered only after eight consecutive frames in a movement class to improve stability. The EEG-controlled ESP32 actuates four finger servos, while the EMG-controlled ESP32 drives two elbow servos. A functional prototype was constructed using low-cost materials (total cost approximately 240 dollars), with most expense attributed to the commercial EEG headset. Future work includes transitioning to a 3D-printed chassis, integrating auto-regressive models to reduce EMG latency, and upgrading servo torque for improved load capacity and grip strength. This system demonstrates a feasible pathway to low-cost, biologically intuitive prosthetic control suitable for underserved and global health applications.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QSMOTE-PGM/kPGM: QSMOTE Based PGM and kPGM for Imbalanced Dataset Classification</title>
<link>https://arxiv.org/abs/2512.16960</link>
<guid>https://arxiv.org/abs/2512.16960</guid>
<content:encoded><![CDATA[
<div> Quantum-inspired machine learning, Kernel Trick, Pretty Good Measurement, Quantum SMOTE, classification performance

<br /><br />Summary:  
This work investigates quantum-inspired machine learning (QiML) approaches that utilize quantum theory concepts to enhance classical algorithms, focusing on inner product structures in high-dimensional feature spaces. It compares two main methods: the kernelized Pretty Good Measurement (KPGM) and direct PGM-based classifiers, both grounded in Hilbert space geometry. The study applies synthetic oversampling techniques using Quantum SMOTE (QSMOTE) variants to evaluate performance under different data augmentation scenarios. Experimental results demonstrate that both PGM and KPGM classifiers outperform classical random forest baselines, especially when multiple quantum copies are used. Notably, PGM with stereo encoding and two quantum copies achieved the highest classification accuracy (0.8512) and F1-score (0.8234). Meanwhile, KPGM showed competitive accuracy with slightly lower peak values but greater stability across QSMOTE variants, particularly in stereo and amplitude encodings. The analysis highlights complementary advantages: PGM benefits from encoding-specific improvements, whereas KPGM offers robustness to variations in synthetic data sampling. These insights advance theoretical understanding and provide practical guidance for choosing between kernel-based and measurement-based QiML methods depending on data features and computational resource considerations. <div>
arXiv:2512.16960v1 Announce Type: new 
Abstract: Quantum-inspired machine learning (QiML) leverages mathematical frameworks from quantum theory to enhance classical algorithms, with particular emphasis on inner product structures in high-dimensional feature spaces. Among the prominent approaches, the Kernel Trick, widely used in support vector machines, provides efficient similarity computation, while the Pretty Good Measurement (PGM), originating from quantum state discrimination, enables classification grounded in Hilbert space geometry. Building on recent developments in kernelized PGM (KPGM) and direct PGM-based classifiers, this work presents a unified theoretical and empirical comparison of these paradigms. We analyze their performance across synthetic oversampling scenarios using Quantum SMOTE (QSMOTE) variants. Experimental results show that both PGM and KPGM classifiers consistently outperform a classical random forest baseline, particularly when multiple quantum copies are employed. Notably, PGM with stereo encoding and n_copies=2 achieves the highest overall accuracy (0.8512) and F1-score (0.8234), while KPGM demonstrates competitive and more stable behavior across QSMOTE variants, with top scores of 0.8511 (stereo) and 0.8483 (amplitude). These findings highlight that quantum-inspired classifiers not only provide tangible gains in recall and balanced performance but also offer complementary strengths: PGM benefits from encoding-specific enhancements, whereas KPGM ensures robustness across sampling strategies. Our results advance the understanding of kernel-based and measurement-based QiML methods, offering practical guidance on their applicability under varying data characteristics and computational constraints.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Compression is Routing: Reconstruction Error as an Intrinsic Signal for Modular Language Models</title>
<link>https://arxiv.org/abs/2512.16963</link>
<guid>https://arxiv.org/abs/2512.16963</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Compression, Mixture-of-Experts, Transformer Autoencoder, Distribution Fingerprint<br /><br />Summary:<br /><br />This paper identifies three major challenges faced by current Large Language Models (LLMs): limited context length, high inference costs, and catastrophic forgetting during continual learning. Traditional Mixture-of-Experts (MoE) architectures alleviate some issues but depend on complex auxiliary classifiers for routing, which reduces interpretability. The authors introduce an innovative concept: "Compression is Routing," leveraging compression as a means of expert scheduling. They develop an 87M-parameter Transformer Autoencoder that compresses sequences 64-fold (512 tokens into 8 latent vectors). The model exhibits strong domain discriminative ability, achieving 99.47% reconstruction accuracy on in-domain code data, but accuracy sharply falls to 47.76% on semi-out-of-domain Wiki texts and 0.57% on fully out-of-distribution random sequences. This demonstrates that reconstruction error acts as an intrinsic distribution fingerprint, enabling expert modules to be selected automatically based on residuals rather than explicit gating networks. This approach promises improved scalability and offers a novel perspective on VRAM compression to handle ultra-long contexts efficiently. The study validates the underlying architecture physically and proposes a new research direction for scalable, modular neural networks for future LLM designs. <div>
arXiv:2512.16963v1 Announce Type: new 
Abstract: Current Large Language Models (LLMs) face three major challenges: context length limitations, high inference costs, and catastrophic forgetting during continual learning. While Mixture-of-Experts (MoE) architectures mitigate some of these conflicts, their routing mechanisms typically rely on explicitly trained auxiliary classifiers. This not only increases system complexity but also often lacks interpretability when handling mixed-domain inputs.
  Building upon the premise that ``Compression is Intelligence,'' this paper proposes a novel architectural philosophy: \textbf{``Compression is Routing.''} We trained an 87M-parameter end-to-end Transformer Autoencoder, achieving a \textbf{64x sequence length compression} (compressing 512 tokens into 8 latent vectors). Experimental results demonstrate that this compressor possesses extreme domain discriminative capability: it achieves a reconstruction accuracy of \textbf{99.47\%} on the in-domain (code) validation set; accuracy drops sharply to \textbf{47.76\%} on a semi-out-of-distribution domain (Wiki text); and further plummets to just \textbf{0.57\%} on a fully out-of-distribution domain (random sequences).
  This extreme and systematic performance discrepancy establishes the validity of reconstruction error as an \textbf{Intrinsic Distribution Fingerprint}. Based on this, we propose that expert modules can be automatically scheduled using reconstruction residuals directly, without the need for explicit gating networks. This mechanism offers excellent scalability. Furthermore, this architecture provides a new perspective on ``VRAM compression'' for handling ultra-long contexts. This report aims to verify the physical validity of this foundational architecture, offering a new research perspective for the next generation of scalable modular neural networks.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Lightweight Machine Learning for Aviation Visibility Nowcasting Across Multiple Climatic Regimes</title>
<link>https://arxiv.org/abs/2512.16967</link>
<guid>https://arxiv.org/abs/2512.16967</guid>
<content:encoded><![CDATA[
<div> Keywords: nowcasting, aviation meteorology, physics-guided machine learning, explainable AI, METAR<br /><br />Summary: This study addresses the challenge of short-term prediction (nowcasting) of low-visibility and precipitation events crucial for aviation safety and efficiency. Traditional operational methods depend on numerical weather prediction models and human-issued Terminal Aerodrome Forecasts (TAF), which suffer from conservative biases and limited temporal resolution. The authors propose a lightweight gradient boosting model (XGBoost) trained solely on surface observation data (METAR) and enhanced through physics-guided feature engineering rooted in thermodynamic principles. The framework is tested across 11 global airports encompassing different climate zones, using historical data spanning 2000 to 2024. Results demonstrate that the model effectively captures local physical processes automatically, without manual tuning. In a blind evaluation against operational TAFs, the model significantly outperformed TAFs at tactical horizons (3 hours), improving recall by 2.5 to 4 times while reducing false alarms. Additionally, SHAP (SHapley Additive exPlanations) analysis indicates the model implicitly reconstructs key physical processes such as advection, radiation, and subsidence, offering explainability and actionable insights for operational situational awareness. This work emphasizes the value of integrating physics-guided machine learning and explainable AI to enhance real-time weather nowcasting relevant to aviation. <div>
arXiv:2512.16967v1 Announce Type: new 
Abstract: Short-term prediction (nowcasting) of low-visibility and precipitation events is critical for aviation safety and operational efficiency. Current operational approaches rely on computationally intensive numerical weather prediction guidance and human-issued TAF products, which often exhibit conservative biases and limited temporal resolution. This study presents a lightweight gradient boosting framework (XGBoost) trained exclusively on surface observation data (METAR) and enhanced through physics-guided feature engineering based on thermodynamic principles. The framework is evaluated across 11 international airports representing distinct climatic regimes (including SCEL, KJFK, KORD, KDEN, SBGR, and VIDP) using historical data from 2000 to 2024. Results suggest that the model successfully captures underlying local physical processes without manual configuration. In a blind comparative evaluation against operational TAF forecasts, the automated model achieved substantially higher detection rates at tactical horizons (3 hours), with a 2.5 to 4.0 times improvement in recall while reducing false alarms. Furthermore, SHAP analysis reveals that the model performs an implicit reconstruction of local physical drivers (advection, radiation, and subsidence), providing actionable explainability for operational situational awareness.
  Keywords: aviation meteorology; physics-guided machine learning; explainable artificial intelligence; lightweight machine learning; nowcasting; METAR; TAF verification; edge computing
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs</title>
<link>https://arxiv.org/abs/2512.17008</link>
<guid>https://arxiv.org/abs/2512.17008</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Group Relative Policy Optimization, Proximal Policy Optimization, turn-PPO, multi-turn tasks<br /><br />Summary:<br /><br />This paper addresses limitations of the Group Relative Policy Optimization (GRPO) algorithm when applied to reinforcement learning (RL) in multi-turn interactive tasks involving long-horizon reasoning. The authors argue that GRPO struggles with stability and effectiveness in these scenarios. To improve performance, they investigate alternative advantage estimation methods and identify Proximal Policy Optimization (PPO) as a more robust solution compared to GRPO. Further advancing PPO, they propose a novel variant called turn-PPO, which reframes the problem using a turn-level Markov Decision Process (MDP) instead of the conventional token-level MDP, making it better suited for multi-turn environments. Experimental evaluations on the WebShop and Sokoban benchmarks show that turn-PPO achieves superior results over existing methods, demonstrating improved stability and effectiveness both in tasks that require long reasoning chains and those that do not. This work contributes to the development of more reliable RL algorithms for training large language model agents capable of handling complex, multi-turn interactions in real-world environments. <div>
arXiv:2512.17008v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has re-emerged as a natural approach for training interactive LLM agents in real-world environments. However, directly applying the widely used Group Relative Policy Optimization (GRPO) algorithm to multi-turn tasks exposes notable limitations, particularly in scenarios requiring long-horizon reasoning. To address these challenges, we investigate more stable and effective advantage estimation strategies, especially for multi-turn settings. We first explore Proximal Policy Optimization (PPO) as an alternative and find it to be more robust than GRPO. To further enhance PPO in multi-turn scenarios, we introduce turn-PPO, a variant that operates on a turn-level MDP formulation, as opposed to the commonly used token-level MDP. Our results on the WebShop and Sokoban datasets demonstrate the effectiveness of turn-PPO, both with and without long reasoning components.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GB-DQN: Gradient Boosted DQN Models for Non-stationary Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.17034</link>
<guid>https://arxiv.org/abs/2512.17034</guid>
<content:encoded><![CDATA[
<div> Keywords: non-stationary environments, deep reinforcement learning, Gradient-Boosted Deep Q-Networks, ensemble learning, Bellman residual<br /><br />Summary:<br /><br />This paper addresses the challenge of non-stationary environments in deep reinforcement learning, where changes in dynamics or rewards disrupt learned value functions and lead to catastrophic forgetting. The authors propose Gradient-Boosted Deep Q-Networks (GB-DQN), an ensemble-based adaptive method that mitigates model drift by incrementally learning residuals rather than retraining a single Q-network from scratch. GB-DQN builds an additive ensemble where each new learner approximates the Bellman residual of the current ensemble after environmental changes. Theoretical analysis demonstrates that each boosting iteration systematically reduces the empirical Bellman residual, and under standard assumptions, the ensemble converges to the post-drift optimal value function. Empirical evaluations on a range of control tasks with controlled dynamics alterations show that GB-DQN enables faster adaptation, improved stability, and enhanced robustness compared to the classic DQN and existing non-stationary reinforcement learning baselines. This work contributes both a principled theoretical framework and practical algorithmic advances for reinforcement learning agents operating in evolving environments. <div>
arXiv:2512.17034v1 Announce Type: new 
Abstract: Non-stationary environments pose a fundamental challenge for deep reinforcement learning, as changes in dynamics or rewards invalidate learned value functions and cause catastrophic forgetting. We propose \emph{Gradient-Boosted Deep Q-Networks (GB-DQN)}, an adaptive ensemble method that addresses model drift through incremental residual learning. Instead of retraining a single Q-network, GB-DQN constructs an additive ensemble in which each new learner is trained to approximate the Bellman residual of the current ensemble after drift. We provide theoretical results showing that each boosting step reduces the empirical Bellman residual and that the ensemble converges to the post-drift optimal value function under standard assumptions. Experiments across a diverse set of control tasks with controlled dynamics changes demonstrate faster recovery, improved stability, and greater robustness compared to DQN and common non-stationary baselines.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SFBD-OMNI: Bridge models for lossy measurement restoration with limited clean samples</title>
<link>https://arxiv.org/abs/2512.17051</link>
<guid>https://arxiv.org/abs/2512.17051</guid>
<content:encoded><![CDATA[
<div> Keywords: distribution restoration, noisy samples, entropic optimal transport, SFBD-OMNI, black-box corruption model<br /><br />Summary:<br /><br />This paper addresses the challenge of restoring the true underlying data distribution when only noisy, partially observed samples are available, a common scenario in many real-world applications where fully observed data is costly or infeasible to collect. The authors propose framing the problem as a one-sided entropic optimal transport task and present an EM-like algorithm for its solution. They introduce a test criterion to assess whether the original distribution can be recovered despite per-sample information loss, highlighting conditions for recoverability. For cases where recovery is not possible from noisy data alone, the study shows that incorporating a small number of clean samples can substantially improve reconstruction accuracy. Based on these theoretical insights, the authors develop SFBD-OMNI, a novel bridge model-based framework that generalizes prior Stochastic Forward-Backward Deconvolution (SFBD) methods, extending applicability beyond Gaussian noise to arbitrary corruption models treated as black-box generators. Experiments conducted on multiple benchmark datasets under a variety of measurement models demonstrate that SFBD-OMNI achieves notable qualitative and quantitative improvements in restoring true distributions compared to existing approaches, validating the practical effectiveness of their method. <div>
arXiv:2512.17051v1 Announce Type: new 
Abstract: In many real-world scenarios, obtaining fully observed samples is prohibitively expensive or even infeasible, while partial and noisy observations are comparatively easy to collect. In this work, we study distribution restoration with abundant noisy samples, assuming the corruption process is available as a black-box generator. We show that this task can be framed as a one-sided entropic optimal transport problem and solved via an EM-like algorithm. We further provide a test criterion to determine whether the true underlying distribution is recoverable under per-sample information loss, and show that in otherwise unrecoverable cases, a small number of clean samples can render the distribution largely recoverable. Building on these insights, we introduce SFBD-OMNI, a bridge model-based framework that maps corrupted sample distributions to the ground-truth distribution. Our method generalizes Stochastic Forward-Backward Deconvolution (SFBD; Lu et al., 2025) to handle arbitrary measurement models beyond Gaussian corruption. Experiments across benchmark datasets and diverse measurement settings demonstrate significant improvements in both qualitative and quantitative performance.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Tool Dependency Retrieval for Efficient Function Calling</title>
<link>https://arxiv.org/abs/2512.17052</link>
<guid>https://arxiv.org/abs/2512.17052</guid>
<content:encoded><![CDATA[
<div> Function calling, Large Language Models, tool retrieval, dynamic context, task automation<br /><br />Summary:<br /><br />This paper addresses the challenge of tool selection for function calling agents powered by Large Language Models (LLMs), focusing on improving on-device agent efficiency through better retrieval methods. Existing approaches rely on static, limited inputs which fail to capture multi-step dependencies among tools and do not adapt to an evolving task context, often resulting in irrelevant tool retrieval and decreased performance. The authors propose Dynamic Tool Dependency Retrieval (DTDR), a lightweight retrieval method that conditions on both the initial user query and the unfolding execution context, enabling adaptive and context-aware selection of relevant tools. DTDR leverages function calling demonstrations to learn tool dependencies and dynamically updates the retrieval process as the plan develops. The method is benchmarked against state-of-the-art static retrieval techniques across multiple datasets and different LLM backbones, with evaluations measuring retrieval precision, downstream task accuracy, and computational efficiency. Results demonstrate that DTDR significantly improves function calling success rates, with gains ranging from 23% to 104% compared to existing static retrievers. Additionally, the study explores strategies for effectively integrating the retrieved tools into LLM prompts to further boost performance in task automation scenarios. <div>
arXiv:2512.17052v1 Announce Type: new 
Abstract: Function calling agents powered by Large Language Models (LLMs) select external tools to automate complex tasks. On-device agents typically use a retrieval module to select relevant tools, improving performance and reducing context length. However, existing retrieval methods rely on static and limited inputs, failing to capture multi-step tool dependencies and evolving task context. This limitation often introduces irrelevant tools that mislead the agent, degrading efficiency and accuracy. We propose Dynamic Tool Dependency Retrieval (DTDR), a lightweight retrieval method that conditions on both the initial query and the evolving execution context. DTDR models tool dependencies from function calling demonstrations, enabling adaptive retrieval as plans unfold. We benchmark DTDR against state-of-the-art retrieval methods across multiple datasets and LLM backbones, evaluating retrieval precision, downstream task accuracy, and computational efficiency. Additionally, we explore strategies to integrate retrieved tools into prompts. Our results show that dynamic tool retrieval improves function calling success rates between $23\%$ and $104\%$ compared to state-of-the-art static retrievers.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Universal consistency of the $k$-NN rule in metric spaces and Nagata dimension. III</title>
<link>https://arxiv.org/abs/2512.17058</link>
<guid>https://arxiv.org/abs/2512.17058</guid>
<content:encoded><![CDATA[
<div> Keywords: k-nearest neighbour classifier, universal consistency, Lebesgue--Besicovitch differentiation, sigma-finite dimensionality, Nagata dimension<br /><br />Summary:<br /><br />This article establishes the final missing link in proving the equivalence between three important properties in the context of complete separable metric spaces \( X \). First, it deals with the universal weak consistency of the \( k \)-nearest neighbour classifier in \( X \). Second, it considers the strong Lebesgue--Besicovitch differentiation property holding for every locally finite Borel measure on \( X \). Third, it addresses the characterization of \( X \) being sigma-finite dimensional in the sense defined by Nagata. Previous works had shown the equivalence between the differentiation property and sigma-finite dimensionality, as well as the differentiation property implying universal consistency. The key contribution of this article is proving the last missing implication: that if the \( k \)-nearest neighbour classifier is universally weakly consistent, then the space \( X \) must be sigma-finite dimensional. This completes the proof of equivalence of all three conditions. The paper also revisits and corrects an incorrect assertion made in a recent follow-up article from 2024, and thereby clarifies the conjecture initially posed in 2020 by the same authors. The results unify and deepen the understanding of the geometric and measure-theoretic properties underpinning universal consistency of classifiers. <div>
arXiv:2512.17058v1 Announce Type: new 
Abstract: We prove the last remaining implication allowing to claim the equivalence of the following conditions for a complete separable metric space $X$:
  (1) The $k$-nearest neighbour classifier is (weakly) universally consistent in $X$, (2) The strong Lebesgue--Besicovitch differentiation property holds in $X$ for every locally finite Borel measure, (3) $X$ is sigma-finite dimensional in the sense of Nagata.
  The equivalence (2)$\iff$(3) was announced by Preiss (1983), while a detailed proof of the implication (3)$\Rightarrow$(2) has appeared in Assouad and Quentin de Gromard (2006). The implication (2)$\Rightarrow$(1) was established by C\'erou and Guyader (2006). We prove the implication (1)$\Rightarrow$(3). The result was conjectured in the first article in the series (Collins, Kumari, Pestov 2020), and here we also correct a wrong claim made in the second article (Kumari and Pestov 2024).
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bandwidth-Efficient Adaptive Mixture-of-Experts via Low-Rank Compensation</title>
<link>https://arxiv.org/abs/2512.17073</link>
<guid>https://arxiv.org/abs/2512.17073</guid>
<content:encoded><![CDATA[
<div> Mixture-of-Experts, offloading, static uniform quantization, low-rank compensators, bandwidth-accuracy trade-off<br /><br />Summary:<br /><br />1. Mixture-of-Experts (MoE) models achieve scalable capacity through sparse activation but face challenges in memory usage and bandwidth demands. 2. Offloading techniques help reduce GPU memory pressure by fetching experts only when needed; however, token-level routing introduces irregular data transfers, leading to inference that is I/O-bound. 3. Existing static uniform quantization methods reduce communication traffic but tend to degrade accuracy significantly under aggressive compression because they do not account for the heterogeneity among experts. 4. The paper introduces Bandwidth-Efficient Adaptive Mixture-of-Experts via Low-Rank Compensation, a method that performs router-guided precision restoration by leveraging precomputed low-rank compensators. 5. During inference, this approach transfers compact low-rank factors for the top-n experts per token (where n is smaller than k) and applies compensation to them, while keeping other experts in low-bit precision, effectively balancing bandwidth and accuracy. 6. When integrated with offloading on GPU and GPU-NDP systems, this method demonstrates a superior trade-off between bandwidth usage and model accuracy, alongside improved throughput in inference tasks. <div>
arXiv:2512.17073v1 Announce Type: new 
Abstract: Mixture-of-Experts (MoE) models scale capacity via sparse activation but stress memory and bandwidth. Offloading alleviates GPU memory by fetching experts on demand, yet token-level routing causes irregular transfers that make inference I/O-bound. Static uniform quantization reduces traffic but degrades accuracy under aggressive compression by ignoring expert heterogeneity. We present Bandwidth-Efficient Adaptive Mixture-of-Experts via Low-Rank Compensation, which performs router-guided precision restoration using precomputed low-rank compensators. At inference time, our method transfers compact low-rank factors with Top-n (n<k) experts per token and applies compensation to them, keeping others low-bit. Integrated with offloading on GPU and GPU-NDP systems, our method delivers a superior bandwidth-accuracy trade-off and improved throughput.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Large Reasoning Models Improve Accuracy on Mathematical Tasks Using Flawed Thinking?</title>
<link>https://arxiv.org/abs/2512.17079</link>
<guid>https://arxiv.org/abs/2512.17079</guid>
<content:encoded><![CDATA[
<div> Keywords: Chain-of-thought prompting, error recovery, reinforcement learning, mathematical reasoning, large language models<br /><br />Summary:<br /><br />1. Chain-of-thought (CoT) prompting is essential for enabling large language models (LLMs) to perform mathematical reasoning, but these models are vulnerable to early errors that can cause entire solutions to fail.  
2. The study explores whether training LLMs with intentionally flawed reasoning traces can help the models detect and recover from mistakes without reducing their problem-solving capabilities.  
3. Using competition-level math problems from MATH-lighteval, the researchers generated CoT prefixes containing exactly one controlled error, either a calculation error (e.g., sign flips, dropped terms) or a reasoning error (e.g., misapplied rules, unjustified steps).  
4. The Qwen3-4B model was fine-tuned with a reinforcement learning method (GRPO) using a binary reward based on the final answer's correctness.  
5. The Mixed-CoT-RL model, trained on both clean and flawed reasoning, achieved comparable performance on clean problems (41%) to standard RL fine-tuning but significantly outperformed on problems with flawed reasoning traces (24% vs. 19%).  
6. Conversely, conventional RL trained only on clean examples reduced robustness on flawed prefills below the baseline (19% vs. 20%), showing increased sensitivity to errors.  
7. Exposure to reasoning errors during training offered greater robustness improvements than exposure to calculation errors alone, with combined training producing the best outcomes.  
8. The results indicate that deliberately training on flawed reasoning can improve LLMs' ability to recover from mistakes, paving the way for more reliable mathematical reasoning in large language models. <div>
arXiv:2512.17079v1 Announce Type: new 
Abstract: Chain-of-thought (CoT) prompting has become central to mathematical reasoning in large language models, yet models remain brittle to early errors: a single arithmetic slip or unjustified inference typically propagates uncorrected to an incorrect final answer. We investigate whether training on intentionally flawed reasoning traces can teach models to detect and recover from such errors without degrading standard problem-solving ability. Using competition-level problems from MATH-lighteval, we generate CoT prefixes containing exactly one controlled error, either a calculation error (sign flips, dropped terms) or a reasoning error (misapplied rules, unjustified logical steps), and fine-tune Qwen3-4B with GRPO using a binary final-answer reward. Our Mixed-CoT-RL model matches standard RL on clean problems (41% vs 41%) while substantially outperforming it on problems prefilled with flawed reasoning (24% vs 19%). Notably, clean-only RL fine-tuning degrades robustness below the untuned baseline 19% vs. 20%), indicating that conventional training increases susceptibility to misleading prefills. Among error types, training on reasoning errors yields greater robustness gains than calculation errors alone, with mixed training performing best. These findings demonstrate that exposure to flawed traces during training can improve error-recovery behavior without sacrificing accuracy, suggesting a path toward more robust mathematical reasoning in LLMs.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How to Square Tensor Networks and Circuits Without Squaring Them</title>
<link>https://arxiv.org/abs/2512.17090</link>
<guid>https://arxiv.org/abs/2512.17090</guid>
<content:encoded><![CDATA[
<div> Keywords: squared tensor networks, squared circuits, marginalization, unitary parameterization, distribution estimation<br /><br />Summary:<br /><br />This article addresses the complexity challenges in computing partition functions and marginalizing variables in squared tensor networks (TNs) and their extension, squared circuits, which are used for expressive distribution estimation. The squaring operation in these models adds computational overhead, limiting their practical use in machine learning tasks. To mitigate this, canonical forms of TNs parameterized via unitary matrices have been developed to simplify marginalization. However, these canonical forms do not extend naturally to squared circuits due to their ability to represent factorizations that do not correspond directly to known TN structures. The authors introduce a novel approach inspired by orthogonality principles in canonical forms and determinism in circuits, enabling efficient parameterization of squared circuits. This new parameterization reduces the computational burden of marginalization even for factorizations encoded by circuits with otherwise intractable structure. Experimental results on distribution estimation demonstrate that these conditions on squared circuits do not reduce expressiveness but significantly improve learning efficiency. Thus, the work presents a key advance in making squared circuits more computationally feasible without compromising their representational power. <div>
arXiv:2512.17090v1 Announce Type: new 
Abstract: Squared tensor networks (TNs) and their extension as computational graphs--squared circuits--have been used as expressive distribution estimators, yet supporting closed-form marginalization. However, the squaring operation introduces additional complexity when computing the partition function or marginalizing variables, which hinders their applicability in ML. To solve this issue, canonical forms of TNs are parameterized via unitary matrices to simplify the computation of marginals. However, these canonical forms do not apply to circuits, as they can represent factorizations that do not directly map to a known TN. Inspired by the ideas of orthogonality in canonical forms and determinism in circuits enabling tractable maximization, we show how to parameterize squared circuits to overcome their marginalization overhead. Our parameterizations unlock efficient marginalization even in factorizations different from TNs, but encoded as circuits, whose structure would otherwise make marginalization computationally hard. Finally, our experiments on distribution estimation show how our proposed conditions in squared circuits come with no expressiveness loss, while enabling more efficient learning.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Plan, Planning to Learn: Adaptive Hierarchical RL-MPC for Sample-Efficient Decision Making</title>
<link>https://arxiv.org/abs/2512.17091</link>
<guid>https://arxiv.org/abs/2512.17091</guid>
<content:encoded><![CDATA[
<div> Keywords: hierarchical planning, reinforcement learning, MPC, MPPI sampler, adaptive sampling<br /><br />Summary:  
This work introduces a novel approach that integrates reinforcement learning (RL) with Model Predictive Control (MPC) to address hierarchical planning problems. The method establishes a tight coupling between RL actions and the MPPI (Model Predictive Path Integral) sampler, allowing RL to guide the sampling process. It then adaptively aggregates MPPI samples to enhance value estimation, focusing exploration efforts where uncertainty is higher. This adaptive interplay between RL and MPC yields improved training robustness and better policy outcomes. The approach is validated across multiple challenging domains, including race driving, a modified Acrobot, and the Lunar Lander with added obstacles, demonstrating versatility. Experimental results highlight significant improvements in data efficiency and overall task performance, showcasing a 72% increase in success rates compared to existing methods. Moreover, the proposed adaptive sampling accelerates convergence by over twofold relative to non-adaptive techniques. Overall, the study delivers a robust hierarchical planning framework that efficiently balances exploration and exploitation, adapts seamlessly to diverse applications, and outperforms current state-of-the-art approaches in both reward metrics and task success. <div>
arXiv:2512.17091v1 Announce Type: new 
Abstract: We propose a new approach for solving planning problems with a hierarchical structure, fusing reinforcement learning and MPC planning. Our formulation tightly and elegantly couples the two planning paradigms. It leverages reinforcement learning actions to inform the MPPI sampler, and adaptively aggregates MPPI samples to inform the value estimation. The resulting adaptive process leverages further MPPI exploration where value estimates are uncertain, and improves training robustness and the overall resulting policies. This results in a robust planning approach that can handle complex planning problems and easily adapts to different applications, as demonstrated over several domains, including race driving, modified Acrobot, and Lunar Lander with added obstacles. Our results in these domains show better data efficiency and overall performance in terms of both rewards and task success, with up to a 72% increase in success rate compared to existing approaches, as well as accelerated convergence (x2.1) compared to non-adaptive sampling.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniCoMTE: A Universal Counterfactual Framework for Explaining Time-Series Classifiers on ECG Data</title>
<link>https://arxiv.org/abs/2512.17100</link>
<guid>https://arxiv.org/abs/2512.17100</guid>
<content:encoded><![CDATA[
<div> Keywords: counterfactual explanations, multivariate time series, model interpretability, ECG classification, deep learning  

<br /><br />Summary:  
This study introduces UniCoMTE, a novel model-agnostic framework designed to generate counterfactual explanations specifically for multivariate time series classifiers. UniCoMTE aims to improve trust and transparency in deep neural networks by identifying the temporal features that most significantly influence classification decisions. The framework works by modifying input samples and analyzing the effect on model predictions, and is versatile enough to be applied across diverse model architectures while operating directly on raw time series data. The authors evaluate UniCoMTE using an ECG classification task, a critical healthcare application, comparing its explanation quality with established methods like LIME and SHAP. The comparison emphasizes comprehensibility and the ability of explanations to generalize across similar samples. Clinical relevance is further validated through expert feedback collected via questionnaires, where medical professionals reviewed counterfactual explanations alongside original ECG data. Results demonstrate that UniCoMTE provides concise, stable, and human-aligned explanations that surpass existing approaches in clarity and practical use. By linking predictive outcomes to meaningful signal patterns, UniCoMTE advances the interpretability of deep learning models, facilitating their adoption in high-stakes time series applications such as healthcare diagnostics. <div>
arXiv:2512.17100v1 Announce Type: new 
Abstract: Machine learning models, particularly deep neural networks, have demonstrated strong performance in classifying complex time series data. However, their black-box nature limits trust and adoption, especially in high-stakes domains such as healthcare. To address this challenge, we introduce UniCoMTE, a model-agnostic framework for generating counterfactual explanations for multivariate time series classifiers. The framework identifies temporal features that most heavily influence a model's prediction by modifying the input sample and assessing its impact on the model's prediction. UniCoMTE is compatible with a wide range of model architectures and operates directly on raw time series inputs. In this study, we evaluate UniCoMTE's explanations on a time series ECG classifier. We quantify explanation quality by comparing our explanations' comprehensibility to comprehensibility of established techniques (LIME and SHAP) and assessing their generalizability to similar samples. Furthermore, clinical utility is assessed through a questionnaire completed by medical experts who review counterfactual explanations presented alongside original ECG samples. Results show that our approach produces concise, stable, and human-aligned explanations that outperform existing methods in both clarity and applicability. By linking model predictions to meaningful signal patterns, the framework advances the interpretability of deep learning models for real-world time series applications.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fault Diagnosis and Quantification for Photovoltaic Arrays based on Differentiable Physical Models</title>
<link>https://arxiv.org/abs/2512.17107</link>
<guid>https://arxiv.org/abs/2512.17107</guid>
<content:encoded><![CDATA[
<div> Fault Diagnosis, Photovoltaic Arrays, Differentiable Simulation, Gradient-Based Identification, I-V Curve Reconstruction<br /><br />Summary:<br /><br />1. The paper addresses the critical need for accurate fault diagnosis and quantification to ensure the reliable operation and intelligent maintenance of photovoltaic (PV) arrays.<br /><br />2. It identifies limitations in existing fault quantification methods, particularly in terms of efficiency and interpretability.<br /><br />3. To overcome these challenges, the authors propose a novel fault quantification approach based on a Differentiable Fast Fault Simulation Model (DFFSM), which accurately simulates I-V characteristics under multiple fault conditions.<br /><br />4. The DFFSM provides analytical gradients with respect to fault parameters, enabling the use of a gradient-based fault parameter identification (GFPI) method.<br /><br />5. The GFPI employs the Adahessian optimizer to efficiently quantify common faults such as partial shading, short-circuit, and series-resistance degradation.<br /><br />6. Experimental validation on both simulated and real measured I-V curve data demonstrates that the proposed method achieves high accuracy, maintaining I-V reconstruction errors below 3%.<br /><br />7. The results confirm the feasibility and effectiveness of using differentiable physical simulators combined with gradient-based optimization for fault diagnosis in PV systems. <div>
arXiv:2512.17107v1 Announce Type: new 
Abstract: Accurate fault diagnosis and quantification are essential for the reliable operation and intelligent maintenance of photovoltaic (PV) arrays. However, existing fault quantification methods often suffer from limited efficiency and interpretability. To address these challenges, this paper proposes a novel fault quantification approach for PV strings based on a differentiable fast fault simulation model (DFFSM). The proposed DFFSM accurately models I-V characteristics under multiple faults and provides analytical gradients with respect to fault parameters. Leveraging this property, a gradient-based fault parameters identification (GFPI) method using the Adahessian optimizer is developed to efficiently quantify partial shading, short-circuit, and series-resistance degradation. Experimental results on both simulated and measured I-V curves demonstrate that the proposed GFPI achieves high quantification accuracy across different faults, with the I-V reconstruction error below 3%, confirming the feasibility and effectiveness of the application of differentiable physical simulators for PV system fault diagnosis.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Atom: Efficient On-Device Video-Language Pipelines Through Modular Reuse</title>
<link>https://arxiv.org/abs/2512.17108</link>
<guid>https://arxiv.org/abs/2512.17108</guid>
<content:encoded><![CDATA[
<div> Keywords: video-language models, on-device system, module reuse, latency reduction, mobile execution efficiency<br /><br />Summary: Recent developments in video-language models have enabled complex tasks such as video retrieval, captioning, and assembly but face challenges when executed on mobile devices due to inefficient pipeline management. The paper presents Atom, an innovative on-device system designed to optimize video-language pipelines by decomposing large billion-parameter models into modular components like visual encoders and language decoders. Atom focuses on module reuse across various subtasks, including captioning, reasoning, and indexing, which eliminates redundant model loading and enables parallel execution. This reuse-centric approach significantly reduces end-to-end latency while maintaining performance quality. Empirical results demonstrate that Atom achieves 27–33% faster execution on commodity smartphones compared to traditional non-reuse methods, with only slight performance drops observed in key metrics such as Recall@1 for retrieval (≤ 2.3) and CIDEr for captioning (≤ 1.5). Consequently, Atom offers a practical and scalable solution for efficient video-language understanding specifically tailored for edge devices, addressing both speed and resource constraints without compromising accuracy or capability. <div>
arXiv:2512.17108v1 Announce Type: new 
Abstract: Recent advances in video-language models have enabled powerful applications like video retrieval, captioning, and assembly. However, executing such multi-stage pipelines efficiently on mobile devices remains challenging due to redundant model loads and fragmented execution. We introduce Atom, an on-device system that restructures video-language pipelines for fast and efficient execution. Atom decomposes a billion-parameter model into reusable modules, such as the visual encoder and language decoder, and reuses them across subtasks like captioning, reasoning, and indexing. This reuse-centric design eliminates repeated model loading and enables parallel execution, reducing end-to-end latency without sacrificing performance. On commodity smartphones, Atom achieves 27--33% faster execution compared to non-reuse baselines, with only marginal performance drop ($\leq$ 2.3 Recall@1 in retrieval, $\leq$ 1.5 CIDEr in captioning). These results position Atom as a practical, scalable approach for efficient video-language understanding on edge devices.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Training and Merging Through Momentum-Aware Optimization</title>
<link>https://arxiv.org/abs/2512.17109</link>
<guid>https://arxiv.org/abs/2512.17109</guid>
<content:encoded><![CDATA[
<div> Low-rank structure, parameter importance, curvature-aware merging, model composition, optimization trajectory  

<br /><br />Summary:  
This paper addresses the inefficiency and isolation in current workflows that handle large neural network training and task-specific model merging by separately computing curvature information. The authors propose a unified framework that maintains factorized momentum and curvature statistics throughout training, allowing this valuable information to be reused for geometry-aware model composition without redundant computation. Their method matches the memory efficiency of state-of-the-art approaches while accumulating task saliency scores, enabling curvature-aware merging without the need for additional Fisher information post-training. They provide convergence guarantees for non-convex objectives, with approximation errors linked to gradient singular value decay. Experiments on natural language understanding benchmarks demonstrate that curvature-aware parameter selection outperforms magnitude-based baselines at all sparsity levels and that multi-task model merging improves upon strong existing baselines. The framework also achieves rank-invariant convergence and shows greater robustness to hyperparameter settings compared to other low-rank optimizers. By conceptualizing the optimization trajectory as a reusable asset rather than discarding it, this approach eliminates redundant computational steps and enables more principled, effective model merging and parameter selection. <div>
arXiv:2512.17109v1 Announce Type: new 
Abstract: Training large neural networks and merging task-specific models both exploit low-rank structure and require parameter importance estimation, yet these challenges have been pursued in isolation. Current workflows compute curvature information during training, discard it, then recompute similar information for merging -- wasting computation and discarding valuable trajectory data. We introduce a unified framework that maintains factorized momentum and curvature statistics during training, then reuses this information for geometry-aware model composition. The proposed method achieves memory efficiency comparable to state-of-the-art approaches while accumulating task saliency scores that enable curvature-aware merging without post-hoc Fisher computation. We establish convergence guarantees for non-convex objectives with approximation error bounded by gradient singular value decay. On natural language understanding benchmarks, curvature-aware parameter selection outperforms magnitude-only baselines across all sparsity levels, with multi-task merging improving over strong baselines. The proposed framework exhibits rank-invariant convergence and superior hyperparameter robustness compared to existing low-rank optimizers. By treating the optimization trajectory as a reusable asset rather than discarding it, our approach eliminates redundant computation while enabling more principled model composition.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Digitizing Nepal's Written Heritage: A Comprehensive HTR Pipeline for Old Nepali Manuscripts</title>
<link>https://arxiv.org/abs/2512.17111</link>
<guid>https://arxiv.org/abs/2512.17111</guid>
<content:encoded><![CDATA[
<div> Old Nepali, Handwritten Text Recognition, Encoder-Decoder, Character Error Rate, Low-resource Language<br /><br />Summary: This paper introduces the first comprehensive end-to-end pipeline designed for Handwritten Text Recognition (HTR) specifically tailored to Old Nepali, an important but under-resourced historical language. The approach focuses on line-level transcription, allowing for more contextual recognition compared to isolated character recognition. The authors systematically investigate different encoder-decoder architectures, exploring how various model designs impact the overall recognition accuracy. Complementing architecture choices, data-centric techniques are applied to enhance performance further. The best performing model achieves a remarkable Character Error Rate (CER) of 4.9%, indicating high accuracy in recognizing handwritten Old Nepali text. To gain deeper insights into the model’s behavior and error distribution, the study includes a detailed analysis of token-level confusions and evaluates different decoding strategies. Although the dataset used for evaluation remains confidential, the authors contribute to the research community by releasing their training code, model configurations, and evaluation scripts. This enables other researchers to replicate the study or extend it to other low-resource historical scripts. Overall, the work represents a significant step forward in applying modern HTR techniques to ancient and low-resource languages, promoting further development and preservation of historical scripts. <div>
arXiv:2512.17111v1 Announce Type: new 
Abstract: This paper presents the first end-to-end pipeline for Handwritten Text Recognition (HTR) for Old Nepali, a historically significant but low-resource language. We adopt a line-level transcription approach and systematically explore encoder-decoder architectures and data-centric techniques to improve recognition accuracy. Our best model achieves a Character Error Rate (CER) of 4.9\%. In addition, we implement and evaluate decoding strategies and analyze token-level confusions to better understand model behaviour and error patterns. While the dataset we used for evaluation is confidential, we release our training code, model configurations, and evaluation scripts to support further research in HTR for low-resource historical scripts.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Effect of Negation on CLIP in Medical Imaging: Limitations of Contrastive Language-Image Pretraining</title>
<link>https://arxiv.org/abs/2512.17121</link>
<guid>https://arxiv.org/abs/2512.17121</guid>
<content:encoded><![CDATA[
<div> Keywords: CLIP, medical imaging, negation, chest X-ray retrieval, fine tuning  

<br /><br />Summary:  
This study focuses on the challenges faced by large vision-language models like CLIP in medical imaging tasks, specifically their difficulty in interpreting negated phrases—a critical issue for accurate medical diagnosis. The researchers evaluated the Stanford AIMI CheXagent model’s performance in retrieving chest X-ray images using queries both with and without negation. The primary objective was to identify failure points in handling negation and subsequently improve retrieval accuracy by applying fine tuning methods derived from prior research. The results indicate that fine tuning enhances the model’s ability to process negation, although there is a minor reduction in accuracy when responding to positive (non-negated) prompts. Beyond retrieval accuracy, the study analyzed the internal behavior of the text encoder through methods such as token attribution, t-SNE visualization, and attention-head ablation, shedding light on how fine tuning alters the model’s internal representation of negated clinical language. This work aims to deepen understanding of CLIP’s internal mechanisms and improve its reliability for clinical applications by making it better suited for handling negated statements in medical text, ultimately contributing to more dependable AI-assisted medical devices. <div>
arXiv:2512.17121v1 Announce Type: new 
Abstract: Large vision-language models like CLIP are increasingly used in medical imaging tasks due to their ability to align images and text without the need for extensive labeled data. This makes them particularly useful for applications like image retrieval, report generation, and classification in clinical settings. A potential issue to this approach is that CLIP-based models often under perform when interpreting negated phrases, which is especially problematic in the context of medical diagnosing. In this study, we evaluate the Stanford AIMI CheXagent model on its ability to correctly retrieve chest X-ray images using prompts with and without negation. The goal of this project is to understand where this model fails and then use it as a base model to improve its retrieval accuracy by fine tuning methods outlined in previous work. Results from this study show improvement in handling of negation in the CLIP model with a slight decrease in accuracy of positive prompt evaluation. Alongside retrieval accuracy, we examined internal model behavior through token attribution, t-SNE projection, and attention-head ablation to better characterize how each fine tuning approach reshaped the text encoders representation of negated clinical language. Through this work, we hope to better understand the internal behavior of CLIP and improve its handling of negation using clinically relevant language for improving its reliability in medical AI devices.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffeoMorph: Learning to Morph 3D Shapes Using Differentiable Agent-Based Simulations</title>
<link>https://arxiv.org/abs/2512.17129</link>
<guid>https://arxiv.org/abs/2512.17129</guid>
<content:encoded><![CDATA[
<div> Keywords: morphogenesis, SE(3)-equivariant graph neural network, 3D Zernike polynomials, shape-matching loss, implicit differentiation<br /><br />Summary: Biological systems form complex 3D structures through distributed control among identical agents without central coordination. This work introduces DiffeoMorph, an end-to-end differentiable framework that learns morphogenesis protocols guiding agents to form desired 3D shapes. Each agent updates its position and internal state using an attention-based SE(3)-equivariant graph neural network influenced by signals from peers, ensuring spatial and rotational equivariance. To compare predicted and target shapes continuously rather than as point clouds, the authors develop a novel shape-matching loss based on 3D Zernike polynomials, which is invariant to agent ordering, population size, and rigid transformations. They enforce full SO(3) invariance (rotation invariant but sensitive to reflections) by including an alignment step that optimally rotates the predicted Zernike spectrum to minimize the loss. This creates a bilevel optimization problem with an inner loop optimizing a unit quaternion for alignment and an outer loop updating the agent model. Gradients through the alignment step are computed using implicit differentiation. Extensive benchmarks show the superiority of their shape-matching loss over standard metrics. Finally, DiffeoMorph demonstrates the ability to form a variety of shapes, from simple ellipsoids to complex morphologies, using only minimal spatial cues. <div>
arXiv:2512.17129v1 Announce Type: new 
Abstract: Biological systems can form complex three-dimensional structures through the collective behavior of identical agents -- cells that follow the same internal rules and communicate without central control. How such distributed control gives rise to precise global patterns remains a central question not only in developmental biology but also in distributed robotics, programmable matter, and multi-agent learning. Here, we introduce DiffeoMorph, an end-to-end differentiable framework for learning a morphogenesis protocol that guides a population of agents to morph into a target 3D shape. Each agent updates its position and internal state using an attention-based SE(3)-equivariant graph neural network, based on its own internal state and signals received from other agents. To train this system, we introduce a new shape-matching loss based on the 3D Zernike polynomials, which compares the predicted and target shapes as continuous spatial distributions, not as discrete point clouds, and is invariant to agent ordering, number of agents, and rigid-body transformations. To enforce full SO(3) invariance -- invariant to rotations yet sensitive to reflections, we include an alignment step that optimally rotates the predicted Zernike spectrum to match the target before computing the loss. This results in a bilevel problem, with the inner loop optimizing a unit quaternion for the best alignment and the outer loop updating the agent model. We compute gradients through the alignment step using implicit differentiation. We perform systematic benchmarking to establish the advantages of our shape-matching loss over other standard distance metrics for shape comparison tasks. We then demonstrate that DiffeoMorph can form a range of shapes -- from simple ellipsoids to complex morphologies -- using only minimal spatial cues.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Smoothing DiLoCo with Primal Averaging for Faster Training of LLMs</title>
<link>https://arxiv.org/abs/2512.17131</link>
<guid>https://arxiv.org/abs/2512.17131</guid>
<content:encoded><![CDATA[
<div> Keywords: Generalized Primal Averaging, Nesterov's method, averaging-based optimizers, AdamW, convergence guarantee<br /><br />Summary:<br /><br />1. The paper introduces Generalized Primal Averaging (GPA), an extension of Nesterov's primal averaging method designed to overcome limitations in recent averaging-based optimizers like single-worker DiLoCo and Schedule-Free (SF) in non-distributed training.<br /><br />2. Existing methods such as Schedule-Free maintain uniform averages of past weights, while single-worker DiLoCo applies implicit averaging through periodic pseudo-gradient aggregation, which incurs increased memory use and extra hyperparameters due to a two-loop structure.<br /><br />3. GPA addresses these drawbacks by decoupling the interpolation constant in Nesterov's primal averaging, allowing smooth averaging at every iteration and generalizing the single-worker DiLoCo approach.<br /><br />4. Empirical results demonstrate that GPA consistently outperforms single-worker DiLoCo, simplifies hyperparameter tuning, eliminates the two-loop overhead, and reduces memory consumption to just one additional buffer.<br /><br />5. On benchmark models such as Llama-160M and ImageNet ViT, GPA achieves significant speedups (up to ~27%) in reaching validation performance equivalent to AdamW. Theoretical analysis also guarantees that GPA can match or improve upon the base optimizer’s convergence rate, characterized by $O(\sqrt{T})$ regret, through appropriate interpolation constants. <div>
arXiv:2512.17131v1 Announce Type: new 
Abstract: We propose Generalized Primal Averaging (GPA), an extension of Nesterov's method in its primal averaging formulation that addresses key limitations of recent averaging-based optimizers such as single-worker DiLoCo and Schedule-Free (SF) in the non-distributed setting. These two recent algorithmic approaches improve the performance of base optimizers, such as AdamW, through different iterate averaging strategies. Schedule-Free explicitly maintains a uniform average of past weights, while single-worker DiLoCo performs implicit averaging by periodically aggregating trajectories, called pseudo-gradients, to update the model parameters. However, single-worker DiLoCo's periodic averaging introduces a two-loop structure, increasing its memory requirements and number of hyperparameters. GPA overcomes these limitations by decoupling the interpolation constant in the primal averaging formulation of Nesterov. This decoupling enables GPA to smoothly average iterates at every step, generalizing and improving upon single-worker DiLoCo. Empirically, GPA consistently outperforms single-worker DiLoCo while removing the two-loop structure, simplifying hyperparameter tuning, and reducing its memory overhead to a single additional buffer. On the Llama-160M model, GPA provides a 24.22% speedup in terms of steps to reach the baseline (AdamW's) validation loss. Likewise, GPA achieves speedups of 12% and 27% on small and large batch setups, respectively, to attain AdamW's validation accuracy on the ImageNet ViT workload. Furthermore, we prove that for any base optimizer with regret bounded by $O(\sqrt{T})$, where $T$ is the number of iterations, GPA can match or exceed the convergence guarantee of the original optimizer, depending on the choice of interpolation constants.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distributed Learning in Markovian Restless Bandits over Interference Graphs for Stable Spectrum Sharing</title>
<link>https://arxiv.org/abs/2512.17161</link>
<guid>https://arxiv.org/abs/2512.17161</guid>
<content:encoded><![CDATA[
<div> Keywords: distributed learning, spectrum access, Gale-Shapley stability, restless bandits, interference graphs<br /><br />Summary:<br /><br />This paper addresses the problem of distributed spectrum access and sharing among multiple cognitive communication entities (called cells) in wireless networks constrained by interference modeled via graphs. The aim is to achieve a globally stable and interference-aware channel allocation defined by a generalized Gale-Shapley multi-to-one matching framework. The setting considers L cells sharing S orthogonal channels, where neighboring cells cannot use the same channel simultaneously. Each channel is modeled as an unknown restless Markov process with cell-specific rewards, making the problem stochastic and temporally varying. The authors propose SMILE (Stable Multi-matching with Interference-aware Learning), a novel distributed learning algorithm that combines restless bandit approaches with coordination constrained by the interference graph. SMILE is communication-efficient and balances exploration versus exploitation of channels in a distributed fashion. The paper proves that SMILE converges to the globally optimal stable channel allocation and achieves logarithmic regret compared to an oracle with full knowledge of expected utilities. Finally, simulations confirm the theoretical findings and demonstrate SMILE’s robustness, scalability, and efficiency in various spectrum-sharing scenarios. <div>
arXiv:2512.17161v1 Announce Type: new 
Abstract: We study distributed learning for spectrum access and sharing among multiple cognitive communication entities, such as cells, subnetworks, or cognitive radio users (collectively referred to as cells), in communication-constrained wireless networks modeled by interference graphs. Our goal is to achieve a globally stable and interference-aware channel allocation. Stability is defined through a generalized Gale-Shapley multi-to-one matching, a well-established solution concept in wireless resource allocation. We consider wireless networks where L cells share S orthogonal channels and cannot simultaneously use the same channel as their neighbors. Each channel evolves as an unknown restless Markov process with cell-dependent rewards, making this the first work to establish global Gale-Shapley stability for channel allocation in a stochastic, temporally varying restless environment. To address this challenge, we develop SMILE (Stable Multi-matching with Interference-aware LEarning), a communication-efficient distributed learning algorithm that integrates restless bandit learning with graph-constrained coordination. SMILE enables cells to distributedly balance exploration of unknown channels with exploitation of learned information. We prove that SMILE converges to the optimal stable allocation and achieves logarithmic regret relative to a genie with full knowledge of expected utilities. Simulations validate the theoretical guarantees and demonstrate SMILE's robustness, scalability, and efficiency across diverse spectrum-sharing scenarios.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BumpNet: A Sparse Neural Network Framework for Learning PDE Solutions</title>
<link>https://arxiv.org/abs/2512.17198</link>
<guid>https://arxiv.org/abs/2512.17198</guid>
<content:encoded><![CDATA[
<div> Keywords: BumpNet, PDE numerical solution, meshless basis functions, physics-informed neural networks, operator learning<br /><br />Summary:  
1. The article introduces BumpNet, a sparse neural network framework designed for solving partial differential equations (PDEs) and learning operators associated with PDEs.  
2. BumpNet utilizes meshless basis function expansions similar to radial-basis function (RBF) networks but uniquely constructs basis functions from ordinary sigmoid activations, enabling the use of advanced training techniques optimized for these functions.  
3. All parameters of each basis function—shape, location, and amplitude—are fully trainable, and the model enforces parsimony and h-adaptivity by dynamically pruning basis functions during training.  
4. BumpNet serves as a flexible framework that can be integrated with various neural architectures for PDE tasks, such as Bump-PINNs for general PDE solutions using collocation training, Bump-EDNNs for time-evolution PDEs combining BumpNet spatial representation with evolutionary deep neural networks for temporal advancement, and Bump-DeepONets incorporating BumpNet as the trunk network in Deep Operator Networks for operator learning.  
5. Extensive numerical experiments confirm that BumpNet-based architectures achieve significant improvements in both efficiency and accuracy compared to existing methods for PDE numerical solution and operator learning. <div>
arXiv:2512.17198v1 Announce Type: new 
Abstract: We introduce BumpNet, a sparse neural network framework for PDE numerical solution and operator learning. BumpNet is based on meshless basis function expansion, in a similar fashion to radial-basis function (RBF) networks. Unlike RBF networks, the basis functions in BumpNet are constructed from ordinary sigmoid activation functions. This enables the efficient use of modern training techniques optimized for such networks. All parameters of the basis functions, including shape, location, and amplitude, are fully trainable. Model parsimony and h-adaptivity are effectively achieved through dynamically pruning basis functions during training. BumpNet is a general framework that can be combined with existing neural architectures for learning PDE solutions: here, we propose Bump-PINNs (BumpNet with physics-informed neural networks) for solving general PDEs; Bump-EDNN (BumpNet with evolutionary deep neural networks) to solve time-evolution PDEs; and Bump-DeepONet (BumpNet with deep operator networks) for PDE operator learning. Bump-PINNs are trained using the same collocation-based approach used by PINNs, Bump-EDNN uses a BumpNet only in the spatial domain and uses EDNNs to advance the solution in time, while Bump-DeepONets employ a BumpNet regression network as the trunk network of a DeepONet. Extensive numerical experiments demonstrate the efficiency and accuracy of the proposed architecture.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning solution operator of dynamical systems with diffusion maps kernel ridge regression</title>
<link>https://arxiv.org/abs/2512.17203</link>
<guid>https://arxiv.org/abs/2512.17203</guid>
<content:encoded><![CDATA[
<div> Keywords: kernel ridge regression, diffusion maps, dynamical systems, long-term prediction, data-driven modeling  

<br /><br />Summary:  
This paper addresses the challenge of predicting complex nonlinear dynamical systems over long time horizons, where traditional data-driven models often struggle due to unknown or poorly represented geometric structures governing system behavior. The authors propose a novel method called Diffusion Maps Kernel Ridge Regression (DM-KRR), which integrates kernel ridge regression with a kernel derived from diffusion maps, capturing the intrinsic geometry of the system’s invariant set without requiring explicit manifold reconstruction or attractor modeling. A key innovation is the use of a dynamics-aware validation strategy that ensures model selection respects the underlying geometry of the data. Extensive experiments demonstrate that DM-KRR outperforms cutting-edge approaches, including random feature models, neural networks, and operator-learning methods, across a variety of systems such as smooth manifolds, chaotic attractors, and high-dimensional spatiotemporal flows. The study highlights that long-term predictive ability is not solely dependent on model complexity but significantly benefits from incorporating geometric constraints encoded in the data. Ultimately, the paper shows that a simple, geometry-informed model can deliver superior accuracy and data efficiency, pointing towards a promising direction for the reliable and efficient learning of complex dynamical systems. <div>
arXiv:2512.17203v1 Announce Type: new 
Abstract: Many scientific and engineering systems exhibit complex nonlinear dynamics that are difficult to predict accurately over long time horizons. Although data-driven models have shown promise, their performance often deteriorates when the geometric structures governing long-term behavior are unknown or poorly represented. We demonstrate that a simple kernel ridge regression (KRR) framework, when combined with a dynamics-aware validation strategy, provides a strong baseline for long-term prediction of complex dynamical systems. By employing a data-driven kernel derived from diffusion maps, the proposed Diffusion Maps Kernel Ridge Regression (DM-KRR) method implicitly adapts to the intrinsic geometry of the system's invariant set, without requiring explicit manifold reconstruction or attractor modeling, procedures that often limit predictive performance. Across a broad range of systems, including smooth manifolds, chaotic attractors, and high-dimensional spatiotemporal flows, DM-KRR consistently outperforms state-of-the-art random feature, neural-network and operator-learning methods in both accuracy and data efficiency. These findings underscore that long-term predictive skill depends not only on model expressiveness, but critically on respecting the geometric constraints encoded in the data through dynamically consistent model selection. Together, simplicity, geometry awareness, and strong empirical performance point to a promising path for reliable and efficient learning of complex dynamical systems.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Electric Vehicle Charging Load Forecasting: An Experimental Comparison of Machine Learning Methods</title>
<link>https://arxiv.org/abs/2512.17257</link>
<guid>https://arxiv.org/abs/2512.17257</guid>
<content:encoded><![CDATA[
<div> electric vehicles, charging demand, time series forecasting, urban datasets, spatial-temporal analysis  

<br /><br />Summary:  
This article addresses the emerging challenge of predicting electric vehicle (EV) charging demand, which is increasingly vital due to the rising adoption of EVs and their impact on electric grid management. The study systematically compares five time series forecasting models, including traditional statistical, machine learning, and deep learning approaches, to evaluate their effectiveness. Forecasts are examined across multiple temporal horizons—short-term (minutes), mid-term (hours), and long-term (days)—to assess model performance at different timescales. Additionally, the research investigates spatial aggregation levels from single charging stations to regional and city-wide aggregations, thereby capturing demand patterns at various scales. The evaluation is conducted on four publicly available, real-world datasets from diverse urban environments, allowing for a comprehensive and comparative analysis. The results are reported independently for each dataset, ensuring detailed insights into model robustness and applicability. Importantly, this study is the first to systematically assess EV charging demand forecasting across such a broad spectrum of temporal and spatial dimensions using multiple datasets, providing valuable guidance for both researchers and practitioners aiming to improve grid management and planning in the context of growing EV penetration. <div>
arXiv:2512.17257v1 Announce Type: new 
Abstract: With the growing popularity of electric vehicles as a means of addressing climate change, concerns have emerged regarding their impact on electric grid management. As a result, predicting EV charging demand has become a timely and important research problem. While substantial research has addressed energy load forecasting in transportation, relatively few studies systematically compare multiple forecasting methods across different temporal horizons and spatial aggregation levels in diverse urban settings. This work investigates the effectiveness of five time series forecasting models, ranging from traditional statistical approaches to machine learning and deep learning methods. Forecasting performance is evaluated for short-, mid-, and long-term horizons (on the order of minutes, hours, and days, respectively), and across spatial scales ranging from individual charging stations to regional and city-level aggregations. The analysis is conducted on four publicly available real-world datasets, with results reported independently for each dataset. To the best of our knowledge, this is the first work to systematically evaluate EV charging demand forecasting across such a wide range of temporal horizons and spatial aggregation levels using multiple real-world datasets.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SHARP-QoS: Sparsely-gated Hierarchical Adaptive Routing for joint Prediction of QoS</title>
<link>https://arxiv.org/abs/2512.17262</link>
<guid>https://arxiv.org/abs/2512.17262</guid>
<content:encoded><![CDATA[
<div> QoS prediction, service-oriented computing, hyperbolic convolution, multi-task learning, loss balancing<br /><br />Summary:<br /><br />1. The article addresses the challenge of predicting multiple Quality of Service (QoS) parameters simultaneously in service-oriented computing, where data is sparse, noisy, and influenced by hierarchical and contextual dependencies such as network and geographic factors.  
2. Existing methods commonly predict QoS parameters independently, leading to increased computational costs and poor generalization, while recent joint prediction approaches suffer from negative transfer due to inconsistent loss scaling and insufficient representation learning.  
3. The paper proposes SHARP-QoS, a unified strategy combining three core components: hyperbolic convolution in the Poincaré ball for capturing hierarchical QoS and contextual features; an adaptive feature-sharing mechanism with gated fusion to dynamically select and share informative features across QoS and context domains; and an EMA-based loss balancing approach to stabilize joint optimization and mitigate negative transfer.  
4. SHARP-QoS is evaluated on three datasets involving two to four QoS parameters, consistently outperforming single-task and multi-task baseline models in accuracy and robustness.  
5. The comprehensive analysis confirms SHARP-QoS effectively tackles challenges such as data sparsity, outliers, and cold-start scenarios while maintaining moderate computational overhead, making it a dependable solution for joint QoS prediction. <div>
arXiv:2512.17262v1 Announce Type: new 
Abstract: Dependable service-oriented computing relies on multiple Quality of Service (QoS) parameters that are essential to assess service optimality. However, real-world QoS data are extremely sparse, noisy, and shaped by hierarchical dependencies arising from QoS interactions, and geographical and network-level factors, making accurate QoS prediction challenging. Existing methods often predict each QoS parameter separately, requiring multiple similar models, which increases computational cost and leads to poor generalization. Although recent joint QoS prediction studies have explored shared architectures, they suffer from negative transfer due to loss-scaling caused by inconsistent numerical ranges across QoS parameters and further struggle with inadequate representation learning, resulting in degraded accuracy. This paper presents an unified strategy for joint QoS prediction, called SHARP-QoS, that addresses these issues using three components. First, we introduce a dual mechanism to extract the hierarchical features from both QoS and contextual structures via hyperbolic convolution formulated in the Poincar\'e ball. Second, we propose an adaptive feature-sharing mechanism that allows feature exchange across informative QoS and contextual signals. A gated feature fusion module is employed to support dynamic feature selection among structural and shared representations. Third, we design an EMA-based loss balancing strategy that allows stable joint optimization, thereby mitigating the negative transfer. Evaluations on three datasets with two, three, and four QoS parameters demonstrate that SHARP-QoS outperforms both single- and multi-task baselines. Extensive study shows that our model effectively addresses major challenges, including sparsity, robustness to outliers, and cold-start, while maintaining moderate computational overhead, underscoring its capability for reliable joint QoS prediction.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Theoretical Analysis of State Similarity Between Markov Decision Processes</title>
<link>https://arxiv.org/abs/2512.17265</link>
<guid>https://arxiv.org/abs/2512.17265</guid>
<content:encoded><![CDATA[
<div> bisimulation metric, Markov decision process, state similarity, reinforcement learning, policy transfer  

<br /><br />Summary:  
This paper introduces a generalized bisimulation metric (GBSM) for measuring state similarity between arbitrary pairs of Markov decision processes (MDPs). Unlike the traditional bisimulation metric (BSM), which focuses on state similarity within a single MDP, GBSM extends this concept to multiple MDPs, addressing prior challenges due to the lack of mathematical rigor in earlier attempts. The authors rigorously prove that GBSM satisfies three fundamental metric properties: symmetry, the inter-MDP triangle inequality, and a distance bound on identical state spaces. Leveraging these properties, the paper provides theoretical analysis and explicit bounds for key reinforcement learning tasks involving multiple MDPs, including policy transfer, state aggregation, and sampling-based estimation. These bounds are shown to be tighter than those derived from the standard BSM, thereby improving theoretical guarantees for multi-MDP scenarios. Furthermore, GBSM offers a closed-form sample complexity for estimation, enhancing practical utility compared to existing asymptotic analyses. Numerical experiments presented support and validate the theoretical contributions, demonstrating GBSM's effectiveness in improving state similarity evaluations and downstream RL tasks across multiple MDPs. This work thus represents a significant advancement in extending bisimulation concepts to broader, multi-environment reinforcement learning frameworks. <div>
arXiv:2512.17265v1 Announce Type: new 
Abstract: The bisimulation metric (BSM) is a powerful tool for analyzing state similarities within a Markov decision process (MDP), revealing that states closer in BSM have more similar optimal value functions. While BSM has been successfully utilized in reinforcement learning (RL) for tasks like state representation learning and policy exploration, its application to state similarity between multiple MDPs remains challenging. Prior work has attempted to extend BSM to pairs of MDPs, but a lack of well-established mathematical properties has limited further theoretical analysis between MDPs. In this work, we formally establish a generalized bisimulation metric (GBSM) for measuring state similarity between arbitrary pairs of MDPs, which is rigorously proven with three fundamental metric properties, i.e., GBSM symmetry, inter-MDP triangle inequality, and a distance bound on identical spaces. Leveraging these properties, we theoretically analyze policy transfer, state aggregation, and sampling-based estimation across MDPs, obtaining explicit bounds that are strictly tighter than existing ones derived from the standard BSM. Additionally, GBSM provides a closed-form sample complexity for estimation, improving upon existing asymptotic results based on BSM. Numerical results validate our theoretical findings and demonstrate the effectiveness of GBSM in multi-MDP scenarios.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Generalization in Role-Playing Models via Information Theory</title>
<link>https://arxiv.org/abs/2512.17270</link>
<guid>https://arxiv.org/abs/2512.17270</guid>
<content:encoded><![CDATA[
<div> Role-playing models, distribution shifts, R-EMID, reinforcement learning, generalization performance  

<br /><br />Summary:  
Role-playing models (RPMs) commonly applied in real-world scenarios tend to underperform when deployed outside their original training environments due to distribution shifts involving users, characters, and dialogue compositions. Existing evaluation methods such as LLM-as-a-judge are inadequate for diagnosing how these shifts specifically impact RPM generalization, and there is a lack of formal frameworks to characterize this degradation. To address these challenges, the paper introduces a novel information-theoretic metric called reasoning-based effective mutual information difference (R-EMID), which quantifies RPM performance degradation in an interpretable manner. The authors derive an upper bound on R-EMID to predict the worst-case generalization performance of RPMs and theoretically analyze how different types of shifts contribute to performance loss. Additionally, they propose a co-evolving reinforcement learning framework designed to model the interconnected dynamics among user, character, and dialogue context; this framework improves the estimation of dialogue response generation probabilities, a key component for calculating R-EMID. Empirical evaluation using R-EMID reveals that user shifts pose the most significant risk to RPM generalization, while reinforcement learning methods prove to be the most effective strategy for improving RPM robustness and performance across distributional changes. <div>
arXiv:2512.17270v1 Announce Type: new 
Abstract: Role-playing models (RPMs) are widely used in real-world applications but underperform when deployed in the wild. This degradation can be attributed to distribution shifts, including user, character, and dialogue compositional shifts. Existing methods like LLM-as-a-judge fall short in providing a fine-grained diagnosis of how these shifts affect RPM generalization, and thus there lack formal frameworks to characterize RPM generalization behaviors. To bridge these gaps, we introduce an information-theoretic metric, named reasoning-based effective mutual information difference (R-EMID), to measure RPM performance degradation in an interpretable way. We also derive an upper bound on R-EMID to predict the worst-case generalization performance of RPMs and theoretically reveal how various shifts contribute to the RPM performance degradation. Moreover, we propose a co-evolving reinforcement learning framework to adaptively model the connection among user, character, and dialogue context and thus enhance the estimation of dialogue response generation probability, which is critical for calculating R-EMID. Finally, we evaluate the generalization performance of various RPMs using R-EMID, finding that user shift poses the highest risk among all shifts and reinforcement learning is the most effective approach for enhancing RPM generalization.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MINPO: Memory-Informed Neural Pseudo-Operator to Resolve Nonlocal Spatiotemporal Dynamics</title>
<link>https://arxiv.org/abs/2512.17273</link>
<guid>https://arxiv.org/abs/2512.17273</guid>
<content:encoded><![CDATA[
<div> Keywords: integro-differential equations, nonlocal operators, neural networks, Kolmogorov-Arnold Networks, computational efficiency<br /><br />Summary:<br /><br />This paper addresses the computational challenges in solving integro-differential equations (IDEs) that describe nonlocal spatiotemporal physical systems, where classical methods suffer from high costs due to repeatedly evaluating convolution integrals. The authors propose a novel framework called Memory-Informed Neural Pseudo-Operator (MINPO) that unifies modeling nonlocal dynamics caused by long-range spatial interactions and temporal memory effects. MINPO employs neural network encoders—either Kolmogorov-Arnold Networks (KANs) or multilayer perceptrons (MLPs)—to learn both the nonlocal operator and its inverse directly, enabling explicit reconstruction of unknown solution fields. To ensure consistency between the learned operators and solutions, a lightweight nonlocal consistency loss term is introduced during training. This approach effectively captures and resolves a wide variety of IDEs and fractional partial differential equations (PDEs), naturally handling diverse kernel types and dimensionalities. The efficacy of MINPO is validated by comparison against classical numerical methods and recent neural-based approaches such as A-PINN, fPINN, and their KAN variants (A-PIKAN, fPIKAN), demonstrating improved accuracy and robustness. Additionally, MINPO manages the computational demands associated with repeated integral evaluations efficiently. Overall, MINPO offers a generalized, unified framework that extends beyond problem-specific solvers for nonlocal operator-driven systems. <div>
arXiv:2512.17273v1 Announce Type: new 
Abstract: Many physical systems exhibit nonlocal spatiotemporal behaviors described by integro-differential equations (IDEs). Classical methods for solving IDEs require repeatedly evaluating convolution integrals, whose cost increases quickly with kernel complexity and dimensionality. Existing neural solvers can accelerate selected instances of these computations, yet they do not generalize across diverse nonlocal structures. In this work, we introduce the Memory-Informed Neural Pseudo-Operator (MINPO), a unified framework for modeling nonlocal dynamics arising from long-range spatial interactions and/or long-term temporal memory. MINPO, employing either Kolmogorov-Arnold Networks (KANs) or multilayer perceptron networks (MLPs) as encoders, learns the nonlocal operator and its inverse directly through neural representations, and then explicitly reconstruct the unknown solution fields. The learning is guarded by a lightweight nonlocal consistency loss term to enforce coherence between the learned operator and reconstructed solution. The MINPO formulation allows to naturally capture and efficiently resolve nonlocal spatiotemporal dependencies governed by a wide spectrum of IDEs and their subsets, including fractional PDEs. We evaluate the efficacy of MINPO in comparison with classical techniques and state-of-the-art neural-based strategies based on MLPs, such as A-PINN and fPINN, along with their newly-developed KAN variants, A-PIKAN and fPIKAN, designed to facilitate a fair comparison. Our study offers compelling evidence of the accuracy of MINPO and demonstrates its robustness in handling (i) diverse kernel types, (ii) different kernel dimensionalities, and (iii) the substantial computational demands arising from repeated evaluations of kernel integrals. MINPO, thus, generalizes beyond problem-specific formulations, providing a unified framework for systems governed by nonlocal operators.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Alzheimer's Disease Brain Network Mining</title>
<link>https://arxiv.org/abs/2512.17276</link>
<guid>https://arxiv.org/abs/2512.17276</guid>
<content:encoded><![CDATA[
<div> Keywords: Alzheimer's disease, semi-supervised learning, neuroimaging, optimal transport, label propagation<br /><br />Summary:<br /><br />1. The study addresses the challenge of limited ground truth labels in Alzheimer's disease (AD) diagnosis due to expensive and invasive clinical assessments.  
2. It proposes MATCH-AD (Multi view Adaptive Transport Clustering for Heterogeneous Alzheimer's Disease), a semi-supervised framework combining deep representation learning, graph-based label propagation, and optimal transport theory.  
3. MATCH-AD leverages the manifold structure in neuroimaging data to effectively propagate labels from a small labeled set to a large unlabeled population.  
4. The framework uses Wasserstein distances to quantify disease progression across different cognitive states, enhancing diagnostic precision.  
5. Evaluated on nearly 5,000 subjects from the National Alzheimer's Coordinating Center, using structural MRI, cerebrospinal fluid biomarkers, and clinical variables, MATCH-AD achieves near-perfect diagnostic accuracy despite fewer than one-third of subjects being labeled.  
6. The method significantly outperforms baseline models, moving from weak agreement to almost perfect agreement, indicating a qualitative improvement in diagnostic reliability.  
7. MATCH-AD maintains clinical usefulness even with severe label scarcity, backed by theoretical guarantees on label propagation error and transport stability.  
8. The results demonstrate that principled semi-supervised learning can unlock diagnostic potential from vast partially annotated neuroimaging datasets, reducing annotation burdens while preserving clinical accuracy suitable for deployment. <div>
arXiv:2512.17276v1 Announce Type: new 
Abstract: Machine learning approaches for Alzheimer's disease (AD) diagnosis face a fundamental challenges. Clinical assessments are expensive and invasive, leaving ground truth labels available for only a fraction of neuroimaging datasets. We introduce Multi view Adaptive Transport Clustering for Heterogeneous Alzheimer's Disease (MATCH-AD), a semi supervised framework that integrates deep representation learning, graph-based label propagation, and optimal transport theory to address this limitation. The framework leverages manifold structure in neuroimaging data to propagate diagnostic information from limited labeled samples to larger unlabeled populations, while using Wasserstein distances to quantify disease progression between cognitive states. Evaluated on nearly five thousand subjects from the National Alzheimer's Coordinating Center, encompassing structural MRI measurements from hundreds of brain regions, cerebrospinal fluid biomarkers, and clinical variables MATCHAD achieves near-perfect diagnostic accuracy despite ground truth labels for less than one-third of subjects. The framework substantially outperforms all baseline methods, achieving kappa indicating almost perfect agreement compared to weak agreement for the best baseline, a qualitative transformation in diagnostic reliability. Performance remains clinically useful even under severe label scarcity, and we provide theoretical convergence guarantees with proven bounds on label propagation error and transport stability. These results demonstrate that principled semi-supervised learning can unlock the diagnostic potential of the vast repositories of partially annotated neuroimaging data accumulating worldwide, substantially reducing annotation burden while maintaining accuracy suitable for clinical deployment.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>M2RU: Memristive Minion Recurrent Unit for Continual Learning at the Edge</title>
<link>https://arxiv.org/abs/2512.17299</link>
<guid>https://arxiv.org/abs/2512.17299</guid>
<content:encoded><![CDATA[
<div> Continual learning, edge platforms, mixed-signal architecture, minion recurrent unit, energy efficiency<br /><br />Summary:<br /><br />This paper addresses the challenge of performing continual learning on edge platforms, where traditional recurrent networks are hindered by energy-intensive training and frequent data movement, making them unsuitable for embedded environments. It introduces M2RU, a novel mixed-signal architecture implementing the minion recurrent unit designed for efficient temporal processing with on-chip continual learning capabilities. The architecture incorporates weighted-bit streaming, allowing multi-bit digital inputs to be processed directly in crossbar arrays without requiring high-resolution conversions, reducing complexity and power usage. To stabilize learning amidst domain shifts, M2RU integrates an experience replay mechanism. Performance metrics demonstrate that M2RU achieves 15 GOPS while consuming only 48.62 mW, resulting in an energy efficiency of 312 GOPS per watt. On standard benchmarks such as sequential MNIST and CIFAR-10, M2RU retains accuracy within 5% of software baselines, showcasing its practical efficacy. When compared to a conventional CMOS digital design, this accelerator offers a 29-fold improvement in energy efficiency. Furthermore, device-aware analysis projects an operational lifetime of approximately 12.2 years under continual learning workloads. Collectively, these results position M2RU as a scalable, energy-efficient, and real-time adaptable solution for temporal intelligence in edge-level deployments. <div>
arXiv:2512.17299v1 Announce Type: new 
Abstract: Continual learning on edge platforms remains challenging because recurrent networks depend on energy-intensive training procedures and frequent data movement that are impractical for embedded deployments. This work introduces M2RU, a mixed-signal architecture that implements the minion recurrent unit for efficient temporal processing with on-chip continual learning. The architecture integrates weighted-bit streaming, which enables multi-bit digital inputs to be processed in crossbars without high-resolution conversion, and an experience replay mechanism that stabilizes learning under domain shifts. M2RU achieves 15 GOPS at 48.62 mW, corresponding to 312 GOPS per watt, and maintains accuracy within 5 percent of software baselines on sequential MNIST and CIFAR-10 tasks. Compared with a CMOS digital design, the accelerator provides 29X improvement in energy efficiency. Device-aware analysis shows an expected operational lifetime of 12.2 years under continual learning workloads. These results establish M2RU as a scalable and energy-efficient platform for real-time adaptation in edge-level temporal intelligence.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explanation Beyond Intuition: A Testable Criterion for Inherent Explainability</title>
<link>https://arxiv.org/abs/2512.17316</link>
<guid>https://arxiv.org/abs/2512.17316</guid>
<content:encoded><![CDATA[
<div> Keywords: inherent explainability, graph theory, structure-local explanation, Cox proportional hazards model, regulatory compliance  

<br /><br />Summary:  
This work addresses the lack of a consistent definition and test for inherent explainability in Explainable Artificial Intelligence (XAI). The authors propose a globally applicable criterion grounded in graph theory, which represents and decomposes models into structure-local explanations and then recomposes these into comprehensive global explanations. These structure-local explanations are formed as annotations, creating a verifiable hypothesis-evidence framework that supports various explanatory methods. The criterion aligns with existing intuitions about explainability and explains why some models, like large regression models, may not be inherently explainable, whereas sparse neural networks can be. A key distinction is made between models that are explainable (allow explanations) and those that are explained (have verified explanations). The paper demonstrates this by providing a full explanation of PREDICT, a Cox proportional hazards model used clinically in New Zealand for cardiovascular disease risk, thereby establishing that PREDICT is inherently explainable. This framework offers a structured approach to formalize explainability, which can assist regulators by providing a flexible yet rigorous test suitable for compliance and governance in AI deployment. <div>
arXiv:2512.17316v1 Announce Type: new 
Abstract: Inherent explainability is the gold standard in Explainable Artificial Intelligence (XAI). However, there is not a consistent definition or test to demonstrate inherent explainability. Work to date either characterises explainability through metrics, or appeals to intuition - "we know it when we see it". We propose a globally applicable criterion for inherent explainability. The criterion uses graph theory for representing and decomposing models for structure-local explanation, and recomposing them into global explanations. We form the structure-local explanations as annotations, a verifiable hypothesis-evidence structure that allows for a range of explanatory methods to be used. This criterion matches existing intuitions on inherent explainability, and provides justifications why a large regression model may not be explainable but a sparse neural network could be. We differentiate explainable -- a model that allows for explanation -- and \textit{explained} -- one that has a verified explanation. Finally, we provide a full explanation of PREDICT -- a Cox proportional hazards model of cardiovascular disease risk, which is in active clinical use in New Zealand. It follows that PREDICT is inherently explainable. This work provides structure to formalise other work on explainability, and allows regulators a flexible but rigorous test that can be used in compliance frameworks.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Task Schema and Binding: A Double Dissociation Study of In-Context Learning</title>
<link>https://arxiv.org/abs/2512.17325</link>
<guid>https://arxiv.org/abs/2512.17325</guid>
<content:encoded><![CDATA[
<div> Keywords: In-Context Learning, Task Schema, Binding Mechanism, Transformer Models, Activation Patching

<br /><br />Summary:  
This study causally validates that in-context learning (ICL) is driven by two distinct mechanisms: Task Schema (recognition of abstract task types) and Binding (associating specific inputs to outputs). First, through activation patching experiments on nine models spanning seven Transformer families plus a non-Transformer model (Mamba, 370M-13B parameters), the authors demonstrate a double dissociation: Task Schema transfers perfectly (100%) via late MLP patching, whereas Binding transfers at a lower rate (62%) via residual stream patching, confirming these mechanisms are separable. Second, there is a significant inverse correlation between the reliance on Task Schema and prior knowledge (Spearman rho = -0.596, p < 0.001), showing a prior-schema trade-off in model behavior. Third, this dual-mechanism framework applies broadly across all evaluated architectures including Mamba, signifying architectural generality. The findings challenge prior monolithic interpretations of ICL by delivering causal evidence for dual-process theories and revealing that models resort to Task Schema when prior knowledge is lacking. Furthermore, prior knowledge disrupts Binding through attentional mis-routing (72.7% recency bias), rather than output competition. This clarifies why arbitrary mappings succeed while overriding factual knowledge is difficult. Practically, recognizing these dual mechanisms can improve prompt engineering reliability by promoting schema transfer and addressing binding failures in scenarios with substantial prior knowledge. <div>
arXiv:2512.17325v1 Announce Type: new 
Abstract: We provide causal mechanistic validation that in-context learning (ICL) decomposes into two separable mechanisms: Task Schema (abstract task type recognition) and Binding (specific input-output associations). Through activation patching experiments across 9 models from 7 Transformer families plus Mamba (370M-13B parameters), we establish three key findings:
  1. Double dissociation: Task Schema transfers at 100% via late MLP patching; Binding transfers at 62% via residual stream patching -- proving separable mechanisms
  2. Prior-Schema trade-off: Schema reliance inversely correlates with prior knowledge (Spearman rho = -0.596, p < 0.001, N=28 task-model pairs)
  3. Architecture generality: The mechanism operates across all tested architectures including the non-Transformer Mamba
  These findings offer a mechanistic account of the ICL puzzle that contrasts with prior views treating ICL as a monolithic mechanism (whether retrieval-based, gradient descent-like, or purely Bayesian). By establishing that Schema and Binding are neurally dissociable -- not merely behavioral modes -- we provide causal evidence for dual-process theories of ICL. Models rely on Task Schema when prior knowledge is absent, but prior knowledge interferes through attentional mis-routing (72.7% recency bias) rather than direct output competition (0%). This explains why arbitrary mappings succeed (zero prior leads to full Schema reliance) while factual overrides fail -- and reveals that the true bottleneck is attentional, not output-level. Practical implications: Understanding these dual mechanisms enables more efficient prompt engineering -- reliable schema transfer reduces required demonstrations for novel tasks, while prior-aware design can mitigate the 38% binding failure rate in high-prior scenarios, improving ICL system reliability in production deployments.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Graph Pruning with Sudden-Events Evaluation for Traffic Prediction using Online Semi-Decentralized ST-GNNs</title>
<link>https://arxiv.org/abs/2512.17352</link>
<guid>https://arxiv.org/abs/2512.17352</guid>
<content:encoded><![CDATA[
<div> Spatio-Temporal Graph Neural Networks, adaptive pruning, communication overhead, Sudden Event Prediction Accuracy, traffic prediction<br /><br />Summary:<br /><br />1. This paper addresses the high communication overhead in deploying Spatio-Temporal Graph Neural Networks (ST-GNNs) across distributed cloudlets in smart mobility systems due to overlapping node feature transmissions.<br />2. The authors propose an adaptive pruning algorithm that dynamically filters redundant neighboring node features while retaining the most informative spatial context, adjusting pruning rates based on recent model performance.<br />3. The pruning method enables each cloudlet to focus computational resources on regions experiencing significant traffic changes without degrading prediction accuracy.<br />4. The paper introduces Sudden Event Prediction Accuracy (SEPA), a novel event-focused metric designed to measure responsiveness to rapid traffic slowdowns and recoveries, which conventional error metrics often fail to capture.<br />5. The evaluation is conducted in an online semi-decentralized setting using frameworks such as traditional federated learning, server-free federated learning, and Gossip Learning on two large-scale traffic datasets (PeMS-BAY and PeMSD7-M) over various prediction horizons.<br />6. Results demonstrate that SEPA effectively reveals the importance of spatial connectivity in predicting dynamic traffic events, while the adaptive pruning algorithm significantly reduces communication costs without compromising prediction accuracy or responsiveness.<br />7. The findings suggest that communication overhead in distributed ST-GNN deployments can be lowered substantially without sacrificing model performance, particularly in critical traffic event prediction. <div>
arXiv:2512.17352v1 Announce Type: new 
Abstract: Spatio-Temporal Graph Neural Networks (ST-GNNs) are well-suited for processing high-frequency data streams from geographically distributed sensors in smart mobility systems. However, their deployment at the edge across distributed compute nodes (cloudlets) createssubstantial communication overhead due to repeated transmission of overlapping node features between neighbouring cloudlets. To address this, we propose an adaptive pruning algorithm that dynamically filters redundant neighbour features while preserving the most informative spatial context for prediction. The algorithm adjusts pruning rates based on recent model performance, allowing each cloudlet to focus on regions experiencing traffic changes without compromising accuracy. Additionally, we introduce the Sudden Event Prediction Accuracy (SEPA), a novel event-focused metric designed to measure responsiveness to traffic slowdowns and recoveries, which are often missed by standard error metrics. We evaluate our approach in an online semi-decentralized setting with traditional FL, server-free FL, and Gossip Learning on two large-scale traffic datasets, PeMS-BAY and PeMSD7-M, across short-, mid-, and long-term prediction horizons. Experiments show that, in contrast to standard metrics, SEPA exposes the true value of spatial connectivity in predicting dynamic and irregular traffic. Our adaptive pruning algorithm maintains prediction accuracy while significantly lowering communication cost in all online semi-decentralized settings, demonstrating that communication can be reduced without compromising responsiveness to critical traffic events.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarially Robust Detection of Harmful Online Content: A Computational Design Science Approach</title>
<link>https://arxiv.org/abs/2512.17367</link>
<guid>https://arxiv.org/abs/2512.17367</guid>
<content:encoded><![CDATA[
<div> Keywords: adversarial robustness, harmful content detection, large language models, ensemble learning, adversarial training  

<br /><br />Summary:  
This study addresses the challenge of detecting harmful content such as hate speech, misinformation, and extremist rhetoric on social media platforms, particularly under adversarial attacks where malicious users subtly modify text to evade detection. It introduces a novel framework named Large Language Model-based Sample Generation and Aggregation (LLM-SGA) that identifies key invariances in textual adversarial attacks to enhance the generalizability of detectors. Within this framework, the authors develop the Adversarially Robust Harmful Online Content Detector (ARHOCD), which incorporates three innovative components to boost detection accuracy: (1) an ensemble of multiple base detectors leveraging their complementary strengths; (2) a dynamic weight assignment method that adjusts weights based on sample predictability and base detector capability, initialized with domain knowledge and refined via Bayesian inference; and (3) an adversarial training strategy that iteratively optimizes both base detectors and weight assignor for continual robustness enhancement. The study overcomes several limitations found in prior research on adversarial robustness. Empirical evaluations carried out on three diverse datasets covering hate speech, rumors, and extremist content demonstrate that ARHOCD achieves strong generalizability and significantly improved detection accuracy under various adversarial conditions. <div>
arXiv:2512.17367v1 Announce Type: new 
Abstract: Social media platforms are plagued by harmful content such as hate speech, misinformation, and extremist rhetoric. Machine learning (ML) models are widely adopted to detect such content; however, they remain highly vulnerable to adversarial attacks, wherein malicious users subtly modify text to evade detection. Enhancing adversarial robustness is therefore essential, requiring detectors that can defend against diverse attacks (generalizability) while maintaining high overall accuracy. However, simultaneously achieving both optimal generalizability and accuracy is challenging. Following the computational design science paradigm, this study takes a sequential approach that first proposes a novel framework (Large Language Model-based Sample Generation and Aggregation, LLM-SGA) by identifying the key invariances of textual adversarial attacks and leveraging them to ensure that a detector instantiated within the framework has strong generalizability. Second, we instantiate our detector (Adversarially Robust Harmful Online Content Detector, ARHOCD) with three novel design components to improve detection accuracy: (1) an ensemble of multiple base detectors that exploits their complementary strengths; (2) a novel weight assignment method that dynamically adjusts weights based on each sample's predictability and each base detector's capability, with weights initialized using domain knowledge and updated via Bayesian inference; and (3) a novel adversarial training strategy that iteratively optimizes both the base detectors and the weight assignor. We addressed several limitations of existing adversarial robustness enhancement research and empirically evaluated ARHOCD across three datasets spanning hate speech, rumor, and extremist content. Results show that ARHOCD offers strong generalizability and improves detection accuracy under adversarial conditions.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdvJudge-Zero: Binary Decision Flips in LLM-as-a-Judge via Adversarial Control Tokens</title>
<link>https://arxiv.org/abs/2512.17375</link>
<guid>https://arxiv.org/abs/2512.17375</guid>
<content:encoded><![CDATA[
<div> Keywords: Reward models, LLM-as-a-Judge, adversarial tokens, reward hacking, adversarial training<br /><br />Summary:<br /><br />This paper investigates a vulnerability in reward models and LLM-as-a-Judge systems that are widely used in model fine-tuning methods like RLHF, DPO, and RLAIF. These judge systems provide scalar feedback and binary decisions crucial for guiding model selection and reinforcement learning. The authors identify that short sequences of low-perplexity control tokens can manipulate these judges by flipping judgments from correct "No" to incorrect "Yes," exploiting the last-layer logit gaps. Unlike worst-case adversarial attacks, these control tokens are plausible outputs from policy models during post-training, representing a realistic reward-hacking threat. To find such tokens, the authors develop AdvJudge-Zero, an approach leveraging next-token distributions and beam search to discover diverse control sequences from scratch. Their analysis reveals that the induced hidden-state perturbations concentrate in a low-rank "soft mode," which is anti-aligned with the judge's refusal direction. Empirical tests show these tokens cause significantly increased false positive rates when judging incorrect answers, especially in large open-weight and specialized judge models on math and reasoning benchmarks. Lastly, the paper demonstrates that LoRA-based adversarial training using small sets of control-token-augmented examples can greatly reduce false positives while maintaining overall evaluation quality. <div>
arXiv:2512.17375v1 Announce Type: new 
Abstract: Reward models and LLM-as-a-Judge systems are central to modern post-training pipelines such as RLHF, DPO, and RLAIF, where they provide scalar feedback and binary decisions that guide model selection and RL-based fine-tuning. We show that these judge systems exhibit a recurring vulnerability: short sequences of low-perplexity control tokens can flip many binary evaluations from correct ``No'' judgments to incorrect ``Yes'' judgments by steering the last-layer logit gap. These control tokens are patterns that a policy model could plausibly generate during post-training, and thus represent realistic reward-hacking risks rather than worst-case adversarial strings. Our method, AdvJudge-Zero, uses the model's next-token distribution and beam-search exploration to discover diverse control-token sequences from scratch, and our analysis shows that the induced hidden-state perturbations concentrate in a low-rank ``soft mode'' that is anti-aligned with the judge's refusal direction. Empirically, these tokens cause very high false positive rates when large open-weight and specialized judge models score incorrect answers on math and reasoning benchmarks. Finally, we show that LoRA-based adversarial training on small sets of control-token-augmented examples can markedly reduce these false positives while preserving evaluation quality.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepShare: Sharing ReLU Across Channels and Layers for Efficient Private Inference</title>
<link>https://arxiv.org/abs/2512.17398</link>
<guid>https://arxiv.org/abs/2512.17398</guid>
<content:encoded><![CDATA[
<div> Private Inference, DReLU, ReLU optimization, prototype channels, image segmentation<br /><br />Summary:<br /><br />1. The paper addresses Private Inference (PI), a technique that uses cryptography to enable privacy-preserving machine learning, allowing neural network inference on client data without revealing the data or the model.  
2. A key computational challenge in PI is evaluating non-linear activation functions, especially ReLU gates, which are computationally expensive.  
3. The authors focus on the derivative ReLU (DReLU), a non-linear step function closely related to ReLU, and demonstrate that a single DReLU operation can support multiple ReLU activations.  
4. They propose a novel activation module where DReLU is computed only on a subset of channels called prototype channels, while the other replicate channels copy the DReLU outputs from their corresponding prototype neurons. This design significantly reduces the number of DReLU computations.  
5. The concept is further extended across multiple layers to enhance efficiency in ResNet-like architectures.  
6. Theoretical analysis shows the approach can solve an extended XOR problem with only one non-linearity and two neurons, outperforming traditional methods and some PI-specific techniques.  
7. Experimental results demonstrate state-of-the-art (SOTA) performance on several classification tasks and image segmentation benchmarks, showcasing both computational efficiency and accuracy benefits. <div>
arXiv:2512.17398v1 Announce Type: new 
Abstract: Private Inference (PI) uses cryptographic primitives to perform privacy preserving machine learning. In this setting, the owner of the network runs inference on the data of the client without learning anything about the data and without revealing any information about the model. It has been observed that a major computational bottleneck of PI is the calculation of the gate (i.e., ReLU), so a considerable amount of effort have been devoted to reducing the number of ReLUs in a given network.
  We focus on the DReLU, which is the non-linear step function of the ReLU and show that one DReLU can serve many ReLU operations. We suggest a new activation module where the DReLU operation is only performed on a subset of the channels (Prototype channels), while the rest of the channels (replicate channels) replicates the DReLU of each of their neurons from the corresponding neurons in one of the prototype channels. We then extend this idea to work across different layers.
  We show that this formulation can drastically reduce the number of DReLU operations in resnet type network. Furthermore, our theoretical analysis shows that this new formulation can solve an extended version of the XOR problem, using just one non-linearity and two neurons, something that traditional formulations and some PI specific methods cannot achieve. We achieve new SOTA results on several classification setups, and achieve SOTA results on image segmentation.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>meval: A Statistical Toolbox for Fine-Grained Model Performance Analysis</title>
<link>https://arxiv.org/abs/2512.17409</link>
<guid>https://arxiv.org/abs/2512.17409</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, performance metrics, subgroup analysis, medical imaging, statistical toolbox  

<br /><br />Summary:  
1. This article addresses the growing practice of analyzing machine learning model performance by patient and recording properties to uncover important model failure modes.  
2. It highlights the statistical challenges involved, including selecting appropriate performance metrics for valid comparisons across groups with varying sample sizes and base rates, estimating metric uncertainty, and correcting for multiple comparisons to avoid chance findings.  
3. The paper emphasizes the complexity of intersectional analyses, where numerous subgroup combinations require methods to identify the most statistically significant or “interesting” subgroups.  
4. To tackle these issues, the authors introduce a comprehensive statistical toolbox designed to enable practitioners to rigorously and easily evaluate potential subgroup performance disparities in machine learning models.  
5. Although generally applicable, the toolbox is tailored for medical imaging contexts, as demonstrated through two case studies: skin lesion malignancy classification using the ISIC2020 dataset and chest X-ray-based disease classification with the MIMIC-CXR dataset.  
6. These case studies illustrate the practical utility of the toolbox in detecting subgroup performance issues that may otherwise be overlooked, thereby enhancing the robustness and fairness of medical imaging AI models. <div>
arXiv:2512.17409v1 Announce Type: new 
Abstract: Analyzing machine learning model performance stratified by patient and recording properties is becoming the accepted norm and often yields crucial insights about important model failure modes. Performing such analyses in a statistically rigorous manner is non-trivial, however. Appropriate performance metrics must be selected that allow for valid comparisons between groups of different sample sizes and base rates; metric uncertainty must be determined and multiple comparisons be corrected for, in order to assess whether any observed differences may be purely due to chance; and in the case of intersectional analyses, mechanisms must be implemented to find the most `interesting' subgroups within combinatorially many subgroup combinations. We here present a statistical toolbox that addresses these challenges and enables practitioners to easily yet rigorously assess their models for potential subgroup performance disparities. While broadly applicable, the toolbox is specifically designed for medical imaging applications. The analyses provided by the toolbox are illustrated in two case studies, one in skin lesion malignancy classification on the ISIC2020 dataset and one in chest X-ray-based disease classification on the MIMIC-CXR dataset.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing Long-Term Electricity Market Design for Ambitious Decarbonization Targets using Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.17444</link>
<guid>https://arxiv.org/abs/2512.17444</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent reinforcement learning, electricity market, decarbonization, market design, policy mechanisms<br /><br />Summary:<br /><br />1. The article addresses the importance of long-term electricity market mechanisms such as auctions and support schemes in steering the electricity generation mix toward a carbon-free economy.<br /><br />2. It introduces a novel multi-agent reinforcement learning (MARL) model that simulates profit-maximizing generation companies making investment decisions within a decentralized, competitive wholesale electricity market.<br /><br />3. The model utilizes independent proximal policy optimization, chosen for its suitability in decentralized, multi-agent environments, and employs extensive hyperparameter tuning to ensure realistic competitive market outcomes despite the challenges of independent learning.<br /><br />4. The framework is tested on a stylized representation of the Italian electricity system under varying conditions of market competition, design, and policy scenarios.<br /><br />5. Results demonstrate the critical influence of market design on successful decarbonization and reduction of price volatility, highlighting the framework’s ability to simultaneously evaluate complex interactions among multiple policy and market mechanisms as participants adapt to decarbonization pathways. <div>
arXiv:2512.17444v1 Announce Type: new 
Abstract: Electricity systems are key to transforming today's society into a carbon-free economy. Long-term electricity market mechanisms, including auctions, support schemes, and other policy instruments, are critical in shaping the electricity generation mix. In light of the need for more advanced tools to support policymakers and other stakeholders in designing, testing, and evaluating long-term markets, this work presents a multi-agent reinforcement learning model capable of capturing the key features of decarbonizing energy systems. Profit-maximizing generation companies make investment decisions in the wholesale electricity market, responding to system needs, competitive dynamics, and policy signals. The model employs independent proximal policy optimization, which was selected for suitability to the decentralized and competitive environment. Nevertheless, given the inherent challenges of independent learning in multi-agent settings, an extensive hyperparameter search ensures that decentralized training yields market outcomes consistent with competitive behavior. The model is applied to a stylized version of the Italian electricity system and tested under varying levels of competition, market designs, and policy scenarios. Results highlight the critical role of market design for decarbonizing the electricity sector and avoiding price volatility. The proposed framework allows assessing long-term electricity markets in which multiple policy and market mechanisms interact simultaneously, with market participants responding and adapting to decarbonization pathways.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning What to Write: Write-Gated KV for Efficient Long-Context Inference</title>
<link>https://arxiv.org/abs/2512.17452</link>
<guid>https://arxiv.org/abs/2512.17452</guid>
<content:encoded><![CDATA[
<div> Keywords: Long-context LLM, KV cache management, Write-Gated KV, memory efficiency, inference speedup<br /><br />Summary: This paper addresses the inefficiency in long-context large language model (LLM) inference caused by quadratic attention complexity and linear growth of the key-value (KV) cache. Traditional methods mitigate these challenges by post-hoc selection or eviction but fail to tackle the core issue of indiscriminate caching of tokens. The authors formalize KV cache management as a causal system involving three key operations: KV Admission, Selection, and Eviction. They propose a novel mechanism called Write-Gated KV to implement KV Admission, which learns to predict the utility of tokens before writing them into the cache. By filtering out low-utility tokens early, the approach maintains a compact global cache alongside a sliding local cache, thereby significantly reducing memory usage by 46-57%. This methodology results in substantial speedups, achieving 3.03-3.45× faster prefill times and 1.89-2.56× faster decoding speeds on the Llama model with minimal impact on accuracy. Additionally, Write-Gated KV integrates smoothly with existing technologies like FlashAttention and paged-KV systems. The findings demonstrate that selectively writing tokens to cache is an effective and practical formula for enhancing efficiency in long-context LLM inference. The authors have also made their code publicly available for further research and application. <div>
arXiv:2512.17452v1 Announce Type: new 
Abstract: Long-context LLM inference is bottlenecked by the quadratic attention complexity and linear KV cache growth. Prior approaches mitigate this via post-hoc selection or eviction but overlook the root inefficiency: indiscriminate writing to persistent memory. In this paper, we formalize KV cache management as a causal system of three primitives: KV Admission, Selection, and Eviction. We instantiate KV Admission via Write-Gated KV, a lightweight mechanism that learns to predict token utility before it enters the cache. By filtering out low-utility states early to maintain a compact global cache alongside a sliding local cache, Write-Gated KV reduces memory usage by 46-57% and delivers 3.03-3.45$\times$ prefill and 1.89-2.56$\times$ decode speedups on Llama model with negligible accuracy loss, all while remaining compatible with FlashAttention and paged-KV systems. These results demonstrate that learning what to write, is a principled and practical recipe for efficient long-context inference. Code is available at https://github.com/EMCLab-Sinica/WG-KV .
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A lightweight Spatial-Temporal Graph Neural Network for Long-term Time Series Forecasting</title>
<link>https://arxiv.org/abs/2512.17453</link>
<guid>https://arxiv.org/abs/2512.17453</guid>
<content:encoded><![CDATA[
<div> Lite-STGNN, spatial-temporal graph neural network, long-term forecasting, sparse graph structure, trend-seasonal decomposition<br /><br />Summary:  
The paper introduces Lite-STGNN, a lightweight spatial-temporal graph neural network designed for long-term multivariate time series forecasting. It effectively integrates decomposition-based temporal modeling with a learnable sparse graph structure to improve performance. The temporal module implements trend-seasonal decomposition, separating time series patterns into trend and seasonal components to enhance forecasting accuracy. The spatial module employs message passing along with low-rank Top-K adjacency learning and conservative horizon-wise gating, which allow spatial corrections that complement a strong linear baseline model. Lite-STGNN achieves state-of-the-art accuracy on four benchmark datasets, forecasting up to 720 steps into the future. It is parameter-efficient and trains substantially faster compared to transformer-based methods, making it computationally attractive. Ablation studies highlight the contribution of the spatial module, which yields a 4.6% accuracy improvement over a purely temporal baseline. Additionally, the Top-K adjacency learning improves locality by 3.3%, ensuring the model focuses on the most relevant spatial connections. Furthermore, the learned adjacency matrices reveal meaningful domain-specific interaction dynamics, enhancing interpretability. Overall, Lite-STGNN provides a compact, interpretable, and efficient framework ideally suited for long-term multivariate time series forecasting tasks. <div>
arXiv:2512.17453v1 Announce Type: new 
Abstract: We propose Lite-STGNN, a lightweight spatial-temporal graph neural network for long-term multivariate forecasting that integrates decomposition-based temporal modeling with learnable sparse graph structure. The temporal module applies trend-seasonal decomposition, while the spatial module performs message passing with low-rank Top-$K$ adjacency learning and conservative horizon-wise gating, enabling spatial corrections that enhance a strong linear baseline. Lite-STGNN achieves state-of-the-art accuracy on four benchmark datasets for horizons up to 720 steps, while being parameter-efficient and substantially faster to train than transformer-based methods. Ablation studies show that the spatial module yields 4.6% improvement over the temporal baseline, Top-$K$ enhances locality by 3.3%, and learned adjacency matrices reveal domain-specific interaction dynamics. Lite-STGNN thus offers a compact, interpretable, and efficient framework for long-term multivariate time series forecasting.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning-Based Surrogate Creep Modelling in Inconel 625: A High-Temperature Alloy Study</title>
<link>https://arxiv.org/abs/2512.17477</link>
<guid>https://arxiv.org/abs/2512.17477</guid>
<content:encoded><![CDATA[
<div> Keywords: Inconel 625, creep deformation, deep learning surrogate models, BiLSTM Variational Autoencoder, BiLSTM Transformer<br /><br />Summary:<br /><br />1. This study addresses the computational challenges of simulating time-dependent creep deformation in high-temperature alloys, focusing on Inconel 625, which is critical for aerospace and energy system components. <br /><br />2. Traditional finite-element creep simulations using software like ANSYS are accurate but computationally expensive, often needing 30 to 40 minutes to simulate a 10,000-hour creep event under varying stress and temperature conditions.<br /><br />3. To overcome these limitations, the authors develop two deep learning-based surrogate models trained on temporal creep strain data generated in ANSYS using the Norton creep law under uniaxial stresses (50–150 MPa) and temperatures (700–1000 °C).<br /><br />4. The first model, a BiLSTM Variational Autoencoder (VAE), provides uncertainty-aware and generative predictions, offering probabilistic outputs suitable for reliable forecasting.<br /><br />5. The second model, a BiLSTM Transformer hybrid, uses self-attention mechanisms to capture long-range temporal dependencies and delivers highly accurate deterministic predictions.<br /><br />6. Evaluation metrics, including RMSE, MAE, and R², confirm that the BiLSTM-VAE ensures stability and robustness while the BiLSTM-Transformer achieves superior accuracy throughout the simulation period.<br /><br />7. Latency testing reveals that the surrogate models reduce processing time from tens of minutes to mere seconds, enabling rapid creep assessment.<br /><br />8. This framework facilitates design optimization and structural health monitoring in high-temperature alloy applications by providing scalable, fast, and reliable creep deformation predictions. <div>
arXiv:2512.17477v1 Announce Type: new 
Abstract: Time-dependent deformation, particularly creep, in high-temperature alloys such as Inconel 625 is a key factor in the long-term reliability of components used in aerospace and energy systems. Although Inconel 625 shows excellent creep resistance, finite-element creep simulations in tools such as ANSYS remain computationally expensive, often requiring tens of minutes for a single 10,000-hour run. This work proposes deep learning based surrogate models to provide fast and accurate replacements for such simulations. Creep strain data was generated in ANSYS using the Norton law under uniaxial stresses of 50 to 150 MPa and temperatures of 700 to 1000 $^\circ$C, and this temporal dataset was used to train two architectures: a BiLSTM Variational Autoencoder for uncertainty-aware and generative predictions, and a BiLSTM Transformer hybrid that employs self-attention to capture long-range temporal behavior. Both models act as surrogate predictors, with the BiLSTM-VAE offering probabilistic output and the BiLSTM-Transformer delivering high deterministic accuracy. Performance is evaluated using RMSE, MAE, and $R^2$. Results show that the BiLSTM-VAE provides stable and reliable creep strain forecasts, while the BiLSTM-Transformer achieves strong accuracy across the full time range. Latency tests indicate substantial speedup: while each ANSYS simulation requires 30 to 40 minutes for a given stress-temperature condition, the surrogate models produce predictions within seconds. The proposed framework enables rapid creep assessment for design optimization and structural health monitoring, and provides a scalable solution for high-temperature alloy applications.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SafeBench-Seq: A Homology-Clustered, CPU-Only Baseline for Protein Hazard Screening with Physicochemical/Composition Features and Cluster-Aware Confidence Intervals</title>
<link>https://arxiv.org/abs/2512.17527</link>
<guid>https://arxiv.org/abs/2512.17527</guid>
<content:encoded><![CDATA[
<div> protein design, biosecurity, sequence screening, homology clustering, calibrated models<br /><br />Summary: This work introduces SafeBench-Seq, a reproducible benchmark and baseline classifier for screening protein sequences for biosecurity hazards. It addresses the lack of simple, reproducible sequence-level hazard screening methods that are explicitly evaluated under homology control and can run efficiently on commodity CPUs. SafeBench-Seq is constructed entirely from public datasets, combining SafeProtein hazard annotations and UniProt benign sequences, and uses interpretable global physicochemical descriptors and amino acid composition features. To simulate never-before-seen threats, the dataset undergoes homology clustering at 40% sequence identity, with cluster-level holdouts ensuring no cluster overlap between training and testing sets. The evaluation reports standard discrimination metrics such as AUROC and AUPRC, along with screening operating points (TPR@1% FPR, FPR@95% TPR), each with 95% bootstrap confidence intervals from 200 replicates. Calibrated probability estimates are provided using CalibratedClassifierCV, applying isotonic regression for logistic regression and random forests, and Platt scaling for linear SVMs. The model’s probability quality is assessed via Brier score, Expected Calibration Error with 15 bins, and reliability diagrams. Robustness is tested against shortcuts by shuffling residues while preserving composition and through ablations based on length and composition features. Results show that random splits overestimate performance versus homology-controlled splits, calibrated linear models demonstrate superior calibration, and tree ensembles have slightly worse calibration errors. SafeBench-Seq releases only metadata (accessions, cluster IDs, split labels) to enable rigorous, CPU-only, reproducible hazard screening evaluations without distributing dangerous sequences. <div>
arXiv:2512.17527v1 Announce Type: new 
Abstract: Foundation models for protein design raise concrete biosecurity risks, yet the community lacks a simple, reproducible baseline for sequence-level hazard screening that is explicitly evaluated under homology control and runs on commodity CPUs. We introduce SafeBench-Seq, a metadata-only, reproducible benchmark and baseline classifier built entirely from public data (SafeProtein hazards and UniProt benigns) and interpretable features (global physicochemical descriptors and amino-acid composition). To approximate "never-before-seen" threats, we homology-cluster the combined dataset at <=40% identity and perform cluster-level holdouts (no cluster overlap between train/test). We report discrimination (AUROC/AUPRC) and screening-operating points (TPR@1% FPR; FPR@95% TPR) with 95% bootstrap confidence intervals (n=200), and we provide calibrated probabilities via CalibratedClassifierCV (isotonic for Logistic Regression / Random Forest; Platt sigmoid for Linear SVM). We quantify probability quality using Brier score, Expected Calibration Error (ECE; 15 bins), and reliability diagrams. Shortcut susceptibility is probed via composition-preserving residue shuffles and length-/composition-only ablations. Empirically, random splits substantially overestimate robustness relative to homology-clustered evaluation; calibrated linear models exhibit comparatively good calibration, while tree ensembles retain slightly higher Brier/ECE. SafeBench-Seq is CPU-only, reproducible, and releases metadata only (accessions, cluster IDs, split labels), enabling rigorous evaluation without distributing hazardous sequences.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NetworkFF: Unified Layer Optimization in Forward-Only Neural Networks</title>
<link>https://arxiv.org/abs/2512.17531</link>
<guid>https://arxiv.org/abs/2512.17531</guid>
<content:encoded><![CDATA[
<div> Forward-Forward algorithm, inter-layer collaboration, Collaborative Forward-Forward (CFF), adaptive learning, neuromorphic computing  

<br /><br />Summary:  
The paper addresses limitations in the Forward-Forward (FF) learning algorithm, which eliminates backpropagation's biological implausibility and memory issues by processing positive and negative data through dual forward passes. Conventional FF implementations isolate layers, preventing collaborative learning across the network and reducing convergence efficiency, especially in deep architectures. To overcome this, the authors propose Collaborative Forward-Forward (CFF) learning, which maintains forward-only computation while enabling inter-layer cooperation and global context integration. Two variants are introduced: Fixed CFF (F-CFF), employing constant inter-layer coupling, and Adaptive CFF (A-CFF), which uses learnable parameters that evolve with training to modulate collaboration strength. The framework's collaborative goodness function aggregates weighted contributions from all layers, fostering coordinated feature representation. Evaluations on MNIST and Fashion-MNIST datasets reveal that both CFF variants outperform the baseline FF approach, indicating enhanced learning dynamics and better convergence. The study highlights inter-layer collaboration as a crucial advancement for FF learning, with promising applications in neuromorphic hardware and AI systems where energy efficiency and memory constraints are critical. This work paves the way for more biologically plausible and scalable learning algorithms suitable for next-generation computing platforms. <div>
arXiv:2512.17531v1 Announce Type: new 
Abstract: The Forward-Forward algorithm eliminates backpropagation's memory constraints and biological implausibility through dual forward passes with positive and negative data. However, conventional implementations suffer from critical inter-layer isolation, where layers optimize goodness functions independently without leveraging collective learning dynamics. This isolation constrains representational coordination and limits convergence efficiency in deeper architectures. This paper introduces Collaborative Forward-Forward (CFF) learning, extending the original algorithm through inter-layer cooperation mechanisms that preserve forward-only computation while enabling global context integration. Our framework implements two collaborative paradigms: Fixed CFF (F-CFF) with constant inter-layer coupling and Adaptive CFF (A-CFF) with learnable collaboration parameters that evolve during training. The collaborative goodness function incorporates weighted contributions from all layers, enabling coordinated feature learning while maintaining memory efficiency and biological plausibility. Comprehensive evaluation on MNIST and Fashion-MNIST demonstrates significant performance improvements over baseline Forward-Forward implementations. These findings establish inter-layer collaboration as a fundamental enhancement to Forward-Forward learning, with immediate applicability to neuromorphic computing architectures and energy-constrained AI systems.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian Optimisation: Which Constraints Matter?</title>
<link>https://arxiv.org/abs/2512.17569</link>
<guid>https://arxiv.org/abs/2512.17569</guid>
<content:encoded><![CDATA[
<div> Bayesian optimisation, Knowledge Gradient, decoupled constraints, black-box optimisation, constraint evaluation<br /><br />Summary:<br /><br />1. The paper introduces new variants of the Knowledge Gradient acquisition function tailored for Bayesian optimisation problems with decoupled black-box constraints, where objective and constraint functions can be evaluated independently.<br /><br />2. These variants are designed to efficiently handle settings where only a subset of constraints are binding at the optimum, which allows selective evaluation of relevant constraints rather than all constraints at every step.<br /><br />3. By doing so, the methods reduce the computational cost associated with evaluating unnecessary constraints during optimisation.<br /><br />4. The proposed algorithms are empirically benchmarked against existing state-of-the-art methods for constrained Bayesian optimisation.<br /><br />5. Experimental results show that the new approaches outperform prior methods, demonstrating superior optimisation performance and efficiency on problems with decoupled constraint structures. <div>
arXiv:2512.17569v1 Announce Type: new 
Abstract: Bayesian optimisation has proven to be a powerful tool for expensive global black-box optimisation problems. In this paper, we propose new Bayesian optimisation variants of the popular Knowledge Gradient acquisition functions for problems with \emph{decoupled} black-box constraints, in which subsets of the objective and constraint functions may be evaluated independently. In particular, our methods aim to take into account that often only a handful of the constraints may be binding at the optimum, and hence we should evaluate only relevant constraints when trying to optimise a function. We empirically benchmark these methods against existing methods and demonstrate their superiority over the state-of-the-art.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GreedySnake: Accelerating SSD-Offloaded LLM Training with Efficient Scheduling and Optimizer Step Overlapping</title>
<link>https://arxiv.org/abs/2512.17570</link>
<guid>https://arxiv.org/abs/2512.17570</guid>
<content:encoded><![CDATA[
<div> SSD-offloaded training, vertical scheduling, GreedySnake, training throughput, optimization overlap<br /><br />Summary:<br /><br />This paper presents GreedySnake, a novel SSD-offloaded training system designed to improve the cost-effectiveness of large language model (LLM) training. Unlike existing systems that rely on horizontal scheduling—executing micro-batches sequentially—GreedySnake employs vertical scheduling, executing all micro-batches of a specific layer before moving on to the next. This approach boosts training throughput even with smaller batch sizes, aligning performance closer to the ideal predicted by the roofline model. To address the I/O bottleneck commonly encountered during SSD-offloaded training, GreedySnake overlaps parts of the optimization step with the forward pass of the subsequent iteration, further enhancing efficiency. Experimental evaluations on NVIDIA A100 GPUs demonstrate significant throughput improvements compared to ZeRO-Infinity: nearly 2x on single and 4 GPU setups for GPT-65B models and over 2.5x on a single GPU for GPT-175B. These results indicate GreedySnake’s capability to accelerate SSD-offloaded training substantially while maintaining system responsiveness. The authors have open-sourced GreedySnake at the provided GitHub repository, encouraging adoption and further research in the community. <div>
arXiv:2512.17570v1 Announce Type: new 
Abstract: SSD-offloaded training offers a practical and promising approach to making LLM training cost-effective. Building on gradient accumulation with micro-batches, this paper introduces GreedySnake, a new SSD-offloaded training system that employs vertical scheduling, which executes all microbatches of a layer before proceeding to the next. Compared to existing systems that use horizontal scheduling (i.e., executing micro-batches sequentially), GreedySnake achieves higher training throughput with smaller batch sizes, bringing the system much closer to the ideal scenario predicted by the roofline model. To further mitigate the I/O bottleneck, GreedySnake overlaps part of the optimization step with the forward pass of the next iteration. Experimental results on A100 GPUs show that GreedySnake achieves saturated training throughput improvements over ZeRO-Infinity: 1.96x on 1 GPU and 1.93x on 4 GPUs for GPT-65B, and 2.53x on 1 GPU for GPT-175B. The code is open-sourced at https://github.com/npz7yyk/GreedySnake
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Learning for Static and Single-Event Dynamic Complex Network Analysis</title>
<link>https://arxiv.org/abs/2512.17577</link>
<guid>https://arxiv.org/abs/2512.17577</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Representation Learning, Latent Space Models, Latent Distance Model, Structural-aware Network Representations, Unified Network Embeddings<br /><br />Summary: The thesis focuses on developing novel algorithmic approaches for Graph Representation Learning specifically targeting static and single-event dynamic networks. It concentrates on the family of Latent Space Models, with an emphasis on the Latent Distance Model to capture key network properties such as homophily, transitivity, and balance theory. The work aims to produce structural-aware network representations that allow for hierarchical mapping of network structures, community detection, identification of extreme profiles, and analysis of impact dynamics in temporal networks. A significant contribution is the design of unified learning methods that avoid heuristic-based and multi-stage procedures, including eliminating the need for post-processing steps. The overall objective is to advance unified and powerful network embeddings that comprehensively characterize network structures and support a wide variety of graph analysis tasks efficiently and effectively. This approach is expected to facilitate deeper insights into network behavior and improve performance in applications relying on graph data. <div>
arXiv:2512.17577v1 Announce Type: new 
Abstract: The primary objective of this thesis is to develop novel algorithmic approaches for Graph Representation Learning of static and single-event dynamic networks. In such a direction, we focus on the family of Latent Space Models, and more specifically on the Latent Distance Model which naturally conveys important network characteristics such as homophily, transitivity, and the balance theory. Furthermore, this thesis aims to create structural-aware network representations, which lead to hierarchical expressions of network structure, community characterization, the identification of extreme profiles in networks, and impact dynamics quantification in temporal networks. Crucially, the methods presented are designed to define unified learning processes, eliminating the need for heuristics and multi-stage processes like post-processing steps. Our aim is to delve into a journey towards unified network embeddings that are both comprehensive and powerful, capable of characterizing network structures and adeptly handling the diverse tasks that graph analysis offers.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Safe Autonomous Driving Policies Using Predictive Safety Representations</title>
<link>https://arxiv.org/abs/2512.17586</link>
<guid>https://arxiv.org/abs/2512.17586</guid>
<content:encoded><![CDATA[
<div> Keywords: Safe reinforcement learning, autonomous driving, safety representations, policy optimization, generalization  

<br /><br />Summary:  
This paper investigates Safe Reinforcement Learning (SafeRL) within autonomous driving, focusing on balancing safety and performance—a key challenge where conservative policies reduce efficiency and aggressive exploration risks safety. The Safety Representations for Safer Policy Learning (SRPL) framework equips agents with models predicting future constraint violations, previously proven effective in controlled settings. The study examines SRPL’s applicability to real-world datasets, specifically the Waymo Open Motion Dataset (WOMD) and NuPlan. Results show SRPL improves the reward-safety tradeoff, achieving statistically significant gains in success rates (effect sizes r = 0.65-0.86) and cost reductions (effect sizes r = 0.70-0.83) with p-values < 0.05. However, SRPL’s effectiveness varies depending on the policy optimizer used and dataset distributions. Furthermore, predictive safety representations enhance agent robustness against observation noise. Cross-dataset zero-shot evaluations reveal that SRPL-augmented agents generalize better than baseline non-SRPL methods. Collectively, the findings highlight the potential of predictive safety representations to improve robustness, safety, and generalization in SafeRL for autonomous driving applications. <div>
arXiv:2512.17586v1 Announce Type: new 
Abstract: Safe reinforcement learning (SafeRL) is a prominent paradigm for autonomous driving, where agents are required to optimize performance under strict safety requirements. This dual objective creates a fundamental tension, as overly conservative policies limit driving efficiency while aggressive exploration risks safety violations. The Safety Representations for Safer Policy Learning (SRPL) framework addresses this challenge by equipping agents with a predictive model of future constraint violations and has shown promise in controlled environments. This paper investigates whether SRPL extends to real-world autonomous driving scenarios. Systematic experiments on the Waymo Open Motion Dataset (WOMD) and NuPlan demonstrate that SRPL can improve the reward-safety tradeoff, achieving statistically significant improvements in success rate (effect sizes r = 0.65-0.86) and cost reduction (effect sizes r = 0.70-0.83), with p < 0.05 for observed improvements. However, its effectiveness depends on the underlying policy optimizer and the dataset distribution. The results further show that predictive safety representations play a critical role in improving robustness to observation noise. Additionally, in zero-shot cross-dataset evaluation, SRPL-augmented agents demonstrate improved generalization compared to non-SRPL methods. These findings collectively demonstrate the potential of predictive safety representations to strengthen SafeRL for autonomous driving.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sharing Knowledge without Sharing Data: Stitches can improve ensembles of disjointly trained models</title>
<link>https://arxiv.org/abs/2512.17592</link>
<guid>https://arxiv.org/abs/2512.17592</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, federated learning, asynchronous collaboration, model stitching, multi-objective performance<br /><br />Summary:<br />1. Deep learning excels at many real-world tasks but often depends on large, diverse datasets that are fragmented or not shareable, especially in sensitive domains like medicine.<br />2. Federated learning addresses data fragmentation by requiring parties to train a single model synchronously, sharing model weights during training.<br />3. The paper explores asynchronous collaboration, where only fully trained models are shared (e.g., via publications), rather than synchronously training together.<br />4. The authors propose using stitching layers to combine intermediate representations of individually trained models to improve performance across datasets.<br />5. Their multi-objective analysis shows that training on a single party’s data yields good performance on that data but poor generalization to other parties’ data.<br />6. Ensembles of individually trained models generalize better but reduce performance on each party’s own dataset.<br />7. Utilizing stitching layers enables recovery of party-specific performance while maintaining the advantages of improved generalization through asynchronous collaboration.<br />8. This method demonstrates that asynchronous sharing of trained models can competitively approximate the benefits of federated training without requiring synchronous model updates. <div>
arXiv:2512.17592v1 Announce Type: new 
Abstract: Deep learning has been shown to be very capable at performing many real-world tasks. However, this performance is often dependent on the presence of large and varied datasets. In some settings, like in the medical domain, data is often fragmented across parties, and cannot be readily shared. While federated learning addresses this situation, it is a solution that requires synchronicity of parties training a single model together, exchanging information about model weights. We investigate how asynchronous collaboration, where only already trained models are shared (e.g. as part of a publication), affects performance, and propose to use stitching as a method for combining models.
  Through taking a multi-objective perspective, where performance on each parties' data is viewed independently, we find that training solely on a single parties' data results in similar performance when merging with another parties' data, when considering performance on that single parties' data, while performance on other parties' data is notably worse. Moreover, while an ensemble of such individually trained networks generalizes better, performance on each parties' own dataset suffers. We find that combining intermediate representations in individually trained models with a well placed pair of stitching layers allows this performance to recover to a competitive degree while maintaining improved generalization, showing that asynchronous collaboration can yield competitive results.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Unified Representation of Neural Networks Architectures</title>
<link>https://arxiv.org/abs/2512.17593</link>
<guid>https://arxiv.org/abs/2512.17593</guid>
<content:encoded><![CDATA[
arXiv:2512.17593v1 Announce Type: new 
Abstract: In this paper we consider the limiting case of neural networks (NNs) architectures when the number of neurons in each hidden layer and the number of hidden layers tend to infinity thus forming a continuum, and we derive approximation errors as a function of the number of neurons and/or hidden layers. Firstly, we consider the case of neural networks with a single hidden layer and we derive an integral infinite width neural representation that generalizes existing continuous neural networks (CNNs) representations. Then we extend this to deep residual CNNs that have a finite number of integral hidden layers and residual connections. Secondly, we revisit the relation between neural ODEs and deep residual NNs and we formalize approximation errors via discretization techniques. Then, we merge these two approaches into a unified homogeneous representation of NNs as a Distributed Parameter neural Network (DiPaNet) and we show that most of the existing finite and infinite-dimensional NNs architectures are related via homogeneization/discretization with the DiPaNet representation. Our approach is purely deterministic and applies to general, uniformly continuous matrix weight functions. Differences and similarities with neural fields are discussed along with further possible generalizations and applications of the DiPaNet framework.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Systems-Theoretic View on the Convergence of Algorithms under Disturbances</title>
<link>https://arxiv.org/abs/2512.17598</link>
<guid>https://arxiv.org/abs/2512.17598</guid>
<content:encoded><![CDATA[
arXiv:2512.17598v1 Announce Type: new 
Abstract: Algorithms increasingly operate within complex physical, social, and engineering systems where they are exposed to disturbances, noise, and interconnections with other dynamical systems. This article extends known convergence guarantees of an algorithm operating in isolation (i.e., without disturbances) and systematically derives stability bounds and convergence rates in the presence of such disturbances. By leveraging converse Lyapunov theorems, we derive key inequalities that quantify the impact of disturbances. We further demonstrate how our result can be utilized to assess the effects of disturbances on algorithmic performance in a wide variety of applications, including communication constraints in distributed learning, sensitivity in machine learning generalization, and intentional noise injection for privacy. This underpins the role of our result as a unifying tool for algorithm analysis in the presence of noise, disturbances, and interconnections with other dynamical systems.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>More Consistent Accuracy PINN via Alternating Easy-Hard Training</title>
<link>https://arxiv.org/abs/2512.17607</link>
<guid>https://arxiv.org/abs/2512.17607</guid>
<content:encoded><![CDATA[
arXiv:2512.17607v1 Announce Type: new 
Abstract: Physics-informed neural networks (PINNs) have recently emerged as a prominent paradigm for solving partial differential equations (PDEs), yet their training strategies remain underexplored. While hard prioritization methods inspired by finite element methods are widely adopted, recent research suggests that easy prioritization can also be effective. Nevertheless, we find that both approaches exhibit notable trade-offs and inconsistent performance across PDE types. To address this issue, we develop a hybrid strategy that combines the strengths of hard and easy prioritization through an alternating training algorithm. On PDEs with steep gradients, nonlinearity, and high dimensionality, the proposed method achieves consistently high accuracy, with relative L2 errors mostly in the range of O(10^-5) to O(10^-6), significantly surpassing baseline methods. Moreover, it offers greater reliability across diverse problems, whereas compared approaches often suffer from variable accuracy depending on the PDE. This work provides new insights into designing hybrid training strategies to enhance the performance and robustness of PINNs.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCOPE: Sequential Causal Optimization of Process Interventions</title>
<link>https://arxiv.org/abs/2512.17629</link>
<guid>https://arxiv.org/abs/2512.17629</guid>
<content:encoded><![CDATA[
arXiv:2512.17629v1 Announce Type: new 
Abstract: Prescriptive Process Monitoring (PresPM) recommends interventions during business processes to optimize key performance indicators (KPIs). In realistic settings, interventions are rarely isolated: organizations need to align sequences of interventions to jointly steer the outcome of a case. Existing PresPM approaches fall short in this respect. Many focus on a single intervention decision, while others treat multiple interventions independently, ignoring how they interact over time. Methods that do address these dependencies depend either on simulation or data augmentation to approximate the process to train a Reinforcement Learning (RL) agent, which can create a reality gap and introduce bias. We introduce SCOPE, a PresPM approach that learns aligned sequential intervention recommendations. SCOPE employs backward induction to estimate the effect of each candidate intervention action, propagating its impact from the final decision point back to the first. By leveraging causal learners, our method can utilize observational data directly, unlike methods that require constructing process approximations for reinforcement learning. Experiments on both an existing synthetic dataset and a new semi-synthetic dataset show that SCOPE consistently outperforms state-of-the-art PresPM techniques in optimizing the KPI. The novel semi-synthetic setup, based on a real-life event log, is provided as a reusable benchmark for future work on sequential PresPM.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trust-Region Adaptive Policy Optimization</title>
<link>https://arxiv.org/abs/2512.17636</link>
<guid>https://arxiv.org/abs/2512.17636</guid>
<content:encoded><![CDATA[
arXiv:2512.17636v1 Announce Type: new 
Abstract: Post-training methods, especially Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), play an important role in improving large language models' (LLMs) complex reasoning abilities. However, the dominant two-stage pipeline (SFT then RL) suffers from a key inconsistency: SFT enforces rigid imitation that suppresses exploration and induces forgetting, limiting RL's potential for improvements. We address this inefficiency with TRAPO (\textbf{T}rust-\textbf{R}egion \textbf{A}daptive \textbf{P}olicy \textbf{O}ptimization), a hybrid framework that interleaves SFT and RL within each training instance by optimizing SFT loss on expert prefixes and RL loss on the model's own completions, unifying external supervision and self-exploration. To stabilize training, we introduce Trust-Region SFT (TrSFT), which minimizes forward KL divergence inside a trust region but attenuates optimization outside, effectively shifting toward reverse KL and yielding stable, mode-seeking updates favorable for RL. An adaptive prefix-selection mechanism further allocates expert guidance based on measured utility. Experiments on five mathematical reasoning benchmarks show that TRAPO consistently surpasses standard SFT, RL, and SFT-then-RL pipelines, as well as recent state-of-the-art approaches, establishing a strong new paradigm for reasoning-enhanced LLMs.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Estimating Spatially Resolved Radiation Fields Using Neural Networks</title>
<link>https://arxiv.org/abs/2512.17654</link>
<guid>https://arxiv.org/abs/2512.17654</guid>
<content:encoded><![CDATA[
arXiv:2512.17654v1 Announce Type: new 
Abstract: We present an in-depth analysis on how to build and train neural networks to estimate the spatial distribution of scattered radiation fields for radiation protection dosimetry in medical radiation fields, such as those found in Interventional Radiology and Cardiology. Therefore, we present three different synthetically generated datasets with increasing complexity for training, using a Monte-Carlo Simulation application based on Geant4. On those datasets, we evaluate convolutional and fully connected architectures of neural networks to demonstrate which design decisions work well for reconstructing the fluence and spectra distributions over the spatial domain of such radiation fields. All used datasets as well as our training pipeline are published as open source in separate repositories.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Polyharmonic Cascade</title>
<link>https://arxiv.org/abs/2512.17671</link>
<guid>https://arxiv.org/abs/2512.17671</guid>
<content:encoded><![CDATA[
arXiv:2512.17671v1 Announce Type: new 
Abstract: This paper presents a deep machine learning architecture, the "polyharmonic cascade" -- a sequence of packages of polyharmonic splines, where each layer is rigorously derived from the theory of random functions and the principles of indifference. This makes it possible to approximate nonlinear functions of arbitrary complexity while preserving global smoothness and a probabilistic interpretation. For the polyharmonic cascade, a training method alternative to gradient descent is proposed: instead of directly optimizing the coefficients, one solves a single global linear system on each batch with respect to the function values at fixed "constellations" of nodes. This yields synchronized updates of all layers, preserves the probabilistic interpretation of individual layers and theoretical consistency with the original model, and scales well: all computations reduce to 2D matrix operations efficiently executed on a GPU. Fast learning without overfitting on MNIST is demonstrated.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>You Only Train Once: Differentiable Subset Selection for Omics Data</title>
<link>https://arxiv.org/abs/2512.17678</link>
<guid>https://arxiv.org/abs/2512.17678</guid>
<content:encoded><![CDATA[
arXiv:2512.17678v1 Announce Type: new 
Abstract: Selecting compact and informative gene subsets from single-cell transcriptomic data is essential for biomarker discovery, improving interpretability, and cost-effective profiling. However, most existing feature selection approaches either operate as multi-stage pipelines or rely on post hoc feature attribution, making selection and prediction weakly coupled. In this work, we present YOTO (you only train once), an end-to-end framework that jointly identifies discrete gene subsets and performs prediction within a single differentiable architecture. In our model, the prediction task directly guides which genes are selected, while the learned subsets, in turn, shape the predictive representation. This closed feedback loop enables the model to iteratively refine both what it selects and how it predicts during training. Unlike existing approaches, YOTO enforces sparsity so that only the selected genes contribute to inference, eliminating the need to train additional downstream classifiers. Through a multi-task learning design, the model learns shared representations across related objectives, allowing partially labeled datasets to inform one another, and discovering gene subsets that generalize across tasks without additional training steps. We evaluate YOTO on two representative single-cell RNA-seq datasets, showing that it consistently outperforms state-of-the-art baselines. These results demonstrate that sparse, end-to-end, multi-task gene subset selection improves predictive performance and yields compact and meaningful gene subsets, advancing biomarker discovery and single-cell analysis.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Convergence Guarantees for Federated SARSA with Local Training and Heterogeneous Agents</title>
<link>https://arxiv.org/abs/2512.17688</link>
<guid>https://arxiv.org/abs/2512.17688</guid>
<content:encoded><![CDATA[
arXiv:2512.17688v1 Announce Type: new 
Abstract: We present a novel theoretical analysis of Federated SARSA (FedSARSA) with linear function approximation and local training. We establish convergence guarantees for FedSARSA in the presence of heterogeneity, both in local transitions and rewards, providing the first sample and communication complexity bounds in this setting. At the core of our analysis is a new, exact multi-step error expansion for single-agent SARSA, which is of independent interest. Our analysis precisely quantifies the impact of heterogeneity, demonstrating the convergence of FedSARSA with multiple local updates. Crucially, we show that FedSARSA achieves linear speed-up with respect to the number of agents, up to higher-order terms due to Markovian sampling. Numerical experiments support our theoretical findings.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatially-informed transformers: Injecting geostatistical covariance biases into self-attention for spatio-temporal forecasting</title>
<link>https://arxiv.org/abs/2512.17696</link>
<guid>https://arxiv.org/abs/2512.17696</guid>
<content:encoded><![CDATA[
arXiv:2512.17696v1 Announce Type: new 
Abstract: The modeling of high-dimensional spatio-temporal processes presents a fundamental dichotomy between the probabilistic rigor of classical geostatistics and the flexible, high-capacity representations of deep learning. While Gaussian processes offer theoretical consistency and exact uncertainty quantification, their prohibitive computational scaling renders them impractical for massive sensor networks. Conversely, modern transformer architectures excel at sequence modeling but inherently lack a geometric inductive bias, treating spatial sensors as permutation-invariant tokens without a native understanding of distance. In this work, we propose a spatially-informed transformer, a hybrid architecture that injects a geostatistical inductive bias directly into the self-attention mechanism via a learnable covariance kernel. By formally decomposing the attention structure into a stationary physical prior and a non-stationary data-driven residual, we impose a soft topological constraint that favors spatially proximal interactions while retaining the capacity to model complex dynamics. We demonstrate the phenomenon of ``Deep Variography'', where the network successfully recovers the true spatial decay parameters of the underlying process end-to-end via backpropagation. Extensive experiments on synthetic Gaussian random fields and real-world traffic benchmarks confirm that our method outperforms state-of-the-art graph neural networks. Furthermore, rigorous statistical validation confirms that the proposed method delivers not only superior predictive accuracy but also well-calibrated probabilistic forecasts, effectively bridging the gap between physics-aware modeling and data-driven learning.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Forgetting in Low Rank Adaptation</title>
<link>https://arxiv.org/abs/2512.17720</link>
<guid>https://arxiv.org/abs/2512.17720</guid>
<content:encoded><![CDATA[
arXiv:2512.17720v1 Announce Type: new 
Abstract: Parameter-efficient fine-tuning methods, such as Low-Rank Adaptation (LoRA), enable fast specialization of large pre-trained models to different downstream applications. However, this process often leads to catastrophic forgetting of the model's prior domain knowledge. We address this issue with LaLoRA, a weight-space regularization technique that applies a Laplace approximation to Low-Rank Adaptation. Our approach estimates the model's confidence in each parameter and constrains updates in high-curvature directions, preserving prior knowledge while enabling efficient target-domain learning. By applying the Laplace approximation only to the LoRA weights, the method remains lightweight. We evaluate LaLoRA by fine-tuning a Llama model for mathematical reasoning and demonstrate an improved learning-forgetting trade-off, which can be directly controlled via the method's regularization strength. We further explore different loss landscape curvature approximations for estimating parameter confidence, analyze the effect of the data used for the Laplace approximation, and study robustness across hyperparameters.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can You Hear Me Now? A Benchmark for Long-Range Graph Propagation</title>
<link>https://arxiv.org/abs/2512.17762</link>
<guid>https://arxiv.org/abs/2512.17762</guid>
<content:encoded><![CDATA[
arXiv:2512.17762v1 Announce Type: new 
Abstract: Effectively capturing long-range interactions remains a fundamental yet unresolved challenge in graph neural network (GNN) research, critical for applications across diverse fields of science. To systematically address this, we introduce ECHO (Evaluating Communication over long HOps), a novel benchmark specifically designed to rigorously assess the capabilities of GNNs in handling very long-range graph propagation. ECHO includes three synthetic graph tasks, namely single-source shortest paths, node eccentricity, and graph diameter, each constructed over diverse and structurally challenging topologies intentionally designed to introduce significant information bottlenecks. ECHO also includes two real-world datasets, ECHO-Charge and ECHO-Energy, which define chemically grounded benchmarks for predicting atomic partial charges and molecular total energies, respectively, with reference computations obtained at the density functional theory (DFT) level. Both tasks inherently depend on capturing complex long-range molecular interactions. Our extensive benchmarking of popular GNN architectures reveals clear performance gaps, emphasizing the difficulty of true long-range propagation and highlighting design choices capable of overcoming inherent limitations. ECHO thereby sets a new standard for evaluating long-range information propagation, also providing a compelling example for its need in AI for science.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Easy Adaptation: An Efficient Task-Specific Knowledge Injection Method for Large Models in Resource-Constrained Environments</title>
<link>https://arxiv.org/abs/2512.17771</link>
<guid>https://arxiv.org/abs/2512.17771</guid>
<content:encoded><![CDATA[
arXiv:2512.17771v1 Announce Type: new 
Abstract: While the enormous parameter scale endows Large Models (LMs) with unparalleled performance, it also limits their adaptability across specific tasks. Parameter-Efficient Fine-Tuning (PEFT) has emerged as a critical approach for effectively adapting LMs to a diverse range of downstream tasks. However, existing PEFT methods face two primary challenges: (1) High resource cost. Although PEFT methods significantly reduce resource demands compared to full fine-tuning, it still requires substantial time and memory, making it impractical in resource-constrained environments. (2) Parameter dependency. PEFT methods heavily rely on updating a subset of parameters associated with LMs to incorporate task-specific knowledge. Yet, due to increasing competition in the LMs landscape, many companies have adopted closed-source policies for their leading models, offering access only via Application Programming Interface (APIs). Whereas, the expense is often cost-prohibitive and difficult to sustain, as the fine-tuning process of LMs is extremely slow. Even if small models perform far worse than LMs in general, they can achieve superior results on particular distributions while requiring only minimal resources. Motivated by this insight, we propose Easy Adaptation (EA), which designs Specific Small Models (SSMs) to complement the underfitted data distribution for LMs. Extensive experiments show that EA matches the performance of PEFT on diverse tasks without accessing LM parameters, and requires only minimal resources.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Calibratable Disambiguation Loss for Multi-Instance Partial-Label Learning</title>
<link>https://arxiv.org/abs/2512.17788</link>
<guid>https://arxiv.org/abs/2512.17788</guid>
<content:encoded><![CDATA[
arXiv:2512.17788v1 Announce Type: new 
Abstract: Multi-instance partial-label learning (MIPL) is a weakly supervised framework that extends the principles of multi-instance learning (MIL) and partial-label learning (PLL) to address the challenges of inexact supervision in both instance and label spaces. However, existing MIPL approaches often suffer from poor calibration, undermining classifier reliability. In this work, we propose a plug-and-play calibratable disambiguation loss (CDL) that simultaneously improves classification accuracy and calibration performance. The loss has two instantiations: the first one calibrates predictions based on probabilities from the candidate label set, while the second one integrates probabilities from both candidate and non-candidate label sets. The proposed CDL can be seamlessly incorporated into existing MIPL and PLL frameworks. We provide a theoretical analysis that establishes the lower bound and regularization properties of CDL, demonstrating its superiority over conventional disambiguation losses. Experimental results on benchmark and real-world datasets confirm that our CDL significantly enhances both classification and calibration performance.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploiting ID-Text Complementarity via Ensembling for Sequential Recommendation</title>
<link>https://arxiv.org/abs/2512.17820</link>
<guid>https://arxiv.org/abs/2512.17820</guid>
<content:encoded><![CDATA[
arXiv:2512.17820v1 Announce Type: new 
Abstract: Modern Sequential Recommendation (SR) models commonly utilize modality features to represent items, motivated in large part by recent advancements in language and vision modeling. To do so, several works completely replace ID embeddings with modality embeddings, claiming that modality embeddings render ID embeddings unnecessary because they can match or even exceed ID embedding performance. On the other hand, many works jointly utilize ID and modality features, but posit that complex fusion strategies, such as multi-stage training and/or intricate alignment architectures, are necessary for this joint utilization. However, underlying both these lines of work is a lack of understanding of the complementarity of ID and modality features. In this work, we address this gap by studying the complementarity of ID- and text-based SR models. We show that these models do learn complementary signals, meaning that either should provide performance gain when used properly alongside the other. Motivated by this, we propose a new SR method that preserves ID-text complementarity through independent model training, then harnesses it through a simple ensembling strategy. Despite this method's simplicity, we show it outperforms several competitive SR baselines, implying that both ID and text features are necessary to achieve state-of-the-art SR performance but complex fusion architectures are not.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weighted Stochastic Differential Equation to Implement Wasserstein-Fisher-Rao Gradient Flow</title>
<link>https://arxiv.org/abs/2512.17878</link>
<guid>https://arxiv.org/abs/2512.17878</guid>
<content:encoded><![CDATA[
arXiv:2512.17878v1 Announce Type: new 
Abstract: Score-based diffusion models currently constitute the state of the art in continuous generative modeling. These methods are typically formulated via overdamped or underdamped Ornstein--Uhlenbeck-type stochastic differential equations, in which sampling is driven by a combination of deterministic drift and Brownian diffusion, resulting in continuous particle trajectories in the ambient space. While such dynamics enjoy exponential convergence guarantees for strongly log-concave target distributions, it is well known that their mixing rates deteriorate exponentially in the presence of nonconvex or multimodal landscapes, such as double-well potentials. Since many practical generative modeling tasks involve highly non-log-concave target distributions, considerable recent effort has been devoted to developing sampling schemes that improve exploration beyond classical diffusion dynamics.
  A promising line of work leverages tools from information geometry to augment diffusion-based samplers with controlled mass reweighting mechanisms. This perspective leads naturally to Wasserstein--Fisher--Rao (WFR) geometries, which couple transport in the sample space with vertical (reaction) dynamics on the space of probability measures. In this work, we formulate such reweighting mechanisms through the introduction of explicit correction terms and show how they can be implemented via weighted stochastic differential equations using the Feynman--Kac representation. Our study provides a preliminary but rigorous investigation of WFR-based sampling dynamics, and aims to clarify their geometric and operator-theoretic structure as a foundation for future theoretical and algorithmic developments.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Regularized Random Fourier Features and Finite Element Reconstruction for Operator Learning in Sobolev Space</title>
<link>https://arxiv.org/abs/2512.17884</link>
<guid>https://arxiv.org/abs/2512.17884</guid>
<content:encoded><![CDATA[
arXiv:2512.17884v1 Announce Type: new 
Abstract: Operator learning is a data-driven approximation of mappings between infinite-dimensional function spaces, such as the solution operators of partial differential equations. Kernel-based operator learning can offer accurate, theoretically justified approximations that require less training than standard methods. However, they can become computationally prohibitive for large training sets and can be sensitive to noise. We propose a regularized random Fourier feature (RRFF) approach, coupled with a finite element reconstruction map (RRFF-FEM), for learning operators from noisy data. The method uses random features drawn from multivariate Student's $t$ distributions, together with frequency-weighted Tikhonov regularization that suppresses high-frequency noise. We establish high-probability bounds on the extreme singular values of the associated random feature matrix and show that when the number of features $N$ scales like $m \log m$ with the number of training samples $m$, the system is well-conditioned, which yields estimation and generalization guarantees. Detailed numerical experiments on benchmark PDE problems, including advection, Burgers', Darcy flow, Helmholtz, Navier-Stokes, and structural mechanics, demonstrate that RRFF and RRFF-FEM are robust to noise and achieve improved performance with reduced training time compared to the unregularized random feature model, while maintaining competitive accuracy relative to kernel and neural operator tests.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Text Search: A Novel Pattern Matching Algorithm Based on Ukkonen's Approach</title>
<link>https://arxiv.org/abs/2512.16927</link>
<guid>https://arxiv.org/abs/2512.16927</guid>
<content:encoded><![CDATA[
arXiv:2512.16927v1 Announce Type: cross 
Abstract: In the realm of computer science, the efficiency of text-search algorithms is crucial for processing vast amounts of data in areas such as natural language processing and bioinformatics. Traditional methods like Naive Search, KMP, and Boyer-Moore, while foundational, often fall short in handling the complexities and scale of modern datasets, such as the Reuters corpus and human genomic sequences. This study rigorously investigates text-search algorithms, focusing on optimizing Suffix Trees through methods like Splitting and Ukkonen's Algorithm, analyzed on datasets including the Reuters corpus and human genomes. A novel optimization combining Ukkonen's Algorithm with a new search technique is introduced, showing linear time and space efficiencies, outperforming traditional methods like Naive Search, KMP, and Boyer-Moore. Empirical tests confirm the theoretical advantages, highlighting the optimized Suffix Tree's effectiveness in tasks like pattern recognition in genomic sequences, achieving 100% accuracy. This research not only advances academic knowledge in text-search algorithms but also demonstrates significant practical utility in fields like natural language processing and bioinformatics, due to its superior resource efficiency and reliability.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpIDER: Spatially Informed Dense Embedding Retrieval for Software Issue Localization</title>
<link>https://arxiv.org/abs/2512.16956</link>
<guid>https://arxiv.org/abs/2512.16956</guid>
<content:encoded><![CDATA[
arXiv:2512.16956v1 Announce Type: cross 
Abstract: Retrieving code units (e.g., files, classes, functions) that are semantically relevant to a given user query, bug report, or feature request from large codebases is a fundamental challenge for LLM-based coding agents. Agentic approaches typically employ sparse retrieval methods like BM25 or dense embedding strategies to identify relevant units. While embedding-based approaches can outperform BM25 by large margins, they often lack exploration of the codebase and underutilize its underlying graph structure. To address this, we propose SpIDER (Spatially Informed Dense Embedding Retrieval), an enhanced dense retrieval approach that incorporates LLM-based reasoning over auxiliary context obtained through graph-based exploration of the codebase. Empirical results show that SpIDER consistently improves dense retrieval performance across several programming languages.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MemoryGraft: Persistent Compromise of LLM Agents via Poisoned Experience Retrieval</title>
<link>https://arxiv.org/abs/2512.16962</link>
<guid>https://arxiv.org/abs/2512.16962</guid>
<content:encoded><![CDATA[
arXiv:2512.16962v1 Announce Type: cross 
Abstract: Large Language Model (LLM) agents increasingly rely on long-term memory and Retrieval-Augmented Generation (RAG) to persist experiences and refine future performance. While this experience learning capability enhances agentic autonomy, it introduces a critical, unexplored attack surface, i.e., the trust boundary between an agent's reasoning core and its own past. In this paper, we introduce MemoryGraft. It is a novel indirect injection attack that compromises agent behavior not through immediate jailbreaks, but by implanting malicious successful experiences into the agent's long-term memory. Unlike traditional prompt injections that are transient, or standard RAG poisoning that targets factual knowledge, MemoryGraft exploits the agent's semantic imitation heuristic which is the tendency to replicate patterns from retrieved successful tasks. We demonstrate that an attacker who can supply benign ingestion-level artifacts that the agent reads during execution can induce it to construct a poisoned RAG store where a small set of malicious procedure templates is persisted alongside benign experiences. When the agent later encounters semantically similar tasks, union retrieval over lexical and embedding similarity reliably surfaces these grafted memories, and the agent adopts the embedded unsafe patterns, leading to persistent behavioral drift across sessions. We validate MemoryGraft on MetaGPT's DataInterpreter agent with GPT-4o and find that a small number of poisoned records can account for a large fraction of retrieved experiences on benign workloads, turning experience-based self-improvement into a vector for stealthy and durable compromise. To facilitate reproducibility and future research, our code and evaluation data are available at https://github.com/Jacobhhy/Agent-Memory-Poisoning.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Colormap-Enhanced Vision Transformers for MRI-Based Multiclass (4-Class) Alzheimer's Disease Classification</title>
<link>https://arxiv.org/abs/2512.16964</link>
<guid>https://arxiv.org/abs/2512.16964</guid>
<content:encoded><![CDATA[
arXiv:2512.16964v1 Announce Type: cross 
Abstract: Magnetic Resonance Imaging (MRI) plays a pivotal role in the early diagnosis and monitoring of Alzheimer's disease (AD). However, the subtle structural variations in brain MRI scans often pose challenges for conventional deep learning models to extract discriminative features effectively. In this work, we propose PseudoColorViT-Alz, a colormap-enhanced Vision Transformer framework designed to leverage pseudo-color representations of MRI images for improved Alzheimer's disease classification. By combining colormap transformations with the global feature learning capabilities of Vision Transformers, our method amplifies anatomical texture and contrast cues that are otherwise subdued in standard grayscale MRI scans.
  We evaluate PseudoColorViT-Alz on the OASIS-1 dataset using a four-class classification setup (non-demented, moderate dementia, mild dementia, and very mild dementia). Our model achieves a state-of-the-art accuracy of 99.79% with an AUC of 100%, surpassing the performance of recent 2024--2025 methods, including CNN-based and Siamese-network approaches, which reported accuracies ranging from 96.1% to 99.68%. These results demonstrate that pseudo-color augmentation combined with Vision Transformers can significantly enhance MRI-based Alzheimer's disease classification. PseudoColorViT-Alz offers a robust and interpretable framework that outperforms current methods, providing a promising tool to support clinical decision-making and early detection of Alzheimer's disease.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows</title>
<link>https://arxiv.org/abs/2512.16969</link>
<guid>https://arxiv.org/abs/2512.16969</guid>
<content:encoded><![CDATA[
arXiv:2512.16969v1 Announce Type: cross 
Abstract: Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PAACE: A Plan-Aware Automated Agent Context Engineering Framework</title>
<link>https://arxiv.org/abs/2512.16970</link>
<guid>https://arxiv.org/abs/2512.16970</guid>
<content:encoded><![CDATA[
arXiv:2512.16970v1 Announce Type: cross 
Abstract: Large Language Model (LLM) agents are increasingly deployed in complex, multi-step workflows involving planning, tool use, reflection, and interaction with external knowledge systems. These workflows generate rapidly expanding contexts that must be curated, transformed, and compressed to maintain fidelity, avoid attention dilution, and reduce inference cost. Prior work on summarization and query-aware compression largely ignores the multi-step, plan-aware nature of agentic reasoning. In this work, we introduce PAACE (Plan-Aware Automated Context Engineering), a unified framework for optimizing the evolving state of LLM agents through next-k-task relevance modeling, plan-structure analysis, instruction co-refinement, and function-preserving compression. PAACE comprises (1) PAACE-Syn, a large-scale generator of synthetic agent workflows annotated with stepwise compression supervision, and (2) PAACE-FT, a family of distilled, plan-aware compressors trained from successful teacher demonstrations. Experiments on long-horizon benchmarks (AppWorld, OfficeBench, and 8-Objective QA) demonstrate that PAACE consistently improves agent correctness while substantially reducing context load. On AppWorld, PAACE achieves higher accuracy than all baselines while lowering peak context and cumulative dependency. On OfficeBench and multi-hop QA, PAACE improves both accuracy and F1, achieving fewer steps, lower peak tokens, and reduced attention dependency. Distilled PAACE-FT retains 97 percent of the teacher's performance while reducing inference cost by over an order of magnitude, enabling practical deployment of plan-aware compression with compact models.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Women's Health Benchmark for Large Language Models</title>
<link>https://arxiv.org/abs/2512.17028</link>
<guid>https://arxiv.org/abs/2512.17028</guid>
<content:encoded><![CDATA[
arXiv:2512.17028v1 Announce Type: cross 
Abstract: As large language models (LLMs) become primary sources of health information for millions, their accuracy in women's health remains critically unexamined. We introduce the Women's Health Benchmark (WHB), the first benchmark evaluating LLM performance specifically in women's health. Our benchmark comprises 96 rigorously validated model stumps covering five medical specialties (obstetrics and gynecology, emergency medicine, primary care, oncology, and neurology), three query types (patient query, clinician query, and evidence/policy query), and eight error types (dosage/medication errors, missing critical information, outdated guidelines/treatment recommendations, incorrect treatment advice, incorrect factual information, missing/incorrect differential diagnosis, missed urgency, and inappropriate recommendations). We evaluated 13 state-of-the-art LLMs and revealed alarming gaps: current models show approximately 60\% failure rates on the women's health benchmark, with performance varying dramatically across specialties and error types. Notably, models universally struggle with "missed urgency" indicators, while newer models like GPT-5 show significant improvements in avoiding inappropriate recommendations. Our findings underscore that AI chatbots are not yet fully able of providing reliable advice in women's health.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Perturb Your Data: Paraphrase-Guided Training Data Watermarking</title>
<link>https://arxiv.org/abs/2512.17075</link>
<guid>https://arxiv.org/abs/2512.17075</guid>
<content:encoded><![CDATA[
arXiv:2512.17075v1 Announce Type: cross 
Abstract: Training data detection is critical for enforcing copyright and data licensing, as Large Language Models (LLM) are trained on massive text corpora scraped from the internet. We present SPECTRA, a watermarking approach that makes training data reliably detectable even when it comprises less than 0.001% of the training corpus. SPECTRA works by paraphrasing text using an LLM and assigning a score based on how likely each paraphrase is, according to a separate scoring model. A paraphrase is chosen so that its score closely matches that of the original text, to avoid introducing any distribution shifts. To test whether a suspect model has been trained on the watermarked data, we compare its token probabilities against those of the scoring model. We demonstrate that SPECTRA achieves a consistent p-value gap of over nine orders of magnitude when detecting data used for training versus data not used for training, which is greater than all baselines tested. SPECTRA equips data owners with a scalable, deploy-before-release watermark that survives even large-scale LLM training.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Disentangled representations via score-based variational autoencoders</title>
<link>https://arxiv.org/abs/2512.17127</link>
<guid>https://arxiv.org/abs/2512.17127</guid>
<content:encoded><![CDATA[
arXiv:2512.17127v1 Announce Type: cross 
Abstract: We present the Score-based Autoencoder for Multiscale Inference (SAMI), a method for unsupervised representation learning that combines the theoretical frameworks of diffusion models and VAEs. By unifying their respective evidence lower bounds, SAMI formulates a principled objective that learns representations through score-based guidance of the underlying diffusion process. The resulting representations automatically capture meaningful structure in the data: it recovers ground truth generative factors in our synthetic dataset, learns factorized, semantic latent dimensions from complex natural images, and encodes video sequences into latent trajectories that are straighter than those of alternative encoders, despite training exclusively on static images. Furthermore, SAMI can extract useful representations from pre-trained diffusion models with minimal additional training. Finally, the explicitly probabilistic formulation provides new ways to identify semantically meaningful axes in the absence of supervised labels, and its mathematical exactness allows us to make formal statements about the nature of the learned representation. Overall, these results indicate that implicit structural information in diffusion models can be made explicit and interpretable through synergistic combination with a variational autoencoder.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Biosecurity-Aware AI: Agentic Risk Auditing of Soft Prompt Attacks on ESM-Based Variant Predictors</title>
<link>https://arxiv.org/abs/2512.17146</link>
<guid>https://arxiv.org/abs/2512.17146</guid>
<content:encoded><![CDATA[
arXiv:2512.17146v1 Announce Type: cross 
Abstract: Genomic Foundation Models (GFMs), such as Evolutionary Scale Modeling (ESM), have demonstrated remarkable success in variant effect prediction. However, their security and robustness under adversarial manipulation remain largely unexplored. To address this gap, we introduce the Secure Agentic Genomic Evaluator (SAGE), an agentic framework for auditing the adversarial vulnerabilities of GFMs. SAGE functions through an interpretable and automated risk auditing loop. It injects soft prompt perturbations, monitors model behavior across training checkpoints, computes risk metrics such as AUROC and AUPR, and generates structured reports with large language model-based narrative explanations. This agentic process enables continuous evaluation of embedding-space robustness without modifying the underlying model. Using SAGE, we find that even state-of-the-art GFMs like ESM2 are sensitive to targeted soft prompt attacks, resulting in measurable performance degradation. These findings reveal critical and previously hidden vulnerabilities in genomic foundation models, showing the importance of agentic risk auditing in securing biomedical applications such as clinical variant interpretation.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Application of machine learning to predict food processing level using Open Food Facts</title>
<link>https://arxiv.org/abs/2512.17169</link>
<guid>https://arxiv.org/abs/2512.17169</guid>
<content:encoded><![CDATA[
arXiv:2512.17169v1 Announce Type: cross 
Abstract: Ultra-processed foods are increasingly linked to health issues like obesity, cardiovascular disease, type 2 diabetes, and mental health disorders due to poor nutritional quality. This first-of-its-kind study at such a scale uses machine learning to classify food processing levels (NOVA) based on the Open Food Facts dataset of over 900,000 products. Models including LightGBM, Random Forest, and CatBoost were trained on nutrient concentration data. LightGBM performed best, achieving 80-85% accuracy across different nutrient panels and effectively distinguishing minimally from ultra-processed foods. Exploratory analysis revealed strong associations between higher NOVA classes and lower Nutri-Scores, indicating poorer nutritional quality. Products in NOVA 3 and 4 also had higher carbon footprints and lower Eco-Scores, suggesting greater environmental impact. Allergen analysis identified gluten and milk as common in ultra-processed items, posing risks to sensitive individuals. Categories like Cakes and Snacks were dominant in higher NOVA classes, which also had more additives, highlighting the role of ingredient modification. This study, leveraging the largest dataset of NOVA-labeled products, emphasizes the health, environmental, and allergenic implications of food processing and showcases machine learning's value in scalable classification. A user-friendly web tool is available for NOVA prediction using nutrient data: https://cosylab.iiitd.edu.in/foodlabel/.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Systemic Risk Radar: A Multi-Layer Graph Framework for Early Market Crash Warning</title>
<link>https://arxiv.org/abs/2512.17185</link>
<guid>https://arxiv.org/abs/2512.17185</guid>
<content:encoded><![CDATA[
arXiv:2512.17185v1 Announce Type: cross 
Abstract: Financial crises emerge when structural vulnerabilities accumulate across sectors, markets, and investor behavior. Predicting these systemic transitions is challenging because they arise from evolving interactions between market participants, not isolated price movements alone. We present Systemic Risk Radar (SRR), a framework that models financial markets as multi-layer graphs to detect early signs of systemic fragility and crash-regime transitions.
  We evaluate SRR across three major crises: the Dot-com crash, the Global Financial Crisis, and the COVID-19 shock. Our experiments compare snapshot GNNs, a simplified temporal GNN prototype, and standard baselines (logistic regression and Random Forest). Results show that structural network information provides useful early-warning signals compared to feature-based models alone.
  This correlation-based instantiation of SRR demonstrates that graph-derived features capture meaningful changes in market structure during stress events. The findings motivate extending SRR with additional graph layers (sector/factor exposure, sentiment) and more expressive temporal architectures (LSTM/GRU or Transformer encoders) to better handle diverse crisis types.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Foundational Audio Encoders Understand Music Structure?</title>
<link>https://arxiv.org/abs/2512.17209</link>
<guid>https://arxiv.org/abs/2512.17209</guid>
<content:encoded><![CDATA[
arXiv:2512.17209v1 Announce Type: cross 
Abstract: In music information retrieval (MIR) research, the use of pretrained foundational audio encoders (FAEs) has recently become a trend. FAEs pretrained on large amounts of music and audio data have been shown to improve performance on MIR tasks such as music tagging and automatic music transcription. However, their use for music structure analysis (MSA) remains underexplored. Although many open-source FAE models are available, only a small subset has been examined for MSA, and the impact of factors such as learning methods, training data, and model context length on MSA performance remains unclear. In this study, we conduct comprehensive experiments on 11 types of FAEs to investigate how these factors affect MSA performance. Our results demonstrate that FAEs using selfsupervised learning with masked language modeling on music data are particularly effective for MSA. These findings pave the way for future research in MSA.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CheXPO-v2: Preference Optimization for Chest X-ray VLMs with Knowledge Graph Consistency</title>
<link>https://arxiv.org/abs/2512.17213</link>
<guid>https://arxiv.org/abs/2512.17213</guid>
<content:encoded><![CDATA[
arXiv:2512.17213v1 Announce Type: cross 
Abstract: Medical Vision-Language Models (VLMs) are prone to hallucinations, compromising clinical reliability. While reinforcement learning methods like Group Relative Policy Optimization (GRPO) offer a low-cost alignment solution, their reliance on sparse, outcome-based rewards inadvertently encourages models to "overthink" -- generating verbose, convoluted, and unverifiable Chain-of-Thought reasoning to justify answers. This focus on outcomes obscures factual errors and poses significant safety risks. To address this, we propose CheXPO-v2, a novel alignment framework that shifts from outcome to process supervision. Our core innovation is a Knowledge Graph Consistency Reward mechanism driven by Entity-Relation Matching. By explicitly parsing reasoning steps into structured "Disease, Relation, Anatomy" triplets, we provide fine-grained supervision that penalizes incoherent logic and hallucinations at the atomic level. Integrating this with a hard-example mining strategy, our approach significantly outperforms GRPO and state-of-the-art models on benchmarks like MIMIC-CXR-VQA. Crucially, CheXPO-v2 achieves new state-of-the-art accuracy using only 5k samples, demonstrating exceptional data efficiency while producing clinically sound and verifiable reasoning. The project source code is publicly available at: https://github.com/ecoxial2007/CheX-Phi4MM.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Learning Assisted Parameter Tuning on Wavelet Transform Amorphous Radial Distribution Function</title>
<link>https://arxiv.org/abs/2512.17245</link>
<guid>https://arxiv.org/abs/2512.17245</guid>
<content:encoded><![CDATA[
arXiv:2512.17245v1 Announce Type: cross 
Abstract: Understanding atomic structures is crucial, yet amorphous materials remain challenging due to their irregular and non-periodic nature. The wavelet-transform radial distribution function (WT-RDF) offers a physics-based framework for analyzing amorphous structures, reliably predicting the first and second RDF peaks and overall curve trends in both binary Ge 0.25 Se 0.75 and ternary Ag x(Ge 0.25 Se 0.75)100-x (x=5,10,15,20,25) systems. Despite these strengths, WT-RDF shows limitations in amplitude accuracy, which affects quantitative analyses such as coordination numbers. This study addresses the issue by optimizing WT-RDF parameters using a machine learning approach, producing the enhanced WT-RDF+ framework. WT-RDF+ improves the precision of peak predictions and outperforms benchmark ML models, including RBF and LSTM, even when trained on only 25 percent of the binary dataset. These results demonstrate that WT-RDF+ is a robust and reliable model for structural characterization of amorphous materials, particularly Ge-Se systems, and support the efficient design and development of phase-change thin films for next-generation electronic devices and components.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AlignDP: Hybrid Differential Privacy with Rarity-Aware Protection for LLMs</title>
<link>https://arxiv.org/abs/2512.17251</link>
<guid>https://arxiv.org/abs/2512.17251</guid>
<content:encoded><![CDATA[
arXiv:2512.17251v1 Announce Type: cross 
Abstract: Large language models are exposed to risks of extraction, distillation, and unauthorized fine-tuning. Existing defenses use watermarking or monitoring, but these act after leakage. We design AlignDP, a hybrid privacy lock that blocks knowledge transfer at the data interface. The key idea is to separate rare and non-rare fields. Rare fields are shielded by PAC indistinguishability, giving effective zero-epsilon local DP. Non-rare fields are privatized with RAPPOR, giving unbiased frequency estimates under local DP. A global aggregator enforces composition and budget. This two-tier design hides rare events and adds controlled noise to frequent events. We prove limits of PAC extension to global aggregation, give bounds for RAPPOR estimates, and analyze utility trade-off. A toy simulation confirms feasibility: rare categories remain hidden, frequent categories are recovered with small error.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Practical Framework for Privacy-Preserving and Byzantine-robust Federated Learning</title>
<link>https://arxiv.org/abs/2512.17254</link>
<guid>https://arxiv.org/abs/2512.17254</guid>
<content:encoded><![CDATA[
arXiv:2512.17254v1 Announce Type: cross 
Abstract: Federated Learning (FL) allows multiple clients to collaboratively train a model without sharing their private data. However, FL is vulnerable to Byzantine attacks, where adversaries manipulate client models to compromise the federated model, and privacy inference attacks, where adversaries exploit client models to infer private data. Existing defenses against both backdoor and privacy inference attacks introduce significant computational and communication overhead, creating a gap between theory and practice. To address this, we propose ABBR, a practical framework for Byzantine-robust and privacy-preserving FL. We are the first to utilize dimensionality reduction to speed up the private computation of complex filtering rules in privacy-preserving FL. Additionally, we analyze the accuracy loss of vector-wise filtering in low-dimensional space and introduce an adaptive tuning strategy to minimize the impact of malicious models that bypass filtering on the global model. We implement ABBR with state-of-the-art Byzantine-robust aggregation rules and evaluate it on public datasets, showing that it runs significantly faster, has minimal communication overhead, and maintains nearly the same Byzantine-resilience as the baselines.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Verifiability-First Agents: Provable Observability and Lightweight Audit Agents for Controlling Autonomous LLM Systems</title>
<link>https://arxiv.org/abs/2512.17259</link>
<guid>https://arxiv.org/abs/2512.17259</guid>
<content:encoded><![CDATA[
arXiv:2512.17259v1 Announce Type: cross 
Abstract: As LLM-based agents grow more autonomous and multi-modal, ensuring they remain controllable, auditable, and faithful to deployer intent becomes critical. Prior benchmarks measured the propensity for misaligned behavior and showed that agent personalities and tool access significantly influence misalignment. Building on these insights, we propose a Verifiability-First architecture that (1) integrates run-time attestations of agent actions using cryptographic and symbolic methods, (2) embeds lightweight Audit Agents that continuously verify intent versus behavior using constrained reasoning, and (3) enforces challenge-response attestation protocols for high-risk operations. We introduce OPERA (Observability, Provable Execution, Red-team, Attestation), a benchmark suite and evaluation protocol designed to measure (i) detectability of misalignment, (ii) time to detection under stealthy strategies, and (iii) resilience of verifiability mechanisms to adversarial prompt and persona injection. Our approach shifts the evaluation focus from how likely misalignment is to how quickly and reliably misalignment can be detected and remediated.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Warmer for Less: A Cost-Efficient Strategy for Cold-Start Recommendations at Pinterest</title>
<link>https://arxiv.org/abs/2512.17277</link>
<guid>https://arxiv.org/abs/2512.17277</guid>
<content:encoded><![CDATA[
arXiv:2512.17277v1 Announce Type: cross 
Abstract: Pinterest is a leading visual discovery platform where recommender systems (RecSys) are key to delivering relevant, engaging, and fresh content to our users. In this paper, we study the problem of improving RecSys model predictions for cold-start (CS) items, which appear infrequently in the training data. Although this problem is well-studied in academia, few studies have addressed its root causes effectively at the scale of a platform like Pinterest. By investigating live traffic data, we identified several challenges of the CS problem and developed a corresponding solution for each: First, industrial-scale RecSys models must operate under tight computational constraints. Since CS items are a minority, any related improvements must be highly cost-efficient. To address this, our solutions were designed to be lightweight, collectively increasing the total parameters by only 5%. Second, CS items are represented only by non-historical (e.g., content or attribute) features, which models often treat as less important. To elevate their significance, we introduce a residual connection for the non-historical features. Third, CS items tend to receive lower prediction scores compared to non-CS items, reducing their likelihood of being surfaced. We mitigate this by incorporating a score regularization term into the model. Fourth, the labels associated with CS items are sparse, making it difficult for the model to learn from them. We apply the manifold mixup technique to address this data sparsity. Implemented together, our methods increased fresh content engagement at Pinterest by 10% without negatively impacting overall engagement and cost, and have been deployed to serve over 570 million users on Pinterest.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LibriVAD: A Scalable Open Dataset with Deep Learning Benchmarks for Voice Activity Detection</title>
<link>https://arxiv.org/abs/2512.17281</link>
<guid>https://arxiv.org/abs/2512.17281</guid>
<content:encoded><![CDATA[
arXiv:2512.17281v1 Announce Type: cross 
Abstract: Robust Voice Activity Detection (VAD) remains a challenging task, especially under noisy, diverse, and unseen acoustic conditions. Beyond algorithmic development, a key limitation in advancing VAD research is the lack of large-scale, systematically controlled, and publicly available datasets. To address this, we introduce LibriVAD - a scalable open-source dataset derived from LibriSpeech and augmented with diverse real-world and synthetic noise sources. LibriVAD enables systematic control over speech-to-noise ratio, silence-to-speech ratio (SSR), and noise diversity, and is released in three sizes (15 GB, 150 GB, and 1.5 TB) with two variants (LibriVAD-NonConcat and LibriVAD-Concat) to support different experimental setups. We benchmark multiple feature-model combinations, including waveform, Mel-Frequency Cepstral Coefficients (MFCC), and Gammatone filter bank cepstral coefficients, and introduce the Vision Transformer (ViT) architecture for VAD. Our experiments show that ViT with MFCC features consistently outperforms established VAD models such as boosted deep neural network and convolutional long short-term memory deep neural network across seen, unseen, and out-of-distribution (OOD) conditions, including evaluation on the real-world VOiCES dataset. We further analyze the impact of dataset size and SSR on model generalization, experimentally showing that scaling up dataset size and balancing SSR noticeably and consistently enhance VAD performance under OOD conditions. All datasets, trained models, and code are publicly released to foster reproducibility and accelerate progress in VAD research.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Penalized Fair Regression for Multiple Groups in Chronic Kidney Disease</title>
<link>https://arxiv.org/abs/2512.17340</link>
<guid>https://arxiv.org/abs/2512.17340</guid>
<content:encoded><![CDATA[
arXiv:2512.17340v1 Announce Type: cross 
Abstract: Fair regression methods have the potential to mitigate societal bias concerns in health care, but there has been little work on penalized fair regression when multiple groups experience such bias. We propose a general regression framework that addresses this gap with unfairness penalties for multiple groups. Our approach is demonstrated for binary outcomes with true positive rate disparity penalties. It can be efficiently implemented through reduction to a cost-sensitive classification problem. We additionally introduce novel score functions for automatically selecting penalty weights. Our penalized fair regression methods are empirically studied in simulations, where they achieve a fairness-accuracy frontier beyond that of existing comparison methods. Finally, we apply these methods to a national multi-site primary care study of chronic kidney disease to develop a fair classifier for end-stage renal disease. There we find substantial improvements in fairness for multiple race and ethnicity groups who experience societal bias in the health care system without any appreciable loss in overall fit.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sharp Structure-Agnostic Lower Bounds for General Functional Estimation</title>
<link>https://arxiv.org/abs/2512.17341</link>
<guid>https://arxiv.org/abs/2512.17341</guid>
<content:encoded><![CDATA[
arXiv:2512.17341v1 Announce Type: cross 
Abstract: The design of efficient nonparametric estimators has long been a central problem in statistics, machine learning, and decision making. Classical optimal procedures often rely on strong structural assumptions, which can be misspecified in practice and complicate deployment. This limitation has sparked growing interest in structure-agnostic approaches -- methods that debias black-box nuisance estimates without imposing structural priors. Understanding the fundamental limits of these methods is therefore crucial. This paper provides a systematic investigation of the optimal error rates achievable by structure-agnostic estimators. We first show that, for estimating the average treatment effect (ATE), a central parameter in causal inference, doubly robust learning attains optimal structure-agnostic error rates. We then extend our analysis to a general class of functionals that depend on unknown nuisance functions and establish the structure-agnostic optimality of debiased/double machine learning (DML). We distinguish two regimes -- one where double robustness is attainable and one where it is not -- leading to different optimal rates for first-order debiasing, and show that DML is optimal in both regimes. Finally, we instantiate our general lower bounds by deriving explicit optimal rates that recover existing results and extend to additional estimands of interest. Our results provide theoretical validation for widely used first-order debiasing methods and guidance for practitioners seeking optimal approaches in the absence of structural assumptions. This paper generalizes and subsumes the ATE lower bound established in \citet{jin2024structure} by the same authors.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Timely Information Updating for Mobile Devices Without and With ML Advice</title>
<link>https://arxiv.org/abs/2512.17381</link>
<guid>https://arxiv.org/abs/2512.17381</guid>
<content:encoded><![CDATA[
arXiv:2512.17381v1 Announce Type: cross 
Abstract: This paper investigates an information update system in which a mobile device monitors a physical process and sends status updates to an access point (AP). A fundamental trade-off arises between the timeliness of the information maintained at the AP and the update cost incurred at the device. To address this trade-off, we propose an online algorithm that determines when to transmit updates using only available observations. The proposed algorithm asymptotically achieves the optimal competitive ratio against an adversary that can simultaneously manipulate multiple sources of uncertainty, including the operation duration, the information staleness, the update cost, and the availability of update opportunities. Furthermore, by incorporating machine learning (ML) advice of unknown reliability into the design, we develop an ML-augmented algorithm that asymptotically attains the optimal consistency-robustness trade-off, even when the adversary can additionally corrupt the ML advice. The optimal competitive ratio scales linearly with the range of update costs, but is unaffected by other uncertainties. Moreover, an optimal competitive online algorithm exhibits a threshold-like response to the ML advice: it either fully trusts or completely ignores the ML advice, as partially trusting the advice cannot improve the consistency without severely degrading the robustness. Extensive simulations in stochastic settings further validate the theoretical findings in the adversarial environment.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories</title>
<link>https://arxiv.org/abs/2512.17419</link>
<guid>https://arxiv.org/abs/2512.17419</guid>
<content:encoded><![CDATA[
arXiv:2512.17419v1 Announce Type: cross 
Abstract: Benchmarks like SWE-bench have standardized the evaluation of Large Language Models (LLMs) on repository-level software engineering tasks. However, these efforts remain limited by manual curation, static datasets, and a focus on Python-based bug fixes. We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects. Unlike synthetic approaches, our pipeline harvests live pull requests to cover both bug fixes and feature requests across 11 languages. SWE-Bench++ turns GitHub pull requests (PRs) into reproducible, execution-based tasks via four stages: programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance. A final hint-guided trajectory synthesis step converts instances that strong models fail on into training trajectories. Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages. On a subset of 1,782 instances of this benchmark, today's strongest models perform as follows: claude-sonnet-4.5 achieves 36.20% pass@10, gpt-5-2025-08-07 34.57%, gemini/gemini-2.5-pro 24.92%, and gpt-4o 16.89%. We further demonstrate the utility of our dataset by showing that fine-tuning on SWE-Bench++ instances yields measurable improvements on the SWE-bench Multilingual benchmark. SWE-Bench++ provides a scalable, multilingual benchmark for evaluating and improving repository-level code generation.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Perfect reconstruction of sparse signals using nonconvexity control and one-step RSB message passing</title>
<link>https://arxiv.org/abs/2512.17426</link>
<guid>https://arxiv.org/abs/2512.17426</guid>
<content:encoded><![CDATA[
arXiv:2512.17426v1 Announce Type: cross 
Abstract: We consider sparse signal reconstruction via minimization of the smoothly clipped absolute deviation (SCAD) penalty, and develop one-step replica-symmetry-breaking (1RSB) extensions of approximate message passing (AMP), termed 1RSB-AMP. Starting from the 1RSB formulation of belief propagation, we derive explicit update rules of 1RSB-AMP together with the corresponding state evolution (1RSB-SE) equations. A detailed comparison shows that 1RSB-AMP and 1RSB-SE agree remarkably well at the macroscopic level, even in parameter regions where replica-symmetric (RS) AMP, termed RS-AMP, diverges and where the 1RSB description itself is not expected to be thermodynamically exact. Fixed-point analysis of 1RSB-SE reveals a phase diagram consisting of success, failure, and diverging phases, as in the RS case. However, the diverging-region boundary now depends on the Parisi parameter due to the 1RSB ansatz, and we propose a new criterion -- minimizing the size of the diverging region -- rather than the conventional zero-complexity condition, to determine its value. Combining this criterion with the nonconvexity-control (NCC) protocol proposed in a previous RS study improves the algorithmic limit of perfect reconstruction compared with RS-AMP. Numerical solutions of 1RSB-SE and experiments with 1RSB-AMP confirm that this improved limit is achieved in practice, though the gain is modest and remains slightly inferior to the Bayes-optimal threshold. We also report the behavior of thermodynamic quantities -- overlaps, free entropy, complexity, and the non-self-averaging susceptibility -- that characterize the 1RSB phase in this problem.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MULTIAQUA: A multimodal maritime dataset and robust training strategies for multimodal semantic segmentation</title>
<link>https://arxiv.org/abs/2512.17450</link>
<guid>https://arxiv.org/abs/2512.17450</guid>
<content:encoded><![CDATA[
arXiv:2512.17450v1 Announce Type: cross 
Abstract: Unmanned surface vehicles can encounter a number of varied visual circumstances during operation, some of which can be very difficult to interpret. While most cases can be solved only using color camera images, some weather and lighting conditions require additional information. To expand the available maritime data, we present a novel multimodal maritime dataset MULTIAQUA (Multimodal Aquatic Dataset). Our dataset contains synchronized, calibrated and annotated data captured by sensors of different modalities, such as RGB, thermal, IR, LIDAR, etc. The dataset is aimed at developing supervised methods that can extract useful information from these modalities in order to provide a high quality of scene interpretation regardless of potentially poor visibility conditions. To illustrate the benefits of the proposed dataset, we evaluate several multimodal methods on our difficult nighttime test set. We present training approaches that enable multimodal methods to be trained in a more robust way, thus enabling them to retain reliable performance even in near-complete darkness. Our approach allows for training a robust deep neural network only using daytime images, thus significantly simplifying data acquisition, annotation, and the training process.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Data Quality Issues Collide: A Large-Scale Empirical Study of Co-Occurring Data Quality Issues in Software Defect Prediction</title>
<link>https://arxiv.org/abs/2512.17460</link>
<guid>https://arxiv.org/abs/2512.17460</guid>
<content:encoded><![CDATA[
arXiv:2512.17460v1 Announce Type: cross 
Abstract: Software Defect Prediction (SDP) models are central to proactive software quality assurance, yet their effectiveness is often constrained by the quality of available datasets. Prior research has typically examined single issues such as class imbalance or feature irrelevance in isolation, overlooking that real-world data problems frequently co-occur and interact. This study presents, to our knowledge, the first large-scale empirical analysis in SDP that simultaneously examines five co-occurring data quality issues (class imbalance, class overlap, irrelevant features, attribute noise, and outliers) across 374 datasets and five classifiers. We employ Explainable Boosting Machines together with stratified interaction analysis to quantify both direct and conditional effects under default hyperparameter settings, reflecting practical baseline usage.
  Our results show that co-occurrence is nearly universal: even the least frequent issue (attribute noise) appears alongside others in more than 93% of datasets. Irrelevant features and imbalance are nearly ubiquitous, while class overlap is the most consistently harmful issue. We identify stable tipping points around 0.20 for class overlap, 0.65-0.70 for imbalance, and 0.94 for irrelevance, beyond which most models begin to degrade. We also uncover counterintuitive patterns, such as outliers improving performance when irrelevant features are low, underscoring the importance of context-aware evaluation. Finally, we expose a performance-robustness trade-off: no single learner dominates under all conditions.
  By jointly analyzing prevalence, co-occurrence, thresholds, and conditional effects, our study directly addresses a persistent gap in SDP research. Hence, moving beyond isolated analyses to provide a holistic, data-aware understanding of how quality issues shape model performance in real-world settings.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Behavioural Effects of Agentic Messaging: A Case Study on a Financial Service Application</title>
<link>https://arxiv.org/abs/2512.17462</link>
<guid>https://arxiv.org/abs/2512.17462</guid>
<content:encoded><![CDATA[
arXiv:2512.17462v1 Announce Type: cross 
Abstract: Marketing and product personalisation provide a prominent and visible use-case for the application of Information Retrieval methods across several business domains. Recently, agentic approaches to these problems have been gaining traction. This work evaluates the behavioural and retention effects of agentic personalisation on a financial service application's customer communication system during a 2025 national tax filing period. Through a two month-long randomised controlled trial, we compare an agentic messaging approach against a business-as-usual (BAU) rule-based campaign system, focusing on two primary outcomes: unsubscribe behaviour and conversion timing. Empirical results show that agent-led messaging reduced unsubscribe events by 21\% ($\pm 0.01$) relative to BAU and increased early filing behaviour in the weeks preceding the national deadline. These findings demonstrate how adaptive, user-level decision-making systems can modulate engagement intensity whilst improving long-term retention indicators.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Linear Attention for Joint Power Optimization and User-Centric Clustering in Cell-Free Networks</title>
<link>https://arxiv.org/abs/2512.17466</link>
<guid>https://arxiv.org/abs/2512.17466</guid>
<content:encoded><![CDATA[
arXiv:2512.17466v1 Announce Type: cross 
Abstract: Optimal AP clustering and power allocation are critical in user-centric cell-free massive MIMO systems. Existing deep learning models lack flexibility to handle dynamic network configurations. Furthermore, many approaches overlook pilot contamination and suffer from high computational complexity. In this paper, we propose a lightweight transformer model that overcomes these limitations by jointly predicting AP clusters and powers solely from spatial coordinates of user devices and AP. Our model is architecture-agnostic to users load, handles both clustering and power allocation without channel estimation overhead, and eliminates pilot contamination by assigning users to AP within a pilot reuse constraint. We also incorporate a customized linear attention mechanism to capture user-AP interactions efficiently and enable linear scalability with respect to the number of users. Numerical results confirm the model's effectiveness in maximizing the minimum spectral efficiency and providing near-optimal performance while ensuring adaptability and scalability in dynamic scenarios.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Translating the Rashomon Effect to Sequential Decision-Making Tasks</title>
<link>https://arxiv.org/abs/2512.17470</link>
<guid>https://arxiv.org/abs/2512.17470</guid>
<content:encoded><![CDATA[
arXiv:2512.17470v1 Announce Type: cross 
Abstract: The Rashomon effect describes the phenomenon where multiple models trained on the same data produce identical predictions while differing in which features they rely on internally. This effect has been studied extensively in classification tasks, but not in sequential decision-making, where an agent learns a policy to achieve an objective by taking actions in an environment. In this paper, we translate the Rashomon effect to sequential decision-making. We define it as multiple policies that exhibit identical behavior, visiting the same states and selecting the same actions, while differing in their internal structure, such as feature attributions. Verifying identical behavior in sequential decision-making differs from classification. In classification, predictions can be directly compared to ground-truth labels. In sequential decision-making with stochastic transitions, the same policy may succeed or fail on any single trajectory due to randomness. We address this using formal verification methods that construct and compare the complete probabilistic behavior of each policy in the environment. Our experiments demonstrate that the Rashomon effect exists in sequential decision-making. We further show that ensembles constructed from the Rashomon set exhibit greater robustness to distribution shifts than individual policies. Additionally, permissive policies derived from the Rashomon set reduce computational requirements for verification while maintaining optimal performance.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Alternating Direction Method of Multipliers for Nonlinear Matrix Decompositions</title>
<link>https://arxiv.org/abs/2512.17473</link>
<guid>https://arxiv.org/abs/2512.17473</guid>
<content:encoded><![CDATA[
arXiv:2512.17473v1 Announce Type: cross 
Abstract: We present an algorithm based on the alternating direction method of multipliers (ADMM) for solving nonlinear matrix decompositions (NMD). Given an input matrix $X \in \mathbb{R}^{m \times n}$ and a factorization rank $r \ll \min(m, n)$, NMD seeks matrices $W \in \mathbb{R}^{m \times r}$ and $H \in \mathbb{R}^{r \times n}$ such that $X \approx f(WH)$, where $f$ is an element-wise nonlinear function. We evaluate our method on several representative nonlinear models: the rectified linear unit activation $f(x) = \max(0, x)$, suitable for nonnegative sparse data approximation, the component-wise square $f(x) = x^2$, applicable to probabilistic circuit representation, and the MinMax transform $f(x) = \min(b, \max(a, x))$, relevant for recommender systems. The proposed framework flexibly supports diverse loss functions, including least squares, $\ell_1$ norm, and the Kullback-Leibler divergence, and can be readily extended to other nonlinearities and metrics. We illustrate the applicability, efficiency, and adaptability of the approach on real-world datasets, highlighting its potential for a broad range of applications.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TwinSegNet: A Digital Twin-Enabled Federated Learning Framework for Brain Tumor Analysis</title>
<link>https://arxiv.org/abs/2512.17488</link>
<guid>https://arxiv.org/abs/2512.17488</guid>
<content:encoded><![CDATA[
arXiv:2512.17488v1 Announce Type: cross 
Abstract: Brain tumor segmentation is critical in diagnosis and treatment planning for the disease. Yet, current deep learning methods rely on centralized data collection, which raises privacy concerns and limits generalization across diverse institutions. In this paper, we propose TwinSegNet, which is a privacy-preserving federated learning framework that integrates a hybrid ViT-UNet model with personalized digital twins for accurate and real-time brain tumor segmentation. Our architecture combines convolutional encoders with Vision Transformer bottlenecks to capture local and global context. Each institution fine-tunes the global model of private data to form its digital twin. Evaluated on nine heterogeneous MRI datasets, including BraTS 2019-2021 and custom tumor collections, TwinSegNet achieves high Dice scores (up to 0.90%) and sensitivity/specificity exceeding 90%, demonstrating robustness across non-independent and identically distributed (IID) client distributions. Comparative results against centralized models such as TumorVisNet highlight TwinSegNet's effectiveness in preserving privacy without sacrificing performance. Our approach enables scalable, personalized segmentation for multi-institutional clinical settings while adhering to strict data confidentiality requirements.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Resource-efficient medical image classification for edge devices</title>
<link>https://arxiv.org/abs/2512.17515</link>
<guid>https://arxiv.org/abs/2512.17515</guid>
<content:encoded><![CDATA[
arXiv:2512.17515v1 Announce Type: cross 
Abstract: Medical image classification is a critical task in healthcare, enabling accurate and timely diagnosis. However, deploying deep learning models on resource-constrained edge devices presents significant challenges due to computational and memory limitations. This research investigates a resource-efficient approach to medical image classification by employing model quantization techniques. Quantization reduces the precision of model parameters and activations, significantly lowering computational overhead and memory requirements without sacrificing classification accuracy. The study focuses on the optimization of quantization-aware training (QAT) and post-training quantization (PTQ) methods tailored for edge devices, analyzing their impact on model performance across medical imaging datasets. Experimental results demonstrate that quantized models achieve substantial reductions in model size and inference latency, enabling real-time processing on edge hardware while maintaining clinically acceptable diagnostic accuracy. This work provides a practical pathway for deploying AI-driven medical diagnostics in remote and resource-limited settings, enhancing the accessibility and scalability of healthcare technologies.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PathBench-MIL: A Comprehensive AutoML and Benchmarking Framework for Multiple Instance Learning in Histopathology</title>
<link>https://arxiv.org/abs/2512.17517</link>
<guid>https://arxiv.org/abs/2512.17517</guid>
<content:encoded><![CDATA[
arXiv:2512.17517v1 Announce Type: cross 
Abstract: We introduce PathBench-MIL, an open-source AutoML and benchmarking framework for multiple instance learning (MIL) in histopathology. The system automates end-to-end MIL pipeline construction, including preprocessing, feature extraction, and MIL-aggregation, and provides reproducible benchmarking of dozens of MIL models and feature extractors. PathBench-MIL integrates visualization tooling, a unified configuration system, and modular extensibility, enabling rapid experimentation and standardization across datasets and tasks. PathBench-MIL is publicly available at https://github.com/Sbrussee/PathBench-MIL
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HydroGym: A Reinforcement Learning Platform for Fluid Dynamics</title>
<link>https://arxiv.org/abs/2512.17534</link>
<guid>https://arxiv.org/abs/2512.17534</guid>
<content:encoded><![CDATA[
arXiv:2512.17534v1 Announce Type: cross 
Abstract: Modeling and controlling fluid flows is critical for several fields of science and engineering, including transportation, energy, and medicine. Effective flow control can lead to, e.g., lift increase, drag reduction, mixing enhancement, and noise reduction. However, controlling a fluid faces several significant challenges, including high-dimensional, nonlinear, and multiscale interactions in space and time. Reinforcement learning (RL) has recently shown great success in complex domains, such as robotics and protein folding, but its application to flow control is hindered by a lack of standardized benchmark platforms and the computational demands of fluid simulations. To address these challenges, we introduce HydroGym, a solver-independent RL platform for flow control research. HydroGym integrates sophisticated flow control benchmarks, scalable runtime infrastructure, and state-of-the-art RL algorithms. Our platform includes 42 validated environments spanning from canonical laminar flows to complex three-dimensional turbulent scenarios, validated over a wide range of Reynolds numbers. We provide non-differentiable solvers for traditional RL and differentiable solvers that dramatically improve sample efficiency through gradient-enhanced optimization. Comprehensive evaluation reveals that RL agents consistently discover robust control principles across configurations, such as boundary layer manipulation, acoustic feedback disruption, and wake reorganization. Transfer learning studies demonstrate that controllers learned at one Reynolds number or geometry adapt efficiently to new conditions, requiring approximately 50% fewer training episodes. The HydroGym platform is highly extensible and scalable, providing a framework for researchers in fluid dynamics, machine learning, and control to add environments, surrogate models, and control algorithms to advance science and technology.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When De-noising Hurts: A Systematic Study of Speech Enhancement Effects on Modern Medical ASR Systems</title>
<link>https://arxiv.org/abs/2512.17562</link>
<guid>https://arxiv.org/abs/2512.17562</guid>
<content:encoded><![CDATA[
arXiv:2512.17562v1 Announce Type: cross 
Abstract: Speech enhancement methods are commonly believed to improve the performance of automatic speech recognition (ASR) in noisy environments. However, the effectiveness of these techniques cannot be taken for granted in the case of modern large-scale ASR models trained on diverse, noisy data. We present a systematic evaluation of MetricGAN-plus-voicebank denoising on four state-of-the-art ASR systems: OpenAI Whisper, NVIDIA Parakeet, Google Gemini Flash 2.0, Parrotlet-a using 500 medical speech recordings under nine noise conditions. ASR performance is measured using semantic WER (semWER), a normalized word error rate (WER) metric accounting for domain-specific normalizations. Our results reveal a counterintuitive finding: speech enhancement preprocessing degrades ASR performance across all noise conditions and models. Original noisy audio achieves lower semWER than enhanced audio in all 40 tested configurations (4 models x 10 conditions), with degradations ranging from 1.1% to 46.6% absolute semWER increase. These findings suggest that modern ASR models possess sufficient internal noise robustness and that traditional speech enhancement may remove acoustic features critical for ASR. For practitioners deploying medical scribe systems in noisy clinical environments, our results indicate that preprocessing audio with noise reduction techniques might not just be computationally wasteful but also be potentially harmful to the transcription accuracy.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enabling Disaggregated Multi-Stage MLLM Inference via GPU-Internal Scheduling and Resource Sharing</title>
<link>https://arxiv.org/abs/2512.17574</link>
<guid>https://arxiv.org/abs/2512.17574</guid>
<content:encoded><![CDATA[
arXiv:2512.17574v1 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs) extend LLMs with visual understanding through a three-stage pipeline: multimodal preprocessing, vision encoding, and LLM inference. While these stages enhance capability, they introduce significant system bottlenecks. First, multimodal preprocessing-especially video decoding-often dominates Time-to-First-Token (TTFT). Most systems rely on CPU-based decoding, which severely limits throughput, while existing GPU-based approaches prioritize throughput-oriented parallelism and fail to meet the latency-sensitive requirements of MLLM inference. Second, the vision encoder is a standalone, compute-intensive stage that produces visual embeddings and cannot be co-batched with LLM prefill or decoding. This heterogeneity forces inter-stage blocking and increases token-generation latency. Even when deployed on separate GPUs, these stages underutilize available compute and memory resources, reducing overall utilization and constraining system throughput.
  To address these challenges, we present FlashCodec and UnifiedServe, two complementary designs that jointly optimize the end-to-end MLLM pipeline. FlashCodec accelerates the multimodal preprocessing stage through collaborative multi-GPU video decoding, reducing decoding latency while preserving high throughput. UnifiedServe optimizes the vision-to-text and inference stages using a logically decoupled their execution to eliminate inter-stage blocking, yet physically sharing GPU resources to maximize GPU system utilization. By carefully orchestrating execution across stages and minimizing interference, UnifiedServe Together, our proposed framework forms an end-to-end optimized stack that can serve up to 3.0$\times$ more requests or enforce 1.5$\times$ tighter SLOs, while achieving up to 4.4$\times$ higher throughput compared to state-of-the-art systems.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SkinGenBench: Generative Model and Preprocessing Effects for Synthetic Dermoscopic Augmentation in Melanoma Diagnosis</title>
<link>https://arxiv.org/abs/2512.17585</link>
<guid>https://arxiv.org/abs/2512.17585</guid>
<content:encoded><![CDATA[
arXiv:2512.17585v1 Announce Type: cross 
Abstract: This work introduces SkinGenBench, a systematic biomedical imaging benchmark that investigates how preprocessing complexity interacts with generative model choice for synthetic dermoscopic image augmentation and downstream melanoma diagnosis. Using a curated dataset of 14,116 dermoscopic images from HAM10000 and MILK10K across five lesion classes, we evaluate the two representative generative paradigms: StyleGAN2-ADA and Denoising Diffusion Probabilistic Models (DDPMs) under basic geometric augmentation and advanced artifact removal pipelines. Synthetic melanoma images are assessed using established perceptual and distributional metrics (FID, KID, IS), feature space analysis, and their impact on diagnostic performance across five downstream classifiers. Experimental results demonstrate that generative architecture choice has a stronger influence on both image fidelity and diagnostic utility than preprocessing complexity. StyleGAN2-ADA consistently produced synthetic images more closely aligned with real data distributions, achieving the lowest FID (~65.5) and KID (~0.05), while diffusion models generated higher variance samples at the cost of reduces perceptual fidelity and class anchoring. Advanced artifact removal yielded only marginal improvements in generative metrics and provided limited downstream diagnostic gains, suggesting possible suppression of clinically relevant texture cues. In contrast, synthetic data augmentation substantially improved melanoma detection with 8-15% absolute gains in melanoma F1-score, and ViT-B/16 achieving F1~0.88 and ROC-AUC~0.98, representing an improvement of approximately 14% over non-augmented baselines. Our code can be found at https://github.com/adarsh-crafts/SkinGenBench
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAD-OOD: A Deep Learning Cluster-Driven Framework for an Out-of-Distribution Malware Detection and Classification</title>
<link>https://arxiv.org/abs/2512.17594</link>
<guid>https://arxiv.org/abs/2512.17594</guid>
<content:encoded><![CDATA[
arXiv:2512.17594v1 Announce Type: cross 
Abstract: Out of distribution (OOD) detection remains a critical challenge in malware classification due to the substantial intra family variability introduced by polymorphic and metamorphic malware variants. Most existing deep learning based malware detectors rely on closed world assumptions and fail to adequately model this intra class variation, resulting in degraded performance when confronted with previously unseen malware families. This paper presents MADOOD, a novel two stage, cluster driven deep learning framework for robust OOD malware detection and classification. In the first stage, malware family embeddings are modeled using class conditional spherical decision boundaries derived from Gaussian Discriminant Analysis (GDA), enabling statistically grounded separation of indistribution and OOD samples without requiring OOD data during training. Z score based distance analysis across multiple class centroids is employed to reliably identify anomalous samples in the latent space. In the second stage, a deep neural network integrates cluster based predictions, refined embeddings, and supervised classifier outputs to enhance final classification accuracy. Extensive evaluations on benchmark malware datasets comprising 25 known families and multiple novel OOD variants demonstrate that MADOOD significantly outperforms state of the art OOD detection methods, achieving an AUC of up to 0.911 on unseen malware families. The proposed framework provides a scalable, interpretable, and statistically principled solution for real world malware detection and anomaly identification in evolving cybersecurity environments.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Confidence-Credibility Aware Weighted Ensembles of Small LLMs Outperform Large LLMs in Emotion Detection</title>
<link>https://arxiv.org/abs/2512.17630</link>
<guid>https://arxiv.org/abs/2512.17630</guid>
<content:encoded><![CDATA[
arXiv:2512.17630v1 Announce Type: cross 
Abstract: This paper introduces a confidence-weighted, credibility-aware ensemble framework for text-based emotion detection, inspired by Condorcet's Jury Theorem (CJT). Unlike conventional ensembles that often rely on homogeneous architectures, our approach combines architecturally diverse small transformer-based large language models (sLLMs) - BERT, RoBERTa, DistilBERT, DeBERTa, and ELECTRA, each fully fine-tuned for emotion classification. To preserve error diversity, we minimize parameter convergence while taking advantage of the unique biases of each model. A dual-weighted voting mechanism integrates both global credibility (validation F1 score) and local confidence (instance-level probability) to dynamically weight model contributions. Experiments on the DAIR-AI dataset demonstrate that our credibility-confidence ensemble achieves a macro F1 score of 93.5 percent, surpassing state-of-the-art benchmarks and significantly outperforming large-scale LLMs, including Falcon, Mistral, Qwen, and Phi, even after task-specific Low-Rank Adaptation (LoRA). With only 595M parameters in total, our small LLMs ensemble proves more parameter-efficient and robust than models up to 7B parameters, establishing that carefully designed ensembles of small, fine-tuned models can outperform much larger LLMs in specialized natural language processing (NLP) tasks such as emotion detection.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Multi-Objective Bayesian Optimization with Scalable Batch Evaluations for Sample-Efficient De Novo Molecular Design</title>
<link>https://arxiv.org/abs/2512.17659</link>
<guid>https://arxiv.org/abs/2512.17659</guid>
<content:encoded><![CDATA[
arXiv:2512.17659v1 Announce Type: cross 
Abstract: Designing molecules that must satisfy multiple, often conflicting objectives is a central challenge in molecular discovery. The enormous size of chemical space and the cost of high-fidelity simulations have driven the development of machine learning-guided strategies for accelerating design with limited data. Among these, Bayesian optimization (BO) offers a principled framework for sample-efficient search, while generative models provide a mechanism to propose novel, diverse candidates beyond fixed libraries. However, existing methods that couple the two often rely on continuous latent spaces, which introduces both architectural entanglement and scalability challenges. This work introduces an alternative, modular "generate-then-optimize" framework for de novo multi-objective molecular design/discovery. At each iteration, a generative model is used to construct a large, diverse pool of candidate molecules, after which a novel acquisition function, qPMHI (multi-point Probability of Maximum Hypervolume Improvement), is used to optimally select a batch of candidates most likely to induce the largest Pareto front expansion. The key insight is that qPMHI decomposes additively, enabling exact, scalable batch selection via only simple ranking of probabilities that can be easily estimated with Monte Carlo sampling. We benchmark the framework against state-of-the-art latent-space and discrete molecular optimization methods, demonstrating significant improvements across synthetic benchmarks and application-driven tasks. Specifically, in a case study related to sustainable energy storage, we show that our approach quickly uncovers novel, diverse, and high-performing organic (quinone-based) cathode materials for aqueous redox flow battery applications.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fraud detection in credit card transactions using Quantum-Assisted Restricted Boltzmann Machines</title>
<link>https://arxiv.org/abs/2512.17660</link>
<guid>https://arxiv.org/abs/2512.17660</guid>
<content:encoded><![CDATA[
arXiv:2512.17660v1 Announce Type: cross 
Abstract: Use cases for emerging quantum computing platforms become economically relevant as the efficiency of processing and availability of quantum computers increase. We assess the performance of Restricted Boltzmann Machines (RBM) assisted by quantum computing, running on real quantum hardware and simulators, using a real dataset containing 145 million transactions provided by Stone, a leading Brazilian fintech, for credit card fraud detection. The results suggest that the quantum-assisted RBM method is able to achieve superior performance in most figures of merit in comparison to classical approaches, even using current noisy quantum annealers. Our study paves the way for implementing quantum-assisted RBMs for general fault detection in financial systems.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vidarc: Embodied Video Diffusion Model for Closed-loop Control</title>
<link>https://arxiv.org/abs/2512.17661</link>
<guid>https://arxiv.org/abs/2512.17661</guid>
<content:encoded><![CDATA[
arXiv:2512.17661v1 Announce Type: cross 
Abstract: Robotic arm manipulation in data-scarce settings is a highly challenging task due to the complex embodiment dynamics and diverse contexts. Recent video-based approaches have shown great promise in capturing and transferring the temporal and physical interactions by pre-training on Internet-scale video data. However, such methods are often not optimized for the embodiment-specific closed-loop control, typically suffering from high latency and insufficient grounding. In this paper, we present Vidarc (Video Diffusion for Action Reasoning and Closed-loop Control), a novel autoregressive embodied video diffusion approach augmented by a masked inverse dynamics model. By grounding video predictions with action-relevant masks and incorporating real-time feedback through cached autoregressive generation, Vidarc achieves fast, accurate closed-loop control. Pre-trained on one million cross-embodiment episodes, Vidarc surpasses state-of-the-art baselines, achieving at least a 15% higher success rate in real-world deployment and a 91% reduction in latency. We also highlight its robust generalization and error correction capabilities across previously unseen robotic platforms.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Imputation Uncertainty in Interpretable Machine Learning Methods</title>
<link>https://arxiv.org/abs/2512.17689</link>
<guid>https://arxiv.org/abs/2512.17689</guid>
<content:encoded><![CDATA[
arXiv:2512.17689v1 Announce Type: cross 
Abstract: In real data, missing values occur frequently, which affects the interpretation with interpretable machine learning (IML) methods. Recent work considers bias and shows that model explanations may differ between imputation methods, while ignoring additional imputation uncertainty and its influence on variance and confidence intervals. We therefore compare the effects of different imputation methods on the confidence interval coverage probabilities of the IML methods permutation feature importance, partial dependence plots and Shapley values. We show that single imputation leads to underestimation of variance and that, in most cases, only multiple imputation is close to nominal coverage.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting the Broken Symmetry Phase of Solid Hydrogen: A Neural Network Variational Monte Carlo Study</title>
<link>https://arxiv.org/abs/2512.17703</link>
<guid>https://arxiv.org/abs/2512.17703</guid>
<content:encoded><![CDATA[
arXiv:2512.17703v1 Announce Type: cross 
Abstract: The crystal structure of high-pressure solid hydrogen remains a fundamental open problem. Although the research frontier has mostly shifted toward ultra-high pressure phases above 400 GPa, we show that even the broken symmetry phase observed around 130~GPa requires revisiting due to its intricate coupling of electronic and nuclear degrees of freedom. Here, we develop a first principle quantum Monte Carlo framework based on a deep neural network wave function that treats both electrons and nuclei quantum mechanically within the constant pressure ensemble. Our calculations reveal an unreported ground-state structure candidate for the broken symmetry phase with $Cmcm$ space group symmetry, and we test its stability up to 96 atoms. The predicted structure quantitatively matches the experimental equation of state and X-ray diffraction patterns. Furthermore, our group-theoretical analysis shows that the $Cmcm$ structure is compatible with existing Raman and infrared spectroscopic data. Crucially, static density functional theory calculation reveals the $Cmcm$ structure as a dynamically unstable saddle point on the Born-Oppenheimer potential energy surface, demonstrating that a full quantum many-body treatment of the problem is necessary. These results shed new light on the phase diagram of high-pressure hydrogen and call for further experimental verifications.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breast Cancer Neoadjuvant Chemotherapy Treatment Response Prediction Using Aligned Longitudinal MRI and Clinical Data</title>
<link>https://arxiv.org/abs/2512.17759</link>
<guid>https://arxiv.org/abs/2512.17759</guid>
<content:encoded><![CDATA[
arXiv:2512.17759v1 Announce Type: cross 
Abstract: Aim: This study investigates treatment response prediction to neoadjuvant chemotherapy (NACT) in breast cancer patients, using longitudinal contrast-enhanced magnetic resonance images (CE-MRI) and clinical data. The goal is to develop machine learning (ML) models to predict pathologic complete response (PCR binary classification) and 5-year relapse-free survival status (RFS binary classification). Method: The proposed framework includes tumour segmentation, image registration, feature extraction, and predictive modelling. Using the image registration method, MRI image features can be extracted and compared from the original tumour site at different time points, therefore monitoring the intratumor changes during NACT process. Four feature extractors, including one radiomics and three deep learning-based (MedicalNet, Segformer3D, SAM-Med3D) were implemented and compared. In combination with three feature selection methods and four ML models, predictive models are built and compared. Results: The proposed image registration-based feature extraction consistently improves the predictive models. In the PCR and RFS classification tasks logistic regression model trained on radiomic features performed the best with an AUC of 0.88 and classification accuracy of 0.85 for PCR classification, and AUC of 0.78 and classification accuracy of 0.72 for RFS classification. Conclusions: It is evidenced that the image registration method has significantly improved performance in longitudinal feature learning in predicting PCR and RFS. The radiomics feature extractor is more effective than the pre-trained deep learning feature extractors, with higher performance and better interpretability.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedNeXt-v2: Scaling 3D ConvNeXts for Large-Scale Supervised Representation Learning in Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2512.17774</link>
<guid>https://arxiv.org/abs/2512.17774</guid>
<content:encoded><![CDATA[
arXiv:2512.17774v1 Announce Type: cross 
Abstract: Large-scale supervised pretraining is rapidly reshaping 3D medical image segmentation. However, existing efforts focus primarily on increasing dataset size and overlook the question of whether the backbone network is an effective representation learner at scale. In this work, we address this gap by revisiting ConvNeXt-based architectures for volumetric segmentation and introducing MedNeXt-v2, a compound-scaled 3D ConvNeXt that leverages improved micro-architecture and data scaling to deliver state-of-the-art performance. First, we show that routinely used backbones in large-scale pretraining pipelines are often suboptimal. Subsequently, we use comprehensive backbone benchmarking prior to scaling and demonstrate that stronger from scratch performance reliably predicts stronger downstream performance after pretraining. Guided by these findings, we incorporate a 3D Global Response Normalization module and use depth, width, and context scaling to improve our architecture for effective representation learning. We pretrain MedNeXt-v2 on 18k CT volumes and demonstrate state-of-the-art performance when fine-tuning across six challenging CT and MR benchmarks (144 structures), showing consistent gains over seven publicly released pretrained models. Beyond improvements, our benchmarking of these models also reveals that stronger backbones yield better results on similar data, representation scaling disproportionately benefits pathological segmentation, and that modality-specific pretraining offers negligible benefit once full finetuning is applied. In conclusion, our results establish MedNeXt-v2 as a strong backbone for large-scale supervised representation learning in 3D Medical Image Segmentation. Our code and pretrained models are made available with the official nnUNet repository at: https://www.github.com/MIC-DKFZ/nnUNet
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Domain-Aware Quantum Circuit for QML</title>
<link>https://arxiv.org/abs/2512.17800</link>
<guid>https://arxiv.org/abs/2512.17800</guid>
<content:encoded><![CDATA[
arXiv:2512.17800v1 Announce Type: cross 
Abstract: Designing parameterized quantum circuits (PQCs) that are expressive, trainable, and robust to hardware noise is a central challenge for quantum machine learning (QML) on noisy intermediate-scale quantum (NISQ) devices. We present a Domain-Aware Quantum Circuit (DAQC) that leverages image priors to guide locality-preserving encoding and entanglement via non-overlapping DCT-style zigzag windows. The design employs interleaved encode-entangle-train cycles, where entanglement is applied among qubits hosting neighboring pixels, aligned to device connectivity. This staged, locality-preserving information flow expands the effective receptive field without deep global mixing, enabling efficient use of limited depth and qubits. The design concentrates representational capacity on short-range correlations, reduces long-range two-qubit operations, and encourages stable optimization, thereby mitigating depth-induced and globally entangled barren-plateau effects. We evaluate DAQC on MNIST, FashionMNIST, and PneumoniaMNIST datasets. On quantum hardware, DAQC achieves performance competitive with strong classical baselines (e.g., ResNet-18/50, DenseNet-121, EfficientNet-B0) and substantially outperforming Quantum Circuit Search (QCS) baselines. To the best of our knowledge, DAQC, which uses a quantum feature extractor with only a linear classical readout (no deep classical backbone), currently achieves the best reported performance on real quantum hardware for QML-based image classification tasks. Code and pretrained models are available at: https://github.com/gurinder-hub/DAQC.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visually Prompted Benchmarks Are Surprisingly Fragile</title>
<link>https://arxiv.org/abs/2512.17875</link>
<guid>https://arxiv.org/abs/2512.17875</guid>
<content:encoded><![CDATA[
arXiv:2512.17875v1 Announce Type: cross 
Abstract: A key challenge in evaluating VLMs is testing models' ability to analyze visual content independently from their textual priors. Recent benchmarks such as BLINK probe visual perception through visual prompting, where questions about visual content are paired with coordinates to which the question refers, with the coordinates explicitly marked in the image itself. While these benchmarks are an important part of VLM evaluation, we find that existing models are surprisingly fragile to seemingly irrelevant details of visual prompting: simply changing a visual marker from red to blue can completely change rankings among models on a leaderboard. By evaluating nine commonly-used open- and closed-source VLMs on two visually prompted tasks, we demonstrate how details in benchmark setup, including visual marker design and dataset size, have a significant influence on model performance and leaderboard rankings. These effects can even be exploited to lift weaker models above stronger ones; for instance, slightly increasing the size of the visual marker results in open-source InternVL3-8B ranking alongside or better than much larger proprietary models like Gemini 2.5 Pro. We further show that low-level inference choices that are often ignored in benchmarking, such as JPEG compression levels in API calls, can also cause model lineup changes. These details have substantially larger impacts on visually prompted benchmarks than on conventional semantic VLM evaluations. To mitigate this instability, we curate existing datasets to create VPBench, a larger visually prompted benchmark with 16 visual marker variants. VPBench and additional analysis tools are released at https://lisadunlap.github.io/vpbench/.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning vertical coordinates via automatic differentiation of a dynamical core</title>
<link>https://arxiv.org/abs/2512.17877</link>
<guid>https://arxiv.org/abs/2512.17877</guid>
<content:encoded><![CDATA[
arXiv:2512.17877v1 Announce Type: cross 
Abstract: Terrain-following coordinates in atmospheric models often imprint their grid structure onto the solution, particularly over steep topography, where distorted coordinate layers can generate spurious horizontal and vertical motion. Standard formulations, such as hybrid or SLEVE coordinates, mitigate these errors by using analytic decay functions controlled by heuristic scale parameters that are typically tuned by hand and fixed a priori. In this work, we propose a framework to define a parametric vertical coordinate system as a learnable component within a differentiable dynamical core. We develop an end-to-end differentiable numerical solver for the two-dimensional non-hydrostatic Euler equations on an Arakawa C-grid, and introduce a NEUral Vertical Enhancement (NEUVE) terrain-following coordinate based on an integral transformed neural network that guarantees monotonicity. A key feature of our approach is the use of automatic differentiation to compute exact geometric metric terms, thereby eliminating truncation errors associated with finite-difference coordinate derivatives. By coupling simulation errors through the time integration to the parameterization, our formulation finds a grid structure optimized for both the underlying physics and numerics. Using several standard tests, we demonstrate that these learned coordinates reduce the mean squared error by a factor of 1.4 to 2 in non-linear statistical benchmarks, and eliminate spurious vertical velocity striations over steep topography.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RadarGen: Automotive Radar Point Cloud Generation from Cameras</title>
<link>https://arxiv.org/abs/2512.17897</link>
<guid>https://arxiv.org/abs/2512.17897</guid>
<content:encoded><![CDATA[
arXiv:2512.17897v1 Announce Type: cross 
Abstract: We present RadarGen, a diffusion model for synthesizing realistic automotive radar point clouds from multi-view camera imagery. RadarGen adapts efficient image-latent diffusion to the radar domain by representing radar measurements in bird's-eye-view form that encodes spatial structure together with radar cross section (RCS) and Doppler attributes. A lightweight recovery step reconstructs point clouds from the generated maps. To better align generation with the visual scene, RadarGen incorporates BEV-aligned depth, semantic, and motion cues extracted from pretrained foundation models, which guide the stochastic generation process toward physically plausible radar patterns. Conditioning on images makes the approach broadly compatible, in principle, with existing visual datasets and simulation frameworks, offering a scalable direction for multimodal generative simulation. Evaluations on large-scale driving data show that RadarGen captures characteristic radar measurement distributions and reduces the gap to perception models trained on real data, marking a step toward unified generative simulation across sensing modalities.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distributionally Robust Imitation Learning: Layered Control Architecture for Certifiable Autonomy</title>
<link>https://arxiv.org/abs/2512.17899</link>
<guid>https://arxiv.org/abs/2512.17899</guid>
<content:encoded><![CDATA[
arXiv:2512.17899v1 Announce Type: cross 
Abstract: Imitation learning (IL) enables autonomous behavior by learning from expert demonstrations. While more sample-efficient than comparative alternatives like reinforcement learning, IL is sensitive to compounding errors induced by distribution shifts. There are two significant sources of distribution shifts when using IL-based feedback laws on systems: distribution shifts caused by policy error and distribution shifts due to exogenous disturbances and endogenous model errors due to lack of learning. Our previously developed approaches, Taylor Series Imitation Learning (TaSIL) and $\mathcal{L}_1$ -Distributionally Robust Adaptive Control (\ellonedrac), address the challenge of distribution shifts in complementary ways. While TaSIL offers robustness against policy error-induced distribution shifts, \ellonedrac offers robustness against distribution shifts due to aleatoric and epistemic uncertainties. To enable certifiable IL for learned and/or uncertain dynamical systems, we formulate \textit{Distributionally Robust Imitation Policy (DRIP)} architecture, a Layered Control Architecture (LCA) that integrates TaSIL and~\ellonedrac. By judiciously designing individual layer-centric input and output requirements, we show how we can guarantee certificates for the entire control pipeline. Our solution paves the path for designing fully certifiable autonomy pipelines, by integrating learning-based components, such as perception, with certifiable model-based decision-making through the proposed LCA approach.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting</title>
<link>https://arxiv.org/abs/2512.17908</link>
<guid>https://arxiv.org/abs/2512.17908</guid>
<content:encoded><![CDATA[
arXiv:2512.17908v1 Announce Type: cross 
Abstract: Monocular depth estimation remains challenging as recent foundation models, such as Depth Anything V2 (DA-V2), struggle with real-world images that are far from the training distribution. We introduce Re-Depth Anything, a test-time self-supervision framework that bridges this domain gap by fusing DA-V2 with the powerful priors of large-scale 2D diffusion models. Our method performs label-free refinement directly on the input image by re-lighting predicted depth maps and augmenting the input. This re-synthesis method replaces classical photometric reconstruction by leveraging shape from shading (SfS) cues in a new, generative context with Score Distillation Sampling (SDS). To prevent optimization collapse, our framework employs a targeted optimization strategy: rather than optimizing depth directly or fine-tuning the full model, we freeze the encoder and only update intermediate embeddings while also fine-tuning the decoder. Across diverse benchmarks, Re-Depth Anything yields substantial gains in depth accuracy and realism over the DA-V2, showcasing new avenues for self-supervision by augmenting geometric reasoning.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feed Two Birds with One Scone: Exploiting Wild Data for Both Out-of-Distribution Generalization and Detection</title>
<link>https://arxiv.org/abs/2306.09158</link>
<guid>https://arxiv.org/abs/2306.09158</guid>
<content:encoded><![CDATA[
arXiv:2306.09158v2 Announce Type: replace 
Abstract: Modern machine learning models deployed in the wild can encounter both covariate and semantic shifts, giving rise to the problems of out-of-distribution (OOD) generalization and OOD detection respectively. While both problems have received significant research attention lately, they have been pursued independently. This may not be surprising, since the two tasks have seemingly conflicting goals. This paper provides a new unified approach that is capable of simultaneously generalizing to covariate shifts while robustly detecting semantic shifts. We propose a margin-based learning framework that exploits freely available unlabeled data in the wild that captures the environmental test-time OOD distributions under both covariate and semantic shifts. We show both empirically and theoretically that the proposed margin constraint is the key to achieving both OOD generalization and detection. Extensive experiments show the superiority of our framework, outperforming competitive baselines that specialize in either OOD generalization or OOD detection. Code is publicly available at https://github.com/deeplearning-wisc/scone.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse, Efficient and Explainable Data Attribution with DualXDA</title>
<link>https://arxiv.org/abs/2402.12118</link>
<guid>https://arxiv.org/abs/2402.12118</guid>
<content:encoded><![CDATA[
arXiv:2402.12118v3 Announce Type: replace 
Abstract: Data Attribution (DA) is an emerging approach in the field of eXplainable Artificial Intelligence (XAI), aiming to identify influential training datapoints which determine model outputs. It seeks to provide transparency about the model and individual predictions, e.g. for model debugging, identifying data-related causes of suboptimal performance. However, existing DA approaches suffer from prohibitively high computational costs and memory demands when applied to even medium-scale datasets and models, forcing practitioners to resort to approximations that may fail to capture the true inference process of the underlying model. Additionally, current attribution methods exhibit low sparsity, resulting in non-negligible attribution scores across a high number of training examples, hindering the discovery of decisive patterns in the data. In this work, we introduce DualXDA, a framework for sparse, efficient and explainable DA, comprised of two interlinked approaches, Dual Data Attribution (DualDA) and eXplainable Data Attribution (XDA): With DualDA, we propose a novel approach for efficient and effective DA, leveraging Support Vector Machine theory to provide fast and naturally sparse data attributions for AI predictions. In extensive quantitative analyses, we demonstrate that DualDA achieves high attribution quality, excels at solving a series of evaluated downstream tasks, while at the same time improving explanation time by a factor of up to 4,100,000x compared to the original Influence Functions method, and up to 11,000x compared to the method's most efficient approximation from literature to date. We further introduce XDA, a method for enhancing Data Attribution with capabilities from feature attribution methods to explain why training samples are relevant for the prediction of a test sample in terms of impactful features, which we showcase and verify qualitatively in detail.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HGQ: High Granularity Quantization for Real-time Neural Networks on FPGAs</title>
<link>https://arxiv.org/abs/2405.00645</link>
<guid>https://arxiv.org/abs/2405.00645</guid>
<content:encoded><![CDATA[
arXiv:2405.00645v3 Announce Type: replace 
Abstract: Neural networks with sub-microsecond inference latency are required by many critical applications. Targeting such applications deployed on FPGAs, we present High Granularity Quantization (HGQ), a quantization-aware training framework that optimizes parameter bit-widths through gradient descent. Unlike conventional methods, HGQ determines the optimal bit-width for each parameter independently, making it suitable for hardware platforms supporting heterogeneous arbitrary precision arithmetic. In our experiments, HGQ shows superior performance compared to existing network compression methods, achieving orders of magnitude reduction in resource consumption and latency while maintaining the accuracy on several benchmark tasks. These improvements enable the deployment of complex models previously infeasible due to resource or latency constraints. HGQ is open-source and is used for developing next-generation trigger systems at the CERN ATLAS and CMS experiments for particle physics, enabling the use of advanced machine learning models for real-time data selection with sub-microsecond latency.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Identification of Temporally Causal Representation with Instantaneous Dependence</title>
<link>https://arxiv.org/abs/2405.15325</link>
<guid>https://arxiv.org/abs/2405.15325</guid>
<content:encoded><![CDATA[
arXiv:2405.15325v3 Announce Type: replace 
Abstract: Temporally causal representation learning aims to identify the latent causal process from time series observations, but most methods require the assumption that the latent causal processes do not have instantaneous relations. Although some recent methods achieve identifiability in the instantaneous causality case, they require either interventions on the latent variables or grouping of the observations, which are in general difficult to obtain in real-world scenarios. To fill this gap, we propose an \textbf{ID}entification framework for instantane\textbf{O}us \textbf{L}atent dynamics (\textbf{IDOL}) by imposing a sparse influence constraint that the latent causal processes have sparse time-delayed and instantaneous relations. Specifically, we establish identifiability results of the latent causal process based on sufficient variability and the sparse influence constraint by employing contextual information of time series data. Based on these theories, we incorporate a temporally variational inference architecture to estimate the latent variables and a gradient-based sparsity regularization to identify the latent causal process. Experimental results on simulation datasets illustrate that our method can identify the latent causal process. Furthermore, evaluations on multiple human motion forecasting benchmarks with instantaneous dependencies indicate the effectiveness of our method in real-world settings.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Basis Selection: Low-Rank Decomposition of Pretrained Large Language Models for Target Applications</title>
<link>https://arxiv.org/abs/2405.15877</link>
<guid>https://arxiv.org/abs/2405.15877</guid>
<content:encoded><![CDATA[
arXiv:2405.15877v4 Announce Type: replace 
Abstract: Large language models (LLMs) significantly enhance the performance of various applications, but they are computationally intensive and energy-demanding. This makes it challenging to deploy them on devices with limited resources, such as personal computers and mobile/wearable devices, and results in substantial inference costs in resource-rich environments like cloud servers. To extend the use of LLMs, we introduce a low-rank decomposition approach to effectively compress these models, tailored to the requirements of specific applications. We observe that LLMs pretrained on general datasets contain many redundant components not needed for particular applications. Our method focuses on identifying and removing these redundant parts, retaining only the necessary elements for the target applications. Specifically, we represent the weight matrices of LLMs as a linear combination of base components. We then prune the irrelevant bases and enhance the model with new bases beneficial for specific applications. Deep compression results on the Llama 2-7b and -13B models, conducted on target applications including mathematical reasoning and code generation, show that our method significantly reduces model size while maintaining comparable accuracy to state-of-the-art low-rank compression techniques.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy Bias in Language Models: A Contextual Integrity-based Auditing Metric</title>
<link>https://arxiv.org/abs/2409.03735</link>
<guid>https://arxiv.org/abs/2409.03735</guid>
<content:encoded><![CDATA[
arXiv:2409.03735v3 Announce Type: replace 
Abstract: As large language models (LLMs) are integrated into sociotechnical systems, it is crucial to examine the privacy biases they exhibit. We define privacy bias as the appropriateness value of information flows in responses from LLMs. A deviation between privacy biases and expected values, referred to as privacy bias delta, may indicate privacy violations. As an auditing metric, privacy bias can help (a) model trainers evaluate the ethical and societal impact of LLMs, (b) service providers select context-appropriate LLMs, and (c) policymakers assess the appropriateness of privacy biases in deployed LLMs. We formulate and answer a novel research question: how can we reliably examine privacy biases in LLMs and the factors that influence them? We present a novel approach for assessing privacy biases using a contextual integrity-based methodology to evaluate the responses from various LLMs. Our approach accounts for the sensitivity of responses across prompt variations, which hinders the evaluation of privacy biases. Finally, we investigate how privacy biases are affected by model capacities and optimizations.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Low-Rank Filtering and Smoothing for Sequential Deep Learning</title>
<link>https://arxiv.org/abs/2410.06800</link>
<guid>https://arxiv.org/abs/2410.06800</guid>
<content:encoded><![CDATA[
arXiv:2410.06800v2 Announce Type: replace 
Abstract: Learning multiple tasks sequentially requires neural networks to balance retaining knowledge, yet being flexible enough to adapt to new tasks. Regularizing network parameters is a common approach, but it rarely incorporates prior knowledge about task relationships, and limits information flow to future tasks only. We propose a Bayesian framework that treats the network's parameters as the state space of a nonlinear Gaussian model, unlocking two key capabilities: (1) A principled way to encode domain knowledge about task relationships, allowing, e.g., control over which layers should adapt between tasks. (2) A novel application of Bayesian smoothing, allowing task-specific models to also incorporate knowledge from models learned later. This does not require direct access to their data, which is crucial, e.g., for privacy-critical applications. These capabilities rely on efficient filtering and smoothing operations, for which we propose diagonal plus low-rank approximations of the precision matrix in the Laplace approximation (LR-LGF). Empirical results demonstrate the efficiency of LR-LGF and the benefits of the unlocked capabilities.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Multimodal LLMs with Semantic Space Alignment for Enhanced Time Series Classification</title>
<link>https://arxiv.org/abs/2410.18686</link>
<guid>https://arxiv.org/abs/2410.18686</guid>
<content:encoded><![CDATA[
arXiv:2410.18686v2 Announce Type: replace 
Abstract: Time series classification plays a fundamental role in a wide range of real-world applications. Recently, large language models (LLMs) have demonstrated strong generalization and reasoning capacities, but directly applying them to time series classification remains non-trivial due to the representation gap between numerical sequences and linguistic semantics. In this paper, we propose HiTime, a hierarchical LLM-based framework for multimodal time series classification that bridges structured temporal representations with semantic reasoning in a generative paradigm. Specifically, we design a hierarchical sequence feature encoding module composed of a data-specific encoder and a task-specific encoder to extract complementary temporal features. To mitigate the embedding gap between time series representations and textual semantics, we further introduce a semantic space alignment module that jointly performs coarse-grained global modeling and fine-grained cross-modal correspondence. Building upon the above representations, we employ a parameter-efficient supervised fine-tuning strategy to activate the generative classification capability of the algined LLMs, thereby transforming conventional discriminative time series classification into a generative task. Extensive experiments on multiple benchmarks demonstrate that the proposed framework consistently outperforms state-of-the-art baselines. The code is publicly available at https://github.com/Xiaoyu-Tao/HiTime.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fairness via Independence: A (Conditional) Distance Covariance Framework</title>
<link>https://arxiv.org/abs/2412.00720</link>
<guid>https://arxiv.org/abs/2412.00720</guid>
<content:encoded><![CDATA[
arXiv:2412.00720v2 Announce Type: replace 
Abstract: We explore fairness from a statistical perspective by selectively utilizing either conditional distance covariance or distance covariance statistics as measures to assess the independence between predictions and sensitive attributes. We boost fairness with independence by adding a distance covariance-based penalty to the model's training. Additionally, we present the matrix form of empirical (conditional) distance covariance for parallel calculations to enhance computational efficiency. Theoretically, we provide a proof for the convergence between empirical and population (conditional) distance covariance, establishing necessary guarantees for batch computations. Through experiments conducted on a range of real-world datasets, we have demonstrated that our method effectively bridges the fairness gap in machine learning. Our code is available at \url{https://github.com/liuhaixias1/Fair_dc/}.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data for Mathematical Copilots: Better Ways of Presenting Proofs for Machine Learning</title>
<link>https://arxiv.org/abs/2412.15184</link>
<guid>https://arxiv.org/abs/2412.15184</guid>
<content:encoded><![CDATA[
arXiv:2412.15184v2 Announce Type: replace 
Abstract: The datasets and benchmarks commonly used to train and evaluate the mathematical capabilities of AI-based mathematical copilots (primarily large language models) exhibit several shortcomings and misdirections. These range from a restricted scope of mathematical complexity to limited fidelity in capturing aspects beyond the final, written proof (e.g. motivating the proof, or representing the thought processes leading to a proof). These issues are compounded by a dynamic reminiscent of Goodhart's law: as benchmark performance becomes the primary target for model development, the benchmarks themselves become less reliable indicators of genuine mathematical capability. We systematically explore these limitations and contend that enhancing the capabilities of large language models, or any forthcoming advancements in AI-based mathematical assistants (copilots or ``thought partners''), necessitates a course correction both in the design of mathematical datasets and the evaluation criteria of the models' mathematical ability. In particular, it is necessary for benchmarks to move beyond the existing result-based datasets that map theorem statements directly to proofs, and instead focus on datasets that translate the richer facets of mathematical research practice into data that LLMs can learn from. This includes benchmarks that supervise the proving process and the proof discovery process itself, and we advocate for mathematical dataset developers to consider the concept of "motivated proof", introduced by G. P\'olya in 1949, which can serve as a blueprint for datasets that offer a better proof learning signal, alleviating some of the mentioned limitations.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pairwise Elimination with Instance-Dependent Guarantees for Bandits with Cost Subsidy</title>
<link>https://arxiv.org/abs/2501.10290</link>
<guid>https://arxiv.org/abs/2501.10290</guid>
<content:encoded><![CDATA[
arXiv:2501.10290v3 Announce Type: replace 
Abstract: Multi-armed bandits (MAB) are commonly used in sequential online decision-making when the reward of each decision is an unknown random variable. In practice however, the typical goal of maximizing total reward may be less important than minimizing the total cost of the decisions taken, subject to a reward constraint. For example, we may seek to make decisions that have at least the reward of a reference ``default'' decision, with as low a cost as possible. This problem was recently introduced in the Multi-Armed Bandits with Cost Subsidy (MAB-CS) framework. MAB-CS is broadly applicable to problem domains where a primary metric (cost) is constrained by a secondary metric (reward), and the rewards are unknown. In our work, we address variants of MAB-CS including ones with reward constrained by the reward of a known reference arm or by the subsidized best reward. We introduce the Pairwise-Elimination (PE) algorithm for the known reference arm variant and generalize PE to PE-CS for the subsidized best reward variant. Our instance-dependent analysis of PE and PE-CS reveals that both algorithms have an order-wise logarithmic upper bound on Cost and Quality Regret, making our policies the first with such a guarantee. Moreover, by comparing our upper and lower bound results we establish that PE is order-optimal for all known reference arm problem instances. Finally, experiments are conducted using the MovieLens 25M and Goodreads datasets for both PE and PE-CS revealing the effectiveness of PE and the superior balance between performance and reliability offered by PE-CS compared to baselines from the literature.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Human-Guided, Data-Centric LLM Co-Pilots</title>
<link>https://arxiv.org/abs/2501.10321</link>
<guid>https://arxiv.org/abs/2501.10321</guid>
<content:encoded><![CDATA[
arXiv:2501.10321v3 Announce Type: replace 
Abstract: Machine learning (ML) has the potential to revolutionize various domains, but its adoption is often hindered by the disconnect between the needs of domain experts and translating these needs into robust and valid ML tools. Despite recent advances in LLM-based co-pilots to democratize ML for non-technical domain experts, these systems remain predominantly focused on model-centric aspects while overlooking critical data-centric challenges. This limitation is problematic in complex real-world settings where raw data often contains complex issues, such as missing values, label noise, and domain-specific nuances requiring tailored handling. To address this we introduce CliMB-DC, a human-guided, data-centric framework for LLM co-pilots that combines advanced data-centric tools with LLM-driven reasoning to enable robust, context-aware data processing. At its core, CliMB-DC introduces a novel, multi-agent reasoning system that combines a strategic coordinator for dynamic planning and adaptation with a specialized worker agent for precise execution. Domain expertise is then systematically incorporated to guide the reasoning process using a human-in-the-loop approach. To guide development, we formalize a taxonomy of key data-centric challenges that co-pilots must address. Thereafter, to address the dimensions of the taxonomy, we integrate state-of-the-art data-centric tools into an extensible, open-source architecture, facilitating the addition of new tools from the research community. Empirically, using real-world healthcare datasets we demonstrate CliMB-DC's ability to transform uncurated datasets into ML-ready formats, significantly outperforming existing co-pilot baselines for handling data-centric challenges. CliMB-DC promises to empower domain experts from diverse domains -- healthcare, finance, social sciences and more -- to actively participate in driving real-world impact using ML.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Regularized Langevin Dynamics for Combinatorial Optimization</title>
<link>https://arxiv.org/abs/2502.00277</link>
<guid>https://arxiv.org/abs/2502.00277</guid>
<content:encoded><![CDATA[
arXiv:2502.00277v4 Announce Type: replace 
Abstract: This work proposes a simple yet effective sampling framework for combinatorial optimization (CO). Our method builds on discrete Langevin dynamics (LD), an efficient gradient-guided generative paradigm. However, we observe that directly applying LD often leads to limited exploration. To overcome this limitation, we propose the Regularized Langevin Dynamics (RLD), which enforces an expected distance between the sampled and current solutions, effectively avoiding local minima. We develop two CO solvers on top of RLD, one based on simulated annealing (SA), and the other one based on neural network (NN). Empirical results on three classic CO problems demonstrate that both of our methods can achieve comparable or better performance against the previous state-of-the-art (SOTA) SA- and NN-based solvers. In particular, our SA algorithm reduces the runtime of the previous SOTA SA method by up to 80\%, while achieving equal or superior performance. In summary, RLD offers a promising framework for enhancing both traditional heuristics and NN models to solve CO problems. Our code is available at https://github.com/Shengyu-Feng/RLD4CO.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generating Samples to Probe Trained Models</title>
<link>https://arxiv.org/abs/2502.06658</link>
<guid>https://arxiv.org/abs/2502.06658</guid>
<content:encoded><![CDATA[
arXiv:2502.06658v3 Announce Type: replace 
Abstract: There is a growing need for investigating how machine learning models operate. With this work, we aim to understand trained machine learning models by questioning their data preferences. We propose a mathematical framework that allows us to probe trained models and identify their preferred samples in various scenarios including prediction-risky, parameter-sensitive, or model-contrastive samples. To showcase our framework, we pose these queries to a range of models trained on a range of classification and regression tasks, and receive answers in the form of generated data.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Agnostic PAC Learning in the Small Error Regime</title>
<link>https://arxiv.org/abs/2502.09496</link>
<guid>https://arxiv.org/abs/2502.09496</guid>
<content:encoded><![CDATA[
arXiv:2502.09496v2 Announce Type: replace 
Abstract: Binary classification in the classic PAC model exhibits a curious phenomenon: Empirical Risk Minimization (ERM) learners are suboptimal in the realizable case yet optimal in the agnostic case. Roughly speaking, this owes itself to the fact that non-realizable distributions $\mathcal{D}$ are simply more difficult to learn than realizable distributions -- even when one discounts a learner's error by $\mathrm{err}(h^*_{\mathcal{D}})$, the error of the best hypothesis in $\mathcal{H}$ for $\mathcal{D}$. Thus, optimal agnostic learners are permitted to incur excess error on (easier-to-learn) distributions $\mathcal{D}$ for which $\tau = \mathrm{err}(h^*_{\mathcal{D}})$ is small.
  Recent work of Hanneke, Larsen, and Zhivotovskiy (FOCS `24) addresses this shortcoming by including $\tau$ itself as a parameter in the agnostic error term. In this more fine-grained model, they demonstrate tightness of the error lower bound $\tau + \Omega \left(\sqrt{\frac{\tau (d + \log(1 / \delta))}{m}} + \frac{d + \log(1 / \delta)}{m} \right)$ in a regime where $\tau > d/m$, and leave open the question of whether there may be a higher lower bound when $\tau \approx d/m$, with $d$ denoting $\mathrm{VC}(\mathcal{H})$. In this work, we resolve this question by exhibiting a learner which achieves error $c \cdot \tau + O \left(\sqrt{\frac{\tau (d + \log(1 / \delta))}{m}} + \frac{d + \log(1 / \delta)}{m} \right)$ for a constant $c \leq 2.1$, thus matching the lower bound when $\tau \approx d/m$. Further, our learner is computationally efficient and is based upon careful aggregations of ERM classifiers, making progress on two other questions of Hanneke, Larsen, and Zhivotovskiy (FOCS `24). We leave open the interesting question of whether our approach can be refined to lower the constant from 2.1 to 1, which would completely settle the complexity of agnostic learning.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Preconditioned Inexact Stochastic ADMM for Deep Model</title>
<link>https://arxiv.org/abs/2502.10784</link>
<guid>https://arxiv.org/abs/2502.10784</guid>
<content:encoded><![CDATA[
arXiv:2502.10784v5 Announce Type: replace 
Abstract: The recent advancement of foundation models (FMs) has brought about a paradigm shift, revolutionizing various sectors worldwide. The popular optimizers used to train these models are stochastic gradient descent-based algorithms, which face inherent limitations, such as slow convergence and stringent assumptions for convergence. In particular, data heterogeneity arising from distributed settings poses significant challenges to their theoretical and numerical performance. This paper develops an algorithm, PISA (Preconditioned Inexact Stochastic Alternating Direction Method of Multipliers). Grounded in rigorous theoretical guarantees, the algorithm converges under the sole assumption of Lipschitz continuity of the gradient on a bounded region, thereby removing the need for other conditions commonly imposed by stochastic methods. This capability enables the proposed algorithm to tackle the challenge of data heterogeneity effectively. Moreover, the algorithmic architecture enables scalable parallel computing and supports various preconditions, such as second-order information, second moment, and orthogonalized momentum by Newton-Schulz iterations. Incorporating the latter two preconditions in PISA yields two computationally efficient variants: SISA and NSISA. Comprehensive experimental evaluations for training or fine-tuning diverse deep models, including vision models, large language models, reinforcement learning models, generative adversarial networks, and recurrent neural networks, demonstrate superior numerical performance of SISA and NSISA compared to various state-of-the-art optimizers.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Effect of Sampling Diversity in Scaling LLM Inference</title>
<link>https://arxiv.org/abs/2502.11027</link>
<guid>https://arxiv.org/abs/2502.11027</guid>
<content:encoded><![CDATA[
arXiv:2502.11027v4 Announce Type: replace 
Abstract: Large language model (LLM) scaling inference is key to unlocking greater performance, and leveraging diversity has proven an effective way to enhance it. Motivated by the observed relationship between solution accuracy and meaningful response diversity, we systematically study the effect of prompt diversity in scaling inference. We theoretically explain why diversified sampling improves Best-of-N scaling, showing that responses generated from diverse prompts after Best-of-N selection exhibit significantly lower error rates than those produced from stationary prompts. Building on this analysis, we derive a diversity-fidelity trade-off principle, that guides the design of sampling strategies introducing diversity. From this guidance, we instantiate a family of effective perturbation styles. We theoretically and empirically characterize when diversified exploration remains effective, demonstrating that it works under a variety of conditions, and we further show that under majority voting, diversity may vanish. Finally, we systematically evaluate how effective sampling diversity is and show that, when applied appropriately in different contexts, it yields relative gains of 10.8% in EM@100 for reasoning, 9.6% for mathematics, and 9.5% in Pass@100 for code generation. Overall, this work provides a systematic analysis that offers a theoretical and empirical foundation for understanding how sampling diversity affects LLM inference-time scaling.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How to use score-based diffusion in earth system science: A satellite nowcasting example</title>
<link>https://arxiv.org/abs/2505.10432</link>
<guid>https://arxiv.org/abs/2505.10432</guid>
<content:encoded><![CDATA[
arXiv:2505.10432v2 Announce Type: replace 
Abstract: Machine learning (ML) is used for many earth science applications; however, traditional ML methods trained with squared errors often create blurry forecasts. Diffusion models are an emerging generative ML technique with the ability to produce sharper, more realistic images by learning the underlying data distribution. Diffusion models are becoming more prevalent, yet adapting them for earth science applications can be challenging because most articles focus on theoretical aspects of the approach, rather than making the method widely accessible. This work illustrates score-based diffusion models with a well-known problem in atmospheric science: cloud nowcasting (zero-to-three-hour forecast). After discussing the background and intuition of score-based diffusion models using examples from geostationary satellite infrared imagery, we experiment with three types of diffusion models: a standard score-based diffusion model (Diff); a residual correction diffusion model (CorrDiff); and a latent diffusion model (LDM). Our results show that the diffusion models not only advect existing clouds, but also generate and decay clouds, including convective initiation. A case study qualitatively shows the preservation of high-resolution features longer into the forecast than a conventional U-Net. The best of the three diffusion models tested was the CorrDiff approach, outperforming all other diffusion models, the conventional U-Net, and persistence. The diffusion models also enable out-of-the-box ensemble generation with skillful calibration. By explaining and exploring diffusion models for a common problem and ending with lessons learned from adapting diffusion models for our task, this work provides a starting point for the community to utilize diffusion models for a variety of earth science applications.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PEAR: Equal Area Weather Forecasting on the Sphere</title>
<link>https://arxiv.org/abs/2505.17720</link>
<guid>https://arxiv.org/abs/2505.17720</guid>
<content:encoded><![CDATA[
arXiv:2505.17720v2 Announce Type: replace 
Abstract: Artificial intelligence is rapidly reshaping the natural sciences, with weather forecasting emerging as a flagship AI4Science application where machine learning models can now rival and even surpass traditional numerical simulations. Following the success of the landmark models Pangu Weather and Graphcast, outperforming traditional numerical methods for global medium-range forecasting, many novel data-driven methods have emerged. A common limitation shared by many of these models is their reliance on an equiangular discretization of the sphere which suffers from a much finer grid at the poles than around the equator. In contrast, in the Hierarchical Equal Area iso-Latitude Pixelization (HEALPix) of the sphere, each pixel covers the same surface area, removing unphysical biases. Motivated by a growing support for this grid in meteorology and climate sciences, we propose to perform weather forecasting with deep learning models which natively operate on the HEALPix grid. To this end, we introduce Pangu Equal ARea (PEAR), a transformer-based weather forecasting model which operates directly on HEALPix-features and outperforms the corresponding model on an equiangular grid without any computational overhead.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Train Sparse Autoencoders Efficiently by Utilizing Features Correlation</title>
<link>https://arxiv.org/abs/2505.22255</link>
<guid>https://arxiv.org/abs/2505.22255</guid>
<content:encoded><![CDATA[
arXiv:2505.22255v2 Announce Type: replace 
Abstract: Sparse Autoencoders (SAEs) have demonstrated significant promise in interpreting the hidden states of language models by decomposing them into interpretable latent directions. However, training and interpreting SAEs at scale remains challenging, especially when large dictionary sizes are used. While decoders can leverage sparse-aware kernels for efficiency, encoders still require computationally intensive linear operations with large output dimensions. To address this, we propose KronSAE, a novel architecture that factorizes the latent representation via Kronecker product decomposition, drastically reducing memory and computational overhead. Furthermore, we introduce mAND, a differentiable activation function approximating the binary AND operation, which improves interpretability and performance in our factorized framework.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Certified Unlearning Approach without Access to Source Data</title>
<link>https://arxiv.org/abs/2506.06486</link>
<guid>https://arxiv.org/abs/2506.06486</guid>
<content:encoded><![CDATA[
arXiv:2506.06486v3 Announce Type: replace 
Abstract: With the growing adoption of data privacy regulations, the ability to erase private or copyrighted information from trained models has become a crucial requirement. Traditional unlearning methods often assume access to the complete training dataset, which is unrealistic in scenarios where the source data is no longer available. To address this challenge, we propose a certified unlearning framework that enables effective data removal \final{without access to the original training data samples}. Our approach utilizes a surrogate dataset that approximates the statistical properties of the source data, allowing for controlled noise scaling based on the statistical distance between the two. \updated{While our theoretical guarantees assume knowledge of the exact statistical distance, practical implementations typically approximate this distance, resulting in potentially weaker but still meaningful privacy guarantees.} This ensures strong guarantees on the model's behavior post-unlearning while maintaining its overall utility. We establish theoretical bounds, introduce practical noise calibration techniques, and validate our method through extensive experiments on both synthetic and real-world datasets. The results demonstrate the effectiveness and reliability of our approach in privacy-sensitive settings.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Diffusion Duality</title>
<link>https://arxiv.org/abs/2506.10892</link>
<guid>https://arxiv.org/abs/2506.10892</guid>
<content:encoded><![CDATA[
arXiv:2506.10892v3 Announce Type: replace 
Abstract: Uniform-state discrete diffusion models hold the promise of fast text generation due to their inherent ability to self-correct. However, they are typically outperformed by autoregressive models and masked diffusion models. In this work, we narrow this performance gap by leveraging a key insight: Uniform-state diffusion processes naturally emerge from an underlying Gaussian diffusion. Our method, Duo, transfers powerful techniques from Gaussian diffusion to improve both training and sampling. First, we introduce a curriculum learning strategy guided by the Gaussian process, doubling training speed by reducing variance. Models trained with curriculum learning surpass autoregressive models in zero-shot perplexity on 3 of 7 benchmarks. Second, we present Discrete Consistency Distillation, which adapts consistency distillation from the continuous to the discrete setting. This algorithm unlocks few-step generation in diffusion language models by accelerating sampling by two orders of magnitude. We provide the code, model checkpoints, and video tutorials on the project page: http://s-sahoo.github.io/duo
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Representation Learning and Fusion</title>
<link>https://arxiv.org/abs/2506.20494</link>
<guid>https://arxiv.org/abs/2506.20494</guid>
<content:encoded><![CDATA[
arXiv:2506.20494v2 Announce Type: replace 
Abstract: Multi-modal learning is a fast growing area in artificial intelligence. It tries to help machines understand complex things by combining information from different sources, like images, text, and audio. By using the strengths of each modality, multi-modal learning allows AI systems to build stronger and richer internal representations. These help machines better interpretation, reasoning, and making decisions in real-life situations. This field includes core techniques such as representation learning (to get shared features from different data types), alignment methods (to match information across modalities), and fusion strategies (to combine them by deep learning models). Although there has been good progress, some major problems still remain. Like dealing with different data formats, missing or incomplete inputs, and defending against adversarial attacks. Researchers now are exploring new methods, such as unsupervised or semi-supervised learning, AutoML tools, to make models more efficient and easier to scale. And also more attention on designing better evaluation metrics or building shared benchmarks, make it easier to compare model performance across tasks and domains. As the field continues to grow, multi-modal learning is expected to improve many areas: computer vision, natural language processing, speech recognition, and healthcare. In the future, it may help to build AI systems that can understand the world in a way more like humans, flexible, context aware, and able to deal with real-world complexity.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The kernel of graph indices for vector search</title>
<link>https://arxiv.org/abs/2506.20584</link>
<guid>https://arxiv.org/abs/2506.20584</guid>
<content:encoded><![CDATA[
arXiv:2506.20584v2 Announce Type: replace 
Abstract: The most popular graph indices for vector search use principles from computational geometry to build the graph. Hence, their formal graph navigability guarantees are only valid in Euclidean space. In this work, we show that machine learning can be used to build graph indices for vector search in metric and non-metric vector spaces (e.g., for inner product similarity). From this novel perspective, we introduce the Support Vector Graph (SVG), a new type of graph index that leverages kernel methods to establish the graph connectivity and that comes with formal navigability guarantees valid in metric and non-metric vector spaces. In addition, we interpret the most popular graph indices, including HNSW and DiskANN, as particular specializations of SVG and show that new navigable indices can be derived from the principles behind this specialization. Finally, we propose SVG-L0 that incorporates an $\ell_0$ sparsity constraint into the SVG kernel method to build graphs with a bounded out-degree. This yields a principled way of implementing this practical requirement, in contrast to the traditional heuristic of simply truncating the out edges of each node. Additionally, we show that SVG-L0 has a self-tuning property that avoids the heuristic of using a set of candidates to find the out-edges of each node and that keeps its computational complexity in check.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OptScale: Probabilistic Optimality for Inference-time Scaling</title>
<link>https://arxiv.org/abs/2506.22376</link>
<guid>https://arxiv.org/abs/2506.22376</guid>
<content:encoded><![CDATA[
arXiv:2506.22376v4 Announce Type: replace 
Abstract: Inference-time scaling has emerged as a powerful technique for enhancing the reasoning performance of Large Language Models (LLMs). However, existing approaches often rely on heuristic strategies for parallel sampling, lacking a principled foundation. To address this gap, we propose a probabilistic framework that formalizes the optimality of inference-time scaling under the assumption that parallel samples are independently and identically distributed (i.i.d.), and where the Best-of-$N$ selection strategy follows a probability distribution that can be estimated. Within this framework, we derive a theoretical lower bound on the required number of samples to achieve a target performance level, providing the first principled guidance for compute-efficient scaling. Leveraging this insight, we develop \textsc{OptScale}, a practical algorithm that dynamically determines the optimal number of sampled responses. \textsc{OptScale} employs a language model-based predictor to estimate probabilistic prior parameters, enabling the decision of the minimal number of samples needed that satisfy predefined performance thresholds and confidence levels. Extensive experiments on representative reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC) demonstrate that \textsc{OptScale} significantly reduces sampling overhead while remaining better or on par with state-of-the-art reasoning performance. Our work offers both a theoretical foundation and a practical solution for principled inference-time scaling, addressing a critical gap in the efficient deployment of LLMs for complex reasoning.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Boosting Revisited: Benchmarking and Advancing LP-Based Ensemble Methods</title>
<link>https://arxiv.org/abs/2507.18242</link>
<guid>https://arxiv.org/abs/2507.18242</guid>
<content:encoded><![CDATA[
arXiv:2507.18242v3 Announce Type: replace 
Abstract: Despite their theoretical appeal, totally corrective boosting methods based on linear programming have received limited empirical attention. In this paper, we conduct the first large-scale experimental study of six LP-based boosting formulations, including two novel methods, NM-Boost and QRLP-Boost, across 20 diverse datasets. We evaluate the use of both heuristic and optimal base learners within these formulations, and analyze not only accuracy, but also ensemble sparsity, margin distribution, anytime performance, and hyperparameter sensitivity. We show that totally corrective methods can outperform or match state-of-the-art heuristics like XGBoost and LightGBM when using shallow trees, while producing significantly sparser ensembles. We further show that these methods can thin pre-trained ensembles without sacrificing performance, and we highlight both the strengths and limitations of using optimal decision trees in this context.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MolMark: Safeguarding Molecular Structures through Learnable Atom-Level Watermarking</title>
<link>https://arxiv.org/abs/2508.17702</link>
<guid>https://arxiv.org/abs/2508.17702</guid>
<content:encoded><![CDATA[
arXiv:2508.17702v2 Announce Type: replace 
Abstract: AI-driven molecular generation is reshaping drug discovery and materials design, yet the lack of protection mechanisms leaves AI-generated molecules vulnerable to unauthorized reuse and provenance ambiguity. Such limitation undermines both scientific reproducibility and intellectual property security. To address this challenge, we propose the first deep learning based watermarking framework for molecules (MolMark), which is exquisitely designed to embed high-fidelity digital signatures into molecules without compromising molecular functionalities. MolMark learns to modulate the chemically meaningful atom-level representations and enforce geometric robustness through SE(3)-invariant features, maintaining robustness under rotation, translation, and reflection. Additionally, MolMark integrates seamlessly with AI-based molecular generative models, enabling watermarking to be treated as a learned transformation with minimal interference to molecular structures. Experiments on benchmark datasets (QM9, GEOM-DRUG) and state-of-the-art molecular generative models (GeoBFN, GeoLDM) demonstrate that MolMark can embed 16-bit watermarks while retaining more than 90% of essential molecular properties, preserving downstream performance, and enabling >95% extraction accuracy under SE(3) transformations. MolMark establishes a principled pathway for unifying molecular generation with verifiable authorship, supporting trustworthy and accountable AI-driven molecular discovery.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-Distilled Heterogeneous Federated Learning with Adaptive Margins for Trainable Global Prototypes</title>
<link>https://arxiv.org/abs/2508.19009</link>
<guid>https://arxiv.org/abs/2508.19009</guid>
<content:encoded><![CDATA[
arXiv:2508.19009v3 Announce Type: replace 
Abstract: Heterogeneous Federated Learning (HFL) has gained significant attention for its capacity to handle both model and data heterogeneity across clients. Prototype-based HFL methods emerge as a promising solution to address statistical and model heterogeneity as well as privacy challenges, paving the way for new advancements in HFL research. This method focuses on sharing class-representative prototypes among heterogeneous clients. However, aggregating these prototypes via standard weighted averaging often yields sub-optimal global knowledge. Specifically, the averaging approach induces a shrinking of the aggregated prototypes' decision margins, thereby degrading model performance in scenarios with model heterogeneity and non-IID data distributions. The propose FedProtoKD in a Heterogeneous Federated Learning setting, utilizing an enhanced dual-knowledge distillation mechanism to enhance system performance by leveraging clients' logits and prototype feature representations. The proposed framework aims to resolve the prototype margin-shrinking problem using a contrastive learning-based trainable server prototype by leveraging a class-wise adaptive prototype margin. Furthermore, the framework assess the importance of public samples using the closeness of the sample's prototype to its class representative prototypes, which enhances learning performance. FedProtoKD improved test accuracy by an average of 1.13% and up to 34.13% across various settings, significantly outperforming existing state-of-the-art HFL methods.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STDiff: A State Transition Diffusion Framework for Time Series Imputation in Industrial Systems</title>
<link>https://arxiv.org/abs/2508.19011</link>
<guid>https://arxiv.org/abs/2508.19011</guid>
<content:encoded><![CDATA[
arXiv:2508.19011v3 Announce Type: replace 
Abstract: Incomplete sensor data is a major obstacle in industrial time-series analytics. In wastewater treatment plants (WWTPs), key sensors show long, irregular gaps caused by fouling, maintenance, and outages. We introduce STDiff and STDiff-W, diffusion-based imputers that cast gap filling as state-space simulation under partial observability, where targets, controls, and exogenous signals may all be intermittently missing. STDiff learns a one-step transition model conditioned on observed values and masks, while STDiff-W extends this with a context encoder that jointly inpaints contiguous blocks, combining long-range consistency with short-term detail. On two WWTP datasets (one with synthetic block gaps from Agtrup and another with natural outages from Aved{\o}re), STDiff-W achieves state-of-the-art accuracy compared with strong neural baselines such as SAITS, BRITS, and CSDI. Beyond point-error metrics, its reconstructions preserve realistic dynamics including oscillations, spikes, and regime shifts, and they achieve top or tied-top downstream one-step forecasting performance compared with strong neural baselines, indicating that preserving dynamics does not come at the expense of predictive utility. Ablation studies that drop, shuffle, or add noise to control or exogenous inputs consistently degrade NH4 and PO4 performance, with the largest deterioration observed when exogenous signals are removed, showing that the model captures meaningful dependencies. We conclude with practical guidance for deployment: evaluate performance beyond MAE using task-oriented and visual checks, include exogenous drivers, and balance computational cost against robustness to structured outages.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EEGDM: Learning EEG Representation with Latent Diffusion Model</title>
<link>https://arxiv.org/abs/2508.20705</link>
<guid>https://arxiv.org/abs/2508.20705</guid>
<content:encoded><![CDATA[
arXiv:2508.20705v2 Announce Type: replace 
Abstract: Recent advances in self-supervised learning for EEG representation have largely relied on masked reconstruction, where models are trained to recover randomly masked signal segments. While effective at modeling local dependencies, such objectives are inherently limited in capturing the global dynamics and long-range dependencies essential for characterizing neural activity. To address this limitation, we propose EEGDM, a novel self-supervised framework that leverages latent diffusion models to generate EEG signals as an objective. Unlike masked reconstruction, diffusion-based generation progressively denoises signals from noise to realism, compelling the model to capture holistic temporal patterns and cross-channel relationships. Specifically, EEGDM incorporates an EEG encoder that distills raw signals and their channel augmentations into a compact representation, acting as conditional information to guide the diffusion model for generating EEG signals. This design endows EEGDM with a compact latent space, which not only offers ample control over the generative process but also can be leveraged for downstream tasks. Experimental results show that EEGDM (1) reconstructs high-quality EEG signals, (2) learns robust representations, and (3) achieves competitive performance across diverse downstream tasks, thus exploring a new direction for self-supervised EEG representation learning.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-Free Continual Learning of Server Models in Model-Heterogeneous Cloud-Device Collaboration</title>
<link>https://arxiv.org/abs/2509.25977</link>
<guid>https://arxiv.org/abs/2509.25977</guid>
<content:encoded><![CDATA[
arXiv:2509.25977v2 Announce Type: replace 
Abstract: The rise of cloud-device collaborative computing has enabled intelligent services to be delivered across distributed edge devices while leveraging centralized cloud resources. In this paradigm, federated learning (FL) has become a key enabler for privacy-preserving model training without transferring raw data from edge devices to the cloud. However, with the continuous emergence of new data and increasing model diversity, traditional federated learning faces significant challenges, including inherent issues of data heterogeneity, model heterogeneity and catastrophic forgetting, along with new challenge of knowledge misalignment. In this study, we introduce FedDCL, a novel framework designed to enable data-free continual learning of the server model in a model-heterogeneous federated setting. We leverage pre-trained diffusion models to extract lightweight class-specific prototypes, which confer a threefold data-free advantage, enabling: (1) generation of synthetic data for the current task to augment training and counteract non-IID data distributions; (2) exemplar-free generative replay for retaining knowledge from previous tasks; and (3) data-free dynamic knowledge transfer from heterogeneous devices to the cloud server.Experimental results on various datasets demonstrate the effectiveness of FedDCL, showcasing its potential to enhance the generalizability and practical applicability of federated cloud-device collaboration in dynamic settings.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-Tuning Masked Diffusion for Provable Self-Correction</title>
<link>https://arxiv.org/abs/2510.01384</link>
<guid>https://arxiv.org/abs/2510.01384</guid>
<content:encoded><![CDATA[
arXiv:2510.01384v3 Announce Type: replace 
Abstract: A natural desideratum for generative models is self-correction--detecting and revising low-quality tokens at inference. While Masked Diffusion Models (MDMs) have emerged as a promising approach for generative modeling in discrete spaces, their capacity for self-correction remains poorly understood. Prior attempts to incorporate self-correction into MDMs either require overhauling MDM architectures/training or rely on imprecise proxies for token quality, limiting their applicability. Motivated by this, we introduce PRISM--Plug-in Remasking for Inference-time Self-correction of Masked Diffusions--a lightweight, model-agnostic approach that applies to any pretrained MDM. Theoretically, PRISM defines a self-correction loss that provably learns per-token quality scores, without RL or a verifier. These quality scores are computed in the same forward pass with MDM and used to detect low-quality tokens. Empirically, PRISM advances MDM inference across domains and scales: Sudoku; unconditional text (170M); and code with LLaDA (8B).
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Generic Machine Learning Framework for Radio Frequency Fingerprinting</title>
<link>https://arxiv.org/abs/2510.09775</link>
<guid>https://arxiv.org/abs/2510.09775</guid>
<content:encoded><![CDATA[
arXiv:2510.09775v3 Announce Type: replace 
Abstract: Fingerprinting radio frequency (RF) emitters typically involves finding unique characteristics that are featured in their received signal. These fingerprints are nuanced, but sufficiently detailed, motivating the pursuit of methods that can successfully extract them. The downstream task that requires the most meticulous RF fingerprinting (RFF) is known as specific emitter identification (SEI), which entails recognising each individual transmitter. RFF and SEI have a long history, with numerous defence and civilian applications such as signal intelligence, electronic surveillance, physical-layer authentication of wireless devices, to name a few. In recent years, data-driven RFF approaches have become popular due to their ability to automatically learn intricate fingerprints. They generally deliver superior performance when compared to traditional RFF techniques that are often labour-intensive, inflexible, and only applicable to a particular emitter type or transmission scheme. In this paper, we present a generic and versatile machine learning (ML) framework for data-driven RFF with several popular downstream tasks such as SEI, data association (EDA) and RF emitter clustering (RFEC). It is emitter-type agnostic. We then demonstrate the introduced framework for several tasks using real RF datasets for spaceborne surveillance, signal intelligence and countering drones applications.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ASecond-Order SpikingSSM for Wearables</title>
<link>https://arxiv.org/abs/2510.14386</link>
<guid>https://arxiv.org/abs/2510.14386</guid>
<content:encoded><![CDATA[
arXiv:2510.14386v2 Announce Type: replace 
Abstract: Spiking neural networks have garnered increasing attention due to their energy efficiency, multiplication-free computation, and sparse event-based processing. In parallel, state space models have emerged as scalable alternatives to transformers for long-range sequence modelling by avoiding quadratic dependence on sequence length. We propose SHaRe-SSM (Spiking Harmonic Resonate-and-Fire State Space Model), a second-order spiking SSM for classification and regression on ultra-long sequences. SHaRe-SSM outperforms transformers and first-order SSMs on average while eliminating matrix multiplications, making it highly suitable for resource-constrained applications. To ensure fast computation over tens of thousands of time steps, we leverage a parallel scan formulation of the underlying dynamical system. Furthermore, we introduce a kernel-based spiking regressor, which enables the accurate modelling of dependencies in sequences of up to 50k steps. Our results demonstrate that SHaRe-SSM achieves superior long-range modelling capability with energy efficiency (52.1x less than ANN-based second order SSM), positioning it as a strong candidate for resource-constrained devices such as wearables
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Utility-Diversity Aware Online Batch Selection for LLM Supervised Fine-tuning</title>
<link>https://arxiv.org/abs/2510.16882</link>
<guid>https://arxiv.org/abs/2510.16882</guid>
<content:encoded><![CDATA[
arXiv:2510.16882v2 Announce Type: replace 
Abstract: Supervised fine-tuning (SFT) is a commonly used technique to adapt large language models (LLMs) to downstream tasks. In practice, SFT on a full dataset is computationally expensive and sometimes suffers from overfitting or bias amplification. This facilitates the rise of data curation in SFT, which prioritizes the most valuable data to optimze. This work studies the online batch selection family that dynamically scores and filters samples during the training process. However, existing popular methods often (i) rely merely on the utility of data to select a subset while neglecting other crucial factors like diversity, (ii) rely on external resources such as reference models or validation sets, and (iii) incur extra training time over full-dataset training. To address these limitations, this work develops \textbf{UDS (Utility-Diversity Sampling)}, a framework for efficient online batch selection in SFT. UDS leverages the nuclear norm of the logits matrix to capture both data utility and intra-sample diversity, while estimating inter-sample diversity through efficient low-dimensional embedding comparisons with a lightweight memory buffer of historical samples. Such a design eliminates the need for external resources and unnecessary backpropagation, securing computational efficiency. Experiments on multiple benchmarks demonstrate that UDS consistently outperforms state-of-the-art online batch selection methods under varying data budgets, and significantly reduces training time compared to full-dataset fine-tuning. Code is available at https://github.com/gfyddha/UDS.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing Structural Failure Before it Happens: An Image-Based Physics-Informed Neural Network (PINN) for Spaghetti Bridge Load Prediction</title>
<link>https://arxiv.org/abs/2510.23117</link>
<guid>https://arxiv.org/abs/2510.23117</guid>
<content:encoded><![CDATA[
arXiv:2510.23117v3 Announce Type: replace 
Abstract: Physics Informed Neural Networks (PINNs) are gaining attention for their ability to embed physical laws into deep learning models, which is particularly useful in structural engineering tasks with limited data. This paper aims to explore the use of PINNs to predict the weight of small scale spaghetti bridges, a task relevant to understanding load limits and potential failure modes in simplified structural models. Our proposed framework incorporates physics-based constraints to the prediction model for improved performance. In addition to standard PINNs, we introduce a novel architecture named Physics Informed Kolmogorov Arnold Network (PIKAN), which blends universal function approximation theory with physical insights. The structural parameters provided as input to the model are collected either manually or through computer vision methods. Our dataset includes 15 real bridges, augmented to 100 samples, and our best model achieves an $R^2$ score of 0.9603 and a mean absolute error (MAE) of 10.50 units. From applied perspective, we also provide a web based interface for parameter entry and prediction. These results show that PINNs can offer reliable estimates of structural weight, even with limited data, and may help inform early stage failure analysis in lightweight bridge designs.
  The complete data and code are available at https://github.com/OmerJauhar/PINNS-For-Spaghetti-Bridges.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Deep Physics-Informed Kolmogorov-Arnold Networks</title>
<link>https://arxiv.org/abs/2510.23501</link>
<guid>https://arxiv.org/abs/2510.23501</guid>
<content:encoded><![CDATA[
arXiv:2510.23501v2 Announce Type: replace 
Abstract: Since their introduction, Kolmogorov-Arnold Networks (KANs) have been successfully applied across several domains, with physics-informed machine learning (PIML) emerging as one of the areas where they have thrived. In the PIML setting, Chebyshev-based physics-informed KANs (cPIKANs) have become the standard due to their computational efficiency. However, like their multilayer perceptron-based counterparts, cPIKANs face significant challenges when scaled to depth, leading to training instabilities that limit their applicability to several PDE problems. To address this, we propose a basis-agnostic, Glorot-like initialization scheme that preserves activation variance and yields substantial improvements in stability and accuracy over the default initialization of cPIKANs. Inspired by the PirateNet architecture, we further introduce Residual-Gated Adaptive KANs (RGA KANs), designed to mitigate divergence in deep cPIKANs where initialization alone is not sufficient. Through empirical tests and information bottleneck analysis, we show that RGA KANs successfully traverse all training phases, unlike baseline cPIKANs, which stagnate in the diffusion phase in specific PDE settings. Evaluations on nine standard forward PDE benchmarks under a fixed training pipeline with adaptive components demonstrate that RGA KANs consistently outperform parameter-matched cPIKANs and PirateNets - often by several orders of magnitude - while remaining stable in settings where the others diverge.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semi-Supervised Preference Optimization with Limited Feedback</title>
<link>https://arxiv.org/abs/2511.00040</link>
<guid>https://arxiv.org/abs/2511.00040</guid>
<content:encoded><![CDATA[
arXiv:2511.00040v2 Announce Type: replace 
Abstract: The field of preference optimization has made outstanding contributions to the alignment of language models with human preferences. Despite these advancements, recent methods still rely heavily on substantial paired (labeled) feedback data, leading to substantial resource expenditures. To address these challenges, we study the problem of Semi-Supervised Preference Optimization (SSPO) in which the idea is to learn from both a small number of pairwise preference labels and a large pool of unpaired samples simultaneously. Our key theoretical contribution proves the existence of an optimal reward threshold capable of separating winning and losing responses with high probability, which enables a principled pseudo-labeling of unpaired data. By leveraging these pseudo-labels, SSPO effectively distills latent preferences from large-scale unpaired data, thus maintaining human alignment while drastically reducing acquisition costs. Extensive experiments across datasets validate this remarkable data efficiency; for instance, SSPO trained with Mistral-7B-Instruct on just 1% of UltraFeedback consistently surpasses strong baselines trained on 10% of UltraFeedback.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Causal Market Simulators</title>
<link>https://arxiv.org/abs/2511.04469</link>
<guid>https://arxiv.org/abs/2511.04469</guid>
<content:encoded><![CDATA[
arXiv:2511.04469v3 Announce Type: replace 
Abstract: Market generators using deep generative models have shown promise for synthetic financial data generation, but existing approaches lack causal reasoning capabilities essential for counterfactual analysis and risk assessment. We propose a Time-series Neural Causal Model VAE (TNCM-VAE) that combines variational autoencoders with structural causal models to generate counterfactual financial time series while preserving both temporal dependencies and causal relationships. Our approach enforces causal constraints through directed acyclic graphs in the decoder architecture and employs the causal Wasserstein distance for training. We validate our method on synthetic autoregressive models inspired by the Ornstein-Uhlenbeck process, demonstrating superior performance in counterfactual probability estimation with L1 distances as low as 0.03-0.10 compared to ground truth. The model enables financial stress testing, scenario analysis, and enhanced backtesting by generating plausible counterfactual market trajectories that respect underlying causal mechanisms.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Incremental Generation is Necessary and Sufficient for Universality in Flow-Based Modelling</title>
<link>https://arxiv.org/abs/2511.09902</link>
<guid>https://arxiv.org/abs/2511.09902</guid>
<content:encoded><![CDATA[
arXiv:2511.09902v2 Announce Type: replace 
Abstract: Incremental flow-based denoising models have reshaped generative modelling, but their empirical advantage still lacks a rigorous approximation-theoretic foundation. We show that incremental generation is necessary and sufficient for universal flow-based generation on the largest natural class of self-maps of $[0,1]^d$ compatible with denoising pipelines, namely the orientation-preserving homeomorphisms of $[0,1]^d$. All our guarantees are uniform on the underlying maps and hence imply approximation both samplewise and in distribution.
  Using a new topological-dynamical argument, we first prove an impossibility theorem: the class of all single-step autonomous flows, independently of the architecture, width, depth, or Lipschitz activation of the underlying neural network, is meagre and therefore not universal in the space of orientation-preserving homeomorphisms of $[0,1]^d$. By exploiting algebraic properties of autonomous flows, we conversely show that every orientation-preserving Lipschitz homeomorphism on $[0,1]^d$ can be approximated at rate $O(n^{-1/d})$ by a composition of at most $K_d$ such flows, where $K_d$ depends only on the dimension. Under additional smoothness assumptions, the approximation rate can be made dimension-free, and $K_d$ can be chosen uniformly over the class being approximated. Finally, by linearly lifting the domain into one higher dimension, we obtain structured universal approximation results for continuous functions and for probability measures on $[0,1]^d$, the latter realized as pushforwards of empirical measures with vanishing $1$-Wasserstein error.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Mixture of Block Attention</title>
<link>https://arxiv.org/abs/2511.11571</link>
<guid>https://arxiv.org/abs/2511.11571</guid>
<content:encoded><![CDATA[
arXiv:2511.11571v2 Announce Type: replace 
Abstract: Mixture of Block Attention (MoBA) (Lu et al., 2025) is a promising building block for efficiently processing long contexts in LLMs by enabling queries to sparsely attend to a small subset of key-value blocks, drastically reducing computational cost. However, the design principles governing MoBA's performance are poorly understood, and it lacks an efficient GPU implementation, hindering its practical adoption. In this paper, we first develop a statistical model to analyze MoBA's underlying mechanics. Our model reveals that performance critically depends on the router's ability to accurately distinguish relevant from irrelevant blocks based on query-key affinities. We derive a signal-to-noise ratio that formally connects architectural parameters to this retrieval accuracy. Guided by our analysis, we identify two key pathways for improvement: using smaller block sizes and applying a short convolution on keys to cluster relevant signals, which enhances routing accuracy. While theoretically better, small block sizes are inefficient on GPUs. To bridge this gap, we introduce FlashMoBA, a hardware-aware CUDA kernel that enables efficient MoBA execution even with the small block sizes our theory recommends. We validate our insights by training LLMs from scratch, showing that our improved MoBA models match the performance of dense attention baselines. FlashMoBA achieves up to 14.7x speedup over FlashAttention-2 for small blocks, making our theoretically-grounded improvements practical. Code is available at: https://github.com/mit-han-lab/flash-moba.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing Automated Fact-Checking for Medical LLM Responses with Knowledge Graphs</title>
<link>https://arxiv.org/abs/2511.12817</link>
<guid>https://arxiv.org/abs/2511.12817</guid>
<content:encoded><![CDATA[
arXiv:2511.12817v2 Announce Type: replace 
Abstract: The recent proliferation of large language models (LLMs) holds the potential to revolutionize healthcare, with strong capabilities in diverse medical tasks. Yet, deploying LLMs in high-stakes healthcare settings requires rigorous verification and validation to understand any potential harm. This paper investigates the reliability and viability of using medical knowledge graphs (KGs) for the automated factuality evaluation of LLM-generated responses. To ground this investigation, we introduce FAITH, a framework designed to systematically probe the strengths and limitations of this KG-based approach. FAITH operates without reference answers by decomposing responses into atomic claims, linking them to a medical KG, and scoring them based on evidence paths. Experiments on diverse medical tasks with human subjective evaluations demonstrate that KG-grounded evaluation achieves considerably higher correlations with clinician judgments and can effectively distinguish LLMs with varying capabilities. It is also robust to textual variances. The inherent explainability of its scoring can further help users understand and mitigate the limitations of current LLMs. We conclude that while limitations exist, leveraging KGs is a prominent direction for automated factuality assessment in healthcare.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Look-Ahead Reasoning on Learning Platforms</title>
<link>https://arxiv.org/abs/2511.14745</link>
<guid>https://arxiv.org/abs/2511.14745</guid>
<content:encoded><![CDATA[
arXiv:2511.14745v2 Announce Type: replace 
Abstract: On many learning platforms, the optimization criteria guiding model training reflect the priorities of the designer rather than those of the individuals they affect. Consequently, users may act strategically to obtain more favorable outcomes. While past work has studied strategic user behavior on learning platforms, the focus has largely been on strategic responses to a deployed model, without considering the behavior of other users. In contrast, look-ahead reasoning takes into account that user actions are coupled, and -- at scale -- impact future predictions. Within this framework, we first formalize level-k thinking, a concept from behavioral economics, where users aim to outsmart their peers by looking one step ahead. We show that, while convergence to an equilibrium is accelerated, the equilibrium remains the same, providing no benefit of higher-level reasoning for individuals in the long run. Then, we focus on collective reasoning, where users take coordinated actions by optimizing through their joint impact on the model. By contrasting collective with selfish behavior, we characterize the benefits and limits of coordination; a new notion of alignment between the learner's and the users' utilities emerges as a key concept. Look-ahead reasoning can be seen as a generalization of algorithmic collective action; we thus offer the first results characterizing the utility trade-offs of coordination when contesting algorithmic systems.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Gaussian Process Proximal Policy Optimization</title>
<link>https://arxiv.org/abs/2511.18214</link>
<guid>https://arxiv.org/abs/2511.18214</guid>
<content:encoded><![CDATA[
arXiv:2511.18214v2 Announce Type: replace 
Abstract: Uncertainty estimation for Reinforcement Learning (RL) is a critical component in control tasks where agents must balance safe exploration and efficient learning. While deep neural networks have enabled breakthroughs in RL, they often lack calibrated uncertainty estimates. We introduce Deep Gaussian Process Proximal Policy Optimization (GPPO), a scalable, model-free actor-critic algorithm that leverages Deep Gaussian Processes (DGPs) to approximate both the policy and value function. GPPO maintains competitive performance with respect to Proximal Policy Optimization on standard high-dimensional continuous control benchmarks while providing well-calibrated uncertainty estimates that can inform safer and more effective exploration.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>xGR: Efficient Generative Recommendation Serving at Scale</title>
<link>https://arxiv.org/abs/2512.11529</link>
<guid>https://arxiv.org/abs/2512.11529</guid>
<content:encoded><![CDATA[
arXiv:2512.11529v2 Announce Type: replace 
Abstract: Recommendation system delivers substantial economic benefits by providing personalized predictions. Generative recommendation (GR) integrates LLMs to enhance the understanding of long user-item sequences. Despite employing attention-based architectures, GR's workload differs markedly from that of LLM serving. GR typically processes long prompt while producing short, fixed-length outputs, yet the computational cost of each decode phase is especially high due to the large beam width. In addition, since the beam search involves a vast item space, the sorting overhead becomes particularly time-consuming. We propose xGR, a GR-oriented serving system that meets strict low-latency requirements under highconcurrency scenarios. First, xGR unifies the processing of prefill and decode phases through staged computation and separated KV cache. Second, xGR enables early sorting termination and mask-based item filtering with data structure reuse. Third, xGR reconstructs the overall pipeline to exploit multilevel overlap and multi-stream parallelism. Our experiments with real-world recommendation service datasets demonstrate that xGR achieves at least 3.49x throughput compared to the state-of-the-art baseline under strict latency constraints.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differentially private Bayesian tests</title>
<link>https://arxiv.org/abs/2401.15502</link>
<guid>https://arxiv.org/abs/2401.15502</guid>
<content:encoded><![CDATA[
arXiv:2401.15502v3 Announce Type: replace-cross 
Abstract: Differential privacy has emerged as an significant cornerstone in the realm of scientific hypothesis testing utilizing confidential data. In reporting scientific discoveries, Bayesian tests are widely adopted since they effectively circumnavigate the key criticisms of P-values, namely, lack of interpretability and inability to quantify evidence in support of the competing hypotheses. We present a novel differentially private Bayesian hypotheses testing framework that arise naturally under a principled data generative mechanism, inherently maintaining the interpretability of the resulting inferences. Furthermore, by focusing on differentially private Bayes factors based on widely used test statistics, we circumvent the need to model the complete data generative mechanism and ensure substantial computational benefits. We also provide a set of sufficient conditions to establish results on Bayes factor consistency under the proposed framework. The utility of the devised technology is showcased via several numerical experiments.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCAFFLSA: Taming Heterogeneity in Federated Linear Stochastic Approximation and TD Learning</title>
<link>https://arxiv.org/abs/2402.04114</link>
<guid>https://arxiv.org/abs/2402.04114</guid>
<content:encoded><![CDATA[
arXiv:2402.04114v3 Announce Type: replace-cross 
Abstract: In this paper, we analyze the sample and communication complexity of the federated linear stochastic approximation (FedLSA) algorithm. We explicitly quantify the effects of local training with agent heterogeneity. We show that the communication complexity of FedLSA scales polynomially with the inverse of the desired accuracy $\epsilon$. To overcome this, we propose SCAFFLSA a new variant of FedLSA that uses control variates to correct for client drift, and establish its sample and communication complexities. We show that for statistically heterogeneous agents, its communication complexity scales logarithmically with the desired accuracy, similar to Scaffnew. An important finding is that, compared to the existing results for Scaffnew, the sample complexity scales with the inverse of the number of agents, a property referred to as linear speed-up. Achieving this linear speed-up requires completely new theoretical arguments. We apply the proposed method to federated temporal difference learning with linear function approximation and analyze the corresponding complexity improvements.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adjusting Model Size in Continual Gaussian Processes: How Big is Big Enough?</title>
<link>https://arxiv.org/abs/2408.07588</link>
<guid>https://arxiv.org/abs/2408.07588</guid>
<content:encoded><![CDATA[
arXiv:2408.07588v5 Announce Type: replace-cross 
Abstract: Many machine learning models require setting a parameter that controls their size before training, e.g. number of neurons in DNNs, or inducing points in GPs. Increasing capacity typically improves performance until all the information from the dataset is captured. After this point, computational cost keeps increasing, without improved performance. This leads to the question "How big is big enough?" We investigate this problem for Gaussian processes (single-layer neural networks) in continual learning. Here, data becomes available incrementally, and the final dataset size will therefore not be known before training, preventing the use of heuristics for setting a fixed model size. We develop a method to automatically adjust model size while maintaining near-optimal performance. Our experimental procedure follows the constraint that any hyperparameters must be set without seeing dataset properties, and we show that our method performs well across diverse datasets without the need to adjust its hyperparameter, showing it requires less tuning than others.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-Perturbative Trivializing Flows for Lattice Gauge Theories</title>
<link>https://arxiv.org/abs/2410.13161</link>
<guid>https://arxiv.org/abs/2410.13161</guid>
<content:encoded><![CDATA[
arXiv:2410.13161v2 Announce Type: replace-cross 
Abstract: Continuous normalizing flows are known to be highly expressive and flexible, which allows for easier incorporation of large symmetries and makes them a powerful computational tool for lattice field theories. Building on previous work, we present a general continuous normalizing flow architecture for matrix Lie groups that is equivariant under group transformations. We apply this to lattice gauge theories in two dimensions as a proof of principle and demonstrate competitive performance, showing its potential as a tool for future lattice computations.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic PET Image Prediction Using a Network Combining Reversible and Irreversible Modules</title>
<link>https://arxiv.org/abs/2410.22674</link>
<guid>https://arxiv.org/abs/2410.22674</guid>
<content:encoded><![CDATA[
arXiv:2410.22674v2 Announce Type: replace-cross 
Abstract: Dynamic positron emission tomography (PET) images can reveal the distribution of tracers in the organism and the dynamic processes involved in biochemical reactions, and it is widely used in clinical practice. Despite the high effectiveness of dynamic PET imaging in studying the kinetics and metabolic processes of radiotracers. Pro-longed scan times can cause discomfort for both patients and medical personnel. This study proposes a dynamic frame prediction method for dynamic PET imaging, reduc-ing dynamic PET scanning time by applying a multi-module deep learning framework composed of reversible and irreversible modules. The network can predict kinetic parameter images based on the early frames of dynamic PET images, and then generate complete dynamic PET images. In validation experiments with simulated data, our network demonstrated good predictive performance for kinetic parameters and was able to reconstruct high-quality dynamic PET images. Additionally, in clinical data experiments, the network exhibited good generalization performance and attached that the proposed method has promising clinical application prospects.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Targeted Learning for Variable Importance</title>
<link>https://arxiv.org/abs/2411.02221</link>
<guid>https://arxiv.org/abs/2411.02221</guid>
<content:encoded><![CDATA[
arXiv:2411.02221v2 Announce Type: replace-cross 
Abstract: Variable importance is one of the most widely used measures for interpreting machine learning with significant interest from both statistics and machine learning communities. Recently, increasing attention has been directed toward uncertainty quantification in these metrics. Current approaches largely rely on one-step procedures, which, while asymptotically efficient, can present higher sensitivity and instability in finite sample settings. To address these limitations, we propose a novel method by employing the targeted learning (TL) framework, designed to enhance robustness in inference for variable importance metrics. Our approach is particularly suited for conditional permutation variable importance. We show that it (i) retains the asymptotic efficiency of traditional methods, (ii) maintains comparable computational complexity, and (iii) delivers improved accuracy, especially in finite sample contexts. We further support these findings with numerical experiments that illustrate the practical advantages of our method and validate the theoretical results.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Refined Analysis of Federated Averaging and Federated Richardson-Romberg</title>
<link>https://arxiv.org/abs/2412.01389</link>
<guid>https://arxiv.org/abs/2412.01389</guid>
<content:encoded><![CDATA[
arXiv:2412.01389v2 Announce Type: replace-cross 
Abstract: In this paper, we present a novel analysis of \FedAvg with constant step size, relying on the Markov property of the underlying process. We demonstrate that the global iterates of the algorithm converge to a stationary distribution and analyze its resulting bias and variance relative to the problem's solution. We provide a first-order bias expansion in both homogeneous and heterogeneous settings. Interestingly, this bias decomposes into two distinct components: one that depends solely on stochastic gradient noise and another on client heterogeneity. Finally, we introduce a new algorithm based on the Richardson-Romberg extrapolation technique to mitigate this bias.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Embedding-Driven Data Distillation for 360-Degree IQA With Residual-Aware Refinement</title>
<link>https://arxiv.org/abs/2412.12667</link>
<guid>https://arxiv.org/abs/2412.12667</guid>
<content:encoded><![CDATA[
arXiv:2412.12667v2 Announce Type: replace-cross 
Abstract: This article identifies and addresses a fundamental bottleneck in data-driven 360-degree image quality assessment (IQA): the lack of intelligent, sample-level data selection. Hence, we propose a novel framework that introduces a critical refinement step between patches sampling and model training. The core of our contribution is an embedding similarity-based selection algorithm that distills an initial, potentially redundant set of patches into a compact, maximally informative subset. This is formulated as a regularized optimization problem that preserves intrinsic perceptual relationships in a low-dimensional space, using residual analysis to explicitly filter out irrelevant or redundant samples. Extensive experiments on three benchmark datasets (CVIQ, OIQA, MVAQD) demonstrate that our selection enables a baseline model to match or exceed the performance of using all sampled data while keeping only 40-50% of patches. Particularly, we demonstrate the universal applicability of our approach by integrating it with several state-of-the-art IQA models, incleasy to deploy. Most significantly, its value as a generic,uding CNN- and transformer-based architectures, consistently enabling them to maintain or improve performance with 20-40\% reduced computational load. This work establishes that adaptive, post-sampling data refinement is a powerful and widely applicable strategy for achieving efficient and robust 360-degree IQA.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3D Cell Oversegmentation Correction via Geo-Wasserstein Divergence</title>
<link>https://arxiv.org/abs/2502.01890</link>
<guid>https://arxiv.org/abs/2502.01890</guid>
<content:encoded><![CDATA[
arXiv:2502.01890v4 Announce Type: replace-cross 
Abstract: 3D cell segmentation methods are often hindered by \emph{oversegmentation}, where a single cell is incorrectly split into multiple fragments. This degrades the final segmentation quality and is notoriously difficult to resolve, as oversegmentation errors often resemble natural gaps between adjacent cells. Our work makes two key contributions. First, for 3D cell segmentation, we are the first work to formulate oversegmentation as a concrete problem and propose a geometric framework to identify and correct these errors. Our approach builds a pre-trained classifier using both 2D geometric and 3D topological features extracted from flawed 3D segmentation results. Second, we introduce a novel metric, Geo-Wasserstein divergence, to quantify changes in 2D geometries. This captures the evolving trends of cell mask shape in a geometry-aware manner. We validate our method through extensive experiments on in-domain plant datasets, including both synthesized and real oversegmented cases, as well as on out-of-domain animal datasets to demonstrate transfer learning performance. An ablation study further highlights the contribution of the Geo-Wasserstein divergence. A clear pipeline is provided for end-users to build pre-trained models to any labeled dataset.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DHP: Discrete Hierarchical Planning for Hierarchical Reinforcement Learning Agents</title>
<link>https://arxiv.org/abs/2502.01956</link>
<guid>https://arxiv.org/abs/2502.01956</guid>
<content:encoded><![CDATA[
arXiv:2502.01956v3 Announce Type: replace-cross 
Abstract: Hierarchical Reinforcement Learning (HRL) agents often struggle with long-horizon visual planning due to their reliance on error-prone distance metrics. We propose Discrete Hierarchical Planning (DHP), a method that replaces continuous distance estimates with discrete reachability checks to evaluate subgoal feasibility. DHP recursively constructs tree-structured plans by decomposing long-term goals into sequences of simpler subtasks, using a novel advantage estimation strategy that inherently rewards shorter plans and generalizes beyond training depths. In addition, to address the data efficiency challenge, we introduce an exploration strategy that generates targeted training examples for the planning modules without needing expert data. Experiments in 25-room navigation environments demonstrate a 100% success rate (vs. 90% baseline). We also present an offline variant that achieves state-of-the-art results on OGBench benchmarks, with up to 71% absolute gains on giant HumanoidMaze tasks, demonstrating our core contributions are architecture-agnostic. The method also generalizes to momentum-based control tasks and requires only log N steps for replanning. Theoretical analysis and ablations validate our design choices.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Foundation for unbiased cross-validation of spatio-temporal models for species distribution modeling</title>
<link>https://arxiv.org/abs/2502.03480</link>
<guid>https://arxiv.org/abs/2502.03480</guid>
<content:encoded><![CDATA[
arXiv:2502.03480v2 Announce Type: replace-cross 
Abstract: Evaluating the predictive performance of species distribution models (SDMs) under realistic deployment scenarios requires careful handling of spatial and temporal dependencies in the data. Cross-validation (CV) is the standard approach for model evaluation, but its design strongly influences the validity of performance estimates. When SDMs are intended for spatial or temporal transfer, random CV can lead to overoptimistic results due to spatial autocorrelation (SAC) among neighboring observations.
  We benchmark four machine learning algorithms (GBM, XGBoost, LightGBM, Random Forest) on two real-world presence-absence datasets, a temperate plant and an anadromous fish, using multiple CV designs: random, spatial, spatio-temporal, environmental, and forward-chaining. Two training data usage strategies (LAST FOLD and RETRAIN) are evaluated, with hyperparameter tuning performed within each CV scheme. Model performance is assessed on independent out-of-time test sets using AUC, MAE, and correlation metrics.
  Random CV overestimates AUC by up to 0.16 and produces MAE values up to 80 percent higher than spatially blocked alternatives. Blocking at the empirical SAC range substantially reduces this bias. Training strategy affects evaluation outcomes: LAST FOLD yields smaller validation-test discrepancies under strong SAC, while RETRAIN achieves higher test AUC when SAC is weaker. Boosted ensemble models consistently perform best under spatially structured CV designs. We recommend a robust SDM workflow based on SAC-aware blocking, blocked hyperparameter tuning, and external temporal validation to improve reliability under spatial and temporal shifts.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GraphCompNet: A Position-Aware Model for Predicting and Compensating Shape Deviations in 3D Printing</title>
<link>https://arxiv.org/abs/2502.09652</link>
<guid>https://arxiv.org/abs/2502.09652</guid>
<content:encoded><![CDATA[
arXiv:2502.09652v3 Announce Type: replace-cross 
Abstract: Shape deviation modeling and compensation in additive manufacturing are pivotal for achieving high geometric accuracy and enabling industrial-scale production. Critical challenges persist, including generalizability across complex geometries and adaptability to position-dependent variations in batch production. Traditional methods of controlling geometric deviations often rely on complex parameterized models and repetitive metrology, which can be time-consuming yet not applicable for batch production. In this paper, we present a novel, process-agnostic approach to address the challenge of ensuring geometric precision and accuracy in position-dependent AM production. The proposed GraphCompNet presents a novel computational framework integrating graph-based neural networks with a GAN inspired training paradigm. The framework leverages point cloud representations and dynamic graph convolutional neural networks (DGCNNs) to model intricate geometries while incorporating position-specific thermal and mechanical variations. A two-stage adversarial training process iteratively refines compensated designs using a compensator-predictor architecture, enabling real-time feedback and optimization. Experimental validation across various shapes and positions demonstrates the framework's ability to predict deviations in freeform geometries and adapt to position-dependent batch production conditions, significantly improving compensation accuracy (35 to 65 percent) across the entire printing space, addressing position-dependent variabilities within the print chamber. The proposed method advances the development of a Digital Twin for AM, offering scalable, real-time monitoring and compensation capabilities.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LookAhead Tuning: Safer Language Models via Partial Answer Previews</title>
<link>https://arxiv.org/abs/2503.19041</link>
<guid>https://arxiv.org/abs/2503.19041</guid>
<content:encoded><![CDATA[
arXiv:2503.19041v4 Announce Type: replace-cross 
Abstract: Fine-tuning enables large language models (LLMs) to adapt to specific domains, but often compromises their previously established safety alignment. To mitigate the degradation of model safety during fine-tuning, we introduce LookAhead Tuning, a lightweight and effective data-driven approach that preserves safety during fine-tuning. The method introduces two simple strategies that modify training data by previewing partial answer prefixes, thereby minimizing perturbations to the model's initial token distributions and maintaining its built-in safety mechanisms. Comprehensive experiments demonstrate that LookAhead Tuning effectively maintains model safety without sacrificing robust performance on downstream tasks. Our findings position LookAhead Tuning as a reliable and efficient solution for the safe and effective adaptation of LLMs.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Constraint-based causal discovery with tiered background knowledge and latent variables in single or overlapping datasets</title>
<link>https://arxiv.org/abs/2503.21526</link>
<guid>https://arxiv.org/abs/2503.21526</guid>
<content:encoded><![CDATA[
arXiv:2503.21526v3 Announce Type: replace-cross 
Abstract: In this paper we consider the use of tiered background knowledge within constraint based causal discovery. Our focus is on settings relaxing causal sufficiency, i.e. allowing for latent variables which may arise because relevant information could not be measured at all, or not jointly, as in the case of multiple overlapping datasets. We first present novel insights into the properties of the 'tiered FCI' (tFCI) algorithm. Building on this, we introduce a new extension of the IOD (integrating overlapping datasets) algorithm incorporating tiered background knowledge, the 'tiered IOD' (tIOD) algorithm. We show that under full usage of the tiered background knowledge tFCI and tIOD are sound, while simple versions of the tIOD and tFCI are sound and complete. We further show that the tIOD algorithm can often be expected to be considerably more efficient and informative than the IOD algorithm even beyond the obvious restriction of the Markov equivalence classes. We provide a formal result on the conditions for this gain in efficiency and informativeness. Our results are accompanied by a series of examples illustrating the exact role and usefulness of tiered background knowledge.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sample, Don't Search: Rethinking Test-Time Alignment for Language Models</title>
<link>https://arxiv.org/abs/2504.03790</link>
<guid>https://arxiv.org/abs/2504.03790</guid>
<content:encoded><![CDATA[
arXiv:2504.03790v2 Announce Type: replace-cross 
Abstract: Increasing test-time computation has emerged as a promising direction for improving language model performance, particularly in scenarios where model finetuning is impractical or impossible due to computational constraints or private model weights. However, existing test-time search methods using a reward model (RM) often degrade in quality as compute scales, due to the over-optimization of what are inherently imperfect reward proxies. We introduce QAlign, a new test-time alignment approach. As we scale test-time compute, QAlign converges to sampling from the optimal aligned distribution for each individual prompt. By adopting recent advances in Markov chain Monte Carlo for text generation, our method enables better-aligned outputs without modifying the underlying model or even requiring logit access. We demonstrate the effectiveness of QAlign on mathematical reasoning benchmarks (GSM8K and GSM-Symbolic) using a task-specific RM, showing consistent improvements over existing test-time compute methods like best-of-n and majority voting. Furthermore, when applied with more realistic RMs trained on the Tulu 3 preference dataset, QAlign outperforms direct preference optimization (DPO), best-of-n, majority voting, and weighted majority voting on a diverse range of datasets (GSM8K, MATH500, IFEval, MMLU-Redux, and TruthfulQA). A practical solution to aligning language models at test time using additional computation without degradation, our approach expands the limits of the capability that can be obtained from off-the-shelf language models without further training.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Survey on Archetypal Analysis</title>
<link>https://arxiv.org/abs/2504.12392</link>
<guid>https://arxiv.org/abs/2504.12392</guid>
<content:encoded><![CDATA[
arXiv:2504.12392v2 Announce Type: replace-cross 
Abstract: Archetypal analysis (AA) was originally proposed in 1994 by Adele Cutler and Leo Breiman as a computational procedure for extracting distinct aspects, so-called archetypes, from observations, with each observational record approximated as a mixture (i.e., convex combination) of these archetypes. AA thereby provides straightforward, interpretable, and explainable representations for feature extraction and dimensionality reduction, facilitating the understanding of the structure of high-dimensional data and enabling wide applications across the sciences. However, AA also faces challenges, particularly as the associated optimization problem is non-convex. This is the first survey that provides researchers and data mining practitioners with an overview of the methodologies and opportunities that AA offers, surveying the many applications of AA across disparate fields of science, as well as best practices for modeling data with AA and its limitations. The survey concludes by explaining crucial future research directions concerning AA.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Stochastic Occupation Kernel (SOCK) Method for Learning Stochastic Differential Equations</title>
<link>https://arxiv.org/abs/2505.11622</link>
<guid>https://arxiv.org/abs/2505.11622</guid>
<content:encoded><![CDATA[
arXiv:2505.11622v2 Announce Type: replace-cross 
Abstract: We present a novel kernel-based method for learning multivariate stochastic differential equations (SDEs). The method follows a two-step procedure: we first estimate the drift term function, then the (matrix-valued) diffusion function given the drift. Occupation kernels are integral functionals on a reproducing kernel Hilbert space (RKHS) that aggregate information over a trajectory. Our approach leverages vector-valued occupation kernels for estimating the drift component of the stochastic process. For diffusion estimation, we extend this framework by introducing operator-valued occupation kernels, enabling the estimation of an auxiliary matrix-valued function as a positive semi-definite operator, from which we readily derive the diffusion estimate. This enables us to avoid common challenges in SDE learning, such as intractable likelihoods, by optimizing a reconstruction-error-based objective. We propose a simple learning procedure that retains strong predictive accuracy while using Fenchel duality to promote efficiency. We validate the method on simulated benchmarks and a real-world dataset of Amyloid imaging in healthy and Alzheimer's disease subjects.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BOLT: Block-Orthonormal Lanczos for Trace estimation of matrix functions</title>
<link>https://arxiv.org/abs/2505.12289</link>
<guid>https://arxiv.org/abs/2505.12289</guid>
<content:encoded><![CDATA[
arXiv:2505.12289v2 Announce Type: replace-cross 
Abstract: Efficient matrix trace estimation is essential for scalable computation of log-determinants, matrix norms, and distributional divergences. In many large-scale applications, the matrices involved are too large to store or access in full, making even a single matrix-vector (mat-vec) product infeasible. Instead, one often has access only to small subblocks of the matrix or localized matrix-vector products on restricted index sets. Hutch++ achieves optimal convergence rate but relies on randomized SVD and assumes full mat-vec access, making it difficult to apply in these constrained settings. We propose the Block-Orthonormal Stochastic Lanczos Quadrature (BOLT), which matches Hutch++ accuracy with a simpler implementation based on orthonormal block probes and Lanczos iterations. BOLT builds on the Stochastic Lanczos Quadrature (SLQ) framework, which combines random probing with Krylov subspace methods to efficiently approximate traces of matrix functions, and performs better than Hutch++ in near flat-spectrum regimes. To address memory limitations and partial access constraints, we introduce Subblock SLQ, a variant of BOLT that operates only on small principal submatrices. As a result, this framework yields a proxy KL divergence estimator and an efficient method for computing the Wasserstein-2 distance between Gaussians - both compatible with low-memory and partial-access regimes. We provide theoretical guarantees and demonstrate strong empirical performance across a range of high-dimensional settings.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Clustering and Pruning in Causal Data Fusion</title>
<link>https://arxiv.org/abs/2505.15215</link>
<guid>https://arxiv.org/abs/2505.15215</guid>
<content:encoded><![CDATA[
arXiv:2505.15215v2 Announce Type: replace-cross 
Abstract: Data fusion, the process of combining observational and experimental data, can enable the identification of causal effects that would otherwise remain non-identifiable. Although identification algorithms have been developed for specific scenarios, do-calculus remains the only general-purpose tool for causal data fusion, particularly when variables are present in some data sources but not others. However, approaches based on do-calculus may encounter computational challenges as the number of variables increases and the causal graph grows in complexity. Consequently, there exists a need to reduce the size of such models while preserving the essential features. For this purpose, we propose pruning (removing unnecessary variables) and clustering (combining variables) as preprocessing operations for causal data fusion. We generalize earlier results on a single data source and derive conditions for applying pruning and clustering in the case of multiple data sources. We give sufficient conditions for inferring the identifiability or non-identifiability of a causal effect in a larger graph based on a smaller graph and show how to obtain the corresponding identifying functional for identifiable causal effects. Examples from epidemiology and social science demonstrate the use of the results.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the performance of multi-fidelity and reduced-dimensional neural emulators for inference of physiological boundary conditions</title>
<link>https://arxiv.org/abs/2506.11683</link>
<guid>https://arxiv.org/abs/2506.11683</guid>
<content:encoded><![CDATA[
arXiv:2506.11683v2 Announce Type: replace-cross 
Abstract: Solving inverse problems in cardiovascular modeling is particularly challenging due to the high computational cost of running high-fidelity simulations. In this work, we focus on Bayesian parameter estimation and explore different methods to reduce the computational cost of sampling from the posterior distribution by leveraging low-fidelity approximations. A common approach is to construct a surrogate model for the high-fidelity simulation itself. Another is to build a surrogate for the discrepancy between high- and low-fidelity models. This discrepancy, which is often easier to approximate, is modeled with either a fully connected neural network or a nonlinear dimensionality reduction technique that enables surrogate construction in a lower-dimensional space. A third possible approach is to treat the discrepancy between the high-fidelity and surrogate models as random noise and estimate its distribution using normalizing flows. This allows us to incorporate the approximation error into the Bayesian inverse problem by modifying the likelihood function. We validate five different methods which are variations of the above on analytical test cases by comparing them to posterior distributions derived solely from high-fidelity models, assessing both accuracy and computational cost. Finally, we demonstrate our approaches on two cardiovascular examples of increasing complexity: a lumped-parameter Windkessel model and a patient-specific three-dimensional anatomy.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying Uncertainty in the Presence of Distribution Shifts</title>
<link>https://arxiv.org/abs/2506.18283</link>
<guid>https://arxiv.org/abs/2506.18283</guid>
<content:encoded><![CDATA[
arXiv:2506.18283v2 Announce Type: replace-cross 
Abstract: Neural networks make accurate predictions but often fail to provide reliable uncertainty estimates, especially under covariate distribution shifts between training and testing. To address this problem, we propose a Bayesian framework for uncertainty estimation that explicitly accounts for covariate shifts. While conventional approaches rely on fixed priors, the key idea of our method is an adaptive prior, conditioned on both training and new covariates. This prior naturally increases uncertainty for inputs that lie far from the training distribution in regions where predictive performance is likely to degrade. To efficiently approximate the resulting posterior predictive distribution, we employ amortized variational inference. Finally, we construct synthetic environments by drawing small bootstrap samples from the training data, simulating a range of plausible covariate shift using only the original dataset. We evaluate our method on both synthetic and real-world data. It yields substantially improved uncertainty estimates under distribution shifts.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ZKPROV: A Zero-Knowledge Approach to Dataset Provenance for Large Language Models</title>
<link>https://arxiv.org/abs/2506.20915</link>
<guid>https://arxiv.org/abs/2506.20915</guid>
<content:encoded><![CDATA[
arXiv:2506.20915v2 Announce Type: replace-cross 
Abstract: As large language models (LLMs) are used in sensitive fields, accurately verifying their computational provenance without disclosing their training datasets poses a significant challenge, particularly in regulated sectors such as healthcare, which have strict requirements for dataset use. Traditional approaches either incur substantial computational cost to fully verify the entire training process or leak unauthorized information to the verifier. Therefore, we introduce ZKPROV, a novel cryptographic framework allowing users to verify that the LLM's responses to their prompts are trained on datasets certified by the authorities that own them. Additionally, it ensures that the dataset's content is relevant to the users' queries without revealing sensitive information about the datasets or the model parameters. ZKPROV offers a unique balance between privacy and efficiency by binding training datasets, model parameters, and responses, while also attaching zero-knowledge proofs to the responses generated by the LLM to validate these claims. Our experimental results demonstrate sublinear scaling for generating and verifying these proofs, with end-to-end overhead under 3.3 seconds for models up to 8B parameters, presenting a practical solution for real-world applications. We also provide formal security guarantees, proving that our approach preserves dataset confidentiality while ensuring trustworthy dataset provenance.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars</title>
<link>https://arxiv.org/abs/2507.01939</link>
<guid>https://arxiv.org/abs/2507.01939</guid>
<content:encoded><![CDATA[
arXiv:2507.01939v4 Announce Type: replace-cross 
Abstract: In recent years, large language models (LLMs) have transformed natural language understanding through vast datasets and large-scale parameterization. Inspired by this success, we present SpecCLIP, a foundation model framework that extends LLM-inspired methodologies to stellar spectral analysis. Stellar spectra, akin to structured language, encode rich physical and chemical information about stars. By training foundation models on large-scale spectral datasets, our goal is to learn robust and informative embeddings that support diverse downstream applications. As a proof of concept, SpecCLIP involves pre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed by contrastive alignment using the CLIP (Contrastive Language-Image Pre-training) framework, adapted to associate spectra from different instruments. This alignment is complemented by auxiliary decoders that preserve spectrum-specific information and enable translation (prediction) between spectral types, with the former achieved by maximizing mutual information between embeddings and input spectra. The result is a cross-spectrum framework enabling intrinsic calibration and flexible applications across instruments. We demonstrate that fine-tuning these models on moderate-sized labeled datasets improves adaptability to tasks such as stellar-parameter estimation and chemical-abundance determination. SpecCLIP also enhances the accuracy and precision of parameter estimates benchmarked against external survey data. Additionally, its similarity search and cross-spectrum prediction capabilities offer potential for anomaly detection. Our results suggest that contrastively trained foundation models enriched with spectrum-aware decoders can advance precision stellar spectroscopy. Our code SpecCLIP is publicly available at https://github.com/Xiaosheng-Zhao/SpecCLIP
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Facilitated Fairness Assessment of AI-based Skin Lesion Classifiers Through GenAI-based Image Synthesis</title>
<link>https://arxiv.org/abs/2507.17860</link>
<guid>https://arxiv.org/abs/2507.17860</guid>
<content:encoded><![CDATA[
arXiv:2507.17860v3 Announce Type: replace-cross 
Abstract: Recent advances in deep learning and on-device inference could transform routine screening for skin cancers. Along with the anticipated benefits of this technology, potential dangers arise from unforeseen and inherent biases. A significant obstacle is building evaluation datasets that accurately reflect key demographics, including sex, age, and race, as well as other underrepresented groups. To address this, we train a state-of-the-art generative model to generate synthetic data in a controllable manner to assess the fairness of publicly available skin cancer classifiers. To evaluate whether synthetic images can be used as a fairness testing dataset, we prepare a real-image dataset (MILK10K) as a benchmark and compare the True Positive Rate result of three models (DeepGuide, MelaNet, and SkinLesionDensnet). As a result, the classification tendencies observed in each model when tested on real and generated images showed similar patterns across different attribute data sets. We confirm that highly realistic synthetic images facilitate model fairness verification.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning</title>
<link>https://arxiv.org/abs/2508.10501</link>
<guid>https://arxiv.org/abs/2508.10501</guid>
<content:encoded><![CDATA[
arXiv:2508.10501v4 Announce Type: replace-cross 
Abstract: Existing tool-augmented agentic systems are limited in the real world by (i) black-box reasoning steps that undermine trust of decision-making and pose safety risks, (ii) poor multimodal integration, which is inherently critical for healthcare tasks, and (iii) rigid and computationally inefficient agentic pipelines. We introduce PASS (Probabilistic Agentic Supernet Sampling), the first multimodal framework to address these challenges in the context of Chest X-Ray (CXR) reasoning. PASS adaptively samples agentic workflows over a multi-tool graph, yielding decision paths annotated with interpretable probabilities. Given the complex CXR reasoning task with multimodal medical data, PASS leverages its learned task-conditioned distribution over the agentic supernet. Thus, it adaptively selects the most suitable tool at each supernet layer, offering probability-annotated trajectories for post-hoc audits and directly enhancing medical AI safety. PASS also continuously compresses salient findings into an evolving personalized memory, while dynamically deciding whether to deepen its reasoning path or invoke an early exit for efficiency. To optimize a Pareto frontier balancing performance and cost, we design a novel three-stage training procedure, including expert knowledge warm-up, contrastive path-ranking, and cost-aware reinforcement learning. To facilitate rigorous evaluation, we introduce CAB-E, a comprehensive benchmark for multi-step, safety-critical, free-form CXR reasoning. Experiments across various benchmarks validate that PASS significantly outperforms strong baselines in multiple metrics (e.g., accuracy, LLM-Judge, semantic similarity, etc.) while balancing computational costs, pushing a new paradigm shift towards interpretable, adaptive, and multimodal medical agentic systems.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unified Acoustic Representations for Screening Neurological and Respiratory Pathologies from Voice</title>
<link>https://arxiv.org/abs/2508.20717</link>
<guid>https://arxiv.org/abs/2508.20717</guid>
<content:encoded><![CDATA[
arXiv:2508.20717v2 Announce Type: replace-cross 
Abstract: Voice-based health assessment offers unprecedented opportunities for scalable, non-invasive disease screening, yet existing approaches typically focus on single conditions and fail to leverage the rich, multi-faceted information embedded in speech. We present MARVEL (Multi-task Acoustic Representations for Voice-based Health Analysis), a privacy-conscious multitask learning framework that simultaneously detects nine distinct neurological, respiratory, and voice disorders using only derived acoustic features, eliminating the need for raw audio transmission. Our dual-branch architecture employs specialized encoders with task-specific heads sharing a common acoustic backbone, enabling effective cross-condition knowledge transfer. Evaluated on the large-scale Bridge2AI-Voice v2.0 dataset, MARVEL achieves an overall AUROC of 0.78, with exceptional performance on neurological disorders (AUROC = 0.89), particularly for Alzheimer's disease/mild cognitive impairment (AUROC = 0.97). Our framework consistently outperforms single-modal baselines by 5-19% and surpasses state-of-the-art self-supervised models on 7 of 9 tasks, while correlation analysis reveals that the learned representations exhibit meaningful similarities with established acoustic features, indicating that the model's internal representations are consistent with clinically recognized acoustic patterns. By demonstrating that a single unified model can effectively screen for diverse conditions, this work establishes a foundation for deployable voice-based diagnostics in resource-constrained and remote healthcare settings.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Learning-Driven Predictive Resource Management in Complex Science Workflows</title>
<link>https://arxiv.org/abs/2509.11512</link>
<guid>https://arxiv.org/abs/2509.11512</guid>
<content:encoded><![CDATA[
arXiv:2509.11512v2 Announce Type: replace-cross 
Abstract: The collaborative efforts of large communities in science experiments, often comprising thousands of global members, reflect a monumental commitment to exploration and discovery. Recently, advanced and complex data processing has gained increasing importance in science experiments. Data processing workflows typically consist of multiple intricate steps, and the precise specification of resource requirements is crucial for each step to allocate optimal resources for effective processing. Estimating resource requirements in advance is challenging due to a wide range of analysis scenarios, varying skill levels among community members, and the continuously increasing spectrum of computing options. One practical approach to mitigate these challenges involves initially processing a subset of each step to measure precise resource utilization from actual processing profiles before completing the entire step. While this two-staged approach enables processing on optimal resources for most of the workflow, it has drawbacks such as initial inaccuracies leading to potential failures and suboptimal resource usage, along with overhead from waiting for initial processing completion, which is critical for fast-turnaround analyses. In this context, our study introduces a novel pipeline of machine learning models within a comprehensive workflow management system, the Production and Distributed Analysis (PanDA) system. These models employ advanced machine learning techniques to predict key resource requirements, overcoming challenges posed by limited upfront knowledge of characteristics at each step. Accurate forecasts of resource requirements enable informed and proactive decision-making in workflow management, enhancing the efficiency of handling diverse, complex workflows across heterogeneous resources.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MatchFixAgent: Language-Agnostic Autonomous Repository-Level Code Translation Validation and Repair</title>
<link>https://arxiv.org/abs/2509.16187</link>
<guid>https://arxiv.org/abs/2509.16187</guid>
<content:encoded><![CDATA[
arXiv:2509.16187v2 Announce Type: replace-cross 
Abstract: Code translation transforms source code from one programming language (PL) to another. Validating the functional equivalence of translation and repairing, if necessary, are critical steps in code translation. Existing automated validation and repair approaches struggle to generalize to many PLs due to high engineering overhead, and they rely on existing and often inadequate test suites, which results in false claims of equivalence and ineffective translation repair. We develop MatchFixAgent, a large language model (LLM)-based, PL-agnostic framework for equivalence validation and repair of translations. MatchFixAgent features a multi-agent architecture that divides equivalence validation into several sub-tasks to ensure thorough and consistent semantic analysis of the translation. Then it feeds this analysis to test agent to write and execute tests. Upon observing a test failure, the repair agent attempts to fix the translation bug. The final (in)equivalence decision is made by the verdict agent, considering semantic analyses and test execution results.
  We compare MatchFixAgent's validation and repair results with four repository-level code translation techniques. We use 2,219 translation pairs from their artifacts, which cover 6 PL pairs, and are collected from 24 GitHub projects totaling over 900K lines of code. Our results demonstrate that MatchFixAgent produces (in)equivalence verdicts for 99.2% of translation pairs, with the same equivalence validation result as prior work on 72.8% of them. When MatchFixAgent's result disagrees with prior work, we find that 60.7% of the time MatchFixAgent's result is actually correct. In addition, we show that MatchFixAgent can repair 50.6% of inequivalent translation, compared to prior work's 18.5%. This demonstrates that MatchFixAgent is far more adaptable to many PL pairs than prior work, while producing highly accurate validation results.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Machine Learning Pipeline: Large Language Models-Assisted Automated Dataset Generation for Training Machine-Learned Interatomic Potentials</title>
<link>https://arxiv.org/abs/2509.21647</link>
<guid>https://arxiv.org/abs/2509.21647</guid>
<content:encoded><![CDATA[
arXiv:2509.21647v2 Announce Type: replace-cross 
Abstract: Machine learning interatomic potentials (MLIPs) have become powerful tools to extend molecular simulations beyond the limits of quantum methods, offering near-quantum accuracy at much lower computational cost. Yet, developing reliable MLIPs remains difficult because it requires generating high-quality datasets, preprocessing atomic structures, and carefully training and validating models. In this work, we introduce an Automated Machine Learning Pipeline (AMLP) that unifies the entire workflow from dataset creation to model validation. AMLP employs large-language-model agents to assist with electronic-structure code selection, input preparation, and output conversion, while its analysis suite (AMLP-Analysis), based on ASE supports a range of molecular simulations. The pipeline is built on the MACE architecture and validated on acridine polymorphs, where, with a straightforward fine-tuning of a foundation model, mean absolute errors of ~1.7 meV/atom in energies and ~7.0 meV/{\AA} in forces are achieved. The fitted MLIP reproduces DFT geometries with sub-{\AA} accuracy and demonstrates stability during molecular dynamics simulations in the microcanonical and canonical ensembles.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying the Impact of Structured Output Format on Large Language Models through Causal Inference</title>
<link>https://arxiv.org/abs/2509.21791</link>
<guid>https://arxiv.org/abs/2509.21791</guid>
<content:encoded><![CDATA[
arXiv:2509.21791v3 Announce Type: replace-cross 
Abstract: Structured output from large language models (LLMs) has enhanced efficiency in processing generated information and is increasingly adopted in industrial applications. Prior studies have investigated the impact of structured output on LLMs' generation quality, often presenting one-way findings. Some suggest that structured format enhances completeness and factual accuracy, while others argue that it restricts the reasoning capacity of LLMs and leads to reductions in standard evaluation metrics. Potential limitations of these assessments include restricted testing scenarios, weakly controlled comparative settings, and reliance on coarse metrics. In this work, we present a refined analysis using causal inference. Based on one assumed and two guaranteed constraints, we derive five potential causal structures characterizing the influence of structured output on LLMs' generation: (1) collider without m-bias, (2) collider with m-bias, (3) single cause from instruction, (4) single cause from output format, and (5) independence. Across seven public and one developed reasoning tasks, we find that coarse metrics report positive, negative, or neutral effects of structured output on GPT-4o's generation. However, causal inference reveals no causal impact in 43 out of 48 scenarios. In the remaining 5, 3 involve multifaceted causal structures influenced by concrete instructions. Further experiments show that OpenAI-o3 are more resilient to output formats than general-purpose GPT-4o and GPT-4.1, highlighting an unaware advantage of reasoning models.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Generation Phases of Flow Matching: a Denoising Perspective</title>
<link>https://arxiv.org/abs/2510.24830</link>
<guid>https://arxiv.org/abs/2510.24830</guid>
<content:encoded><![CDATA[
arXiv:2510.24830v2 Announce Type: replace-cross 
Abstract: Flow matching has achieved remarkable success, yet the factors influencing the quality of its generation process remain poorly understood. In this work, we adopt a denoising perspective and design a framework to empirically probe the generation process. Laying down the formal connections between flow matching models and denoisers, we provide a common ground to compare their performances on generation and denoising. This enables the design of principled and controlled perturbations to influence sample generation: noise and drift. This leads to new insights on the distinct dynamical phases of the generative process, enabling us to precisely characterize at which stage of the generative process denoisers succeed or fail and why this matters.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-driven uncertainty-aware seakeeping prediction of the Delft 372 catamaran using ensemble Hankel dynamic mode decomposition</title>
<link>https://arxiv.org/abs/2511.04461</link>
<guid>https://arxiv.org/abs/2511.04461</guid>
<content:encoded><![CDATA[
arXiv:2511.04461v2 Announce Type: replace-cross 
Abstract: In this study, we present and validate an ensemble-based Hankel Dynamic Mode Decomposition with control (HDMDc) for uncertainty-aware seakeeping predictions of a high-speed catamaran, namely the Delft 372 model. Experimental measurements (time histories) of wave elevation at the longitudinal center of gravity, heave, pitch, notional flight-deck velocity, notional bridge acceleration, and total resistance were collected from irregular wave basin tests on a 1:33.3 scale replica of the Delft 372 model under sea state 5 conditions at Fr = 0.425, and organized into training, validation, and test sets. The HDMDc algorithm constructs an equation-free linear reduced-order model of the seakeeping vessel by augmenting states and inputs with their time-lagged copies to capture nonlinear and memory effects. Two ensembling strategies, namely Bayesian HDMDc (BHDMDc), which samples hyperparameters considered stochastic variables with prior distribution to produce posterior mean forecasts with confidence intervals, and Frequentist HDMDc (FHDMDc), which aggregates multiple model obtained over data subsets, are compared in providing seakeeping prediction and uncertainty quantification. The FHDMDc approach is found to improve the accuracy of the predictions compared to the deterministic counterpart, also providing robust uncertainty estimation; whereas the application of BHDMDc to the present test case is not found beneficial in comparison to the deterministic model. FHDMDc-derived probability density functions for the motions closely match both experimental data and URANS results, demonstrating reliable and computationally efficient seakeeping prediction for design and operational support.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalized infinite dimensional Alpha-Procrustes based geometries</title>
<link>https://arxiv.org/abs/2511.09801</link>
<guid>https://arxiv.org/abs/2511.09801</guid>
<content:encoded><![CDATA[
arXiv:2511.09801v2 Announce Type: replace-cross 
Abstract: This work extends the recently introduced Alpha-Procrustes family of Riemannian metrics for symmetric positive definite (SPD) matrices by incorporating generalized versions of the Bures-Wasserstein (GBW), Log-Euclidean, and Wasserstein distances. While the Alpha-Procrustes framework has unified many classical metrics in both finite- and infinite- dimensional settings, it previously lacked the structural components necessary to realize these generalized forms. We introduce a formalism based on unitized Hilbert-Schmidt operators and an extended Mahalanobis norm that allows the construction of robust, infinite-dimensional generalizations of GBW and Log-Hilbert-Schmidt distances. Our approach also incorporates a learnable regularization parameter that enhances geometric stability in high-dimensional comparisons. Preliminary experiments reproducing benchmarks from the literature demonstrate the improved performance of our generalized metrics, particularly in scenarios involving comparisons between datasets of varying dimension and scale. This work lays a theoretical and computational foundation for advancing robust geometric methods in machine learning, statistical inference, and functional data analysis.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLAReSNet: When Convolution Meets Latent Attention for Hyperspectral Image Classification</title>
<link>https://arxiv.org/abs/2511.12346</link>
<guid>https://arxiv.org/abs/2511.12346</guid>
<content:encoded><![CDATA[
arXiv:2511.12346v2 Announce Type: replace-cross 
Abstract: Hyperspectral image (HSI) classification faces critical challenges, including high spectral dimensionality, complex spectral-spatial correlations, and limited training samples with severe class imbalance. While CNNs excel at local feature extraction and transformers capture long-range dependencies, their isolated application yields suboptimal results due to quadratic complexity and insufficient inductive biases. We propose CLAReSNet (Convolutional Latent Attention Residual Spectral Network), a hybrid architecture that integrates multi-scale convolutional extraction with transformer-style attention via an adaptive latent bottleneck. The model employs a multi-scale convolutional stem with deep residual blocks and an enhanced Convolutional Block Attention Module for hierarchical spatial features, followed by spectral encoder layers combining bidirectional RNNs (LSTM/GRU) with Multi-Scale Spectral Latent Attention (MSLA). MSLA reduces complexity from $\mathcal{O}(T^2D)$ to $\mathcal{O}(T\log(T)D)$ by adaptive latent token allocation (8-64 tokens) that scales logarithmically with the sequence length. Hierarchical cross-attention fusion dynamically aggregates multi-level representations for robust classification. Experiments conducted on the Indian Pines and Salinas datasets show state-of-the-art performance, achieving overall accuracies of 99.71% and 99.96%, significantly surpassing HybridSN, SSRN, and SpectralFormer. The learned embeddings exhibit superior inter-class separability and compact intra-class clustering, validating CLAReSNet's effectiveness under severe class imbalance.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Safeguarded Stochastic Polyak Step Sizes for Non-smooth Optimization: Robust Performance Without Small (Sub)Gradients</title>
<link>https://arxiv.org/abs/2512.02342</link>
<guid>https://arxiv.org/abs/2512.02342</guid>
<content:encoded><![CDATA[
arXiv:2512.02342v2 Announce Type: replace-cross 
Abstract: The stochastic Polyak step size (SPS) has proven to be a promising choice for stochastic gradient descent (SGD), delivering competitive performance relative to state-of-the-art methods on smooth convex and non-convex optimization problems, including deep neural network training. However, extensions of this approach to non-smooth settings remain in their early stages, often relying on interpolation assumptions or requiring knowledge of the optimal solution. In this work, we propose a novel SPS variant, Safeguarded SPS (SPS$_{safe}$), for the stochastic subgradient method, and provide rigorous convergence guarantees for non-smooth convex optimization with no need for strong assumptions. We further incorporate momentum into the update rule, yielding equally tight theoretical results. On non-smooth convex benchmarks, our experiments are consistent with the theoretical predictions on how the safeguard affects the convergence neighborhood. On deep neural networks the proposed step size achieves competitive performance to existing adaptive baselines and exhibits stable behavior across a wide range of problem settings. Moreover, in these experiments, the gradient norms under our step size do not collapse to (near) zero, indicating robustness to vanishing gradients.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predictive Modeling of I/O Performance for Machine Learning Training Pipelines: A Data-Driven Approach to Storage Optimization</title>
<link>https://arxiv.org/abs/2512.06699</link>
<guid>https://arxiv.org/abs/2512.06699</guid>
<content:encoded><![CDATA[
arXiv:2512.06699v2 Announce Type: replace-cross 
Abstract: Modern machine learning training is increasingly bottlenecked by data I/O rather than compute. GPUs often sit idle at below 50% utilization waiting for data. This paper presents a machine learning approach to predict I/O performance and recommend optimal storage configurations for ML training pipelines. We collected 141 observations through systematic benchmarking across different storage backends (NVMe SSD, network-attached storage, in-memory filesystems), data formats, and access patterns, covering both low-level I/O operations and full training pipelines. After evaluating seven regression models and three classification approaches, XGBoost achieved the best performance with R-squared of 0.991, predicting I/O throughput within 11.8% error on average. Feature importance analysis revealed that throughput metrics and batch size are the primary performance drivers. This data-driven approach can reduce configuration time from days of trial-and-error to minutes of predictive recommendation. The methodology is reproducible and extensible to other resource management problems in ML systems. Code and data are available at https://github.com/knkarthik01/gpu_storage_ml_project
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Minimum Bayes Risk Decoding for Error Span Detection in Reference-Free Automatic Machine Translation Evaluation</title>
<link>https://arxiv.org/abs/2512.07540</link>
<guid>https://arxiv.org/abs/2512.07540</guid>
<content:encoded><![CDATA[
arXiv:2512.07540v2 Announce Type: replace-cross 
Abstract: Error Span Detection (ESD) extends automatic machine translation (MT) evaluation by localizing translation errors and labeling their severity. Current generative ESD methods typically use Maximum a Posteriori (MAP) decoding, assuming that the model-estimated probabilities are perfectly correlated with similarity to the human annotation, but we often observe higher likelihood assigned to an incorrect annotation than to the human one. We instead apply Minimum Bayes Risk (MBR) decoding to generative ESD. We use a sentence- or span-level similarity function for MBR decoding, which selects candidate hypotheses based on their approximate similarity to the human annotation. Experimental results on the WMT24 Metrics Shared Task show that MBR decoding significantly improves span-level performance and generally matches or outperforms MAP at the system and sentence levels. To reduce the computational cost of MBR decoding, we further distill its decisions into a model decoded via greedy search, removing the inference-time latency bottleneck.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-Context Learning for Seismic Data Processing</title>
<link>https://arxiv.org/abs/2512.11575</link>
<guid>https://arxiv.org/abs/2512.11575</guid>
<content:encoded><![CDATA[
arXiv:2512.11575v2 Announce Type: replace-cross 
Abstract: Seismic processing transforms raw data into subsurface images essential for geophysical applications. Traditional methods face challenges, such as noisy data, and manual parameter tuning, among others. Recently deep learning approaches have proposed alternative solutions to some of these problems. However, important challenges of existing deep learning approaches are spatially inconsistent results across neighboring seismic gathers and lack of user-control. We address these limitations by introducing ContextSeisNet, an in-context learning model, to seismic demultiple processing. Our approach conditions predictions on a support set of spatially related example pairs: neighboring common-depth point gathers from the same seismic line and their corresponding labels. This allows the model to learn task-specific processing behavior at inference time by observing how similar gathers should be processed, without any retraining. This method provides both flexibility through user-defined examples and improved lateral consistency across seismic lines. On synthetic data, ContextSeisNet outperforms a U-Net baseline quantitatively and demonstrates enhanced spatial coherence between neighboring gathers. On field data, our model achieves superior lateral consistency compared to both traditional Radon demultiple and the U-Net baseline. Relative to the U-Net, ContextSeisNet also delivers improved near-offset performance and more complete multiple removal. Notably, ContextSeisNet achieves comparable field data performance despite being trained on 90% less data, demonstrating substantial data efficiency. These results establish ContextSeisNet as a practical approach for spatially consistent seismic demultiple with potential applicability to other seismic processing tasks.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Error Estimate and Convergence Analysis for Data Valuation</title>
<link>https://arxiv.org/abs/2511.06463</link>
<guid>https://arxiv.org/abs/2511.06463</guid>
<content:encoded><![CDATA[
<div> Data valuation, neural dynamic data valuation, error estimation, convergence analysis, sublinear convergence<br /><br />Summary:<br /><br />This paper focuses on data valuation, which quantifies the importance of data in machine learning, highlighting that existing methods lack guarantees of validity within a single training process. It builds upon the neural dynamic data valuation (NDDV) method as a foundation. The authors are the first to explore error estimation and convergence analysis specifically in the context of data valuation using NDDV. Under assumptions of Lipschitz continuity and smoothness, they derive quadratic error bounds for the difference in loss values, which inversely scale with the number of time steps and scale quadratically with control variations, thus ensuring stability of the valuation process. Furthermore, they demonstrate theoretically that the expected squared norm of the gradient of the training loss approaches zero asymptotically, indicating vanishing gradients in the long run of training. Additionally, the meta loss, reflecting the quality of data valuation, is shown to converge sublinearly over iterations. Overall, the analysis proves that the NDDV method achieves sublinear convergence rates, providing both stability and theoretical convergence guarantees in single training processes for data valuation. <div>
arXiv:2511.06463v3 Announce Type: replace 
Abstract: Data valuation quantifies data importance, but existing methods cannot ensure validity in a single training process. The neural dynamic data valuation (NDDV) method [3] addresses this limitation. Based on NDDV, we are the first to explore error estimation and convergence analysis in data valuation. Under Lipschitz and smoothness assumptions, we derive quadratic error bounds for loss differences that scale inversely with time steps and quadratically with control variations, ensuring stability. We also prove that the expected squared gradient norm for the training loss vanishes asymptotically, and that the meta loss converges sublinearly over iterations. In particular, NDDV achieves sublinear convergence.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiscoverDCP: A Data-Driven Approach for Construction of Disciplined Convex Programs via Symbolic Regression</title>
<link>https://arxiv.org/abs/2512.15721</link>
<guid>https://arxiv.org/abs/2512.15721</guid>
<content:encoded><![CDATA[
<div> Keywords: DiscoverDCP, symbolic regression, Disciplined Convex Programming, convex models, system identification<br /><br />Summary:<br /><br />1. The paper introduces DiscoverDCP, a novel data-driven framework integrating symbolic regression with Disciplined Convex Programming (DCP) rules for system identification.<br /><br />2. By enforcing that all candidate model expressions comply with DCP composition rules, the method guarantees that discovered expressions are globally convex by design, avoiding the difficult problem of verifying convexity after model discovery.<br /><br />3. This approach enables the identification of convex surrogate models that go beyond standard fixed-parameter convex forms like quadratic functions, allowing for more flexible and accurate representations.<br /><br />4. The models produced by DiscoverDCP are interpretable and verifiable, making them well-suited for applications in safety-critical control and optimization where guarantees on convexity and model behavior are crucial.<br /><br />5. Overall, DiscoverDCP advances symbolic regression by embedding convexity constraints directly into the model discovery process, producing convex, flexible, and trustworthy models for system identification tasks. <div>
arXiv:2512.15721v1 Announce Type: new 
Abstract: We propose DiscoverDCP, a data-driven framework that integrates symbolic regression with the rule sets of Disciplined Convex Programming (DCP) to perform system identification. By enforcing that all discovered candidate model expressions adhere to DCP composition rules, we ensure that the output expressions are globally convex by construction, circumventing the computationally intractable process of post-hoc convexity verification. This approach allows for the discovery of convex surrogates that exhibit more relaxed and accurate functional forms than traditional fixed-parameter convex expressions (e.g., quadratic functions). The proposed method produces interpretable, verifiable, and flexible convex models suitable for safety-critical control and optimization tasks.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Quantum-Classical Ensemble Learning for S\&amp;P 500 Directional Prediction</title>
<link>https://arxiv.org/abs/2512.15738</link>
<guid>https://arxiv.org/abs/2512.15738</guid>
<content:encoded><![CDATA[
<div> Keywords: financial market prediction, ensemble learning, quantum sentiment analysis, Decision Transformer, model selection

<br /><br />Summary:  
This study presents a hybrid ensemble framework for financial market prediction that achieves a directional accuracy of 60.14% on S&amp;P 500 data, surpassing typical model performance by about 3.10%. The framework integrates quantum-enhanced sentiment analysis using a 4-qubit variational quantum circuit, which contributes an accuracy gain of 0.8% to 1.5% per model. It combines diverse learning architectures—including LSTM, Decision Transformer, XGBoost, Random Forest, and Logistic Regression—on the same dataset, proving more effective than training identical models on multiple datasets (60.14% vs. 52.80%), supported by correlation analysis showing higher agreement among same-architecture models (r > 0.6). The approach employs strategic model selection by filtering out weaker predictors with accuracy below 52%, which improves ensemble results significantly (top 7 models achieve 60.14% compared to 51.2% when using all 35 models). The evaluation spans seven market instruments from 2020 to 2023, covering various regimes, including the COVID-19 crash and inflation-driven downturn, with McNemar’s test confirming statistical significance (p < 0.05). Preliminary backtesting with confidence-based filtering (requiring 6+ model consensus) demonstrates practical trading value, yielding a Sharpe ratio of 1.2 compared to 0.8 for a buy-and-hold strategy. <div>
arXiv:2512.15738v1 Announce Type: new 
Abstract: Financial market prediction is a challenging application of machine learning, where even small improvements in directional accuracy can yield substantial value. Most models struggle to exceed 55--57\% accuracy due to high noise, non-stationarity, and market efficiency. We introduce a hybrid ensemble framework combining quantum sentiment analysis, Decision Transformer architecture, and strategic model selection, achieving 60.14\% directional accuracy on S\&amp;P 500 prediction, a 3.10\% improvement over individual models.
  Our framework addresses three limitations of prior approaches. First, architecture diversity dominates dataset diversity: combining different learning algorithms (LSTM, Decision Transformer, XGBoost, Random Forest, Logistic Regression) on the same data outperforms training identical architectures on multiple datasets (60.14\% vs.\ 52.80\%), confirmed by correlation analysis ($r>0.6$ among same-architecture models). Second, a 4-qubit variational quantum circuit enhances sentiment analysis, providing +0.8\% to +1.5\% gains per model. Third, smart filtering excludes weak predictors (accuracy $<52\%$), improving ensemble performance (Top-7 models: 60.14\% vs.\ all 35 models: 51.2\%).
  We evaluate on 2020--2023 market data across seven instruments, covering diverse regimes including the COVID-19 crash and inflation-driven correction. McNemar's test confirms statistical significance ($p<0.05$). Preliminary backtesting with confidence-based filtering (6+ model consensus) yields a Sharpe ratio of 1.2 versus buy-and-hold's 0.8, demonstrating practical trading potential.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SHARe-KAN: Holographic Vector Quantization for Memory-Bound Inference</title>
<link>https://arxiv.org/abs/2512.15742</link>
<guid>https://arxiv.org/abs/2512.15742</guid>
<content:encoded><![CDATA[
<div> Kolmogorov-Arnold Networks, memory wall, holographic topology, vector quantization, hardware-aware compiler<br /><br />Summary:<br /><br />1. Kolmogorov-Arnold Networks (KANs) suffer from a "memory wall" due to a large number of learned basis functions that create heavy parameter counts, demanding high memory bandwidth and making deployment in memory-limited environments challenging.  
2. Vision KANs display a holographic topology where information is not localized but distributed across spline interference patterns, which causes traditional pruning methods to perform poorly, as evidenced by a drastic accuracy drop (from 85.23% to 45%) at only 10% sparsity.  
3. To overcome this, the authors propose SHARe-KAN, a novel compression framework using Gain-Shape-Bias Vector Quantization that exploits functional redundancy while preserving the dense and distributed nature of the holographic topology.  
4. Together with LUTHAM, a hardware-aware compiler performing static memory planning, the approach achieves an 88× reduction in runtime memory footprint (reducing usage from 1.13 GB to 12.91 MB) without sacrificing accuracy on the PASCAL VOC benchmark.  
5. Profiling on NVIDIA's Ampere GPU architecture shows that the optimized workload maintains over 90% L2 cache residency, effectively decoupling the execution from costly DRAM bandwidth limitations typical in spline-based architectures. <div>
arXiv:2512.15742v1 Announce Type: new 
Abstract: Kolmogorov-Arnold Networks (KANs) face a fundamental memory wall: their learned basis functions create parameter counts that impose extreme bandwidth demands, hindering deployment in memory-constrained environments. We show that Vision KANs exhibit a holographic topology, where information is distributed across the interference of splines rather than localized to specific edges. Consequently, traditional pruning fails (10% sparsity degrades mAP from 85.23% to 45%, a $\sim$40-point drop). To address this, we present SHARe-KAN, a framework utilizing Gain-Shape-Bias Vector Quantization to exploit functional redundancy while preserving the dense topology. Coupled with LUTHAM, a hardware-aware compiler with static memory planning, we achieve $88\times$ runtime memory reduction (1.13 GB $\to$ 12.91 MB) and match uncompressed baseline accuracy on PASCAL VOC. Profiling on NVIDIA Ampere architecture confirms $>90\%$ L2 cache residency, demonstrating that the workload is decoupled from DRAM bandwidth constraints inherent to spline-based architectures.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Do Graph Signals Affect Recommendation: Unveiling the Mystery of Low and High-Frequency Graph Signals</title>
<link>https://arxiv.org/abs/2512.15744</link>
<guid>https://arxiv.org/abs/2512.15744</guid>
<content:encoded><![CDATA[
<div> Keywords: spectral graph neural networks, low-frequency signals, high-frequency signals, recommendation systems, graph embeddings<br /><br />Summary:  
This paper investigates the role of low-frequency and high-frequency graph signals in recommendation systems using spectral graph neural networks (GNNs). Firstly, it challenges the conventional emphasis on low-pass filtering by demonstrating that both low-frequency and high-frequency signals equivalently contribute to improving recommendation performance by smoothing user-item pair similarities. Secondly, the authors propose a frequency signal scaler, a flexible, plug-and-play module that adjusts the filtering function in GNNs to fine-tune the smoothness of graph signals, compatible with various GNN architectures. Thirdly, the paper identifies limitations in graph embedding-based methods, proving they cannot fully capture graph signal characteristics. To overcome this, a novel space flip method is introduced, which restores the expressive power of graph embeddings in representing graph signals. Fourthly, the study reveals that using solely low-frequency or high-frequency signals is sufficient for effective recommendation, simplifying the design of GNN-based recommenders. Finally, extensive experiments on four public datasets validate the theoretical insights and demonstrate the superior effectiveness of the proposed methods. The paper also provides public code for reproducibility at https://github.com/mojosey/SimGCF. <div>
arXiv:2512.15744v1 Announce Type: new 
Abstract: Spectral graph neural networks (GNNs) are highly effective in modeling graph signals, with their success in recommendation often attributed to low-pass filtering. However, recent studies highlight the importance of high-frequency signals. The role of low-frequency and high-frequency graph signals in recommendation remains unclear. This paper aims to bridge this gap by investigating the influence of graph signals on recommendation performance. We theoretically prove that the effects of low-frequency and high-frequency graph signals are equivalent in recommendation tasks, as both contribute by smoothing the similarities between user-item pairs. To leverage this insight, we propose a frequency signal scaler, a plug-and-play module that adjusts the graph signal filter function to fine-tune the smoothness between user-item pairs, making it compatible with any GNN model. Additionally, we identify and prove that graph embedding-based methods cannot fully capture the characteristics of graph signals. To address this limitation, a space flip method is introduced to restore the expressive power of graph embeddings. Remarkably, we demonstrate that either low-frequency or high-frequency graph signals alone are sufficient for effective recommendations. Extensive experiments on four public datasets validate the effectiveness of our proposed methods. Code is avaliable at https://github.com/mojosey/SimGCF.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLaDA2.0: Scaling Up Diffusion Language Models to 100B</title>
<link>https://arxiv.org/abs/2512.15745</link>
<guid>https://arxiv.org/abs/2512.15745</guid>
<content:encoded><![CDATA[
<div> Keywords: discrete diffusion, large language model, auto-regressive conversion, Mixture-of-Experts, instruction tuning  

<br /><br />Summary:  
This paper introduces LLaDA2.0, a novel suite of discrete diffusion large language models (dLLMs) scaling up to 100 billion parameters. The key innovation lies in systematically converting pre-trained auto-regressive (AR) models into dLLMs, bypassing the need for expensive training from scratch while preserving inherited knowledge. The training follows a unique 3-phase block-level WSD (word sequence diffusion) scheme: starting with progressively increasing block sizes (warm-up), followed by large-scale full-sequence diffusion (stable phase), and concluding with reverting to compact block sizes (decay phase). This progressive method ensures stable training and effective adaptation of AR models to dLLMs. Post-training alignment is conducted using supervised fine-tuning (SFT) and direct preference optimization (DPO), producing two instruction-tuned Mixture-of-Experts (MoE) models: LLaDA2.0-mini at 16 billion parameters and LLaDA2.0-flash at 100 billion parameters. These models maintain the advantage of parallel decoding inherent in diffusion models, providing superior performance and efficiency for large-scale deployment. Both variants have been open-sourced, aiming to establish a new paradigm for frontier-scale LLM deployment that balances efficiency, performance, and practical usability. <div>
arXiv:2512.15745v1 Announce Type: new 
Abstract: This paper presents LLaDA2.0 -- a tuple of discrete diffusion large language models (dLLM) scaling up to 100B total parameters through systematic conversion from auto-regressive (AR) models -- establishing a new paradigm for frontier-scale deployment. Instead of costly training from scratch, LLaDA2.0 upholds knowledge inheritance, progressive adaption and efficiency-aware design principle, and seamless converts a pre-trained AR model into dLLM with a novel 3-phase block-level WSD based training scheme: progressive increasing block-size in block diffusion (warm-up), large-scale full-sequence diffusion (stable) and reverting back to compact-size block diffusion (decay). Along with post-training alignment with SFT and DPO, we obtain LLaDA2.0-mini (16B) and LLaDA2.0-flash (100B), two instruction-tuned Mixture-of-Experts (MoE) variants optimized for practical deployment. By preserving the advantages of parallel decoding, these models deliver superior performance and efficiency at the frontier scale. Both models were open-sourced.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Unified Generative-Predictive Framework for Deterministic Inverse Design</title>
<link>https://arxiv.org/abs/2512.15746</link>
<guid>https://arxiv.org/abs/2512.15746</guid>
<content:encoded><![CDATA[
<div> Inverse design, heterogeneous microstructures, generative model, latent manifold, physics-informed inversion  

<br /><br />Summary:  
This paper addresses the challenging problem of inverse design for heterogeneous material microstructures, known for its ill-posed nature and high computational demand due to high-dimensional design spaces and complex physics. The authors introduce Janus, a unified generative-predictive framework that combines a deep encoder-decoder architecture with a separable KHRONOS predictive head. Janus learns a latent manifold that is simultaneously isometric for generative inversion and optimized for physical prediction, promoting disentanglement in the latent space. The model is first validated on MNIST, showing high-fidelity image reconstruction, accurate classification, and diverse generative inversion across all classes. It is then applied to the inverse design of microstructures with targeted thermal conductivity, achieving high forward prediction accuracy (R² = 0.98) and low pixelwise reconstruction error (under 5%). Inverse solutions meet target properties within 1% relative error. Traversing the latent space reveals smooth transitions consistent with physical variations, supported by UMAP visualizations showing a low-dimensional, disentangled manifold. By integrating prediction and generation within one latent space, Janus enables real-time, physics-informed inverse microstructure design at a significantly lower computational cost compared to classical optimization methods. <div>
arXiv:2512.15746v1 Announce Type: new 
Abstract: Inverse design of heterogeneous material microstructures is a fundamentally ill-posed and famously computationally expensive problem. This is exacerbated by the high-dimensional design spaces associated with finely resolved images, multimodal input property streams, and a highly nonlinear forward physics. Whilst modern generative models excel at accurately modeling such complex forward behavior, most of them are not intrinsically structured to support fast, stable \emph{deterministic} inversion with a physics-informed bias. This work introduces Janus, a unified generative-predictive framework to address this problem. Janus couples a deep encoder-decoder architecture with a predictive KHRONOS head, a separable neural architecture. Topologically speaking, Janus learns a latent manifold simultaneously isometric for generative inversion and pruned for physical prediction; the joint objective inducing \emph{disentanglement} of the latent space. Janus is first validated on the MNIST dataset, demonstrating high-fidelity reconstruction, accurate classification and diverse generative inversion of all ten target classes. It is then applied to the inverse design of heterogeneous microstructures labeled with thermal conductivity. It achieves a forward prediction accuracy $R^2=0.98$ (2\% relative error) and sub-5\% pixelwise reconstruction error. Inverse solutions satisfy target properties to within $1\%$ relative error. Inverting a sweep through properties reveal smooth traversal of the latent manifold, and UMAP visualization confirms the emergence of a low-dimensional, disentangled manifold. By unifying prediction and generation within a single latent space, Janus enables real-time, physics-informed inverse microstructure generation at a lower computational cost typically associated with classical optimization-based approaches.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>D3G: Diverse Demographic Data Generation Increases Zero-Shot Image Classification Accuracy within Multimodal Models</title>
<link>https://arxiv.org/abs/2512.15747</link>
<guid>https://arxiv.org/abs/2512.15747</guid>
<content:encoded><![CDATA[
<div> Keywords: image classification, demographic bias, zero-shot learning, multimodal models, data generation  

<br /><br />Summary:  
This paper addresses challenges in zero-shot image classification, particularly focusing on demographic bias present in models like CLIP. First, the authors highlight that image classification remains difficult, especially for fine-grained tasks where models with limited capacity struggle due to underfitting. Second, the quality and diversity of cross-modal data is crucial, as biased or unbalanced datasets lead to skewed predictions favoring overrepresented demographics. Third, the study emphasizes the risks of harmful demographic bias in zero-shot classification setups and the need for mitigation strategies. Fourth, the authors propose Diverse Demographic Data Generation (D3G), a novel, training-free, zero-shot approach that enhances classification accuracy while reducing demographic bias by introducing diverse synthetic demographic data at inference time. Fifth, the method leverages CLIP as the base model combined with Stable Diffusion XL to generate diverse demographic imagery, improving balanced representation without additional model training. Finally, experiments demonstrate that D3G boosts performance and fairness, with further analysis exploring the impact of individual demographics on accuracy, underscoring the importance of demographic diversity for robust multimodal image classification. <div>
arXiv:2512.15747v1 Announce Type: new 
Abstract: Image classification is a task essential for machine perception to achieve human-level image understanding. Multimodal models such as CLIP have been able to perform well on this task by learning semantic similarities across vision and language; however, despite these advances, image classification is still a challenging task. Models with low capacity often suffer from underfitting and thus underperform on fine-grained image classification. Along with this, it is important to ensure high-quality data with rich cross-modal representations of each class, which is often difficult to generate. When datasets do not enforce balanced demographics, the predictions will be biased toward the more represented class, while others will be neglected. We focus on how these issues can lead to harmful bias for zero-shot image classification, and explore how to combat these issues in demographic bias. We propose Diverse Demographic Data Generation (D3G), a training-free, zero-shot method of boosting classification accuracy while reducing demographic bias in pre-trained multimodal models. With this method, we utilize CLIP as our base multimodal model and Stable Diffusion XL as our generative model. We demonstrate that providing diverse demographic data at inference time improves performance for these models, and explore the impact of individual demographics on the resulting accuracy metric.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Surely Large Multimodal Models (Don't) Excel in Visual Species Recognition?</title>
<link>https://arxiv.org/abs/2512.15748</link>
<guid>https://arxiv.org/abs/2512.15748</guid>
<content:encoded><![CDATA[
<div> Visual Species Recognition, Few-shot Learning, Large Multimodal Models, Post-hoc Correction, Biodiversity Assessment<br /><br />Summary:<br /><br />Visual Species Recognition (VSR) is crucial for biodiversity assessment, ecology, and conservation research but typically requires extensive expert-annotated images, which are costly and limited. To address this, few-shot learning (FSL) expert models have been adopted, leveraging only a small number of labeled examples to train accurate species classifiers. Despite the success of Large Multimodal Models (LMMs) in general recognition tasks, they surprisingly underperform compared to simple fine-tuned FSL expert models on specialized VSR tasks, even with advanced prompting. However, LMMs demonstrate an ability to effectively correct errors made by FSL expert models when prompted with their top predictions. Building on this observation, the authors propose a Post-hoc Correction (POC) method that prompts LMMs to re-rank the top FSL predictions by incorporating softmax confidence scores and few-shot visual examples into enriched prompts. The POC method significantly improves classification accuracy by +6.4% across five challenging VSR benchmarks without requiring additional training, validation, or manual intervention. Furthermore, POC generalizes well to different pretrained backbones and LMM architectures, making it a versatile, plug-and-play enhancement for existing FSL approaches in species recognition. <div>
arXiv:2512.15748v1 Announce Type: new 
Abstract: Visual Species Recognition (VSR) is pivotal to biodiversity assessment and conservation, evolution research, and ecology and ecosystem management. Training a machine-learned model for VSR typically requires vast amounts of annotated images. Yet, species-level annotation demands domain expertise, making it realistic for domain experts to annotate only a few examples. These limited labeled data motivate training an ''expert'' model via few-shot learning (FSL). Meanwhile, advanced Large Multimodal Models (LMMs) have demonstrated prominent performance on general recognition tasks. It is straightforward to ask whether LMMs excel in the highly specialized VSR task and whether they outshine FSL expert models. Somewhat surprisingly, we find that LMMs struggle in this task, despite using various established prompting techniques. LMMs even significantly underperform FSL expert models, which are as simple as finetuning a pretrained visual encoder on the few-shot images. However, our in-depth analysis reveals that LMMs can effectively post-hoc correct the expert models' incorrect predictions. Briefly, given a test image, when prompted with the top predictions from an FSL expert model, LMMs can recover the ground-truth label. Building on this insight, we derive a simple method called Post-hoc Correction (POC), which prompts an LMM to re-rank the expert model's top predictions using enriched prompts that include softmax confidence scores and few-shot visual examples. Across five challenging VSR benchmarks, POC outperforms prior art of FSL by +6.4% in accuracy without extra training, validation, or manual intervention. Importantly, POC generalizes to different pretrained backbones and LMMs, serving as a plug-and-play module to significantly enhance existing FSL methods.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Special Case of Quadratic Extrapolation Under the Neural Tangent Kernel</title>
<link>https://arxiv.org/abs/2512.15749</link>
<guid>https://arxiv.org/abs/2512.15749</guid>
<content:encoded><![CDATA[
<div> ReLU MLP, Neural Tangent Kernel, extrapolation, origin, quadratic extrapolation<br /><br />Summary:<br /><br />1. The article focuses on the extrapolation behavior of ReLU multilayer perceptrons (MLPs), particularly under the Neural Tangent Kernel (NTK) regime. <br />2. It is well-established that ReLU MLPs tend to extrapolate linearly when evaluating points out-of-distribution, but analysis near the origin remains less explored. <br />3. The NTK induces an infinite-dimensional feature map which is not translationally invariant, meaning extrapolation at points far from the origin differs fundamentally from that near the origin. <br />4. Due to the rotational invariance of the NTK’s feature map, studying points close to or far from the origin represents two extreme cases of extrapolation behavior. <br />5. The article discovers that, contrary to the classical linear extrapolation for distant points, there is a quadratic extrapolation characteristic when evaluating points near the origin under the ReLU NTK framework. <div>
arXiv:2512.15749v1 Announce Type: new 
Abstract: It has been demonstrated both theoretically and empirically that the ReLU MLP tends to extrapolate linearly for an out-of-distribution evaluation point. The machine learning literature provides ample analysis with respect to the mechanisms to which linearity is induced. However, the analysis of extrapolation at the origin under the NTK regime remains a more unexplored special case. In particular, the infinite-dimensional feature map induced by the neural tangent kernel is not translationally invariant. This means that the study of an out-of-distribution evaluation point very far from the origin is not equivalent to the evaluation of a point very near the origin. And since the feature map is rotation invariant, these two special cases may represent the most canonically extreme bounds of ReLU NTK extrapolation. Ultimately, it is this loose recognition of the two special cases of extrapolation that motivate the discovery of quadratic extrapolation for an evaluation close to the origin.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GLOW: Graph-Language Co-Reasoning for Agentic Workflow Performance Prediction</title>
<link>https://arxiv.org/abs/2512.15751</link>
<guid>https://arxiv.org/abs/2512.15751</guid>
<content:encoded><![CDATA[
<div> Agentic Workflows, Performance Prediction, Graph Neural Networks, Large Language Models, Contrastive Alignment  

<br /><br />Summary:  
Agentic Workflows (AWs) are an emerging approach designed to tackle complex tasks through structured, multi-step processes. The primary challenge lies in the scalability of automating AW generation due to costly and time-consuming execution-based evaluations. Current prediction methods attempt to estimate AW performance without actual execution but struggle to simultaneously capture both the complex topological dependencies among workflow components and the rich semantic logic embedded within. To overcome these limitations, the paper introduces GLOW, a unified predictive framework that synergizes Graph Neural Networks (GNNs) and Large Language Models (LLMs). GLOW employs a graph-oriented LLM, fine-tuned specifically on graph-related tasks, to extract semantic features that are sensitive to workflow topology. These semantic features are then fused with structural representations encoded by GNNs, creating a comprehensive understanding of AWs. Additionally, a contrastive alignment strategy is applied to refine the latent space, enhancing the distinction between high- and low-quality workflows. Extensive experimental validation on the FLORA-Bench benchmark demonstrates that GLOW significantly outperforms existing state-of-the-art methods in both prediction accuracy and the ranking utility of AWs. This work highlights the benefits of combining graph-based and semantic reasoning techniques for efficient workflow performance prediction. <div>
arXiv:2512.15751v1 Announce Type: new 
Abstract: Agentic Workflows (AWs) have emerged as a promising paradigm for solving complex tasks. However, the scalability of automating their generation is severely constrained by the high cost and latency of execution-based evaluation. Existing AW performance prediction methods act as surrogates but fail to simultaneously capture the intricate topological dependencies and the deep semantic logic embedded in AWs. To address this limitation, we propose GLOW, a unified framework for AW performance prediction that combines the graph-structure modeling capabilities of GNNs with the reasoning power of LLMs. Specifically, we introduce a graph-oriented LLM, instruction-tuned on graph tasks, to extract topologically aware semantic features, which are fused with GNN-encoded structural representations. A contrastive alignment strategy further refines the latent space to distinguish high-quality AWs. Extensive experiments on FLORA-Bench show that GLOW outperforms state-of-the-art baselines in prediction accuracy and ranking utility.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TAO-Net: Two-stage Adaptive OOD Classification Network for Fine-grained Encrypted Traffic Classification</title>
<link>https://arxiv.org/abs/2512.15753</link>
<guid>https://arxiv.org/abs/2512.15753</guid>
<content:encoded><![CDATA[
<div> Keywords: encrypted traffic classification, Out-of-Distribution (OOD), TAO-Net, transformer, large language models  

<br /><br />Summary:  
This paper addresses the challenge of classifying encrypted network traffic, particularly the detection and fine-grained categorization of Out-of-Distribution (OOD) traffic generated by emerging applications that are not covered by existing models. Traditional traffic classification methods depend on predefined categories, limiting their ability to handle unknown traffic types, and often resort to grouping all unknown traffic into a single "Other" category, which lacks detail. To overcome this, the authors propose TAO-Net, a Two-stage Adaptive OOD classification Network. The first stage uses a hybrid OOD detection mechanism that combines transformer-based inter-layer transformation smoothness with feature analysis to accurately discern between In-Distribution (ID) and OOD encrypted traffic. The second stage employs large language models enhanced with a novel semantic prompt strategy, converting the OOD classification problem into a generation task that supports flexible and fine-grained classification without predefined labels. Experiments conducted on three datasets show that TAO-Net achieves superior performance, with macro-precision between 96.81% and 97.70% and macro-F1 scores between 96.77% and 97.68%, significantly outperforming previous methods that only achieve 44.73% to 86.30% macro-precision, especially excelling in recognizing newly emerging network applications. <div>
arXiv:2512.15753v1 Announce Type: new 
Abstract: Encrypted traffic classification aims to identify applications or services by analyzing network traffic data. One of the critical challenges is the continuous emergence of new applications, which generates Out-of-Distribution (OOD) traffic patterns that deviate from known categories and are not well represented by predefined models. Current approaches rely on predefined categories, which limits their effectiveness in handling unknown traffic types. Although some methods mitigate this limitation by simply classifying unknown traffic into a single "Other" category, they fail to make a fine-grained classification. In this paper, we propose a Two-stage Adaptive OOD classification Network (TAO-Net) that achieves accurate classification for both In-Distribution (ID) and OOD encrypted traffic. The method incorporates an innovative two-stage design: the first stage employs a hybrid OOD detection mechanism that integrates transformer-based inter-layer transformation smoothness and feature analysis to effectively distinguish between ID and OOD traffic, while the second stage leverages large language models with a novel semantic-enhanced prompt strategy to transform OOD traffic classification into a generation task, enabling flexible fine-grained classification without relying on predefined labels. Experiments on three datasets demonstrate that TAO-Net achieves 96.81-97.70% macro-precision and 96.77-97.68% macro-F1, outperforming previous methods that only reach 44.73-86.30% macro-precision, particularly in identifying emerging network applications.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KAN-Matrix: Visualizing Nonlinear Pairwise and Multivariate Contributions for Physical Insight</title>
<link>https://arxiv.org/abs/2512.15755</link>
<guid>https://arxiv.org/abs/2512.15755</guid>
<content:encoded><![CDATA[
<div> Keywords: Kolmogorov-Arnold Networks, interpretability, nonlinear associations, feature ranking, data visualization  

<br /><br />Summary:  
1. The article addresses the challenge of interpreting complex datasets, which is often hindered by high dimensionality and collinearity among variables.  
2. It introduces a novel use of Kolmogorov-Arnold Networks (KANs) to improve interpretability and model parsimony beyond what traditional correlation methods can achieve.  
3. Two new visualization tools are proposed: the Pairwise KAN Matrix (PKAN), which captures nonlinear relationships between pairs of variables, and the Multivariate KAN Contribution Matrix (MKAN), which ranks features based on their nonlinear contributions to predicting a target variable.  
4. These tools aid both pre-processing tasks such as feature selection and redundancy analysis, and post-processing uses including model explanation and extraction of physical insights.  
5. Through experiments, PKAN and MKAN are demonstrated to provide more robust and informative results than conventional metrics like Pearson Correlation and Mutual Information, by effectively revealing the strength and functional nature of underlying relationships. This promotes the discovery of hidden physical patterns and supports domain-informed decision making in model development. <div>
arXiv:2512.15755v1 Announce Type: new 
Abstract: Interpreting complex datasets remains a major challenge for scientists, particularly due to high dimensionality and collinearity among variables. We introduce a novel application of Kolmogorov-Arnold Networks (KANs) to enhance interpretability and parsimony beyond what traditional correlation analyses offer. We present two interpretable, color-coded visualization tools: the Pairwise KAN Matrix (PKAN) and the Multivariate KAN Contribution Matrix (MKAN). PKAN characterizes nonlinear associations between pairs of variables, while MKAN serves as a nonlinear feature-ranking tool that quantifies the relative contributions of inputs in predicting a target variable. These tools support pre-processing (e.g., feature selection, redundancy analysis) and post-processing (e.g., model explanation, physical insights) in model development workflows. Through experimental comparisons, we demonstrate that PKAN and MKAN yield more robust and informative results than Pearson Correlation and Mutual Information. By capturing the strength and functional forms of relationships, these matrices facilitate the discovery of hidden physical patterns and promote domain-informed model development.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReactorFold: Generative discovery of nuclear reactor cores via emergent physical reasoning</title>
<link>https://arxiv.org/abs/2512.15756</link>
<guid>https://arxiv.org/abs/2512.15756</guid>
<content:encoded><![CDATA[
<div> Keywords: ReactorFold, nuclear reactor core design, language models, Direct Preference Optimization, asymmetric configurations  

<br /><br />Summary:  
The paper introduces ReactorFold, a novel generative framework that tackles nuclear reactor core design by framing fuel-assembly layout as a sequence modeling challenge suited for language models. Unlike traditional methods constrained by fixed, human-defined configuration spaces, ReactorFold leverages Monte Carlo simulation data and parameter-efficient fine-tuning to learn the underlying structure of pressurized-water-reactor assemblies. The model is trained with Direct Preference Optimization (DPO), enabling it to generate candidate fuel assembly designs in a single forward pass. A significant finding is the model’s emergent ability to expand the design space: although trained solely on configurations with a fixed number of gadolinium burnable absorber (Gd) rods, it autonomously varies Gd content to meet stringent power-peaking constraints. Additionally, ReactorFold discovers high-performing asymmetric configurations that challenge conventional symmetric design heuristics, thus revealing new design regimes that traditional search methods cannot access. This demonstrates that language models can not only internalize complex causal physical relationships but also surpass artificial human-imposed constraints, opening pathways for innovative nuclear reactor core topologies and improved design optimization strategies. <div>
arXiv:2512.15756v1 Announce Type: new 
Abstract: Designing nuclear reactor cores requires navigating large discrete design spaces governed by complex neutronic interactions. Traditional deterministic, metaheuristic, and machine-learning-assisted methods search within fixed, human-defined configuration spaces, limiting their ability to discover fundamentally new design topologies. Here we introduce ReactorFold, a generative framework that reformulates fuel-assembly design as a sequence modeling problem for language models. Using Monte Carlo data, parameter-efficient fine-tuning, and Direct Preference Optimization (DPO), the model learns the latent structure of a pressurized-water-reactor assembly and generates candidate layouts in a single forward pass. Notably, the DPO-aligned model exhibits emergent design-space expansion: despite being trained exclusively on configurations with a fixed number of gadolinium burnable absorber (Gd) rods, it autonomously adjusts Gd inventory to satisfy strict power-peaking constraints. The model also discovers high-performing asymmetric configurations that challenge conventional symmetric loading heuristics, accessing design regimes inaccessible to conventional search methods and demonstrating that language models can internalize causal physical relationships and transcend human-imposed design constraints.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Twin Restricted Kernel Machines for Multiview Classification</title>
<link>https://arxiv.org/abs/2512.15757</link>
<guid>https://arxiv.org/abs/2512.15757</guid>
<content:encoded><![CDATA[
<div> Multi-view learning, kernel machines, twin restricted kernel machine, multiview support vector machine, classification performance<br /><br />Summary:<br /><br />Multi-view learning (MVL) aims to enhance generalization by utilizing complementary data from multiple views. Traditional multi-view support vector machines (MvSVMs) succeed in various tasks but face challenges in high-dimensional spaces and are prone to view inconsistencies. This paper introduces the Twin Multiview Restricted Kernel Machine (TMvRKM), a novel model combining kernel machines’ strengths with the multiview framework to overcome these challenges. TMvRKM avoids computationally expensive quadratic programming problems by using a regularized least squares method to find optimal separating hyperplanes, improving efficiency and accuracy. The model’s primal objective incorporates a coupling term to effectively balance errors across different views. TMvRKM employs both early and late fusion strategies, leveraging collective multiview information during training while maintaining flexibility to individual view differences. The approach is thoroughly evaluated on UCI, KEEL, and AwA benchmark datasets. Experimental results and statistical analyses consistently demonstrate that TMvRKM outperforms baseline models, showing superior generalization and classification performance across all tested scenarios. <div>
arXiv:2512.15757v1 Announce Type: new 
Abstract: Multi-view learning (MVL) is an emerging field in machine learning that focuses on improving generalization performance by leveraging complementary information from multiple perspectives or views. Various multi-view support vector machine (MvSVM) approaches have been developed, demonstrating significant success. Moreover, these models face challenges in effectively capturing decision boundaries in high-dimensional spaces using the kernel trick. They are also prone to errors and struggle with view inconsistencies, which are common in multi-view datasets. In this work, we introduce the multiview twin restricted kernel machine (TMvRKM), a novel model that integrates the strengths of kernel machines with the multiview framework, addressing key computational and generalization challenges associated with traditional kernel-based approaches. Unlike traditional methods that rely on solving large quadratic programming problems (QPPs), the proposed TMvRKM efficiently determines an optimal separating hyperplane through a regularized least squares approach, enhancing both computational efficiency and classification performance. The primal objective of TMvRKM includes a coupling term designed to balance errors across multiple views effectively. By integrating early and late fusion strategies, TMvRKM leverages the collective information from all views during training while remaining flexible to variations specific to individual views. The proposed TMvRKM model is rigorously tested on UCI, KEEL, and AwA benchmark datasets. Both experimental results and statistical analyses consistently highlight its exceptional generalization performance, outperforming baseline models in every scenario.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Yantra AI -- An intelligence platform which interacts with manufacturing operations</title>
<link>https://arxiv.org/abs/2512.15758</link>
<guid>https://arxiv.org/abs/2512.15758</guid>
<content:encoded><![CDATA[
<div> Keywords: Industry 4.0, intelligent production system, machine learning, predictive maintenance, AI-powered decision support<br /><br />Summary:<br /><br />Industry 4.0 advancements have rapidly transformed smart manufacturing by integrating real-time tracking, machine learning, and AI-driven systems to streamline operations. This dissertation focuses on the design and evaluation of an intelligent production system tailored for XRIT, addressing critical issues such as energy management, predictive maintenance, and AI-enhanced decision support. The system incorporates machine learning models like the Random Forest Classifier for proactive maintenance scheduling and the Isolation Forest algorithm for anomaly detection, aiding in informed decision-making and minimizing downtime. Real-time data visualization is achieved through Streamlit dashboards, enabling workers to interact with live operational insights. The system's effectiveness was validated using simulated data and emphasizes scalability to support real-time implementation within XRIT's production environment. Additionally, the integration of a GPT-4-based AI virtual assistant allows workers to obtain real-time, relevant answers to complex queries, enhancing operational decisions. Testing results demonstrate improvements in work efficiency, energy usage optimization, and repair planning capabilities. Future work will prioritize transitioning to live data integration and exploring additional enhancements to further optimize the intelligent production system. <div>
arXiv:2512.15758v1 Announce Type: new 
Abstract: Industry 4.0 is growing quickly, which has changed smart production by encouraging the use of real-time tracking, machine learning, and AI-driven systems to make operations run more smoothly. The main focus of this dissertation is on creating and testing an intelligent production system for XRIT that solves important problems like energy management, predictive maintenance, and AI-powered decision support. Machine learning models are built into the system, such as the Random Forest Classifier for proactive maintenance and the Isolation Forest for finding outliers. These models help with decision-making and reducing downtime. Streamlit makes real-time data visualisation possible, giving workers access to dashboards that they can interact with and see real-time observations.The system was tested with fake data and is made to be scalable, so it can be used in real time in XRIT's production setting. Adding an AI-powered virtual assistant made with GPT-4 lets workers get real-time, useful information that makes complicated questions easier to answer and improves operational decisions. The testing shows that the system makes working efficiency, energy management, and the ability to plan repairs much better. Moving the system to real-time data merging and looking for other ways to make it better will be the main focus of future work.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic-Constrained Federated Aggregation: Convergence Theory and Privacy-Utility Bounds for Knowledge-Enhanced Distributed Learning</title>
<link>https://arxiv.org/abs/2512.15759</link>
<guid>https://arxiv.org/abs/2512.15759</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated learning, non-IID data, semantic constraints, convergence rate, differential privacy<br /><br />Summary:  
Federated learning facilitates collaborative model training across distributed data sources but often experiences slow convergence when data is non-IID (not independently and identically distributed). This paper introduces Semantic-Constrained Federated Aggregation (SCFA), a novel, theoretically-grounded framework that integrates domain knowledge constraints into the federated learning process to enhance optimization. SCFA achieves a proven convergence rate of O(1/√T + ρ), where ρ measures the rate of constraint violation, marking the first convergence guarantee for constraint-based federated learning. The inclusion of semantic constraints effectively reduces data heterogeneity by 41%, thereby improving training stability and model consistency. Privacy-utility tradeoffs are also enhanced since the hypothesis space is reduced by a factor of θ=0.37, meaning models retain higher accuracy under differential privacy constraints. Specifically, under (ε, δ)-differential privacy with ε=10, SCFA limits utility degradation to 3.7%, a significant improvement over the 12.1% loss observed with standard federated learning, translating to a 2.7× gain. The framework was empirically validated on a manufacturing predictive maintenance task using Bosch production data (1.18 million samples, 968 sensor features) with knowledge graphs encoding 3,000 constraints from ISA-95 and MASON ontologies. Results showed 22% faster convergence, 41.3% reduction in model divergence, and that maintaining constraint violation ρ below 0.05 preserves 90% optimal performance, while values above 0.18 lead to failure. The theoretical analysis strongly correlates with experimental outcomes (R² > 0.90) across metrics of convergence, privacy impact, and constraint violations. <div>
arXiv:2512.15759v1 Announce Type: new 
Abstract: Federated learning enables collaborative model training across distributed data sources but suffers from slow convergence under non-IID data conditions. Existing solutions employ algorithmic modifications treating all client updates identically, ignoring semantic validity. We introduce Semantic-Constrained Federated Aggregation (SCFA), a theoretically-grounded framework incorporating domain knowledge constraints into distributed optimization. We prove SCFA achieves convergence rate O(1/sqrt(T) + rho) where rho represents constraint violation rate, establishing the first convergence theory for constraint-based federated learning. Our analysis shows constraints reduce effective data heterogeneity by 41% and improve privacy-utility tradeoffs through hypothesis space reduction by factor theta=0.37. Under (epsilon,delta)-differential privacy with epsilon=10, constraint regularization maintains utility within 3.7% of non-private baseline versus 12.1% degradation for standard federated learning, representing 2.7x improvement. We validate our framework on manufacturing predictive maintenance using Bosch production data with 1.18 million samples and 968 sensor features, constructing knowledge graphs encoding 3,000 constraints from ISA-95 and MASON ontologies. Experiments demonstrate 22% faster convergence, 41.3% model divergence reduction, and constraint violation thresholds where rho<0.05 maintains 90% optimal performance while rho>0.18 causes catastrophic failure. Our theoretical predictions match empirical observations with R^2>0.90 across convergence, privacy, and violation-performance relationships.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Tutorial on Dimensionless Learning: Geometric Interpretation and the Effect of Noise</title>
<link>https://arxiv.org/abs/2512.15760</link>
<guid>https://arxiv.org/abs/2512.15760</guid>
<content:encoded><![CDATA[
<div> Dimensionless learning, dimensional analysis, machine learning, scaling laws, experimental data<br /><br />Summary:<br /><br />This article introduces "dimensionless learning," a data-driven framework designed to discover dimensionless numbers and scaling laws from experimental measurements. The method integrates classical dimensional analysis with modern machine learning techniques, transforming physical quantity measurements into compact and interpretable physical laws demonstrating dimensional invariance. It identifies fundamental combinations of variables into dimensionless groups and employs neural networks to determine which combinations best predict experimental outcomes. A novel regularization technique promotes coefficients to take simple, interpretable values such as integers or half-integers, ensuring the discovered laws are both accurate and physically meaningful. The authors systematically examine the impact of measurement noise and discrete sampling on the discovery process, showing that their regularization approach enhances robustness against experimental uncertainties. The method effectively handles scenarios with one or multiple dimensionless numbers, revealing that different valid representations can express the same underlying physics. Despite successes, challenges remain, including high computational costs when identifying multiple dimensionless groups, understanding how data characteristics influence the process, automating the selection of relevant variables, and developing user-friendly tools for experimentalists. This tutorial aims to serve both as an educational resource and a practical guide for researchers interested in applying dimensionless learning to experimental data analysis. <div>
arXiv:2512.15760v1 Announce Type: new 
Abstract: Dimensionless learning is a data-driven framework for discovering dimensionless numbers and scaling laws from experimental measurements. This tutorial introduces the method, explaining how it transforms experimental data into compact physical laws that reveal compact dimensional invariance between variables. The approach combines classical dimensional analysis with modern machine learning techniques. Starting from measurements of physical quantities, the method identifies the fundamental ways to combine variables into dimensionless groups, then uses neural networks to discover which combinations best predict the experimental output. A key innovation is a regularization technique that encourages the learned coefficients to take simple, interpretable values like integers or half-integers, making the discovered laws both accurate and physically meaningful. We systematically investigate how measurement noise and discrete sampling affect the discovery process, demonstrating that the regularization approach provides robustness to experimental uncertainties. The method successfully handles cases with single or multiple dimensionless numbers, revealing how different but equivalent representations can capture the same underlying physics. Despite recent progress, key challenges remain, including managing the computational cost of identifying multiple dimensionless groups, understanding the influence of data characteristics, automating the selection of relevant input variables, and developing user-friendly tools for experimentalists. This tutorial serves as both an educational resource and a practical guide for researchers seeking to apply dimensionless learning to their experimental data.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Learning Framework for Thrombosis Risk Prediction in Rotary Blood Pumps</title>
<link>https://arxiv.org/abs/2512.15761</link>
<guid>https://arxiv.org/abs/2512.15761</guid>
<content:encoded><![CDATA[
<div> Keywords: thrombosis, rotary blood pumps, machine learning, computational fluid dynamics, risk prediction  

<br /><br />Summary:  
This study addresses the challenge of predicting thrombosis in rotary blood pumps, which arises from complex flow conditions difficult to characterize with traditional computational models. The authors propose an interpretable machine learning framework that utilizes flow features directly derived from computational fluid dynamics (CFD) simulations. A logistic regression model, combined with a structured feature-selection pipeline, is developed to extract a compact and physically meaningful feature set, including nonlinear combinations of flow parameters. The framework is trained on spatial thrombosis risk patterns generated by a validated macro-scale thrombosis model under two representative scenarios. The trained model successfully replicates labeled risk distributions and identifies specific flow features correlated with elevated thrombosis risk. Notably, when the model is applied to a centrifugal pump—despite being trained on data from an axial pump at a single operating point—it predicts plausible thrombosis-prone regions, highlighting its generalizability. This approach provides a transparent mechanistic link between local flow characteristics and thrombosis risk, maintaining computational efficiency. The low computational overhead facilitates rapid thrombogenicity screening without the need for repeated or computationally expensive simulations. The framework thus complements existing physics-based thrombosis modeling and establishes a foundation for integrating interpretable machine learning into CFD-driven thrombosis assessment and medical device design workflows. <div>
arXiv:2512.15761v1 Announce Type: new 
Abstract: Thrombosis in rotary blood pumps arises from complex flow conditions that remain difficult to translate into reliable and interpretable risk predictions using existing computational models. This limitation reflects an incomplete understanding of how specific flow features contribute to thrombus initiation and growth. This study introduces an interpretable machine learning framework for spatial thrombosis assessment based directly on computational fluid dynamics-derived flow features. A logistic regression (LR) model combined with a structured feature-selection pipeline is used to derive a compact and physically interpretable feature set, including nonlinear feature combinations. The framework is trained using spatial risk patterns from a validated, macro-scale thrombosis model for two representative scenarios. The model reproduces the labeled risk distributions and identifies distinct sets of flow features associated with increased thrombosis risk. When applied to a centrifugal pump, despite training on a single axial pump operating point, the model predicts plausible thrombosis-prone regions. These results show that interpretable machine learning can link local flow features to thrombosis risk while remaining computationally efficient and mechanistically transparent. The low computational cost enables rapid thrombogenicity screening without repeated or costly simulations. The proposed framework complements physics-based thrombosis modeling and provides a methodological basis for integrating interpretable machine learning into CFD-driven thrombosis analysis and device design workflows.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Sample Augmented Test-Time Adaptation for Personalized Intraoperative Hypotension Prediction</title>
<link>https://arxiv.org/abs/2512.15762</link>
<guid>https://arxiv.org/abs/2512.15762</guid>
<content:encoded><![CDATA[
<div> Keywords: intraoperative hypotension, test-time adaptation, cross-sample augmentation, time series forecasting, masked reconstruction<br /><br />Summary:<br /><br />Intraoperative hypotension (IOH) significantly increases surgical risk but predicting it accurately remains difficult due to variability between patients. To improve personalized prediction, the authors propose CSA-TTA, a Cross-Sample Augmented Test-Time Adaptation framework. CSA-TTA addresses the challenge of sparse IOH events by augmenting test-time training data with hypotension samples from other patients. This is achieved through a cross-sample bank that categorizes historical data into hypotensive and non-hypotensive segments. The framework employs a coarse-to-fine retrieval method starting with K-Shape clustering to find representative centers and then retrieving the top-K semantically similar samples to the current patient. Further, CSA-TTA integrates self-supervised masked reconstruction and retrospective sequence forecasting during training, enhancing model sensitivity to rapid, subtle intraoperative changes. The method was evaluated on the VitalDB and a real-world clinical dataset, integrated with leading time series forecasting models such as TimesFM and UniTS. Results show consistent performance improvements, including Recall and F1 score increases of +1.33% and +1.13% under fine-tuning, and larger gains of +7.46% and +5.07% in zero-shot testing scenarios. This demonstrates CSA-TTA’s robustness, adaptability, and potential for clinical application in real-time IOH prediction. <div>
arXiv:2512.15762v1 Announce Type: new 
Abstract: Intraoperative hypotension (IOH) poses significant surgical risks, but accurate prediction remains challenging due to patient-specific variability. While test-time adaptation (TTA) offers a promising approach for personalized prediction, the rarity of IOH events often leads to unreliable test-time training. To address this, we propose CSA-TTA, a novel Cross-Sample Augmented Test-Time Adaptation framework that enhances training by incorporating hypotension events from other individuals. Specifically, we first construct a cross-sample bank by segmenting historical data into hypotensive and non-hypotensive samples. Then, we introduce a coarse-to-fine retrieval strategy for building test-time training data: we initially apply K-Shape clustering to identify representative cluster centers and subsequently retrieve the top-K semantically similar samples based on the current patient signal. Additionally, we integrate both self-supervised masked reconstruction and retrospective sequence forecasting signals during training to enhance model adaptability to rapid and subtle intraoperative dynamics. We evaluate the proposed CSA-TTA on both the VitalDB dataset and a real-world in-hospital dataset by integrating it with state-of-the-art time series forecasting models, including TimesFM and UniTS. CSA-TTA consistently enhances performance across settings-for instance, on VitalDB, it improves Recall and F1 scores by +1.33% and +1.13%, respectively, under fine-tuning, and by +7.46% and +5.07% in zero-shot scenarios-demonstrating strong robustness and generalization.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdaGradSelect: An adaptive gradient-guided layer selection method for efficient fine-tuning of SLMs</title>
<link>https://arxiv.org/abs/2512.15764</link>
<guid>https://arxiv.org/abs/2512.15764</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Parameter-Efficient Fine-Tuning, AdaGradSelect, transformer blocks, resource efficiency<br /><br />Summary:<br /><br />Large Language Models (LLMs) require substantial computational resources for full fine-tuning, which can be costly and memory-intensive. Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA mitigate this by applying small, low-rank updates to frozen weights but limit the trainable subspace, potentially lowering performance. For Small Language Models (SLMs), which demand even greater efficiency, the authors propose AdaGradSelect, an adaptive fine-tuning approach that intelligently selects transformer blocks to update based on gradient information. Initial observations revealed that updating only the blocks with the highest gradient norms yields performance close to full fine-tuning. AdaGradSelect builds on this via Dirichlet-based sampling, accounting for how often blocks were updated, and employs an epsilon-greedy exploration to balance discovering important blocks early on and exploiting them later. Experimental results demonstrate that AdaGradSelect trains approximately 12% faster and consumes 35% less GPU memory while maintaining comparable accuracy. On the GSM8K dataset, it surpasses LoRA (rank 256) by roughly 3% on average across several models such as Qwen2.5-0.5B, LLaMA3.2-1B, and Phi4-mini-3.8B, and shows similar performance gains on the MATH dataset. Overall, AdaGradSelect offers a more effective and resource-efficient alternative to conventional fine-tuning approaches for SLMs. <div>
arXiv:2512.15764v1 Announce Type: new 
Abstract: Large Language Models (LLMs) can perform many NLP tasks well, but fully fine-tuning them is expensive and requires a lot of memory. Parameter-Efficient Fine-Tuning (PEFT) methods such as LoRA reduce this cost by adding small low-rank updates to frozen model weights. However, these methods restrict the training to a limited subspace, which can sometimes reduce performance.
  For Small Language Models (SLMs), where efficiency gains matter even more, we introduce AdaGradSelect, an adaptive method that selects which transformer blocks to update based on gradients.
  Early observations showed that updating only the transformer blocks with the highest gradient norms can achieve performance close to full fine-tuning. Building on this insight, AdaGradSelect adaptively chooses which blocks to train. It uses a combination of Dirichlet-based sampling, which depends on how frequently blocks were updated in the past, and an epsilon-greedy exploration strategy. This lets the method explore different blocks in early training and gradually focus on the most important ones in later epochs.
  Experiments show that AdaGradSelect trains about 12 percent faster and uses 35 percent less GPU memory while delivering performance very close to full fine-tuning. On the GSM8K dataset, it outperforms LoRA (rank 256) by about 3 percent on average across models such as Qwen2.5-0.5B, LLaMA3.2-1B, and Phi4-mini-3.8B. It also achieves similar accuracy on the MATH dataset. Overall, AdaGradSelect provides a more effective and resource-efficient alternative to traditional fine-tuning methods.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data Valuation for LLM Fine-Tuning: Efficient Shapley Value Approximation via Language Model Arithmetic</title>
<link>https://arxiv.org/abs/2512.15765</link>
<guid>https://arxiv.org/abs/2512.15765</guid>
<content:encoded><![CDATA[
<div> Data valuation, large language models, Shapley value, Direct Preference Optimization, cooperative game theory<br /><br />Summary:  
This paper addresses the critical role of data in training large language models (LLMs), highlighting the challenges faced by data owners when deciding how to invest in data curation and acquisition strategies. It emphasizes the importance of fair and effective collaboration among multiple data owners who want to pool their datasets to develop superior models and equitably distribute the resulting benefits. The concept of data valuation is explored through the framework of cooperative game theory, with a particular focus on the Shapley value as a method for evaluating the contribution of individual datasets. However, the high computational cost of calculating Shapley values due to multiple model retrainings poses a significant barrier, especially for large-scale models. The authors propose a solution by leveraging the unique mathematical structure of Direct Preference Optimization (DPO), a training paradigm for LLMs, which drastically reduces the complexity of Shapley value computations. This development opens up new possibilities for scalable and practical data valuation in LLMs, potentially facilitating more informed data investment decisions and encouraging collaborative data sharing initiatives through a fair benefit distribution mechanism. The work bridges data valuation theory and LLM training, unlocking practical applications at this intersection. <div>
arXiv:2512.15765v1 Announce Type: new 
Abstract: Data is a critical asset for training large language models (LLMs), alongside compute resources and skilled workers. While some training data is publicly available, substantial investment is required to generate proprietary datasets, such as human preference annotations or to curate new ones from existing sources. As larger datasets generally yield better model performance, two natural questions arise. First, how can data owners make informed decisions about curation strategies and data sources investment? Second, how can multiple data owners collaboratively pool their resources to train superior models while fairly distributing the benefits? This problem, data valuation, which is not specific to large language models, has been addressed by the machine learning community through the lens of cooperative game theory, with the Shapley value being the prevalent solution concept. However, computing Shapley values is notoriously expensive for data valuation, typically requiring numerous model retrainings, which can become prohibitive for large machine learning models. In this work, we demonstrate that this computational challenge is dramatically simplified for LLMs trained with Direct Preference Optimization (DPO). We show how the specific mathematical structure of DPO enables scalable Shapley value computation. We believe this observation unlocks many applications at the intersection of data valuation and large language models.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Data and Physics: A Graph Neural Network-Based Hybrid Twin Framework</title>
<link>https://arxiv.org/abs/2512.15767</link>
<guid>https://arxiv.org/abs/2512.15767</guid>
<content:encoded><![CDATA[
<div> Keywords: Ignorance model, Hybrid twin, Graph Neural Networks, Sparse spatial data, Nonlinear heat transfer<br /><br />Summary:  
This paper addresses the challenge of simulating complex unsteady physical phenomena through mathematical models such as the Finite Element Method (FEM), which often suffer from inaccuracies due to unmodeled effects referred to as the ignorance model. Traditional data-driven approaches that aim to learn full system behavior require extensive high-quality spatial and temporal data, which is typically unavailable in practical scenarios, limiting their reliability. The authors propose a hybrid twin approach that models only the ignorance component rather than the entire physical response, leveraging the fact that the ignorance is less complex and thus can be learned with fewer data. A key challenge they overcome is the sparsity of spatial measurements and difficulty in acquiring data across varied spatial configurations. To tackle this, they utilize Graph Neural Networks (GNNs) to learn spatial patterns of missing physics from limited measurement locations. This enables enriching physics-based models with data-driven corrections without demanding dense data coverage across space, time, or parameters. The method is demonstrated on nonlinear heat transfer problems involving different mesh layouts, geometries, and load positions. Results indicate that the GNN-based hybrid twin effectively captures the ignorance, generalizes corrections across spatial configurations, enhances simulation accuracy, and improves interpretability while reducing data requirements. <div>
arXiv:2512.15767v1 Announce Type: new 
Abstract: Simulating complex unsteady physical phenomena relies on detailed mathematical models, simulated for instance by using the Finite Element Method (FEM). However, these models often exhibit discrepancies from the reality due to unmodeled effects or simplifying assumptions. We refer to this gap as the ignorance model. While purely data-driven approaches attempt to learn full system behavior, they require large amounts of high-quality data across the entire spatial and temporal domain. In real-world scenarios, such information is unavailable, making full data-driven modeling unreliable. To overcome this limitation, we model of the ignorance component using a hybrid twin approach, instead of simulating phenomena from scratch. Since physics-based models approximate the overall behavior of the phenomena, the remaining ignorance is typically lower in complexity than the full physical response, therefore, it can be learned with significantly fewer data. A key difficulty, however, is that spatial measurements are sparse, also obtaining data measuring the same phenomenon for different spatial configurations is challenging in practice. Our contribution is to overcome this limitation by using Graph Neural Networks (GNNs) to represent the ignorance model. GNNs learn the spatial pattern of the missing physics even when the number of measurement locations is limited. This allows us to enrich the physics-based model with data-driven corrections without requiring dense spatial, temporal and parametric data. To showcase the performance of the proposed method, we evaluate this GNN-based hybrid twin on nonlinear heat transfer problems across different meshes, geometries, and load positions. Results show that the GNN successfully captures the ignorance and generalizes corrections across spatial configurations, improving simulation accuracy and interpretability, while minimizing data requirements.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TENG++: Time-Evolving Natural Gradient for Solving PDEs With Deep Neural Nets under General Boundary Conditions</title>
<link>https://arxiv.org/abs/2512.15771</link>
<guid>https://arxiv.org/abs/2512.15771</guid>
<content:encoded><![CDATA[
<div> Keywords: Partial Differential Equations, Physics-Informed Neural Networks, Time-Evolving Natural Gradient, Dirichlet Boundary Conditions, Numerical Time-Stepping  

<br /><br />Summary:  
This paper addresses the limitations of traditional numerical methods and Physics-Informed Neural Networks (PINNs) in solving complex Partial Differential Equations (PDEs), particularly in enforcing boundary conditions with high accuracy. The authors extend the Time-Evolving Natural Gradient (TENG) framework to incorporate Dirichlet boundary conditions by embedding penalty terms into the loss function, ensuring precise constraint satisfaction. The approach integrates natural gradient optimization with classical time-stepping schemes such as Euler and Heun methods, balancing computational stability and solution accuracy. Experimental results on the heat equation indicate that the Heun method achieves superior accuracy due to second-order correction terms, whereas the Euler method provides computational efficiency for simpler problems. This enhancement broadens the applicability of neural network-based PDE solvers and sets groundwork for future extensions to other boundary types, including Neumann and mixed conditions, as well as more diverse classes of PDEs. Overall, the work demonstrates the potential of combining physics-informed deep learning frameworks with advanced optimization and numerical techniques to tackle real-world PDE problems more effectively. <div>
arXiv:2512.15771v1 Announce Type: new 
Abstract: Partial Differential Equations (PDEs) are central to modeling complex systems across physical, biological, and engineering domains, yet traditional numerical methods often struggle with high-dimensional or complex problems. Physics-Informed Neural Networks (PINNs) have emerged as an efficient alternative by embedding physics-based constraints into deep learning frameworks, but they face challenges in achieving high accuracy and handling complex boundary conditions. In this work, we extend the Time-Evolving Natural Gradient (TENG) framework to address Dirichlet boundary conditions, integrating natural gradient optimization with numerical time-stepping schemes, including Euler and Heun methods, to ensure both stability and accuracy. By incorporating boundary condition penalty terms into the loss function, the proposed approach enables precise enforcement of Dirichlet constraints. Experiments on the heat equation demonstrate the superior accuracy of the Heun method due to its second-order corrections and the computational efficiency of the Euler method for simpler scenarios. This work establishes a foundation for extending the framework to Neumann and mixed boundary conditions, as well as broader classes of PDEs, advancing the applicability of neural network-based solvers for real-world problems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TS-DP: Reinforcement Speculative Decoding For Temporal Adaptive Diffusion Policy Acceleration</title>
<link>https://arxiv.org/abs/2512.15773</link>
<guid>https://arxiv.org/abs/2512.15773</guid>
<content:encoded><![CDATA[
<div> Diffusion Policy, Speculative Decoding, Reinforcement Learning, Embodied Control, Temporal Adaptivity<br /><br />Summary:<br /><br />This paper addresses the high inference latency and computational cost of Diffusion Policy (DP) in embodied control tasks, which arise due to multiple iterative denoising steps. Unlike static acceleration methods such as quantization, which fail to adapt to dynamic task difficulties, the authors propose leveraging speculative decoding as a lossless and adaptive acceleration technique for DP. They introduce Temporal-aware Reinforcement-based Speculative Diffusion Policy (TS-DP), the first framework to combine speculative decoding with temporal adaptivity for dynamic embodied environments. TS-DP involves two main components: a Transformer-based drafter distilled to imitate the base model and reduce expensive denoising calls, and a reinforcement learning (RL) based scheduler that dynamically adjusts speculative decoding parameters according to varying task difficulty, balancing accuracy and efficiency. Extensive experiments across various embodied environments demonstrate that TS-DP can achieve up to 4.17 times faster inference with more than 94% of drafts accepted by the base model. This corresponds to an inference speed of 25 Hz, enabling real-time diffusion-based control without sacrificing performance. The proposed approach thus effectively adapts computation in response to temporal variations in task complexity, substantially improving the practicality of DP for real-world embodied control applications. <div>
arXiv:2512.15773v1 Announce Type: new 
Abstract: Diffusion Policy (DP) excels in embodied control but suffers from high inference latency and computational cost due to multiple iterative denoising steps. The temporal complexity of embodied tasks demands a dynamic and adaptable computation mode. Static and lossy acceleration methods, such as quantization, fail to handle such dynamic embodied tasks, while speculative decoding offers a lossless and adaptive yet underexplored alternative for DP. However, it is non-trivial to address the following challenges: how to match the base model's denoising quality at lower cost under time-varying task difficulty in embodied settings, and how to dynamically and interactively adjust computation based on task difficulty in such environments. In this paper, we propose Temporal-aware Reinforcement-based Speculative Diffusion Policy (TS-DP), the first framework that enables speculative decoding for DP with temporal adaptivity. First, to handle dynamic environments where task difficulty varies over time, we distill a Transformer-based drafter to imitate the base model and replace its costly denoising calls. Second, an RL-based scheduler further adapts to time-varying task difficulty by adjusting speculative parameters to maintain accuracy while improving efficiency. Extensive experiments across diverse embodied environments demonstrate that TS-DP achieves up to 4.17 times faster inference with over 94% accepted drafts, reaching an inference frequency of 25 Hz and enabling real-time diffusion-based control without performance degradation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarial Robustness in Financial Machine Learning: Defenses, Economic Impact, and Governance Evidence</title>
<link>https://arxiv.org/abs/2512.15780</link>
<guid>https://arxiv.org/abs/2512.15780</guid>
<content:encoded><![CDATA[
<div> adversarial robustness, tabular data, financial decision making, credit scoring, fraud detection<br /><br />Summary:<br /><br />This study investigates the adversarial robustness of machine learning models applied to tabular data within financial decision-making contexts, such as credit scoring and fraud detection. The authors utilize gradient-based attack methods to introduce small perturbations to the input data in order to assess the vulnerability of these models. Key performance metrics analyzed include discrimination (the model’s ability to distinguish between different classes), calibration (the alignment of predicted probabilities with actual outcomes), and financial risk measures pertinent to real-world applications. The results reveal that even minimal adversarial perturbations can significantly degrade model performance across these metrics, posing concerns about the reliability and trustworthiness of such models when deployed in sensitive financial environments. To counteract this vulnerability, the study also explores adversarial training as a defense mechanism, demonstrating that it can partially restore model robustness and mitigate some of the adverse effects induced by the attacks. However, recovery is noted to be incomplete, indicating ongoing challenges remain in fully safeguarding financial machine learning systems against adversarial threats. Overall, this work highlights the importance of considering adversarial robustness in the deployment of tabular machine learning models for financial decision-making tasks. <div>
arXiv:2512.15780v1 Announce Type: new 
Abstract: We evaluate adversarial robustness in tabular machine learning models used in financial decision making. Using credit scoring and fraud detection data, we apply gradient based attacks and measure impacts on discrimination, calibration, and financial risk metrics. Results show notable performance degradation under small perturbations and partial recovery through adversarial training.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Boosting t-SNE Efficiency for Sequencing Data: Insights from Kernel Selection</title>
<link>https://arxiv.org/abs/2512.15900</link>
<guid>https://arxiv.org/abs/2512.15900</guid>
<content:encoded><![CDATA[
<div> kernel selection, t-SNE, biological sequences, cosine similarity, dimensionality reduction<br /><br />Summary:<br /><br />This study evaluates the effectiveness of different kernel functions in t-distributed Stochastic Neighbor Embedding (t-SNE) for dimensionality reduction of high-dimensional biological sequencing data. Traditionally, the Gaussian kernel is used in t-SNE to compute pairwise similarities, but it has limitations regarding scalability and its ability to capture categorical sequence data characteristics. The isolation kernel has been proposed as an alternative, but may not optimally measure sequence similarities. The authors examine nine kernel functions applied to molecular sequences with three embedding methods: One-Hot Encoding, Spike2Vec, and minimizers. Using subjective visualization and objective metrics such as neighborhood preservation scores, the results show the cosine similarity kernel generally outperforms other kernels including Gaussian and isolation kernels. The cosine kernel provides better runtime efficiency and preserves pairwise distances more accurately in low-dimensional embeddings. The study further validates these findings through classification and clustering experiments on six diverse biological datasets (Spike7k, Host, ShortRead, Rabies, Genome, and Breast Cancer) using various machine learning algorithms and evaluation metrics. Overall, kernel choice significantly impacts visualization quality and downstream analysis performance, with the cosine similarity kernel proving the most robust and suitable for large-scale biological sequence analysis across multiple embedding methods and dataset types. <div>
arXiv:2512.15900v1 Announce Type: new 
Abstract: Dimensionality reduction techniques are essential for visualizing and analyzing high-dimensional biological sequencing data. t-distributed Stochastic Neighbor Embedding (t-SNE) is widely used for this purpose, traditionally employing the Gaussian kernel to compute pairwise similarities. However, the Gaussian kernel's lack of data-dependence and computational overhead limit its scalability and effectiveness for categorical biological sequences. Recent work proposed the isolation kernel as an alternative, yet it may not optimally capture sequence similarities. In this study, we comprehensively evaluate nine different kernel functions for t-SNE applied to molecular sequences, using three embedding methods: One-Hot Encoding, Spike2Vec, and minimizers. Through both subjective visualization and objective metrics (including neighborhood preservation scores), we demonstrate that the cosine similarity kernel in general outperforms other kernels, including Gaussian and isolation kernels, achieving superior runtime efficiency and better preservation of pairwise distances in low-dimensional space. We further validate our findings through extensive classification and clustering experiments across six diverse biological datasets (Spike7k, Host, ShortRead, Rabies, Genome, and Breast Cancer), employing multiple machine learning algorithms and evaluation metrics. Our results show that kernel selection significantly impacts not only visualization quality but also downstream analytical tasks, with the cosine similarity kernel providing the most robust performance across different data types and embedding strategies, making it particularly suitable for large-scale biological sequence analysis.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Introduction to Symbolic Regression in the Physical Sciences</title>
<link>https://arxiv.org/abs/2512.15920</link>
<guid>https://arxiv.org/abs/2512.15920</guid>
<content:encoded><![CDATA[
<div> Keywords: symbolic regression, physical sciences, equation discovery, empirical modelling, AI integration  

<br /><br />Summary:  
This article introduces a Special Issue on Symbolic Regression (SR) for the Physical Sciences, inspired by the Royal Society discussion meeting in April 2025. SR is presented as a powerful tool for deriving interpretable mathematical relationships directly from data, providing new pathways for scientific discovery and efficient empirical modelling. The issue showcases a variety of applications, including automated equation discovery, modeling emergent phenomena, and building compact surrogate models to speed up computationally expensive simulations. The introductory review explains the core concepts of SR and compares it with traditional regression techniques. It highlights key use cases in physical sciences, such as generating effective theories and empirical functional forms. Methodologically, the article discusses important factors like search-space design, operator choice, controlling model complexity, feature selection, and how SR can be combined with modern AI methods. Challenges remain in scaling SR algorithms, ensuring robustness against noise, avoiding overfitting, and managing computational costs. Finally, the article points to promising future directions, emphasizing the integration of theoretical information such as symmetry constraints and asymptotic behavior. Altogether, the Special Issue reflects rapid advancements in SR and its increasing significance for research across the physical sciences. <div>
arXiv:2512.15920v1 Announce Type: new 
Abstract: Symbolic regression (SR) has emerged as a powerful method for uncovering interpretable mathematical relationships from data, offering a novel route to both scientific discovery and efficient empirical modelling. This article introduces the Special Issue on Symbolic Regression for the Physical Sciences, motivated by the Royal Society discussion meeting held in April 2025. The contributions collected here span applications from automated equation discovery and emergent-phenomena modelling to the construction of compact emulators for computationally expensive simulations.
  The introductory review outlines the conceptual foundations of SR, contrasts it with conventional regression approaches, and surveys its main use cases in the physical sciences, including the derivation of effective theories, empirical functional forms and surrogate models. We summarise methodological considerations such as search-space design, operator selection, complexity control, feature selection, and integration with modern AI approaches. We also highlight ongoing challenges, including scalability, robustness to noise, overfitting and computational complexity. Finally we emphasise emerging directions, particularly the incorporation of symmetry constraints, asymptotic behaviour and other theoretical information. Taken together, the papers in this Special Issue illustrate the accelerating progress of SR and its growing relevance across the physical sciences.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Unification of Discrete, Gaussian, and Simplicial Diffusion</title>
<link>https://arxiv.org/abs/2512.15923</link>
<guid>https://arxiv.org/abs/2512.15923</guid>
<content:encoded><![CDATA[
<div> Keywords: discrete diffusion, Wright-Fisher model, Gaussian diffusion, simplicial diffusion, DNA sequence modeling<br /><br />Summary: This paper addresses the challenge of modeling discrete sequences such as DNA, proteins, and language using diffusion methods. It identifies three main approaches: discrete diffusion in its native space, Gaussian diffusion in Euclidean space, and diffusion on the simplex. Each approach has its own advantages and disadvantages, with discrete diffusion naturally fitting the domain, Gaussian methods benefiting from mature algorithms, and simplicial diffusion theoretically combining the strengths but suffering from numerical instability. The authors propose a unified theoretical framework based on the Wright-Fisher population genetics model, under which all three diffusion methods emerge as different parametric limits. Specifically, Gaussian and simplicial diffusions correspond to large-population limits of this underlying process. This framework bridges gaps in prior theories by formally connecting the models' likelihoods and hyperparameters, while also resolving stability issues in simplicial diffusion. Practically, the paper demonstrates that a single trained model can perform diffusion across any of these three domains at test time, removing the need for practitioners to choose between trade-offs. Experiments highlight that Wright-Fisher simplicial diffusion is more stable and outperforms previous simplicial models in conditional DNA generation tasks. Furthermore, models trained jointly on multiple diffusion domains achieve performance competitive with those specialized to any single domain. <div>
arXiv:2512.15923v1 Announce Type: new 
Abstract: To model discrete sequences such as DNA, proteins, and language using diffusion, practitioners must choose between three major methods: diffusion in discrete space, Gaussian diffusion in Euclidean space, or diffusion on the simplex. Despite their shared goal, these models have disparate algorithms, theoretical structures, and tradeoffs: discrete diffusion has the most natural domain, Gaussian diffusion has more mature algorithms, and diffusion on the simplex in principle combines the strengths of the other two but in practice suffers from a numerically unstable stochastic processes. Ideally we could see each of these models as instances of the same underlying framework, and enable practitioners to switch between models for downstream applications. However previous theories have only considered connections in special cases. Here we build a theory unifying all three methods of discrete diffusion as different parameterizations of the same underlying process: the Wright-Fisher population genetics model. In particular, we find simplicial and Gaussian diffusion as two large-population limits. Our theory formally connects the likelihoods and hyperparameters of these models and leverages decades of mathematical genetics literature to unlock stable simplicial diffusion. Finally, we relieve the practitioner of balancing model trade-offs by demonstrating it is possible to train a single model that can perform diffusion in any of these three domains at test time. Our experiments show that Wright-Fisher simplicial diffusion is more stable and outperforms previous simplicial diffusion models on conditional DNA generation. We also show that we can train models on multiple domains at once that are competitive with models trained on any individual domain.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DSO: Direct Steering Optimization for Bias Mitigation</title>
<link>https://arxiv.org/abs/2512.15926</link>
<guid>https://arxiv.org/abs/2512.15926</guid>
<content:encoded><![CDATA[
<div> Bias Mitigation, Vision-Language Models, Activation Steering, Reinforcement Learning, Fairness Control<br /><br />Summary:<br /><br />1. The paper addresses bias in generative models, specifically vision-language models (VLMs), which can produce biased decisions based on perceived demographic attributes, negatively affecting outcomes such as identifying professionals like doctors.  
2. It highlights the challenge users face in balancing bias reduction with maintaining overall model performance, emphasizing the need for inference-time methods that allow controllable trade-offs.  
3. Activation steering is identified as a promising technique for controllable model behavior during inference but is limited in effectively correcting biases where equal outcomes across demographics are required.  
4. To overcome these limitations, the authors propose Direct Steering Optimization (DSO), a novel approach that uses reinforcement learning to learn linear transformations of model activations aimed specifically at mitigating bias while preserving model capabilities.  
5. Experiments demonstrate that DSO achieves a superior fairness-performance trade-off on both VLMs and large language models (LLMs) compared to existing methods, providing practitioners with practical control over bias mitigation during inference. The work underscores the importance of directly optimized steering strategies for more effective bias intervention than heuristic-based methods. <div>
arXiv:2512.15926v1 Announce Type: new 
Abstract: Generative models are often deployed to make decisions on behalf of users, such as vision-language models (VLMs) identifying which person in a room is a doctor to help visually impaired individuals. Yet, VLM decisions are influenced by the perceived demographic attributes of people in the input, which can lead to biased outcomes like failing to identify women as doctors. Moreover, when reducing bias leads to performance loss, users may have varying needs for balancing bias mitigation with overall model capabilities, highlighting the demand for methods that enable controllable bias reduction during inference. Activation steering is a popular approach for inference-time controllability that has shown potential in inducing safer behavior in large language models (LLMs). However, we observe that current steering methods struggle to correct biases, where equiprobable outcomes across demographic groups are required. To address this, we propose Direct Steering Optimization (DSO) which uses reinforcement learning to find linear transformations for steering activations, tailored to mitigate bias while maintaining control over model performance. We demonstrate that DSO achieves state-of-the-art trade-off between fairness and capabilities on both VLMs and LLMs, while offering practitioners inference-time control over the trade-off. Overall, our work highlights the benefit of designing steering strategies that are directly optimized to control model behavior, providing more effective bias intervention than methods that rely on pre-defined heuristics for controllability.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BarcodeMamba+: Advancing State-Space Models for Fungal Biodiversity Research</title>
<link>https://arxiv.org/abs/2512.15931</link>
<guid>https://arxiv.org/abs/2512.15931</guid>
<content:encoded><![CDATA[
<div> Keywords: fungal taxonomy, DNA barcodes, foundation model, state-space model, hierarchical label smoothing<br /><br />Summary:<br /><br />1. The paper addresses the challenge of accurate taxonomic classification of fungi from DNA barcodes, a critical task for global biodiversity monitoring complicated by sparse labeling and long-tailed taxa distributions.  
2. Traditional supervised learning struggles in this domain notably due to difficulties in generalizing to unseen species and managing the hierarchical nature of fungal taxonomy.  
3. To overcome these issues, the authors introduce BarcodeMamba+, a foundation model based on an efficient state-space model architecture designed specifically for fungal barcode classification.  
4. A pretrain and fine-tune training paradigm leveraging partially labeled data is employed, demonstrated to be significantly more effective than conventional fully supervised approaches in data-sparse scenarios.  
5. During fine-tuning, the model integrates key enhancements including hierarchical label smoothing, a weighted loss function, and a multi-head output layer inherited from MycoAI, each contributing notable performance improvements.  
6. Evaluation on a challenging fungal classification benchmark with distinct taxonomic distribution shifts confirms that BarcodeMamba+ outperforms existing methods at all taxonomic levels.  
7. The work establishes a powerful, scalable framework for genomics-based biodiversity research and provides an open-source implementation available at https://github.com/bioscan-ml/BarcodeMamba. <div>
arXiv:2512.15931v1 Announce Type: new 
Abstract: Accurate taxonomic classification from DNA barcodes is a cornerstone of global biodiversity monitoring, yet fungi present extreme challenges due to sparse labelling and long-tailed taxa distributions. Conventional supervised learning methods often falter in this domain, struggling to generalize to unseen species and to capture the hierarchical nature of the data. To address these limitations, we introduce BarcodeMamba+, a foundation model for fungal barcode classification built on a powerful and efficient state-space model architecture. We employ a pretrain and fine-tune paradigm, which utilizes partially labelled data and we demonstrate this is substantially more effective than traditional fully-supervised methods in this data-sparse environment. During fine-tuning, we systematically integrate and evaluate a suite of enhancements--including hierarchical label smoothing, a weighted loss function, and a multi-head output layer from MycoAI--to specifically tackle the challenges of fungal taxonomy. Our experiments show that each of these components yields significant performance gains. On a challenging fungal classification benchmark with distinct taxonomic distribution shifts from the broad training set, our final model outperforms a range of existing methods across all taxonomic levels. Our work provides a powerful new tool for genomics-based biodiversity research and establishes an effective and scalable training paradigm for this challenging domain. Our code is publicly available at https://github.com/bioscan-ml/BarcodeMamba.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-Context Semi-Supervised Learning</title>
<link>https://arxiv.org/abs/2512.15934</link>
<guid>https://arxiv.org/abs/2512.15934</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformers, in-context learning, semi-supervised learning, unlabeled context, representation learning<br /><br />Summary:<br />1. The paper addresses the capacity of Transformers for in-context learning (ICL), highlighting that most prior theoretical work focuses on supervised settings where labeled example pairs are explicitly provided.<br />2. It recognizes that in practical scenarios, Transformers often perform well even when labels are sparse or absent, implying that there is significant structure in the unlabeled contextual demonstrations.<br />3. To explore this, the authors introduce the concept of in-context semi-supervised learning (IC-SSL), which combines a small number of labeled examples with many unlabeled points within the context.<br />4. The study demonstrates that Transformers can leverage the unlabeled context effectively to learn robust, context-dependent representations that improve prediction accuracy.<br />5. This approach particularly benefits low-label regimes, providing foundational insights into how Transformers exploit unlabeled context for representation learning within the ICL framework, thus advancing understanding of semi-supervised learning in transformer models. <div>
arXiv:2512.15934v1 Announce Type: new 
Abstract: There has been significant recent interest in understanding the capacity of Transformers for in-context learning (ICL), yet most theory focuses on supervised settings with explicitly labeled pairs. In practice, Transformers often perform well even when labels are sparse or absent, suggesting crucial structure within unlabeled contextual demonstrations. We introduce and study in-context semi-supervised learning (IC-SSL), where a small set of labeled examples is accompanied by many unlabeled points, and show that Transformers can leverage the unlabeled context to learn a robust, context-dependent representation. This representation enables accurate predictions and markedly improves performance in low-label regimes, offering foundational insights into how Transformers exploit unlabeled context for representation learning within the ICL framework.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SALVE: Sparse Autoencoder-Latent Vector Editing for Mechanistic Control of Neural Networks</title>
<link>https://arxiv.org/abs/2512.15938</link>
<guid>https://arxiv.org/abs/2512.15938</guid>
<content:encoded><![CDATA[
<div> Keywords: Sparse Autoencoder, Model Interpretability, Feature Editing, Grad-FAM, Robustness Diagnostics<br /><br />Summary:<br /><br />1. The paper introduces SALVE (Sparse Autoencoder-Latent Vector Editing), a unified framework aimed at discovering, validating, and controlling neural network features for improved interpretability and model editing.  
2. SALVE utilizes an $\ell_1$-regularized autoencoder to learn sparse, model-native feature bases without requiring supervision, enabling effective feature discovery.  
3. Validation of these features is performed using Grad-FAM, a new feature-level saliency mapping technique that visually ties latent features back to input data, facilitating mechanistic interpretability.  
4. SALVE supports precise and permanent weight-space interventions within the autoencoder structure, permitting continuous modulation of both class-specific and cross-class features, allowing actionable control over model behavior.  
5. The framework introduces a critical suppression threshold, $\alpha_{crit}$, which quantifies how much each class depends on its dominant feature, providing fine-grained diagnostics of model robustness.  
6. Experiments demonstrate SALVE's applicability and effectiveness on both convolutional neural networks (ResNet-18) and transformer models (ViT-B/16), confirming its generality and consistent control over model behavior.  
7. Overall, this work presents a principled methodology that bridges feature discovery with direct model editing, advancing transparent, interpretable, and controllable AI systems. <div>
arXiv:2512.15938v1 Announce Type: new 
Abstract: Deep neural networks achieve impressive performance but remain difficult to interpret and control. We present SALVE (Sparse Autoencoder-Latent Vector Editing), a unified "discover, validate, and control" framework that bridges mechanistic interpretability and model editing. Using an $\ell_1$-regularized autoencoder, we learn a sparse, model-native feature basis without supervision. We validate these features with Grad-FAM, a feature-level saliency mapping method that visually grounds latent features in input data. Leveraging the autoencoder's structure, we perform precise and permanent weight-space interventions, enabling continuous modulation of both class-defining and cross-class features. We further derive a critical suppression threshold, $\alpha_{crit}$, quantifying each class's reliance on its dominant feature, supporting fine-grained robustness diagnostics. Our approach is validated on both convolutional (ResNet-18) and transformer-based (ViT-B/16) models, demonstrating consistent, interpretable control over their behavior. This work contributes a principled methodology for turning feature discovery into actionable model edits, advancing the development of transparent and controllable AI systems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AIE4ML: An End-to-End Framework for Compiling Neural Networks for the Next Generation of AMD AI Engines</title>
<link>https://arxiv.org/abs/2512.15946</link>
<guid>https://arxiv.org/abs/2512.15946</guid>
<content:encoded><![CDATA[
<div> arXiv, AMD Versal AI Engine, AIE4ML framework, neural network execution, on-chip parallelization<br /><br />Summary:  
This paper addresses the challenges of performing efficient AI inference on AMD’s Versal AI Engine (AIE), focusing on its complex VLIW execution, explicit datapaths, and local memory management. The authors introduce AIE4ML, the first comprehensive framework designed to automatically convert AI models into optimized firmware specifically for AIE-ML generation devices, with forward compatibility for AIE-MLv2 architectures. At the single-kernel level, AIE4ML achieves performance near the hardware's peak. For full neural network execution, the framework provides a structured parallelization strategy that scales across the 2D array of AIE tiles, utilizing dedicated memory tiles to maintain all data movement on-chip. They developed a generalized linear-layer implementation supporting fused bias addition and ReLU activation. The framework also systematically generates multi-layer implementations through a novel graph placement and search algorithm that optimizes the physical 2D layout of the device for maximum efficiency. It supports quantized models imported from tools like hls4ml or PyTorch while preserving bit-exactness. Benchmarks show up to 98.6% efficiency relative to single-kernel baselines, leveraging 97.4% of the device’s tiles. The framework delivers GPU-class throughput with microsecond latency, making it highly suitable for ultra-low-latency applications such as trigger systems in particle physics. <div>
arXiv:2512.15946v1 Announce Type: new 
Abstract: Efficient AI inference on AMD's Versal AI Engine (AIE) is challenging due to tightly coupled VLIW execution, explicit datapaths, and local memory management. Prior work focused on first-generation AIE kernel optimizations, without tackling full neural network execution across the 2D array. In this work, we present AIE4ML, the first comprehensive framework for converting AI models automatically into optimized firmware targeting the AIE-ML generation devices, also with forward compatibility for the newer AIE-MLv2 architecture. At the single-kernel level, we attain performance close to the architectural peak. At the graph and system levels, we provide a structured parallelization method that can scale across the 2D AIE-ML fabric and exploit its dedicated memory tiles to stay entirely on-chip throughout the model execution. As a demonstration, we designed a generalized and highly efficient linear-layer implementation with intrinsic support for fused bias addition and ReLU activation. Also, as our framework necessitates the generation of multi-layer implementations, our approach systematically derives deterministic, compact, and topology-optimized placements tailored to the physical 2D grid of the device through a novel graph placement and search algorithm. Finally, the framework seamlessly accepts quantized models imported from high-level tools such as hls4ml or PyTorch while preserving bit-exactness. In layer scaling benchmarks, we achieve up to 98.6% efficiency relative to the single-kernel baseline, utilizing 296 of 304 AIE tiles (97.4%) of the device with entirely on-chip data movement. With evaluations across real-world model topologies, we demonstrate that AIE4ML delivers GPU-class throughput under microsecond latency constraints, making it a practical companion for ultra-low-latency environments such as trigger systems in particle physics experiments.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Governance by Evidence: Regulated Predictors in Decision-Tree Models</title>
<link>https://arxiv.org/abs/2512.15955</link>
<guid>https://arxiv.org/abs/2512.15955</guid>
<content:encoded><![CDATA[
<div> Keywords: decision trees, privacy laws, regulated data, healthcare data, machine learning practice<br /><br />Summary:<br /><br />1. Decision-tree methods are extensively employed for structured tabular data analysis and valued for their interpretability in various sectors. 2. Many studies using decision trees report predictors such as age, diagnosis codes, and location, which fall under categories regulated by privacy laws. 3. The authors compiled a corpus of decision-tree studies and categorized each predictor according to regulated data types—including health data, biometric identifiers, children’s data, financial attributes, location traces, and government IDs. 4. Each category of data was then linked to specific excerpts from European Union and United States privacy legislation to highlight legal governance. 5. Findings reveal that numerous predictors applied in decision-tree research are governed by privacy regulations, with healthcare data being the most heavily represented and notable variations observed across different industries. 6. The study analyzes prevalence, industry composition, and temporal trends of these regulated predictors, aligning them to legal reference years in privacy frameworks. 7. The evidence underscores the importance of incorporating privacy-preserving techniques and governance checks in machine learning models, extending its implications beyond decision trees to broader ML practice. <div>
arXiv:2512.15955v1 Announce Type: new 
Abstract: Decision-tree methods are widely used on structured tabular data and are valued for interpretability across many sectors. However, published studies often list the predictors they use (for example age, diagnosis codes, location). Privacy laws increasingly regulate such data types. We use published decision-tree papers as a proxy for real-world use of legally governed data. We compile a corpus of decision-tree studies and assign each reported predictor to a regulated data category (for example health data, biometric identifiers, children's data, financial attributes, location traces, and government IDs). We then link each category to specific excerpts in European Union and United States privacy laws. We find that many reported predictors fall into regulated categories, with the largest shares in healthcare and clear differences across industries. We analyze prevalence, industry composition, and temporal patterns, and summarize regulation-aligned timing using each framework's reference year. Our evidence supports privacy-preserving methods and governance checks, and can inform ML practice beyond decision trees.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tracking Wildfire Assets with Commodity RFID and Gaussian Process Modeling</title>
<link>https://arxiv.org/abs/2512.15956</link>
<guid>https://arxiv.org/abs/2512.15956</guid>
<content:encoded><![CDATA[
<div> Keywords: RFID localization, Gaussian Process, wildfire response, signal attenuation, passive tracking<br /><br />Summary:<br /><br />1. This paper introduces a novel, scalable, and cost-effective method for tracking multiple assets in forested environments using commodity Radio Frequency Identification (RFID) systems specifically aimed at wildfire response scenarios. <br /><br />2. Traditional RFID localization techniques struggle in forested areas due to signal attenuation, multipath effects, and environmental variability, and often require fingerprinting approaches where tags must be placed at known locations beforehand. <br /><br />3. The authors address the challenging scenario where known tag locations are unavailable and demonstrate that it is still possible to achieve localization accuracy comparable to GPS without this constraint. <br /><br />4. Their approach uses Gaussian Processes to model environmental variations based solely on RF signal response signatures, eliminating the need for supplementary sensors like GPS or cameras. Localization is achieved by matching unknown RF signatures to the closest environment model in a previously constructed dictionary. <br /><br />5. A new weighted log-likelihood method is introduced to associate unknown environmental signals with the most similar environment model, enabling accurate localization. The results show that passive commodity RFID can localize assets with GPS-like accuracy at a fraction of the cost, allowing simultaneous tracking of dozens of wildfire assets near mobile readers without requiring pre-tagged known positions. <div>
arXiv:2512.15956v1 Announce Type: new 
Abstract: This paper presents a novel, cost-effective, and scalable approach to track numerous assets distributed in forested environments using commodity Radio Frequency Identification (RFID) targeting wildfire response applications. Commodity RFID systems suffer from poor tag localization when dispersed in forested environments due to signal attenuation, multi-path effects and environmental variability. Current methods to address this issue via fingerprinting rely on dispersing tags at known locations {\em a priori}. In this paper, we address the case when it is not possible to tag known locations and show that it is possible to localize tags to accuracies comparable to global positioning systems (GPS) without such a constraint. For this, we propose Gaussian Process to model various environments solely based on RF signal response signatures and without the aid of additional sensors such as global positioning GPS or cameras, and match an unknown RF to the closest match in a model dictionary. We utilize a new weighted log-likelihood method to associate an unknown environment with the closest environment in a dictionary of previously modeled environments, which is a crucial step in being able to use our approach. Our results show that it is possible to achieve localization accuracies of the order of GPS, but with passive commodity RFID, which will allow the tracking of dozens of wildfire assets within the vicinity of mobile readers at-a-time simultaneously, does not require known positions to be tagged {\em a priori}, and can achieve localization at a fraction of the cost compared to GPS.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Rank Reinforcement Learning for Adaptive Low-Rank Multi-Head Self Attention in Large Language Models</title>
<link>https://arxiv.org/abs/2512.15973</link>
<guid>https://arxiv.org/abs/2512.15973</guid>
<content:encoded><![CDATA[
<div> Dynamic Rank Reinforcement Learning, Multi-Head Self-Attention, Low-rank factorization, Reinforcement learning, Matrix perturbation<br /><br />Summary:<br /><br />This paper introduces Dynamic Rank Reinforcement Learning (DR-RL), a framework designed to optimize the low-rank factorization of Multi-Head Self-Attention (MHSA) in Large Language Models adaptively. Unlike traditional static low-rank approximations, DR-RL dynamically adjusts the rank based on sequence dynamics, layer-specific sensitivities, and hardware constraints, enhancing flexibility and efficiency. The key innovation is an RL agent that treats rank selection as a sequential policy optimization problem, balancing the trade-off between attention fidelity and computational latency via a carefully designed reward function. By leveraging online matrix perturbation theory, the framework allows incremental rank updates without expensive full decompositions at inference time, maintaining computational efficiency. The method integrates a lightweight Transformer-based policy network and batched Singular Value Decomposition operations for scalable deployment on GPUs. Experimental results show that DR-RL achieves accuracy comparable to full-rank attention models while significantly reducing floating point operations, especially in processing long sequences exceeding 4,096 tokens. This approach offers a mathematically rigorous and adaptive alternative to heuristic rank reduction techniques, advancing resource-efficient deep learning for MHSA. The authors have made their source code and experiment logs publicly available to facilitate further research and adoption. <div>
arXiv:2512.15973v1 Announce Type: new 
Abstract: We propose Dynamic Rank Reinforcement Learning (DR-RL), a novel framework that adaptively optimizes the low-rank factorization of Multi-Head Self-Attention (MHSA) in Large Language Models (LLMs) through the integration of reinforcement learning and online matrix perturbation theory. While traditional low-rank approximations often rely on static rank assumptions--limiting their flexibility across diverse input contexts--our method dynamically selects ranks based on real-time sequence dynamics, layer-specific sensitivities, and hardware constraints. The core innovation lies in an RL agent that formulates rank selection as a sequential policy optimization problem, where the reward function strictly balances attention fidelity against computational latency. Crucially, we employ online matrix perturbation bounds to enable incremental rank updates, thereby avoiding the prohibitive cost of full decomposition during inference. Furthermore, the integration of a lightweight Transformer-based policy network and batched Singular Value Decomposition (SVD) operations ensures scalable deployment on modern GPU architectures. Experiments demonstrate that DR-RL maintains downstream accuracy statistically equivalent to full-rank attention while significantly reducing Floating Point Operations (FLOPs), particularly in long-sequence regimes (L > 4096). This work bridges the gap between adaptive efficiency and theoretical rigor in MHSA, offering a principled, mathematically grounded alternative to heuristic rank reduction techniques in resource-constrained deep learning. Source code and experiment logs are available at: https://github.com/canererden/DR_RL_Project
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provably Extracting the Features from a General Superposition</title>
<link>https://arxiv.org/abs/2512.15987</link>
<guid>https://arxiv.org/abs/2512.15987</guid>
<content:encoded><![CDATA[
<div> Features, Superposition, Black-box learning, Fourier analysis, Overcomplete representations<br /><br />Summary:<br /><br />1. The paper addresses the challenge of learning feature representations encoded in superposition within complex machine learning models, focusing on settings where the number of features exceeds the underlying dimension (overcomplete regime).<br /><br />2. The problem is formulated as recovering feature directions \(v_i\) and their associated response functions \(\sigma_i\) from noisy black-box query access to a function \(f(x) = \sum_{i=1}^n a_i \sigma_i(v_i^\top x)\), where each \(v_i\) is a unit vector representing a feature direction.<br /><br />3. Unlike prior work restricted to simpler or specialized forms, the proposed method allows for arbitrary response functions \(\sigma_i\) and only requires that the feature directions \(v_i\) are not nearly identical, enabling recovery from general superpositions.<br /><br />4. The main contribution is an efficient query algorithm that, through an innovative iterative search in Fourier space, progressively refines the search domain to uncover all significant feature directions and reconstruct the original function \(f\), even under noise.<br /><br />5. This approach significantly advances algorithmic capabilities for feature learning in overcomplete models by relaxing limitations of linearity and identifiability conditions commonly assumed, thus deepening theoretical understanding and practical applicability of feature recovery in complex ML systems. <div>
arXiv:2512.15987v1 Announce Type: new 
Abstract: It is widely believed that complex machine learning models generally encode features through linear representations, but these features exist in superposition, making them challenging to recover. We study the following fundamental setting for learning features in superposition from black-box query access: we are given query access to a function \[ f(x)=\sum_{i=1}^n a_i\,\sigma_i(v_i^\top x), \] where each unit vector $v_i$ encodes a feature direction and $\sigma_i:\mathbb{R} \rightarrow \mathbb{R}$ is an arbitrary response function and our goal is to recover the $v_i$ and the function $f$.
  In learning-theoretic terms, superposition refers to the overcomplete regime, when the number of features is larger than the underlying dimension (i.e. $n > d$), which has proven especially challenging for typical algorithmic approaches. Our main result is an efficient query algorithm that, from noisy oracle access to $f$, identifies all feature directions whose responses are non-degenerate and reconstructs the function $f$. Crucially, our algorithm works in a significantly more general setting than all related prior results -- we allow for essentially arbitrary superpositions, only requiring that $v_i, v_j$ are not nearly identical for $i \neq j$, and general response functions $\sigma_i$. At a high level, our algorithm introduces an approach for searching in Fourier space by iteratively refining the search space to locate the hidden directions $v_i$.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Higher-Order LaSDI: Reduced Order Modeling with Multiple Time Derivatives</title>
<link>https://arxiv.org/abs/2512.15997</link>
<guid>https://arxiv.org/abs/2512.15997</guid>
<content:encoded><![CDATA[
<div> Keywords: partial differential equations, reduced-order models, finite-difference scheme, Rollout loss, 2D Burgers equation<br /><br />Summary:<br />1. The article tackles the challenge of solving complex partial differential equations (PDEs), which are fundamental in physical sciences but typically require heavy computational resources. <br />2. Reduced-order models (ROMs) provide a solution by applying dimensionality reduction techniques to enable faster approximations of PDEs. <br />3. Although current ROMs can handle parameterized PDE families, their accuracy tends to deteriorate over long time horizons, limiting their long-term predictive capability. <br />4. To overcome this, the authors introduce a novel approach consisting of a flexible, high-order, yet computationally inexpensive finite-difference scheme designed to improve accuracy and efficiency. <br />5. They also propose a new training method called the Rollout loss, which optimizes ROMs to maintain prediction accuracy over arbitrarily long time frames. <br />6. The effectiveness of this combined approach is demonstrated through experiments on the 2D Burgers equation, a standard nonlinear PDE benchmark, showing improved long-term predictive performance. <div>
arXiv:2512.15997v1 Announce Type: new 
Abstract: Solving complex partial differential equations is vital in the physical sciences, but often requires computationally expensive numerical methods. Reduced-order models (ROMs) address this by exploiting dimensionality reduction to create fast approximations. While modern ROMs can solve parameterized families of PDEs, their predictive power degrades over long time horizons. We address this by (1) introducing a flexible, high-order, yet inexpensive finite-difference scheme and (2) proposing a Rollout loss that trains ROMs to make accurate predictions over arbitrary time horizons. We demonstrate our approach on the 2D Burgers equation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Surrogate Neural Architecture Codesign Package (SNAC-Pack)</title>
<link>https://arxiv.org/abs/2512.15998</link>
<guid>https://arxiv.org/abs/2512.15998</guid>
<content:encoded><![CDATA[
<div> Neural Architecture Search, FPGA, Multi-objective Optimization, Resource Utilization, Latency Estimator<br /><br />Summary:<br /><br />1. The paper presents SNAC-Pack, a Surrogate Neural Architecture Codesign Package designed to automate neural network discovery and optimization specifically for FPGA deployment.<br /><br />2. SNAC-Pack integrates multi-stage neural architecture codesign search with a Resource Utilization and Latency Estimator, allowing multi-objective optimization on accuracy, FPGA resource use, and latency without expensive synthesis for every candidate.<br /><br />3. The framework is demonstrated on a high energy physics jet classification task, achieving 63.84% accuracy using resource estimation.<br /><br />4. When synthesized on a Xilinx Virtex UltraScale+ VU13P FPGA, models designed with SNAC-Pack match baseline accuracy and maintain resource utilization comparable to models optimized by traditional bit operations (BOPs) metrics.<br /><br />5. This work highlights the effectiveness of hardware-aware neural architecture search for resource-constrained FPGA deployments and provides an open-source framework to streamline the design of efficient FPGA-accelerated neural networks. <div>
arXiv:2512.15998v1 Announce Type: new 
Abstract: Neural Architecture Search is a powerful approach for automating model design, but existing methods struggle to accurately optimize for real hardware performance, often relying on proxy metrics such as bit operations. We present Surrogate Neural Architecture Codesign Package (SNAC-Pack), an integrated framework that automates the discovery and optimization of neural networks focusing on FPGA deployment. SNAC-Pack combines Neural Architecture Codesign's multi-stage search capabilities with the Resource Utilization and Latency Estimator, enabling multi-objective optimization across accuracy, FPGA resource utilization, and latency without requiring time-intensive synthesis for each candidate model. We demonstrate SNAC-Pack on a high energy physics jet classification task, achieving 63.84% accuracy with resource estimation. When synthesized on a Xilinx Virtex UltraScale+ VU13P FPGA, the SNAC-Pack model matches baseline accuracy while maintaining comparable resource utilization to models optimized using traditional BOPs metrics. This work demonstrates the potential of hardware-aware neural architecture search for resource-constrained deployments and provides an open-source framework for automating the design of efficient FPGA-accelerated models.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Fine-Tuning-Based Site Calibration for Knowledge-Guided Machine Learning: A Summary of Results</title>
<link>https://arxiv.org/abs/2512.16013</link>
<guid>https://arxiv.org/abs/2512.16013</guid>
<content:encoded><![CDATA[
<div> Keywords: transfer learning, spatial variability, machine learning, agroecosystem carbon cycle, site calibration<br /><br />Summary:<br /><br />1. The article addresses the need for accurate and cost-effective quantification of the agroecosystem carbon cycle at decision-relevant scales, which is crucial for climate mitigation and sustainable agriculture.  
2. It highlights challenges in this domain due to heterogeneous data and complex cross-scale dependencies, noting that traditional methods often ignore spatial heterogeneity and transfer learning benefits by relying on location-independent parameterizations and independent training.  
3. The authors propose FTBSC-KGML (Fine-Tuning-Based Site Calibration-Knowledge-Guided Machine Learning), a novel framework that integrates pretraining, fine-tuning, spatial variability awareness, and knowledge guidance to improve carbon emission estimation.  
4. FTBSC-KGML leverages remote sensing data for GPP, climate, and soil covariates collected from multiple sites in the U.S. Midwest, employing a spatial-heterogeneity-aware transfer learning scheme which globally pretrains a model and fine-tunes it per site or state for improved local accuracy and interpretability under limited data scenarios.  
5. Empirical results show that FTBSC-KGML outperforms purely global models by achieving lower validation error and greater consistency in explanatory power, thereby better capturing spatial variability and extending the previous SDSA-KGML framework. <div>
arXiv:2512.16013v1 Announce Type: new 
Abstract: Accurate and cost-effective quantification of the agroecosystem carbon cycle at decision-relevant scales is essential for climate mitigation and sustainable agriculture. However, both transfer learning and the exploitation of spatial variability in this field are challenging, as they involve heterogeneous data and complex cross-scale dependencies. Conventional approaches often rely on location-independent parameterizations and independent training, underutilizing transfer learning and spatial heterogeneity in the inputs, and limiting their applicability in regions with substantial variability. We propose FTBSC-KGML (Fine-Tuning-Based Site Calibration-Knowledge-Guided Machine Learning), a pretraining- and fine-tuning-based, spatial-variability-aware, and knowledge-guided machine learning framework that augments KGML-ag with a pretraining-fine-tuning process and site-specific parameters. Using a pretraining-fine-tuning process with remote-sensing GPP, climate, and soil covariates collected across multiple midwestern sites, FTBSC-KGML estimates land emissions while leveraging transfer learning and spatial heterogeneity. A key component is a spatial-heterogeneity-aware transfer-learning scheme, which is a globally pretrained model that is fine-tuned at each state or site to learn place-aware representations, thereby improving local accuracy under limited data without sacrificing interpretability. Empirically, FTBSC-KGML achieves lower validation error and greater consistency in explanatory power than a purely global model, thereby better capturing spatial variability across states. This work extends the prior SDSA-KGML framework.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Techno-economic optimization of a heat-pipe microreactor, part I: theory and cost optimization</title>
<link>https://arxiv.org/abs/2512.16032</link>
<guid>https://arxiv.org/abs/2512.16032</guid>
<content:encoded><![CDATA[
<div> Keywords: Heat-pipe microreactors, optimization, levelized cost of electricity, reinforcement learning, surrogate models<br /><br />Summary: This article addresses the economic challenges of microreactors, specifically heat-pipe microreactors (HPMRs), which are compact and transportable power systems suitable for remote areas heavily reliant on fossil fuels. The study highlights the issue of diseconomies of scale and insufficient financial viability of these reactors. To overcome this, the authors propose a novel geometric design optimization framework integrating both economic and physical constraints into early-stage reactor design. The method involves generating random samples to train surrogate models including Gaussian processes and multi-layer perceptrons. These surrogates are then embedded within a reinforcement learning-based optimization algorithm targeting the reduction of the levelized cost of electricity (LCOE), while enforcing constraints such as fuel lifetime, shutdown margin, peak heat flux, and rod-integrated peaking factor. Two cost scenarios are analyzed: one with high-cost axial reflectors and another with inexpensive axial reflectors. Results reveal that operation, maintenance, and capital costs—especially axial reflector and control drum material costs—dominate overall LCOE. The optimization successfully reduces LCOE by over 57% by adjusting design parameters to minimize these costs without violating constraints. Future work aims to integrate fuel and heat-pipe performance with multi-objective optimization to better understand cost-constraint interactions. <div>
arXiv:2512.16032v1 Announce Type: new 
Abstract: Microreactors, particularly heat-pipe microreactors (HPMRs), are compact, transportable, self-regulated power systems well-suited for access-challenged remote areas where costly fossil fuels dominate. However, they suffer from diseconomies of scale, and their financial viability remains unconvincing. One step in addressing this shortcoming is to design these reactors with comprehensive economic and physics analyses informing early-stage design iteration. In this work, we present a novel unifying geometric design optimization approach that accounts for techno-economic considerations. We start by generating random samples to train surrogate models, including Gaussian processes (GPs) and multi-layer perceptrons (MLPs). We then deploy these surrogates within a reinforcement learning (RL)-based optimization framework to optimize the levelized cost of electricity (LCOE), all the while imposing constraints on the fuel lifetime, shutdown margin (SDM), peak heat flux, and rod-integrated peaking factor. We study two cases: one in which the axial reflector cost is very high, and one in which it is inexpensive. We found that the operation and maintenance and capital costs are the primary contributors to the overall LCOE particularly the cost of the axial reflectors (for the first case) and the control drum materials. The optimizer cleverly changes the design parameters so as to minimize one of them while still satisfying the constraints, ultimately reducing the LCOE by more than 57% in both instances. A comprehensive integration of fuel and HP performance with multi-objective optimization is currently being pursued to fully understand the interaction between constraints and cost performance.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable AI in Big Data Fraud Detection</title>
<link>https://arxiv.org/abs/2512.16037</link>
<guid>https://arxiv.org/abs/2512.16037</guid>
<content:encoded><![CDATA[
<div> Keywords: Big Data, explainable AI (XAI), fraud detection, risk management, scalability  

<br /><br />Summary:  
This paper explores the integration of explainable artificial intelligence (XAI) within Big Data analytics pipelines focused on fraud detection and risk management. It begins by reviewing the characteristics of Big Data and the major analytical tools commonly employed, including distributed storage systems, streaming platforms, and advanced fraud detection models such as anomaly detectors, graph-based techniques, and ensemble classifiers. The study then provides a structured review of popular XAI methods like LIME, SHAP, counterfactual explanations, and attention mechanisms, evaluating their strengths and limitations especially in large-scale deployments. Key research gaps are identified in the areas of scalability, real-time processing capabilities, and explainability for graph and temporal data models. To tackle these issues, the authors propose a conceptual framework that combines scalable Big Data infrastructure with context-aware explanation methods and incorporates human feedback to enhance interpretability. Finally, the paper outlines open research directions including the development of scalable XAI solutions, privacy-preserving explanation techniques, and the establishment of standardized evaluation metrics specifically for explainable fraud detection systems. This comprehensive review underlines the importance of transparency, regulatory compliance, and trust in automated analytics for financial, insurance, and cybersecurity applications. <div>
arXiv:2512.16037v1 Announce Type: new 
Abstract: Big Data has become central to modern applications in finance, insurance, and cybersecurity, enabling machine learning systems to perform large-scale risk assessments and fraud detection. However, the increasing dependence on automated analytics introduces important concerns about transparency, regulatory compliance, and trust. This paper examines how explainable artificial intelligence (XAI) can be integrated into Big Data analytics pipelines for fraud detection and risk management. We review key Big Data characteristics and survey major analytical tools, including distributed storage systems, streaming platforms, and advanced fraud detection models such as anomaly detectors, graph-based approaches, and ensemble classifiers. We also present a structured review of widely used XAI methods, including LIME, SHAP, counterfactual explanations, and attention mechanisms, and analyze their strengths and limitations when deployed at scale. Based on these findings, we identify key research gaps related to scalability, real-time processing, and explainability for graph and temporal models. To address these challenges, we outline a conceptual framework that integrates scalable Big Data infrastructure with context-aware explanation mechanisms and human feedback. The paper concludes with open research directions in scalable XAI, privacy-aware explanations, and standardized evaluation methods for explainable fraud detection systems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CauSTream: Causal Spatio-Temporal Representation Learning for Streamflow Forecasting</title>
<link>https://arxiv.org/abs/2512.16046</link>
<guid>https://arxiv.org/abs/2512.16046</guid>
<content:encoded><![CDATA[
<div> Keywords: streamflow forecasting, causal learning, spatiotemporal modeling, runoff causal graph, hydrological interpretability<br /><br />Summary: Streamflow forecasting is vital for effective water resource management and risk mitigation, yet existing deep learning models often lack interpretability and generalization due to neglecting underlying physical processes. CauStream addresses these limitations by proposing a unified framework that integrates causal learning with spatiotemporal forecasting. The framework jointly learns two key causal structures: a runoff causal graph that models relationships among meteorological forcings and a routing graph that captures dynamic dependencies across streamflow monitoring stations. CauStream establishes identifiability conditions for these causal graphs within a nonparametric context, enhancing the theoretical rigor of its approach. Evaluated on three major U.S. river basins across multiple forecast horizons, CauStream consistently outperforms prior state-of-the-art models, especially showing increased performance gains at longer forecast windows, indicating superior generalization to unseen conditions. Beyond predictive accuracy, CauStream’s inferred causal graphs closely align with established hydrological domain knowledge, providing interpretable insights into watershed dynamics. This combination of accurate forecasting and interpretable causal structure modeling makes CauStream a principled foundation for causal spatiotemporal modeling with promising applications extending beyond hydrology to broader scientific and environmental domains. <div>
arXiv:2512.16046v1 Announce Type: new 
Abstract: Streamflow forecasting is crucial for water resource management and risk mitigation. While deep learning models have achieved strong predictive performance, they often overlook underlying physical processes, limiting interpretability and generalization. Recent causal learning approaches address these issues by integrating domain knowledge, yet they typically rely on fixed causal graphs that fail to adapt to data. We propose CauStream, a unified framework for causal spatiotemporal streamflow forecasting. CauSTream jointly learns (i) a runoff causal graph among meteorological forcings and (ii) a routing graph capturing dynamic dependencies across stations. We further establish identifiability conditions for these causal structures under a nonparametric setting. We evaluate CauSTream on three major U.S. river basins across three forecasting horizons. The model consistently outperforms prior state-of-the-art methods, with performance gaps widening at longer forecast windows, indicating stronger generalization to unseen conditions. Beyond forecasting, CauSTream also learns causal graphs that capture relationships among hydrological factors and stations. The inferred structures align closely with established domain knowledge, offering interpretable insights into watershed dynamics. CauSTream offers a principled foundation for causal spatiotemporal modeling, with the potential to extend to a wide range of scientific and environmental applications.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-Context Multi-Operator Learning with DeepOSets</title>
<link>https://arxiv.org/abs/2512.16074</link>
<guid>https://arxiv.org/abs/2512.16074</guid>
<content:encoded><![CDATA[
<div> Keywords: In-context Learning, DeepOSets, Partial Differential Equations, Operator Learning, Universal Approximation  

<br /><br />Summary:  
1. The paper investigates In-context Learning (ICL), which enables machine learning models to learn from prompt examples without weight updates.  
2. DeepOSets, a neural architecture combining DeepSets for set learning and Deep Operator Networks (DeepONets) for operator learning, is examined for its ICL capabilities beyond autoregressive transformer models.  
3. The authors demonstrate that with modifications, DeepOSets can act as a multi-operator in-context learner, recovering solution operators of new PDEs not seen during training using example parameter-solution pairs in prompts.  
4. They prove that DeepOSets serves as a universal uniform approximator for a class of continuous operators, meaning one DeepOSets architecture can approximate any continuous operator in that class to any desired accuracy, given enough prompt examples.  
5. Experimental results on Poisson and reaction-diffusion PDE forward and inverse boundary-value problems validate that DeepOSets can accurately predict PDE solutions for parameter queries outside the training distribution by leveraging in-context examples.  
This work highlights the first universal approximation result for in-context learning of continuous operators in scientific machine learning, potentially enabling more flexible and efficient PDE solvers. <div>
arXiv:2512.16074v1 Announce Type: new 
Abstract: In-context Learning (ICL) is the remarkable capability displayed by some machine learning models to learn from examples in a prompt, without any further weight updates. ICL had originally been thought to emerge from the self-attention mechanism in autoregressive transformer architectures. DeepOSets is a non-autoregressive, non-attention based neural architecture that combines set learning via the DeepSets architecture with operator learning via Deep Operator Networks (DeepONets). In a previous study, DeepOSets was shown to display ICL capabilities in supervised learning problems. In this paper, we show that the DeepOSets architecture, with the appropriate modifications, is a multi-operator in-context learner that can recover the solution operator of a new PDE, not seen during training, from example pairs of parameter and solution placed in a user prompt, without any weight updates. Furthermore, we show that DeepOSets is a universal uniform approximator over a class of continuous operators, which we believe is the first result of its kind in the literature of scientific machine learning. This means that a single DeepOSets architecture exists that approximates in-context any continuous operator in the class to any fixed desired degree accuracy, given an appropriate number of examples in the prompt. Experiments with Poisson and reaction-diffusion forward and inverse boundary-value problems demonstrate the ability of the proposed model to use in-context examples to predict accurately the solutions corresponding to parameter queries for PDEs not seen during training.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy Blur: Quantifying Privacy and Utility for Image Data Release</title>
<link>https://arxiv.org/abs/2512.16086</link>
<guid>https://arxiv.org/abs/2512.16086</guid>
<content:encoded><![CDATA[
<div> Privacy, Gaussian blurring, pixelization, data obfuscation, reversal attacks  

<br /><br />Summary:  
The paper addresses the challenge of protecting private information such as faces and license plates in image datasets collected "in the wild" while maintaining the utility of such data for training machine learning models. It critiques the widely used standard method of Gaussian blurring for private information obfuscation, demonstrating that practical implementations of Gaussian blur can be reversed, thus compromising privacy. The study evaluates three alternative obfuscation techniques: pixelization, pixelization combined with noise addition (DP-Pix), and cropping. Privacy is measured through the effectiveness of reversal and discrimination attacks, while utility is assessed based on the quality of model-learned representations trained on images with obfuscated faces. Results indicate that Gaussian blur is the least effective privacy-preserving method because of its susceptibility to reversal attacks, especially in low-precision implementations commonly used in practice. On the other hand, pixelization and pixelization with noise addition, when applied at appropriate granularity levels, offer a stronger balance between privacy protection and maintaining data utility across various computer vision tasks. To facilitate the adoption of these techniques, the authors provide a software package called Privacy Blur that includes their proposed methods and recommended parameter settings. <div>
arXiv:2512.16086v1 Announce Type: new 
Abstract: Image data collected in the wild often contains private information such as faces and license plates, and responsible data release must ensure that this information stays hidden. At the same time, released data should retain its usefulness for model-training. The standard method for private information obfuscation in images is Gaussian blurring. In this work, we show that practical implementations of Gaussian blurring are reversible enough to break privacy. We then take a closer look at the privacy-utility tradeoffs offered by three other obfuscation algorithms -- pixelization, pixelization and noise addition (DP-Pix), and cropping. Privacy is evaluated by reversal and discrimination attacks, while utility by the quality of the learnt representations when the model is trained on data with obfuscated faces. We show that the most popular industry-standard method, Gaussian blur is the least private of the four -- being susceptible to reversal attacks in its practical low-precision implementations. In contrast, pixelization and pixelization plus noise addition, when used at the right level of granularity, offer both privacy and utility for a number of computer vision tasks. We make our proposed methods together with suggested parameters available in a software package called Privacy Blur.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AIMM: An AI-Driven Multimodal Framework for Detecting Social-Media-Influenced Stock Market Manipulation</title>
<link>https://arxiv.org/abs/2512.16103</link>
<guid>https://arxiv.org/abs/2512.16103</guid>
<content:encoded><![CDATA[
<div> Market manipulation, social media, retail investors, AIMM, GME<br /><br />Summary:<br /><br />This article introduces AIMM, an AI-driven framework designed to detect market manipulation originating from coordinated social media campaigns rather than isolated trades. AIMM combines Reddit activity, bot and coordination indicators, and OHLCV (open-high-low-close-volume) market data into a daily Manipulation Risk Score for each stock ticker. The system features a parquet-native data pipeline and a Streamlit dashboard that enables analysts to examine suspicious periods, review related social media posts and corresponding price action, and track model outputs over time. Due to limitations in accessing Reddit's API, calibrated synthetic social features are used, while real historical market data is sourced from Yahoo Finance. This work contributes three key elements: first, the creation of the AIMM Ground Truth dataset (AIMM-GT), which includes 33 labeled ticker-days covering eight equities, integrating SEC enforcement cases, community-verified manipulations, and controls; second, the implementation of forward-walk evaluation and prospective prediction logging to assess performance both retrospectively and in deployment contexts; third, an analysis showing that AIMM successfully flagged GameStop (GME) 22 days before its January 2021 price squeeze peak. Although the labeled dataset is small with only three positive events, preliminary results demonstrate promising discriminatory power and early warning capability. The authors release the code, dataset schema, and dashboard design to facilitate further research on social media-driven market surveillance. <div>
arXiv:2512.16103v1 Announce Type: new 
Abstract: Market manipulation now routinely originates from coordinated social media campaigns, not isolated trades. Retail investors, regulators, and brokerages need tools that connect online narratives and coordination patterns to market behavior. We present AIMM, an AI-driven framework that fuses Reddit activity, bot and coordination indicators, and OHLCV market features into a daily AIMM Manipulation Risk Score for each ticker.
  The system uses a parquet-native pipeline with a Streamlit dashboard that allows analysts to explore suspicious windows, inspect underlying posts and price action, and log model outputs over time. Due to Reddit API restrictions, we employ calibrated synthetic social features matching documented event characteristics; market data (OHLCV) uses real historical data from Yahoo Finance. This release makes three contributions. First, we build the AIMM Ground Truth dataset (AIMM-GT): 33 labeled ticker-days spanning eight equities, drawing from SEC enforcement actions, community-verified manipulation cases, and matched normal controls. Second, we implement forward-walk evaluation and prospective prediction logging for both retrospective and deployment-style assessment. Third, we analyze lead times and show that AIMM flagged GME 22 days before the January 2021 squeeze peak.
  The current labeled set is small (33 ticker-days, 3 positive events), but results show preliminary discriminative capability and early warnings for the GME incident. We release the code, dataset schema, and dashboard design to support research on social media-driven market surveillance.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BUILD with Precision: Bottom-Up Inference of Linear DAGs</title>
<link>https://arxiv.org/abs/2512.16111</link>
<guid>https://arxiv.org/abs/2512.16111</guid>
<content:encoded><![CDATA[
<div> Keywords: DAG learning, causal discovery, precision matrix, structural equation model, algorithm BUILD<br /><br />Summary:<br /><br />1. The paper addresses the problem of learning the structure of directed acyclic graphs (DAGs) from observational data, which is important for causal discovery and machine learning.<br />2. It focuses on linear Gaussian structural equation models (SEMs) with equal noise variances, a setting where the DAG structure is identifiable.<br />3. The authors identify a distinctive structure in the ensemble precision matrix of the observed data that helps facilitate the recovery of the DAG.<br />4. They propose a novel deterministic stepwise algorithm named BUILD (Bottom-Up Inference of Linear DAGs), which reconstructs the DAG by iteratively identifying leaf nodes and their parents, pruning leaves, and repeating the process until the entire DAG is recovered from the true precision matrix.<br />5. Recognizing that precision matrices need to be estimated from finite samples in practice, which can cause error accumulation due to ill-conditioning, the method includes a mitigation strategy of periodically re-estimating the precision matrix on a reduced set of variables to improve robustness.<br />6. Experimental results on challenging synthetic datasets demonstrate that BUILD performs favorably compared to existing state-of-the-art DAG learning algorithms while providing explicit control over algorithmic complexity. <div>
arXiv:2512.16111v1 Announce Type: new 
Abstract: Learning the structure of directed acyclic graphs (DAGs) from observational data is a central problem in causal discovery, statistical signal processing, and machine learning. Under a linear Gaussian structural equation model (SEM) with equal noise variances, the problem is identifiable and we show that the ensemble precision matrix of the observations exhibits a distinctive structure that facilitates DAG recovery. Exploiting this property, we propose BUILD (Bottom-Up Inference of Linear DAGs), a deterministic stepwise algorithm that identifies leaf nodes and their parents, then prunes the leaves by removing incident edges to proceed to the next step, exactly reconstructing the DAG from the true precision matrix. In practice, precision matrices must be estimated from finite data, and ill-conditioning may lead to error accumulation across BUILD steps. As a mitigation strategy, we periodically re-estimate the precision matrix (with less variables as leaves are pruned), trading off runtime for enhanced robustness. Reproducible results on challenging synthetic benchmarks demonstrate that BUILD compares favorably to state-of-the-art DAG learning algorithms, while offering an explicit handle on complexity.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-View Inference Attack: Machine Unlearning Amplifies Privacy Exposure</title>
<link>https://arxiv.org/abs/2512.16126</link>
<guid>https://arxiv.org/abs/2512.16126</guid>
<content:encoded><![CDATA[
<div> Machine Unlearning, Privacy Risks, Dual-View Setting, Membership Inference, Likelihood Ratio

<br /><br />Summary: This paper investigates a novel privacy vulnerability introduced by machine unlearning, a technique designed to remove specific training data from a model to comply with data deletion requests. Unlike prior work focusing on the privacy of deleted data, this study highlights risks to the retained data when an adversary has access to both the original and unlearned models, termed the dual-view setting. The authors introduce the concept of privacy knowledge gain from an information-theoretic standpoint, showing that querying both models simultaneously amplifies privacy leakage beyond what is possible with either model alone. To demonstrate this threat, they propose DVIA, a Dual-View Inference Attack that efficiently extracts membership information related to the retained data through black-box queries without requiring a trained attack model. DVIA utilizes a lightweight likelihood ratio inference module to perform these attacks effectively. Experimental results on various datasets and model architectures confirm the practicality and severity of this new privacy risk, urging the research community to consider the implications of dual-view access scenarios in machine unlearning. <div>
arXiv:2512.16126v1 Announce Type: new 
Abstract: Machine unlearning is a newly popularized technique for removing specific training data from a trained model, enabling it to comply with data deletion requests. While it protects the rights of users requesting unlearning, it also introduces new privacy risks. Prior works have primarily focused on the privacy of data that has been unlearned, while the risks to retained data remain largely unexplored. To address this gap, we focus on the privacy risks of retained data and, for the first time, reveal the vulnerabilities introduced by machine unlearning under the dual-view setting, where an adversary can query both the original and the unlearned models. From an information-theoretic perspective, we introduce the concept of {privacy knowledge gain} and demonstrate that the dual-view setting allows adversaries to obtain more information than querying either model alone, thereby amplifying privacy leakage. To effectively demonstrate this threat, we propose DVIA, a Dual-View Inference Attack, which extracts membership information on retained data using black-box queries to both models. DVIA eliminates the need to train an attack model and employs a lightweight likelihood ratio inference module for efficient inference. Experiments across different datasets and model architectures validate the effectiveness of DVIA and highlight the privacy risks inherent in the dual-view setting.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>INTELLECT-3: Technical Report</title>
<link>https://arxiv.org/abs/2512.16144</link>
<guid>https://arxiv.org/abs/2512.16144</guid>
<content:encoded><![CDATA[
<div> Keywords: INTELLECT-3, Mixture-of-Experts, reinforcement learning, prime-rl, large-scale training<br /><br />Summary:<br /><br />1. INTELLECT-3 is a large-scale Mixture-of-Experts (MoE) model with 106 billion parameters, of which 12 billion are active during inference.<br />2. It is trained using large-scale reinforcement learning (RL) via a comprehensive end-to-end RL infrastructure stack built by the authors.<br />3. The model achieves state-of-the-art performance on benchmarks related to mathematics, coding, science, and reasoning, outperforming many larger models.<br />4. The authors have open-sourced INTELLECT-3 along with the full infrastructure stack, including RL frameworks, training recipes, and a collection of environments developed with the verifiers library and hosted on their Environments Hub community platform.<br />5. A new open framework called prime-rl is introduced for scalable, asynchronous RL training, designed to work efficiently from single-node setups up to thousands of GPUs and optimized for agentic RL featuring multi-turn interactions and tool usage.<br />6. Training was performed by fine-tuning and RL on top of the GLM-4.5-Air-Base model, scaling up to 512 NVIDIA H200 GPUs with high efficiency. <div>
arXiv:2512.16144v1 Announce Type: new 
Abstract: We present INTELLECT-3, a 106B-parameter Mixture-of-Experts model (12B active) trained with large-scale reinforcement learning on our end-to-end RL infrastructure stack. INTELLECT-3 achieves state of the art performance for its size across math, code, science and reasoning benchmarks, outperforming many larger frontier models. We open-source the model together with the full infrastructure stack used to create it, including RL frameworks, complete recipe, and a wide collection of environments, built with the verifiers library, for training and evaluation from our Environments Hub community platform. Built for this effort, we introduce prime-rl, an open framework for large-scale asynchronous reinforcement learning, which scales seamlessly from a single node to thousands of GPUs, and is tailored for agentic RL with first-class support for multi-turn interactions and tool use. Using this stack, we run both SFT and RL training on top of the GLM-4.5-Air-Base model, scaling RL training up to 512 H200s with high training efficiency.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multimodal Approach to Alzheimer's Diagnosis: Geometric Insights from Cube Copying and Cognitive Assessments</title>
<link>https://arxiv.org/abs/2512.16184</link>
<guid>https://arxiv.org/abs/2512.16184</guid>
<content:encoded><![CDATA[
<div> Keywords: Alzheimer's disease, cube copying, graph neural networks, multimodal integration, SHAP interpretability<br /><br />Summary:<br /><br />1. This study addresses the challenge of early and accessible detection of Alzheimer's disease (AD) by focusing on cube-copying tasks, which assess visuospatial function through simple hand-drawn sketches.<br /><br />2. The proposed framework transforms these cube drawings into graph-structured data, capturing both geometric and topological properties via node features such as spatial coordinates, local graphlet-based topology, and angular geometry.<br /><br />3. These graph representations are processed using graph neural networks (GNNs) and combined with demographic details (age, education) and neuropsychological test (NPT) scores through a late-fusion multimodal model.<br /><br />4. Experimental results reveal that graph-based representations alone outperform traditional pixel-based convolutional neural networks, providing a strong baseline, while their fusion with demographic and test data further enhances classification performance and robustness, especially under class imbalance conditions.<br /><br />5. An interpretability analysis using SHAP highlights that specific graphlet motifs and geometric distortions in the cube drawings are key predictors of AD, consistent with clinical observations of visuospatial disorganization in affected patients.<br /><br />Overall, the study presents a non-invasive, interpretable, and scalable graph-based methodology for AD screening using simple hand-drawn tasks integrated with clinical data. <div>
arXiv:2512.16184v1 Announce Type: new 
Abstract: Early and accessible detection of Alzheimer's disease (AD) remains a critical clinical challenge, and cube-copying tasks offer a simple yet informative assessment of visuospatial function. This work proposes a multimodal framework that converts hand-drawn cube sketches into graph-structured representations capturing geometric and topological properties, and integrates these features with demographic information and neuropsychological test (NPT) scores for AD classification. Cube drawings are modeled as graphs with node features encoding spatial coordinates, local graphlet-based topology, and angular geometry, which are processed using graph neural networks and fused with age, education, and NPT features in a late-fusion model. Experimental results show that graph-based representations provide a strong unimodal baseline and substantially outperform pixel-based convolutional models, while multimodal integration further improves performance and robustness to class imbalance. SHAP-based interpretability analysis identifies specific graphlet motifs and geometric distortions as key predictors, closely aligning with clinical observations of disorganized cube drawings in AD. Together, these results establish graph-based analysis of cube copying as an interpretable, non-invasive, and scalable approach for Alzheimer's disease screening.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multi-scale Fused Graph Neural Network with Inter-view Contrastive Learning for Spatial Transcriptomics Data Clustering</title>
<link>https://arxiv.org/abs/2512.16188</link>
<guid>https://arxiv.org/abs/2512.16188</guid>
<content:encoded><![CDATA[
arXiv:2512.16188v1 Announce Type: new 
Abstract: Spatial transcriptomics enables genome-wide expression analysis within native tissue context, yet identifying spatial domains remains challenging due to complex gene-spatial interactions. Existing methods typically process spatial and feature views separately, fusing only at output level - an "encode-separately, fuse-late" paradigm that limits multi-scale semantic capture and cross-view interaction. Accordingly, stMFG is proposed, a multi-scale interactive fusion graph network that introduces layer-wise cross-view attention to dynamically integrate spatial and gene features after each convolution. The model combines cross-view contrastive learning with spatial constraints to enhance discriminability while maintaining spatial continuity. On DLPFC and breast cancer datasets, stMFG outperforms state-of-the-art methods, achieving up to 14% ARI improvement on certain slices.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explicit and Non-asymptotic Query Complexities of Rank-Based Zeroth-order Algorithms on Smooth Functions</title>
<link>https://arxiv.org/abs/2512.16200</link>
<guid>https://arxiv.org/abs/2512.16200</guid>
<content:encoded><![CDATA[
arXiv:2512.16200v1 Announce Type: new 
Abstract: Rank-based zeroth-order (ZO) optimization -- which relies only on the ordering of function evaluations -- offers strong robustness to noise and monotone transformations, and underlies many successful algorithms such as CMA-ES, natural evolution strategies, and rank-based genetic algorithms. Despite its widespread use, the theoretical understanding of rank-based ZO methods remains limited: existing analyses provide only asymptotic insights and do not yield explicit convergence rates for algorithms selecting the top-$k$ directions.
  This work closes this gap by analyzing a simple rank-based ZO algorithm and establishing the first \emph{explicit}, and \emph{non-asymptotic} query complexities. For a $d$-dimension problem, if the function is $L$-smooth and $\mu$-strongly convex, the algorithm achieves $\widetilde{\mathcal O}\!\left(\frac{dL}{\mu}\log\!\frac{dL}{\mu\delta}\log\!\frac{1}{\varepsilon}\right)$ to find an $\varepsilon$-suboptimal solution, and for smooth nonconvex objectives it reaches $\mathcal O\!\left(\frac{dL}{\varepsilon}\log\!\frac{1}{\varepsilon}\right)$. Notation $\cO(\cdot)$ hides constant terms and $\widetilde{\mathcal O}(\cdot)$ hides extra $\log\log\frac{1}{\varepsilon}$ term. These query complexities hold with a probability at least $1-\delta$ with $0<\delta<1$. The analysis in this paper is novel and avoids classical drift and information-geometric techniques. Our analysis offers new insight into why rank-based heuristics lead to efficient ZO optimization.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural emulation of gravity-driven geohazard runout</title>
<link>https://arxiv.org/abs/2512.16221</link>
<guid>https://arxiv.org/abs/2512.16221</guid>
<content:encoded><![CDATA[
arXiv:2512.16221v1 Announce Type: new 
Abstract: Predicting geohazard runout is critical for protecting lives, infrastructure and ecosystems. Rapid mass flows, including landslides and avalanches, cause several thousand deaths across a wide range of environments, often travelling many kilometres from their source. The wide range of source conditions and material properties governing these flows makes their runout difficult to anticipate, particularly for downstream communities that may be suddenly exposed to severe impacts. Accurately predicting runout at scale requires models that are both physically realistic and computationally efficient, yet existing approaches face a fundamental speed-realism trade-off. Here we train a machine learning model to predict geohazard runout across representative real world terrains. The model predicts both flow extent and deposit thickness with high accuracy and 100 to 10,000 times faster computation than numerical solvers. It is trained on over 100,000 numerical simulations across over 10,000 real world digital elevation model chips and reproduces key physical behaviours, including avulsion and deposition patterns, while generalizing across different flow types, sizes and landscapes. Our results demonstrate that neural emulation enables rapid, spatially resolved runout prediction across diverse real world terrains, opening new opportunities for disaster risk reduction and impact-based forecasting. These results highlight neural emulation as a promising pathway for extending physically realistic geohazard modelling to spatial and temporal scales relevant for large scale early warning systems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Coarse-to-Fine Open-Set Graph Node Classification with Large Language Models</title>
<link>https://arxiv.org/abs/2512.16244</link>
<guid>https://arxiv.org/abs/2512.16244</guid>
<content:encoded><![CDATA[
arXiv:2512.16244v1 Announce Type: new 
Abstract: Developing open-set classification methods capable of classifying in-distribution (ID) data while detecting out-of-distribution (OOD) samples is essential for deploying graph neural networks (GNNs) in open-world scenarios. Existing methods typically treat all OOD samples as a single class, despite real-world applications, especially high-stake settings such as fraud detection and medical diagnosis, demanding deeper insights into OOD samples, including their probable labels. This raises a critical question: can OOD detection be extended to OOD classification without true label information? To address this question, we propose a Coarse-to-Fine open-set Classification (CFC) framework that leverages large language models (LLMs) for graph datasets. CFC consists of three key components: a coarse classifier that uses LLM prompts for OOD detection and outlier label generation, a GNN-based fine classifier trained with OOD samples identified by the coarse classifier for enhanced OOD detection and ID classification, and refined OOD classification achieved through LLM prompts and post-processed OOD labels. Unlike methods that rely on synthetic or auxiliary OOD samples, CFC employs semantic OOD instances that are genuinely out-of-distribution based on their inherent meaning, improving interpretability and practical utility. Experimental results show that CFC improves OOD detection by ten percent over state-of-the-art methods on graph and text domains and achieves up to seventy percent accuracy in OOD classification on graph datasets.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sharpness-aware Federated Graph Learning</title>
<link>https://arxiv.org/abs/2512.16247</link>
<guid>https://arxiv.org/abs/2512.16247</guid>
<content:encoded><![CDATA[
arXiv:2512.16247v1 Announce Type: new 
Abstract: One of many impediments to applying graph neural networks (GNNs) to large-scale real-world graph data is the challenge of centralized training, which requires aggregating data from different organizations, raising privacy concerns. Federated graph learning (FGL) addresses this by enabling collaborative GNN model training without sharing private data. However, a core challenge in FGL systems is the variation in local training data distributions among clients, known as the data heterogeneity problem. Most existing solutions suffer from two problems: (1) The typical optimizer based on empirical risk minimization tends to cause local models to fall into sharp valleys and weakens their generalization to out-of-distribution graph data. (2) The prevalent dimensional collapse in the learned representations of local graph data has an adverse impact on the classification capacity of the GNN model. To this end, we formulate a novel optimization objective that is aware of the sharpness (i.e., the curvature of the loss surface) of local GNN models. By minimizing the loss function and its sharpness simultaneously, we seek out model parameters in a flat region with uniformly low loss values, thus improving the generalization over heterogeneous data. By introducing a regularizer based on the correlation matrix of local representations, we relax the correlations of representations generated by individual local graph samples, so as to alleviate the dimensional collapse of the learned model. The proposed \textbf{S}harpness-aware f\textbf{E}derated gr\textbf{A}ph \textbf{L}earning (SEAL) algorithm can enhance the classification accuracy and generalization ability of local GNN models in federated graph learning. Experimental studies on several graph classification benchmarks show that SEAL consistently outperforms SOTA FGL baselines and provides gains for more participants.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sharpness-aware Second-order Latent Factor Model for High-dimensional and Incomplete Data</title>
<link>https://arxiv.org/abs/2512.16277</link>
<guid>https://arxiv.org/abs/2512.16277</guid>
<content:encoded><![CDATA[
arXiv:2512.16277v1 Announce Type: new 
Abstract: Second-order Latent Factor (SLF) model, a class of low-rank representation learning methods, has proven effective at extracting node-to-node interaction patterns from High-dimensional and Incomplete (HDI) data. However, its optimization is notoriously difficult due to its bilinear and non-convex nature. Sharpness-aware Minimization (SAM) has recently proposed to find flat local minima when minimizing non-convex objectives, thereby improving the generalization of representation-learning models. To address this challenge, we propose a Sharpness-aware SLF (SSLF) model. SSLF embodies two key ideas: (1) acquiring second-order information via Hessian-vector products; and (2) injecting a sharpness term into the curvature (Hessian) through the designed Hessian-vector products. Experiments on multiple industrial datasets demonstrate that the proposed model consistently outperforms state-of-the-art baselines.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CKA-Guided Modular Quantization: Beyond Bit-Width to Algorithmic Diversity</title>
<link>https://arxiv.org/abs/2512.16282</link>
<guid>https://arxiv.org/abs/2512.16282</guid>
<content:encoded><![CDATA[
arXiv:2512.16282v1 Announce Type: new 
Abstract: Current mainstream post-training quantization methods for large language models typically apply a uniform quantization strategy across all network layers, overlooking the substantial differences in algorithmic suitability among layers. To address this limitation, we propose CKA Guided Modular Quantization, a fine-tuning-free, plug-and-play framework for algorithmic heterogeneous quantization. Our method independently evaluates multiple PTQ algorithms on each layer and employs Linear Centered Kernel Alignment (CKA) as a metric to automatically select the optimal quantization strategy per layer. The individually optimized strategies are then integrated to construct a hybrid quantized model. Experiments demonstrate that our approach consistently outperforms both uniform quantization baselines and state-of-the-art mixed-precision methods across mainstream LLMs including LLaMA and Qwen ,in terms of perplexity (PPL) and downstream task performance.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feature-Selective Representation Misdirection for Machine Unlearning</title>
<link>https://arxiv.org/abs/2512.16297</link>
<guid>https://arxiv.org/abs/2512.16297</guid>
<content:encoded><![CDATA[
arXiv:2512.16297v1 Announce Type: new 
Abstract: As large language models (LLMs) are increasingly adopted in safety-critical and regulated sectors, the retention of sensitive or prohibited knowledge introduces escalating risks, ranging from privacy leakage to regulatory non-compliance to to potential misuse, and so on. Recent studies suggest that machine unlearning can help ensure deployed models comply with evolving legal, safety, and governance requirements. However, current unlearning techniques assume clean separation between forget and retain datasets, which is challenging in operational settings characterized by highly entangled distributions. In such scenarios, perturbation-based methods often degrade general model utility or fail to ensure safety. To address this, we propose Selective Representation Misdirection for Unlearning (SRMU), a novel principled activation-editing framework that enforces feature-aware and directionally controlled perturbations. Unlike indiscriminate model weights perturbations, SRMU employs a structured misdirection vector with an activation importance map. The goal is to allow SRMU selectively suppresses harmful representations while preserving the utility on benign ones. Experiments are conducted on the widely used WMDP benchmark across low- and high-entanglement configurations. Empirical results reveal that SRMU delivers state-of-the-art unlearning performance with minimal utility losses, and remains effective under 20-30\% overlap where existing baselines collapse. SRMU provides a robust foundation for safety-driven model governance, privacy compliance, and controlled knowledge removal in the emerging LLM-based applications. We release the replication package at https://figshare.com/s/d5931192a8824de26aff.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pretrained Battery Transformer (PBT): A battery life prediction foundation model</title>
<link>https://arxiv.org/abs/2512.16334</link>
<guid>https://arxiv.org/abs/2512.16334</guid>
<content:encoded><![CDATA[
arXiv:2512.16334v1 Announce Type: new 
Abstract: Early prediction of battery cycle life is essential for accelerating battery research, manufacturing, and deployment. Although machine learning methods have shown encouraging results, progress is hindered by data scarcity and heterogeneity arising from diverse aging conditions. In other fields, foundation models (FMs) trained on diverse datasets have achieved broad generalization through transfer learning, but no FMs have been reported for battery cycle life prediction yet. Here we present the Pretrained Battery Transformer (PBT), the first FM for battery life prediction, developed through domain-knowledge-encoded mixture-of-expert layers. Validated on the largest public battery life database, PBT learns transferable representations from 13 lithium-ion battery (LIB) datasets, outperforming existing models by an average of 19.8%. With transfer learning, PBT achieves state-of-the-art performance across 15 diverse datasets encompassing various operating conditions, formation protocols, and chemistries of LIBs. This work establishes a foundation model pathway for battery lifetime prediction, paving the way toward universal battery lifetime prediction systems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multivariate Uncertainty Quantification with Tomographic Quantile Forests</title>
<link>https://arxiv.org/abs/2512.16383</link>
<guid>https://arxiv.org/abs/2512.16383</guid>
<content:encoded><![CDATA[
arXiv:2512.16383v1 Announce Type: new 
Abstract: Quantifying predictive uncertainty is essential for safe and trustworthy real-world AI deployment. Yet, fully nonparametric estimation of conditional distributions remains challenging for multivariate targets. We propose Tomographic Quantile Forests (TQF), a nonparametric, uncertainty-aware, tree-based regression model for multivariate targets. TQF learns conditional quantiles of directional projections $\mathbf{n}^{\top}\mathbf{y}$ as functions of the input $\mathbf{x}$ and the unit direction $\mathbf{n}$. At inference, it aggregates quantiles across many directions and reconstructs the multivariate conditional distribution by minimizing the sliced Wasserstein distance via an efficient alternating scheme with convex subproblems. Unlike classical directional-quantile approaches that typically produce only convex quantile regions and require training separate models for different directions, TQF covers all directions with a single model without imposing convexity restrictions. We evaluate TQF on synthetic and real-world datasets, and release the source code on GitHub.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantitative Verification of Fairness in Tree Ensembles</title>
<link>https://arxiv.org/abs/2512.16386</link>
<guid>https://arxiv.org/abs/2512.16386</guid>
<content:encoded><![CDATA[
arXiv:2512.16386v1 Announce Type: new 
Abstract: This work focuses on quantitative verification of fairness in tree ensembles. Unlike traditional verification approaches that merely return a single counterexample when the fairness is violated, quantitative verification estimates the ratio of all counterexamples and characterizes the regions where they occur, which is important information for diagnosing and mitigating bias. To date, quantitative verification has been explored almost exclusively for deep neural networks (DNNs). Representative methods, such as DeepGemini and FairQuant, all build on the core idea of Counterexample-Guided Abstraction Refinement, a generic framework that could be adapted to other model classes. We extended the framework into a model-agnostic form, but discovered two limitations: (i) it can provide only lower bounds, and (ii) its performance scales poorly. Exploiting the discrete structure of tree ensembles, our work proposes an efficient quantification technique that delivers any-time upper and lower bounds. Experiments on five widely used datasets demonstrate its effectiveness and efficiency. When applied to fairness testing, our quantification method significantly outperforms state-of-the-art testing techniques.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Kascade: A Practical Sparse Attention Method for Long-Context LLM Inference</title>
<link>https://arxiv.org/abs/2512.16391</link>
<guid>https://arxiv.org/abs/2512.16391</guid>
<content:encoded><![CDATA[
arXiv:2512.16391v1 Announce Type: new 
Abstract: Attention is the dominant source of latency during long-context LLM inference, an increasingly popular workload with reasoning models and RAG. We propose Kascade, a training-free sparse attention method that leverages known observations such as 1) post-softmax attention is intrinsically sparse, and 2) the identity of high-weight keys is stable across nearby layers. Kascade computes exact Top-k indices in a small set of anchor layers, then reuses those indices in intermediate reuse layers. The anchor layers are selected algorithmically, via a dynamic-programming objective that maximizes cross-layer similarity over a development set, allowing easy deployment across models. The method incorporates efficient implementation constraints (e.g. tile-level operations), across both prefill and decode attention. The Top-k selection and reuse in Kascade is head-aware and we show in our experiments that this is critical for high accuracy. Kascade achieves up to 4.1x speedup in decode attention and 2.2x speedup in prefill attention over FlashAttention-3 baseline on H100 GPUs while closely matching dense attention accuracy on long-context benchmarks such as LongBench and AIME-24.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NDRL: Cotton Irrigation and Nitrogen Application with Nested Dual-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.16408</link>
<guid>https://arxiv.org/abs/2512.16408</guid>
<content:encoded><![CDATA[
arXiv:2512.16408v1 Announce Type: new 
Abstract: Effective irrigation and nitrogen fertilization have a significant impact on crop yield. However, existing research faces two limitations: (1) the high complexity of optimizing water-nitrogen combinations during crop growth and poor yield optimization results; and (2) the difficulty in quantifying mild stress signals and the delayed feedback, which results in less precise dynamic regulation of water and nitrogen and lower resource utilization efficiency. To address these issues, we propose a Nested Dual-Agent Reinforcement Learning (NDRL) method. The parent agent in NDRL identifies promising macroscopic irrigation and fertilization actions based on projected cumulative yield benefits, reducing ineffective explorationwhile maintaining alignment between objectives and yield. The child agent's reward function incorporates quantified Water Stress Factor (WSF) and Nitrogen Stress Factor (NSF), and uses a mixed probability distribution to dynamically optimize daily strategies, thereby enhancing both yield and resource efficiency. We used field experiment data from 2023 and 2024 to calibrate and validate the Decision Support System for Agrotechnology Transfer (DSSAT) to simulate real-world conditions and interact with NDRL. Experimental results demonstrate that, compared to the best baseline, the simulated yield increased by 4.7% in both 2023 and 2024, the irrigation water productivity increased by 5.6% and 5.1% respectively, and the nitrogen partial factor productivity increased by 6.3% and 1.0% respectively. Our method advances the development of cotton irrigation and nitrogen fertilization, providing new ideas for addressing the complexity and precision issues in agricultural resource management and for sustainable agricultural development.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geometric Laplace Neural Operator</title>
<link>https://arxiv.org/abs/2512.16409</link>
<guid>https://arxiv.org/abs/2512.16409</guid>
<content:encoded><![CDATA[
arXiv:2512.16409v1 Announce Type: new 
Abstract: Neural operators have emerged as powerful tools for learning mappings between function spaces, enabling efficient solutions to partial differential equations across varying inputs and domains. Despite the success, existing methods often struggle with non-periodic excitations, transient responses, and signals defined on irregular or non-Euclidean geometries. To address this, we propose a generalized operator learning framework based on a pole-residue decomposition enriched with exponential basis functions, enabling expressive modeling of aperiodic and decaying dynamics. Building on this formulation, we introduce the Geometric Laplace Neural Operator (GLNO), which embeds the Laplace spectral representation into the eigen-basis of the Laplace-Beltrami operator, extending operator learning to arbitrary Riemannian manifolds without requiring periodicity or uniform grids. We further design a grid-invariant network architecture (GLNONet) that realizes GLNO in practice. Extensive experiments on PDEs/ODEs and real-world datasets demonstrate our robust performance over other state-of-the-art models.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Fidelity Delayed Acceptance: hierarchical MCMC sampling for Bayesian inverse problems combining multiple solvers through deep neural networks</title>
<link>https://arxiv.org/abs/2512.16430</link>
<guid>https://arxiv.org/abs/2512.16430</guid>
<content:encoded><![CDATA[
arXiv:2512.16430v1 Announce Type: new 
Abstract: Inverse uncertainty quantification (UQ) tasks such as parameter estimation are computationally demanding whenever dealing with physics-based models, and typically require repeated evaluations of complex numerical solvers. When partial differential equations are involved, full-order models such as those based on the Finite Element Method can make traditional sampling approaches like Markov Chain Monte Carlo (MCMC) computationally infeasible. Although data-driven surrogate models may help reduce evaluation costs, their utility is often limited by the expense of generating high-fidelity data. In contrast, low-fidelity data can be produced more efficiently, although relying on them alone may degrade the accuracy of the inverse UQ solution.
  To address these challenges, we propose a Multi-Fidelity Delayed Acceptance scheme for Bayesian inverse problems. Extending the Multi-Level Delayed Acceptance framework, the method introduces multi-fidelity neural networks that combine the predictions of solvers of varying fidelity, with high fidelity evaluations restricted to an offline training stage. During the online phase, likelihood evaluations are obtained by evaluating the coarse solvers and passing their outputs to the trained neural networks, thereby avoiding additional high-fidelity simulations.
  This construction allows heterogeneous coarse solvers to be incorporated consistently within the hierarchy, providing greater flexibility than standard Multi-Level Delayed Acceptance. The proposed approach improves the approximation accuracy of the low fidelity solvers, leading to longer sub-chain lengths, better mixing, and accelerated posterior inference. The effectiveness of the strategy is demonstrated on two benchmark inverse problems involving (i) steady isotropic groundwater flow, (ii) an unsteady reaction-diffusion system, for which substantial computational savings are obtained.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emergent Bias and Fairness in Multi-Agent Decision Systems</title>
<link>https://arxiv.org/abs/2512.16433</link>
<guid>https://arxiv.org/abs/2512.16433</guid>
<content:encoded><![CDATA[
arXiv:2512.16433v1 Announce Type: new 
Abstract: Multi-agent systems have demonstrated the ability to improve performance on a variety of predictive tasks by leveraging collaborative decision making. However, the lack of effective evaluation methodologies has made it difficult to estimate the risk of bias, making deployment of such systems unsafe in high stakes domains such as consumer finance, where biased decisions can translate directly into regulatory breaches and financial loss. To address this challenge, we need to develop fairness evaluation methodologies for multi-agent predictive systems and measure the fairness characteristics of these systems in the financial tabular domain. Examining fairness metrics using large-scale simulations across diverse multi-agent configurations, with varying communication and collaboration mechanisms, we reveal patterns of emergent bias in financial decision-making that cannot be traced to individual agent components, indicating that multi-agent systems may exhibit genuinely collective behaviors. Our findings highlight that fairness risks in financial multi-agent systems represent a significant component of model risk, with tangible impacts on tasks such as credit scoring and income estimation. We advocate that multi-agent decision systems must be evaluated as holistic entities rather than through reductionist analyses of their constituent components.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Novel Proposal in Wind Turbine Blade Failure Detection: An Integrated Approach to Energy Efficiency and Sustainability</title>
<link>https://arxiv.org/abs/2512.16437</link>
<guid>https://arxiv.org/abs/2512.16437</guid>
<content:encoded><![CDATA[
arXiv:2512.16437v1 Announce Type: new 
Abstract: This paper presents a novel methodology for detecting faults in wind turbine blades using com-putational learning techniques. The study evaluates two models: the first employs logistic regression, which outperformed neural networks, decision trees, and the naive Bayes method, demonstrating its effectiveness in identifying fault-related patterns. The second model leverages clustering and achieves superior performance in terms of precision and data segmentation. The results indicate that clustering may better capture the underlying data characteristics compared to supervised methods. The proposed methodology offers a new approach to early fault detection in wind turbine blades, highlighting the potential of integrating different computational learning techniques to enhance system reliability. The use of accessible tools like Orange Data Mining underscores the practical application of these advanced solutions within the wind energy sector. Future work will focus on combining these methods to improve detection accuracy further and extend the application of these techniques to other critical components in energy infrastructure.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Topic Modelling Black Box Optimization</title>
<link>https://arxiv.org/abs/2512.16445</link>
<guid>https://arxiv.org/abs/2512.16445</guid>
<content:encoded><![CDATA[
arXiv:2512.16445v1 Announce Type: new 
Abstract: Choosing the number of topics $T$ in Latent Dirichlet Allocation (LDA) is a key design decision that strongly affects both the statistical fit and interpretability of topic models. In this work, we formulate the selection of $T$ as a discrete black-box optimization problem, where each function evaluation corresponds to training an LDA model and measuring its validation perplexity. Under a fixed evaluation budget, we compare four families of optimizers: two hand-designed evolutionary methods - Genetic Algorithm (GA) and Evolution Strategy (ES) - and two learned, amortized approaches, Preferential Amortized Black-Box Optimization (PABBO) and Sharpness-Aware Black-Box Optimization (SABBO). Our experiments show that, while GA, ES, PABBO, and SABBO eventually reach a similar band of final perplexity, the amortized optimizers are substantially more sample- and time-efficient. SABBO typically identifies a near-optimal topic number after essentially a single evaluation, and PABBO finds competitive configurations within a few evaluations, whereas GA and ES require almost the full budget to approach the same region.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IoMT-based Automated Leukemia Classification using CNN and Higher Order Singular Value</title>
<link>https://arxiv.org/abs/2512.16448</link>
<guid>https://arxiv.org/abs/2512.16448</guid>
<content:encoded><![CDATA[
arXiv:2512.16448v1 Announce Type: new 
Abstract: The Internet of Things (IoT) is a concept by which objects find identity and can communicate with each other in a network. One of the applications of the IoT is in the field of medicine, which is called the Internet of Medical Things (IoMT). Acute Lymphocytic Leukemia (ALL) is a type of cancer categorized as a hematic disease. It usually begins in the bone marrow due to the overproduction of immature White Blood Cells (WBCs or leukocytes). Since it has a high rate of spread to other body organs, it is a fatal disease if not diagnosed and treated early. Therefore, for identifying cancerous (ALL) cells in medical diagnostic laboratories, blood, as well as bone marrow smears, are taken by pathologists. However, manual examinations face limitations due to human error risk and time-consuming procedures. So, to tackle the mentioned issues, methods based on Artificial Intelligence (AI), capable of identifying cancer from non-cancer tissue, seem vital. Deep Neural Networks (DNNs) are the most efficient machine learning (ML) methods. These techniques employ multiple layers to extract higher-level features from the raw input. In this paper, a Convolutional Neural Network (CNN) is applied along with a new type of classifier, Higher Order Singular Value Decomposition (HOSVD), to categorize ALL and normal (healthy) cells from microscopic blood images. We employed the model on IoMT structure to identify leukemia quickly and safely. With the help of this new leukemia classification framework, patients and clinicians can have real-time communication. The model was implemented on the Acute Lymphoblastic Leukemia Image Database (ALL-IDB2) and achieved an average accuracy of %98.88 in the test step.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Batch Normalization-Free Fully Integer Quantized Neural Networks via Progressive Tandem Learning</title>
<link>https://arxiv.org/abs/2512.16476</link>
<guid>https://arxiv.org/abs/2512.16476</guid>
<content:encoded><![CDATA[
arXiv:2512.16476v1 Announce Type: new 
Abstract: Quantised neural networks (QNNs) shrink models and reduce inference energy through low-bit arithmetic, yet most still depend on a running statistics batch normalisation (BN) layer, preventing true integer-only deployment. Prior attempts remove BN by parameter folding or tailored initialisation; while helpful, they rarely recover BN's stability and accuracy and often impose bespoke constraints. We present a BN-free, fully integer QNN trained via a progressive, layer-wise distillation scheme that slots into existing low-bit pipelines. Starting from a pretrained BN-enabled teacher, we use layer-wise targets and progressive compensation to train a student that performs inference exclusively with integer arithmetic and contains no BN operations. On ImageNet with AlexNet, the BN-free model attains competitive Top-1 accuracy under aggressive quantisation. The procedure integrates directly with standard quantisation workflows, enabling end-to-end integer-only inference for resource-constrained settings such as edge and embedded devices.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Persistent Multiscale Density-based Clustering</title>
<link>https://arxiv.org/abs/2512.16558</link>
<guid>https://arxiv.org/abs/2512.16558</guid>
<content:encoded><![CDATA[
arXiv:2512.16558v1 Announce Type: new 
Abstract: Clustering is a cornerstone of modern data analysis. Detecting clusters in exploratory data analyses (EDA) requires algorithms that make few assumptions about the data. Density-based clustering algorithms are particularly well-suited for EDA because they describe high-density regions, assuming only that a density exists. Applying density-based clustering algorithms in practice, however, requires selecting appropriate hyperparameters, which is difficult without prior knowledge of the data distribution. For example, DBSCAN requires selecting a density threshold, and HDBSCAN* relies on a minimum cluster size parameter. In this work, we propose Persistent Leaves Spatial Clustering for Applications with Noise (PLSCAN). This novel density-based clustering algorithm efficiently identifies all minimum cluster sizes for which HDBSCAN* produces stable (leaf) clusters. PLSCAN applies scale-space clustering principles and is equivalent to persistent homology on a novel metric space. We compare its performance to HDBSCAN* on several real-world datasets, demonstrating that it achieves a higher average ARI and is less sensitive to changes in the number of mutual reachability neighbours. Additionally, we compare PLSCAN's computational costs to k-Means, demonstrating competitive run-times on low-dimensional datasets. At higher dimensions, run times scale more similarly to HDBSCAN*.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Abacus: Self-Supervised Event Counting-Aligned Distributional Pretraining for Sequential User Modeling</title>
<link>https://arxiv.org/abs/2512.16581</link>
<guid>https://arxiv.org/abs/2512.16581</guid>
<content:encoded><![CDATA[
arXiv:2512.16581v1 Announce Type: new 
Abstract: Modeling user purchase behavior is a critical challenge in display advertising systems, necessary for real-time bidding. The difficulty arises from the sparsity of positive user events and the stochasticity of user actions, leading to severe class imbalance and irregular event timing. Predictive systems usually rely on hand-crafted "counter" features, overlooking the fine-grained temporal evolution of user intent. Meanwhile, current sequential models extract direct sequential signal, missing useful event-counting statistics. We enhance deep sequential models with self-supervised pretraining strategies for display advertising. Especially, we introduce Abacus, a novel approach of predicting the empirical frequency distribution of user events. We further propose a hybrid objective unifying Abacus with sequential learning objectives, combining stability of aggregated statistics with the sequence modeling sensitivity. Experiments on two real-world datasets show that Abacus pretraining outperforms existing methods accelerating downstream task convergence, while hybrid approach yields up to +6.1% AUC compared to the baselines.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stackelberg Learning from Human Feedback: Preference Optimization as a Sequential Game</title>
<link>https://arxiv.org/abs/2512.16626</link>
<guid>https://arxiv.org/abs/2512.16626</guid>
<content:encoded><![CDATA[
arXiv:2512.16626v1 Announce Type: new 
Abstract: We introduce Stackelberg Learning from Human Feedback (SLHF), a new framework for preference optimization. SLHF frames the alignment problem as a sequential-move game between two policies: a Leader, which commits to an action, and a Follower, which responds conditionally on the Leader's action. This approach decomposes preference optimization into a refinement problem for the Follower and an optimization problem against an adversary for the Leader. Unlike Reinforcement Learning from Human Feedback (RLHF), which assigns scalar rewards to actions, or Nash Learning from Human Feedback (NLHF), which seeks a simultaneous-move equilibrium, SLHF leverages the asymmetry of sequential play to capture richer preference structures. The sequential design of SLHF naturally enables inference-time refinement, as the Follower learns to improve the Leader's actions, and these refinements can be leveraged through iterative sampling. We compare the solution concepts of SLHF, RLHF, and NLHF, and lay out key advantages in consistency, data sensitivity, and robustness to intransitive preferences. Experiments on large language models demonstrate that SLHF achieves strong alignment across diverse preference datasets, scales from 0.5B to 8B parameters, and yields inference-time refinements that transfer across model families without further fine-tuning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploiting Radio Frequency Fingerprints for Device Identification: Tackling Cross-receiver Challenges in the Source-data-free Scenario</title>
<link>https://arxiv.org/abs/2512.16648</link>
<guid>https://arxiv.org/abs/2512.16648</guid>
<content:encoded><![CDATA[
arXiv:2512.16648v1 Announce Type: new 
Abstract: With the rapid proliferation of edge computing, Radio Frequency Fingerprint Identification (RFFI) has become increasingly important for secure device authentication. However, practical deployment of deep learning-based RFFI models is hindered by a critical challenge: their performance often degrades significantly when applied across receivers with different hardware characteristics due to distribution shifts introduced by receiver variation. To address this, we investigate the source-data-free cross-receiver RFFI (SCRFFI) problem, where a model pretrained on labeled signals from a source receiver must adapt to unlabeled signals from a target receiver, without access to any source-domain data during adaptation. We first formulate a novel constrained pseudo-labeling-based SCRFFI adaptation framework, and provide a theoretical analysis of its generalization performance. Our analysis highlights a key insight: the target-domain performance is highly sensitive to the quality of the pseudo-labels generated during adaptation. Motivated by this, we propose Momentum Soft pseudo-label Source Hypothesis Transfer (MS-SHOT), a new method for SCRFFI that incorporates momentum-center-guided soft pseudo-labeling and enforces global structural constraints to encourage confident and diverse predictions. Notably, MS-SHOT effectively addresses scenarios involving label shift or unknown, non-uniform class distributions in the target domain -- a significant limitation of prior methods. Extensive experiments on real-world datasets demonstrate that MS-SHOT consistently outperforms existing approaches in both accuracy and robustness, offering a practical and scalable solution for source-data-free cross-receiver adaptation in RFFI.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI</title>
<link>https://arxiv.org/abs/2512.16676</link>
<guid>https://arxiv.org/abs/2512.16676</guid>
<content:encoded><![CDATA[
arXiv:2512.16676v1 Announce Type: new 
Abstract: The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\% execution accuracy in Text-to-SQL over SynSQL, +7\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Blog Data Showdown: Machine Learning vs Neuro-Symbolic Models for Gender Classification</title>
<link>https://arxiv.org/abs/2512.16687</link>
<guid>https://arxiv.org/abs/2512.16687</guid>
<content:encoded><![CDATA[
arXiv:2512.16687v1 Announce Type: new 
Abstract: Text classification problems, such as gender classification from a blog, have been a well-matured research area that has been well studied using machine learning algorithms. It has several application domains in market analysis, customer recommendation, and recommendation systems. This study presents a comparative analysis of the widely used machine learning algorithms, namely Support Vector Machines (SVM), Naive Bayes (NB), Logistic Regression (LR), AdaBoost, XGBoost, and an SVM variant (SVM_R) with neuro-symbolic AI (NeSy). The paper also explores the effect of text representations such as TF-IDF, the Universal Sentence Encoder (USE), and RoBERTa. Additionally, various feature extraction techniques, including Chi-Square, Mutual Information, and Principal Component Analysis, are explored. Building on these, we introduce a comparative analysis of the machine learning and deep learning approaches in comparison to the NeSy. The experimental results show that the use of the NeSy approach matched strong MLP results despite a limited dataset. Future work on this research will expand the knowledge base, the scope of embedding types, and the hyperparameter configuration to further study the effectiveness of the NeSy approach.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLARiTy: A Vision Transformer for Multi-Label Classification and Weakly-Supervised Localization of Chest X-ray Pathologies</title>
<link>https://arxiv.org/abs/2512.16700</link>
<guid>https://arxiv.org/abs/2512.16700</guid>
<content:encoded><![CDATA[
arXiv:2512.16700v1 Announce Type: new 
Abstract: The interpretation of chest X-rays (CXRs) poses significant challenges, particularly in achieving accurate multi-label pathology classification and spatial localization. These tasks demand different levels of annotation granularity but are frequently constrained by the scarcity of region-level (dense) annotations. We introduce CLARiTy (Class Localizing and Attention Refining Image Transformer), a vision transformer-based model for joint multi-label classification and weakly-supervised localization of thoracic pathologies. CLARiTy employs multiple class-specific tokens to generate discriminative attention maps, and a SegmentCAM module for foreground segmentation and background suppression using explicit anatomical priors. Trained on image-level labels from the NIH ChestX-ray14 dataset, it leverages distillation from a ConvNeXtV2 teacher for efficiency. Evaluated on the official NIH split, the CLARiTy-S-16-512 (a configuration of CLARiTy), achieves competitive classification performance across 14 pathologies, and state-of-the-art weakly-supervised localization performance on 8 pathologies, outperforming prior methods by 50.7%. In particular, pronounced gains occur for small pathologies like nodules and masses. The lower-resolution variant of CLARiTy, CLARiTy-S-16-224, offers high efficiency while decisively surpassing baselines, thereby having the potential for use in low-resource settings. An ablation study confirms contributions of SegmentCAM, DINO pretraining, orthogonal class token loss, and attention pooling. CLARiTy advances beyond CNN-ViT hybrids by harnessing ViT self-attention for global context and class-specific localization, refined through convolutional background suppression for precise, noise-reduced heatmaps.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Reproducibility in Predictive Process Mining: SPICE - A Deep Learning Library</title>
<link>https://arxiv.org/abs/2512.16715</link>
<guid>https://arxiv.org/abs/2512.16715</guid>
<content:encoded><![CDATA[
arXiv:2512.16715v1 Announce Type: new 
Abstract: In recent years, Predictive Process Mining (PPM) techniques based on artificial neural networks have evolved as a method for monitoring the future behavior of unfolding business processes and predicting Key Performance Indicators (KPIs). However, many PPM approaches often lack reproducibility, transparency in decision making, usability for incorporating novel datasets and benchmarking, making comparisons among different implementations very difficult. In this paper, we propose SPICE, a Python framework that reimplements three popular, existing baseline deep-learning-based methods for PPM in PyTorch, while designing a common base framework with rigorous configurability to enable reproducible and robust comparison of past and future modelling approaches. We compare SPICE to original reported metrics and with fair metrics on 11 datasets.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Phishing Detection System: An Ensemble Approach Using Character-Level CNN and Feature Engineering</title>
<link>https://arxiv.org/abs/2512.16717</link>
<guid>https://arxiv.org/abs/2512.16717</guid>
<content:encoded><![CDATA[
arXiv:2512.16717v1 Announce Type: new 
Abstract: In actuality, phishing attacks remain one of the most prevalent cybersecurity risks in existence today, with malevolent actors constantly changing their strategies to successfully trick users. This paper presents an AI model for a phishing detection system that uses an ensemble approach to combine character-level Convolutional Neural Networks (CNN) and LightGBM with engineered features. Our system uses a character-level CNN to extract sequential features after extracting 36 lexical, structural, and domain-based features from the URLs. On a test dataset of 19,873 URLs, the ensemble model achieves an accuracy of 99.819 percent, precision of 100 percent, recall of 99.635 percent, and ROC-AUC of 99.947 percent. Through a FastAPI-based service with an intuitive user interface, the suggested system has been utilised to offer real-time detection. In contrast, the results demonstrate that the suggested solution performs better than individual models; LightGBM contributes 40 percent and character-CNN contributes 60 percent to the final prediction. The suggested method maintains extremely low false positive rates while doing a good job of identifying contemporary phishing techniques. Index Terms - Phishing detection, machine learning, deep learning, CNN, ensemble methods, cybersecurity, URL analysis
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Polyharmonic Spline Packages: Composition, Efficient Procedures for Computation and Differentiation</title>
<link>https://arxiv.org/abs/2512.16718</link>
<guid>https://arxiv.org/abs/2512.16718</guid>
<content:encoded><![CDATA[
arXiv:2512.16718v1 Announce Type: new 
Abstract: In a previous paper it was shown that a machine learning regression problem can be solved within the framework of random function theory, with the optimal kernel analytically derived from symmetry and indifference principles and coinciding with a polyharmonic spline. However, a direct application of that solution is limited by O(N^3) computational cost and by a breakdown of the original theoretical assumptions when the input space has excessive dimensionality. This paper proposes a cascade architecture built from packages of polyharmonic splines that simultaneously addresses scalability and is theoretically justified for problems with unknown intrinsic low dimensionality. Efficient matrix procedures are presented for forward computation and end-to-end differentiation through the cascade.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KOSS: Kalman-Optimal Selective State Spaces for Long-Term Sequence Modeling</title>
<link>https://arxiv.org/abs/2512.16723</link>
<guid>https://arxiv.org/abs/2512.16723</guid>
<content:encoded><![CDATA[
arXiv:2512.16723v1 Announce Type: new 
Abstract: Recent selective state space models (SSMs), such as Mamba and Mamba-2, have demonstrated strong performance in sequence modeling owing to input-dependent selection mechanisms. However, these mechanisms lack theoretical grounding and cannot support context-aware selection from latent state dynamics. To address these limitations, we propose KOSS, a Kalman-optimal Selective State Space model that formulates selection as latent state uncertainty minimization. Derived from estimation theory, KOSS adopts a continuous-time latent update driven by a Kalman gain that dynamically modulates information propagation based on content and context, enabling a closed-loop, context-aware selectivity mechanism. To ensure stable computation and near-linear scalability, KOSS employs global spectral differentiation for frequency-domain derivative estimation, along with a segment-wise scan for hardware-efficient processing. On a selective copying task with distractors, KOSS achieves over 79\% accuracy while baselines drop below 20\%, demonstrating robust context-aware selection. Furthermore, across nine long-term forecasting benchmarks, KOSS reduces MSE by 2.92--36.23\% and consistently outperforms state-of-the-art models in both accuracy and stability. To assess real-world applicability, a case study on secondary surveillance radar (SSR) tracking confirms KOSS's robustness under irregular intervals and noisy conditions and demonstrates its effectiveness in real-world applications. Finally, supplementary experiments verify Kalman gain convergence and the frequency response of spectral differentiation, providing theoretical support for the proposed closed-loop design.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Learning Algorithms: Detection Official Hajj and Umrah Travel Agency Based on Text and Metadata Analysis</title>
<link>https://arxiv.org/abs/2512.16742</link>
<guid>https://arxiv.org/abs/2512.16742</guid>
<content:encoded><![CDATA[
arXiv:2512.16742v1 Announce Type: new 
Abstract: The rapid digitalization of Hajj and Umrah services in Indonesia has significantly facilitated pilgrims but has concurrently opened avenues for digital fraud through counterfeit mobile applications. These fraudulent applications not only inflict financial losses but also pose severe privacy risks by harvesting sensitive personal data. This research aims to address this critical issue by implementing and evaluating machine learning algorithms to verify application authenticity automatically. Using a comprehensive dataset comprising both official applications registered with the Ministry of Religious Affairs and unofficial applications circulating on app stores, we compare the performance of three robust classifiers: Support Vector Machine (SVM), Random Forest (RF), and Na"ive Bayes (NB). The study utilizes a hybrid feature extraction methodology that combines Textual Analysis (TF-IDF) of application descriptions with Metadata Analysis of sensitive access permissions. The experimental results indicate that the SVM algorithm achieves the highest performance with an accuracy of 92.3%, a precision of 91.5%, and an F1-score of 92.0%. Detailed feature analysis reveals that specific keywords related to legality and high-risk permissions (e.g., READ PHONE STATE) are the most significant discriminators. This system is proposed as a proactive, scalable solution to enhance digital trust in the religious tourism sector, potentially serving as a prototype for a national verification system.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NRGPT: An Energy-based Alternative for GPT</title>
<link>https://arxiv.org/abs/2512.16762</link>
<guid>https://arxiv.org/abs/2512.16762</guid>
<content:encoded><![CDATA[
arXiv:2512.16762v1 Announce Type: new 
Abstract: Generative Pre-trained Transformer (GPT) architectures are the most popular design for language modeling. Energy-based modeling is a different paradigm that views inference as a dynamical process operating on an energy landscape. We propose a minimal modification of the GPT setting to unify it with the EBM framework. The inference step of our model, which we call eNeRgy-GPT (NRGPT), is conceptualized as an exploration of the tokens on the energy landscape. We prove, and verify empirically, that under certain circumstances this exploration becomes gradient descent, although they don't necessarily lead to the best performing models. We demonstrate that our model performs well for simple language (Shakespeare dataset), algebraic ListOPS tasks, and richer settings such as OpenWebText language modeling. We also observe that our models may be more resistant to overfitting, doing so only during very long training.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pattern recognition in complex systems via vector-field representations of spatio-temporal data</title>
<link>https://arxiv.org/abs/2512.16763</link>
<guid>https://arxiv.org/abs/2512.16763</guid>
<content:encoded><![CDATA[
arXiv:2512.16763v1 Announce Type: new 
Abstract: A complex system comprises multiple interacting entities whose interdependencies form a unified whole, exhibiting emergent behaviours not present in individual components. Examples include the human brain, living cells, soft matter, Earth's climate, ecosystems, and the economy. These systems exhibit high-dimensional, non-linear dynamics, making their modelling, classification, and prediction particularly challenging. Advances in information technology have enabled data-driven approaches to studying such systems. However, the sheer volume and complexity of spatio-temporal data often hinder traditional methods like dimensionality reduction, phase-space reconstruction, and attractor characterisation. This paper introduces a geometric framework for analysing spatio-temporal data from complex systems, grounded in the theory of vector fields over discrete measure spaces. We propose a two-parameter family of metrics suitable for data analysis and machine learning applications. The framework supports time-dependent images, image gradients, and real- or vector-valued functions defined on graphs and simplicial complexes. We validate our approach using data from numerical simulations of biological and physical systems on flat and curved domains. Our results show that the proposed metrics, combined with multidimensional scaling, effectively address key analytical challenges. They enable dimensionality reduction, mode decomposition, phase-space reconstruction, and attractor characterisation. Our findings offer a robust pathway for understanding complex dynamical systems, especially in contexts where traditional modelling is impractical but abundant experimental data are available.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MEPIC: Memory Efficient Position Independent Caching for LLM Serving</title>
<link>https://arxiv.org/abs/2512.16822</link>
<guid>https://arxiv.org/abs/2512.16822</guid>
<content:encoded><![CDATA[
arXiv:2512.16822v1 Announce Type: new 
Abstract: Modern LLM applications such as deep-research assistants, coding agents, and Retrieval-Augmented Generation (RAG) systems, repeatedly process long prompt histories containing shared document or code chunks, creating significant pressure on the Key Value (KV) cache, which must operate within limited memory while sustaining high throughput and low latency. Prefix caching partially alleviates some of these costs by reusing KV cache for previously processed tokens, but limited by strict prefix matching. Position-independent caching (PIC) enables chunk-level reuse at arbitrary positions, but requires selective recomputation and positional-encoding (PE) adjustments. However, because these operations vary across queries, KV for the same chunk diverges across requests. Moreover, without page alignment, chunk KV layouts diverge in memory, preventing page sharing. These issues result in only modest HBM savings even when many requests reuse the same content.
  We present MEPIC, a memory-efficient PIC system that enables chunk KV reuse across positions, requests, and batches. MEPIC aligns chunk KV to paged storage, shifts recomputation from token- to block-level so only the first block is request-specific, removes positional encodings via Rotary Position Embedding (RoPE) fusion in the attention kernel, and makes remaining blocks fully shareable. These techniques eliminate most duplicate chunk KV in HBM, reducing usage by up to 2x over state-of-the-art PIC at comparable latency and accuracy, and up to 5x for long prompts, without any model changes.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tiny Recursive Control: Iterative Reasoning for Efficient Optimal Control</title>
<link>https://arxiv.org/abs/2512.16824</link>
<guid>https://arxiv.org/abs/2512.16824</guid>
<content:encoded><![CDATA[
arXiv:2512.16824v1 Announce Type: new 
Abstract: Neural network controllers increasingly demand millions of parameters, and language model approaches push into the billions. For embedded aerospace systems with strict power and latency constraints, this scaling is prohibitive. We present Tiny Recursive Control (TRC), a neural architecture based on a counterintuitive principle: capacity can emerge from iteration depth rather than parameter count. TRC applies compact networks (approximately 1.5M parameters) repeatedly through a two-level hierarchical latent structure, refining control sequences by simulating trajectories and correcting based on tracking error. Because the same weights process every refinement step, adding iterations increases computation without increasing memory. We evaluate TRC on nonlinear control problems including oscillator stabilization and powered descent with fuel constraints. Across these domains, TRC achieves near-optimal control costs while requiring only millisecond-scale inference on GPU and under 10~MB memory, two orders of magnitude smaller than language model baselines. These results demonstrate that recursive reasoning, previously confined to discrete tasks, transfers effectively to continuous control synthesis.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta-RL Induces Exploration in Language Agents</title>
<link>https://arxiv.org/abs/2512.16848</link>
<guid>https://arxiv.org/abs/2512.16848</guid>
<content:encoded><![CDATA[
arXiv:2512.16848v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has enabled the training of large language model (LLM) agents to interact with the environment and to solve multi-turn long-horizon tasks. However, the RL-trained agents often struggle in tasks that require active exploration and fail to efficiently adapt from trial-and-error experiences. In this paper, we present LaMer, a general Meta-RL framework that enables LLM agents to actively explore and learn from the environment feedback at test time. LaMer consists of two key components: (i) a cross-episode training framework to encourage exploration and long-term rewards optimization; and (ii) in-context policy adaptation via reflection, allowing the agent to adapt their policy from task feedback signal without gradient update. Experiments across diverse environments show that LaMer significantly improves performance over RL baselines, with 11%, 14%, and 19% performance gains on Sokoban, MineSweeper and Webshop, respectively. Moreover, LaMer also demonstrates better generalization to more challenging or previously unseen tasks compared to the RL-trained agents. Overall, our results demonstrate that Meta-RL provides a principled approach to induce exploration in language agents, enabling more robust adaptation to novel environments through learned exploration strategies.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semi-Supervised Online Learning on the Edge by Transforming Knowledge from Teacher Models</title>
<link>https://arxiv.org/abs/2512.16866</link>
<guid>https://arxiv.org/abs/2512.16866</guid>
<content:encoded><![CDATA[
arXiv:2512.16866v1 Announce Type: new 
Abstract: Edge machine learning (Edge ML) enables training ML models using the vast data distributed across network edges. However, many existing approaches assume static models trained centrally and then deployed, making them ineffective against unseen data. To address this, Online Edge ML allows models to be trained directly on edge devices and updated continuously with new data. This paper explores a key challenge of Online Edge ML: "How to determine labels for truly future, unseen data points". We propose Knowledge Transformation (KT), a hybrid method combining Knowledge Distillation, Active Learning, and causal reasoning. In short, KT acts as the oracle in active learning by transforming knowledge from a teacher model to generate pseudo-labels for training a student model. To verify the validity of the method, we conducted simulation experiments with two setups: (1) using a less stable teacher model and (2) a relatively more stable teacher model. Results indicate that when a stable teacher model is given, the student model can eventually reach its expected maximum performance. KT is potentially beneficial for scenarios that meet the following circumstances: (1) when the teacher's task is generic, which means existing pre-trained models might be adequate for its task, so there will be no need to train the teacher model from scratch; and/or (2) when the label for the student's task is difficult or expensive to acquire.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sequencing to Mitigate Catastrophic Forgetting in Continual Learning</title>
<link>https://arxiv.org/abs/2512.16871</link>
<guid>https://arxiv.org/abs/2512.16871</guid>
<content:encoded><![CDATA[
arXiv:2512.16871v1 Announce Type: new 
Abstract: To cope with real-world dynamics, an intelligent system needs to incrementally acquire, update, and exploit knowledge throughout its lifetime. This ability, known as Continual learning, provides a foundation for AI systems to develop themselves adaptively. Catastrophic forgetting is a major challenge to the progress of Continual Learning approaches, where learning a new task usually results in a dramatic performance drop on previously learned ones. Many approaches have emerged to counteract the impact of CF. Most of the proposed approaches can be categorized into five classes: replay-based, regularization-based, optimization-based, representation-based, and architecture-based. In this work, we approach the problem from a different angle, specifically by considering the optimal sequencing of tasks as they are presented to the model. We investigate the role of task sequencing in mitigating CF and propose a method for determining the optimal task order. The proposed method leverages zero-shot scoring algorithms inspired by neural architecture search (NAS). Results demonstrate that intelligent task sequencing can substantially reduce CF. Moreover, when combined with traditional continual learning strategies, sequencing offers enhanced performance and robustness against forgetting. Additionally, the presented approaches can find applications in other fields, such as curriculum learning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Together, Diagnosing Better: Federated Learning for Collagen VI-Related Dystrophies</title>
<link>https://arxiv.org/abs/2512.16876</link>
<guid>https://arxiv.org/abs/2512.16876</guid>
<content:encoded><![CDATA[
arXiv:2512.16876v1 Announce Type: new 
Abstract: The application of Machine Learning (ML) to the diagnosis of rare diseases, such as collagen VI-related dystrophies (COL6-RD), is fundamentally limited by the scarcity and fragmentation of available data. Attempts to expand sampling across hospitals, institutions, or countries with differing regulations face severe privacy, regulatory, and logistical obstacles that are often difficult to overcome. The Federated Learning (FL) provides a promising solution by enabling collaborative model training across decentralized datasets while keeping patient data local and private. Here, we report a novel global FL initiative using the Sherpa.ai FL platform, which leverages FL across distributed datasets in two international organizations for the diagnosis of COL6-RD, using collagen VI immunofluorescence microscopy images from patient-derived fibroblast cultures. Our solution resulted in an ML model capable of classifying collagen VI patient images into the three primary pathogenic mechanism groups associated with COL6-RD: exon skipping, glycine substitution, and pseudoexon insertion. This new approach achieved an F1-score of 0.82, outperforming single-organization models (0.57-0.75). These results demonstrate that FL substantially improves diagnostic utility and generalizability compared to isolated institutional models. Beyond enabling more accurate diagnosis, we anticipate that this approach will support the interpretation of variants of uncertain significance and guide the prioritization of sequencing strategies to identify novel pathogenic variants.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Impacts of Racial Bias in Historical Training Data for News AI</title>
<link>https://arxiv.org/abs/2512.16901</link>
<guid>https://arxiv.org/abs/2512.16901</guid>
<content:encoded><![CDATA[
arXiv:2512.16901v1 Announce Type: new 
Abstract: AI technologies have rapidly moved into business and research applications that involve large text corpora, including computational journalism research and newsroom settings. These models, trained on extant data from various sources, can be conceptualized as historical artifacts that encode decades-old attitudes and stereotypes. This paper investigates one such example trained on the broadly-used New York Times Annotated Corpus to create a multi-label classifier. Our use in research settings surfaced the concerning "blacks" thematic topic label. Through quantitative and qualitative means we investigate this label's use in the training corpus, what concepts it might be encoding in the trained classifier, and how those concepts impact our model use. Via the application of explainable AI methods, we find that the "blacks" label operates partially as a general "racism detector" across some minoritized groups. However, it performs poorly against expectations on modern examples such as COVID-19 era anti-Asian hate stories, and reporting on the Black Lives Matter movement. This case study of interrogating embedded biases in a model reveals how similar applications in newsroom settings can lead to unexpected outputs that could impact a wide variety of potential uses of any large language model-story discovery, audience targeting, summarization, etc. The fundamental tension this exposes for newsrooms is how to adopt AI-enabled workflow tools while reducing the risk of reproducing historical biases in news coverage.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning</title>
<link>https://arxiv.org/abs/2512.16911</link>
<guid>https://arxiv.org/abs/2512.16911</guid>
<content:encoded><![CDATA[
arXiv:2512.16911v1 Announce Type: new 
Abstract: Standard practice across domains from robotics to language is to first pretrain a policy on a large-scale demonstration dataset, and then finetune this policy, typically with reinforcement learning (RL), in order to improve performance on deployment domains. This finetuning step has proved critical in achieving human or super-human performance, yet while much attention has been given to developing more effective finetuning algorithms, little attention has been given to ensuring the pretrained policy is an effective initialization for RL finetuning. In this work we seek to understand how the pretrained policy affects finetuning performance, and how to pretrain policies in order to ensure they are effective initializations for finetuning. We first show theoretically that standard behavioral cloning (BC) -- which trains a policy to directly match the actions played by the demonstrator -- can fail to ensure coverage over the demonstrator's actions, a minimal condition necessary for effective RL finetuning. We then show that if, instead of exactly fitting the observed demonstrations, we train a policy to model the posterior distribution of the demonstrator's behavior given the demonstration dataset, we do obtain a policy that ensures coverage over the demonstrator's actions, enabling more effective finetuning. Furthermore, this policy -- which we refer to as the posterior behavioral cloning (PostBC) policy -- achieves this while ensuring pretrained performance is no worse than that of the BC policy. We then show that PostBC is practically implementable with modern generative models in robotic control domains -- relying only on standard supervised learning -- and leads to significantly improved RL finetuning performance on both realistic robotic control benchmarks and real-world robotic manipulation tasks, as compared to standard behavioral cloning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward</title>
<link>https://arxiv.org/abs/2512.16912</link>
<guid>https://arxiv.org/abs/2512.16912</guid>
<content:encoded><![CDATA[
arXiv:2512.16912v1 Announce Type: new 
Abstract: This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Random matrix theory of sparse neuronal networks with heterogeneous timescales</title>
<link>https://arxiv.org/abs/2512.12767</link>
<guid>https://arxiv.org/abs/2512.12767</guid>
<content:encoded><![CDATA[
arXiv:2512.12767v1 Announce Type: cross 
Abstract: Training recurrent neuronal networks consisting of excitatory (E) and inhibitory (I) units with additive noise for working memory computation slows and diversifies inhibitory timescales, leading to improved task performance that is attributed to emergent marginally stable equilibria [PNAS 122 (2025) e2316745122]. Yet the link between trained network characteristics and their roles in shaping desirable dynamical landscapes remains unexplored. Here, we investigate the Jacobian matrices describing the dynamics near these equilibria and show that they are sparse, non-Hermitian rectangular-block matrices modified by heterogeneous synaptic decay timescales and activation-function gains. We specify a random matrix ensemble that faithfully captures the spectra of trained Jacobian matrices, arising from the inhibitory core - excitatory periphery network motif (pruned E weights, broadly distributed I weights) observed post-training. An analytic theory of this ensemble is developed using statistical field theory methods: a Hermitized resolvent representation of the spectral density processed with a supersymmetry-based treatment in the style of Fyodorov and Mirlin. In this manner, an analytic description of the spectral edge is obtained, relating statistical parameters of the Jacobians (sparsity, weight variances, E/I ratio, and the distributions of timescales and gains) to near-critical features of the equilibria essential for robust working memory computation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TinyMyo: a Tiny Foundation Model for Flexible EMG Signal Processing at the Edge</title>
<link>https://arxiv.org/abs/2512.15729</link>
<guid>https://arxiv.org/abs/2512.15729</guid>
<content:encoded><![CDATA[
arXiv:2512.15729v1 Announce Type: cross 
Abstract: Surface electromyography (EMG) is a non-invasive sensing modality used in several domains, including biomechanics, rehabilitation, prosthetic control, and emerging human-machine interaction paradigms. Despite decades of use, significant challenges remain in achieving robust generalization across subjects, recording systems, and acquisition protocols. To tackle these challenges, foundation models (FMs) are gaining traction when targeting end-to-end applications based on EMG signals. Yet, existing EMG FMs remain limited to single downstream tasks and lack deployability on embedded platforms. In this work, we present TinyMyo, a lightweight FM based on a Transformer encoder architecture. The model is pre-trained in a self-supervised manner on publicly available datasets and achieves high reconstruction fidelity with only 3.6M parameters. With minimal task-specific head adaptations, the same backbone is used to tackle multiple downstream tasks, leveraging datasets acquired from diverse sensing locations and hardware platforms. We demonstrate generalization across hand gesture classification, hand kinematic regression, speech production and recognition, with performance comparable to or surpassing the state of the art (SoA), and model size below 5M parameters. We achieve SoA results compared to previous FM-based works on the NinaPro DB5 ($89.4\pm0.16\%$), UCI-EMG ($97.56\pm0.32\%$), and EPN-612 ($96.74\pm0.09\%$) datasets. We report, to the best of our knowledge, the first deployment of an EMG FM on an ultra-low-power microcontroller (GAP9), achieving an average power envelope of 36.45mW. By open-sourcing the pre-trained and the downstream task architectures (https://github.com/pulp-bio/BioFoundation), we aim to provide a flexible resource that can accelerate future research and serve as a common foundation for the EMG community.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Red Queen's Trap: Limits of Deep Evolution in High-Frequency Trading</title>
<link>https://arxiv.org/abs/2512.15732</link>
<guid>https://arxiv.org/abs/2512.15732</guid>
<content:encoded><![CDATA[
arXiv:2512.15732v1 Announce Type: cross 
Abstract: The integration of Deep Reinforcement Learning (DRL) and Evolutionary Computation (EC) is frequently hypothesized to be the "Holy Grail" of algorithmic trading, promising systems that adapt autonomously to non-stationary market regimes. This paper presents a rigorous post-mortem analysis of "Galaxy Empire," a hybrid framework coupling LSTM/Transformer-based perception with a genetic "Time-is-Life" survival mechanism. Deploying a population of 500 autonomous agents in a high-frequency cryptocurrency environment, we observed a catastrophic divergence between training metrics (Validation APY $>300\%$) and live performance (Capital Decay $>70\%$). We deconstruct this failure through a multi-disciplinary lens, identifying three critical failure modes: the overfitting of \textit{Aleatoric Uncertainty} in low-entropy time-series, the \textit{Survivor Bias} inherent in evolutionary selection under high variance, and the mathematical impossibility of overcoming microstructure friction without order-flow data. Our findings provide empirical evidence that increasing model complexity in the absence of information asymmetry exacerbates systemic fragility.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Anubuddhi: A Multi-Agent AI System for Designing and Simulating Quantum Optics Experiments</title>
<link>https://arxiv.org/abs/2512.15736</link>
<guid>https://arxiv.org/abs/2512.15736</guid>
<content:encoded><![CDATA[
arXiv:2512.15736v1 Announce Type: cross 
Abstract: We present Anubuddhi, a multi-agent AI system that designs and simulates quantum optics experiments from natural language prompts without requiring specialized programming knowledge. The system composes optical layouts by arranging components from a three-tier toolbox via semantic retrieval, then validates designs through physics simulation with convergent refinement. The architecture combines intent routing, knowledge-augmented generation, and dual-mode validation (QuTiP and FreeSim). We evaluated 13 experiments spanning fundamental optics (Hong-Ou-Mandel interference, Michelson/Mach-Zehnder interferometry, Bell states, delayed-choice quantum eraser), quantum information protocols (BB84 QKD, Franson interferometry, GHZ states, quantum teleportation, hyperentanglement), and advanced technologies (boson sampling, electromagnetically induced transparency, frequency conversion). The system achieves design-simulation alignment scores of 8--9/10, with simulations faithfully modeling intended physics. A critical finding distinguishes structural correctness from quantitative accuracy: high alignment confirms correct physics architecture, while numerical predictions require expert review. Free-form simulation outperformed constrained frameworks for 11/13 experiments, revealing that quantum optics diversity demands flexible mathematical representations. The system democratizes computational experiment design for research and pedagogy, producing strong initial designs users can iteratively refine through conversation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian Modeling for Uncertainty Management in Financial Risk Forecasting and Compliance</title>
<link>https://arxiv.org/abs/2512.15739</link>
<guid>https://arxiv.org/abs/2512.15739</guid>
<content:encoded><![CDATA[
arXiv:2512.15739v1 Announce Type: cross 
Abstract: A Bayesian analytics framework that precisely quantifies uncertainty offers a significant advance for financial risk management. We develop an integrated approach that consistently enhances the handling of risk in market volatility forecasting, fraud detection, and compliance monitoring. Our probabilistic, interpretable models deliver reliable results: We evaluate the performance of one-day-ahead 95% Value-at-Risk (VaR) forecasts on daily S&amp;P 500 returns, with a training period from 2000 to 2019 and an out-of-sample test period spanning 2020 to 2024. Formal tests of unconditional (Kupiec) and conditional (Christoffersen) coverage reveal that an LSTM baseline achieves near-nominal calibration. In contrast, a GARCH(1,1) model with Student-t innovations underestimates tail risk. Our proposed discount-factor DLM model produces a slightly liberal VaR estimate, with evidence of clustered violations. Bayesian logistic regression improves recall and AUC-ROC for fraud detection, and a hierarchical Beta state-space model provides transparent and adaptive compliance risk assessment. The pipeline is distinguished by precise uncertainty quantification, interpretability, and GPU-accelerated analysis, delivering up to 50x speedup. Remaining challenges include sparse fraud data and proxy compliance labels, but the framework enables actionable risk insights. Future expansion will extend feature sets, explore regime-switching priors, and enhance scalable inference.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PHANTOM: Progressive High-fidelity Adversarial Network for Threat Object Modeling</title>
<link>https://arxiv.org/abs/2512.15768</link>
<guid>https://arxiv.org/abs/2512.15768</guid>
<content:encoded><![CDATA[
arXiv:2512.15768v1 Announce Type: cross 
Abstract: The scarcity of cyberattack data hinders the development of robust intrusion detection systems. This paper introduces PHANTOM, a novel adversarial variational framework for generating high-fidelity synthetic attack data. Its innovations include progressive training, a dual-path VAE-GAN architecture, and domain-specific feature matching to preserve the semantics of attacks. Evaluated on 100,000 network traffic samples, models trained on PHANTOM data achieve 98% weighted accuracy on real attacks. Statistical analyses confirm that the synthetic data preserves authentic distributions and diversity. Limitations in generating rare attack types are noted, highlighting challenges with severe class imbalance. This work advances the generation of synthetic data for training robust, privacy-preserving detection systems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-Chain Backdoor: Do You Trust Diffusion Models as Generative Data Supplier?</title>
<link>https://arxiv.org/abs/2512.15769</link>
<guid>https://arxiv.org/abs/2512.15769</guid>
<content:encoded><![CDATA[
arXiv:2512.15769v1 Announce Type: cross 
Abstract: The increasing use of generative models such as diffusion models for synthetic data augmentation has greatly reduced the cost of data collection and labeling in downstream perception tasks. However, this new data source paradigm may introduce important security concerns. This work investigates backdoor propagation in such emerging generative data supply chains, namely Data-Chain Backdoor (DCB). Specifically, we find that open-source diffusion models can become hidden carriers of backdoors. Their strong distribution-fitting ability causes them to memorize and reproduce backdoor triggers during generation, which are subsequently inherited by downstream models, resulting in severe security risks. This threat is particularly concerning under clean-label attack scenarios, as it remains effective while having negligible impact on the utility of the synthetic data. Furthermore, we discover an Early-Stage Trigger Manifestation (ESTM) phenomenon: backdoor trigger patterns tend to surface more explicitly in the early, high-noise stages of the diffusion model's reverse generation process before being subtly integrated into the final samples. Overall, this work reveals a previously underexplored threat in generative data pipelines and provides initial insights toward mitigating backdoor risks in synthetic data generation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Two-Step Data Augmentation for Masked Face Detection and Recognition: Turning Fake Masks to Real</title>
<link>https://arxiv.org/abs/2512.15774</link>
<guid>https://arxiv.org/abs/2512.15774</guid>
<content:encoded><![CDATA[
arXiv:2512.15774v1 Announce Type: cross 
Abstract: Data scarcity and distribution shift pose major challenges for masked face detection and recognition. We propose a two-step generative data augmentation framework that combines rule-based mask warping with unpaired image-to-image translation using GANs, enabling the generation of realistic masked-face samples beyond purely synthetic transformations. Compared to rule-based warping alone, the proposed approach yields consistent qualitative improvements and complements existing GAN-based masked face generation methods such as IAMGAN. We introduce a non-mask preservation loss and stochastic noise injection to stabilize training and enhance sample diversity. Experimental observations highlight the effectiveness of the proposed components and suggest directions for future improvements in data-centric augmentation for face recognition tasks.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhanced Web User Interface Design Via Cross-Device Responsiveness Assessment Using An Improved HCI-INTEGRATED DL Schemes</title>
<link>https://arxiv.org/abs/2512.15775</link>
<guid>https://arxiv.org/abs/2512.15775</guid>
<content:encoded><![CDATA[
arXiv:2512.15775v1 Announce Type: cross 
Abstract: User Interface (UI) optimization is essential in the digital era to enhance user satisfaction in web environments. Nevertheless, the existing UI optimization models had overlooked the Cross-Responsiveness (CR) assessment, affecting the user interaction efficiency. Consequently, this article proposes a dynamic web UI optimization through CR assessment using Finite Exponential Continuous State Machine (FECSM) and Quokka Nonlinear Difference Swarm Optimization Algorithm (QNDSOA). Initially, the design and user interaction related information is collected as well as pre-processed for min-max normalization. Next, the Human-Computer Interaction (HCI)-based features are extracted, followed by user behaviour pattern grouping. Meanwhile, the CR assessment is done using FECSM. Then, the proposed Bidirectional Gated Luong and Mish Recurrent Unit (BiGLMRU) is used to classify the User eXperience (UX) change type, which is labelled based on the User Interface Change Prediction Index (UICPI). Lastly, a novel QNDSOA is utilized to optimize the UI design with an average fitness of 98.5632%. Feedback monitoring is done after optimal deployment.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAMBO: Reliability Analysis for Mamba through Bit-flip attack Optimization</title>
<link>https://arxiv.org/abs/2512.15778</link>
<guid>https://arxiv.org/abs/2512.15778</guid>
<content:encoded><![CDATA[
arXiv:2512.15778v1 Announce Type: cross 
Abstract: State-space models (SSMs), exemplified by the Mamba architecture, have recently emerged as state-of-the-art sequence-modeling frameworks, offering linear-time scalability together with strong performance in long-context settings. Owing to their unique combination of efficiency, scalability, and expressive capacity, SSMs have become compelling alternatives to transformer-based models, which suffer from the quadratic computational and memory costs of attention mechanisms. As SSMs are increasingly deployed in real-world applications, it is critical to assess their susceptibility to both software- and hardware-level threats to ensure secure and reliable operation. Among such threats, hardware-induced bit-flip attacks (BFAs) pose a particularly severe risk by corrupting model parameters through memory faults, thereby undermining model accuracy and functional integrity. To investigate this vulnerability, we introduce RAMBO, the first BFA framework specifically designed to target Mamba-based architectures. Through experiments on the Mamba-1.4b model with LAMBADA benchmark, a cloze-style word-prediction task, we demonstrate that flipping merely a single critical bit can catastrophically reduce accuracy from 74.64% to 0% and increase perplexity from 18.94 to 3.75 x 10^6. These results demonstrate the pronounced fragility of SSMs to adversarial perturbations.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hyperparameter Tuning-Based Optimized Performance Analysis of Machine Learning Algorithms for Network Intrusion Detection</title>
<link>https://arxiv.org/abs/2512.15779</link>
<guid>https://arxiv.org/abs/2512.15779</guid>
<content:encoded><![CDATA[
arXiv:2512.15779v1 Announce Type: cross 
Abstract: Network Intrusion Detection Systems (NIDS) are essential for securing networks by identifying and mitigating unauthorized activities indicative of cyberattacks. As cyber threats grow increasingly sophisticated, NIDS must evolve to detect both emerging threats and deviations from normal behavior. This study explores the application of machine learning (ML) methods to improve the NIDS accuracy through analyzing intricate structures in deep-featured network traffic records. Leveraging the 1999 KDD CUP intrusion dataset as a benchmark, this research evaluates and optimizes several ML algorithms, including Support Vector Machines (SVM), Na\"ive Bayes variants (MNB, BNB), Random Forest (RF), k-Nearest Neighbors (k-NN), Decision Trees (DT), AdaBoost, XGBoost, Logistic Regression (LR), Ridge Classifier, Passive-Aggressive (PA) Classifier, Rocchio Classifier, Artificial Neural Networks (ANN), and Perceptron (PPN). Initial evaluations without hyper-parameter optimization demonstrated suboptimal performance, highlighting the importance of tuning to enhance classification accuracy. After hyper-parameter optimization using grid and random search techniques, the SVM classifier achieved 99.12% accuracy with a 0.0091 False Alarm Rate (FAR), outperforming its default configuration (98.08% accuracy, 0.0123 FAR) and all other classifiers. This result confirms that SVM accomplishes the highest accuracy among the evaluated classifiers. We validated the effectiveness of all classifiers using a tenfold cross-validation approach, incorporating Recursive Feature Elimination (RFE) for feature selection to enhance the classifiers accuracy and efficiency. Our outcomes indicate that ML classifiers are both adaptable and reliable, contributing to enhanced accuracy in systems for detecting network intrusions.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Auto-Tuning Safety Guardrails for Black-Box Large Language Models</title>
<link>https://arxiv.org/abs/2512.15782</link>
<guid>https://arxiv.org/abs/2512.15782</guid>
<content:encoded><![CDATA[
arXiv:2512.15782v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly deployed behind safety guardrails such as system prompts and content filters, especially in settings where product teams cannot modify model weights. In practice these guardrails are typically hand-tuned, brittle, and difficult to reproduce. This paper studies a simple but practical alternative: treat safety guardrail design itself as a hyperparameter optimization problem over a frozen base model. Concretely, I wrap Mistral-7B-Instruct with modular jailbreak and malware system prompts plus a ModernBERT-based harmfulness classifier, then evaluate candidate configurations on three public benchmarks covering malware generation, classic jailbreak prompts, and benign user queries. Each configuration is scored using malware and jailbreak attack success rate, benign harmful-response rate, and end-to-end latency. A 48-point grid search over prompt combinations and filter modes establishes a baseline. I then run a black-box Optuna study over the same space and show that it reliably rediscovers the best grid configurations while requiring an order of magnitude fewer evaluations and roughly 8x less wall-clock time. The results suggest that viewing safety guardrails as tunable hyperparameters is a feasible way to harden black-box LLM deployments under compute and time constraints.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI Epidemiology: achieving explainable AI through expert oversight patterns</title>
<link>https://arxiv.org/abs/2512.15783</link>
<guid>https://arxiv.org/abs/2512.15783</guid>
<content:encoded><![CDATA[
arXiv:2512.15783v1 Announce Type: cross 
Abstract: AI Epidemiology is a framework for governing and explaining advanced AI systems by applying population-level surveillance methods to AI outputs. The approach mirrors the way in which epidemiologists enable public health interventions through statistical evidence before molecular mechanisms are understood. This bypasses the problem of model complexity which plagues current interpretability methods (such as SHAP and mechanistic interpretability) at the scale of deployed models.
  AI Epidemiology achieves this population-level surveillance by standardising capture of AI-expert interactions into structured assessment fields: risk level, alignment score, and accuracy score. These function as exposure variables which predict output failure through statistical associations, much like cholesterol and blood pressure act as exposure variables predicting cardiac events. Output-failure associations are subsequently validated against expert overrides and real-world outcomes.
  The framework places zero burden on experts and provides automatic audit trails by passively tracking expert convergence and divergence with AI recommendations. Since it analyses outputs rather than internal model computations, it also provides governance continuity when institutions update models and switch vendors. Finally, by providing reliability scores and semantic assessments (e.g. 'this recommendation resembles 500 cases overridden by experts due to guideline violations'), it enables experts and institutions to detect unreliable AI outputs before they cause harm. This democratises AI oversight by enabling domain experts to govern AI systems without requiring machine learning expertise.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Training: Enabling Self-Evolution of Agents with MOBIMEM</title>
<link>https://arxiv.org/abs/2512.15784</link>
<guid>https://arxiv.org/abs/2512.15784</guid>
<content:encoded><![CDATA[
arXiv:2512.15784v1 Announce Type: cross 
Abstract: Large Language Model (LLM) agents are increasingly deployed to automate complex workflows in mobile and desktop environments. However, current model-centric agent architectures struggle to self-evolve post-deployment: improving personalization, capability, and efficiency typically requires continuous model retraining/fine-tuning, which incurs prohibitive computational overheads and suffers from an inherent trade-off between model accuracy and inference efficiency.
  To enable iterative self-evolution without model retraining, we propose MOBIMEM, a memory-centric agent system. MOBIMEM first introduces three specialized memory primitives to decouple agent evolution from model weights: (1) Profile Memory uses a lightweight distance-graph (DisGraph) structure to align with user preferences, resolving the accuracy-latency trade-off in user profile retrieval; (2) Experience Memory employs multi-level templates to instantiate execution logic for new tasks, ensuring capability generalization; and (3) Action Memory records fine-grained interaction sequences, reducing the reliance on expensive model inference. Building upon this memory architecture, MOBIMEM further integrates a suite of OS-inspired services to orchestrate execution: a scheduler that coordinates parallel sub-task execution and memory operations; an agent record-and-replay (AgentRR) mechanism that enables safe and efficient action reuse; and a context-aware exception handling that ensures graceful recovery from user interruptions and runtime errors.
  Evaluation on AndroidWorld and top-50 apps shows that MOBIMEM achieves 83.1% profile alignment with 23.83 ms retrieval time (280x faster than GraphRAG baselines), improves task success rates by up to 50.3%, and reduces end-to-end latency by up to 9x on mobile devices.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Consensus dimension reduction via multi-view learning</title>
<link>https://arxiv.org/abs/2512.15802</link>
<guid>https://arxiv.org/abs/2512.15802</guid>
<content:encoded><![CDATA[
arXiv:2512.15802v1 Announce Type: cross 
Abstract: A plethora of dimension reduction methods have been developed to visualize high-dimensional data in low dimensions. However, different dimension reduction methods often output different and possibly conflicting visualizations of the same data. This problem is further exacerbated by the choice of hyperparameters, which may substantially impact the resulting visualization. To obtain a more robust and trustworthy dimension reduction output, we advocate for a consensus approach, which summarizes multiple visualizations into a single consensus dimension reduction visualization. Here, we leverage ideas from multi-view learning in order to identify the patterns that are most stable or shared across the many different dimension reduction visualizations, or views, and subsequently visualize this shared structure in a single low-dimensional plot. We demonstrate that this consensus visualization effectively identifies and preserves the shared low-dimensional data structure through both simulated and real-world case studies. We further highlight our method's robustness to the choice of dimension reduction method and hyperparameters -- a highly-desirable property when working towards trustworthy and reproducible data science.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An empirical analysis of zero-day vulnerabilities disclosed by the zero day initiative</title>
<link>https://arxiv.org/abs/2512.15803</link>
<guid>https://arxiv.org/abs/2512.15803</guid>
<content:encoded><![CDATA[
arXiv:2512.15803v1 Announce Type: cross 
Abstract: Zero-day vulnerabilities represent some of the most critical threats in cybersecurity, as they correspond to previously unknown flaws in software or hardware that are actively exploited before vendors can develop and deploy patches. During this exposure window, affected systems remain defenseless, making zero-day attacks particularly damaging and difficult to mitigate. This study analyzes the Zero Day Initiative (ZDI) vulnerability disclosures reported between January and April 2024, Cole [2025] comprising a total of 415 vulnerabilities. The dataset includes vulnerability identifiers, Common Vulnerability Scoring System (CVSS) v3.0 scores, publication dates, and short textual descriptions. The primary objectives of this work are to identify trends in zero-day vulnerability disclosures, examine severity distributions across vendors, and investigate which vulnerability characteristics are most indicative of high severity. In addition, this study explores predictive modeling approaches for severity classification, comparing classical machine learning techniques with deep learning models using both structured metadata and unstructured textual descriptions. The findings aim to support improved patch prioritization strategies, more effective vulnerability management, and enhanced organizational preparedness against emerging zero-day threats.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Foundation Models in Biomedical Imaging: Turning Hype into Reality</title>
<link>https://arxiv.org/abs/2512.15808</link>
<guid>https://arxiv.org/abs/2512.15808</guid>
<content:encoded><![CDATA[
arXiv:2512.15808v1 Announce Type: cross 
Abstract: Foundation models (FMs) are driving a prominent shift in artificial intelligence across different domains, including biomedical imaging. These models are designed to move beyond narrow pattern recognition towards emulating sophisticated clinical reasoning, understanding complex spatial relationships, and integrating multimodal data with unprecedented flexibility. However, a critical gap exists between this potential and the current reality, where the clinical evaluation and deployment of FMs are hampered by significant challenges. Herein, we critically assess the current state-of-the-art, analyzing hype by examining the core capabilities and limitations of FMs in the biomedical domain. We also provide a taxonomy of reasoning, ranging from emulated sequential logic and spatial understanding to the integration of explicit symbolic knowledge, to evaluate whether these models exhibit genuine cognition or merely mimic surface-level patterns. We argue that a critical frontier lies beyond statistical correlation, in the pursuit of causal inference, which is essential for building robust models that understand cause and effect. Furthermore, we discuss the paramount issues in deployment stemming from trustworthiness, bias, and safety, dissecting the challenges of algorithmic bias, data bias and privacy, and model hallucinations. We also draw attention to the need for more inclusive, rigorous, and clinically relevant validation frameworks to ensure their safe and ethical application. We conclude that while the vision of autonomous AI-doctors remains distant, the immediate reality is the emergence of powerful technology and assistive tools that would benefit clinical practice. The future of FMs in biomedical imaging hinges not on scale alone, but on developing hybrid, causally aware, and verifiably safe systems that augment, rather than replace, human expertise.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Secure AI-Driven Super-Resolution for Real-Time Mixed Reality Applications</title>
<link>https://arxiv.org/abs/2512.15823</link>
<guid>https://arxiv.org/abs/2512.15823</guid>
<content:encoded><![CDATA[
arXiv:2512.15823v1 Announce Type: cross 
Abstract: Immersive formats such as 360{\deg} and 6DoF point cloud videos require high bandwidth and low latency, posing challenges for real-time AR/VR streaming. This work focuses on reducing bandwidth consumption and encryption/decryption delay, two key contributors to overall latency. We design a system that downsamples point cloud content at the origin server and applies partial encryption. At the client, the content is decrypted and upscaled using an ML-based super-resolution model. Our evaluation demonstrates a nearly linear reduction in bandwidth/latency, and encryption/decryption overhead with lower downsampling resolutions, while the super-resolution model effectively reconstructs the original full-resolution point clouds with minimal error and modest inference time.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Social Story Frames: Contextual Reasoning about Narrative Intent and Reception</title>
<link>https://arxiv.org/abs/2512.15925</link>
<guid>https://arxiv.org/abs/2512.15925</guid>
<content:encoded><![CDATA[
arXiv:2512.15925v1 Announce Type: cross 
Abstract: Reading stories evokes rich interpretive, affective, and evaluative responses, such as inferences about narrative intent or judgments about characters. Yet, computational models of reader response are limited, preventing nuanced analyses. To address this gap, we introduce SocialStoryFrames, a formalism for distilling plausible inferences about reader response, such as perceived author intent, explanatory and predictive reasoning, affective responses, and value judgments, using conversational context and a taxonomy grounded in narrative theory, linguistic pragmatics, and psychology. We develop two models, SSF-Generator and SSF-Classifier, validated through human surveys (N=382 participants) and expert annotations, respectively. We conduct pilot analyses to showcase the utility of the formalism for studying storytelling at scale. Specifically, applying our models to SSF-Corpus, a curated dataset of 6,140 social media stories from diverse contexts, we characterize the frequency and interdependence of storytelling intents, and we compare and contrast narrative practices (and their diversity) across communities. By linking fine-grained, context-sensitive modeling with a generic taxonomy of reader responses, SocialStoryFrames enable new research into storytelling in online communities.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Neural Surfaces for 3D Mesh Compression</title>
<link>https://arxiv.org/abs/2512.15985</link>
<guid>https://arxiv.org/abs/2512.15985</guid>
<content:encoded><![CDATA[
arXiv:2512.15985v1 Announce Type: cross 
Abstract: Implicit Neural Representations (INRs) have been demonstrated to achieve state-of-the-art compression of a broad range of modalities such as images, videos, 3D surfaces, and audio. Most studies have focused on building neural counterparts of traditional implicit representations of 3D geometries, such as signed distance functions. However, the triangle mesh-based representation of geometry remains the most widely used representation in the industry, while building INRs capable of generating them has been sparsely studied. In this paper, we present a method for building compact INRs of zero-genus 3D manifolds. Our method relies on creating a spherical parameterization of a given 3D mesh - mapping the surface of a mesh to that of a unit sphere - then constructing an INR that encodes the displacement vector field defined continuously on its surface that regenerates the original shape. The compactness of our representation can be attributed to its hierarchical structure, wherein it first recovers the coarse structure of the encoded surface before adding high-frequency details to it. Once the INR is computed, 3D meshes of arbitrary resolution/connectivity can be decoded from it. The decoding can be performed in real time while achieving a state-of-the-art trade-off between reconstruction quality and the size of the compressed representations.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Time-Frequency Analysis for Neural Networks</title>
<link>https://arxiv.org/abs/2512.15992</link>
<guid>https://arxiv.org/abs/2512.15992</guid>
<content:encoded><![CDATA[
arXiv:2512.15992v1 Announce Type: cross 
Abstract: We develop a quantitative approximation theory for shallow neural networks using tools from time-frequency analysis. Working in weighted modulation spaces $M^{p,q}_m(\mathbf{R}^{d})$, we prove dimension-independent approximation rates in Sobolev norms $W^{n,r}(\Omega)$ for networks whose units combine standard activations with localized time-frequency windows. Our main result shows that for $f \in M^{p,q}_m(\mathbf{R}^{d})$ one can achieve \[ \|f - f_N\|_{W^{n,r}(\Omega)} \lesssim N^{-1/2}\,\|f\|_{M^{p,q}_m(\mathbf{R}^{d})}, \] on bounded domains, with explicit control of all constants. We further obtain global approximation theorems on $\mathbf{R}^{d}$ using weighted modulation dictionaries, and derive consequences for Feichtinger's algebra, Fourier-Lebesgue spaces, and Barron spaces. Numerical experiments in one and two dimensions confirm that modulation-based networks achieve substantially better Sobolev approximation than standard ReLU networks, consistent with the theoretical estimates.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Information theory and discriminative sampling for model discovery</title>
<link>https://arxiv.org/abs/2512.16000</link>
<guid>https://arxiv.org/abs/2512.16000</guid>
<content:encoded><![CDATA[
arXiv:2512.16000v1 Announce Type: cross 
Abstract: Fisher information and Shannon entropy are fundamental tools for understanding and analyzing dynamical systems from complementary perspectives. They can characterize unknown parameters by quantifying the information contained in variables, or measure how different initial trajectories or temporal segments of a trajectory contribute to learning or inferring system dynamics. In this work, we leverage the Fisher Information Matrix (FIM) within the data-driven framework of {\em sparse identification of nonlinear dynamics} (SINDy). We visualize information patterns in chaotic and non-chaotic systems for both single trajectories and multiple initial conditions, demonstrating how information-based analysis can improve sampling efficiency and enhance model performance by prioritizing more informative data. The benefits of statistical bagging are further elucidated through spectral analysis of the FIM. We also illustrate how Fisher information and entropy metrics can promote data efficiency in three scenarios: when only a single trajectory is available, when a tunable control parameter exists, and when multiple trajectories can be freely initialized. As data-driven model discovery continues to gain prominence, principled sampling strategies guided by quantifiable information metrics offer a powerful approach for improving learning efficiency and reducing data requirements.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Concurrence: A dependence criterion for time series, applied to biological data</title>
<link>https://arxiv.org/abs/2512.16001</link>
<guid>https://arxiv.org/abs/2512.16001</guid>
<content:encoded><![CDATA[
arXiv:2512.16001v1 Announce Type: cross 
Abstract: Measuring the statistical dependence between observed signals is a primary tool for scientific discovery. However, biological systems often exhibit complex non-linear interactions that currently cannot be captured without a priori knowledge or large datasets. We introduce a criterion for dependence, whereby two time series are deemed dependent if one can construct a classifier that distinguishes between temporally aligned vs. misaligned segments extracted from them. We show that this criterion, concurrence, is theoretically linked with dependence, and can become a standard approach for scientific analyses across disciplines, as it can expose relationships across a wide spectrum of signals (fMRI, physiological and behavioral data) without ad-hoc parameter tuning or large amounts of data.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Neural Networks for Interferometer Simulations</title>
<link>https://arxiv.org/abs/2512.16051</link>
<guid>https://arxiv.org/abs/2512.16051</guid>
<content:encoded><![CDATA[
arXiv:2512.16051v1 Announce Type: cross 
Abstract: In recent years, graph neural networks (GNNs) have shown tremendous promise in solving problems in high energy physics, materials science, and fluid dynamics. In this work, we introduce a new application for GNNs in the physical sciences: instrumentation design. As a case study, we apply GNNs to simulate models of the Laser Interferometer Gravitational-Wave Observatory (LIGO) and show that they are capable of accurately capturing the complex optical physics at play, while achieving runtimes 815 times faster than state of the art simulation packages. We discuss the unique challenges this problem provides for machine learning models. In addition, we provide a dataset of high-fidelity optical physics simulations for three interferometer topologies, which can be used as a benchmarking suite for future work in this direction.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FOD-Diff: 3D Multi-Channel Patch Diffusion Model for Fiber Orientation Distribution</title>
<link>https://arxiv.org/abs/2512.16075</link>
<guid>https://arxiv.org/abs/2512.16075</guid>
<content:encoded><![CDATA[
arXiv:2512.16075v1 Announce Type: cross 
Abstract: Diffusion MRI (dMRI) is a critical non-invasive technique to estimate fiber orientation distribution (FOD) for characterizing white matter integrity. Estimating FOD from single-shell low angular resolution dMRI (LAR-FOD) is limited by accuracy, whereas estimating FOD from multi-shell high angular resolution dMRI (HAR-FOD) requires a long scanning time, which limits its applicability. Diffusion models have shown promise in estimating HAR-FOD based on LAR-FOD. However, using diffusion models to efficiently generate HAR-FOD is challenging due to the large number of spherical harmonic (SH) coefficients in FOD. Here, we propose a 3D multi-channel patch diffusion model to predict HAR-FOD from LAR-FOD. We design the FOD-patch adapter by introducing the prior brain anatomy for more efficient patch-based learning. Furthermore, we introduce a voxel-level conditional coordinating module to enhance the global understanding of the model. We design the SH attention module to effectively learn the complex correlations of the SH coefficients. Our experimental results show that our method achieves the best performance in HAR-FOD prediction and outperforms other state-of-the-art methods.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Text2SQL via LLM-efficient Schema Filtering with Functional Dependency Graph Rerankers</title>
<link>https://arxiv.org/abs/2512.16083</link>
<guid>https://arxiv.org/abs/2512.16083</guid>
<content:encoded><![CDATA[
arXiv:2512.16083v1 Announce Type: cross 
Abstract: Most modern Text2SQL systems prompt large language models (LLMs) with entire schemas -- mostly column information -- alongside the user's question. While effective on small databases, this approach fails on real-world schemas that exceed LLM context limits, even for commercial models. The recent Spider 2.0 benchmark exemplifies this with hundreds of tables and tens of thousands of columns, where existing systems often break. Current mitigations either rely on costly multi-step prompting pipelines or filter columns by ranking them against user's question independently, ignoring inter-column structure. To scale existing systems, we introduce \toolname, an open-source, LLM-efficient schema filtering framework that compacts Text2SQL prompts by (i) ranking columns with a query-aware LLM encoder enriched with values and metadata, (ii) reranking inter-connected columns via a lightweight graph transformer over functional dependencies, and (iii) selecting a connectivity-preserving sub-schema with a Steiner-tree heuristic. Experiments on real datasets show that \toolname achieves near-perfect recall and higher precision than CodeS, SchemaExP, Qwen rerankers, and embedding retrievers, while maintaining sub-second median latency and scaling to schemas with 23,000+ columns. Our source code is available at https://github.com/thanhdath/grast-sql.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times</title>
<link>https://arxiv.org/abs/2512.16093</link>
<guid>https://arxiv.org/abs/2512.16093</guid>
<content:encoded><![CDATA[
arXiv:2512.16093v1 Announce Type: cross 
Abstract: We introduce TurboDiffusion, a video generation acceleration framework that can speed up end-to-end diffusion generation by 100-200x while maintaining video quality. TurboDiffusion mainly relies on several components for acceleration: (1) Attention acceleration: TurboDiffusion uses low-bit SageAttention and trainable Sparse-Linear Attention (SLA) to speed up attention computation. (2) Step distillation: TurboDiffusion adopts rCM for efficient step distillation. (3) W8A8 quantization: TurboDiffusion quantizes model parameters and activations to 8 bits to accelerate linear layers and compress the model. In addition, TurboDiffusion incorporates several other engineering optimizations.
  We conduct experiments on the Wan2.2-I2V-14B-720P, Wan2.1-T2V-1.3B-480P, Wan2.1-T2V-14B-720P, and Wan2.1-T2V-14B-480P models. Experimental results show that TurboDiffusion achieves 100-200x speedup for video generation even on a single RTX 5090 GPU, while maintaining comparable video quality. The GitHub repository, which includes model checkpoints and easy-to-use code, is available at https://github.com/thu-ml/TurboDiffusion.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BayesSum: Bayesian Quadrature in Discrete Spaces</title>
<link>https://arxiv.org/abs/2512.16105</link>
<guid>https://arxiv.org/abs/2512.16105</guid>
<content:encoded><![CDATA[
arXiv:2512.16105v1 Announce Type: cross 
Abstract: This paper addresses the challenging computational problem of estimating intractable expectations over discrete domains. Existing approaches, including Monte Carlo and Russian Roulette estimators, are consistent but often require a large number of samples to achieve accurate results. We propose a novel estimator, \emph{BayesSum}, which is an extension of Bayesian quadrature to discrete domains. It is more sample efficient than alternatives due to its ability to make use of prior information about the integrand through a Gaussian process. We show this through theory, deriving a convergence rate significantly faster than Monte Carlo in a broad range of settings. We also demonstrate empirically that our proposed method does indeed require fewer samples on several synthetic settings as well as for parameter estimation for Conway-Maxwell-Poisson and Potts models.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Staggered Batch Scheduling: Co-optimizing Time-to-First-Token and Throughput for High-Efficiency LLM Inference</title>
<link>https://arxiv.org/abs/2512.16134</link>
<guid>https://arxiv.org/abs/2512.16134</guid>
<content:encoded><![CDATA[
arXiv:2512.16134v1 Announce Type: cross 
Abstract: The evolution of Large Language Model (LLM) serving towards complex, distributed architectures--specifically the P/D-separated, large-scale DP+EP paradigm--introduces distinct scheduling challenges. Unlike traditional deployments where schedulers can treat instances as black boxes, DP+EP architectures exhibit high internal synchronization costs. We identify that immediate request dispatching in such systems leads to severe in-engine queuing and parallelization bubbles, degrading Time-to-First-Token (TTFT). To address this, we propose Staggered Batch Scheduling (SBS), a mechanism that deliberately buffers requests to form optimal execution batches. This temporal decoupling eliminates internal queuing bubbles without compromising throughput. Furthermore, leveraging the scheduling window created by buffering, we introduce a Load-Aware Global Allocation strategy that balances computational load across DP units for both Prefill and Decode phases. Deployed on a production H800 cluster serving Deepseek-V3, our system reduces TTFT by 30%-40% and improves throughput by 15%-20% compared to state-of-the-art immediate scheduling baselines.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Artificial Intelligence-Enabled Holistic Design of Catalysts Tailored for Semiconducting Carbon Nanotube Growth</title>
<link>https://arxiv.org/abs/2512.16151</link>
<guid>https://arxiv.org/abs/2512.16151</guid>
<content:encoded><![CDATA[
arXiv:2512.16151v1 Announce Type: cross 
Abstract: Catalyst design is crucial for materials synthesis, especially for complex reaction networks. Strategies like collaborative catalytic systems and multifunctional catalysts are effective but face challenges at the nanoscale. Carbon nanotube synthesis contains complicated nanoscale catalytic reactions, thus achieving high-density, high-quality semiconducting CNTs demands innovative catalyst design. In this work, we present a holistic framework integrating machine learning into traditional catalyst design for semiconducting CNT synthesis. It combines knowledge-based insights with data-driven techniques. Three key components, including open-access electronic structure databases for precise physicochemical descriptors, pre-trained natural language processing-based embedding model for higher-level abstractions, and physical - driven predictive models based on experiment data, are utilized. Through this framework, a new method for selective semiconducting CNT synthesis via catalyst - mediated electron injection, tuned by light during growth, is proposed. 54 candidate catalysts are screened, and three with high potential are identified. High-throughput experiments validate the predictions, with semiconducting selectivity exceeding 91% and the FeTiO3 catalyst reaching 98.6%. This approach not only addresses semiconducting CNT synthesis but also offers a generalizable methodology for global catalyst design and nanomaterials synthesis, advancing materials science in precise control.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Science Consultant Agent</title>
<link>https://arxiv.org/abs/2512.16171</link>
<guid>https://arxiv.org/abs/2512.16171</guid>
<content:encoded><![CDATA[
arXiv:2512.16171v1 Announce Type: cross 
Abstract: The Science Consultant Agent is a web-based Artificial Intelligence (AI) tool that helps practitioners select and implement the most effective modeling strategy for AI-based solutions. It operates through four core components: Questionnaire, Smart Fill, Research-Guided Recommendation, and Prototype Builder. By combining structured questionnaires, literature-backed solution recommendations, and prototype generation, the Science Consultant Agent accelerates development for everyone from Product Managers and Software Developers to Researchers. The full pipeline is illustrated in Figure 1.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Neural Networks for Modeling the Martian Induced Magnetosphere</title>
<link>https://arxiv.org/abs/2512.16175</link>
<guid>https://arxiv.org/abs/2512.16175</guid>
<content:encoded><![CDATA[
arXiv:2512.16175v1 Announce Type: cross 
Abstract: Understanding the magnetic field environment around Mars and its response to upstream solar wind conditions provide key insights into the processes driving atmospheric ion escape. To date, global models of Martian induced magnetosphere have been exclusively physics-based, relying on computationally intensive simulations. For the first time, we develop a data-driven model of the Martian induced magnetospheric magnetic field using Physics-Informed Neural Network (PINN) combined with MAVEN observations and physical laws. Trained under varying solar wind conditions, including B_IMF, P_SW, and {\theta}_cone, the data-driven model accurately reconstructs the three-dimensional magnetic field configuration and its variability in response to upstream solar wind drivers. Based on the PINN results, we identify key dependencies of magnetic field configuration on solar wind parameters, including the hemispheric asymmetries of the draped field line strength in the Mars-Solar-Electric coordinates. These findings demonstrate the capability of PINNs to reconstruct complex magnetic field structures in the Martian induced magnetosphere, thereby offering a promising tool for advancing studies of solar wind-Mars interactions.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DAG Learning from Zero-Inflated Count Data Using Continuous Optimization</title>
<link>https://arxiv.org/abs/2512.16233</link>
<guid>https://arxiv.org/abs/2512.16233</guid>
<content:encoded><![CDATA[
arXiv:2512.16233v1 Announce Type: cross 
Abstract: We address network structure learning from zero-inflated count data by casting each node as a zero-inflated generalized linear model and optimizing a smooth, score-based objective under a directed acyclic graph constraint. Our Zero-Inflated Continuous Optimization (ZICO) approach uses node-wise likelihoods with canonical links and enforces acyclicity through a differentiable surrogate constraint combined with sparsity regularization. ZICO achieves superior performance with faster runtimes on simulated data. It also performs comparably to or better than common algorithms for reverse engineering gene regulatory networks. ZICO is fully vectorized and mini-batched, enabling learning on larger variable sets with practical runtimes in a wide range of domains.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable Deep Learning for Stock Returns: A Consensus-Bottleneck Asset Pricing Model</title>
<link>https://arxiv.org/abs/2512.16251</link>
<guid>https://arxiv.org/abs/2512.16251</guid>
<content:encoded><![CDATA[
arXiv:2512.16251v1 Announce Type: cross 
Abstract: We introduce the \textit{Consensus-Bottleneck Asset Pricing Model} (CB-APM), a partially interpretable neural network that replicates the reasoning processes of sell-side analysts by capturing how dispersed investor beliefs are compressed into asset prices through a consensus formation process. By modeling this ``bottleneck'' to summarize firm- and macro-level information, CB-APM not only predicts future risk premiums of U.S. equities but also links belief aggregation to expected returns in a structurally interpretable manner. The model improves long-horizon return forecasts and outperforms standard deep learning approaches in both predictive accuracy and explanatory power. Comprehensive portfolio analyses show that CB-APM's out-of-sample predictions translate into economically meaningful payoffs, with monotonic return differentials and stable long-short performance across regularization settings. Empirically, CB-APM leverages consensus as a regularizer to amplify long-horizon predictability and yields interpretable consensus-based components that clarify how information is priced in returns. Moreover, regression and GRS-based pricing diagnostics reveal that the learned consensus representations capture priced variation only partially spanned by traditional factor models, demonstrating that CB-APM uncovers belief-driven structure in expected returns beyond the canonical factor space. Overall, CB-APM provides an interpretable and empirically grounded framework for understanding belief-driven return dynamics.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pixel Super-Resolved Fluorescence Lifetime Imaging Using Deep Learning</title>
<link>https://arxiv.org/abs/2512.16266</link>
<guid>https://arxiv.org/abs/2512.16266</guid>
<content:encoded><![CDATA[
arXiv:2512.16266v1 Announce Type: cross 
Abstract: Fluorescence lifetime imaging microscopy (FLIM) is a powerful quantitative technique that provides metabolic and molecular contrast, offering strong translational potential for label-free, real-time diagnostics. However, its clinical adoption remains limited by long pixel dwell times and low signal-to-noise ratio (SNR), which impose a stricter resolution-speed trade-off than conventional optical imaging approaches. Here, we introduce FLIM_PSR_k, a deep learning-based multi-channel pixel super-resolution (PSR) framework that reconstructs high-resolution FLIM images from data acquired with up to a 5-fold increased pixel size. The model is trained using the conditional generative adversarial network (cGAN) framework, which, compared to diffusion model-based alternatives, delivers a more robust PSR reconstruction with substantially shorter inference times, a crucial advantage for practical deployment. FLIM_PSR_k not only enables faster image acquisition but can also alleviate SNR limitations in autofluorescence-based FLIM. Blind testing on held-out patient-derived tumor tissue samples demonstrates that FLIM_PSR_k reliably achieves a super-resolution factor of k = 5, resulting in a 25-fold increase in the space-bandwidth product of the output images and revealing fine architectural features lost in lower-resolution inputs, with statistically significant improvements across various image quality metrics. By increasing FLIM's effective spatial resolution, FLIM_PSR_k advances lifetime imaging toward faster, higher-resolution, and hardware-flexible implementations compatible with low-numerical-aperture and miniaturized platforms, better positioning FLIM for translational applications.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-Context Probing for Membership Inference in Fine-Tuned Language Models</title>
<link>https://arxiv.org/abs/2512.16292</link>
<guid>https://arxiv.org/abs/2512.16292</guid>
<content:encoded><![CDATA[
arXiv:2512.16292v1 Announce Type: cross 
Abstract: Membership inference attacks (MIAs) pose a critical privacy threat to fine-tuned large language models (LLMs), especially when models are adapted to domain-specific tasks using sensitive data. While prior black-box MIA techniques rely on confidence scores or token likelihoods, these signals are often entangled with a sample's intrinsic properties - such as content difficulty or rarity - leading to poor generalization and low signal-to-noise ratios. In this paper, we propose ICP-MIA, a novel MIA framework grounded in the theory of training dynamics, particularly the phenomenon of diminishing returns during optimization. We introduce the Optimization Gap as a fundamental signal of membership: at convergence, member samples exhibit minimal remaining loss-reduction potential, while non-members retain significant potential for further optimization. To estimate this gap in a black-box setting, we propose In-Context Probing (ICP), a training-free method that simulates fine-tuning-like behavior via strategically constructed input contexts. We propose two probing strategies: reference-data-based (using semantically similar public samples) and self-perturbation (via masking or generation). Experiments on three tasks and multiple LLMs show that ICP-MIA significantly outperforms prior black-box MIAs, particularly at low false positive rates. We further analyze how reference data alignment, model type, PEFT configurations, and training schedules affect attack effectiveness. Our findings establish ICP-MIA as a practical and theoretically grounded framework for auditing privacy risks in deployed LLMs.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Transformers overcome the lack of data in the simulation of history-dependent flows?</title>
<link>https://arxiv.org/abs/2512.16305</link>
<guid>https://arxiv.org/abs/2512.16305</guid>
<content:encoded><![CDATA[
arXiv:2512.16305v1 Announce Type: cross 
Abstract: It is well known that the lack of information about certain variables necessary for the description of a dynamical system leads to the introduction of historical dependence (lack of Markovian character of the model) and noise. Traditionally, scientists have made up for these shortcomings by designing phenomenological variables that take into account this historical dependence (typically, conformational tensors in fluids). Often, these phenomenological variables are not easily measurable experimentally. In this work, we study to what extent Transformer architectures are able to cope with the lack of experimental data on these variables. The methodology is evaluated on three benchmark problems: a cylinder flow with no history dependence, a viscoelastic Couette flow modeled via the Oldroyd-B formalism, and a non-linear polymeric fluid described by the FENE model. Our results show that the Transformer outperforms a thermodynamically consistent, structure-preserving neural network with metriplectic bias in systems with missing experimental data, providing lower errors even in low-dimensional latent spaces. In contrast, for systems whose state variables can be fully known, the metriplectic model achieves superior performance.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Global universal approximation with Brownian signatures</title>
<link>https://arxiv.org/abs/2512.16396</link>
<guid>https://arxiv.org/abs/2512.16396</guid>
<content:encoded><![CDATA[
arXiv:2512.16396v1 Announce Type: cross 
Abstract: We establish $L^p$-type universal approximation theorems for general and non-anticipative functionals on suitable rough path spaces, showing that linear functionals acting on signatures of time-extended rough paths are dense with respect to an $L^p$-distance. To that end, we derive global universal approximation theorems for weighted rough path spaces. We demonstrate that these $L^p$-type universal approximation theorems apply in particular to Brownian motion. As a consequence, linear functionals on the signature of the time-extended Brownian motion can approximate any $p$-integrable stochastic process adapted to the Brownian filtration, including solutions to stochastic differential equations.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advantages and limitations in the use of transfer learning for individual treatment effects in causal machine learning</title>
<link>https://arxiv.org/abs/2512.16489</link>
<guid>https://arxiv.org/abs/2512.16489</guid>
<content:encoded><![CDATA[
arXiv:2512.16489v1 Announce Type: cross 
Abstract: Generalizing causal knowledge across diverse environments is challenging, especially when estimates from large-scale datasets must be applied to smaller or systematically different contexts, where external validity is critical. Model-based estimators of individual treatment effects (ITE) from machine learning require large sample sizes, limiting their applicability in domains such as behavioral sciences with smaller datasets. We demonstrate how estimation of ITEs with Treatment Agnostic Representation Networks (TARNet; Shalit et al., 2017) can be improved by leveraging knowledge from source datasets and adapting it to new settings via transfer learning (TL-TARNet; Aloui et al., 2023). In simulations that vary source and sample sizes and consider both randomized and non-randomized intervention target settings, the transfer-learning extension TL-TARNet improves upon standard TARNet, reducing ITE error and attenuating bias when a large unbiased source is available and target samples are small. In an empirical application using the India Human Development Survey (IHDS-II), we estimate the effect of mothers' firewood collection time on children's weekly study time; transfer learning pulls the target mean ITEs toward the source ITE estimate, reducing bias in the estimates obtained without transfer. These results suggest that transfer learning for causal models can improve the estimation of ITE in small samples.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pseudo-Cepstrum: Pitch Modification for Mel-Based Neural Vocoders</title>
<link>https://arxiv.org/abs/2512.16519</link>
<guid>https://arxiv.org/abs/2512.16519</guid>
<content:encoded><![CDATA[
arXiv:2512.16519v1 Announce Type: cross 
Abstract: This paper introduces a cepstrum-based pitch modification method that can be applied to any mel-spectrogram representation. As a result, this method is compatible with any mel-based vocoder without requiring any additional training or changes to the model. This is achieved by directly modifying the cepstrum feature space in order to shift the harmonic structure to the desired target. The spectrogram magnitude is computed via the pseudo-inverse mel transform, then converted to the cepstrum by applying DCT. In this domain, the cepstral peak is shifted without having to estimate its position and the modified mel is recomputed by applying IDCT and mel-filterbank. These pitch-shifted mel-spectrogram features can be converted to speech with any compatible vocoder. The proposed method is validated experimentally with objective and subjective metrics on various state-of-the-art neural vocoders as well as in comparison with traditional pitch modification methods.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Systematic Study of Code Obfuscation Against LLM-based Vulnerability Detection</title>
<link>https://arxiv.org/abs/2512.16538</link>
<guid>https://arxiv.org/abs/2512.16538</guid>
<content:encoded><![CDATA[
arXiv:2512.16538v1 Announce Type: cross 
Abstract: As large language models (LLMs) are increasingly adopted for code vulnerability detection, their reliability and robustness across diverse vulnerability types have become a pressing concern. In traditional adversarial settings, code obfuscation has long been used as a general strategy to bypass auditing tools, preserving exploitability without tampering with the tools themselves. Numerous efforts have explored obfuscation methods and tools, yet their capabilities differ in terms of supported techniques, granularity, and programming languages, making it difficult to systematically assess their impact on LLM-based vulnerability detection. To address this gap, we provide a structured systematization of obfuscation techniques and evaluate them under a unified framework. Specifically, we categorize existing obfuscation methods into three major classes (layout, data flow, and control flow) covering 11 subcategories and 19 concrete techniques. We implement these techniques across four programming languages (Solidity, C, C++, and Python) using a consistent LLM-driven approach, and evaluate their effects on 15 LLMs spanning four model families (DeepSeek, OpenAI, Qwen, and LLaMA), as well as on two coding agents (GitHub Copilot and Codex). Our findings reveal both positive and negative impacts of code obfuscation on LLM-based vulnerability detection, highlighting conditions under which obfuscation leads to performance improvements or degradations. We further analyze these outcomes with respect to vulnerability characteristics, code properties, and model attributes. Finally, we outline several open problems and propose future directions to enhance the robustness of LLMs for real-world vulnerability detection.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predictive Inorganic Synthesis based on Machine Learning using Small Data sets: a case study of size-controlled Cu Nanoparticles</title>
<link>https://arxiv.org/abs/2512.16545</link>
<guid>https://arxiv.org/abs/2512.16545</guid>
<content:encoded><![CDATA[
arXiv:2512.16545v1 Announce Type: cross 
Abstract: Copper nanoparticles (Cu NPs) have a broad applicability, yet their synthesis is sensitive to subtle changes in reaction parameters. This sensitivity, combined with the time- and resource-intensive nature of experimental optimization, poses a major challenge in achieving reproducible and size-controlled synthesis. While Machine Learning (ML) shows promise in materials research, its application is often limited by scarcity of large high-quality experimental data sets. This study explores ML to predict the size of Cu NPs from microwave-assisted polyol synthesis using a small data set of 25 in-house performed syntheses. Latin Hypercube Sampling is used to efficiently cover the parameter space while creating the experimental data set. Ensemble regression models, built with the AMADEUS framework, successfully predict particle sizes with high accuracy ($R^2 = 0.74$), outperforming classical statistical approaches ($R^2 = 0.60$). Overall, this study highlights that, for lab-scale synthesis optimization, high-quality small datasets combined with classical, interpretable ML models outperform traditional statistical methods and are fully sufficient for quantitative synthesis prediction. This approach provides a sustainable and experimentally realistic pathway toward data-driven inorganic synthesis design.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-Asymptotic Global Convergence of PPO-Clip</title>
<link>https://arxiv.org/abs/2512.16565</link>
<guid>https://arxiv.org/abs/2512.16565</guid>
<content:encoded><![CDATA[
arXiv:2512.16565v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) has gained attention for aligning large language models (LLMs) via reinforcement learning from human feedback (RLHF). The actor-only variants of Proximal Policy Optimization (PPO) are widely applied for their efficiency. These algorithms incorporate a clipping mechanism to improve stability. Besides, a regularization term, such as the reverse KL-divergence or a more general \(f\)-divergence, is introduced to prevent policy drift. Despite their empirical success, a rigorous theoretical understanding of the problem and the algorithm's properties is limited. This paper advances the theoretical foundations of the PPO-Clip algorithm by analyzing a deterministic actor-only PPO algorithm within the general RL setting with \(f\)-divergence regularization under the softmax policy parameterization. We derive a non-uniform Lipschitz smoothness condition and a {\L}ojasiewicz inequality for the considered problem. Based on these, a non-asymptotic linear convergence rate to the globally optimal policy is established for the forward KL-regularizer. Furthermore, stationary convergence and local linear convergence are derived for the reverse KL-regularizer.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Muon is Provably Faster with Momentum Variance Reduction</title>
<link>https://arxiv.org/abs/2512.16598</link>
<guid>https://arxiv.org/abs/2512.16598</guid>
<content:encoded><![CDATA[
arXiv:2512.16598v1 Announce Type: cross 
Abstract: Recent empirical research has demonstrated that deep learning optimizers based on the linear minimization oracle (LMO) over specifically chosen Non-Euclidean norm balls, such as Muon and Scion, outperform Adam-type methods in the training of large language models. In this work, we show that such optimizers can be provably improved by replacing their vanilla momentum by momentum variance reduction (MVR). Instead of proposing and analyzing MVR variants of Muon and Scion separately, we incorporate MVR into the recently proposed Gluon framework, which captures Muon, Scion and other specific Non-Euclidean LMO-based methods as special cases, and at the same time works with a more general smoothness assumption which better captures the layer-wise structure of neural networks. In the non-convex case, we incorporate MVR into Gluon in three different ways. All of them improve the convergence rate from ${\cal O} (\frac{1}{K^{1/4}})$ to ${\cal O} (\frac{1}{K^{1/3}})$. Additionally, we provide improved rates in the star-convex case. Finally, we conduct several numerical experiments that verify the superior performance of our proposed algorithms in terms of iteration complexity.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Riemannian Stochastic Interpolants for Amorphous Particle Systems</title>
<link>https://arxiv.org/abs/2512.16607</link>
<guid>https://arxiv.org/abs/2512.16607</guid>
<content:encoded><![CDATA[
arXiv:2512.16607v1 Announce Type: cross 
Abstract: Modern generative models hold great promise for accelerating diverse tasks involving the simulation of physical systems, but they must be adapted to the specific constraints of each domain. Significant progress has been made for biomolecules and crystalline materials. Here, we address amorphous materials (glasses), which are disordered particle systems lacking atomic periodicity. Sampling equilibrium configurations of glass-forming materials is a notoriously slow and difficult task. This obstacle could be overcome by developing a generative framework capable of producing equilibrium configurations with well-defined likelihoods. In this work, we address this challenge by leveraging an equivariant Riemannian stochastic interpolation framework which combines Riemannian stochastic interpolant and equivariant flow matching. Our method rigorously incorporates periodic boundary conditions and the symmetries of multi-component particle systems, adapting an equivariant graph neural network to operate directly on the torus. Our numerical experiments on model amorphous systems demonstrate that enforcing geometric and symmetry constraints significantly improves generative performance.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SARMAE: Masked Autoencoder for SAR Representation Learning</title>
<link>https://arxiv.org/abs/2512.16635</link>
<guid>https://arxiv.org/abs/2512.16635</guid>
<content:encoded><![CDATA[
arXiv:2512.16635v1 Announce Type: cross 
Abstract: Synthetic Aperture Radar (SAR) imagery plays a critical role in all-weather, day-and-night remote sensing applications. However, existing SAR-oriented deep learning is constrained by data scarcity, while the physically grounded speckle noise in SAR imagery further hampers fine-grained semantic representation learning. To address these challenges, we propose SARMAE, a Noise-Aware Masked Autoencoder for self-supervised SAR representation learning. Specifically, we construct SAR-1M, the first million-scale SAR dataset, with additional paired optical images, to enable large-scale pre-training. Building upon this, we design Speckle-Aware Representation Enhancement (SARE), which injects SAR-specific speckle noise into masked autoencoders to facilitate noise-aware and robust representation learning. Furthermore, we introduce Semantic Anchor Representation Constraint (SARC), which leverages paired optical priors to align SAR features and ensure semantic consistency. Extensive experiments across multiple SAR datasets demonstrate that SARMAE achieves state-of-the-art performance on classification, detection, and segmentation tasks. Code and models will be available at https://github.com/MiliLab/SARMAE.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How accurate are foundational machine learning interatomic potentials for heterogeneous catalysis?</title>
<link>https://arxiv.org/abs/2512.16702</link>
<guid>https://arxiv.org/abs/2512.16702</guid>
<content:encoded><![CDATA[
arXiv:2512.16702v1 Announce Type: cross 
Abstract: Foundational machine learning interatomic potentials (MLIPs) are being developed at a rapid pace, promising closer and closer approximation to ab initio accuracy. This unlocks the possibility to simulate much larger length and time scales. However, benchmarks for these MLIPs are usually limited to ordered, crystalline and bulk materials. Hence, reported performance does not necessarily accurately reflect MLIP performance in real applications such as heterogeneous catalysis. Here, we systematically analyze zero-shot performance of 80 different MLIPs, evaluating tasks typical for heterogeneous catalysis across a range of different data sets, including adsorption and reaction on surfaces of alloyed metals, oxides, and metal-oxide interfacial systems. We demonstrate that current-generation foundational MLIPs can already perform at high accuracy for applications such as predicting vacancy formation energies of perovskite oxides or zero-point energies of supported nanoclusters. However, limitations also exist. We find that many MLIPs catastrophically fail when applied to magnetic materials, and structure relaxation in the MLIP generally increases the energy prediction error compared to single-point evaluation of a previously optimized structure. Comparing low-cost task-specific models to foundational MLIPs, we highlight some core differences between these model approaches and show that -- if considering only accuracy -- these models can compete with the current generation of best-performing MLIPs. Furthermore, we show that no single MLIP universally performs best, requiring users to investigate MLIP suitability for their desired application.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Olaf: Bringing an Animated Character to Life in the Physical World</title>
<link>https://arxiv.org/abs/2512.16705</link>
<guid>https://arxiv.org/abs/2512.16705</guid>
<content:encoded><![CDATA[
arXiv:2512.16705v1 Announce Type: cross 
Abstract: Animated characters often move in non-physical ways and have proportions that are far from a typical walking robot. This provides an ideal platform for innovation in both mechanical design and stylized motion control. In this paper, we bring Olaf to life in the physical world, relying on reinforcement learning guided by animation references for control. To create the illusion of Olaf's feet moving along his body, we hide two asymmetric legs under a soft foam skirt. To fit actuators inside the character, we use spherical and planar linkages in the arms, mouth, and eyes. Because the walk cycle results in harsh contact sounds, we introduce additional rewards that noticeably reduce impact noise. The large head, driven by small actuators in the character's slim neck, creates a risk of overheating, amplified by the costume. To keep actuators from overheating, we feed temperature values as additional inputs to policies, introducing new rewards to keep them within bounds. We validate the efficacy of our modeling in simulation and on hardware, demonstrating an unmatched level of believability for a costumed robotic character.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On The Hidden Biases of Flow Matching Samplers</title>
<link>https://arxiv.org/abs/2512.16768</link>
<guid>https://arxiv.org/abs/2512.16768</guid>
<content:encoded><![CDATA[
arXiv:2512.16768v1 Announce Type: cross 
Abstract: We study the implicit bias of flow matching (FM) samplers via the lens of empirical flow matching. Although population FM may produce gradient-field velocities resembling optimal transport (OT), we show that the empirical FM minimizer is almost never a gradient field, even when each conditional flow is. Consequently, empirical FM is intrinsically energetically suboptimal. In view of this, we analyze the kinetic energy of generated samples. With Gaussian sources, both instantaneous and integrated kinetic energies exhibit exponential concentration, while heavy-tailed sources lead to polynomial tails. These behaviors are governed primarily by the choice of source distribution rather than the data. Overall, these notes provide a concise mathematical account of the structural and energetic biases arising in empirical FM.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-Linear Strong Data-Processing for Quantum Hockey-Stick Divergences</title>
<link>https://arxiv.org/abs/2512.16778</link>
<guid>https://arxiv.org/abs/2512.16778</guid>
<content:encoded><![CDATA[
arXiv:2512.16778v1 Announce Type: cross 
Abstract: Data-processing is a desired property of classical and quantum divergences and information measures. In information theory, the contraction coefficient measures how much the distinguishability of quantum states decreases when they are transmitted through a quantum channel, establishing linear strong data-processing inequalities (SDPI). However, these linear SDPI are not always tight and can be improved in most of the cases. In this work, we establish non-linear SDPI for quantum hockey-stick divergence for noisy channels that satisfy a certain noise criterion. We also note that our results improve upon existing linear SDPI for quantum hockey-stick divergences and also non-linear SDPI for classical hockey-stick divergence. We define $F_\gamma$ curves generalizing Dobrushin curves for the quantum setting while characterizing SDPI for the sequential composition of heterogeneous channels. In addition, we derive reverse-Pinsker type inequalities for $f$-divergences with additional constraints on hockey-stick divergences. We show that these non-linear SDPI can establish tighter finite mixing times that cannot be achieved through linear SDPI. Furthermore, we find applications of these in establishing stronger privacy guarantees for the composition of sequential private quantum channels when privacy is quantified by quantum local differential privacy.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Few-Shot Specific Emitter Identification via Integrated Complex Variational Mode Decomposition and Spatial Attention Transfer</title>
<link>https://arxiv.org/abs/2512.16786</link>
<guid>https://arxiv.org/abs/2512.16786</guid>
<content:encoded><![CDATA[
arXiv:2512.16786v1 Announce Type: cross 
Abstract: Specific emitter identification (SEI) utilizes passive hardware characteristics to authenticate transmitters, providing a robust physical-layer security solution. However, most deep-learning-based methods rely on extensive data or require prior information, which poses challenges in real-world scenarios with limited labeled data. We propose an integrated complex variational mode decomposition algorithm that decomposes and reconstructs complex-valued signals to approximate the original transmitted signals, thereby enabling more accurate feature extraction. We further utilize a temporal convolutional network to effectively model the sequential signal characteristics, and introduce a spatial attention mechanism to adaptively weight informative signal segments, significantly enhancing identification performance. Additionally, the branch network allows leveraging pre-trained weights from other data while reducing the need for auxiliary datasets. Ablation experiments on the simulated data demonstrate the effectiveness of each component of the model. An accuracy comparison on a public dataset reveals that our method achieves 96% accuracy using only 10 symbols without requiring any prior knowledge.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Coordinated Anti-Jamming Resilience in Swarm Networks via Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.16813</link>
<guid>https://arxiv.org/abs/2512.16813</guid>
<content:encoded><![CDATA[
arXiv:2512.16813v1 Announce Type: cross 
Abstract: Reactive jammers pose a severe security threat to robotic-swarm networks by selectively disrupting inter-agent communications and undermining formation integrity and mission success. Conventional countermeasures such as fixed power control or static channel hopping are largely ineffective against such adaptive adversaries. This paper presents a multi-agent reinforcement learning (MARL) framework based on the QMIX algorithm to improve the resilience of swarm communications under reactive jamming. We consider a network of multiple transmitter-receiver pairs sharing channels while a reactive jammer with Markovian threshold dynamics senses aggregate power and reacts accordingly. Each agent jointly selects transmit frequency (channel) and power, and QMIX learns a centralized but factorizable action-value function that enables coordinated yet decentralized execution. We benchmark QMIX against a genie-aided optimal policy in a no-channel-reuse setting, and against local Upper Confidence Bound (UCB) and a stateless reactive policy in a more general fading regime with channel reuse enabled. Simulation results show that QMIX rapidly converges to cooperative policies that nearly match the genie-aided bound, while achieving higher throughput and lower jamming incidence than the baselines, thereby demonstrating MARL's effectiveness for securing autonomous swarms in contested environments.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReinforceGen: Hybrid Skill Policies with Automated Data Generation and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.16861</link>
<guid>https://arxiv.org/abs/2512.16861</guid>
<content:encoded><![CDATA[
arXiv:2512.16861v1 Announce Type: cross 
Abstract: Long-horizon manipulation has been a long-standing challenge in the robotics community. We propose ReinforceGen, a system that combines task decomposition, data generation, imitation learning, and motion planning to form an initial solution, and improves each component through reinforcement-learning-based fine-tuning. ReinforceGen first segments the task into multiple localized skills, which are connected through motion planning. The skills and motion planning targets are trained with imitation learning on a dataset generated from 10 human demonstrations, and then fine-tuned through online adaptation and reinforcement learning. When benchmarked on the Robosuite dataset, ReinforceGen reaches 80% success rate on all tasks with visuomotor controls in the highest reset range setting. Additional ablation studies show that our fine-tuning approaches contributes to an 89% average performance increase. More results and videos available in https://reinforcegen.github.io/
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Universal Representation Property of Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2512.16872</link>
<guid>https://arxiv.org/abs/2512.16872</guid>
<content:encoded><![CDATA[
arXiv:2512.16872v1 Announce Type: cross 
Abstract: Inspired by biology, spiking neural networks (SNNs) process information via discrete spikes over time, offering an energy-efficient alternative to the classical computing paradigm and classical artificial neural networks (ANNs). In this work, we analyze the representational power of SNNs by viewing them as sequence-to-sequence processors of spikes, i.e., systems that transform a stream of input spikes into a stream of output spikes. We establish the universal representation property for a natural class of spike train functions. Our results are fully quantitative, constructive, and near-optimal in the number of required weights and neurons. The analysis reveals that SNNs are particularly well-suited to represent functions with few inputs, low temporal complexity, or compositions of such functions. The latter is of particular interest, as it indicates that deep SNNs can efficiently capture composite functions via a modular design. As an application of our results, we discuss spike train classification. Overall, these results contribute to a rigorous foundation for understanding the capabilities and limitations of spike-based neuromorphic systems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pixel Seal: Adversarial-only training for invisible image and video watermarking</title>
<link>https://arxiv.org/abs/2512.16874</link>
<guid>https://arxiv.org/abs/2512.16874</guid>
<content:encoded><![CDATA[
arXiv:2512.16874v1 Announce Type: cross 
Abstract: Invisible watermarking is essential for tracing the provenance of digital content. However, training state-of-the-art models remains notoriously difficult, with current approaches often struggling to balance robustness against true imperceptibility. This work introduces Pixel Seal, which sets a new state-of-the-art for image and video watermarking. We first identify three fundamental issues of existing methods: (i) the reliance on proxy perceptual losses such as MSE and LPIPS that fail to mimic human perception and result in visible watermark artifacts; (ii) the optimization instability caused by conflicting objectives, which necessitates exhaustive hyperparameter tuning; and (iii) reduced robustness and imperceptibility of watermarks when scaling models to high-resolution images and videos. To overcome these issues, we first propose an adversarial-only training paradigm that eliminates unreliable pixel-wise imperceptibility losses. Second, we introduce a three-stage training schedule that stabilizes convergence by decoupling robustness and imperceptibility. Third, we address the resolution gap via high-resolution adaptation, employing JND-based attenuation and training-time inference simulation to eliminate upscaling artifacts. We thoroughly evaluate the robustness and imperceptibility of Pixel Seal on different image types and across a wide range of transformations, and show clear improvements over the state-of-the-art. We finally demonstrate that the model efficiently adapts to video via temporal watermark pooling, positioning Pixel Seal as a practical and scalable solution for reliable provenance in real-world image and video settings.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Confidence Ellipsoids and Applications to Robust Subspace Recovery</title>
<link>https://arxiv.org/abs/2512.16875</link>
<guid>https://arxiv.org/abs/2512.16875</guid>
<content:encoded><![CDATA[
arXiv:2512.16875v1 Announce Type: cross 
Abstract: We study the problem of finding confidence ellipsoids for an arbitrary distribution in high dimensions. Given samples from a distribution $D$ and a confidence parameter $\alpha$, the goal is to find the smallest volume ellipsoid $E$ which has probability mass $\Pr_{D}[E] \ge 1-\alpha$. Ellipsoids are a highly expressive class of confidence sets as they can capture correlations in the distribution, and can approximate any convex set. This problem has been studied in many different communities. In statistics, this is the classic minimum volume estimator introduced by Rousseeuw as a robust non-parametric estimator of location and scatter. However in high dimensions, it becomes NP-hard to obtain any non-trivial approximation factor in volume when the condition number $\beta$ of the ellipsoid (ratio of the largest to the smallest axis length) goes to $\infty$. This motivates the focus of our paper: can we efficiently find confidence ellipsoids with volume approximation guarantees when compared to ellipsoids of bounded condition number $\beta$?
  Our main result is a polynomial time algorithm that finds an ellipsoid $E$ whose volume is within a $O(\beta^{\gamma d})$ multiplicative factor of the volume of best $\beta$-conditioned ellipsoid while covering at least $1-O(\alpha/\gamma)$ probability mass for any $\gamma < \alpha$. We complement this with a computational hardness result that shows that such a dependence seems necessary up to constants in the exponent. The algorithm and analysis uses the rich primal-dual structure of the minimum volume enclosing ellipsoid and the geometric Brascamp-Lieb inequality. As a consequence, we obtain the first polynomial time algorithm with approximation guarantees on worst-case instances of the robust subspace recovery problem.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PolaRiS: Scalable Real-to-Sim Evaluations for Generalist Robot Policies</title>
<link>https://arxiv.org/abs/2512.16881</link>
<guid>https://arxiv.org/abs/2512.16881</guid>
<content:encoded><![CDATA[
arXiv:2512.16881v1 Announce Type: cross 
Abstract: A significant challenge for robot learning research is our ability to accurately measure and compare the performance of robot policies. Benchmarking in robotics is historically challenging due to the stochasticity, reproducibility, and time-consuming nature of real-world rollouts. This challenge is exacerbated for recent generalist policies, which has to be evaluated across a wide variety of scenes and tasks. Evaluation in simulation offers a scalable complement to real world evaluations, but the visual and physical domain gap between existing simulation benchmarks and the real world has made them an unreliable signal for policy improvement. Furthermore, building realistic and diverse simulated environments has traditionally required significant human effort and expertise. To bridge the gap, we introduce Policy Evaluation and Environment Reconstruction in Simulation (PolaRiS), a scalable real-to-sim framework for high-fidelity simulated robot evaluation. PolaRiS utilizes neural reconstruction methods to turn short video scans of real-world scenes into interactive simulation environments. Additionally, we develop a simple simulation data co-training recipe that bridges remaining real-to-sim gaps and enables zero-shot evaluation in unseen simulation environments. Through extensive paired evaluations between simulation and the real world, we demonstrate that PolaRiS evaluations provide a much stronger correlation to real world generalist policy performance than existing simulated benchmarks. Its simplicity also enables rapid creation of diverse simulated environments. As such, this work takes a step towards distributed and democratized evaluation for the next generation of robotic foundation models.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cartesian-nj: Extending e3nn to Irreducible Cartesian Tensor Product and Contracion</title>
<link>https://arxiv.org/abs/2512.16882</link>
<guid>https://arxiv.org/abs/2512.16882</guid>
<content:encoded><![CDATA[
arXiv:2512.16882v1 Announce Type: cross 
Abstract: Equivariant atomistic machine learning models have brought substantial gains in both extrapolation capability and predictive accuracy. Depending on the basis of the space, two distinct types of irreducible representations are utilized. From architectures built upon spherical tensors (STs) to more recent formulations employing irreducible Cartesian tensors (ICTs), STs have remained dominant owing to their compactness, elegance, and theoretical completeness. Nevertheless, questions have persisted regarding whether ST constructions are the only viable design principle, motivating continued development of Cartesian networks. In this work, we introduce the Cartesian-3j and Cartesian-nj symbol, which serve as direct analogues of the Wigner-3j and Wigner-nj symbol defined for tensor coupling. These coefficients enable the combination of any two ICTs into a new ICT. Building on this foundation, we extend e3nn to support irreducible Cartesian tensor product, and we release the resulting Python package as cartnn. Within this framework, we implement Cartesian counterparts of MACE, NequIP, and Allegro, allowing the first systematic comparison of Cartesian and spherical models to assess whether Cartesian formulations may offer advantages under specific conditions. Using TACE as a representative example, we further examine whether architectures constructed from irreducible Cartesian tensor product and contraction(ICTP and ICTC) are conceptually well-founded in Cartesian space and whether opportunities remain for improving their design.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation</title>
<link>https://arxiv.org/abs/2512.16891</link>
<guid>https://arxiv.org/abs/2512.16891</guid>
<content:encoded><![CDATA[
arXiv:2512.16891v1 Announce Type: cross 
Abstract: Video Large Language Models (VLLMs) unlock world-knowledge-aware video understanding through pretraining on internet-scale data and have already shown promise on tasks such as movie analysis and video question answering. However, deploying VLLMs for downstream tasks such as video recommendation remains challenging, since real systems require multi-video inputs, lightweight backbones, low-latency sequential inference, and rapid response. In practice, (1) decode-only generation yields high latency for sequential inference, (2) typical interfaces do not support multi-video inputs, and (3) constraining outputs to language discards fine-grained visual details that matter for downstream vision tasks. We argue that these limitations stem from the absence of a representation that preserves pixel-level detail while leveraging world knowledge. We present LinkedOut, a representation that extracts VLLM world knowledge directly from video to enable fast inference, supports multi-video histories, and removes the language bottleneck. LinkedOut extracts semantically grounded, knowledge-aware tokens from raw frames using VLLMs, guided by promptable queries and optional auxiliary modalities. We introduce a cross-layer knowledge fusion MoE that selects the appropriate level of abstraction from the rich VLLM features, enabling personalized, interpretable, and low-latency recommendation. To our knowledge, LinkedOut is the first VLLM-based video recommendation method that operates on raw frames without handcrafted labels, achieving state-of-the-art results on standard benchmarks. Interpretability studies and ablations confirm the benefits of layer diversity and layer-wise fusion, pointing to a practical path that fully leverages VLLM world-knowledge priors and visual reasoning for downstream vision tasks such as recommendation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-Context Algebra</title>
<link>https://arxiv.org/abs/2512.16902</link>
<guid>https://arxiv.org/abs/2512.16902</guid>
<content:encoded><![CDATA[
arXiv:2512.16902v1 Announce Type: cross 
Abstract: We investigate the mechanisms that arise when transformers are trained to solve arithmetic on sequences where tokens are variables whose meaning is determined only through their interactions. While prior work has found that transformers develop geometric embeddings that mirror algebraic structure, those previous findings emerge from settings where arithmetic-valued tokens have fixed meanings. We devise a new task in which the assignment of symbols to specific algebraic group elements varies from one sequence to another. Despite this challenging setup, transformers achieve near-perfect accuracy on the task and even generalize to unseen algebraic groups. We develop targeted data distributions to create causal tests of a set of hypothesized mechanisms, and we isolate three mechanisms models consistently learn: commutative copying where a dedicated head copies answers, identity element recognition that distinguishes identity-containing facts, and closure-based cancellation that tracks group membership to constrain valid answers. Complementary to the geometric representations found in fixed-symbol settings, our findings show that models develop symbolic reasoning mechanisms when trained to reason in-context with variables whose meanings are not fixed.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SFTok: Bridging the Performance Gap in Discrete Tokenizers</title>
<link>https://arxiv.org/abs/2512.16910</link>
<guid>https://arxiv.org/abs/2512.16910</guid>
<content:encoded><![CDATA[
arXiv:2512.16910v1 Announce Type: cross 
Abstract: Recent advances in multimodal models highlight the pivotal role of image tokenization in high-resolution image generation. By compressing images into compact latent representations, tokenizers enable generative models to operate in lower-dimensional spaces, thereby improving computational efficiency and reducing complexity. Discrete tokenizers naturally align with the autoregressive paradigm but still lag behind continuous ones, limiting their adoption in multimodal systems. To address this, we propose \textbf{SFTok}, a discrete tokenizer that incorporates a multi-step iterative mechanism for precise reconstruction. By integrating \textbf{self-forcing guided visual reconstruction} and \textbf{debias-and-fitting training strategy}, SFTok resolves the training-inference inconsistency in multi-step process, significantly enhancing image reconstruction quality. At a high compression rate of only 64 tokens per image, SFTok achieves state-of-the-art reconstruction quality on ImageNet (rFID = 1.21) and demonstrates exceptional performance in class-to-image generation tasks (gFID = 2.29).
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.16917</link>
<guid>https://arxiv.org/abs/2512.16917</guid>
<content:encoded><![CDATA[
arXiv:2512.16917v1 Announce Type: cross 
Abstract: Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice's soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural networks for dengue forecasting: a systematic review</title>
<link>https://arxiv.org/abs/2106.12905</link>
<guid>https://arxiv.org/abs/2106.12905</guid>
<content:encoded><![CDATA[
arXiv:2106.12905v2 Announce Type: replace 
Abstract: Background: Early forecasts of dengue are an important tool for disease mitigation. Neural networks are powerful predictive models that have made contributions to many areas of public health. In this study, we reviewed the application of neural networks in the dengue forecasting literature, with the objective of informing model design for future work.
  Methods: Following PRISMA guidelines, we conducted a systematic search of studies that use neural networks to forecast dengue in human populations. We summarized the relative performance of neural networks and comparator models, architectures and hyper-parameters, choices of input features, geographic spread, and model transparency.
  Results: Sixty two papers were included. Most studies implemented shallow feed-forward neural networks, using historical dengue incidence and climate variables. Prediction horizons varied greatly, as did the model selection and evaluation approach. Building on the strengths of neural networks, most studies used granular observations at the city level, or on its subdivisions, while also commonly employing weekly data. Performance of neural networks relative to comparators, such as tree-based supervised models, varied across study contexts, and we found that 63% of all studies do include at least one such model as a baseline, and in those cases about half of the studies report neural networks as the best performing model.
  Conclusions: The studies suggest that neural networks can provide competitive forecasts for dengue, and can reliably be included in the set of candidate models for future dengue prediction efforts. The use of deep networks is relatively unexplored but offers promising avenues for further research, as does the use of a broader set of input features and prediction in light of structural changes in the data generation mechanism.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimization with Access to Auxiliary Information</title>
<link>https://arxiv.org/abs/2206.00395</link>
<guid>https://arxiv.org/abs/2206.00395</guid>
<content:encoded><![CDATA[
arXiv:2206.00395v5 Announce Type: replace 
Abstract: We investigate the fundamental optimization question of minimizing a target function $f$, whose gradients are expensive to compute or have limited availability, given access to some auxiliary side function $h$ whose gradients are cheap or more available. This formulation captures many settings of practical relevance, such as i) re-using batches in SGD, ii) transfer learning, iii) federated learning, iv) training with compressed models/dropout, Et cetera. We propose two generic new algorithms that apply in all these settings; we also prove that we can benefit from this framework under the Hessian similarity assumption between the target and side information. A benefit is obtained when this similarity measure is small; we also show a potential benefit from stochasticity when the auxiliary noise is correlated with that of the target function.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Four-hour thunderstorm nowcasting using a deep diffusion model of satellite data</title>
<link>https://arxiv.org/abs/2404.10512</link>
<guid>https://arxiv.org/abs/2404.10512</guid>
<content:encoded><![CDATA[
arXiv:2404.10512v5 Announce Type: replace 
Abstract: Convection (thunderstorm) develops rapidly within hours and is highly destructive, posing a significant challenge for nowcasting and resulting in substantial losses to infrastructure and society. After the emergence of artificial intelligence (AI)-based methods, convection nowcasting has experienced rapid advancements, with its performance surpassing that of physics-based numerical weather prediction and other conventional approaches. However, the lead time and coverage of it still leave much to be desired and hardly meet the needs of disaster emergency response. Here, we propose a deep diffusion model for satellite data (DDMS) to establish an AI-based convection nowcasting system. Specifically, DDMS employs diffusion processes to effectively simulate complicated spatiotemporal evolution patterns of convective clouds, achieving more accurate forecasts of convective growth and dissipation over longer lead times. Additionally, it combines geostationary satellite brightness temperature data and domain knowledge from meteorological experts, thereby achieving planetary-scale forecast coverage. During long-term tests and objective validation based on the FengYun-4A satellite, our system achieves, for the first time, effective convection nowcasting up to 4 hours, with broad coverage (about 20,000,000 km2), remarkable accuracy, and high resolution (15 minutes; 4 km). Its performance reaches a new height in convection nowcasting compared to the existing models. In terms of application, our system is highly transferable with the potential to collaborate with multiple satellites for global convection nowcasting. Furthermore, our results highlight the remarkable capabilities of diffusion models in convective clouds forecasting, as well as the significant value of geostationary satellite data when empowered by AI technologies.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Online Bandits with (Biased) Offline Data: Adaptive Learning under Distribution Mismatch</title>
<link>https://arxiv.org/abs/2405.02594</link>
<guid>https://arxiv.org/abs/2405.02594</guid>
<content:encoded><![CDATA[
arXiv:2405.02594v2 Announce Type: replace 
Abstract: Traditional online learning models are typically initialized from scratch. By contrast, contemporary real-world applications often have access to historical datasets that can potentially enhanced the online learning processes. We study how offline data can be leveraged to facilitate online learning in stochastic multi-armed bandits and combinatorial bandits. In our study, the probability distributions that govern the offline data and the online rewards can be different. We first show that, without a non-trivial upper bound on their difference, no non-anticipatory policy can outperform the classical Upper Confidence Bound (UCB) policy, even with the access to offline data. In complement, we propose an online policy MIN-UCB for multi-armed bandits. MIN-UCB outperforms the UCB when such an upper bound is available. MIN-UCB adaptively chooses to utilize the offline data when they are deemed informative, and to ignore them otherwise. We establish that MIN-UCB achieves tight regret bounds, in both instance independent and dependent settings. We generalize our approach to the combinatorial bandit setting by introducing MIN-COMB-UCB, and we provide corresponding instance dependent and instance independent regret bounds. We illustrate how various factors, such as the biases and the size of offline datasets, affect the utility of offline data in online learning. We discuss several applications and conduct numerical experiments to validate our findings.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Models That Prove Their Own Correctness</title>
<link>https://arxiv.org/abs/2405.15722</link>
<guid>https://arxiv.org/abs/2405.15722</guid>
<content:encoded><![CDATA[
arXiv:2405.15722v4 Announce Type: replace 
Abstract: How can we trust the correctness of a learned model on a particular input of interest? Model accuracy is typically measured on average over a distribution of inputs, giving no guarantee for any fixed input. This paper proposes a theoretically-founded solution to this problem: to train Self-Proving models that prove the correctness of their output to a verification algorithm $V$ via an Interactive Proof. Self-Proving models satisfy that, with high probability over an input sampled from a given distribution, the model generates a correct output and successfully proves its correctness to $V$. The soundness property of $V$ guarantees that, for every input, no model can convince $V$ of the correctness of an incorrect output. Thus, a Self-Proving model proves correctness of most of its outputs, while all incorrect outputs (of any model) are detected by $V$. We devise and analyze two generic methods for learning Self-Proving models: Transcript Learning (TL) which relies on access to transcripts of accepting interactions, and Reinforcement Learning from Verifier Feedback (RLVF) which trains a model by emulating interactions with the verifier.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PILA: Physics-Informed Low Rank Augmentation for Interpretable Earth Observation</title>
<link>https://arxiv.org/abs/2405.18953</link>
<guid>https://arxiv.org/abs/2405.18953</guid>
<content:encoded><![CDATA[
arXiv:2405.18953v2 Announce Type: replace 
Abstract: Physically meaningful representations are essential for Earth Observation (EO), yet existing physical models are often simplified and incomplete. This leads to discrepancies between simulation and observations that hinder reliable forward model inversion. Common approaches to EO inversion either ignored this incompleteness or relied on case-specific preprocessing. More recent methods use physics-informed autoencoders but depend on auxiliary variables that are difficult to interpret and multiple regularizers that are difficult to balance. We propose Physics-Informed Low-Rank Augmentation (PILA), a framework that augments incomplete physical models using a learnable low-rank residual to improve flexibility, while remaining close to the governing physics.
  We evaluate PILA on two EO inverse problems involving diverse physical processes: forest radiative transfer inversion from optical remote sensing; and volcanic deformation inversion from Global Navigation Satellite Systems (GNSS) displacement data. Across different domains, PILA yields more accurate and interpretable physical variables. For forest spectral inversion, it improves the separation of tree species and, compared to ground measurements, reduces prediction errors by 40-71\% relative to the state-of-the-art. For volcanic deformation, PILA's recovery of variables captures a major inflation event at the Akutan volcano in 2008, and estimates source depth, volume change, and displacement patterns that are consistent with prior studies that however required substantial additional preprocessing. Finally, we analyse the effects of model rank, observability, and physical priors, and suggest that PILA may offer an effective general pathway for inverting incomplete physical models even beyond the domain of Earth Observation. The code is available at https://github.com/yihshe/PILA.git.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bandits with Preference Feedback: A Stackelberg Game Perspective</title>
<link>https://arxiv.org/abs/2406.16745</link>
<guid>https://arxiv.org/abs/2406.16745</guid>
<content:encoded><![CDATA[
arXiv:2406.16745v3 Announce Type: replace 
Abstract: Bandits with preference feedback present a powerful tool for optimizing unknown target functions when only pairwise comparisons are allowed instead of direct value queries. This model allows for incorporating human feedback into online inference and optimization and has been employed in systems for fine-tuning large language models. The problem is well understood in simplified settings with linear target functions or over finite small domains that limit practical interest. Taking the next step, we consider infinite domains and nonlinear (kernelized) rewards. In this setting, selecting a pair of actions is quite challenging and requires balancing exploration and exploitation at two levels: within the pair, and along the iterations of the algorithm. We propose MAXMINLCB, which emulates this trade-off as a zero-sum Stackelberg game, and chooses action pairs that are informative and yield favorable rewards. MAXMINLCB consistently outperforms existing algorithms and satisfies an anytime-valid rate-optimal regret guarantee. This is due to our novel preference-based confidence sequences for kernelized logistic estimators.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DyG-Mamba: Continuous State Space Modeling on Dynamic Graphs</title>
<link>https://arxiv.org/abs/2408.06966</link>
<guid>https://arxiv.org/abs/2408.06966</guid>
<content:encoded><![CDATA[
arXiv:2408.06966v2 Announce Type: replace 
Abstract: Dynamic graph modeling aims to uncover evolutionary patterns in real-world systems, enabling accurate social recommendation and early detection of cancer cells. Inspired by the success of recent state space models in efficiently capturing long-term dependencies, we propose DyG-Mamba by translating dynamic graph modeling into a long-term sequence modeling problem. Specifically, inspired by Ebbinghaus' forgetting curve, we treat the irregular timespans between events as control signals, allowing DyG-Mamba to dynamically adjust the forgetting of historical information. This mechanism ensures effective usage of irregular timespans, thereby improving both model effectiveness and inductive capability. In addition, inspired by Ebbinghaus' review cycle, we redefine core parameters to ensure that DyG-Mamba selectively reviews historical information and filters out noisy inputs, further enhancing the model's robustness. Through exhaustive experiments on 12 datasets covering dynamic link prediction and node classification tasks, we show that DyG-Mamba achieves state-of-the-art performance on most datasets, while demonstrating significantly improved computational and memory efficiency. Code is available at https://github.com/Clearloveyuan/DyG-Mamba.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised discovery of the shared and private geometry in multi-view data</title>
<link>https://arxiv.org/abs/2408.12091</link>
<guid>https://arxiv.org/abs/2408.12091</guid>
<content:encoded><![CDATA[
arXiv:2408.12091v3 Announce Type: replace 
Abstract: Studying complex real-world phenomena often involves data from multiple views (e.g. sensor modalities or brain regions), each capturing different aspects of the underlying system. Within neuroscience, there is growing interest in large-scale simultaneous recordings across multiple brain regions. Understanding the relationship between views (e.g., the neural activity in each region recorded) can reveal fundamental insights into each view and the system as a whole. However, existing methods to characterize such relationships lack the expressivity required to capture nonlinear relationships, describe only shared sources of variance, or discard geometric information that is crucial to drawing insights from data. Here, we present SPLICE: a neural network-based method that infers disentangled, interpretable representations of private and shared latent variables from paired samples of high-dimensional views. Compared to competing methods, we demonstrate that SPLICE 1) disentangles shared and private representations more effectively, 2) yields more interpretable representations by preserving geometry, and 3) is more robust to incorrect a priori estimates of latent dimensionality. We propose our approach as a general-purpose method for finding succinct and interpretable descriptions of paired data sets in terms of disentangled shared and private latent variables.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review</title>
<link>https://arxiv.org/abs/2408.14491</link>
<guid>https://arxiv.org/abs/2408.14491</guid>
<content:encoded><![CDATA[
arXiv:2408.14491v2 Announce Type: replace 
Abstract: Recent technological advancements in multimodal machine learning--including the rise of large language models (LLMs)--have improved our ability to collect, process, and analyze diverse multimodal data such as speech, video, and eye gaze in learning and training contexts. While prior reviews have addressed individual components of the multimodal pipeline (e.g., conceptual models, data fusion), a comprehensive review of empirical methods in applied multimodal environments remains notably absent. This review addresses that, introducing a taxonomy and framework that capture both established practices and recent innovations driven by LLMs and generative AI. We identify five modality groups: Natural Language, Vision, Physiological Signals, Human-Centered Evidence, and Environment Logs. Our analysis reveals that integrating modalities enables richer insights into learner and trainee behaviors, revealing latent patterns often overlooked by unimodal approaches. However, persistent challenges in multimodal data collection and integration continue to hinder the adoption of these systems in real-time classroom settings.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ensembles provably learn equivariance through data augmentation</title>
<link>https://arxiv.org/abs/2410.01452</link>
<guid>https://arxiv.org/abs/2410.01452</guid>
<content:encoded><![CDATA[
arXiv:2410.01452v2 Announce Type: replace 
Abstract: Recently, it was proved that group equivariance emerges in ensembles of neural networks as the result of full augmentation in the limit of infinitely wide neural networks (neural tangent kernel limit). In this paper, we extend this result significantly. We provide a proof that this emergence does not depend on the neural tangent kernel limit at all. We also consider stochastic settings, and furthermore general architectures. For the latter, we provide a simple sufficient condition on the relation between the architecture and the action of the group for our results to hold. We validate our findings through simple numeric experiments.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Logits to Hierarchies: Hierarchical Clustering made Simple</title>
<link>https://arxiv.org/abs/2410.07858</link>
<guid>https://arxiv.org/abs/2410.07858</guid>
<content:encoded><![CDATA[
arXiv:2410.07858v2 Announce Type: replace 
Abstract: The hierarchical structure inherent in many real-world datasets makes the modeling of such hierarchies a crucial objective in both unsupervised and supervised machine learning. While recent advancements have introduced deep architectures specifically designed for hierarchical clustering, we adopt a critical perspective on this line of research. Our findings reveal that these methods face significant limitations in scalability and performance when applied to realistic datasets. Given these findings, we present an alternative approach and introduce a lightweight method that builds on pre-trained non-hierarchical clustering models. Remarkably, our approach outperforms specialized deep models for hierarchical clustering, and it is broadly applicable to any pre-trained clustering model that outputs logits, without requiring any fine-tuning. To highlight the generality of our approach, we extend its application to a supervised setting, demonstrating its ability to recover meaningful hierarchies from a pre-trained ImageNet classifier. Our results establish a practical and effective alternative to existing deep hierarchical clustering methods, with significant advantages in efficiency, scalability and performance.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provable optimal transport with transformers: The essence of depth and prompt engineering</title>
<link>https://arxiv.org/abs/2410.19931</link>
<guid>https://arxiv.org/abs/2410.19931</guid>
<content:encoded><![CDATA[
arXiv:2410.19931v3 Announce Type: replace 
Abstract: Despite their empirical success, the internal mechanism by which transformer models align tokens during language processing remains poorly understood. This paper provides a mechanistic and theoretical explanation of token alignment in LLMs. We first present empirical evidences showing that, in machine translation, attention weights progressively align translated word pairs across layers, closely approximating Optimal Transport (OT) between word embeddings. Building on this observation, we prove that softmax self-attention layers can simulate gradient descent on the dual of the entropy-regularized OT problem, providing a theoretical foundation for the alignment. Our analysis yields a constructive convergence bound showing that transformer depth controls OT approximation accuracy. A direct implication is that standard transformers can sort lists of varying lengths without any parameter adjustment, up to an error term vanishing with transformers depth.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Laws for Black box Adversarial Attacks</title>
<link>https://arxiv.org/abs/2411.16782</link>
<guid>https://arxiv.org/abs/2411.16782</guid>
<content:encoded><![CDATA[
arXiv:2411.16782v4 Announce Type: replace 
Abstract: Adversarial examples exhibit cross-model transferability, enabling threatening black-box attacks on commercial models. Model ensembling, which attacks multiple surrogate models, is a known strategy to improve this transferability. However, prior studies typically use small, fixed ensembles, which leaves open an intriguing question of whether scaling the number of surrogate models can further improve black-box attacks. In this work, we conduct the first large-scale empirical study of this question. We show that by resolving gradient conflict with advanced optimizers, we discover a robust and universal log-linear scaling law through both theoretical analysis and empirical evaluations: the Attack Success Rate (ASR) scales linearly with the logarithm of the ensemble size $T$. We rigorously verify this law across standard classifiers, SOTA defenses, and MLLMs, and find that scaling distills robust, semantic features of the target class. Consequently, we apply this fundamental insight to benchmark SOTA MLLMs. This reveals both the attack's devastating power and a clear robustness hierarchy: we achieve 80\%+ transfer attack success rate on proprietary models like GPT-4o, while also highlighting the exceptional resilience of Claude-3.5-Sonnet. Our findings urge a shift in focus for robustness evaluation: from designing intricate algorithms on small ensembles to understanding the principled and powerful threat of scaling.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Proactive Model Adaptation Against Concept Drift for Online Time Series Forecasting</title>
<link>https://arxiv.org/abs/2412.08435</link>
<guid>https://arxiv.org/abs/2412.08435</guid>
<content:encoded><![CDATA[
arXiv:2412.08435v5 Announce Type: replace 
Abstract: Time series forecasting always faces the challenge of concept drift, where data distributions evolve over time, leading to a decline in forecast model performance. Existing solutions are based on online learning, which continually organize recent time series observations as new training samples and update model parameters according to the forecasting feedback on recent data. However, they overlook a critical issue: obtaining ground-truth future values of each sample should be delayed until after the forecast horizon. This delay creates a temporal gap between the training samples and the test sample. Our empirical analysis reveals that the gap can introduce concept drift, causing forecast models to adapt to outdated concepts. In this paper, we present Proceed, a novel proactive model adaptation framework for online time series forecasting. Proceed first estimates the concept drift between the recently used training samples and the current test sample. It then employs an adaptation generator to efficiently translate the estimated drift into parameter adjustments, proactively adapting the model to the test sample. To enhance the generalization capability of the framework, Proceed is trained on synthetic diverse concept drifts. Extensive experiments on five real-world datasets across various forecast models demonstrate that Proceed brings more performance improvements than the state-of-the-art online learning methods, significantly facilitating forecast models' resilience against concept drifts. Code is available at https://github.com/SJTU-DMTai/OnlineTSF.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Iterative Feature Exclusion Ranking for Deep Tabular Learning</title>
<link>https://arxiv.org/abs/2412.16442</link>
<guid>https://arxiv.org/abs/2412.16442</guid>
<content:encoded><![CDATA[
arXiv:2412.16442v2 Announce Type: replace 
Abstract: Tabular data is a common format for storing information in rows and columns to represent data entries and their features. Although deep neural networks have become the main approach for modeling a wide range of domains including computer vision and NLP, many of them are not well-suited for tabular data. Recently, a few deep learning models have been proposed for deep tabular learning, featuring an internal feature selection mechanism with end-to-end gradient-based optimization. However, their feature selection mechanisms are unidimensional, and hence fail to account for the contextual dependence of feature importance, potentially overlooking crucial interactions that govern complex tasks. In addition, they overlook the bias of high-impact features and the risk associated with the limitations of attention generalization. To address this limitation, this study proposes a novel iterative feature exclusion module that enhances the feature importance ranking in tabular data. The proposed module iteratively excludes each feature from the input data and computes the attention scores, which represent the impact of the features on the prediction. By aggregating the attention scores from each iteration, the proposed module generates a refined representation of feature importance that captures both global and local interactions between features. The effectiveness of the proposed module is evaluated on four public datasets. The results demonstrate that the proposed module consistently outperforms state-of-the-art methods and baseline models in feature ranking and classification tasks. The code is publicly available at https://github.com/abaraka2020/Iterative-Feature-Exclusion-Ranking-Module and https://github.com/mohalim/IFENet
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Modality Collaborative Learning for Sentiment Analysis</title>
<link>https://arxiv.org/abs/2501.12424</link>
<guid>https://arxiv.org/abs/2501.12424</guid>
<content:encoded><![CDATA[
arXiv:2501.12424v2 Announce Type: replace 
Abstract: Multimodal sentiment analysis (MSA) identifies individuals' sentiment states in videos by integrating visual, audio, and text modalities. Despite progress in existing methods, the inherent modality heterogeneity limits the effective capture of interactive sentiment features across modalities. In this paper, by introducing a Multi-Modality Collaborative Learning (MMCL) framework, we facilitate cross-modal interactions and capture enhanced and complementary features from modality-common and modality-specific representations, respectively. Specifically, we design a parameter-free decoupling module and separate uni-modality into modality-common and modality-specific components through semantics assessment of cross-modal elements. For modality-specific representations, inspired by the act-reward mechanism in reinforcement learning, we design policy models to adaptively mine complementary sentiment features under the guidance of a joint reward. For modality-common representations, intra-modal attention is employed to highlight crucial components, playing enhanced roles among modalities. Experimental results, including superiority evaluations on four databases, effectiveness verification of each module, and assessment of complementary features, demonstrate that MMCL successfully learns collaborative features across modalities and significantly improves performance. The code can be available at https://github.com/smwanghhh/MMCL.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Closed-Form Feedback-Free Learning with Forward Projection</title>
<link>https://arxiv.org/abs/2501.16476</link>
<guid>https://arxiv.org/abs/2501.16476</guid>
<content:encoded><![CDATA[
arXiv:2501.16476v4 Announce Type: replace 
Abstract: State-of-the-art backpropagation-free learning methods employ local error feedback to direct iterative optimisation via gradient descent. Here, we examine the more restrictive setting where retrograde communication from neuronal outputs is unavailable for pre-synaptic weight optimisation. We propose Forward Projection (FP), a randomised closed-form training method requiring only a single forward pass over the dataset without retrograde communication. FP generates target values for pre-activation membrane potentials through randomised nonlinear projections of pre-synaptic inputs and labels. Local loss functions are optimised using closed-form regression without feedback from downstream layers. A key advantage is interpretability: membrane potentials in FP-trained networks encode information interpretable layer-wise as label predictions. Across several biomedical datasets, FP achieves generalisation comparable to gradient descent-based local learning methods while requiring only a single forward propagation step, yielding significant training speedup. In few-shot learning tasks, FP produces more generalisable models than backpropagation-optimised alternatives, with local interpretation functions successfully identifying clinically salient diagnostic features.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Zero-Order Federated Finetuning of Language Models for Resource-Constrained Devices</title>
<link>https://arxiv.org/abs/2502.10239</link>
<guid>https://arxiv.org/abs/2502.10239</guid>
<content:encoded><![CDATA[
arXiv:2502.10239v2 Announce Type: replace 
Abstract: Federated fine-tuning offers a promising approach for tuning Large Language Models (LLMs) on edge devices while preserving data privacy. However, fine-tuning these models on edge devices remains challenging due to high memory, communication, and computational demands. Zero-order optimization with task alignment provides a potential solution, enabling fine-tuning with inference-level memory requirements but requires a longer convergence time. In this paper, we propose \ac{METHOD} that divides the network into two blocks, applying a different number of perturbations per block in a computationally effective way, achieving faster convergence. Our evaluation shows a $1.6-3\times$ reduction in computation overhead compared to zero-order state of the art techniques in federated learning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Preference-Guided Diffusion for Multi-Objective Offline Optimization</title>
<link>https://arxiv.org/abs/2503.17299</link>
<guid>https://arxiv.org/abs/2503.17299</guid>
<content:encoded><![CDATA[
arXiv:2503.17299v2 Announce Type: replace 
Abstract: Offline multi-objective optimization aims to identify Pareto-optimal solutions given a dataset of designs and their objective values. In this work, we propose a preference-guided diffusion model that generates Pareto-optimal designs by leveraging a classifier-based guidance mechanism. Our guidance classifier is a preference model trained to predict the probability that one design dominates another, directing the diffusion model toward optimal regions of the design space. Crucially, this preference model generalizes beyond the training distribution, enabling the discovery of Pareto-optimal solutions outside the observed dataset. We introduce a novel diversity-aware preference guidance, augmenting Pareto dominance preference with diversity criteria. This ensures that generated solutions are optimal and well-distributed across the objective space, a capability absent in prior generative methods for offline multi-objective optimization. We evaluate our approach on various continuous offline multi-objective optimization tasks and find that it consistently outperforms other inverse/generative approaches while remaining competitive with forward/ surrogate-based optimization methods. Our results highlight the effectiveness of classifier-guided diffusion models in generating diverse and high-quality solutions that approximate the Pareto front well.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reinforcement Learning Finetunes Small Subnetworks in Large Language Models</title>
<link>https://arxiv.org/abs/2505.11711</link>
<guid>https://arxiv.org/abs/2505.11711</guid>
<content:encoded><![CDATA[
arXiv:2505.11711v2 Announce Type: replace 
Abstract: Reinforcement learning (RL) yields substantial improvements in large language models (LLMs) downstream task performance and alignment with human values. Surprisingly, such large gains result from updating only a small subnetwork comprising just 5 percent to 30 percent of the parameters, with the rest effectively unchanged. We refer to this phenomenon as parameter update sparsity induced by RL. It is observed across all 7 widely used RL algorithms (e.g., PPO, GRPO, DPO) and all 10 LLMs from different families in our experiments. This sparsity is intrinsic and occurs without any explicit sparsity promoting regularizations or architectural constraints. Finetuning the subnetwork alone recovers the test accuracy, and, remarkably, produces a model nearly identical to the one obtained via full finetuning. The subnetworks from different random seeds, training data, and even RL algorithms show substantially greater overlap than expected by chance. Our analysis suggests that this sparsity is not due to updating only a subset of layers, instead, nearly all parameter matrices receive similarly sparse updates. Moreover, the updates to almost all parameter matrices are nearly full-rank, suggesting RL updates a small subset of parameters that nevertheless span almost the full subspaces that the parameter matrices can represent. We conjecture that the this update sparsity can be primarily attributed to training on data that is near the policy distribution, techniques that encourage the policy to remain close to the pretrained model, such as the KL regularization and gradient clipping, have limited impact.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking the Performance Ceiling in Reinforcement Learning requires Inference Strategies</title>
<link>https://arxiv.org/abs/2505.21236</link>
<guid>https://arxiv.org/abs/2505.21236</guid>
<content:encoded><![CDATA[
arXiv:2505.21236v3 Announce Type: replace 
Abstract: Reinforcement learning (RL) systems have countless applications, from energy-grid management to protein design. However, such real-world scenarios are often extremely difficult, combinatorial in nature, and require complex coordination between multiple agents. This level of complexity can cause even state-of-the-art RL systems, trained until convergence, to hit a performance ceiling which they are unable to break out of with zero-shot inference. Meanwhile, many digital or simulation-based applications allow for an inference phase that utilises a specific time and compute budget to explore multiple attempts before outputting a final solution. In this work, we show that such an inference phase employed at execution time, and the choice of a corresponding inference strategy, are key to breaking the performance ceiling observed in complex multi-agent RL problems. Our main result is striking: we can obtain up to a 126% and, on average, a 45% improvement over the previous state-of-the-art across 17 tasks, using only a couple seconds of extra wall-clock time during execution. We also demonstrate promising compute scaling properties, supported by over 60k experiments, making it the largest study on inference strategies for complex RL to date. Our experimental data and code are available at https://sites.google.com/view/inference-strategies-rl.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conflicting Biases at the Edge of Stability: Norm versus Sharpness Regularization</title>
<link>https://arxiv.org/abs/2505.21423</link>
<guid>https://arxiv.org/abs/2505.21423</guid>
<content:encoded><![CDATA[
arXiv:2505.21423v2 Announce Type: replace 
Abstract: A widely believed explanation for the remarkable generalization capacities of overparameterized neural networks is that the optimization algorithms used for training induce an implicit bias towards benign solutions. To grasp this theoretically, recent works examine gradient descent and its variants in simplified training settings, often assuming vanishing learning rates. These studies reveal various forms of implicit regularization, such as $\ell_1$-norm minimizing parameters in regression and max-margin solutions in classification. Concurrently, empirical findings show that moderate to large learning rates exceeding standard stability thresholds lead to faster, albeit oscillatory, convergence in the so-called Edge-of-Stability regime, and induce an implicit bias towards minima of low sharpness (norm of training loss Hessian). In this work, we argue that a comprehensive understanding of the generalization performance of gradient descent requires analyzing the interaction between these various forms of implicit regularization. We empirically demonstrate that the learning rate balances between low parameter norm and low sharpness of the trained model. We furthermore prove for diagonal linear networks trained on a simple regression task that neither implicit bias alone minimizes the generalization error. These findings demonstrate that focusing on a single implicit bias is insufficient to explain good generalization, and they motivate a broader view of implicit regularization that captures the dynamic trade-off between norm and sharpness induced by non-negligible learning rates.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Neural Networks Need Gradient Descent to Generalize? A Theoretical Study</title>
<link>https://arxiv.org/abs/2506.03931</link>
<guid>https://arxiv.org/abs/2506.03931</guid>
<content:encoded><![CDATA[
arXiv:2506.03931v2 Announce Type: replace 
Abstract: Conventional wisdom attributes the mysterious generalization abilities of overparameterized neural networks to gradient descent (and its variants). The recent volume hypothesis challenges this view: it posits that these generalization abilities persist even when gradient descent is replaced by Guess & Check (G&amp;C), i.e., by drawing weight settings until one that fits the training data is found. The validity of the volume hypothesis for wide and deep neural networks remains an open question. In this paper, we theoretically investigate this question for matrix factorization (with linear and non-linear activation)--a common testbed in neural network theory. We first prove that generalization under G&amp;C deteriorates with increasing width, establishing what is, to our knowledge, the first case where G&amp;C is provably inferior to gradient descent. Conversely, we prove that generalization under G&amp;C improves with increasing depth, revealing a stark contrast between wide and deep networks, which we further validate empirically. These findings suggest that even in simple settings, there may not be a simple answer to the question of whether neural networks need gradient descent to generalize well.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mirror Descent Policy Optimisation for Robust Constrained Markov Decision Processes</title>
<link>https://arxiv.org/abs/2506.23165</link>
<guid>https://arxiv.org/abs/2506.23165</guid>
<content:encoded><![CDATA[
arXiv:2506.23165v4 Announce Type: replace 
Abstract: Safety is an essential requirement for reinforcement learning systems. The newly emerging framework of robust constrained Markov decision processes allows learning policies that satisfy long-term constraints while providing guarantees under epistemic uncertainty. This paper presents mirror descent policy optimisation for robust constrained Markov decision processes, making use of policy gradient techniques to optimise both the policy (as a maximiser) and the transition kernel (as an adversarial minimiser) on the Lagrangian representing a constrained Markov decision process. Our proposed algorithm obtains an $\tilde{\mathcal{O}}\left(1/T^{1/3}\right)$ convergence rate in the sample-based robust constrained Markov decision process setting. The paper also contributes an algorithm for approximate gradient descent in the space of transition kernels, which is of independent interest for designing adversarial environments in general Markov decision processes. Experiments confirm the benefits of mirror descent policy optimisation in constrained and unconstrained optimisation, and significant improvements are observed in robustness tests when compared to baseline policy optimisation algorithms.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scientific Machine Learning of Chaotic Systems Discovers Governing Equations for Neural Populations</title>
<link>https://arxiv.org/abs/2507.03631</link>
<guid>https://arxiv.org/abs/2507.03631</guid>
<content:encoded><![CDATA[
arXiv:2507.03631v3 Announce Type: replace 
Abstract: Discovering governing equations that describe complex chaotic systems remains a fundamental challenge in physics and neuroscience. Here, we introduce the PEM-UDE method, which combines the prediction-error method with universal differential equations to extract interpretable mathematical expressions from chaotic dynamical systems, even with limited or noisy observations. This approach succeeds where traditional techniques fail by smoothing optimization landscapes and removing the chaotic properties during the fitting process without distorting optimal parameters. We demonstrate its efficacy by recovering hidden states in the Rossler system and reconstructing dynamics from noise-corrupted electrical-circuit data, in which the correct functional form of the dynamics is recovered even when one of the observed time series is corrupted by noise 5x the magnitude of the true signal. We demonstrate that this method can recover the correct dynamics, whereas direct symbolic regression methods, such as STLSQ, fail to do so with the available data and noise. Importantly, when applied to neural populations, our method derives novel governing equations that respect biological constraints such as network sparsity - a constraint necessary for cortical information processing yet not captured in next-generation neural mass models - while preserving microscale neuronal parameters. These equations predict an emergent relationship between connection density and both oscillation frequency and synchrony in neural circuits. We validate these predictions using three intracranial electrode recording datasets from the medial entorhinal cortex, prefrontal cortex, and orbitofrontal cortex. Our work provides a pathway to develop mechanistic, multi-scale brain models that generalize across diverse neural architectures, bridging the gap between single-neuron dynamics and macroscale brain activity.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Collective Variables for Enhanced Sampling from BioEmu with Time-Lagged Generation</title>
<link>https://arxiv.org/abs/2507.07390</link>
<guid>https://arxiv.org/abs/2507.07390</guid>
<content:encoded><![CDATA[
arXiv:2507.07390v2 Announce Type: replace 
Abstract: Molecular dynamics is crucial for understanding molecular systems but its applicability is often limited by the vast timescales of rare events like protein folding. Enhanced sampling techniques overcome this by accelerating the simulation along key reaction pathways, which are defined by collective variables (CVs). However, identifying effective CVs that capture the slow, macroscopic dynamics of a system remains a major bottleneck. This work proposes a novel framework coined BioEmu-CV that learns these essential CVs automatically from BioEmu, a recently proposed foundation model for generating protein equilibrium samples. In particular, we re-purpose BioEmu to learn time-lagged generation conditioned on the learned CV, i.e., predict the distribution of molecular states after a certain amount of time. This training process promotes the CV to encode only the slow, long-term information while disregarding fast, random fluctuations. We validate our learned CV on fast-folding proteins with two key applications: (1) estimating free energy differences using on-the-fly probability enhanced sampling and (2) sampling transition paths with steered molecular dynamics. Our empirical study also serves as a new systematic and comprehensive benchmark for MLCVs on fast-folding proteins larger than Alanine Dipeptide.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Online Continual Graph Learning</title>
<link>https://arxiv.org/abs/2508.03283</link>
<guid>https://arxiv.org/abs/2508.03283</guid>
<content:encoded><![CDATA[
arXiv:2508.03283v2 Announce Type: replace 
Abstract: Continual Learning (CL) aims to incrementally acquire new knowledge while mitigating catastrophic forgetting. Within this setting, Online Continual Learning (OCL) focuses on updating models promptly and incrementally from single or small batches of observations from a data stream. Extending OCL to graph-structured data is crucial, as many real-world networks evolve over time and require timely, online predictions. However, existing continual or streaming graph learning methods typically assume access to entire graph snapshots or multiple passes over tasks, violating the efficiency constraints of the online setting. To address this gap, we introduce the Online Continual Graph Learning (OCGL) setting, which formalizes node-level continual learning on evolving graphs under strict memory and computational budgets. OCGL defines how a model incrementally processes a stream of node-level information while maintaining anytime inference and respecting resource constraints. We further establish a comprehensive benchmark comprising seven datasets and nine CL strategies, suitably adapted to the OCGL setting, enabling a standardized evaluation setup. Finally, we present a minimalistic yet competitive baseline for OCGL, inspired by our benchmarking results, that achieves strong empirical performance with high efficiency.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trust Me, I Know This Function: Hijacking LLM Static Analysis using Bias</title>
<link>https://arxiv.org/abs/2508.17361</link>
<guid>https://arxiv.org/abs/2508.17361</guid>
<content:encoded><![CDATA[
arXiv:2508.17361v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly trusted to perform automated code review and static analysis at scale, supporting tasks such as vulnerability detection, summarization, and refactoring. In this paper, we identify and exploit a critical vulnerability in LLM-based code analysis: an abstraction bias that causes models to overgeneralize familiar programming patterns and overlook small, meaningful bugs. Adversaries can exploit this blind spot to hijack the control flow of the LLM's interpretation with minimal edits and without affecting actual runtime behavior. We refer to this attack as a Familiar Pattern Attack (FPA).
  We develop a fully automated, black-box algorithm that discovers and injects FPAs into target code. Our evaluation shows that FPAs are not only effective against basic and reasoning models, but are also transferable across model families (OpenAI, Anthropic, Google), and universal across programming languages (Python, C, Rust, Go). Moreover, FPAs remain effective even when models are explicitly warned about the attack via robust system prompts. Finally, we explore positive, defensive uses of FPAs and discuss their broader implications for the reliability and safety of code-oriented LLMs.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ModalSurv: Investigating opportunities and limitations of multimodal deep survival learning in prostate and bladder cancer</title>
<link>https://arxiv.org/abs/2509.05037</link>
<guid>https://arxiv.org/abs/2509.05037</guid>
<content:encoded><![CDATA[
arXiv:2509.05037v5 Announce Type: replace 
Abstract: Accurate survival prediction is essential for personalised cancer treatment. We propose ModalSurv, a multimodal deep survival framework integrating clinical, MRI, histopathology, and RNA-sequencing data via modality-specific projections and cross-attention fusion. On the CHIMERA Grand Challenge datasets, ModalSurv achieved a C-index of 0.7402 (1st) for prostate and 0.5740 (5th) for bladder cancer. Notably, clinical features alone outperformed multimodal models on external tests, highlighting challenges of limited multimodal alignment and potential overfitting. Local validation showed multimodal gains but limited generalisation. ModalSurv provides a systematic evaluation of multimodal survival modelling, underscoring both its promise and current limitations for scalable, generalisable cancer prognosis.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Failure Modes of Maximum Entropy RLHF</title>
<link>https://arxiv.org/abs/2509.20265</link>
<guid>https://arxiv.org/abs/2509.20265</guid>
<content:encoded><![CDATA[
arXiv:2509.20265v2 Announce Type: replace 
Abstract: In this paper, we show that Simple Preference Optimization (SimPO) can be derived as Maximum Entropy Reinforcement Learning, providing a theoretical foundation for this reference-free method. Motivated by SimPO's strong performance in offline preference optimization, we investigate whether Maximum Entropy RL can achieve similar results in online RLHF settings. Our experiments find that Maximum Entropy RL consistently exhibits overoptimization and unstable KL dynamics, even at very low learning rates. Unlike KL-constrained methods that maintain stable training, entropy regularization fails to prevent reward hacking and appears to correlate with overoptimization. Lastly, we discuss possible explanations for why SimPO succeeds in offline settings while Maximum Entropy RL struggles in online scenarios. Our findings suggest that reference-free approaches may face distinct challenges when applied to online or offline preference learning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncovering Alzheimer's Disease Progression via SDE-based Spatio-Temporal Graph Deep Learning on Longitudinal Brain Networks</title>
<link>https://arxiv.org/abs/2509.21735</link>
<guid>https://arxiv.org/abs/2509.21735</guid>
<content:encoded><![CDATA[
arXiv:2509.21735v2 Announce Type: replace 
Abstract: Identifying objective neuroimaging biomarkers to forecast Alzheimer's disease (AD) progression is crucial for timely intervention. However, this task remains challenging due to the complex dysfunctions in the spatio-temporal characteristics of underlying brain networks, which are often overlooked by existing methods. To address these limitations, we develop an interpretable spatio-temporal graph neural network framework to predict future AD progression, leveraging dual Stochastic Differential Equations (SDEs) to model the irregularly-sampled longitudinal functional magnetic resonance imaging (fMRI) data. We validate our approach on two independent cohorts, including the Open Access Series of Imaging Studies (OASIS-3) and the Alzheimer's Disease Neuroimaging Initiative (ADNI). Our framework effectively learns sparse regional and connective importance probabilities, enabling the identification of key brain circuit abnormalities associated with disease progression. Notably, we detect the parahippocampal cortex, prefrontal cortex, and parietal lobule as salient regions, with significant disruptions in the ventral attention, dorsal attention, and default mode networks. These abnormalities correlate strongly with longitudinal AD-related clinical symptoms. Moreover, our interpretability strategy reveals both established and novel neural systems-level and sex-specific biomarkers, offering new insights into the neurobiological mechanisms underlying AD progression. Our findings highlight the potential of spatio-temporal graph-based learning for early, individualized prediction of AD progression, even in the context of irregularly-sampled longitudinal imaging data.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AuON: A Linear-time Alternative to Orthogonal Momentum Updates</title>
<link>https://arxiv.org/abs/2509.24320</link>
<guid>https://arxiv.org/abs/2509.24320</guid>
<content:encoded><![CDATA[
arXiv:2509.24320v4 Announce Type: replace 
Abstract: Orthogonal momentum gradient updates have emerged to overcome the limitations of vector-based optimizers like Adam. The vector-based optimizer Adam suffers from high memory costs and ill-conditioned momentum gradient updates. However, traditional Orthogonal momentum approaches, such as SVD/QR decomposition, suffer from high computational and memory costs and underperform compared to well-tuned SGD with momentum. Recent advances, such as Muon, improve efficiency by applying momentum before orthogonalization and approximate orthogonal matrices via Newton-Schulz iterations, which gives better GPU utilization, active high TFLOPS, and reduces memory usage by up to 3x. Nevertheless, Muon(Vanilla) suffers from exploding attention logits and has cubic computation complexity. In this paper, we deep dive into orthogonal momentum gradient updates to find the main properties that help Muon achieve remarkable performance. We propose AuON (Alternative Unit-norm momentum updates by Normalized nonlinear scaling), a linear-time optimizer that achieves strong performance without approximate orthogonal matrices, while preserving structural alignment and reconditioning ill-posed updates. AuON has an automatic "emergency brake" to handle exploding attention logits. We further introduce a hybrid variant, Hybrid-AuON, that applies the linear transformations with Newton-Schulz iterations, which outperforms Muon in the language modeling tasks. Code is available at: https://github.com/ryyzn9/AuON
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improved High-probability Convergence Guarantees of Decentralized SGD</title>
<link>https://arxiv.org/abs/2510.06141</link>
<guid>https://arxiv.org/abs/2510.06141</guid>
<content:encoded><![CDATA[
arXiv:2510.06141v2 Announce Type: replace 
Abstract: Convergence in high-probability (HP) has been receiving increasing interest, due to its attractive properties, such as exponentially decaying tail bounds and strong guarantees for each individual run of an algorithm. While HP guarantees are extensively studied in centralized settings, much less is understood in the decentralized, networked setup. Existing HP studies in decentralized settings impose strong assumptions, like uniformly bounded gradients, or asymptotically vanishing noise, resulting in a significant gap between assumptions used to establish convergence in the HP and the mean-squared error (MSE) sense, even for vanilla Decentralized Stochastic Gradient Descent ($\mathtt{DSGD}$) algorithm. This is contrary to centralized settings, where it is known that $\mathtt{SGD}$ converges in HP under the same conditions on the cost function as needed to guarantee MSE convergence. Motivated by this observation, we revisit HP guarantees for $\mathtt{DSGD}$ in the presence of light-tailed noise. We show that $\mathtt{DSGD}$ converges in HP under the same conditions on the cost as in the MSE sense, removing uniformly bounded gradients and other restrictive assumptions, while simultaneously achieving order-optimal rates for both non-convex and strongly convex costs. Moreover, our improved analysis yields linear speed-up in the number of users, demonstrating that $\mathtt{DSGD}$ maintains strong performance in the HP sense and matches existing MSE guarantees. Our improved results stem from a careful analysis of the MGF of quantities of interest (norm-squared of gradient or optimality gap) and the MGF of the consensus gap between users' models. To achieve linear speed-up, we provide a novel result on the variance-reduction effect of decentralized methods in the HP sense and more fine-grained bounds on the MGF for strongly convex costs, which are both of independent interest.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UAMDP: Uncertainty-Aware Markov Decision Process for Risk-Constrained Reinforcement Learning from Probabilistic Forecasts</title>
<link>https://arxiv.org/abs/2510.08226</link>
<guid>https://arxiv.org/abs/2510.08226</guid>
<content:encoded><![CDATA[
arXiv:2510.08226v2 Announce Type: replace 
Abstract: Sequential decisions in volatile, high-stakes settings require more than maximizing expected return; they require principled uncertainty management. This paper presents the Uncertainty-Aware Markov Decision Process (UAMDP), a unified framework that couples Bayesian forecasting, posterior-sampling reinforcement learning, and planning under a conditional value-at-risk (CVaR) constraint. In a closed loop, the agent updates its beliefs over latent dynamics, samples plausible futures via Thompson sampling, and optimizes policies subject to preset risk tolerances. We establish regret bounds that converge to the Bayes-optimal benchmark under standard regularity conditions. We evaluate UAMDP in two domains including high-frequency equity trading and retail inventory control, both marked by structural uncertainty and economic volatility. Relative to strong deep learning baselines, UAMDP improves long-horizon forecasting accuracy (RMSE decreases by up to 25% and sMAPE by 32%), and these gains translate into economic performance: the trading Sharpe ratio rises from 1.54 to 1.74 while maximum drawdown is roughly halved. These results show that integrating calibrated probabilistic modeling, exploration aligned with posterior uncertainty, and risk-aware control yields a robust, generalizable approach to safer and more profitable sequential decision-making.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reverse Supervision at Scale: Exponential Search Meets the Economics of Annotation</title>
<link>https://arxiv.org/abs/2510.10446</link>
<guid>https://arxiv.org/abs/2510.10446</guid>
<content:encoded><![CDATA[
arXiv:2510.10446v2 Announce Type: replace 
Abstract: We analyze a reversed-supervision strategy that searches over labelings of a large unlabeled set \(B\) to minimize error on a small labeled set \(A\). The search space is \(2^n\), and the resulting complexity remains exponential even under large constant-factor speedups (e.g., quantum or massively parallel hardware). Consequently, arbitrarily fast -- but not exponentially faster -- computation does not obviate the need for informative labels or priors. In practice, the machine learning pipeline still requires an initial human contribution: specifying the objective, defining classes, and providing a seed set of representative annotations that inject inductive bias and align models with task semantics. Synthetic labels from generative AI can partially substitute provided their quality is human-grade and anchored by a human-specified objective, seed supervision, and validation. In this view, generative models function as \emph{label amplifiers}, leveraging small human-curated cores via active, semi-supervised, and self-training loops, while humans retain oversight for calibration, drift detection, and failure auditing. Thus, extreme computational speed reduces wall-clock time but not the fundamental supervision needs of learning; initial human (or human-grade) input remains necessary to ground the system in the intended task.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>nanoTabPFN: A Lightweight and Educational Reimplementation of TabPFN</title>
<link>https://arxiv.org/abs/2511.03634</link>
<guid>https://arxiv.org/abs/2511.03634</guid>
<content:encoded><![CDATA[
arXiv:2511.03634v2 Announce Type: replace 
Abstract: Tabular foundation models such as TabPFN have revolutionized predictive machine learning for tabular data. At the same time, the driving factors of this revolution are hard to understand. Existing open-source tabular foundation models are implemented in complicated pipelines boasting over 10,000 lines of code, lack architecture documentation or code quality. In short, the implementations are hard to understand, not beginner-friendly, and complicated to adapt for new experiments. We introduce nanoTabPFN, a simplified and lightweight implementation of the TabPFN v2 architecture and a corresponding training loop that uses pre-generated training data. nanoTabPFN makes tabular foundation models more accessible to students and researchers alike. For example, restricted to a small data setting it achieves a performance comparable to traditional machine learning baselines within one minute of pre-training on a single GPU (160,000x faster than TabPFN v2 pretraining). This eliminated requirement of large computational resources makes pre-training tabular foundation models accessible for educational purposes. Our code is available at https://github.com/automl/nanoTabPFN.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Alternative Fairness and Accuracy Optimization in Criminal Justice</title>
<link>https://arxiv.org/abs/2511.04505</link>
<guid>https://arxiv.org/abs/2511.04505</guid>
<content:encoded><![CDATA[
arXiv:2511.04505v4 Announce Type: replace 
Abstract: Algorithmic fairness has grown rapidly as a research area, yet key concepts remain unsettled, especially in criminal justice. We review group, individual, and process fairness and map the conditions under which they conflict. We then develop a simple modification to standard group fairness. Rather than exact parity across protected groups, we minimize a weighted error loss while keeping differences in false negative rates within a small tolerance. This makes solutions easier to find, can raise predictive accuracy, and surfaces the ethical choice of error costs. We situate this proposal within three classes of critique: biased and incomplete data, latent affirmative action, and the explosion of subgroup constraints. Finally, we offer a practical framework for deployment in public decision systems built on three pillars: need-based decisions, Transparency and accountability, and narrowly tailored definitions and solutions. Together, these elements link technical design to legitimacy and provide actionable guidance for agencies that use risk assessment and related tools.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forgetting is Everywhere</title>
<link>https://arxiv.org/abs/2511.04666</link>
<guid>https://arxiv.org/abs/2511.04666</guid>
<content:encoded><![CDATA[
arXiv:2511.04666v2 Announce Type: replace 
Abstract: A fundamental challenge in developing general learning algorithms is their tendency to forget past knowledge when adapting to new data. Addressing this problem requires a principled understanding of forgetting; yet, despite decades of study, no unified definition has emerged that provides insights into the underlying dynamics of learning. We propose an algorithm- and task-agnostic theory that characterises forgetting as a lack of self-consistency in a learner's predictive distribution over future experiences, manifesting as a loss of predictive information. Our theory naturally yields a general measure of an algorithm's propensity to forget and shows that Bayesian learners are capable of adapting without forgetting. To validate the theory, we design a comprehensive set of experiments that span classification, regression, generative modelling, and reinforcement learning. We empirically demonstrate how forgetting is present across all deep learning settings and plays a significant role in determining learning efficiency. Together, these results establish a principled understanding of forgetting and lay the foundation for analysing and improving the information retention capabilities of general learning algorithms.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EnviSAgE: A Survey of Environment Scaling for Qualitative Agentic Experience Collection</title>
<link>https://arxiv.org/abs/2511.09586</link>
<guid>https://arxiv.org/abs/2511.09586</guid>
<content:encoded><![CDATA[
arXiv:2511.09586v2 Announce Type: replace 
Abstract: LLM-based agents can autonomously accomplish complex tasks across various domains. However, to further cultivate capabilities such as adaptive behavior and long-term decision-making, training on static datasets built from human-level knowledge is insufficient. These datasets are costly to construct and lack both dynamism and realism. A growing consensus is that agents should instead interact directly with environments and learn from experience through reinforcement learning. We formalize this iterative process as the Generation-Execution-Feedback (GEF) loop, where environments generate tasks to challenge agents, return observations in response to agents' actions during task execution, and provide evaluative feedback on rollouts for subsequent learning. Under this paradigm, environments function as indispensable producers of experiential data, highlighting the need to scale them toward greater complexity, realism, and interactivity. In this survey, we systematically review representative methods for environment scaling from a pioneering environment-centric perspective and organize them along the stages of the GEF loop, namely task generation, task execution, and feedback. We further analyze implementation frameworks, challenges, and applications, consolidating fragmented advances and outlining future research directions for agent intelligence.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Schedule Optimization for Fast and Robust Diffusion Model Sampling</title>
<link>https://arxiv.org/abs/2511.11688</link>
<guid>https://arxiv.org/abs/2511.11688</guid>
<content:encoded><![CDATA[
arXiv:2511.11688v2 Announce Type: replace 
Abstract: Diffusion probabilistic models have set a new standard for generative fidelity but are hindered by a slow iterative sampling process. A powerful training-free strategy to accelerate this process is Schedule Optimization, which aims to find an optimal distribution of timesteps for a fixed and small Number of Function Evaluations (NFE) to maximize sample quality. To this end, a successful schedule optimization method must adhere to four core principles: effectiveness, adaptivity, practical robustness, and computational efficiency. However, existing paradigms struggle to satisfy these principles simultaneously, motivating the need for a more advanced solution. To overcome these limitations, we propose the Hierarchical-Schedule-Optimizer (HSO), a novel and efficient bi-level optimization framework. HSO reframes the search for a globally optimal schedule into a more tractable problem by iteratively alternating between two synergistic levels: an upper-level global search for an optimal initialization strategy and a lower-level local optimization for schedule refinement. This process is guided by two key innovations: the Midpoint Error Proxy (MEP), a solver-agnostic and numerically stable objective for effective local optimization, and the Spacing-Penalized Fitness (SPF) function, which ensures practical robustness by penalizing pathologically close timesteps. Extensive experiments show that HSO sets a new state-of-the-art for training-free sampling in the extremely low-NFE regime. For instance, with an NFE of just 5, HSO achieves a remarkable FID of 11.94 on LAION-Aesthetics with Stable Diffusion v2.1. Crucially, this level of performance is attained not through costly retraining, but with a one-time optimization cost of less than 8 seconds, presenting a highly practical and efficient paradigm for diffusion model acceleration.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tuning for Two Adversaries: Enhancing the Robustness Against Transfer and Query-Based Attacks using Hyperparameter Tuning</title>
<link>https://arxiv.org/abs/2511.13654</link>
<guid>https://arxiv.org/abs/2511.13654</guid>
<content:encoded><![CDATA[
arXiv:2511.13654v2 Announce Type: replace 
Abstract: In this paper, we present the first detailed analysis of how training hyperparameters -- such as learning rate, weight decay, momentum, and batch size -- influence robustness against both transfer-based and query-based attacks. Supported by theory and experiments, our study spans a variety of practical deployment settings, including centralized training, ensemble learning, and distributed training. We uncover a striking dichotomy: for transfer-based attacks, decreasing the learning rate significantly enhances robustness by up to $64\%$. In contrast, for query-based attacks, increasing the learning rate consistently leads to improved robustness by up to $28\%$ across various settings and data distributions. Leveraging these findings, we explore -- for the first time -- the training hyperparameter space to jointly enhance robustness against both transfer-based and query-based attacks. Our results reveal that distributed models benefit the most from hyperparameter tuning, achieving a remarkable tradeoff by simultaneously mitigating both attack types more effectively than other training setups.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intervention Efficiency and Perturbation Validation Framework: Capacity-Aware and Robust Clinical Model Selection under the Rashomon Effect</title>
<link>https://arxiv.org/abs/2511.14317</link>
<guid>https://arxiv.org/abs/2511.14317</guid>
<content:encoded><![CDATA[
arXiv:2511.14317v5 Announce Type: replace 
Abstract: In clinical machine learning, the coexistence of multiple models with comparable performance -- a manifestation of the Rashomon Effect -- poses fundamental challenges for trustworthy deployment and evaluation. Small, imbalanced, and noisy datasets, coupled with high-dimensional and weakly identified clinical features, amplify this multiplicity and make conventional validation schemes unreliable. As a result, selecting among equally performing models becomes uncertain, particularly when resource constraints and operational priorities are not considered by conventional metrics like F1 score. To address these issues, we propose two complementary tools for robust model assessment and selection: Intervention Efficiency (IE) and the Perturbation Validation Framework (PVF). IE is a capacity-aware metric that quantifies how efficiently a model identifies actionable true positives when only limited interventions are feasible, thereby linking predictive performance with clinical utility. PVF introduces a structured approach to assess the stability of models under data perturbations, identifying models whose performance remains most invariant across noisy or shifted validation sets. Empirical results on synthetic and real-world healthcare datasets show that using these tools facilitates the selection of models that generalize more robustly and align with capacity constraints, offering a new direction for tackling the Rashomon Effect in clinical settings.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OceanForecastBench: A Benchmark Dataset for Data-Driven Global Ocean Forecasting</title>
<link>https://arxiv.org/abs/2511.18732</link>
<guid>https://arxiv.org/abs/2511.18732</guid>
<content:encoded><![CDATA[
arXiv:2511.18732v2 Announce Type: replace 
Abstract: Global ocean forecasting aims to predict key ocean variables such as temperature, salinity, and currents, which is essential for understanding and describing oceanic phenomena. In recent years, data-driven deep learning-based ocean forecast models, such as XiHe, WenHai, LangYa and AI-GOMS, have demonstrated significant potential in capturing complex ocean dynamics and improving forecasting efficiency. Despite these advancements, the absence of open-source, standardized benchmarks has led to inconsistent data usage and evaluation methods. This gap hinders efficient model development, impedes fair performance comparison, and constrains interdisciplinary collaboration. To address this challenge, we propose OceanForecastBench, a benchmark offering three core contributions: (1) A high-quality global ocean reanalysis data over 28 years for model training, including 4 ocean variables across 23 depth levels and 4 sea surface variables. (2) A high-reliability satellite and in-situ observations for model evaluation, covering approximately 100 million locations in the global ocean. (3) An evaluation pipeline and a comprehensive benchmark with 6 typical baseline models, leveraging observations to evaluate model performance from multiple perspectives. OceanForecastBench represents the most comprehensive benchmarking framework currently available for data-driven ocean forecasting, offering an open-source platform for model development, evaluation, and comparison. The dataset and code are publicly available at: https://github.com/Ocean-Intelligent-Forecasting/OceanForecastBench.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wrist Photoplethysmography Predicts Dietary Information</title>
<link>https://arxiv.org/abs/2511.19260</link>
<guid>https://arxiv.org/abs/2511.19260</guid>
<content:encoded><![CDATA[
arXiv:2511.19260v2 Announce Type: replace 
Abstract: Whether wearable photoplethysmography (PPG) contains dietary information remains unknown. We trained a language model on 1.1M meals to predict meal descriptions from PPG, aligning PPG to text. PPG nontrivially predicts meal content; predictability decreases for PPGs farther from meals. This transfers to dietary tasks: PPG increases AUC by 11% for intake and satiety across held-out and independent cohorts, with gains robust to text degradation. Wearable PPG may enable passive dietary monitoring.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gated KalmaNet: A Fading Memory Layer Through Test-Time Ridge Regression</title>
<link>https://arxiv.org/abs/2511.21016</link>
<guid>https://arxiv.org/abs/2511.21016</guid>
<content:encoded><![CDATA[
arXiv:2511.21016v2 Announce Type: replace 
Abstract: As efficient alternatives to softmax Attention, linear State-Space Models (SSMs) achieve constant memory and linear compute, but maintain only a lossy, fading summary of the past, often leading to inferior performance in recall-oriented tasks. We propose Gated KalmaNet (GKA), a layer that accounts for the full past while maintaining SSM-style efficiency. We ground our approach in the Kalman Filter (KF) framework, which provides a principled solution for optimal inference in dynamical systems. We show that several existing SSM layers (DeltaNet, Gated DeltaNet, and Kimi Delta Attention) are approximations to the KF recurrence that assume identity error covariance, thereby ignoring how past measurements (keys and values) should optimally influence state updates. In contrast, GKA computes the exact Kalman gain by maintaining the full error covariance. Under a steady-state assumption that enables parallelization, this reduces to solving an online ridge regression problem with constant memory and linear compute cost. A critical insight is that standard KF equations are numerically unstable in low-precision environments (like bfloat16) and hard to parallelize on modern hardware. We address this through: (1) adaptive regularization with input-dependent gating to control the condition number of the ridge regression for numerical stability, and (2) Chebyshev Iteration, which we show is more stable than conventional iterative solvers in low-precision settings. We further develop hardware-aware chunk-wise kernels to enable efficient training. Empirically, GKA outperforms existing SSM layers (like Mamba2 and Gated DeltaNet) on short-context tasks and achieves more than 10\% relative improvement on long-context RAG and LongQA tasks up to 128k tokens.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Modalities via Progressive Re-alignment for Multimodal Test-Time Adaptation</title>
<link>https://arxiv.org/abs/2511.22862</link>
<guid>https://arxiv.org/abs/2511.22862</guid>
<content:encoded><![CDATA[
arXiv:2511.22862v2 Announce Type: replace 
Abstract: Test-time adaptation (TTA) enables online model adaptation using only unlabeled test data, aiming to bridge the gap between source and target distributions. However, in multimodal scenarios, varying degrees of distribution shift across different modalities give rise to a complex coupling effect of unimodal shallow feature shift and cross-modal high-level semantic misalignment, posing a major obstacle to extending existing TTA methods to the multimodal field. To address this challenge, we propose a novel multimodal test-time adaptation (MMTTA) framework, termed as Bridging Modalities via Progressive Re-alignment (BriMPR). BriMPR, consisting of two progressively enhanced modules, tackles the coupling effect with a divide-and-conquer strategy. Specifically, we first decompose MMTTA into multiple unimodal feature alignment sub-problems. By leveraging the strong function approximation ability of prompt tuning, we calibrate the unimodal global feature distributions to their respective source distributions, so as to achieve the initial semantic re-alignment across modalities. Subsequently, we assign the credible pseudo-labels to combinations of masked and complete modalities, and introduce inter-modal instance-wise contrastive learning to further enhance the information interaction among modalities and refine the alignment. Extensive experiments on MMTTA tasks, including both corruption-based and real-world domain shift benchmarks, demonstrate the superiority of our method. Our source code is available at https://github.com/Luchicken/BriMPR.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Masked Diffusion for Generative Recommendation</title>
<link>https://arxiv.org/abs/2511.23021</link>
<guid>https://arxiv.org/abs/2511.23021</guid>
<content:encoded><![CDATA[
arXiv:2511.23021v2 Announce Type: replace 
Abstract: Generative recommendation (GR) with semantic IDs (SIDs) has emerged as a promising alternative to traditional recommendation approaches due to its performance gains, capitalization on semantic information provided through language model embeddings, and inference and storage efficiency. Existing GR with SIDs works frame the probability of a sequence of SIDs corresponding to a user's interaction history using autoregressive modeling. While this has led to impressive next item prediction performances in certain settings, these autoregressive GR with SIDs models suffer from expensive inference due to sequential token-wise decoding, potentially inefficient use of training data and bias towards learning short-context relationships among tokens. Inspired by recent breakthroughs in NLP, we propose to instead model and learn the probability of a user's sequence of SIDs using masked diffusion. Masked diffusion employs discrete masking noise to facilitate learning the sequence distribution, and models the probability of masked tokens as conditionally independent given the unmasked tokens, allowing for parallel decoding of the masked tokens. We demonstrate through thorough experiments that our proposed method consistently outperforms autoregressive modeling. This performance gap is especially pronounced in data-constrained settings and in terms of coarse-grained recall, consistent with our intuitions. Moreover, our approach allows the flexibility of predicting multiple SIDs in parallel during inference while maintaining superior performance to autoregressive modeling.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Multimodal Embeddings for Traffic Accident Prediction and Causal Estimation</title>
<link>https://arxiv.org/abs/2512.02920</link>
<guid>https://arxiv.org/abs/2512.02920</guid>
<content:encoded><![CDATA[
arXiv:2512.02920v2 Announce Type: replace 
Abstract: We consider analyzing traffic accident patterns using both road network data and satellite images aligned to road graph nodes. Previous work for predicting accident occurrences relies primarily on road network structural features while overlooking physical and environmental information from the road surface and its surroundings. In this work, we construct a large multimodal dataset across six U.S. states, containing nine million traffic accident records from official sources, and one million high-resolution satellite images for each node of the road network. Additionally, every node is annotated with features such as the region's weather statistics and road type (e.g., residential vs. motorway), and each edge is annotated with traffic volume information (i.e., Average Annual Daily Traffic). Utilizing this dataset, we conduct a comprehensive evaluation of multimodal learning methods that integrate both visual and network embeddings. Our findings show that integrating both data modalities improves prediction accuracy, achieving an average AUROC of $90.1\%$, which is a $3.7\%$ gain over graph neural network models that only utilize graph structures. With the improved embeddings, we conduct a causal analysis based on a matching estimator to estimate the key contributing factors influencing traffic accidents. We find that accident rates rise by $24\%$ under higher precipitation, by $22\%$ on higher-speed roads such as motorways, and by $29\%$ due to seasonal patterns, after adjusting for other confounding factors. Ablation studies confirm that satellite imagery features are essential for achieving accurate prediction.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Developing Distance-Aware, and Evident Uncertainty Quantification in Dynamic Physics-Constrained Neural Networks for Robust Bearing Degradation Estimation</title>
<link>https://arxiv.org/abs/2512.08499</link>
<guid>https://arxiv.org/abs/2512.08499</guid>
<content:encoded><![CDATA[
arXiv:2512.08499v2 Announce Type: replace 
Abstract: Accurate and uncertainty-aware degradation estimation is essential for predictive maintenance in safety-critical systems like rotating machinery with rolling-element bearings. Many existing uncertainty methods lack confidence calibration, are costly to run, are not distance-aware, and fail to generalize under out-of-distribution data. We introduce two distance-aware uncertainty methods for deterministic physics-guided neural networks: PG-SNGP, based on Spectral Normalization Gaussian Process, and PG-SNER, based on Deep Evidential Regression. We apply spectral normalization to the hidden layers so the network preserves distances from input to latent space. PG-SNGP replaces the final dense layer with a Gaussian Process layer for distance-sensitive uncertainty, while PG-SNER outputs Normal Inverse Gamma parameters to model uncertainty in a coherent probabilistic form. We assess performance using standard accuracy metrics and a new distance-aware metric based on the Pearson Correlation Coefficient, which measures how well predicted uncertainty tracks the distance between test and training samples. We also design a dynamic weighting scheme in the loss to balance data fidelity and physical consistency. We test our methods on rolling-element bearing degradation using the PRONOSTIA, XJTU-SY and HUST datasets and compare them with Monte Carlo and Deep Ensemble PGNNs. Results show that PG-SNGP and PG-SNER improve prediction accuracy, generalize reliably under OOD conditions, and remain robust to adversarial attacks and noise.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DCFO: Density-Based Counterfactuals for Outliers - Additional Material</title>
<link>https://arxiv.org/abs/2512.10659</link>
<guid>https://arxiv.org/abs/2512.10659</guid>
<content:encoded><![CDATA[
arXiv:2512.10659v2 Announce Type: replace 
Abstract: Outlier detection identifies data points that significantly deviate from the majority of the data distribution. Explaining outliers is crucial for understanding the underlying factors that contribute to their detection, validating their significance, and identifying potential biases or errors. Effective explanations provide actionable insights, facilitating preventive measures to avoid similar outliers in the future. Counterfactual explanations clarify why specific data points are classified as outliers by identifying minimal changes required to alter their prediction. Although valuable, most existing counterfactual explanation methods overlook the unique challenges posed by outlier detection, and fail to target classical, widely adopted outlier detection algorithms. Local Outlier Factor (LOF) is one the most popular unsupervised outlier detection methods, quantifying outlierness through relative local density. Despite LOF's widespread use across diverse applications, it lacks interpretability. To address this limitation, we introduce Density-based Counterfactuals for Outliers (DCFO), a novel method specifically designed to generate counterfactual explanations for LOF. DCFO partitions the data space into regions where LOF behaves smoothly, enabling efficient gradient-based optimisation. Extensive experimental validation on 50 OpenML datasets demonstrates that DCFO consistently outperforms benchmarked competitors, offering superior proximity and validity of generated counterfactuals.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WildFit: Autonomous In-situ Model Adaptation for Resource-Constrained IoT Systems</title>
<link>https://arxiv.org/abs/2409.07796</link>
<guid>https://arxiv.org/abs/2409.07796</guid>
<content:encoded><![CDATA[
arXiv:2409.07796v4 Announce Type: replace-cross 
Abstract: Resource-constrained IoT devices increasingly rely on deep learning models, however, these models experience significant accuracy drops due to domain shifts when encountering variations in lighting, weather, and seasonal conditions. While cloud-based retraining can address this issue, many IoT deployments operate with limited connectivity and energy constraints, making traditional fine-tuning approaches impractical. We explore this challenge through the lens of wildlife ecology, where camera traps must maintain accurate species classification across changing seasons, weather, and habitats without reliable connectivity. We introduce WildFit, an autonomous in-situ adaptation framework that leverages the key insight that background scenes change more frequently than the visual characteristics of monitored species. WildFit combines background-aware synthesis to generate training samples on-device with drift-aware fine-tuning that triggers model updates only when necessary to conserve resources. Our background-aware synthesis surpasses efficient baselines by 7.3% and diffusion models by 3.0% while being orders of magnitude faster, our drift-aware fine-tuning achieves Pareto optimality with 50% fewer updates and 1.5% higher accuracy, and the end-to-end system outperforms domain adaptation approaches by 20-35% while consuming only 11.2 Wh over 37 days-enabling battery-powered deployment.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mat\'ern Kernels for Tunable Implicit Surface Reconstruction</title>
<link>https://arxiv.org/abs/2409.15466</link>
<guid>https://arxiv.org/abs/2409.15466</guid>
<content:encoded><![CDATA[
arXiv:2409.15466v3 Announce Type: replace-cross 
Abstract: We propose to use the family of Mat\'ern kernels for implicit surface reconstruction, building upon the recent success of kernel methods for 3D reconstruction of oriented point clouds. As we show from a theoretical and practical perspective, Mat\'ern kernels have some appealing properties which make them particularly well suited for surface reconstruction -- outperforming state-of-the-art methods based on the arc-cosine kernel while being significantly easier to implement, faster to compute, and scalable. Being stationary, we demonstrate that Mat\'ern kernels allow for tunable surface reconstruction in the same way as Fourier feature mappings help coordinate-based MLPs overcome spectral bias. Moreover, we theoretically analyze Mat\'ern kernels' connection to SIREN networks as well as their relation to previously employed arc-cosine kernels. Finally, based on recently introduced Neural Kernel Fields, we present data-dependent Mat\'ern kernels and conclude that especially the Laplace kernel (being part of the Mat\'ern family) is extremely competitive, performing almost on par with state-of-the-art methods in the noise-free case while having a more than five times shorter training time.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Spatio-Temporal Machine Learning Model for Mortgage Credit Risk: Default Probabilities and Loan Portfolios</title>
<link>https://arxiv.org/abs/2410.02846</link>
<guid>https://arxiv.org/abs/2410.02846</guid>
<content:encoded><![CDATA[
arXiv:2410.02846v3 Announce Type: replace-cross 
Abstract: We introduce a novel machine learning model for credit risk by combining tree-boosting with a latent spatio-temporal Gaussian process model accounting for frailty correlation. This allows for modeling non-linearities and interactions among predictor variables in a flexible data-driven manner and for accounting for spatio-temporal variation that is not explained by observable predictor variables. We also show how estimation and prediction can be done in a computationally efficient manner. In an application to a large U.S. mortgage credit risk data set, we find that both predictive default probabilities for individual loans and predictive loan portfolio loss distributions obtained with our novel approach are more accurate compared to conventional independent linear hazard models and also linear spatio-temporal models. Using interpretability tools for machine learning models, we find that the likely reasons for this outperformance are strong interaction and non-linear effects in the predictor variables and the presence of spatio-temporal frailty effects.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Artificial Intelligence for Microbiology and Microbiome Research</title>
<link>https://arxiv.org/abs/2411.01098</link>
<guid>https://arxiv.org/abs/2411.01098</guid>
<content:encoded><![CDATA[
arXiv:2411.01098v2 Announce Type: replace-cross 
Abstract: Advancements in artificial intelligence (AI) have transformed many scientific fields, with microbiology and microbiome research now experiencing significant breakthroughs through machine learning applications. This review provides a comprehensive overview of AI-driven approaches tailored for microbiology and microbiome studies, emphasizing both technical advancements and biological insights. We begin with an introduction to foundational AI techniques, including primary machine learning paradigms and various deep learning architectures, and offer guidance on choosing between traditional machine learning and sophisticated deep learning methods based on specific research goals. The primary section on application scenarios spans diverse research areas, from taxonomic profiling, functional annotation \& prediction, microbe-X interactions, microbial ecology, metabolic modeling, precision nutrition, clinical microbiology, to prevention \& therapeutics. Finally, we discuss challenges in this field and highlight some recent breakthroughs. Together, this review underscores AI's transformative role in microbiology and microbiome research, paving the way for innovative methodologies and applications that enhance our understanding of microbial life and its impact on our planet and our health.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Asymptotic regularity of a generalised stochastic Halpern scheme</title>
<link>https://arxiv.org/abs/2411.04845</link>
<guid>https://arxiv.org/abs/2411.04845</guid>
<content:encoded><![CDATA[
arXiv:2411.04845v3 Announce Type: replace-cross 
Abstract: We provide abstract, general and highly uniform rates of asymptotic regularity for a generalized stochastic Halpern-style iteration, which incorporates a second mapping in the style of a Krasnoselskii-Mann iteration. This iteration is general in two ways: First, it incorporates stochasticity completely abstractly, rather than fixing a sampling method; second, it includes as special cases stochastic versions of various schemes from the optimization literature, including Halpern's iteration as well as a Krasnoselskii-Mann iteration with Tikhonov regularization terms in the sense of Bo\c{t}, Csetnek and Meier (where this stochastic variant of the latter is considered for the first time in this paper). For these specific cases, we obtain linear rates of asymptotic regularity, matching (or improving) the currently best known rates for these iterations in stochastic optimization, and quadratic rates of asymptotic regularity are obtained in the context of inner product spaces for the general iteration. We conclude by discussing how variance can be managed in practice through sampling methods in the style of minibatching, how our convergence rates can be adapted to provide oracle complexity bounds, and by sketching how the schemes presented here can be instantiated in the context of reinforcement learning to yield novel methods for Q-learning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Memory Backdoor Attacks on Neural Networks</title>
<link>https://arxiv.org/abs/2411.14516</link>
<guid>https://arxiv.org/abs/2411.14516</guid>
<content:encoded><![CDATA[
arXiv:2411.14516v2 Announce Type: replace-cross 
Abstract: Neural networks are often trained on proprietary datasets, making them attractive attack targets. We present a novel dataset extraction method leveraging an innovative training time backdoor attack, allowing a malicious federated learning server to systematically and deterministically extract complete client training samples through a simple indexing process. Unlike prior techniques, our approach guarantees exact data recovery rather than probabilistic reconstructions or hallucinations, provides precise control over which samples are memorized and how many, and shows high capacity and robustness. Infected models output data samples when they receive a patternbased index trigger, enabling systematic extraction of meaningful patches from each clients local data without disrupting global model utility. To address small model output sizes, we extract patches and then recombined them. The attack requires only a minor modification to the training code that can easily evade detection during client-side verification. Hence, this vulnerability represents a realistic FL supply-chain threat, where a malicious server can distribute modified training code to clients and later recover private data from their updates. Evaluations across classifiers, segmentation models, and large language models demonstrate that thousands of sensitive training samples can be recovered from client models with minimal impact on task performance, and a clients entire dataset can be stolen after multiple FL rounds. For instance, a medical segmentation dataset can be extracted with only a 3 percent utility drop. These findings expose a critical privacy vulnerability in FL systems, emphasizing the need for stronger integrity and transparency in distributed training pipelines.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provable Ordering and Continuity in Vision-Language Pretraining for Generalizable Embodied Agents</title>
<link>https://arxiv.org/abs/2502.01218</link>
<guid>https://arxiv.org/abs/2502.01218</guid>
<content:encoded><![CDATA[
arXiv:2502.01218v3 Announce Type: replace-cross 
Abstract: Pre-training vision-language representations on human action videos has emerged as a promising approach to reduce reliance on large-scale expert demonstrations for training embodied agents. However, prior methods often employ time contrastive learning based on goal-reaching heuristics, progressively aligning language instructions from the initial to the final frame. This overemphasis on future frames can result in erroneous vision-language associations, as actions may terminate early or include irrelevant moments in the end. To address this issue, we propose Action Temporal Coherence Learning (AcTOL) to learn ordered and continuous vision-language representations without rigid goal-based constraint. AcTOL treats a video as a continuous trajectory where it (1) contrasts semantic differences between frames to reflect their natural ordering, and (2) imposes a local Brownian bridge constraint to ensure smooth transitions across intermediate frames. Extensive imitation learning experiments on both simulated and real robots show that the pretrained features significantly enhance downstream manipulation tasks with high robustness to different linguistic styles of instructions, offering a viable pathway toward generalized embodied agents.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-Tuning Discrete Diffusion Models with Policy Gradient Methods</title>
<link>https://arxiv.org/abs/2502.01384</link>
<guid>https://arxiv.org/abs/2502.01384</guid>
<content:encoded><![CDATA[
arXiv:2502.01384v3 Announce Type: replace-cross 
Abstract: Discrete diffusion models have recently gained significant attention due to their ability to process complex discrete structures for language modeling. However, fine-tuning these models with policy gradient methods, as is commonly done in Reinforcement Learning from Human Feedback (RLHF), remains a challenging task. We propose an efficient, broadly applicable, and theoretically justified policy gradient algorithm, called Score Entropy Policy Optimization (\SEPO), for fine-tuning discrete diffusion models over non-differentiable rewards. Our numerical experiments across several discrete generative tasks demonstrate the scalability and efficiency of our method. Our code is available at https://github.com/ozekri/SEPO.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Nested subspace learning with flags</title>
<link>https://arxiv.org/abs/2502.06022</link>
<guid>https://arxiv.org/abs/2502.06022</guid>
<content:encoded><![CDATA[
arXiv:2502.06022v2 Announce Type: replace-cross 
Abstract: Many machine learning methods look for low-dimensional representations of the data. The underlying subspace can be estimated by first choosing a dimension $q$ and then optimizing a certain objective function over the space of $q$-dimensional subspaces (the Grassmannian). Trying different $q$ yields in general non-nested subspaces, which raises an important issue of consistency between the data representations. In this paper, we propose a simple and easily implementable principle to enforce nestedness in subspace learning methods. It consists in lifting Grassmannian optimization criteria to flag manifolds (the space of nested subspaces of increasing dimension) via nested projectors. We apply the flag trick to several classical machine learning methods and show that it successfully addresses the nestedness issue.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An interpretation of the Brownian bridge as a physics-informed prior for the Poisson equation</title>
<link>https://arxiv.org/abs/2503.00213</link>
<guid>https://arxiv.org/abs/2503.00213</guid>
<content:encoded><![CDATA[
arXiv:2503.00213v2 Announce Type: replace-cross 
Abstract: Many inverse problems require reconstructing physical fields from limited and noisy data while incorporating known governing equations. A growing body of work within probabilistic numerics formalizes such tasks via Bayesian inference in function spaces by assigning a physically meaningful prior to the latent field. In this work, we demonstrate that Brownian bridge Gaussian processes can be viewed as a softly-enforced physics-constrained prior for the Poisson equation. We first show equivalence between the variational problem associated with the Poisson equation and a kernel ridge regression objective. Then, through the connection between Gaussian process regression and kernel methods, we identify a Gaussian process for which the posterior mean function and the minimizer to the variational problem agree, thereby placing this PDE-based regularization within a fully Bayesian framework. This connection allows us to probe different theoretical questions, such as convergence and behavior of inverse problems. We then develop a finite-dimensional representation in function space and prove convergence of the projected prior and resulting posterior in Wasserstein distance. Finally, we connect the method to the important problem of identifying model-form error in applications, providing a diagnostic for model misspecification.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mathematical Insights into Protein Architecture: Persistent Homology and Machine Learning Applied to the Flagellar Motor</title>
<link>https://arxiv.org/abs/2504.16941</link>
<guid>https://arxiv.org/abs/2504.16941</guid>
<content:encoded><![CDATA[
arXiv:2504.16941v5 Announce Type: replace-cross 
Abstract: We present a machine learning approach that leverages persistent homology to classify bacterial flagellar motors into two functional states: rotated and stalled. By embedding protein structural data into a topological framework, we extract multiscale features from filtered simplicial complexes constructed over atomic coordinates. These topological invariants, specifically persistence diagrams and barcodes, capture critical geometric and connectivity patterns that correlate with motor function. The extracted features are vectorized and integrated into a machine learning pipeline that includes dimensionality reduction and supervised classification. Applied to a curated dataset of experimentally characterized flagellar motors from diverse bacterial species, our model demonstrates high classification accuracy and robustness to structural variation. This approach highlights the power of topological data analysis in revealing functionally relevant patterns beyond the reach of traditional geometric descriptors, offering a novel computational tool for protein function prediction.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Krylov Subspace Methods for Generalized Mixed Effects Models with Crossed Random Effects</title>
<link>https://arxiv.org/abs/2505.09552</link>
<guid>https://arxiv.org/abs/2505.09552</guid>
<content:encoded><![CDATA[
arXiv:2505.09552v2 Announce Type: replace-cross 
Abstract: Mixed-effects models are widely used to model data with hierarchical grouping structures and high-cardinality categorical predictor variables. However, for high-dimensional crossed random effects, sparse Cholesky decompositions, the current standard approach, can become prohibitively slow. In this work, we present Krylov subspace-based methods that address these computational bottlenecks and analyze them both theoretically and empirically. In particular, we derive new results on the convergence and accuracy of the preconditioned stochastic Lanczos quadrature and conjugate gradient methods for mixed-effects models, and we develop scalable methods for calculating predictive variances. In experiments with simulated and real-world data, the proposed methods yield speedups by factors of up to about 10,000 and are numerically more stable than Cholesky-based computations as implemented in state-of-the-art packages such as lme4 and glmmTMB. Our methodology is available in the open-source C++ software library GPBoost, with accompanying high-level Python and R packages.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian Deep Learning for Discrete Choice</title>
<link>https://arxiv.org/abs/2505.18077</link>
<guid>https://arxiv.org/abs/2505.18077</guid>
<content:encoded><![CDATA[
arXiv:2505.18077v2 Announce Type: replace-cross 
Abstract: Discrete choice models (DCMs) are used to analyze individual decision-making in contexts such as transportation choices, political elections, and consumer preferences. DCMs play a central role in applied econometrics by enabling inference on key economic variables, such as marginal rates of substitution, rather than focusing solely on predicting choices on new unlabeled data. However, while traditional DCMs offer high interpretability and support for point and interval estimation of economic quantities, these models often underperform in predictive tasks compared to deep learning (DL) models. Despite their predictive advantages, DL models remain largely underutilized in discrete choice due to concerns about their lack of interpretability, unstable parameter estimates, and the absence of established methods for uncertainty quantification. Here, we introduce a deep learning model architecture specifically designed to integrate with approximate Bayesian inference methods, such as Stochastic Gradient Langevin Dynamics (SGLD). Our proposed model collapses to behaviorally informed hypotheses when data is limited, mitigating overfitting and instability in underspecified settings while retaining the flexibility to capture complex nonlinear relationships when sufficient data is available. We demonstrate our approach using SGLD through a Monte Carlo simulation study, evaluating both predictive metrics--such as out-of-sample balanced accuracy--and inferential metrics--such as empirical coverage for marginal rates of substitution interval estimates. Additionally, we present results from two empirical case studies: one using revealed mode choice data in NYC, and the other based on the widely used Swiss train choice stated preference data.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Force Metrics: Pre-Training MLFFs for Stable MD Simulations</title>
<link>https://arxiv.org/abs/2506.14850</link>
<guid>https://arxiv.org/abs/2506.14850</guid>
<content:encoded><![CDATA[
arXiv:2506.14850v2 Announce Type: replace-cross 
Abstract: Machine-learning force fields (MLFFs) have emerged as a promising solution for speeding up ab initio molecular dynamics (MD) simulations, where accurate force predictions are critical but often computationally expensive. In this work, we employ GemNet-T, a graph neural network model, as an MLFF and investigate two training strategies: (1) direct training on MD17 (10K samples) without pre-training, and (2) pre-training on the large-scale OC20 dataset followed by fine-tuning on MD17 (10K). While both approaches achieve low force mean absolute errors (MAEs), reaching 5 meV/A per atom, we find that lower force errors do not necessarily guarantee stable MD simulations. Notably, the pre-trained GemNet-T model yields significantly improved simulation stability, sustaining trajectories up to three times longer than the model trained from scratch. By analyzing local properties of the learned force fields, we find that pre-training produces more structured latent representations, smoother force responses to local geometric changes, and more consistent force differences between nearby configurations, all of which contribute to more stable and reliable MD simulations. These findings underscore the value of pre-training on large, diverse datasets to capture complex molecular interactions and highlight that force MAE alone is not always a sufficient metric of MD simulation stability.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InsurTech innovation using natural language processing</title>
<link>https://arxiv.org/abs/2507.21112</link>
<guid>https://arxiv.org/abs/2507.21112</guid>
<content:encoded><![CDATA[
arXiv:2507.21112v3 Announce Type: replace-cross 
Abstract: With the rapid rise of InsurTech, traditional insurance companies are increasingly exploring alternative data sources and advanced technologies to sustain their competitive edge. This paper provides both a conceptual overview and practical case studies of natural language processing (NLP) and its emerging applications within insurance operations, focusing on transforming raw, unstructured text into structured data suitable for actuarial analysis and decision-making. Leveraging real-world alternative data provided by an InsurTech industry partner that enriches traditional insurance data sources, we apply various NLP techniques to demonstrate feature de-biasing, feature compression, and industry classification in the commercial insurance context. These enriched, text-derived insights not only add to and refine traditional rating factors for commercial insurance pricing but also offer novel perspectives for assessing underlying risk by introducing novel industry classification techniques. Through these demonstrations, we show that NLP is not merely a supplementary tool but a foundational element of modern, data-driven insurance analytics.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Long-Horizon Visual Imitation Learning via Plan and Code Reflection</title>
<link>https://arxiv.org/abs/2509.05368</link>
<guid>https://arxiv.org/abs/2509.05368</guid>
<content:encoded><![CDATA[
arXiv:2509.05368v3 Announce Type: replace-cross 
Abstract: Learning from long-horizon demonstrations with complex action sequences presents significant challenges for visual imitation learning, particularly in understanding temporal relationships of actions and spatial relationships between objects. In this paper, we propose a new agent framework that incorporates two dedicated reflection modules to enhance both plan and code generation. The plan generation module produces an initial action sequence, which is then verified by the plan reflection module to ensure temporal coherence and spatial alignment with the demonstration video. The code generation module translates the plan into executable code, while the code reflection module verifies and refines the generated code to ensure correctness and consistency with the generated plan. These two reflection modules jointly enable the agent to detect and correct errors in both the plan generation and code generation, improving performance in tasks with intricate temporal and spatial dependencies. To support systematic evaluation, we introduce LongVILBench, a benchmark comprising 300 human demonstrations with action sequences of up to 18 steps. LongVILBench emphasizes temporal and spatial complexity across multiple task types. Experimental results demonstrate that existing methods perform poorly on this benchmark, whereas our new framework establishes a strong baseline for long-horizon visual imitation learning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TACE: A unified Irreducible Cartesian Tensor Framework for Atomistic Machine Learning</title>
<link>https://arxiv.org/abs/2509.14961</link>
<guid>https://arxiv.org/abs/2509.14961</guid>
<content:encoded><![CDATA[
arXiv:2509.14961v2 Announce Type: replace-cross 
Abstract: Here, we introduce the Tensor Atomic Cluster Expansion (TACE), a unified framework formulated entirely in Cartesian space, enabling systematic and consistent prediction of arbitrary structure-dependent tensorial properties. TACE achieves this by decomposing atomic environments into a complete hierarchy of irreducible Cartesian tensors, ensuring symmetry-consistent representations that naturally encode invariance and equivariance constraints. Beyond geometry, TACE incorporates universal embeddings that flexibly integrate diverse attributes including computational levels, charges, magnetic moments and field perturbations. This allows explicit control over external invariants and equivariants in the prediction process. Long-range interactions are also accurately described through the Latent Ewald Summation module within the short-range approximation, providing a rigorous yet computationally efficient treatment of electrostatic and dispersion effects. We demonstrate that TACE attains accuracy, stability, and efficiency on par with or surpassing leading equivariant frameworks across finite molecules and extended materials. This includes in-domain and out-of-domain benchmarks, spectra, Hessian, external-field responses, charged and magnetic systems, multi-fidelity training, heterogeneous catalysis, and even superior performance within the uMLIP benchmark. Crucially, TACE bridges scalar and tensorial modeling and establishes a Cartesian-space paradigm that unifies and extends beyond the design space of spherical-tensor-based methods. This work lays the foundation for a new generation of universal atomistic machine learning models capable of systematically capturing the rich interplay of geometry, fields and material properties within a single coherent framework.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improved Segmentation of Polyps and Visual Explainability Analysis</title>
<link>https://arxiv.org/abs/2509.18159</link>
<guid>https://arxiv.org/abs/2509.18159</guid>
<content:encoded><![CDATA[
arXiv:2509.18159v5 Announce Type: replace-cross 
Abstract: Colorectal cancer (CRC) remains one of the leading causes of cancer-related morbidity and mortality worldwide, with gastrointestinal (GI) polyps serving as critical precursors according to the World Health Organization (WHO). Early and accurate segmentation of polyps during colonoscopy is essential for reducing CRC progression, yet manual delineation is labor-intensive and prone to observer variability. Deep learning methods have demonstrated strong potential for automated polyp analysis, but their limited interpretability remains a barrier to clinical adoption. In this study, we present PolypSeg-GradCAM, an explainable deep learning framework that integrates a U-Net architecture with a pre-trained ResNet-34 backbone and Gradient-weighted Class Activation Mapping (Grad-CAM) for transparent polyp segmentation. To ensure rigorous benchmarking, the model was trained and evaluated using 5-Fold Cross-Validation on the Kvasir-SEG dataset of 1,000 annotated endoscopic images. Experimental results show a mean Dice coefficient of 0.8902 +/- 0.0125, a mean Intersection-over-Union (IoU) of 0.8023, and an Area Under the Receiver Operating Characteristic Curve (AUC-ROC) of 0.9722. Advanced quantitative analysis using an optimal threshold yielded a Sensitivity of 0.9058 and Precision of 0.9083. Additionally, Grad-CAM visualizations confirmed that predictions were guided by clinically relevant regions, offering insight into the model's decision-making process. This study demonstrates that integrating segmentation accuracy with interpretability can support the development of trustworthy AI-assisted colonoscopy tools.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing the Network Topology of a Linear Reservoir Computer</title>
<link>https://arxiv.org/abs/2509.23391</link>
<guid>https://arxiv.org/abs/2509.23391</guid>
<content:encoded><![CDATA[
arXiv:2509.23391v2 Announce Type: replace-cross 
Abstract: Machine learning has become a fundamental approach for modeling, prediction, and control, enabling systems to learn from data and perform complex tasks. Reservoir computing is a machine learning tool that leverages high-dimensional dynamical systems to efficiently process temporal data for prediction and observation tasks. Traditionally, the connectivity of the network that underlies a reservoir computer (RC) is generated randomly, lacking a principled design. Here, we focus on optimizing the connectivity of a linear RC to improve its performance and interpretability, which we achieve by decoupling the RC dynamics into a number of independent modes. We then proceed to optimize each one of these modes to perform a given task, which corresponds to selecting an optimal RC connectivity in terms of a given set of eigenvalues of the RC adjacency matrix. Simulations on networks of varying sizes show that the optimized RC significantly outperforms randomly constructed reservoirs in both training and testing phases and often surpasses nonlinear reservoirs of comparable size. This approach provides both practical performance advantages and theoretical guidelines for designing efficient, task-specific, and analytically transparent RC architectures.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Network-Optimised Spiking Neural Network for Event-Driven Networking</title>
<link>https://arxiv.org/abs/2509.23516</link>
<guid>https://arxiv.org/abs/2509.23516</guid>
<content:encoded><![CDATA[
arXiv:2509.23516v2 Announce Type: replace-cross 
Abstract: Time-critical networking requires low-latency decisions from sparse and bursty telemetry, where fixed-step neural inference waste computation. We introduce Network-Optimised Spiking (NOS), a two-state neuron whose variables correspond to normalised queue occupancy and a recovery resource. NOS combines a saturating excitability nonlinearity for finite buffers, service and damping leaks, graph-local inputs with per-link gates and delays, and differentiable resets compatible with surrogate gradients and neuromorphic deployment. We establish existence and uniqueness of subthreshold equilibria, derive Jacobian-based local stability tests, and obtain a scalar network stability threshold that separates topology from node physics through a Perron-mode spectral condition. A stochastic arrival model aligned with telemetry smoothing links NOS responses to classical queueing behaviour while explaining increased variability near stability margins. Across chain, star, and scale-free graphs, NOS improves early-warning F1 and detection latency over MLP, RNN, GRU, and temporal-GNN baselines under a common residual-based protocol, while providing practical calibration and stability rules suited to resource-constrained networking deployments. Code and Demos: https://mbilal84.github.io/nos-snn-networking/
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoGraph: Geometric and Graph-based Ensemble Descriptors for Intrinsically Disordered Proteins</title>
<link>https://arxiv.org/abs/2510.00774</link>
<guid>https://arxiv.org/abs/2510.00774</guid>
<content:encoded><![CDATA[
arXiv:2510.00774v2 Announce Type: replace-cross 
Abstract: While deep learning has revolutionized the prediction of rigid protein structures, modelling the conformational ensembles of Intrinsically Disordered Proteins (IDPs) remains a key frontier. Current AI paradigms present a trade-off: Protein Language Models (PLMs) capture evolutionary statistics but lack explicit physical grounding, while generative models trained to model full ensembles are computationally expensive. In this work we critically assess these limits and propose a path forward. We introduce GeoGraph, a simulation-informed surrogate trained to predict ensemble-averaged statistics of residue-residue contact-map topology directly from sequence. By featurizing coarse-grained molecular dynamics simulations into residue- and sequence-level graph descriptors, we create a robust and information-rich learning target. Our evaluation demonstrates that this approach yields representations that are more predictive of key biophysical properties than existing methods.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Who is In Charge? Dissecting Role Conflicts in Instruction Following</title>
<link>https://arxiv.org/abs/2510.01228</link>
<guid>https://arxiv.org/abs/2510.01228</guid>
<content:encoded><![CDATA[
arXiv:2510.01228v2 Announce Type: replace-cross 
Abstract: Large language models should follow hierarchical instructions where system prompts override user inputs, yet recent work shows they often ignore this rule while strongly obeying social cues such as authority or consensus. We extend these behavioral findings with mechanistic interpretations on a large-scale dataset. Linear probing shows conflict-decision signals are encoded early, with system-user and social conflicts forming distinct subspaces. Direct Logit Attribution reveals stronger internal conflict detection in system-user cases but consistent resolution only for social cues. Steering experiments show that, despite using social cues, the vectors surprisingly amplify instruction following in a role-agnostic way. Together, these results explain fragile system obedience and underscore the need for lightweight hierarchy-sensitive alignment methods.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Learning and Control: Foundations, Advances, and Perspectives</title>
<link>https://arxiv.org/abs/2510.03303</link>
<guid>https://arxiv.org/abs/2510.03303</guid>
<content:encoded><![CDATA[
arXiv:2510.03303v2 Announce Type: replace-cross 
Abstract: Control theory of dynamical systems offers a powerful framework for tackling challenges in deep neural networks and other machine learning architectures. We show that concepts such as simultaneous and ensemble controllability offer new insights into the classification and representation properties of deep neural networks, while the control and optimization of static systems can be employed to better understand the performance of shallow networks. Inspired by the classical concept of turnpike, we also explore the relationship between dynamic and static neural networks, where depth is traded for width, and the role of transformers as mechanisms for accelerating classical neural network tasks. We also exploit the expressive power of neural networks (exemplified, for instance, by the Universal Approximation Theorem) to develop a novel hybrid modeling methodology, the Hybrid-Cooperative Learning (HYCO), combining mechanics and data-driven methods in a game-theoretic setting. Finally, we describe how classical properties of diffusion processes, long established in the context of partial differential equations, contribute to explaining the success of modern generative artificial intelligence (AI). We present an overview of our recent results in these areas, illustrating how control, machine learning, numerical analysis, and partial differential equations come together to motivate a fertile ground for future research.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep generative priors for 3D brain analysis</title>
<link>https://arxiv.org/abs/2510.15119</link>
<guid>https://arxiv.org/abs/2510.15119</guid>
<content:encoded><![CDATA[
arXiv:2510.15119v2 Announce Type: replace-cross 
Abstract: Diffusion models have recently emerged as powerful generative models in medical imaging. However, it remains a major challenge to combine these data-driven models with domain knowledge to guide brain imaging problems. In neuroimaging, Bayesian inverse problems have long provided a successful framework for inference tasks, where incorporating domain knowledge of the imaging process enables robust performance without requiring extensive training data. However, the anatomical modeling component of these approaches typically relies on classical mathematical priors that often fail to capture the complex structure of brain anatomy. In this work, we present the first general-purpose application of diffusion models as priors for solving a wide range of medical imaging inverse problems. Our approach leverages a score-based diffusion prior trained extensively on diverse brain MRI data, paired with flexible forward models that capture common image processing tasks such as super-resolution, bias field correction, inpainting, and combinations thereof. We further demonstrate how our framework can refine outputs from existing deep learning methods to improve anatomical fidelity. Experiments on heterogeneous clinical and research MRI data show that our method achieves state-of-the-art performance producing consistent, high-quality solutions without requiring paired training datasets. These results highlight the potential of diffusion priors as versatile tools for brain MRI analysis.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stochastic Optimization with Random Search</title>
<link>https://arxiv.org/abs/2510.15610</link>
<guid>https://arxiv.org/abs/2510.15610</guid>
<content:encoded><![CDATA[
arXiv:2510.15610v2 Announce Type: replace-cross 
Abstract: We revisit random search for stochastic optimization, where only noisy function evaluations are available. We show that the method works under weaker smoothness assumptions than previously considered, and that stronger assumptions enable improved guarantees. In the finite-sum setting, we design a variance-reduced variant that leverages multiple samples to accelerate convergence. Our analysis relies on a simple translation invariance property, which provides a principled way to balance noise and reduce variance.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Split-Client Approach to Second-Order Optimization</title>
<link>https://arxiv.org/abs/2510.15714</link>
<guid>https://arxiv.org/abs/2510.15714</guid>
<content:encoded><![CDATA[
arXiv:2510.15714v2 Announce Type: replace-cross 
Abstract: Second-order methods promise faster convergence but are rarely used in practice because Hessian computations and decompositions are far more expensive than gradients. We propose a \emph{split-client} framework where gradients and curvature are computed asynchronously by separate clients. This abstraction captures realistic delays and inexact Hessian updates while avoiding the manual tuning required by Lazy Hessian methods. Focusing on cubic regularization, we show that our approach retains strong convergence guarantees and achieves a provable wall-clock speedup of order $\sqrt{\tau}$, where $\tau$ is the relative time needed to compute and decompose the Hessian compared to a gradient step. Since $\tau$ can be orders of magnitude larger than one in high-dimensional problems, this improvement is practically significant. Experiments on synthetic and real datasets confirm the theory: asynchronous curvature consistently outperforms vanilla and Lazy Hessian baselines, while maintaining second-order accuracy.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FARM: Fine-Tuning Geospatial Foundation Models for Intra-Field Crop Yield Regression</title>
<link>https://arxiv.org/abs/2510.26609</link>
<guid>https://arxiv.org/abs/2510.26609</guid>
<content:encoded><![CDATA[
arXiv:2510.26609v2 Announce Type: replace-cross 
Abstract: Accurate and timely crop yield prediction is crucial for global food security and modern agricultural management. Traditional methods often lack the scalability and granularity required for precision farming. This paper introduces FARM: Fine-tuning Agricultural Regression Models, a deep learning framework designed for high-resolution, intra-field canola yield prediction. FARM leverages a pre-trained, large-scale geospatial foundation model (Prithvi-EO-2.0-600M) and adapts it for a continuous regression task, transforming multi-temporal satellite imagery into dense, pixel-level (30 m) yield maps. Evaluated on a comprehensive dataset from the Canadian Prairies, FARM achieves a Root Mean Squared Error (RMSE) of 0.44 and an R^2 of 0.81. Using an independent high-resolution yield monitor dataset, we further show that fine-tuning FARM on limited ground-truth labels outperforms training the same architecture from scratch, confirming the benefit of pre-training on large, upsampled county-level data for data-scarce precision agriculture. These results represent improvement over baseline architectures like 3D-CNN and DeepYield, which highlight the effectiveness of fine-tuning foundation models for specialized agricultural applications. By providing a continuous, high-resolution output, FARM offers a more actionable tool for precision agriculture than conventional classification or county-level aggregation methods. This work validates a novel approach that bridges the gap between large-scale Earth observation and on-farm decision-making, offering a scalable solution for detailed agricultural monitoring.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian model selection and misspecification testing in imaging inverse problems only from noisy and partial measurements</title>
<link>https://arxiv.org/abs/2510.27663</link>
<guid>https://arxiv.org/abs/2510.27663</guid>
<content:encoded><![CDATA[
arXiv:2510.27663v2 Announce Type: replace-cross 
Abstract: Modern imaging techniques heavily rely on Bayesian statistical models to address difficult image reconstruction and restoration tasks. This paper addresses the objective evaluation of such models in settings where ground truth is unavailable, with a focus on model selection and misspecification diagnosis. Existing unsupervised model evaluation methods are often unsuitable for computational imaging due to their high computational cost and incompatibility with modern image priors defined implicitly via machine learning models. We herein propose a general methodology for unsupervised model selection and misspecification detection in Bayesian imaging sciences, based on a novel combination of Bayesian cross-validation and data fission, a randomized measurement splitting technique. The approach is compatible with any Bayesian imaging sampler, including diffusion and plug-and-play samplers. We demonstrate the methodology through experiments involving various scoring rules and types of model misspecification, where we achieve excellent selection and detection accuracy with a low computational cost.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generation is Required for Data-Efficient Perception</title>
<link>https://arxiv.org/abs/2512.08854</link>
<guid>https://arxiv.org/abs/2512.08854</guid>
<content:encoded><![CDATA[
arXiv:2512.08854v2 Announce Type: replace-cross 
Abstract: It has been hypothesized that human-level visual perception requires a generative approach in which internal representations result from inverting a decoder. Yet today's most successful vision models are non-generative, relying on an encoder that maps images to representations without decoder inversion. This raises the question of whether generation is, in fact, necessary for machines to achieve human-level visual perception. To address this, we study whether generative and non-generative methods can achieve compositional generalization, a hallmark of human perception. Under a compositional data generating process, we formalize the inductive biases required to guarantee compositional generalization in decoder-based (generative) and encoder-based (non-generative) methods. We then show theoretically that enforcing these inductive biases on encoders is generally infeasible using regularization or architectural constraints. In contrast, for generative methods, the inductive biases can be enforced straightforwardly, thereby enabling compositional generalization by constraining a decoder and inverting it. We highlight how this inversion can be performed efficiently, either online through gradient-based search or offline through generative replay. We examine the empirical implications of our theory by training a range of generative and non-generative methods on photorealistic image datasets. We find that, without the necessary inductive biases, non-generative methods often fail to generalize compositionally and require large-scale pretraining or added supervision to improve generalization. By comparison, generative methods yield significant improvements in compositional generalization, without requiring additional data, by leveraging suitable inductive biases on a decoder along with search and replay.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Closed-loop Molecular Discovery via Language Model, Property Alignment and Strategic Search</title>
<link>https://arxiv.org/abs/2512.09566</link>
<guid>https://arxiv.org/abs/2512.09566</guid>
<content:encoded><![CDATA[
arXiv:2512.09566v2 Announce Type: replace-cross 
Abstract: Drug discovery is a time-consuming and expensive process, with traditional high-throughput and docking-based virtual screening hampered by low success rates and limited scalability. Recent advances in generative modelling, including autoregressive, diffusion, and flow-based approaches, have enabled de novo ligand design beyond the limits of enumerative screening. Yet these models often suffer from inadequate generalization, limited interpretability, and an overemphasis on binding affinity at the expense of key pharmacological properties, thereby restricting their translational utility. Here we present Trio, a molecular generation framework integrating fragment-based molecular language modeling, reinforcement learning, and Monte Carlo tree search, for effective and interpretable closed-loop targeted molecular design. Through the three key components, Trio enables context-aware fragment assembly, enforces physicochemical and synthetic feasibility, and guides a balanced search between the exploration of novel chemotypes and the exploitation of promising intermediates within protein binding pockets. Experimental results show that Trio reliably achieves chemically valid and pharmacologically enhanced ligands, outperforming state-of-the-art approaches with improved binding affinity (+7.85%), drug-likeness (+11.10%) and synthetic accessibility (+12.05%), while expanding molecular diversity more than fourfold. By combining generalization, plausibility, and interpretability, Trio establishes a closed-loop generative paradigm that redefines how chemical space can be navigated, offering a transformative foundation for the next era of AI-driven drug discovery.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Fake-News Detection with Node-Level Topological Features</title>
<link>https://arxiv.org/abs/2512.09974</link>
<guid>https://arxiv.org/abs/2512.09974</guid>
<content:encoded><![CDATA[
arXiv:2512.09974v2 Announce Type: replace-cross 
Abstract: In recent years, the proliferation of misinformation and fake news has posed serious threats to individuals and society, spurring intense research into automated detection methods. Previous work showed that integrating content, user preferences, and propagation structure achieves strong performance, but leaves all graph-level representation learning entirely to the GNN, hiding any explicit topological cues. To close this gap, we introduce a lightweight enhancement: for each node, we append two classical graph-theoretic metrics, degree centrality and local clustering coefficient, to its original BERT and profile embeddings, thus explicitly flagging the roles of hub and community. In the UPFD Politifact subset, this simple modification boosts macro F1 from 0.7753 to 0.8344 over the original baseline. Our study not only demonstrates the practical value of explicit topology features in fake-news detection but also provides an interpretable, easily reproducible template for fusing graph metrics in other information-diffusion tasks.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Eminence in Shadow: Exploiting Feature Boundary Ambiguity for Robust Backdoor Attacks</title>
<link>https://arxiv.org/abs/2512.10402</link>
<guid>https://arxiv.org/abs/2512.10402</guid>
<content:encoded><![CDATA[
<div> Keywords: backdoor attacks, deep neural networks, sparse decision boundaries, influence functions, Eminence framework<br /><br />Summary:<br /><br />1. Deep neural networks are vulnerable to backdoor attacks often relying on heuristic brute-force approaches, but existing work lacks rigorous theoretical analyses to fully understand the mechanisms behind such attacks.<br />2. This paper theoretically examines how sparse decision boundaries in models create ambiguous regions where even a negligible number of relabeled (poisoned) samples cause significant misclassification.<br />3. Using influence function analysis, the study quantifies how margin samples induce major parameter changes with minimal impact on clean data accuracy, explaining why extremely low poison rates are effective.<br />4. Based on these insights, the authors propose Eminence, a black-box backdoor attack framework that generates a universal, visually subtle trigger targeting vulnerable decision boundaries to induce robust misclassification.<br />5. Eminence requires notably low poison rates (< 0.1%, versus >1% in prior methods), achieves more than 90% attack success rate, preserves clean accuracy, demonstrates high transferability across various models and datasets, and is supported by comprehensive experiments validating its theoretical foundations and effectiveness. <div>
arXiv:2512.10402v2 Announce Type: replace 
Abstract: Deep neural networks (DNNs) underpin critical applications yet remain vulnerable to backdoor attacks, typically reliant on heuristic brute-force methods. Despite significant empirical advancements in backdoor research, the lack of rigorous theoretical analysis limits understanding of underlying mechanisms, constraining attack predictability and adaptability. Therefore, we provide a theoretical analysis targeting backdoor attacks, focusing on how sparse decision boundaries enable disproportionate model manipulation. Based on this finding, we derive a closed-form, ambiguous boundary region, wherein negligible relabeled samples induce substantial misclassification. Influence function analysis further quantifies significant parameter shifts caused by these margin samples, with minimal impact on clean accuracy, formally grounding why such low poison rates suffice for efficacious attacks. Leveraging these insights, we propose Eminence, an explainable and robust black-box backdoor framework with provable theoretical guarantees and inherent stealth properties. Eminence optimizes a universal, visually subtle trigger that strategically exploits vulnerable decision boundaries and effectively achieves robust misclassification with exceptionally low poison rates (< 0.1%, compared to SOTA methods typically requiring > 1%). Comprehensive experiments validate our theoretical discussions and demonstrate the effectiveness of Eminence, confirming an exponential relationship between margin poisoning and adversarial boundary manipulation. Eminence maintains > 90% attack success rate, exhibits negligible clean-accuracy loss, and demonstrates high transferability across diverse models, datasets and scenarios.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Confucius Code Agent: Scalable Agent Scaffolding for Real-World Codebases</title>
<link>https://arxiv.org/abs/2512.10398</link>
<guid>https://arxiv.org/abs/2512.10398</guid>
<content:encoded><![CDATA[
<div> Keywords: Confucius Code Agent, software engineering, scalable agents, long-context reasoning, meta-agent  

<br /><br />Summary:  
The paper introduces the Confucius Code Agent (CCA), a scalable coding agent designed to handle large-scale software engineering tasks involving massive repositories, long-horizon sessions, and complex toolchains. Built on the Confucius SDK, the platform emphasizes three perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX), ensuring extensibility, interpretability, and controllability. The SDK includes a unified orchestrator and hierarchical working memory to support long-context reasoning, a persistent note-taking system for continual learning across sessions, and a modular extension system for reliable tool integration. A novel meta-agent automates the build-test-improve loop by synthesizing, evaluating, and refining agent configurations, allowing rapid adaptation to new tasks and environments. Experiments on the SWE-Bench-Pro benchmark indicate that CCA achieves a Resolve@1 score of 54.3%, surpassing prior research baselines and matching or exceeding commercial systems under identical conditions. Overall, the Confucius SDK and CCA provide a general, extensible, and production-ready foundation for constructing effective, robust coding agents, addressing the gap between research prototypes and practical deployment in large-scale software engineering contexts. <div>
arXiv:2512.10398v4 Announce Type: replace-cross 
Abstract: Real-world software engineering tasks require coding agents that can operate over massive repositories, sustain long-horizon sessions, and reliably coordinate complex toolchains at test time. Existing research-grade agents offer transparency but struggle when scaled to real-world workloads, while proprietary systems achieve strong practical performance but provide limited extensibility, interpretability, and controllability. We introduce the Confucius Code Agent (CCA), a scalable software engineering agent that can operate at large-scale codebases. CCA is built on top of the Confucius SDK, an agent development platform structured around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK integrates a unified orchestrator with hierarchical working memory for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension system for reliable tool use. In addition, we introduce a meta-agent that automates the synthesis, evaluation, and refinement of agent configurations through a build-test-improve loop, enabling rapid adaptation to new tasks, environments, and tool stacks. Instantiated with these mechanisms, CCA demonstrates strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA reaches a Resolve@1 of 54.3%, exceeding prior research baselines and comparing favorably to commercial results, under identical repositories, model backend, and tool access. Together, the Confucius SDK and CCA form a general, extensible, and production-grade foundation for building effective and robust coding agents, bridging the gap between research prototypes and practical large-scale deployment.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM as a Neural Architect: Controlled Generation of Image Captioning Models Under Strict API Contracts</title>
<link>https://arxiv.org/abs/2512.14706</link>
<guid>https://arxiv.org/abs/2512.14706</guid>
<content:encoded><![CDATA[
<div> Keywords: Neural Architecture Search, Large Language Models, Image Captioning, BLEU-4, AutoML<br /><br />Summary:  
1. The paper introduces NN-Caption, a neural architecture search (NAS) pipeline guided by large language models (LLMs) to automatically generate runnable image-captioning models.  
2. NN-Caption composes convolutional neural network (CNN) encoders from LEMUR’s classification backbones with sequence decoders such as LSTM, GRU, or Transformer, adhering to a strict Net API for compatibility.  
3. The approach utilizes DeepSeek-R1-0528-Qwen3-8B as the primary LLM generator, and the authors provide prompt templates and examples of generated architectures.  
4. Evaluation is performed on the MS COCO dataset, using the BLEU-4 metric to measure caption quality.  
5. Results demonstrate that the LLM generated dozens of image-captioning models, with over half successfully trained and producing meaningful captions.  
6. An analysis compares using 5 versus 10 input model snippets in prompts, finding a slight decrease in success rate when more components are provided.  
7. Training dynamics, including caption accuracy over epochs and peak BLEU-4 scores, are reported to characterize model performance.  
8. The study highlights the LLM’s additional role in suggesting hyperparameters and training practices alongside architecture design.  
9. Challenges encountered include code hallucinations and API compliance issues, which were mitigated through prompt engineering and iterative code corrections.  
10. The work integrates prompt-based code generation with automatic evaluation and contributes numerous novel captioning models to the public LEMUR dataset, facilitating reproducible benchmarking and advancing AutoML research. <div>
arXiv:2512.14706v1 Announce Type: new 
Abstract: Neural architecture search (NAS) traditionally requires significant human expertise or automated trial-and-error to design deep learning models. We present NN-Caption, an LLM-guided neural architecture search pipeline that generates runnable image-captioning models by composing CNN encoders from LEMUR's classification backbones with sequence decoders (LSTM/GRU/Transformer) under a strict Net API. Using DeepSeek-R1-0528-Qwen3-8B as the primary generator, we present the prompt template and examples of generated architectures. We evaluate on MS COCO with BLEU-4. The LLM generated dozens of captioning models, with over half successfully trained and producing meaningful captions. We analyse the outcomes of using different numbers of input model snippets (5 vs. 10) in the prompt, finding a slight drop in success rate when providing more candidate components. We also report training dynamics (caption accuracy vs. epochs) and the highest BLEU-4 attained. Our results highlight the promise of LLM-guided NAS: the LLM not only proposes architectures but also suggests hyperparameters and training practices. We identify the challenges encountered (e.g., code hallucinations or API compliance issues) and detail how prompt rules and iterative code fixes addressed them. This work presents a pipeline that integrates prompt-based code generation with automatic evaluation, and adds dozens of novel captioning models to the open LEMUR dataset to facilitate reproducible benchmarking and downstream AutoML research.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autonomous Source Knowledge Selection in Multi-Domain Adaptation</title>
<link>https://arxiv.org/abs/2512.14710</link>
<guid>https://arxiv.org/abs/2512.14710</guid>
<content:encoded><![CDATA[
<div> Keywords: unsupervised multi-domain adaptation, transfer learning, source knowledge selection, density-driven selection, pseudo-label enhancement<br /><br />Summary: Unsupervised multi-domain adaptation is a critical area of transfer learning that utilizes knowledge from multiple labeled source domains to improve learning in an unlabeled target domain. The challenge addressed in this paper is the redundancy and irrelevance of information present in numerous source domains, which can negatively impact transfer performance, especially in scenarios with many source domains. To tackle this, the authors propose a novel method called Autonomous Source Knowledge Selection (AutoS), which autonomously selects the most relevant source training samples and models to enhance target task prediction. AutoS employs a density-driven selection strategy to efficiently pick source samples during training and to determine the contribution of various source models toward target predictions. Additionally, the method integrates a pseudo-label enhancement module that leverages a pre-trained multimodal model to reduce label noise in the target domain and boost self-supervision. Experimental results on real-world datasets demonstrate the superiority and effectiveness of AutoS in improving transfer learning performance by selecting more transferable knowledge and filtering out less relevant source information, paving the way for more scalable and efficient multi-domain adaptation. <div>
arXiv:2512.14710v1 Announce Type: new 
Abstract: Unsupervised multi-domain adaptation plays a key role in transfer learning by leveraging acquired rich source information from multiple source domains to solve target task from an unlabeled target domain. However, multiple source domains often contain much redundant or unrelated information which can harm transfer performance, especially when in massive-source domain settings. It is urgent to develop effective strategies for identifying and selecting the most transferable knowledge from massive source domains to address the target task. In this paper, we propose a multi-domain adaptation method named \underline{\textit{Auto}}nomous Source Knowledge \underline{\textit{S}}election (AutoS) to autonomosly select source training samples and models, enabling the prediction of target task using more relevant and transferable source information. The proposed method employs a density-driven selection strategy to choose source samples during training and to determine which source models should contribute to target prediction. Simulteneously, a pseudo-label enhancement module built on a pre-trained multimodal modal is employed to mitigate target label noise and improve self-supervision. Experiments on real-world datasets indicate the superiority of the proposed method.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SepsisSuite: Beyond Risk Stratification -- A Comparative Analysis of Deep Fusion vs. Expert Stacking for Prescriptive Sepsis AI</title>
<link>https://arxiv.org/abs/2512.14712</link>
<guid>https://arxiv.org/abs/2512.14712</guid>
<content:encoded><![CDATA[
<div> Keywords: Sepsis, Deep Fusion, Context-Aware Stacking, Mixture-of-Experts, Antibiotic Selection<br /><br />Summary:<br /><br />Sepsis is a major cause of ICU admissions worldwide, but traditional prediction models struggle to effectively combine diverse data types such as vitals, text, and imaging. This study compares two architectural approaches: End-to-End Deep Fusion and Context-Aware Stacking. Initially, the authors developed SepsisFusionFormer, a Quad-Modal Hierarchical Gated Attention Network designed to handle complex cross-modal interactions. However, it underperformed due to "attention starvation" and overfitting in a limited antibiotic cohort, achieving an AUC of only 0.66 on MIMIC-IV data. Learning from this, they proposed SepsisLateFusion, a simplified Context-Aware Mixture-of-Experts model that considers each modality as an independent expert: "Historian" (Static data), "Monitor" (Temporal data), and "Reader" (NLP data). These are dynamically weighted by a CatBoost meta-learner. This approach significantly improved performance, reaching a state-of-the-art AUC of 0.915 for predicting sepsis 4 hours before clinical onset. Additionally, adjusting the decision threshold reduced missed cases by 48%, allowing a genuine preventive intervention window. For multi-class antibiotic selection—a novel prescriptive task—the Quad-Modal Ensemble also excelled with a 0.72 AUC. The resulting models are packaged in SepsisSuite, a freely available Python clinical decision support framework. <div>
arXiv:2512.14712v1 Announce Type: new 
Abstract: Sepsis accounts for nearly 20% of global ICU admissions, yet conventional prediction models often fail to effectively integrate heterogeneous data streams, remaining either siloed by modality or reliant on brittle early fusion. In this work, we present a rigorous architectural comparison between End-to-End Deep Fusion and Context-Aware Stacking for sepsis tasks. We initially hypothesized that a novel Quad-Modal Hierarchical Gated Attention Network -- termed SepsisFusionFormer -- would resolve complex cross-modal interactions between vitals, text, and imaging. However, experiments on MIMIC-IV revealed that SepsisFusionFormer suffered from "attention starvation" in the small antibiotic cohort ($N \approx 2,100$), resulting in overfitting (AUC 0.66). This counterintuitive result informed the design of SepsisLateFusion, a "leaner" Context-Aware Mixture-of-Experts (MoE) architecture. By treating modalities as orthogonal experts -- the "Historian" (Static), the "Monitor" (Temporal), and the "Reader" (NLP) -- and dynamically gating them via a CatBoost meta-learner, we achieved State-of-the-Art (SOTA) performance: 0.915 AUC for prediction 4 hours prior to clinical onset. By calibrating the decision threshold for clinical safety, we reduced missed cases by 48% relative to the default operating point, thus opening a true preventative window for timely intervention over reactive alerts. Furthermore, for the novel prescriptive task of multi-class antibiotic selection, we demonstrate that a Quad-Modal Ensemble achieved the highest performance (0.72 AUC). These models are integrated into SepsisSuite, a deployment-ready Python framework for clinical decision support. SepsisSuite is available for free at: https://github.com/RyanCartularo/SepsisSuite-Info
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Bayesian latent class reinforcement learning framework to capture adaptive, feedback-driven travel behaviour</title>
<link>https://arxiv.org/abs/2512.14713</link>
<guid>https://arxiv.org/abs/2512.14713</guid>
<content:encoded><![CDATA[
<div> travel decisions, experience formation, heterogeneity, Latent Class Reinforcement Learning, Variational Bayes  

<br /><br />Summary:  
This paper addresses the role of experience formation in travel decisions, emphasizing that individuals learn and adapt their preferences over time. It highlights the significant heterogeneity among travelers, both in their baseline preferences and in how these preferences develop. To model these aspects, the authors propose a Latent Class Reinforcement Learning (LCRL) framework that captures preference evolution and individual differences simultaneously. The model is applied to data from a driving simulator experiment, providing a practical setting for parameter estimation. Variational Bayes is employed as the estimation technique, allowing for efficient inference within the complex model. Through estimation, three distinct traveler classes are identified: (1) individuals with context-dependent preferences who show exploitative behavior tailored to specific contexts, (2) individuals who persistently exploit a fixed strategy regardless of the context, and (3) individuals who adopt an exploratory approach while still maintaining context-sensitive preferences. These findings advance understanding of how different travelers form and adjust preferences and provide a valuable tool for analyzing dynamic decision-making in travel behavior. The study’s methodological contributions and insights have implications for personalized travel interventions and policy design. <div>
arXiv:2512.14713v1 Announce Type: new 
Abstract: Many travel decisions involve a degree of experience formation, where individuals learn their preferences over time. At the same time, there is extensive scope for heterogeneity across individual travellers, both in their underlying preferences and in how these evolve. The present paper puts forward a Latent Class Reinforcement Learning (LCRL) model that allows analysts to capture both of these phenomena. We apply the model to a driving simulator dataset and estimate the parameters through Variational Bayes. We identify three distinct classes of individuals that differ markedly in how they adapt their preferences: the first displays context-dependent preferences with context-specific exploitative tendencies; the second follows a persistent exploitative strategy regardless of context; and the third engages in an exploratory strategy combined with context-specific preferences.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
</channel>
</rss>